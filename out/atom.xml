<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-02T04:14:05.801597+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45781298</id><title>SQLite concurrency and why you should care about it</title><updated>2025-11-02T04:14:15.009334+00:00</updated><content>&lt;doc fingerprint="7b4e765eeca27f9a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SQLite concurrency and why you should care about it&lt;/head&gt;
    &lt;p&gt;SQLite is a powerful database engine, but due to its design, it has limitations that should not be overlooked.&lt;/p&gt;
    &lt;p&gt;Jellyfin has used a SQLite-based database for storing most of its data for years, but it has also encountered issues on many systems. In this blog post, I will explain how we address these limitations and how developers using SQLite can apply the same solutions.&lt;/p&gt;
    &lt;p&gt;This will be a technical blog post intended for developers and everyone wanting to learn about concurrency.&lt;/p&gt;
    &lt;p&gt;Also Jellyfin's implementation of locking for SQLite should be fairly easy to be implemented into another EF Core application if you are facing the same issue.&lt;/p&gt;
    &lt;p&gt;- JPVenson&lt;/p&gt;
    &lt;head rend="h2"&gt;The Premise&lt;/head&gt;
    &lt;p&gt;SQLite is a file-based database engine running within your application and allows you to store data in a relational structure. Overall it gives your application the means of storing structured data as a single file and without having to depend on another application to do so. Naturally this also comes at a price. If your application fully manages this file, the assumption must be made that your application is the sole owner of this file, and nobody else will tinker with it while you are writing data to it.&lt;/p&gt;
    &lt;p&gt;So an application that wants to use SQLite as its database needs to be the only one accessing it. Having established this fact, an important thought arises: if only a single write operation should be performed on a single file at a time, this rule must also apply to operations within the same application.&lt;/p&gt;
    &lt;head rend="h2"&gt;The W-A-L mode&lt;/head&gt;
    &lt;p&gt;SQLite has a feature that tries to get around this limitation: the Write-Ahead-Log (WAL). The WAL is a separate file that acts as a journal of operations that should be applied to an SQLite file. This allows multiple parallel writes to take place and get enqueued into the WAL. When another part of the application wants to read data, it reads from the actual database, then scans the WAL for modifications and applies them on the fly. This is not a foolproof solution; there are still scenarios where WAL does not prevent locking conflicts.&lt;/p&gt;
    &lt;head rend="h2"&gt;SQLite transactions&lt;/head&gt;
    &lt;p&gt;A transaction is supposed to ensure two things. Modifications made within a transaction can be reverted, either when something goes wrong or when the application decides it should and optionally a transaction may also block other readers from reading data that is modified within a transaction. This is where it gets spicy and we come to the real reason why I am writing this blog post. For some reason on some systems that run Jellyfin when a transaction takes place the SQLite engine reports the database is locked and instead of waiting for the transaction to be resolved the engine refuses to wait and just crashes. This seems to be a not uncommon issue and there are many reports to be found on the issue.&lt;/p&gt;
    &lt;p&gt;The factor that makes this issue so bad is that it does not happen reliably. So far we only have one team member where this can be (somewhat) reliably be reproduced which makes this an even worse a bug. From the reports this issue happens across all operating systems, drive speeds and with or without virtualization. So we do not have any deciding factor identified that even contributes to the likelihood of the issue happening.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Jellyfin factor&lt;/head&gt;
    &lt;p&gt;Having established the general theory on how SQLite behaves, we also have to look at the specifics of Jellyfins usage of SQLite. During normal operations on a recommended setup (Non-Networked Storage and preferably SSD) its unusual for any problems to arise, however the way Jellyfin utilises the SQLite db up to 10.11 is very suboptimal. In versions prior to 10.11 Jellyfin had a bug in its parallel task limit which resulted in exponential overscheduling of library scan operations which hammered the database engine with thousands of parallel write requests that an SQLite engine is simply not able to handle. While most SQLite engine implementations have retry behavior, they also have timeouts and checks in place to prevent limitless waiting so if we stress the engine enough, it just fails with an error. That and very long running and frankly unoptimized transactions could lead to the database just being overloaded with requests and flaking out.&lt;/p&gt;
    &lt;head rend="h2"&gt;The solution&lt;/head&gt;
    &lt;p&gt;Since we moved the codebase over to EF Core proper, we have the tools to actually do something about this as EF Core gives us a structured abstraction level. EF Core supports a way of hooking into every command execution or transaction by creating Interceptors. With an interceptor we can finally do the straight forward idea of just "not" writing to the database in parallel in a transparent way to the caller. The overall idea is to have multiple strategies of locking. Because all levels of synchronization will inevitably come at the cost of performance, we only want to do it when it is really necessary. So, I decided on three locking strategies:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;No-Lock&lt;/item&gt;
      &lt;item&gt;Optimistic locking&lt;/item&gt;
      &lt;item&gt;Pessimistic locking&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a default, the no-lock behavior does exactly what the name implies. Nothing. This is the default because my research shows that for 99% all of this is not an issue and every interaction at this level will slow down the whole application.&lt;/p&gt;
    &lt;p&gt;Both the optimistic and pessimistic behaviors use two interceptors—one for transactions and one for commands—and override &lt;code&gt;SaveChanges&lt;/code&gt; in &lt;code&gt;JellyfinDbContext&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Optimistic locking behavior&lt;/head&gt;
    &lt;p&gt;Optimistic locking means to assume the operation in question will succeed and only handle issues afterwards. In essence this can be boiled down to "Try and Retry and Retry ..." for a set number of times until either we succeed with the operation or fail entirely. This still leaves the possibility that we will not actually be able to perform a write, but the introduced overhead is far less than the Pessimistic locking behavior.&lt;/p&gt;
    &lt;p&gt;The idea behind how this works is simple: every time two operations try to write to the database, one will always win. The other will fail, wait some time, then retry a few times.&lt;/p&gt;
    &lt;p&gt;Jellyfin uses the &lt;code&gt;Polly&lt;/code&gt; library perform the retry behavior and will only retry operations it will find have been locked due to this exact issue.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pessimistic locking behavior&lt;/head&gt;
    &lt;p&gt;Pessimistic locking always locks when a write to SQLite should be performed. Essentially every time an transaction is started or a write operation on the database is done though EF Core, Jellyfin will wait until all other read operations are finished and then block all other operations may they be read or write until the write in question has been performed. This however means, that Jellyfin can only ever perform a single write to the database, even if it would technically does not need to.&lt;/p&gt;
    &lt;p&gt;In theory, an application should have no issue reading from table "Alice" while writing to table "Bob" however to eliminate all possible sources of concurrency related locking, Jellyfin will only ever allow a single write performed on its database in this mode. While this will absolutely result in the most stable operation, it will undoubtedly also be the slowest.&lt;/p&gt;
    &lt;p&gt;Jellyfin uses a ReaderWriterLockSlim to lock the operations, that means we allow an unlimited number of reads to happen concurrently while only one write may ever be done on the database.&lt;/p&gt;
    &lt;head rend="h3"&gt;The future Smart locking behavior&lt;/head&gt;
    &lt;p&gt;In the future we might also consider combining both modes, to get the best of both worlds.&lt;/p&gt;
    &lt;head rend="h1"&gt;The result&lt;/head&gt;
    &lt;p&gt;Initial testing showed that with both modes, we had great success in handling the underlying issue. While we are not yet sure why this happens only on some systems when others work, we at least now have an option for users previously left out of using Jellyfin.&lt;/p&gt;
    &lt;p&gt;When I was researching this topic, I found many reports all over the internet facing the same error but nobody was able to provide a conclusive explanation whats really happening here. There have been similar proposals made to handle it but there wasn't a "ready to drop in" solution that handles all the different cases or only code that required massive modifications to every EF Core query. Jellyfin's implementation of the locking behaviors should be a copy-paste solution for everyone having the same issues as its using interceptors and the caller has no idea of the actual locking behavior.&lt;/p&gt;
    &lt;p&gt;Best of luck,&lt;/p&gt;
    &lt;p&gt;- JPVenson&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jellyfin.org/posts/SQLite-locking/"/><published>2025-11-01T12:59:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45781397</id><title>CharlotteOS – An Experimental Modern Operating System</title><updated>2025-11-02T04:14:14.423227+00:00</updated><content>&lt;doc fingerprint="25a44afe193d139f"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;catten&lt;/code&gt; is an operating system kernel developed as a key component of the CharlotteOS project but it is designed to be flexible enough that we hope it can also find use in many other places. It seeks to be a monolithic kernel with low-level system call interfaces that borrows ideas from exokernels and other novel systems like Plan 9 and Fuchsia. Its design allows for almost any higher level interface to be layered on top and also includes a typesafe system namespace (akin to the namespaces found in Fuschsia and Plan 9 but more flexible and typesafe) with URIs as paths which has the added benefit of allowing access to the namespace of another host over a network without having to mount anything locally all while being secured by granular capabilities and a persistent mandatory access control policy.&lt;/p&gt;
    &lt;p&gt;catten is still in early development, and core subsystems are actively being built. We welcome contributions—feel free to grab an issue from the tracker, suggest features, or participate in discussions on our repository, Discord server or Matrix instance.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;catten&lt;/code&gt;is written in Rust and ISA specific assembly languages&lt;/item&gt;
      &lt;item&gt;x86_64 assembly should use Intel syntax as implemented by &lt;code&gt;rustc&lt;/code&gt;and&lt;code&gt;llvm-mc&lt;/code&gt;exclusively&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;C language dependencies are allowed if vetted by maintainers.&lt;/item&gt;
      &lt;item&gt;Any dependencies in languages other than Rust, C, and assembly are strictly forbidden.&lt;/item&gt;
      &lt;item&gt;Always prefer a high-quality Rust equivalent over an external C library unless there is good reason to do otherwise&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Processor: &lt;list rend="ul"&gt;&lt;item&gt;x86_64 (Primary ISA) &lt;list rend="ul"&gt;&lt;item&gt;x2APIC LAPIC operating mode&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;x86_64 (Primary ISA) &lt;/item&gt;
      &lt;item&gt;Firmware: &lt;list rend="ul"&gt;&lt;item&gt;Unified Extensible Firmware Interface (UEFI)&lt;/item&gt;&lt;item&gt;Advanced Configuration and Power Interface (ACPI)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Memory: &lt;list rend="ul"&gt;&lt;item&gt;Recommended: &amp;gt;= 1 GiB&lt;/item&gt;&lt;item&gt;Required: 128 MiB&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Storage: &lt;list rend="ul"&gt;&lt;item&gt;Recommended: &amp;gt;= 64 GiB&lt;/item&gt;&lt;item&gt;Required: 4 GiB&lt;/item&gt;&lt;item&gt;Device Types: &lt;list rend="ul"&gt;&lt;item&gt;Non-Volatile Memory Express (NVMe)&lt;/item&gt;&lt;item&gt;USB Mass Storage Device Class&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Output: &lt;list rend="ul"&gt;&lt;item&gt;Display Adapter: Any adapter capable of providing framebuffers via the UEFI Graphics Output Protocol&lt;/item&gt;&lt;item&gt;Serial: &lt;list rend="ul"&gt;&lt;item&gt;NS16550 compatible UART&lt;/item&gt;&lt;item&gt;USB CDC ACM (Virtual UART)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Input: &lt;list rend="ul"&gt;&lt;item&gt;Keyboard &lt;list rend="ul"&gt;&lt;item&gt;PS/2&lt;/item&gt;&lt;item&gt;USB HID&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Serial &lt;list rend="ul"&gt;&lt;item&gt;NS16550 compatible UART&lt;/item&gt;&lt;item&gt;USB CDC ACM (Virtual UART)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Keyboard &lt;/item&gt;
      &lt;item&gt;Networking: &lt;list rend="ul"&gt;&lt;item&gt;USB CDC Network Control Model&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please reach out to us on Matrix or Discord if you are interested in contributing.&lt;/p&gt;
    &lt;p&gt;This kernel is licensed under the GNU General Public License version 3.0 (or at your option, any later version). By contributing to this project you agree to license your contributions under those same terms exclusively.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/charlotte-os/Catten"/><published>2025-11-01T13:12:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45782136</id><title>Updated practice for review articles and position papers in ArXiv CS category</title><updated>2025-11-02T04:14:13.797746+00:00</updated><content>&lt;doc fingerprint="ff55b99b0494c981"&gt;
  &lt;main&gt;
    &lt;p&gt;arXiv’s computer science (CS) category has updated its moderation practice with respect to review (or survey) articles and position papers. Before being considered for submission to arXiv’s CS category, review articles and position papers must now be accepted at a journal or a conference and complete successful peer review. When submitting review articles or position papers, authors must include documentation of successful peer review to receive full consideration. Review/survey articles or position papers submitted to arXiv without this documentation will be likely to be rejected and not appear on arXiv.&lt;lb/&gt; This change is being implemented due to the unmanageable influx of review articles and position papers to arXiv CS.&lt;/p&gt;
    &lt;p&gt;Is this a policy change?&lt;/p&gt;
    &lt;p&gt;Technically, no! If you take a look at arXiv’s policies for specific content types you’ll notice that review articles and position papers are not (and have never been) listed as part of the accepted content types. Review articles and position papers have, in the past, only been accepted at moderator discretion, because the few we received were of high quality and of interest to arXiv readers and the scientific community at large.&lt;/p&gt;
    &lt;p&gt;Why is the arXiv CS category making this change?&lt;/p&gt;
    &lt;p&gt;In the past few years, arXiv has been flooded with papers. Generative AI / large language models have added to this flood by making papers – especially papers not introducing new research results – fast and easy to write. While categories across arXiv have all seen a major increase in submissions, it’s particularly pronounced in arXiv’s CS category.&lt;/p&gt;
    &lt;p&gt;The goal of this change of practice is to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Help arXiv readers more easily find valuable review articles and position papers written by subject matter experts&lt;/item&gt;
      &lt;item&gt;Free up moderators to focus on the content types officially accepted by arXiv, reduce submission hold times, and keep the pace of scientific discovery going!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Above all, the core purpose of arXiv is to share research papers and facilitate scientific discovery quickly and freely. We are making this change in support of that mission.&lt;/p&gt;
    &lt;p&gt;In the past, arXiv CS received a relatively small amount of review or survey articles, and those we did receive were of extremely high quality, written by senior researchers at the request of publications like Annual Reviews, Proceedings of the IEEE, and Computing Surveys. Position paper submissions to arXiv were similarly rare, and usually produced by scientific societies or government study groups (for example,the Computing Research Association of the National Academies of Science, Engineering, and Medicine). While, as now, these papers were not content types officially accepted by arXiv, the arXiv moderators accepted them because of their scholarly value to the research community.&lt;/p&gt;
    &lt;p&gt;Fast forward to present day – submissions to arXiv in general have risen dramatically, and we now receive hundreds of review articles every month. The advent of large language models have made this type of content relatively easy to churn out on demand, and the majority of the review articles we receive are little more than annotated bibliographies, with no substantial discussion of open research issues.&lt;/p&gt;
    &lt;p&gt;arXiv believes that there are position papers and review articles that are of value to the scientific community, and we would like to be able to share them on arXiv. However, our team of volunteer moderators do not have the time or bandwidth to review the hundreds of these articles we receive without taking time away from our core purpose, which is to share research articles.&lt;/p&gt;
    &lt;p&gt;Reasonable and trusted outside refereed venues already exist (conferences and journals) which solicit position papers and review articles on subjects of concern or interest to our readers (such as concerns over privacy, ethics, safety, and security of recent CS technologies, particularly applications of artificial intelligence) and as part of that process, they conduct in-depth review to assure quality, evidential support of opinions, and scholarly value. Since arXiv does not have the resources to conduct this quality-control in-house for content types that we do not officially accept, this change of practice is allowing us to rely on these refereed venues to do so for us so that we can still share position papers and review articles of value on arXiv.&lt;/p&gt;
    &lt;p&gt;How do I submit my review article or position paper to arXiv? Before submission to arXiv, have your review article or position paper accepted to a refereed venue with peer review like a journal or a conference. Review articles or position papers must be accepted to a journal or conference before being submitted to arXiv and you must have documentation of complete and successful peer review.&lt;/p&gt;
    &lt;p&gt;Please note: the review conducted at conference workshops generally does not meet the same standard of rigor of traditional peer review and is not enough to have your review article or position paper accepted to arXiv.&lt;/p&gt;
    &lt;p&gt;How do I show my review article or position paper has successfully completed peer review? When you submit to arXiv, please include the peer reviewed journal reference and DOI metadata. If you do not provide this, your review article or position paper will likely be rejected.&lt;/p&gt;
    &lt;p&gt;Can I resubmit my position paper or review article after being rejected? If your position paper or review article was rejected because it did not complete a successful peer review process, you can submit an appeal request to resubmit if your article has since completed a successful peer review process. Do not resubmit your position paper or review article without an accepted appeal. Here are the instructions for how to appeal.&lt;/p&gt;
    &lt;p&gt;I have a scientific paper studying the impact of science and technology in society. Can I submit this to arXiv without peer review? Yes, arXiv has always released these types of scientific papers, for example in cs.CY or physics.soc-ph. These are scientific research papers and are not subject to this moderation practice change.&lt;/p&gt;
    &lt;p&gt;Will other categories on arXiv also change their practice re: review articles and position papers? Each category of arXiv has different moderators, who are subject matter experts with a terminal degree in their particular subject, to best serve the scholarly pursuits, goals, and standards of their category. While all moderators adhere to arXiv policy, the only policy arXiv has in place with regard to review articles and position papers is that they are not a generally accepted content type. The goal of the moderators of each category is to make sure the work being submitted is actually science, and that it is of potential interest to the scientific community. If other categories see a similar rise in LLM-written review articles and position papers, they may choose to change their moderation practices in a similar manner to better serve arXiv authors and readers. We will make these updates public if and when they do occur.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/"/><published>2025-11-01T14:58:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45782981</id><title>GHC now runs in the browser</title><updated>2025-11-02T04:14:13.498713+00:00</updated><content>&lt;doc fingerprint="4f3a90df9d363b44"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;ghc itself can now run purely client-side in the browser, here’s a haskell playground demo. terms and conditions apply, and i’ll write up more detailed explanation some time later, but i thought this is a cool thing to show off how far the ghc wasm backend has advanced &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 81 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;jaror
2&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;This is very cool! I wonder how easy it would be to load some packages; cabal in the browser when? I’m also wondering how usable Agda in the browser would be.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;jaror
3&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;I think I found a bug: ghc options persist even after I change them. Edit: this has been fixed!&lt;/p&gt;
        &lt;p&gt;Also &lt;code&gt;-with-rtsopts=-s&lt;/code&gt; does not work, sadly. Edit: Ah, that’s because it is interpreted.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;ad-si
4&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;This is awesome!&lt;lb/&gt; Perfect for building a fully interactive Haskell online course! &lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Excellent work - the efforts to bring Haskell to WASM are a huge boon to our ecosystem and userbase!&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Can’t run it on my tablet (wasm), curious: is this running the type checker or also code gen to wasm?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;jaror
8&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;It runs the code, but it seems like it uses the bytecode interpreter.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;What modifications were to GHC for it to be compiled to WASM?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;cabal won’t work in the browser due to lack of process support; but it’s possible to use &lt;code&gt;wasm32-wasi-cabal&lt;/code&gt; to precompile some third party packages to wasm and make this playground support them as well.&lt;/p&gt;
        &lt;p&gt;you might be interested to check GitHub - agda-web/agda-wasm-dist: Distributions of Agda executable compiled into WebAssembly.; afaik they even compiled GitHub - agda/agda-language-server: Language Server for Agda to wasm, not sure how usable it is currently&lt;/p&gt;
        &lt;p&gt;thanks for the report! i pushed an update which should have fixed it.&lt;/p&gt;
        &lt;p&gt;that’s right; ghc in browser can’t invoke the c compiler and it can only interpret haskell modules via bytecode.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;thanks for the reports! a few seconds of freeze during start-time is expected, since it needs to download ~50M of a rootfs tarball and extract it, then link the ghc library and all its dependencies. as for safari, it’s strange since i i just landed a workaround for a webkit bug that breaks the wasm dynamic linker a few days ago, i’ll take a closer look later.&lt;/p&gt;
        &lt;p&gt;the ghc library already mostly works when compiled to wasm, and it can parse/typecheck/desugar stuff. the bottleneck is the linker/loader part, for it to be useful it needs to be able to dynamically load and execute haskell code. i landed a couple of ghc patches recently to push towards that direction, and the last one that gets us towards the haskell playground (not landed yet) is Draft: Support running GHC fully client-side in the browser (!15000) · Merge requests · Glasgow Haskell Compiler / GHC · GitLab&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 8 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Which packages are installed by default?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;now chrome will consume even more memory &lt;/p&gt;
        &lt;p&gt;Awesome work!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;the &lt;code&gt;ghc&lt;/code&gt; library and its transitive dependencies.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;jarm
15&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Can you elaborate? Do you have any end-to-end examples/instructions for this?&lt;/p&gt;
        &lt;p&gt;I am a live coding musician and have been trying to get Tidal running on the web for years: tidal: Pattern language for improvised music&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;see also:&lt;/p&gt;
        &lt;p&gt; which I presume isn’t wasm&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://discourse.haskell.org/t/ghc-now-runs-in-your-browser/13169"/><published>2025-11-01T16:29:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45783114</id><title>Chat Control proposal fails again after public opposition</title><updated>2025-11-02T04:14:13.201837+00:00</updated><content>&lt;doc fingerprint="b70c19598a0da651"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Chat control proposal fails again after massive public opposition&lt;/head&gt;
    &lt;p&gt;The European Union Council has once again retreated from its controversial Chat Control proposal, a plan that would have required widespread scanning of encrypted messages. The withdrawal by the current Danish presidency represents yet another chapter in a long-running battle between privacy advocates and lawmakers who believe they can compromise encryption in the name of public safety. While this latest defeat is a victory for digital rights, the fight is far from over, and the fundamental misunderstanding of encryption technology continues to plague policy discussions across Europe.&lt;/p&gt;
    &lt;head rend="h2"&gt;A zombie proposal that refuses to die&lt;/head&gt;
    &lt;p&gt;Since its introduction in 2022, Chat Control has become what privacy advocates call a zombie proposal, repeatedly resurrected despite consistent opposition from civil society, technical experts, and the public. The Electronic Frontier Foundation and more than 80 civil society organizations have strongly opposed the legislation, which would mandate client-side scanning of encrypted communications under the guise of combating child sexual abuse material.&lt;/p&gt;
    &lt;p&gt;The pattern has become predictable. EU lawmakers introduce the proposal, claiming it includes safeguards for privacy. Technical experts explain why those safeguards are illusory. Public pressure mounts. The proposal is withdrawn or modified. Then, after a brief hiatus, it returns with minor tweaks, and the cycle begins anew. This latest withdrawal by the Danish presidency follows the same script, but the underlying issues remain unresolved.&lt;/p&gt;
    &lt;p&gt;What makes this particularly frustrating is that the fundamental problem with Chat Control has never been addressed. The proposal seeks to create what privacy experts call a “backdoor” into encryption, allowing authorities to scan messages before they’re encrypted or after they’re decrypted. Proponents argue this preserves encryption while enabling content moderation, but this reveals a dangerous misunderstanding of how encryption actually works. Creating any mechanism to access encrypted content inherently weakens the entire system, making it vulnerable not just to authorized access but to malicious actors as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;The technical impossibility of “safe” scanning&lt;/head&gt;
    &lt;p&gt;The core issue with Chat Control and similar proposals lies in a fundamental misunderstanding of encryption technology. End-to-end encryption works because only the sender and recipient possess the keys to decrypt messages. Any third party, whether a government agency or a tech company, cannot read the contents. This is not a design choice but a mathematical certainty that ensures the security of billions of communications daily.&lt;/p&gt;
    &lt;p&gt;Client-side scanning, the technical approach favored by Chat Control advocates, attempts to circumvent this limitation by analyzing messages on users’ devices before encryption or after decryption. While this might sound like a clever workaround, it fundamentally breaks the security model of encryption. If a device can scan and report on message content, so can malware, hackers, or authoritarian governments who might compel tech companies to expand the scope of scanning.&lt;/p&gt;
    &lt;p&gt;Security researchers have repeatedly demonstrated that there is no way to create a scanning system that only works for “good guys.” Apple learned this lesson the hard way in 2021 when it proposed a similar system for detecting child abuse imagery in iCloud photos. The backlash from security experts was swift and devastating, forcing the company to abandon the plan. The same security vulnerabilities that would enable Chat Control would inevitably be exploited by malicious actors, putting everyone at greater risk.&lt;/p&gt;
    &lt;p&gt;Moreover, the scope creep inherent in surveillance technologies is well documented. A system initially designed to detect illegal content could easily be expanded to monitor political dissent, religious expression, or any other communication governments deem problematic. Countries around the world are watching the EU’s actions closely. If Chat Control were to pass, it would set a dangerous precedent that authoritarian regimes would eagerly exploit, claiming they’re simply following Europe’s lead in implementing “reasonable” content moderation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Public pressure and the power of resistance&lt;/head&gt;
    &lt;p&gt;The withdrawal of Chat Control demonstrates the critical importance of sustained public engagement in technology policy. Unlike previous instances where technical proposals sailed through legislative processes with little public awareness, this fight has been characterized by unprecedented mobilization from civil society organizations, technology companies, security researchers, and ordinary citizens concerned about their digital rights.&lt;/p&gt;
    &lt;p&gt;Organizations like the Electronic Frontier Foundation, European Digital Rights, and numerous national privacy advocacy groups have played a crucial role in educating the public about the risks of Chat Control. Their efforts have included detailed technical explanations, legal analysis, and coordination of opposition campaigns that have reached millions of Europeans. This groundswell of opposition has made it politically toxic for lawmakers to support the proposal, at least in its current form.&lt;/p&gt;
    &lt;p&gt;The effectiveness of this resistance offers important lessons for future policy battles. First, technical expertise matters. When security researchers speak with a unified voice about the impossibility of safe backdoors, it becomes harder for politicians to dismiss concerns as alarmist. Second, coalition-building across different sectors strengthens opposition. When civil liberties groups, tech companies, and individual users all oppose a policy, it suggests the problems are real and widespread. Third, sustained pressure is essential because, as Chat Control demonstrates, bad proposals rarely die on the first attempt.&lt;/p&gt;
    &lt;p&gt;However, this victory should be tempered with realism. The forces pushing for Chat Control have not given up, and the underlying political dynamics that gave rise to the proposal remain unchanged. Politicians face genuine pressure to be seen as “doing something” about online harms, particularly regarding child safety. Until alternative approaches that don’t compromise encryption gain political traction, proposals like Chat Control will continue to resurface.&lt;/p&gt;
    &lt;head rend="h2"&gt;The path forward requires education and alternatives&lt;/head&gt;
    &lt;p&gt;The repeated resurrection of Chat Control points to a deeper problem in how technology policy is made. Many lawmakers genuinely believe they can have both strong encryption and government access to encrypted content. This belief persists despite unanimous opposition from the cryptographic community because the political incentives favor appearing tough on crime over understanding complex technical realities.&lt;/p&gt;
    &lt;p&gt;Breaking this cycle requires a fundamental shift in how we approach online safety. Rather than seeking technological magic bullets that promise security without trade-offs, policymakers need to invest in solutions that actually work. This includes better funding for law enforcement training and tools that don’t require breaking encryption, improved international cooperation on criminal investigations, and addressing the root causes of online exploitation through social programs and education.&lt;/p&gt;
    &lt;p&gt;Technology companies also bear responsibility for developing and promoting genuinely privacy-preserving safety features. End-to-end encrypted platforms can implement abuse prevention measures that don’t involve content scanning, such as metadata analysis, user reporting systems, and account-level restrictions for suspicious behavior. While these approaches may be less comprehensive than mass surveillance, they achieve meaningful safety improvements without the catastrophic privacy trade-offs of backdoors.&lt;/p&gt;
    &lt;p&gt;Looking ahead, the privacy community cannot simply celebrate the withdrawal of Chat Control and move on. The next presidency of the EU Council will bring new opportunities for the proposal to resurface in yet another modified form. Sustained vigilance, continued public education, and proactive development of alternative safety measures will be essential. The fight to protect encryption is not a single battle but an ongoing campaign that requires long-term commitment from everyone who values digital privacy and security.&lt;/p&gt;
    &lt;p&gt;The withdrawal of Chat Control is a victory, but it’s a temporary one. The fundamental challenge remains: convincing policymakers that some trade-offs are not worth making, and that breaking encryption to combat illegal content creates far more problems than it solves. Until that message truly sinks in, the zombie proposal will keep rising from the grave, and the privacy community must remain ready to defeat it again and again.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://andreafortuna.org/2025/11/01/chat-control-proposal-fails-again-after-massive-public-opposition/"/><published>2025-11-01T16:42:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45783640</id><title>Show HN: Why write code if the LLM can just do the thing? (web app experiment)</title><updated>2025-11-02T04:14:12.696036+00:00</updated><content>&lt;doc fingerprint="a15b850a73e66ead"&gt;
  &lt;main&gt;
    &lt;p&gt;A web server with no application logic. Just an LLM with three tools.&lt;/p&gt;
    &lt;p&gt;One day we won't need code. LLMs will output video at 120fps, sample inputs in realtime, and just... be our computers. No apps, no code, just intent and execution.&lt;/p&gt;
    &lt;p&gt;That's science fiction.&lt;/p&gt;
    &lt;p&gt;But I got curious: with a few hours this weekend and today's level of tech, how far can we get?&lt;/p&gt;
    &lt;p&gt;I expected this to fail spectacularly.&lt;/p&gt;
    &lt;p&gt;Everyone's focused on AI that writes code. You know the usual suspects, Claude Code, Cursor, Copilot, all that. But that felt like missing the bigger picture. So I built something to test a different question: what if you skip code generation entirely? A web server with zero application code. No routes, no controllers, no business logic. Just an HTTP server that asks an LLM "what should I do?" for every request.&lt;/p&gt;
    &lt;p&gt;The goal: prove how far away we really are from that future.&lt;/p&gt;
    &lt;p&gt;Contact manager. Basic CRUD: forms, database, list views, persistence.&lt;/p&gt;
    &lt;p&gt;Why? Because most software is just CRUD dressed up differently. If this works at all, it would be something.&lt;/p&gt;
    &lt;code&gt;// The entire backend
const result = await generateText({
  model,
  tools: {
    database,      // Run SQL queries
    webResponse,   // Return HTML/JSON
    updateMemory   // Save user feedback
  },
  prompt: `Handle this HTTP request: ${method} ${path}`,
});&lt;/code&gt;
    &lt;p&gt;Three tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;database&lt;/code&gt;- Execute SQL on SQLite. AI designs the schema.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;webResponse&lt;/code&gt;- Return any HTTP response. AI generates the HTML, JavaScript, JSON or whatever fits.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;updateMemory&lt;/code&gt;- Persist feedback to markdown. AI reads it on next request.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The AI infers what to return from the path alone. Hit &lt;code&gt;/contacts&lt;/code&gt; and you get an HTML page. Hit &lt;code&gt;/api/contacts&lt;/code&gt; and you get JSON:&lt;/p&gt;
    &lt;code&gt;// What the AI generates for /api/contacts
{
  "contacts": [
    { "id": 1, "name": "Alice", "email": "alice@example.com" },
    { "id": 2, "name": "Bob", "email": "bob@example.com" }
  ]
}&lt;/code&gt;
    &lt;p&gt;Every page has a feedback widget. Users type "make buttons bigger" or "use dark theme" and the AI implements it.&lt;/p&gt;
    &lt;p&gt;It works. That's annoying.&lt;/p&gt;
    &lt;p&gt;Every click or form submission took 30-60 seconds. Traditional web apps respond in 10-100 milliseconds. That's 300-6000x slower. Each request cost $0.01-0.05 in API tokens—100-1000x more expensive than traditional compute. The AI spent 75-85% of its time reasoning, forgot what UI it generated 5 seconds ago, and when it hallucinated broken SQL that was an immediate 500 error. Colors drifted between requests. Layouts changed. I tried prompt engineering tricks like "⚡ THINK QUICKLY" and it made things slower because the model spent more time reasoning about how to be fast.&lt;/p&gt;
    &lt;p&gt;But despite all that, forms actually submitted correctly. Data persisted across restarts. The UI was usable. APIs returned valid JSON. User feedback got implemented. The AI invented, without any examples, sensible database schemas with proper types and indexes, parameterized SQL queries that were safe from injection, REST-ish API conventions, responsive Bootstrap layouts, form validation, and error handling for edge cases. All emergent behavior from giving it three tools and a prompt.&lt;/p&gt;
    &lt;p&gt;So yes, the capability exists. The AI can handle application logic. It's just catastrophically slow, absurdly expensive, and has the memory of a goldfish.&lt;/p&gt;
    &lt;p&gt;The capability exists. The AI can handle application logic.&lt;/p&gt;
    &lt;p&gt;The problems are all performance: speed (300-6000x slower), cost (100-1000x more expensive), consistency (no design memory), reliability (hallucinations → errors).&lt;/p&gt;
    &lt;p&gt;But these feel like problems of degree, not kind:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inference: improving ~10x/year&lt;/item&gt;
      &lt;item&gt;Cost: heading toward zero&lt;/item&gt;
      &lt;item&gt;Context: growing (eventual design memory?)&lt;/item&gt;
      &lt;item&gt;Errors: dropping&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the fact that I built a working CRUD app with zero application code, despite it being slow and expensive, suggests we might be closer to "AI just does the thing" than "AI helps write code."&lt;/p&gt;
    &lt;p&gt;In this project, what's left is infrastructure: HTTP setup, tool definitions, database connections. The application logic is gone. But the real vision? 120 inferences per second rendering displays with constant realtime input sampling. That becomes the computer. No HTTP servers, no databases, no infrastructure layer at all. Just intent and execution.&lt;/p&gt;
    &lt;p&gt;I think we don't realize how much code, as a thing, is mostly transitional.&lt;/p&gt;
    &lt;code&gt;npm install&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;.env&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-haiku-20240307&lt;/code&gt;
    &lt;code&gt;npm start&lt;/code&gt;
    &lt;p&gt;Visit &lt;code&gt;http://localhost:3001&lt;/code&gt;. First request: 30-60s.&lt;/p&gt;
    &lt;p&gt;What to try:&lt;/p&gt;
    &lt;p&gt;Check out &lt;code&gt;prompt.md&lt;/code&gt; and customize it. Change what app it builds, add features, modify the behavior. That's the whole interface.&lt;/p&gt;
    &lt;p&gt;Out of the box it builds a contact manager. But try:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/game&lt;/code&gt;- Maybe you get a game?&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/dashboard&lt;/code&gt;- Could be anything&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/api/stats&lt;/code&gt;- Might invent an API&lt;/item&gt;
      &lt;item&gt;Type feedback: "make this purple" or "add a search box"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/samrolken/nokode"/><published>2025-11-01T17:45:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45783699</id><title>Visible from space, Sudan's bloodied sands expose a massacre of thousands</title><updated>2025-11-02T04:14:12.454001+00:00</updated><content>&lt;doc fingerprint="af14cd1f0e004f24"&gt;
  &lt;main&gt;
    &lt;p&gt;The hot sand around the Sudanese city of El Fasher is stained red with the blood of more than 2,000 massacred civilians.&lt;/p&gt;
    &lt;p&gt;The pools of blood are so thick, the piles of bodies so exposed, that the ethnic purge allegedly committed by Sudanese paramilitary rebels is visible from space.&lt;/p&gt;
    &lt;p&gt;Militia groups defending the city alongside the army alleged the Rapid Support Forces (RSF) rebel group “committed heinous crimes against innocent civilians” and said most of the dead were women, children and the elderly.&lt;/p&gt;
    &lt;p&gt;A video purported to show a child soldier murdering a grown man in cold blood. Another supposedly showed RSF fighters executing civilians moments after pretending to release them.&lt;/p&gt;
    &lt;p&gt;The total death toll could not immediately be confirmed, but satellite pictures taken after the city fell over the weekend following an 18-month siege showed evidence of mass killings.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.telegraph.co.uk/world-news/2025/10/28/sudan-bloodied-sands-massacre-thousands/"/><published>2025-11-01T17:50:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45784179</id><title>Claude Code can debug low-level cryptography</title><updated>2025-11-02T04:14:12.057015+00:00</updated><content>&lt;doc fingerprint="c6650996270f1fd4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Claude Code Can Debug Low-level Cryptography&lt;/head&gt;
    &lt;p&gt;Over the past few days I wrote a new Go implementation of ML-DSA, a post-quantum signature algorithm specified by NIST last summer. I livecoded it all over four days, finishing it on Thursday evening. Except… Verify was always rejecting valid signatures.&lt;/p&gt;
    &lt;code&gt;$ bin/go test crypto/internal/fips140/mldsa
--- FAIL: TestVector (0.00s)
    mldsa_test.go:47: Verify: mldsa: invalid signature
    mldsa_test.go:84: Verify: mldsa: invalid signature
    mldsa_test.go:121: Verify: mldsa: invalid signature
FAIL
FAIL     crypto/internal/fips140/mldsa   2.142s
FAIL
&lt;/code&gt;
    &lt;p&gt;I was exhausted, so I tried debugging for half an hour and then gave up, with the intention of coming back to it the next day with a fresh mind.&lt;/p&gt;
    &lt;p&gt;On a whim, I figured I would let Claude Code take a shot while I read emails and resurfaced from hyperfocus. I mostly expected it to flail in some maybe-interesting way, or rule out some issues.&lt;/p&gt;
    &lt;p&gt;Instead, it rapidly figured out a fairly complex low-level bug in my implementation of a relatively novel cryptography algorithm. I am sharing this because it made me realize I still don’t have a good intuition for when to invoke AI tools, and because I think it’s a fantastic case study for anyone who’s still skeptical about their usefulness.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Full disclosure: Anthropic gave me a few months of Claude Max for free. They reached out one day and told me they were giving it away to some open source maintainers. Maybe it’s a ploy to get me hooked so I’ll pay for it when the free coupon expires. Maybe they hoped I’d write something like this. Maybe they are just nice. Anyway, they made no request or suggestion to write anything public about Claude Code. Now you know.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Finding the bug&lt;/head&gt;
    &lt;p&gt;I started Claude Code v2.0.28 with Opus 4.1 and no system prompts, and gave it the following prompt (typos included):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I implemented ML-DSA in the Go standard library, and it all works except that verification always rejects the signatures. I know the signatures are right because they match the test vector.&lt;/p&gt;
      &lt;p&gt;YOu can run the tests with “bin/go test crypto/internal/fips140/mldsa”&lt;/p&gt;
      &lt;p&gt;You can find the code in src/crypto/internal/fips140/mldsa&lt;/p&gt;
      &lt;p&gt;Look for potential reasons the signatures don’t verify. ultrathink&lt;/p&gt;
      &lt;p&gt;I spot-checked and w1 is different from the signing one.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;To my surprise, it pinged me a few minutes later with a complete fix.&lt;/p&gt;
    &lt;p&gt;Maybe I shouldn’t be surprised! Maybe it would have been clear to anyone more familiar with AI tools that this was a good AI task: a well-scoped issue with failing tests. On the other hand, this is a low-level issue in a fresh implementation of a complex, relatively novel algorithm.&lt;/p&gt;
    &lt;p&gt;It figured out that I had merged &lt;code&gt;HighBits&lt;/code&gt; and &lt;code&gt;w1Encode&lt;/code&gt; into a single function for using it from Sign, and then reused it from Verify where &lt;code&gt;UseHint&lt;/code&gt; already produces the high bits, effectively taking the high bits of w1 twice in Verify.&lt;/p&gt;
    &lt;p&gt;Looking at the log, it loaded the implementation into the context and then immediately figured it out, without any exploratory tool use! After that it wrote itself a cute little test that reimplemented half of verification to confirm the hypothesis, wrote a mediocre fix, and checked the tests pass.&lt;/p&gt;
    &lt;p&gt;I threw the fix away and refactored &lt;code&gt;w1Encode&lt;/code&gt; to take high bits as input, and changed the type of the high bits, which is both clearer and saves a round-trip through Montgomery representation. Still, this 100% saved me a bunch of debugging time.&lt;/p&gt;
    &lt;head rend="h2"&gt;A second synthetic experiment&lt;/head&gt;
    &lt;p&gt;On Monday, I had also finished implementing signing with failing tests. There were two bugs, which I fixed in the following couple evenings.&lt;/p&gt;
    &lt;p&gt;The first one was due to somehow computing a couple hardcoded constants (1 and -1 in the Montgomery domain) wrong. It was very hard to find, requiring a lot of deep printfs and guesswork. Took me maybe an hour or two.&lt;/p&gt;
    &lt;p&gt;The second one was easier: a value that ends up encoded in the signature was too short (32 bits instead of 32 bytes). It was relatively easy to tell because only the first four bytes of the signature were the same, and then the signature lengths were different.&lt;/p&gt;
    &lt;p&gt;I figured these would be an interesting way to validate Claude’s ability to help find bugs in low-level cryptography code, so I checked out the old version of the change with the bugs (yay Jujutsu!) and kicked off a fresh Claude Code session with this prompt:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I am implementing ML-DSA in the Go standard library, and I just finished implementing signing, but running the tests against a known good test vector it looks like it goes into an infinite loop, probably because it always rejects in the Fiat-Shamir with Aborts loop.&lt;/p&gt;
      &lt;p&gt;You can run the tests with “bin/go test crypto/internal/fips140/mldsa”&lt;/p&gt;
      &lt;p&gt;You can find the code in src/crypto/internal/fips140/mldsa&lt;/p&gt;
      &lt;p&gt;Figure out why it loops forever, and get the tests to pass. ultrathink&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It spent some time doing printf debugging and chasing down incorrect values very similarly to how I did it, and then figured out and fixed the wrong constants. Took Claude definitely less than it took me. Impressive.&lt;/p&gt;
    &lt;p&gt;It gave up after fixing that bug even if the tests still failed, so I started a fresh session (on the assumption that the context on the wrong constants would do more harm than good investigating an independent bug), and gave it this prompt:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I am implementing ML-DSA in the Go standard library, and I just finished implementing signing, but running the tests against a known good test vector they don’t match.&lt;/p&gt;
      &lt;p&gt;You can run the tests with “bin/go test crypto/internal/fips140/mldsa”&lt;/p&gt;
      &lt;p&gt;You can find the code in src/crypto/internal/fips140/mldsa&lt;/p&gt;
      &lt;p&gt;Figure out what is going on. ultrathink&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It took a couple wrong paths, thought for quite a bit longer, and then found this one too. I honestly expected it to fail initially.&lt;/p&gt;
    &lt;p&gt;It’s interesting how Claude found the “easier” bug more difficult. My guess is that maybe the large random-looking outputs of the failing tests did not play well with its attention.&lt;/p&gt;
    &lt;p&gt;The fix it proposed was updating only the allocation’s length and not its capacity, but whatever, the point is finding the bug, and I’ll usually want to throw away the fix and rewrite it myself anyway.&lt;/p&gt;
    &lt;p&gt;Three out of three one-shot debugging hits with no help is extremely impressive. Importantly, there is no need to trust the LLM or review its output when its job is just saving me an hour or two by telling me where the bug is, for me to reason about it and fix it.&lt;/p&gt;
    &lt;p&gt;As ever, I wish we had better tooling for using LLMs which didn’t look like chat or autocomplete or “make me a PR.” For example, how nice would it be if every time tests fail, an LLM agent was kicked off with the task of figuring out why, and only notified us if it did before we fixed it?&lt;/p&gt;
    &lt;p&gt;For more low-level cryptography &lt;del&gt;bugs&lt;/del&gt; implementations, follow me on Bluesky at @filippo.abyssdomain.expert or on Mastodon at @filippo@abyssdomain.expert. I promise I almost never post about AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;The picture&lt;/head&gt;
    &lt;p&gt;Enjoy the silliest floof. Surely this will help redeem me in the eyes of folks who consider AI less of a tool and more of something to be hated or loved.&lt;/p&gt;
    &lt;p&gt;My work is made possible by Geomys, an organization of professional Go maintainers, which is funded by Smallstep, Ava Labs, Teleport, Tailscale, and Sentry. Through our retainer contracts they ensure the sustainability and reliability of our open source maintenance work and get a direct line to my expertise and that of the other Geomys maintainers. (Learn more in the Geomys announcement.) Here are a few words from some of them!&lt;/p&gt;
    &lt;p&gt;Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.&lt;/p&gt;
    &lt;p&gt;Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://words.filippo.io/claude-debugging/"/><published>2025-11-01T18:41:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45784455</id><title>Word2vec-style vector arithmetic on docs embeddings</title><updated>2025-11-02T04:14:11.789909+00:00</updated><content>&lt;doc fingerprint="31113c6d4e027e9c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;word2vec-style vector arithmetic on docs embeddings§&lt;/head&gt;
    &lt;p&gt;2025 October 29&lt;/p&gt;
    &lt;p&gt;word2vec popularized the idea of representing words as vectors where semantically similar words are positioned close to each other in the vector space. Nowadays these vectors are usually called embeddings.&lt;/p&gt;
    &lt;p&gt;A neat consequence of the word2vec approach is that adding and subtracting vectors produces semantically logical results. From Efficient Estimations of Word Representations in Vector Space (the word2vec paper):&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that&lt;/p&gt;&lt;code&gt;vector("King")&lt;/code&gt;-&lt;code&gt;vector("Man")&lt;/code&gt;+&lt;code&gt;vector("Woman")&lt;/code&gt;results in a vector that is closest to the vector representation of the word&lt;code&gt;Queen&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;Does word2vec-style vector arithmetic work in technical writing contexts?&lt;/p&gt;
    &lt;head rend="h2"&gt;Experiments§&lt;/head&gt;
    &lt;p&gt;word2vec was published in 2013. Embedding models have come a long way since then. word2vec models could only operate on single words. A vector always represented a single word. Modern embedding models can operate on arbitrary text. A vector can now represent a word, paragraph, section, document, set of documents, etc.&lt;/p&gt;
    &lt;p&gt;My experiments follow the same basic pattern of &lt;code&gt;vector("King")&lt;/code&gt; -
&lt;code&gt;vector("Man")&lt;/code&gt; + &lt;code&gt;vector("Woman")&lt;/code&gt;, with one difference. The experiments
start out with a vector representing the full text of a document, not a
single-word vector.&lt;/p&gt;
    &lt;head rend="h3"&gt;Same topic, different domain§&lt;/head&gt;
    &lt;p&gt;This is the first experiment. Starting with the vector for the full text of Testing Your Database from the Supabase docs, subtract the vector for the word &lt;code&gt;supabase&lt;/code&gt;, and then add the vector for the word &lt;code&gt;angular&lt;/code&gt;. The
resultant vector should be semantically close to the concept of “testing in
Angular”.&lt;/p&gt;
    &lt;head rend="h3"&gt;Different topic, same domain§&lt;/head&gt;
    &lt;p&gt;This is the second experiment. Starting with the vector for the full text of Testing Your Database from the Supabase docs, subtract the vector for the word &lt;code&gt;testing&lt;/code&gt;, and then add the vector for the word &lt;code&gt;vectors&lt;/code&gt;. The
resultant vector should be semantically close to the concept of “vectors in
Supabase”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Task types§&lt;/head&gt;
    &lt;p&gt;From previous research I’ve learned that task types noticeably affect Gemini Embedding’s outputs. EmbeddingGemma (the model I’ll be using in the experiments) also supports tasks types. I’ll run both experiments twice: once with default task types, and again with customized task types.&lt;/p&gt;
    &lt;head rend="h2"&gt;Verification§&lt;/head&gt;
    &lt;p&gt;There’s no way to directly verify that the resultant vectors are semantically close to the expected concepts. What I can do instead is generate vectors from the full texts of various docs, and then compare the resultant vectors from the experiments against the vectors of these various docs using cosine similarity.&lt;/p&gt;
    &lt;p&gt;Here’s the full list of docs that I use in the experiments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Background Processing Using Web Workers (Angular)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Refer To Locales By ID (Angular)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Testing (Angular)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Testing Services (Angular)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LINESTRING (CockroachDB)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Test Your Application Locally (CockroachDB)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;analysis_test (Skylib)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;bzl_library (Skylib)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;diff_test (Skylib)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Actionability (Playwright)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;JUnit (Playwright)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Writing Tests (Playwright)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Branching (Supabase)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Testing Your Database (Supabase)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Testing Your Edge Functions (Supabase)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Vector Columns (Supabase)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the Same topic, different domain experiment (Testing Your Database - &lt;code&gt;supabase&lt;/code&gt; + &lt;code&gt;angular&lt;/code&gt;) I expect the resultant vector to be most similar to
Testing or Testing Services from the Angular docs. And for the
Different topic, same domain experiment (Testing Your Database - &lt;code&gt;testing&lt;/code&gt; +
&lt;code&gt;vectors&lt;/code&gt;) I expect the resultant vector to be most similar to Vector
Columns from the Supabase docs.&lt;/p&gt;
    &lt;p&gt;Note that I picked short docs because EmbeddingGemma only supports 2048 tokens of input and I didn’t feel like dealing with chunking. Most of the docs revolve around testing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results§&lt;/head&gt;
    &lt;p&gt;In the Same topic, different domain experiment (Testing Your Database - &lt;code&gt;supabase&lt;/code&gt; + &lt;code&gt;angular&lt;/code&gt;) the resultant vector is most similar to Testing
and Testing Services from the Angular docs, as expected, when custom task
types are enabled:&lt;/p&gt;
    &lt;code&gt;[INFO] Running "same topic, different domain" experiment with customized task types
[INFO] Results:
[INFO] "Testing" (Angular) =&amp;gt; 0.751456081867218
[INFO] "Testing Services" (Angular) =&amp;gt; 0.6292878985404968
[INFO] "Background Processing Using Web Workers" (Angular) =&amp;gt; 0.5090276598930359
[INFO] "Testing Your Database" (Supabase) =&amp;gt; 0.5084458589553833
[INFO] "Refer To Locales By ID" (Angular) =&amp;gt; 0.46428176760673523
[INFO] "Test Your Application Locally" (CockroachDB) =&amp;gt; 0.4586600363254547
[INFO] "Writing Tests" (Playwright) =&amp;gt; 0.4434031546115875
[INFO] "JUnit" (Playwright) =&amp;gt; 0.4156876802444458
[INFO] "Actionability" (Playwright) =&amp;gt; 0.396766722202301
[INFO] "analysis_test" (Skylib) =&amp;gt; 0.3869394063949585
[INFO] "Testing Your Edge Functions" (Supabase) =&amp;gt; 0.368389755487442
[INFO] "diff_test" (Skylib) =&amp;gt; 0.3524951934814453
[INFO] "bzl_library" (Skylib) =&amp;gt; 0.29295891523361206
[INFO] "LINESTRING" (CockroachDB) =&amp;gt; 0.2778087854385376
[INFO] "Branching" (Supabase) =&amp;gt; 0.26931506395339966
[INFO] "Vector Columns" (Supabase) =&amp;gt; 0.23397961258888245
&lt;/code&gt;
    &lt;p&gt;When using the default task types, the resultant vector is most similar to Testing Your Database i.e. the doc that the experiment started with:&lt;/p&gt;
    &lt;code&gt;[INFO] Running "same topic, different domain" experiment with default task types
[INFO] Results:
[INFO] "Testing Your Database" (Supabase) =&amp;gt; 0.6590374708175659
[INFO] "Testing" (Angular) =&amp;gt; 0.571465790271759
[INFO] "Testing Services" (Angular) =&amp;gt; 0.46747612953186035
[INFO] "Test Your Application Locally" (CockroachDB) =&amp;gt; 0.43749818205833435
[INFO] "Testing Your Edge Functions" (Supabase) =&amp;gt; 0.4073418378829956
[INFO] "Writing Tests" (Playwright) =&amp;gt; 0.3561333119869232
[INFO] "Background Processing Using Web Workers" (Angular) =&amp;gt; 0.3353777527809143
[INFO] "Vector Columns" (Supabase) =&amp;gt; 0.3085843324661255
[INFO] "LINESTRING" (CockroachDB) =&amp;gt; 0.30450767278671265
[INFO] "Branching" (Supabase) =&amp;gt; 0.29775649309158325
[INFO] "analysis_test" (Skylib) =&amp;gt; 0.2946781814098358
[INFO] "Actionability" (Playwright) =&amp;gt; 0.2879413962364197
[INFO] "JUnit" (Playwright) =&amp;gt; 0.2845016121864319
[INFO] "Refer To Locales By ID" (Angular) =&amp;gt; 0.2824022173881531
[INFO] "diff_test" (Skylib) =&amp;gt; 0.26220911741256714
[INFO] "bzl_library" (Skylib) =&amp;gt; 0.2447129189968109
&lt;/code&gt;
    &lt;p&gt;In the Different topic, same domain experiment (Testing Your Database - &lt;code&gt;testing&lt;/code&gt; + &lt;code&gt;vectors&lt;/code&gt;) the resultant vector is most similar to
Vector Columns, regardless of whether default or custom task types were used.&lt;/p&gt;
    &lt;p&gt;Custom task types:&lt;/p&gt;
    &lt;code&gt;[INFO] Running "different topic, same domain" experiment with customized task types
[INFO] Results:
[INFO] "Vector Columns" (Supabase) =&amp;gt; 0.6380605697631836
[INFO] "Testing Your Database" (Supabase) =&amp;gt; 0.44831225275993347
[INFO] "LINESTRING" (CockroachDB) =&amp;gt; 0.32693782448768616
[INFO] "Background Processing Using Web Workers" (Angular) =&amp;gt; 0.2737721800804138
[INFO] "Testing Your Edge Functions" (Supabase) =&amp;gt; 0.25883781909942627
[INFO] "Branching" (Supabase) =&amp;gt; 0.2509428560733795
[INFO] "Refer To Locales By ID" (Angular) =&amp;gt; 0.2328835278749466
[INFO] "bzl_library" (Skylib) =&amp;gt; 0.2133977860212326
[INFO] "Test Your Application Locally" (CockroachDB) =&amp;gt; 0.20613139867782593
[INFO] "Testing" (Angular) =&amp;gt; 0.16262517869472504
[INFO] "Actionability" (Playwright) =&amp;gt; 0.14792931079864502
[INFO] "Writing Tests" (Playwright) =&amp;gt; 0.14344163239002228
[INFO] "Testing Services" (Angular) =&amp;gt; 0.13723336160182953
[INFO] "diff_test" (Skylib) =&amp;gt; 0.12111848592758179
[INFO] "JUnit" (Playwright) =&amp;gt; 0.11599748581647873
[INFO] "analysis_test" (Skylib) =&amp;gt; 0.0979730486869812
&lt;/code&gt;
    &lt;p&gt;Default task types:&lt;/p&gt;
    &lt;code&gt;[INFO] Running "different topic, same domain" experiment with default task types
[INFO] Results:
[INFO] "Vector Columns" (Supabase) =&amp;gt; 0.6698287129402161
[INFO] "Testing Your Database" (Supabase) =&amp;gt; 0.6086233854293823
[INFO] "Testing Your Edge Functions" (Supabase) =&amp;gt; 0.36533844470977783
[INFO] "LINESTRING" (CockroachDB) =&amp;gt; 0.34430524706840515
[INFO] "Branching" (Supabase) =&amp;gt; 0.3141021430492401
[INFO] "Test Your Application Locally" (CockroachDB) =&amp;gt; 0.29872700572013855
[INFO] "Background Processing Using Web Workers" (Angular) =&amp;gt; 0.28414368629455566
[INFO] "bzl_library" (Skylib) =&amp;gt; 0.26424312591552734
[INFO] "Refer To Locales By ID" (Angular) =&amp;gt; 0.2537899315357208
[INFO] "Testing" (Angular) =&amp;gt; 0.23542608320713043
[INFO] "Writing Tests" (Playwright) =&amp;gt; 0.22030793130397797
[INFO] "Testing Services" (Angular) =&amp;gt; 0.20675960183143616
[INFO] "Actionability" (Playwright) =&amp;gt; 0.1959698647260666
[INFO] "diff_test" (Skylib) =&amp;gt; 0.19095730781555176
[INFO] "JUnit" (Playwright) =&amp;gt; 0.1832783967256546
[INFO] "analysis_test" (Skylib) =&amp;gt; 0.15578024089336395
&lt;/code&gt;
    &lt;p&gt;So, yes, it seems like word2vec-style vector arithmetic can work in technical writing contexts. Make sure to set your task types correctly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discussion§&lt;/head&gt;
    &lt;p&gt;I still don’t really understand how it’s possible to semantically represent an entire document as a single vector, let alone how adding and subtracting single-word vectors from full-document vectors works.&lt;/p&gt;
    &lt;p&gt;How do we actually use this in technical writing workflows or documentation experiences? I’m not sure. I was just curious to learn whether or not it would work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix§&lt;/head&gt;
    &lt;head rend="h3"&gt;Source code§&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;experiments.py&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;from json import load
from os import environ
from sys import exit

from requests import get
from sentence_transformers import SentenceTransformer


class Doc:

    def __init__(self, topic, domain, url, length, embedding):
        self.topic = topic
        self.domain = domain
        self.url = url
        self.length = length
        self.embedding = embedding
        self.similarity = None


def init_docs(model, task_types):
    with open("data.json", "r") as f:
        data = load(f)
    tokenizer = model.tokenizer
    docs = []
    max_length = tokenizer.model_max_length
    for item in data:
        url = item["url"]
        response = get(url)
        text = response.text
        length = len(tokenizer.encode(text))
        topic = item["topic"]
        domain = item["domain"]
        if length &amp;gt; max_length:
            exit(f"[ERROR] Document is too large: {topic}, {domain}")
        prompt = "title: {topic} | text: "
        embedding = model.encode(text, prompt=prompt) if task_types else model.encode(text)
        doc = Doc(topic, domain, url, length, embedding)
        docs.append(doc)
    return docs


def create_domain_query(model, task_types):
    url = "https://raw.githubusercontent.com/supabase/supabase/refs/heads/master/apps/docs/content/guides/database/testing.mdx"
    response = get(url)
    text = response.text
    if task_types:
        doc = model.encode(text, prompt_name="Retrieval-query")
        supabase = model.encode("supabase", prompt_name="Retrieval-query")
        angular = model.encode("angular", prompt_name="Retrieval-query")
    else:
        doc = model.encode(text)
        supabase = model.encode("supabase")
        angular = model.encode("angular")
    return doc - supabase + angular


def create_topic_query(model, task_types):
    url = "https://raw.githubusercontent.com/supabase/supabase/refs/heads/master/apps/docs/content/guides/database/testing.mdx"
    response = get(url)
    text = response.text
    if task_types:
        doc = model.encode(text, prompt_name="Retrieval-query")
        testing = model.encode("testing", prompt_name="Retrieval-query")
        vectors = model.encode("vectors", prompt_name="Retrieval-query")
    else:
        doc = model.encode(text)
        testing = model.encode("testing")
        vectors = model.encode("vectors")
    return doc - testing + vectors


def run_experiments():
    environ["TOKENIZERS_PARALLELISM"] = "false"
    model = SentenceTransformer("google/embeddinggemma-300m")
    for task_types in [True, False]:
        print(f'[INFO] Running "same topic, different domain" experiment with {"customized" if task_types else "default"} task types')
        docs = init_docs(model, task_types)
        query = create_domain_query(model, task_types)
        for doc in docs:
            similarity = model.similarity(query, doc.embedding).item()
            doc.similarity = similarity
        docs.sort(key=lambda doc: doc.similarity, reverse=True)
        print(f'[INFO] Results:')
        for doc in docs:
            print(f'[INFO] "{doc.topic}" ({doc.domain}) =&amp;gt; {doc.similarity}')
        print()
        print(f'[INFO] Running "different topic, same domain" experiment with {"customized" if task_types else "default"} task types')
        docs = init_docs(model, task_types)
        query = create_topic_query(model, task_types)
        for doc in docs:
            similarity = model.similarity(query, doc.embedding).item()
            doc.similarity = similarity
        docs.sort(key=lambda doc: doc.similarity, reverse=True)
        print(f'[INFO] Results:')
        for doc in docs:
            print(f'[INFO] "{doc.topic}" ({doc.domain}) =&amp;gt; {doc.similarity}')
        print()


if __name__ == "__main__":
    run_experiments()
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;data.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[
  {
    "domain": "Angular",
    "topic": "Background Processing Using Web Workers",
    "url": "https://raw.githubusercontent.com/angular/angular/refs/heads/main/adev/src/content/ecosystem/web-workers.md" 
  },
  {
    "domain": "Angular",
    "topic": "Refer To Locales By ID",
    "url": "https://raw.githubusercontent.com/angular/angular/refs/heads/main/adev/src/content/guide/i18n/locale-id.md" 
  },
  {
    "domain": "Angular",
    "topic": "Testing",
    "url": "https://raw.githubusercontent.com/angular/angular/refs/heads/main/adev/src/content/guide/testing/overview.md" 
  },
  {
    "domain": "Angular",
    "topic": "Testing Services",
    "url": "https://raw.githubusercontent.com/angular/angular/refs/heads/main/adev/src/content/guide/testing/services.md" 
  },
  {
    "domain": "CockroachDB",
    "topic": "LINESTRING",
    "url": "https://raw.githubusercontent.com/cockroachdb/docs/refs/heads/main/src/current/v25.4/linestring.md" 
  },
  {
    "domain": "CockroachDB",
    "topic": "Test Your Application Locally",
    "url": "https://raw.githubusercontent.com/cockroachdb/docs/refs/heads/main/src/current/v25.4/local-testing.md" 
  },
  {
    "domain": "Skylib",
    "topic": "analysis_test",
    "url": "https://raw.githubusercontent.com/bazelbuild/bazel-skylib/refs/heads/main/docs/analysis_test_doc.md"
  },
  {
    "domain": "Skylib",
    "topic": "bzl_library",
    "url": "https://raw.githubusercontent.com/bazelbuild/bazel-skylib/refs/heads/main/docs/bzl_library.md"
  },
  {
    "domain": "Skylib",
    "topic": "diff_test",
    "url": "https://raw.githubusercontent.com/bazelbuild/bazel-skylib/refs/heads/main/docs/diff_test_doc.md"
  },
  {
    "domain": "Playwright",
    "topic": "Actionability",
    "url": "https://raw.githubusercontent.com/microsoft/playwright/refs/heads/main/docs/src/actionability.md"
  },
  {
    "domain": "Playwright",
    "topic": "JUnit",
    "url": "https://raw.githubusercontent.com/microsoft/playwright/refs/heads/main/docs/src/junit-java.md"
  },
  {
    "domain": "Playwright",
    "topic": "Writing Tests",
    "url": "https://raw.githubusercontent.com/microsoft/playwright/refs/heads/main/docs/src/writing-tests-java.md"
  },
  {
    "domain": "Supabase",
    "topic": "Branching",
    "url": "https://raw.githubusercontent.com/supabase/supabase/refs/heads/master/apps/docs/content/guides/deployment/branching.mdx"
  },
  {
    "domain": "Supabase",
    "topic": "Testing Your Database",
    "url": "https://raw.githubusercontent.com/supabase/supabase/refs/heads/master/apps/docs/content/guides/database/testing.mdx"
  },
  {
    "domain": "Supabase",
    "topic": "Testing Your Edge Functions",
    "url": "https://raw.githubusercontent.com/supabase/supabase/refs/heads/master/apps/docs/content/guides/functions/unit-test.mdx"
  },
  {
    "domain": "Supabase",
    "topic": "Vector Columns",
    "url": "https://raw.githubusercontent.com/supabase/supabase/refs/heads/master/apps/docs/content/guides/ai/vector-columns.mdx"
  }
]
&lt;/code&gt;
    &lt;p&gt;Note that I forgot to pin the URLs to specific commits. I.e. I used the &lt;code&gt;HEAD&lt;/code&gt; version of each URL. If you run the experiments a year or two from now
(October 2025), your cosine similarity scores will probably be different,
because the underlying text of the documents will probably have changed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://technicalwriting.dev/embeddings/arithmetic/index.html"/><published>2025-11-01T19:14:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45784596</id><title>Beginner-friendly, unofficial documentation for Helix text editor</title><updated>2025-11-02T04:14:11.615286+00:00</updated><content>&lt;doc fingerprint="2e1d8a1e0335f69e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Basics&lt;/head&gt;
    &lt;p&gt;To get started check out the installation instructions in order to follow along with the tutorial.&lt;/p&gt;
    &lt;head rend="h3"&gt;Opening a file&lt;/head&gt;
    &lt;p&gt;Create a new text file and open it with Helix by running &lt;code&gt;hx file.txt&lt;/code&gt;. This is what you’ll see:&lt;/p&gt;
    &lt;quote&gt;1 ~ NOR file.txt 1 sel 1:1 Loaded 1 file.&lt;/quote&gt;
    &lt;p&gt;Notice the &lt;code&gt;NOR&lt;/code&gt; in the bottom-left corner, this indicates that you are currently in Normal mode. In this mode, typing letters like e and n won’t insert them as text, but rather have specific commands which we will explore later.&lt;/p&gt;
    &lt;p&gt;To actually insert some text, press i, which is the command to get into Insert mode, indicated by the &lt;code&gt;INS&lt;/code&gt; in the bottom-left corner. In this mode, the letters you type will be inserted directly into the document.&lt;/p&gt;
    &lt;p&gt;Try it out by writing &lt;code&gt;Hello helix!&lt;/code&gt;.&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! ~ INS file.txt [+] 1 sel 1:13&lt;/quote&gt;
    &lt;p&gt;To get back into Normal mode press Esc. This will change the color of your cursor and you will see &lt;code&gt;NOR&lt;/code&gt; again, indicating that you are in normal mode now.&lt;/p&gt;
    &lt;head rend="h3"&gt;Movement&lt;/head&gt;
    &lt;p&gt;To move your cursor you could use arrow keys, both in Normal and Insert modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;↑ move cursor up&lt;/item&gt;
      &lt;item&gt;↓ move cursor down&lt;/item&gt;
      &lt;item&gt;→ moves cursor right&lt;/item&gt;
      &lt;item&gt;← moves cursor left&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, this isn’t encouraged due to the fact that hand will be doing a lot of back-and-forth movement between the arrow keys and the keyboard.&lt;/p&gt;
    &lt;p&gt;Instead, it is recommended to rest your fingers on the “home row”, which is comprised of the row of keys &lt;code&gt;a s d f g h j k l&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Instead of stretching to reach the arrow keys, use normal mode and h, j, k and l to move your cursor:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;h: moves cursor 1 character to the left.&lt;/item&gt;
      &lt;item&gt;j: moves cursor 1 line above.&lt;/item&gt;
      &lt;item&gt;k: moves cursor 1 line below.&lt;/item&gt;
      &lt;item&gt;l: moves cursor 1 character to the right.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Try holding down h and l to move horizontally across the text you just wrote!&lt;/p&gt;
    &lt;head rend="h3"&gt;Paste&lt;/head&gt;
    &lt;p&gt;We only have one line of text, so let’s duplicate it several times. Type:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;x, which will select the entire line.&lt;/item&gt;
      &lt;item&gt;y, which will yank (copy) the selection to clipboard.&lt;/item&gt;
      &lt;item&gt;p, which will paste the contents of the selection after the cursor.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Spam p a few times to create several lines.&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello helix! 4 Hello helix! 5 Hello helix! ~ NOR file.txt [+] 1 sel 3:8&lt;/quote&gt;
    &lt;p&gt;Now you can try using the h, j, k and l motions to traverse the text!&lt;/p&gt;
    &lt;head rend="h3"&gt;Word-based Movement&lt;/head&gt;
    &lt;p&gt;Let’s say we want to replace one of the &lt;code&gt;helix&lt;/code&gt; words with &lt;code&gt;world&lt;/code&gt;. To do this, place your cursor on one of the h letters:&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello helix! 4 Hello helix! 5 Hello helix! ~ NOR file.txt [+] 1 sel 3:7&lt;/quote&gt;
    &lt;p&gt;e is a motion which moves to the end of the current word. Type e and it will move your cursor to the end of the &lt;code&gt;helix&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It doesn’t just move your cursor there, though. The entire &lt;code&gt;helix&lt;/code&gt; word becomes highlighted:&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello helix! 4 Hello helix! 5 Hello helix! ~ NOR file.txt [+] 1 sel 3:11&lt;/quote&gt;
    &lt;p&gt;If we now press b, which moves to the beginning of the current word, it’ll move us back to where we just were.&lt;/p&gt;
    &lt;p&gt;Try this out a few times, press e and then b to select various sections of the text. If you want to remove your selection press ;.&lt;/p&gt;
    &lt;p&gt;Let’s highlight our &lt;code&gt;helix&lt;/code&gt; word again:&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello helix! 4 Hello helix! 5 Hello helix! ~ NOR file.txt [+] 1 sel 3:11&lt;/quote&gt;
    &lt;head rend="h3"&gt;Selection-first Approach&lt;/head&gt;
    &lt;p&gt;Helix’s philosophy is that each action will act on a selection.&lt;/p&gt;
    &lt;p&gt;Every time text is modified (an action), you will fully anticipate the result — because you can clearly see the area of text which is highlighted, and thus will be modified.&lt;/p&gt;
    &lt;p&gt;For example, we currently have the word &lt;code&gt;helix&lt;/code&gt; selected. To change it to &lt;code&gt;world&lt;/code&gt;, press c,&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello ! 4 Hello helix! 5 Hello helix! ~ INS file.txt [+] 1 sel 3:7&lt;/quote&gt;
    &lt;p&gt;c removes the contents of the current selection and places us in Insert mode, where you can then write your new word. Exit back to Normal mode by pressing esc.&lt;/p&gt;
    &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello world! 4 Hello helix! 5 Hello helix! ~ NOR file.txt [+] 1 sel 3:12&lt;/quote&gt;
    &lt;head rend="h3"&gt;Delete&lt;/head&gt;
    &lt;p&gt;The d command deletes the current selection and copies what has been deleted into a clipboard.&lt;/p&gt;
    &lt;p&gt;Let’s test it out by doing the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Select the line we just changed with x.&lt;/p&gt;
        &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello world! 4 Hello helix! 5 Hello helix! ~ NOR file.txt [+] 1 sel 3:13&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;d to delete this line.&lt;/p&gt;
        &lt;quote&gt;1 Hello helix! 2 Hello helix! 3 Hello helix! 4 Hello helix! ~ NOR file.txt [+] 1 sel 3:1&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Spam p a few times to create some duplicates.&lt;/p&gt;
        &lt;quote&gt;4 Hello world! 5 Hello world! 6 Hello world! 7 Hello helix! ~ NOR file.txt [+] 1 sel 6:13&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s remove all of our previous &lt;code&gt;Hello helix!&lt;/code&gt; by doing the following for each &lt;code&gt;Hello helix!&lt;/code&gt; line:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Select the line with x.&lt;/item&gt;
      &lt;item&gt;d to delete it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now we have something like this:&lt;/p&gt;
    &lt;quote&gt;1 Hello world! 2 Hello world! 3 Hello world! ~ NOR file.txt [+] 1 sel 2:1&lt;/quote&gt;
    &lt;head rend="h3"&gt;Undo and Redo&lt;/head&gt;
    &lt;p&gt;What if we made a mistake, and want to go back? The u command will undo our most recent action. It’s similar to Ctrl + z in other editors.&lt;/p&gt;
    &lt;p&gt;Try pressing down u a few times to get to our previous state, before we made all those modifications:&lt;/p&gt;
    &lt;quote&gt;3 Hello helix! 4 Hello world! 5 Hello world! 6 Hello helix! ~ NOR file.txt [+] 1 sel 5:13&lt;/quote&gt;
    &lt;p&gt;U is similar to Ctrl + Shift + z in other editors. It will undo the last undo. It’s the inverse of u.&lt;/p&gt;
    &lt;p&gt;Press U until we get back to our most recent state:&lt;/p&gt;
    &lt;quote&gt;1 Hello world! 2 Hello world! 3 Hello world! ~ NOR file.txt [+] 1 sel 1:1 Already at newest change&lt;/quote&gt;
    &lt;head rend="h3"&gt;Checkpoint&lt;/head&gt;
    &lt;p&gt;Feel free to make modifications to your file using the commands we have learned so far:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;h, j, k and l moves 1 character left, down, up and right.&lt;/item&gt;
      &lt;item&gt;i enters Insert mode.&lt;/item&gt;
      &lt;item&gt;esc enters Normal mode.&lt;/item&gt;
      &lt;item&gt;x selects the entire line.&lt;/item&gt;
      &lt;item&gt;y yanks the selection.&lt;/item&gt;
      &lt;item&gt;p pastes the recently copied selection.&lt;/item&gt;
      &lt;item&gt;e selects and moves to the end of the current word.&lt;/item&gt;
      &lt;item&gt;b selects and moves to the beginning of the current word.&lt;/item&gt;
      &lt;item&gt;; removes the extra selection.&lt;/item&gt;
      &lt;item&gt;d deletes the current selection, without exiting Normal mode.&lt;/item&gt;
      &lt;item&gt;c changes the current selection, by deleting it and entering Insert mode.&lt;/item&gt;
      &lt;item&gt;u will undo the most recent change.&lt;/item&gt;
      &lt;item&gt;U will undo the most recent undo.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you are happy with your modifications, enter Normal mode and type :.&lt;/p&gt;
    &lt;p&gt;: enters command mode, which has commands you type out.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;:w&lt;/code&gt;will write the current file.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;:q&lt;/code&gt;will quit the current file.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;:q!&lt;/code&gt;will quit without saving.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;:wq&lt;/code&gt;will both write and quit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;┌──────────────────────────────┐ │ Write changes to disk and │ │ close the current view. │ │ Accepts an optional path │ └──────────────────────────────┘ write-quit-all write-quit-all! :wq&lt;/quote&gt;
    &lt;head rend="h2"&gt;More Commands&lt;/head&gt;
    &lt;p&gt;Let’s try out more Helix commands! Open the file again with &lt;code&gt;hx file.txt&lt;/code&gt;. Select the entire file by pressing %&lt;/p&gt;
    &lt;quote&gt;1 Hello world! 2 Hello world! 3 Hello world! ~ NOR file.txt [+] 1 sel 3:13&lt;/quote&gt;
    &lt;p&gt;Now, delete the selection with d.&lt;/p&gt;
    &lt;quote&gt;~ NOR file.txt [+] 1 sel 1:1&lt;/quote&gt;
    &lt;head rend="h3"&gt;Goto Word&lt;/head&gt;
    &lt;p&gt;Using b to go to the beginning of the word and e to go to the end is useful if you are already at the word you want. But if you are far away, a very powerful command is goto word — gw.&lt;/p&gt;
    &lt;quote&gt;1 aue atates asll 2 ard ape anouds alll 3 aje ahn afll 4 add abe clouds aall 5 acd aee agon aill 6 akd ame aoon aqll NOR file.txt [+] 1 sel 4:13 gw&lt;/quote&gt;
    &lt;p&gt;gw will create two letters at the start of every word in sight. When you type those two letters, you instantly jump to the specified word.&lt;/p&gt;
    &lt;p&gt;Let’s say we want to jump to the &lt;code&gt;plates&lt;/code&gt; word. The first two characters have been replaced by &lt;code&gt;at&lt;/code&gt; and highlighted. If we write &lt;code&gt;at&lt;/code&gt;, we will highlight that word!&lt;/p&gt;
    &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the clouds will 5 and the moon will 6 and the moon will NOR file.txt [+] 1 sel 1:10&lt;/quote&gt;
    &lt;head rend="h3"&gt;Replace&lt;/head&gt;
    &lt;p&gt;You can also replace a selection with contents of a register.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Select the &lt;code&gt;moon&lt;/code&gt;word by using gw and yank it with y.&lt;/item&gt;
      &lt;item&gt;Select the &lt;code&gt;sun&lt;/code&gt;word and replace it with&lt;code&gt;moon&lt;/code&gt;with R.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Search&lt;/head&gt;
    &lt;p&gt;Go to the first line by using gg.&lt;/p&gt;
    &lt;p&gt;To search for a word, use / command. Type &lt;code&gt;will&lt;/code&gt; which is going to highlight the next &lt;code&gt;will&lt;/code&gt; keyword, and then Enter ↵ to select it.&lt;/p&gt;
    &lt;p&gt;Since there are several &lt;code&gt;will&lt;/code&gt;s in the text, you can cycle between them:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;n cycles to the next match.&lt;/item&gt;
      &lt;item&gt;N cycles to the previous match.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;More ways to enter Insert Mode&lt;/head&gt;
    &lt;p&gt;Select the &lt;code&gt;clouds&lt;/code&gt; word using gw. If you press i, you will go into insert mode at the beginning of the selection. There are also 5 more ways to enter Insert mode:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a for append go into insert mode at the end of the selection&lt;/item&gt;
      &lt;item&gt;I go into insert mode at the beginning of the current line&lt;/item&gt;
      &lt;item&gt;A to append at the end of the current line&lt;/item&gt;
      &lt;item&gt;o add a newline below and go into insert mode&lt;/item&gt;
      &lt;item&gt;O add a newline above and go into insert mode&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Try all of them out!&lt;/p&gt;
    &lt;head rend="h3"&gt;Registers&lt;/head&gt;
    &lt;p&gt;Helix has a concept called registers, which is like having many clipboards at once.&lt;/p&gt;
    &lt;p&gt;To interact with them, prefix yank and delete commands with a ” and then the name of the register.&lt;/p&gt;
    &lt;p&gt;For example, the contents of the system clipboard are stored inside the &lt;code&gt;+&lt;/code&gt; register.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Paste the following content into the file with “+p:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Navigate to the last line by using ge for goto end.&lt;/p&gt;
        &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the moon will ~ NOR file.txt [+] 1 sel 4:2&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Select the last line with x and then yank it with y.&lt;/p&gt;
        &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the moon will ~ NOR file.txt [+] 1 sel 4:18 yanked 1 selection to register "&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Navigate to the second line by using 2gg.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Select the second line by using x and then yank into into the e register with “ey.&lt;/p&gt;
        &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the moon will ~ NOR file.txt [+] 1 sel 2:20 yanked 1 selection to register e&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Navigate to the third line by using 3gg.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Paste what we copied previously by using p.&lt;/p&gt;
        &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the moon will 5 and the moon will ~ NOR file.txt [+] 1 sel 4:18&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notice how we haven’t pasted the 2nd line’s contents, but rather the last lines’? Because we yanked the 2nd line’s contents into the e register. To paste from it, use “ep.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;” signals to the editor that we are going to use a register.&lt;/item&gt;
      &lt;item&gt;e uses the e register.&lt;/item&gt;
      &lt;item&gt;p pastes contents of the e register.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the moon will 5 and the clouds will 6 and the moon will NOR file.txt [+] 1 sel 5:20&lt;/quote&gt;
    &lt;p&gt;Take note of the fact that when we press p, it pastes the contents of the register after the line. To paste before, we undo with u and use P to paste before.&lt;/p&gt;
    &lt;quote&gt;1 The plates will 2 and the clouds will 3 The sun will 4 and the clouds will 5 and the moon will 6 and the moon will NOR file.txt [+] 1 sel 4:20&lt;/quote&gt;
    &lt;head rend="h3"&gt;Move to characters&lt;/head&gt;
    &lt;p&gt;You can also search for individual characters by using t, which stands for till.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Copy the text below&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Select the entire file with %&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Override the selection by using Space + R.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Go to the first line with gg and use t; to go to the next semicolon. Repeat this several several times.&lt;/p&gt;
    &lt;p&gt;To move in the opposite direction, use T; to the previous semicolon.&lt;/p&gt;
    &lt;p&gt;Using t and T motions will move your cursor to just before the next or the previous occurrence of the character.&lt;/p&gt;
    &lt;p&gt;For example, te to go to the next e. T” to go to just before the previous double-quote.&lt;/p&gt;
    &lt;p&gt;The f for find is similar to t, but instead it places your cursor at the occurrence of the character. Try using f; several times. F goes the opposite way.&lt;/p&gt;
    &lt;head rend="h3"&gt;Counts&lt;/head&gt;
    &lt;p&gt;Each motion also accepts an optional count, which defaults to 1 if you don’t provide it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For example, use 2f; which would be the same as f;f;.&lt;/item&gt;
      &lt;item&gt;Or 7b which would be the same as bbbbbbb.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Page Navigation&lt;/head&gt;
    &lt;p&gt;Currently the text is fairly short, but we can fix that. Select everything with %, yank y and then paste it 100 times with 100p. This will create a very big file.&lt;/p&gt;
    &lt;p&gt;We can use these commands to scroll large chunks of text at once:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ctrl + d to scroll down half a page&lt;/item&gt;
      &lt;item&gt;Ctrl + u to scroll up half a page&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Next steps&lt;/head&gt;
    &lt;p&gt;Now you know the basics of movement in Helix, it’s time to learn about the more advanced features Helix provides.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Explore advanced text manipulation techniques, such as surrounds and text objects.&lt;/item&gt;
      &lt;item&gt;Make full use of Helix’s powerful editing model by understanding how to use multiple cursor and macros&lt;/item&gt;
      &lt;item&gt;Learn how to enable language support and auto-formatters.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://helix-editor.vercel.app/start-here/basics/"/><published>2025-11-01T19:33:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45785840</id><title>SailfishOS: A Linux-based European alternative to dominant mobile OSes</title><updated>2025-11-02T04:14:10.861930+00:00</updated><content>&lt;doc fingerprint="f389bb2315a326c8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Info&lt;/head&gt;
    &lt;head rend="h1"&gt;Sailfish OS history&lt;/head&gt;
    &lt;p&gt;Sailfish OS’s heritage lies in Nokia times, especially in the MeeGo operating system. Prior to 2011 Nokia and Intel had a vision of an open mobile operating system. Together they invested around 1 billion USD to the project and created an open source based operating system called MeeGo, which was used as a basis for several devices, such as the iconic Nokia N9. Although the Nokia N9 became the beacon of open source operating systems, Nokia decided to end the project and chose instead to continue with Microsoft’s Windows Phone OS. The rest of that is another story.&lt;/p&gt;
    &lt;p&gt;The passionate team behind MeeGo refused to quit working on the project they’ve believed in. They, or currently ‘we’ saved MeeGo by setting up a new company, Jolla Ltd., to develop the swipe-based MeeGo into the flowing user experience that is Sailfish OS. We quickly enhanced Sailfish OS to run Android apps and it became hardware compatible with Android chipsets. In November 2013, we launched the beta version of Sailfish OS to the market with the Jolla smartphone. Shortly after this, we released Sailfish OS version 1.0 and the first Sailfish OS product, the Jolla smartphone entered 36 markets during one year. In 2015, Sailfish OS 2.0. was released along with the Jolla Tablet, and a strengthened focus on the company’s licensing strategy.&lt;/p&gt;
    &lt;p&gt;In 2018 Sailfish OS matured to its third generation offering a packetized, secure solution for various corporate and governmental environments, and a smooth and secure mobile OS for tech-savvy consumers through the Sailfish X community program. The fourth generation, Sailfish 4, introduced in February 2021, boasts a multitude of new enablers to support different ecosystem projects, be it private corporate solutions or public sector governmental deployments.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why Sailfish OS?&lt;/head&gt;
    &lt;p&gt;Sailfish OS is a mature mobile OS platform; it has been on the market since 2013, it has been market-proven on several mobile devices, and it is being deployed in many mobile ecosystem projects around the world. It is the only open source based and independent mobile OS without any ties to big corporations, supported with strong IPR including full IP rights and trademarks.&lt;/p&gt;
    &lt;p&gt;Sailfish OS is developed by Finnish mobile company Jolla by a team of efficient and high-skilled engineers working together since 2011. The Jolla team is supported by a worldwide Sailfish community contributing to the open source codebase. With a strong, respected, and award-winning brand and reputation, Sailfish OS is a perfect strategic solution for corporations and governments, and a sought-after OS for mobile enthusiasts looking for an alternative.&lt;/p&gt;
    &lt;head rend="h1"&gt;OS architecture&lt;/head&gt;
    &lt;p&gt;Sailfish operating system is built like a classic Linux distribution. The signature Sailfish UI has been developed by Jolla using QML, a powerful user experience design language provided by Qt framework. The QML language and features give Sailfish OS the ability to provide a rich set of UI elements, to create animated, touch-enabled UIs and lightweight applications. Jolla has created the UI building blocks to build native applications with custom components called Sailfish Silica.&lt;/p&gt;
    &lt;p&gt;Sailfish OS also includes the capability to run Android™ applications. It is based on Android libraries, ensuring performance comparable to the native environment.&lt;/p&gt;
    &lt;p&gt;With Sailfish using Qt5 and Wayland technology, existing hardware adaptations made for Android can be leveraged, significantly easing the hardware adaptation work required to support the OS.&lt;/p&gt;
    &lt;p&gt;If you are interested in Open Source software part, you can download source code from here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sailfishos.org/info/"/><published>2025-11-01T22:05:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45785858</id><title>Visopsys: OS maintained by a single developer since 1997</title><updated>2025-11-02T04:14:10.069413+00:00</updated><content>&lt;doc fingerprint="a1fd7da091abb4c3"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Welcome!&lt;/head&gt;
    &lt;p&gt;Visopsys is an alternative operating system for PC compatible computers. In development since 1997, this system is small, fast, and open source. It features a simple but functional graphical interface, pre-emptive multitasking, and virtual memory. Though it attempts to be compatible in a number of ways, Visopsys is not a clone of any other operating system. You can demo the distribution from a “live” USB stick, CD/DVD, or floppy disk … (read more).&lt;/p&gt;
    &lt;head rend="h4"&gt;Features of Visopsys&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Small &amp;amp; fast&lt;/item&gt;
      &lt;item&gt;Graphical user interface&lt;/item&gt;
      &lt;item&gt;Fully multitasking&lt;/item&gt;
      &lt;item&gt;100% protected mode&lt;/item&gt;
      &lt;item&gt;Open source, free software&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;News&lt;/head&gt;
    &lt;p&gt;[21.09.2023] – Version 0.92 is now available from the download page&lt;lb/&gt; [30.07.2021] – Version 0.91 was released&lt;lb/&gt; [16.04.2020] – Version 0.9 was released&lt;lb/&gt; [21.01.2020] – Version 0.85 was released&lt;lb/&gt; [15.05.2019] – Version 0.84 was released&lt;lb/&gt; [09.08.2018] – Version 0.83 was released&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://visopsys.org/"/><published>2025-11-01T22:07:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45785986</id><title>OpenDesk by the Centre for Digital Sovereignty</title><updated>2025-11-02T04:14:09.169837+00:00</updated><content>&lt;doc fingerprint="5f97b34a74f9de43"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;Use case: MPK relies on openDesk for secure collaboration&lt;/head&gt;&lt;p&gt;With its target of 160,000 licences across Germany’s public administration by the end of 2025, openDesk is proving its strength not only in major institutions such as the Robert Koch Institute, but also in smaller, strategically important settings.&lt;/p&gt;Best practices&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.opendesk.eu/en/product"/><published>2025-11-01T22:26:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45786192</id><title>Show HN: KeyLeak Detector – Scan websites for exposed API keys and secrets</title><updated>2025-11-02T04:14:08.666070+00:00</updated><content>&lt;doc fingerprint="b76ad838dcb0f900"&gt;
  &lt;main&gt;
    &lt;p&gt;A web application that scans websites for potential API keys, secrets, and sensitive information leaks. This tool helps developers and security professionals identify and fix security vulnerabilities in their web applications.&lt;/p&gt;
    &lt;p&gt;Referenced Project: Key detection patterns inspired by Keyleaksecret.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Scans web pages for common secret patterns (API keys, passwords, tokens, etc.)&lt;/item&gt;
      &lt;item&gt;Checks response headers for sensitive information&lt;/item&gt;
      &lt;item&gt;Validates security headers&lt;/item&gt;
      &lt;item&gt;User-friendly web interface&lt;/item&gt;
      &lt;item&gt;Real-time scanning results&lt;/item&gt;
      &lt;item&gt;Categorizes findings by severity&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Clone this repository:&lt;/p&gt;
        &lt;quote&gt;git clone &amp;lt;repository-url&amp;gt; cd keyleak-detector&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create and activate a virtual environment (recommended):&lt;/p&gt;
        &lt;quote&gt;python -m venv venv source venv/bin/activate # On Windows: venv\Scripts\activate&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install the required dependencies:&lt;/p&gt;
        &lt;quote&gt;pip install -r requirements.txt&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install Playwright browsers (required for scanning):&lt;/p&gt;
        &lt;quote&gt;playwright install chromium playwright install-deps&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Start the application:&lt;/p&gt;
        &lt;quote&gt;python app.py&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Open your web browser and navigate to:&lt;/p&gt;
        &lt;code&gt;http://localhost:5002&lt;/code&gt;
        &lt;p&gt;Note: The app runs on port 5002 instead of 5000 as port 5000 is commonly used by AirPlay on macOS.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enter the URL you want to scan in the input field and click "Scan Now"&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;View the results, which will show any potential security issues found&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The application uses a combination of browser automation and network traffic analysis to find secrets:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Browser Automation: Uses Playwright to load the target website in a headless browser&lt;/item&gt;
      &lt;item&gt;Network Monitoring: Intercepts HTTP requests and responses using mitmproxy&lt;/item&gt;
      &lt;item&gt;Content Analysis: Analyzes JavaScript, HTML, headers, and dynamic content&lt;/item&gt;
      &lt;item&gt;Pattern Matching: Uses regex patterns to detect various types of secrets&lt;/item&gt;
      &lt;item&gt;Smart Filtering: Filters false positives using context-aware analysis&lt;/item&gt;
      &lt;item&gt;Categorization: Groups findings by severity (Critical, High, Medium, Low)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The scanner detects 50+ types of sensitive information including:&lt;/p&gt;
    &lt;p&gt;Cloud Provider Credentials:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AWS Access Keys &amp;amp; Secret Keys&lt;/item&gt;
      &lt;item&gt;Google API Keys &amp;amp; OAuth Tokens&lt;/item&gt;
      &lt;item&gt;Google Cloud Service Account Keys&lt;/item&gt;
      &lt;item&gt;Google Vertex AI API Keys&lt;/item&gt;
      &lt;item&gt;Firebase API Keys&lt;/item&gt;
      &lt;item&gt;Heroku API Keys&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Service Credentials:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stripe API Keys&lt;/item&gt;
      &lt;item&gt;Slack Tokens&lt;/item&gt;
      &lt;item&gt;GitHub Tokens &amp;amp; OAuth&lt;/item&gt;
      &lt;item&gt;GitLab Tokens&lt;/item&gt;
      &lt;item&gt;Mailgun, Mailchimp, Twilio API Keys&lt;/item&gt;
      &lt;item&gt;npm Tokens&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;LLM/AI Inference Provider Keys:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI API Keys (GPT-4, ChatGPT, etc.)&lt;/item&gt;
      &lt;item&gt;Anthropic API Keys (Claude)&lt;/item&gt;
      &lt;item&gt;Google Gemini &amp;amp; Vertex AI API Keys&lt;/item&gt;
      &lt;item&gt;Hugging Face Tokens&lt;/item&gt;
      &lt;item&gt;Cohere API Keys&lt;/item&gt;
      &lt;item&gt;OpenRouter API Keys&lt;/item&gt;
      &lt;item&gt;Replicate API Keys&lt;/item&gt;
      &lt;item&gt;Together AI API Keys&lt;/item&gt;
      &lt;item&gt;Perplexity AI API Keys&lt;/item&gt;
      &lt;item&gt;Mistral AI API Keys&lt;/item&gt;
      &lt;item&gt;AI21 Labs API Keys&lt;/item&gt;
      &lt;item&gt;Anyscale API Keys&lt;/item&gt;
      &lt;item&gt;DeepInfra API Keys&lt;/item&gt;
      &lt;item&gt;Groq API Keys&lt;/item&gt;
      &lt;item&gt;Fireworks AI API Keys&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Database Credentials:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MongoDB, PostgreSQL, MySQL, Redis connection strings&lt;/item&gt;
      &lt;item&gt;SQL Server connection strings&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Authentication:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;JWT Tokens&lt;/item&gt;
      &lt;item&gt;Bearer Tokens&lt;/item&gt;
      &lt;item&gt;OAuth Tokens&lt;/item&gt;
      &lt;item&gt;Session Tokens&lt;/item&gt;
      &lt;item&gt;Basic Auth credentials&lt;/item&gt;
      &lt;item&gt;API Keys&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sensitive Data:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Private SSH Keys&lt;/item&gt;
      &lt;item&gt;Credit Card Numbers&lt;/item&gt;
      &lt;item&gt;Social Security Numbers&lt;/item&gt;
      &lt;item&gt;Email Addresses&lt;/item&gt;
      &lt;item&gt;Phone Numbers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Other:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Webhook URLs&lt;/item&gt;
      &lt;item&gt;Callback URLs&lt;/item&gt;
      &lt;item&gt;Hardcoded passwords&lt;/item&gt;
      &lt;item&gt;Encrypted credentials in JavaScript&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the scanner detects potential secrets, it provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Severity classification (Critical, High, Medium, Low)&lt;/item&gt;
      &lt;item&gt;Context information showing where the secret was found&lt;/item&gt;
      &lt;item&gt;Actionable recommendations for remediation&lt;/item&gt;
      &lt;item&gt;Best practices for secure credential management&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FOR EDUCATIONAL AND AUTHORIZED TESTING PURPOSES ONLY&lt;/p&gt;
    &lt;p&gt;This tool is provided for educational purposes and authorized security testing only. By using this software, you agree to the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You may ONLY scan websites and applications that you own or have explicit written permission to test&lt;/item&gt;
      &lt;item&gt;Unauthorized scanning of third-party websites may be illegal in your jurisdiction&lt;/item&gt;
      &lt;item&gt;The authors and contributors are NOT responsible for any misuse or damage caused by this tool&lt;/item&gt;
      &lt;item&gt;Users are solely responsible for ensuring compliance with all applicable laws and regulations&lt;/item&gt;
      &lt;item&gt;This tool is provided "AS IS" without warranty of any kind, express or implied&lt;/item&gt;
      &lt;item&gt;The authors assume NO liability for any consequences resulting from the use or misuse of this software&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By using KeyLeak Detector, you acknowledge that you have read, understood, and agreed to these terms. If you do not agree, do not use this tool.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ DO scan your own websites and applications&lt;/item&gt;
      &lt;item&gt;✅ DO scan websites where you have explicit written authorization&lt;/item&gt;
      &lt;item&gt;✅ DO use for security research with proper permissions&lt;/item&gt;
      &lt;item&gt;✅ DO use for educational purposes in controlled environments&lt;/item&gt;
      &lt;item&gt;❌ DON'T scan websites without explicit permission&lt;/item&gt;
      &lt;item&gt;❌ DON'T use for malicious purposes&lt;/item&gt;
      &lt;item&gt;❌ DON'T share or exploit found credentials&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Always obtain written permission before scanning any system&lt;/item&gt;
      &lt;item&gt;Handle scan results securely and responsibly&lt;/item&gt;
      &lt;item&gt;If you find valid credentials, rotate them immediately&lt;/item&gt;
      &lt;item&gt;Report findings through responsible disclosure programs&lt;/item&gt;
      &lt;item&gt;Be cautious when scanning production environments&lt;/item&gt;
      &lt;item&gt;Understand and comply with applicable laws in your jurisdiction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;THE AUTHORS AND CONTRIBUTORS OF THIS SOFTWARE DISCLAIM ALL LIABILITY FOR ANY MISUSE, DAMAGES, OR LEGAL CONSEQUENCES ARISING FROM THE USE OF THIS TOOL. USERS ASSUME FULL RESPONSIBILITY FOR THEIR ACTIONS.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! Please feel free to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repository: https://github.com/Amal-David/keyleak-detector&lt;/item&gt;
      &lt;item&gt;Create a feature branch (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Commit your changes (&lt;code&gt;git commit -m 'Add some AmazingFeature'&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Push to the branch (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Open a Pull Request&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Found a bug or have a feature request? Please open an issue on GitHub: https://github.com/Amal-David/keyleak-detector/issues&lt;/p&gt;
    &lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;Created and maintained by Amal David&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Amal-David/keyleak-detector"/><published>2025-11-01T22:52:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45786324</id><title>Pomelli</title><updated>2025-11-02T04:14:08.340566+00:00</updated><content>&lt;doc fingerprint="71941a302cb5a8f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Create on-brand marketing content for your business with Pomelli&lt;/head&gt;
    &lt;p&gt;Creating impactful, on-brand content can often require significant investment in time, budget, and design expertise. For small to medium-sized businesses (SMBs), this can be a major obstacle. That’s where Pomelli, our newest experiment from Google Labs in partnership with Google DeepMind, comes in. Pomelli is an AI marketing tool designed to help SMBs more easily generate scalable, on-brand social media campaigns to help grow their businesses.&lt;/p&gt;
    &lt;p&gt;Pomelli uses AI to understand your unique business and generate effective, tailored campaigns in just three steps.&lt;/p&gt;
    &lt;p&gt;1. Build your business DNA&lt;/p&gt;
    &lt;p&gt;Enter your website, and Pomelli will analyze it and create a "Business DNA" profile for your brand. By analyzing your website and existing images, Pomelli is able to automatically extract and understand your business’ unique brand identity. This profile includes your tone of voice, custom fonts, images and color palette.&lt;/p&gt;
    &lt;p&gt;All content Pomelli generates is grounded in this DNA, ensuring your content feels more authentic and consistent across all channels.&lt;/p&gt;
    &lt;p&gt;2. Generate tailored campaign ideas&lt;/p&gt;
    &lt;p&gt;Once your Business DNA is established, Pomelli generates tailored campaign ideas specifically for your business. This feature tackles the common pain point of coming up with fresh, strategic ideas, allowing you to quickly pick a campaign focus. If you have your own idea, you can type in a prompt to create content tailored exactly to your vision.&lt;/p&gt;
    &lt;p&gt;3. Edit and create high-quality, branded creatives&lt;/p&gt;
    &lt;p&gt;Finally, Pomelli creates a set of high-quality, on-brand marketing assets designed to help grow your brand across various channels, like your social media, your site and your ads. Browse through the generations and select the assets that best fit your campaign goals. You're in full control to make edits to the text or images right inside the tool. All assets can be downloaded and are ready to be used across your channels.&lt;/p&gt;
    &lt;p&gt;Pomelli is launching today as a public beta experiment in the United States, Canada, Australia and New Zealand in English. It’s an early experiment and it might take some time to get things right. Our goal is to build the highest quality experiments, so your feedback is appreciated. Give it a try and let us know what you think.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/google-labs/pomelli/"/><published>2025-11-01T23:09:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45786738</id><title>How I use every Claude Code feature</title><updated>2025-11-02T04:14:08.017800+00:00</updated><content>&lt;doc fingerprint="36cf0efb2d9d82af"&gt;
  &lt;main&gt;
    &lt;p&gt;I use Claude Code. A lot.&lt;/p&gt;
    &lt;p&gt;As a hobbyist, I run it in a VM several times a week on side projects, often with &lt;code&gt;--dangerously-skip-permissions&lt;/code&gt; to vibe code whatever idea is on my mind. Professionally, part of my team builds the AI-IDE rules and tooling for our engineering team that consumes several billion tokens per month just for codegen.&lt;/p&gt;
    &lt;p&gt;The CLI agent space is getting crowded and between Claude Code, Gemini CLI, Cursor, and Codex CLI, it feels like the real race is between Anthropic and OpenAI. But TBH when I talk to other developers, their choice often comes down to what feels like superficials—a “lucky” feature implementation or a system prompt “vibe” they just prefer. At this point these tools are all pretty good. I also feel like folks often also over index on the output style or UI. Like to me the “you’re absolutely right!” sycophancy isn’t a notable bug; it’s a signal that you’re too in-the-loop. Generally my goal is to “shoot and forget”—to delegate, set the context, and let it work. Judging the tool by the final PR and not how it gets there.&lt;/p&gt;
    &lt;p&gt;Having stuck to Claude Code for the last few months, this post is my set of reflections on Claude Code’s entire ecosystem. We’ll cover nearly every feature I use (and, just as importantly, the ones I don’t), from the foundational &lt;code&gt;CLAUDE.md&lt;/code&gt; file and custom slash commands to the powerful world of Subagents, Hooks, and GitHub Actions. This post ended up a bit long and I’d recommend it as more of a reference than something to read in entirety. &lt;/p&gt;
    &lt;head rend="h2"&gt;CLAUDE.md&lt;/head&gt;
    &lt;p&gt;The single most important file in your codebase for using Claude Code effectively is the root &lt;code&gt;CLAUDE.md&lt;/code&gt;. This file is the agent’s “constitution,” its primary source of truth for how your specific repository works.&lt;/p&gt;
    &lt;p&gt;How you treat this file depends on the context. For my hobby projects, I let Claude dump whatever it wants in there.&lt;/p&gt;
    &lt;p&gt;For my professional work, our monorepo’s &lt;code&gt;CLAUDE.md&lt;/code&gt; is strictly maintained and currently sits at 13KB (I could easily see it growing to 25KB). &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;It only documents tools and APIs used by 30% (arbitrary) or more of our engineers (else tools are documented in product or library specific markdown files)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;We’ve even started allocating effectively a max token count for each internal tool’s documentation, almost like selling “ad space” to teams. If you can’t explain your tool concisely, it’s not ready for the&lt;/p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Tips and Common Anti-Patterns&lt;/head&gt;
    &lt;p&gt;Over time, we’ve developed a strong, opinionated philosophy for writing an effective &lt;code&gt;CLAUDE.md&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Start with Guardrails, Not a Manual. Your&lt;/p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;should start small, documenting based on what Claude is getting wrong.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Don’t&lt;/p&gt;&lt;code&gt;@&lt;/code&gt;-File Docs. If you have extensive documentation elsewhere, it’s tempting to&lt;code&gt;@&lt;/code&gt;-mention those files in your&lt;code&gt;CLAUDE.md&lt;/code&gt;. This bloats the context window by embedding the entire file on every run. But if you just mention the path, Claude will often ignore it. You have to pitch the agent on why and when to read the file. “For complex … usage or if you encounter a&lt;code&gt;FooBarError&lt;/code&gt;, see&lt;code&gt;path/to/docs.md&lt;/code&gt;for advanced troubleshooting steps.”&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Don’t Just Say “Never.” Avoid negative-only constraints like “Never use the&lt;/p&gt;&lt;code&gt;--foo-bar&lt;/code&gt;flag.” The agent will get stuck when it thinks it must use that flag. Always provide an alternative.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Use&lt;/p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;as a Forcing Function. If your CLI commands are complex and verbose, don’t write paragraphs of documentation to explain them. That’s patching a human problem. Instead, write a simple bash wrapper with a clear, intuitive API and document that. Keeping your&lt;code&gt;CLAUDE.md&lt;/code&gt;as short as possible is a fantastic forcing function for simplifying your codebase and internal tooling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a simplified snapshot:&lt;/p&gt;
    &lt;code&gt;# Monorepo

## Python
- Always ...
- Test with &amp;lt;command&amp;gt;
... 10 more ...

## &amp;lt;Internal CLI Tool&amp;gt;
... 10 bullets, focused on the 80% of use cases ...
- &amp;lt;usage example&amp;gt;
- Always ...
- Never &amp;lt;x&amp;gt;, prefer &amp;lt;Y&amp;gt;

For &amp;lt;complex usage&amp;gt; or &amp;lt;error&amp;gt; see path/to/&amp;lt;tool&amp;gt;_docs.md

...&lt;/code&gt;
    &lt;p&gt;Finally, we keep this file synced with an &lt;code&gt;AGENTS.md&lt;/code&gt; file to maintain compatibility with other AI IDEs that our engineers might be using.&lt;/p&gt;
    &lt;p&gt;If you are looking for more tips for writing markdown for coding agents see “AI Can’t Read Your Docs”, “AI-powered Software Engineering”, and “How Cursor (AI IDE) Works”.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Treat your &lt;code&gt;CLAUDE.md&lt;/code&gt; as a high-level, curated set of guardrails and pointers. Use it to guide where you need to invest in more AI (and human) friendly tools, rather than trying to make it a comprehensive manual.&lt;/p&gt;
    &lt;head rend="h2"&gt;Compact, Context, &amp;amp; Clear&lt;/head&gt;
    &lt;p&gt;I recommend running &lt;code&gt;/context&lt;/code&gt; mid coding session at least once to understand how you are using your 200k token context window (even with Sonnet-1M, I don’t trust that the full context window is actually used effectively). For us a fresh session in our monorepo costs a baseline ~20k tokens (10%) with the remaining 180k for making your change — which can fill up quite fast.&lt;/p&gt;
    &lt;p&gt;I have three main workflows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/compact&lt;/code&gt;(Avoid): I avoid this as much as possible. The automatic compaction is opaque, error-prone, and not well-optimized.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/clear&lt;/code&gt;+&lt;code&gt;/catchup&lt;/code&gt;(Simple Restart): My default reboot. I&lt;code&gt;/clear&lt;/code&gt;the state, then run a custom&lt;code&gt;/catchup&lt;/code&gt;command to make Claude read all changed files in my git branch.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;“Document &amp;amp; Clear” (Complex Restart): For large tasks. I have Claude dump its plan and progress into a&lt;/p&gt;&lt;code&gt;.md&lt;/code&gt;,&lt;code&gt;/clear&lt;/code&gt;the state, then start a new session by telling it to read the&lt;code&gt;.md&lt;/code&gt;and continue.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Takeaway: Don’t trust auto-compaction. Use &lt;code&gt;/clear&lt;/code&gt; for simple reboots and the “Document &amp;amp; Clear” method to create durable, external “memory” for complex tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Slash Commands&lt;/head&gt;
    &lt;p&gt;I think of slash commands as simple shortcuts for frequently used prompts, nothing more. My setup is minimal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/catchup&lt;/code&gt;: The command I mentioned earlier. It just prompts Claude to read all changed files in my current git branch.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/pr&lt;/code&gt;: A simple helper to clean up my code, stage it, and prepare a pull request.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IMHO if you have a long list of complex, custom slash commands, you’ve created an anti-pattern. To me the entire point of an agent like Claude is that you can type almost whatever you want and get a useful, mergable result. The moment you force an engineer (or non-engineer) to learn a new, documented-somewhere list of essential magic commands just to get work done, you’ve failed.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use slash commands as simple, personal shortcuts, not as a replacement for building a more intuitive &lt;code&gt;CLAUDE.md&lt;/code&gt; and better-tooled agent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Subagents&lt;/head&gt;
    &lt;p&gt;On paper, custom subagents are Claude Code’s most powerful feature for context management. The pitch is simple: a complex task requires &lt;code&gt;X&lt;/code&gt; tokens of input context (e.g., how to run tests), accumulates &lt;code&gt;Y&lt;/code&gt; tokens of working context, and produces a &lt;code&gt;Z&lt;/code&gt; token answer. Running &lt;code&gt;N&lt;/code&gt; tasks means &lt;code&gt;(X + Y + Z) * N&lt;/code&gt; tokens in your main window.&lt;/p&gt;
    &lt;p&gt;The subagent solution is to farm out the &lt;code&gt;(X + Y) * N&lt;/code&gt; work to specialized agents, which only return the final &lt;code&gt;Z&lt;/code&gt; token answers, keeping your main context clean.&lt;/p&gt;
    &lt;p&gt;I find they are a powerful idea that, in practice, custom subagents create two new problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;They Gatekeep Context: If I make a&lt;/p&gt;&lt;code&gt;PythonTests&lt;/code&gt;subagent, I’ve now hidden all testing context from my main agent. It can no longer reason holistically about a change. It’s now forced to invoke the subagent just to know how to validate its own code.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They Force Human Workflows: Worse, they force Claude into a rigid, human-defined workflow. I’m now dictating how it must delegate, which is the very problem I’m trying to get the agent to solve for me.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My preferred alternative is to use Claude’s built-in &lt;code&gt;Task(...)&lt;/code&gt; feature to spawn clones of the general agent.&lt;/p&gt;
    &lt;p&gt;I put all my key context in the &lt;code&gt;CLAUDE.md&lt;/code&gt;. Then, I let the main agent decide when and how to delegate work to copies of itself. This gives me all the context-saving benefits of subagents without the drawbacks. The agent manages its own orchestration dynamically.&lt;/p&gt;
    &lt;p&gt;In my “Building Multi-Agent Systems (Part 2)” post, I called this the “Master-Clone” architecture, and I strongly prefer it over the “Lead-Specialist” model that custom subagents encourage.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Custom subagents are a brittle solution. Give your main agent the context (in &lt;code&gt;CLAUDE.md&lt;/code&gt;) and let it use its own &lt;code&gt;Task/Explore(...)&lt;/code&gt; feature to manage delegation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Resume, Continue, &amp;amp; History&lt;/head&gt;
    &lt;p&gt;On a simple level, I use &lt;code&gt;claude --resume&lt;/code&gt; and &lt;code&gt;claude --continue&lt;/code&gt; frequently. They’re great for restarting a bugged terminal or quickly rebooting an older session. I’ll often &lt;code&gt;claude --resume&lt;/code&gt; a session from days ago just to ask the agent to summarize how it overcame a specific error, which I then use to improve our &lt;code&gt;CLAUDE.md&lt;/code&gt; and internal tooling.&lt;/p&gt;
    &lt;p&gt;More in the weeds, Claude Code stores all session history in &lt;code&gt;~/.claude/projects/&lt;/code&gt; to tap into the raw historical session data. I have scripts that run meta-analysis on these logs, looking for common exceptions, permission requests, and error patterns to help improve agent-facing context.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use &lt;code&gt;claude --resume&lt;/code&gt; and &lt;code&gt;claude --continue &lt;/code&gt;to restart sessions and uncover buried historical context.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hooks&lt;/head&gt;
    &lt;p&gt;Hooks are huge. I don’t use them for hobby projects, but they are critical for steering Claude in a complex enterprise repo. They are the deterministic “must-do” rules that complement the “should-do” suggestions in &lt;code&gt;CLAUDE.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We use two types:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Block-at-Submit Hooks: This is our primary strategy. We have a&lt;/p&gt;&lt;code&gt;PreToolUse&lt;/code&gt;hook that wraps any&lt;code&gt;Bash(git commit)&lt;/code&gt;command. It checks for a&lt;code&gt;/tmp/agent-pre-commit-pass&lt;/code&gt;file, which our test script only creates if all tests pass. If the file is missing, the hook blocks the commit, forcing Claude into a “test-and-fix” loop until the build is green.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hint Hooks: These are simple, non-blocking hooks that provide “fire-and-forget” feedback if the agent is doing something suboptimal.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We intentionally do not use “block-at-write” hooks (e.g., on &lt;code&gt;Edit&lt;/code&gt; or &lt;code&gt;Write&lt;/code&gt;). Blocking an agent mid-plan confuses or even “frustrates” it. It’s far more effective to let it finish its work and then check the final, completed result at the commit stage.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use hooks to enforce state validation at commit time (&lt;code&gt;block-at-submit&lt;/code&gt;). Avoid blocking at write time—let the agent finish its plan, then check the final result.&lt;/p&gt;
    &lt;head rend="h2"&gt;Planning Mode&lt;/head&gt;
    &lt;p&gt;Planning is essential for any “large” feature change with an AI IDE.&lt;/p&gt;
    &lt;p&gt;For my hobby projects, I exclusively use the built-in planning mode. It’s a way to align with Claude before it starts, defining both how to build something and the “inspection checkpoints” where it needs to stop and show me its work. Using this regularly builds a strong intuition for what minimal context is needed to get a good plan without Claude botching the implementation.&lt;/p&gt;
    &lt;p&gt;In our work monorepo, we’ve started rolling out a custom planning tool built on the Claude Code SDK. Its similar to native plan mode but heavily prompted to align its outputs with our existing technical design format. It also enforces our internal best practices—from code structure to data privacy and security—out of the box. This lets our engineers “vibe plan” a new feature as if they were a senior architect (or at least that’s the pitch).&lt;/p&gt;
    &lt;p&gt;The Takeaway: Always use the built-in planning mode for complex changes to align on a plan before the agent starts working.&lt;/p&gt;
    &lt;head rend="h2"&gt;Skills&lt;/head&gt;
    &lt;p&gt;I agree with Simon Willison’s: Skills are (maybe) a bigger deal than MCP.&lt;/p&gt;
    &lt;p&gt;If you’ve been following my posts, you’ll know I’ve drifted away from MCP for most dev workflows, preferring to build simple CLIs instead (as I argued in “AI Can’t Read Your Docs”). My mental model for agent autonomy has evolved into three stages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Single Prompt: Giving the agent all context in one massive prompt. (Brittle, doesn’t scale).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tool Calling: The “classic” agent model. We hand-craft tools and abstract away reality for the agent. (Better, but creates new abstractions and context bottlenecks).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scripting: We give the agent access to the raw environment—binaries, scripts, and docs—and it writes code on the fly to interact with them.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With this model in mind, Agent Skills are the obvious next feature. They are the formal productization of the “Scripting” layer.&lt;/p&gt;
    &lt;p&gt;If, like me, you’ve already been favoring CLIs over MCP, you’ve been implicitly getting the benefit of Skills all along. The &lt;code&gt;SKILL.md&lt;/code&gt; file is just a more organized, shareable, and discoverable way to document these CLIs and scripts and expose them to the agent.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Skills are the right abstraction. They formalize the “scripting”-based agent model, which is more robust and flexible than the rigid, API-like model that MCP represents.&lt;/p&gt;
    &lt;head rend="h2"&gt;MCP (Model Context Protocol)&lt;/head&gt;
    &lt;p&gt;Skills don’t mean MCP is dead (see also “Everything Wrong with MCP”). Previously, many built awful, context-heavy MCPs with dozens of tools that just mirrored a REST API (&lt;code&gt;read_thing_a()&lt;/code&gt;, &lt;code&gt;read_thing_b()&lt;/code&gt;, &lt;code&gt;update_thing_c()&lt;/code&gt;). &lt;/p&gt;
    &lt;p&gt;The “Scripting” model (now formalized by Skills) is better, but it needs a secure way to access the environment. This to me is the new, more focused role for MCP.&lt;/p&gt;
    &lt;p&gt;Instead of a bloated API, an MCP should be a simple, secure gateway that provides a few powerful, high-level tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;download_raw_data(filters…)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;take_sensitive_gated_action(args…)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;execute_code_in_environment_with_state(code…)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this model, MCP’s job isn’t to abstract reality for the agent; its job is to manage the auth, networking, and security boundaries and then get out of the way. It provides the entry point for the agent, which then uses its scripting and &lt;code&gt;markdown&lt;/code&gt; context to do the actual work.&lt;/p&gt;
    &lt;p&gt;The only MCP I still use is for Playwright, which makes sense—it’s a complex, stateful environment. All my stateless tools (like Jira, AWS, GitHub) have been migrated to simple CLIs.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use MCPs that act as data gateways. Give the agent one or two high-level tools (like a raw data dump API) that it can then script against.&lt;/p&gt;
    &lt;head rend="h2"&gt;Claude Code SDK&lt;/head&gt;
    &lt;p&gt;Claude Code isn’t just an interactive CLI; it’s also a powerful SDK for building entirely new agents—for both coding and non-coding tasks. I’ve started using it as my default agent framework over tools like LangChain/CrewAI for most new hobby projects.&lt;/p&gt;
    &lt;p&gt;I use it in three main ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Massive Parallel Scripting: For large-scale refactors, bug fixes, or migrations, I don’t use the interactive chat. I write simple bash scripts that call&lt;/p&gt;&lt;code&gt;claude -p “in /pathA change all refs from foo to bar”&lt;/code&gt;in parallel. This is far more scalable and controllable than trying to get the main agent to manage dozens of subagent tasks.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Building Internal Chat Tools: The SDK is perfect for wrapping complex processes in a simple chat interface for non-technical users. Like an installer that, on error, falls back to the Claude Code SDK to just fix the problem for the user. Or an in-house “v0-at-home” tool that lets our design team vibe-code mock frontends in our in-house UI framework, ensuring their ideas are high-fidelity and the code is more directly usable in frontend production code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rapid Agent Prototyping: This is my most common use. It’s not just for coding. If I have an idea for any agentic task (e.g., a “threat investigation agent” that uses custom CLIs or MCPs), I use the Claude Code SDK to quickly build and test the prototype before committing to a full, deployed scaffolding.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Takeaway: The Claude Code SDK is a powerful, general-purpose agent framework. Use it for batch-processing code, building internal tools, and rapidly prototyping new agents before you reach for more complex frameworks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Claude Code GHA&lt;/head&gt;
    &lt;p&gt;The Claude Code GitHub Action (GHA) is probably one of my favorite and most slept on features. It’s a simple concept: just run Claude Code in a GHA. But this simplicity is what makes it so powerful.&lt;/p&gt;
    &lt;p&gt;It’s similar to Cursor’s background agents or the Codex managed web UI but is far more customizable. You control the entire container and environment, giving you more access to data and, crucially, much stronger sandboxing and audit controls than any other product provides. Plus, it supports all the advanced features like Hooks and MCP.&lt;/p&gt;
    &lt;p&gt;We’ve used it to build custom “PR-from-anywhere” tooling. Users can trigger a PR from Slack, Jira, or even a CloudWatch alert, and the GHA will fix the bug or add the feature and return a fully tested PR1.&lt;/p&gt;
    &lt;p&gt;Since the GHA logs are the full agent logs, we have an ops process to regularly review these logs at a company level for common mistakes, bash errors, or unaligned engineering practices. This creates a data-driven flywheel: Bugs -&amp;gt; Improved CLAUDE.md / CLIs -&amp;gt; Better Agent.&lt;/p&gt;
    &lt;code&gt;$ query-claude-gha-logs --since 5d | claude -p “see what the other claudes were getting stuck on and fix it, then put up a PR“&lt;/code&gt;
    &lt;p&gt;The Takeaway: The GHA is the ultimate way to operationalize Claude Code. It turns it from a personal tool into a core, auditable, and self-improving part of your engineering system.&lt;/p&gt;
    &lt;head rend="h2"&gt;settings.json&lt;/head&gt;
    &lt;p&gt;Finally, I have a few specific &lt;code&gt;settings.json&lt;/code&gt; configurations that I’ve found essential for both hobby and professional work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;HTTPS_PROXY&lt;/code&gt;/&lt;code&gt;HTTP_PROXY&lt;/code&gt;: This is great for debugging. I’ll use it to inspect the raw traffic to see exactly what prompts Claude is sending. For background agents, it’s also a powerful tool for fine-grained network sandboxing.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MCP_TOOL_TIMEOUT&lt;/code&gt;/&lt;code&gt;BASH_MAX_TIMEOUT_MS&lt;/code&gt;: I bump these. I like running long, complex commands, and the default timeouts are often too conservative. I’m honestly not sure if this is still needed now that bash background tasks are a thing, but I keep it just in case.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;: At work, we use our enterprise API keys (via apiKeyHelper). It shifts us from a “per-seat” license to “usage-based” pricing, which is a much better model for how we work.&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;It accounts for the massive variance in developer usage (We’ve seen 1:100x differences between engineers).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;It lets engineers to tinker with non-Claude-Code LLM scripts, all under our single enterprise account.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;“permissions”&lt;/code&gt;: I’ll occasionally self-audit the list of commands I’ve allowed Claude to auto-run.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Takeaway: Your &lt;code&gt;settings.json&lt;/code&gt; is a powerful place for advanced customization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;That was a lot, but hopefully, you find it useful. If you’re not already using a CLI-based agent like Claude Code or Codex CLI, you probably should be. There are rarely good guides for these advanced features, so the only way to learn is to dive in.&lt;/p&gt;
    &lt;p&gt;To me, a fairly interesting philosophical question is how many reviewers should a PR get that was generated directly from a customer request (no internal human prompter)? We’ve settled on 2 human approvals for any AI-initiated PR for now, but it is kind of a weird paradigm shift (for me at least) when it’s no longer a human making something for another human to review.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.sshh.io/p/how-i-use-every-claude-code-feature"/><published>2025-11-02T00:13:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45786888</id><title>Why "everyone dies" gets AGI all wrong</title><updated>2025-11-02T04:14:07.945600+00:00</updated><content/><link href="https://bengoertzel.substack.com/p/why-everyone-dies-gets-agi-all-wrong"/><published>2025-11-02T00:40:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45786914</id><title>Anonymous credentials: rate-limit bots and agents without compromising privacy</title><updated>2025-11-02T04:14:07.251960+00:00</updated><content>&lt;doc fingerprint="a42ac4d20989ea8d"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The way we interact with the Internet is changing. Not long ago, ordering a pizza meant visiting a website, clicking through menus, and entering your payment details. Soon, you might just ask your phone to order a pizza that matches your preferences. A program on your device or on a remote server, which we call an AI agent, would visit the website and orchestrate the necessary steps on your behalf.&lt;/p&gt;
      &lt;p&gt;Of course, agents can do much more than order pizza. Soon we might use them to buy concert tickets, plan vacations, or even write, review, and merge pull requests. While some of these tasks will eventually run locally, for now, most are powered by massive AI models running in the biggest datacenters in the world. As agentic AI increases in popularity, we expect to see a large increase in traffic from these AI platforms and a corresponding drop in traffic from more conventional sources (like your phone).&lt;/p&gt;
      &lt;p&gt;This shift in traffic patterns has prompted us to assess how to keep our customers online and secure in the AI era. On one hand, the nature of requests are changing: Websites optimized for human visitors will have to cope with faster, and potentially greedier, agents. On the other hand, AI platforms may soon become a significant source of attacks, originating from malicious users of the platforms themselves.&lt;/p&gt;
      &lt;p&gt;Unfortunately, existing tools for managing such (mis)behavior are likely too coarse-grained to manage this transition. For example, when Cloudflare detects that a request is part of a known attack pattern, the best course of action often is to block all subsequent requests from the same source. When the source is an AI agent platform, this could mean inadvertently blocking all users of the same platform, even honest ones who just want to order pizza. We started addressing this problem earlier this year. But as agentic AI grows in popularity, we think the Internet will need more fine-grained mechanisms of managing agents without impacting honest users.&lt;/p&gt;
      &lt;p&gt;At the same time, we firmly believe that any such security mechanism must be designed with user privacy at its core. In this post, we'll describe how to use anonymous credentials (AC) to build these tools. Anonymous credentials help website operators to enforce a wide range of security policies, like rate-limiting users or blocking a specific malicious user, without ever having to identify any user or track them across requests.&lt;/p&gt;
      &lt;p&gt;Anonymous credentials are under development at IETF in order to provide a standard that can work across websites, browsers, platforms. It's still in its early stages, but we believe this work will play a critical role in keeping the Internet secure and private in the AI era. We will be contributing to this process as we work towards real-world deployment. This is still early days. If you work in this space, we hope you will follow along and contribute as well.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Letâs build a small agent&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;To help us discuss how AI agents are affecting web servers, letâs build an agent ourselves. Our goal is to have an agent that can order a pizza from a nearby pizzeria. Without an agent, you would open your browser, figure out which pizzeria is nearby, view the menu and make selections, add any extras (double pepperoni), and proceed to checkout with your credit card. With an agent, itâs the same flow âexcept the agent is opening and orchestrating the browser on your behalf.&lt;/p&gt;
      &lt;p&gt;In the traditional flow, thereâs a human all along the way, and each step has a clear intent: list all pizzerias within 3 Km of my current location; pick a pizza from the menu; enter my credit card; and so on. An agent, on the other hand, has to infer each of these actions from the prompt "order me a pizza."&lt;/p&gt;
      &lt;p&gt;In this section, weâll build a simple program that takes a prompt and can make outgoing requests. Hereâs an example of a simple Worker that takes a specific prompt and generates an answer accordingly. You can find the code on GitHub:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;export default {
   async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise&amp;lt;Response&amp;gt; {
       const out = await env.AI.run("@cf/meta/llama-3.1-8b-instruct-fp8", {
           prompt: `I'd like to order a pepperoni pizza with extra cheese.
                    Please deliver it to Cloudflare Austin office.
                    Price should not be more than $20.`,
       });


       return new Response(out.response);
   },
} satisfies ExportedHandler&amp;lt;Env&amp;gt;;&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;In this context, the LLM provides its best answer. It gives us a plan and instruction, but does not perform the action on our behalf. You and I are able to take a list of instructions and act upon it because we have agency and can affect the world. To allow our agent to interact with more of the world, weâre going to give it control over a web browser.&lt;/p&gt;
      &lt;p&gt;Cloudflare offers a Browser Rendering service that can bind directly into our Worker. Letâs do that. The following code uses Stagehand, an automation framework that makes it simple to control the browser. We pass it an instance of Cloudflare remote browser, as well as a client for Workers AI.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { Stagehand } from "@browserbasehq/stagehand";
import { endpointURLString } from "@cloudflare/playwright";
import { WorkersAIClient } from "./workersAIClient"; // wrapper to convert cloudflare AI


export default {
   async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise&amp;lt;Response&amp;gt; {
       const stagehand = new Stagehand({
           env: "LOCAL",
           localBrowserLaunchOptions: { cdpUrl: endpointURLString(env.BROWSER) },
           llmClient: new WorkersAIClient(env.AI),
           verbose: 1,
       });
       await stagehand.init();


       const page = stagehand.page;
       await page.goto("https://mini-ai-agent.cloudflareresearch.com/llm");


       const { extraction } = await page.extract("what are the pizza available on the menu?");
       return new Response(extraction);
   },
} satisfies ExportedHandler&amp;lt;Env&amp;gt;;&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;You can access that code for yourself on https://mini-ai-agent.cloudflareresearch.com/llm. Hereâs the response we got on October 10, 2025:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;Margherita Classic: $12.99
Pepperoni Supreme: $14.99
Veggie Garden: $13.99
Meat Lovers: $16.99
Hawaiian Paradise: $15.49&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Using the screenshot API of browser rendering, we can also inspect what the agent is doing. Here's how the browser renders the page in the example above:&lt;/p&gt;
      &lt;p&gt;Stagehand allows us to identify components on the page, such as &lt;code&gt;page.act(âClick on pepperoni pizzaâ)&lt;/code&gt; and &lt;code&gt;page.act(âClick on Pay nowâ)&lt;/code&gt;. This eases interaction between the developer and the browser.&lt;/p&gt;
      &lt;p&gt;To go further, and instruct the agent to perform the whole flow autonomously, we have to use the appropriately named agent mode of Stagehand. This feature is not yet supported by Cloudflare Workers, but is provided below for completeness.&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { Stagehand } from "@browserbasehq/stagehand";
import { endpointURLString } from "@cloudflare/playwright";
import { WorkersAIClient } from "./workersAIClient";


export default {
   async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise&amp;lt;Response&amp;gt; {
       const stagehand = new Stagehand({
           env: "LOCAL",
           localBrowserLaunchOptions: { cdpUrl: endpointURLString(env.BROWSER) },
           llmClient: new WorkersAIClient(env.AI),
           verbose: 1,
       });
       await stagehand.init();
       
       const agent = stagehand.agent();
       const result = await agent.execute(`I'd like to order a pepperoni pizza with extra cheese.
                                           Please deliver it to Cloudflare Austin office.
                                           Price should not be more than $20.`);


       return new Response(result.message);
   },
} satisfies ExportedHandler&amp;lt;Env&amp;gt;;&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;We can see that instead of adding step-by-step instructions, the agent is provided control. To actually pay, it would need access to a payment method such as a virtual credit card.&lt;/p&gt;
      &lt;p&gt;The prompt had some subtlety in that weâve scoped the location to Cloudflareâs Austin office. This is because while the agent responds to us, it needs to understand our context. In this case, the agent operates out of Cloudflare edge, a location remote to us. This implies we are unlikely to pick up a pizza from this data center if it was ever delivered.&lt;/p&gt;
      &lt;p&gt;The more capabilities we provide to the agent, the more it has the ability to create some disruption. Instead of someone having to make 5 clicks at a slow rate of 1 request per 10 seconds, theyâd have a program running in a data center possibly making all 5 requests in a second.&lt;/p&gt;
      &lt;p&gt;This agent is simple, but now imagine many thousands of these â some benign, some not â running at datacenter speeds. This is the challenge origins will face.&lt;/p&gt;
      &lt;p&gt;For humans to interact with the online world, they need a web browser and some peripherals with which to direct the behavior of that browser. Agents are another way of directing a browser, so it may be tempting to think that not much is actually changing from the origin's point of view. Indeed, the most obvious change from the origin's point of view is merely where traffic comes from:&lt;/p&gt;
      &lt;p&gt;The reason this change is significant has to do with the tools the server has to manage traffic. Websites generally try to be as permissive as possible, but they also need to manage finite resources (bandwidth, CPU, memory, storage, and so on). There are a few basic ways to do this:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Global security policy: A server may opt to slow down, CAPTCHA, or even temporarily block requests from all users. This policy may be applied to an entire site, a specific resource, or to requests classified as being part of a known or likely attack pattern. Such mechanisms may be deployed in reaction to an observed spike in traffic, as in a DDoS attack, or in anticipation of a spike in legitimate traffic, as in Waiting Room.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Incentives: Servers sometimes try to incentivize users to use the site when more resources are available. For instance, a server price may be lower depending on the location or request time. This could be implemented with a Cloudflare Snippet.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;While both tools can be effective, they also sometimes cause significant collateral damage. For example, while rate limiting a website's login endpoint can help prevent credential stuffing attacks, it also degrades the user experience for non-attackers. Before resorting to such measures, servers will first try to apply the security policy (whether a rate limit, a CAPTCHA, or an outright block) to individual users or groups of users.&lt;/p&gt;
      &lt;p&gt;However, in order to apply a security policy to individuals, the server needs some way of identifying them. Historically, this has been done via some combination of IP addresses, User-Agent, an account tied to the user identity (if available), and other fingerprints. Like most cloud service providers, Cloudflare has a dedicated offering for per-user rate limits based on such heuristics.&lt;/p&gt;
      &lt;p&gt;Fingerprinting works for the most part. However, it's unequitably distributed. On mobile, users have an especially difficult time solving CAPTCHAs, when using a VPN theyâre more likely to be blocked, and when using reading mode they can mess up their fingerprint, preventing rendering of the page.&lt;/p&gt;
      &lt;p&gt;Likewise, agentic AI only exacerbates the limitations of fingerprinting. Not only will more traffic be concentrated on a smaller source IP range, the agents themselves will run the same software and hardware platform, making it harder to distinguish honest from malicious users.&lt;/p&gt;
      &lt;p&gt;Something that could help is Web Bot Auth, which would allow agents to identify to the origin which platform they're operated by. However, we wouldn't want to extend this mechanism â intended for identifying the platform itself â to identifying individual users of the platforms, as this would create an unacceptable privacy risk for these users.&lt;/p&gt;
      &lt;p&gt;We need some way of implementing security controls for individual users without identifying them. But how? The Privacy Pass protocol provides a partial solution.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Privacy Pass and its limitations&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Today, one of the most prominent use cases for Privacy Pass is to rate limit requests from a user to an origin, as we have discussed before. The protocol works roughly as follows. The client is issued a number of tokens. Each time it wants to make a request, it redeems one of its tokens to the origin; the origin allows the request through only if the token is fresh, i.e., has never been observed before by the origin.&lt;/p&gt;
      &lt;p&gt;In order to use Privacy Pass for per-user rate-limiting, it's necessary to limit the number of tokens issued to each user (e.g., 100 tokens per user per hour). To rate limit an AI agent, this role would be fulfilled by the AI platform. To obtain tokens, the user would log in with the platform, and said platform would allow the user to get tokens from the issuer. The AI platform fulfills the attester role in Privacy Pass parlance. The attester is the party guaranteeing the per-user property of the rate limit. The AI platform, as an attester, is incentivized to enforce this token distribution as it stakes its reputation: Should it allow for too many tokens to be issued, the issuer could distrust them.&lt;/p&gt;
      &lt;p&gt;The issuance and redemption protocols are designed to have two properties:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Tokens are unforgeable: only the issuer can issue valid tokens.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Tokens are unlinkable: no party, including the issuer, attester, or origin, can tell which user a token was issued to. &lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;These properties can be achieved using a cryptographic primitive called a blind signature scheme. In a conventional signature scheme, the signer uses its private key to produce a signature for a message. Later on, a verifier can use the signerâs public key to verify the signature. Blind signature schemes work in the same way, except that the message to be signed is blinded such that the signer doesn't know the message it's signing. The client âblindsâ the message to be signed and sends it to the server, which then computes a blinded signature over the blinded message. The client obtains the final signature by unblinding the signature. &lt;/p&gt;
      &lt;p&gt;This is exactly how the standardised Privacy Pass issuance protocols are defined by RFC 9578:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt; Issuance: The user generates a random message $k$ which we call the nullifier. Concretely, this is just a random, 32-byte string. It then blinds the nullifier and sends it to the issuer. The issuer replies with a blind signature. Finally, the user unblinds the signature to get $\sigma$, a signature for the nullifier $k$. The token is the pair $(k, \sigma)$. &lt;/item&gt;
        &lt;item&gt; Redemption: When the user presents $(k, \sigma)$, the origin checks that $\sigma$ is a valid signature for the nullifier $k$ and that $k$ is fresh. If both conditions hold, then it accepts and lets the request through. &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Blind signatures are simple, cheap, and perfectly suited for many applications. However, they have some limitations that make them unsuitable for our use case.&lt;/p&gt;
      &lt;p&gt;First, the communication cost of the issuance protocol is too high. For each token issued, the user sends a 256-byte, blinded nullifier and the issuer replies with a 256-byte blind signature (assuming RSA-2048 is used). That's 0.5KB of additional communication per request, or 500KB for every 1,000 requests. This is manageable as weâve seen in a previous experiment for Privacy Pass, but not ideal. Ideally, the bandwidth would be sublinear in the rate limit we want to enforce. An alternative to blind signatures with lower compute time are Oblivious Pseudorandom Functions (VOPRF), but the bandwidth is still asymptotically linear. Weâve discussed them in the past, as they served as the basis for early deployments of Privacy Pass.&lt;/p&gt;
      &lt;p&gt;Second, blind signatures can't be used to rate-limit on a per-origin basis. Ideally, when issuing $N$ tokens to the client, the client would be able to redeem at most $N$ tokens at any origin server that can verify the token's validity. However, the client can't safely redeem the same token at more than one server because it would be possible for the servers to link those redemptions to the same client. What's needed is some mechanism for what we'll call late origin-binding: transforming a token for redemption at a particular origin in a way that's unlinkable to other redemptions of the same token.&lt;/p&gt;
      &lt;p&gt;Third, once a token is issued, it can't be revoked: it remains valid as long as the issuer's public key is valid. This makes it impossible for an origin to block a specific user if it detects an attack, or if its tokens are compromised. The origin can block the offending request, but the user can continue to make requests using its remaining token budget.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Anonymous credentials and the future of Privacy Pass&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;As noted by Chaum in 1985, an anonymous credential system allows users to obtain a credential from an issuer, and later prove possession of this credential, in an unlinkable way, without revealing any additional information. Also, it is possible to demonstrate that some attributes are attached to the credential.&lt;/p&gt;
      &lt;p&gt;One way to think of an anonymous credential is as a kind of blind signature with some additional capabilities: late-binding (link a token to an origin after issuance), multi-show (generate multiple tokens from a single issuer response), and expiration distinct from key rotation (token validity decoupled of the issuer cryptographic key validity). In the redemption flow for Privacy Pass, the client presents the unblinded message and signature to the server. To accept the redemption, the server needs to verify the signature. In an AC system, the client only presents a part of the message. In order for the server to accept the request, the client needs to prove to the server that it knows a valid signature for the entire message without revealing the whole thing.&lt;/p&gt;
      &lt;p&gt;The flow we described above would therefore include this additional presentation step. &lt;/p&gt;
      &lt;p&gt;Note that the tokens generated through blind signatures or VOPRFs can only be used once, so they can be regarded as single-use tokens. However, there exists a type of anonymous credentials that allows tokens to be used multiple times. For this to work, the issuer grants a credential to the user, who can later derive at most N many single-use tokens for redemption. Therefore, the user can send multiple requests, at the expense of a single issuance session. &lt;/p&gt;
      &lt;p&gt;The table below describes how blind signatures and anonymous credentials provide features of interest to rate limiting.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Feature&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Blind Signature&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Anonymous Credential&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Issuing Cost&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Linear complexity: issuing 10 signatures is 10x as expensive as issuing one signature&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Sublinear complexity: signing 10 attributes is cheaper than 10 individual signatures&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Proof Capability&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Only prove that a message has been signed&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Allow efficient proving of partial statements (i.e., attributes)&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;State Management&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Stateless&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Stateful&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Attributes&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;No attributes&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Public (e.g. expiry time) and private state&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt; Let's see how a simple anonymous credential scheme works. The client's message consists of the pair $(k, C)$, where $k$ is a nullifier and $C$ is a counter representing the remaining number of times the client can access a resource. The value of the counter is controlled by the server: when the client redeems its credential, it presents both the nullifier and the counter. In response, the server checks that signature of the message is valid and that the nullifier is fresh, as before. Additionally, the server also &lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;checks that the counter is greater than zero; and&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;decrements the counter issuing a new credential for the updated counter and a fresh nullifier.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;A blind signature could be used to meet this functionality. However, whereas the nullifier can be blinded as before, it would be necessary to handle the counter in plaintext so that the server can check that the counter is valid (Step 1) and update it (Step 2). This creates an obvious privacy risk since the server, which is in control of the counter, can use it to link multiple presentations by the same client. For example, when you reach out to buy a pepperoni pizza, the origin could assign you a special counter value, which eases fingerprinting when you present it a second time. Fortunately, there exist anonymous credentials designed to close this kind of privacy gap.&lt;/p&gt;
      &lt;p&gt;The scheme above is a simplified version of Anonymous Credit Tokens (ACT), one of the anonymous credential schemes being considered for adoption by the Privacy Pass working group at IETF. The key feature of ACT is its statefulness: upon successful redemption, the server re-issues a new credential with updated nullifier and counter values. This creates a feedback loop between the client and server that can be used to express a variety of security policies.&lt;/p&gt;
      &lt;p&gt;By design, it's not possible to present ACT credentials multiple times simultaneously: the first presentation must be completed so that the re-issued credential can be presented in the next request. Parallelism is the key feature of Anonymous Rate-limited Credential (ARC), another scheme under discussion at the Privacy Pass working group. ARCs can be presented across multiple requests in parallel up to the presentation limit determined during issuance.&lt;/p&gt;
      &lt;p&gt;Another important feature of ARC is its support for late origin-binding: when a client is issued an ARC with presentation limit $N$, it can safely use its credential to present up to $N$ times to any origin that can verify the credential.&lt;/p&gt;
      &lt;p&gt;These are just examples of relevant features of some anonymous credentials. Some applications may benefit from a subset of them; others may need additional features. Fortunately, both ACT and ARC can be constructed from a small set of cryptographic primitives that can be easily adapted for other purposes.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Building blocks for anonymous credentials&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;ARC and ACT share two primitives in common: algebraic MACs, which provide for limited computations on the blinded message; and zero-knowledge proofs (ZKP) for proving validity of the part of the message not revealed to the server. Let's take a closer look at each.&lt;/p&gt;
      &lt;p&gt;A Message Authenticated Code (MAC) is a cryptographic tag used to verify a message's authenticity (that it comes from the claimed sender) and integrity (that it has not been altered). Algebraic MACs are built from mathematical structures like group actions. The algebraic structure gives them some additional functionality, one of them being a homomorphism that we can blind easily to conceal the actual value of the MAC. Adding a random value on an algebraic MAC blinds the value.&lt;/p&gt;
      &lt;p&gt;Unlike blind signatures, both ACT and ARC are only privately verifiable, meaning the issuer and the origin must both have the issuer's private key. Taking Cloudflare as an example, this means that a credential issued by Cloudflare can only be redeemed by an origin behind Cloudflare. Publicly verifiable variants of both are possible, but at an additional cost.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Zero-Knowledge Proofs for linear relations&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Zero knowledge proofs (ZKP) allow us to prove a statement is true without revealing the exact value that makes the statement true. The ZKP is constructed by a prover in such a way that it can only be generated by someone who actually possesses the secret. The verifier can then run a quick mathematical check on this proof. If the check passes, the verifier is convinced that the prover's initial statement is valid. The crucial property is that the proof itself is just data that confirms the statement; it contains no other information that could be used to reconstruct the original secret.&lt;/p&gt;
      &lt;p&gt;For ARC and ACT, we want to prove linear relations of secrets. In ARC, a user needs to prove that different tokens are linked to the same original secret credential. For example, a user can generate a proof showing that a request token was derived from a valid issued credential. The system can verify this proof to confirm the tokens are legitimately connected, all without ever learning the underlying secret credential that ties them together. This allows the system to validate user actions while guaranteeing their privacy.&lt;/p&gt;
      &lt;p&gt;Proving simple linear relations can be extended to prove a number of powerful statements, for example that a number is in range. For example, this is useful to prove that you have a positive balance on your account. To prove your balance is positive, you prove that you can encode your balance in binary. Letâs say you can at most have 1024 credits in your account. To prove your balance is non-zero when it is, for example, 12, you prove two things simultaneously: first, that you have a set of binary bits, in this case 12=(1100)2, and second, that a linear equation using these bits (8*1 + 4*1 + 2*0 + 1*0) correctly adds up to your total committed balance. This convinces the verifier that the number is validly constructed without them learning the exact value. This is how it works for powers of two, but it can easily be extended to arbitrary ranges.&lt;/p&gt;
      &lt;p&gt;The mathematical structure of algebraic MACs allows easy blinding and evaluation. The structure also allows for an easy proof that a MAC has been evaluated with the private key without revealing the MAC. In addition, ARC could use ZKPs to prove that a nonce has not been spent before. In contrast, ACT uses ZKPs to prove we have enough of a balance left on our token. The balance is subtracted homomorphically using more group structure.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;How much does this all cost?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Anonymous credentials allow for more flexibility, and have the potential to reduce the communication cost, compared to blind signatures in certain applications. To identify such applications, we need to measure the concrete communication cost of these new protocols. In addition, we need to understand how their CPU usage compares to blind signatures and oblivious pseudorandom functions.&lt;/p&gt;
      &lt;p&gt;We measure the time that each participant spends at each stage of some AC schemes. We also report the size of messages transmitted across the network. For ARC, ACT, and VOPRF, we'll use ristretto255 as the prime group and SHAKE128 for hashing. For Blind RSA, we'll use a 2048-bit modulus and SHA-384 for hashing.&lt;/p&gt;
      &lt;p&gt;Each algorithm was implemented in Go, on top of the CIRCL library. We plan to open source the code once the specifications of ARC and ACT begin to stabilize.&lt;/p&gt;
      &lt;p&gt;Letâs take a look at the most widely used deployment in Privacy Pass: Blind RSA. Redemption time is low, and most of the cost lies with the server at issuance time. Communication cost is mostly constant and in the order of 256 bytes.&lt;/p&gt;
      &lt;div&gt;
        &lt;table&gt;
          &lt;row&gt;
            &lt;cell class="tg-amwm" colspan="2" rowspan="2" role="head"&gt;Blind RSA&lt;lb/&gt;RFC9474(RSA-2048+SHA384)&lt;/cell&gt;
            &lt;cell class="tg-baqh" colspan="2" role="head"&gt;1 Token&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-baqh" role="head"&gt;Time&lt;/cell&gt;
            &lt;cell class="tg-baqh" role="head"&gt;Message Size&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax" rowspan="3"&gt;Issuance&lt;/cell&gt;
            &lt;cell class="tg-0lax"&gt;Client (Blind)&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;63 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;256 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Server (Evaluate)&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;2.69 ms&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;256 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Client (Finalize)&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;37 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;256 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax" rowspan="2"&gt;Redemption&lt;/cell&gt;
            &lt;cell class="tg-0lax"&gt;Client&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt; â&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;300 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Server&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;37 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;â&lt;/cell&gt;
          &lt;/row&gt;
        &lt;/table&gt;
      &lt;/div&gt;
      &lt;p&gt;When looking at VOPRF, verification time on the server is slightly higher than for Blind RSA, but communication cost and issuance are much faster. Evaluation time on the server is 10x faster for 1 token, and more than 25x faster when using amortized token issuance. Communication cost per token is also more appealing, with a message size at least 3x lower.&lt;/p&gt;
      &lt;div&gt;
        &lt;table&gt;
          &lt;row&gt;
            &lt;cell class="tg-amwm" colspan="2" rowspan="2" role="head"&gt;VOPRF&lt;lb/&gt;RFC9497(Ristretto255+SHA512)&lt;/cell&gt;
            &lt;cell class="tg-baqh" colspan="2" role="head"&gt;1 Token&lt;/cell&gt;
            &lt;cell class="tg-baqh" colspan="2" role="head"&gt;1000 Amortized issuances&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-baqh" role="head"&gt;Time&lt;/cell&gt;
            &lt;cell class="tg-baqh" role="head"&gt;Message Size&lt;/cell&gt;
            &lt;cell class="tg-baqh" role="head"&gt;Time &lt;lb/&gt;(per token)&lt;/cell&gt;
            &lt;cell class="tg-baqh" role="head"&gt;Message Size &lt;lb/&gt;(per token)&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax" rowspan="3"&gt;Issuance&lt;/cell&gt;
            &lt;cell class="tg-0lax"&gt;Client (Blind)&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;54 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;32 B&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;54 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;32 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Server (Evaluate)&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;260 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;96 B&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;99 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;32.064 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Client (Finalize)&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;376 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;64 B&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;173 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;64 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax" rowspan="2"&gt;Redemption&lt;/cell&gt;
            &lt;cell class="tg-0lax"&gt;Client&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt; â&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;96 B&lt;/cell&gt;
            &lt;cell class="tg-baqh" colspan="2" rowspan="2"&gt;â&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Server&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;57 Âµs&lt;/cell&gt;
            &lt;cell class="tg-lqy6"&gt;â&lt;/cell&gt;
          &lt;/row&gt;
        &lt;/table&gt;
      &lt;/div&gt;
      &lt;p&gt;This makes VOPRF tokens appealing for applications requiring a lot of tokens that can accept a slightly higher redemption cost, and that donât need public verifiability.&lt;/p&gt;
      &lt;p&gt;Now, letâs take a look at the figures for ARC and ACT anonymous credential schemes. For both schemes we measure the time to issue a credential that can be presented at most $N=1000$ times.&lt;/p&gt;
      &lt;div&gt;
        &lt;table&gt;
          &lt;row&gt;
            &lt;cell class="tg-sd4m" rowspan="2" role="head"&gt;Issuance&lt;lb/&gt;Credential Generation&lt;/cell&gt;
            &lt;cell class="tg-har8" colspan="2" role="head"&gt;ARC&lt;/cell&gt;
            &lt;cell class="tg-y4nt" colspan="2" role="head"&gt;ACT&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-7nm9" role="head"&gt;Time&lt;/cell&gt;
            &lt;cell class="tg-7nm9" role="head"&gt;Message Size&lt;/cell&gt;
            &lt;cell class="tg-8mqg" role="head"&gt;Time&lt;/cell&gt;
            &lt;cell class="tg-8mqg" role="head"&gt;Message Size&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-ktyi"&gt;Client (Request)&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;323 Âµs&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;224 B&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;64 Âµs&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;141 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Server (Response)&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;1349 Âµs&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;448 B&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;251 Âµs&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;176 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-ktyi"&gt;Client (Finalize)&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;1293 Âµs&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;128 B&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;204 Âµs&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;176 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row/&gt;
          &lt;row&gt;
            &lt;cell class="tg-dgl5" rowspan="2"&gt;Redemption&lt;lb/&gt;Credential Presentation&lt;/cell&gt;
            &lt;cell class="tg-har8" colspan="2"&gt;ARC&lt;/cell&gt;
            &lt;cell class="tg-y4nt" colspan="2"&gt;ACT&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-7nm9"&gt;Time&lt;/cell&gt;
            &lt;cell class="tg-7nm9"&gt;Message Size&lt;/cell&gt;
            &lt;cell class="tg-8mqg"&gt;Time&lt;/cell&gt;
            &lt;cell class="tg-8mqg"&gt;Message Size&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-ktyi"&gt;Client (Present)&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;735 Âµs&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;288 B&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt; 1740 Âµs&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;1867 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-ktyi"&gt;Server (Verify/Refund)&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;740 Âµs&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;â&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;1785 Âµs&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;141 B&lt;/cell&gt;
          &lt;/row&gt;
          &lt;row&gt;
            &lt;cell class="tg-0lax"&gt;Client (Update)&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;â&lt;/cell&gt;
            &lt;cell class="tg-l4d4"&gt;â&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;508 Âµs&lt;/cell&gt;
            &lt;cell class="tg-dzqp"&gt;176 B&lt;/cell&gt;
          &lt;/row&gt;
        &lt;/table&gt;
      &lt;/div&gt;
      &lt;p&gt;As we would hope, the communication cost and the serverâs runtime is much lower than a batched issuance with either Blind RSA or VOPRF. For example, a VOPRF issuance of 1000 tokens takes 99 ms (99 Âµs per token) vs 1.35 ms for issuing one ARC credential that allows for 1000 presentations. This is about 70x faster. The trade-off is that presentation is more expensive, both for the client and server.&lt;/p&gt;
      &lt;p&gt;How about ACT? Like ARC, we would expect the communication cost of issuance grows much slower with respect to the credits issued. Our implementation bears this out. However, there are some interesting performance differences between ARC and ACT: issuance is much cheaper for ACT than it is for ARC, but redemption is the opposite.&lt;/p&gt;
      &lt;p&gt;What's going on? The answer has largely to do with what each party needs to prove with ZKPs at each step. For example, during ACT redemption, the client proves to the server (in zero-knowledge) that its counter $C$ is in the desired range, i.e., $0 \leq C \leq N$. The proof size is on the order of $\log_{2} N$, which accounts for the larger message size. In the current version, ARC redemption does not involve range proofs, but a range proof may be added in a future version. Meanwhile, the statements the client and server need to prove during ARC issuance are a bit more complicated than for ARC presentation, which accounts for the difference in runtime there.&lt;/p&gt;
      &lt;p&gt;The advantage of anonymous credentials, as discussed in the previous sections, is that issuance only has to be performed once. When a server evaluates its cost, it takes into account the cost of all issuances and the cost of all verifications. At present, only accounting for credentials costs, itâs cheaper for a server to issue and verify tokens than verify an anonymous credential presentation.&lt;/p&gt;
      &lt;p&gt;The advantage of multiple-use anonymous credentials is that instead of the issuer generating $N$ tokens, the bulk of computation is offloaded to the clients. This is more scoped. Late origin binding allows them to work for multiple origins/namespace, range proof to decorrelate expiration from key rotation, and refund to provide a dynamic rate limit. Their current applications are dictated by the limitation of single-use token based schemes, more than by the added efficiency they provide. This seems to be an exciting area to explore, and see if closing the gap is possible.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Managing agents with anonymous credentials&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Managing agents will likely require features from both ARC and ACT.&lt;/p&gt;
      &lt;p&gt;ARC already has much of the functionality we need: it supports rate limiting, is communication-efficient, and it supports late origin-binding. Its main downside is that, once an ARC credential is issued, it can't be revoked. A malicious user can always make up to N requests to any origin it wants.&lt;/p&gt;
      &lt;p&gt;We can allow for a limited form of revocation by pairing ARC with blind signatures (or VOPRF). Each presentation of the ARC credential is accompanied by a Privacy Pass token: upon successful presentation, the client is issued another Privacy Pass token it can use during the next presentation. To revoke a credential, the server would simply not re-issue the token:&lt;/p&gt;
      &lt;p&gt;This scheme is already quite useful. However, it has some important limitations:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Parallel presentation across origins is not possible: the client must wait for the request to one origin to succeed before it can initiate a request to a second origin.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Revocation is global rather than per-origin, meaning the credential is not only revoked for the origin to whom it was presented, but for every origin it can be presented to. We suspect this will be undesirable in some cases. For example, an origin may want to revoke if a request violates its &lt;code&gt;robots.txt&lt;/code&gt; policy; but the same request may have been accepted by other origins.  &lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;A more fundamental limitation of this design is that the decision to revoke can only be made on the basis of a single request â the one in which the credential was presented. It may be risky to decide to block a user on the basis of a single request; in practice, attack patterns may only emerge across many requests. ACT's statefulness enables at least a rudimentary form of this kind of defense. Consider the following scheme:&lt;/p&gt;
      &lt;p&gt;Benign requests wouldn't change the state by much (if at all), but suspicious requests might impact the state in a way that gets the user closer to their rate limit much faster.&lt;/p&gt;
      &lt;p&gt;To see how this idea works in practice, let's look at a working example that uses the Model Context Protocol. The demo below is built using MCP Tools. Tools are extensions the AI agent can call to extend its capabilities. They don't need to be integrated at release time within the MCP client. This provides a nice and easy prototyping avenue for anonymous credentials.&lt;/p&gt;
      &lt;p&gt;Tools are offered by the server via an MCP compatible interface. You can see details on how to build such MCP servers in a previous blog.&lt;/p&gt;
      &lt;p&gt;In our pizza context, this could look like a pizzeria that offers you a voucher. Each voucher gets you 3 pizza slices. Mocking a design, an integration within a chat application could look as follows:&lt;/p&gt;
      &lt;p&gt;The first panel presents all tools exposed by the MCP server. The second one showcases an interaction performed by the agent calling these tools.&lt;/p&gt;
      &lt;p&gt;To look into how such a flow would be implemented, letâs write the MCP tools, offer them in an MCP server, and manually orchestrate the calls with the MCP Inspector.&lt;/p&gt;
      &lt;p&gt;The MCP server should provide two tools:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;&lt;code&gt;act-issue &lt;/code&gt;which issues an ACT credential valid for 3 requests. The code used here is an earlier version of the IETF draft which has some limitations.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;&lt;code&gt;act-redeem&lt;/code&gt; makes a presentation of the local credential, and fetches our pizza menu.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;First, we run &lt;code&gt;act-issue&lt;/code&gt;. At this stage, we could ask the agent to run an OAuth flow, fetch an internal authentication endpoint, or to compute a proof of work.&lt;/p&gt;
      &lt;p&gt;This gives us 3 credits to spend against an origin. Then, we run &lt;code&gt;act-redeem&lt;/code&gt;&lt;/p&gt;
      &lt;p&gt;Et voilÃ . If we run &lt;code&gt;act-redeem&lt;/code&gt; once more, we see we have one fewer credit.&lt;/p&gt;
      &lt;p&gt;You can test it yourself, here are the source codes available. The MCP server is written in Rust to integrate with the ACT rust library. The browser-based client works similarly, check it out.&lt;/p&gt;
      &lt;p&gt;In this post, weâve presented a concrete approach to rate limit agent traffic. It is in full control of the client, and is built to protect the user's privacy. It uses emerging standards for anonymous credentials, integrates with MCP, and can be readily deployed on Cloudflare Workers.&lt;/p&gt;
      &lt;p&gt;We're on the right track, but there are still questions that remain. As we touched on before, a notable limitation of both ARC and ACT is that they are only privately verifiable. This means that the issuer and origin need to share a private key, for issuing and verifying the credential respectively. There are likely to be deployment scenarios for which this isn't possible. Fortunately, there may be a path forward for these cases using pairing-based cryptography, as in the BBS signature specification making its way through IETF. Weâre also exploring post-quantum implications in a concurrent post.&lt;/p&gt;
      &lt;p&gt;If you are an agent platform, an agent developer, or a browser, all our code is available on GitHub for you to experiment. Cloudflare is actively working on vetting this approach for real-world use cases.&lt;/p&gt;
      &lt;p&gt;The specification and discussion are happening within the IETF and W3C. This ensures the protocols are built in the open, and receive participation from experts. Improvements are still to be made to clarify the right performance-to-privacy tradeoff, or even the story to deploy on the open web.&lt;/p&gt;
      &lt;p&gt;If youâd like to help us, weâre hiring 1,111 interns over the course of next year, and have open positions.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/private-rate-limiting/"/><published>2025-11-02T00:45:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45787036</id><title>A Few Words About Async</title><updated>2025-11-02T04:14:06.965976+00:00</updated><content>&lt;doc fingerprint="767298572c2192f8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;(Quite) A Few Words About Async&lt;/head&gt;
    &lt;p&gt;I’ve had a few conversations about async code recently (and not so recently) and seen some code that seems to make wrong assumptions about async, so I figured out it was time to have a serious chat about async, what it’s for, what it guarantees and what it doesn’t.&lt;/p&gt;
    &lt;p&gt;Most of the code in this entry will be written with Python syntax (and often Python libraries), but with a few minor exceptions, we’ll be discussing concepts that are valid across languages.&lt;/p&gt;
    &lt;head rend="h1"&gt;There’s more than one kind of performance&lt;/head&gt;
    &lt;p&gt;We all know about performance, right? I mean, there is some task we want our code to do (display something on screen, or request some data from a database, or download a file, or solve a mathematical problem, …) and that task should be finished as fast as possible. For bonus points, it should use as little memory as possible.&lt;/p&gt;
    &lt;p&gt;Right?&lt;/p&gt;
    &lt;p&gt;Well… not quite.&lt;/p&gt;
    &lt;p&gt;This kind of performance is called throughput. And having a high throughput is almost always a good thing. But in the 21st century, focusing on throughput is generally the wrong goal.&lt;/p&gt;
    &lt;p&gt;You see, these days, most applications1 look something like this:&lt;/p&gt;
    &lt;code&gt;while True:
    while (event := get_next_event()):
        on_event(event)
    wait_until_there_is_an_event()&lt;/code&gt;
    &lt;p&gt;Maybe you’re writing that loop yourself, but usually, not. You might be writing &lt;code&gt;asyncio.run&lt;/code&gt; or &lt;code&gt;#[tokio::main]&lt;/code&gt; or &lt;code&gt;Eio_main.run&lt;/code&gt;, or
just running your code in the browser or Node.js or BEAM,
or in plenty of other ways, but you’re not escaping the event loop.&lt;/p&gt;
    &lt;p&gt;If you’re writing a video game, you receive an event whenever the user presses a key, and also whenever it’s time to repaint the screen. If you’re writing a GUI tool, you receive an event whenever the user clicks on a button. If you’re writing a web server, you get an event whenever you receive a connection or some data.&lt;/p&gt;
    &lt;p&gt;And yes, you want to finish the task quickly. In fact, if you’re writing any kind of user-facing application, whether it’s a text processor or a video game, you have about 16ms to finish the task. You can do many things in 16ms. But there are also quite a few things that you can’t do, including opening a file 2 or getting a response from your web server 3.&lt;/p&gt;
    &lt;p&gt;If you’re writing a web server, you have more leeway. In most cases, you can afford to wait 1 second, possibly even 2. But there are plenty of tasks that your web server may need to complete and that will take more than 2 seconds. For instance, extracting lots of data from a busy database, or getting anything remotely coherent from a LLM.&lt;/p&gt;
    &lt;p&gt;So… now what?&lt;/p&gt;
    &lt;p&gt;Now we need to redefine performance. And in fact, there are plenty of definitions of performance. The one we’re going to focus on in this discussion latency: how long until something happens. You may not have finished opening your file, getting a response from your web server, or received anything that looks remotely coherent from Claude, but you need to respond something quickly.&lt;/p&gt;
    &lt;p&gt;Also, if you’re interested in the notion of Response Time Limits, it dates back to 1993: https://www.nngroup.com/articles/response-times-3-important-limits/. I seem to recall that Microsoft lead some further research when they were developing the original Surface tables (before the tablets). DoubleClick/Google also published some additional refinements in the specific case of web applications and mobile web applications. Sadly, I haven’t found the links.&lt;/p&gt;
    &lt;head rend="h1"&gt;The need for non-blocking code&lt;/head&gt;
    &lt;p&gt;Let’s start with a simple example:&lt;/p&gt;
    &lt;code&gt;class ComputeFibonacciEvent:
    arg: int

def fibonacci(n: int) -&amp;gt; int:
    if n &amp;lt;= 1:
        return 1
    return fibonacci(n - 1) + fibonacci(n - 2)

def on_event(event):
    if isinstance(event, ComputeFibonacciEvent):
        result = fibonacci(event.arg)
        print(f"fibonacci({event.arg})={result}")
    else:
        ...&lt;/code&gt;
    &lt;p&gt;Yes, I’m very aware that you can rewrite Fibonacci in a more efficient manner, buy let’s keep this awfully inefficient implementation. Feel free to replace this with any other task that is slow to execute.&lt;/p&gt;
    &lt;p&gt;Now, does our event loop fit within our 16ms budget? For a sufficiently large value of &lt;code&gt;arg&lt;/code&gt;, it might not. But if we exceed our 16ms budget, we are blocking
our application from repainting the screen, or taking new HTTP requests, etc.
and that’s bad.&lt;/p&gt;
    &lt;p&gt;So how do we make our computation fit?&lt;/p&gt;
    &lt;p&gt;Well, there are many solutions, but all of them are variants around the following idea:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Make&lt;/p&gt;&lt;code&gt;fibonacci&lt;/code&gt;non-blocking.&lt;/quote&gt;
    &lt;head rend="h1"&gt;A few more definitions&lt;/head&gt;
    &lt;p&gt;Non-blocking is not the most common word you’ll find around the web. You’ll often read about asynchronous, concurrent, parallel. These are four distinct concepts that people tend to confuse hopelessly.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Code is non-blocking if it never blocks any critical thread.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In this conversation, we’re interested in the thread containing the event loop. Non-blocking is an objective. It’s also a guarantee provided by some functions in your libraries or your operating system.&lt;/p&gt;
    &lt;p&gt;How do you achieve this? Well, that’s what this entire post is all about.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Code is asynchronous if it structured to present explicit dependencies between executions.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Asynchronous is about code structure. Typically, this involves callbacks, or events, or some abstraction on top of them.&lt;/p&gt;
    &lt;p&gt;Asynchronous does NOT guarantee that your code is non-blocking. In fact, the only guarantee is that if you refactor your code to become non-blocking, you won’t break everything.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Code is concurrent if you can schedule independent tasks to run.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Concurrency is also a programming style. Concurrency does not guarantee when the tasks run. There are concurrency toolkits that simply wait until a task is complete before running the next one. There are also concurrency primitives that will interleave the execution of two concurrent tasks, attempting to ensure that each of them progresses regularly. If this is done automatically, this is called preemptive multitasking. If the code requires specific annotations, this is called cooperative multitasking. Most developers use the word “concurrent” only if it involves some kind of multitasking.&lt;/p&gt;
    &lt;p&gt;Concurrency also does not guarantee that an operation is non-blocking.&lt;/p&gt;
    &lt;p&gt;Concurrent and asynchronous are often confused, but they’re different things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;code can be concurrent without being asynchronous, e.g. if you launch multiple tasks and synchronize them implicitly either by assuming run-to-completion or by using locks;&lt;/item&gt;
      &lt;item&gt;code can be asynchronous without being concurrent, e.g. Haskell is all about letting you specify dependencies, but without &lt;code&gt;par&lt;/code&gt;, you can’t launch tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Code is parallel if two tasks can run at the same physical instant.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Parallelism is a property of the language, operating system, hardware and system load. Code that is executed in parallel in one run could be executed sequentially in another.&lt;/p&gt;
    &lt;p&gt;If parallelism is guaranteed, then it can be used to guarantee that an operation is non-blocking.&lt;/p&gt;
    &lt;p&gt;Concurrent and parallel are also often confused, but they’re different things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;code can be concurrent without being parallel, e.g. threads in pure Python will let you launch tasks, and only one of them will ever be executed at a time;&lt;/item&gt;
      &lt;item&gt;it’s a bit of a stretch, but code could be parallel without being concurrent, e.g. if the garbage-collector or some tasks runs in parallel but the developer does not have access to primitives to launch additional parallel tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;The case for (and against) threads&lt;/head&gt;
    &lt;p&gt;We live in the 21st century, so (on most platforms) we have access to threads. Threads are always a mean to achieve concurrency and, depending on resource constraints and your programming language, may be a mean to achieve parallelism.&lt;/p&gt;
    &lt;p&gt;Let’s try to use them.&lt;/p&gt;
    &lt;code&gt;import threading

def on_event(event):
    if isinstance(event, ComputeFibonacciEvent):
        def background():
            result = fibonacci(event.arg)
            print(f"fibonacci({event.arg})={result}")
        thread = threading.Thread(target=background)
        thread.start()
    else:
        ...&lt;/code&gt;
    &lt;p&gt;Based on a quick benchmark, creating and launching each thread in Python takes about 6µs on my machine, so we’re well within budget. Yes, running fibonacci in the background can still take an arbitrary amount of time, but that’s expected. So… mission accomplished?&lt;/p&gt;
    &lt;p&gt;Well… yes and no.&lt;/p&gt;
    &lt;p&gt;If you look at your favorite web backend or video game or desktop application, you’ll see that this is not the solution that the developers have picked.&lt;/p&gt;
    &lt;head rend="h2"&gt;Threads are tricky&lt;/head&gt;
    &lt;p&gt;Part of it is the difficulty. Programming with threads has long been considered too difficult for mere mortals, with the need to use (and understand!) thread-safety, mutexes, atomic operations and, sometimes, thread-local storage. While the specific case in the example is trivially thread-safe, it is really easy to write code that appears thread-safe but relies on some well-hidden global state (as is very common in Python libraries, for instance). I have also, quite a few times, seen code misusing mutexes (typically by protecting the wrong variable or protecting it at the wrong moment, or sometimes by blocking the main thread with a mutex, hence making the entire exercise pointless) or atomicity (by mis-understanding the memory model), so yes, being wary of threads makes all sorts of sense.&lt;/p&gt;
    &lt;p&gt;In fact, to this day, I am not aware of any multi-threading safe GUI toolkit, and even languages that make it simple to write multi-threaded code, such as Go, do not make it simple to write correct multi-threaded code 4. Even the Rust stdlib got one function wrong for a long time.&lt;/p&gt;
    &lt;p&gt;Or, in the words of David Baron:&lt;/p&gt;
    &lt;p&gt;By the way, I write that the snippet is trivially safe, but that’s actually not certain. What happens if &lt;code&gt;print&lt;/code&gt; is called by several threads at the same time? In such cases,
the original implementations of &lt;code&gt;printf&lt;/code&gt; in C would cause all sorts of memory breakages.
Nowadays, it’s probably safe… but how do you check that? Rust explicitly uses locks
around stdout and stderr, to avoid any threading problems, but most other languages and
frameworks don’t.&lt;/p&gt;
    &lt;head rend="h2"&gt;Threads cost resources&lt;/head&gt;
    &lt;p&gt;Another reason is resource limitations. Each process can launch a finite number of threads. Each user can launch a finite number of threads. Each kernel can launch a finite number of threads. And when you’re writing and deploying your application, you often don’t know that number, which means that you can’t rely upon it: for all you know, your application will run without thread support (I have noticed this once with a Docker deployment).&lt;/p&gt;
    &lt;p&gt;Oh, and your threads typically eat some memory (8kb physical and 8Mb virtual on Linux, last time I checked), which contributes (a bit) to making threads a complicated proposition: if you launch a thread to make an operation non-blocking, and if launching threads may fail (sometimes in hard-to-catch fashion), how should you handle the case in which your thread launch fail? Can you even do it?&lt;/p&gt;
    &lt;p&gt;These resource limitations are the reason for which web backends cannot just rely on threads. Because it’s seldom a good idea to let your users (who could be malicious, or using buggy clients, or stampeding after an article on Hacker News) control how many resources you use.&lt;/p&gt;
    &lt;p&gt;Now, these resource limitations have been known and worked around for decades, through the use of thread pools:&lt;/p&gt;
    &lt;code&gt;from concurrent.futures import ThreadPoolExecutor
thread_pool = ThreadPoolExecutor() # Place a limit on the number of threads used.

def on_event(event):
    if isinstance(event, ComputeFibonacciEvent):
        def background():
            result = fibonacci(event.arg)
            print(f"fibonacci({event.arg})={result}")
        thread_pool.submit(background)
    else:
        ...&lt;/code&gt;
    &lt;p&gt;And in many cases (again, assuming that your code is thread-safe), this works.&lt;/p&gt;
    &lt;p&gt;When doesn’t it work?&lt;/p&gt;
    &lt;p&gt;First, it generally doesn’t work if you’re writing a library. You don’t know the thread policy of your callers, so you could accidentally be introducing thread-unsafety in the caller’s code by using a thread pool, or multiplying the number of thread pools, hence breaking the constraint limits. Please don’t do that. If you wish to work with a thread pool, ask your client code to provide it.&lt;/p&gt;
    &lt;p&gt;The second problem appears if your threads are, for some reason, blocked. This can happen, for instance, if any of your threads needs to access a database, or a remote server, if you have any kind of call to &lt;code&gt;sleep&lt;/code&gt; or if, for some reason, your &lt;code&gt;print&lt;/code&gt; has been rerouted
to a file that is, for some reason slow. In such cases, can quickly saturate the thread
pool with threads doing nothing (well, waiting for completion of some activity that is not
controlled by your code). Any further task will just have to wait until one of the waiting
threads has finished its work.&lt;/p&gt;
    &lt;p&gt;In other words, you have completely lost throughput. If you’re writing a webserver, this suddenly means that you need more webservers to be able to serve all your users, which increases your cloud bills and the energy bill paid by the planet. If you’re writing a video game, sure, the framerate remains good, but the actions of the PCs and NPCs feel sluggish. If you’re writing a desktop app, sure, the UI remains responsive, but your users wait forever.&lt;/p&gt;
    &lt;head rend="h2"&gt;Threads may be GILed&lt;/head&gt;
    &lt;p&gt;Let’s get one nasty thing out of the way: if you’re writing Python or Ruby (or pretty old versions of OCaml), your threads are never, ever, going to run in parallel. That’s because these languages rely on a Global Interpreter Lock, designed specifically to make sure that only one thread is ever running at a given instant.&lt;/p&gt;
    &lt;p&gt;Why? Well, this makes the rest of the implementation of the language much, much simpler, in particular refcounting/garbage-collection. This also avoids lots of pitfalls that would make your life miserable. This also allows a number of optimizations (both in the VM/interpreter and by at user-level) that would become very much unsafe if the code truly ran in parallel.&lt;/p&gt;
    &lt;p&gt;Note that (at least in Python), native code (e.g. PyTorch, NumPy, any code written with PyO3, etc.) can release the GIL, which means that its content can run in background threads without blocking other threads. Most of the time, it’s a good thing, but if the developer of the native code doesn’t know what they’re doing, this can quickly lead to memory corruption.&lt;/p&gt;
    &lt;p&gt;What does this mean for performance? It means that if your code is not calling into code that releases the GIL, it’s always going to be quite slower in multi-threaded mode than in single-threaded mode. How do you know if code releases the GIL? Sadly, it’s pretty much never documented, so you have to experiment to find out.&lt;/p&gt;
    &lt;p&gt;Also, the case of OCaml demonstrates that you can bring an ecosystem from GIL-ed to fully multicore (OCaml ≥ 5), but suggests that it may take a pretty long time. Python seems to be slowly heading in this direction, but I don’t think we’ll see anything usable before 20305.&lt;/p&gt;
    &lt;head rend="h2"&gt;Threads are (kinda) slow&lt;/head&gt;
    &lt;p&gt;Operating System threads need to context-switch to provide preemptive multitasking, i.e. some thread runs on a CPU/core, then the thread is put on hold while some other code runs on that CPU/core. This is transparent to the user, but there is a cost.&lt;/p&gt;
    &lt;p&gt;Roughly speaking, to context-switch between two tasks, the OS scheduler will need to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;interrupt the user-level code on that CPU and place the CPU into kernel mode;&lt;/item&gt;
      &lt;item&gt;backup the registers, including the program counter;&lt;/item&gt;
      &lt;item&gt;backup the pointers to the thread stack, the thread-local storage, etc;&lt;/item&gt;
      &lt;item&gt;deactivate interrupt and signal-handling and backup their state;&lt;/item&gt;
      &lt;item&gt;replace registers, pointers, interrupt handlers, signal handlers with those of the new thread;&lt;/item&gt;
      &lt;item&gt;return to user land and resume execution of the code onto the CPU.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There is a cost to all this.&lt;/p&gt;
    &lt;p&gt;I’ve been too lazy to benchmark, but I’ve seen benchmarks on a machine more recent than mine that indicate a cost of 2-5µs during each context-switch. That’s time spent executing code that’s not serving your needs. If your code needs to context-switch 5000 times per core per second (that’s an arbitrary number), you have eaten 10-25ms of your budget just on context-switching, so that’s roughly equivalent to one frame per second which you may need to somehow compensate 6.&lt;/p&gt;
    &lt;p&gt;There are, of course, other costs to threads. Every lock you need to acquire/release has a raw synchronization cost, plus a contention cost. In particular, you very much want to avoid grabbing a lock on the main thread, as this makes thread synchronization a blocking operation. Even atomic operation you need to perform may have a performance cost, in particular on your cache. Etc.&lt;/p&gt;
    &lt;p&gt;In most cases, you probably don’t care. In particular, if you’re writing Python, you have bigger performance issues. However, if you’re writing performance-sensitive code (e.g. a video game, a video player, a browser), threads are not just part of the solution but also sometimes part of the performance problems you need to deal with.&lt;/p&gt;
    &lt;head rend="h2"&gt;What about green threads?&lt;/head&gt;
    &lt;p&gt;Green threads are threads implemented purely in user-land, i.e. they behave as threads but they don’t go through OS-level scheduling.&lt;/p&gt;
    &lt;p&gt;Scheduling between green threads is very similar to scheduling OS threads, but two things make it faster:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;scheduling doesn’t need to cross between user land and kernel;&lt;/item&gt;
      &lt;item&gt;there are fewer things to backup and restore (in particular, generally no interrupt and signal handlers).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Similarly, lock synchronization may be faster.&lt;/p&gt;
    &lt;p&gt;On the other hand, pure green threads do not benefit from multiple cores or CPUs, which strongly decreases the usefulness of these threads.&lt;/p&gt;
    &lt;p&gt;Using pure green threads is rather uncommon these days. However, a few languages have so-called M:N schedulers, which combine green threads and OS threads. We’ll speak more of this in the section dedicated to Go.&lt;/p&gt;
    &lt;head rend="h2"&gt;So, threads?&lt;/head&gt;
    &lt;p&gt;In other words, while threads are a necessary component of any solution, they are not the magic bullet that we can hope.&lt;/p&gt;
    &lt;head rend="h1"&gt;The case for (and against) processes&lt;/head&gt;
    &lt;p&gt;For a long time, Linux did not support threads. This has never prevented developers from writing concurrent/parallel code. One of the workarounds for this lack of threads was to use multiple processes. Similarly, the solution to running code across multiple CPUs or cores in GIL-based languages has traditionally been to use multiple processes. In the 90s, OCaml even had a dialect called JoCaml, which featured a rather excellent paradigm for parallelism and distribution.&lt;/p&gt;
    &lt;p&gt;With a high-level API, spawning execution on a process is fairly simple:&lt;/p&gt;
    &lt;code&gt;from concurrent.futures import ProcessPoolExecutor
process_pool = ProcessPoolExecutor()

def on_event(event):
    if isinstance(event, ComputeFibonacciEvent):
        future = process_pool.submit(fibonacci, event.arg)
        future.add_done_callback(lambda result: print(f"fibonacci({event.arg})={result}"))
    else:
        ...&lt;/code&gt;
    &lt;p&gt;This has immediate benefits: processes are not limited by a GIL, so code will typically run in parallel. Also, garbage-collectors are indendent across processes, so a slow garbage-collection on one process will generally not block another process.&lt;/p&gt;
    &lt;p&gt;There are a few drawbacks to the approach, though.&lt;/p&gt;
    &lt;head rend="h2"&gt;Processes are expensive&lt;/head&gt;
    &lt;p&gt;Each process runs its own copy of Python (or Ruby, or JavaScript, etc.), including an in-memory copy of not only the standard library, but every single dependency, a garbage-collector, etc. Also, if your language is JITed, each process runs its own JIT, which means its own copy of all the profiling data, and the optimized native code. The memory costs quickly add up.&lt;/p&gt;
    &lt;p&gt;In fact, one could argue that running multiple processes for a non-system language only makes sense if RAM is free and infinite.&lt;/p&gt;
    &lt;p&gt;Also, just as there are limits to the number of threads, there are limits to the number of processes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Communications are expensive&lt;/head&gt;
    &lt;p&gt;Communications between threads is simple: you just send the data by passing references.&lt;/p&gt;
    &lt;p&gt;Communication between processes (aka Inter Process Communication or IPC), though? That’s another story. There are a few ways to do things.&lt;/p&gt;
    &lt;p&gt;You can use shared memory:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;process A serializes all the data it wishes to send to some local buffer;&lt;/item&gt;
      &lt;item&gt;now that process A knows how much memory is needed to represent the data, it locks (internally) a segment of shared memory for this specific communication with process B;&lt;/item&gt;
      &lt;item&gt;process A copies all this data to the shared memory segment;&lt;/item&gt;
      &lt;item&gt;process A sends a signal to process B to inform process B that it should read the data (it’s a system call, so this needs to go through the kernel);&lt;/item&gt;
      &lt;item&gt;process B receives a signal from process A (it’s an interrupt, so this may happen during garbage-collection for instance, or during a file access, etc. so the following steps may need to be delayed until garbage-collection is complete);&lt;/item&gt;
      &lt;item&gt;process B finds the data in the shared memory and copies it to some local buffer;&lt;/item&gt;
      &lt;item&gt;process B sends a signal to process A to inform process A that the data can now be deallocated (again, system call);&lt;/item&gt;
      &lt;item&gt;process A receives the signal from process B;&lt;/item&gt;
      &lt;item&gt;process A releases (internally) the locked segment;&lt;/item&gt;
      &lt;item&gt;in parallel with 8-9, process B deserializes and checks for corruption the data receives from process A;&lt;/item&gt;
      &lt;item&gt;process B has finally received the message.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yeah, that’s not just complicated (that part is hidden from the user by a nice IPC library), it’s expensive.&lt;/p&gt;
    &lt;p&gt;You can simplify things quite a bit by going through sockets or pipes instead of shared memory, but at the expense of making more system calls, plus you’ll need to somehow make your pipe I/O non-blocking, which brings us back to our original problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Processes are (kinda) slow&lt;/head&gt;
    &lt;p&gt;Everything I wrote about threads being (kinda) slow? Well, all of this is true of processes, except that processes have way more data to save/restore in their Process Control Block: memory mappings, file descriptors, etc.&lt;/p&gt;
    &lt;p&gt;Also, locks between processes (which are fortunately needed much less often than locks within a process) typically go through the file system, so they’re a bit more expensive than locks between threads.&lt;/p&gt;
    &lt;head rend="h2"&gt;So, processes?&lt;/head&gt;
    &lt;p&gt;Processes are sometimes the right tool for the task, but their costs are steep, so you’ll need to be very careful about picking processes to solve your problem. Unless you only have access to processes, in which case… well, you don’t have a choice, do you?&lt;/p&gt;
    &lt;head rend="h1"&gt;Chunks&lt;/head&gt;
    &lt;p&gt;Threads and processes are a fairly high-level and expensive constructs. Perhaps we got off on the wrong foot, and the right way to solve our problem is to approach it from the other end. What if we rewrote our function &lt;code&gt;fibonacci&lt;/code&gt; to have it compute things concurrently, manually making sure that
we never block the event loop?&lt;/p&gt;
    &lt;p&gt;This would, of course, be easier if our implementation wasn’t (non-tail) recursive, but this can be done.&lt;/p&gt;
    &lt;code&gt;@dataclass
class ComputeFibonacciEvent(BaseEvent):
    """
    Event: We'd like to compute `fibonacci(arg)`.
    """

    arg: int
    """
    The value for which we wish to compute fibonacci.
    """

    id: UUID
    """
    A unique id for this event.
    """

    parent_id: UUID | None
    """
    If `None`, this is a toplevel request. Otherwise, the `id` of another
    `ComputeFibonacciEvent` on behalf of which we're performing this
    computation.
    """

@dataclass
class CompletedFibonacciEvent(BaseEvent):
    """
    Event: We have finished computing `fibonacci(arg)`.
    """

    arg: int
    """
    The value for which we requested to compute fibonacci.
    """

    parent_id: UUID | None
    """
    If `None`, this was a toplevel request. Otherwise, the `id` of another
    `ComputeFibonacciEvent` on behalf of which we're performing this
    computation.
    """

    result: int
    """
    The value of `fibonacci(arg)`.
    """

@dataclass
class PendingFibonacci:
    """
    Rendez-vous mechanism, holding the pending or partial state
    of computing `fibonacci(arg)`.
    """
    arg: int
    """
    The value for which we requested to compute fibonacci.
    """

    parent_id: UUID | None
    """
    If `None`, this was a toplevel request. Otherwise, the `id` of another
    `ComputeFibonacciEvent` on behalf of which we're performing this
    computation.
    """

    first: int | None = None
    """
    If `None`, we haven't computed `fibonacci(arg - 1)` yet. Otherwise,
    the value of `fibonacci(arg - 1)`.
    """

pending_fibonaccis: dict[UUID, PendingFibonacci] = dict()
"""
A mapping of event id =&amp;gt; PendingFibonacci.
"""

def handle_event(event: BaseEvent):
    if isinstance(event, ComputeFibonacciEvent):
        if event.arg &amp;lt;= 1:
            event_queue.put(CompletedFibonacciEvent(
                parent_id=event.parent_id,
                result=1,
                arg=event.arg,
            ))
        else:
            # Enqueue the left and right computations.
            event_queue.put(ComputeFibonacciEvent(
                id = uuid4(),
                parent_id=event.id,
                arg = event.arg - 1,
            ))
            event_queue.put(ComputeFibonacciEvent(
                id = uuid4(),
                parent_id=event.id,
                arg = event.arg - 2,
            ))
            # Store what we need to propagate the result.
            pending_fibonaccis[event.id] = PendingFibonacci(
                parent_id=event.parent_id,
                arg=event.arg,
            )
    elif isinstance(event, CompletedFibonacciEvent):
        pending = pending_fibonaccis[event.parent_id]
        if pending.first is None:
            pending.first = event.result
            # We still need to wait for the second computation.
        else:
            # We have obtained both computations.
            result = pending.first = event.result
            if pending.parent_id is None:
                #... and we're done!
                print(f"fibonacci({event.arg}) = {result}")
            else:
                #...continue popping!
                event_queue.put(CompletedFibonacciEvent(
                    parent_id=pending.parent,
                    result=result,
                    arg=event.arg
                ))&lt;/code&gt;
    &lt;p&gt;Ouch. That’s… quite a rewrite. We just turned a trivial four-lines function into a 180 lines, impossible-to-debug, monster.&lt;/p&gt;
    &lt;p&gt;But this, as a mechanism, works. At each step of the event loop, the computation is trivial and non-blocking. Alright, I’m cheating a bit: the call to &lt;code&gt;print&lt;/code&gt; remains blocking, but you could also rewrite it into a sequence
of non-blocking calls. In fact, if you look at Firefox or nginx, for
instance, you’ll find plenty of code written like this, usually
placing requests for long external operations (for instance, writing to
the database or to the network), then waking up once the request has
progressed to the next step, enqueuing the next step of the work as
a new event, etc.&lt;/p&gt;
    &lt;p&gt;Of course, the code I’ve written above is not nearly thread-safe. It could be made thread-safe, and hopefully take advantage of parallelism, at some cost in terms of throughput.&lt;/p&gt;
    &lt;p&gt;But before we start thinking about parallelism, let’s see how we can improve that monster.&lt;/p&gt;
    &lt;head rend="h1"&gt;Continuation-passing style (CPS)&lt;/head&gt;
    &lt;p&gt;Continuation-passing style is a programming style in which functions never return a result. Instead, each function receives as (usually last) argument a closure (the “continuation”) with instructions on what to do with the result.&lt;/p&gt;
    &lt;p&gt;If you have ever programmed with old-style Node, that’s exactly how it used to work. If you have ever programmed with monads, this involves the same idea.&lt;/p&gt;
    &lt;p&gt;So, let’s rewrite our code to use CPS:&lt;/p&gt;
    &lt;code&gt;def fibonacci_cps(n: int, then: Callable[[int]]):
    """
    Compute `fibonacci(n)`, then call `then`.
    """
    if n &amp;lt;= 1:
        return then(1)
    # Once we have the result of `fibonacci(n - 1)`, compute
    # `fibonacci(n - 2)`, then sum both.
    def with_left(left: int):
        fibonacci_cps(
            n=n - 2,
            then=lambda right: then(left + right)
        )
    # Compute `fibonacci(n- 1)`.
    fibonacci_cps(
        n=n - 1,
        then=with_left)


def handle_event(event: BaseEvent):
    if isinstance(event, ComputeFibonacciEvent):
        fibonacci_cps(event.arg, lambda result: print(f"fibonacci({event.arg})={result}"))
    elif isinstance(event, SleepEvent):
        event.thunk()&lt;/code&gt;
    &lt;p&gt;That’s nicer. So far, it’s blocking, but it’s nicer.&lt;/p&gt;
    &lt;p&gt;Now, by moving to CPS, we have removed the need to return, which means that we can delay computation. For instance, without breaking the rest of the code, we can add &lt;code&gt;wait&lt;/code&gt; instructions in the middle of &lt;code&gt;fibonacci_cps&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;First, let’s add some general-purpose support code:&lt;/p&gt;
    &lt;code&gt;@dataclass
class SleepEvent(BaseEvent):
    """
    Wait until the next tick of the event loop before running `thunk`.
    """
    thunk: Callable[[]]

def wait[T](continuation: Callable[[T]]) -&amp;gt; Callable[[T]]:
    """
    Wait until the next tick of the event loop before running `continuation`.
    """
    def result(arg: T):
        def thunk() -&amp;gt; None:
            return continuation(arg)
        event_queue.put(SleepEvent(
            thunk=thunk
        ))
    return result


def handle_event(event: BaseEvent):
    if isinstance(event, SleepEvent):
        event.thunk()
    else
        # ... As previously&lt;/code&gt;
    &lt;p&gt;Once we have this, we may rewrite &lt;code&gt;fibonacci_cps&lt;/code&gt; as follows:&lt;/p&gt;
    &lt;code&gt;def fibonacci_cps(n: int, then: Callable[[int]]):
    """
    Compute `fibonacci(n)`, then call `then`.
    """
    if n &amp;lt;= 1:
        return wait(then)(1)
    # Once we have the result of `fibonacci(n - 1)`, compute
    # `fibonacci(n - 2)`, then sum both.
    def with_left(left: int):
        fibonacci_cps(
            n=n - 2,
            then=lambda right: wait(then)(left + right)
        )
    # Compute `fibonacci(n- 1)`.
    def compute_left():
        fibonacci_cps(
            n=n - 1,
            then=with_left)
    wait(compute_left)&lt;/code&gt;
    &lt;p&gt;…and with this, we have made our code non-blocking. And of course, &lt;code&gt;wait&lt;/code&gt; and &lt;code&gt;SleepEvent&lt;/code&gt;
can be reused for any other CPS function. We could go further and customize just how many
ticks we wait, or dispatch tasks to various CPUs if we wanted.&lt;/p&gt;
    &lt;p&gt;If you recall our earlier definitions, writing our code as CPS makes it asynchronous. And we just demonstrated how to refactor our asynchronous code to also be non-blocking.&lt;/p&gt;
    &lt;p&gt;CPS calls are the real reason for which Node.js was initially lauded as “fast”. It wasn’t about CPU speed, but about the fact that, thanks to CPS, you can run (literally) millions of concurrent tasks, especially tasks that spend most of their time waiting for network or database read/writes, without taxing the CPU too much.&lt;/p&gt;
    &lt;p&gt;So far, so good. Of course, we have still increased the code size for &lt;code&gt;fibonacci&lt;/code&gt; from 4
lines to 18 and made it harder to read. Also, if you’re interested in performance, we
have allocated a lot of closures, which we’re going to pay in garbage-collection time.&lt;/p&gt;
    &lt;p&gt;Can we do better?&lt;/p&gt;
    &lt;head rend="h1"&gt;Generators / iterators&lt;/head&gt;
    &lt;p&gt;Generators are function-like objects that can be called repeatedly to return a succession of values. From the point of view of languages that support CPS natively (more precisely, languages that support &lt;code&gt;call/cc&lt;/code&gt; or &lt;code&gt;delimcc&lt;/code&gt;), generators are simple abstractions upon
simple use cases for continuations.&lt;/p&gt;
    &lt;p&gt;As it turns out, generators have been used in several languages and frameworks to achieve concurrency.&lt;/p&gt;
    &lt;p&gt;Let’s start with some support code:&lt;/p&gt;
    &lt;code&gt;@dataclass
class ContinueEvent(BaseEvent):
    """
    Schedule an execution for the next tick of the event loop.
    """
    generator: Generator[None, None, None]


def handle_event(event: BaseEvent):
    if isinstance(event, ComputeFibonacciEvent):
        # Start computation.
        def generator() -&amp;gt; Generator[None, None, None]:
            fibo = fibonacci(event.n)
            try:
                while True:
                    next(fibo)
                    # Not ready yet.
                    yield None
            except StopIteration as e:
                result: int = e.value
                print(f"fibonacci{event.n}={result}")

        event_queue.put(ContinueEvent(generator()))
    elif isinstance(event, ContinueEvent):
        # Continue computation
        try:
            # Are we done yet?
            next(event.generator)

            # Continue next tick.
            event_queue.put(event)
        except StopIteration:
            # Done
            pass
    else:
        raise NotImplementedError&lt;/code&gt;
    &lt;p&gt;The general idea is that &lt;code&gt;handle_event&lt;/code&gt; receives instances of &lt;code&gt;ContinueEvent&lt;/code&gt; and keeps
calling &lt;code&gt;next(event.generator)&lt;/code&gt;. If &lt;code&gt;next(event.generator)&lt;/code&gt; returns (without raising), it
means that the code requested a pause. For our implementation of Fibonacci’s function,
this will happen because we decided to break the function call into several non-blocking
segments, but for anything involving, say, network calls, it will mean that the network
call isn’t finished yet.&lt;/p&gt;
    &lt;p&gt;In practice, here’s our new version of &lt;code&gt;fibonacci&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;def fibonacci(n: int) -&amp;gt; Generator[None, None, int]:
    """
    An implementation of fibonacci.

    Yields None to reschedule the computation to the next tick of the event loop.
    After that, returns `int` with the result of `fibonacci(n)`.
    """
    if n &amp;lt;= 1:
        return 1
    yield None # Take a break.

    waiting_left = fibonacci(n - 1)
    try:
        while True:
            next(waiting_left)
            # Not `StopIteration` raised, which means we need to take a break.
            yield None
    except StopIteration as e:
        left: int = e.value

    waiting_right = fibonacci(n - 2)
    try:
        while True:
            next(waiting_right)
            # Not `StopIteration` raised, which means we need to take a break.
            yield None
    except StopIteration as e:
        right:int = e.value

    return left + right&lt;/code&gt;
    &lt;p&gt;Alright, it is still pretty long, but it is quite readable, in a Go style of things, with lots of easy-to-skip copy &amp;amp; paste code. In fact, if we had some syntactic support, we could imagine rewriting this entire function as:&lt;/p&gt;
    &lt;code&gt;def fibonacci(n: int) -&amp;gt; Generator[None, None, int]:
    """
    An implementation of fibonacci.

    Yields None to reschedule the computation to the next tick of the event loop.
    After that, returns `int` with the result of `fibonacci(n)`.
    """
    if n &amp;lt;= 1:
        return 1
    yield None # Take a break.

    left  = await fibonacci(n - 1) # Pseudo-syntax.
    right = await fibonacci(n - 2) # Pseudo-syntax.

    return left + right&lt;/code&gt;
    &lt;p&gt;…but let’s not get too much ahead of ourselves.&lt;/p&gt;
    &lt;p&gt;What are the benefits?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The transformation from our original &lt;code&gt;fibonacci&lt;/code&gt;is trivial and, in fact, mostly automatizable.&lt;/item&gt;
      &lt;item&gt;In many cases, this transformation can be optimized by a compiler, to have small to no overhead, by opposition to CPS.&lt;/item&gt;
      &lt;item&gt;It is easy to add or remove &lt;code&gt;yield None&lt;/code&gt;, which in turn means that you can control context-switches, good both for performance and to avoid multi-threading pitfalls. Sadly, there is a non-trivial cognitive cost in most languages. From the languages I know, only Rust manages to offload (most of) the cognitive cost of paying attention to race conditions onto the compiler.&lt;/item&gt;
      &lt;item&gt;Writing an implementation of &lt;code&gt;ContinueEvent&lt;/code&gt;that dispatches tasks to multiple CPUs is quite simple.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What are the downsides?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We’re still allocating memory dynamically, which isn’t great wrt performance.&lt;/item&gt;
      &lt;item&gt;There are still no guarantees about being non-blocking. In particular, if we forget to call &lt;code&gt;yield None&lt;/code&gt;, our entire transformation will be pointless.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that this rewrite as generators is, once again, a concurrent rewrite, which makes no guarantee about non-blocking.&lt;/p&gt;
    &lt;p&gt;Can we do better?&lt;/p&gt;
    &lt;head rend="h1"&gt;A small JavaScript aside&lt;/head&gt;
    &lt;p&gt;In JavaScript/TypeScript, these days, instead of CPS or generators to achieve concurrency, developers tend to use &lt;code&gt;Promise&lt;/code&gt;. While JavaScript supports generators, the need to make
JavaScript code non-blocking (and in particular, historically, the JavaScript code that
powers the user interface of Firefox) predates the implementation of generators in the
language.&lt;/p&gt;
    &lt;p&gt;Without syntactic support, the implementation of Fibonacci would look more like&lt;/p&gt;
    &lt;code&gt;/**
 * Sleep a few milliseconds. Non-blocking.
 */
function sleep(ms: number): Promise&amp;lt;void&amp;gt; {
    return new Promise((then) =&amp;gt; setTimeout(then, ms));
}

function fibonacci(n: number): Promise&amp;lt;number&amp;gt; {
    if (n &amp;lt;= 1) {
        return Promise.resolve(1);
    }
    return sleep(0).then(() =&amp;gt; {
        return fibonacci(n - 1).then((left) =&amp;gt; {
            return fibonacci(n - 2).then((right) =&amp;gt; {
                return left + right
            })
        })
    });
}&lt;/code&gt;
    &lt;p&gt;which is essentially a higher-level API on top of CPS.&lt;/p&gt;
    &lt;p&gt;In practice, JavaScript has a double event loop, with &lt;code&gt;Promise.then()&lt;/code&gt; being resolved
in the inner event loop (micro-ticks) and events (including &lt;code&gt;setTimeout&lt;/code&gt;) being resolved
in the outer event loop (ticks). This simplifies considerably some user-facing APIs,
but we don’t need to enter the details here.&lt;/p&gt;
    &lt;p&gt;This formulation is a bit easier to read than CPS, plus provides a better path for error-handling (not displayed here), but still a bit hard on the eyes and also requires plenty of allocations. Also, in terms of performance, this would not be very friendly to multi-threading. Since JavaScript does not support what we usually call multi-threading 7, that’s ok for JavaScript, but explains why the same solution isn’t pushed forward in other languages.&lt;/p&gt;
    &lt;p&gt;So, the question remains: can we do better?&lt;/p&gt;
    &lt;head rend="h1"&gt;async/await&lt;/head&gt;
    &lt;p&gt;Well, yes, we can. In fact, in Python or Rust, I don’t think I’ve ever seen application code written by human beings in the above style (I have seen much in JavaScript applications and some as part of Python or Rust libraries and frameworks, though). What we do, instead, is introduce some syntactic sugar that lets us write&lt;/p&gt;
    &lt;code&gt;async def fibonacci(n: int) -&amp;gt; int:
    """
    An implementation of fibonacci.

    Yields back time to the scheduler at each non-trivial recursive call.
    """
    if n &amp;lt;= 1:
        return 1
    await asyncio.sleep(0) # Take a break.

    left  = await fibonacci(n - 1)
    right = await fibonacci(n - 2)
    return left + right&lt;/code&gt;
    &lt;p&gt;This, to a very close approximation, is syntactic sugar for the code we wrote above. You can run it with &lt;code&gt;asyncio.run&lt;/code&gt;, the de facto standard executor for &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;async/await in Rust&lt;/head&gt;
    &lt;p&gt;The code is quite similar in Rust, and will compile essentially to the same loop with &lt;code&gt;yield&lt;/code&gt; as in Python:&lt;/p&gt;
    &lt;code&gt;async fn fibonacci(n: u32) -&amp;gt; u64 {
    if n &amp;lt;= 1 {
        return 1
    }
    tokio::time::sleep(Duration::new(0, 0)).await; // Take a break.

    let left  = Box::pin(fibonacci(n - 1)).await;  // We can't store recursive calls on the fixed-size pseudo-stack, so we need to allocate memory.
    let right = Box::pin(fibonacci(n - 2)).await;  // Since we'll rewrite it, we also want it to remain in place (hence the `pin`).
    left + right
}&lt;/code&gt;
    &lt;p&gt;Interestingly, as of this writing, &lt;code&gt;yield&lt;/code&gt; is not part of Rust’s surface level language.
However, it is used internally for this purpose. The generator will, in turn, compile
to a much more complicated finite state machine which we won’t detail here (if you want to look at it, see this playground and click “MIR” instead of “Build”). This Rust code
is, of course, thread-safe, and will in fact be dispatched to available CPUs if you execute it with
tokio, the de facto standard executor for &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; on non-embedded platforms.&lt;/p&gt;
    &lt;p&gt;Note that while both Rust and Python compile to similar-looking generators, there are quite a few differences in the underlying machinery, besides threads and in-memory representation. In particular, where the Python executor needs to poll by calling the generator (&lt;code&gt;Awaitable&lt;/code&gt;) repeatedly until it eventually advances, the Rust executor
expects the be informed by the generator (&lt;code&gt;Future&lt;/code&gt;) once it is ready to be polled once again.&lt;/p&gt;
    &lt;p&gt;Rust also supports canceling futures.&lt;/p&gt;
    &lt;head rend="h2"&gt;async/await in JavaScript&lt;/head&gt;
    &lt;p&gt;The surface-level JavaScript/TypeScript code is, again, quite similar, and will compile to the &lt;code&gt;Promise&lt;/code&gt;-based code above:&lt;/p&gt;
    &lt;code&gt;/**
 * Sleep a few milliseconds. Non-blocking.
 */
function sleep(ms: number): Promise&amp;lt;void&amp;gt; {
    return new Promise((then) =&amp;gt; setTimeout(then, ms));
}

async function fibonacci(n: number): Promise&amp;lt;number&amp;gt; {
    if (n &amp;lt;= 1) {
        return 1;
    }
    await sleep(0);
    let left  = await fibonacci(n - 1);
    let right = await fibonacci(n - 2);
    return left + right;
}&lt;/code&gt;
    &lt;p&gt;This will execute with your browser or Node’s built-in executor.&lt;/p&gt;
    &lt;p&gt;Note that, despite similar surface-level syntax, the underlying behavior is quite different from either Rust or Python: Promises do not rely on the executor to be polled, put to sleep or awakened. Rather, Promises essentially schedule themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;Executors?&lt;/head&gt;
    &lt;p&gt;If you wonder what an executor is, well, it’s an event loop we have been discussing throughout this piece, along with the basic events required to handle &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; and generally whatever you need to
build your own event loop on top of it if you need one.&lt;/p&gt;
    &lt;head rend="h2"&gt;The case for and against&lt;/head&gt;
    &lt;p&gt;Now, is there any drawback to &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;? Yes, there are a few, and they
differ across implementations.&lt;/p&gt;
    &lt;p&gt;The first drawback, in some languages, is stacks. If you’ve ever attempted to debug async code in Python or read through stack traces, you may have suffered. This used to be the case in JavaScript, too, although there have been improvements. This problem is not universal, as Rust, C# or F#, for instance, have proper async backtraces.&lt;/p&gt;
    &lt;p&gt;A second drawback is that &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; has a performance cost. CPU-bound code
written with &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; will simply never be as fast or as memory-efficient
as the equivalent synchronous code. The cost is higher in Python (which requires
polling and numerous allocations) or JavaScript (which supports wakeups but
requires even more allocations) than in Rust (which supports wakeups and can
often avoid allocations), but it exists regardless. Of course, if you’re writing
I/O-bound code, the cost of I/O is typically several orders of magnitude larger
than any &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; overhead, so you will probably not observe any difference.&lt;/p&gt;
    &lt;p&gt;But the biggest problem I’ve seen, by far, is that most developers don’t seem to understand &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So let me insist: &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; is a mechanism to easily make your code
asynchronous. It will make your code concurrent. It’s also a tool that
you can use to make your code non-blocking, but by itself,
it doesn’t make your code magically non-blocking. In particular, if you
call blocking code from async code, you will block.&lt;/p&gt;
    &lt;p&gt;Also, since every call to &lt;code&gt;await&lt;/code&gt; might hide a context-switch to another
scheduled concurrent task, you will encounter most of the same problems as
with multi-threaded code: you will encounter data races, you will need locks
and possibly task-local storage and, to make things worse, your usual locks
won’t work – for instance, Rust’s tokio offers its own async implementation
of &lt;code&gt;Mutex&lt;/code&gt;, &lt;code&gt;RwLock&lt;/code&gt;, etc. In fact, these implementations are typically
slower than their thread counterparts.&lt;/p&gt;
    &lt;p&gt;And finally, &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; is a form of function coloring. You can’t wait
for an &lt;code&gt;async&lt;/code&gt; function to wait without &lt;code&gt;await&lt;/code&gt; and you can’t use &lt;code&gt;await&lt;/code&gt; in
anything other than an &lt;code&gt;async&lt;/code&gt; function. This means that you can’t pass an
&lt;code&gt;async&lt;/code&gt; function as callback to a function that expects a sync function,
including your &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;, &lt;code&gt;fold&lt;/code&gt; or list comprehensions. This limits
code reuse and means that, once in a while, it will entirely prevent you
from using an API – I recently encountered the problem in Python, with a
scikit optimization function that required a sync callback, but the function
could only implemented as &lt;code&gt;async&lt;/code&gt;, since it relied on further &lt;code&gt;async&lt;/code&gt; functions.
It’s seldom a problem, but when it is, you are without a solution.&lt;/p&gt;
    &lt;p&gt;As I’ve briefly alluded to, &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; is also available in a bunch of other languages, including
F#, C#8, Haskell, Swift, Kotlin and (to some extent) C++. All languages that I’ve checked out, with the exception of JavaScript, compile &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; in the same manner as Python or Rust.&lt;/p&gt;
    &lt;head rend="h2"&gt;Async/await and non-blocking&lt;/head&gt;
    &lt;p&gt;Are &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; sufficient to make the code non-blocking? Well, I’ve already answered
that question above, but let me illustrate and explain.&lt;/p&gt;
    &lt;p&gt;If you look again at the desugared Python code, full of &lt;code&gt;while True&lt;/code&gt; and &lt;code&gt;yield None&lt;/code&gt;, you
will notice that &lt;code&gt;await&lt;/code&gt; doesn’t yield control to the executor. In fact, you can run
code full of &lt;code&gt;await&lt;/code&gt; and never context-switch to another task:&lt;/p&gt;
    &lt;code&gt;import asyncio

async def noop():
    # Do nothing.
    return

async def foo():
    for i in range(100):
        await noop() # This await will not make your foo() and bar() interleave.
        print("foo")

async def bar():
    for i in range(100):
        await noop() # This await will not make your foo() and bar() interleave.
        print("bar")

class Main:
    def __init__(self):
        self.task_1 = None
        self.task_2 = None

    async def run(self):
        # Note: We need to prevent tasks from being garbage-collected.
        self.task_1 = asyncio.create_task(foo())
        self.task_2 = asyncio.create_task(bar())

main = Main()

asyncio.run(main.run())
# Prints 100x foo() then 100x bar()&lt;/code&gt;
    &lt;p&gt;In other words, &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;, by themselves, will not make your execution
non-blocking.&lt;/p&gt;
    &lt;p&gt;Does Rust &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; make code magically non-blocking?&lt;/p&gt;
    &lt;code&gt;async fn noop() {
    // Do nothing
}


async fn foo() {
    for _ in 0..100 {
        noop().await;
        println!("foo")
    }
}

async fn bar() {
    for _ in 0..100 {
        noop().await;
        println!("bar")
    }
}

#[tokio::main]
async fn main() {
    let foo = tokio::task::spawn(foo());
    let bar = tokio::task::spawn(bar());
    let _ = foo.await;
    let _ = bar.await;
}
// Prints `foo` and `bar` randomly interleaved.
&lt;/code&gt;
    &lt;p&gt;Victory? Well, not quite. Tokio has detected that the computer has several cores and uses multi-threading. But what happens if we remove support for multi-threading? Let’s rewrite &lt;code&gt;main&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;fn main() {
    // Force the configuration to use a single thread.
    let runtime = tokio::runtime::Builder::new_current_thread()
        .build()
        .unwrap();
    runtime.block_on(async {
        let foo = tokio::task::spawn(foo());
        let bar = tokio::task::spawn(bar());
        let _ = foo.await;
        let _ = bar.await;
    })
}
// Prints 100x `foo` then 100x `bar`.
&lt;/code&gt;
    &lt;p&gt;Yup, if we remove support for multi-threading, execution is sequential once again. So no, in Rust either, &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; won’t make your code magically
non-blocking, although &lt;code&gt;tokio::task::spawn&lt;/code&gt; might.&lt;/p&gt;
    &lt;p&gt;What about JavaScript?&lt;/p&gt;
    &lt;code&gt;async function noop() {
    // Do nothing
}


async function foo() {
    for(let i = 0; i &amp;lt; 100; ++i) {
        await noop();
        console.debug("foo");
    }
}

async function bar() {
    for(let i = 0; i &amp;lt; 100; ++i) {
        await noop();
        console.debug("bar");
    }
}

function main() {
    Promise.race([foo(), bar()])
}
// Prints `foo` `bar` `foo` `bar` `foo` `bar` ... 100x each.
&lt;/code&gt;
    &lt;p&gt;Wait, what? In JavaScript, &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; makes code execute as concurrently
as one can hope?&lt;/p&gt;
    &lt;p&gt;Unfortunately, no.&lt;/p&gt;
    &lt;p&gt;If you recall, I mentioned that JavaScript has a double event loop, with micro-ticks and ticks. &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt; return instances of &lt;code&gt;Promise&lt;/code&gt; and each call to &lt;code&gt;await&lt;/code&gt; is
desugared to a &lt;code&gt;Promise.then()&lt;/code&gt;. Some of the early prototypes of &lt;code&gt;Promise&lt;/code&gt; had &lt;code&gt;Promise.then&lt;/code&gt;
execute the code immediately, but developers were surprised, because they expected
&lt;code&gt;await&lt;/code&gt; to, well, sleep. Other of the early prototypes called &lt;code&gt;setTimeout&lt;/code&gt;, but this
meant that &lt;code&gt;Promise&lt;/code&gt; and &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; could not be used naturally alongside some APIs such
as IndexedDB or Fetch, which committed their operations at the end of the current event, and
this proved also quite surprising for developers. So in the end, the standardized version of
&lt;code&gt;Promise&lt;/code&gt; introduce the micro-ticks and &lt;code&gt;Promise.then()&lt;/code&gt; automatically enqueues the closure
to be executed at the next micro-tick, but still in the same event. This meant that (unless
developers call &lt;code&gt;setTimeout&lt;/code&gt; or wait for I/O), Promises cannot be used to automatically
chunkify work, but also made &lt;code&gt;Promise.then()&lt;/code&gt; much faster to execute (and presumably
easier on the garbage-collector, I didn’t benchmark that).&lt;/p&gt;
    &lt;p&gt;So, in JavaScript &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; will encourage you to think about code as if it
were non-blocking, but it’s also not sufficient to make your code non-blocking.&lt;/p&gt;
    &lt;head rend="h2"&gt;Async and non-blocking I/O&lt;/head&gt;
    &lt;p&gt;As mentioned earlier, I/O can be very slow. HTTP calls or database calls can take unbounded amounts of time, and even disk I/O can slow things down considerably, especially on network shares.&lt;/p&gt;
    &lt;p&gt;So far, we have cheated by focusing on a purely mathematical function. But in real applications, you will need to deal with I/O and other blocking calls. After all, as mentioned previously, if you call blocking code from asynchronous code, well, you’re still blocking.&lt;/p&gt;
    &lt;p&gt;Luckily for you, your favorite async framework will usually provide non-blocking and ready-to-use async operations for these tasks. Let’s take a look at how these operations are made non-blocking.&lt;/p&gt;
    &lt;p&gt;There are typically two cases. In the first case, the operating system or lower-layer libraries may already provide non-blocking calls for such operations, e.g. &lt;code&gt;epoll&lt;/code&gt;, &lt;code&gt;io_uring&lt;/code&gt;, &lt;code&gt;kqueue&lt;/code&gt; or I/O Completion Ports. Generally speaking,
these primitives will let applications or libraries:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;(un)register to be informed when an operation is possible;&lt;/item&gt;
      &lt;item&gt;(un)register to be informed when an operation is complete;&lt;/item&gt;
      &lt;item&gt;schedule an operation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While the specifics are different across these primitives, the general idea is not dissimilar to what we have shown, for instance, earlier, when dividing fibonacci in chunks. In fact, the implementation of &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; in Rust is optimized for such a
wakeup mechanism.&lt;/p&gt;
    &lt;p&gt;In practice, things are a bit more complicated. In fact, I don’t know of any &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; embedding
on top of &lt;code&gt;io_uring&lt;/code&gt; in any language yet, because it doesn’t quite match this model. But
generally, that’s the idea.&lt;/p&gt;
    &lt;p&gt;In the second case, there is no non-blocking call for such operations. So we have to resort to threads. In such cases, the framework will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;request a thread from a thread pool;&lt;/item&gt;
      &lt;item&gt;dispatch the blocking call to the thread;&lt;/item&gt;
      &lt;item&gt;once the blocking call is complete, wake the executor with the result.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In Python, this may look like:&lt;/p&gt;
    &lt;code&gt;class NonBlockingIOManager:
    def __init__(self):
        self.pool = ThreadPoolExecutor()

    async def file_read(self, path: str) -&amp;gt; str:
        """
        Non-blocking file read.
        """
        def task():
            with open(path) as file:
                return file.read()
        return await asyncio.get_event_loop().run_in_executor(
            self.pool,
            task)&lt;/code&gt;
    &lt;p&gt;This is not ideal, but in the absence of a better solution, it works.&lt;/p&gt;
    &lt;p&gt;And in fact, in JavaScript, &lt;code&gt;Promise&lt;/code&gt; was designed to help integrate results
coming from multiple threads/processes in such a manner (but without dealing
with threads itself).&lt;/p&gt;
    &lt;head rend="h1"&gt;What about Go?&lt;/head&gt;
    &lt;p&gt;Go is a bit of an outlier, and one of the few programming languages that do not provide any kind of support for &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;, or any of the methods explored
so far. Nevertheless, this language is considered top-of-the-class (by some criteria)
for concurrent programming and web servers.&lt;/p&gt;
    &lt;p&gt;Where Python, C#, Rust, JavaScript or (so far) Java have decided to make user-level concurrency explicit by relying on &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; or lower-level constructions,
Go’s designers have decided to make it implicit by provided a transparent M:N
scheduler.&lt;/p&gt;
    &lt;code&gt;func fibonacci(n uint32) uint64 {
    if n &amp;lt;= 1 {
        return 1
    }
    // (probably) no need to sleep manually
    return fibonacci(n-1) + fibonacci(n-2)
}

type FibonacciEvent struct {
    Arg uint32
}

func main() {
    for {
        e &amp;lt;- nextEvent;
        switch event := e.(type) {
            case FibonacciEvent:
                go fibonacci(event.Arg)
        }
    }
}&lt;/code&gt;
    &lt;p&gt;How does it work?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The Go environment contains a pool of threads and pre-emptive scheduler.&lt;/item&gt;
      &lt;item&gt;Launching a goroutine with &lt;code&gt;go&lt;/code&gt;allocates dynamically a new stack for this goroutine.&lt;/item&gt;
      &lt;item&gt;A goroutine runs on a thread from the thread pool.&lt;/item&gt;
      &lt;item&gt;The Go scheduler may stop a goroutine at any moment, saving its program counter, task-local storage, etc. and replacing them with those of another goroutine.&lt;/item&gt;
      &lt;item&gt;During compilation of C calls, the CGo compiler instruments the (non-blessed) C code with a little assembly that calls into the runtime environment.&lt;/item&gt;
      &lt;item&gt;At runtime, when calling a C function, Go’s executor expects that the call is blocking, monitors how long the thread is blocked by the C function, and past some delay, removes the thread from the thread pool and replaces it with a new thread. Once execution of the C function has completed, the thread is not returned to the thread pool.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The immediate benefit comes in terms of resource usage/cognitive load. You can launch as many tasks (“goroutines”) as you want, without having to deal with &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; or care about
memory usage, and these tasks will be executed concurrently, hopefully in parallel. Moreover,
Go functions are not colored by synchronicity. This means that interfaces or callbacks don’t
need to care whether they accept sync or async implementations, as there is no difference.&lt;/p&gt;
    &lt;p&gt;The immediate drawback is that, since concurrency is, to a large extent, implicit, it makes the code harder to reason about. I don’t think that this is a real problem, but I’ve seen it lead to developers writing concurrency-unsafe code and running it concurrently without paying attention. I think it’s more a problem of education than language.&lt;/p&gt;
    &lt;p&gt;One could argue that implicit concurrency makes it harder to optimize code, but since very few developers actually care about such level of code optimization, and since Go is one of the fastest languages around, I consider this a minor impediment. Of course, if you need more performance, use Rust.&lt;/p&gt;
    &lt;p&gt;There are real issues with goroutines, such as the implicit capture by reference or the ease of accidentally copying a lock instead of sharing it, but these are not due to the concurrency model or implementation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other languages with M:N scheduling&lt;/head&gt;
    &lt;p&gt;Now, I mention that Go is an outlier, but it’s by no mean the only language with M:N scheduling. Erlang has been the poster child for M:N scheduling since the 90s, Haskell has supported it since the early 2000s, Rust used to support it but removed the feature before 1.0, and Java is moving towards supporting it. I believe that OCaml supports it, too, but I’m still investigating that.&lt;/p&gt;
    &lt;p&gt;Rust removing the feature is an interesting case for why M:N scheduling is not always the solution.&lt;/p&gt;
    &lt;p&gt;This happened for a variety of reasons. It took some time for Rust to decide what kind of language it was. Initial versions of Rust offered garbage-collection, a M:N scheduler, built-in support for real-time programming. Each of these features was convenient, but made Rust harder to maintain or port to new architectures.&lt;/p&gt;
    &lt;p&gt;In particular, as it became clear that Rust was a really good language to write very low-level code, including firmware, memory allocators, embedded code and OS code, these features became blockers, as they relied on having an operating system and an allocator in the first place. To make Rust a true system language, these features had to leave (and be turned into libraries).&lt;/p&gt;
    &lt;p&gt;In addition, the Rust ethos prefers explicit costs to implicit ones. Many languages claim that (including Go and Python), but few languages actually follow up on this design principle (the ones I can think of are Rust, Zig, and of course C and C++). Having a M:N scheduler requires allocating and growing stacks implicitly, which goes against this ethos: allocating memory has a cost, imposes a choice of allocator on the user (which would make Rust much less usable for e.g. browser engines or game engines), and may fail.&lt;/p&gt;
    &lt;p&gt;Not only that, but Rust is designed to both call into C and be callable from C transparently and at zero implicit cost and zero implicit risk. However, to be able to call into C without blocking, you need a machinery comparable to what CGo provides. This machinery requires, once again, implicit allocations, hence hidden costs and hidden points of failure.&lt;/p&gt;
    &lt;p&gt;Also, M:N scheduling simply made no sense on some architectures. For instance, making M:N work doesn’t make sense on architectures that do not support dynamic memory allocation or (embedded) operating systems that do not support native threads.&lt;/p&gt;
    &lt;p&gt;And finally, M:N scheduling is complicated and was holding the language back. So the decision was made to move M:N scheduling into a crate called &lt;code&gt;libgreen&lt;/code&gt;. Then, as work on &lt;code&gt;Future&lt;/code&gt;, then &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;
advanced, the Rust community decided that &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; served much better the Rust ethos of explicit
costs than M:N scheduling, and decision was taken to drop the feature.&lt;/p&gt;
    &lt;p&gt;None of this means that M:N scheduling is bad, incidentally. But it does illustrate that it is not always a desired feature.&lt;/p&gt;
    &lt;head rend="h1"&gt;What about OCaml?&lt;/head&gt;
    &lt;p&gt;OCaml is a very interesting case. It has supported the equivalent of Go’s channels since the late 1990s and, with version 5.0, it finally gained support for multicore. Recent benchmarks suggest that it can be actually much faster than Go on some tasks (I didn’t take the time to check these benchmarks, so don’t trust me on this).&lt;/p&gt;
    &lt;p&gt;In OCaml, using the Eio executor, our Fibonacci would look like:&lt;/p&gt;
    &lt;code&gt;let rec fibonacci (n:int) =
  if n &amp;lt;= 1 then (
    1
  ) else (
    Fiber.yield(); (* take a break *)
    fibonacci (n - 1) + fibonacci (n - 2)
  )&lt;/code&gt;
    &lt;p&gt;this implementation sits somewhere Go’s and Python/Rust/JavaScript. As Go, it doesn’t need &lt;code&gt;async&lt;/code&gt;
or &lt;code&gt;await&lt;/code&gt;. As Python/Rust/JavaScript/C++, it expects an explicit operation (here &lt;code&gt;Fiber.yield()&lt;/code&gt;)
to allow cooperative context-switching.&lt;/p&gt;
    &lt;p&gt;But in fact, OCaml is its own beast. For one thing, as far as I understand, the OCaml compiler doesn’t go through any compilation step specific to multi-tasking, either to compile &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;
into &lt;code&gt;yield&lt;/code&gt; or &lt;code&gt;Promise&lt;/code&gt; or anything similar, or to introduce implicit cooperative context-switching.
In fact, the above code can run, without change or recompilation, either as a backgrounded, non-blocking
task, or as a blocking task, depending on the context.&lt;/p&gt;
    &lt;p&gt;How does this work?&lt;/p&gt;
    &lt;p&gt;Well, the implementation of &lt;code&gt;Fiber.yield()&lt;/code&gt; actually raises an effect. Effects are almost identical
to exceptions, with one key difference: where exception handlers can either handle the exception or
let it propagate, effect handlers can also decide to resume the work at the site the effect was
raised. In a way, effects are a generalization of both exceptions (they add the ability to resume)
and generators (they add the ability to cross an arbitrary stack depth).&lt;/p&gt;
    &lt;p&gt;This is an extremely powerful mechanism that can be used to provide context-customizable retries, logging, mockable I/O, etc. and of course concurrency. For instance, we could write:&lt;/p&gt;
    &lt;code&gt;try some_function() with
| effect Yield continuation -&amp;gt; (
    (* Proceed in the next tick *)
    enqueue_event (Continue continuation)
)&lt;/code&gt;
    &lt;p&gt;or, if we don’t want concurrency&lt;/p&gt;
    &lt;code&gt;try some_function() with
| effect Yield continuation -&amp;gt; (
    (* Proceed immediately *)
    continuation ()
)&lt;/code&gt;
    &lt;p&gt;… and we could even have both on the same stack to force a function to run without concurrency locally (e.g. for performance reasons) despite running in a concurrent or even parallel executor.&lt;/p&gt;
    &lt;p&gt;It’s a very interesting mechanism that I’m still test-driving. I’m not sure about the performance implications of the continuation. As far as I understand, context-switching between two fibers is (or can be) as fast as in Go, but stack management is more complicated, as the code executed in the &lt;code&gt;try&lt;/code&gt; needs its stack, the code executed in the &lt;code&gt;effect&lt;/code&gt; expression needs its stack, both
of these stacks stack on top of the code that calls the entire &lt;code&gt;try ... with ...&lt;/code&gt; expression, and
&lt;code&gt;continuation()&lt;/code&gt; require its own stack.&lt;/p&gt;
    &lt;p&gt;There’s also the issue that, much like exceptions, effects don’t show up in the type of a function, which can lead to accidentally uncaught effects. Maybe an updated &lt;code&gt;ocamlexn&lt;/code&gt; could
solve that?&lt;/p&gt;
    &lt;p&gt;Note: As pointed out by /u/phischu on Reddit, there are of course several more experimental languages that expand this feature, including Lexa and Effekt.&lt;/p&gt;
    &lt;head rend="h1"&gt;Which tool should I use?&lt;/head&gt;
    &lt;head rend="h2"&gt;Threads&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Threads are good, but tricky to use. By all means, please learn how to use them correctly and safely.&lt;/item&gt;
      &lt;item&gt;On most platforms, threads are needed at some level to handle CPU-heavy tasks.&lt;/item&gt;
      &lt;item&gt;On most platforms, threads are needed at some level to make some I/O non-blocking.&lt;/item&gt;
      &lt;item&gt;If you are in a position where millisecond matters, you will need to profile your thread switches.&lt;/item&gt;
      &lt;item&gt;Don’t forget to check whether you are constrained by a GIL.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Processes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Processes are good, but heavy.&lt;/item&gt;
      &lt;item&gt;You may need to use them for security/sandboxing/crash isolation.&lt;/item&gt;
      &lt;item&gt;They can be useful for performance, typically in GIL-constrained languages, but the cost of communications can hurt a lot, so you’ll have to benchmark carefully.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Chunkification&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Chunkification makes your code really hard to read.&lt;/item&gt;
      &lt;item&gt;Chunkification always makes your code slower.&lt;/item&gt;
      &lt;item&gt;You can often avoid doing it manually, e.g. by using &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;6.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Async/await&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;is useless if you perform blocking calls, whether they’re blocking on CPU or on I/O.&lt;/item&gt;
      &lt;item&gt;if you own the CPU-bound blocking code, you can use &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;to chunkify it cleanly into non-blocking code.&lt;/item&gt;
      &lt;item&gt;otherwise, the only generic way to turn blocking calls into non-blocking calls is to use threads (or sometimes, processes).&lt;/item&gt;
      &lt;item&gt;once you have non-blocking I/O, &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;shines at using your CPU efficiently in tasks that are constrained by I/O.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;is function coloring, which may prevent you from using it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;M:N scheduler and variants&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I can’t think of any good reason to not take advantage of a M:N scheduler if it’s available.&lt;/item&gt;
      &lt;item&gt;The risks are almost identical to multi-threading, so please take time to learn how to write thread-safe code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;I hope that these few pages of code have helped clarify why &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; was designed, what
it is good for and what it won’t do for you, as well as a few alternatives.&lt;/p&gt;
    &lt;p&gt;If I find the opportunity, I’ll try and write a followup with benchmarks.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Other than Unix command-line tools. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sure, it will generally open in &amp;lt;16ms. But once in a while, it won’t, especially if it’s a network mount (it can take several seconds), or if your file system is already quite busy. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;As I write this, downloading the&lt;/p&gt;&lt;code&gt;index.html&lt;/code&gt;from my blog (whithout any other resource) takes about 30ms. That’s roughly the duration of two frames. ↩︎&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Among other things, Go stops being type-safe in presence of data race conditions. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;And just as I write that, I see the ChangeLog for Python 3.14, which suggests that free threading is moving on to Phase II. Who knows, maybe it will be faster? ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;/u/asb kindly pointed out on Reddit that there are efforts to decrease the cost of scheduling: https://lkml.org/lkml/2020/7/22/1202 . ↩︎ ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It supports a multi-process paradigm, which may be implemented with threads, but that’s a bit different. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In fact, F# was the first mainstream language to feature&lt;/p&gt;&lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt;, I believe, followed by C#. ↩︎&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Copyright: Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)&lt;/p&gt;
    &lt;p&gt;Author: David Teller&lt;/p&gt;
    &lt;p&gt;Posted on: July 8, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://yoric.github.io/post/quite-a-few-words-about-async/"/><published>2025-11-02T01:10:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45787571</id><title>Crossfire: High-performance lockless spsc/mpsc/mpmc channels for Rust</title><updated>2025-11-02T04:14:06.369503+00:00</updated><content>&lt;doc fingerprint="6e579a034ced13c0"&gt;
  &lt;main&gt;
    &lt;p&gt;High-performance lockless spsc/mpsc/mpmc channels.&lt;/p&gt;
    &lt;p&gt;It supports async contexts, and communication between async and blocking contexts.&lt;/p&gt;
    &lt;p&gt;The low level is based on crossbeam-queue.&lt;/p&gt;
    &lt;p&gt;For the concept, please refer to the wiki.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;V1.0: Released in 2022.12 and used in production.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;V2.0: Released in 2025.6. Refactored the codebase and API by removing generic types from the ChannelShared type, which made it easier to code with.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;v2.1: Released in 2025.9. Removed the dependency on crossbeam-channel and implemented with a modified version of crossbeam-queue, which brings performance improvements for both async and blocking contexts.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Being a lockless channel, crossfire outperforms other async-capable channels. And thanks to a lighter notification mechanism, in a blocking context, some cases are even better than the original crossbeam-channel,&lt;/p&gt;
    &lt;p&gt;More benchmark data is posted on wiki.&lt;/p&gt;
    &lt;p&gt;Also, being a lockless channel, the algorithm relies on spinning and yielding. Spinning is good on multi-core systems, but not friendly to single-core systems (like virtual machines). So we provide a function &lt;code&gt;detect_backoff_cfg()&lt;/code&gt; to detect the running platform.
Calling it within the initialization section of your code, will get a 2x performance boost on VPS.&lt;/p&gt;
    &lt;p&gt;The benchmark is written in the criterion framework. You can run the benchmark by:&lt;/p&gt;
    &lt;code&gt;cargo bench --bench crossfire
&lt;/code&gt;
    &lt;p&gt;NOTE: Because v2.1 has push the speed to a level no one has gone before, it can put a pure pressure to the async runtime. Some hidden bug (especially atomic ops on weaker ordering platform) might occur:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;arch&lt;/cell&gt;
        &lt;cell role="head"&gt;runtime&lt;/cell&gt;
        &lt;cell role="head"&gt;workflow&lt;/cell&gt;
        &lt;cell role="head"&gt;status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;x86_64&lt;/cell&gt;
        &lt;cell&gt;threaded&lt;/cell&gt;
        &lt;cell&gt;cron_master_threaded_x86&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tokio 1.47.1&lt;/cell&gt;
        &lt;cell&gt;cron_master_tokio_x86&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;async-std&lt;/cell&gt;
        &lt;cell&gt;cron_master_async_std_x86&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;smol&lt;/cell&gt;
        &lt;cell&gt;cron_master_smol-x86&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;arm&lt;/cell&gt;
        &lt;cell&gt;threaded&lt;/cell&gt;
        &lt;cell&gt; cron_master_threaded_arm&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tokio &amp;gt;= 1.48 (tokio PR #7622)&lt;/cell&gt;
        &lt;cell&gt; cron_master_tokio_arm&lt;/cell&gt;
        &lt;cell&gt; SHOULD UPGRADE tokio to 1.48&lt;p&gt;CURRENT-THREAD runtime still verifying&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;async-std&lt;/cell&gt;
        &lt;cell&gt;cron_master_async_std_arm&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;smol&lt;/cell&gt;
        &lt;cell&gt;cron_master_smol_arm&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;miri (emulation)&lt;/cell&gt;
        &lt;cell&gt;threaded&lt;/cell&gt;
        &lt;cell&gt;miri_tokio&lt;p&gt;miri_tokio_cur&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;STABLE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tokio&lt;/cell&gt;
        &lt;cell&gt;still verifying&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;async-std&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;(timerfd_create) not supported by miri&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;smol&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;(timerfd_create) not supported by miri&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;v2.0.26 (legacy):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;arch&lt;/cell&gt;
        &lt;cell role="head"&gt;runtime&lt;/cell&gt;
        &lt;cell role="head"&gt;workflow&lt;/cell&gt;
        &lt;cell role="head"&gt;status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;x86_64&lt;/cell&gt;
        &lt;cell&gt;threaded&lt;/cell&gt;
        &lt;cell&gt;cron_2.0_x86&lt;/cell&gt;
        &lt;cell&gt;PASSED&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tokio 1.47.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;async-std&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;arm&lt;/cell&gt;
        &lt;cell&gt;threaded&lt;/cell&gt;
        &lt;cell&gt;cron_2.0_arm&lt;/cell&gt;
        &lt;cell&gt;PASSED&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;tokio-1.47.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;async-std&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Debug locally:&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;--features trace_log&lt;/code&gt; to run the bench or test until it hangs, then press &lt;code&gt;ctrl+c&lt;/code&gt; or send &lt;code&gt;SIGINT&lt;/code&gt;,  there will be latest log dump to /tmp/crossfire_ring.log (refer to tests/common.rs &lt;code&gt;_setup_log()&lt;/code&gt;)&lt;/p&gt;
    &lt;p&gt;Debug with github workflow: #37&lt;/p&gt;
    &lt;p&gt;There are 3 modules: spsc, mpsc, mpmc, providing functions to allocate different types of channels.&lt;/p&gt;
    &lt;p&gt;The SP or SC interface is only for non-concurrent operation. It's more memory-efficient than MP or MC implementations, and sometimes slightly faster.&lt;/p&gt;
    &lt;p&gt;The return types in these 3 modules are different:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;mpmc::bounded_blocking() : (tx blocking, rx blocking)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;mpmc::bounded_async() : (tx async, rx async)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;mpmc::bounded_tx_async_rx_blocking() : (tx async, rx blocking)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;mpmc::bounded_tx_blocking_rx_async() : (tx blocking, rx async)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;mpmc::unbounded_blocking() : (tx non-blocking, rx blocking)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;mpmc::unbounded_async() : (tx non-blocking, rx async)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;NOTE : For a bounded channel, a 0 size case is not supported yet. (Temporary rewrite as 1 size).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Context&lt;/cell&gt;
        &lt;cell role="head"&gt;Sender (Producer)&lt;/cell&gt;
        &lt;cell role="head"&gt;Receiver (Consumer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Single&lt;/cell&gt;
        &lt;cell&gt;Multiple&lt;/cell&gt;
        &lt;cell&gt;Single&lt;/cell&gt;
        &lt;cell&gt;Multiple&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Blocking&lt;/cell&gt;
        &lt;cell&gt;BlockingTxTrait&lt;/cell&gt;
        &lt;cell&gt;BlockingRxTrait&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Tx&lt;/cell&gt;
        &lt;cell&gt;MTx&lt;/cell&gt;
        &lt;cell&gt;Rx&lt;/cell&gt;
        &lt;cell&gt;MRx&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Async&lt;/cell&gt;
        &lt;cell&gt;AsyncTxTrait&lt;/cell&gt;
        &lt;cell&gt;AsyncRxTrait&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;AsyncTx&lt;/cell&gt;
        &lt;cell&gt;MAsyncTx&lt;/cell&gt;
        &lt;cell&gt;AsyncRx&lt;/cell&gt;
        &lt;cell&gt;MAsyncRx&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For the SP / SC version, &lt;code&gt;AsyncTx&lt;/code&gt;, &lt;code&gt;AsyncRx&lt;/code&gt;, &lt;code&gt;Tx&lt;/code&gt;, and &lt;code&gt;Rx&lt;/code&gt; are not &lt;code&gt;Clone&lt;/code&gt; and without &lt;code&gt;Sync&lt;/code&gt;.
Although can be moved to other threads, but not allowed to use send/recv while in an Arc.
(Refer to the compile_fail examples in the type document).&lt;/p&gt;
    &lt;p&gt;The benefit of using the SP / SC API is completely lockless waker registration, in exchange for a performance boost.&lt;/p&gt;
    &lt;p&gt;The sender/receiver can use the &lt;code&gt;From&lt;/code&gt; trait to convert between blocking and async context
counterparts.&lt;/p&gt;
    &lt;p&gt;Error types are the same as crossbeam-channel: &lt;code&gt;TrySendError&lt;/code&gt;, &lt;code&gt;SendError&lt;/code&gt;, &lt;code&gt;TryRecvError&lt;/code&gt;, &lt;code&gt;RecvError&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;tokio&lt;/code&gt;: Enable send_timeout, recv_timeout API for async context, based on&lt;code&gt;tokio&lt;/code&gt;. And will detect the right backoff strategy for the type of runtime (multi-threaded / current-thread).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;async_std&lt;/code&gt;: Enable send_timeout, recv_timeout API for async context, based on&lt;code&gt;async-std&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tested on tokio-1.x and async-std-1.x, crossfire is runtime-agnostic.&lt;/p&gt;
    &lt;p&gt;The following scenarios are considered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;AsyncTx::send()&lt;/code&gt;and&lt;code&gt;AsyncRx:recv()&lt;/code&gt;operations are cancellation-safe in an async context. You can safely use the select! macro and timeout() function in tokio/futures in combination with recv(). On cancellation, [SendFuture] and [RecvFuture] will trigger drop(), which will clean up the state of the waker, making sure there is no mem-leak and deadlock. But you cannot know the true result from SendFuture, since it's dropped upon cancellation. Thus, we suggest using&lt;code&gt;AsyncTx::send_timeout()&lt;/code&gt;instead.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When the "tokio" or "async_std" feature is enabled, we also provide two additional functions:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;AsyncTx::send_timeout()&lt;/code&gt;, which will return the message that failed to be sent in [SendTimeoutError]. We guarantee the result is atomic. Alternatively, you can use&lt;code&gt;AsyncTx::send_with_timer()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AsyncRx::recv_timeout()&lt;/code&gt;, we guarantee the result is atomic. Alternatively, you can use&lt;code&gt;crate::AsyncRx::recv_with_timer()&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Between blocking context and async context, and between different async runtime instances.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The async waker footprint.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When using a multi-producer and multi-consumer scenario, there's a small memory overhead to pass along a &lt;code&gt;Weak&lt;/code&gt;
reference of wakers.
Because we aim to be lockless, when the sending/receiving futures are canceled (like tokio::time::timeout()),
it might trigger an immediate cleanup if the try-lock is successful, otherwise will rely on lazy cleanup.
(This won't be an issue because weak wakers will be consumed by actual message send and recv).
On an idle-select scenario, like a notification for close, the waker will be reused as much as possible
if poll() returns pending.&lt;/p&gt;
    &lt;p&gt;Cargo.toml:&lt;/p&gt;
    &lt;code&gt;[dependencies]
crossfire = "2.1"&lt;/code&gt;
    &lt;code&gt;extern crate crossfire;
use crossfire::*;
#[macro_use]
extern crate tokio;
use tokio::time::{sleep, interval, Duration};

#[tokio::main]
async fn main() {
    let (tx, rx) = mpsc::bounded_async::&amp;lt;i32&amp;gt;(100);
    for _ in 0..10 {
        let _tx = tx.clone();
        tokio::spawn(async move {
            for i in 0i32..10 {
                let _ = _tx.send(i).await;
                sleep(Duration::from_millis(100)).await;
                println!("sent {}", i);
            }
        });
    }
    drop(tx);
    let mut inv = tokio::time::interval(Duration::from_millis(500));
    loop {
        tokio::select! {
            _ = inv.tick() =&amp;gt;{
                println!("tick");
            }
            r = rx.recv() =&amp;gt; {
                if let Ok(_i) = r {
                    println!("recv {}", _i);
                } else {
                    println!("rx closed");
                    break;
                }
            }
        }
    }
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/frostyplanet/crossfire-rs"/><published>2025-11-02T03:07:15+00:00</published></entry></feed>