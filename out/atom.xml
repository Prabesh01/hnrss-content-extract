<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-27T16:12:02.945000+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46391514</id><title>Package managers keep using Git as a database, it never works out</title><updated>2025-12-27T16:14:25.615090+00:00</updated><content>&lt;doc fingerprint="b32422190df047d9"&gt;
  &lt;main&gt;
    &lt;p&gt;Using git as a database is a seductive idea. You get version history for free. Pull requests give you a review workflow. It’s distributed by design. GitHub will host it for free. Everyone already knows how to use it.&lt;/p&gt;
    &lt;p&gt;Package managers keep falling for this. And it keeps not working out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cargo&lt;/head&gt;
    &lt;p&gt;The crates.io index started as a git repository. Every Cargo client cloned it. This worked fine when the registry was small, but the index kept growing. Users would see progress bars like “Resolving deltas: 74.01%, (64415/95919)” hanging for ages, the visible symptom of Cargo’s libgit2 library grinding through delta resolution on a repository with thousands of historic commits.&lt;/p&gt;
    &lt;p&gt;The problem was worst in CI. Stateless environments would download the full index, use a tiny fraction of it, and throw it away. Every build, every time.&lt;/p&gt;
    &lt;p&gt;RFC 2789 introduced a sparse HTTP protocol. Instead of cloning the whole index, Cargo now fetches files directly over HTTPS, downloading only the metadata for dependencies your project actually uses. (This is the “full index replication vs on-demand queries” tradeoff in action.) By April 2025, 99% of crates.io requests came from Cargo versions where sparse is the default. The git index still exists, still growing by thousands of commits per day, but most users never touch it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Homebrew&lt;/head&gt;
    &lt;p&gt;GitHub explicitly asked Homebrew to stop using shallow clones. Updating them was “an extremely expensive operation” due to the tree layout and traffic of homebrew-core and homebrew-cask.&lt;/p&gt;
    &lt;p&gt;Users were downloading 331MB just to unshallow homebrew-core. The .git folder approached 1GB on some machines. Every &lt;code&gt;brew update&lt;/code&gt; meant waiting for git to grind through delta resolution.&lt;/p&gt;
    &lt;p&gt;Homebrew 4.0.0 in February 2023 switched to JSON downloads for tap updates. The reasoning was blunt: “they are expensive to git fetch and git clone and GitHub would rather we didn’t do that… they are slow to git fetch and git clone and this provides a bad experience to end users.”&lt;/p&gt;
    &lt;p&gt;Auto-updates now run every 24 hours instead of every 5 minutes, and they’re much faster because there’s no git fetch involved.&lt;/p&gt;
    &lt;head rend="h2"&gt;CocoaPods&lt;/head&gt;
    &lt;p&gt;CocoaPods is the package manager for iOS and macOS development. It hit the limits hard. The Specs repo grew to hundreds of thousands of podspecs across a deeply nested directory structure. Cloning took minutes. Updating took minutes. CI time vanished into git operations.&lt;/p&gt;
    &lt;p&gt;GitHub imposed CPU rate limits. The culprit was shallow clones, which force GitHub’s servers to compute which objects the client already has. The team tried various band-aids: stopping auto-fetch on &lt;code&gt;pod install&lt;/code&gt;, converting shallow clones to full clones, sharding the repository.&lt;/p&gt;
    &lt;p&gt;The CocoaPods blog captured it well: “Git was invented at a time when ‘slow network’ and ‘no backups’ were legitimate design concerns. Running endless builds as part of continuous integration wasn’t commonplace.”&lt;/p&gt;
    &lt;p&gt;CocoaPods 1.8 gave up on git entirely for most users. A CDN became the default, serving podspec files directly over HTTP. The migration saved users about a gigabyte of disk space and made &lt;code&gt;pod install&lt;/code&gt; nearly instant for new setups.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nixpkgs&lt;/head&gt;
    &lt;p&gt;Nix already solved the client-side problem. The package manager fetches expressions as tarballs via channels, served from S3 and CDN, not git clones. Binary caches serve built packages over HTTP. End users never touch the git repository.&lt;/p&gt;
    &lt;p&gt;But the repository itself is stress-testing GitHub’s infrastructure. In November 2025, GitHub contacted the NixOS team about periodic maintenance jobs failing and causing “issues achieving consensus between replicas.” If unresolved, the repository could have become read-only.&lt;/p&gt;
    &lt;p&gt;The repository totals 83GB with half a million tree objects and 20,000 forks. A local clone is only 2.5GB. The rest is GitHub’s fork network storing every pull request branch and merge commit. The CI queries mergeability daily, creating new merge commits each time.&lt;/p&gt;
    &lt;head rend="h2"&gt;vcpkg&lt;/head&gt;
    &lt;p&gt;vcpkg is Microsoft’s C++ package manager. It uses git tree hashes to version its ports, with the curated registry at github.com/Microsoft/vcpkg containing over 2,000 libraries.&lt;/p&gt;
    &lt;p&gt;The problem is that vcpkg needs to retrieve specific versions of ports by their git tree hash. When you specify a &lt;code&gt;builtin-baseline&lt;/code&gt; in your vcpkg.json (functioning like a lockfile for reproducible builds), vcpkg looks up historical commits to find the exact port versions you need. This only works if you have the full commit history.&lt;/p&gt;
    &lt;p&gt;Shallow clones break everything. GitHub Actions uses shallow clones by default. DevContainers shallow-clone vcpkg to save space. CI systems optimize for fast checkouts. All of these result in the same error: “vcpkg was cloned as a shallow repository… Try again with a full vcpkg clone.”&lt;/p&gt;
    &lt;p&gt;The workarounds are ugly. One proposed solution involves parsing vcpkg.json to extract the baseline hash, deriving the commit date, then fetching with &lt;code&gt;--shallow-since=&amp;lt;date&amp;gt;&lt;/code&gt;. Another suggests including twelve months of history, hoping projects upgrade before their baseline falls off the cliff. For GitHub Actions, you need &lt;code&gt;fetch-depth: 0&lt;/code&gt; in your checkout step, downloading the entire repository history just to resolve dependencies.&lt;/p&gt;
    &lt;p&gt;A vcpkg team member explained the fundamental constraint: “Port versions don’t use commit hashes, we use the git tree hash of the port directory. As far as I know, there is no way to deduce the commit that added a specific tree hash.” An in-product fix is infeasible. The architecture baked in git deeply enough that there’s no escape hatch.&lt;/p&gt;
    &lt;p&gt;Unlike Cargo, Homebrew, and CocoaPods, vcpkg hasn’t announced plans to move away from git registries. Custom registries must still be git repositories. The documentation describes filesystem registries as an alternative, but these require local or mounted paths rather than HTTP access. There’s no CDN, no sparse protocol, no HTTP-based solution on the horizon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Go modules&lt;/head&gt;
    &lt;p&gt;Grab’s engineering team went from 18 minutes for &lt;code&gt;go get&lt;/code&gt; to 12 seconds after deploying a module proxy. That’s not a typo. Eighteen minutes down to twelve seconds.&lt;/p&gt;
    &lt;p&gt;The problem was that &lt;code&gt;go get&lt;/code&gt; needed to fetch each dependency’s source code just to read its go.mod file and resolve transitive dependencies. Cloning entire repositories to get a single file.&lt;/p&gt;
    &lt;p&gt;Go had security concerns too. The original design wanted to remove version control tools entirely because “these fragment the ecosystem: packages developed using Bazaar or Fossil, for example, are effectively unavailable to users who cannot or choose not to install these tools.” Beyond fragmentation, the Go team worried about security bugs in version control systems becoming security bugs in &lt;code&gt;go get&lt;/code&gt;. You’re not just importing code; you’re importing the attack surface of every VCS tool on the developer’s machine.&lt;/p&gt;
    &lt;p&gt;GOPROXY became the default in Go 1.13. The proxy serves source archives and go.mod files independently over HTTP. Go also introduced a checksum database (sumdb) that records cryptographic hashes of module contents. This protects against force pushes silently changing tagged releases, and ensures modules remain available even if the original repository is deleted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond package managers&lt;/head&gt;
    &lt;p&gt;The same pattern shows up wherever developers try to use git as a database.&lt;/p&gt;
    &lt;p&gt;Git-based wikis like Gollum (used by GitHub and GitLab) become “somewhat too slow to be usable” at scale. Browsing directory structure takes seconds per click. Loading pages takes longer. GitLab plans to move away from Gollum entirely.&lt;/p&gt;
    &lt;p&gt;Git-based CMS platforms like Decap hit GitHub’s API rate limits. A Decap project on GitHub scales to about 10,000 entries if you have a lot of collection relations. A new user with an empty cache makes a request per entry to populate it, burning through the 5,000 request limit quickly. If your site has lots of content or updates frequently, use a database instead.&lt;/p&gt;
    &lt;p&gt;Even GitOps tools that embrace git as a source of truth have to work around its limitations. ArgoCD’s repo server can run out of disk space cloning repositories. A single commit invalidates the cache for all applications in that repo. Large monorepos need special scaling considerations.&lt;/p&gt;
    &lt;head rend="h2"&gt;The pattern&lt;/head&gt;
    &lt;p&gt;The hosting problems are symptoms. The underlying issue is that git inherits filesystem limitations, and filesystems make terrible databases.&lt;/p&gt;
    &lt;p&gt;Directory limits. Directories with too many files become slow. CocoaPods had 16,000 pod directories in a single Specs folder, requiring huge tree objects and expensive computation. Their fix was hash-based sharding: split directories by the first few characters of a hashed name, so no single directory has too many entries. Git itself does this internally with its objects folder, splitting into 256 subdirectories. You’re reinventing B-trees, badly.&lt;/p&gt;
    &lt;p&gt;Case sensitivity. Git is case-sensitive, but macOS and Windows filesystems typically aren’t. Check out a repo containing both &lt;code&gt;File.txt&lt;/code&gt; and &lt;code&gt;file.txt&lt;/code&gt; on Windows, and the second overwrites the first. Azure DevOps had to add server-side enforcement to block pushes with case-conflicting paths.&lt;/p&gt;
    &lt;p&gt;Path length limits. Windows restricts paths to 260 characters, a constraint dating back to DOS. Git supports longer paths, but Git for Windows inherits the OS limitation. This is painful with deeply nested node_modules directories, where &lt;code&gt;git status&lt;/code&gt; fails with “Filename too long” errors.&lt;/p&gt;
    &lt;p&gt;Missing database features. Databases have CHECK constraints and UNIQUE constraints; git has nothing, so every package manager builds its own validation layer. Databases have locking; git doesn’t. Databases have indexes for queries like “all packages depending on X”; with git you either traverse every file or build your own index. Databases have migrations for schema changes; git has “rewrite history and force everyone to re-clone.”&lt;/p&gt;
    &lt;p&gt;The progression is predictable. Start with a flat directory of files. Hit filesystem limits. Implement sharding. Hit cross-platform issues. Build server-side enforcement. Build custom indexes. Eventually give up and use HTTP or an actual database. You’ve built a worse version of what databases already provide, spread across git hooks, CI pipelines, and bespoke tooling.&lt;/p&gt;
    &lt;p&gt;None of this means git is bad. Git excels at what it was designed for: distributed collaboration on source code, with branching, merging, and offline work. The problem is using it for something else entirely. Package registries need fast point queries for metadata. Git gives you a full-document sync protocol when you need a key-value lookup.&lt;/p&gt;
    &lt;p&gt;If you’re building a package manager and git-as-index seems appealing, look at Cargo, Homebrew, CocoaPods, vcpkg, Go. They all had to build workarounds as they grew, causing pain for users and maintainers. The pull request workflow is nice. The version history is nice. You will hit the same walls they did.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nesbitt.io/2025/12/24/package-managers-keep-using-git-as-a-database.html"/><published>2025-12-26T12:46:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46392910</id><title>Show HN: Witr – Explain why a process is running on your Linux system</title><updated>2025-12-27T16:14:25.133412+00:00</updated><content>&lt;doc fingerprint="471a943f3dafc432"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1. Purpose&lt;/item&gt;
      &lt;item&gt;2. Goals&lt;/item&gt;
      &lt;item&gt;3. Core Concept&lt;/item&gt;
      &lt;item&gt;4. Supported Targets&lt;/item&gt;
      &lt;item&gt;5. Output Behavior&lt;/item&gt;
      &lt;item&gt;6. Flags &amp;amp; Options&lt;/item&gt;
      &lt;item&gt;7. Example Outputs&lt;/item&gt;
      &lt;item&gt;8. Installation&lt;/item&gt;
      &lt;item&gt;9. Platform Support&lt;/item&gt;
      &lt;item&gt;10. Success Criteria&lt;/item&gt;
      &lt;item&gt;11. AI Assistance Disclaimer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;witr exists to answer a single question:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Why is this running?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When something is running on a system—whether it is a process, a service, or something bound to a port—there is always a cause. That cause is often indirect, non-obvious, or spread across multiple layers such as supervisors, containers, services, or shells.&lt;/p&gt;
    &lt;p&gt;Existing tools (&lt;code&gt;ps&lt;/code&gt;, &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;lsof&lt;/code&gt;, &lt;code&gt;ss&lt;/code&gt;, &lt;code&gt;systemctl&lt;/code&gt;, &lt;code&gt;docker ps&lt;/code&gt;) expose state and metadata. They show what is running, but leave the user to infer why by manually correlating outputs across tools.&lt;/p&gt;
    &lt;p&gt;witr makes that causality explicit.&lt;/p&gt;
    &lt;p&gt;It explains where a running thing came from, how it was started, and what chain of systems is responsible for it existing right now, in a single, human-readable output.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Explain why a process exists, not just that it exists&lt;/item&gt;
      &lt;item&gt;Reduce time‑to‑understanding during debugging and outages&lt;/item&gt;
      &lt;item&gt;Work with zero configuration&lt;/item&gt;
      &lt;item&gt;Be safe, read‑only, and non‑destructive&lt;/item&gt;
      &lt;item&gt;Prefer clarity over completeness&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Not a monitoring tool&lt;/item&gt;
      &lt;item&gt;Not a performance profiler&lt;/item&gt;
      &lt;item&gt;Not a replacement for systemd/docker tooling&lt;/item&gt;
      &lt;item&gt;Not a remediation or auto‑fix tool&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;witr treats everything as a process question.&lt;/p&gt;
    &lt;p&gt;Ports, services, containers, and commands all eventually map to PIDs. Once a PID is identified, witr builds a causal chain explaining why that PID exists.&lt;/p&gt;
    &lt;p&gt;At its core, witr answers:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What is running?&lt;/item&gt;
      &lt;item&gt;How did it start?&lt;/item&gt;
      &lt;item&gt;What is keeping it running?&lt;/item&gt;
      &lt;item&gt;What context does it belong to?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;witr supports multiple entry points that converge to PID analysis.&lt;/p&gt;
    &lt;code&gt;witr node
witr nginx&lt;/code&gt;
    &lt;p&gt;A single positional argument (without flags) is treated as a process or service name. If multiple matches are found, witr will prompt for disambiguation by PID.&lt;/p&gt;
    &lt;code&gt;witr --pid 14233&lt;/code&gt;
    &lt;p&gt;Explains why a specific process exists.&lt;/p&gt;
    &lt;code&gt;witr --port 5000&lt;/code&gt;
    &lt;p&gt;Explains the process(es) listening on a port.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single screen by default (best effort)&lt;/item&gt;
      &lt;item&gt;Deterministic ordering&lt;/item&gt;
      &lt;item&gt;Narrative-style explanation&lt;/item&gt;
      &lt;item&gt;Best-effort detection with explicit uncertainty&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What the user asked about.&lt;/p&gt;
    &lt;p&gt;Executable, PID, user, command, start time and restart count.&lt;/p&gt;
    &lt;p&gt;A causal ancestry chain showing how the process came to exist. This is the core value of witr.&lt;/p&gt;
    &lt;p&gt;The primary system responsible for starting or supervising the process (best effort).&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;systemd unit&lt;/item&gt;
      &lt;item&gt;docker container&lt;/item&gt;
      &lt;item&gt;pm2&lt;/item&gt;
      &lt;item&gt;cron&lt;/item&gt;
      &lt;item&gt;interactive shell&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Only one primary source is selected.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Working directory&lt;/item&gt;
      &lt;item&gt;Git repository name and branch&lt;/item&gt;
      &lt;item&gt;Docker container name / image&lt;/item&gt;
      &lt;item&gt;Public vs private bind&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Non‑blocking observations such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Process is running as root&lt;/item&gt;
      &lt;item&gt;Process is listening on a public interface (0.0.0.0 / ::)&lt;/item&gt;
      &lt;item&gt;Restarted multiple times (warning only if above threshold)&lt;/item&gt;
      &lt;item&gt;Process is using high memory (&amp;gt;1GB RSS)&lt;/item&gt;
      &lt;item&gt;Process has been running for over 90 days&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;--pid &amp;lt;n&amp;gt;         Explain a specific PID
--port &amp;lt;n&amp;gt;        Explain port usage
--short           One-line summary
--tree            Show full process ancestry tree
--json            Output result as JSON
--warnings        Show only warnings
--no-color        Disable colorized output
--env             Show only environment variables for the process
--help            Show this help message
&lt;/code&gt;
    &lt;p&gt;A single positional argument (without flags) is treated as a process or service name.&lt;/p&gt;
    &lt;code&gt;witr node&lt;/code&gt;
    &lt;code&gt;Target      : node

Process     : node (pid 14233)
User        : pm2
Command     : node index.js
Started     : 2 days ago (Mon 2025-02-02 11:42:10 +05:30)
Restarts    : 1

Why It Exists :
  systemd (pid 1) → pm2 (pid 5034) → node (pid 14233)

Source      : pm2

Working Dir : /opt/apps/expense-manager
Git Repo    : expense-manager (main)
Listening   : 127.0.0.1:5001
&lt;/code&gt;
    &lt;code&gt;witr --port 5000 --short&lt;/code&gt;
    &lt;code&gt;systemd (pid 1) → PM2 v5.3.1: God (pid 1481580) → python (pid 1482060)
&lt;/code&gt;
    &lt;code&gt;witr --pid 1482060 --tree&lt;/code&gt;
    &lt;code&gt;systemd (pid 1)
  └─ PM2 v5.3.1: God (pid 1481580)
    └─ python (pid 1482060)
&lt;/code&gt;
    &lt;code&gt;witr node&lt;/code&gt;
    &lt;code&gt;Multiple matching processes found:

[1] PID 12091  node server.js  (docker)
[2] PID 14233  node index.js   (pm2)
[3] PID 18801  node worker.js  (manual)

Re-run with:
  witr --pid &amp;lt;pid&amp;gt;
&lt;/code&gt;
    &lt;code&gt;witr nginx&lt;/code&gt;
    &lt;code&gt;Ambiguous target: "nginx"

The name matches multiple entities:

[1] PID 2311   nginx: master process   (service)
[2] PID 24891  nginx: worker process   (manual)

witr cannot determine intent safely.
Please re-run with an explicit PID:
  witr --pid &amp;lt;pid&amp;gt;
&lt;/code&gt;
    &lt;p&gt;witr is distributed as a single static Linux binary.&lt;/p&gt;
    &lt;p&gt;The easiest way to install witr is via the install script.&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/pranshuparmar/witr/main/install.sh | bash&lt;/code&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/pranshuparmar/witr/main/install.sh -o install.sh
cat install.sh
chmod +x install.sh
./install.sh&lt;/code&gt;
    &lt;p&gt;The script will:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Detect your CPU architecture (&lt;code&gt;amd64&lt;/code&gt;or&lt;code&gt;arm64&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Download the latest released binary and man page&lt;/item&gt;
      &lt;item&gt;Install it to &lt;code&gt;/usr/local/bin/witr&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Install the man page to &lt;code&gt;/usr/local/share/man/man1/witr.1&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You may be prompted for your password to write to system directories.&lt;/p&gt;
    &lt;p&gt;If you prefer manual installation, follow these simple steps for your architecture:&lt;/p&gt;
    &lt;code&gt;# Download the binary
curl -fsSL https://github.com/pranshuparmar/witr/releases/latest/download/witr-linux-amd64 -o witr-linux-amd64

# Verify checksum (Optional, should print OK)
curl -fsSL https://github.com/pranshuparmar/witr/releases/latest/download/SHA256SUMS -o SHA256SUMS
grep witr-linux-amd64 SHA256SUMS | sha256sum -c -

# Rename and install
mv witr-linux-amd64 witr &amp;amp;&amp;amp; chmod +x witr
sudo mv witr /usr/local/bin/witr

# Install the man page (Optional)
sudo curl -fsSL https://github.com/pranshuparmar/witr/releases/latest/download/witr.1 -o /usr/local/share/man/man1/witr.1
sudo mandb &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 || true&lt;/code&gt;
    &lt;code&gt;# Download the binary
curl -fsSL https://github.com/pranshuparmar/witr/releases/latest/download/witr-linux-arm64 -o witr-linux-arm64

# Verify checksum (Optional, should print OK)
curl -fsSL https://github.com/pranshuparmar/witr/releases/latest/download/SHA256SUMS -o SHA256SUMS
grep witr-linux-arm64 SHA256SUMS | sha256sum -c -

# Rename and install
mv witr-linux-arm64 witr &amp;amp;&amp;amp; chmod +x witr
sudo mv witr /usr/local/bin/witr

# Install the man page (Optional)
sudo curl -fsSL https://github.com/pranshuparmar/witr/releases/latest/download/witr.1 -o /usr/local/share/man/man1/witr.1
sudo mandb &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 || true&lt;/code&gt;
    &lt;p&gt;Explanation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download only the binary for your architecture and the SHA256SUMS file.&lt;/item&gt;
      &lt;item&gt;Verify the checksum for your binary only (prints OK if valid).&lt;/item&gt;
      &lt;item&gt;Rename to witr, make it executable, and move to your PATH.&lt;/item&gt;
      &lt;item&gt;Install man page.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;witr --version
man witr&lt;/code&gt;
    &lt;p&gt;To completely remove witr:&lt;/p&gt;
    &lt;code&gt;sudo rm -f /usr/local/bin/witr
sudo rm -f /usr/local/share/man/man1/witr.1&lt;/code&gt;
    &lt;p&gt;If you use Nix, you can build witr from source and run without installation:&lt;/p&gt;
    &lt;code&gt;nix run github:pranshuparmar/witr -- --port 5000&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;witr inspects &lt;code&gt;/proc&lt;/code&gt; and may require elevated permissions to explain certain processes.&lt;/p&gt;
    &lt;p&gt;If you are not seeing the expected information (e.g., missing process ancestry, user, working directory or environment details), try running witr with sudo for elevated permissions:&lt;/p&gt;
    &lt;code&gt;sudo witr [your arguments]&lt;/code&gt;
    &lt;p&gt;witr is successful if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A user can answer "why is this running?" within seconds&lt;/item&gt;
      &lt;item&gt;It reduces reliance on multiple tools&lt;/item&gt;
      &lt;item&gt;Output is understandable under stress&lt;/item&gt;
      &lt;item&gt;Users trust it during incidents&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project was developed with assistance from AI/LLMs (including GitHub Copilot, ChatGPT, and related tools), supervised by a human who occasionally knew what he was doing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/pranshuparmar/witr"/><published>2025-12-26T15:20:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46393936</id><title>Experts explore new mushroom which causes fairytale-like hallucinations</title><updated>2025-12-27T16:12:10.144716+00:00</updated><content>&lt;doc fingerprint="a778f61f8d52f750"&gt;
  &lt;main&gt;
    &lt;p&gt;uv installs packages faster than pip by an order of magnitude. The usual explanation is “it’s written in Rust.” That’s true, but it doesn’t explain much. Plenty of tools are written in Rust without being notably fast. The interesting question is what design decisions made the difference.&lt;/p&gt;
    &lt;p&gt;Charlie Marsh’s Jane Street talk and a Xebia engineering deep-dive cover the technical details well. The interesting parts are the design decisions: standards that enable fast paths, things uv drops that pip supports, and optimizations that don’t require Rust at all.&lt;/p&gt;
    &lt;head rend="h2"&gt;The standards that made uv possible&lt;/head&gt;
    &lt;p&gt;pip’s slowness isn’t a failure of implementation. For years, Python packaging required executing code to find out what a package needed.&lt;/p&gt;
    &lt;p&gt;The problem was setup.py. You couldn’t know a package’s dependencies without running its setup script. But you couldn’t run its setup script without installing its build dependencies. PEP 518 in 2016 called this out explicitly: “You can’t execute a setup.py file without knowing its dependencies, but currently there is no standard way to know what those dependencies are in an automated fashion without executing the setup.py file.”&lt;/p&gt;
    &lt;p&gt;This chicken-and-egg problem forced pip to download packages, execute untrusted code, fail, install missing build tools, and try again. Every install was potentially a cascade of subprocess spawns and arbitrary code execution. Installing a source distribution was essentially &lt;code&gt;curl | bash&lt;/code&gt; with extra steps.&lt;/p&gt;
    &lt;p&gt;The fix came in stages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PEP 518 (2016) created pyproject.toml, giving packages a place to declare build dependencies without code execution. The TOML format was borrowed from Rust’s Cargo, which makes a Rust tool returning to fix Python packaging feel less like coincidence.&lt;/item&gt;
      &lt;item&gt;PEP 517 (2017) separated build frontends from backends, so pip didn’t need to understand setuptools internals.&lt;/item&gt;
      &lt;item&gt;PEP 621 (2020) standardized the &lt;code&gt;[project]&lt;/code&gt;table, so dependencies could be read by parsing TOML rather than running Python.&lt;/item&gt;
      &lt;item&gt;PEP 658 (2022) put package metadata directly in the Simple Repository API, so resolvers could fetch dependency information without downloading wheels at all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PEP 658 went live on PyPI in May 2023. uv launched in February 2024. uv could be fast because the ecosystem finally had the infrastructure to support it. A tool like uv couldn’t have shipped in 2020. The standards weren’t there yet.&lt;/p&gt;
    &lt;p&gt;Other ecosystems figured this out earlier. Cargo has had static metadata from the start. npm’s package.json is declarative. Python’s packaging standards finally bring it to parity.&lt;/p&gt;
    &lt;head rend="h2"&gt;What uv drops&lt;/head&gt;
    &lt;p&gt;Speed comes from elimination. Every code path you don’t have is a code path you don’t wait for.&lt;/p&gt;
    &lt;p&gt;uv’s compatibility documentation is a list of things it doesn’t do:&lt;/p&gt;
    &lt;p&gt;No .egg support. Eggs were the pre-wheel binary format. pip still handles them; uv doesn’t even try. The format has been obsolete for over a decade.&lt;/p&gt;
    &lt;p&gt;No pip.conf. uv ignores pip’s configuration files entirely. No parsing, no environment variable lookups, no inheritance from system-wide and per-user locations.&lt;/p&gt;
    &lt;p&gt;No bytecode compilation by default. pip compiles .py files to .pyc during installation. uv skips this step, shaving time off every install. You can opt in if you want it.&lt;/p&gt;
    &lt;p&gt;Virtual environments required. pip lets you install into system Python by default. uv inverts this, refusing to touch system Python without explicit flags. This removes a whole category of permission checks and safety code.&lt;/p&gt;
    &lt;p&gt;Stricter spec enforcement. pip accepts malformed packages that technically violate packaging specs. uv rejects them. Less tolerance means less fallback logic.&lt;/p&gt;
    &lt;p&gt;Ignoring requires-python upper bounds. When a package says it requires &lt;code&gt;python&amp;lt;4.0&lt;/code&gt;, uv ignores the upper bound and only checks the lower. This reduces resolver backtracking dramatically since upper bounds are almost always wrong. Packages declare &lt;code&gt;python&amp;lt;4.0&lt;/code&gt; because they haven’t tested on Python 4, not because they’ll actually break. The constraint is defensive, not predictive.&lt;/p&gt;
    &lt;p&gt;First-index wins by default. When multiple package indexes are configured, pip checks all of them. uv picks from the first index that has the package, stopping there. This prevents dependency confusion attacks and avoids extra network requests.&lt;/p&gt;
    &lt;p&gt;Each of these is a code path pip has to execute and uv doesn’t.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimizations that don’t need Rust&lt;/head&gt;
    &lt;p&gt;Some of uv’s speed comes from Rust. But not as much as you’d think. Several key optimizations could be implemented in pip today:&lt;/p&gt;
    &lt;p&gt;HTTP range requests for metadata. Wheel files are zip archives, and zip archives put their file listing at the end. uv tries PEP 658 metadata first, falls back to HTTP range requests for the zip central directory, then full wheel download, then building from source. Each step is slower and riskier. The design makes the fast path cover 99% of cases. None of this requires Rust.&lt;/p&gt;
    &lt;p&gt;Parallel downloads. pip downloads packages one at a time. uv downloads many at once. Any language can do this.&lt;/p&gt;
    &lt;p&gt;Global cache with hardlinks. pip copies packages into each virtual environment. uv keeps one copy globally and uses hardlinks (or copy-on-write on filesystems that support it). Installing the same package into ten venvs takes the same disk space as one. Any language with filesystem access can do this.&lt;/p&gt;
    &lt;p&gt;Python-free resolution. pip needs Python running to do anything, and invokes build backends as subprocesses to get metadata from legacy packages. uv parses TOML and wheel metadata natively, only spawning Python when it hits a setup.py-only package that has no other option.&lt;/p&gt;
    &lt;p&gt;PubGrub resolver. uv uses the PubGrub algorithm, originally from Dart’s pub package manager. Both pip and PubGrub use backtracking, but PubGrub applies conflict-driven clause learning from SAT solvers: when it hits a dead end, it analyzes why and skips similar dead ends later. This makes it faster on complex dependency graphs and better at explaining failures. pip could adopt PubGrub without rewriting in Rust.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Rust actually matters&lt;/head&gt;
    &lt;p&gt;Some optimizations do require Rust:&lt;/p&gt;
    &lt;p&gt;Zero-copy deserialization. uv uses rkyv to deserialize cached data without copying it. The data format is the in-memory format. Libraries like FlatBuffers achieve this in other languages, but rkyv integrates tightly with Rust’s type system.1&lt;/p&gt;
    &lt;p&gt;Thread-level parallelism. Python’s GIL forces parallel work into separate processes, with IPC overhead and data copying. Rust can parallelize across threads natively, sharing memory without serialization boundaries. This matters most for resolution, where the solver explores many version combinations.1&lt;/p&gt;
    &lt;p&gt;No interpreter startup. Every time pip spawns a subprocess, it pays Python’s startup cost. uv is a single static binary with no runtime to initialize.&lt;/p&gt;
    &lt;p&gt;Compact version representation. uv packs versions into u64 integers where possible, making comparison and hashing fast. Over 90% of versions fit in one u64. This is micro-optimization that compounds across millions of comparisons.&lt;/p&gt;
    &lt;p&gt;These are real advantages. But they’re smaller than the architectural wins from dropping legacy support and exploiting modern standards.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design over language&lt;/head&gt;
    &lt;p&gt;uv is fast because of what it doesn’t do, not because of what language it’s written in. The standards work of PEP 518, 517, 621, and 658 made fast package management possible. Dropping eggs, pip.conf, and permissive parsing made it achievable. Rust makes it a bit faster still.&lt;/p&gt;
    &lt;p&gt;pip could implement parallel downloads, global caching, and metadata-only resolution tomorrow. It doesn’t, largely because backwards compatibility with fifteen years of edge cases takes precedence. But it means pip will always be slower than a tool that starts fresh with modern assumptions.&lt;/p&gt;
    &lt;p&gt;Other package managers could learn from this: static metadata, no code execution to discover dependencies, and the ability to resolve everything upfront before downloading. Cargo and npm have operated this way for years. If your ecosystem requires running arbitrary code to find out what a package needs, you’ve already lost.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nhmu.utah.edu/articles/experts-explore-new-mushroom-which-causes-fairytale-hallucinations"/><published>2025-12-26T17:07:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46393992</id><title>How uv got so fast</title><updated>2025-12-27T16:12:09.892659+00:00</updated><content>&lt;doc fingerprint="a778f61f8d52f750"&gt;
  &lt;main&gt;
    &lt;p&gt;uv installs packages faster than pip by an order of magnitude. The usual explanation is “it’s written in Rust.” That’s true, but it doesn’t explain much. Plenty of tools are written in Rust without being notably fast. The interesting question is what design decisions made the difference.&lt;/p&gt;
    &lt;p&gt;Charlie Marsh’s Jane Street talk and a Xebia engineering deep-dive cover the technical details well. The interesting parts are the design decisions: standards that enable fast paths, things uv drops that pip supports, and optimizations that don’t require Rust at all.&lt;/p&gt;
    &lt;head rend="h2"&gt;The standards that made uv possible&lt;/head&gt;
    &lt;p&gt;pip’s slowness isn’t a failure of implementation. For years, Python packaging required executing code to find out what a package needed.&lt;/p&gt;
    &lt;p&gt;The problem was setup.py. You couldn’t know a package’s dependencies without running its setup script. But you couldn’t run its setup script without installing its build dependencies. PEP 518 in 2016 called this out explicitly: “You can’t execute a setup.py file without knowing its dependencies, but currently there is no standard way to know what those dependencies are in an automated fashion without executing the setup.py file.”&lt;/p&gt;
    &lt;p&gt;This chicken-and-egg problem forced pip to download packages, execute untrusted code, fail, install missing build tools, and try again. Every install was potentially a cascade of subprocess spawns and arbitrary code execution. Installing a source distribution was essentially &lt;code&gt;curl | bash&lt;/code&gt; with extra steps.&lt;/p&gt;
    &lt;p&gt;The fix came in stages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PEP 518 (2016) created pyproject.toml, giving packages a place to declare build dependencies without code execution. The TOML format was borrowed from Rust’s Cargo, which makes a Rust tool returning to fix Python packaging feel less like coincidence.&lt;/item&gt;
      &lt;item&gt;PEP 517 (2017) separated build frontends from backends, so pip didn’t need to understand setuptools internals.&lt;/item&gt;
      &lt;item&gt;PEP 621 (2020) standardized the &lt;code&gt;[project]&lt;/code&gt;table, so dependencies could be read by parsing TOML rather than running Python.&lt;/item&gt;
      &lt;item&gt;PEP 658 (2022) put package metadata directly in the Simple Repository API, so resolvers could fetch dependency information without downloading wheels at all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PEP 658 went live on PyPI in May 2023. uv launched in February 2024. uv could be fast because the ecosystem finally had the infrastructure to support it. A tool like uv couldn’t have shipped in 2020. The standards weren’t there yet.&lt;/p&gt;
    &lt;p&gt;Other ecosystems figured this out earlier. Cargo has had static metadata from the start. npm’s package.json is declarative. Python’s packaging standards finally bring it to parity.&lt;/p&gt;
    &lt;head rend="h2"&gt;What uv drops&lt;/head&gt;
    &lt;p&gt;Speed comes from elimination. Every code path you don’t have is a code path you don’t wait for.&lt;/p&gt;
    &lt;p&gt;uv’s compatibility documentation is a list of things it doesn’t do:&lt;/p&gt;
    &lt;p&gt;No .egg support. Eggs were the pre-wheel binary format. pip still handles them; uv doesn’t even try. The format has been obsolete for over a decade.&lt;/p&gt;
    &lt;p&gt;No pip.conf. uv ignores pip’s configuration files entirely. No parsing, no environment variable lookups, no inheritance from system-wide and per-user locations.&lt;/p&gt;
    &lt;p&gt;No bytecode compilation by default. pip compiles .py files to .pyc during installation. uv skips this step, shaving time off every install. You can opt in if you want it.&lt;/p&gt;
    &lt;p&gt;Virtual environments required. pip lets you install into system Python by default. uv inverts this, refusing to touch system Python without explicit flags. This removes a whole category of permission checks and safety code.&lt;/p&gt;
    &lt;p&gt;Stricter spec enforcement. pip accepts malformed packages that technically violate packaging specs. uv rejects them. Less tolerance means less fallback logic.&lt;/p&gt;
    &lt;p&gt;Ignoring requires-python upper bounds. When a package says it requires &lt;code&gt;python&amp;lt;4.0&lt;/code&gt;, uv ignores the upper bound and only checks the lower. This reduces resolver backtracking dramatically since upper bounds are almost always wrong. Packages declare &lt;code&gt;python&amp;lt;4.0&lt;/code&gt; because they haven’t tested on Python 4, not because they’ll actually break. The constraint is defensive, not predictive.&lt;/p&gt;
    &lt;p&gt;First-index wins by default. When multiple package indexes are configured, pip checks all of them. uv picks from the first index that has the package, stopping there. This prevents dependency confusion attacks and avoids extra network requests.&lt;/p&gt;
    &lt;p&gt;Each of these is a code path pip has to execute and uv doesn’t.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimizations that don’t need Rust&lt;/head&gt;
    &lt;p&gt;Some of uv’s speed comes from Rust. But not as much as you’d think. Several key optimizations could be implemented in pip today:&lt;/p&gt;
    &lt;p&gt;HTTP range requests for metadata. Wheel files are zip archives, and zip archives put their file listing at the end. uv tries PEP 658 metadata first, falls back to HTTP range requests for the zip central directory, then full wheel download, then building from source. Each step is slower and riskier. The design makes the fast path cover 99% of cases. None of this requires Rust.&lt;/p&gt;
    &lt;p&gt;Parallel downloads. pip downloads packages one at a time. uv downloads many at once. Any language can do this.&lt;/p&gt;
    &lt;p&gt;Global cache with hardlinks. pip copies packages into each virtual environment. uv keeps one copy globally and uses hardlinks (or copy-on-write on filesystems that support it). Installing the same package into ten venvs takes the same disk space as one. Any language with filesystem access can do this.&lt;/p&gt;
    &lt;p&gt;Python-free resolution. pip needs Python running to do anything, and invokes build backends as subprocesses to get metadata from legacy packages. uv parses TOML and wheel metadata natively, only spawning Python when it hits a setup.py-only package that has no other option.&lt;/p&gt;
    &lt;p&gt;PubGrub resolver. uv uses the PubGrub algorithm, originally from Dart’s pub package manager. Both pip and PubGrub use backtracking, but PubGrub applies conflict-driven clause learning from SAT solvers: when it hits a dead end, it analyzes why and skips similar dead ends later. This makes it faster on complex dependency graphs and better at explaining failures. pip could adopt PubGrub without rewriting in Rust.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Rust actually matters&lt;/head&gt;
    &lt;p&gt;Some optimizations do require Rust:&lt;/p&gt;
    &lt;p&gt;Zero-copy deserialization. uv uses rkyv to deserialize cached data without copying it. The data format is the in-memory format. Libraries like FlatBuffers achieve this in other languages, but rkyv integrates tightly with Rust’s type system.1&lt;/p&gt;
    &lt;p&gt;Thread-level parallelism. Python’s GIL forces parallel work into separate processes, with IPC overhead and data copying. Rust can parallelize across threads natively, sharing memory without serialization boundaries. This matters most for resolution, where the solver explores many version combinations.1&lt;/p&gt;
    &lt;p&gt;No interpreter startup. Every time pip spawns a subprocess, it pays Python’s startup cost. uv is a single static binary with no runtime to initialize.&lt;/p&gt;
    &lt;p&gt;Compact version representation. uv packs versions into u64 integers where possible, making comparison and hashing fast. Over 90% of versions fit in one u64. This is micro-optimization that compounds across millions of comparisons.&lt;/p&gt;
    &lt;p&gt;These are real advantages. But they’re smaller than the architectural wins from dropping legacy support and exploiting modern standards.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design over language&lt;/head&gt;
    &lt;p&gt;uv is fast because of what it doesn’t do, not because of what language it’s written in. The standards work of PEP 518, 517, 621, and 658 made fast package management possible. Dropping eggs, pip.conf, and permissive parsing made it achievable. Rust makes it a bit faster still.&lt;/p&gt;
    &lt;p&gt;pip could implement parallel downloads, global caching, and metadata-only resolution tomorrow. It doesn’t, largely because backwards compatibility with fifteen years of edge cases takes precedence. But it means pip will always be slower than a tool that starts fresh with modern assumptions.&lt;/p&gt;
    &lt;p&gt;Other package managers could learn from this: static metadata, no code execution to discover dependencies, and the ability to resolve everything upfront before downloading. Cargo and npm have operated this way for years. If your ecosystem requires running arbitrary code to find out what a package needs, you’ve already lost.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nesbitt.io/2025/12/26/how-uv-got-so-fast.html"/><published>2025-12-26T17:13:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46395106</id><title>How Lewis Carroll computed determinants (2023)</title><updated>2025-12-27T16:12:09.803533+00:00</updated><content>&lt;doc fingerprint="358c12c671a59fa2"&gt;
  &lt;main&gt;
    &lt;p&gt;Charles Dodgson, better known by his pen name Lewis Carroll, discovered a method of calculating determinants now known variously as the method of contractants, Dodgson condensation, or simply condensation.&lt;/p&gt;
    &lt;p&gt;The method was devised for ease of computation by hand, but it has features that make it a practical method for computation by machine.&lt;/p&gt;
    &lt;head rend="h2"&gt;Overview&lt;/head&gt;
    &lt;p&gt;The basic idea is to repeatedly condense a matrix, replacing it by a matrix with one less row and one less column. Each element is replaced by the determinant of the 2×2 matrix formed by that element and its neighbors to the south, east, and southeast. The bottom row and rightmost column have no such neighbors and are removed. There is one additional part of the algorithm that will be easier to describe after introducing some notation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Details&lt;/head&gt;
    &lt;p&gt;Let A be the matrix whose determinant we want to compute and let A(k) be the matrix obtained after k steps of the condensation algorithm.&lt;/p&gt;
    &lt;p&gt;The matrix A(1) is computed as described in the overview:&lt;/p&gt;
    &lt;p&gt;Starting with A(2) the terms are similar, except each 2×2 determinant is divided by an element from two steps back:&lt;/p&gt;
    &lt;p&gt;Dodgson’s original paper from 1867 is quite readable, surprisingly so given that math notation and terminology changes over time.&lt;/p&gt;
    &lt;p&gt;One criticism I have of the paper is that it is hard to understand which element should be in the denominator, whether the subscripts should be i and j or i+1 and j+1. His first example doesn’t clarify this because these elements happen to be equal in the example.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example&lt;/head&gt;
    &lt;p&gt;Here’s an example using condensation to find the determinant of a 4×4 matrix.&lt;/p&gt;
    &lt;p&gt;We can verify this with Mathematica:&lt;/p&gt;
    &lt;quote&gt;Det[{{3, 1, 4, 1}, {5, 9, 2, 6}, {0, 7, 1, 0}, {2, 0, 2, 3}}]&lt;/quote&gt;
    &lt;p&gt;which also produces 228.&lt;/p&gt;
    &lt;head rend="h2"&gt;Division&lt;/head&gt;
    &lt;p&gt;The algorithm above involves a division and so we should avoid dividing by zero. Dodgson says to&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Arrange the given block, if necessary, so that no ciphers [zeros] occur in its interior. This may be done either by transposing rows or columns, or by adding to certain rows the several terms of other rows multiplied by certain multipliers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;He expands on this remark and gives examples. I’m not sure whether this preparation is necessary only to avoid division by zero, but it does avoid the problem of dividing by a zero.&lt;/p&gt;
    &lt;p&gt;If the original matrix has all integer entries, then the division in Dodgson’s condensation algorithm is exact. The sequence of matrices produced by the algorithm will all have integer entries.&lt;/p&gt;
    &lt;head rend="h2"&gt;Efficiency&lt;/head&gt;
    &lt;p&gt;Students usually learn cofactor expansion as their first method of calculating determinants. This rule is easy to explain, but inefficient since the number of steps required is O(n!).&lt;/p&gt;
    &lt;p&gt;The more efficient way to compute determinants is by Gaussian elimination with partial pivoting. As with condensation, one must avoid dividing by zero, hence the partial pivoting.&lt;/p&gt;
    &lt;p&gt;Gaussian elimination takes O(n³) operations, and so does Dodgson’s condensation algorithm. Condensation is easy to teach and easy to carry out by hand, but unlike cofactor expansion it scales well.&lt;/p&gt;
    &lt;p&gt;If a matrix has all integer entries, Gaussian elimination can produce non-integer values in intermediate steps. Condensation does not. Also, condensation is inherently parallelizable: each of the 2 × 2 determinants can be calculated simultaneously.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.johndcook.com/blog/2023/07/10/lewis-carroll-determinants/"/><published>2025-12-26T19:03:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46397379</id><title>Always bet on text (2014)</title><updated>2025-12-27T16:12:08.960886+00:00</updated><content>&lt;doc fingerprint="767cce1f3dd3c327"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Captcha Check&lt;/head&gt;
    &lt;p&gt;Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button!&lt;/p&gt;
    &lt;p&gt;Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://graydon2.dreamwidth.org/193447.html"/><published>2025-12-26T23:09:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46397609</id><title>Exe.dev</title><updated>2025-12-27T16:12:08.755507+00:00</updated><content>&lt;doc fingerprint="5649e739e2d32d86"&gt;
  &lt;main&gt;
    &lt;p&gt;Login ssh exe.dev _ The disk persists. You have sudo.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://exe.dev/"/><published>2025-12-26T23:42:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46397991</id><title>Publishing your work increases your luck</title><updated>2025-12-27T16:12:08.367950+00:00</updated><content>&lt;doc fingerprint="284a195f3fe6261d"&gt;
  &lt;main&gt;
    &lt;p&gt;No matter how hard you work, it still takes a little bit of luck for something to hit. That can be discouraging, since luck feels like a force outside our control. But the good news is that we can increase our chances of encountering good luck. That may sound like magic, but it’s not supernatural. The trick is to increase the number of opportunities we have for good fortune to find us. The simple act of publishing your work is one of the best ways to invite a little more luck into your life.&lt;/p&gt;
    &lt;p&gt;Before we get into the “how,” it’s important to get on the same page about the “what.” What are we talking about when we say “luck?” There are a lot of definitions that could apply, but let’s stick with a simple one: Luck is when something unexpected and good happens to you. Unexpected and good. Who doesn’t want to increase the odds of something unexpected and good?&lt;/p&gt;
    &lt;p&gt;In our world, luck can include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Having your OSS library take off&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Being invited to speak at a conference&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Landing a new job&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Getting a new consulting client&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Being invited onto a podcast&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Making new friends in your community&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;None of these things are totally in your control, which can at times feel frustrating.&lt;/p&gt;
    &lt;p&gt;How can we increase the odds of finding luck? By being a person who works in public. By doing work and being public about it, you build a reputation for yourself. You build a track record. You build a public body of work that speaks on your behalf better than any resume ever could.&lt;/p&gt;
    &lt;p&gt;The goal is not to become famous, the goal is to increase the chances of luck finding us. For me, one of the most helpful ways to think about this has always been the concept of the “Luck Surface Area,” described in an old post by Jason Roberts. He wrote (and note, the emphasis is mine):&lt;/p&gt;
    &lt;p&gt;"The amount of serendipity that will occur in your life, your Luck Surface Area, is directly proportional to the degree to which you do something you’re passionate about combined with the total number of people to whom this is effectively communicated."&lt;/p&gt;
    &lt;p&gt;Going further, he codifies it into a formula where:&lt;/p&gt;
    &lt;code&gt;Luck = [Doing Things] * [Telling People]&lt;/code&gt;
    &lt;p&gt;The more things you do multiplied by the more people you tell, the larger your Luck Surface Area becomes. The larger your Luck Surface Area, the more likely you are to catch luck as it flows by.&lt;/p&gt;
    &lt;head rend="h3"&gt;Source: Jason Roberts&lt;/head&gt;
    &lt;head rend="h2"&gt;Doing the work&lt;/head&gt;
    &lt;p&gt;Before you can publish your work, you have to actually do the work. The good news for you is that by even reading this Guide on The ReadME Project, you’ve probably already self-selected into a group of people for whom “doing things” comes somewhat naturally. You’re a developer, a designer, a creator, an author, or something else entirely. Whatever moniker you want to give yourself, you’re built to do things, and that’s the important part.&lt;/p&gt;
    &lt;p&gt;If that doesn’t ring true for you, you may fall into one of two groups:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You actually are doing things, you’ve just trained yourself to think that anything you do isn’t worth sharing.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You want to be doing things, but you can’t bring yourself to get started.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re in the first group, you may need to step back and reframe the work you’re already doing. This is a common blind spot for people who are executing at a high level! They’ve forgotten just how much they know. They think that they’re not doing anything interesting because they assume that everyone knows as much as they do. This effect is only exacerbated when everyone in your immediate vicinity is at a similar—or higher—skill level. As you become more of an expert, your quality bar gets higher and higher and you forget that everything you know is not known by everyone.&lt;/p&gt;
    &lt;p&gt;If you’re in this group I want to give you a challenge: Watch the communities where you hang out and see what people are sharing and what gets noticed. Is it something you could have done? Is it something you’ve already done? At its worst this could lead you in the direction of becoming bitter, critical, and thinking that you’re smarter than everyone. To that I say “resist!” There is no life there. My encouragement to you is to view that as objective evidence that people want to know all of the things that you already know! There is a huge opportunity for you, should you decide to start sharing your work.&lt;/p&gt;
    &lt;p&gt;If you’re in the second group, you just need to start. Start anywhere, start on anything, start something. You’ll never come up with the perfect idea for an OSS library, a business, a podcast, or an article by just thinking about it. Start on something, today. It won’t be the perfect version of the thing you have in your head, but you’ll be in motion. Motion begets motion, progress begets progress. Pick the smallest thing you can do and get started.&lt;/p&gt;
    &lt;p&gt;Doing the work is the most important part. It’s the nucleus around which everything else revolves. What that “work” looks like, though, is entirely up to you! That’s the fun part. It can take any form and be in any domain. Wherever your curiosity or expertise draw you, dive into that.&lt;/p&gt;
    &lt;p&gt;Projects outside of work are a good place to dive into your curiosity.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;If you want to make a thermal receipt printer that prints GitHub issues, you should.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you want to turn a prefabricated shed into an office, go for it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you want to go all in on an SVG drawing tool, do it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you want to write tens of thousands of words about the infrastructure of modern money, that’s a newsletter.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Your curiosity will naturally pull you in certain directions, so don’t be afraid to go super deep into a topic that you’re interested in. When a person is truly interested in the thing they’re writing or talking about, their excitement is contagious. Whatever you’re excited about, be excited about it publicly. Whatever you’re curious about, be curious about it publicly. People will want to follow along and you’ll inspire people along the way.&lt;/p&gt;
    &lt;p&gt;Projects at work can be a good place to dive into your expertise.&lt;/p&gt;
    &lt;p&gt;It's likely you're constantly solving problems and learning interesting things at your job. This is a great opportunity to take what you’re already doing and repurpose it for the benefit of others. You can turn those learnings into blog posts, conference talks, meetups, podcasts, or open source projects.&lt;/p&gt;
    &lt;p&gt;Of course not everything you do at work is shareable. If the specifics aren’t shareable, the concepts, lessons, and takeaways likely are. While you’re working, keep a scratch pad open and jot down any problems you come across, interesting patterns you see, or things you found confusing. Do this for a month and you’ll have more things to share than you know what to do with!&lt;/p&gt;
    &lt;p&gt;You’ve done the work, now it's time to tell people.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hitting the publish button&lt;/head&gt;
    &lt;p&gt;This part of the formula can be harder for most of us. Most of us really enjoy the building aspect but start to get a little shy when it comes to telling people about the stuff we’ve built. That could be for any number of reasons: fear, embarrassment, self-preservation, or an aversion to being perceived as hawking your wares.&lt;/p&gt;
    &lt;p&gt;It’s a valuable exercise to investigate whether or not you resonate with any of those reasons. Are you afraid people are going to make fun of what you built? Are you embarrassed that it isn’t up to your own (admittedly high) standards? Are you waiting for some elusive perfect moment? Do you have an aversion to “marketing” and don’t want to become the thing you hate? Whatever it is for you, I encourage you to really dig into it and see if that fear is worth keeping around.&lt;/p&gt;
    &lt;p&gt;Sharing things you’re learning or making is not prideful. People are drawn to other people in motion. People want to follow along, people want to learn things, people want to be a part of your journey. It’s not bragging to say, “I’ve made a thing and I think it’s cool!” Bringing people along is a good thing for everyone. By publishing your work you’re helping people learn. You’re inspiring others to create.&lt;/p&gt;
    &lt;p&gt;You can “publish” anywhere. For me that’s mostly Twitter because that’s where most of my peers hang out. It doesn’t have to be Twitter for you. It could be GitHub, a newsletter, a podcast, forums, your blog, YouTube, or something completely different that’s not even on my radar. Anywhere that’s not your hard drive counts!&lt;/p&gt;
    &lt;p&gt;Publishing is a skill, it’s something you can learn. You’ll need to build your publishing skill just like you built every other skill you have.&lt;/p&gt;
    &lt;p&gt;Don’t be afraid to publish along the way. You don’t have to wait until you’re done to drop a perfect, finished artifact from the sky (in fact, you may use that as an excuse to never publish). People like stories, so use that to your benefit. Share the wins, the losses, and the thought processes. Bring us along! If you haven’t been in the habit of sharing your work, it’s going to feel weird when you start. That’s normal! Keep going, you get used to it.&lt;/p&gt;
    &lt;p&gt;You’ve done the work. You’ve hit the publish button. You’ve done your part!&lt;/p&gt;
    &lt;head rend="h2"&gt;Capturing the luck&lt;/head&gt;
    &lt;p&gt;You’ve increased the odds that good, unexpected things will come your way. The exact form is hard to predict, but here are a few potential outcomes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;People start to know you as the person that talks about X, Y, and Z.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You start to get emails from people saying that they read your stuff and liked it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You get a DM about a job you might be interested in.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;People ask you if you’re taking on new clients.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Someone you’ve never met or interacted with will mention you as being an expert in your area.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A meetup asks you to come talk about the things you’ve been sharing.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You become friends with other people in your industry.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your OSS library starts gaining mindshare.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is not a random list of made-up examples, it’s a list of things that have literally happened to me once I got over my fears and started sharing my work. I had been doing the work all along, but was too afraid to publish. Once I overcame that fear, my Luck Surface Area expanded and good, unexpected things started happening.&lt;/p&gt;
    &lt;p&gt;The formula is simple.&lt;/p&gt;
    &lt;p&gt;Do the work. Don’t be afraid to dive deep into your curiosity and your expertise. We need more people that are intensely curious. We need more people with deep expertise.&lt;/p&gt;
    &lt;p&gt;Tell people. Press publish, bring us along, share the journey. Tell us what you’ve learned, what you’ve built, or what you’re excited about.&lt;/p&gt;
    &lt;p&gt;The formula may be simple, but I’ll admit it’s not always easy. It’s scary to put yourself out there. It’s hard to open yourself up to criticism. People online can be mean. But for every snarky comment, there are ten times as many people quietly following along and admiring not only your work, but your bravery to put it out publicly. And at some point, one of those people quietly following along will reach out with a life-changing opportunity and you’ll think, “Wow, that was lucky.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/readme/guides/publishing-your-work"/><published>2025-12-27T00:43:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46398201</id><title>QNX Self-Hosted Developer Desktop</title><updated>2025-12-27T16:12:08.131972+00:00</updated><content>&lt;doc fingerprint="38a649794034b286"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;QNX Self-Hosted Developer Desktop -- Initial Release&lt;/head&gt;
    &lt;p&gt;Try out the initial release of the QNX Developer Desktop -- a self-hosted development environment for QNX. No more cross-compilation!&lt;/p&gt;
    &lt;p&gt;The team and I are beyond excited to share what we've been cooking up over the last little while: a full desktop environment running on QNX 8.0, with support for self-hosted compilation! This environment both makes it easier for newly-minted QNX developers to get started with building for QNX, but it also vastly simplifies the process of porting Linux applications and libraries to QNX 8.0.&lt;/p&gt;
    &lt;p&gt;This self-hosted target environment is pre-loaded with many of the ports you'll find on the QNX Open-source Dashboard. (The portal currently includes over 1,400 ports across various targets, QNX versions, and architectures, of which more than 600 are unique ports!)&lt;/p&gt;
    &lt;p&gt;In this initial release, you can grab a copy of the QEMU image and give it a try for yourself. There's still so much more to add, but it's in a great place today for this first release. The team is really passionate about this one, and we're eagerly looking forward to your feedback!&lt;/p&gt;
    &lt;head rend="h1"&gt;What's Included&lt;/head&gt;
    &lt;p&gt;For the initial release of Desktop, we tried to cover all the basics: windowing, terminal, IDEs, browser, file management, and samples. To that end, here's what makes up the QNX Developer Desktop:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A customizable XFCE desktop environment running on Wayland&lt;/item&gt;
      &lt;item&gt;The tools you need to compile and/or run your code (&lt;code&gt;clang&lt;/code&gt;, gcc,&lt;code&gt;clang++&lt;/code&gt;, Python,&lt;code&gt;make&lt;/code&gt;,&lt;code&gt;cmake&lt;/code&gt;,&lt;code&gt;git&lt;/code&gt;, etc)&lt;/item&gt;
      &lt;item&gt;A web browser (can you join the QNX Discord from the QNX Desktop? 🏅👀)&lt;/item&gt;
      &lt;item&gt;Ports of popular IDEs/editors, like Geany, Emacs, Neovim, and vim&lt;/item&gt;
      &lt;item&gt;Thunar, for file management&lt;/item&gt;
      &lt;item&gt;Preloaded samples, like Hello World in C, C++, and Python, and GTK demos OpenGL ES demos&lt;/item&gt;
      &lt;item&gt;... and of course, a terminal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;System Requirements&lt;/head&gt;
    &lt;p&gt;This environment runs as a virtual machine, using QEMU on Ubuntu. To try the image, you'll need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ubuntu 22.04 or 24.04&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Try It Yourself&lt;/head&gt;
    &lt;p&gt;(Keep in mind this is the first release, so it takes a minute to get started and it's a bit rough around the edges.)&lt;/p&gt;
    &lt;p&gt;With a free QNX license, you can find this release in QNX Software Center. On the Available tab of the Manage Installation pane, search for "quick start" and install the "QNX SDP 8.0 Quick Start Target Image for QEMU".&lt;/p&gt;
    &lt;p&gt;You'll find the image in your QNX installation directory, usually &lt;code&gt;~/qnx800/images&lt;/code&gt; by default. Follow the &lt;code&gt;README.md&lt;/code&gt; file in the &lt;code&gt;qemu&lt;/code&gt; directory to extract &amp;amp; combine the multiple QNX packages downloaded under the hood.&lt;/p&gt;
    &lt;p&gt;Next, follow the PDF instructions found in the new &lt;code&gt;./qemu_qsti/docs/&lt;/code&gt; directory to install the required dependencies and boot up.&lt;/p&gt;
    &lt;head rend="h1"&gt;What's Next&lt;/head&gt;
    &lt;p&gt;This is just the very first release! Over the next few months and beyond, we'll drop more updates of Desktop. You can look forward to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;QEMU images for Windows &amp;amp; macOS, and native images for x86&lt;/item&gt;
      &lt;item&gt;A native Desktop image on Raspberry Pi&lt;/item&gt;
      &lt;item&gt;Enhanced documentation&lt;/item&gt;
      &lt;item&gt;Features to help use this self-hosted environment in CI jobs&lt;/item&gt;
      &lt;item&gt;More samples &amp;amp; stability&lt;/item&gt;
      &lt;item&gt;... and more! Have suggestions? Let us know.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Get Help and Share Feedback&lt;/head&gt;
    &lt;p&gt;Lastly, if you want some help with your QNX journey, you can find the QNX team and community:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;in Discord here: discord.gg/Jj4EkkrFTT&lt;/item&gt;
      &lt;item&gt;on Reddit at: reddit.com/r/qnx&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://devblog.qnx.com/qnx-self-hosted-developer-desktop-initial-release/"/><published>2025-12-27T01:16:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46398666</id><title>Inside the proton, the ‘most complicated thing you could possibly imagine’ (2022)</title><updated>2025-12-27T16:12:07.827120+00:00</updated><content>&lt;doc fingerprint="496cd10f413617a3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Inside the Proton, the ‘Most Complicated Thing You Could Possibly Imagine’&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;More than a century after Ernest Rutherford discovered the positively charged particle at the heart of every atom, physicists are still struggling to fully understand the proton.&lt;/p&gt;
    &lt;p&gt;High school physics teachers describe them as featureless balls with one unit each of positive electric charge — the perfect foils for the negatively charged electrons that buzz around them. College students learn that the ball is actually a bundle of three elementary particles called quarks. But decades of research have revealed a deeper truth, one that’s too bizarre to fully capture with words or images.&lt;/p&gt;
    &lt;p&gt;“This is the most complicated thing that you could possibly imagine,” said Mike Williams, a physicist at the Massachusetts Institute of Technology. “In fact, you can’t even imagine how complicated it is.”&lt;/p&gt;
    &lt;p&gt;The proton is a quantum mechanical object that exists as a haze of probabilities until an experiment forces it to take a concrete form. And its forms differ drastically depending on how researchers set up their experiment. Connecting the particle’s many faces has been the work of generations. “We’re kind of just starting to understand this system in a complete way,” said Richard Milner, a nuclear physicist at MIT.&lt;/p&gt;
    &lt;p&gt;As the pursuit continues, the proton’s secrets keep tumbling out. Most recently, a monumental data analysis published in August found that the proton contains traces of particles called charm quarks that are heavier than the proton itself.&lt;/p&gt;
    &lt;p&gt;The proton “has been humbling to humans,” Williams said. “Every time you think you kind of have a handle on it, it throws you some curveballs.”&lt;/p&gt;
    &lt;p&gt;Recently, Milner, together with Rolf Ent at Jefferson Lab, MIT filmmakers Chris Boebel and Joe McMaster, and animator James LaPlante, set out to transform a set of arcane plots that compile the results of hundreds of experiments into a series of animations of the shape-shifting proton. We’ve incorporated their animations into our own attempt to unveil its secrets.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cracking Open the Proton&lt;/head&gt;
    &lt;p&gt;Proof that the proton contains multitudes came from the Stanford Linear Accelerator Center (SLAC) in 1967. In earlier experiments, researchers had pelted it with electrons and watched them ricochet off like billiard balls. But SLAC could hurl electrons more forcefully, and researchers saw that they bounced back differently. The electrons were hitting the proton hard enough to shatter it — a process called deep inelastic scattering — and were rebounding from point-like shards of the proton called quarks. “That was the first evidence that quarks actually exist,” said Xiaochao Zheng, a physicist at the University of Virginia.&lt;/p&gt;
    &lt;p&gt;After SLAC’s discovery, which won the Nobel Prize in Physics in 1990, scrutiny of the proton intensified. Physicists have carried out hundreds of scattering experiments to date. They infer various aspects of the object’s interior by adjusting how forcefully they bombard it and by choosing which scattered particles they collect in the aftermath.&lt;/p&gt;
    &lt;p&gt;By using higher-energy electrons, physicists can ferret out finer features of the target proton. In this way, the electron energy sets the maximum resolving power of a deep inelastic scattering experiment. More powerful particle colliders offer a sharper view of the proton.&lt;/p&gt;
    &lt;p&gt;Higher-energy colliders also produce a wider array of collision outcomes, letting researchers choose different subsets of the outgoing electrons to analyze. This flexibility has proved key to understanding quarks, which careen about inside the proton with different amounts of momentum.&lt;/p&gt;
    &lt;p&gt;By measuring the energy and trajectory of each scattered electron, researchers can tell if it has glanced off a quark carrying a large chunk of the proton’s total momentum or just a smidgen. Through repeated collisions, they can take something like a census — determining whether the proton’s momentum is mostly bound up in a few quarks, or distributed over many.&lt;/p&gt;
    &lt;p&gt;Even SLAC’s proton-splitting collisions were gentle by today’s standards. In those scattering events, electrons often shot out in ways suggesting that they had crashed into quarks carrying a third of the proton’s total momentum. The finding matched a theory from Murray Gell-Mann and George Zweig, who in 1964 posited that a proton consists of three quarks.&lt;/p&gt;
    &lt;p&gt;Gell-Mann and Zweig’s “quark model” remains an elegant way to imagine the proton. It has two “up” quarks with electric charges of +2/3 each and one “down” quark with a charge of −1/3, for a total proton charge of +1.&lt;/p&gt;
    &lt;p&gt;But the quark model is an oversimplification that has serious shortcomings.&lt;/p&gt;
    &lt;p&gt;It fails, for instance, when it comes to a proton’s spin, a quantum property analogous to angular momentum. The proton has half a unit of spin, as do each of its up and down quarks. Physicists initially supposed that — in a calculation echoing the simple charge arithmetic — the half-units of the two up quarks minus that of the down quark must equal half a unit for the proton as a whole. But in 1988, the European Muon Collaboration reported that the quark spins add up to far less than one-half. Similarly, the masses of two up quarks and one down quark only comprise about 1% of the proton’s total mass. These deficits drove home a point physicists were already coming to appreciate: The proton is much more than three quarks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Much More Than Three Quarks&lt;/head&gt;
    &lt;p&gt;The Hadron-Electron Ring Accelerator (HERA), which operated in Hamburg, Germany, from 1992 to 2007, slammed electrons into protons roughly a thousand times more forcefully than SLAC had. In HERA experiments, physicists could select electrons that had bounced off of extremely low-momentum quarks, including ones carrying as little as 0.005% of the proton’s total momentum. And detect them they did: HERA’s electrons rebounded from a maelstrom of low-momentum quarks and their antimatter counterparts, antiquarks.&lt;/p&gt;
    &lt;p&gt;The results confirmed a sophisticated and outlandish theory that had by then replaced Gell-Mann and Zweig’s quark model. Developed in the 1970s, it was a quantum theory of the “strong force” that acts between quarks. The theory describes quarks as being roped together by force-carrying particles called gluons. Each quark and each gluon has one of three types of “color” charge, labeled red, green and blue; these color-charged particles naturally tug on each other and form a group — such as a proton — whose colors add up to a neutral white. The colorful theory became known as quantum chromodynamics, or QCD.&lt;/p&gt;
    &lt;p&gt;According to QCD, gluons can pick up momentary spikes of energy. With this energy, a gluon splits into a quark and an antiquark — each carrying just a tiny bit of momentum — before the pair annihilates and disappears. It’s this “sea” of transient gluons, quarks and antiquarks that HERA, with its greater sensitivity to lower-momentum particles, detected firsthand.&lt;/p&gt;
    &lt;p&gt;HERA also picked up hints of what the proton would look like in more powerful colliders. As physicists adjusted HERA to look for lower-momentum quarks, these quarks — which come from gluons — showed up in greater and greater numbers. The results suggested that in even higher-energy collisions, the proton would appear as a cloud made up almost entirely of gluons.&lt;/p&gt;
    &lt;p&gt;The gluon dandelion is exactly what QCD predicts. “The HERA data are direct experimental proof that QCD describes nature,” Milner said.&lt;/p&gt;
    &lt;p&gt;But the young theory’s victory came with a bitter pill: While QCD beautifully described the dance of short-lived quarks and gluons revealed by HERA’s extreme collisions, the theory is useless for understanding the three long-lasting quarks seen in SLAC’s gentle bombardment.&lt;/p&gt;
    &lt;p&gt;QCD’s predictions are easy to understand only when the strong force is relatively weak. And the strong force weakens only when quarks are extremely close together, as they are in short-lived quark-antiquark pairs. Frank Wilczek, David Gross and David Politzer identified this defining feature of QCD in 1973, winning the Nobel Prize for it 31 years later.&lt;/p&gt;
    &lt;p&gt;But for gentler collisions like SLAC’s, where the proton acts like three quarks that mutually keep their distance, these quarks pull on each other strongly enough that QCD calculations become impossible. Thus, the task of further demystifying the three-quark view of the proton has fallen largely to experimentalists. (Researchers who run “digital experiments,” in which QCD predictions are simulated on supercomputers, have also made key contributions.) And it’s in this low-resolution picture that physicists keep finding surprises.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Charming New View&lt;/head&gt;
    &lt;p&gt;Recently, a team led by Juan Rojo of the National Institute for Subatomic Physics in the Netherlands and VU University Amsterdam analyzed more than 5,000 proton snapshots taken over the last 50 years, using machine learning to infer the motions of quarks and gluons inside the proton in a way that sidesteps theoretical guesswork.&lt;/p&gt;
    &lt;p&gt;The new scrutiny picked up a background blur in the images that had escaped past researchers. In relatively soft collisions just barely breaking the proton open, most of the momentum was locked up in the usual three quarks: two ups and a down. But a small amount of momentum appeared to come from a “charm” quark and charm antiquark — colossal elementary particles that each outweigh the entire proton by more than one-third.&lt;/p&gt;
    &lt;p&gt;Short-lived charms frequently show up in the “quark sea” view of the proton (gluons can split into any of six different quark types if they have enough energy). But the results from Rojo and colleagues suggest that the charms have a more permanent presence, making them detectable in gentler collisions. In these collisions, the proton appears as a quantum mixture, or superposition, of multiple states: An electron usually encounters the three lightweight quarks. But it will occasionally encounter a rarer “molecule” of five quarks, such as an up, down and charm quark grouped on one side and an up quark and charm antiquark on the other.&lt;/p&gt;
    &lt;p&gt;Such subtle details about the proton’s makeup could prove consequential. At the Large Hadron Collider, physicists search for new elementary particles by bashing high-speed protons together and seeing what pops out; to understand the results, researchers need to know what’s in a proton to begin with. The occasional apparition of giant charm quarks would throw off the odds of making more exotic particles.&lt;/p&gt;
    &lt;p&gt;And when protons called cosmic rays hurtle here from outer space and slam into protons in Earth’s atmosphere, charm quarks popping up at the right moments would shower Earth with extra-energetic neutrinos, researchers calculated in 2021. These could confound observers searching for high-energy neutrinos coming from across the cosmos.&lt;/p&gt;
    &lt;p&gt;Rojo’s collaboration plans to continue exploring the proton by searching for an imbalance between charm quarks and antiquarks. And heavier constituents, such as the top quark, could make even rarer and harder-to-detect appearances.&lt;/p&gt;
    &lt;p&gt;Next-generation experiments will seek still more unknown features. Physicists at Brookhaven National Laboratory hope to fire up the Electron-Ion Collider in the 2030s and pick up where HERA left off, taking higher-resolution snapshots that will enable the first 3D reconstructions of the proton. The EIC will also use spinning electrons to create detailed maps of the spins of the internal quarks and gluons, just as SLAC and HERA mapped out their momentums. This should help researchers to finally pin down the origin of the proton’s spin, and to address other fundamental questions about the baffling particle that makes up most of our everyday world.&lt;/p&gt;
    &lt;p&gt;Correction: October 20, 2022&lt;lb/&gt; A previous version of the article erroneously implied that lower-momentum quarks live shorter lives than higher-momentum quarks in the quark sea. The text has been updated to clarify that all these quarks are lower-momentum and shorter-lived than those in the three quark-picture.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/inside-the-proton-the-most-complicated-thing-imaginable-20221019/"/><published>2025-12-27T03:00:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46399576</id><title>More dynamic cronjobs</title><updated>2025-12-27T16:12:07.606544+00:00</updated><content>&lt;doc fingerprint="29aaa31a29a63e9c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;More dynamic cronjobs&lt;/head&gt;&lt;p&gt;• ~600 words • 3 minute read&lt;/p&gt;&lt;p&gt;I remember learning about cronjobs in the early 2000s. I could tell the computer to go do something, on a recurring basis, forever, even when I wasn't there. They felt like magic!&lt;/p&gt;&lt;p&gt;We didn't have Crontab.guru or AI to ask for figuring out some of the more complex specifications. Just the man pages and good old-fashioned trial and error—mostly error in my case.&lt;/p&gt;&lt;p&gt;But while you could do fun, complex specifications of recurring intervals, you couldn't quite specify something quite as dynamic as "run this script every Tuesday at 7am unless it's the last Tuesday of the month..."&lt;/p&gt;&lt;p&gt;Or at least, you couldn't strictly through the crontab specification syntax. But I had a recent, mildly embarrassing epiphany that it's not hard at all to add arbitrary checks to your crontab to account for more complex and dynamic scenarios.&lt;/p&gt;&lt;p&gt;Want to run a script every Tuesday of the month at 7am except for the last Tuesday? That's easy—set up your crontab to run every Tuesday at 7am, but add a little check to make sure the next week is still part of the same month:&lt;/p&gt;&lt;code&gt;0 7 * * Tue [ "$(date -v+7d '+%m')" = "$(date '+%m')" ] &amp;amp;&amp;amp; /path/to/your_command
&lt;/code&gt;&lt;p&gt;If it's not part of the same month, that means we're on the last Tuesday for the month and the script won't run.&lt;/p&gt;&lt;p&gt;Note: The &lt;code&gt;-v&lt;/code&gt; flag is for the macOS/BSD flavors of &lt;code&gt;date&lt;/code&gt;. On Linux you'd want to use &lt;code&gt;-d +7 days&lt;/code&gt; instead.&lt;/p&gt;&lt;p&gt;This really has nothing to do with cronjobs at all and everything to do with the POSIX "test" command which is the thing we're using with those square brackets. I'm used to seeing and utilizing them in shell scripts, but for whatever reason I never thought to reach for that tool here in the crontab.&lt;/p&gt;&lt;p&gt;You could just as easily rewrite it like this, skipping the bracket shorthand, which is probably easier to read:&lt;/p&gt;&lt;code&gt;0 7 * * Tue test "$(date -v+7d '+%m')" = "$(date '+%m')" &amp;amp;&amp;amp; /path/to/your_command
&lt;/code&gt;&lt;p&gt;It never crossed my mind until recently to add slightly more complex checks at the crontab level.&lt;/p&gt;&lt;head rend="h3"&gt;Other clever cronjob things you can do:&lt;/head&gt;&lt;head rend="h4"&gt;Holiday-only cronjobs&lt;/head&gt;&lt;p&gt;Maybe fetch a list of all the US Holidays for a given year and store them in a handy &lt;code&gt;HOLIDAYS.txt&lt;/code&gt; file somewhere:&lt;/p&gt;&lt;code&gt;curl -s https://date.nager.at/api/v3/PublicHolidays/2025/US | jq -r '.[].date' &amp;gt; HOLIDAYS.txt
&lt;/code&gt;&lt;p&gt;Now you can update your cronjob to run every Tuesday at 7am except on Holidays:&lt;/p&gt;&lt;code&gt;0 7 * * Tue ! grep -qx "$(date +%F)" HOLIDAYS.txt &amp;amp;&amp;amp; /path/to/your_command
&lt;/code&gt;&lt;p&gt;Or inversely, maybe run a holiday-only script that checks once a day&lt;/p&gt;&lt;code&gt;@daily grep -qx "$(date +%F)" HOLIDAYS.txt &amp;amp;&amp;amp; /path/to/your_special_holiday_command
&lt;/code&gt;&lt;head rend="h4"&gt;Only run on sunny days&lt;/head&gt;&lt;p&gt;The National Weather Service makes all kinds of fun data available (if you can find it...). How about a script that runs every hour, but only when the weather is clear?&lt;/p&gt;&lt;code&gt;@hourly curl -s "https://api.weather.gov/gridpoints/TOP/32,81/forecast/hourly" | jq -r '.properties.periods[0].shortForecast' | grep -qi clear &amp;amp;&amp;amp; /path/to/your_command
&lt;/code&gt;&lt;p&gt;Or maybe when the weather is cloudy?&lt;/p&gt;&lt;code&gt;@hourly curl -s "https://api.weather.gov/gridpoints/TOP/32,81/forecast/hourly" | jq -r '.properties.periods[0].shortForecast' | grep -qi cloudy &amp;amp;&amp;amp; /path/to/your_command
&lt;/code&gt;&lt;head rend="h4"&gt;Only run when there's something newsworthy&lt;/head&gt;&lt;p&gt;Or maybe we get in line with every-other-startup I'm aware of and throw AI at the problem, only running our script when the LLM gods have decided there is something newsworthy:&lt;/p&gt;&lt;code&gt;@hourly curl -s "https://news.google.com/rss?hl=en-US&amp;amp;gl=US&amp;amp;ceid=US:en" | llm --system "Reply strictly 'yes' or 'no'. Does anything in the news today suggest it is a good reason to run a script that I only want to send when the world is on fire and crazy and terrible things are happening?"  | tr -d '[:space:]' | tr '[:upper:]' '[:lower:]' | grep -qx yes &amp;amp;&amp;amp; /path/to/oh_no
&lt;/code&gt;--&lt;p&gt;Published on Sunday, September 21st 2025. Read this post as Markdown or plain-text.&lt;/p&gt;&lt;p&gt;If you enjoyed reading this consider signing-up for my newsletter, sharing it on Hacker News or hiring me.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://george.mand.is/2025/09/more-dynamic-cronjobs/"/><published>2025-12-27T06:10:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46400242</id><title>Verdichtung</title><updated>2025-12-27T16:12:07.543434+00:00</updated><content>&lt;doc fingerprint="ba60107f19a937ff"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;2025-12-26 - Verdichtung&lt;/head&gt;
    &lt;p&gt;Verdichtung is German word that loosely means "densification". It represents an alternative to urban sprawl⁰ that is prevalent in US cities and has been the dominant strategy for Zurich's expansion.&lt;/p&gt;
    &lt;p&gt;What it means is that even though there would theoretically be new space available on the outskirts, the city, or canton, instead of dedicating it to new construction it prefers¹ to repurpose existing buildings by increasing the number of floors or by reducing the distance between buildings.&lt;/p&gt;
    &lt;p&gt;I am actually currently living in such a building created from Verdichtung where before this was partially a parking lot and partially an annex of another building. Looking at historical maps, there are many such occurences&lt;/p&gt;
    &lt;p&gt;Two Google Earth images of Schlieren from 2009 and 2024 showing the practicalities of verdichtung. Look out for the central area turning multiple smaller housing blocks into a larger one and eliminating "wasted space".&lt;/p&gt;
    &lt;p&gt;But top-down maps only tell half the story as they often look directly from above, Verdichtung also often means an increase in height. This does create more living space overall but it also creates some weird apartments that feel way too close together and that often can only serve people that do not &lt;del&gt;are financially unable to&lt;/del&gt; have any demands for either sunlight, privacy, or quietness. For example, a parking lot not far away was repurposed into a hotel-like complex where balconies made out of pure concrete are facing each other.&lt;/p&gt;
    &lt;p&gt;Example sketch (not real but pretty much 1:1 of an apartment complex built close to where I live in 2025) of what Verdichtung looks like in practice. There was a parking lot before here, now we have balconies made out of pure concrete facing each other with no sunlight, privacy, or coziness. The prices are sure high though!&lt;/p&gt;
    &lt;head rend="h2"&gt;Effect on ownership?&lt;/head&gt;
    &lt;p&gt;Ownership in Zurich has undergone a quiet shift in the last years; where private individuals held 41% of the stock in 2010, they now account for barely 31% in 2024 (data from stadt-zuerich.ch). While this generally follows the trend of Switzerland simply having the lowest home ownership in Europe (link), it could be empirically argued that Verdichtung has played a role in this shift as demolishing and building denser is highly complex and expensive, something which would be nearly impossible to do by a private individual, especially so since your neighbors can essentially complain and delay the project for a while. Compare that to you just buying a new plot of land with reasonably well defined rules and building there or just renovating an existing building and it is night and day.&lt;/p&gt;
    &lt;p&gt;Hence projects doing Verdichtung generally lead to four ownership scenarios for land that might have been previously owned by individuals:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A private company does the project and rents out the flats generating very good stable and predictable revenue per m2.&lt;/item&gt;
      &lt;item&gt;A Genossenschaft (cooperative) or a pension fund does 1.&lt;/item&gt;
      &lt;item&gt;A private company does the project and sells the flats to individuals (less often than 1., companies tend to love the stable income of renting out).&lt;/item&gt;
      &lt;item&gt;The city does the project and rents out the flats, often dedicating a percentage of the flats to social housing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Neither cooperatives nor the city typically sell flats. Mostly because it's a massive effort for them to acquire the property, and they really love recurring revenue and absolutely would hate to deal with short-term income as they are generally non-profit institutions.&lt;/p&gt;
    &lt;p&gt;Looking at the above, we see that in most scenarios, Verdichtung leads to a shift from private ownership to either private companies or cooperatives and private individuals renting out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Zurich's ups and downs&lt;/head&gt;
    &lt;p&gt;Zurich has a weird history when it comes to its urban population where its population peaked in the 1970s (at ~422,000), then dropped due to a push for suburbanization and only recently reached the same level again. This is pretty different from many other urban centers as urbanization has been rampant in Europe in the last decades and has been influenced by various factors like the different tax rates across the canton.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; The population of ZRH over time, rough estimate, source: Federal Statistical Office of Switzerland.&lt;/p&gt;
    &lt;head rend="h2"&gt;Any alternatives?&lt;/head&gt;
    &lt;p&gt;Short-term, this seems like much less nice than living in a suburban area but long-term this is much more sustainable. Has it been able to meet demands? No. Zurich's buying market is one of the worst in the world according to the UBS Bubble Index (ranked #3 globally in 2024) having very high prices and some risk of a market correction. Are there many alternatives? No. Not really as free areas are sparse and the city is sandwiched between the lake, the Uetliberg and Züriberg hills, and an airport that is just 10 km away from the city. The reality of the housing market reflects this sharply as prices drop steeply as soon as you hop over a ridge or float down the Limmat.&lt;/p&gt;
    &lt;p&gt;Verdichtung, however does have some upsides. Commute times within the city often are within the magical boundary of 15 minutes (see &lt;code&gt;15 minute city&lt;/code&gt;) and, due to the short distances and okayish infrastructure, allow for using a bicycle².&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrap-up&lt;/head&gt;
    &lt;p&gt;Verdichtung is the price we pay for not having suburbs but we at least get great infrastructure at a reasonable cost. You could say it's Swiss efficiency applied to the housing problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;⁰ Zersiedlung is the Swiss-German term for urban sprawl.&lt;/p&gt;
    &lt;p&gt;¹ If we look under the hood, this preference is a mandate based on the federal-level Raumplanungsgesetz (RPG) linked here. Which says that development has to be done inwards while considering an adequate "quality of housing". Interestingly, and, if you're familiar with Swiss law unsurprisingly, the formulation is extremely vague as it also mandates decentralization, environmental protection, and integration of foreigners so ultimately it's kind of a catch-all that can be used to justify almost anything 🤷♂️ and is one of those things that only works in Switzerland where checks and balances are abundant and abuse is relatively rare.&lt;/p&gt;
    &lt;p&gt;² At least when you're OK that your bicycle lane can randomly stop at a narrow point or street corner that is not wide enough and then continue when the road widens. Example below:&lt;/p&gt;
    &lt;p&gt;The popular YouTube channel &lt;code&gt;Not Just Bikes&lt;/code&gt; released a comprehensive video about ZRH if you're interested in more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alexeygy.github.io/blog/verdichtung/"/><published>2025-12-27T08:42:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46400251</id><title>Show HN: Ez FFmpeg – Video editing in plain English</title><updated>2025-12-27T16:12:07.466041+00:00</updated><content/><link href="http://npmjs.com/package/ezff"/><published>2025-12-27T08:45:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46401190</id><title>Splice a Fibre</title><updated>2025-12-27T16:12:07.299306+00:00</updated><link href="https://react-networks-lib.rackout.net/fibre"/><published>2025-12-27T11:57:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46401499</id><title>OrangePi 6 Plus Review: The New Frontier for ARM64 SBC Performance</title><updated>2025-12-27T16:12:07.031907+00:00</updated><content>&lt;doc fingerprint="3753087659829594"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;OrangePi 6 Plus Review: The New Frontier for ARM64 SBC Performance&lt;/head&gt;
    &lt;p&gt;So after our previous reviews (that started mainly around RISC-V since we are really interested in this new architecture) of SBC, we continue to review what’s available these days in the world of small, versatile computers. Today this is going to be about the OrangePi 6 Plus, following our previous review of the OrangePi 5 ultra board.&lt;/p&gt;
    &lt;p&gt;This is NOT a super small, credit card format SBC. We are talking about something that’s definitely larger, and that comes directly with an integrated heatsink.&lt;/p&gt;
    &lt;p&gt;At the bottom there’s a wealth of ports, and it’s where you will install the necessary wireless module if you want Wifi and Bluetooth.&lt;/p&gt;
    &lt;p&gt;Now let’s dive into what you can do with this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ports&lt;/head&gt;
    &lt;p&gt;Here is what you can get from the top of the device. Note that most of it will be hidden from view as the board comes pre-installed with the heatsink that covers the SOC and the memory chips.&lt;/p&gt;
    &lt;p&gt;And the bottom view.&lt;/p&gt;
    &lt;p&gt;You can tell just from the format that you should have higher expectations from the hardware.&lt;/p&gt;
    &lt;head rend="h2"&gt;Specs of the OrangePi 6 Plus&lt;/head&gt;
    &lt;p&gt;In a table format:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Specification&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SoC&lt;/cell&gt;
        &lt;cell&gt;CIX CD8180 / CD8160 (12-core 64-bit)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;CPU Architecture&lt;/cell&gt;
        &lt;cell&gt;4× Cortex-A720 (High-perf) + 4× Cortex-A720 (Main) + 4× Cortex-A520 (Efficiency)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;GPU&lt;/cell&gt;
        &lt;cell&gt;Arm Immortalis-G720 MC10 (Ray Tracing &amp;amp; 8K Decoding support)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;NPU (AI)&lt;/cell&gt;
        &lt;cell&gt;Up to 45 TOPS (System-wide); ~30 TOPS Dedicated NPU&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;RAM&lt;/cell&gt;
        &lt;cell&gt;16GB / 32GB / 64GB LPDDR5 (128-bit)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;2× M.2 2280 slots (PCIe 4.0 x4 NVMe), 1× MicroSD (TF) slot&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Networking&lt;/cell&gt;
        &lt;cell&gt;Dual 5GbE (5000Mbps) Ethernet ports&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Wireless&lt;/cell&gt;
        &lt;cell&gt;M.2 Key-E (2230) slot for Wi-Fi 6E/7 &amp;amp; Bluetooth 5.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Video Output&lt;/cell&gt;
        &lt;cell&gt;1× HDMI 2.1 (8K@60Hz), 1× DP 1.4, 2× USB-C (DP Alt Mode), 1× eDP&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;USB Ports&lt;/cell&gt;
        &lt;cell&gt;2× USB 3.0 Type-A, 2× USB 2.0 Type-A, 2× Full-function USB Type-C&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Camera (MIPI)&lt;/cell&gt;
        &lt;cell&gt;2× 4-lane MIPI CSI interfaces&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Expansion&lt;/cell&gt;
        &lt;cell&gt;40-pin GPIO header (UART, I2C, SPI, PWM, etc.)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Audio&lt;/cell&gt;
        &lt;cell&gt;3.5mm Headphone/Mic jack, 2× Speaker headers, 1× Analog MIC header&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Power Supply&lt;/cell&gt;
        &lt;cell&gt;100W Dual USB Type-C PD (20V/5A)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Dimensions&lt;/cell&gt;
        &lt;cell&gt;115mm × 100mm&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;As you can see from the specs, this is no joke. 16GB RAM by default, 12-cores processor, with a powerful GPU (Immortalis G720), and a NPU that has a claimed performance up to 30 TOPS. Not just that, but there’s numerous ports on this SBC, with 2 full sized M2 2280 slots! You can tell that the IO is not going to be a joke here.&lt;/p&gt;
    &lt;p&gt;If you are wondering about the SOC itself, we have more info about what to expect from CIX.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;Specification&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SoC Model&lt;/cell&gt;
        &lt;cell&gt;CIX CD8180 / CD8160 (Codename: CIX P1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Architecture&lt;/cell&gt;
        &lt;cell&gt;Armv9.2-A (64-bit)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Total CPU Cores&lt;/cell&gt;
        &lt;cell&gt;12 Cores (Tri-cluster configuration)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Big Cores&lt;/cell&gt;
        &lt;cell&gt;4× Cortex-A720 @ Up to 2.8 GHz (Performance)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Medium Cores&lt;/cell&gt;
        &lt;cell&gt;4× Cortex-A720 @ Up to 2.4 GHz (Mainstream)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Little Cores&lt;/cell&gt;
        &lt;cell&gt;4× Cortex-A520 @ 1.8 GHz (Efficiency)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;L3 Cache&lt;/cell&gt;
        &lt;cell&gt;12MB Shared L3 Cache&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;GPU&lt;/cell&gt;
        &lt;cell&gt;Arm Immortalis-G720 MC10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Graphics Features&lt;/cell&gt;
        &lt;cell&gt;Hardware Ray Tracing, Vulkan 1.3, OpenGL ES 3.2, OpenCL 3.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;NPU (AI Engine)&lt;/cell&gt;
        &lt;cell&gt;Arm-China Zhouyi: 30 TOPS (Dedicated); ~45 TOPS (Total System AI)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;AI Precision&lt;/cell&gt;
        &lt;cell&gt;INT4, INT8, INT16, FP16, TF32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;VPU (Video)&lt;/cell&gt;
        &lt;cell&gt;Linlon V8: 8K@60fps Decode (AV1/H.265/VP9), 8K@30fps Encode (H.265)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Memory Interface&lt;/cell&gt;
        &lt;cell&gt;128-bit LPDDR5 / LPDDR5X (Up to 5500 MT/s)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Memory Bandwidth&lt;/cell&gt;
        &lt;cell&gt;Up to 96 GB/s (Theoretical peak)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;PCIe Support&lt;/cell&gt;
        &lt;cell&gt;PCIe Gen4 (Supports x8, x4, and x2 configurations)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;System Security&lt;/cell&gt;
        &lt;cell&gt;Integrated Security Engine (Standard Arm SystemReady / ACPI support)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;2.8Ghz ARM Big Cores processors! That’s no joke, this is way beyond the typical frequency we see for small boards where things are usually below 2 Ghz. The memory interface also has a huge bandwidth, and we get full PCI4 with 8 lanes! This means that this board could probably be attached to an external GPU (eGPU) and be able to drive it (provided adequate software support).&lt;/p&gt;
    &lt;p&gt;About the NPU, the usual problem on Linux is that there is poor software support, and it’s certainly not in the mainline either. To leverage the 30 TOPS of dedicated AI power, you cannot simply use standard versions of PyTorch or TensorFlow out of the box. You must use the NeuralONE AI SDK. This also means that the NPU cannot use regular weights, and need to compile them into a different format to make them work. On paper, the NPU is highly versatile and supports the following data types: INT4, INT8, INT16, FP16, BF16, and TF32. The board has extensive documentation on a bunch of embedded models for vision detection, automation, such as YOLOv8 (Vision), ResNet50, OpenPose, and DeepLabv3.&lt;/p&gt;
    &lt;p&gt;Now let’s jump into the actual user experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Software Support&lt;/head&gt;
    &lt;head rend="h3"&gt;Desktop Experience&lt;/head&gt;
    &lt;head rend="h4"&gt;Debian Bookworm&lt;/head&gt;
    &lt;p&gt;There is a Debian Bookworm (12) image available at the time of writing. I was actually waiting for a 24.04 Ubuntu image to be made available, and that was the plan at some point according to my OrangePi contact, but they had to shift priorities apparently and now there is no ETA for the Ubuntu image. So instead of waiting, I decided to go ahead and review what’s possible with this Debian image. As you know, Debian Bookworm is not that new anymore (the base is from Mid-2023), and is now superseded by Debian Trixie (released in 2025). This Debian image comes with two kernels available, 6.1 and 6.6. The 6.6 kernel is also from 2023, and it’s not surprising you don’t get a more recent kernel, since the Linux support for various parts of the boards, not being upstreamed and mainlined, is very likely to be stuck on an older version. This is usually what causes headaches down the road: maybe some general functionality can be upstreamed, but will the NPU have working drivers for a more recent kernel? Your guess is as good as mine.&lt;/p&gt;
    &lt;p&gt;You can burn the image directly on the NVME drive - no need to boot on a MicroSD card this time around. Note that at the beginning, my OrangePi did not boot, but I could see from the BIOS (yes, this board comes with a BIOS-like interface!) that everything was supposed to be seen by the SBC. Turns out that the firmware required an update to be able to boot on this Linux kernel, and after imaging the latest firmware on a USB stick and booting on it, a few minutes later, things worked as expected.&lt;/p&gt;
    &lt;p&gt;In any case, we get a GNOME desktop after boot. And everything works pretty much as you’d expect. Once thing that is immediately apparent when you start with the desktop is how snappy everything is. SBC boards like the Raspberry Pi 4 provide a good desktop experience but have some general sluggishness to them. On this board, this is pretty much like having a X86_64 experience. It recognized immediately my Ultra Wide Display and supported the 3440 x 1440 resolution without a hitch. Everything from navigating the desktop and settings is very fast. You get Chromium by default (and Firefox ESR in the repos) and the browser experience is very clean and fast, too. This board has absolutely no problem to play Youtube streams, even at 4k. This thing is FAST!&lt;/p&gt;
    &lt;p&gt;A quick look at vulkaninfo shows that we have working Vulkan drivers on this board! This is something that was initially very exciting, but it turns out that there are some limitations. More on that later.&lt;/p&gt;
    &lt;p&gt;Turns out the Vulkan driver is limited to some early 1.3 version. You have options to upgrade Mesa, by using Debian backports - it gets you to a 25.07 Mesa version, where you don’t rely anymore on the proprietary driver - no, this time you get Panfrost, which means a more robust driver potentially… except that you are stuck on the 6.6 Linux Kernel, and Panfrost requires a more recent kernel to work properly (6.10+ apparently). So the solution would be to move to a more recent kernel, right? Do the Debian backports have it? Yes. Problem is, moving away from 6.6 will break a lot of patches that are necessary for the hardware of this board to work. Such as HDMI out at resolutions higher than 1080p, and NPU support! So, it’s a major trade-off.&lt;/p&gt;
    &lt;head rend="h4"&gt;Getting Bluetooth to work&lt;/head&gt;
    &lt;p&gt;Even though bluetooth shows in the GNOME desktop controls, and that it could see some of my peripherals, it could not connect to any of my external audio devices. But I don’t give up so fast. Turns out there’s an issue with a missing pipewire dependency to allow for bluetooth audio to work. Here’s how you can get it done:&lt;/p&gt;
    &lt;code&gt;sudo apt install libspa-0.2-bluetooth pipewire-audio-client-libraries
&lt;/code&gt;
    &lt;p&gt;Afterwards you can launch bluetoothctl from the command line, and you can execute the following commands one by one&lt;/p&gt;
    &lt;code&gt;power on
agent on
default-agent
scan on
&lt;/code&gt;
    &lt;p&gt;After the scan is activated you should see bluetooth device popping up in the terminal. Note the ID of the bluetooth device you want to connect, and then:&lt;/p&gt;
    &lt;code&gt;pair &amp;lt;device id&amp;gt;
trust &amp;lt;device id&amp;gt;
connect &amp;lt;devic_id&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Once this is fixed, things work as expected. And now I get audio!&lt;/p&gt;
    &lt;head rend="h4"&gt;Compiling OBS&lt;/head&gt;
    &lt;p&gt;OBS is not available as flatpak for arm64, and not in the repos either. This means, that you are in for a compilation from sources. This is not the thing that scares me. But in our situation, it’s a little more convoluted that I expected. First, turns out that OBS did not like the fact that the Cmake was relatively old. So I had to get one of the recent Cmake binaries. Thanksfully they have arm64 binaries already available on their website, so that was easy.&lt;/p&gt;
    &lt;p&gt;Next, OBS would complain of a too old FFmpeg version. Not too surprising for something that depends so heavily on it. So I had to go for a full FFmpeg compilation. Here’s what I did to save you time:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/FFmpeg/FFmpeg.git
cd ffmpeg
git checkout release/7.1 # in order to have a stable release, not the master branch in itself

./configure --prefix=/usr/local --enable-shared --disable-static --enable-gpl --enable-libx264 --enable-libx265
make -j$(nproc)

sudo make install
&lt;/code&gt;
    &lt;p&gt;Finally, compiling OBS is a game of cat and mouse, you need to basically give up one extension after the other as new errors arise. Most of the things you need to remove are not critical (NVENC, not relevant on non-nvidia hardware, browser support, not really our thing either) and at the end you get a fairly long series of command.&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/obsproject/obs-studio.git
cd obs-studio

# remove the obs-browser plugin from the obs folder root directlroy
mv plugins/obs-browser plugins/obs-browser.bak

# from the obs root folder
git submodule update --init --recursive

# a couple of exports to avoid the compilation to fail because of FFmpeg warnings
export CFLAGS="-Wno-error=deprecated-declarations"
export CXXFLAGS="-Wno-error=deprecated-declarations"

# since the flags can be ignored, we also edit the following file from the root directory
echo 'target_compile_options(obs-ffmpeg PRIVATE "-Wno-error=deprecated-declarations")' &amp;gt;&amp;gt; plugins/obs-ffmpeg/CMakeLists.txt

# use the path to the cmake version you just downloaded
&amp;lt;path_to_newer_cmake_binary&amp;gt;/cmake -DCMAKE_PREFIX_PATH=/usr/local -DFFMPEG_INCLUDE_DIR=/usr/local/include  -DPKG_CONFIG_PATH=/usr/local/lib/pkgconfig -DENABLE_AJA=OFF -DUNIX_STRUCTURE=1 -DENABLE_GIO=OFF -DENABLE_VPL=OFF  -DENABLE_QSV11=OFF -DENABLE_BROWSER=OFF  -DENABLE_WEBRTC=OFF -DENABLE_NATIVE_NVENC=OFF -DCMAKE_COMPILE_WARNING_AS_ERROR=OFF ..

make -j$(nproc)

sudo make install

&lt;/code&gt;
    &lt;p&gt;It took a little while, but it worked!&lt;/p&gt;
    &lt;p&gt;I must admit, I did not expect that it would go so well in the end. Now, thanks to this, you will get a lot of videos from the board in action that no other site reviewing that board has been able to offer.&lt;/p&gt;
    &lt;head rend="h4"&gt;Noise and Temperature&lt;/head&gt;
    &lt;p&gt;The board is very quiet by default. Even when the fan is activated, you barely hear it, at least under fairly typical conditions. Things change when you push the board to its full power, for example a long compilation time. In such conditions, the fan becomes louder with a woosh kind of sound. You will definitely hear it when doing benchmarks, and in my case, when running LLMs, for example.&lt;/p&gt;
    &lt;p&gt;In terms of temperature, the control is very good. At 100% usage even for long durations, while the fan becomes clearly noticeable, it maintains the temperature under 60 C (this is winter right now, and the temperature remained at 58 C). Kudos for the good engineering there. It’s as good as the custom cooling solution for my RTX3090.&lt;/p&gt;
    &lt;head rend="h4"&gt;Power Draw&lt;/head&gt;
    &lt;p&gt;I don’t have a way to measure the power draw currently, but based on reports from other publications it looks like we are looking at:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;15W at idle, which is fairly high&lt;/item&gt;
      &lt;item&gt;30+W during usage - this is why they recommend to use the provided PSU with USB-C. Note that any charger that can provide 45W should do the trick as well.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, if you are looking for something that has almost no footprint at idle, this is not it. At idle the OrangePi 5 Ultra consumes more than 3 times less, so that sound more like something you’d use for a server.&lt;/p&gt;
    &lt;head rend="h4"&gt;Benchmarks&lt;/head&gt;
    &lt;p&gt;A quick run at Geekbench 6 shows a very strong single core score, and an exceptional multi-core score.&lt;/p&gt;
    &lt;p&gt;Now the details in single score:&lt;/p&gt;
    &lt;p&gt;And in multi-core where we can see that the OrangePi 6 absolutely crushes what you can find on a Raspberry Pi 5.&lt;/p&gt;
    &lt;p&gt;Of course, this is a relatively cheap system that is not going to win the benchmark charts. But look at the results. On single core, we get a score equivalent to a i5-10500 running at a similar frequency (2.3 Ghz).&lt;/p&gt;
    &lt;p&gt;On multicore this is much more impressive, thanks to its twelve cores. And it gets very close, according to the bench, to what an AMD Ryzen 7 4800H (8 cores) can deliver.&lt;/p&gt;
    &lt;p&gt;This is clearly a powerhouse, CPU-wise. For a starting price at 199 USD for the 16GB version, this is a fantastic value proposition. We reviewed the OrangePi 5 Ultra a few months back and the price point is very close (around 160 USD), and unless you need something very small and fanless, the OrangePi 6 Plus is (very clearly) a better deal.&lt;/p&gt;
    &lt;head rend="h4"&gt;Gaming&lt;/head&gt;
    &lt;p&gt;Since we have both a fairly powerful SOC, and a working Vulkan driver, this means that we can expect Box64 to do some magic for us there. I compiled Box64 to make it possible to launch Steam, and for some reason there is some Vulkan related error that prevents Steam from launching. Too bad. I still have a GOG account with a few games that have Linux clients. I tried the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Beholder 2&lt;/item&gt;
      &lt;item&gt;Shadow Warrior 2013&lt;/item&gt;
      &lt;item&gt;Oxenfree&lt;/item&gt;
      &lt;item&gt;Day of the Tentacle Remastered&lt;/item&gt;
      &lt;item&gt;Torchlight 2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And with some degree of success! Beholder 2 worked just fine, and are definitely playable on this board, while the framerate remains between the 20s and 30s FPS (here a little slower on this video because of software OBS capture).&lt;/p&gt;
    &lt;p&gt;Here’s Oxenfree’s Linux x86_64 client running in Full HD on the OrangePi 6 Plus, using Box64:&lt;/p&gt;
    &lt;p&gt;Torchlight 2 runs nicely too, at something like 20 to 30 FPS in Full HD.&lt;/p&gt;
    &lt;p&gt;Shadow Warrior refused to launch, complaining about the graphics drivers not being recent enough (turns out that Zink provides OpenGL support, and there is some issue to detect that the OpenGL version is properly supported… my guess). Day of the Tentacle crashes at start too, not sure why. When games work, the performance is not staggering but convincing for a somewhat small SoC at the end of the day. AMD and Intel are certainly ahead in terms of graphics performance on integrated chips, but they have much larger processors and much more expensive ones, too.&lt;/p&gt;
    &lt;p&gt;I also tried FOSS games or engines:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GZDoom (compiled from source, the flatpak version sucks in performance!)&lt;/item&gt;
      &lt;item&gt;Luanti (ex-Minetest)&lt;/item&gt;
      &lt;item&gt;IOQuake3 (compiled from source)&lt;/item&gt;
      &lt;item&gt;OAD (from Debian repos)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GZDoom works exceptionally well with the OpenGLES renderer at Full HD. It’s fast, responsive. Stable at 60 FPS on Full HD. And Doom is still as fun as it was in the days, or more so if you run Brutal Doom.&lt;/p&gt;
    &lt;p&gt;While I don’t have a sample video here, Quake 3 Arena with the IOQuake3 engine works perfectly, and runs at 60 fps on Full HD without a sweat.&lt;/p&gt;
    &lt;p&gt;Luanti (ex Minetest) works extremely well - I turned most of the details to the max at 1080p and it kept at a solid 60 FPS. Sure, that’s no AAA game, but it’s a good alternative to Minecraft.&lt;/p&gt;
    &lt;p&gt;0Ad works extremely well, even in full screen at my Ultra Wide screen resolution. I took a video at FUll HD, and while OBS does slow the framerate a little, you can still see it’s very smooth.&lt;/p&gt;
    &lt;p&gt;0ad has come a long way, I really need to revisit a recent version of the game to see how much it has changed!&lt;/p&gt;
    &lt;head rend="h3"&gt;Server use&lt;/head&gt;
    &lt;p&gt;This is a very capable board that would make a very powerful server. The Debian image comes with Docker, and as we have seen before during the OrangePi 5 Ultra review, the landscape of ready-to-use Docker Hub images is huge and can get you started with numerous server-side applications in no time. Since we have at least 16Gb of memory, very fast I/O (PCIe4!), and a very fast CPU, this SBC will be able to wonders with a wide variety of applications. The only problem is the power draw. It looks like we are stuck with 15W as an idle baseline, and that seems a bit too much for some light server use.&lt;/p&gt;
    &lt;head rend="h3"&gt;AI applications&lt;/head&gt;
    &lt;p&gt;This board comes with a very large repository (60 GB!) that you can install and sync automatically, with tons of applications and demos you can try out. There is a large part of their documentation dedicated to that part in their manual. And for what I could try, it seems to work as long as you follow their instructions.&lt;/p&gt;
    &lt;p&gt;In my case I like to work with LLMs for a range of applications, so I was interested to see how fast this board could run some small-ish models. One of the major limitations is that we don’t have working NPU support for llama.cpp (oh no!). I was thinking, “no worries, we have a vulkan driver so let’s use Vulkan instead”. I like to be optimistic sometimes. Turns out that the Vulkan driver is below the minimum Vulkan drivers specs required by llama.cpp recently, and I would need to go back to a end 2024 build to be able to compile for Vulkan support. Going back one year on llama.cpp… not an option, sorry (too many models would not be supported going back so far in time). So, we are left with using the raw power of the CPU. Which means it won’t be quiet - expect some good old noise fan in such use cases.&lt;/p&gt;
    &lt;p&gt;Anyway, I went for Qwen3 1.7b - downloaded the safetensors weights, then converted them to gguf, followed by a quantization step to make them IQ4_S. Running the model at this kind of quantization gives us about 14 tokens per second when doing inference. Definitely usable, very usable even, while this is a small model.&lt;/p&gt;
    &lt;p&gt;Ideally, you’d want to have a board that can run a 7b model with proper hardware acceleration and no fan usage. Not sure if having actual NPU support in the future would help for that or not. In other words, we are not there yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Alternatives&lt;/head&gt;
    &lt;p&gt;As usual there is not a single vendor who can provide you with a CIX experience. This time the main competitor is Radxa, and they have two boards called Orion 6 that feature the same chip - one that is much bigger in footprint (mini-ITX size), and another one that is more similar to the OrangePi 6 Plus size (the Orion 6N). While I don’t have access to the Radxa ones, I can’t comment on how good their support is, but they are very likely to suffer from the same limitations, software wise.&lt;/p&gt;
    &lt;p&gt;As expected, they also only provide a Debian Bookworm image under the Radxa OS nickname so this is no different from what you can get on the OrangePi 6 Plus. Price-wise, they are both available at around similar price points, so you could make you decision based on the best deal you can get.&lt;/p&gt;
    &lt;head rend="h2"&gt;Verdict&lt;/head&gt;
    &lt;p&gt;This is a very impressive SBC, with an exceptional value proposition. Its performance profile puts if far away from the toy category from what we have seen before in the SBC category, and puts it right into the desktop performance realm. It’s still relatively small so you could easily attach it behind a monitor and power a personal computer this way. As a Linux user, things are so fast that you’d be very surprised this is an ARM board running under the hood. For server use, you’d probably want to avoid this one, because of the fairly heavily baseline power consumption.&lt;/p&gt;
    &lt;p&gt;As usual, the pitfalls are always going to be the same. This time around you get an older Debian image (bookworm), with an older kernel (6.6), and some proprietary drivers that you have to live with. Ultimately if you want to upgrade the software running on your SBC that will mean breaking things, and there’s often a fairly long time (if ever) for some hardware components to be supported in newer distros. It’s not necessarily a deal breaker these days. This board is fast enough to compile software fairly quickly if needed. You have Flatpak as well providing some good coverage for a lot of application (even if performance may be sub-par). There are ongoing efforts to mainline the GPU found in the chip, so it could be that a year from now we are in a better place when it comes to proper hardware support beyond kernel 6.6.&lt;/p&gt;
    &lt;p&gt;In any case, this is a surprising new entry in terms of performance / price point. This puts the bar very high for future SBCs ARM64 SBCs. On a personal note, I’d like to see some serious hardware dedicated to running AI models (instead of large desktops chaining multiple GPUs), and maybe highly customized ARM64 SBCs are going to become one option at some point.&lt;/p&gt;
    &lt;p&gt;If you are interested in getting one, there are many resellers, but one of the most direct ones are on Aliexpress:&lt;/p&gt;
    &lt;p&gt;Since the difference of price between 16 GB and 32GB is so small currently, it would make total sense to go for the 32GB.&lt;/p&gt;
    &lt;p&gt;Note: we were provided with a review unit from OrangePi (more specifically the OrangePi 6 Plus 16GB version).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://boilingsteam.com/orange-pi-6-plus-review/"/><published>2025-12-27T12:51:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46401539</id><title>Apple releases open-source model that instantly turns 2D photos into 3D views</title><updated>2025-12-27T16:12:06.428817+00:00</updated><content>&lt;doc fingerprint="3783a21824b9c2d6"&gt;
  &lt;main&gt;
    &lt;p&gt;This software project accompanies the research paper: Sharp Monocular View Synthesis in Less Than a Second by Lars Mescheder, Wei Dong, Shiwei Li, Xuyang Bai, Marcel Santos, Peiyun Hu, Bruno Lecouat, Mingmin Zhen, Amaël Delaunoy, Tian Fang, Yanghai Tsin, Stephan Richter and Vladlen Koltun.&lt;/p&gt;
    &lt;p&gt;We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25–34% and DISTS by 21–43% versus the best prior model, while lowering the synthesis time by three orders of magnitude.&lt;/p&gt;
    &lt;p&gt;We recommend to first create a python environment:&lt;/p&gt;
    &lt;code&gt;conda create -n sharp python=3.13
&lt;/code&gt;
    &lt;p&gt;Afterwards, you can install the project using&lt;/p&gt;
    &lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;
    &lt;p&gt;To test the installation, run&lt;/p&gt;
    &lt;code&gt;sharp --help
&lt;/code&gt;
    &lt;p&gt;To run prediction:&lt;/p&gt;
    &lt;code&gt;sharp predict -i /path/to/input/images -o /path/to/output/gaussians
&lt;/code&gt;
    &lt;p&gt;The model checkpoint will be downloaded automatically on first run and cached locally at &lt;code&gt;~/.cache/torch/hub/checkpoints/&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Alternatively, you can download the model directly:&lt;/p&gt;
    &lt;code&gt;wget https://ml-site.cdn-apple.com/models/sharp/sharp_2572gikvuh.pt
&lt;/code&gt;
    &lt;p&gt;To use a manually downloaded checkpoint, specify it with the &lt;code&gt;-c&lt;/code&gt; flag:&lt;/p&gt;
    &lt;code&gt;sharp predict -i /path/to/input/images -o /path/to/output/gaussians -c sharp_2572gikvuh.pt
&lt;/code&gt;
    &lt;p&gt;The results will be 3D gaussian splats (3DGS) in the output folder. The 3DGS &lt;code&gt;.ply&lt;/code&gt; files are compatible to various public 3DGS renderers. We follow the OpenCV coordinate convention (x right, y down, z forward). The 3DGS scene center is roughly at (0, 0, +z). When dealing with 3rdparty renderers, please scale and rotate to re-center the scene accordingly.&lt;/p&gt;
    &lt;p&gt;Additionally you can render videos with a camera trajectory. While the gaussians prediction works for all CPU, CUDA, and MPS, rendering videos via the &lt;code&gt;--render&lt;/code&gt; option currently requires a CUDA GPU. The gsplat renderer takes a while to initialize at the first launch.&lt;/p&gt;
    &lt;code&gt;sharp predict -i /path/to/input/images -o /path/to/output/gaussians --render

# Or from the intermediate gaussians:
sharp render -i /path/to/output/gaussians -o /path/to/output/renderings
&lt;/code&gt;
    &lt;p&gt;Please refer to the paper for both quantitative and qualitative evaluations. Additionally, please check out this qualitative examples page containing several video comparisons against related work.&lt;/p&gt;
    &lt;p&gt;If you find our work useful, please cite the following paper:&lt;/p&gt;
    &lt;code&gt;@inproceedings{Sharp2025:arxiv,
  title      = {Sharp Monocular View Synthesis in Less Than a Second},
  author     = {Lars Mescheder and Wei Dong and Shiwei Li and Xuyang Bai and Marcel Santos and Peiyun Hu and Bruno Lecouat and Mingmin Zhen and Ama\"{e}l Delaunoy and Tian Fang and Yanghai Tsin and Stephan R. Richter and Vladlen Koltun},
  journal    = {arXiv preprint arXiv:2512.10685},
  year       = {2025},
  url        = {https://arxiv.org/abs/2512.10685},
}&lt;/code&gt;
    &lt;p&gt;Our codebase is built using multiple opensource contributions, please see ACKNOWLEDGEMENTS for more details.&lt;/p&gt;
    &lt;p&gt;Please check out the repository LICENSE before using the provided code and LICENSE_MODEL for the released models.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/apple/ml-sharp"/><published>2025-12-27T12:58:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46401612</id><title>Floor796</title><updated>2025-12-27T16:12:05.536884+00:00</updated><link href="https://floor796.com/"/><published>2025-12-27T13:13:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46401832</id><title>Show HN: An immutable ostree-based Arch Linux image</title><updated>2025-12-27T16:12:04.990551+00:00</updated><content>&lt;doc fingerprint="2bdc8915261ab52c"&gt;
  &lt;main&gt;
    &lt;p&gt;An ostree-based full fledged Arch Linux distribution, with no package manager. Basically Fedora Silverblue but with CachyOS' Zen4 packages, because we all want that 1% performance improvement.&lt;/p&gt;
    &lt;p&gt;It's a largely vibe coded attempt at essentially gerrymandering a standard GNOME-based Arch Linux setup into Fedora's immutable system. And yes, it works perfectly in its current state.&lt;/p&gt;
    &lt;p&gt;This README isn't written by an AI though so you can at least take it at face value.&lt;/p&gt;
    &lt;p&gt;Warning: This project is work-in-progress and even though it is very usable and stable it might miss some random stuff here and there.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install Silverblue, or Kinoite, or Bazzite, or whatever&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo bootc switch ghcr.io/myyc/vyy-zen4:latest
&lt;/code&gt;
    &lt;p&gt;Reboot. &lt;code&gt;bootc upgrade&lt;/code&gt; to update it. The kernel is signed so you can
enable secure boot after trusting the key.&lt;/p&gt;
    &lt;code&gt;sudo mokutil --import /usr/share/vyy/secureboot.cer
&lt;/code&gt;
    &lt;p&gt;There are no &lt;code&gt;zen3&lt;/code&gt; builds uploaded but you can build one
yourself.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install Silverblue, or Kinoite, or Bazzite, or whatever&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;scripts/dev.sh zen3&lt;/code&gt;as root (yes, sorry).&lt;/item&gt;
      &lt;item&gt;From there, run &lt;code&gt;build-vyy-root.sh zen3&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Commit to your ostree repo, deploy and reboot.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo ostree commit --repo=/ostree/repo \
    --branch=vyy --owner-uid=0 --owner-gid=0 \
    --subject="vyy YYYYMMDD" \
    --skip-list=/path/to/vyy/config/ostree-skip-list \
    /path/to/vyy/work
sudo ostree admin deploy vyy
&lt;/code&gt;
    &lt;p&gt;You will lose out-of-the-box secure boot this way but you can create your own keys. As long as the paths are right the scripts will do it for you.&lt;/p&gt;
    &lt;p&gt;You could also use &lt;code&gt;scripts/daily-build.sh&lt;/code&gt; and the systemd
units if you have a server to run this on; edit the placeholders
first though.&lt;/p&gt;
    &lt;p&gt;All the scripts default to &lt;code&gt;zen4&lt;/code&gt; but you can launch them with
&lt;code&gt;zen3&lt;/code&gt; or &lt;code&gt;generic&lt;/code&gt; as arguments. I haven't tested these two, but
they should work.&lt;/p&gt;
    &lt;p&gt;The "install Silverblue" part is pretty much what makes this distro solid avoiding most possible sources of human error. Partitioning, encryption (if you want) and the bootloader are Fedora's defaults, which should be more than enough for most people – Silverblue by default configures &lt;code&gt;btrfs&lt;/code&gt; and has native encryption, so hooray.&lt;/p&gt;
    &lt;p&gt;This is also why you can use this sort of thing on relatively critical devices – I literally developed all of this on the only device I had access too, for work included. It doesn't boot? Roll back and forget.&lt;/p&gt;
    &lt;p&gt;No package manager. The system is immutable. Want to add stuff? Fork and edit &lt;code&gt;config/packages.conf&lt;/code&gt;. Or use &lt;code&gt;distrobox&lt;/code&gt;. It's included.&lt;/p&gt;
    &lt;p&gt;The core thing is just a basic pacstrap setup with a bunch of packages added. You can see all of them in &lt;code&gt;confg/packages.txt&lt;/code&gt;.
The main build script doesn't do much else, besides perhaps
setting the locale.&lt;/p&gt;
    &lt;p&gt;Most of the hammering is done by &lt;code&gt;restructure.sh&lt;/code&gt; which is
invoked by &lt;code&gt;build-vyy-root.sh&lt;/code&gt; so you might as well check that
too since it runs within a rootful container.&lt;/p&gt;
    &lt;p&gt;Globally, it does a few things, e.g.:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Building the initramfs with the ostree module (and others)&lt;/item&gt;
      &lt;item&gt;Moving everything system inside &lt;code&gt;/usr&lt;/code&gt;(including most of /etc)&lt;/item&gt;
      &lt;item&gt;Ensuring the root is compatible with ostree&lt;/item&gt;
      &lt;item&gt;Allowing &lt;code&gt;sudo&lt;/code&gt;for wheel users (otherwise you're locked out)&lt;/item&gt;
      &lt;item&gt;Some sane defaults for &lt;code&gt;pam&lt;/code&gt;(including the fingerprint)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So like ... it doesn't actually depend on them yet. The only core one is &lt;code&gt;bootc&lt;/code&gt;. If you want to build locally you don't need it,
you only need vanilla ostree. You don't even need it if you want
to update from your own ostree repo.&lt;/p&gt;
    &lt;p&gt;The other packages are more of a personal convenience. They're the Mullvad CLI, &lt;code&gt;ibus-m17n&lt;/code&gt; which is required for certain
input methods, and &lt;code&gt;raw-thumbnailer&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Fuck off&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/myyc/vyy"/><published>2025-12-27T13:47:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46401938</id><title>NMH BASIC</title><updated>2025-12-27T16:12:03.857314+00:00</updated><content>&lt;doc fingerprint="b738281b4ea5b2d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Download: nmhbas23c.zip (version 1.2, 74KB) | nmhbas25c.zip (version 2.1, 90KB) | man page&lt;/p&gt;
    &lt;p&gt;This is a small BASIC interpreter that I wrote in the early 1990s. For some reason I think it is one of the coolest programs I have ever written. Maybe because it is just a bit under 5K bytes large and still does something useful. Maybe it is just nostalgia.&lt;/p&gt;
    &lt;p&gt;One of the more interesting programs I have written in NMH BASIC is a variant of the well-known mine sweeper game that runs in text mode. Not just text mode, actually, but (tele)typewriter mode, as it reprints the playing field after every move.&lt;/p&gt;
    &lt;p&gt;The screenshots use Viacheslav Slavinsky's excellent GlassTTY font, a TrueType font that perfectly resembles the one used in the DEC VT-220 terminal. The same font, at bigger magnification, is used in the NMH BASIC logo.&lt;/p&gt;
    &lt;p&gt;What is maybe interesting about the mine sweeper clone is that it uses a stackless floodfill algorithm that stores its state in the playing field itself and needs no dynamic memory at all. I have recently described it in the paper A Stackless Floodfill Automaton (PDF, 34KB). A demo showing an animation of the algorithm is included in the NMH BASIC package.&lt;/p&gt;
    &lt;p&gt;There are other programs in the package, most of them rather simple, like an implementation of the Hangman game, the (rather pointless) Nim game, a banner printer, a random number generator, etc. NMH BASIC does not have a RNG, so a 15-bit linear feedback shift register is implemented in BASIC to generate pseudo-random numbers.&lt;/p&gt;
    &lt;p&gt;The first program I have ever written in NMH BASIC was the inevitable prime number sieve. I have no idea how often I have loaded and run it in the past decades - until the Floodfill demo became my new favorite. Here is the code of my first NMH BASIC program (the backslash is the division remainder operator):&lt;/p&gt;
    &lt;quote&gt;10 REM 'PRINT PRIME NUMBERS' 20 REM 'M = NUMBER OF PRIMES TO PRINT' 100 LET M = 1000 105 DIM Z(M) 110 LET Z(0) = 2 : LET T = 1 : LET P = 1 115 PRINT 2, 120 IF T &amp;gt;= M GOTO 200 130 LET P = P+2 : LET O = 1 140 FOR I = 0 TO T-1 150 IF P\Z(I) = 0 LET O = 0 : LET I = T 160 NEXT 170 IF O = 0 GOTO 120 180 LET Z(T) = P : LET T = T+1 185 PRINT P, 190 GOTO 120 200 END&lt;/quote&gt;
    &lt;p&gt;I wrote the first version of NMH BASIC in 1994, recycling some parts that I had written in the years 1991..1993. The first version that I wrote in 1994 was a prototype in BASYL-II which I then translated, function by function, to 8086 assembly language. The resulting executable had a size of about 4700bytes and because the token representation that the interpreter uses internally is quite efficient, you could do interesting things with NMH BASIC in as little as 12Kbytes of memory. I had named the interpreter 12K-BASIC initially, but soon learned that others had had that idea before me.&lt;/p&gt;
    &lt;p&gt;Of course in 1994 memory was already measured in megabytes, so you might say that writing a tiny BASIC interpreter was kind of pointless at that time. It depends I would say; it is better than getting drunk in a bar, and now, almost 30 years later, I still enjoy playing with this little program. So much, in fact, that I decided to translate the original code to T3X/0, so that I can play with it on Unix without having to use an emulator.&lt;/p&gt;
    &lt;p&gt;All the above versions are included in the package: the original BASYL-II version, the assembly language version, and the new T3X version. You can recompile the T3X version with T3X/0 and the 8086 assembly language version with TASM or MASM. You need to create a COM file or it will not run. A precompiled COM file and Tcode machine executable (as well as a Tcode machine for Unix) are also included in the package.&lt;/p&gt;
    &lt;p&gt;There is also a simplified version of the interpreter that runs under CP/M. A COM file for CP/M (BASICS.COM) is also included in the archive. The CP/M version currently needs 32K bytes of TPA to run.&lt;/p&gt;
    &lt;p&gt;The NMH BASIC language contains some interesting (IMHO) hacks to make its implementation simpler.&lt;/p&gt;
    &lt;p&gt;All variables have either single-character names or names consisting of a character and a digit, like A0, B2, Z9, etc. The expressions A and A0 and A(0) all refer to the same variable. If you do not use A0..A9, you can use A as a 10-slot array A(0) .. A(9). Or you can use A5 in the place of A(5) if you are refering to a fixed slot.&lt;/p&gt;
    &lt;p&gt;It is getting even weirder. A(10) is the same as B or B0 or B(0). A(22) is equal to C2 or C(2) and, finally, A(259) would be equal to Z(9). So, for instance, if you do not use any Z's, you can use Y as a 20-slot array. In this case the command &lt;code&gt;DIM Y(20)&lt;/code&gt; is really
a null-operation. It merely serves as a reminder that Y is a 20-slot
array (and Z should not be used).
&lt;/p&gt;
    &lt;p&gt;You could also use Y as a 50-alot array by dimensioning it with &lt;code&gt;DIM Y(50)&lt;/code&gt;. In this case the elements of Z will still be used, but
40 additional slots will be allocated to integer variable storage,
so Z becomes a 40-slot array and Y a 50-slot array. It probably
goes without saying that most programs either use single-character
variables as 10-slot arrays or dimension Z, if a larger array is
needed.
&lt;/p&gt;
    &lt;p&gt;This also means that &lt;code&gt;DIM Y(50)&lt;/code&gt; and &lt;code&gt;DIM Z(50)&lt;/code&gt; in the same program
would just allocate 50 integer slots to Z and the last 40 slots of Y would
overlap with the slots of Z. Having multiple large arrays in the
same NMH BASIC program requires some hacking, like using Z(0)..Z(99)
for one array and Z(100)..Z(199) for the other.
&lt;/p&gt;
    &lt;p&gt;Note the definition of "large" above. NMH BASIC uses 12Kbytes of memory in total: for integer variables, string variables, program memory, stacks, and the machine code of the interpreter itself! You could probably write a version of this interpreter that would run on a CP/M machine with as little as 16K bytes of transient program area (but I have never done so).&lt;/p&gt;
    &lt;p&gt;The interpreter performs I/O on "units", where each unit is assigned a file or device when the interpreter is started. NMH BASIC programs cannot open or close any files. They can only redirect input and output to the assigned units. I/O is sequential, i.e. each unit is like a tape drive. The following program prints the data stored on unit #5:&lt;/p&gt;
    &lt;quote&gt;100 LET X = IOCTL(5, 100) : INPUT #5 110 INPUT A$ : IF ASC(A$) = 255 INPUT #0 : END 120 PRINT A$ : GOTO 110&lt;/quote&gt;
    &lt;p&gt;The statement &lt;code&gt;INPUT #5&lt;/code&gt; redirects input to unit #5, so from that point
on all INPUT statements will read from that unit. (Analogously,
&lt;code&gt;PRINT #5&lt;/code&gt; would redirect output to unit #5.) When a string read from
a unit contains the value 255 in its first slot, there is no more
input available from the current input unit. &lt;code&gt;INPUT #0&lt;/code&gt; connects input
back to the keyboard. Note that &lt;code&gt;PRINT #1&lt;/code&gt; would connect output back
to the screen.
&lt;/p&gt;
    &lt;p&gt;There is an IOCTL function that can perform several "services" on a unit, like rewinding it, appending to it (moving the read/write pointer to the end of the unit), or truncating it (or writing an EOF marker on a tape). The IOCTL call in the above example rewinds the unit.&lt;/p&gt;
    &lt;p&gt;The maximum length of a line or string is 64 bytes. Reading anything longer, either via INPUT or by entering it at the interpreter prompt, will result in an error. The CR,LF characters that separate lines are not counted.&lt;/p&gt;
    &lt;p&gt;I have forgotten how other BASIC dialects handle this, but I suspect that NMH BASIC is the odd one out here: in an IF statement the entire rest of the line is executed conditionally. For example&lt;/p&gt;
    &lt;quote&gt;IF 1 = 1 PRINT 'FOO' : PRINT 'BAR'&lt;/quote&gt;
    &lt;p&gt;would print both FOO and BAR. There is no THEN or ELSE keyword. The first keyword after the condition of IF starts the conditional part of the IF statement. When the condition in IF is false, the interpreter advances to the next line. An alternative branch is implemented with jump around jumps using GOTO:&lt;/p&gt;
    &lt;quote&gt;100 IF condition GOTO 130 110 alternative statements 120 GOTO 140 130 consequent statements 140 REM&lt;/quote&gt;
    &lt;p&gt;Or, if the condition and statements are short:&lt;/p&gt;
    &lt;quote&gt;100 IF condition statements 110 IF # condition statements&lt;/quote&gt;
    &lt;p&gt;The # operator implements the logical NOT. It had high precedence in NMH BASIC up to version 1.2, but has very low precedence in NMH BASIC II. Interestingly, this change did not affect any programs in the archive. There is a logical AND, but not a logical OR in IF. If there are multiple conditions separated by commas then the conditional statements will only execute, if all conditions are true. For example, the statement&lt;/p&gt;
    &lt;quote&gt;IF 0 &amp;lt; C, C &amp;lt; 11 STOP&lt;/quote&gt;
    &lt;p&gt;will stop program execution, if C is in the range 1..10. To implement a logical OR, multiple IF statemements with the same conditional part (or jumps around jumps) have to be used.&lt;/p&gt;
    &lt;p&gt;NMH BASIC 1.x listed programs with blanks between all adjacent tokens. NMH BASIC II uses a more compact representation. Either way is merely a characteristic of the LIST routine, though. You can enter a program as&lt;/p&gt;
    &lt;quote&gt;FOR I=1TO10:PRINT A(I):NEXT&lt;/quote&gt;
    &lt;p&gt;but the LIST command will print it as&lt;/p&gt;
    &lt;quote&gt;FOR I = 1 TO 10 : PRINT A ( I ) : NEXT&lt;/quote&gt;
    &lt;p&gt;in NMH BASIC and as&lt;/p&gt;
    &lt;quote&gt;FOR I = 1 TO 10 : PRINT A(I) : NEXT&lt;/quote&gt;
    &lt;p&gt;in NMH BASIC II.&lt;/p&gt;
    &lt;p&gt;This has the weird side effect that sometimes you can SAVE a program but cannot LOAD it later, because some lines will be shorter than 64 characters when you enter them, but LIST (and hence SAVE) will blow them up to a bigger size.&lt;/p&gt;
    &lt;p&gt;This is mostly a problem in NMH BASIC 1.x, which inserts blanks between all tokens. For example:&lt;/p&gt;
    &lt;quote&gt;100 IF ASC(MID$(A$, I, 1)) = ASC('X') LET X = X+1 : GOTO 120 ----+----1----+----2----+----3----+----4----+----5----+----6---| 100 IF ASC( MID$( A$ , I , 1 ) ) = ASC( 'X' ) LET X = X + 1 : GOTO 120&lt;/quote&gt;
    &lt;p&gt;There is no workaround. When a program cannot be loaded, the only remedy is to edit it with a text editor and fix it, either by removing unnecessary blanks or, even better, by splitting the offending line. E.g.:&lt;/p&gt;
    &lt;quote&gt;100 LET C = ASC( MID$( A$ , I , 1 ) ) 105 IF C = ASC( 'X' ) LET X = X + 1 : GOTO 120&lt;/quote&gt;
    &lt;p&gt;Finally, it is a good idea to keep NMH BASIC programs in DOS text format with CR/LF line separators, even on Unix systems, because otherwise the DOS version of the interpreter will refuse to load them.&lt;/p&gt;
    &lt;p&gt;In December 2024 I changed a few things and pubished a new version of NMH BASIC, which I called, for lack of imagination, NMH BASIC II. The new version changes the precedence of the # (logical NOT) operator from highest to lowest (this was a mistake in the original version!) and uses a more compact and more comprehensible LIST format, which is also used for saving programs. Because some code was simplified in the interpreter at the same time, the new version is one byte smaller than the original version.&lt;/p&gt;
    &lt;p&gt;Download: nmhbas3_30.zip (version 3.0, 105KB) | man page&lt;/p&gt;
    &lt;p&gt;Version 3.x of NMH BASIC is an in-progress version that differs from the previous versions in a few points that are described in detail in the manual. Most prominently, the CMPS ("compare strings") function has been replaced with string comparison operators, so, for example,&lt;/p&gt;
    &lt;quote&gt;IF CMPS(A$, 'FOO') = 0 PRINT 'YEP'&lt;/quote&gt;
    &lt;p&gt;would now be written as&lt;/p&gt;
    &lt;quote&gt;IF A$ = 'FOO' PRINT 'YEP'&lt;/quote&gt;
    &lt;p&gt;Then, NMH BASIC III supports baudot-encoded units. This means that any unit connected to the interpreter can be written to and read from using five-channel baudot-encoding (CCITT-2, US-TTY). So the interpreter can, in theory, save and load programs to/from five-hole paper tape.&lt;/p&gt;
    &lt;p&gt;The T3X/0 version of the interpreter is currently fully working. There also is a more efficient Z80 version written in assembly language, which is work in slow progress. It currently runs all the example programs, but lacks baudot-encoded units and may still have a few bugs. An 8086 version written in assembly language may appear later.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://t3x.org/nmhbasic/index.html"/><published>2025-12-27T14:05:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46402611</id><title>A Century of Noether's Theorem</title><updated>2025-12-27T16:12:03.558905+00:00</updated><content>&lt;doc fingerprint="a49c240a54f5908a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Physics &amp;gt; History and Philosophy of Physics&lt;/head&gt;&lt;p&gt; [Submitted on 6 Feb 2019 (v1), last revised 9 Jul 2019 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Colloquium: A Century of Noether's Theorem&lt;/head&gt;View PDF&lt;quote&gt;Abstract:In the summer of 1918, Emmy Noether published the theorem that now bears her name, establishing a profound two-way connection between symmetries and conservation laws. The influence of this insight is pervasive in physics; it underlies all of our theories of the fundamental interactions and gives meaning to conservation laws that elevates them beyond useful empirical rules. Noether's papers, lectures, and personal interactions with students and colleagues drove the development of abstract algebra, establishing her in the pantheon of twentieth-century mathematicians. This essay traces her path from Erlangen through Göttingen to a brief but happy exile at Bryn Mawr College in Pennsylvania, illustrating the importance of "Noether's Theorem" for the way we think today. The text draws on a colloquium presented at Fermilab on 15 August 2018.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Chris Quigg [view email]&lt;p&gt;[v1] Wed, 6 Feb 2019 00:51:17 UTC (183 KB)&lt;/p&gt;&lt;p&gt;[v2] Tue, 9 Jul 2019 18:15:15 UTC (183 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;physics.hist-ph&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/1902.01989"/><published>2025-12-27T15:42:02+00:00</published></entry></feed>