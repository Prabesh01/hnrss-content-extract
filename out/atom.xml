<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-05T21:33:51.020001+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45822513</id><title>Optimism associated with exceptional longevity (2019)</title><updated>2025-11-05T21:34:52.611100+00:00</updated><content/><link href="https://www.pnas.org/doi/10.1073/pnas.1900712116"/><published>2025-11-05T13:16:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45822539</id><title>NY smartphone ban has made lunch loud again</title><updated>2025-11-05T21:34:52.454679+00:00</updated><content>&lt;doc fingerprint="366ad004a8c395ce"&gt;
  &lt;main&gt;
    &lt;p&gt;These days, lunchtime at Benjamin N. Cardozo High School in Queens is a boisterous affair, a far cry from before the smartphone ban went into effect, when most students spent their spare time scrolling and teachers said you could hear a pin drop.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis year's gotten way louder,‚Äù said Jimena Garcia, 15. ‚ÄúSometimes I would take naps in the lunchroom, but now I can't because of the noise. But it's fun.‚Äù&lt;/p&gt;
    &lt;p&gt;On a recent fall afternoon, Garcia and her friends crowded around a lunch table in the large cafeteria playing Jenga, occasionally shrieking and gasping as the tower began to lean and fall.&lt;/p&gt;
    &lt;p&gt;The faculty donated board games to help ease kids into the phone-free era. Student volunteers oversaw a table stacked with games: checkers, chess, Yahtzee, Scrabble, Clue, Life and Trivial Pursuit. For many of the kids, it was their first time playing the games, and they said they were enjoying it.&lt;/p&gt;
    &lt;p&gt;‚Äú I do like how this phone ban is allowing students to just connect with each other, make new friendships,‚Äù said 17-year-old Alyssa Ko, the school president. ‚ÄúBecause some people use their phone to just hide away.‚Äù&lt;/p&gt;
    &lt;p&gt;The ban prohibits all internet-enabled devices throughout the school day, although there are exceptions for some students with disabilities, kids learning English who need translation apps, and in cases where a teacher says a device can be used for a lesson.&lt;/p&gt;
    &lt;p&gt;Schools were given flexibility to choose their own storage plans, and Cardozo, which rolled out metal detectors this fall after a student was found with a gun, requires its 3,100 students to keep their phones in internet-blocking magnetic pouches. Other schools have installed storage lockers, or have kids keep their phones zipped up in backpacks.&lt;/p&gt;
    &lt;p&gt;As students adjust to lo-fi life, teachers seem pleased with the results. According to an October survey from the state teachers union, New York State United Teachers, 89% of school staff members said the new policies improved their schools' environments, and 76% said kids are more engaged in lessons.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhen students put down their phones, they pick up books ‚Äî and build friendships,‚Äù said NYSUT president Melinda Person.&lt;/p&gt;
    &lt;p&gt;The initial feedback reflects national trends. New York is one of 31 states, plus Washington, D.C. that have banned smartphones in schools, according to an EdWeek tracker. In a national survey from the University of Pennsylvania, teachers said banning phones helps kids focus.&lt;/p&gt;
    &lt;p&gt;‚ÄúTeachers seem really happy with the changes that they're seeing in the classroom with the electronic device ban,‚Äù Cardozo principal Meagan Colby said. ‚ÄúThey're telling us that there's a lot more student interaction, a lot more discussion among students, a lot better focus. Overall productivity in the classroom and engagement is higher.‚Äù&lt;/p&gt;
    &lt;p&gt;Senior Raya Osagie, 16, said she has to ‚Äúthink more in class‚Äù because she used to Google answers or use artificial intelligence. ‚ÄúNow when we get computers, I actually have to [do] deep research instead of going straight to AI,‚Äù she said.&lt;/p&gt;
    &lt;p&gt;Students said they‚Äôve also seen their classmates reading physical books more.&lt;/p&gt;
    &lt;p&gt;In the cafeteria, Ryan Tripathi, 16, was paging through ‚ÄúLord of the Flies,‚Äù which he said is slow-going. ‚ÄúI'm just not used to reading,‚Äù he said. ‚ÄúI‚Äôm usually on my phone.‚Äù&lt;/p&gt;
    &lt;p&gt;Tripathi said it‚Äôs good that people are reading more and classroom discussions have become more lively. But he said he‚Äôs ‚Äúnot the biggest fan‚Äù of the smartphone ban. ‚ÄùSometimes you just want to go on your phone and you don't have the ability to do that anymore,‚Äù he said.&lt;/p&gt;
    &lt;p&gt;Enakshi Barua, 14, said she‚Äôs also opposed to the ban, on principle.&lt;/p&gt;
    &lt;p&gt;‚Äú Students are distracted by the phones, but I also don‚Äôt believe they should take away our privileges," Barua said. "I feel like the trust isn‚Äôt there between the students and teachers. So I feel like that should be built instead of banning the phones.‚Äù&lt;/p&gt;
    &lt;p&gt;At Cardozo, a few kids break the rules, teachers said. Some either put ‚Äúburner‚Äù phones in their pouches or bang them open. Shanna Burrows, who oversees restorative justice at the school, said staff members are collecting around 30 contraband phones a day. There‚Äôs a strike system with escalating punishments that include keeping phones for days, weeks or months, and meetings with parents. Under the state law, schools are not allowed to suspend students solely for smartphone-ban infractions.&lt;/p&gt;
    &lt;p&gt;Students said they have found other ways to push boundaries, like passing old-fashioned notes. ‚Äú Especially when you're trying to talk but not have the teacher notice ‚Ä¶ it would just be [that] we‚Äôd send a text message or write on our notes app,‚Äù Ko said. ‚ÄúPassing notes is more common now.‚Äù&lt;/p&gt;
    &lt;p&gt;Ko said other analog activities have also made a comeback, including cards, hangman, tic-tac-toe and Polaroid cameras. ‚ÄúThere are just a lot of memories that we make throughout high school that we want to capture,‚Äù she said. ‚ÄúI actually have a lot of Polaroids on my wall.‚Äù&lt;/p&gt;
    &lt;p&gt;Tiana Millen, assistant principal of climate and culture, said she‚Äôd also like to see another analog technology make a comeback: the clock.&lt;/p&gt;
    &lt;p&gt;‚ÄúThey don't know how to read the clocks," she said. "So I make jokes with them and say, ‚ÄòWe're going to have classes just on how to read the clocks.‚Äô‚Äù&lt;/p&gt;
    &lt;p&gt;If they did, she said, they‚Äôd see they‚Äôre getting to class on time more than they used to; hallway traffic is moving better now that kids aren‚Äôt so focused on their phones.&lt;/p&gt;
    &lt;p&gt;This story has been updated.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gothamist.com/news/ny-smartphone-ban-has-made-lunch-loud-again"/><published>2025-11-05T13:20:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45822559</id><title>Radiant Computer</title><updated>2025-11-05T21:34:51.629697+00:00</updated><content>&lt;doc fingerprint="899b796b21f8ecf7"&gt;
  &lt;main&gt;&lt;p&gt;Radiant Computer&lt;/p&gt;&lt;p&gt;Radiant's purpose is to explore what personal computing could be when designed from first principles.&lt;/p&gt;We believe the current trajectory of personal computing is leading us to a less free world, and that only a new computing movement rooted in human dignity, creation and autonomy can change its course.&lt;p&gt;Radiant is a computer reimagined from the ground up, a clean-slate design free from the historical baggage that plagues modern systems, and free from Big Tech's influence.&lt;/p&gt;&lt;p&gt;It's a computer designed to help you learn, create, play, and explore. It's a space to focus, free from distractions. A return to the simple joy of computing: just you and your ideas.&lt;/p&gt;&lt;p&gt;Computers today are designed around engagement and surveillance business models rather than user needs. App stores are filled with adware. Operating systems prioritize data collection over user agency. Social media algorithms optimize for addiction. Big Tech fundamentally reshaped computing from a tool for human empowerment into an attention extraction machine.&lt;/p&gt;&lt;p&gt;Radiant proposes an alternative vision for computing. It doesn't ship with a web browser; it has its own network reminiscent of the early Internet: no social media, no scripts, no trackers. It's a from-scratch system that doesn't retrace the footsteps of the contemporary OS. It's a new paradigm for personal computing that uses modern advances mindfully and deliberately. It's fully open, from hardware to software. It's an offline-first space, designed for focus and creation.&lt;/p&gt;&lt;p&gt;Code is computing's native medium; the material you shape to build your own tools, stories, and spaces. Radiant is designed to make that accessible to everyone. It's a tool for personal computing where every application and every surface, exists as code you can read, edit, and extend. It's a system you can truly own. One that is designed to bring the joy of computing to everyone.&lt;/p&gt;&lt;p&gt;Writing software doesn't have to be daunting, but the platforms and tools we're stuck with make it so. Radiant aims to change that and truly empower users to create. Furthermore, advances in generative A.I. will make coding accessible to a much broader audience. One of our goals is to explore how an A.I.-native computer system can enhance the creative process, all while keeping data private.&lt;/p&gt;&lt;p&gt;Radiant belongs to a more humane future: a personal computer that welcomes curiosity, invites experimentation, and keeps the power in your hands. This is personal computing for the next generation.&lt;/p&gt;&lt;p&gt;Find out more about the Radiant system, or read our design principles and tenets.&lt;/p&gt;&lt;p&gt;Radiant is an ongoing research project in personal computing. If this resonates with you, write us at üìß letters@radiant.computer or follow us on ü¶ã Bluesky @radiant.computer.&lt;/p&gt;&lt;p&gt;You can learn more about Radiant by browsing our log or notes.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://radiant.computer"/><published>2025-11-05T13:22:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45822902</id><title>Microsoft Can't Keep EU Data Safe from US Authorities</title><updated>2025-11-05T21:34:51.499112+00:00</updated><content>&lt;doc fingerprint="3f1a0016b62c5443"&gt;
  &lt;main&gt;
    &lt;p&gt;Microsoft has admitted that it can't protect EU data from U.S. snooping.&lt;/p&gt;
    &lt;p&gt;In sworn testimony before a French Senate inquiry into the role of public procurement in promoting digital sovereignty, Anton Carniaux, Microsoft France's director of public and legal affairs, was asked whether he could guarantee that French citizen data would never be transmitted to U.S. authorities without explicit French authorization. And, he replied, "No, I cannot guarantee it."&lt;/p&gt;
    &lt;p&gt;He said that the company resisted requests from the US authorities "when they are not well-founded", but that under the U.S. Cloud Act, U.S. companies can be forced to hand over data, regardless of where it is stored.&lt;/p&gt;
    &lt;p&gt;Carniaux did say that the situation had never arisen. However, the admission raises serious concerns around European data sovereignty.&lt;/p&gt;
    &lt;p&gt;‚ÄúMicrosoft has openly admitted what many have long known: under laws like the Cloud Act, US authorities can compel access to data held by American cloud providers, regardless of where that data physically resides. UK or EU servers make no difference when jurisdiction lies elsewhere, and local subsidiaries or ‚Äòtrusted‚Äô partnerships don‚Äôt change that reality," commented Mark Boost, CEO of cloud provider Civo.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis is more than a technicality. It is a real-world issue that can impact national security, personal privacy and business competitiveness."&lt;/p&gt;
    &lt;p&gt;The inquiry centers around Project Bleu - a partnership between Microsoft, Orange and Capgemini. There were concerns about the Health Data Hub medical research platform, which is hosted on Microsoft Azure. Senate members said they couldn't be sure that the two platforms were sufficiently separated, and that sensitive health data wouldn't be shared.&lt;/p&gt;
    &lt;p&gt;Carniaux's admission will increase concerns that the EU can't afford to be reliant on the big U.S. cloud providers such as Microsoft and AWS - even when they claim to be offering sovereign cloud services.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe French Senate has set a precedent by demanding answers, and the UK and Europe have an opportunity to do the same," said Boost. "We‚Äôre already seeing a shift towards building homegrown solutions that support true data sovereignty rather than data residency."&lt;/p&gt;
    &lt;p&gt;However, a recent European Parliament report found that U.S. firms account for 69% of the cloud infrastructure market share in Europe, while EU suppliers hold only 13%.&lt;/p&gt;
    &lt;p/&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.forbes.com/sites/emmawoollacott/2025/07/22/microsoft-cant-keep-eu-data-safe-from-us-authorities/"/><published>2025-11-05T14:00:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45822982</id><title>A P2P Vision for QUIC (2024)</title><updated>2025-11-05T21:34:51.018170+00:00</updated><content>&lt;doc fingerprint="23b498505021acb9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A p2p Vision for QUIC&lt;/head&gt;
    &lt;p&gt;This article was co-authored by Christian Huitema. You can find his blog at privateoctopus.com.&lt;/p&gt;
    &lt;p&gt;Over the years, the IETF has standardized numerous protocols for establishing IP packet flows through NATs and firewalls, including STUN, ICE, and TURN.&lt;/p&gt;
    &lt;p&gt;This is an inherently messy topic, and I can highly recommend reading Eric Rescorla‚Äôs blog post series about NATs (part 1, part 2, part 3). I won‚Äôt go into details about how exactly NATs work (again, read the ekr‚Äôs blog posts!), but in a nutshell, they rewrite the IP of packets passing through the NAT.&lt;/p&gt;
    &lt;p&gt;(192.168.1.10) participant NAT as NAT&lt;/p&gt;
    &lt;p&gt;(203.0.113.5) participant Server as Server&lt;/p&gt;
    &lt;p&gt;(198.51.100.20) Client-&amp;gt;&amp;gt;NAT: Packet (Src: 192.168.1.10:1234) NAT-&amp;gt;&amp;gt;Server: Packet (Src: 203.0.113.5:4321) Server--&amp;gt;&amp;gt;NAT: Response (Dst: 203.0.113.5:4321) NAT--&amp;gt;&amp;gt;Client: Response (Dst: 192.168.1.10:1234)&lt;/p&gt;
    &lt;p&gt;This allows multiple clients behind that NAT to share the same external IP address. Clients are able to reach any server on the internet, but it doesn‚Äôt allow nodes on the internet to reach the client, since the NAT won‚Äôt forward packets to the client, unless it determines that they belong to an existing flow. In that sense, the NAT acts as a firewall.&lt;/p&gt;
    &lt;p&gt;Although simple in theory, there are a lot of different ways to implement a NAT. For our purposes, the main difference lies in how the port numbers for the outgoing packets are allocated. Depending on the port allocation logic, it might or might not possible to establish a direct connection between two peers.&lt;/p&gt;
    &lt;p&gt;The problem that p2p network engineers hope to solve is the following: How can two nodes that are both behind a NAT, respectively, connect to each other, no matter the kind of NAT?&lt;/p&gt;
    &lt;p&gt;In this post, we explore how QUIC can be leveraged to provide a comprehensive solution for NAT traversal, encompassing everything from address discovery to UDP proxying, potentially simplifying and improving upon traditional p2p networking approaches.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Traditional Way&lt;/head&gt;
    &lt;head rend="h3"&gt;Address Discovery using STUN&lt;/head&gt;
    &lt;p&gt;Depending on the deployment scenario, a new node joining the network might or might not know its (public) IP address. Traditionally, applications use STUN (RFC 8489) to discover their public IP address. In a nutshell, a STUN client sends a ‚ÄúBinding Request‚Äù to a STUN server. The ‚ÄúBinding Response‚Äù of the server encodes the IP source address and the source port that the server observed on the client‚Äôs request.&lt;/p&gt;
    &lt;p&gt;The client can infer from the responses to the STUN requests if it is located behind a NAT. The client might even compare responses from different STUN servers and attempt to infer the type of NAT, although this is notoriously difficult to get right. The IETF has pretty much given up on this approach (see section 2 of RFC 5389).&lt;/p&gt;
    &lt;p&gt;The STUN protocol can run over UDP, TCP, DTLS (RFC 7350) or TLS.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hole Punching Coordination using ICE&lt;/head&gt;
    &lt;p&gt;As we‚Äôve seen above, once the NAT has seen the first packet to a remote server pass through, the NAT opens up the return path, allowing packets from the outside world to reach the node. The idea behind hole punching is to have both peers send packets simultaneously, each of them ‚Äúpunching‚Äù a hole in their respective firewall, establishing a direct flow of packets between the two nodes.&lt;/p&gt;
    &lt;p&gt;The traditional hole punching process is specified by ICE RFC 8445. ICE starts with an address gathering phase, in which the two peers separately contact STUN servers to obtain lists of candidate IP addresses and ports. They may add to that list a set of TURN addresses (see next section, ‚Äúrelaying‚Äù).&lt;/p&gt;
    &lt;p&gt;One of the endpoints creates a list of available addresses, ordered by priority, and sends it to its peer. The peer compares that to its own list, establishes a list of ‚Äúcandidate pairs‚Äù, and sends it back. At that point, both endpoints have the same list of candidate pairs, and start the ‚Äúconnectivity‚Äù check. Each host will try to send a STUN binding request from its selected address to the paired address, and respond to STUN requests that it might receive from the peer. If at least one of these trials succeeds, the peers have established a new connection over this address pair. If several succeed, they keep the preferred pair.&lt;/p&gt;
    &lt;head rend="h3"&gt;Relaying using TURN&lt;/head&gt;
    &lt;p&gt;Unfortunately, no matter how hard you try, there is a certain percentage of nodes for whom hole punching will never work. This is because their NAT behaves in an unpredictable way. While most NATs are well-behaved, some aren‚Äôt. This is one of the sad facts of life that network engineers have to deal with.&lt;/p&gt;
    &lt;p&gt;The only solution for this problem is to employ the help of a third party, i.e. a server that is not located behind a NAT, and therefore can be reached by peers directly, without any hole punching. This server can then relay traffic between the two peers.&lt;/p&gt;
    &lt;p&gt;Of course, this comes at a cost. The path via the relay might be slower (in terms of latency and / or bandwidth) than a (hypothetical) direct path would have been. And relaying traffic is not for free for the operator of the relay: both processing resources as well as bandwidth cost money. However, we don‚Äôt really have a choice here, and despite these shortcomings, having a relayed connection might be preferable to having no connectivity at all.&lt;/p&gt;
    &lt;p&gt;The traditional solution relies on TURN servers, specified in RFC 5766 to provide these ‚Äúlast resort‚Äù connectivity. The node behind a NAT can ask the TURN server to open an UDP or TCP port. This ‚ÄúTURN Port‚Äù is usually specialized: the client specifies the address of the peer that will be able to send data to that port, or to which data will be sent. The corresponding IP address and port number will be sent to the peer, and will be the basis for some last resort ‚Äúcandidate pairs‚Äù used in the coordinated hole punching.&lt;/p&gt;
    &lt;p&gt;The previous section mentioned that in some cases more than one tried address pair will succeed. This is particularly true for the pairs that include a TURN provided address. This is why the trials will try to collect all the working pairs and pick the higher priority one. If both a ‚Äúhole punching‚Äù and a ‚ÄúTURN‚Äù pair succeed, they will typically only retain the ‚Äúhole punching‚Äù pair.&lt;/p&gt;
    &lt;head rend="h2"&gt;The QUIC Way&lt;/head&gt;
    &lt;head rend="h3"&gt;QUIC Connection Migration&lt;/head&gt;
    &lt;p&gt;RFC 9000 defines how clients can migrate an existing QUIC connection to a different IP:port tuple. When we designed this mechanism, the primary use case we envisioned was solving the ‚Äúparking lot problem‚Äù. Imagine you have a mobile phone, and you walk from your office (where the phone has WiFi) to the parking lot (with no / bad WiFi coverage). In this case, the client could detect that the WiFi connection is worsening, and migrate the connection to its cellular interface. Crucially, this would keep all connection state (e.g. open streams, datagram flows, etc.) intact, and would therefore be transparent to the application.&lt;/p&gt;
    &lt;p&gt;On detecting a network interface change, e.g. leaving the office, QUIC Path Migration works by first sending a so-called probing packet to the server. The purpose of this packet is to probe if the path actually works. The client includes a PATH_CHALLENGE frame in this packet, to which the server responds with a PATH_RESPONSE frame. This makes sure that the new path actually works (for example, that the path doesn‚Äôt block UDP packets), and supports QUIC (for example, allows packets that satisfy QUIC‚Äôs MTU requirements).&lt;/p&gt;
    &lt;p&gt;On the wire, this path probing procedure looks pretty similar to a hole punch attempt. We just need a tiny modification to make this work in the p2p use case: If we could get the server to send probe packets as well, we could kill two birds with one stone: We‚Äôd punch a hole through the firewall, and at the same time verify connectivity on the path.&lt;/p&gt;
    &lt;p&gt;Of course this is not the only thing needed to achieve hole punching. Before the nodes can even send probe packets, we need to learn about the peer‚Äôs reflexive address, and be able to coordinate the timing. We‚Äôll come to this in a bit, but first we‚Äôll describe how we can replace STUN to discover our reflexive addresses.&lt;/p&gt;
    &lt;head rend="h3"&gt;QUIC Address Discovery&lt;/head&gt;
    &lt;p&gt;Typically nodes use STUN to discover their reflexive addresses. In essence, STUN is a request-response protocol here, where the client requests the server to report the observed address of the request packet.&lt;/p&gt;
    &lt;p&gt;In principle, we could achieve the same inside of a QUIC connection: The server could report the address of the client using, for example, a newly defined QUIC frame, and vice versa. This is exactly what the QUIC Address Discovery draft specifies.&lt;/p&gt;
    &lt;p&gt;The mechanism is really simple: every time a new path is established (incl. the path used for the handshake), endpoints inform each other of the observed address. This is a very efficient mechanism: Since the OBSERVED_ADDRESS frame is defined as a probing frame, it can be bundled with the PATH_CHALLENGE and PATH_RESPONSE frames used to probe a new path.&lt;/p&gt;
    &lt;p&gt;Performing address discovery over QUIC comes with multiple advantages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;QUIC packets are encrypted. An observer is not able to observe the exchange of OBSERVED_ADDRESS frames, nor interfere with this exchange (e.g. by tampering with the frame contents).&lt;/item&gt;
      &lt;item&gt;It doesn‚Äôt require running any additional services (i.e. a STUN server / client). It‚Äôs sufficient to enable the Address Discovery extension on a large enough number of nodes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course, in either case the client has to trust that the server is sending honest responses. A misbehaving server could respond with spoofed addresses, causing the ‚Äúhole punching‚Äù packets to later be sent to these addresses. This is not hard to defend against: Clients can obtain some protection against such attacks by contacting several servers and comparing their responses.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hole Punching Coordination&lt;/head&gt;
    &lt;p&gt;The node has now learned its reflexive addresses, and we know how to use QUIC‚Äôs connection migration mechanism to establish the NAT port mappings required to allow the establishment of a direct path. We now want to establish a connection to another node, behind its respective NAT.For the moment, we‚Äôll assume that the two nodes are able to communicate via a (proxied) QUIC connection. We‚Äôll see how this works in detail in the next section. The only thing that matters for now is that the nodes are able to communicate with each other.&lt;/p&gt;
    &lt;p&gt;The NAT traversal draft defines how two nodes can negotiate hole punching attempts with each other. Out of convenience, the process is driven almost entirely by the client (i.e. the node that initiated the QUIC connection). This is not because the roles of the peers are fundamentally different (they are both peers in the same p2p network), but it leads to significant simplifications of the protocol. It also reduces the difference to RFC 9000, where connection migration can only be initiated by the client.&lt;/p&gt;
    &lt;p&gt;The server informs the clients about its reflexive address using ADD_ADDRESS frames. Multiple ADD_ADDRESS frames can be sent if the server has multiple reflexive addresses.&lt;/p&gt;
    &lt;p&gt;The ICE RFC goes into great detail on how to form candidate pairs from both nodes‚Äô reflexive addresses, because both nodes need to agree on the ordering of the candidate pairs. Since the client is driving this process, we don‚Äôt need to specify any address matching logic that client and server would need to agree on.&lt;/p&gt;
    &lt;p&gt;Once the client has formed address pairs (and once it feels like it‚Äôs the right time to start a hole punch attempt), it sends a PUNCH_ME_NOW frame to the server. The PUNCH_ME_NOW contains both the client‚Äôs and the server‚Äôs reflexive addresses.&lt;/p&gt;
    &lt;p&gt;Immediately after sending the PUNCH_ME_NOW frame, the client starts path probing on the path formed by these two addresses. Equivalently, as soon as the server receives the PUNCH_ME_NOW frame, it starts path probing the path from its end. Timing is crucial here: As we‚Äôve seen above, the path probing packets create the NAT binding required to allow the other side‚Äôs packets to make it through the NAT.&lt;/p&gt;
    &lt;p&gt;Both the client and the server will send PATH_CHALLENGE frames on a new path when sending or responding to &lt;code&gt;PUNCH_ME_NOW&lt;/code&gt;. They will need to allocate a yet unused Connection ID to the new path that they
are trying to establish. This implies that the number of parallel attempts is limited by the number of available Connection ID. This has both upsides and downside. On the one hand, having a limit reduces the amount of resource that a peer can be forced to consume, which makes the protocol more stable.
On the other hand, if the limits are reached, the next attempt will only be possible after one of the previous challenges has been abandoned, and the peer has provided a replacement Connection ID. This might be a slow process.&lt;/p&gt;
    &lt;p&gt;Whether that process is too slow is debatable. Endpoints that plan to engage in p2p hole punching may be configured to provide a number of Connection IDs sufficient for most practical attempts. Also, the initial path will be available until the migration succeeds, which means the application endpoints do not need to wait the success of the negotiation to start exchanging data.&lt;/p&gt;
    &lt;p&gt;Adding new QUIC frames like &lt;code&gt;ADD_ADDRESS&lt;/code&gt; or &lt;code&gt;PUNCH_ME_NOW&lt;/code&gt; is somewhat controversial. Misbehaving peers could send spoofed addresses in these frames, causing the peer to send hole punching packets to third parties. This is similar to the request forgery attacks described in the security section of RFC 9000, and calls at least for the same kind of defenses. This is something we will keep in mind when evolving the NAT traversal draft.&lt;/p&gt;
    &lt;head rend="h3"&gt;Relaying UDP packets over HTTP&lt;/head&gt;
    &lt;p&gt;RFC 9298 defines how UDP packets can be proxied in HTTP. The exchange starts a regular HTTP request: The client sends a so-called Extended CONNECT request to the proxy on a QUIC stream, instructing the proxy to open a UDP socket and proxy a flow of UDP packets to a target server.&lt;/p&gt;
    &lt;p&gt;Once the proxy has accepted the proxying request, UDP packets are sent in HTTP Datagrams (RFC 9297), which themselves are a thin wrapper around QUIC DATAGRAM frames. QUIC Datagrams are a new QUIC frame defined in RFC 9221, which are sent in QUIC packets exchanged after completion of the QUIC handshake. They‚Äôre therefore encrypted the same way that any other data exchanged over the QUIC connection is. However, if a packet containing a DATAGRAM is lost, the DATAGRAM frame is not retransmitted. This makes DATAGRAMs suitable to proxy unreliable packets, such as UDP packets.&lt;/p&gt;
    &lt;p&gt;Multiple UDP flows to different target servers can be proxied in the same QUIC connection.&lt;/p&gt;
    &lt;p&gt;Proxying UDP packets is almost what we need to make relaying work in the p2p scenario, but not quite: While the client can reach any IP via the proxy, it‚Äôs still not possible for other nodes to communicate with the client (unless contacted first by the client).&lt;/p&gt;
    &lt;p&gt;Fortunately, there‚Äôs already a draft describing how to Proxy UDP Listeners in HTTP. The primary use case for this draft is running WebRTC over CONNECT-UDP. This is a very similar problem to the one we‚Äôre trying to solve: WebRTC peers actually use the ICE protocol to establish a direct connection, and for that they need to know their reflexive transport addresses.&lt;/p&gt;
    &lt;p&gt;The mechanism is pretty straight-forward: The proxy allocates a new IP:port for the client, and forwards all UDP packets on this socket to the client. Of course, it also has to include the 2-tuple that the packet originated from.&lt;/p&gt;
    &lt;p&gt;The simplificity of this approach is at the same time its biggest limitation: Since there are only 65535 port numbers (many of which are reserved), a proxy can only handle a limited number of clients at the same time. To be clear, this still allows tens of thousands of concurrent clients, and many deployment scenarios will never run into this limit.&lt;/p&gt;
    &lt;p&gt;It might be possible to work around this limit in the future by using a similar approach as the QUIC-aware proxying draft.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preparing for Multipath&lt;/head&gt;
    &lt;p&gt;The QUIC Working Group is finalizing the Multipath Extensions for QUIC. As the name suggests, this extensions allow multiple paths to be used simultaneously. For the p2p use case, this means that endpoints could keep the initial path available, even after a direct path was created by NAT traversal. These paths could either be used for load sharing or as a backup.&lt;/p&gt;
    &lt;p&gt;To get these benefits, we will need minor adaptations of the mechanism described here ‚Äì effectively, managing connection IDs and path IDs in a multipath version of the &lt;code&gt;PUNCH_ME_NOW&lt;/code&gt; frame. We should work on that once the Multipath Extension for QUIC has made more progress in the IETF.&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting All the Pieces Together&lt;/head&gt;
    &lt;p&gt;Now that we‚Äôve explored all the components, let‚Äôs put them together and build a small p2p application running on top of QUIC.&lt;/p&gt;
    &lt;p&gt;When the node boots up, it first connects to a few hard-coded boot nodes. The majority of these nodes support the QUIC Address Discovery extension, so the node is able to learn that it‚Äôs behind a NAT, and what the NAT‚Äôs public addresses are.&lt;/p&gt;
    &lt;p&gt;It then connects to a relay and reserves an IP:port tuple with the relay. The node can now advertise this address to other peers in the network, for example by registering in some kind of peer directory, or by registering itself with the p2p network‚Äôs DHT.&lt;/p&gt;
    &lt;p&gt;At this point, other nodes can connect to the relay at this port, and have all their packets relayed. We‚Äôve achieved the first goal: we have established connectivity. The relayed connections can immediately be used to exchange application data. Now the goal is to lighten the load on the relay server, and to obtain a direct (and potentially lower-latency, higher-throughput) to the peer.&lt;/p&gt;
    &lt;p&gt;The nodes employ the mechanism described in the NAT traversal draft to punch holes through their respective NATs. This hole punching procedure might take a few attempts, depending on the number of candidate pairs (and if hole punching attempts are run in parallel), but should generally only take a few seconds.&lt;/p&gt;
    &lt;p&gt;Most importantly, this is entirely transparent to the application: The application can start to use the relayed connection, and use it all the while the QUIC stack tries to establish the direct path.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where are we on this?&lt;/head&gt;
    &lt;p&gt;So far, there‚Äôs no implementation of this protocol in production, but a lot of the documents have made their way through the IETF process and have now become widely deployed RFCs.&lt;/p&gt;
    &lt;p&gt;Specifically, the remaining pieces of the puzzle are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Proxy UDP Listeners in HTTP draft, which allows clients to reserve an IP:port tuple on a relay server.&lt;/item&gt;
      &lt;item&gt;The QUIC Address Discovery draft, which allows endpoints to learn about their public addresses. The current version of this draft is implemented by two different QUIC stacks: picoquic and a fork of quinn.&lt;/item&gt;
      &lt;item&gt;The NAT Traversal for QUIC draft, which defines how to coordinate hole punching attempts between peers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The quic-go project and the QUIC Interop Runner are community-funded projects.&lt;/p&gt;
    &lt;p&gt;If you find my work useful, please considering sponsoring:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://seemann.io/posts/2024-10-26---p2p-quic/"/><published>2025-11-05T14:06:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45823059</id><title>Removing XSLT for a more secure browser</title><updated>2025-11-05T21:34:50.259865+00:00</updated><content>&lt;doc fingerprint="be4a8c104cf770b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Published: October 29, 2025&lt;/p&gt;
    &lt;p&gt;Chrome intends to deprecate and remove XSLT from the browser. This document details how you can migrate your code before the removal in late-2026.&lt;/p&gt;
    &lt;p&gt;Chromium has officially deprecated XSLT, including the XSLTProcessor JavaScript API and the XML stylesheet processing instruction. We intend to remove support from version 155 (November 17, 2026). The Firefox and WebKit projects have also indicated plans to remove XSLT from their browser engines. This document provides some history and context, explains how we are removing XSLT to make Chrome safer, and provides a path for migrating before these features are removed from the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is being removed?&lt;/head&gt;
    &lt;p&gt;There are two APIs in the browser that implement XSLT, and both are being removed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The XSLTProcessor class (for example, &lt;code&gt;new XSLTProcessor()&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;The XSLT Processing Instruction (for example, &lt;code&gt;&amp;lt;?xml-stylesheet ‚Ä¶ ?&amp;gt;&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Timeline For Chrome&lt;/head&gt;
    &lt;p&gt;Chrome has the following plan:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Chrome 142 (Oct 28, 2025): Early warning console messages added to Chrome.&lt;/item&gt;
      &lt;item&gt;Chrome 143 (Dec 2, 2025): Official deprecation of the API - deprecation warning messages begin to show in the console and in lighthouse.&lt;/item&gt;
      &lt;item&gt;Chrome 148 (March 10, 2026 Canary): Canary, Dev, and Beta releases begin disabling XSLT by default, as an early-warning.&lt;/item&gt;
      &lt;item&gt;Chrome 152 (Aug 25, 2026): Origin Trial (OT) and Enterprise Policy (EP) go live for testing. These allow sites and enterprises to continue using features past the removal date.&lt;/item&gt;
      &lt;item&gt;Chrome 155 (Nov 17, 2026): XSLT stops functioning on Stable releases, for all users other than Origin Trial and Enterprise Policy participants.**&lt;/item&gt;
      &lt;item&gt;Chrome 164 (Aug 17, 2027): Origin Trial and Enterprise Policy stop functioning. XSLT is disabled for all users.**&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What is XSLT?&lt;/head&gt;
    &lt;p&gt;XSLT, or Extensible Stylesheet Language Transformations, is a language used to transform XML documents, commonly into other formats such as HTML. It uses an XSLT stylesheet file to define the rules for this conversion, and an XML file containing the data used as input.&lt;/p&gt;
    &lt;p&gt;In browsers, when an XML file is received that links to an XSLT stylesheet, the browser uses the rules in that stylesheet to rearrange, format, and convert the raw XML data into a structured page (often HTML) that can be rendered for the user.&lt;/p&gt;
    &lt;p&gt;For example, an XSLT stylesheet could take the following XML input:&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0"?&amp;gt;
&amp;lt;?xml-stylesheet type="text/xsl" href="demo.xsl" ?&amp;gt;
&amp;lt;page&amp;gt;
 &amp;lt;message&amp;gt;
  Hello World.
 &amp;lt;/message&amp;gt;
&amp;lt;/page&amp;gt;
&lt;/code&gt;
    &lt;p&gt;and this XSL stylesheet:&lt;/p&gt;
    &lt;code&gt;&amp;lt;xsl:stylesheet xmlns:xsl="http://www.w3.org/1999/XSL/Transform"&amp;gt;
  &amp;lt;xsl:output method="html"/&amp;gt;
  &amp;lt;xsl:template match="/page/message"&amp;gt;
    &amp;lt;body&amp;gt;
      &amp;lt;p&amp;gt;Message: &amp;lt;xsl:value-of select="."/&amp;gt;&amp;lt;/p&amp;gt;
    &amp;lt;/body&amp;gt;
  &amp;lt;/xsl:template&amp;gt;
&amp;lt;/xsl:stylesheet&amp;gt;
&lt;/code&gt;
    &lt;p&gt;and process them into this HTML for the browser to display: HTML&lt;/p&gt;
    &lt;code&gt;&amp;lt;body&amp;gt;
  &amp;lt;p&amp;gt;Message: Hello World.&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&lt;/code&gt;
    &lt;p&gt;In addition to the XSL processing instruction shown in the previous example, there's also the XSLTProcessor JavaScript API which can be used to process local XML documents with local XSLT stylesheets.&lt;/p&gt;
    &lt;head rend="h2"&gt;History of XSLT&lt;/head&gt;
    &lt;p&gt;XSLT was recommended by the World Wide Web Consortium (W3C) on November 16, 1999, as a language for transforming XML documents into other formats, most commonly HTML for display in web browsers. Before the official 1.0 recommendation, Microsoft took an early initiative by shipping a proprietary implementation based on a W3C working draft in Internet Explorer 5.0, released in March 1999. Following the official standard, Mozilla implemented native XSLT 1.0 support in Netscape 6 in late 2000. Other major browsers, including Safari, Opera, and later Chrome, also incorporated native XSLT 1.0 processors, making client-side XML-to-HTML transformations a viable web technology in the early 2000s.&lt;/p&gt;
    &lt;p&gt;The XSLT language itself continued to evolve, with the release of XSLT 2.0 in 2007 and XSLT 3.0 in 2017, which introduced powerful features like regular expressions, improved data types, and the ability to process JSON. Browser support, however, stagnated. Today, all major web browser engines only provide native support for the original XSLT 1.0 from 1999. This lack of advancement, coupled with the rise of the use of JSON as a wire format, and JavaScript libraries and frameworks (like jQuery, React, and Vue.js) that offer more flexible and powerful DOM manipulation and templating, has led to a significant decline in the use of client-side XSLT. Its role within the web browser has been largely superseded by these JavaScript-based technologies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does XSLT need to be removed?&lt;/head&gt;
    &lt;p&gt;The continued inclusion of XSLT 1.0 in web browsers presents a significant and unnecessary security risk. The underlying libraries that process these transformations, such as libxslt (used by Chromium browsers), are complex, aging C/C++ codebases. This type of code is notoriously susceptible to memory safety vulnerabilities like buffer overflows, which can lead to arbitrary code execution. For example, security audits and bug trackers have repeatedly identified high-severity vulnerabilities in these parsers (e.g., CVE-2025-7425 and CVE-2022-22834, both in libxslt). Because client-side XSLT is now a niche, rarely-used feature, these libraries receive far less maintenance and security scrutiny than core JavaScript engines, yet they represent a direct, potent attack surface for processing untrusted web content. Indeed, XSLT is the source of several recent high-profile security exploits that continue to put browser users at risk. The security risks of maintaining this brittle, legacy functionality far outweighs its limited modern utility.&lt;/p&gt;
    &lt;p&gt;Furthermore, the original purpose of client-side XSLT‚Äîtransforming data into renderable HTML‚Äîhas been superseded by safer, more ergonomic, and better-maintained JavaScript APIs. Modern web development relies on things like the Fetch API to retrieve data (typically JSON) and the DOMParser API to safely parse XML or HTML strings into a DOM structure within the browser's secure JavaScript sandbox. Frameworks like React, Vue, and Svelte then manage the rendering of this data efficiently and securely. This modern toolchain is actively developed, benefits from the massive security investment in JavaScript engines, and is what virtually all web developers use today. Indeed, only about 0.02% of web page loads today actually use XSLT at all, with less than 0.001% using XSLT processing instructions.&lt;/p&gt;
    &lt;p&gt;This is not a Chrome or Chromium-only action: the other two major browser engines also support the removal of XSLT from the web platform: WebKit, Gecko.&lt;/p&gt;
    &lt;p&gt;For these reasons, deprecating and removing XSLT reduce the browser's attack surface for all users, simplify the web platform, and allow engineering resources to be focused on securing the technologies that actually power the modern web, with no practical loss of capability for developers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Improving XML parsing security&lt;/head&gt;
    &lt;p&gt;Similar to the severe security issues in libxslt, severe security issues were recently reported against libxml2 which is used in Chromium for parsing, serialization and testing the well-formedness of XML. To address future security issues with XML parsing In Chromium we plan to phase out the usage of libxml2 and replace XML parsing with a memory-safe XML parsing library written in Rust. Importantly, we won't be removing XML from the browser; only XSLT is being considered for removal here. We intend to ensure that replacing libxml2 is entirely transparent to web developers.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to migrate&lt;/head&gt;
    &lt;p&gt;There are a few alternative paths for migration.&lt;/p&gt;
    &lt;head rend="h3"&gt;JSON&lt;/head&gt;
    &lt;p&gt;For sites that are fully built on XML and XSL there is no one-size-fits all way to make the transition. Migration options include moving the XSLT processing pipeline to the server side and sending down the rendered HTML to the client, or migrating server-side XML API endpoints to JSON, and performing client-side rendering using JavaScript to transform JSON into HTML DOM and CSS.&lt;/p&gt;
    &lt;head rend="h3"&gt;Client-side XSLT in JavaScript&lt;/head&gt;
    &lt;p&gt;There are a few client-side (JavaScript-based) XSLT libraries available, but the largest by far is produced by Saxonica (view the comprehensive documentation for Saxonica). The implementation goes well beyond the XSLT 1.0 implementation in web browsers, implementing full support for the latest v3.0 standard, and eventually the in-progress v4.0 standard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Polyfill&lt;/head&gt;
    &lt;p&gt;There is a polyfill that attempts to allow existing code, which depends on web browsers' implementations of XSLT 1.0, to continue functioning, while not using native XSLT features from the browser. The polyfill is located on GitHub.&lt;/p&gt;
    &lt;p&gt;The polyfill contains a functional WASM-based polyfilled replacement for the XSLTProcessor class, so existing JavaScript code can continue to work as-is:&lt;/p&gt;
    &lt;code&gt;&amp;lt;script src="xslt-polyfill.min.js"&amp;gt;&amp;lt;/script&amp;gt;

&amp;lt;script&amp;gt;
const xsltProcessor = new XSLTProcessor();
xsltProcessor.importStylesheet(xsltDoc);
const fragment = xsltProcessor.transformToFragment(xmlDoc, document);
&amp;lt;/script&amp;gt;
&lt;/code&gt;
    &lt;p&gt;The polyfill also provides an automatic utility function for an easy way to replace XML documents that use XSLT processing instructions:&lt;/p&gt;
    &lt;p&gt;For an original &lt;code&gt;demo.xml&lt;/code&gt; file like this:&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0"?&amp;gt;
&amp;lt;?xml-stylesheet type="text/xsl" href="demo.xsl"?&amp;gt;
&amp;lt;ROOT&amp;gt;
...content...
&lt;/code&gt;
    &lt;p&gt;One line can be added to invoke the polyfill and transform the document with the referenced XSLT stylesheet:&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0"?&amp;gt;
&amp;lt;?xml-stylesheet type="text/xsl" href="demo.xsl"?&amp;gt;
&amp;lt;ROOT&amp;gt;
&amp;lt;script src="xslt-polyfill.min.js"
   xmlns="http://www.w3.org/1999/xhtml"&amp;gt;&amp;lt;/script&amp;gt;
...content...
&lt;/code&gt;
    &lt;p&gt;In this case, the new &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; element loads the polyfill, which detects the
XML document type and the XSLT processing instruction and transparently loads
it, replacing the document.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extension&lt;/head&gt;
    &lt;p&gt;There's also a Chrome extension that can be added to supported browsers, which will apply the same XSLT polyfill to all raw XML pages that contain XSLT processing instructions or calls to XSLTProcessor. This can be used for applications where the source XML or XSLT cannot be changed, to maintain functionality.&lt;/p&gt;
    &lt;p&gt;In particular, when XSLT is disabled, Chrome now shows a warning banner that links directly to an extension search page, to help users locate an extension:&lt;/p&gt;
    &lt;head rend="h2"&gt;Specific use cases&lt;/head&gt;
    &lt;p&gt;In the discussion in HTML standards, several concrete use cases were identified. This section talks specifically about each of them, to recommend paths forward for developers publishing XML resources that use XSLT today.&lt;/p&gt;
    &lt;head rend="h3"&gt;RSS and Atom Feeds&lt;/head&gt;
    &lt;p&gt;In many existing RSS or Atom feeds, XSLT is used to make raw XML feeds human-readable when viewed directly in a browser. The primary use case is that when a user accidentally clicks on a site's RSS feed link, rather than pasting that link into their RSS reader, they get a formatted HTML response that they can read, rather than the raw XML itself.&lt;/p&gt;
    &lt;p&gt;There are two paths forward for this use case. The "standard" HTML way to do this is to add &lt;code&gt;&amp;lt;link rel="alternate" type="application/rss+xml"&amp;gt;&lt;/code&gt; to an
(HTML-based) site, rather than adding an explicit (user-visible) &lt;code&gt;&amp;lt;a
href="something.xml"&amp;gt;&lt;/code&gt; that users might accidentally click. This solution allows
RSS readers to find the feed if a user pastes in just the website URL, but it
also allows human users to see the regular HTML content without getting confused
by a link to an XML resource. This also follows the normal web paradigm that
HTML is for humans and XML is for machines. Of course this doesn't solve the
case where a user just "has" an RSS link from somewhere, and they paste it into
their web browser (rather than their RSS reader).&lt;/p&gt;
    &lt;p&gt;When that solution isn't wanted, the polyfill offers another path. As mentioned previously, the RSS/Atom XML feed can be augmented with one line, &lt;code&gt;&amp;lt;script
src="xslt-polyfill.min.js" xmlns="http://www.w3.org/1999/xhtml"&amp;gt;&amp;lt;/script&amp;gt;&lt;/code&gt;,
which will maintain the existing behavior of XSLT-based transformation to HTML.
That shouldn't affect RSS reader's ability to continue parsing the XML, since
the &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; is a direct child of the root element.&lt;/p&gt;
    &lt;head rend="h3"&gt;API output for embedded devices&lt;/head&gt;
    &lt;p&gt;Some commercial embedded devices measure or otherwise generate XML data for consumption by users on the local network. Some of these devices do this by generating a single XML data feed that uses XSLT to transform it into a human-readable HTML format. That allows the API to be directly viewed in a browser without needing additional code on the device or in the browser.&lt;lb/&gt; Since this is a very application specific use case, the shape of the solution might vary. For applications where the source code of the embedded device can be updated, any of the options described previously (JSON, Polyfill) could work. In particular, however, many such devices are difficult or impossible to update, for various reasons. In that case, the extension is likely the best option, since it allows client browsers to continue to read the data in exactly the same way, without modifying the device.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lazy templating for web sites&lt;/head&gt;
    &lt;p&gt;Web developers sometimes use XSLT on the client side to apply presentation markup to semantic markup, functioning as a lazy templating language that is separate from the JavaScript ecosystem.&lt;/p&gt;
    &lt;p&gt;There are two solutions to this more general problem. For an existing site built in this way, the easiest solution is likely just to add the polyfill to maintain existing functionality. Or perhaps perform the XSLT transformation on the server side, and serve the resulting HTML to the client, rather than the raw XML. The more long-term solution for such properties would be to migrate to a more modern JavaScript or JSON-based framework.&lt;/p&gt;
    &lt;p&gt;If you encounter a specific problem in Chrome related to this XSLT deprecation, report a bug here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://developer.chrome.com/docs/web-platform/deprecating-xslt"/><published>2025-11-05T14:14:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45823141</id><title>The shadows lurking in the equations</title><updated>2025-11-05T21:34:50.026005+00:00</updated><content>&lt;doc fingerprint="370746cbee2f5ea1"&gt;
  &lt;main&gt;
    &lt;p&gt;For all the history of computational mathematical visualization, graphing equations has been done in binary mode - where graphs show only where an equation is EXACTLY equal. But when you only see in black-and-white, some things are invisible. For all this time, lurking beneath the error == 0 surface, mathematical shadows have been lurking in the equations.&lt;/p&gt;
    &lt;p&gt;FuzzyGraph, on the other hand, visualizes equations in Non-Binary mode - showing not only where an equation are exactly equal, but also where the equation nearly equal and where the equation is far from equal (where the error is high). Sometimes, these high error areas form clear visual shadow-like features.&lt;/p&gt;
    &lt;p&gt;Let's look at some examples...&lt;/p&gt;
    &lt;p&gt;Here is the "Slash Dot" Equation ( \( \frac{y}{x^2+y^2} = \frac{x+1}{x^2+y^2} \)) as both a conventional and fuzzy graph...&lt;/p&gt;
    &lt;p&gt;Note the giant black hole that is present in the Fuzzy/Non-Binary graph, but invisible in conventional/Binary graphing. This "black hole" feature represents a region of high error in the equation.&lt;/p&gt;
    &lt;p&gt;Let's look at another example: \(y = \frac{x}{x^2 + y^2} \) &lt;/p&gt;
    &lt;p&gt;Notice that the black hole eye-looking features are COMPLETELY INVISIBLE in the conventional/binary mode of graphing.&lt;/p&gt;
    &lt;p&gt;To get a better idea of what these black hole things are, let's look at a simpler example. First let's look at the opposite of a black hole - a simple star/particle example: \( x^2 + y^2 = 0 \). For this equation, there is only 1 solution: (0, 0). So if you graph this in a conventional graphing app, it will only show a single dot at (0, 0). But in FuzzyGraph, it looks like a fuzzy particle or something.&lt;/p&gt;
    &lt;p&gt;But now, let's invert this to get the "Black Hole Equation": \( \frac{1}{x^2+y^2} = 0 \)...&lt;/p&gt;
    &lt;p&gt;In this case, there is absolutely nothing to show on a conventional graph, as there are actual solutions to this equations. However, there is still a mathematical topography which can be visualized (as can be seen in the fuzzy graph).&lt;/p&gt;
    &lt;p&gt;Not all of the Shadows are like black holes.&lt;/p&gt;
    &lt;p&gt;In this example, let's start by combining 2 lines together: \(y=x\) and \(y=-x\).&lt;/p&gt;
    &lt;p&gt;We can visually add 2 equations together by refactoring them so they are both equal to 0, and then multiplying the two refactored equations together. \(y=x\) can be changed to \(y-x=0\), and \(y=-x\) can be refactored to \(y+x=0\).&lt;/p&gt;
    &lt;p&gt;We can then combine 2 into a single equation these like this: \( (y-x) \times (y+x) = 0 \)&lt;/p&gt;
    &lt;p&gt;And now, let's invert one of the equations using division: \( \frac{x-y}{x+y} = 0 \)&lt;/p&gt;
    &lt;p&gt;So as you can see, the line that was inverted (under the division line) is now a Shadow Line. And this seems like a more "correct" way to visualize this than as the conventional graph shows it (which is indistinguishable from the simpler equation, \(y-x=0\)).&lt;/p&gt;
    &lt;p&gt;This equation works almost exactly as the previous. And like before, let's start with multiplication to combine 2 equations (in this case, a circle and a vertical line equation): \( x \times (x^2+y^2-1) = 0 \).&lt;/p&gt;
    &lt;p&gt;But now, let's invert the circle by using division, which makes the equation: \( \frac{x}{x^2+y^2-1} = 0 \).&lt;/p&gt;
    &lt;p&gt;Note that the Shadow Circle is invisible in the conventional graph. In fact, the conventional graph looks identical to a conventional graph of the \(x=0\) equation (as if the denominator was not there).&lt;/p&gt;
    &lt;p&gt;In all of these previous examples, the "shadows" have represented areas of high error. But in this last example, we'll see some hidden details that represent areas of low error - areas that are nearly solutions to the equation.&lt;/p&gt;
    &lt;p&gt;Consider the equation, \( y=4 sin(x)+ sin(2.7y) \), as both a conventional graph and a fuzzy graph:&lt;/p&gt;
    &lt;p&gt;Note the floating dots in the fuzzy graph version that are not there in the conventional/binary graph. These are like underwater islands - underwater mountains that are just below the surface of the water (or in this case, the \( error == 0 \) surface). These hidden islands represent area that are near-solutions to the equation (which are only visible in FuzzyGraph).&lt;/p&gt;
    &lt;p&gt;Their presense hints that we can tweak the equation slightly to cause them to burst above the surface of the water (which should also make them visible in conventional graphs).&lt;/p&gt;
    &lt;p&gt;So let's change the equation from: &lt;lb/&gt; \( y=4 sin(x)+ sin(2.7y) \) to: &lt;lb/&gt; \( y=4 sin(x)+ sin(2.8y) \)...&lt;/p&gt;
    &lt;p&gt;And as you can see, those previously-hidden islands are now visible in the conventional graph.&lt;/p&gt;
    &lt;p&gt;So Fuzzy/non-binary graphing can help us see features of the mathematical topography that are completely invisible with conventional/binary.&lt;/p&gt;
    &lt;p&gt;Date published: 2025-11-05&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gods.art/articles/equation_shadows.html"/><published>2025-11-05T14:21:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45823186</id><title>Carice TC2 ‚Äì A non-digital electric car</title><updated>2025-11-05T21:34:47.645928+00:00</updated><content>&lt;doc fingerprint="e68401540d9d7dbb"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;the 100% electric Carice TC2&lt;/head&gt;
    &lt;head rend="h1"&gt;a real retro head-turner&lt;/head&gt;
    &lt;p&gt;Reserve your spot now on next year‚Äôs production batch. Spaces limited.&lt;/p&gt;
    &lt;head rend="h1"&gt;the 100% electric carice TC2&lt;/head&gt;
    &lt;head rend="h2"&gt;a real retro head-turner&lt;/head&gt;
    &lt;p&gt;Reserve your spot now on next year‚Äôs production batch. Spaces limited.&lt;/p&gt;
    &lt;head rend="h6"&gt;The new experience&lt;/head&gt;
    &lt;head rend="h2"&gt;carice TC2&lt;/head&gt;
    &lt;p&gt;Meet the all-new electric Carice TC2: extremely lightweight, dynamic and elegant. When you get into your Carice, you escape and for a moment you forget about the everyday hassle. Whether you are the one driving the Carice or the passenger being driven around, it will be hard to hide that smile. With a TC2 you own something extraordinary, a piece of art.&lt;/p&gt;
    &lt;head rend="h6"&gt;The new experience&lt;/head&gt;
    &lt;head rend="h2"&gt;Carice TC2&lt;/head&gt;
    &lt;p&gt;We are busy with anything and everything, all the time. When you get into your Carice, you escape and for a moment you forget about the everyday hassle. You just relax and enjoy the drive. Whether you are the one driving the Carice or the passenger being driven around, it will be hard to hide that smile. The TC2 is a piece of art, just for you to enjoy.&lt;/p&gt;
    &lt;head rend="h6"&gt;why A Carice&lt;/head&gt;
    &lt;head rend="h2"&gt;the ultimate freedom&lt;/head&gt;
    &lt;p&gt;The result of years of hard work and dedication is the striking Carice TC2: it is the ultimate car to enjoy that sunny day in style and enjoy your drive and unwind.&lt;/p&gt;
    &lt;head rend="h5"&gt;all electric&lt;/head&gt;
    &lt;p&gt;The Carice TC2 is fully electric and has no emissions. This car is built to be fun for everybody ‚Äì not just the driver. It is our mission to combine 21st-century technology with the look and feel of the cars of the past.&lt;/p&gt;
    &lt;head rend="h5"&gt;the essence&lt;/head&gt;
    &lt;p&gt;If you just take away unnecessary things for long enough, you will get back to the essence of driving. The Carice TC2 is elegant, stylish and at the same time uncomplicated. This delivers electric driving in its most pure and elementary form.&lt;/p&gt;
    &lt;head rend="h5"&gt;featherlight&lt;/head&gt;
    &lt;p&gt;Because the Carice TC2 is available from 590 kg including battery pack, it handles exceptionally dynamic yet comfortable. Moreover, power consumption is very low due to this weight. Therefore, the TC2 delivers a driving experience like no other. Very compact, yet big enough!&lt;/p&gt;
    &lt;head rend="h3"&gt;‚Äì time to forget about time ‚Äì&lt;/head&gt;
    &lt;head rend="h6"&gt;Seen on&lt;/head&gt;
    &lt;head rend="h6"&gt;Seen on&lt;/head&gt;
    &lt;head rend="h6"&gt;About us&lt;/head&gt;
    &lt;head rend="h2"&gt;carice craftsmanship&lt;/head&gt;
    &lt;p&gt;Built and designed from the ground up in the Netherlands by people with a lifelong love of classic cars, the TC2 is made to resemble the playful and elegant looks of every car that you loved as a kid. This passion for cars translates into a high level of attention to detail and commitment to meet your needs. Carice is expanding their extensive history in automotive design and development every day. Find out about our latest events and achievements here.&lt;/p&gt;
    &lt;head rend="h6"&gt;gallery&lt;/head&gt;
    &lt;head rend="h2"&gt;modern classic&lt;/head&gt;
    &lt;p&gt;From the eye-catching dashboard, the classically styled steering wheel to the matching upholstery: everything in a Carice TC2 is made to stand out in all its simplicity. With a Carice you don‚Äôt just own another car: you get something extraordinary, a piece of art.&lt;/p&gt;
    &lt;head rend="h6"&gt;configure&lt;/head&gt;
    &lt;head rend="h2"&gt;configure your carice&lt;/head&gt;
    &lt;p&gt;To personalize your TC2, you can choose from a wide range of different colors for the paint, upholstery and rooftop. There is always a combination that fits your style.&lt;/p&gt;
    &lt;head rend="h2"&gt;specifications&lt;/head&gt;
    &lt;p&gt;There is no better way to experience the Carice TC2 than by seeing it and driving it. The elegant lines, attention to detail and phenomenal handling can‚Äôt be captured in a list, but some features can. You can find them below.&lt;/p&gt;
    &lt;head rend="h5"&gt;sizes and masses&lt;/head&gt;
    &lt;head rend="h5"&gt;battery&lt;/head&gt;
    &lt;head rend="h5"&gt;other&lt;/head&gt;
    &lt;p&gt;* Some specifications may differ, depending on the individual configuration of the TC2&lt;/p&gt;
    &lt;head rend="h6"&gt;contact&lt;/head&gt;
    &lt;head rend="h2"&gt;send us a message&lt;/head&gt;
    &lt;head rend="h6"&gt;FAQ&lt;/head&gt;
    &lt;head rend="h2"&gt;frequently asked questions&lt;/head&gt;
    &lt;head rend="h5"&gt;when can i order my Carice TC2?&lt;/head&gt;
    &lt;p&gt;You can already order your Carice. If you are interested you can contact us via the links on the website and the contact form to register your interest or book a test drive.&lt;/p&gt;
    &lt;head rend="h5"&gt;is the Carice TC2 a new car?&lt;/head&gt;
    &lt;p&gt;Yes! We have been designing and developing the TC2 ourselves from the ground up, and are now manufacturing the first series of TC2‚Äôs in the Netherlands. After more than 10 years of developing, testing and optimizing an extremely lightweight chassis around our electric drivetrain, you can now get a phenomenal handling and elegant TC2 yourself and enjoy driving in its most elementary form.&lt;/p&gt;
    &lt;head rend="h5"&gt;what is the price of a Carice TC2?&lt;/head&gt;
    &lt;p&gt;Prices for a TC2 start at ‚Ç¨44.500 excluding taxes (‚Ç¨53.854 including 21% btw/Dutch tax).&lt;/p&gt;
    &lt;head rend="h5"&gt;in what countries can i drive the Carice?&lt;/head&gt;
    &lt;p&gt;The Carice TC2 complies with the European regulations and can therefore be driven in all EU countries and countries that adopt those regulations, like Switzerland, the United Kingdom, Monaco and Norway.&lt;/p&gt;
    &lt;head rend="h5"&gt;what is the estimated delivery time?&lt;/head&gt;
    &lt;p&gt;At the moment we are making the TC2 exclusively on order, as every car is configured differently. We can provide you with an estimation on the delivery time and you can reserve a spot on the production list by placing an order.&lt;/p&gt;
    &lt;head rend="h5"&gt;how can i configure my Carice?&lt;/head&gt;
    &lt;p&gt;There are a lot of options for you to choose between. Different colors, wheels, upholstery, soft top, accessories, battery pack, charging gear and so on. If you are interested in buying a Carice TC2, please get in touch.&lt;/p&gt;
    &lt;head rend="h5"&gt;what is the range of a Carice TC2&lt;/head&gt;
    &lt;p&gt;Depending on the configuration of your TC2, you can drive more than 300km, which can bring you to the most beautiful places.&lt;/p&gt;
    &lt;head rend="h5"&gt;i have decided: i want one soon! how to proceed?&lt;/head&gt;
    &lt;p&gt;The current production batch is sold out, but there are a few cars left for the next production run. If you are sure you want one, you can secure one of these cars by paying a deposit of 75% of the purchasing price. Please contact us for the details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.caricecars.com/"/><published>2025-11-05T14:25:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45823234</id><title>Ask HN: My family business runs on a 1993-era text-based-UI (TUI). Anybody else?</title><updated>2025-11-05T21:34:46.967055+00:00</updated><content>&lt;doc fingerprint="3ec6db1c2cb327ac"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Is anybody still using TUI applications for business?&lt;/p&gt;
      &lt;p&gt;My family company is a wholesale distribution firm (with lightweight manufacturing) and has been using the same TUI application (on prem unix box) since 1993. We use it for customer management, ordering, invoicing, kit management/build tickets, financials - everything. We've transitioned from green screen terminals to modern emulators, but the core system remains. I spent many summers running serial and ethernet cables.&lt;/p&gt;
      &lt;p&gt;I left the business years ago to become a full time software engineer, but I got my start as a script kiddie writing automations for this system with Microsoft Access, VBA, and SendKeys to automate data entry. Amazingly, they still have a Windows XP machine running many of those tasks I wrote back in 2004! It's brittle, but cumulatively has probably saved years of time. That XP machine could survive a nuclear winter lol.&lt;/p&gt;
      &lt;p&gt;I recently stepped back in to help my parents and spent a day converting many of those old scripts to a more modern system (with actual error-handling instead of strategic sleep()s and prayers) using Python and telnetlib3. I had a blast and still love this application. I can fly around in it. Training new people was always a pain, but for those that got it‚Äîthey had super powers.&lt;/p&gt;
      &lt;p&gt;This got me thinking: Are other companies still using this type of interface to drive their core operations? I‚Äôm reflecting on whether the only reason my family's business still uses this system is because of the efficiency hacks I put in place 20+ years ago. Without them, would they have been forced to switch to a modern cloud/GUI system? I‚Äôm not sure if I‚Äôm blinded by nostalgia or if this application is truly as wonderful as I remember it.&lt;/p&gt;
      &lt;p&gt;I‚Äôd love to hear if and how these are still being utilized in the real world.&lt;/p&gt;
      &lt;p&gt;P.S. The system we use was originally sold by ADP and has had different names (D2K, Prophet21). I believe Epicor owns it now (Activant before).&lt;/p&gt;
      &lt;p&gt;P.P.S. Is anybody migrating their old TUI automation scripts to a more modern framework or creating new ones? I‚Äôm super curious to compare notes and see what other people are doing.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45823234"/><published>2025-11-05T14:29:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45823831</id><title>Ruby and Its Neighbors: Smalltalk</title><updated>2025-11-05T21:34:46.827556+00:00</updated><content>&lt;doc fingerprint="34329b3385e7083a"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Ruby And Its Neighbors: Smalltalk&lt;/head&gt;
    &lt;p&gt;Last time, we talked about Perl as an influence on Ruby, this time, we‚Äôll talk about the other major influence on Ruby: Smalltalk.&lt;/p&gt;
    &lt;p&gt;Smalltalk had a different kind of influence, since almost nothing of Smalltalk‚Äôs syntax made into Ruby. But many of the details of how objects work are directly inspired by Smalltalk, including the idea that every piece of data is part of the object system.&lt;/p&gt;
    &lt;p&gt;Also unlike Perl, I spent a good couple of years working in Smalltalk, and it is one of my favorite languages that I‚Äôll never likely use in anger again.&lt;/p&gt;
    &lt;head rend="h2"&gt;(A Personal) History of Smalltalk&lt;/head&gt;
    &lt;p&gt;Smalltalk originated in the same Xerox PARC team that invented the windowed interface, ethernet, and the laser printer, and who knows what else, they may have invented ice cream and rainbows.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a whole story about what project Smalltalk was invented to be a part of, and a whole alternate history of computing and how people interact with computers that we are going to largely ignore. (If you are interested, start by searching for ‚ÄúAlan Kay Dynabook‚Äù.)&lt;/p&gt;
    &lt;p&gt;Smalltalk went through a few different iterations in the 1970s, but the version that we know today is a direct descendent of Smalltalk-80, which was the first version released to the wider world.&lt;/p&gt;
    &lt;p&gt;For most of the 80s and 90s, Smalltalk was something that doesn‚Äôt really exist today ‚Äì a programming language and environment that companies paid money to use. Lots of money. The major player was ParcPlace, which was a spinoff of Xerox that provided Smalltalk tools. Their commercial product was originally called ObjectWorks, later changed to VisualWorks, and eventually sold off and presumably slowly losing customers after the late 90s.&lt;/p&gt;
    &lt;p&gt;Smalltalk was pretty big in the industry for a while. Most of the aviation industry ran on it in the 90s, the big payroll project that was the basis for Extreme Programming was a Smalltalk project, there was reasonably high demand for Smalltalk programmers through at least the mid 1990s. I taught an undergrad OO class in Smalltalk in 1997 and 1998 to students that wanted to be learning C++, and I remember telling them that Smalltalk programmers were paid more.&lt;/p&gt;
    &lt;p&gt;I first encountered Smalltalk as a grad student in about 1993, where Georgia Tech used ObjectWorks to teach Smalltalk and Object-Oriented programming (there‚Äôs a whole other sidebar about how Object-Oriented languages came to prominence in the 90s, and the arguments over that but again, another day). ObjectWorks was pricey, and there was also a lower-cost vendor called Digitalk, and eventually I also used a product called Smalltalk Agents, which has apparently totally vanished from the entire internet.&lt;/p&gt;
    &lt;p&gt;In 1995, a bunch of the original Xerox Smalltalk team was together at Apple, and they decided to release an open-source Smalltalk VM. What they did was very interesting. They wrote a very, very small kernel in very vanilla C, and then 95% of the environment was then built in Smalltalk on top of that. Oh, and even the vanilla C was written in Smalltalk, they wrote a Smalltalk to C compiler. They called their new Smalltalk ‚ÄúSqueak‚Äù, which made a lot more sense when they all moved en masse to Disney.&lt;/p&gt;
    &lt;p&gt;The fact that Squeak was largely written in itself made it fairly easy to port to new systems, and it was quickly available on just about anything with a microchip.&lt;/p&gt;
    &lt;p&gt;I‚Äôm pretty sure that I first saw Squeak at the OOPSLA conference in 1997. (Object-Oriented Programming, Systems, Languages &amp;amp; Applications, since you asked) At this conference I somehow got to do a team-building exercise with Adelde Goldberg from the original Xerox PARC team, which is not relevant to anything but seemed very cool at the time. I was already using Smalltalk in my projects, but Squeak was immediately interesting and my extended research team started doing cool stuff. Like, what I‚Äôm pretty sure was the first Wiki tool outside the original C2 Wiki, was written in Squeak. (Apparently at least one is still running).&lt;/p&gt;
    &lt;head rend="h2"&gt;Smalltalk‚Äôs Environment&lt;/head&gt;
    &lt;p&gt;It‚Äôs important to understand that Smalltalk‚Äôs development is a different evolutionary tree from nearly every currently popular programming language, in that Smalltalk is in no way, shape, or form influenced by Unix or C. Perl, Ruby, Python, JavaScript, Swift, Kotlin and on and on, all come from a universe where they expect to run Unix libraries, and where C syntax is normal. The Unix philosophy of ‚Äúsmall pieces, loosely joined‚Äù is not a part of Smalltalk‚Äôs DNA at all.&lt;/p&gt;
    &lt;p&gt;Smalltalk is basically its own operating system, and the syntax is different from C-style languages in ways big and small. For example, the first element of an array is‚Ä¶ 1. Which, when you think about how people count, actually makes sense.&lt;/p&gt;
    &lt;p&gt;It‚Äôs hard to separate Smalltalk the language from Smalltalk the environment, although I suppose technically you could have the language without the whole shebang (and I think there was a GNU Smalltalk that tried this), really the environment is part of the appeal.&lt;/p&gt;
    &lt;p&gt;Your main interfaces to the smalltalk system are a Workspace and a Browser. A workspace is analogous to REPL session, you can type in arbitrary Smalltalk code and have the system ‚Äúdo it‚Äù to execute the code, ‚Äúprint it‚Äù to execute the code and output the result. There are some other actions like ‚Äúdebug it‚Äù or ‚Äúinspect it‚Äù, but that‚Äôs the basic idea. Unlike a Unix REPL, there‚Äôs no prompt, and you don‚Äôt automatically invoke code by hitting return, you have to select code and then invoke the menu item or the keyboard shortcut for the code you want to act on.&lt;/p&gt;
    &lt;p&gt;The Browser is where you write code. There a a few different versions in most Smalltalks, here‚Äôs the main one, this is from a modern Smalltalk called Cuis.&lt;/p&gt;
    &lt;p&gt;At the top, we have four window panes ‚Äì left to right we have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Categories ‚Äì groups of classes that are related in some way. Cuis nicely puts each group in a pulldown list. Categories have no particular syntactic meaning, they are just there to make browsing easier.&lt;/item&gt;
      &lt;item&gt;Classes ‚Äì one entry for each class in the currently selected category, at the bottom of this pane is a toggle for ‚Äúclass‚Äù vs. ‚Äúinstance‚Äù which determines what kinds of messages are shown in the next two panes.&lt;/item&gt;
      &lt;item&gt;Protocols ‚Äì a protocol is a user-defined group of messages. Smalltalk internally uses ‚Äúmessage‚Äù rather than ‚Äúmethod‚Äù because of how Alan Kay thinks about objects. Again, protocols are for the programmer, not the system.&lt;/item&gt;
      &lt;item&gt;Messages ‚Äì each messages in the currently selected protocol is listed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The bottom pane is the code editor, and if a message is selected in the code pane, its code is displayed there.&lt;/p&gt;
    &lt;p&gt;You probably have questions:&lt;/p&gt;
    &lt;p&gt;Does this mean that you can see the source code for the entire Smalltalk system?&lt;/p&gt;
    &lt;p&gt;Yes, yes it does.&lt;/p&gt;
    &lt;p&gt;**Can you modify any code in the system? **&lt;/p&gt;
    &lt;p&gt;Yes, yes you can.&lt;/p&gt;
    &lt;p&gt;Even, like, deep system code?&lt;/p&gt;
    &lt;p&gt;Yes.&lt;/p&gt;
    &lt;p&gt;Isn‚Äôt that dangerous?&lt;/p&gt;
    &lt;p&gt;As a Ruby developer, you should know that it‚Äôs only as dangerous as the developers who use it.&lt;/p&gt;
    &lt;p&gt;How do you edit a message?&lt;/p&gt;
    &lt;p&gt;Just display the existing message in the browser, edit the message and select ‚Äúsave‚Äù. The Smalltalk system will parse the code, stop if there are syntax errors, but if not, the updated method will be saved to the system. A side effect is you can‚Äôt save code that isn‚Äôt syntactically parsable, even as a draft.&lt;/p&gt;
    &lt;p&gt;Okay, but how do you create a message?&lt;/p&gt;
    &lt;p&gt;The ‚Äúreal‚Äù way is to select a protocol but not a message, and Smalltalk will put a template in the edit window. Write your message in the editor and save it. Alternately, you can just change the name of a message in the edit window, and a new method with that name will be created, without deleting the old message.&lt;/p&gt;
    &lt;p&gt;And how do you create a class?&lt;/p&gt;
    &lt;p&gt;Similarly.&lt;/p&gt;
    &lt;p&gt;If you select a category and not a class, you‚Äôll get this in the code editor pane:&lt;/p&gt;
    &lt;code&gt;Object subclass: #NameOfSubclass
	instanceVariableNames: ''
	classVariableNames: ''
	poolDictionaries: ''
	category: 'Kernel-Chronology'
&lt;/code&gt;
    &lt;p&gt;The thing to note is that this is not actually template, it‚Äôs actually code: a message, waiting for you to fill in the arguments, replacing &lt;code&gt;#NameOfSubclass&lt;/code&gt; and adding the instance variables and so on. You don‚Äôt save this, you ‚Äúdo it‚Äù, just like if you were in a workspace. The message call is evaluated, and Smalltalk creates a new class.&lt;/p&gt;
    &lt;p&gt;But wait, if all the code is in the image and isn‚Äôt in text files, how do people work together and share code?&lt;/p&gt;
    &lt;p&gt;Don‚Äôt worry about it.&lt;/p&gt;
    &lt;p&gt;Seriously, though, worry about it.&lt;/p&gt;
    &lt;p&gt;This has always been a problem. Smalltalk allows you to share ‚Äúchange sets‚Äù, effectively the code differences between one point and another. Classically, one person would export their change set, and other team members would import it. Different Smalltalks have built up more sophisticated tools over time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Smalltalk‚Äôs Syntax&lt;/head&gt;
    &lt;p&gt;Smalltalk‚Äôs syntax is very simple, relative to Ruby and Perl.&lt;/p&gt;
    &lt;p&gt;Wait a sec, I literally wrote this for a chapter in a book about Smalltalk literally 25 years ago, here‚Äôs a slight paraphrase:&lt;/p&gt;
    &lt;p&gt;Every line of Smalltalk is evaluated the same way.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Every variable is an object. There are no basic types that are not objects.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Every expression is a message being passed to an object, there is basically no expression syntax that is not a message.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;All messages return a value. (The return value is specified by&lt;/p&gt;&lt;code&gt;^&lt;/code&gt;, if the method does not specify a return value, it implicitly returns&lt;code&gt;self&lt;/code&gt;, the instance that received the message.)&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There are three kinds of messages:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Unary messages like &lt;code&gt;3 negated&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;Binary messages like &lt;code&gt;a + b&lt;/code&gt;, these actually are messages you can define, there is a small set of them, and they are special cases in the parser.&lt;/item&gt;
          &lt;item&gt;Keyword messages such as &lt;code&gt;anArray at: 3 put: 7&lt;/code&gt;. This syntax got used by ObjectiveC and later Swift, so you may be familiar with it. It‚Äôs&lt;code&gt;receiver &amp;lt;messagepart&amp;gt;: &amp;lt;argument&amp;gt;&lt;/code&gt;where you can have multiple message parts. If you are referring to the message, typically you just say the message parts, so this message would be called&lt;code&gt;at:put:&lt;/code&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Unary messages like &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smalltalk does not have operator precedence. All code is evaluated strictly from left to right. Unary messages first, binary messages second, keyword messages last. Parenthesis can be used to force order of operations or to make things clearer.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The assignment operator is&lt;/p&gt;&lt;code&gt;:=&lt;/code&gt;(Smalltalk uses&lt;code&gt;=&lt;/code&gt;for boolean equality), the right hand side is evaluated and the value is assigned to the result of the left hand side.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And that‚Äôs basically it, with a couple of ways to create literals like strings, arrays, dictionaries, local variables, and blocks.&lt;/p&gt;
    &lt;p&gt;So, for 10 points and control of the board, what does this do?&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;hypotenuse := 3 squared + 4 squared sqrt&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The unary messages are evaluated first:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;hypotenuse := 9 + 16 sqrt&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;There‚Äôs still a unary message&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;hypotenuse := 9 + 4&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Now we can do the binary message:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;hypotenuse = 13&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Oops.&lt;/p&gt;
    &lt;p&gt;To get what you actually want, you need parentheses:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;hypotenuse := (3 squared + 4 squared) sqrt&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Smalltalk does not have special syntax for loops, all loop behavior is defined by methods on &lt;code&gt;Array&lt;/code&gt; and the like, very similar to Ruby‚Äôs &lt;code&gt;Enumerable&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Smalltalk does not have special syntax to create messages or classes. Message creation is managed by the editor (which internally calls a message that adds the new code), class creation is just another method ‚Äì in Squeak, that method is &lt;code&gt;Object#subclass:instanceVariableNames:classVariableNames:poolDictionaries:category:&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Smalltalk does not have special syntax for boolean logic, all logic behavior is defined by the classes &lt;code&gt;True&lt;/code&gt; and &lt;code&gt;False&lt;/code&gt;. Ruby sort of does this, but Ruby does have &lt;code&gt;if&lt;/code&gt; as special syntax. Smalltalk does not, you‚Äôd write a Smalltalk conditional as just another message:&lt;/p&gt;
    &lt;code&gt;(x &amp;gt; 10) ifTrue: [ x squared ] ifFalse: [ x sqrt ]
&lt;/code&gt;
    &lt;p&gt;The square brackets are blocks, and behave very similar to Ruby blocks, except that you can treat them as just normal variables and normal arguments. You can even, as in this case, have multiple arguments that take blocks.&lt;/p&gt;
    &lt;p&gt;The implementation if the method &lt;code&gt;ifTrue:ifFalse&lt;/code&gt; is simple. For the &lt;code&gt;True&lt;/code&gt; class, it just takes the true block and executes it by passing it the message &lt;code&gt;value&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;ifTrue: trueAlternativeBlock ifFalse: falseAlternativeBlock
	^trueAlternativeBlock value
&lt;/code&gt;
    &lt;p&gt;And for the false class, the exact opposite:&lt;/p&gt;
    &lt;code&gt;ifTrue: trueAlternativeBlock ifFalse: falseAlternativeBlock
	^falseAlternativeBlock value
&lt;/code&gt;
    &lt;p&gt;Smalltalk doesn‚Äôt have a &lt;code&gt;case&lt;/code&gt; or &lt;code&gt;switch&lt;/code&gt; statement, typically if you want behavior like that you‚Äôd define a dictionary of keys to blocks or you would use the object system and polymorphism and double dispatch.&lt;/p&gt;
    &lt;head rend="h2"&gt;Smalltalk‚Äôs Object Model&lt;/head&gt;
    &lt;p&gt;There‚Äôs a lot about Smalltalk‚Äôs object model that sound familiar to a Ruby developer:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There‚Äôs a base class called &lt;code&gt;Object&lt;/code&gt;that everything inherits from.&lt;/item&gt;
      &lt;item&gt;Instance variables are private. Getters and setters default to having the same name as the instance variable.&lt;/item&gt;
      &lt;item&gt;Method lookup happens at the point of the method call.&lt;/item&gt;
      &lt;item&gt;Classes are instances of the class &lt;code&gt;Class&lt;/code&gt;(sort of).&lt;/item&gt;
      &lt;item&gt;There‚Äôs a thing called a ‚ÄúMetaclass‚Äù&lt;/item&gt;
      &lt;item&gt;There‚Äôs a method that‚Äôs the method of last resort ‚Äì in Ruby, it‚Äôs &lt;code&gt;method_missing&lt;/code&gt;, but in Smalltalk it‚Äôs called&lt;code&gt;doesNotUnderstand&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are a couple of differences as well&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Smalltalk‚Äôs meta classes are structured differently, I explained this once and I‚Äôm not sure I ever want to explain it again.&lt;/item&gt;
      &lt;item&gt;Smalltalk doesn‚Äôt have multiple inheritance or mixins or modules or anything like that. Although there have been some attempts to add these features, the traditional Smalltalk way to do this is through delegation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But overall, Smalltalk and Ruby are similar enough that a huge amount of Kent Beck‚Äôs Smalltalk Best Practice Patterns is applicable to Ruby as long as you translate the syntax.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Happened?&lt;/head&gt;
    &lt;p&gt;Unlike Perl, I actually did use Smallalk to build a few real applications that had real users. I miss it a lot.&lt;/p&gt;
    &lt;p&gt;I find that when I try to explain Smalltalk to people, it‚Äôs easy to explain the syntax and the object model. What‚Äôs hard to explain is how it is to work in a Smalltalk environment.&lt;/p&gt;
    &lt;p&gt;You‚Äôve likely used powerful coding editors and terminals. Smalltalk is just different. You are in the running environment.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tests start instantly, and in general run very fast. There‚Äôs a dedicated test runner window. Some smalltalk integrate tests with the regular browser, so you can see test status from the code browsers.&lt;/item&gt;
      &lt;item&gt;Debugging is amazing, you can investigate the state of any object in the system, you can change that state, you can easily execute arbitrary code. You can have a test halt on exception, update the code and re-run from the point failure. It‚Äôs hard to describe how fluid it is, especially since I‚Äôm no longer expert enough to do it fluently.&lt;/item&gt;
      &lt;item&gt;While the editor doesn‚Äôt have all the niceties of the IDE‚Äôs you are used to, it‚Äôs very powerful in its own way. If you save code with a message name that does not appear in the image at all, Smalltalk will typically ask you if you want to define it right there. A lot of the things we ask a Language Server to do, Smalltalk just kind of does, because the image has access to everything.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the all-encompassing nature of the environment was also Smalltalk‚Äôs downfall. As more and more of the general computing environment became Unix and the ‚Äúsmall pieces loosely joined‚Äù philosophy, Smalltalk got harder and harder to integrate. Smalltalk isn‚Äôt a scripting language, it was late to develop connectivity to external databases, its model of team interaction is fundamentally different from Unix source control. The image-based system has some drawbacks ‚Äì you do get amazing access to the system, but it can be hard to tell where your code ends and the system begins. Code could depend on the state of the image in ways that were hard to replicate in deploys.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Did Ruby Take From Smalltalk?&lt;/head&gt;
    &lt;p&gt;Smalltalk‚Äôs legacy in Ruby is primarily the object model ‚Äì the idea that everything is an object and everything is manageable via method calls, and that message calls are evaluated at the point of call, as late as possible. Ruby takes that idea and translates it into a syntax that is more familiar to programmers used to C/Perl/Java.&lt;/p&gt;
    &lt;p&gt;I‚Äôm not sure this is exactly on point as far as Smalltalk‚Äôs influence on Ruby, but my Ruby style has always been very aggressive about creating new classes and objects. I‚Äôm quite confident that a reason for that style is that I came from Smalltalk first and not Java, Smalltalk style is much more amenable to small classes.&lt;/p&gt;
    &lt;p&gt;On my first largish Smalltalk project, users were simulating a chemical plant‚Äôs pipe system by placing tiles with pipes in them, and I frequently needed to do logic based on relative directions. I clearly remember creating a &lt;code&gt;Direction&lt;/code&gt; class with basically four live instances, &lt;code&gt;up&lt;/code&gt;, &lt;code&gt;down&lt;/code&gt;, &lt;code&gt;left&lt;/code&gt;, &lt;code&gt;right&lt;/code&gt;, and just enough logic inside to say that &lt;code&gt;up.turn_left&lt;/code&gt; equals &lt;code&gt;left&lt;/code&gt;, but &lt;code&gt;down.turn_left&lt;/code&gt; equals &lt;code&gt;right&lt;/code&gt;. It was useful enough that I remember how much fun it was to build it even  now, nearly thirty years later.&lt;/p&gt;
    &lt;p&gt;Of all the other programming languages I‚Äôve used, Ruby is the one that most clearly encourages that style of coding.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://noelrappin.com/blog/2025/11/ruby-and-its-neighbors-smalltalk/"/><published>2025-11-05T15:24:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45824658</id><title>Norway reviews cybersecurity after remote-access feature found in Chinese buses</title><updated>2025-11-05T21:34:44.996127+00:00</updated><content>&lt;doc fingerprint="fcf890b619964575"&gt;
  &lt;main&gt;
    &lt;p&gt;Norway has launched a cybersecurity review after public transport operator Ruter discovered that electric buses supplied by Chinese manufacturer Yutong contained hidden SIM cards enabling potential remote access.&lt;/p&gt;
    &lt;p&gt;According to Ruter, internal tests at a secure facility found Romanian SIM cards inside the buses, theoretically allowing the Chinese supplier to shut down vehicles or interfere via software updates. The transport operator stressed there is no evidence of misuse but said the discovery moves concerns ‚Äúfrom suspicion to concrete knowledge‚Äù.&lt;/p&gt;
    &lt;p&gt;Ruter has removed the SIM cards and is strengthening procurement rules, internal firewalls, and cloud-security requirements to ensure full local control over transport operations.&lt;/p&gt;
    &lt;p&gt;Norway‚Äôs Minister of Transport Jon-Ivar Nyg√•rd told national broadcaster NRK that the government is assessing supplier risks from countries outside Norway‚Äôs security alliances, noting the need to protect critical infrastructure.&lt;/p&gt;
    &lt;p&gt;Around 1,300 electric buses operate in Norway, including approximately 850 units from Yutong, with 300 running in Oslo and Akershus. Ruter said the likelihood of attempted interference remains low, but the situation underscores the growing cybersecurity challenges linked to foreign technology suppliers.&lt;/p&gt;
    &lt;p&gt;The case comes as Chinese electric buses are increasingly adopted across global markets, including Southeast Asia, raising wider questions about digital security and strategic dependencies in public transport systems.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs unlikely these buses would ever be misused,‚Äù Ruter CEO Bernt Reitan Jenssen said, ‚Äúbut we must take the risk seriously.‚Äù&lt;/p&gt;
    &lt;p&gt;Source: Carscoops&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://scandasia.com/norway-reviews-cybersecurity-after-hidden-remote-access-feature-found-in-chinese-buses/"/><published>2025-11-05T16:18:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45824864</id><title>Why aren't smart people happier?</title><updated>2025-11-05T21:34:44.653354+00:00</updated><content>&lt;doc fingerprint="2b2a121caf818e99"&gt;
  &lt;main&gt;
    &lt;p&gt;Adam Mastroianni is the author of Experimental History. He studies how people perceive and misperceive their social worlds. His work has been featured in Science, Nature, and The Tonight Show with Jimmy Fallon. He has a PhD in psychology from Harvard and a certificate of completion from 137 different escape rooms. He‚Äôs originally from Monroeville, Ohio (pop. 1,400) and currently lives in New York City.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a definition of intelligence that lots of psychologists can get behind:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Intelligence is a very general mental capability that, among other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience. It is not merely book learning, a narrow academic skill, or test-taking smarts. Rather, it reflects a broader and deeper capability for comprehending our surroundings-‚Äúcatching on,‚Äù ‚Äúmaking sense‚Äù of things, or ‚Äúfiguring out‚Äù what to do [‚Ä¶] Intelligence, so defined, can be measured, and intelligence tests measure it well.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Intelligence sounds pretty great. Who doesn‚Äôt want to ‚Äúcatch on‚Äù and ‚Äúmake sense‚Äù? Hell, ‚Äúfiguring out‚Äù what to do is pretty much all of life!&lt;/p&gt;
    &lt;p&gt;Naturally, people with more of this mental horsepower must live happier lives. When they encounter a problem, they should use their superior problem-solving ability to solve it. Smarter people should do a better job making plans and getting what they want, and they should learn more from their mistakes and subsequently make fewer of them. All of this should add up to a life that makes smart people go ‚Äúthis life rules!‚Äù&lt;/p&gt;
    &lt;p&gt;So smarter people are happier, right?&lt;/p&gt;
    &lt;p&gt;Well, this meta-analysis says no. Another says maybe a teeny tiny bit. This large, nationally-representative study from the UK finds that people who score the lowest on an intelligence test are a little less happy than everyone else, but that‚Äôs pretty much it.&lt;/p&gt;
    &lt;p&gt;I also pulled data from the General Social Survey, which includes (a) a short vocabulary test that seems to correlate reasonably well with longer intelligence tests (you can try it here), and (b) a simple measure of happiness: ‚ÄúTaken all together, how would you say things are these days‚Äîwould you say that you are very happy, pretty happy, or not too happy?‚Äù Across 50 years of data and 30,346 people, the folks who scored higher on the vocab test were a tiny bit less happy (r = -.06, p &amp;lt; .001).&lt;/p&gt;
    &lt;head rend="h1"&gt;WHAT‚ÄôS GOING ON HERE?&lt;/head&gt;
    &lt;p&gt;Maybe our tests are bad. The psychological study of intelligence has a long, bleak history of racism and prejudice against poor people (‚Äúthree generations of imbeciles are enough‚Äù), so we should be skeptical coming in. Psychologists have been trying to construct bias-free tests for a long time, but it‚Äôs hard. Plus, people score higher on IQ tests when you pay them for performance, so what looks like a test of intelligence may in part be a test of how hard you‚Äôre willing to try.&lt;/p&gt;
    &lt;p&gt;But even if intelligence tests only measure something like ‚Äúability to succeed in an unfair society‚Äù or ‚Äúwillingness to try hard,‚Äù it only deepens the mystery. Shouldn‚Äôt those people end up with happier lives, however unfair that may be?&lt;/p&gt;
    &lt;p&gt;And the tests likely do tap something more than just privilege and effort. There‚Äôs plenty of skepticism toward intelligence tests in psychology, but even the biggest skeptics agree that IQ can predict things like how well you do in school and what kind of job you get, even accounting for all the criticisms. So why doesn‚Äôt it also predict living a life that you like?&lt;/p&gt;
    &lt;head rend="h1"&gt;SPEARING SPEARMAN&lt;/head&gt;
    &lt;p&gt;I think there‚Äôs one guy to blame for this big mystery, and his name is Charles Spearman.&lt;/p&gt;
    &lt;p&gt;Way back in 1904, Spearman noticed something weird: the same kids who did well in one subject in school tended to do well in other subjects, too. The correlations were never perfect, of course, but they were pretty darn high, even across subjects that seemed pretty different from each other, like French and math. How come?&lt;/p&gt;
    &lt;p&gt;Spearman figured there must be some general mental ability that humans use to solve all kinds of problems. He later wrote:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This continued tendency to success of the same person throughout all variations of both form and subject-matter‚Äîthat is to say throughout all conscious aspects of cognition whatever‚Äîappears only explicable by some factor lying deeper than the phenomena of consciousness.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Helpfully, he also drew us a picture:&lt;/p&gt;
    &lt;p&gt;This is, I think, exactly where everything went wrong with the study of intelligence for the next 119 years. It‚Äôs not that Spearman‚Äôs results were inaccurate‚Äîin fact, they‚Äôve been replicated over and over. At this point, pretty much every paper on intelligence has to start out like this review from 2006:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In the study of intelligence, one empirical phenomenon is well established: Test scores on cognitive tasks show a positive manifold, that is, they are invariably positively intercorrelated, albeit to varying degrees. This implies that people who score well on one cognitive test are likely to score well on other cognitive tests. The positive manifold is a robust phenomenon.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Spearman‚Äôs stats were sound, but his interpretation was wrong. He did not, as he claimed, observe a ‚Äúcontinued tendency to success throughout all variations of both form and subject-matter,‚Äù nor has anybody else. It merely looks as if we‚Äôve varied all the forms and the subject-matters because we have the wrong theory about what makes them different.&lt;/p&gt;
    &lt;p&gt;We think tests of math, vocabulary, French, music, etc. are all different because some are about words and others are about numbers and others are about sounds. But psychology, like all sciences, is all about discovering the differences between seemingly similar things, and discovering the similarities between seemingly different things. If psychologists ever had to march into battle, a good candidate for our crests may be the famous M√ºller-Lyer illusion, the two lines that look like they‚Äôre different lengths but aren‚Äôt:&lt;/p&gt;
    &lt;p&gt;Just like those lines, I think all of our various tests of intelligence aren‚Äôt as different as they seem. They‚Äôre all full of problems that have a few important things in common:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;There are stable relationships between the variables.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There‚Äôs no disagreement about whether the problems are problems, or whether they‚Äôve been solved.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There have clear boundaries; there is a finite amount of relevant information and possible actions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The problems are repeatable. Although the details may change, the process for solving the problems does not.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I think a good name for problems like these is well-defined. Well-defined problems can be very difficult, but they aren‚Äôt mystical. You can write down instructions for solving them. And you can put them on a test. In fact, standardized tests items must be well-defined problems, because they require indisputable answers. Matching a word to its synonym, finding the area of a trapezoid, putting pictures in the correct order‚Äîall common tasks on IQ tests‚Äîare well-defined problems.&lt;/p&gt;
    &lt;p&gt;Spearman was right that people differ in their ability to solve well-defined problems. But he was wrong that well-defined problems are the only kind of problems. ‚ÄúWhy can‚Äôt I find someone to spend my life with?‚Äù ‚ÄúShould I be a dentist or a dancer?‚Äù and ‚ÄúHow do I get my child to stop crying?‚Äù are all important but poorly defined problems. ‚ÄúHow can we all get along?‚Äù is not a multiple-choice question. Neither is ‚ÄúWhat do I do when my parents get old?‚Äù And getting better at rotating shapes or remembering state capitals is not going to help you solve them.&lt;/p&gt;
    &lt;p&gt;We all share some blame with Spearman, of course, because everybody talks about smarts as if they‚Äôre one thing. Google ‚Äúsmartest people in the world‚Äù and most of the results will be physicists, mathematicians, computer scientists, and chess masters. These are all difficult problems, but they are well-defined, and that makes it easy to rank people. The best chess player in the world is the one who can beat everybody else. The best mathematician is the one who can solve the problems that nobody else could solve. That makes it seem like the best chess players and mathematicians are not just the smartest in their fields, but the smartest in the whole world.&lt;/p&gt;
    &lt;head rend="h1"&gt;THE POORLY DEFINED PROBLEM OF BEING ALIVE&lt;/head&gt;
    &lt;p&gt;There is, unfortunately no good word for ‚Äúskill at solving poorly defined problems.‚Äù Insight, creativity, agency, self-knowledge‚Äîthey‚Äôre all part of it, but not all of it. Wisdom comes the closest, but it suggests a certain fustiness and grandeur, and poorly defined problems aren‚Äôt just dramatic questions like ‚Äúhow do you live a good life‚Äù; they‚Äôre also everyday questions like ‚Äúhow do you host a good party‚Äù and ‚Äúhow do you figure out what to do today.‚Äù&lt;/p&gt;
    &lt;p&gt;One way to spot people who are good at solving poorly defined problems is to look for people who feel good about their lives; ‚Äúhow do I live a life I like‚Äù is a humdinger of a poorly defined problem. The rules aren‚Äôt stable: what makes you happy may make me miserable. The boundaries aren‚Äôt clear: literally anything I do could make me more happy or less happy. The problems are not repeatable: what made me happy when I was 21 may not make me happy when I‚Äôm 31. Nobody else can be completely sure whether I‚Äôm happy or not, and sometimes I‚Äôm not even sure. In fact, some people might claim that I‚Äôm not really happy, no matter what I say, unless I accept Jesus into my heart or reach nirvana or fall in love‚Äîif I think I‚Äôm happy before all that, I‚Äôm simply mistaken about what happiness is!&lt;/p&gt;
    &lt;p&gt;This is why the people who score well on intelligence tests and win lots of chess games are no happier than the people who flunk the tests and lose at chess: well-defined and poorly defined problems require completely different problem-solving skills. Life ain‚Äôt chess! Nobody agrees on the rules, the pieces do whatever they want, and the board covers the whole globe, as well as the inside of your head and possibly several metaphysical planes as well.&lt;/p&gt;
    &lt;head rend="h1"&gt;IF YOU‚ÄôRE SO SMART, WHY ARE YOU SO DUMB?&lt;/head&gt;
    &lt;p&gt;Here‚Äôs another way of looking at it.&lt;/p&gt;
    &lt;p&gt;Say you want to test people‚Äôs math ability. You design a test, administer it to a bunch of people, do all your psychometrics, etc. You‚Äôre feeling pretty good about your math test. And then you find that some of the people who ace your test later say things like ‚Äútwo plus two is 19‚Äù and ‚Äú88 is the biggest number.‚Äù You‚Äôd feel pretty embarrassed about your math test because it‚Äôs clearly not measuring mathematical ability, if it‚Äôs measuring anything at all.&lt;/p&gt;
    &lt;p&gt;This is exactly the situation we‚Äôre in with tests that claim to measure people‚Äôs ‚Äúreasoning‚Äù and ‚Äúproblem-solving ability.‚Äù Christopher Langan, a guy who can score eye-popping numbers on IQ tests, believes that 9/11 was an inside job meant specifically to distract the public from his theories, and he claims that banks won‚Äôt give him a loan because he‚Äôs white. John Sununu supposedly has IQ of 176, but he still had to resign from being George H.W. Bush‚Äôs chief of staff because he flew to his dentist appointments using military jets. Bobby Fischer is one of the greatest chess players of all time, but he also claimed that Hitler was a good dude, the Holocaust didn‚Äôt happen, and ‚ÄúJews murder Christian children for their blood and they‚Äôre doing it even today.‚Äù Then there‚Äôs the ever-lengthening list of professors at elite universities who have been disciplined or dismissed for doing things like sexually harassing colleagues and students or completely making up data or hanging out with a known pedophile. These are supposed to be some of the smartest people in the world, endowed with exceptional problem-solving abilities. And yet they‚Äôre still unable to solve basic but poorly defined problems like ‚Äúmaintain a basic grip on reality‚Äù and ‚Äúbe a good person‚Äù and ‚Äúdon‚Äôt make any life-altering blunders.‚Äù&lt;/p&gt;
    &lt;head rend="h1"&gt;GAZE UPON OUR WORKS AND DESPAIR&lt;/head&gt;
    &lt;p&gt;And here‚Äôs another way of looking at it.&lt;/p&gt;
    &lt;p&gt;Over the last generation, we have solved tons of well-defined problems. We eradicated smallpox and polio. We landed on the moon. We built better cars, refrigerators, and televisions. We even got ~15 IQ points smarter! And how did our incredible success make us feel?&lt;/p&gt;
    &lt;p&gt;Well:&lt;/p&gt;
    &lt;p&gt;All that progress didn‚Äôt make us a bit happier. I think there‚Äôs an important lesson here: if solving a bunch of well-defined problems did not make our predecessors happier, it probably won‚Äôt make us happier, either. The barrier between you and everlasting bliss is probably not the size of your television, nor your ability to solve Raven‚Äôs Progressive Matrices.&lt;/p&gt;
    &lt;p&gt;(To be clear, I still think it‚Äôs good we did all this. Polio sucks and going to the moon is awesome.)&lt;/p&gt;
    &lt;p&gt;I wish we knew more about how to make that bright green line go up, but we just haven‚Äôt yet defined the problem of ‚Äúliving a happy life‚Äù. We know that if you‚Äôre starving, lonely, or in pain, you‚Äôll probably get happier if you get food, friends, and relief. After that, the returns diminish very quickly. You could read all the positive psychology you want, take the online version of The Science of Wellbeing (‚ÄùYale‚Äôs Most Popular Course Ever!‚Äù), read my post on hacking the hedonic treadmill, meditate, exercise, and keep a gratitude journal‚Äîand after all that, maybe you‚Äôll be a smidge happier. Whatever else you think will put a big, permanent smile on your face, you‚Äôre probably wrong.&lt;/p&gt;
    &lt;p&gt;So if you‚Äôre really looking for a transformative change in your happiness, you might be better off reading something ancient. The great thinkers of the distant past seemed obsessed with figuring out how to live good lives: Socrates, Plato, Aristotle, Epicurus, Buddha, Confucius, Jesus, Marcus Aurelius, St. Augustine, even up through Thoreau and Vivekananda. But at some point, this kind of stuff apparently fell out of fashion.&lt;/p&gt;
    &lt;p&gt;And hey, maybe that‚Äôs because there‚Äôs just no more progress to make on the poorly defined problem of ‚Äúhow do we live.‚Äù But most well-defined problems were once defined poorly. For example, ‚Äúhow do we land on the moon‚Äù was a hopelessly poorly defined problem for most of human history. It only makes sense if you know that the moon is a big rock you can land on and not, say, a god floating in the sky. We slowly put some definitions around that problem, and then one day we sent an actual dude to the moon and he walked around and was like ‚ÄúI‚Äôm on the moon now.‚Äù If we can do that, maybe we can also figure out how to live good lives. It certainly seems worth it to keep trying.&lt;/p&gt;
    &lt;head rend="h1"&gt;BUT AREN‚ÄôT THERE MULTIPLE INTELLIGENCES?&lt;/head&gt;
    &lt;p&gt;I‚Äôm not the first to propose that ‚Äúgeneral‚Äù intelligence is more than one thing. Pretty much as soon as Spearman started claiming that intelligence is mainly one thing, other people started saying that intelligence is actually many things. (That‚Äôs science, baby!) Today, the most popular version of this theory claims there‚Äôs something like eight intelligences, ranging from ‚Äúvisual-spatial‚Äù to ‚Äúbodily-kinesthetic.‚Äù I‚Äôm sympathetic to this take because it tries to account for all the different weird and wonderful things that humans can do. But it‚Äôs got two big problems.&lt;/p&gt;
    &lt;p&gt;Problem #1: People very rarely try to find any evidence for it. And when they do, they find that the people who score high on one of the many intelligences tend to score high on the others, too, just as Spearman would‚Äôve predicted a hundred years ago.&lt;/p&gt;
    &lt;p&gt;Problem #2: When you label every human activity as its own intelligence, you give up any hope of understanding anything about the structure of problems in the world or how people solve them. We can make up whatever categories we want; they aren‚Äôt given by God. The only reason to use some categories and not others is that some categories are useful and others aren‚Äôt.&lt;/p&gt;
    &lt;p&gt;For instance, we could have created a periodic table that organized the elements alphabetically, or by color, or by how good they taste. Instead we organize them by atomic number, not because it‚Äôs their ‚Äútrue‚Äù order, but because it‚Äôs useful. It helps us realize things like, ‚ÄúHey, we‚Äôve got a number 62 and a number 64‚ÄîI wonder if there‚Äôs a number 63 out there. We should go looking for it.‚Äù&lt;/p&gt;
    &lt;p&gt;So we should pick the way of categorizing intelligence that gives us the most bang for our buck. ‚ÄúIntelligence is many things‚Äù can‚Äôt explain why people perform similarly across supposedly different tests, and ‚Äúintelligence is mostly one thing‚Äù can‚Äôt answer a basic question like ‚Äúwhy smart people aren‚Äôt happier?‚Äù But we can handle both of those challenges when we split intelligence into skill at solving well-defined and poorly defined problems.&lt;/p&gt;
    &lt;p&gt;And that‚Äôs not all we can do.&lt;/p&gt;
    &lt;head rend="h1"&gt;OH BOY HERE COMES THE PART ABOUT AI&lt;/head&gt;
    &lt;p&gt;People think of AI as a big glob of problem-solving ability. If you make the glob bigger, it can solve harder problems. That‚Äôs certainly been true so far: gigantic globs of AI can now drive cars, defeat our greatest chess players, and predict how proteins will fold.&lt;/p&gt;
    &lt;p&gt;All this has happened very quickly, which may make it seem like we‚Äôre careening toward a ‚Äúgeneral‚Äù artificial intelligence that can do all the things humans can. But if you split problems into well-defined and poorly defined, you‚Äôll notice that all of AI‚Äôs progress has been on defined problems. That‚Äôs what artificial intelligence does. In order to get AI to solve a problem, we have to give it data to learn from, and picking that data requires defining the problem.&lt;/p&gt;
    &lt;p&gt;That doesn‚Äôt mean the problems AI has solved so far are stupid or trivial. They‚Äôre really important and interesting! They‚Äôre just all well-defined problems. And we should expect that pattern to continue: for any well-defined problem, AI will eventually outperform humans. But for poorly defined problems, AI is hopeless. To solve those, we need humans running around doing weird human stuff.&lt;/p&gt;
    &lt;p&gt;‚ÄúWhat about GPT-3‚Äîit can write movie scripts! And what about DALLE-2‚Äîit can paint pictures!‚Äù These AIs perform a clever trick: they make it seem like they‚Äôre solving poorly defined problems when, under the hood, they‚Äôre really solving well-defined problems. GPT-3 doesn‚Äôt actually write movie scripts; it predicts what words should come next. DALLE-2 doesn‚Äôt actually paint pictures; it matches words to images. These problems aren‚Äôt easy to solve‚Äîthat‚Äôs why you need such a big glob of AI. But they obey clear, unchanging rules, they have bright boundaries, and you know precisely when you‚Äôve solved them. They are well-defined problems. (This is also why AI art isn‚Äôt art).&lt;/p&gt;
    &lt;p&gt;If you booted up a super-smart AI in ancient Greece, fed it all human knowledge, and asked it how to land on the moon, it would respond ‚ÄúYou can‚Äôt land on the moon. The moon is a god floating in the sky.‚Äù How would you get it to realize the moon is actually a big rock? That‚Äôs a great, poorly defined problem, and I don‚Äôt expect AI to solve it anytime soon.&lt;/p&gt;
    &lt;head rend="h1"&gt;SHOUTOUT TO MY GRANDMA&lt;/head&gt;
    &lt;p&gt;Here‚Äôs one last advantage of dividing intelligence into well-defined problem-solving and poorly defined problem-solving: it reminds us to give some respect where respect is due.&lt;/p&gt;
    &lt;p&gt;We‚Äôve got no problem fawning over people who are good at solving well-defined problems. They get to be called ‚Äúprofessor‚Äù and ‚Äúdoctor.‚Äù We pay them lots of money to teach us stuff. They get to join exclusive clubs like Mensa and the Prometheus Society. (By the way, Mensa‚Äôs page explaining IQ doesn‚Äôt mention anything about the dark history of using intelligence tests to hurt people, and you might expect a bunch of smarty-pantses to, you know, use their brains to discuss things with a bit more nuance. But what do I know, I‚Äôm just a big dummy.)&lt;/p&gt;
    &lt;p&gt;People who are good at solving poorly defined problems don‚Äôt get the same kind of kudos. They don‚Äôt get any special titles or clubs. There is no test they can take that will spit out a big, honking number that will make everybody respect them.&lt;/p&gt;
    &lt;p&gt;And that‚Äôs a shame. My grandma does not know how to use the ‚Äúinput‚Äù button on her TV‚Äôs remote control, but she does know how to raise a family full of good people who love each other, how to carry on through a tragedy, and how to make the perfect pumpkin pie. We sometimes condescendingly refer to this kind of wisdom as ‚Äúfolksy‚Äù or ‚Äúhomespun,‚Äù as if answering multiple-choice questions is real intelligence, and living a good, full life is just some down-home, gee-whiz, cutesy thing that little old ladies do.&lt;/p&gt;
    &lt;p&gt;Excluding this kind of intelligence from our definitions doesn‚Äôt just hurt our grandmas‚Äîit hurts us too. If you don‚Äôt value the ability to solve poorly defined problems, you‚Äôll never get more of it. You won‚Äôt seek out people who have that ability and try to learn from them, nor will you listen to them when they have something important to say. You‚Äôll spend your whole life trying to solve problems with cleverness when what you really need is wisdom. And you‚Äôll wonder why it never really seems to work. All of your optimizing, your straining to achieve and advance, your ruthless crusade to eliminate all of the well-defined problems from your life‚Äîit doesn‚Äôt actually seem make your life any better.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre stuck trying to solve poorly defined problems with your slick, well-defined problem-solving skills and you‚Äôre lucky enough to have a grandma like mine still on this Earth, my god, go see her. Shut up and listen to her for a while. And once you‚Äôve learned something, maybe ask her if she needs help with her TV.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theseedsofscience.pub/p/why-arent-smart-people-happier"/><published>2025-11-05T16:32:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45825965</id><title>ChatGPT terms disallow its use in providing legal and medical advice to others</title><updated>2025-11-05T21:34:44.237225+00:00</updated><content>&lt;doc fingerprint="ec8cff953dd634b6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ChatGPT users can‚Äôt use service for tailored legal and medical advice, OpenAI says&lt;/head&gt;
    &lt;p&gt;Updated:&lt;/p&gt;
    &lt;p&gt;Published:&lt;/p&gt;
    &lt;head rend="h3"&gt;Here Are The 60 Best Advent Calendars For 2025 You Can Get In Canada (So Far)&lt;/head&gt;
    &lt;head rend="h3"&gt;I‚Äôve been Using This Canadian Shampoo And Conditioner For Over A Month, And It‚Äôs Totally Changed My Scalp And Hair Health&lt;/head&gt;
    &lt;head rend="h3"&gt;20 Foolproof Gifts To Order If You Want To Get Your Holiday Shopping Done Early&lt;/head&gt;
    &lt;head rend="h3"&gt;I Tried It: A Laundry Basket So Smart It Solved Our Biggest Household Argument&lt;/head&gt;
    &lt;head rend="h3"&gt;20 Things From Amazon Canada That CTV Shopping Trends Readers Loved Ordering In October&lt;/head&gt;
    &lt;head rend="h3"&gt;How To Choose The Best Vacuum Sealer For You (And A Few Of Our Favourite Models For 2025)&lt;/head&gt;
    &lt;head rend="h3"&gt;13 Budget-Friendly Beauty Products That Are Dupes Of More Expensive Items&lt;/head&gt;
    &lt;head rend="h3"&gt;12 Products For Damaged Hair That‚Äôll Help Bring Your Fried Tresses Back To Life&lt;/head&gt;
    &lt;head rend="h3"&gt;15 Of The Best Korean Beauty Skincare Finds For Fall 2025&lt;/head&gt;
    &lt;head rend="h3"&gt;27 Of The Absolute Best Last-Minute Beauty Discounts To Take Advantage Of Before The Amazon Prime Big Deal Days Sale Ends&lt;/head&gt;
    &lt;p&gt;The Shopping Trends team is independent of the journalists at CTV News. We may earn a commission when you use our links to shop. Read about us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ctvnews.ca/sci-tech/article/openai-updates-policies-so-chatgpt-wont-provide-medical-or-legal-advice/"/><published>2025-11-05T18:11:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45826266</id><title>Dillo, a multi-platform graphical web browser</title><updated>2025-11-05T21:34:43.897526+00:00</updated><content>&lt;doc fingerprint="9650cf58848088e4"&gt;
  &lt;main&gt;
    &lt;p&gt;Dillo is a multi-platform graphical web browser, known for its speed and small footprint, that is developed with a focus on personal security and privacy. It is built with the FLTK 1.3 GUI toolkit.&lt;/p&gt;
    &lt;p&gt;Screenshot of the Dillo Website rendered in Dillo:&lt;/p&gt;
    &lt;p&gt;To install Dillo follow the installation guide.&lt;/p&gt;
    &lt;p&gt;This repository contains mostly the original code of Dillo with some minor patches. Additional patches or pull requests are welcome.&lt;/p&gt;
    &lt;p&gt;See also other related forks: dillo-plus, dilloNG, D+ browser and Mobilized Dillo.&lt;/p&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;As of December 2023, the host &lt;code&gt;dillo.org&lt;/code&gt; is no longer under control
of Dillo developers. A copy of the old website is archived in
GitHub Pages and the Wayback Machine (May 2022).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/dillo-browser/dillo"/><published>2025-11-05T18:40:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45826348</id><title>The state of SIMD in Rust in 2025</title><updated>2025-11-05T21:34:43.734748+00:00</updated><content>&lt;doc fingerprint="3f28d332981d8b8c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The state of SIMD in Rust in 2025&lt;/head&gt;
    &lt;p&gt;If you‚Äôre already familiar with SIMD, the table below is all you need.&lt;/p&gt;
    &lt;p&gt;And if you‚Äôre not, you will understand the table by the end of this article!&lt;/p&gt;
    &lt;head rend="h2"&gt;What‚Äôs SIMD? Why SIMD?&lt;/head&gt;
    &lt;p&gt;Hardware that does arithmetic is cheap, so any CPU made this century has plenty of it. But you still only have one instruction decoding block and it is hard to get it to go fast, so the arithmetic hardware is vastly underutilized.&lt;/p&gt;
    &lt;p&gt;To get around the instruction decoding bottleneck, you can feed the CPU a batch of numbers all at once for a single arithmetic operation like addition. Hence the name: ‚Äúsingle instruction, multiple data,‚Äù or SIMD for short.&lt;/p&gt;
    &lt;p&gt;Instead of adding two numbers together, you can add two batches or ‚Äúvectors‚Äù of numbers and it takes about the same amount of time.&lt;/p&gt;
    &lt;p&gt;On recent x86 chips these batches can be up to 512 bits in size, so in theory you can get an 8x speedup for math on &lt;code&gt;u64&lt;/code&gt; or a 64x speedup on &lt;code&gt;u8&lt;/code&gt;!&lt;/p&gt;
    &lt;head rend="h2"&gt;Instruction sets&lt;/head&gt;
    &lt;p&gt;Historically, SIMD instructions were added after the CPU architecture was already designed, so SIMD is an extension with its own marketing name on each architecture.&lt;/p&gt;
    &lt;p&gt;ARM calls theirs ‚ÄúNEON‚Äù, and all 64-bit ARM CPUs have it.&lt;/p&gt;
    &lt;p&gt;WebAssembly doesn‚Äôt have a marketing department, so they just call theirs ‚ÄúWebAssembly 128-bit packed SIMD extension‚Äù.&lt;/p&gt;
    &lt;p&gt;64-bit x86 shipped with one called ‚ÄúSSE2‚Äù which has basic instructions for 128-bit vectors, but later they added a whole menagerie of extensions on top of that, with SSE 4.2 adding more operations, AVX and AVX2 adding 256-bit vectors and AVX-512 adding 512-bit vectors.&lt;/p&gt;
    &lt;p&gt;The word ‚Äúlater‚Äù in the above paragraph creates a problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Does this CPU have that instruction?&lt;/head&gt;
    &lt;p&gt;If you‚Äôre running a program on an x86 CPU, it‚Äôs not a given that the CPU has any particular SIMD extension. So by default the compiler isn‚Äôt allowed to use instructions beyond SSE2 because that won‚Äôt work on all x86 CPUs.&lt;/p&gt;
    &lt;p&gt;There are two ways around this problem.&lt;/p&gt;
    &lt;p&gt;If you work for a company that only ever runs their binaries on their own servers or on a public cloud, you can just assert that they‚Äôre all recent enough to at least have AVX2 that was introduced over 10 years ago, and have the program crash or misbehave if it ever runs on anything without AVX2:&lt;/p&gt;
    &lt;code&gt;RUSTFLAGS='-C target-cpu=x86‚Äì64-v3' cargo build --release&lt;/code&gt;
    &lt;p&gt;However, if you are distributing the binaries for other people to run, that‚Äôs not really an option.&lt;/p&gt;
    &lt;p&gt;Instead you can do something called function multiversioning: compile the same function multiple times for different SIMD extensions, and when the program actually runs, check what features the CPU supports and select the appropriate version based on that.&lt;/p&gt;
    &lt;p&gt;Fortunately, this problem only exists on x86.&lt;/p&gt;
    &lt;p&gt;ARM made its NEON mandatory in all 64-bit CPUs and then didn‚Äôt bother expanding the width beyond 128 bits. (Technically SVE exists, but in 2025 it is still mostly on paper, and Rust support for it is still in progress).&lt;/p&gt;
    &lt;p&gt;WebAssembly makes you compile two different binaries, one with SIMD and one without, and use JavaScript to check if the browser supports SIMD.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solution space&lt;/head&gt;
    &lt;p&gt;There are four approaches to SIMD in Rust, in ascending order of effort:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Automatic vectorization&lt;/item&gt;
      &lt;item&gt;Fancy iterators&lt;/item&gt;
      &lt;item&gt;Portable SIMD abstractions&lt;/item&gt;
      &lt;item&gt;Raw intrinsics&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Automatic vectorization&lt;/head&gt;
    &lt;p&gt;The easiest approach to SIMD is letting the compiler do it for you.&lt;/p&gt;
    &lt;p&gt;It works surprisingly well, as long as you structure your code in a way that is amenable to vectorization. This article covers it:&lt;/p&gt;
    &lt;p&gt;You can check if it‚Äôs working with cargo-show-asm or godbolt.org, but your benchmarks are the ultimate judge of the results.&lt;/p&gt;
    &lt;p&gt;Sadly there is a limit on the complexity of the code that the compiler will vectorize, and it may change between compiler versions. If something vectorizes today that doesn‚Äôt necessarily mean it still will in a year from now.&lt;/p&gt;
    &lt;p&gt;The other drawback of this method is that the optimizer won‚Äôt even touch anything involving floats (&lt;code&gt;f32&lt;/code&gt; and &lt;code&gt;f64&lt;/code&gt; types). It‚Äôs not permitted to change any observable outputs of the program, and reordering float operations may alter the result due to precision loss. (There is a way to tell the compiler not to worry about precision loss, but it‚Äôs currently nightly-only).&lt;/p&gt;
    &lt;p&gt;So right now, if you need to process floats, autovectorization is a no-go unless you can use nightly builds of the Rust compiler.&lt;/p&gt;
    &lt;p&gt;(Floats are cursed even without SIMD. Something as simple as summing an array of them in a usable way turns out to be really hard).&lt;/p&gt;
    &lt;p&gt;There is no built-in way to multiversion functions, but the multiversion crate works great with autovectorization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fancy iterators&lt;/head&gt;
    &lt;p&gt;Just like rayon lets you run your iterators in parallel by swapping &lt;code&gt;.iter()&lt;/code&gt; with &lt;code&gt;.par_iter()&lt;/code&gt;, there have been attempts to do the same for SIMD. After all, what is SIMD but another kind of parallelism?&lt;/p&gt;
    &lt;p&gt;This is the approach that the faster crate takes. That crate has been abandoned for years, and it doesn‚Äôt look like this approach has panned out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Portable SIMD abstractions&lt;/head&gt;
    &lt;p&gt;The idea is to let you write your algorithm by explicitly operating on chunks of data, something like &lt;code&gt;[f32; 8]&lt;/code&gt; but wrapped in a custom type, and then provide custom implementations of operations like &lt;code&gt;+&lt;/code&gt; that compile down into SIMD instructions.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;std::simd&lt;/code&gt; is exactly that. It supports all instruction sets LLVM supports, so its platform support is unparalleled. It pairs well with the multiversion crate. Sadly it‚Äôs nightly-only and will remain such for the foreseeable future, so it‚Äôs unusable in most situations.&lt;/p&gt;
    &lt;p&gt;The wide crate is a mature, established option. It supports NEON, WASM and all the x86 instruction sets. But it doesn‚Äôt support multiversioning at all, save for very exotic and limited approaches like cargo-multivers.&lt;/p&gt;
    &lt;p&gt;The pulp crate is a great design with built-in multiversioning, and is quite mature and complete. It powers faer, so its performance is clearly proven. The drawbacks are that it doesn‚Äôt support WASM, and that on x86 it only supports targeting AVX2 and AVX-512 but not the older extensions. But AVX2 was introduced in 2012 and in the Steam hardware survey 95% systems have it, so that might not be a big deal.&lt;/p&gt;
    &lt;p&gt;The macerator crate is a fork of pulp with vastly expanded instruction set support. It supports all x86 extensions, WASM, NEON, and even the LoongArch SIMD extensions. It‚Äôs used only by burn-ndarray, and even there it‚Äôs an optional dependency. It sounds great on paper, but it‚Äôs oddly obscure and therefore unproven. I‚Äôd probably write code using pulp, then replace it with macerator and see if everything still works and runs as fast as it should.&lt;/p&gt;
    &lt;p&gt;The fearless_simd crate is largely a copy of pulp‚Äôs design made for use in vello. It‚Äôs far less mature than pulp, but it‚Äôs under active development. As of this writing it supports NEON, WASM and SSE4.2, but not the newer x86 extensions. Seems too immature just yet, but something to keep an eye on.&lt;/p&gt;
    &lt;p&gt;simdeez is a rather old crate that supports all instruction sets except AVX-512 and comes with built-in multiversioning. What gives me pause is that despite existing for many years, it‚Äôs still barely used. Everyone else who needed SIMD built their own instead of using it. And its README says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Currently things are well fleshed out for i32, i64, f32, and f64 types.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So I guess the other types aren‚Äôt complete?&lt;/p&gt;
    &lt;p&gt;TL;DR: use &lt;code&gt;std::simd&lt;/code&gt; if you don‚Äôt mind nightly, &lt;code&gt;wide&lt;/code&gt; if you don‚Äôt need multiversioning, and otherwise &lt;code&gt;pulp&lt;/code&gt; or &lt;code&gt;macerator&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If it‚Äôs not 2025 when you‚Äôre reading this, check out &lt;code&gt;fearless_simd&lt;/code&gt;, because &lt;code&gt;std::simd&lt;/code&gt; is still in nightly in your glorious future, isn‚Äôt it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Raw intrinsics&lt;/head&gt;
    &lt;p&gt;If you want to get really close to the metal, there are always the raw intrinsics, just one step removed from the processor instructions.&lt;/p&gt;
    &lt;p&gt;The problem looming over any use of raw intrinsics is that you have to manually write them for every platform and instruction set you‚Äôre targeting. Whereas &lt;code&gt;std::simd&lt;/code&gt; or &lt;code&gt;wide&lt;/code&gt; let you write your logic once and compile it down to the assembly automatically, with intrinsics you have to write a separate implementation for every single platform and instruction set (SSE, AVX, NEON‚Ä¶) you care to support. That‚Äôs a lot of code!&lt;/p&gt;
    &lt;p&gt;It‚Äôs really not helped by the fact that they are all named something like &lt;code&gt;_mm256_srli_epi32&lt;/code&gt; and your code ends up as a long list of calls to these arcanely named functions. And wrappers that help readability introduce their own problems, such as clashes with multiversioning or unsafe code or arcane macros.&lt;/p&gt;
    &lt;p&gt;You also have to build your own multiversioning. Or rather, you have to manually dispatch to the dedicated implementation you have manually written for each instruction set. &lt;code&gt;std::is_x86_feature_detected!&lt;/code&gt; macro takes care of the feature detection, but it is somewhat slow. In some cases it is beneficial to detect available features exactly once and then cache the results, but you have to implement that manually too.&lt;/p&gt;
    &lt;p&gt;On the bright side, this year writing intrinsics got markedly less awful. Most of them are no longer &lt;code&gt;unsafe&lt;/code&gt; to call in Rust 1.86 and later, and the safe_unaligned_simd crate provides safe wrappers for the rest.&lt;/p&gt;
    &lt;p&gt;So at least this approach is no longer &lt;code&gt;unsafe&lt;/code&gt; on top of all the other problems it has!&lt;/p&gt;
    &lt;head rend="h2"&gt;Which one is right for you?&lt;/head&gt;
    &lt;p&gt;The right tool for the job ultimately depends on the use case.&lt;/p&gt;
    &lt;p&gt;Want zero dependencies and little up-front hassle? Autovectorization. Porting existing C code or targeting very specific hardware? Intrinsics. Anything else? Portable SIMD abstraction.&lt;/p&gt;
    &lt;p&gt;And now that you made it this far, you can understand the table at the top of the article, which will help guide your decision!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://shnatsel.medium.com/the-state-of-simd-in-rust-in-2025-32c263e5f53d"/><published>2025-11-05T18:45:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45826500</id><title>Internet Archive's legal fights are over, but its founder mourns what was lost</title><updated>2025-11-05T21:34:43.428763+00:00</updated><content>&lt;doc fingerprint="ca919bd5fba6537b"&gt;
  &lt;main&gt;
    &lt;p&gt;Last month, the Internet Archive‚Äôs Wayback Machine archived its trillionth webpage, and the nonprofit invited its more than 1,200 library partners and 800,000 daily users to join a celebration of the moment. To honor ‚Äúthree decades of safeguarding the world‚Äôs online heritage,‚Äù the city of San Francisco declared October 22 to be ‚ÄúInternet Archive Day.‚Äù The Archive was also recently designated a federal depository library by Sen. Alex Padilla (D-Calif.), who proclaimed the organization a ‚Äúperfect fit‚Äù to expand ‚Äúaccess to federal government publications amid an increasingly digital landscape.‚Äù&lt;/p&gt;
    &lt;p&gt;The Internet Archive might sound like a thriving organization, but it only recently emerged from years of bruising copyright battles that threatened to bankrupt the beloved library project. In the end, the fight led to more than 500,000 books being removed from the Archive‚Äôs ‚ÄúOpen Library.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWe survived,‚Äù Internet Archive founder Brewster Kahle told Ars. ‚ÄúBut it wiped out the Library.‚Äù&lt;/p&gt;
    &lt;p&gt;An Internet Archive spokesperson confirmed to Ars that the archive currently faces no major lawsuits and no active threats to its collections. Kahle thinks ‚Äúthe world became stupider‚Äù when the Open Library was gutted‚Äîbut he‚Äôs moving forward with new ideas.&lt;/p&gt;
    &lt;head rend="h2"&gt;History of the Internet Archive&lt;/head&gt;
    &lt;p&gt;Kahle has been striving since 1996 to transform the Internet Archive into a digital Library of Alexandria‚Äîbut ‚Äúwith a better fire protection plan,‚Äù joked Kyle Courtney, a copyright lawyer and librarian who leads the nonprofit eBook Study Group, which helps states update laws to protect libraries.&lt;/p&gt;
    &lt;p&gt;When the Wayback Machine was born in 2001 as a way to take snapshots of the web, Kahle told The New York Times that building free archives was ‚Äúworth it.‚Äù He was also excited that the Wayback Machine had drawn renewed media attention to libraries.&lt;/p&gt;
    &lt;p&gt;At the time, law professor Lawrence Lessig predicted that the Internet Archive would face copyright battles, but he also believed that the Wayback Machine would change the way the public understood copyright fights.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/tech-policy/2025/11/the-internet-archive-survived-major-copyright-losses-whats-next/"/><published>2025-11-05T18:59:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45826995</id><title>New gel restores dental enamel and could revolutionise tooth repair</title><updated>2025-11-05T21:33:52.648503+00:00</updated><content/><link href="https://www.nottingham.ac.uk/news/new-gel-restores-dental-enamel-and-could-revolutionise-tooth-repair"/><published>2025-11-05T19:44:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45827042</id><title>3D Geological Models in Minecraft</title><updated>2025-11-05T21:33:51.694608+00:00</updated><content>&lt;doc fingerprint="5c13435e25d6930d"&gt;
  &lt;main&gt;
    &lt;p&gt;Download our 3D geology models as Minecraft worlds and learn how geology can influence the landscape.&lt;/p&gt;
    &lt;p&gt;We have built five worlds to allow you to explore underground and learn about the geology. Four of these worlds show small sites around the UK and use real geological data to show what the geology looks like under the surface. The fifth world shows a simplified geology of the whole of mainland Great Britain. These models show how geology can influence landscapes and land use, and can help teach geology to a younger audience.&lt;/p&gt;
    &lt;p&gt;At the end of section, there is also a user-generated volcano world created by three students from Nottingham.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Getting started: requirements and installation&lt;/item&gt;
      &lt;item&gt;How did we make the BGS Minecraft worlds?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Getting started: requirements and installation&lt;/head&gt;
    &lt;p&gt;To start you off on your Minecraft journey we have built a spawn point, which includes some information about the blocks and which real world geology they represent. You can use this to learn more about the geology of the area by spawning back to the platform at any point.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a licenced copy of Minecraft&lt;/item&gt;
      &lt;item&gt;50 MB free disk space per local area world&lt;/item&gt;
      &lt;item&gt;6 GB for the Great Britain world&lt;/item&gt;
      &lt;item&gt;more than 4 GB of RAM&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the 3D model as a zip file archive (between 1 and 65 MB).&lt;/item&gt;
      &lt;item&gt;Unzip the archive to a temporary location.&lt;/item&gt;
      &lt;item&gt;Start Minecraft.&lt;/item&gt;
      &lt;item&gt;On the home screen, click ‚ÄòOptions‚Äô.&lt;/item&gt;
      &lt;item&gt;Click ‚ÄòResource Packs‚Äô.&lt;/item&gt;
      &lt;item&gt;Click ‚ÄòOpen resource pack folder‚Äô. This will open a new window showing the contents of your Minecraft ‚ÄòResource Packs‚Äô folder.&lt;/item&gt;
      &lt;item&gt;Navigate to the folder (called .minecraft on Windows).&lt;/item&gt;
      &lt;item&gt;Open the ‚Äòsaves‚Äô folder.&lt;/item&gt;
      &lt;item&gt;Move the unzipped archive to this ‚Äòsaves‚Äô folder.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How did we make the BGS Minecraft worlds?&lt;/head&gt;
    &lt;p&gt;We decided to use glass blocks that were coloured by traditional geology map colours. This allows you to dig into the model and ‚Äòfall‚Äô between each geological layer. You can then explore under the ground in Minecraft because the glass blocks allow you to see through them. When experimenting with these 3D Minecraft worlds we discovered that, by using glass blocks, the player could see the true extent of the geology straight away.&lt;/p&gt;
    &lt;head rend="h3"&gt;What data did we use?&lt;/head&gt;
    &lt;p&gt;To build each of these models we used a number of different datasets, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3D geology ‚Äî geological surfaces taken from BGS 3D models&lt;/item&gt;
      &lt;item&gt;elevation ‚Äî taken from the surface of the 3D geology models&lt;/item&gt;
      &lt;item&gt;topography ‚Äî OS VectorMap District&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The worlds&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Glasgow Minecraft world&lt;/item&gt;
      &lt;item&gt;Ingleborough Minecraft world&lt;/item&gt;
      &lt;item&gt;York Minecraft world&lt;/item&gt;
      &lt;item&gt;West Thurrock Minecraft world&lt;/item&gt;
      &lt;item&gt;Great Britain Minecraft world&lt;/item&gt;
      &lt;item&gt;Volcano Minecraft world&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Glasgow Minecraft world&lt;/head&gt;
    &lt;p&gt;The geology in this world is in two parts. The first has the shallower rocks and soils directly underneath Glasgow, called ‚Äòsuperficial geology‚Äô. We have also included artificial ground in this world.&lt;/p&gt;
    &lt;p&gt;The second, lower part, is called the ‚Äòbedrock geology‚Äô (this is the hard rock, not what Minecraft calls Bedrock) and these rocks date from the Carboniferous Period, including the Upper Coal Measures.&lt;/p&gt;
    &lt;p&gt;When you first land in the world, there will be signposts with information to get you started. Once you‚Äôve got your bearings you can fly around the 3D world by double tapping then holding the space bar, which will cause you to fly up into the air. This world includes the topography (features on the ground such as roads and buildings) that will help you to find your way around the world.&lt;/p&gt;
    &lt;head rend="h3"&gt;Geology key&lt;/head&gt;
    &lt;p&gt;The geological units for each world have been grouped together in order to accommodate the limited Minecraft glass block palette of 16 colours. Where possible, we have tried to emulate the BGS geological unit colours.&lt;/p&gt;
    &lt;p&gt;This Glasgow Minecraft world is based on the BGS central Glasgow 3D geological model. The geological model gives engineers and construction companies a subsurface view of the geology underneath Glasgow to help save time and money during construction projects. It also provides insight into the potential for geothermal heat from the water found in flooded mine workings ‚Äî see UK Geoenergy Observatories Glasgow.&lt;/p&gt;
    &lt;p&gt;Covering about 100 km2, the area shows the superficial deposits of the central Glasgow area, including glaciated material from successive ice sheets. The model was built within BGS‚Äôs GOCAD¬Æ and GSI3D software, using a digital elevation model of 20 m cell resolution, and is based on datasets held by BGS, such as digital maps and boreholes.&lt;/p&gt;
    &lt;p&gt;The superficial deposits in the Minecraft world are mostly glacial tills, reflecting the advances and retreats of ice sheets. The tills often rest directly on bedrock geology and comprise a mixture of clay, sand and silt with pebbles, cobbles and boulders.&lt;/p&gt;
    &lt;p&gt;Other types of superficial deposits in the area are related to several marine inundations (raising and lowering of the sea level) during and since the last glaciation, the development of river terraces, the deposition of estuary sediments, and local lakes, some infilled partly by peat.&lt;/p&gt;
    &lt;p&gt;The bedrock geology in the Minecraft world dates back to the Carboniferous Period (485‚Äì541 million years ago). The majority of the rocks were deposited in an environment such as a river delta or a shallow sea and consist of sandstone, coal and limestone. The rocks were identified using borehole material and descriptions from borehole drill wells and using things found in the rock, such as fossils and minerals, to give the rocks an age.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ingleborough Minecraft world&lt;/head&gt;
    &lt;p&gt;This Minecraft world uses the classic geological study area of Ingleborough, Yorkshire Dales, as an introduction to 3D geological models. This world is modelled at 1:250 000 scale and covers an area of 18 km2. You can fly over the landscape at the surface or dig below ground to expose some of the limestone landscape beneath Ingleborough.&lt;/p&gt;
    &lt;p&gt;When you first land in the world there will be signposts with information to get you started. Once you‚Äôve got your bearings, you can fly around the 3D world by double tapping then holding the space bar, which will cause you to fly up into the air. This world includes the topography (map features on the ground such as roads and buildings) that will help you to find your way around the world.&lt;/p&gt;
    &lt;head rend="h3"&gt;Geology key&lt;/head&gt;
    &lt;p&gt;The geological units for this world have been grouped together in order to accommodate the limited Minecraft glass block palette of 16 colours. Where possible, we have tried to emulate the BGS geological unit colour.&lt;/p&gt;
    &lt;p&gt;The geological model was built by BGS in 2010 using GSI3D and is calculated using a digital elevation model (DEM) with a 5 m grid spacing, it extends to about 100 m OD.&lt;/p&gt;
    &lt;p&gt;The Minecraft world contains three areas of worked ground, one in the Great Scar Limestone Group in the north and two in the Ingleton Group in the main valley. The world contains three superficial rock units; alluvium from the rivers Doe, Kingsdale and Ribble, peat on the flanks of Ingleborough and till in the south-west and north-east.&lt;/p&gt;
    &lt;p&gt;The till has been modelled in areas of drumlin development to accentuate the landforms. Till is present elsewhere in the area, but is not displayed in the world. The till in this region was laid down by the Devensian ice sheet, which covered the whole region.&lt;/p&gt;
    &lt;p&gt;The bedrock geology is made up of several layers. The youngest late Carboniferous deposits of the Pennine Lower Coal Measures Formation comprise interbedded sandstones, siltstones and mudstones with several coal seams. The early Carboniferous limestones of the Great Scar Limestone and Yoredale groups contain classic karst features such as Gaping Gill. The limestone is up to 500 m thick.&lt;/p&gt;
    &lt;p&gt;In the south-west, the Dent Formation, of Ordovician age (444‚Äì485 million years ago) and comprised of shallow marine sediments, limestone and pyroclastic rocks is encountered south of the North Craven Fault. Most of the area is unconformably underlain by the Ordovician Ingleton Group of sandstones, siltstones and conglomerates, which is over 300 m thick.&lt;/p&gt;
    &lt;head rend="h2"&gt;Download Ingleborough world&lt;/head&gt;
    &lt;head rend="h2"&gt;York Minecraft world&lt;/head&gt;
    &lt;p&gt;This Minecraft world covers 50 km2 of terrain to the north and east of York including in the north the village of Haxby. The area lies within the low lying Vale of York where deposits from the last glaciation (Devensian) often rest directly on the Triassic bedrock.&lt;/p&gt;
    &lt;p&gt;When you first land in the world there will be signposts with information to get you started. Once you‚Äôve got your bearings you can fly around the 3D world by double tapping then holding the space bar, which will cause you to fly up into the air. This world includes the topography (map features on the ground, such as roads and buildings) which will help you to find your way around the world.&lt;/p&gt;
    &lt;head rend="h3"&gt;Geology key&lt;/head&gt;
    &lt;p&gt;The geological units for each world have been grouped together in order to accommodate the limited Minecraft glass block palette of 16 colours. Where possible we have tried to emulate the BGS geological unit colours.&lt;/p&gt;
    &lt;p&gt;The York model was built by BGS in 2005 as a result of a geological survey and model building programme for the Selby and York areas. The rocks were identified using borehole material and descriptions from borehole drill wells and using things found in the rock, such as fossils and minerals, to give the rocks an age.&lt;/p&gt;
    &lt;p&gt;The model extends to about 50 m depth and has been calculated using a digital elevation model (DEM). At high vertical exaggerations, despite processing, some residual effects, mainly due to local woodland, can still be seen in the top model surface.&lt;/p&gt;
    &lt;p&gt;The world shows the large flat areas of alluvium of the River Ouse in the extreme south-west and its tributary the Foss.&lt;/p&gt;
    &lt;p&gt;The Vale of York Till forms the core of the York Moraine, which is a low ridge near the southern end of the world. During the glaciation, meltwater became ponded at several times in the Vale of York due to ice blocking drainage via the Humber into the North Sea Basin and beyond.&lt;/p&gt;
    &lt;p&gt;The bedrock in the area is Triassic Sherwood Sandstone, which is a significant aquifer in the Vale of York.&lt;/p&gt;
    &lt;head rend="h2"&gt;Download York world&lt;/head&gt;
    &lt;head rend="h2"&gt;West Thurrock Minecraft world&lt;/head&gt;
    &lt;p&gt;The West Thurrock world covers about 10 km2 along the eastern flank of the M25 motorway on the north bank of the River Thames. It depicts an extensively modified and developed landscape with several extensive areas of artificial ground, including the building of the Lakeside shopping complex within an old chalk quarry.&lt;/p&gt;
    &lt;p&gt;When you first land in the world there will be signposts with information to get you started. Once you‚Äôve got your bearings you can fly around the 3D world by double tapping then holding the space bar, which will cause you to fly up into the air. This world includes the topography (map features on the ground, such as roads and buildings) that will help you to find your way around the world.&lt;/p&gt;
    &lt;head rend="h3"&gt;Geology key&lt;/head&gt;
    &lt;p&gt;The geological units for each world have been grouped together in order to accommodate the limited Minecraft glass block palette of 16 colours. Where possible, we have tried to emulate the BGS geological unit colours.&lt;/p&gt;
    &lt;p&gt;The original geological model was built in 2009 in GSI3D. It shows a developed landscape with several extensive areas of artificial ground, including the building of the Lakeside shopping complex within an old chalk quarry.&lt;/p&gt;
    &lt;p&gt;The southern part of the world occupies the River Thames‚Äôs floodplain. Here, interbedded alluvial silts, clays and peats overlie gravel deposits. Together, these sediments are up to 15 m in thickness. Older river terrace deposits comprising sands and gravels fringe the alluvium and are preserved as erosional remnants farther north.&lt;/p&gt;
    &lt;p&gt;The bedrock geology is dominated by chalk, which underlies the whole area with the world extending to a depth of about 150 m. In the northern portion of the world, younger bedrock units are, in ascending order, the Thanet Sand, Lambeth, Harwich and London Clay formations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Download West Thurrock world&lt;/head&gt;
    &lt;head rend="h2"&gt;Great Britain world&lt;/head&gt;
    &lt;p&gt;BGS has also developed a Minecraft world devoted to the geology of the whole of mainland Great Britain. This world shows an interpretation of the Ordnance Survey open map data on the surface and the rough position of the real geology beneath.&lt;/p&gt;
    &lt;p&gt;You will start at the BGS office in Cardiff. Look around you for signposts with information to get you started. Once you‚Äôve got your bearings you can fly around the landscape by double tapping and holding the spacebar.&lt;/p&gt;
    &lt;p&gt;This Minecraft world uses the BGS Soil Parent Material Map to describe the geology across the whole of mainland Great Britain. The country was divided into smaller chunks (100 √ó 100km grid squares) so that Minecraft could cope with the whole area.&lt;/p&gt;
    &lt;p&gt;For each 100 √ó 100km square we:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;loaded the height of the land surface from an elevation of Great Britain, using a modified and scaled version of NEXTMap Britain data&lt;/item&gt;
      &lt;item&gt;loaded the geology from the Soil Parent Material Map&lt;/item&gt;
      &lt;item&gt;loaded the OS VectorMap¬Æ District raster files&lt;/item&gt;
      &lt;item&gt;for each chunk we used the height data to add blocks repeatedly up to the specified height and using the real world geology for that location&lt;/item&gt;
      &lt;item&gt;for the topmost block, we decided which material best matched the OS VectorMap¬Æ District data&lt;/item&gt;
      &lt;item&gt;finally, we compiled groups of 32√ó32 chunks into regions, and then compiled the whole lot into one zip file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Minecraft GB geology world represents the geology at the surface, but that geology does not change as you dig down into the world, as BGS doesn‚Äôt have 3D data covering the whole of the UK. It‚Äôs important to know that this is why the GB geology world is different to the larger scale models described above, which do have 3D geology included.&lt;/p&gt;
    &lt;p&gt;To do this, we have used BGS‚Äôs Soil Parent Material dataset. A parent material is a soil science name for a weathered rock or deposit from, and within which, a soil has formed. In the UK, parent materials provide the basic foundations and building blocks of the soil, influencing their texture, structure, drainage and chemistry. By using the parent material dataset we can get a general understanding of the types of geology to be found across Great Britain.&lt;/p&gt;
    &lt;p&gt;Using the methodology developed by the Ordnance Survey, we took the data from the OS VectorMap¬Æ District raster to decide the material of each block.&lt;/p&gt;
    &lt;head rend="h2"&gt;Volcano Minecraft world&lt;/head&gt;
    &lt;p&gt;This Minecraft world was created by three students from Nottingham. Created during summer 2020 during the COVID-19 lockdown, this volcano model was produced as a result of a short collaboration between BGS geospatial data staff and students from a local secondary school.&lt;/p&gt;
    &lt;p&gt;The volcano world includes a magma chamber, moving ‚Äòredstone‚Äô magma and side vents. At the surface, the world shows a farming village that benefits from the fertile volcanic soil that surrounds many volcanoes.&lt;/p&gt;
    &lt;p&gt;When you first land in the world there will be signposts with information to get you started. Once you‚Äôve got your bearings you can fly around the 3D world by double tapping then holding the space bar, which will cause you to fly up into the air.&lt;/p&gt;
    &lt;head rend="h2"&gt;Download volcano world&lt;/head&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minecraft is ¬©2009 ‚Äì 2015 Mojang AB.&lt;/item&gt;
      &lt;item&gt;Models built using Safe Software‚Äôs FME 2015.&lt;/item&gt;
      &lt;item&gt;Contains Ordnance Survey data ¬© Crown copyright and database right 2015.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;You may also be interested in&lt;/head&gt;
    &lt;head rend="h4"&gt;Discovering Geology&lt;/head&gt;
    &lt;p&gt;Discovering Geology introduces a range of geoscience topics to school-age students and learners of all ages.&lt;/p&gt;
    &lt;head rend="h4"&gt;Maps and resources&lt;/head&gt;
    &lt;p&gt;Download and print free educational resources.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bgs.ac.uk/discovering-geology/maps-and-resources/maps/minecraft-3d-geological-models/"/><published>2025-11-05T19:49:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45827190</id><title>Solarpunk is already happening in Africa</title><updated>2025-11-05T21:33:51.604045+00:00</updated><content/><link href="https://climatedrift.substack.com/p/why-solarpunk-is-already-happening"/><published>2025-11-05T20:00:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45827352</id><title>I Stopped Being a Climate Catastrophist</title><updated>2025-11-05T21:33:51.320927+00:00</updated><content>&lt;doc fingerprint="7a86017761ecbdde"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I Stopped Being a Climate Catastrophist&lt;/head&gt;
    &lt;head rend="h3"&gt;And why so many climate pragmatists can‚Äôt quit catastrophism&lt;/head&gt;
    &lt;p&gt;By Ted Nordhaus&lt;/p&gt;
    &lt;p&gt;Recently, in an exchange on X, my former colleague Tyler Norris observed that over the years, my views about climate risk have evolved substantially. Norris posted a screenshot of a page from the book Break Through, where Michael Shellenberger and I argued that if the world kept burning fossil fuels at current rates, catastrophe was virtually assured:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Over the next 50 years, if we continue to burn as much coal and oil as we‚Äôve been burning, the heating of the earth will cause the sea levels to rise and the Amazon to collapse, and, according to scenarios commissioned by the Pentagon, will trigger a series of wars over the basic resources like food and water.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Norris is right. I no longer believe this hyperbole. Yes, the world will continue to warm as long as we keep burning fossil fuels. And sea levels will rise. About 9 inches over the last century, perhaps another 2 or 3 feet over the course of the rest of this century. But the rest of it? Not so much.&lt;/p&gt;
    &lt;p&gt;There is little reason to think that the Amazon is at risk of collapsing over the next 50 years. Agricultural yield and output will almost certainly continue to rise, if not necessarily at the same rate as it has over the last 50 years. There has been no observable increase in meteorological drought globally that might trigger the resource wars that the Pentagon was scenario planning back then.&lt;/p&gt;
    &lt;p&gt;At the time that we published Break Through, I, along with most climate scientists and advocates, believed that business as usual emissions would lead to around five degrees of warming by the end of this century. As Zeke Hausfather, Glen Peters, Roger Pielke Jr, and Justin Richie have demonstrated over the last decade or so, that assumption was never plausible.&lt;/p&gt;
    &lt;p&gt;There have been some revisionist claims that the reason for the downgrading of business as usual warming assumptions is due to the success of climate and clean energy policies over the last several decades. But five degrees of warming by the end of this century was no more plausible in 2007, when Break Through was published, than it is today. The class of scenarios upon which it was based assumed very high population growth, very high economic growth, and slow technological change. None of these trends individually track at all with actual long term global trends. Fertility rates have been falling, global economic growth slowing, and the global economy decarbonizing for decades.&lt;/p&gt;
    &lt;p&gt;Nor is there good reason to think that the combination of these three trends could possibly be sustained in concert. High economic growth is strongly associated with falling fertility rates. Technological change is the primary driver of long term economic growth. A future with low rates of technological change is not one that is consistent with high economic growth. And a future characterized by high rates of economic growth is not one that is consistent with high rates of population growth.&lt;/p&gt;
    &lt;p&gt;As a result of these dynamics, most estimates of worst case warming by the end of the century now suggest 3 degrees or less. But as consensus around these estimates has shifted, the reaction to this good news among much of the climate science and advocacy community has not been to become less catastrophic. Rather, it has been to simply shift the locus of catastrophe from five to three degrees of warming. Climate advocates have arguably become more catastrophic about climate change in recent years, not less.&lt;/p&gt;
    &lt;p&gt;This is all the more confounding given that the good news extends well beyond projections of long term warming. Despite close to a degree and a half of warming over the last century or so, global mortality from climate and weather extremes has fallen by a factor of 25 or more on a per capita basis. As Pielke documented recently, the world is on track this year for what is almost certainly the lowest level of climate related mortality in recorded human history, not only on a per capita basis but on an absolute basis as well. The economic costs of climate extremes continue to rise, but this is almost entirely due to affluence, population growth, and the migration of global populations towards climate hazards, mainly cities that tend to be located in coastal regions and flood plains.&lt;/p&gt;
    &lt;p&gt;So I think the far more interesting question that Norris raises, at least implicitly, is not why my colleagues and I at Breakthrough have revised our priors about climate risk but why so many progressive environmentalists like Norris have not.&lt;/p&gt;
    &lt;head rend="h2"&gt;When Is Weather Climate Change?&lt;/head&gt;
    &lt;p&gt;For me, the cognitive dissonance began as I became familiar with Roger Pielke Jr‚Äôs work on normalized hurricane losses, in the late 2000s. This was around the time that a lot of messaging from the climate advocacy community had started to focus on extreme weather events, not just as harbingers for the storms of our grandchildren, to borrow the title of James Hansen‚Äôs 2009 book, but as being fueled by climate change in the present.&lt;/p&gt;
    &lt;p&gt;Hansen himself had been under no such illusion, writing that ‚Äúlocal climate change remains small compared with day-to-day weather fluctuations.‚Äù But by this point, the advocacy community had figured out that framing climate change as a future risk would not prove sufficient politically to transform the US and global energy systems in the way that most believed necessary. This became a particularly urgent concern for the movement after the failure of the Waxman-Markey cap and trade legislation in 2010. And so the movement set about attempting to move the locus of climate catastrophe from the future to the present.&lt;/p&gt;
    &lt;p&gt;If you want to know why Pielke has been so demonized over the last 15 years by climate activists and activist climate scientists, it‚Äôs because he got in the way of this new narrative. Pielke‚Äôs work, going back to the mid-1990s showed, again and again, that the normalized economic costs of climate related disasters weren‚Äôt increasing, despite the documented warming of the climate. And unlike a lot of researchers who sometimes produce studies that cut against the climate movement‚Äôs chosen narratives, he wasn‚Äôt willing to be quiet about it. Pielke got in the way of the advocacy community at the moment that it was determined to argue that present day disasters were driven by climate change and got run over.&lt;/p&gt;
    &lt;p&gt;But the cognitive dissonance for me went well beyond that. It wasn‚Äôt just that Pielke had produced strong evidence that undermined a key claim of the climate advocacy community. It wasn‚Äôt even witnessing Pielke‚Äôs cancellation, which was brutal. It came, rather, as I came to understand why you couldn‚Äôt find a climate change signal in the disaster loss data, despite close to a degree and a half of warming over the last century or so.&lt;/p&gt;
    &lt;p&gt;That comes down to two linked factors that determine how climate becomes weather and, in turn, how weather contributes to climate related natural disasters. Taking the second of these first, climate related natural disasters are not simply the result of bad weather. They happen at the intersection of weather and human societies. What determines the cost of a climate related disaster, in both human and economic terms, is not just how extreme the weather is. It is also how many people and how much wealth is affected by the extreme weather event and how vulnerable they are to that event. Over the same period that the climate has warmed by 1.5 degrees, the global population has more than quadrupled, per capita income has increased by a factor of ten, and the scale of infrastructure, social services, and technology that protect people and wealth from climate extremes has expanded massively. These latter factors simply overwhelm the climate signal.&lt;/p&gt;
    &lt;p&gt;But it's not just that these other factors‚Äîexposure and vulnerability to climate hazards‚Äîare such huge factors in determining the costs of climate related disasters. Hence, the second problem with claims that climate change causes natural disasters is that anthropogenic climate change is simply a much smaller factor at the local and regional scale than natural climate variability. There is nothing in the climate science literature that has changed this basic fact since Hansen made the same observation over 15 years ago.&lt;/p&gt;
    &lt;p&gt;Over the last several years, some climate scientists, including Hausfather and Hansen, have pointed to anomalously high surface and ocean temperatures as evidence that warming may be accelerating, perhaps even faster than model ensembles have suggested. But even in the case where climate sensitivity proves to be relatively high, additional anthropogenic warming is an order of magnitude less than the oscillations of natural variability.&lt;/p&gt;
    &lt;p&gt;This basic physical reality can get lost in the enormous climate impacts literature, with its confusing terminology and findings around concepts like attribution and detection. Arguments about whether anthropogenic climate change has had any impact on extreme events of various sorts quickly get mixed up with arguments about whether climate change is a major factor, much less the major factor in extreme events.&lt;/p&gt;
    &lt;p&gt;Consider this twitter debate last year about the effects of climate change on tropical cyclones generally and Hurricane Helene specifically, featuring a number of former Breakthrough Institute staff and senior fellows, including Norris, Jesse Jenkins, Hausfather, and Pielke. In the back and forth, Hausfather cites a study concluding that climate change had resulted in a 10% increase in rainfall associated with hurricanes and tropical storms during the 2020 North Atlantic hurricane season. Norris cites an outlier study from Lawrence Berkeley researchers estimating that climate change could have increased rainfall in parts of Georgia and North Carolina by as much as 50%. Jenkins links to a NOAA factsheet summarizing a range of data and modeling on evidence for intensification of tropical cyclones globally and in various regional basins and argues that IPCC assessments of the role that climate change is playing in disasters like Helene are outdated. Pielke and others point back to IPCC and other broader literature assessments which conclude that there is weak evidence for detection and attribution of increased tropical cyclone frequency or intensity due to climate change to date.&lt;/p&gt;
    &lt;p&gt;Depending on how much weight one gives to individual studies and models, versus broader literature reviews and scientific assessments, you can find some evidence for some intensification of some features of tropical cyclone behavior and frequency in some places. But what you won‚Äôt find, Norris‚Äô reference to a single unpublished and unpeer-reviewed study notwithstanding, is good evidence that climate change has affected those things very much.&lt;/p&gt;
    &lt;p&gt;The absence of an anthropogenic climate signal in most climate and weather phenomena is not paradoxical. It is simply not possible given the amount of anthropogenic warming the planet has experienced. When scientists, journalists, and activists say that climate change made a given extreme event far more likely, what they are actually saying is that an event that is somewhat more intense than it would have been absent climate change could have been made so by climate change. To take the simplest example, a heatwave that is 1.5 degrees warmer than it would have been without climate change was made vastly more likely to occur due to climate change. The claim is tautological.&lt;/p&gt;
    &lt;p&gt;Put these two factors together‚Äîthe outsized influence that exposure and vulnerability have on the cost of extreme climate and weather phenomena, and the very modest intensification that climate change contributes to these events, when it plays any role at all‚Äîand what should be clear is that climate change is contributing very little to present day disasters. It is a relatively small factor in the frequency and intensity of climate hazards that are experienced by human societies, which in turn play a small role in the human and economic costs of climate related disasters compared to non-climate factors.&lt;/p&gt;
    &lt;p&gt;This also means that the scale of anthropogenic climate change that would be necessary to very dramatically intensify those hazards, such that they overwhelm the non-climate factors in determining the consequences of future climate related events, is implausibly large. The amount of warming that is conceivable even in plausible worst case scenarios, in other words, is not remotely consistent with the sorts of catastrophic outcomes that I once believed in, where tens or hundreds of million, perhaps even billions of lives were at stake.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Sting in the Tail?&lt;/head&gt;
    &lt;p&gt;For a long time, even after I had come to terms with the fundamental disconnect between what climate advocates were saying about extreme events and the role that climate change could conceivably be playing, I held on to the possibility of catastrophic climate futures based upon uncertainty. The sting, as they say, is in the tail, meaning so-called fat tails in the climate risk distribution. These are tipping points or similar low probability, high consequence scenarios that aren‚Äôt factored into central estimates. The ice sheets could collapse much faster than we understand or the gulf stream might shut down, bringing frigid temperatures to western Europe, or permafrost and methane hydrates frozen in the sea floor might rapidly melt, accelerating warming.&lt;/p&gt;
    &lt;p&gt;These and many other so-called tipping points commonly invoked as reason for precaution are the known unknowns of climate risk‚Äîspecific phenomena that we know might happen without being able to specify very precisely their probability and magnitude, the timeframe over which they might occur, or the threshold of warming and other factors that might trigger them.&lt;/p&gt;
    &lt;p&gt;But like the supposed collapse of the Amazon, once you look more closely at these risks they don‚Äôt add up to catastrophic outcomes for humanity. While sensationalist news stories frequently refer to the collapse of the gulf stream, what they are really referring to is the slowing of the Atlantic Meridian Overturning Circulation (AMOC). AMOC helps transport warm water to the North Atlantic and moderates winter temperatures across western Europe. But its collapse, much less its slowing, would not result in a hard freeze across all of Europe. Indeed, under plausible conditions in which it might significantly slow, it would act as a negative feedback, counterbalancing warming, which is happening faster across the European continent than almost any place else in the world.&lt;/p&gt;
    &lt;p&gt;Permafrost and methane hydrate thawing, meanwhile, are slow processes not fast ones. Even irreversible melting would occur over millennial timescales, fast in geological terms but very slow in human terms. The same is true of accelerated melting of ice caps. Even under very high warming scenarios, broadly acknowledged today as improbable, the Greenland and West Antarctic ice sheets contribute around a meter of sea level rise by the end of this century. Those processes would continue far into the future. But even very accelerated scenarios for rapid disintegration of ice sheets unfold over many centuries, not decades.&lt;/p&gt;
    &lt;p&gt;Moreover, the problem with grounding strong precautionary claims in these known unknowns is that doing so demands strong remedies in the present in response to future risks that are both unquantifiable and unfalsifiable, a problem made even worse by the fact that ‚Äúfat tail‚Äù proponents generally then proceed to ignore the fact that the unknown, unquantifiable, and unfalsifiable risks they are referring to are incredibly low probability and instead set about centering them in the climate discourse.&lt;/p&gt;
    &lt;p&gt;I recently took issue with Varun Sivaram‚Äôs misuse of the concept in his recent ‚ÄúClimate Realism‚Äù project at the Council on Foreign Relations. And a follow up conversation he had with Dan Raimi on the Resources for the Future podcast is illustrative of the problem.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Sivaram: I think it is terribly wishful thinking to think that climate change poses a manageable risk‚Ä¶ and that the climate scientists have got it all wrong‚Ä¶ I do believe, however, that the tail risks, the greater-than-5-percent-chance risks, are so material that they signal the end of society as we know it in the United States‚Ä¶&lt;/p&gt;
      &lt;p&gt;Raimi: That argument is very consistent with Martin Weitzman‚Äôs famous arguments about tail risks in the economic community, which I think have been very well understood but not necessarily applied in the policy context. You‚Äôre making this really clear argument that those tail risks need to be more central in our planning and in our thinking about the future.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Sivaram, here, is quite blatantly conflating tail risks, which are definitionally below 5% risks, with central risks. This has nothing to do with climate scientists getting the basics of climate change wrong and everything to do with Sivaram getting statistical risk wrong. Raimi, a long-time fellow at what is arguably the premier energy resources and economics think tank in the United States just goes along with it, attributing this notion to Martin Weitzman and suggesting that tail risks are well understood in the economics community.&lt;/p&gt;
    &lt;p&gt;To the contrary, Weitzman‚Äôs entire point was literally the opposite, that outsized risks in the tail of the climate risk distribution were poorly understood and might not exist at all. Other than price a bit more risk into the central estimates, Weitzman was explicit that there wasn‚Äôt actually anything to be done about the problem. And remember, what Sivaram is arguing must be done in response to these misplaced tail risks is to bring the full weight of American soft and hard power to bear on poor countries around the world to prevent them from developing their economies with fossil fuels, even as he concedes that the United States is unlikely to quickly move away from them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Clean Energy Without Catastrophism&lt;/head&gt;
    &lt;p&gt;As with Norris and Jenkins, I‚Äôve known Sivaram and Raimi for a long time. I agree with them on many subjects: on the value of clean energy and public support for energy technology innovation, the need for the global poor to have much greater access to energy, and the damage both psychic and political that doomism does to efforts to shift the world toward greener energy. They are all well intentioned.&lt;/p&gt;
    &lt;p&gt;And yet, all make representations about climate science and climate risk that are dubious, if not false. And my question is why? Why do so many smart people, most trained as scientists, engineers, lawyers, or public policy experts, and all who will tell you, and I say this not ironically, that they ‚Äúbelieve in science,‚Äù get the science of climate risk so badly wrong?&lt;/p&gt;
    &lt;p&gt;There are, in my view, several reasons. The first is that highly educated people with high levels of science literacy are no less likely to get basic scientific issues wrong than anyone else when the facts conflict with their social identities and ideological commitments. Yale Law Professor Dan Kahan has shown that people who are highly concerned about climate change actually have less accurate views about climate change overall than climate skeptics and that this remains true even among partisans with high levels of education and general science literacy. Elsewhere, Kahan and others have demonstrated that on many issues, highly educated people are often more likely to stubbornly hold onto erroneous beliefs because they are more expert at defending their political views and ideological commitments.&lt;/p&gt;
    &lt;p&gt;The second reason is that there are strong social, political, and professional incentives if you make a living doing left of center climate and energy policy to get climate risk wrong. The capture of Democratic and progressive politics by environmentalism over the last generation has been close to total. There is little tolerance on the Left for any expression of materialist politics that challenge foundational claims of the environmental movement. Meanwhile the climate movement has effectively conflated consensus science about the reality and anthropogenic origins of climate change with catastrophist claims about climate risk for which there is no consensus whatsoever.&lt;/p&gt;
    &lt;p&gt;Whether you are an academic researcher, a think tank policy wonk, a program officer at an environmental or liberal philanthropy, or a Democratic Congressional staffer, there is simply no benefit and plenty of downside to questioning, much less challenging, the central notion that climate change is an existential threat to the human future. It‚Äôs a good way to lose friends or even your job. It won‚Äôt help you get your next job or your next grant. And so everyone, mostly falls in line. Better to go along to get along.&lt;/p&gt;
    &lt;p&gt;Finally, there is a widespread belief that one can‚Äôt make a strong case for clean energy and technological innovation absent the catastrophic specter of climate change. ‚ÄúWhy bother with nuclear power or clean energy if climate change is not a catastrophic risk,‚Äù is a frequent response. And this view simply ignores the entire history of modern energy innovation. Over the last two centuries, the world has moved inexorably from dirtier and more carbon intensive technologies to cleaner ones. Burning coal, despite its significant environmental impacts, is cleaner than burning wood and dung. Burning gas is cleaner than coal. And obviously producing energy with wind, solar, and nuclear is cleaner than doing so with fossil fuels.&lt;/p&gt;
    &lt;p&gt;There is a view among most climate and clean energy advocates that the risk of climate change both demands and is necessary to justify a much faster transition toward cleaner energy technologies. But as a practical matter, there is no evidence whatsoever that 35 years of increasingly dire rhetoric and claims about climate change have had any impact on the rate at which the global energy system has decarbonized and by some measure, the world decarbonized faster over the 35 years prior to climate change emerging as a global concern than it did in the 35 years since.&lt;/p&gt;
    &lt;p&gt;This argument ultimately becomes circular. It‚Äôs not that there is no reason to support cleaner energy absent fear of catastrophic climate change. It‚Äôs that there is no reason to support a rapid transformation of the global energy economy at the speed and scale necessary to avoid catastrophic climate change if the specter of catastrophic climate change is not looming. Which is arguably true but is also a proposition that depends upon not asking particularly hard questions about the nature of climate risk.&lt;/p&gt;
    &lt;p&gt;Despite some tonal, tactical, and strategic differences, this basic view of climate risk, and corresponding demand for a rapid transformation of the global energy economy is broadly shared by the climate activists and the pragmatists. The impulse is millenarian, not meliorist. Underneath the real politik, technocratic wonkery, and appeals to scientific authority is a desire to remake the world.&lt;/p&gt;
    &lt;p&gt;For all its worldly and learned affect, what that has resulted in is the creation of an insular climate discourse on the Left that may be cleverer by half than right wing dismissals of climate change but is no less prone to making misleading claims about the subject, ignoring countervailing evidence, and demonizing dissent. And it has produced a politics that is simultaneously grandiose and maximalist and, increasingly, deeply out of touch with popular sentiment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.breakthroughjournal.org/p/why-i-stopped-being-a-climate-catastrophist"/><published>2025-11-05T20:13:37+00:00</published></entry></feed>