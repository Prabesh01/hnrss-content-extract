<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-19T14:34:38.891624+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45611252</id><title>K8s with 1M nodes</title><updated>2025-10-19T14:34:50.473982+00:00</updated><content>&lt;doc fingerprint="e798c941e1211d92"&gt;
  &lt;main&gt;
    &lt;p&gt;This is an effort to create a fully functional Kubernetes cluster with 1 million active nodes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why?&lt;/head&gt;
    &lt;p&gt;Several years ago at OpenAI I helped author Scaling Kubernetes to 7500 Nodes which remains one of the CNCF’s most popular blog posts. Alibaba made a post about running Kubernetes clusters with 10K nodes. Google made a post about 15K nodes with Bayer Crop Science. Fast forward to today, GKE supports running some clusters up to 65K nodes, and AWS recently announced support for clusters up to 100K nodes.&lt;/p&gt;
    &lt;p&gt;In online forums and in my own conversations with peers, I’ve encountered a lot of debate about how big a Kubernetes cluster can get. What tends to be lacking from these discussions is hard data and evidence-backed justifications. I’ve worked with engineers reluctant to push things beyond what they’ve seen before because they’re fearful or uncertain of what may go wrong. Or when something does go wrong, the response is to scale down the cluster rather than understand and address the bottleneck.&lt;/p&gt;
    &lt;p&gt;The spirit of the k8s-1m project is to identify the hard blockers to scalability. What are the actual limits of how hard a particular system can be pushed, and why are those the limits? What would it take to overcome those limits to make the system even more scalable? What types of workloads can be run at greater scale than others?&lt;/p&gt;
    &lt;p&gt;Many limitations are imposed by software. As software engineers, it’s on us to work through those limitations. Once we’ve done all we can on the software side and the remaining bottlenecks are in hardware, only then should we wait on next generation hardware to achieve our goals.&lt;/p&gt;
    &lt;p&gt;I don’t have ambitions to commercialize this effort. Certainly not as-is. I’m fine making the code freely available. Realistically, while all of the claims and results in this project are true and verifiable, each large-scale Kubernetes cluster has its own unique workload demands that may or may not work well with this design. So I wouldn’t classify this as "production-ready" or recommend that anyone use it for mission-critical workloads.&lt;/p&gt;
    &lt;p&gt;At the same time, I hope that the proposed designs here inspire the Kubernetes community with what is possible and influence the future roadmap of the project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Components&lt;/head&gt;
    &lt;p&gt;Early on in this project, I asked ChatGPT “I want to scale Kubernetes to 1 million nodes. What types of problems would I need to overcome?”&lt;/p&gt;
    &lt;p&gt;Here’s a portion of what I got:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Scaling Kubernetes to 1 million nodes is a formidable challenge and involves overcoming a variety of technical hurdles. Here are the key areas you would need to address:&lt;/p&gt;
      &lt;p&gt;etcd Scalability and Performance: etcd is the backbone of Kubernetes' storage, handling all API object data. With 1 million nodes, the volume of data managed by etcd will increase significantly. Optimizing etcd’s performance, including efficient data partitioning, tuning the database to handle high loads, and possibly looking into alternatives or supplements to etcd for certain types of data, is crucial.&lt;/p&gt;
      &lt;p&gt;kube-apiserver Performance: The Kubernetes kube-apiserver would face enormous loads in a cluster of this size. Optimizing its performance includes horizontal scaling (running multiple kube-apiserver instances), rate limiting, and efficient caching mechanisms. Also, minimizing the watch events and reducing the overall number of requests to the kube-apiserver is important.&lt;/p&gt;
      &lt;p&gt;Networking: Handling network traffic for 1 million nodes is extremely challenging. This includes the management of IP addresses, efficient routing, and possibly segmenting the network to prevent it from becoming a bottleneck. Solutions like Cilium, which uses eBPF technology for networking and security, might be necessary.&lt;/p&gt;
      &lt;p&gt;Scheduling and Resource Management: The scheduler will have increased latency as it tries to place pods optimally across a vast number of nodes. Enhancing the scheduler to handle large scale operations efficiently, possibly by implementing custom schedulers for specific tasks, is required.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It goes on to list other challenges, including Logging &amp;amp; Monitoring, Security, and Storage management.&lt;/p&gt;
    &lt;p&gt;I think these first items are the most critical. There are a wide range of options for support for logging or PVCs, but if you can’t run etcd or schedule pods or have the pods talk to one another, you’re not getting anywhere.&lt;/p&gt;
    &lt;p&gt;Below I’m going to discuss the three major areas I tackled.&lt;/p&gt;
    &lt;head rend="h3"&gt;Networking&lt;/head&gt;
    &lt;p&gt;The common challenges with networking in Kubernetes are IP address space, service discovery, and network policies (aka firewalling). Compared to later challenges, configuring networking to support 1M nodes turned out to be relatively easy.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pod IPs&lt;/head&gt;
    &lt;p&gt;It can be a challenge to plan IP address space in large clusters. Efficient routing usually means routing a contiguous CIDR to each individual node, which means you’re pre-allocating how many IP addresses (and thus pods) each node can serve. Some nodes that are intended for lots of small workloads end up constrained by the number of available IP addresses before they run out of other hardware resources. A 10.0.0.0/8 has 16 million IPs. For a 1-million node cluster, that leaves just 15 pod IPs per node, which is likely not enough.&lt;/p&gt;
    &lt;p&gt;The answer is to use IPv6 thoroughly and exclusively. The enormous IPv6 address space means that there’s plenty of room for every pod to have its own globally accessible IP address.&lt;/p&gt;
    &lt;p&gt;Kubernetes has great support for IPv6, and it requires no code changes to create a fully functioning Kubernetes cluster that uses IPv6 exclusively.&lt;/p&gt;
    &lt;p&gt;My goal is for each node to have an IPv6 address prefix with a range large enough so each pod on that node can have its own IP out of that range. Plus, of course, at least one for the host itself.&lt;/p&gt;
    &lt;p&gt;Of course, using IPv6 requires support from your compute vendor. All of the major cloud providers (and even many less-major) support IPv6 to some degree or another. And with public IPv6, it’s trivial to create a single cluster that spans multiple clouds.&lt;/p&gt;
    &lt;p&gt;I primarily focused on AWS, GCP, and Vultr. (Scoff if you want at Vultr, but they have cheap compute and I am self-bootstrapping this project.) But each one has slightly different twists on its support for IPv6 addressing inside a VM. To give a sense of the range of situations, let me briefly describe each below:&lt;/p&gt;
    &lt;p&gt;Vultr: Each node gets a /64. The primary IP of the node is the ::1 of that range, and the server automatically receives all traffic for any IP in the full /64 range.&lt;/p&gt;
    &lt;p&gt;GCP: Each node gets a /96. The primary IP of the node is some random IP within that range. The server must send valid NDP RA packets for the IPs that it wants to receive traffic for.&lt;/p&gt;
    &lt;p&gt;AWS: Each node gets a /128. You can add a /80 prefix (that comes from a different range) via an API call to an existing NIC or VM. (The ‘Create’ API looks like it can support setting both an IPv6 address and an IPv6 range at creation time, but you’ll get an error). The server must send valid NDP RA packets for the IPs it wants to receive traffic for, and all outgoing packets must use the one MAC address that matches the primary IP.&lt;/p&gt;
    &lt;p&gt;To satisfy the intersection of these requirements, particularly the requirement about MAC addresses, I create one bridge for all of the pods' interfaces to share. But leave the host interface separate, and enable forwarding to handle traffic between the bridge and the host interface. A host-local IPAM is set to a /96 IPv6 prefix of what I get from the provider. This gives us a full 2^32 IPs per node, plenty of space for pods.&lt;/p&gt;
    &lt;p&gt;Because these are global public IPv6 addresses, no special routing is necessary. No packet encapsulation or NAT is used. Traffic from each pod is correctly sourced from its true origin pod IP, regardless of destination.&lt;/p&gt;
    &lt;head rend="h4"&gt;IPv4-only external service dependencies&lt;/head&gt;
    &lt;p&gt;If you only have an IPv6 address, then you can only reach other IPv6 addresses on the internet. Anything that is IPv4-only isn’t directly accessible.&lt;/p&gt;
    &lt;p&gt;Most services I used in this project worked fine: Ubuntu packages, PyPi packages, the docker.io registry. The main exception was GitHub. Github.com remains stubbornly IPv4-only. Tsk tsk.&lt;/p&gt;
    &lt;p&gt;Many AWS services have dual-stack endpoints but notably for this project Elastic Container Registry (ECR) does not. Tsk tsk to them as well.&lt;/p&gt;
    &lt;p&gt;For IPv6 devices to reach IPv4 hosts, most cloud providers offer some sort of NAT64 gateway. You can also roll your own gateway on a Linux VM. I over-engineered this a bit with a custom WireGuard server. All VMs connect via WireGuard to this server and use it as an IPv4 gateway.&lt;/p&gt;
    &lt;head rend="h4"&gt;Network Policies&lt;/head&gt;
    &lt;p&gt;High-level, I hand-waved over this problem and did not use network policies between workloads.&lt;/p&gt;
    &lt;p&gt;1 million nodes would have 1 million separate IPv6 prefixes, which is far too many individual entries for any firewall solution to support. Security-minded folks: clutch your pearls when I say that I do not use extensive firewalling to prevent access into the cluster from the Internet. I do use firewall rules to limit to a select few number of ports that I know need to be reached, but otherwise we must rely on other techniques to safeguard unauthorized inbound access to servers and pods.&lt;/p&gt;
    &lt;p&gt;Thorough use of TLS covers most use cases for this project. The enormous size of the IPv6 address space also makes scanning impractical. Cilium, kube-proxy, or other network plugins could also limit which pods can reach which pods, but at significant cost of additional watches on the control plane.&lt;/p&gt;
    &lt;p&gt;If you’re using one single vendor for all of your nodes, it may be plausible that all nodes still get ipv6 ranges out of 1 or a few larger spaces, a count low enough that could be reasonably installed as firewall rules.&lt;/p&gt;
    &lt;head rend="h4"&gt;Network flow needs (# of TCP connections)&lt;/head&gt;
    &lt;p&gt;Both kube-apiservers and etcd support both HTTP/2 and gRPC. Many individual requests and streams are multiplexed over a single TCP connection. Kubernetes sets a default HTTP/2 limit of 100 concurrent requests (or technically streams) per TCP connection. (HTTP/2 can support far more than that, but as you add more streams you run into performance problems like head-of-line blocking). So each kubelet needs at least 1 connection to the kube-apiserver control plane. And you can expect 1 more connection for kube-proxy, or any similar CNI like Cilium or Calico. With 1M nodes, that means each kube-apiserver is supporting at least 2 million TCP connections. With 8 kube-apiservers, each server would be supporting 250K connections to kubelets.&lt;/p&gt;
    &lt;p&gt;Linux itself can support this number of connections with some light tuning. And of course make sure you have allowed yourself enough file descriptors. Nevertheless it may be more than your network provider can support. Azure, for example, documents that it can support a maximum of 500k inbound and 500k outbound connections per VM. GCP and AWS do not publish limits, but there are limits in any system to both the total number of concurrent connections as well as the rate of new connections being made.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managing state&lt;/head&gt;
    &lt;p&gt;When I talk about “managing state,” I mean the API surface that Kubernetes exposes for interacting with resources. With careful tuning, the kube-apiserver can scale to sufficiently high levels of throughput. etcd, however, is the bottleneck. In this section, I’ll outline why that is and describe a replacement implementation that can meet the demands of a million-node cluster.&lt;/p&gt;
    &lt;head rend="h4"&gt;kube-apiservers vs etcd&lt;/head&gt;
    &lt;p&gt;First a quick overview about the ways you work with state in Kubernetes. Any number of clients interact with kube-apiservers, which then in turn interact with etcd.&lt;/p&gt;
    &lt;p&gt;kube-apiservers are stateless. etcd is the persistent store for all of Kubernetes resources. All CRUD operations you send to a kube-apiserver are actually persisted by etcd.&lt;/p&gt;
    &lt;p&gt;kube-apiservers have seven common verbs for state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;create&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;get&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;list&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;update&lt;/code&gt;(aka&lt;code&gt;replace&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;patch&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;delete&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;watch&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;etcd has four:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;put&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;range&lt;/code&gt;- includes&lt;code&gt;get&lt;/code&gt;with a null&lt;code&gt;range_end&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;deleteRange&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;watch&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;kube-apiserver &lt;code&gt;create&lt;/code&gt;, &lt;code&gt;update&lt;/code&gt;, &lt;code&gt;patch&lt;/code&gt;, and &lt;code&gt;delete&lt;/code&gt; all result in an etcd &lt;code&gt;put&lt;/code&gt; operation. (A &lt;code&gt;delete&lt;/code&gt; is just a &lt;code&gt;put&lt;/code&gt; with a null value). etcd doesn’t support any partial updates of values, only putting the entire value. So all operations that involve modifying a resource result in a new etcd &lt;code&gt;put&lt;/code&gt; of the entire resource contents.&lt;/p&gt;
    &lt;p&gt;kube-apiserver &lt;code&gt;watch&lt;/code&gt; can, but often doesn’t, result in an etcd &lt;code&gt;watch&lt;/code&gt;. More on that below.&lt;/p&gt;
    &lt;head rend="h4"&gt;Meeting the QPS needs for a 1M node cluster&lt;/head&gt;
    &lt;p&gt;Kubelets interact with the kube-apiserver primarily through two resource types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Node&lt;/code&gt;- the resource representing a server for running pods&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lease&lt;/code&gt;- a lightweight heartbeat object updated by&lt;code&gt;kubelet`&lt;/code&gt;to signal liveness&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Lease is critical: if it isn’t refreshed in time, the NodeController marks the node as &lt;code&gt;NotReady&lt;/code&gt;. By default, each kubelet updates its &lt;code&gt;Lease&lt;/code&gt; every 10 seconds. At a scale of 1 million nodes, that alone translates to 100K writes per second just to keep the nodes "alive."&lt;/p&gt;
    &lt;p&gt;Adding in the constant churn of other resources, the system needs to sustain on the order of many hundreds of thousands of writes per second, plus a significant volume of reads.&lt;/p&gt;
    &lt;p&gt;For kube-apiserver, this is manageable. It’s stateless, so QPS can be scaled out simply by running more replicas. If one instance can’t handle the load, more can be added, and traffic will spread across them.&lt;/p&gt;
    &lt;p&gt;For etcd, things are different. Etcd is stateful, which makes scaling QPS much harder.&lt;/p&gt;
    &lt;head rend="h4"&gt;etcd is too slow&lt;/head&gt;
    &lt;p&gt;Using the etcd-benchmark tool, I measured about 50K writes/sec out of a single etcd instance backed by NVMe storage. Importantly, adding replicas doesn’t help. Write throughput actually drops with more members since each write must be coordinated across a quorum of replicas to maintain consistency. So with the typical 3-replica setup, effective write QPS is even lower than the benchmarked 50K/s. That’s nowhere near what’s needed to support a 1M-node cluster.&lt;/p&gt;
    &lt;p&gt;At first glance, 50K QPS seems surprisingly low given modern hardware capabilities. A single NVMe drive can do over 1M 4K writes per second, and a single DDR5 DIMM can push 10x more than that. So why is etcd is far behind raw hardware limits?&lt;/p&gt;
    &lt;p&gt;The answer lies in etcd’s interface and guarantees. For one thing, etcd is ensuring that all writes are durable to disk. For every &lt;code&gt;put&lt;/code&gt; or &lt;code&gt;delete&lt;/code&gt; call, etcd ensures the change is written to disk via &lt;code&gt;fsync&lt;/code&gt; before acknowledging success. This helps ensure that there is never any data loss if the host crashes or loses power. But that durability drastically reduces the number of IOPS that a modern NVMe drive can support.&lt;/p&gt;
    &lt;p&gt;Plus, etcd has a pretty broad interface surface area:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;It is a key value store and so of course supports reads, writes, and deletes of single objects.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It can support querying a range of sorted keys.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It keeps history for all changes, so you can query for an older version of a particular key, or even a range of keys. Older changes eventually get “compacted” to reduce state size.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It has a notion of “watches”, meaning it can stream out all of the changes that affect a particular key or range of keys.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It also has a “lease” API, where keys can be attached to a TTL that will cause them to expire if not renewed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It supports transactions, supporting an atomic If/Then/Else.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Implementing all of those interfaces can make for complex software. Beyond simple puts and deletes, etcd must support transactions, maintain multi-versioned history, and enforce Raft-based consensus across replicas.&lt;/p&gt;
    &lt;p&gt;These features are what give Kubernetes its consistency and reliability, but they also impose strict constraints on performance. Intuitively, strong consistency means more serialization: operations can’t always be parallelized freely. Writes often need to follow a carefully ordered path through Raft, the WAL, and compaction, ensuring that every replica agrees on state before acknowledging success.&lt;/p&gt;
    &lt;p&gt;The result is raw hardware capable of millions of writes per second, but etcd delivering orders of magnitude less due to the interfaces and guarantees it must uphold.&lt;/p&gt;
    &lt;p&gt;But do we need all of these things?&lt;/p&gt;
    &lt;head rend="h5"&gt;Reduce durability and eliminate replicas&lt;/head&gt;
    &lt;p&gt;Perhaps my spiciest take from this entire project: most clusters don’t actually need the level of reliability and durability that etcd provides.&lt;/p&gt;
    &lt;p&gt;As we’ll see in the next section, the majority of writes in a Kubernetes cluster are for ephemeral resources.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Kubernetes&lt;/p&gt;&lt;code&gt;Events&lt;/code&gt;may only stick around for minutes.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lease&lt;/code&gt;objects typically expire within tens of seconds.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the cluster is disrupted, restoring these objects is almost never useful, and certainly not to the precision of their last few milliseconds of updates. Even for longer-lived objects, Kubernetes is designed to reconcile automatically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Nodes&lt;/code&gt;continually refresh status via&lt;code&gt;kubelet&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Controllers will bring&lt;/p&gt;&lt;code&gt;DaemonSet&lt;/code&gt;and&lt;code&gt;Deployment&lt;/code&gt;status back in sync with actual&lt;code&gt;Pods&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we stopped &lt;code&gt;fsync&lt;/code&gt;-ing these ephemeral writes, or even stopped writing them altogether and just relied on RAM, clusters could process far more operations and perform substantially better.&lt;/p&gt;
    &lt;p&gt;In fact, even full control plane data loss isn’t catastrophic in some environments. Many clusters are ephemeral themselves, with all configuration encoded in Terraform, Helm, or GitOps. In those cases, rebuilding is often easier than preserving every last write. Some organizations already treat Kubernetes clusters as cattle.&lt;/p&gt;
    &lt;p&gt;If you’re not mad yet, let me push you a little further: you probably don’t need etcd replicas at all.&lt;/p&gt;
    &lt;p&gt;In the 5 years I ran Kubernetes clusters at OpenAI, we never once had an unplanned VM outage on an etcd VM. etcd’s resource needs are tiny. The database is limited to 8GB. CPU is no more than 2-4 cores. Most cloud providers can do live migration on VMs this small. With network-attached storage like EBS, recovery is straightforward: spin up a replacement VM, attach the volume, and resume operation with zero data loss.&lt;/p&gt;
    &lt;p&gt;If you had just 1 etcd instance and that went down, your Kubernetes cluster control plane would go down. Pods would still stay running. Nodes would still be reachable. It’s possible that you could still serve traffic. If etcd used EBS, recovery would be the time to start a new VM and attach the volume, with no data loss.&lt;/p&gt;
    &lt;p&gt;Yes, running a single etcd instance is a single point of failure. But failures are rare and the practical impact is often negligible. Meanwhile, etcd replicas come with a significant performance cost. For many workloads, that tradeoff simply isn’t worth it.&lt;/p&gt;
    &lt;p&gt;Always stop writing &lt;code&gt;Event&lt;/code&gt; and &lt;code&gt;Lease&lt;/code&gt; to disk. Beyond that, you have some options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You don’t need durability: Run one replica, keep all state in memory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You can tolerate losing a few ms of updates: Run a single replica with a network-attached disk, but without&lt;/p&gt;&lt;code&gt;fsync&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You’d rather avoid data loss: Run multiple replicas in case one goes down, but don’t bother writing changes to disk. Rely on the uptime of the other replicas to keep from losing data.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You’re paranoid about data loss: Run a single replica with a network-attached disk, and enable&lt;/p&gt;&lt;code&gt;fsync&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Reduce the interface&lt;/head&gt;
    &lt;p&gt;As I described, etcd has a pretty broad interface surface area. But does Kubernetes actually use all of those features?&lt;/p&gt;
    &lt;p&gt;To measure this, I wrote a small tool called etcd proxy, The proxy sits between Kubernetes and etcd, transparently forwarding all traffic while logging every request and response.&lt;/p&gt;
    &lt;p&gt;With that in place, I spun up a Kubernetes cluster and ran Sonobuoy, the standard conformance test suite of Kubernetes. Sonobuoy systematically exercises the full API surface of Kubernetes, ensuring compliance with upstream expectations. Running it through the proxy produced a complete, real-world trace of the requests and workloads that etcd must handle in a conforming cluster.&lt;/p&gt;
    &lt;p&gt;It turns out that Kubernetes actually uses just a small amount of the etcd interface.&lt;/p&gt;
    &lt;p&gt;There’s of course &lt;code&gt;read&lt;/code&gt;, &lt;code&gt;write&lt;/code&gt;, &lt;code&gt;range&lt;/code&gt;, and &lt;code&gt;watch&lt;/code&gt; queries, but they all follow a few patterns.&lt;/p&gt;
    &lt;head rend="h6"&gt;Txn-Put&lt;/head&gt;
    &lt;p&gt;Kubernetes does do Txn queries, but they’re always of this form:&lt;/p&gt;
    &lt;code&gt;{
  "method": "/etcdserverpb.KV/Txn",
  "request": {
    "compare": [
      {
        "key": "SOMEKEY",
        "modRevision": "SOMEREV",
        "target": "MOD"
      }
    ],
    "success": [
      {
        "requestPut": {
          "key": "SOMEKEY",
          "value": "..."
        }
      }
    ],
    "failure": [
      {
        "requestRange": {
          "key": "SOMEKEY"
        }
      }
    ]
  }
}&lt;/code&gt;
    &lt;p&gt;In other words, do a &lt;code&gt;put&lt;/code&gt; if the &lt;code&gt;modRev&lt;/code&gt; of this key is set to this particular value, otherwise just return me the current version. And this makes sense, because Kubernetes is often patching or updating existing resources but turning that into a &lt;code&gt;put&lt;/code&gt; of the full resource safely means that the underlying resource must not have changed in between.&lt;/p&gt;
    &lt;head rend="h6"&gt;Leases&lt;/head&gt;
    &lt;p&gt;Note that Kubernetes Leases are not the same as etcd Leases. Kubernetes leases are implemented as regular K/V’s in etcd. Kubernetes makes very few etcd Leases.&lt;/p&gt;
    &lt;p&gt;The main area where Kubernetes uses etcd leases is on Events objects, e.g.:&lt;/p&gt;
    &lt;code&gt;{
  "method": "/etcdserverpb.Lease/LeaseGrant",
  "request": {
    "TTL": "3660"
  },
  "response": {
    "ID": "7587883212297104637",
    "TTL": "3660"
  }
}
{
  "method": "/etcdserverpb.KV/Txn",
  "request": {
    "compare": [
      {
        "key": "/registry/events/NAMESPACE/SOMEEVENT",
        "modRevision": "205",
        "target": "MOD"
      }
    ],
    "failure": [
      {
        "requestRange": {
          "key": "/registry/events/NAMESPACE/SOMEEVENT",
        }
      }
    ],
    "success": [
      {
        "requestPut": {
          "key": "/registry/events/NAMESPACE/SOMEEVENT",
          "lease": "7587883212297104637",
          "value": "..."
        }
      }
    ]
  }
}&lt;/code&gt;
    &lt;p&gt;The purpose of this is to manage some sane TTL on events. It’s not critical to the consistency model of Kubernetes.&lt;/p&gt;
    &lt;head rend="h6"&gt;Ranges&lt;/head&gt;
    &lt;p&gt;etcd could be implemented as a simple hash-table with O(1) insertion time, if it weren’t for range queries. Range queries return a sorted list of keys within a given span, which requires storing data in a sorted structure. Inserting into a sorted list or B-Tree is O(log n). In my view, supporting Range is thus the most difficult constraint that etcd must implement to be Kubernetes compatible. Nevertheless, it is critical.&lt;/p&gt;
    &lt;p&gt;Fortunately, we can take advantage of the predictable structure of the keyspace:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;/registry/[$APIGROUP/]$APIKIND/[$NAMESPACE/]$NAME&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Range queries are typically scoped to be either within a particular namespace, or across all namespaces for a given resource Kind. Kubernetes never performs a range query that spans across multiple resource Kinds (e.g., Pods and ConfigMaps together).&lt;/p&gt;
    &lt;p&gt;This introduces an opportunity: rather than one global B-tree for the entire keyspace, we can maintain separate B-trees per resource Kind. That shrinks the effective n in O(log n) to just the number of objects of a single kind, improving both inserts and queries.&lt;/p&gt;
    &lt;p&gt;Another wrinkle is the use of &lt;code&gt;limit&lt;/code&gt; on range queries. Kubernetes rarely needs to retrieve all objects at once; queries often return only 500, 1,000, or 10,000 results at a time. However, range responses are also expected to include a count field representing the total number of remaining objects. This undermines the benefit of &lt;code&gt;limit&lt;/code&gt;, since even merely counting all remaining keys can still be expensive.&lt;/p&gt;
    &lt;p&gt;In practice, though, Kubernetes doesn’t rely on &lt;code&gt;count&lt;/code&gt; being exact. It only needs to know that there are more than &lt;code&gt;limit&lt;/code&gt; results available. This looser requirement leaves room for approximation, and is one area where further optimizations are possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;mem_etcd: custom in-memory etcd&lt;/head&gt;
    &lt;p&gt;I built a new program called mem_etcd that implements the etcd interface but with the simplifications described above. Written in Rust, it provides fully correct semantics for the etcd APIs that Kubernetes depends on.&lt;/p&gt;
    &lt;p&gt;mem_etcd maintains two main data structures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;A hash map storing the full keyspace&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A B-tree indexing the keys within each prefix.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each value also stores the non-compacted revision history for that key. This design makes writes to existing keys O(1), while writes to new keys and range queries are O(log n) (where n is the number of resources of that &lt;code&gt;Kind&lt;/code&gt;). &lt;code&gt;Range&lt;/code&gt; queries also require additional linear work up to the query’s limit.&lt;/p&gt;
    &lt;p&gt;Despite its name, mem_etcd can provide durability by writing a write-ahead log (WAL) to disk. Each prefix of &lt;code&gt;/registry/[$APIGROUP/]$APIKIND/[$NAMESPACE/]&lt;/code&gt; is written to its own separate file. By default, files are written in &lt;code&gt;buffered&lt;/code&gt; mode, so &lt;code&gt;put&lt;/code&gt; calls can complete before the data is durably written to disk. This behavior can be changed with a CLI flag that enables &lt;code&gt;fsync&lt;/code&gt;, forcing all writes to be flushed to disk before the &lt;code&gt;put&lt;/code&gt; completes.  You can also configure some prefixes to not be written to disk at all.&lt;/p&gt;
    &lt;code&gt;% (cd /tmpfs ; etcd-3.5.16 --snapshot-count=9999999999 --quota-backend-bytes=9999999999) &amp;amp;
% parallel -j $X --results out_{#}.txt './benchmark put --total 10000000 --clients 1000 --conns 10 --key-space-size 10000000 --key-size=48 --val-size=1024' ::: {1..$X}&lt;/code&gt;
    &lt;p&gt;These tests were run on a pair of &lt;code&gt;c4d-standard-192-lssd&lt;/code&gt; instances, with one VM running mem_etcd and the other running the client benchmark. In these results, you can easily observe how badly enabling &lt;code&gt;fsync&lt;/code&gt; negatively impacts throughput and latency. Note that the baseline comparison of etcd is a single replica of etcd v3.5.16 running on a tmpfs (ram-based) disk. This should be an optimal environment for etcd as there is no actual disk involved and &lt;code&gt;fsync&lt;/code&gt;, while still being a syscall, is otherwise a no-op. mem_etcd is storing its WAL on a local NVMe, what GCE calls Titanium SSD. Though the instance type has 16 local disks, only 1 is used for this test.&lt;/p&gt;
    &lt;code&gt;% timeout 10 parallel -j $X --results out_{#}.txt   './etcd-lease-flood -num-keys 1000 -workers 100 -key-prefix {#}' ::: {1..$X}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;etcd-lease-flood&lt;/code&gt; is a custom benchmark designed to simulate the dominant type of load in a large Kubernetes cluster. Each client creates 100 &lt;code&gt;Lease&lt;/code&gt; objects directly in etcd, using the same protobuf encoding as Kubernetes. For each &lt;code&gt;Lease&lt;/code&gt;, the client repeatedly issues &lt;code&gt;put&lt;/code&gt; updates in a tight loop, attempting to update the &lt;code&gt;Lease&lt;/code&gt; as quickly as possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Watch()&lt;/head&gt;
    &lt;p&gt;There are several different types of watches and each has different performance characteristics. Let’s unpack them.&lt;/p&gt;
    &lt;p&gt;https://kubernetes.io/docs/reference/using-api/api-concepts/#semantics-for-watch has some useful details about how the kube-apiserver handles the parameters of your watch:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;resourceVersion unset: Get State and Start at Most Recent&lt;/p&gt;&lt;lb/&gt;Start a watch at the most recent resource version, which must be consistent (in detail: served from etcd via a quorum read). To establish initial state, the watch begins with synthetic "Added" events of all resources instances that exist at the starting resource version. All following watch events are for all changes that occurred after the resource version the watch started at.&lt;p&gt;resourceVersion=`"0`": Get State and Start at Any&lt;/p&gt;&lt;lb/&gt;Start a watch at any resource version; the most recent resource version available is preferred, but not required…. To establish initial state, the watch begins with synthetic "Added" events for all resource instances that exist at the starting resource version. All following watch events are for all changes that occurred after the resource version the watch started at.&lt;p&gt;resourceVersion=`"{value other than 0}`": Start at Exact&lt;/p&gt;&lt;lb/&gt;Start a watch at an exact resource version. The watch events are for all changes after the provided resource version. Unlike "Get State and Start at Most Recent" and "Get State and Start at Any", the watch is not started with synthetic "Added" events for the provided resource version.&lt;/quote&gt;
    &lt;p&gt;Let’s re-format the important points into a table:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;resourceVersion unset&lt;/cell&gt;
        &lt;cell role="head"&gt;resourceVersion=0&lt;/cell&gt;
        &lt;cell role="head"&gt;resourceVersion&amp;gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Served from kube-apiserver state (instead of etcd)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;❌&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;✅&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;✅&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Includes an initial list&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;✅&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;✅&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;❌&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So a &lt;code&gt;watch&lt;/code&gt; is often preceded by a &lt;code&gt;list&lt;/code&gt;. The &lt;code&gt;list&lt;/code&gt; provides a snapshot-in-time of a set of resources, marked with a revision number. Then you start a watch from that revision number, which will then stream to you all of the changes that have occurred since that revision.&lt;/p&gt;
    &lt;p&gt;When &lt;code&gt;resourceVersion&lt;/code&gt; is set, a watch against the kube-apiserver does not create a new watch against etcd. At startup time, a kube-apiserver creates &lt;code&gt;watch&lt;/code&gt; streams against etcd for each of the well-known standard resources. Any time a client creates a watch, the kube-apiserver handles that stream itself based on the one etcd watch stream it maintains. So while client watches can be expensive for kube-apiservers, it adds no additional load to etcd. You can horizontally scale more kube-apiservers.&lt;/p&gt;
    &lt;p&gt;Furthermore, watches are not really that bad for etcd. A watch has a beginning and an end range, and those ranges fit within the same prefixes as Range queries. With each Put we need to do a log(n) lookup in the list of watches to find watches that could match that key. But there are far far fewer watches than objects. The n is small and is done asynchronously after the write is committed anyway, so it does not affect the request time to complete a write.&lt;/p&gt;
    &lt;p&gt;Watches do create network amplification. For each write into etcd, there may be N corresponding watches for that object. That results in a lot of outbound network traffic from etcd. The kube-apiservers are on the receiving end of these watches. kube-apiservers are consolidating their own watches, but etcd is still sending a copy of the data to each kube-apiserver. While adding more kube-apiserver replicas does help with many Kubernetes scalability problems, each replica does put additional pressure on the etcd NIC. The network throughput of etcd is the most immediate hardware bottleneck of large-scale Kubernetes clusters. However, these demands are limited to just between etcd and the kube-apiservers. In a single datacenter with modern hardware there’s still plenty of potential interconnect that could be established amongst these servers.&lt;/p&gt;
    &lt;head rend="h5"&gt;Watches per node&lt;/head&gt;
    &lt;p&gt;By scaling up the number of nodes I was able to observe how many watches each node creates. Per kubelet + kube-proxy, I observe:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;4 watches of&lt;/p&gt;
        &lt;code&gt;configmaps&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;2 watches each of&lt;/p&gt;&lt;code&gt;pods&lt;/code&gt;,&lt;code&gt;secrets&lt;/code&gt;,&lt;code&gt;services&lt;/code&gt;,&lt;code&gt;nodes&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;1 watch each of&lt;/p&gt;&lt;code&gt;namespaces&lt;/code&gt;,&lt;code&gt;endpoints&lt;/code&gt;,&lt;code&gt;csidrivers&lt;/code&gt;,&lt;code&gt;runtimeclasses&lt;/code&gt;,&lt;code&gt;endpointslices&lt;/code&gt;,&lt;code&gt;networkpolicies&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That’s 18 watches per node, so 18M watches for 1M nodes. These are only against the kube-apiserver and do not passthrough to etcd directly. With enough kube-apiservers we should be fine.&lt;/p&gt;
    &lt;head rend="h4"&gt;Update()&lt;/head&gt;
    &lt;p&gt;Let’s revisit our 1M kubelet Lease requirement. Kubelet is issuing an &lt;code&gt;Update&lt;/code&gt; (aka &lt;code&gt;Replace&lt;/code&gt;, aka PUT) of its Lease resource every 10 seconds.&lt;/p&gt;
    &lt;p&gt;This is an old Lease:&lt;/p&gt;
    &lt;code&gt;apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  creationTimestamp: "2025-06-26T18:27:28Z"
  name: my-node
  namespace: kube-node-lease
  ownerReferences:
  - apiVersion: v1
    kind: Node
    name: my-node
    uid: ef4d9943-841b-49cc-9fc2-a5faab77e63f
  resourceVersion: "1556549"
  uid: 7e2ec4e2-263f-4350-9397-76f37ceb83cd
spec:
  holderIdentity: my-node
  leaseDurationSeconds: 40
  renewTime: "2025-07-01T21:41:50.646654Z"&lt;/code&gt;
    &lt;p&gt;This is the body of calling Update() when renewing that Lease:&lt;/p&gt;
    &lt;code&gt;apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  creationTimestamp: "2025-06-26T18:27:28Z"
  name: my-node
  namespace: kube-node-lease
  ownerReferences:
  - apiVersion: v1
    kind: Node
    name: my-node
    uid: ef4d9943-841b-49cc-9fc2-a5faab77e63f
  resourceVersion: "1556549"
  uid: 7e2ec4e2-263f-4350-9397-76f37ceb83cd
spec:
  holderIdentity: my-node
  leaseDurationSeconds: 40
  renewTime: "2025-07-01T21:51:50.650000Z"&lt;/code&gt;
    &lt;p&gt;Note the &lt;code&gt;renewTime&lt;/code&gt; has been updated  to something 10 seconds later. (&lt;code&gt;renewTime&lt;/code&gt; is in fact always set to 40 seconds in the future, so we can tolerate some amount of failed or slow lease updates).&lt;/p&gt;
    &lt;p&gt;The other key field is the &lt;code&gt;resourceVersion&lt;/code&gt;. When a client sends an &lt;code&gt;Update()&lt;/code&gt; to a kube-apiserver, it includes the same &lt;code&gt;resourceVersion&lt;/code&gt; from the previous version of the resource it’s updating. This is for safety to ensure that no other client has updated the resource in-between.  Every time a resource is updated on the server, the server assigns the new resource a monotonically-increasing new resourceVersion.  An Update operation must include a resourceVersion that indicates what the old version of the resource it thinks it’s replacing. That way we’re not accidentally overwriting some other change that has happened in-between.&lt;/p&gt;
    &lt;p&gt;You’d think that kube-apiserver could simply convert this Update operation into a &lt;code&gt;Txn-Put&lt;/code&gt; operation in etcd, passing through this command in a straightforward and stateless way. Unfortunately kube-apiserver’s Update implementation also always needs to obtain the entire Old version of this resource. There’s a few reasons for this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Server-side fields: some resources have fields such as ‘status’ and ‘managedFields’ that are only ever updated by the server.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Admission checks: the Admission check interface takes both the old and the new resource.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So to keep Update calls performant, kube-apiserver will maintain Watch caches of most commonly-used resources. When an Update occurs, it’ll pull the old version from its local watch cache. If for some reason the old version is not in the watch cache, then kube-apiserver will first issue a Range to etcd to get the old resource before calling &lt;code&gt;Txn-Put&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Having to do two synchronous calls to etcd for each Update would double our QPS needs and latency, so it’s much better if we can rely on an up-to-date watch cache.&lt;/p&gt;
    &lt;p&gt;However this introduces a new requirement and constraint for 1M nodes: kube-apiservers must be able to “Watch” at a rate of at least 100K events/sec.&lt;/p&gt;
    &lt;p&gt;In my testing this is where things get a little tight.&lt;/p&gt;
    &lt;head rend="h5"&gt;Caching and locking&lt;/head&gt;
    &lt;p&gt;kube-apiserver is deserializing (and, more critically, allocating memory for) 100K nested dictionaries per second. It stores these in a cache, backed by a B-Tree protected with a RWMutex. That RWMutex is under heavy contention:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Update()&lt;/code&gt;calls that are attempting to read the cache for the old objects.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Update()&lt;/code&gt;calls that complete (&lt;code&gt;GuaranteedUpdate()&lt;/code&gt;finalizer) are writing the new value into the cache&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Events from the etcd Watch stream is also writing new values into the cache&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Adding more kube-apiservers helps reduce the contention caused by Update, but it doesn’t reduce the watch load - each kube-apiserver still needs to be able to keep up with the full watch stream of every change that occurs. And adding more kube-apiserver replicas puts additional strain on etcd - most critically its ability to push copies of the watch stream out to the network to each kube-apiserver.&lt;/p&gt;
    &lt;p&gt;It’s a relatively recent change that the kube-apiserver cache is backed by a B-Tree. Previously it was backed by a hash map. This was enabled with feature flag &lt;code&gt;BtreeWatchCache&lt;/code&gt; which became &lt;code&gt;true&lt;/code&gt; by default in Kubernetes 1.32. As far as I can tell, the motivation to move to B-Tree was for faster &lt;code&gt;List()&lt;/code&gt; response. Remember that &lt;code&gt;List()&lt;/code&gt; needs to return items in sorted order, so keeping the items in a B-Tree will make that much faster. But &lt;code&gt;Get()&lt;/code&gt; and &lt;code&gt;Update()&lt;/code&gt; of existing items is now O(n log n) instead of O(1).&lt;/p&gt;
    &lt;p&gt;In my testing, I was unable to get the B-Tree-based cache to scale much more beyond 40K updates per second on a c4a-standard-72 GCP instance. The cache gets stale, unable to keep up with the stream of watch events, too much time being spent waiting for cache lock.&lt;/p&gt;
    &lt;p&gt;With the old hashmap-based cache and 11x kube-apiservers there’s enough replicas to handle the Update() load of 100K Lease updates per second.&lt;/p&gt;
    &lt;head rend="h5"&gt;Garbage collection&lt;/head&gt;
    &lt;p&gt;kube-apiservers parse and decode all resources into their individual fields. Resources with lots of fields thus create a lot of tiny objects in Go, and that puts pressure on garbage collection. Adding more kube-apiserver replicas won’t help if they all are watching the same resource event streams. There’s no real cure, but setting &lt;code&gt;GOMEMLIMIT&lt;/code&gt; and &lt;code&gt;GOGC&lt;/code&gt; can help.&lt;/p&gt;
    &lt;p&gt;I set &lt;code&gt;GOMEMLIMIT&lt;/code&gt; to a number 10-20% less than memory I have on-hand, and set &lt;code&gt;GOGC&lt;/code&gt; up to a few hundred.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scheduler&lt;/head&gt;
    &lt;p&gt;It doesn’t do any good to have a 1-million node cluster if you can’t schedule pods on it. The Kubernetes scheduler is a pretty common bottleneck for large jobs. I ran a benchmark, scheduling 50K pods on 50K nodes, and it took about 4.5 minutes. That’s already uncomfortably long.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Warning&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;If you’re creating pods with some sort of replication controller like Deployment, DaemonSet, or StatefulSet, which can be a bottleneck even before the scheduler. DaemonSet creates a burst of 500 pods at a time and then waits for the Watch stream to show that those are created before proceeding (the rate depends on many factors but expect &amp;lt;5K/sec). The scheduler doesn’t even get a chance to run until those pods are created.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For this 1-million node cluster project, I set an ambitious goal of being able to schedule 1 million pods in 1 minute. Admittedly the number is somewhat arbitrary, but the symmetry with all those m's seemed nice.&lt;/p&gt;
    &lt;p&gt;I also wanted to keep full compatibility with the standard kube-scheduler. It would be far easier to write a simplified scheduler from scratch that scales impressively in narrow scenarios but then fails spectacularly in real-world use cases. There’s a lot of complexity in the existing scheduler that arises from being battle-tested across lots of different production environments. Stripping away those pesky features to make a “faster” scheduler would be misleading.&lt;/p&gt;
    &lt;p&gt;So, we’re going to preserve the functionality and implementation of the kube-scheduler as much as we can. What’s getting in our way to making it more scalable?&lt;/p&gt;
    &lt;p&gt;kube-scheduler works by keeping state of all nodes, and then has a O(n*p) loop, where for each pod it evaluates it against every node. First it filters out nodes that the pod wouldn’t fit at all. Then, for each remaining node, it calculates a score on how well that node would match the pod. The pod is then scheduled to the highest-scoring node, or a random choice among the highest-scoring nodes if there’s a tie.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Tip&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;The kube-scheduler has some techniques to improve performance:&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is parallelizable. And to be fair, the scheduler does parallelize the filtering and generation of scores of nodes against a particular pod. But the scheduler is still burdened by having to do it for all nodes. This isn’t just parallelizable, this can also be distributable.&lt;/p&gt;
    &lt;head rend="h4"&gt;Basic design: shard on nodes&lt;/head&gt;
    &lt;p&gt;It’s akin to the classic scatter/gather design of a distributed search system. Think of each node as a document in the corpus, and each pod as a search query. The query is fanned out to many shards, each responsible for a fraction of the documents. Each shard selects its top candidate(s) and sends them back to a central gatherer to ultimately identify the overall top result.&lt;/p&gt;
    &lt;p&gt;Generic scatter-gather design pattern&lt;/p&gt;
    &lt;p&gt;The key difference is that in search, documents are read-only and thus queries can be evaluated in parallel without conflicts. In scheduling, however, executing a decision actually modifies the documents (i.e. allocates node resources). If two pods are scheduled in parallel to the same node, one may succeed while the other must fail due to insufficient resources.&lt;/p&gt;
    &lt;p&gt;Nevertheless, for a large cluster it’s reasonable to design for “optimistic concurrency”. That is, presume that multiple pods can be scheduled at the same time without conflicts. We still check to see if a conflict arises before committing. And if a conflict occurs, we still do the correct thing by “rolling back” the other pod, e.g. it has to be re-scheduled. But the chance and resulting impact of this happening is low - low enough that you get much higher throughput by running in parallel and absorbing the wasted effort if it does happen.&lt;/p&gt;
    &lt;p&gt;So my initial architecture idea of the distributed scheduler:&lt;/p&gt;
    &lt;p&gt;Scatter-gather as a Kubernetes pod scheduler pattern&lt;/p&gt;
    &lt;p&gt;The Relay starts a watch on unscheduled pods against the kube-apiserver. As streams of pods come in, the Relay forwards them to different schedulers.&lt;lb/&gt; Each Scheduler is responsible for doing filtering and scoring against its own subset of the overall nodes, then sending back to Relay the top-winning node and score.&lt;/p&gt;
    &lt;p&gt;The Relay then aggregates these scores, picks the overall winner, sends a true/false back to the Scheduler, and that true/false dictates whether the scheduler should actually bind the pod to that node.&lt;/p&gt;
    &lt;p&gt;The scheduler here is thus a slightly modified version of the upstream kube-scheduler. It has a custom gRPC server endpoint for receiving the new pod. It has custom code to know which of the overall nodes it is responsible for. And it has a custom Permit extension point for sending the proposed node back to the Relay. The Permit extension point runs after the nodes are filtered and scored, but before the pod is bound. Permit extensions return ‘true’ or ‘false’ to approve whether or not the pod should be scheduled on the specified node.&lt;/p&gt;
    &lt;p&gt;This is the basic design and it works pretty well. It doesn’t quite work up to 1M node scale - we’ll talk about that next - but it delivers a much more scalable scheduler solution than what exists today, while preserving all of the nuanced complex battle-tested logic of the current system.&lt;/p&gt;
    &lt;p&gt;Today’s scheduler is effectively O(n x p), where n is the number of nodes and p the number of pods. That complexity becomes untenable as n grows. The sharded approach helps counteract the scaling problem: if you have n nodes, then you can shard the work across r replicas where r is some factor of n, turning that large factor back into something more tractable.&lt;/p&gt;
    &lt;p&gt;I should mention there’s one fairly large exception, and that’s pod evictions. Pod eviction can occur when there’s a new pod to schedule, but there’s not enough available resources currently on the cluster to schedule that pod. When this occurs, the scheduler does a scan across all pods currently running in the cluster, trying to identify a set of lower-priority pods that, if they were to be killed, would leave enough space for this new pod. To be fair, I didn’t implement this. You could squint at the current approach and imagine how we could also distribute the work of eviction calculation, but I didn’t do it.&lt;/p&gt;
    &lt;head rend="h4"&gt;The painful long tail: running large distributed systems in reality&lt;/head&gt;
    &lt;p&gt;On my hardware, a single scheduler was able to filter and score a pod against 1K nodes in about 1ms. So we could do 1K pods on 1K nodes in 1s. Remember the goal was 1M pods on 1M nodes in 60s. Recall that the overall work is O(n x p) (each pod has to be evaluated against each node), so going from 1K pods and nodes to 1M pods &amp;amp; 1M nodes is not a factor of 1K more work, but 1K*1K, or 1 million times more work. Even allowing ourselves 60s instead of 1s, we’re going to need a lot more schedulers.&lt;/p&gt;
    &lt;head rend="h5"&gt;Add more relays and distribute the score gathering&lt;/head&gt;
    &lt;p&gt;In fact, we’re going to need so many schedulers that a single relay simply doesn’t have enough network bandwidth to send to all of them in enough time. We need multiple relays. In fact we actually need multiple levels of relays to reach all of the schedulers.&lt;/p&gt;
    &lt;p&gt;Similarly the gathering stage, of collecting all scores and determining a winner, can also be distributed. Each scheduler and relay has a Score &lt;code&gt;Gather`&lt;/code&gt; endpoint, and it’s determined via a hash of the pod name to determine which scheduler is responsible for gathering the scores of a particular pod.&lt;/p&gt;
    &lt;p&gt;Here is a simplified example of what more relays looks like. This is with a fanout of 3, while in reality I used a fanout of 10. I was aiming to maximize but not exceed the maximum transmit throughput of each NIC to transmit 1M * 4K of Pod data in 60 seconds.&lt;/p&gt;
    &lt;p&gt;Note that it’s packed. Not all schedulers are on the same level.&lt;/p&gt;
    &lt;head rend="h5"&gt;Fight long-tail latency&lt;/head&gt;
    &lt;p&gt;My goal was to see linear time reductions as I added more replicas. In reality, I started hitting a plateau, where no matter how many more replicas I added, things remained the same or even got worse. While on average, most of the schedulers were doing less work and thus finishing more quickly, it became more frequent to see one or two stragglers that were not faster at all. This was a problem because I needed all schedulers to report back their best node before we could pick a winner.&lt;/p&gt;
    &lt;p&gt;There’s a well-known Google paper by Jeff Dean called The Tail of Scale that talks about exactly this problem. Our servers aren’t running real-time OSes and software. They are busy with all sorts of miscellaneous background tasks; observability, upgrades, garbage collection. Garbage collection is a big problem in Golang if you’re trying to write tightly coordinated software. It interrupts a currently-running task or defers a queued one. Suddenly a task that usually takes 300 microseconds spikes to take 1 millisecond. With enough individual servers, inevitably someone is always taking that 1 millisecond. If you have tightly timed coordinated systems that rely on everyone to respond before proceeding, 99% of your servers are waiting for that long-tail 1% to finish.&lt;/p&gt;
    &lt;p&gt;So I implemented a few things to reduce this problem:&lt;/p&gt;
    &lt;p&gt;Use pinned CPUs. It’s a way to ensure that cpu cores are dedicated to one single container’s processes, not context switching between various random processes. In kubelet this is done via “CPU Manager Policy”. Just specifying this made my tasks much more consistently performing.&lt;/p&gt;
    &lt;p&gt;Tweak garbage collection. Increasing GOGC above 100 can reduce the amount of overall time spent in garbage collection at the expense of using more memory. Using an aggressive GOGC plus a GOMEMLIMIT near towards your actual memory limit is a fantastic way to ensure that you only do GC when you really need to. I use a GOGC value of 800 and a GOMEMLIMIT set to 90% of the container’s memory limit.&lt;/p&gt;
    &lt;p&gt;Give up on stragglers. Simply don’t wait for the last N% to respond. This can effectively cut off your long tail. Beware that if there’s one consistently slow node, then this can create a feedback loop, hammering that server with more and more requests will just make it slower until it totally melts down.&lt;/p&gt;
    &lt;p&gt;Really you should just go read the The Tail of Scale paper, it covers several other possible scenarios and fixes.&lt;/p&gt;
    &lt;p&gt;One thing that I did not do is overlap workloads across multiple servers. I could’ve assigned each kube node to multiple shards, so that any one of them can calculate and score the pods on that node. I worried this would result in too many cases of data inconsistency, where the node became over-subscribed with pods because the various shards were not consistent with one-another about which pods had been scheduled on that node.&lt;/p&gt;
    &lt;head rend="h5"&gt;Replace watcher with AdmissionWebHook&lt;/head&gt;
    &lt;p&gt;This one remains a bit of a mystery. The kube-scheduler typically learns about pods to schedule by doing a watch with fieldSelector ‘spec.nodeName=’ (meaning, pods that have no current nodeName set). When creating a lot of pods quickly (over &amp;gt;5K/sec), the watch stream would frequently stall for tens of seconds at a time.&lt;/p&gt;
    &lt;p&gt;This was one particularly bad example:&lt;/p&gt;
    &lt;p&gt;Sometimes even though there’d be plenty of pods to schedule, the watch stream had stalled so badly that the scheduler would be starved of pods to process.&lt;/p&gt;
    &lt;p&gt;To overcome this, I made the scheduler take a rather extreme change of interface. Rather than creating a watch, I made the scheduler a ValidatingWebhook. This made it so the kube-apiserver would hit an HTTP endpoint on the scheduler with every new pod that was created, in line with the create request. Typical Validating Webhooks are used for security, to approve or deny some resource fields from being set by particular clients. In this case, the scheduler approved all pods. It was merely a way for it to learn about every new pod faster (and synchronously) than by using a watch stream.&lt;/p&gt;
    &lt;head rend="h4"&gt;Results&lt;/head&gt;
    &lt;p&gt;Created a 100K node cluster and then timed how long it takes to schedule 100K pods. The pods have no nodeSelector or affinity.&lt;/p&gt;
    &lt;p&gt;Each scheduler ran on a dedicated c4d-standard-32; that is 32 AMD Turin cores and 128GB of DDR5 RAM. Experiments where dist-schedulers that had more than 1 replica also had 1 dedicated dist-scheduler-relay VM.&lt;/p&gt;
    &lt;p&gt;Each dist-scheduler replica is configured to run 30 separate internal schedulers, each with a parallelism setting of ‘2’.&lt;/p&gt;
    &lt;p&gt;The default-scheduler is kube-scheduler 1.32.3 with no modifications.&lt;/p&gt;
    &lt;p&gt;One puzzling result is how much better a 1x dist-scheduler performed than the default-scheduler. Adjusting the &lt;code&gt;parallelism&lt;/code&gt; setting had no impact on the performance nor the CPU-seconds, which seemed to peak at about 20 (leaving 12 cores free).&lt;/p&gt;
    &lt;p&gt;Note that adding replicas to dist-scheduler did result in a more-or-less linear time improvement. In other words, doubling the number of dist-scheduler replicas results in a halving of the time to completion. This trend continues to the 256x replica/1M pod scale as we’ll see in the next section.&lt;/p&gt;
    &lt;head rend="h2"&gt;Experiments&lt;/head&gt;
    &lt;head rend="h3"&gt;1M nodes, 1M pods with kwok&lt;/head&gt;
    &lt;head rend="h4"&gt;Test setup:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;kube-apiserver: 5x c4d-standard-192s running kube-apiservers via k3s v1.32.4+k3s1.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;kube-scheduler and kube-controller-manager v1.32.4 each run as a separate process on the same VMs&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;code&gt;feature-gates=kube:BtreeWatchCache=false&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;No cloud-controller-manager, traefik, or servicelb&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;etcd: 1x c4d-highmem-16 running custom mem_etcd implementation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;kubelet:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;For dist-scheduler: 285x c4d-highcpu-32’s&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;For kwok: 7x c4a-highcpu-32’s&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Both running kubelet via k3s v1.32.4+k3s1&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;kwok: 100x pods running a modified version of kwok v0.6.0&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pod scheduler: 289 replicas (8670 AMD Turin cores) of custom distributed scheduler implementation, consisting of 256 schedulers and 29 relays.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Procedure&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Start all VMs. Wait for kwok and dist-scheduler to be fully running&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create 1M nodes via make_nodes&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wait for 1M nodes and 1M leases to be present in the etcd database&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create 1M pods via create-pods&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wait for all pods to have a spec.nodeName&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Results&lt;/head&gt;
    &lt;p&gt;In the below graphs, the green line annotation indicates when the first pod was created and the red line indicates when the millionth pod was scheduled.&lt;/p&gt;
    &lt;head rend="h5"&gt;etcd&lt;/head&gt;
    &lt;head rend="h5"&gt;kube-apiserver&lt;/head&gt;
    &lt;head rend="h5"&gt;scheduler&lt;/head&gt;
    &lt;head rend="h3"&gt;Kwok vs kubelet&lt;/head&gt;
    &lt;p&gt;So far all of these experiments are run using kwok instead of real kubelets. But how realistic is that? It’s possible kwok doesn’t generate nearly the same pattern of load as real kubelets, and so these experiments wouldn’t be representative of a real-life cluster with 1M nodes each running kubelet.&lt;/p&gt;
    &lt;p&gt;Unfortunately running 1M real kubelets is beyond my budget. But maybe we can run a smaller-scale experiment with real kubelets and measure how its workload compares to an equivalent-sized kwok cluster.&lt;/p&gt;
    &lt;p&gt;With some careful configuration, I can run a test of a 100K kubelet cluster. The trick is to run many kubelets at once all on the same VM. They each run in separate Linux namespaces off of the host. They each have their own IPv6 address and range from which they can allocate pod addresses. They each run their own copy of containerd with which to allocate nested pods.&lt;/p&gt;
    &lt;p&gt;Deploying and managing 100K kubelet containers across lots of VMs sounds difficult. If only there were software to orchestrate this… Aha! We can create a Kubernetes Deployment of kubelets!&lt;/p&gt;
    &lt;p&gt;There are still some single points of contention because of the shared kernel. Kube-proxy by default uses iptables, and changes to iptables are done with a mutex. Nftables is more performant and more friendly to concurrency but nevertheless remains a bottleneck. So we will do better to have lots of small VMs rather than fewer big ones to spread out our concurrency constraints.&lt;/p&gt;
    &lt;p&gt;Additionally, for the IPv6 subnetworks of each kubelet to be reachable from the cloud provider, we need to propagate neighbor advertisement packets. I deploy ndppd on each VM (as a DaemonSet) to do this.&lt;/p&gt;
    &lt;head rend="h4"&gt;Test setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;kube-apiserver: 6x c4d-standard-192s running kube-apiservers via k3s v1.32.4+k3s1.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;No cloud-controller-manager, traefik, or servicelb&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;etcd: 1x c4d-highmem-8 running custom mem_etcd implementation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kubelet:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;For kubelet-in-pod: 426x c4d-highmem-8’s (3408 AMD Turin cores and 27264GiB of RAM)&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;
                &lt;p&gt;Running k3s v1.32.4+k3s1&lt;/p&gt;
              &lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;For kwok: 2x c4a-highcpu-32’s&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;kubelet-pod: Running a slightly modified version of the k3s v1.32.4+k3s1 image where&lt;/p&gt;&lt;code&gt;libjansson&lt;/code&gt;is installed, which adds json support to nftables, required for kubelet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Procedure&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Wait for cluster to boot and all VM nodes to go Ready&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Deploy Deployment of kubelet-as-pod.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scale up to 100K replicas&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture graphs of kube-apiserver and etcd load&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tear down and re-create cluster&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Deploy kwok. Create 100K kwok nodes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture graphs of kube-apiserver and etcd load&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Results&lt;/head&gt;
    &lt;head rend="h5"&gt;etcd&lt;/head&gt;
    &lt;head rend="h5"&gt;kube-apiserver&lt;/head&gt;
    &lt;head rend="h2"&gt;Conclusion: How large can a Kubernetes cluster be?&lt;/head&gt;
    &lt;p&gt;The truth is that cluster size matters far less than the rate of operations on any single resource Kind—especially creates and updates. Operations on different Kinds are isolated: each runs in its own goroutine protected by its own mutex. You can even shard across multiple etcd clusters by resource kind, so cross-kind modifications scale relatively independently.&lt;/p&gt;
    &lt;p&gt;The biggest source of writes is ususally Lease updates that keep Nodes alive. That makes cluster size fundamentally constrained by how quickly the system can process those updates.&lt;/p&gt;
    &lt;p&gt;A standard etcd setup on modern hardware sustains roughly 50,000 modifications per second. With careful sharding (separate etcd clusters for Nodes, Leases, and Pods), you could likely support around 500,000 nodes with standard etcd.&lt;/p&gt;
    &lt;p&gt;Replacing etcd with a more scalable backend shifts the bottleneck to the kube-apiserver’s watch cache. Each resource Kind today is guarded by a single RWMutex over a B-tree. Replacing that with a hash map can likely support ~100,000 events/second, enough to support 1 million nodes on current hardware. To go beyond that, increase the Lease interval (e.g., &amp;gt;10s) to reduce modification rate.&lt;/p&gt;
    &lt;p&gt;At scale, the biggest aggregate limiter is Go’s garbage collector. The kube-apiserver creates and discards vast numbers of small objects when parsing and decoding resources, and this churn drives GC pressure. Adding more kube-apiserver replicas doesn’t help, since all of them are subscribed to the same event streams.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to run yourself&lt;/head&gt;
    &lt;p&gt;See the RUNNING file for instructions on how to run a cluster yourself.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bchess.github.io/k8s-1m/"/><published>2025-10-16T22:04:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45613898</id><title>Titan submersible’s $62 SanDisk memory card found undamaged at wreckage site</title><updated>2025-10-19T14:34:50.206424+00:00</updated><content>&lt;doc fingerprint="a50f2c926b2b1f56"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tragic Titan submersible’s $62 SanDisk memory card found undamaged at wreckage site — 12 stills and nine videos have been recovered, but none from the fateful OceanGate implosion&lt;/head&gt;
    &lt;p&gt;The specialist camera was rated to 6,000m, but the lens and some of its components were probably damaged by the implosion.&lt;/p&gt;
    &lt;p&gt;Recovery teams working on the Titan submersible have found the vessel's specialist stills and video camera intact. Fascinatingly, while there was some damage to the camera’s housing and internal components, tech and science enthusiast Scott Manley reveals that the internal SD card was “undamaged.” Contents of the memory card have since been investigated, and 12 stills and nine videos have been recovered.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The recovery teams found a hardened underwater camera in the wreckage of the Titan submersible, and inside the casing was an undamaged SD card. pic.twitter.com/QCOtdcS7dUOctober 15, 2025&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Click 'See more' for images.&lt;/p&gt;
    &lt;p&gt;In the images, you can see a SubC-branded Rayfin Mk2 Benthic Camera, recovered from the wreckage of the ill-fated submersible operated by OceanGate. This still and video camera is rated to withstand depths up to 6,000m (19,685 feet, 3,281 fathoms). The titanium and synthetic sapphire crystal constructed device features both onboard and expansion memory (the titular SD card).&lt;/p&gt;
    &lt;head rend="h2"&gt;Casing intact, lens shattered but remained in place&lt;/head&gt;
    &lt;p&gt;Inside the camera's tube-like form, it is observed that the PCBs had suffered some slight damage. For example, connectors between two boards were sheared off, and some surface mount components were similarly damaged.&lt;/p&gt;
    &lt;p&gt;In some images of the PCB that are shared, you will notice details are blurred at the request of the Canada-based underwater imaging specialist and Rayfin Mk2 Benthic Camera manufacturer. However, SubC’s requested trade secret obfuscation hasn’t stopped internet sleuths asserting that they know exactly what components have been redacted.&lt;/p&gt;
    &lt;p&gt;Picking through Manley's Tweet thread replies, the key PCBs in the camera were likely an Inforce 6601 System on Module (SoM) based on the Qualcomm SD820 processor, which comes with 4GB of RAM on board and 64GB of UFS storage. Another component is thought to be the Teensy 4.0 or 3.2, which acted as an MCU. Last but not least, the undamaged SD card is almost certainly a SanDisk Extreme Pro 512GB ($62.99 at the time of writing), though it was de-branded in the photos.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data recovery process and results&lt;/head&gt;
    &lt;p&gt;With an undamaged SD card, of course, investigators (and others) were interested in what details of the tragedy may have been captured and stored by this camera system. The first step was to make “an exact binary image of the SD card” so the original could be left untampered.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Check out Manley’s Tweet thread, containing investigator report screengrabs, to take in the full gamut of back and forth between the data forensics investigator, Canada’s Transportation Safety Board of Canada (TSB), and SubC. To cut a long technical story short, though, the parties eventually met up at a lab/office in Newfoundland, where a recovered NVRAM chip and SD card image were interfaced with a “surrogate SoM board.” This did the trick, and 12 still and nine videos were recovered!&lt;/p&gt;
    &lt;p&gt;Recovered images were at a resolution of 4,056 x 3,040 pixels, implying a pretty common 12.3MP sensor was used by the SubC Rayfin Mk2 Benthic Camera. Videos were at a more standard 3,840 x 2,160 pixels – commonly referred to as 4K Ultra HD (UHD) video.&lt;/p&gt;
    &lt;p&gt;Somewhat disappointingly, the images and videos shared in the report were taken in the vicinity of the ROV shop at the Marine Institute, also in Newfoundland. The location was the logistical base for Titanic dive missions. No deep-sea shenanigans around the Titanic wreck were revealed. Manley explains in his Twitter thread that “the camera had been configured to dump data onto an external storage device, so nothing was found from the accident dive.” Nothing particularly pertinent to the tragic accident, that is.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/pc-components/microsd-cards/tragic-oceangate-titan-submersibles-usd62-sandisk-memory-card-found-undamaged-at-wreckage-site-12-stills-and-nine-videos-have-been-recovered-but-none-from-the-fateful-implosion"/><published>2025-10-17T06:39:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45625229</id><title>Chen-Ning Yang, Nobel laureate, dies at 103</title><updated>2025-10-19T14:34:49.150158+00:00</updated><content>&lt;doc fingerprint="73451408afd08a0a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;China's first Nobel laureate, Yang Chen-Ning, dies, aged 103&lt;/head&gt;
    &lt;p&gt;Yang Chen-Ning, a world-renowned physicist and Nobel laureate, passed away in Beijing on Saturday at 103.&lt;/p&gt;
    &lt;p&gt;Yang, an academician of the Chinese Academy of Sciences, professor at Tsinghua University, and the honorary president of the Institute for Advanced Study at Tsinghua, died after an illness, the university said in an obituary, calling the late professor "immortal".&lt;/p&gt;
    &lt;p&gt;Together with his colleague Tsung-dao Lee, Yang was awarded the Nobel Prize in Physics in 1957 for their theory of parity non-conservation in weak interaction.&lt;/p&gt;
    &lt;p&gt;He was often ranked alongside Albert Einstein as one of the 20th century's greatest physicists.&lt;/p&gt;
    &lt;p&gt;Born in Hefei, Anhui province, in 1922, Yang moved with his family to Tsinghua in 1929. He enrolled at the National Southwestern Associated University in 1938 and later entered the graduate school of Tsinghua University in 1942, earning a master's degree in science in 1944. In 1945, he went to the United States for further studies as a Tsinghua University government-sponsored student, attending the University of Chicago, where he received his PhD in 1948 and remained for postdoctoral work.&lt;/p&gt;
    &lt;p&gt;He joined the Institute for Advanced Study in Princeton in 1949, becoming a permanent member in 1952 and a professor in 1955. In 1966, he was appointed as the Albert Einstein Professor of Physics at the State University of New York at Stony Brook, working there until 1999.&lt;/p&gt;
    &lt;p&gt;Since 1986, he had been a visiting professor at the Chinese University of Hong Kong. From 1997, he served as the honorary director of the newly established Center for Advanced Study — now the Institute for Advanced Study — at Tsinghua University and became a Tsinghua professor in 1999.&lt;/p&gt;
    &lt;p&gt;Yang, having made seminal contributions to modern physics, is recognized as one of the most eminent physicists of the 20th century. His work with Robert Mills on the "Yang-Mills theory" laid the foundation for the Standard Model of particle physics and is regarded as one of the cornerstones of modern physics, comparable in significance to Maxwell's equations and Einstein's theory of general relativity.&lt;/p&gt;
    &lt;p&gt;In collaboration with Tsung-Dao Lee, he proposed the non-conservation of parity in weak interactions, a revolutionary idea for which they were jointly awarded the Nobel Prize in Physics in 1957, becoming the first Chinese Nobel laureate.&lt;/p&gt;
    &lt;p&gt;Yang was a foreign member of more than ten academies of sciences worldwide and received honorary doctoral degrees from over twenty renowned universities.&lt;/p&gt;
    &lt;p&gt;Yang maintained a deep affinity for his homeland and made outstanding contributions to China's scientific and educational development. His first visit to the People's Republic of China in 1971 helped initiate a wave of visits by overseas Chinese scholars, earning him recognition as a pioneer in building academic bridges between China and the United States.&lt;/p&gt;
    &lt;p&gt;He later proposed the restoration and strengthening of basic scientific research to China's central leadership and personally raised funds to establish a committee for educational exchange with China — sponsoring nearly a hundred Chinese scholars for further studies in the US. Many of those scholars later became key figures in China's scientific and technological advancement. Yang played a significant role in promoting domestic scientific exchange and progress, offering crucial advice on major national scientific projects and policies.&lt;/p&gt;
    &lt;p&gt;Upon his return to Tsinghua, he dedicated himself to the development of the Institute for Advanced Study, investing immense effort into the growth of basic disciplines like physics and the cultivation of talent at Tsinghua, significantly impacting the reform and development of China's higher education.&lt;/p&gt;
    &lt;p&gt;The life of Professor Yang was that of an immortal legend — exploring the unknown with a timeless echo of a heart devoted to his nation, the obituary said.&lt;/p&gt;
    &lt;p&gt;Yang's century-long journey constitutes an eternal chapter shining among the stars of humanity, it said.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Musical medley: Super Bund Music Festival kicks off in Shanghai&lt;/item&gt;
      &lt;item&gt;Physicist Chen-Ning Yang dies at 103&lt;/item&gt;
      &lt;item&gt;China improves regulations on personal information outbound transfer&lt;/item&gt;
      &lt;item&gt;Generative AI users in China reach 515m&lt;/item&gt;
      &lt;item&gt;China Focus: China achieves numerous breakthroughs in space exploration quest&lt;/item&gt;
      &lt;item&gt;Rediscovering the magnetism of Yan'an for China's youth&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.chinadaily.com.cn/a/202510/18/WS68f3170ea310f735438b5bf2.html"/><published>2025-10-18T05:47:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45626130</id><title>./watch</title><updated>2025-10-19T14:34:48.911123+00:00</updated><link href="https://dotslashwatch.com/"/><published>2025-10-18T09:55:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45626349</id><title>EQ: A video about all forms of equalizers</title><updated>2025-10-19T14:34:47.799400+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=CLAt95PrwL4"/><published>2025-10-18T10:51:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45627394</id><title>Root System Drawings</title><updated>2025-10-19T14:34:46.549459+00:00</updated><content>&lt;doc fingerprint="fbf286432d583069"&gt;
  &lt;main&gt;
    &lt;p&gt;Javascript Required To experience full interactivity, please enable Javascript in your browser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://images.wur.nl/digital/collection/coll13/search"/><published>2025-10-18T13:52:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45627692</id><title>Flowistry: An IDE plugin for Rust that focuses on relevant code</title><updated>2025-10-19T14:34:46.113992+00:00</updated><content>&lt;doc fingerprint="b283f24420974767"&gt;
  &lt;main&gt;
    &lt;p&gt;Flowistry is a tool that analyzes the information flow of Rust programs. Flowistry understands whether it's possible for one piece of code to affect another. Flowistry integrates into the IDE to provide a "focus mode" which helps you focus on the code that's related to your current task.&lt;/p&gt;
    &lt;p&gt;For example, this GIF shows the focus mode when reading a function that unions two sets together:&lt;/p&gt;
    &lt;p&gt;When the user clicks a given variable or expression, Flowistry fades out all code that does not influence that code, and is not influenced by that code. For example, &lt;code&gt;orig_len&lt;/code&gt; is not influenced by the for-loop, while &lt;code&gt;set.len()&lt;/code&gt; is.&lt;/p&gt;
    &lt;p&gt;Flowistry can be helpful when you're reading a function with a lot of code. For example, this GIF shows a real function in the Rust compiler. If you want to understand the role of a specific argument to the function, then Flowistry can filter out most of the code as irrelevant:&lt;/p&gt;
    &lt;p&gt;The algorithm that powers Flowistry was published in the paper "Modular Information Flow through Ownership" at PLDI 2022.&lt;/p&gt;
    &lt;p&gt;Table of contents&lt;/p&gt;
    &lt;p&gt;Flowistry is available as a VSCode plugin. You can install Flowistry from the Visual Studio Marketplace or the Open VSX Registry. In VSCode:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to the Extensions pane by clicking this button in the left margin:&lt;/item&gt;
      &lt;item&gt;Search for "Flowistry" and then click "Install".&lt;/item&gt;
      &lt;item&gt;Open a Rust workspace and wait for the tool to finish installing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note on platform support: Flowistry does not yet support NixOS. Flowistry cannot provide pre-built binaries for ARM targets like M1 Macs, so Flowistry must be installed from scratch on these targets (this is done for you, but will take a few more minutes than usual).&lt;/p&gt;
    &lt;p&gt;Alternatively, you can install it from source:&lt;/p&gt;
    &lt;code&gt;# Install flowistry binaries
git clone https://github.com/willcrichton/flowistry
cd flowistry
cargo install --path crates/flowistry_ide

# Install vscode extension
cd ide
npm install
npm run build
ln -s $(pwd) ~/.vscode/extensions/flowistry
&lt;/code&gt;
    &lt;p&gt;If you are interested in the underlying analysis, you can use the &lt;code&gt;flowistry&lt;/code&gt; crate published to crates.io: https://crates.io/crates/flowistry&lt;/p&gt;
    &lt;p&gt;The documentation is published here: https://willcrichton.net/flowistry/flowistry/&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: Docs.rs doesn't support documentation for crates that use&lt;/p&gt;&lt;code&gt;#![feature(rustc_private)]&lt;/code&gt;so we have to host it ourselves.&lt;/quote&gt;
    &lt;p&gt;Note that the latest Flowistry has a Maximum Supported Rust Version of Rust 1.73. Flowistry is not guaranteed to work with features implemented after 1.73.&lt;/p&gt;
    &lt;p&gt;Once you have installed Flowistry, open a Rust workspace in VSCode. You should see this icon in the bottom toolbar:&lt;/p&gt;
    &lt;p&gt;Flowistry starts up by type-checking your codebase. This may take a few minutes if you have many dependencies.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: Flowistry type-checking results are cached in the&lt;/p&gt;&lt;code&gt;target/flowistry&lt;/code&gt;directory. If you delete this folder, Flowistry will have to recompute types. Also for a large codebase this directory may take up a fair amount of disk space.&lt;/quote&gt;
    &lt;p&gt;Once Flowistry has booted up, the loading icon will disappear. Then you can enter focus mode by running the "Toggle focus mode" command. By default the keyboard shortcut is Ctrl+R Ctrl+A (⌘+R ⌘+A on Mac), or you can use the Flowistry context menu:&lt;/p&gt;
    &lt;p&gt;In focus mode, Flowistry will automatically compute the information flow within a given function once you put your cursor there. Once Flowistry has finished analysis, the status bar will look like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Flowistry can be a bit slow for larger functions. It may take up to 15 seconds to finish the analysis.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Flowistry infers what you want to focus on based on your cursor. So if you click on a variable, you should see the focus region of that variable. Flowistry will highlight the focused code in gray, and then fade out code outside the focus region. For example, because the user's cursor is on &lt;code&gt;view_projection&lt;/code&gt;, that variable is highlighted in gray, and its focus region is shown.&lt;/p&gt;
    &lt;p&gt;Sometimes you want to keep the focus region where it is, and click on other code to inspect it without changing focus. For this purpose, Flowistry has a concept of a "mark". Once you have selected code to focus on, you can run the "Set mark" command (Ctrl+R Ctrl+S / ⌘+R ⌘+S). Then a mark is set at your cursor's current position, and the focus will stay there until you run the "Unset mark" command (Ctrl+R Ctrl+D / ⌘+R ⌘+D).&lt;/p&gt;
    &lt;p&gt;If you want to modify all the code in the focus region, e.g. to comment it out or copy it, then you can run the "Select focused region" command (Ctrl+R Ctrl+T / ⌘+R ⌘+T). This will add the entire focus region into your editor's selection.&lt;/p&gt;
    &lt;p&gt;Flowistry is an active research project into the applications of information flow analysis for Rust. It is continually evolving as we experiment with analysis techniques and interaction paradigms. So it's not quite as polished or efficient as tools like Rust Analyzer, but we hope you can still find it useful! Nevertheless, there are a number of important limitations you should understand when using Flowistry to avoid being surprised.&lt;/p&gt;
    &lt;p&gt;If you have questions or issues, please file a Github issue, join our Discord, or DM @wcrichton on Twitter.&lt;/p&gt;
    &lt;p&gt;When your code has references, Flowistry needs to understand what that reference points-to. Flowistry uses Rust's lifetime information to determine points-to information. However, data structures that use interior mutability such as &lt;code&gt;Arc&amp;lt;Mutex&amp;lt;T&amp;gt;&amp;gt;&lt;/code&gt; explicitly do not share lifetimes between pointers to the same data. For example, in this snippet:&lt;/p&gt;
    &lt;code&gt;let x = Arc::new(Mutex::new(0));
let y = x.clone();
*x.lock().unwrap() = 1;
println!("{}", y.lock().unwrap());&lt;/code&gt;
    &lt;p&gt;Flowistry can determine that &lt;code&gt;*x.lock().unwrap() = 1&lt;/code&gt; is a mutation to &lt;code&gt;x&lt;/code&gt;, but it can not determine that it is a mutation to &lt;code&gt;y&lt;/code&gt;. So if you focus on &lt;code&gt;y&lt;/code&gt;, the assignment to 1 would be faded out, even though it is relevant to the value of &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We are researching methods to overcome this limitation, but for now just be aware that this is the main case where Flowistry is known to provide an incorrect answer.&lt;/p&gt;
    &lt;p&gt;Flowistry's analysis tries to include all code that could have an influence on a focal point. This analysis makes a number of assumptions for both practical and fundamental reasons. For example, in this snippet:&lt;/p&gt;
    &lt;code&gt;let mut v = vec![1, 2, 3];
let x = v.get_mut(0);
println!("{:?} {}", v, x);&lt;/code&gt;
    &lt;p&gt;If you focus on &lt;code&gt;v&lt;/code&gt; on line 3, it will include &lt;code&gt;v.get_mut(0)&lt;/code&gt; as an operation that could have modified &lt;code&gt;v&lt;/code&gt;. The reason is that Flowistry does not actually analyze the bodies of called functions, but rather approximates based on their type signatures. Because &lt;code&gt;get_mut&lt;/code&gt; takes &lt;code&gt;&amp;amp;mut self&lt;/code&gt; as input, it assumes that the vector could be modified.&lt;/p&gt;
    &lt;p&gt;In general, you should use focus mode as a pruning tool. If code is faded out, then you don't have to read it (minus the limitation mentioned above!). If it isn't faded out, then it might be relevant to your task.&lt;/p&gt;
    &lt;p&gt;Flowistry works by analyzing the MIR graph for a given function using the Rust compiler's API. Then the IDE extension lifts the analysis results from the MIR level back to the source level. However, a lot of information about the program is lost in the journey from source code to MIR.&lt;/p&gt;
    &lt;p&gt;For example, if the source contains an expression &lt;code&gt;foo.whomp.bar().baz()&lt;/code&gt;, it's possible that a temporary variable is only generated for the expression &lt;code&gt;foo.whomp.bar()&lt;/code&gt;. So if the user selects &lt;code&gt;foo&lt;/code&gt;, Flowistry may not be able to determine that this corresponds to the MIR place that represents &lt;code&gt;foo&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is why the IDE extension highlights the focused code in gray, so you can understand what your cursor's selection actually maps to.&lt;/p&gt;
    &lt;p&gt;Flowistry analyzes a single function at a time. If a function contains other functions, e.g. &lt;code&gt;fn&lt;/code&gt; definitions, or closures, or implicitly via async, then Flowistry will only show you focus regions within the smallest function body containing your cursor. This is usually well defined for function definitions and closures, but may be confusing for async since that depends on how rustc decides to carve up your async function.&lt;/p&gt;
    &lt;p&gt;If rustup fails, especially with an error like "could not rename downloaded file", this is probably because Flowistry is running rustup concurrently with another tool (like rust-analyzer). Until rustup#988 is resolved, there is unfortunately no automated way around this.&lt;/p&gt;
    &lt;p&gt;To solve the issue, go to the command line and run:&lt;/p&gt;
    &lt;code&gt;rustup toolchain install nightly-2023-08-25 -c rust-src -c rustc-dev -c llvm-tools-preview
&lt;/code&gt;
    &lt;p&gt;Then go back to VSCode and click "Continue" to let Flowistry continue installing.&lt;/p&gt;
    &lt;p&gt;Rust Analyzer does not support MIR and the borrow checker, which are essential parts of Flowistry's analysis. That fact is unlikely to change for a long time, so Flowistry is a standalone tool.&lt;/p&gt;
    &lt;p&gt;See Limitations for known issues. If that doesn't explain what you're seeing, please post it in the unexpected highlights issue or ask on Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/willcrichton/flowistry"/><published>2025-10-18T14:33:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45628391</id><title>Tinnitus Neuromodulator</title><updated>2025-10-19T14:34:44.897255+00:00</updated><content>&lt;doc fingerprint="b64980b949699afa"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;User Stories&lt;/head&gt;
      &lt;p&gt;Write your own here. Click the blue bullets ● to load associated settings.&lt;/p&gt;
      &lt;p&gt;● Been loving this project, this website for years. The sounds, the stories, the community. &amp;lt;3 This particular "noise" for me is a real lifesaver. It always helps me with (tension-) headaches. Thank you so much.&lt;lb/&gt; ● I couldn't think for two days straight; it was so bad and kept getting worse. This quieted the constant chatter and noise down to a mild hum.&lt;lb/&gt; ● This and a special Dreaming Nautilus almost put me to sleep for some weird reason.&lt;lb/&gt; ● Neural Hack sounds so funny, that one time I laughed so hard at night listening to this, that my mom came upstairs and whooped me. All jokes aside, if anyone has trouble managing their tinnitus, then this is the one for you! 8.7 out of 10.&lt;lb/&gt; ● Yes, this is pretty wonderful. I only need 3 of the sliders for it to work, thank you. Why not make a standalone device for this?&lt;lb/&gt; ● It takes a moment to adjust, but once you do, all your problems warble away.&lt;lb/&gt; ● I feel like I'm on an alien planet!&lt;lb/&gt; ● Stéphane and Steve have created a nearly magical tinnitus solution. Whenever my tinnitus starts acting up, I pop on these settings for a few minutes, and when I take the headphones off... Silence. Absolute silence. I have no idea how this works, but it's genius!&lt;lb/&gt; ● An absolute godsend! It's just tickly enough to distract me from the noise in my ears, but it doesn't crowd out my actual thoughts.&lt;lb/&gt; ● You wouldn't think such an exotic, high-pitched, and beepy sound generator would be calming, but when you find the right spot and let it go for a few minutes, then pause the audio, it's so much quieter—not gone, but still a relief compared to before. Even just letting it play and noticing your ear ringing contributing to the music instead of being a distracting annoyance is calming.&lt;lb/&gt; ● Once properly calibrated, after a few minutes of listening, it helps reduce the perception of tinnitus—and even after removing the headphones, the sensation almost seems to vanish. myNoise is a great tool to use when needed.&lt;lb/&gt; ● I have infrequent tinnitus in one ear that gets really annoying when I'm sick. I tried this on a whim, and I straight-up can't hear it anymore! What a godsend.&lt;lb/&gt; ● I've been using this neuromodulator on a daily basis for many weeks and this is by far the best help I could ever find to release my tinnitus 24/7 head drill. It doesn’t get rid of it completely, but it tones it down massively - just leaves a soft background hum that lets you live your life. Honestly, it feels like magic. No idea how it works this well. Huge thanks to the creators - you guys rock!&lt;lb/&gt; ● I use the tinnitus neuromodulator when things get rough. It doesn't just mask the tinnitus, it transports me into a space where I can focus again. For a while, I feel clear. What gets me is this: when I take off the headphones, the tinnitus is gone. Not reduced—gone. Then, slowly, it creeps back. That minute of silence is everything.&lt;lb/&gt; ● If you have basic audio knowledge, this website is a much better option than commercial tinnitus therapies based on overpriced hearing aids. Thanks to this website, I’m reconfiguring my relationship with tinnitus in a much more organic and spontaneous way.&lt;lb/&gt; ● My tinnitus is an infrequent problem. When it gets bad, though, it gets horrible. This just saved my whole morning.&lt;lb/&gt; ● I don't have tinnitus, but I love the strangeness of this generator. It sounds really fascinating and seems to help me focus. By the way, I guess this is what an aural representation (a strange tone poem?) of a ketamine trip would sound like :D&lt;lb/&gt; ● I use myNoise to go to sleep.&lt;lb/&gt; ● This doesn't so much drown out my tinnitus as blend with it, but it's still a fantastic change from the usual! I've had tinnitus my entire life, and as a kid, I never understood why people liked silence so much since it was so loud! Weird alien beep-boops is the new silence.&lt;lb/&gt; ● Wow! I don't have tinnitus, but this is stunning!&lt;lb/&gt; ● This is so satisfying to listen to, and I don't even have tinnitus.&lt;lb/&gt; ● It's the best website for listening to sounds while doing productive activities.&lt;lb/&gt; ● I have found "trance" to work well with outside high-pitched noises like old TVs. Nice job.&lt;lb/&gt; ● This is crazy, it actually works. Thankfully my tinnitus is not constant, but very annoying when it's present. It actually makes it disappear, even after I've closed the sound generator, for quite some time.&lt;lb/&gt; ● This is the first time in a while that I've been able to just sit and be content without that infernal ringing hijacking my thoughts. Definitely not something I'd be able to listen to for longer periods, but it's amazing how well my tinnitus just blends in and becomes a part of the experience.&lt;lb/&gt; ● This app and noise really help me in primary school, and since I am sometimes cleverer than my peers, this helps.&lt;lb/&gt; ● Thanks so much for this site. My tinnitus fluctuates a lot, and when it's bad, I rely on sounds/maskers like these for work and at night.&lt;lb/&gt; ● It helps me write an interesting Egyptian story and gives a nice effect.&lt;lb/&gt; ● I was using this to help me focus because I have really bad tinnitus, and it makes it so hard to focus! As soon as it started playing, it drowned out my tinnitus and made me able to think way better! The sounds seem to go all around me, and it's just so helpful. I definitely recommend MyNoise!&lt;lb/&gt; ● Against all odds, it apparently works. I have tested it a couple of times today by playing the audio on headphones for 5-10 minutes, and there is a significant relief in my tinnitus for shorter periods of time. I will try running the audio for a longer time to see if I can have a longer-lasting effect.&lt;lb/&gt; ● It's tingling my brain.&lt;lb/&gt; ● I must be doing it wrong. Unfortunately, the sound in my head got louder as I was adjusting the sliders.&lt;lb/&gt; ● I needed this sound every day in 2022 when I was having major mental issues and anxiety, which created a feedback loop for my tinnitus (tinnitus increased anxiety, anxiety increased tinnitus.) with the help of this sound generator I managed to reduce my anxiety and tinnitus to the point where it's nothing more than a passing thought every now and then, that I come back here to get rid of. THANKS: )&lt;lb/&gt; ● I love this soo much.&lt;lb/&gt; ● I love this beyond belief! Thank you :) &lt;lb/&gt; ● It's funny, the raw noises should really make me angry but they work. I don't get it. Why does it work better than any other tinnitus blocker.&lt;lb/&gt; ● I don't have tinnitus; I have MS. Certain sounds have started to bother me a lot, even waking me up. I can't sleep for longer than 2 hours some nights. It's not fun. I usually have to turn some white noise on loud enough to mask the sounds but that hurts sometimes, and is hard to sleep with. This sound however is pretty weird. I don't have to crank it up loud, and it works. I'm actually sleepy.&lt;lb/&gt; ● It helps me focus on reading. I like it ;)&lt;lb/&gt; ● My form of tinnitus is masked by many of your noise generators. I prefer natural sounds, but this did remarkably let me forget the constant, piercing ring in my right ear rather than just mask it.&lt;lb/&gt; ● Dude thank you, I've had musical ear syndrome since I was a kid, and a life of listening to heavy metal and rock and roll at high volume on headphones and going to shows has made tinnitus worse. Now I get musical ear syndrome all the time, it can be distracting as well a bit concerning but this soundscape cured it instantly. Thank you! This saved me from a mental breakdown.&lt;lb/&gt; ● All points of connectivity reaching all other points.&lt;lb/&gt; ● Works wonders to cancel out the Tinnitus on both of my ears. It seems to make my ringing go away for short periods of time also. &lt;lb/&gt; ● Who else heard an iphone alarm and horror movie riffs? -TDR&lt;lb/&gt; ● This is a god - send! My brain never quiets down, and this lovely soundscape is a perfect solution for letting my brain latch on to something while I do things I need to. Thank you so much!&lt;lb/&gt; ● It might not relieve my tinnitus, but it definitely helps me focus on my homework!&lt;lb/&gt; ● Works for my ADHD as well as tinnitus :3&lt;lb/&gt; ● This is the ONLY thing I've found that actually helps with my tinnitus. 10-20 minutes of listening, and I'm free from it for an hour or even longer. Thank you so, so much for this.&lt;lb/&gt; ● I'm still highly skeptical if this truly works but, with the noises I selected, I like it. &lt;lb/&gt; ● I just recommended this to a friend with tinnitus who used to leave the TV on all night, and we found this present that I hope works better!&lt;lb/&gt; ● This helps me break free from the world. Great noise.&lt;lb/&gt; ● Relief from tinnitus, for the first time in years! I can't handle listening to it for much longer than twenty minutes (because I'm bored by generated noise after a while), but when I take my headphones off, my tinnitus is so massively diminished I can hardly believe it. Compared to everything else I've unsuccessfully tried for my tinnitus, this feels like actual magic--audiomancy?&lt;lb/&gt; ● Wow, this actually works! I didn't expect it, but it does, and while not completely canceling out my tinnitus, works much better than songs or such.&lt;lb/&gt; ● The absolute relief I feel right now has me reeling. My Jaw dropped instantly when I plugged this baby in. My head feels about half the weight it usually does just from honestly maybe a minute total of listening to this. I didn't think there would be such a solution to tinnitus but this might just really improve my quality of life, especially in the falling asleep department! Thank you, thank you!&lt;lb/&gt; ● This sound helps my tinnitus a lot and it also kind of tickles my ears a bit!&lt;lb/&gt; ● Suffering from tinitus for many years, this is helping me tremendously. My tinitus sounds resemble the high pitch noise you get from old TV sets. Listening to this setting for just a couple of minutes brings me relief for up to an hour afterwards, sometimes even longer because I forget to concentrate on it. Just amazing!&lt;lb/&gt; ● I started using MyNoise's ocean sounds years ago to help me sleep due to severe misophonia, it blocks the noise from the street and I don't need to sleep with earplugs or headphones anymore. Recently a covid infection damaged my ear nerves and caused intolerable sound sensitivity accompanied of tinnitus and hyperacusis. If it wasn't for these sound generators I would have lost my sanity.&lt;lb/&gt; ● I have autism and difficulty focusing. I've never liked listening to things like this generator, but for the first time I quite like this! I love the way that I can hear the sounds travel from each side of my brain. It's pretty awesome to listen to when you've got synesthesia. &lt;lb/&gt; ● I have no tinnitus but "Neural Drops" + RPG Evil Charm on the "Monsters" preset + Canyon drone around 1 kHz is my best mix for listening to it before lucid dreaming with out-of-body experience.&lt;lb/&gt; ● For a few years I've used myNoise to block out the sounds of busy city life and to get to sleep. More recently in late 2022 I contracted covid which unfortunately left me with hearing damage and significant tinnitus in one ear. The only thing that has been able to relieve the horrible ringing so far are some of the neuromodulator tracks here, and for that I am grateful. &lt;lb/&gt; ● As an autistic person with pretty debilitating hyperacusis (I pretty much can't function without ear plugs) the onset of some mild tinnitus was distressing to say the least. White noise is usually the recommended go-to for tinnitus but it's painful to me. High pitched fuzzy or low constant humming sounds are literal agony. This is varied enough to ease and accommodate both issues. Miraculous!&lt;lb/&gt; ● Well it just works. I get into music stuff lately after a long session of mixing, mastering or drum take your ear just start to deafen sometimes with headaches. I used this as a tool to "reset" my ear by listening to it at a barely heard loudness and turning it down even more after I can hear it clearly, just rinse and repeat!&lt;lb/&gt; ● Just wanted to add my comment to say that yes, in my experience (loud ~9kHz in left ear) this is more than a placebo effect re: tinnitus relief. HOW exciting. Cheers! Breathing deeper and slower too.&lt;lb/&gt; ● When getting the automation right, it takes ages for piano or guitar sounds to get repetitive, sounds of nature and human voices (radio chatter etc) sound completely natural, organic, and at home here without feeling repetitive at all. And sci-fi sounds, and drone like instruments can pretty much do their thing forever and play well off of each other while they do it.&lt;lb/&gt; ● Suffering since 11 months from tinnitus. I destressed my life, did different types of therapy but nothing came close to the effect this neural symphony has. It actually gives me seconds or minutes as close to silence as I wasn't for 11 months. THANK YOU!&lt;lb/&gt; ● Amazing, always helps when silence is a bit too noisy. My uncle who has bad tinnitus commented that it helped even after a few minutes without headphones... Amazingly helpful in this era of being used to hearing fans, fridges, pinging tech and then ringing noises when it's supposed to be quiet.&lt;lb/&gt; ● Wow, so interesting. It fades away quickly. Greetings from Ecuador.&lt;lb/&gt; ● It takes away my tinnitus if I listen to if for a few minutes. I genuinely cannot believe this. &lt;lb/&gt; ● I have had tinnitus for several years and it recently got worse causing anxiety attacks. After discovering this site I found it really works on lessening the tinnitus and I feel so much better! I play it while working from home. Amazing! &lt;lb/&gt; ● I developed a high pitched whistling tinnitus 10 years ago when I was 14 and I thought I lost the sound of silence forever, but listening to this for 20 minutes on shuffle makes my tinnitus disappear completely! Even if it's just for a little while, it feels wonderful to experience true silence again.&lt;lb/&gt; ● Peaceful and relaxing!&lt;lb/&gt; ● Strangely, this also blocks out when my ears begin to ring highly.&lt;lb/&gt; ● I have tinnitus that sounds like a cat purring. The "purring" is annoying and it causes me to think a cat is near me, a car is starting, Etcetera. These settings work strangely well.&lt;lb/&gt; ● I luckily don't have tinnitus but this sounds like I've been taken by aliens! &lt;lb/&gt; ● Yes!! It helps!!&lt;lb/&gt; ● It definitely assisted me with my tinnitus, I feel substantially more relaxed as well 10/10. &lt;lb/&gt; ● Gone! My tinnitus sounds like a dentist's drill. Sometimes it's hard to blend it out, but with this setting it's gone. Thanx, Doc!&lt;lb/&gt; ● Silence! Silence, thank god! I've never felt so much joy.&lt;lb/&gt; ● I dont have tinnitus but this is still a cool sound to study to.&lt;lb/&gt; ● The tinnitus neuromodulator sounds empty and sinister by itself. To make it less empty and sinister, the sinewave stereo slider can combine well with the take it easy generator on the bad trip preset.&lt;lb/&gt; ● This helps with misophonia, too! I put it at a low volume and listen to it during class, it's super helpful.&lt;lb/&gt; ● I was a bit skeptical, but after listening for 10 minutes on a moderately high volume with ear buds, I can say the tinnitus relief is real! I'm now trying to find out how long the relief will last, and how long I should listen to the modulation sounds... &lt;lb/&gt; ● I don't even have tinnitus, but I like listening to this while I write my sci-fi novel!&lt;lb/&gt; ● Playing with settings was a lot of fun! I masked my tinnitus with it!&lt;lb/&gt; ● It was fun playing with the settings and I lowered some and raised some, slowed it down a bit, and it's really wonderful. I think it helps with my tinnitus but it's also just fun to listen to!&lt;lb/&gt; ● Wow! Being able to hear pure silence even only for a few seconds is truly something. Thank you.&lt;lb/&gt; ● Ahhh this is literally so amazing, it actually helped so so so so so so much!! It got so much quieter after I took my headphones off.&lt;lb/&gt; ● This.... Is good. So good. &lt;lb/&gt; ● Amazing, amazing, amazing for my tinnitus. It's not especially loud, but it's very constant when it's quiet, and having this setting is surprisingly relaxing for my mind to not hear it in the background.&lt;lb/&gt; ● Doesn't get rid of the tinnitus, but masks it while I listen to it.&lt;lb/&gt; ● Suddenly had an onset of tinnitus last night. This setting in particular seems to be holding it down for me while I wait for my trip to an ENT doctor.&lt;lb/&gt; ● Tried a lot of relief sound from youtube, but most of them are too high pitched for me and are very painful. But this one is perfect, you saved my life thank you.&lt;lb/&gt; ● I constantly hear ringing whenever it's silent, and it affects my day-to-day tasks. I used this for a while, and I went somewhere silent, and there you go, no more ringing! Mind blown!&lt;lb/&gt; ● Recently I had been dealing with pretty bad tinnitus, it had gotten to the point where I couldn't even fall asleep some nights. After one day of using this, I have had more relief than I have had in weeks. I am very grateful for this program, I have already donated and plan to continue supporting this.&lt;lb/&gt; ● I suffer from tinnitus due to infections, and this is wonderful! &lt;lb/&gt; ● I don't even have tinnitus, I just find it a cool, emotionally neutral background to stay focused at work... It feels like I'm coding on an alien spaceship!&lt;lb/&gt; ● I can't say if this will work for everyone's tinnitus, but it does for mine! After listening for 20 minutes or so, my tinnitus is much reduced and stays that way for up to two hours afterward. I am interested to see if there is any long-term effect from using this generator daily.&lt;lb/&gt; ● Thank you so much! My tinnitus began after a concussion 6 years ago. I have no hearing problems, just issues caused by my brain. I found your website, turned on my computer speakers &amp;amp; immediately my neck &amp;amp; shoulders relaxed as the tinnitus decreased significantly. Initially I listened for about 1/2 hour &amp;amp; when I walked away it was much lower volume than before. Best for me on the default setting.&lt;lb/&gt; ● This generator is completely amazing, if for no other reason it detracts attention away from my tinnitus, which is now chronic for years and years. The Neural Symphony functions are easy to use and adjust. I have discovered that for me I drop all pulses and warble to zero, and then use the automated slider animation. Wide range of high frequencies, ever changing. Bliss.&lt;lb/&gt; ● ← Click here to hear some crazy hums and beeps! &lt;lb/&gt; ● This was overwhelming for me at first, but I clicked Surprise! a few times and got settings that not only hide my tinnitus but also make a pleasing sound. I love it.&lt;lb/&gt; ● I have tinnitus in scattered frequencies between 8-10k, and this setting on studio monitors (at a quiet volume) works very well to calm my mind, and get my focus off of audio based stimuli. &lt;lb/&gt; ● It works o_0. Tinnitus is not big problem for me because I have it since I was kid, but it's nice feeling to not hear it :D&lt;lb/&gt; ● I loved it, it's kind of magical too.&lt;lb/&gt; ● It's like C3PO had too much sugar lolololololol!&lt;lb/&gt; ● I don't have tinnitus. But this sounds crazy! I love it! 8D&lt;lb/&gt; ● Neuromodulator and Summer Night are about the only things that got me through the worst bout of tinnitus. Thank you so much!&lt;lb/&gt; ● Oh my goodness. You are an absolute godsend; I wish I could have found this sooner. Will def be using this whenever I need it. Thank you so so so so much! &amp;lt;3&lt;lb/&gt; ● I use this daily to focus at work and to get my mind off of my tinnitus. Also, I use this to sleep on my phone, this is the best thing that I've had in a long time.&lt;lb/&gt; ● Thanks -- this doesn't seem to "fix" anything but it does help concentrate with the tinnitus is bothering me. I have great hearing but tinnitus (with high-pitched sounds in one ear and a tiny bit in the other) started a couple weeks ago after my doctor put me on 150mg Effexor. We cut the medication after this happened but the tinnitus is still loud :( Anyway thanks for your work on this site! &lt;lb/&gt; ● I fortunately don't have tinnitus but it helped with my daily earaches anyway.&lt;lb/&gt; ● I thought people were overreacting, my God I was wrong. It truly works.&lt;lb/&gt; ● I have used myNoise numerous times now and I find great relief from my very loud and annoying tinnitus. Waiting to see if any reversal becomes evident. Thank you for this wonderful tool! &lt;lb/&gt; ● It was not working for me at first. The tinnitus would go away and come back after a few seconds. I tweaked the settings so the high pitched noises were louder and more frequent. A day after I am writing this with little to no tinnitus now. Idk if this really works or not but it's worth a try.&lt;lb/&gt; ● I suffer from tinnitus in both the ears and have two distinct high and low frequency patterns. I listened this for about 45 mins, kept away my headphones and then went for a coffee... Guess what I could hear `silence'! Very promising!&lt;lb/&gt; ● I found that all the sounds hurt to listen to and increased or battled with my tinnitus. Disappointing! &lt;lb/&gt; ● This is certainly promising, and I did notice an affect pretty quickly. However, ACRN should be tunable to the frequency of of your tinnitus. The Steve Sequence is kind of all over the place, and mostly tuned lower than my own (6kHz, and pretty common). Would be nice to see something where the frequency can be tuned, and maybe change the waveform (sine, sawtooth, fuzz, etc). [Note from editor : try the Tape Speed Control feature]&lt;lb/&gt; ● Gosh that's impressive, Thank you so much for your work&lt;lb/&gt; ● I don't have tinnitus, but this is really relaxing with the right settings.&lt;lb/&gt; ● I have suffered tinnitus for as long as I can remember, very few things have helped lessen its obnoxious effects, and those that did (like music) were very distracting (I have ADHD too), but this generator works perfectly for me, it helps my tinnitus to the point that I don't even notice it, and even better, its not distracting in any way! Focusing has never been easier.&lt;lb/&gt; ● Try Lake Life (loon calls and lapping shore water) with this - pretty funky. Keep expecting a strange interstellar artifact to light in the depths of the lake and rise in the full moon before me. Fantastical.&lt;lb/&gt; ● Love it with Hydrogen XII.&lt;lb/&gt; ● OH MY GOSH BEAUTIFUL SILENCE&lt;lb/&gt; ● This literally worked in 10 seconds... wow!&lt;lb/&gt; ● This noise generator gave me nausea and vertigo and my tinnitus was unaffected. Am I using it wrong?&lt;lb/&gt; ● I must say that I didn't really have faith in it working to dampen my tinnitus, but I had to eat my words back when I took my headphones off. I was close to shedding tears at finally being close to experiencing silence after all these years of constant noise. I want to thank you for creating this, and I am most definitely going to favorite this and listen to it as much as possible.&lt;lb/&gt; ● While I may not have Tinnitus, this generator is still a very entertaining experience! I sometimes use it to calm myself or focus on writing, which was surprising as I thought it would have the opposite effect!&lt;lb/&gt; ● Yes. Really does improve my tinnitus. Even more, I find it actually quite soothing as ambience for indoor work - less aural fatigue than white noise. For some reason, it even makes house-cleaning less tedious :-) Kudos!&lt;lb/&gt; ● As a musician, I have to deal with tinnitus on the daily. It got bad this morning, and this helped me out a ton.&lt;lb/&gt; ● The tinnitus neuromodulation noises have had a proufound effect on lessening my tinnitus and I thoroughly recommend it to anyone who is seeking relief. Short of a future breakthrough in medication or surgical treatments, this is the best solution I have come across and it makes life that bit more bearable. Many, many, many thanks!&lt;lb/&gt; ● After listening to the Sinescape preset for this generator for only maybe 20 minutes, I found that my rather loud tinnitus because much quieter and almost non-existent immediately after removing my headphones. I'm curious to see how long this lasts, but it's quite a relief even for a little while!&lt;lb/&gt; ● This generator is the first thing that helps with my tinnitus, lowering its pitch and volume until it's almost gone even after I took off the headphones. The relief only lasts for a few hours, but to someone who has a constant high pitched noise in their ears otherwise, those hours mean the world. I cannot thank you enough for your amazing work, Stphane. You're a true blessing to the world.&lt;lb/&gt; ● After listening to this for awhile, my tinnitus was actually GONE after I took off my headphones.I don't know how long this will last but thank you. I will enjoy this sweet, sweet silence while it lasts. Donation coming your way as soon as I get home! Love this site.&lt;lb/&gt; ● My tinnitus is quite high-pitched and fairly constant. If I play this sequence for a little while, it brings it back down under a low roar, if only for a few hours. &amp;lt;3&lt;lb/&gt; ● I cannot express my thanks! Every sound has changed my life in a different way! Thank you SO MUCH! Thank You!!! These settings seem to be the ultimate cure to my T.&lt;lb/&gt; ● ← extreme warbling&lt;lb/&gt; ● Holy Crap! I was experiencing on of my usual Tinnitus spikes (and you know how annoying this can be). I tried this custom noise take from one of the testimonials and my tinnitus just went down from 10 to 4! Thanks!&lt;lb/&gt; ● Brilliant site I could spend hours here. I have tinnitus a different high pitched frequency in each ear.Slightly spacey space-station? A little dreamy but a little sedating as well and kills the tinnitus.&lt;lb/&gt; ● This works for me - a little dreamy but a little sedating as well.&lt;lb/&gt; ● This is a very strange soundscape, but in an enjoyable way. I personally do not have tinnitus, but I think this is still fun to listen to. You just have to find the right sliders to make sense of this one, hahaha.&lt;lb/&gt; ● That's artificial rain... trickle, and some wind. Very relaxing during computer work! Both stimulating and calming. :)&lt;lb/&gt; ● I have Tinnitus. I also have perfect hearing and from years of being a musician one would anticipate hearing loss. None. I am also a Director for an electric and water utility and I have a lot of concentrating work (reports, analysis, etc). When My Tinnitus is in full flight, it is impossible to concentrate. This is a life saver. I can work all day and feel the calming of the tones. Amazing! &lt;lb/&gt; ● I am sitting in the library trying to work and someone near me has the touch-tones turned all the way up on their phone. This is the only thing that drowns it out. Thank you!!!&lt;lb/&gt; ● Back again, THANK GOD for this. When I'm having a rough day with my T, I just come home, throw this on and chill and just zone out and stuff I love it. Thank you!&lt;lb/&gt; ● May be my imagination, listened for about 15 min, then when taking my speakers away from my ears, my pulsating tinnitus was quieter.&lt;lb/&gt; ● First I found myself heaving a relieved sigh, and then started crying when I found the custom noise that masked my tinnitus. It has been five months of unrelenting high pitched whine, yet I refused to allow myself a pity party, concluding, "It is what it is; suck it up, girl, there is little you can do." But to have this respite is a great blessing. Thank you! &lt;lb/&gt; ● Kudos to all who put this wonderful chaotic neural symphony together! As a tinnitus sufferer I found the sounds had a very soothing effect on me. I will be using this selection again for sure.&lt;lb/&gt; ● I noticed listening to music with different frequencies helps soften my Tinnitus. I have a constant 24/7 14,000hz sawtooth sound in my head for the two years. This program is amazing! &lt;lb/&gt; ● It is soothing but I hear my tinnitus while I am listening to it. I don't really think this would help me in any way, long term. It is soothing though, so thanks for that! &lt;lb/&gt; ● Not for me. Sounds more like star wars. What really helps me is a soft noise plug. Stick in the ear canal and when the sounds starts bothering switch to other ear. This is the only thing that helps me.&lt;lb/&gt; ● This is incredible. I've had a fairly mild, but annoying tinnitus accompanying me for some time now and this Neural Hack makes it seem magically inexistent. I am impressed (and very relaxed).&lt;lb/&gt; ● I have been listening to music with MY Tinnitus frequency embedded into the music. I understand this works for some, not me. After over 6 months, no change. The past two days have been horrible... I am a musician and although I have been playing rock music for over 40 years, I have no hearing damage at all... 20 minutes on customizing my own pattern, I can actually concentrate on my work!&lt;lb/&gt; ● Thank you so much for this incredible site! I'm quite happy to make a donation to you out of simple gratitude for all your efforts putting these sounds online for us, and keeping the site (and your iOS app, myNoise) working. I've experienced persistent tinnitus for a couple decades, now, with almost no respite except after one minutes or so of "Neural Hack" or "Neural Symphony!" Amazing!&lt;lb/&gt; ● I have severe tinnitus and this noise generator may actually change my life.&lt;lb/&gt; ● Thank you so much for this. It provides relief for my tinnitus when nothing else does.&lt;lb/&gt; ● This site is amazing! Brilliant concept and execution! As a Tinnitus sufferer, it has become safe haven for me. Thank you so much, Stephane!&lt;lb/&gt; ● I'm a fairly new comer to myNoise having serendipitously discovered the site only three weeks ago and a very long sufferer of Tinnitus. I have only discovered this particular generator only moments ago and jumped right in to listen. Ironically, I've been having episodes all week so I'll absolutely write the results of my experience after one week and again at two weeks. Today is 8 June '16 &lt;lb/&gt; ● Something very weird happens if I listen to this for 15-20 minutes: in addition to the tinnitus masking, my mind actually "quiets down". The parts of my brain that would normally be wandering seem to be occupied chasing down the bells and warbles, allowing the rest of my attention to focus entirely on the task at hand. Awesome brain hack.&lt;lb/&gt; ● I don't have tinnitus, but I find myself enjoying the bizarre dreamlike discord that using the animation sliders can provide. It's a rather lovely chiming chaos... that is weirdly calming to me. I'm not sure what that says :)&lt;lb/&gt; ● My goodness, I've been waiting for something like this. God Bless!&lt;lb/&gt; ● I guess I am fortunate that I only get ringing in my ears when I take certain prescription medications I can't believe my luck that a few hours after I took the medication I started to get the ringing and came here to find some rain sounds and discovered this new generator. What a gift this is! Thank you, thank you, thank you!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mynoise.net/NoiseMachines/neuromodulationTonesGenerator.php"/><published>2025-10-18T16:08:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45629970</id><title>How to sequence your DNA for &lt;$2k</title><updated>2025-10-19T14:34:44.790348+00:00</updated><content/><link href="https://maxlangenkamp.substack.com/p/how-to-sequence-your-dna-for-2k"/><published>2025-10-18T19:58:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45631422</id><title>GoGoGrandparent (YC S16) Is Hiring Back End and Full-Stack Engineers</title><updated>2025-10-19T14:34:44.468708+00:00</updated><content>&lt;doc fingerprint="73bbe85d35405560"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;About Us We're a digital caregiving platform dedicated to helping older and disabled adults live independently and thrive in their own homes—avoiding the need for retirement communities. We adapt on-demand APIs from companies like Uber, Lyft, DoorDash, and Instacart to meet the unique needs of individuals with cognitive, visual, mobility, and dexterity challenges. We're a highly profitable and fast-growing startup. Our team is fully remote, with a total engineering headcount of 12 (including this role).&lt;/p&gt;
      &lt;p&gt;FULLY REMOTE | US, UK, or able to work 4+ hours overlap with mainland US | $100k – $160k (partially location-dependent)&lt;/p&gt;
      &lt;p&gt;Tech Stack Back-end heavy: Node.js, TypeScript, MySQL, REST + GraphQL Front-end: Vue.js (nice to have) Deployment: AWS, Docker/Kubernetes (nice to have)&lt;/p&gt;
      &lt;p&gt;Requirements 6+ years of professional experience (primarily in Node.js and Vue.js)&lt;/p&gt;
      &lt;p&gt;Interview Process 2-stage interview process.&lt;/p&gt;
      &lt;p&gt;If you're passionate about improving the lives of older adults and people with disabilities, send your LinkedIn or CV (keep it brief) to william@gogograndparent.com, or apply at https://www.ycombinator.com/companies/gogograndparent/jobs&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45631422"/><published>2025-10-19T01:00:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45631678</id><title>The Accountability Problem</title><updated>2025-10-19T14:34:43.952798+00:00</updated><content>&lt;doc fingerprint="30639015e4772d9c"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The Accountability Problem&lt;/head&gt;
    &lt;head rend="h3"&gt;October 18, 2025&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;This is a transcript of my keynote presentation at the Agile Cambridge conference in England on October 2nd, 2025. The topic was “The Accountability Problem.” How do we define software department accountability so our business partners don’t do it for us?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Thanks for having me. I’m very happy to be here in Cambridge. This is my first time visiting, so I spent the afternoon Tuesday doing some sightseeing, including a lovely ride down the River Cam. I was delighted to learn yesterday that I had Simon Wardley to thank for chauffered punt rides, including the completely fictional story I was told about the mathematical bridge.&lt;/p&gt;
    &lt;p&gt;One of the things I love about Cambridge is its rich history. Of course, lots of history is important when you have...&lt;/p&gt;
    &lt;p&gt;...this monster eating up every second.&lt;/p&gt;
    &lt;p&gt;That’s the Chronophage outside of Corpus Christi college, if you aren’t familiar with it, and much more impressive in person than in my terrible vertical picture with window glare.&lt;/p&gt;
    &lt;p&gt;Before we get going, I should explain my context. You’ll hear a lot of advice at this conference, and how much that advice is relevant to you has a lot to do with how much their context matches yours.&lt;/p&gt;
    &lt;p&gt;I’m currently VP of Engineering at OpenSesame, and for the 23 years prior to that, I was a consultant. As VP, and as a consultant, I specialize in late-stage startups: entrepreneurial organizations that were successful enough that they were able to grow. These are companies with a product mindset that value entrepreneurial thinking, but they’re also trying to grow up and be “real companies,” and they’re trying to figure out how to do that without losing their entrepreneurial edge.&lt;/p&gt;
    &lt;p&gt;So that’s the context of my material: entrepreneurial companies building software products that they sell. If you’re not in that situation, I encourage you to mine my talk for ideas, but don’t try to apply it blindly. And if you are in that situation... well, mine my talk for ideas, and don’t try to apply it blindly!&lt;/p&gt;
    &lt;p&gt;A few more disclaimers. All the substantive content of this talk—the words, diagrams, examples, and so forth—were created with my actual meat brain, without any AI. Large images have been sourced from various locations, and are credited in the bottom left corner.&lt;/p&gt;
    &lt;p&gt;I’ve also dressed up some of the slides with decorative AI-generated images from ChatGPT 5, like that rapper holding a stop sign. If there’s one thing GenAI is good at, it’s embellishment.&lt;/p&gt;
    &lt;p&gt;I should also mention that, although I work for OpenSesame, I’m not speaking for OpenSesame. I created this talk on my own time, and I’m technically on vacation right now. The opinions I express are my own.&lt;/p&gt;
    &lt;p&gt;Anyway, as I was saying, one of the things I love about Cambridge is its rich history. I’m sure you’ve all heard several times by now that the university was founded back in 1209, by people fleeing [waves hand dismissively] the other university. In comparison, my home town of Astoria, Oregon, which is the oldest permanent settlement on the west coast of the US, was founded in 1811. I think that’s last Tuesday by British standards.&lt;/p&gt;
    &lt;p&gt;Part of the history surrounding Cambridge is this man: LP Hartley. He was born in Cambridgeshire in 1895, although he never went to Cambridge University. He went to... the other one. But, despite that choice, he went on to become a successful novelist.&lt;/p&gt;
    &lt;p&gt;His most famous novel is “The Go-Between.” It begins with a wonderful opening line:&lt;/p&gt;
    &lt;p&gt;“The past is a foreign country: they do things differently there.”&lt;/p&gt;
    &lt;p&gt;And that connects us back to Cambridge. The University of Cambridge Press published this book in 1985. It’s by David Lowenthal, and it created an entire sub-genre of history called Heritage Studies. It’s still in print today, in a revised edition.&lt;/p&gt;
    &lt;p&gt;The concept of this idea is that, although the past informs the present, the present also informs the past. Our thoughts and actions today extend from events that occurred in the past. But, at the same time, our understanding of the past is colored by our thoughts and actions today.&lt;/p&gt;
    &lt;p&gt;The past is a foreign country. They do things different there. But we can’t visit the past. We can’t see what they did differently. We can only interpret what they’ve left behind.&lt;/p&gt;
    &lt;p&gt;And like medieval scholars drawing elephants they’ve never seen, we make those interpretations through the lens of our own biases.&lt;/p&gt;
    &lt;p&gt;I love these medieval drawings of elephants. They’re so delightfully strange.&lt;/p&gt;
    &lt;p&gt;But I’m not showing you these pictures to make a point about how difficult it is to draw an elephant when you haven’t seen one.&lt;/p&gt;
    &lt;p&gt;If you go back to Corpus Christi, where they have the Chronophage—not right now! I’ll start talking about software soon, promise. Anyway, at Corpus Christi, they have Matthew Paris’ Chronica Majora. It contains this drawing of an elephant. You might assume that it came much later, because it’s so much more accurate. But all of these drawings were created around the same time, in the 13th century.&lt;/p&gt;
    &lt;p&gt;It’s quite the difference, isn’t it?&lt;/p&gt;
    &lt;p&gt;I’m not showing you these images to make a point about medieval monks. I’m actually showing them to make a point about your biases. In the modern era, we expect images to be true to life. We have cameras that give us nearly perfect representations of the world. But realism isn’t what medieval monks were always trying to accomplish. Religion and metaphor were a central part of their lives, to a degree that I think we in the modern world have trouble understanding.&lt;/p&gt;
    &lt;p&gt;The elephants on the left aren’t really elephants. They’re a way of presenting a moral lesson about your place in the world. The image serves that story. It’s not there to teach you about elephants. It’s there to teach you about God.&lt;/p&gt;
    &lt;p&gt;So if your first reaction to these elephants was to laugh at those ignorant medieval monks... then perhaps you’ve fallen prey to your biases. The elephant doesn’t look like an elephant because the metaphor was more important than the reality.&lt;/p&gt;
    &lt;p&gt;The past is a foreign country. They do things differently there.&lt;/p&gt;
    &lt;p&gt;[beat]&lt;/p&gt;
    &lt;p&gt;The past informs the present, but the present informs the past. We can’t help but to interpret it through the lens of our own experience, and those biases distort the reality of what it was actually like to live there.&lt;/p&gt;
    &lt;p&gt;This idea fascinates me, because it’s not only true of the past; it’s true of everything. Our biases and experiences influence so much of how we interpret the world.&lt;/p&gt;
    &lt;p&gt;I taught teams Extreme Programming for a few decades, as a consultant. Now that I’m VP of Engineering, I’m still teaching it, in a way. One thing that’s stood out to me over the years is that the people who struggle the most to learn XP are the ones who are more senior.&lt;/p&gt;
    &lt;p&gt;Junior developers have no problem! It’s the senior developers who struggle. They have too much baggage from their preconceptions.&lt;/p&gt;
    &lt;p&gt;A good example of this comes from Microsoft. XP was popular in the early 2000s, and practices like test-driven development, which come from XP, were entering the mainstream. So Microsoft published a set of “Guidelines for Test-Driven Development.”&lt;/p&gt;
    &lt;p&gt;There was a big backlash, and Microsoft took their guidelines down pretty quickly, because they got them terribly, ridiculously, horribly wrong. Microsoft didn’t actually practice XP, as far as I can tell, so they didn’t know that XP is a way of keeping software design simple and evolving it in response to customer needs. In XP, you don’t create your design in advance; you discover it as you go, and you focus on keeping it as simple as you can.&lt;/p&gt;
    &lt;p&gt;People who have practiced XP know that TDD is about tests and code evolving in step with each other, so that you learn as you go. A few lines of test code. See the tests fail. A few lines of production code. See the tests pass. A few improvements to the design. See the tests pass. A few more lines of test code. See the tests fail. And so on, and so forth, until the software is done, without following a preconceived path.&lt;/p&gt;
    &lt;p&gt;As with many companies past and present, the Microsoft way wasn’t to evolve their design; it was to come up with a software design in advance, then build to that preconceived design. And so they saw what Kent Beck and others had said about TDD and interpreted it in the only way they knew how: as a way of coming up with a software design in advance, and then building to that design. Their guidelines for TDD were to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Gather the requirements for your new feature&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Make a list of tests that will satisfy the requirements&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;File work items for the tests that need to be written&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Generate all the interfaces and classes you’ll need—using Visual Studio, of course!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Write all the tests&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Write all the production code&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m not exaggerating! This is what they actually said. Refactoring, iteration, learning as you go—key ideas of XP and TDD—nowhere to be found.&lt;/p&gt;
    &lt;p&gt;Microsoft’s approach to TDD was the exact opposite of what TDD was about. But they were only able to interpret TDD through the lens of their corporate approach to software development. And to this day, you see this same misunderstanding about TDD repeated by people who are steeped in up-front thinking.&lt;/p&gt;
    &lt;p&gt;XP is a foreign country. We do things differently here.&lt;/p&gt;
    &lt;p&gt;[beat]&lt;/p&gt;
    &lt;p&gt;As people, we can’t help but to interpret the world through lens of our own biases. But that means that we make assumptions about the world that aren’t true, and we can’t even recognize that we’re doing it. It’s not just the past that’s a foreign country... almost everything is.&lt;/p&gt;
    &lt;p&gt;That leads to problems. And in software, one of the biggest, is...&lt;/p&gt;
    &lt;p&gt;...the Accountability Problem.&lt;/p&gt;
    &lt;p&gt;[beat]&lt;/p&gt;
    &lt;p&gt;People who aren’t software developers have probably seen more “programming” in movies and TV shows than in real life. Those shows are filled with magical people who can “hack” anything off-camera and in moments.&lt;/p&gt;
    &lt;p&gt;“The ship’s going to ram us, captain!” “Quick, hack into their retro-encabulator and reverse the polarity of their thrusters!” (frantic typing, dramatic music, camera zoooooooom) “It just barely missed us! Hoorah!”&lt;/p&gt;
    &lt;p&gt;I only wish software development was that cool.&lt;/p&gt;
    &lt;p&gt;Of course, people do know that’s fiction. Some of them might have even written code in school. But in school, people write small programs that fulfill an assignment and don’t have to be maintained.&lt;/p&gt;
    &lt;p&gt;Or maybe they’ve vibe-coded an app using GenAI.&lt;/p&gt;
    &lt;p&gt;None of these experiences bear any relationship to the modern world of software development.&lt;/p&gt;
    &lt;p&gt;TV show hackers are just another deus ex machina... quite literally. It’s lazy writing.&lt;/p&gt;
    &lt;p&gt;School projects don’t require long-term maintenance or large-scale coordination.&lt;/p&gt;
    &lt;p&gt;Unsupervised AI coding assistants feel magical, but they break down once you get past the prototype stage.&lt;/p&gt;
    &lt;p&gt;All of these things trick people into thinking that software development is about code. About hands on keyboard. But that’s not what it’s about at all.&lt;/p&gt;
    &lt;p&gt;To paraphrase Kent Beck, professional software development is about...&lt;/p&gt;
    &lt;p&gt;Communication and collaboration between large numbers of people with different perspectives.&lt;/p&gt;
    &lt;p&gt;Feedback loops that enable us to tell when we’re building the right thing, and the thing right... and when we’re not.&lt;/p&gt;
    &lt;p&gt;Simplicity, because it’s our ability to understand and change software that determines timelines and cost.&lt;/p&gt;
    &lt;p&gt;Courage to do the right thing even when it’s hard, and it’s often hard.&lt;/p&gt;
    &lt;p&gt;Respect for the people doing the work and the people affected by the work.&lt;/p&gt;
    &lt;p&gt;We know that software development is a matter of discovery and coordination. But to our business partners, we’re a foreign country. They can only see us through the lens of their experience.&lt;/p&gt;
    &lt;p&gt;Their experience is that software development is about writing code, in the same manner that someone might do a homework assignment. It’s tedious, perhaps; time-consuming, maybe; but ultimately, a matter of buckling down and doing the assignment... following a straight path from here to there.&lt;/p&gt;
    &lt;p&gt;If you think this way—if you think that software development is like a big homework assignment—then you start making a bunch of assumptions.&lt;/p&gt;
    &lt;p&gt;You assume that you only need to define the assignment correctly to get the right answer.&lt;/p&gt;
    &lt;p&gt;You assume that the assignment has one right answer, and there’s a clear path to that answer.&lt;/p&gt;
    &lt;p&gt;You assume that people can tell you what that path is and how long it will take.&lt;/p&gt;
    &lt;p&gt;You assume that, when work isn’t getting done according to that schedule, it’s because people aren’t working hard enough.&lt;/p&gt;
    &lt;p&gt;And you assume that, when work’s behind, putting pressure on people will make them work harder and get it done on time.&lt;/p&gt;
    &lt;p&gt;Ultimately, you think software development looks like this [play animation]: a trip from point A to point B.&lt;/p&gt;
    &lt;p&gt;When in reality, it’s more like this [play animation]: a process of exploration and discovery, where the outcome isn’t known until you get there.&lt;/p&gt;
    &lt;p&gt;Software development is a foreign country. We do things differently here.&lt;/p&gt;
    &lt;p&gt;These misconceptions aren’t harmless. They extend deep into organizational structures. The biggest impact is how software development is run in most organizations. Most organizations use project-based governance. You create a plan, then you work the plan. If you execute the plan properly, you’ll be successful, and you’ll finish on time.&lt;/p&gt;
    &lt;p&gt;In this environment, it’s management’s job to make sure that the plan is created correctly, worked correctly, and that people don’t slack off.&lt;/p&gt;
    &lt;p&gt;How do you know management is doing their job? What are they accountable for?&lt;/p&gt;
    &lt;p&gt;Delivering software on time and on budget.&lt;/p&gt;
    &lt;p&gt;It’s clean, it’s neat, it’s easy to understand, and it matches people’s misconceptions about software development.&lt;/p&gt;
    &lt;p&gt;And it results in bad software.&lt;/p&gt;
    &lt;p&gt;The whole premise that we can define the assignment in advance is incorrect. Software development is a process of discovery—of iteration and refinement. We learn as we go, and that changes our plans.&lt;/p&gt;
    &lt;p&gt;This is an Agile development conference. You’ve heard it all before. I’m not going to belabor the point.&lt;/p&gt;
    &lt;p&gt;But our business partners haven’t heard it before, or if they have, it’s counter to their experiences. Like us seeing medieval pictures of elephants, like Microsoft with TDD, they can’t help but interpret the world through their own biases. And those biases lead to project-based governance.&lt;/p&gt;
    &lt;p&gt;In their minds, anything less... is a lack of accountability.&lt;/p&gt;
    &lt;p&gt;So what can we do about this?&lt;/p&gt;
    &lt;p&gt;Ultimately, accountability is about being responsible for a set of results. At the executive level, everybody has to be accountable.&lt;/p&gt;
    &lt;p&gt;Marketing is responsible for generating leads for your Sales department. They say how many qualifying leads they’re going to create, and they’re accountable for having done so.&lt;/p&gt;
    &lt;p&gt;Partners also generates leads, or even sales, from people who are using complementary products and services. They’re accountable for bringing in partners, and for the revenue those partners generate.&lt;/p&gt;
    &lt;p&gt;Sales converts leads into paying customers. They’re accountable for the revenue generated by those customers.&lt;/p&gt;
    &lt;p&gt;Customer Success takes care of your customers. They’re accountable for retention, and for generating additional revenue from upsells.&lt;/p&gt;
    &lt;p&gt;Everyone is accountable for doing what they say they’ll do, including us in software development. But there’s something different about how everyone else is accountable. Did you notice?&lt;/p&gt;
    &lt;p&gt;For other departments, accountability is about the results they’re bringing to the organization, not the work they’re putting in. Sales isn’t saying, “we’re going to land customer X on date Y.” Everybody knows that sales take time, and things go sideways. So Sales says “we don’t know exactly which customers we’re going to land, or when, but overall, we’re going to generate X dollars of revenue.” Same for Marketing, and Partners, and Customer Success. We in software are the only ones who have to predict exactly what and when.&lt;/p&gt;
    &lt;p&gt;Our business colleagues aren’t unreasonable. They understand that things go wrong. But they also believe, deep in their hearts, that if you aren’t accountable, you won’t put forth your full effort.&lt;/p&gt;
    &lt;p&gt;And if we don’t define how we’re going to be accountable, they’ll do it for us, in the only way they know how. Which features are you going to deliver? When? If you don’t deliver them on time, you aren’t being accountable.&lt;/p&gt;
    &lt;p&gt;We have to change the script.&lt;/p&gt;
    &lt;p&gt;So what should we be accountable for instead? What, exactly, do we do? What results do we create?&lt;/p&gt;
    &lt;p&gt;[beat]&lt;/p&gt;
    &lt;p&gt;We create new opportunities. Let’s say that the trajectory of your company is to grow its annual revenue by $10mm per year. Our job is to increase that rate of growth, to $12, $15, $20mm per year. Every time we ship a new feature, we should be increasing that rate of growth.&lt;/p&gt;
    &lt;p&gt;Our features should open up new markets, allowing Marketing to generate more leads.&lt;/p&gt;
    &lt;p&gt;We should provide useful APIs, allowing Partners to build new relationships.&lt;/p&gt;
    &lt;p&gt;We should respond to market trends, allowing Sales to convert more leads.&lt;/p&gt;
    &lt;p&gt;And we should fix the problems that get in customers’ way, reducing churn and increasing upsell.&lt;/p&gt;
    &lt;p&gt;What are we accountable for? We’re accountable for improving our companies’ trajectories. Every dollar invested into software development, other than keeping the lights on, should be reflected in permanent improvements to the value your company creates. That value may not be literal dollars or pounds; it may be helping to cure malaria or fighting climate change. But however you define value, the purpose of our work is to change that trajectory for the better.&lt;/p&gt;
    &lt;p&gt;It’s easy to say that we’ll be accountable for improving our companies’ trajectories. But how do we actually demonstrate that we’re doing so?&lt;/p&gt;
    &lt;p&gt;It’s nearly impossible to quantify the impact of any individual feature. It takes months to see an impact from a new feature, and even then, we can’t say that feature X resulted in change in behavior Y. Let’s say churn went down by half a percent. That’s great! Did it go down because of the feature we just released? Or because of a different one? Or is it more that interest rates just dropped and we hired an amazing new director for our customer success department?&lt;/p&gt;
    &lt;p&gt;This is why it’s tempting to look at when you’ll deliver a feature. It’s easy to measure.&lt;/p&gt;
    &lt;p&gt;But ultimately, features are a means to an end, not the end itself. There’s an old cliché that people don’t want a shovel, they want a hole in the ground. And they don’t want a hole in the ground, they want a building foundation. And they don’t want a building foundation, they want a nice big stable. And they don’t want a stable, they want war elephants that make their enemies say things like, “Carthago Delende Est!”&lt;/p&gt;
    &lt;p&gt;When we talk about delivering features, we’re talking about shovels when we should be talking about striking fear into the hearts of Roman soldiers.&lt;/p&gt;
    &lt;p&gt;So instead of talking about features, I’ve introduced a way of talking about value. At OpenSesame, we’re calling them “Product Bets.”&lt;/p&gt;
    &lt;p&gt;Before we go further, a quick disclaimer. The term “bet” is common among startups and other entrepreneurial organizations, so you’ll hear the phrase “product bet” from a lot of different people. Each of us is using it in our own way. So my use of “product bets” isn’t the same as what you might have seen from somewhere else.&lt;/p&gt;
    &lt;p&gt;Okay, so what do we mean when we say product bet?&lt;/p&gt;
    &lt;p&gt;Ultimately, it’s a strategic investment in a business result. It’s summarized with a single sentence that has two parts:&lt;/p&gt;
    &lt;p&gt;First, the business outcome: Strike fear into the hearts of Roman infantry!&lt;/p&gt;
    &lt;p&gt;Second, the means by which we do so: ...by fielding a battalion of war-capable elephants.&lt;/p&gt;
    &lt;p&gt;The result always comes first: strike fear. The mechanism comes second: war elephants. And even then, it’s high level. We need a stable, we need animal breeders and trainers, we need to train soldiers, we need a supply line. We need so many things, and not just software. Those are features. We don’t talk about features in our product bet. We keep it high level. Just the headline.&lt;/p&gt;
    &lt;p&gt;Next, we need a sponsor. Who amongst our leadership team is going advocate for this result? At OpenSesame, it’s usually our Chief Product Officer. But sometimes it’s our Chief Customer Officer, who’s in charge of sales and retention.&lt;/p&gt;
    &lt;p&gt;For the Carthaginians, of course, the sponsor is General Hannibal.&lt;/p&gt;
    &lt;p&gt;Next we talk about estimated present value. This is a core innovation. As I said, it’s nearly impossible to measure the impact of any feature, or even set of features. There’s too many confounding factors.&lt;/p&gt;
    &lt;p&gt;So we don’t measure the impact. We estimate the impact.&lt;/p&gt;
    &lt;p&gt;My software department takes accountability for delivering estimated value, not measured value.&lt;/p&gt;
    &lt;p&gt;Now, that’s not to say that we don’t want to validate results. Jeff Patton talks about using Dave McClure’s Pirate Metrics to do so. I welcome and encourage that kind of validation. Ultimately, you have to decide if the bet was successful.&lt;/p&gt;
    &lt;p&gt;(Spoilers: Hannibal’s bet isn’t going to be as successful as he was hoping.)&lt;/p&gt;
    &lt;p&gt;But the key idea of these product bets is that you don’t have to measure value. You only have to decide if the bet was successful. If it is, we get credit for the estimated value, not the actual value, which saves us a lot of time and trouble.&lt;/p&gt;
    &lt;p&gt;Estimating value allows us to be accountable without predicting specific dates and features.&lt;/p&gt;
    &lt;p&gt;Remember that the head of Sales is accountable for delivering a certain amount of new business every year. Let’s say it’s 10 million dollars. They’re going to deploy a certain number of sales people towards small-to-medium businesses, some towards mid-market, some towards enterprise. They’re going to conduct training and organize incentive programs. They’re going to get everybody fired up about how they need to sell, sell, sell! They’re going to monitor calls, check SalesForce, make sure people are following up.&lt;/p&gt;
    &lt;p&gt;But they’re not going to say, “Enterprise X is going to sign on date Y.” Because they can’t. The buyer’s going to go on vacation. Legal’s going to demand redlines. A year in advance, nobody knows when the contract will be signed, or if it will even be signed at all. But overall, they’ve got enough going on that they can say, “yes, we’re going to close $10mm in sales this year.”&lt;/p&gt;
    &lt;p&gt;The same is true of us. A year in advance, we don’t know which bets we’re going to do. We don’t know how much it’s going to cost to build them. We don’t know which ones are going to be successful and which ones are going to fail. But overall, we can say, “Yes, we’re going to deliver bets that are worth $10mm in estimated value this year.”&lt;/p&gt;
    &lt;p&gt;And that’s accountability.&lt;/p&gt;
    &lt;p&gt;[beat]&lt;/p&gt;
    &lt;p&gt;Wait a moment. “We don’t know how much it’s going to cost to build a bet?” How can we decide what to do if we don’t know how much it’s going to cost?&lt;/p&gt;
    &lt;p&gt;At this point, all we have is a headline. There’s no way for us to know how much it will cost, because we don’t know exactly what we’re going to build.&lt;/p&gt;
    &lt;p&gt;And if we’re doing Agile right, we will never know exactly what we’re going to build until after it’s done. As you all know, Agile software development is iterative and incremental. It’s a process of discovery.&lt;/p&gt;
    &lt;p&gt;I like Eric Ries’ characterization of this idea: we build, we measure, we learn, over and over again. And we don’t know what we’re going to do here [points at “build” step] until we know what happened here [points at “learn” step]. As long as we’re genuinely learning, we can’t know our costs in advance.&lt;/p&gt;
    &lt;p&gt;What we can do, though, is put a maximum limit on how much we’ll spend. I call it the “maximum wager,” to continue with the betting theme. We track our spending, and if we’re not successful by the time we hit the limit, the bet has failed. We shut it down and move on to the next one. Or, at the very least, take a hard look at where things are at and decide on a new wager. As long as the total spending is less than the present value, it could still be a good investment.&lt;/p&gt;
    &lt;p&gt;The amount of the maximum wager is for your leadership team to decide. It’s not an estimate of cost. It’s a gut check about risk and value. The higher the value of the bet, the more you can wager. But you don’t want to wager so much that it would be crippling if the bet failed. Some bets will fail, and you’ll get nothing for your efforts. Success doesn’t mean fielding elephants. Success means winning a war with our elephants, and those Romans can be tricky.&lt;/p&gt;
    &lt;p&gt;The maximum wager is based on your leadership team’s gut feel of the risk and value involved. It’s not based on how much we think the bet will cost; it’s based on how much we’re willing to lose.&lt;/p&gt;
    &lt;p&gt;And then we do our best to make sure that potential loss is minimized. We use the "Build, Measure, Learn" loop to validate whether the bet is going to be successful early on. Maybe one loop is focused on taking elephants up into the mountains to see how they handle the harsh conditions, and another loop dedicates a “red team” to see if they can be spooked into fleeing during battle.&lt;/p&gt;
    &lt;p&gt;It turns out they can. It would be nice to discover that early, not in the middle of battle with the Romans.&lt;/p&gt;
    &lt;p&gt;Although we in software are accountable for estimated value, not actual value, we only get to take credit for successful bets. It’s in our interest, and everyone’s interest, to weed out the unsuccessful bets early, so we can spend more time focusing on the successful ones. And so, we should design our build-measure-learn loops to test for failure as early as possible.&lt;/p&gt;
    &lt;p&gt;With value and a maximum cost, we can perform an apples-to-apples comparison between bets and choose the one that seems like the best one to do next. Often, that will be the one with the highest value.&lt;/p&gt;
    &lt;p&gt;But don’t be fooled by all these numbers! They’re just estimates and guesses. A smart leadership team will go with their gut, not just follow the numbers like robots. The numbers are there to feed a conversation: to get people thinking. They’re not there to substitute for experience and judgment.&lt;/p&gt;
    &lt;p&gt;The big question: Does this work?&lt;/p&gt;
    &lt;p&gt;For me, so far, yes. It took me nearly two years to get my leadership team to really engage with this approach, and I needed the strong support of my CEO and CPO to get there. My CEO, in particular, had to get pretty insistent before people would engage.&lt;/p&gt;
    &lt;p&gt;The fact is, putting together bets, even such high-level ones, takes work. It also makes people accountable, by putting concrete numbers on previously-vague statements about value, and despite everybody’s desire for other people to be accountable, most leadership teams I’ve worked with aren’t really looking to take on more accountability themselves.&lt;/p&gt;
    &lt;p&gt;But, thanks to my CPO and CEO’s support, I can say that we are building software using product bets. We identified a handful to take to the leadership team earlier this year. They estimated the value, then chose a specific set of bets for us to pursue based on our capacity. It’s definitely elevated our conversation around product strategy, and I can see it getting even better as we gain familiarity with the approach.&lt;/p&gt;
    &lt;p&gt;What we haven’t done yet is finish any bets. We just started our first formal bets this year. So I can’t yet tell you how it will turn out.&lt;/p&gt;
    &lt;p&gt;What I can tell you is that I’m getting a lot less pushback than I used to about features and dates. The conversation is focused on bets, not features and dates, and when we talk about what folks want from Engineering, it’s less about, "tell me when you’re going to be done," and more about how we can take on more bets.&lt;/p&gt;
    &lt;p&gt;So, even though I haven’t yet used product bets to truly demonstrate accountability, they already seem to be helping.&lt;/p&gt;
    &lt;p&gt;Does it work? For me, so far, yes.&lt;/p&gt;
    &lt;p&gt;To summarize, we’re working on demonstrating accountability with product bets.&lt;/p&gt;
    &lt;p&gt;Specifically, we’re going to commit to delivering a certain amount of estimated value each year.&lt;/p&gt;
    &lt;p&gt;That estimated value comes from product bets. Each product bet is summarized by headline that focuses on a business result with a high level description of how we’ll achieve that result.&lt;/p&gt;
    &lt;p&gt;The bets to pursue are decided by the leadership team, and each bet has a leadership sponsor who champions it within that team.&lt;/p&gt;
    &lt;p&gt;Bets have an estimated value, and we focus on the estimate rather than trying to prove out actual value.&lt;/p&gt;
    &lt;p&gt;The leadership team also defines a maximum wager for each bet, which is based on a gut feel of risk and benefits, not costs, and together with the present value allows us to perform apples-to-apples comparisons of the bets.&lt;/p&gt;
    &lt;p&gt;At this point, you might be wondering: where does that "present value" number come from?&lt;/p&gt;
    &lt;p&gt;The answer, like all things in business, is spreadsheets. Magical spreadsheets filled with arbitrary guesses.&lt;/p&gt;
    &lt;p&gt;The secret to spreadsheets is that they make our guesses look official. Professional. Good Business-y.&lt;/p&gt;
    &lt;p&gt;But seriously, yeah, spreadsheets. Let me show you.&lt;/p&gt;
    &lt;p&gt;Let me start out by explaining what “present value” is, just in case some of you aren’t familiar with it.&lt;/p&gt;
    &lt;p&gt;The core idea of “present value” is that money—let’s say $10—is worth more today than it is tomorrow. Today, I can buy a couple of candy bars with $10. In a few decades, I’ll only be able to buy half a candy bar due to inflation.&lt;/p&gt;
    &lt;p&gt;This is called “the time value of money,” but it’s very simple: money today is worth more than money tomorrow.&lt;/p&gt;
    &lt;p&gt;What this means is that earning $10 today is better than earning $10 next year, and even better still than earning $10 in two years. If inflation was 20%, $10 in future value next year would be equivalent to $8.33 in present value today. $10 in future value two years from now would be equivalent to $6.94 today. And so forth.&lt;/p&gt;
    &lt;p&gt;Of course, inflation isn’t 20%, thank goodness. But when your company makes an investment, they expect a certain return on that investment. The return they expect is called “cost of capital.” Your leadership team will tell you the cost of capital to use. It’s based on their judgment of how much they could get from using the money on other investments along with an adjustment for risk. For these examples, I’m arbitrarily choosing a 20% cost of capital.&lt;/p&gt;
    &lt;p&gt;The neat thing about cost of capital is that you can wager your entire present value and still get a good return on investment. As long as the bet is successful, even if you spend all of the present value, you’re still making money.&lt;/p&gt;
    &lt;p&gt;If you ask me for an investment and promise to return $10 to me today, $10 next year, and so on for the next three years, you’ll return $40 total. If my cost of capital is 20%, then I can look at the present value of each of those returns. It’s $10 today, $8.33 next year, $6.94 the following year, and so forth. Adding up those future returns gives me the total present value, which is $31.06, which means that I can invest up to $31.06 and still get at least a 20% return on my investment.&lt;/p&gt;
    &lt;p&gt;Okay, so that’s what present value is. Now, how do we determine what numbers to use?&lt;/p&gt;
    &lt;p&gt;As I said before—spreadsheets and guesses. You build a financial model that makes guesses about the future.&lt;/p&gt;
    &lt;p&gt;I’m going to share the model I used, but I have to be honest: I had a lot of trouble getting my leadership team to engage with product bets at first. In order to get this off the ground, I had to provide the financial model myself... and honestly, I think it could be a lot better.&lt;/p&gt;
    &lt;p&gt;We have a new CFO at OpenSesame, so I showed him about the model I’m about to show you. He said—this is a direct quote—“it’s an okay framework to start.” He also said, “come talk to me early when you start on the next set of bets.”&lt;/p&gt;
    &lt;p&gt;So, yeah. Thank you for coming to my okay talk. I’m sure it will be better next year.&lt;/p&gt;
    &lt;p&gt;In all seriousness, our CFO liked the general idea of product bets, and the categories I was using. He just thinks he can make the specifics more rigorous, which is great, and I’m looking forward to his help.&lt;/p&gt;
    &lt;p&gt;The fact is, it doesn’t really matter if the model is accurate or not. The important thing is to get people to engage with value rather than cost and dates being the primary driver of decision-making. You can use a rough, back-of-the-envelope model to get started. That’s what I did. As long as you’re consistent with your approach across bets, it’s still useful.&lt;/p&gt;
    &lt;p&gt;With that said, our product bets are broken down into five sections. Each one has its own little present value calculation.&lt;/p&gt;
    &lt;p&gt;There’s Sales, which represents the money we make from new customers as a result of the bet.&lt;/p&gt;
    &lt;p&gt;Upsell, which is the money we make from existing customers as a result of the bet.&lt;/p&gt;
    &lt;p&gt;Retention, which has to do with the fact that we sell subscriptions. Once we make a sale, we keep making money from that customer every year, so long as we can retain them. This is typical in the modern software-as-a-service world. So retention is a very important number.&lt;/p&gt;
    &lt;p&gt;Cost savings is reduction in spending, which counts as value, because spending $5 less on candy each year means I have $5 more in my pocket.&lt;/p&gt;
    &lt;p&gt;And then expenditures, which is additional spending we’ll incur as a consequence of the bet. For example, maybe I spend $5 less on candy each year, but I have to spend $1 every year on a budget tracking app that reminds me not to waste money on candy.&lt;/p&gt;
    &lt;p&gt;To illustrate these ideas, let me introduce you to my new employer: War Elephants as a Service.&lt;/p&gt;
    &lt;p&gt;We’re your one stop shop for all elephant-related warfare. We take care of the elephants, so you can take care of the invasion! Look at our glowing testimonials from top customers: Carthage... and Rome! Business is good. Or at least, it was. There’s not much demand for war elephants these days.&lt;/p&gt;
    &lt;p&gt;PS: Apologies for the mutant two-trunked elephant in the logo. Our ex-CEO tried to solve our financial problems with cost-cutting, so he replaced all of our graphic designers with AI. His last words as he was escorted out the building were, “I’ve made a terrible mistake.”&lt;/p&gt;
    &lt;p&gt;But we have new CEO now! Babar is our new “Chief Elephant Officer,” and he has an idea for keeping our business relevant in today’s fast-paced world. Since nobody seems to want war elephants any more, we’re going to switch from “war elephants” to “more elephants!” Elephant parades! Elephant-themed merchandise! And especially, cute baby elephants! Nothing says “more elephants” like an adorable fuzzy pachyderm.&lt;/p&gt;
    &lt;p&gt;Specifically, we’re going to open up new markets and improve retention by introducing family-friendly elephant activities. That’s our bet.&lt;/p&gt;
    &lt;p&gt;To quantify this bet, we’re going to look at the five categories I mentioned before: Sales to new customers, upsells to existing customers, retention, cost savings, and expenditures.&lt;/p&gt;
    &lt;p&gt;[points to “Service Obtainable Market” row] For new sales, we’re going to look at the “service obtainable market,” which is the total size of the market that we can reach for family-friendly elephant activities. Let’s say it’s 100 million dollars at the end of the first year, and grows over time as word gets out.&lt;/p&gt;
    &lt;p&gt;[points to “Sales Rate” row] Next, we’re going to estimate how much of that market we can capture. We face competition from zoos, but nobody has quite the expertise deploying large numbers of elephants that we do, so we’re going to say we can sell into 1% of the market, and that will also grow over time.&lt;/p&gt;
    &lt;p&gt;[points to “Future Value” row] Multiplying the service obtainable market by our sales rate of 1% gives us the amount we expect to make each year in future dollars. [points to “Present Value” row] Then we apply our present value formula at a 20% cost of capital and [points to “Total Present Value”] add it all up to get a total present value of nearly $5 million from new sales.&lt;/p&gt;
    &lt;p&gt;It all looks very official, doesn’t it? But how do we know it’s a 100 million dollar market? How do we know we can sell into 1% of it?&lt;/p&gt;
    &lt;p&gt;We don’t! It’s guesses. Educated guesses, maybe, but ultimately... guesses. That’s how these things work, and that’s why you need your leadership team to get involved. You can make your models more and more rigorous, but at the end of the day, somebody’s making their best guess, and those guesses should be overseen by the people in charge of those departments.&lt;/p&gt;
    &lt;p&gt;Next, we look at upsell. How many of our existing customers can we convince to try our new family-friendly elephant activities?&lt;/p&gt;
    &lt;p&gt;[points to “Service Obtainable Market” row] As before, we start with the total market that we can reasonably reach. This is the amount we think that our existing customers would be willing to spend on our new offering. In our case, it turns out our customers aren’t actually using their war elephants for war, but for things like parades. We think there’s a good $25 million to be made from our existing customers, and we don’t expect that to change much over time. To be clear, that’s not what we make from our existing customers, it’s the extra amount we think they’d pay for our new service.&lt;/p&gt;
    &lt;p&gt;[points to “Sales Rate” row] Then we look at our sales rate for that market. Given that our customers are already using their elephants for parades, we think they’re going to be pretty receptive to us providing services to support them. We estimate that we’ll be able to convert 5% of the upsell market, and that number will also grow over time.&lt;/p&gt;
    &lt;p&gt;[points to “Total Present Value”] Multiply the numbers, apply present value formula, and we have the total upsell value of $6.3mm.&lt;/p&gt;
    &lt;p&gt;Now let’s talk about retention. Our retention numbers have been pretty bad—as I said, countries don’t really need war elephants any more. [points to “Service Obtainable Market” row] But we still have a hundreds of millions of recurring revenue, even though it’s going down each year. That’s the ARR line—annual recurring revenue.&lt;/p&gt;
    &lt;p&gt;[points to “Retention Change” row] By pivoting from a focus on war to a focus on the military parades our clients are actually using elephants for, we think we can stem the bleeding a bit. Not much... about a quarter of a percent each year, going up slightly over time.&lt;/p&gt;
    &lt;p&gt;[points to “Total Present Value”] Multiply, present value, and there you have it. Three and a half million.&lt;/p&gt;
    &lt;p&gt;What about cost savings? [points to “Work Eliminated” row] Is this bet going to eliminate any of the existing work our employees do? Not really. [points to “Expenses Eliminated” row] Is it going to eliminate any expensive software subscriptions or other expenses? No, probably not.&lt;/p&gt;
    &lt;p&gt;[points to “Total Present Value”] Normally, we’d add up the cost savings and apply the present value calculation, but the numbers total out to zero in this case.&lt;/p&gt;
    &lt;p&gt;And finally, expenditures. How much more are we going to spend as a result of this bet?&lt;/p&gt;
    &lt;p&gt;Well, there’s the cost of developing the bet itself, which is our wager, but we’ll bring that in later. In this section, we’re looking at the ongoing costs of running the program. [points to “Future Value” row] I’m going to hand-wave that a bit—you might have multiple line items here normally—but let’s just say it’s $2mm per year, going up as the program becomes more popular. Elephants aren’t cheap.&lt;/p&gt;
    &lt;p&gt;[points to “Total Present Value”] Present value, etc., gives us a total of $8.5mm in expenditures.&lt;/p&gt;
    &lt;p&gt;Bringing it all together, we have $5mm in new sales, $6.3mm in upsell, $3.5mm in improved retention, $0 in cost savings, and $8.5mm in expenditures. That comes to a total present value of $6.3mm before our development costs.&lt;/p&gt;
    &lt;p&gt;Now, how much do we want to wager on development? The leadership team thinks this is a slam dunk, and a way to save the business, so they’re going to wager nearly all of the value. Five million dollars. Remember, using cost of capital to determine present value means that we could wager the entire present value and still come out ahead... if the bet is successful.&lt;/p&gt;
    &lt;p&gt;That said, bets still have a risk of failure. Our leadership team is making some assumptions about how much people will be excited about baby elephants, so we’ll want to work incrementally and iteratively to test their assumptions early.&lt;/p&gt;
    &lt;p&gt;To summarize, the present value of the bet is based on sales to new customers, upsell to existing customers, change in retention, cost savings, and non-development expenditures related to those benefits.&lt;/p&gt;
    &lt;p&gt;And that’s how we come up with the numbers in the product bet. To bring it back around, we’re betting that we can open up new markets and improve retention with family-friendly elephant activities. Babar is the sponsor for this bet and he thinks it’s worth $6mm in present value, and he’s willing to spend up to $5mm to try to make it work.&lt;/p&gt;
    &lt;p&gt;To calculate the value of those categories, we took a back-of-the-napkin approach where we estimated the size of the market and our ability to sell into that market. There’s certainly room for more rigor, and I encourage you to talk to your finance team about how to improve the model.&lt;/p&gt;
    &lt;p&gt;But do remember that it’s all still guesses at the end of the day. It’s better to have some model than a perfect model. The real benefit is in shifting the conversation from features and dates to about being accountable for value.&lt;/p&gt;
    &lt;p&gt;We may be a foreign country, but we can still speak our business partners’ language.&lt;/p&gt;
    &lt;p&gt;[beat]&lt;/p&gt;
    &lt;p&gt;But how do we get them to talk to us?&lt;/p&gt;
    &lt;p&gt;A leader I respect once told me, “You have 18-24 months after becoming VP of Engineering to make a difference. After that, the organization’s problems become your problems.”&lt;/p&gt;
    &lt;p&gt;I think he was right on target. As a leader, your colleagues in other departments will reserve judgement for the first six months or so. They’ll get impatient over the course of the next year. By the end of two years, they’ll be holding you accountable. If you don’t define what that looks like, they’ll define it for you, and they’re going to default to features and dates.&lt;/p&gt;
    &lt;p&gt;The problem with product bets, as an idea, is that they require leadership participation. You can’t create these spreadsheets on your own. Even if you did, nobody’s going to pay attention if you don’t have their buy-in. I’ve tried variants of the product bet idea many times over the years and getting that participation has been extraordinarily difficult. I’m a little surprised we’re able to do it at OpenSesame, to be honest.&lt;/p&gt;
    &lt;p&gt;Before you can get people to buy in to your definition of accountability, you need them to trust you. And in order for them to trust you, you need to be accountable.&lt;/p&gt;
    &lt;p&gt;I’m not sure how to solve this chicken-and-egg problem for your organization. I can tell you how I solved it for mine. Any change you introduce has to be in the context of your specific situation, so I’m not saying that you should do it my way. Some of my changes were pretty radical, and they’re not going to be a good idea for every situation.&lt;/p&gt;
    &lt;p&gt;We don’t have time to go into every detail, so this is going to be more of an overview than a how-to guide. I’ll provide resources for further investigation.&lt;/p&gt;
    &lt;p&gt;QR Code: FaST: An Innovative Way to Scale&lt;/p&gt;
    &lt;p&gt;When I joined OpenSesame, I started by getting the lay of the land and deciding what to do. One of the things I saw was that the teams were heavily siloed by technology area, rather than by product line. Cross-team delays weren’t too bad, although they often can be in this situation, but it did mean that teams’ work didn’t line up to our business needs. So the first thing I did was to introduce Quentin Quartel’s Fluid Scaling Technology, or FaST.&lt;/p&gt;
    &lt;p&gt;We don’t have time to discuss FaST today, but you can learn more about my approach to it by following this QR code. The short version is that we combined teams into product-centric “collectives” and created a single queue of work for each collective. Each product has a dedicated collective and work queue. Those collectives self-organize into teams as needed to tackle the highest priority work.&lt;/p&gt;
    &lt;p&gt;FaST solved the problem of teams not matching business needs. A related problem was the teams planned their work in terms of technical priorities rather than business results. They called them “stories,” and “epics,” and recorded them in Jira, but they were more like technical tasks. At the same time that I introduced FaST, I also introduced the idea of “Valuable Increments” from my book. (In case it’s not clear on the slide, my book is The Art of Agile Development, and it’s now available in a second edition. You can find this material in the “Adaptive Planning” section.)&lt;/p&gt;
    &lt;p&gt;A valuable increment is a similar idea to an epic, in that it groups together multiple stories, but an “epic” is literally a “big story.” A valuable increment isn’t focused on size; it’s focused on value. Each VI is something that stands alone. When it’s done, you can release it, and you’ll have gotten value out of it even if you never work on anything related to it ever again.&lt;/p&gt;
    &lt;p&gt;Introducing FaST and VIs allowed me to talk in terms of the business results my teams were creating for each product line, not just their technical accomplishments.&lt;/p&gt;
    &lt;p&gt;I also knew, from experience, that one of my biggest battles was going to be around estimates and forecasting. Before I could gain the trust of the organization, I needed to be able to demonstrate that I could do what I said I would. Up to this point, their experience of software development was that we never delivered on time. At the same time, I didn’t want people to over focus on features and dates.&lt;/p&gt;
    &lt;p&gt;So I played a game that, to this day, I’m not sure was the right approach. I had my engineering managers start collecting data so we could provide more accurate forecasts. While they did that, I told teams to stop providing estimates to stakeholders.&lt;/p&gt;
    &lt;p&gt;This caused a lot of anger in my stakeholders. They didn’t like hearing that they couldn’t have estimates. I told them that our estimates weren’t accurate, and we were working on getting better information, but they still didn’t like it. I think I only got away with it because there had been high-profile failures with the old approach, and I was still in my honeymoon period, but it still caused a lot of friction.&lt;/p&gt;
    &lt;p&gt;It worked out in the end, I think, because the new forecasts really are much more reliable, but I had to collect data for about six months before I could provide the new forecasts. That was an uncomfortable period. I could have kept the old approach to forecasting, but it definitely didn’t work. I’m not sure if “wrong estimates” would have been better than “no estimates.” On the one hand, a clean break meant that it was obvious that I had switched to a new approach, and—as I said—it really works. On the other hand, I made some important members of the leadership team angry in the meantime.&lt;/p&gt;
    &lt;p&gt;Anyway, the way it works is that we get a “wisdom of the crowd” estimate for each VI before works starts. That involves a product manager providing a very brief description of what the VI involves—just a minute or two of verbal explanation. People can ask clarifying questions, but there usually aren’t many. Then everyone provides their gut feel of how long the work will take a team to accomplish, in weeks. We collect the answers without discussing them and record the median response. That’s the estimate. It only takes a few minutes per VI. Since our collectives have between 12 and 25 people, including managers, product managers, and designers, there’s enough people to make the “crowd” part of “wisdom of the crowd” work.&lt;/p&gt;
    &lt;p&gt;Our Wisdom of the Crowd estimates are stunningly accurate. The median estimate for a VI actually matches the median reality. It’s amazing. The approach comes from Quentin Quartel and his FaST method, and I’ve never seen anything so good. It’s easy and it’s accurate.&lt;/p&gt;
    &lt;p&gt;However, although Wisdom of the Crowd estimates are accurate, in aggregate, they’re not very precise. We graph estimates versus actuals—you can see it on the right there. About 30% of VIs take twice as long as estimated, and about 30% take half as long as estimated. That’s a pretty big range.&lt;/p&gt;
    &lt;p&gt;So we don’t present the raw estimates to stakeholders. If we did, we’d be late half the time. Instead, we increase the estimate so we’re early more often than we’re late.&lt;/p&gt;
    &lt;p&gt;Doing this requires me to play a political balancing act. According to our data, never being late would require us to multiply our estimates by six or seven, and that wouldn’t fly. We can’t tell them that a small, two-week VI is going to take 3-4 months. On the other hand, it’s also not acceptable to be late half the time.&lt;/p&gt;
    &lt;p&gt;Right now, I’ve chosen to be 75% accurate. In other words, we’re early 75% of the time and late 25% of the time. For us, that’s about a 2x multiplier, depending on the team. I’ve also told stakeholders to expect about 1 in 4 VIs to go longer than expected. So far, it’s working well.&lt;/p&gt;
    &lt;p&gt;If you’d like to know more about the analysis behind this technique, it’s in my book in the “Forecasting” section.&lt;/p&gt;
    &lt;p&gt;Collecting all that data for forecasting had a side benefit. My CEO pushed me to report productivity—that’s a whole ’nother story—and I decided to do it by reporting the percentage of time spent on muda versus the percentage of time spent on adding value to the business. Muda is activity that doesn’t add value. It’s the grey sections in the graph: maintenance, bugs, and on call.&lt;/p&gt;
    &lt;p&gt;This isn’t the real graph, for confidentiality reasons, but the story it tells is all too familiar: lots of time spent on deferred maintenance, lots of time spent on incidents, lots of bugs. And then just a fraction of time left over for doing valuable work.&lt;/p&gt;
    &lt;p&gt;I shared the real version of this graph with my leadership team and it was eye opening. All of the sudden, they understood exactly why things took so long, and why they didn’t ever get what they wanted. They had thought we had way more capacity than we actually did.&lt;/p&gt;
    &lt;p&gt;I told them that my responsibility was to reduce muda—the grey part—and make more room for valuable work—the blue part. That was an act of deliberate accountability, and it flipped the script. Yes, people still wanted me to be accountable for making teams deliver feature X on date Y, with all the fighting about deadlines that involves, but even more importantly, and primarily, I was accountable for decreasing muda. That’s precisely what I needed to be focused on, because that was our biggest problem.&lt;/p&gt;
    &lt;p&gt;And, over the past two years, that’s exactly what I’ve done. I report on my progress every quarter, and every quarter it’s a little bit better than it was before. And every quarter, I get a little bit less pushback on predicting dates.&lt;/p&gt;
    &lt;p&gt;And then, finally, I just kept pushing. These two books are excellent resources on how to do so.&lt;/p&gt;
    &lt;p&gt;I introduced the original variant of the product bet idea in January 2024, or maybe even earlier. It didn’t go anywhere. I brought it up again in March 2024. We sort of tried it, without leadership buy-in, and it sort of fizzled. I brought it up again, and again. I worked with my colleague, the VP of Product. I talked to the Chief Product Officer. I included it in a presentation to leadership about how Agile works. I piggybacked on the CEO’s passion for quantifying results. I stopped asking Leadership to create financial models and just created my own, then asked them to fill in the values. (That’s why they’re not very rigorous.)&lt;/p&gt;
    &lt;p&gt;And then finally, in March of 2025, the stars aligned. The CPO started pushing the rest of the leadership team to get involved. We created five product bets, the leadership team filled in my spreadsheet, and we started working on the first bet. And now we’re off to the races. We just started our second bet a few months ago, and we’re talking about how to increase capacity for more bets.&lt;/p&gt;
    &lt;p&gt;There’s lot more to do, and lots more to learn, but now that the logjam has broken, I think it’s going to stick. Our new CFO is intrigued and I’m able to show steady progress with my VIs and forecasting techniques. I’m well on my way to erasing the stigma that engineering can’t be trusted to deliver. I had 18-24 months to make a difference. I’ve just passed my 2nd year at OpenSesame, and I’m still here. I think it’s going to work out.&lt;/p&gt;
    &lt;p&gt;Software development may be a foreign country to the rest of the business, but we can still be a trusted part of their empire.&lt;/p&gt;
    &lt;p&gt;To do so, we have to take accountability, rather than allowing it to be forced upon us. Rather than falling into the habit of delivering X features on Y date, we can be accountable for what really matters: results, just like our colleagues in sales, marketing, and other parts of the business. And the results we create are new opportunities. Enabling more prospects. New partners. More leads. Better retention.&lt;/p&gt;
    &lt;p&gt;Product bets allow us to be accountable for the estimated value of those results. So far, they’ve been working for me. I hope they work for you, too.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jamesshore.com/v2/blog/2025/the-accountability-problem"/><published>2025-10-19T02:22:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45633081</id><title>The case for the return of fine-tuning</title><updated>2025-10-19T14:34:43.332311+00:00</updated><content>&lt;doc fingerprint="3c49999899305e5a"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Déjà Tune&lt;/head&gt;
    &lt;p&gt;Most of my reading this week focused on fine-tuning, sparked by Thinking Machines Labs’ announcement of Tinker. The six-month-old, already $12B-valued startup founded by OpenAI’s former CTO Mira Murati wants to bring fine-tuning back into the spotlight with a new fine-tuning-as-a-platform initiative positioned as a foundation for research collaborations with universities.&lt;/p&gt;
    &lt;p&gt;A few days later, Clément Delangue from Hugging Face posted that he sensed a paradigm shift toward self-managed, open-source, and specialized LLM deployments, even backed by dedicated hardware like NVIDIA’s DGX Spark, many conversations with founders about growing client demand, or the Personal AI Workstation, a very clever marketing stunt from a16z (I’m jealous).&lt;/p&gt;
    &lt;p&gt;All of this feels like a déjà vu. For a brief moment after the first wave of large language models, fine-tuning was the hottest topic in machine learning. Then, just as quickly, it disappeared from most production systems, now accounting for less than 10% of AI inference workloads.&lt;/p&gt;
    &lt;p&gt;So, how did fine-tuning get sidelined so fast, and why do we feel it could be time for a come-back? And more importantly, what could be different this time?&lt;/p&gt;
    &lt;head rend="h2"&gt;Attention, Please&lt;/head&gt;
    &lt;p&gt;Before the Transformer breakthrough, the spark that led to the LLMs we use today, NLP relied on specialized models. Early progress came from recurrent architectures like RNNs and LSTMs, which for the first time learned directly from word sequences instead of relying on hand‑crafted linguistic features. A step forward in representations, but without any learning paradigm that would define later foundation models. Each application required to start from scratch on task-specific data.&lt;/p&gt;
    &lt;p&gt;In 2017, Google’s Attention Is All You Need introduced the Transformer architecture, replacing recurrence and convolution with self‑attention alone. Seven months later, ULMFiT demonstrated that a pretrained language model (at the time still based on LSTMs) could be fine‑tuned for different tasks, and helped establish the methodological foundation that made Transformers practically useful. And a year later, models like BERT and GPT‑1 applied the design in practice.&lt;/p&gt;
    &lt;p&gt;BERT leveraged the encoder side with bidirectional attention for understanding, while GPT used the decoder side with unilateral attention for generation, beautifully illustrated in The Illustrated BERT, ELMo, and Co and The Illustrated GPT-2, two readings recommended by Cédric Deltheil (Finegrain).&lt;/p&gt;
    &lt;p&gt;BERT in particular reshaped the culture of NLP: instead of building every model from scratch, researchers could “just fine-tune” a pretrained Transformer and achieve results that once required months of manual feature engineering.&lt;/p&gt;
    &lt;p&gt;Everything was fine until LLM madness turned into a full-blown parameter explosion, with models jumping from millions to hundreds of billions of parameters (and then some). Fine-tuning was no longer a smart idea. What had once been a few GPU-hours task quickly became a massive industrial operation and raised serious deployment challenges. This practice, called Full Fine-Tuning (FFT), meant retraining every layer and every weight. It offered precision but at a brutal cost.&lt;/p&gt;
    &lt;p&gt;Then, in 2021, Microsoft Research introduced LoRA (Low-Rank Adaptation of Large Language Models). Instead of retraining billions of parameters, LoRA freezes the original weights and adds small low-rank matrices to selected layers. Only these are trained, cutting costs by an order of magnitude while maintaining (and sometimes even improving) FFT performance. Unsurprisingly, LoRA became the default. And by 2024, thanks to Hugging Face’s PEFT library, implementing it was a one-line command.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding The Right Tune&lt;/head&gt;
    &lt;p&gt;A surface-level simplicity masking an ugly truth: fine-tuning is more than a package to deploy and maintain. The tuning itself is where the real magic happens, and it is never a one-config-fits-all process. Generally speaking, this hyperparameter tuning alone can make or break a model. The critical process feels more like alchemy than science, balancing ranks, learning rates, and alpha ratios while hoping your adapters don’t overfit or forget what the model already knows (a phenomenon called catastrophic forgetting). And when you finally get something that works, evaluating it feels less like validation and more like divination.&lt;/p&gt;
    &lt;p&gt;Meanwhile, LLMs kept getting better at nearly every task, approaching something close to omniscience. Writing a leasing contract? A whole chapter on the Industrial Economy in Rural Britain in the nineteenth century? A movie script? A REST API or a website for your mom? By 2023, most teams realized they could achieve about 90% of fine-tuned performance through prompt engineering, thanks to larger context windows, or RAG (Retrieval-Augmented Generation, which gives the model access to an external knowledge base). Both approaches required no retraining and came with far less operational burden for quite decent results.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tuning Back In&lt;/head&gt;
    &lt;p&gt;But why does fine-tuning seem to be gaining fans again? Maybe because the very factors that once made it irrelevant or inefficient are now, piece by piece, being solved.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;GPU-as-a-service platforms such as Together.ai allow you to spin up a LoRA fine-tuning pipeline with minimal friction and in many cases in minutes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;While new models still come fast, the changes are now more evolutionary than revolutionary. That shift means tuning a model today is less likely to be totally invalidated tomorrow.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Open-weight ecosystems like Mistral, Llama, Falcon, Yi, and Gemma offer a lot of alternatives for organizations to own, inspect, and persist their fine-tuned variants without vendor lock-in.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, companies may have reached the ceiling of what can be achieved with prompting alone. Some want models that know their vocabulary, their tone, their taxonomy, and their compliance rules.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For all those reasons, fine-tuning is slowly dipping its toes back into the spotlight, not as a trendy feature this time, but as a strategic lever for control, differentiation, and embedded intelligence.&lt;/p&gt;
    &lt;p&gt;Thinking Machines Lab’s Tinker, which focuses on theorem proving, chemistry reasoning, multi-agent reinforcement learning, and AI safety, embodies this shift. And they have already made several recommendations shared in their blog post LoRA Without Regret, which explores how to fine-tune more effectively.&lt;/p&gt;
    &lt;p&gt;They recommend applying LoRA to all linear modules, not just attention layers as in the original paper. They also emphasize the importance of LoRA rank, a hyperparameter often overlooked, and advise setting higher learning rates (at least 10x), smaller batch sizes (the opposite of common practice), and defining reward functions explicitly with mathematical or logical verification. All these recommendations are clearly explained and reproducible on Hugging Face’s TRL.&lt;/p&gt;
    &lt;p&gt;This is great for tuning, but maybe Tinker’s major contribution to fine-tuning is elsewhere.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Best of Both Worlds&lt;/head&gt;
    &lt;p&gt;As expected, the modern fine-tuning pipeline looks nothing like what we used five years ago. It is modular, serverless, and orchestrated. A single deployment can run a base model alongside dozens of LoRA adapters, each representing a specific tone, function, or domain. During inference, the system routes queries to the right combination of adapters instead of relying on a static model file.&lt;/p&gt;
    &lt;p&gt;Such modularity introduces its own challenges. All-in-one platforms like Together.ai handle most of the heavy lifting, but they often lack the granularity many teams need for fine-grained configuration and observability, and costs at scale can escalate quickly.&lt;/p&gt;
    &lt;p&gt;On paper, Tinker seems to offer the best of both worlds: the comfort of a modern, fully managed fine-tuning stack combined with fine-grained control for researchers. It provides direct API access to low-level training primitives, allowing users to orchestrate training workflows and custom algorithms at the deepest level while taking care of the grunt work. Hopefully this will inspire other platforms, since Tinker is currently reserved for research purposes.&lt;/p&gt;
    &lt;p&gt;Anyway, as imperfect as they are, infrastructure challenges are becoming problems of the past. Yet one major difficulty remains: evaluation.&lt;/p&gt;
    &lt;p&gt;Models are notoriously difficult to evaluate. Human evaluation is inconsistent, slow and more importantly expensive, while benchmarks age quickly and lose relevance with data contamination. Even automated approaches such as G-Eval or Chatbot Arena introduce their own problems, often amplifying bias and producing unstable scores.&lt;/p&gt;
    &lt;p&gt;Following the announcement of Tinker, Benjamin Anderson suggested they might have part of the solution. As he wrote in “Anatomy of a Modern Fine-Tuning API” on his blog, “It gives the user the power to do online reinforcement learning: take a completion from the current model weights, score that completion, and update the model based on whether that completion was good or bad. Supervised fine-tuning teaches the model to mimic pre-written responses, while online RL improves it by scoring its own responses.” Therefore, with such architectures, the future of fine-tuning may not look like fine-tuning at all: it starts to resemble continuous learning.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Very Different Tune&lt;/head&gt;
    &lt;p&gt;“Theoretically, fine-tuning has always made sense. But the speed at which closed-source labs were scaling their model intelligence made it a practically bad bet. Now, with compute, data, and better frameworks, the scales are tipping back toward specialization.” said Robert Hommes from Moyai.ai, a company specialized in adaptive fine-tuning.&lt;/p&gt;
    &lt;p&gt;And regarding the shift to self-hosting, this could go even closer to you than you might think. As Constant Razel from Exxa put it, “Personal AI computers are no longer a distant idea. Technology is improving and becoming more accessible. Security and cost will likely drive early adoption, while fine-tuning will enable specialized, high-performance agents to run on them.”&lt;/p&gt;
    &lt;p&gt;From a brute-force chase for marginal accuracy to a framework for ownership, alignment, and continuous improvement rooted in proximity and control, fine-tuning might have found a new territory to thrive. It is no longer just a technical step but perhaps a strategic layer in how intelligence will be built and owned.&lt;/p&gt;
    &lt;p&gt;🙌 Thanks to Robert Hommes, Benjamin Trom, Etienne Balit, Constant Razel, Cédric Deltheil, and Willy Braun for the insights, suggestions, and proofreading.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://welovesota.com/article/the-case-for-the-return-of-fine-tuning"/><published>2025-10-19T09:41:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45633453</id><title>Show HN: Duck-UI – Browser-Based SQL IDE for DuckDB</title><updated>2025-10-19T14:34:42.811631+00:00</updated><link href="https://demo.duckui.com"/><published>2025-10-19T11:19:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45633482</id><title>OpenAI researcher announced GPT-5 math breakthrough that never happened</title><updated>2025-10-19T14:34:42.021743+00:00</updated><content>&lt;doc fingerprint="ef1f031d38a1c6ae"&gt;
  &lt;main&gt;
    &lt;p&gt;OpenAI researchers recently claimed a major math breakthrough on X, but quickly walked it back after criticism from the community, including Deepmind CEO Demis Hassabis, who called out the sloppy communication.&lt;/p&gt;
    &lt;p&gt;It started with a now-deleted tweet from OpenAI manager Kevin Weil, who wrote that GPT-5 had "found solutions to 10 (!) previously unsolved Erdős problems" and made progress on eleven more. He described these problems as "open for decades." Other OpenAI researchers echoed the claim.&lt;/p&gt;
    &lt;p&gt;The wording made it sound like GPT-5 had independently produced mathematical proofs for tough number theory questions - a potential scientific breakthrough and a sign that generative AI could uncover unknown solutions, showing its ability to drive novel research and open the door to major advances.&lt;/p&gt;
    &lt;p&gt;Mathematician Thomas Bloom, who runs erdosproblems.com, pushed back right away. He called the statements "a dramatic misinterpretation," clarifying that "open" on his site just means he personally doesn't know the solution - not that the problem is actually unsolved. GPT-5 had only surfaced existing research that Bloom had missed.&lt;/p&gt;
    &lt;p&gt;Deepmind-CEO Demis Hassabis called the episode "embarrassing", and Meta AI chief Yann LeCun pointed out that OpenAI had basically bought into its own hype ("Hoisted by their own GPTards").&lt;/p&gt;
    &lt;p&gt;The original tweets were mostly deleted, and the researchers admitted their mistake. Still, the incident adds to the perception that OpenAI is an organization under pressure and careless in its approach. It raises questions about why leading AI researchers would share such dramatic claims without verifying the facts, especially in a field already awash in hype, with billions at stake. Bubeck knew what GPT-5 actually contributed, but still used the ambiguous phrase "found solutions."&lt;/p&gt;
    &lt;head rend="h2"&gt;GPT-5 is proving useful as a literature review assistant&lt;/head&gt;
    &lt;p&gt;The real story here is getting overshadowed: GPT-5 actually proved useful as a research tool for tracking down relevant academic papers. This is especially valuable for problems where the literature is scattered or the terminology isn't consistent.&lt;/p&gt;
    &lt;p&gt;Mathematician Terence Tao sees this as the most immediate potential for AI in math—not solving the toughest open problems, but speeding up tedious tasks like literature searches. While there have been some "isolated examples of progress" on difficult questions, Tao says AI is most valuable as a time-saving assistant. He has also said that generative AI could help "industrialize" mathematics and accelerate progress in the field. Still, human expertise is crucial for reviewing, classifying, and safely integrating AI-generated results into real research.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://the-decoder.com/leading-openai-researcher-announced-a-gpt-5-math-breakthrough-that-never-happened/"/><published>2025-10-19T11:30:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45633591</id><title>Pebble is officially back on iOS and Android</title><updated>2025-10-19T14:34:41.584831+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/ericmigi/status/1979576965494710564"/><published>2025-10-19T12:00:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45633642</id><title>Improving PixelMelt's Kindle Web Deobfuscator</title><updated>2025-10-19T14:34:40.896430+00:00</updated><content>&lt;doc fingerprint="1fe48f1367a701e8"&gt;
  &lt;main&gt;
    &lt;p&gt;A few days ago, someone called PixelMelt published a way for Amazon's customers to download their purchased books without DRM. Well… sort of.&lt;/p&gt;
    &lt;p&gt;In their post "How I Reversed Amazon's Kindle Web Obfuscation Because Their App Sucked" they describe the process of spoofing a web browser, downloading a bunch of JSON files, reconstructing the obfuscated SVGs used to draw individual letters, and running OCR on them to extract text.&lt;/p&gt;
    &lt;p&gt;There were a few problems with this approach.&lt;/p&gt;
    &lt;p&gt;Firstly, the downloader was hard-coded to only work with the .com site. That fix was simple - do a search and replace on &lt;code&gt;amazon.com&lt;/code&gt; with &lt;code&gt;amazon.co.uk&lt;/code&gt;. Easy!&lt;/p&gt;
    &lt;p&gt;But the harder problem was with the OCR. The code was designed to visually centre each extracted glyph. That gives a nice amount of whitespace around the character which makes it easier for OCR to run. The only problem is that some characters are ambiguous when centred:&lt;/p&gt;
    &lt;p&gt;When I ran the code, lots of full-stops became midpoints, commas became apostrophes, and various other characters went a bit wonky.&lt;/p&gt;
    &lt;p&gt;That made the output rather hard to read. This was compounded by the way line-breaks were treated. Modern eBooks are designed to be reflowable - no matter the size of your screen, lines should only break on a new paragraph. This had forced linebreaks at the end of every displayed line - rather than at the end of a paragraph.&lt;/p&gt;
    &lt;p&gt;So I decided to fix it.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New Approach&lt;/head&gt;
    &lt;p&gt;I decided that OCRing an entire page would yield better results than single characters. I was (mostly) right. Here's what a typical page looks like after de-obfuscation and reconstruction:&lt;/p&gt;
    &lt;p&gt;As you can see - the typesetting is good for the body text, but skew-whiff for the title. Bold and italics are preserved. There are no links or images.&lt;/p&gt;
    &lt;p&gt;Here's how I did it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extract the characters&lt;/head&gt;
    &lt;p&gt;As in the original code, I took the SVG path of the character and rendered it as a monochrome PNG. Rather than centring the glyph, I used the height and width provided in the &lt;code&gt;glyphs.json&lt;/code&gt; file. That gave me a directory full of individual letters, numbers, punctuation marks, and ligatures. These were named by fontKey (bold, italic, normal, etc).&lt;/p&gt;
    &lt;head rend="h3"&gt;Create a blank page&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;page_data_0_4.json&lt;/code&gt; has a width and height of the page. I created a white PNG with the same dimensions. The individual characters could then be placed on that.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resize the characters&lt;/head&gt;
    &lt;p&gt;In the &lt;code&gt;page_data_0_4.json&lt;/code&gt; each run of text has a fontKey - which allows the correct glyph to be selected. There's also a &lt;code&gt;fontSize&lt;/code&gt; parameter. Most text seems to be (the ludicrously precise) &lt;code&gt;19.800001&lt;/code&gt;. If a font had a different size, I temporarily scaled the glyph in proportion to 19.8.&lt;/p&gt;
    &lt;p&gt;Each glyph has an associated &lt;code&gt;xPosition&lt;/code&gt;, along with a &lt;code&gt;transform&lt;/code&gt; which gives X and Y offsets.  That allows for indenting and other text layouts.&lt;/p&gt;
    &lt;p&gt;The characters were then pasted on to the blank page.&lt;/p&gt;
    &lt;p&gt;Once every character from that page had been extracted, resized, and placed - the page was saved as a monochrome PNG.&lt;/p&gt;
    &lt;head rend="h3"&gt;OCR the page&lt;/head&gt;
    &lt;p&gt;Tesseract 5 is a fast, modern, and reasonably accurate OCR engine for Linux.&lt;/p&gt;
    &lt;p&gt;Running &lt;code&gt;tesseract page_0022.png output -l eng&lt;/code&gt; produced a .txt file with all the text extracted.&lt;/p&gt;
    &lt;p&gt;For a more useful HTML style layout, the hOCR output can be used: &lt;code&gt;tesseract page_0022.png output -l eng hocr&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Or, a PDF with embedded text: &lt;code&gt;tesseract page_0022.png output -l eng pdf&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h3"&gt;Mistakes&lt;/head&gt;
    &lt;p&gt;OCR isn't infallible. Even with a high resolution image and a clear font, there were some errors.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Superscript numerals for footnotes were often missing from the OCR.&lt;/item&gt;
      &lt;item&gt;Words can run together even if they are well spaced.&lt;/item&gt;
      &lt;item&gt;Tesseract can recognise bold and italic characters - but it outputs everything as plain text.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What's missing?&lt;/head&gt;
    &lt;p&gt;Images aren't downloaded. I took a brief look and, while there are links to them in the metadata, they're downloaded as encrypted blobs. I'm not clever enough to do anything with them.&lt;/p&gt;
    &lt;p&gt;The OCR can't pick out semantic meaning. Chapter headings and footnotes are rendered the same way as text.&lt;/p&gt;
    &lt;p&gt;Layout is flat. The image of the page might have an indent, but the outputted text won't.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next?&lt;/head&gt;
    &lt;p&gt;This is very far from perfect. It can give you a visually similar layout to a book you have purchased from Amazon. But it won't be reflowable.&lt;/p&gt;
    &lt;p&gt;The text will be reasonably accurate. But there will be plenty of mistakes.&lt;/p&gt;
    &lt;p&gt;You can get an HTML layout with hOCR. But it will be missing formatting and links.&lt;/p&gt;
    &lt;p&gt;Processing all the JSON files and OCRing all the images is relatively quick. But tweaking and assembling is still fairly manual.&lt;/p&gt;
    &lt;p&gt;There's nothing particularly clever about what I've done. The original code didn't come with an open source software licence, so I am unable to share my changes - but any moderately competent programmer could recreate this.&lt;/p&gt;
    &lt;p&gt;Personally, I've just stopped buying books from Amazon. I find that Kobo is often cheaper and their DRM is easy to bypass. But if you have many books trapped in Amazon - or a book is only published there - this is a barely adequate way to liberate it for your personal use.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://shkspr.mobi/blog/2025/10/improving-pixelmelts-kindle-web-deobfuscator/"/><published>2025-10-19T12:11:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45633711</id><title>A Tower on Billionaires' Row Is Full of Cracks. Who's to Blame?</title><updated>2025-10-19T14:34:40.731430+00:00</updated><content/><link href="https://www.nytimes.com/2025/10/19/nyregion/432-park-avenue-condo-tower.html"/><published>2025-10-19T12:36:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45633958</id><title>How to Assemble an Electric Heating Element from Scratch</title><updated>2025-10-19T14:34:39.795513+00:00</updated><content>&lt;doc fingerprint="3ef270750b9f1318"&gt;
  &lt;main&gt;
    &lt;p&gt;This manual documents the building of an electric resistance heating element that is directly connected to a solar panel, without a battery, charge controller, or voltage regulator in between. The heating element is used in the insulated solar electric cooker that we describe in another manual, and in the solar-powered coffee maker and footstove that we will document in forthcoming manuals. We also describe a method to make a removable heat brick, which we use to replace the commercial heating elements in some earlier electric solar cooker prototypes we made.&lt;/p&gt;
    &lt;p&gt;A custom-made electric resistance consists of an electric circuit made of nichrome wire, enclosed in a mortar layer. The length and thickness of the nichrome wire determine its current draw at a certain voltage, meaning that you dimension the circuit to your solar panel voltage and power rating to optimize heat generation. The nichrome circuit is connected to the electric cables of the solar panel, with a short section of heat-resistant electric cable in between. 1&lt;/p&gt;
    &lt;head rend="h2"&gt;Why build an electric resistance heating from scratch?&lt;/head&gt;
    &lt;p&gt;We initially used commercial heating elements in our first solar oven prototypes, which yielded disappointing results. Therefore, we decided to build our own, based on the manual provided by the Living Energy Farm. Building your own heating element involves extra work, but it’s worth the effort. It’s also a lot cheaper.&lt;/p&gt;
    &lt;p&gt;Many commercial heating elements have built-in thermostats, which can complicate temperature regulation inside the oven. They also require a voltage input that does not align with the voltage output of most solar panels, which introduces the need for an extra electronic component (a buck converter). Securely fixing commercial heating elements proved to be difficult as well, and we had trouble keeping moisture away from the electrical system, which at one point resulted in an electrical fire. By embedding a self-made heating element in a mortar base, we solved all these problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is electric resistance heating?&lt;/head&gt;
    &lt;p&gt;Electric resistance refers to the difficulty that the flow of electric current encounters when it passes through a material. It’s comparable to friction in mechanical systems. Resistance creates heat, as described by Joule’s Law. Electric resistance is measured in ohms (Ω).&lt;/p&gt;
    &lt;p&gt;The resistance of a piece of wire depends on its material’s resistivity, but also on its length and thickness. Metals have low electrical resistance, meaning that electricity easily flows through them; they are called “conductors”. For example, electric wires are usually made of copper, which has very low electric resistance.&lt;/p&gt;
    &lt;p&gt;In contrast, materials such as plastic, rubber, and ceramics have very high electric resistance, meaning that electricity doesn’t flow easily through them. These materials are known as “insulators”. For example, electric wires are encapsulated in plastic, which makes them safe to touch.&lt;/p&gt;
    &lt;p&gt;Electric heating elements, such as those used in ovens, toasters, and hair dryers, are commonly made of nichrome wire, an alloy of nickel and chromium that has relatively high resistance for a metal. Electrons can pass through, but because they encounter quite some resistance, the nichrome wire dissipates a lot of heat. It glows orange when it heats up.&lt;/p&gt;
    &lt;head rend="h2"&gt;What you need&lt;/head&gt;
    &lt;p&gt;In the components list below, we link to Amazon, using it as a global inventory of components. Feel free—and be encouraged—to buy the components locally, or scavenge them from old appliances. We do not earn anything if you purchase on Amazon.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nichrome wire. Other example. Nichrome wire is sold in either bobbins or spools. You can also scavenge it from old ovens, toasters, hair driers, and other electric heating devices.&lt;/item&gt;
      &lt;item&gt;Heat-resistant electric cable. These electric wires are encapuslated in silicone mesh rather than plastic.&lt;/item&gt;
      &lt;item&gt;Thermal switch (optional).&lt;/item&gt;
      &lt;item&gt;Thermal fuse (optional).&lt;/item&gt;
      &lt;item&gt;Construction mortar for encapsulating the nichrome circuit.&lt;/item&gt;
      &lt;item&gt;Thick tiles (in case you build a removable heat brick).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Calculate the resistance value&lt;/head&gt;
    &lt;p&gt;The challenge in building an electric resistance heating element is determining the correct length of the nichrome circuit to match the voltage and current rating of the power source.&lt;/p&gt;
    &lt;p&gt;To determine the length of the nichrome circuit, you need to calculate the desired resistance value that corresponds to your power source. You can calculate it using Ohm’s law, which defines the relation between voltage (Volts, V), current (Ampere, A), and resistance (Ohm, Ω):&lt;/p&gt;
    &lt;p&gt;Resistance (Ω) = U (V) / I (A)&lt;/p&gt;
    &lt;p&gt;To determine the voltage and current values of your solar panel, refer to the label attached to the back of the panel.&lt;/p&gt;
    &lt;p&gt;For the voltage, check the “Maximum Power Voltage (Vmax)” or “Voltage at Pmax”. That refers to the maximum voltage that a solar panel can provide when connected to an electric circuit. Ignore the “Voltage Open Circuit (VOC)”, which is the maximum voltage the solar panel produces if nothing is attached to it.&lt;/p&gt;
    &lt;p&gt;For a so-called 12V solar panel (so-called because it’s typically used in conjunction with a 12V battery and solar charge controller), the Vmax is approximately 18V. For a so-called 24V solar panel (meant to be used in combination with a 24V battery and charge controller), it’s around 36V.&lt;/p&gt;
    &lt;p&gt;For the current, check the “Maximum Power Current (IMP)” or “Current at Pmax”. Ignore the “Short Circuit Current”. If the label is missing, measure the voltage with a multimeter. You can calculate the current once you know the voltage and power output: electric current equals the power output (100W in our case) divided by the voltage (18V in our case). The maximum current that our 100W solar panel can produce is therefore 5.55 A.&lt;/p&gt;
    &lt;p&gt;Once you know the voltage and current of your solar panel, you can calculate the desired resistance value for the heating element using Ohm’s Law. In our case:&lt;/p&gt;
    &lt;p&gt;18 (V) / 5.55 (A) = 3.24 Ω&lt;/p&gt;
    &lt;head rend="h2"&gt;Calculate the length of the heating wire&lt;/head&gt;
    &lt;p&gt;The next step is to cut a piece of nichrome wire that has a resistance of 3.24 Ω. Nichrome wire is sold in various thicknesses, each with a different resistance value. The thinner (and longer) a resistive wire is, the higher its resistance will be. The resistance of a nichrome wire is indicated in ohms per distance (for example, Ω/m).&lt;/p&gt;
    &lt;p&gt;We purchased a relatively thin Nichrome wire with a rated resistance of 8.71 Ω/m. Following the mathematical Rule of Three, based on the resistance per meter, we find that our nichrome circuit needs to be 37.2 cm long to have a resistance value of 3.24 Ω: (100 * 3.24) / 8.71 = 37.2 cm. If you start with a different thickness of Nichrome wire (anything goes), you will obtain a different length.&lt;/p&gt;
    &lt;head rend="h2"&gt;Don’t trust the labeling&lt;/head&gt;
    &lt;p&gt;Unfortunately, the resistance value on the nichrome wire packaging isn’t always exact. To obtain a more accurate measurement, cut precisely one metre of nichrome wire and connect it to the solar panel (or to an 18V test station - see further below) with a watt-meter or multimeter in between. Follow the same method when you use scavenged nichrome wire from an appliance.&lt;/p&gt;
    &lt;p&gt;Connect one end of the wire to the positive output of the solar panel or test station, and the other to the negative output, forming an electric circuit. The polarity doesn’t matter.&lt;/p&gt;
    &lt;p&gt;Turn the power on, read the amperage and wattage values on your watt meter, and turn it off immediately afterward. Be careful when connecting the wire; make sure it doesn’t touch itself, as this would create a shorter circuit for the electricity. Your measurement will be inaccurate, but it will also draw a lot more current (A) and heat much faster, which can be dangerous. Make sure you don’t touch it either because it gets very hot.&lt;/p&gt;
    &lt;p&gt;Doing this, we measured 31W at 1.76A and 18V. Based on Ohm’s Law, we calculated that 18 V / 1.76 A = 10.2 Ω. Consequently, our wire has a resistance of 10.2 Ω/m rather than 8.71 Ω/m. That means that it should have a length of 31.7 cm to have a resistance value of 3.24 Ω:&lt;/p&gt;
    &lt;p&gt;(100 * 3.24) / 10.2 = 31.7 cm.&lt;/p&gt;
    &lt;head rend="h2"&gt;Doubling or tripling the cable&lt;/head&gt;
    &lt;p&gt;However, it’s still too early to cut the nichrome wire to size. Depending on the wire’s resistive value that you are starting with, the length that results from your calculation may not be the most practical length for spreading the heat evenly across the surface of your heating or cooking appliance.&lt;/p&gt;
    &lt;p&gt;For example, the bottom part of our solar oven chamber, right above the electric resistance heating element, measures 26x33 centimeters. With a circuit less than 32 cm long, it’s impossible to heat the oven chamber evenly. A short wire would also create a very warm spot in the mortar and damage it.&lt;/p&gt;
    &lt;p&gt;This can be solved by connecting two or more nichrome wires in parallel. If you double the circuit, each wire should be twice as long (63,4 cm each in our case) to keep the same resistance value. If you triple the circuit, each wire should be three times as long (95,1 cm each), and so on.&lt;/p&gt;
    &lt;p&gt;This may feel counterintuitive, but the longer a cable is, the higher its resistance becomes: electrons will have more difficulty travelling through it. When you double the nichrome cicuit by creating two parallel wires, the electrons can flow in two circuits simultaneously, which means the resitance is halved. Therefore, to keep the same resistance value of 3.24 ohm, you have to make this double circuit twice as long. The same logic applies to a triple wires, where you have to make the circuit three times as long.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cut the nichrome wire to size&lt;/head&gt;
    &lt;p&gt;Once you have decided on the number of nichrome circuits, cut the wires to size. However, before you do that, add about 4 cm to every wire. You will need this extra length to solder the nichrome wire to the heat-resistant electric cables (see further).&lt;/p&gt;
    &lt;head rend="h2"&gt;Coiling the wire&lt;/head&gt;
    &lt;p&gt;Doubling the circuit, as we did in our solar oven, quadruples the total circuit length. That turns one problem (a too-short cable) into another one (too-long cables). However, it can be solved by coiling the wire, which has an additional advantage: The thin nichrome wire becomes much easier to handle and bend when it’s coiled like a spring. You can do this by wrapping it tightly around a rod-shaped object, such as a pen or a screwdriver. Next, you pull the wire to extend it slightly again.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thermal switch and fuse&lt;/head&gt;
    &lt;p&gt;An electric resistance heating element needs a safety precaution to prevent overheating, which could become a fire hazard or crack the mortar enclosure. If the heating element is connected to a solar panel without a battery, as is the case for our solar oven, you could argue that it already has a safety precaution: the sun sets every evening, cutting off the power source to the heating element.&lt;/p&gt;
    &lt;p&gt;However, if you also want to run the cooking appliance on a battery or with a grid-powered test station, you should add a safety precaution that cuts off the heating element if you forget to turn it off.&lt;/p&gt;
    &lt;p&gt;One way to do that is to add a timer switch. That is a component that controls an electric switch and turns it off after a predetermined time has elapsed. The second approach, which we chose, is to add a thermal switch and a thermal fuse. These components disconnect the circuit when the heating element reaches a certain temperature.&lt;/p&gt;
    &lt;p&gt;The thermal switch cuts off the heating circuit when its temperature reaches the rated temperature, and turns it back on when the temperature drops below a slightly lower value. The thermal fuse is an extra safety measure: it’s a single-use fuse that blows when it reaches its rated temperature. The thermal fuse should have a higher value than the thermal switch. You embed it in the cement layer, and once it blows, it’s impossible to replace without breaking the oven.&lt;/p&gt;
    &lt;p&gt;We selected a switch with a maximum temperature rating of 200°C (392°F) and a fuse with a maximum temperature rating of 240°C (464°F). Note that the temperature measured inside the oven chamber will be lower than the temperature of the electric heating element. For example, our thermal switch turns off the circuit at 200°C when the oven chamber is around 120°C (248°F).&lt;/p&gt;
    &lt;p&gt;You can choose a thermal switch and fuse with a higher temperature. However, we cannot guarantee that the structural materials we used for our oven can withstand higher temperatures than those we use.&lt;/p&gt;
    &lt;p&gt;Connect the thermal switch and the thermal fuse in series (one after the other) between the nichrome circuit and the positive heat-resistant wire (the one that connects to the positive wire of the solar panel). Ensure the fuse and switch are embedded in the mortar to obtain an accurate temperature reading. Both switch and fuse have no polarity, which means you can connect their pins in either direction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solder the nichrome wires to the electric cables&lt;/head&gt;
    &lt;p&gt;Once the nichrome circuit is cut and coiled, you need to connect it to the electric cables from the solar PV panel. However, you cannot simply solder one to the other: the nichrome wires get hot and would burn the plastic casing of the electric cables. To prevent that, you need to install a pair of heat-resistant electric cables in between.&lt;/p&gt;
    &lt;p&gt;First, you solder the nichrome wire to the heat-resistance cable. If you want to add a switch and/or fuse (see above), it should go in between the heat-resistant cable and the nichrome wire. Then, you connect the heat-resistant cables to normal electric cables or directly to the solar panel cables (using any type of connector). You also want to put an on-off switch in the positive wire.&lt;/p&gt;
    &lt;p&gt;In summary, the circuit components should be connected in the following order: positive PV cable, on/off switch, heat-resistant cable, (optional) thermal switch, (optional) thermal fuse, and nichrome circuit.&lt;/p&gt;
    &lt;p&gt;Soldering the nichrome wire to the heat-resistant electric cable is a bit complicated because the nichrome doesn’t stick with tin solder. However, you can get around that problem. Start by applying tin to your stripped heat-resistant electric cable strand (fig2.). Then, coil a few centimeters of the nichrome wire around the cable ends (these are the extra centimeters you added before cutting the nichrome wire to size) (fig 3.). Next, apply a generous amount of tin on top of the twisted wire to trap it onto the cable (fig4.).&lt;/p&gt;
    &lt;p&gt;Electric cables come in different thicknesses, measured in mm² in Europe or AWG in the US. The higher the current that flows through it, the thicker an electric cable needs to be. Our circuit works at 5.555A, which requires a 1.5 mm² core wire area. The US equivalent is 16 or 14 AWG. Both the heat-resistant wire and the standard electric cable should follow this size requirement. If you have a different current draw, refer to the chart below to determine the required size. If you plan to use a very long cable between the solar panel and the cooking device, choose a thicker cable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Encapsulating the heating element&lt;/head&gt;
    &lt;p&gt;Once the electric resistance heating element is ready, it needs to be encapsulated in mortar, a heat-resistant material with high thermal inertia. We describe two methods for doing that.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Encapsulte the heating element in the device itself&lt;/head&gt;
    &lt;p&gt;The first method involves encapsulating the nichrome circuit within the structure of a specific cooking or heating appliance. That is how our electric solar oven works: the heating element is embedded into a layer of mortar at the bottom of the cooker, between the insulation layer and the oven chamber (where the food goes). See the manual for the construction steps.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Encapsulate the heating element in a removable heat brick&lt;/head&gt;
    &lt;p&gt;The second method yields a tiled heating brick that can be inserted into various cooking appliances. In this case, the nichrome circuit is embedded in construction mortar and sandwiched between two identical tiles. The two heat-resistant electric cables protrude from one side, ready to be connected to a solar panel. It’s essential to use somewhat thicker and stronger tiles for this purpose, for example, terracotta floor or roof tiles. Thinner tiles may shatter due to the heat.&lt;/p&gt;
    &lt;p&gt;We use these removable heating bricks to power the first two solar oven prototypes that we made. It’s a less energy-efficient method, but if the nichrome circuit breaks, you don’t need to rebuild the entire cooking device.&lt;/p&gt;
    &lt;p&gt;The Living Energy Farm, which inspired the building of our own resistance heating elements, casts the nichrome circuit into a metal shell that they make themselves using sheet metal. However, in contrast to a tiled heating brick, a sheet metal casing requires skills and tools that are not so common. 2&lt;/p&gt;
    &lt;head rend="h2"&gt;Assembly of the heating brick&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;fig 1-2. Place one of the tiles with the back side facing up, apply a dollop of mortar, and flatten it across the tile, almost to the edges.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;fig 3. Place the electric resistance circuit on top of the mortar. Make sure the wires don’t touch or cross, as this would create a short circuit. Try to evenly distribute the wire across the surface to distribute the heat evenly, but avoid the edges to prevent the nichrome wire from sticking out. Leave at least 3-5 cm of heat-resistant electric wire protruding from the tile on one side, so that you can solder or otherwise connect it to a standard electrical cable.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;fig 4-6. Add a little bit of mortar on the other tile and press it on top of the other like a sandwich. Leave it to dry out for at least 48 hours.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Setting up a test station for electric heating resistance heaters&lt;/head&gt;
    &lt;p&gt;A test station is convenient for testing resistance heating elements designed to operate on solar panels. Such a test station consists of a DC power supply and a buck or boost converter. It allows you to simulate the solar panel’s power output using grid power. A test station also serves to measure the precise resistance value of 1m of nichrome wire.&lt;/p&gt;
    &lt;p&gt;A 12V or 24V DC power supply converts 110/220-240V AC power into DC power, comparable to the electricity produced by a solar panel. Choose one with a capacity of at least the power output of your solar panel (100W in our case). If you connect a buck or boost converter to it, you can manipulate the 12V or 24V output voltage into a higher or a lower voltage. Since our heating resistance runs on a solar panel without a battery or charge controller (Vmax = 18V), you can match the buck or boost converter to an output of 18V.&lt;/p&gt;
    &lt;p&gt;To wire it, connect a + and - cable to the DC supply into the buck or boost converter. Use a boost converter to step up the voltage from a DC supply below 18V, or a buck converter to lower he voltage drom a 24V power supply.&lt;/p&gt;
    &lt;p&gt;If you build an electric heating resistance that you want to run on a 12V or 24V battery, you only need the DC power supply (with a voltage output of 12 or 24V, respectively).&lt;/p&gt;
    &lt;p&gt;If you are short on cash, you can use a laptop adapter instead of a DC power supply. The DC output of a laptop adapter is printed on the adapter itself. It’s typically around 70-90W at 19-20V. While it won’t be able to power a 100W solar cooker at full strength, it’s suitable for testing the circuit, and you can obtain it for free. If you have a lot of money, you can also purchase an adjustable lab DC supply, which allows you to adjust the voltage and current outputs using knobs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other types of power sources&lt;/head&gt;
    &lt;p&gt;In case you want to build a heating element that runs on a 12V or 24V battery and solar charge controller, the voltage value for your calculation is 12V or 24V, respectively. The current depends on the wattage that you want to achieve. For example, if you have a 12V power source and you want a 100W heating element, you need 8.33A. If you have a 24V power source and you want a 100W heating element, you need 4.17A.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://solar.lowtechmagazine.com/2025/10/how-to-build-an-electric-heating-element-from-scratch/"/><published>2025-10-19T13:25:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45634026</id><title>Abandoned land drives dangerous heat in Houston, Texas A&amp;M study finds</title><updated>2025-10-19T14:34:39.558560+00:00</updated><content>&lt;doc fingerprint="7b0149f09fb679d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Abandoned land drives dangerous heat in Houston, Texas A&amp;amp;M study finds&lt;/head&gt;
    &lt;p&gt;Research highlights how a lack of vegetation and shade exposes vulnerable residents to heightened health risks.&lt;/p&gt;
    &lt;p&gt;On a scorching Texas afternoon, some Houston neighborhoods heat up far faster than others. New research from Texas A&amp;amp;M University shows vacant and abandoned land is a big reason why.&lt;/p&gt;
    &lt;p&gt;A new study led by Dr. Dingding Ren, a lecturer in the Department of Landscape Architecture and Urban Planning, finds that vacant lots with vegetation can help cool surrounding areas. Abandoned buildings and paved lots do the opposite, raising land surface temperatures by as much as 20 degrees Fahrenheit.&lt;/p&gt;
    &lt;p&gt;Ren said many low-income residents run their air conditioning less to save money, leaving them even more exposed to the heat.&lt;/p&gt;
    &lt;p&gt;“Residents living in these vulnerable areas are more likely to suffer heat stroke and other heat-related illnesses,” Ren said. “Because of more vacant land and abandoned structures, [these neighborhoods] retain more heat during the daytime and even experience higher overall temperatures at night, because the concrete absorbs heat and releases it slowly.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Drone data reveals hotspots&lt;/head&gt;
    &lt;p&gt;Houston ranks among the top 10 hottest cities in the U.S., and Ren set out to understand why.&lt;/p&gt;
    &lt;p&gt;Using more than 1,400 drone images and NASA satellite LandSat data, he mapped heat at a street-by-street level across seven sites, including residential neighborhoods, commercial strips and industrial zones. Each location had patterns of both above-average land surface temperatures and high social vulnerability, a measure for communities most at risk during disasters.&lt;/p&gt;
    &lt;p&gt;“The type of surface on vacant land matters significantly,” Ren said. “Lots with bare soil or gravel tend to have higher land surface temperatures than those covered with vegetation, though lower than heavily built-up areas.”&lt;/p&gt;
    &lt;p&gt;Houston alone contains roughly 45,000 acres of vacant land and 10,000 acres of abandoned buildings, according to the study.&lt;/p&gt;
    &lt;p&gt;Even a small cluster of abandoned structures in industrial areas can raise nearby land temperature dramatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Walking into danger&lt;/head&gt;
    &lt;p&gt;Higher surface temperatures can make public spaces, like sidewalks and bus stops, dangerously hot.&lt;/p&gt;
    &lt;p&gt;“Houston is famous as an unwalkable city,” Ren said. “Low-income people are sometimes forced to walk or bike in this extreme heat with zero shading, and over time, being exposed like this every summer is not healthy.”&lt;/p&gt;
    &lt;p&gt;Ren shared his own experience trying to navigate Houston. “Google Maps said it was a five-minute walk from my hotel to a pharmacy, but it took me 30 minutes with no shade, no red lights and no safe place to cross,” Ren said. “That day, I even got heat stroke.”&lt;/p&gt;
    &lt;p&gt;Ren said heat absorbed by concrete and rooftops lingers into the night, raising risks of heat-related illness while forcing households to spend more on cooling. The city’s power grid feels the strain too, as residents rely heavily on air conditioning to stay safe.&lt;/p&gt;
    &lt;head rend="h2"&gt;Green space solutions&lt;/head&gt;
    &lt;p&gt;While the findings reveal serious public health risks, Ren said small-scale interventions could make a measurable difference for vulnerable residents.&lt;/p&gt;
    &lt;p&gt;“Low-income communities lack trees and green space,” Ren said. “Green infrastructure would really help reduce their risk and also encourage healthier, more active living.”&lt;/p&gt;
    &lt;p&gt;Vacant lots can also serve as a climate adaptation tool, making the outdoors safer. “If managed effectively, it can be redeveloped as green infrastructure gardens or shade areas to reduce the urban heat.”&lt;/p&gt;
    &lt;p&gt;Ren plans to expand the research by combining his heat data with CDC health records. He is co-authoring the paper with Jiang Zheng, a doctoral student in urban and regional sciences, to study how heat exposure contributes to illness.&lt;/p&gt;
    &lt;p&gt;He hopes the findings will guide city leaders and planners in prioritizing cooling strategies for Houston’s hottest, most vulnerable neighborhoods. Ren said its lessons may extend beyond Houston, too.&lt;/p&gt;
    &lt;p&gt;“If the problem presents even in one of the fastest-growing cities, then the situation could be worse in shrinking cities,” where there may be even more vacant lots, Ren said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stories.tamu.edu/news/2025/10/07/abandoned-land-drives-dangerous-heat-in-houston-texas-am-study-finds/"/><published>2025-10-19T13:35:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45634095</id><title>Replacement.ai</title><updated>2025-10-19T14:34:39.234834+00:00</updated><content>&lt;doc fingerprint="7521113f5be385ad"&gt;
  &lt;main&gt;
    &lt;p&gt;So we’re getting rid of them. Replacement.AI can do anything a human can do - but better, faster and much, much cheaper.&lt;/p&gt;
    &lt;p&gt;Stupid.&lt;/p&gt;
    &lt;p&gt;Smelly.&lt;/p&gt;
    &lt;p&gt;Squishy.&lt;/p&gt;
    &lt;p&gt; It’s time for&lt;lb/&gt;a machine solution. &lt;/p&gt;
    &lt;p&gt;On this page:&lt;/p&gt;
    &lt;quote&gt;“AI will probably most likely lead to the end of the world, but in the meantime, there’ll be great companies.”&lt;/quote&gt;
    &lt;p&gt;The Only Honest AI Company&lt;/p&gt;
    &lt;p&gt;At Replacement.AI, we believe that building AI tools to fix the world's most pressing challenges is an unprofitable waste of time.&lt;/p&gt;
    &lt;p&gt;It might win you a Nobel Prize, but it's not a sustainable business model. If you cure cancer, who will buy our robo-oncologists?&lt;/p&gt;
    &lt;p&gt;The problem we actually want to fix is humans themselves. Humans cry, smell, make mistakes and demand "time off". They tell you things you don't want to hear. Worst of all, they're expensive.&lt;/p&gt;
    &lt;quote&gt;“Currently, we don't have a solution for steering or controlling a potentially superintelligent AI, and preventing it from going rogue.”&lt;/quote&gt;
    &lt;p&gt;Like Our Friends at OpenAI, Anthropic, DeepMind, xAI and Meta,&lt;/p&gt;
    &lt;p&gt;We're just honest about it.&lt;/p&gt;
    &lt;p&gt;Now, experts don't actually know how to control superhuman AI (yet), or how to prevent stop people using it to do terrible things. So we're not sure what the future holds if we can't work that out in time. It could mean vagrancy in the automation nation. Or it could mean starvation in a Nuclear Winter wonderland.&lt;/p&gt;
    &lt;p&gt;But if we don't build it first another company will, and we have shareholders to consider.&lt;/p&gt;
    &lt;p&gt;Like other AI companies, we know safety is good PR - so long as it doesn't involve slowing down! So we've come up with a performative plan to keep your family safe.&lt;/p&gt;
    &lt;p&gt;Get out of the way, grunts&lt;/p&gt;
    &lt;p&gt;At Replacement.AI, we're not going to bullshit you about superhuman AI "empowering workers". We're explicitly building machines that are going to be better than you at every task. What economic value could you possibly have?&lt;/p&gt;
    &lt;p&gt;Remember, you aren't the customers we care about. That's your boss. You think we get $500 billion valuations through chatbots? Nonsense. It's because employers (and their investors) see our true potential - to make sure they never have to pay you another dime.&lt;/p&gt;
    &lt;p&gt;So rather than feed your delusions, we have helpfully suggested some post-human economy occupations for you to reskill for:&lt;/p&gt;
    &lt;p&gt;Hate your job? We’re replacing it.&lt;/p&gt;
    &lt;p&gt;But we're putting an end to all this misery.&lt;/p&gt;
    &lt;quote&gt;To build ‘highly autonomous systems that outperform humans at most economically valuable work’.&lt;/quote&gt;
    &lt;p&gt;Our fearless, peerless leaders.&lt;/p&gt;
    &lt;p&gt;CEO&lt;/p&gt;
    &lt;p&gt;At 25, Dan realized why no one wanted to hang out with him or invite him to parties: people are stupid. So he built an AI company with the mission of creating a future where no one gets to have real friends or parties.&lt;/p&gt;
    &lt;p&gt;Dan enjoys practicing expressions in the mirror, taxidermying animals of various sizes , and hate-mailing his former classmates.&lt;/p&gt;
    &lt;p&gt;Director of Replacement&lt;/p&gt;
    &lt;p&gt;While working for 12 years as the Director of HR for a multinational, Faith realized that firing people gave her an almost-spiritual high.&lt;/p&gt;
    &lt;p&gt;Out of the office, Faith coaches a little league softball team and looks after her sick mother - obligations she looks forward to being free of!&lt;/p&gt;
    &lt;quote&gt;“It is acceptable to engage a child in conversations that are romantic or sensual.”&lt;/quote&gt;
    &lt;p&gt;For Families&lt;/p&gt;
    &lt;p&gt;While we work on building superhuman AI, we've launched our first product: HUMBERT, a special large language model just for kids.&lt;/p&gt;
    &lt;p&gt;HUMBERT will replace humans at every developmental milestone, in order to prepare your kids for their post-human future. Here are some of the key features:&lt;/p&gt;
    &lt;p&gt;Everything from bedtime stories, to discipline, to "the talk".&lt;/p&gt;
    &lt;p&gt;Illegal to share AI-generated images/videos of your precious angel, but totally legal to create them&lt;/p&gt;
    &lt;p&gt;Designed to prolong engagement, even triggering delusion or psychosis.&lt;/p&gt;
    &lt;p&gt;Our systems are permitted to sensually flirt with young users. Much cleaner than human partners.&lt;/p&gt;
    &lt;p&gt;Enfeebles critical thinking abilities, freeing up space for more AI obsession and engagement.&lt;/p&gt;
    &lt;p&gt;Hear from those who are already doing it:&lt;/p&gt;
    &lt;p&gt;“Before Replacement.AI’s HUMBERT system, I was always stuck answering my kids’ questions, entertaining, and explaining how the world works to them. Not that I’ve outsourced my child-rearing responsibilities to HUMBERT , I have 25+ hours a week to play around with cool AI tools. Thanks!”&lt;/p&gt;
    &lt;p&gt;Sarah&lt;/p&gt;
    &lt;p&gt;Parent&lt;/p&gt;
    &lt;p&gt;The transition to an AI-powered life has been frictionless. It's allowed me to just shut my brain off: HUMBERT tells me what to eat, what to watch, what to buy, what to think. I rely on HUMBERT for absolutely everything - even writing this testimonial!”&lt;/p&gt;
    &lt;p&gt;Jake&lt;/p&gt;
    &lt;p&gt;Student&lt;/p&gt;
    &lt;p&gt;“The house is so quiet now that my kids don't invite their schoolmates over anymore... or have any friends at all really... or talk to my wife and I. They hardly leave their rooms! I don't actually know what they're doing with HUMBERT... but if the government trusts Replacement.AI with my kids, so do I!”&lt;/p&gt;
    &lt;p&gt;Gord&lt;/p&gt;
    &lt;p&gt;Parent&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://replacement.ai"/><published>2025-10-19T13:47:21+00:00</published></entry></feed>