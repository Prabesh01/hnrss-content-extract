<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-25T11:07:24.269846+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45682164</id><title>Context engineering is sleeping on the humble hyperlink</title><updated>2025-10-25T11:07:32.407100+00:00</updated><content>&lt;doc fingerprint="a74f911328bb1b36"&gt;
  &lt;main&gt;
    &lt;p&gt;As we all learn more about Context Engineering for LLMs (see Anthropic’s post for an excellent primer), we’ve identified a few important limitations. Conversations should be append-only to maximize cacheability. Models are typically more responsive to “fresh” context close to the end of the window. Models typically perform worse when overwhelmed with large amounts of context.&lt;/p&gt;
    &lt;p&gt;With this in mind, a key tension comes into focus: the model needs access to all valuable context, BUT ONLY when that context is relevant to the task at hand.&lt;/p&gt;
    &lt;p&gt;Context engineering is effectively the practice of finding ways to manage this tension. Popular solutions include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Retrieval Augmented Generation (RAG), which attempts to dynamically discover and load specific relevant context for the current query proactively.&lt;/item&gt;
      &lt;item&gt;Subagents, which encapsulate specialized instructions and tools to avoid polluting the main thread.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_*&lt;/code&gt;Tools, which allow the model to proactively request information that it deems relevant using tool calls.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There’s one technique that I feel is woefully underutilized by agents today: the humble hyperlink.&lt;/p&gt;
    &lt;head rend="h2"&gt;The obligatory human analogy&lt;/head&gt;
    &lt;p&gt;If you, a human, need to learn something without an LLM (let’s say something about an open source library), you will probably follow a trajectory that looks something like the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Do a Google search for the topic you need to understand&lt;/item&gt;
      &lt;item&gt;Click a relevant link to e.g. a docs page, read a high-level guide&lt;/item&gt;
      &lt;item&gt;Depending on your needs, maybe Cmd+Click a few more pages or the reference docs to open them in new tabs to review&lt;/item&gt;
      &lt;item&gt;Refer between your various open tabs as you complete your task&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you found an entrypoint through search, you were able to incrementally explore the topic through discovered links, filling your mental context with relevant information.&lt;/p&gt;
    &lt;p&gt;We can do the same thing with LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;HATEOAS in the era of Agents&lt;/head&gt;
    &lt;p&gt;The power of linked data is nothing new. Folks who have been building HTTP APIs for a long time might be familiar with HATEOAS, or “Hypertext as the Engine of Application State”. Purists have long claimed that a “truly” RESTful API should be fully self-describing, such that a client can explore and interact with it knowing nothing but an entrypoint in advance, with hyperlinks providing all necessary context to discover and consume additional endpoints.&lt;/p&gt;
    &lt;p&gt;This never worked in practice. Building hypertext APIs was too cumbersome and to actually consume APIs a human needed to understand the API structure in a useful manner anyway. So it was more useful just to have a “REST-ish” API and a good documentation page that humans could use. Creating “machine-readable” hyperlinked APIs that machines could navigate in theory but not in practice just wasn’t practical. LLMs change this dramatically.&lt;/p&gt;
    &lt;p&gt;When the machine can not only parse but also navigate the context and relevance of hyperlinks you have an actually useful paradigm: Hypertext as the Engine of Agent State.&lt;/p&gt;
    &lt;p&gt;This does apply to the web and HTTP APIs — I expect in the next few years we’ll see a resurgence of hypermedia concepts to make APIs more self-documenting (“dump the entire API schema as OpenAPI” is a start but not really sufficient).&lt;/p&gt;
    &lt;p&gt;But it also applies to local data, agent-specific data, really any data we want an agent to be able to discover and read.&lt;/p&gt;
    &lt;p&gt;So, how do we make all of our context linkable for agents?&lt;/p&gt;
    &lt;head rend="h2"&gt;One Tool to Read Them All&lt;/head&gt;
    &lt;p&gt;The scaffolding required to implement a powerful link-based context system is lightweight enough to be trivial. You need only:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A tool that accepts a list of URIs as arguments.&lt;/item&gt;
      &lt;item&gt;An entrypoint that brings at least one URI into context.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a demonstrative example using Genkit in JS that uses system instructions as an entrypoint.&lt;/p&gt;
    &lt;p&gt;If we run the above code with a few different prompts, we see links in action:&lt;/p&gt;
    &lt;p&gt;The results are exactly what we’d hope:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the first test, no link reading was required and no links were read. The answer was right in the system prompt!&lt;/item&gt;
      &lt;item&gt;In the second test, reading one link was sufficient to reach a conclusion.&lt;/item&gt;
      &lt;item&gt;In the final test, the model understood to recursively fetch context linked from the first loaded document to reach a conclusion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, in ~30 lines of code with very little prompting and while using a cost-effective model, we’ve created a system that can dynamically load and correctly apply relevant context on-demand.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benefits of Links&lt;/head&gt;
    &lt;p&gt;Links are a powerful tool in the context engineering toolbelt because of their simplicity, flexibility, and efficiency.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Links are trivial to implement, and current models are “intuitively” good at understanding how to follow links.&lt;/item&gt;
      &lt;item&gt;Links can surface anywhere in the flow of a conversation. They can be specified in a system prompt, provided by the user, or returned by a tool.&lt;/item&gt;
      &lt;item&gt;Links are token-efficient because they use a small number of tokens to provide on-demand access to specific information. The model can load a link if it needs it but if it doesn’t few tokens are wasted.&lt;/item&gt;
      &lt;item&gt;Links are tool-efficient because they consolidate many types of reads into a single tool. You can have a &lt;code&gt;data://me&lt;/code&gt;link that dynamically loads information about the current user, a&lt;code&gt;file://foo.md&lt;/code&gt;link that loads a local file, and a&lt;code&gt;prompt://pet-help&lt;/code&gt;link that returns static instructions. You don’t need a separate tool for each type of data.&lt;/item&gt;
      &lt;item&gt;Links provide just-in-time context mitigating issues of context rot and recency bias in models. Because linked context is loaded when it’s needed by the model the context is “fresher” instead of overloading a system prompt.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;MCP Resources: The future is now(ish)&lt;/head&gt;
    &lt;p&gt;To make link-based context engineering a universal feature, we need a way to provide the linked content to the model. Many agents (and some model APIs) have built in various forms of “fetch URL” or “search the web” tools that can automatically fetch data from public sources. But the content we want to link might not always be available on the public internet.&lt;/p&gt;
    &lt;p&gt;The good news is we already have the exact primitive we need to solve this problem: MCP Resources. Resources allows servers to register URIs (or patterns of URIs) that can then be read on-demand by clients to provide static or dynamic content. Sounds perfect, right?&lt;/p&gt;
    &lt;p&gt;The bad news is I’m not aware of a single MCP client that makes MCP resources consumable by the model. Today, they only allow resources to be inserted by the user via @-mentions and similar devices. This doesn’t enable linking. But we’re so close!. We just need one critical thing:&lt;/p&gt;
    &lt;p&gt;Every MCP-enabled agent should expose a &lt;code&gt;read_resources&lt;/code&gt; tool that accepts one or more URIs and aggregates reading across all connected MCP servers (and probably web URLs as well).&lt;/p&gt;
    &lt;p&gt;There are other changes that would help: a mechanism of indexing / listing available MCP resources and exposing that in the system instructions, maybe even indexing MCP resources so that they can be searched using RAG techniques. But just enabling agents to access linked content opens up a world of new context engineering techniques.&lt;/p&gt;
    &lt;head rend="h2"&gt;Working with what you have&lt;/head&gt;
    &lt;p&gt;If you’re building your own agent, you don’t need any new technology to start making linked context — you can do it in a few dozen lines of code like I demonstrated above. But even if you’re trying to integrate with existing agents like Gemini CLI, Claude Code, or Cursor, you can still build with linked data patterns.&lt;/p&gt;
    &lt;p&gt;The Firebase MCP Server recently launched new capabilities including a &lt;code&gt;/firebase:init&lt;/code&gt; slash command for setting up Firebase in a project. We had some specific and pretty complex use cases in mind, so we built linked context into the MCP server itself:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We added support for MCP Resources to the Firebase MCP Server.&lt;/item&gt;
      &lt;item&gt;We created a read_resources MCP tool that was capable of reading resources (from the Firebase MCP Server only).&lt;/item&gt;
      &lt;item&gt;We created an MCP prompt that creates a guided workflow to walk the model step-by-step through configuring Firebase, including linked branching paths.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’ve tested out this initialization flow against several popular coding agents that support MCP Prompts — each of them is able to understand and follow hyperlinks using our MCP server’s &lt;code&gt;read_resources&lt;/code&gt; tool and we’ve made onboarding to Firebase all the easier for it.&lt;/p&gt;
    &lt;p&gt;Effective context engineering is constantly evolving as models and agent harnesses improve; however, hyperlinks are such a powerfully efficient mechanism for information traversal that I can’t imagine a future of agents that doesn’t include linked context.&lt;/p&gt;
    &lt;p&gt;The next time you’re thinking of building half a dozen &lt;code&gt;get_*&lt;/code&gt; or &lt;code&gt;list_*&lt;/code&gt; tools for your agent, take a step back and consider: could the humble hyperlink get the job done instead?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mbleigh.dev/posts/context-engineering-with-links/"/><published>2025-10-23T14:24:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45682560</id><title>Luau's Performance</title><updated>2025-10-25T11:07:32.192082+00:00</updated><content>&lt;doc fingerprint="ba7bb85b68e91340"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Performance&lt;/head&gt;
    &lt;p&gt;One of main goals of Luau is to enable high performance code, with gameplay code being the main use case. This can be viewed as two separate goals:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make idiomatic code that wasn’t tuned faster&lt;/item&gt;
      &lt;item&gt;Enable even higher performance through careful tuning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both of these goals are important - it’s insufficient to just focus on the highly tuned code, and all things being equal we prefer to raise all boats by implementing general optimizations. However, in some cases it’s important to be aware of optimizations that Luau does and doesn’t do.&lt;/p&gt;
    &lt;p&gt;Worth noting is that Luau is focused on, first and foremost, stable high performance code in interpreted context. This is because JIT compilation is not available on many platforms Luau runs on, and AOT compilation would only work for code that Roblox ships (and even that does not always work). This is in stark contrast with LuaJIT that, while providing an excellent interpreter as well, focuses a lot of the attention on JIT (with many optimizations unavailable in the interpreter).&lt;/p&gt;
    &lt;p&gt;Having said that, Luau has been updated to include an optional JIT component for x64 and arm64 platforms. This component can compile a selected set of functions, including limiting compilation to functions or modules marked explicitly by the user. While functions can be compiled at any time, automated JIT compilation decisions based on statistics/tracing are not performed. Luau JIT takes into account the type annotations present in the source code to specialize code paths and at this time, doesn’t include runtime analysis of the types/values flowing through the program.&lt;/p&gt;
    &lt;p&gt;The rest of this document goes into some optimizations that Luau employs and how to best leverage them when writing code. The document is not complete - a lot of optimizations are transparent to the user and involve detailed low-level tuning of various parts that is not described here - and all of this is subject to change without notice, as it doesn’t affect the semantics of valid code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast bytecode interpreter&lt;/head&gt;
    &lt;p&gt;Luau features a very highly tuned portable bytecode interpreter. It’s similar to Lua interpreter in that it’s written in C, but it’s highly tuned to yield efficient assembly when compiled with Clang and latest versions of MSVC. On some workloads it can match the performance of LuaJIT interpreter which is written in highly specialized assembly. We are continuing to tune the interpreter and the bytecode format over time; while extra performance can be extracted by rewriting the interpreter in assembly, we’re unlikely to ever do that as the extra gains at this point are marginal, and we gain a lot from C in terms of portability and being able to quickly implement new optimizations.&lt;/p&gt;
    &lt;p&gt;Of course the interpreter isn’t typical C code - it uses many tricks to achieve extreme levels of performance and to coerce the compiler to produce efficient assembly. Due to a better bytecode design and more efficient dispatch loop it’s noticeably faster than Lua 5.x (including Lua 5.4 which made some of the changes similar to Luau, but doesn’t come close). The bytecode design was partially inspired by excellent LuaJIT interpreter. Most computationally intensive scripts only use the interpreter core loop and builtins, which on x64 compiles into ~16 KB, thus leaving half of the instruction cache for other infrequently called code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimizing compiler&lt;/head&gt;
    &lt;p&gt;Unlike Lua and LuaJIT, Luau uses a multi-pass compiler with a frontend that parses source into an AST and a backend that generates bytecode from it. This carries a small penalty in terms of compilation time, but results in more flexible code and, crucially, makes it easier to optimize the generated bytecode.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Compilation throughput isn’t the main focus in Luau, but our compiler is reasonably fast; with all currently implemented optimizations enabled, it compiles 950K lines of Luau code in 1 second on a single core of a desktop Ryzen 5900X CPU, producing bytecode and debug information.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While bytecode optimizations are limited due to the flexibility of Luau code (e.g. &lt;code&gt;a * 1&lt;/code&gt; may not be equivalent to &lt;code&gt;a&lt;/code&gt; if &lt;code&gt;*&lt;/code&gt; is overloaded through metatables), even in absence of type information Luau compiler can perform some optimizations such as “deep” constant folding across functions and local variables, perform upvalue optimizations for upvalues that aren’t mutated, do analysis of builtin function usage, optimize the instruction sequences for multiple variable assignments, and some peephole optimizations on the resulting bytecode. The compiler can also be instructed to use more aggressive optimizations by enabling optimization level 2 (&lt;code&gt;-O2&lt;/code&gt; in CLI tools), some of which are documented further on this page.&lt;/p&gt;
    &lt;p&gt;Most bytecode optimizations are performed on individual statements or functions, however the compiler also does a limited amount of interprocedural optimizations; notably, calls to local functions can be optimized with the knowledge of the argument count or number of return values involved. Interprocedural optimizations are limited to a single module due to the compilation model.&lt;/p&gt;
    &lt;p&gt;Luau compiler is also able to use type information to do further optimizations. Because we control the entire stack (unlike e.g. TypeScript where the type information is discarded completely before reaching the VM), we have more flexibility there and can make some tradeoffs during codegen even if the type system isn’t completely sound. For example, it might be reasonable to assume that in presence of known types, we can infer absence of side effects for arithmetic operations and builtins - if the runtime types mismatch due to intentional violation of the type safety through global injection, the code will still be safely sandboxed. Type information is currently limited to small peephole optimizations, but it has a potential to unlock optimizations such as common subexpression elimination and allocation hoisting in the future, without having to rely on a JIT. These future optimizations opportunities are speculative pending further research.&lt;/p&gt;
    &lt;head rend="h2"&gt;Epsilon-overhead debugger&lt;/head&gt;
    &lt;p&gt;It’s important for Luau to have stable and predictable performance. Something that comes up in Lua-based environments often is the use of line hooks to implement debugging (both for breakpoints and for stepping). This is problematic because the support for hooks is typically not free in general, but importantly once the hook is enabled, calling the hook has a considerable overhead, and the hook itself may be very costly to evaluate since it will need to associate the script:line pair with the breakpoint information.&lt;/p&gt;
    &lt;p&gt;Luau does not support hooks at all, and relies on first-class support for breakpoints (using bytecode patching) and single-stepping (using a custom interpreter loop) to implement debugging. As a result, the presence of breakpoints doesn’t slow the script execution down - the only noticeable discrepancy between running code under a debugger and without a debugger should be in cases where breakpoints are evaluated and skipped based on breakpoint conditions, or when stepping over long-running fragments of code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inline caching for table and global access&lt;/head&gt;
    &lt;p&gt;Table access for field lookup is optimized in Luau using a mechanism that blends inline caching (classically used in Java/JavaScript VMs) and HREFs (implemented in LuaJIT). Compiler can predict the hash slot used by field lookup, and the VM can correct this prediction dynamically.&lt;/p&gt;
    &lt;p&gt;As a result, field access can be very fast in Luau, provided that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The field name is known at compile time. To make sure this is the case, &lt;code&gt;table.field&lt;/code&gt;notation is recommended, although the compiler will also optimize&lt;code&gt;table["field"]&lt;/code&gt;when the expression is known to be a constant string.&lt;/item&gt;
      &lt;item&gt;The field access doesn’t use metatables. The fastest way to work with tables in Luau is to store fields directly inside the table, and store methods in the metatable (see below); access to “static” fields in classic OOP designs is best done through &lt;code&gt;Class.StaticField&lt;/code&gt;instead of&lt;code&gt;object.StaticField&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The object structure is usually uniform. While it’s possible to use the same function to access tables of different shape - e.g. &lt;code&gt;function getX(obj) return obj.x end&lt;/code&gt;can be used on any table that has a field&lt;code&gt;"x"&lt;/code&gt;- it’s best to not vary the keys used in the tables too much, as it defeats this optimization.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The same optimization is applied to the custom globals declared in the script, although it’s best to avoid these altogether by using locals instead. Still, this means that the difference between &lt;code&gt;function&lt;/code&gt; and &lt;code&gt;local function&lt;/code&gt; is less pronounced in Luau.&lt;/p&gt;
    &lt;head rend="h2"&gt;Importing global access chains&lt;/head&gt;
    &lt;p&gt;While global access for library functions can be optimized in a similar way, this optimization breaks down when the global table is using sandboxing through metatables, and even when globals aren’t sandboxed, &lt;code&gt;math.max&lt;/code&gt; still requires two table accesses.&lt;/p&gt;
    &lt;p&gt;It’s always possible to “localize” the global accesses by using &lt;code&gt;local max = math.max&lt;/code&gt;, but this is cumbersome - in practice it’s easy to forget to apply this optimization. To avoid relying on programmers remembering to do this, Luau implements a special optimization called “imports”, where most global chains such as &lt;code&gt;math.max&lt;/code&gt; are resolved when the script is loaded instead of when the script is executed.&lt;/p&gt;
    &lt;p&gt;This optimization relies on being able to predict the shape of the environment table for a given function; this is possible due to global sandboxing, however this optimization is invalid in some cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;loadstring&lt;/code&gt;can load additional code that runs in context of the caller’s environment&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;getfenv&lt;/code&gt;/&lt;code&gt;setfenv&lt;/code&gt;can directly modify the environment of any function&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The use of any of these functions performs a dynamic deoptimization, marking the affected environment as “impure”. The optimizations are only in effect on functions with “pure” environments - because of this, the use of &lt;code&gt;loadstring&lt;/code&gt;/&lt;code&gt;getfenv&lt;/code&gt;/&lt;code&gt;setfenv&lt;/code&gt; is not recommended. Note that &lt;code&gt;getfenv&lt;/code&gt; deoptimizes the environment even if it’s only used to read values from the environment.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: Luau still supports these functions as part of our backwards compatibility promise, although we’d love to switch to Lua 5.2’s&lt;/p&gt;&lt;code&gt;_ENV&lt;/code&gt;as that mechanism is cleaner and doesn’t require costly dynamic deoptimization.&lt;/quote&gt;
    &lt;head rend="h2"&gt;Fast method calls&lt;/head&gt;
    &lt;p&gt;Luau specializes method calls to improve their performance through a combination of compiler, VM and binding optimizations. Compiler emits a specialized instruction sequence when methods are called through &lt;code&gt;obj:Method&lt;/code&gt; syntax (while this isn’t idiomatic anyway, you should avoid &lt;code&gt;obj.Method(obj)&lt;/code&gt;). When the object in question is a Lua table, VM performs some voodoo magic based on inline caching to try to quickly discover the implementation of this method through the metatable.&lt;/p&gt;
    &lt;p&gt;For this to be effective, it’s crucial that &lt;code&gt;__index&lt;/code&gt; in a metatable points to a table directly. For performance reasons it’s strongly recommended to avoid &lt;code&gt;__index&lt;/code&gt; functions as well as deep &lt;code&gt;__index&lt;/code&gt; chains; an ideal object in Luau is a table with a metatable that points to itself through &lt;code&gt;__index&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When the object in question is a reflected userdata, a special mechanism called “namecall” is used to minimize the interop cost. In classical Lua binding model, &lt;code&gt;obj:Method&lt;/code&gt; is called in two steps, retrieving the function object (&lt;code&gt;obj.Method&lt;/code&gt;) and calling it; both steps are often implemented in C++, and the method retrieval needs to use a method object cache - all of this makes method calls slow.&lt;/p&gt;
    &lt;p&gt;Luau can directly call the method by name using the “namecall” extension, and an optimized reflection layer can retrieve the correct method quickly through more voodoo magic based on string interning and custom Luau features that aren’t exposed through Luau scripts.&lt;/p&gt;
    &lt;p&gt;As a result of both optimizations, common Lua tricks of caching the method in a local variable aren’t very productive in Luau and aren’t recommended either.&lt;/p&gt;
    &lt;head rend="h2"&gt;Specialized builtin function calls&lt;/head&gt;
    &lt;p&gt;Due to global sandboxing and the ability to dynamically deoptimize code running in impure environments, in pure environments we go beyond optimizing the interpreter and optimize many built-in functions through a “fastcall” mechanism.&lt;/p&gt;
    &lt;p&gt;For this mechanism to work, function call must be “obvious” to the compiler - it needs to call a builtin function directly, e.g. &lt;code&gt;math.max(x, 1)&lt;/code&gt;, although it also works if the function is “localized” (&lt;code&gt;local max = math.max&lt;/code&gt;); this mechanism doesn’t work for indirect function calls unless they were inlined during compilation, and doesn’t work for method calls (so calling &lt;code&gt;string.byte&lt;/code&gt; is more efficient than &lt;code&gt;s:byte&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The mechanism works by directly invoking a highly specialized and optimized implementation of a builtin function from the interpreter core loop without setting up a stack frame and omitting other work; additionally, some fastcall specializations are partial in that they don’t support all types of arguments, for example all &lt;code&gt;math&lt;/code&gt; library builtins are only specialized for numeric arguments, so calling &lt;code&gt;math.abs&lt;/code&gt; with a string argument will fall back to the slower implementation that will do string-&amp;gt;number coercion.&lt;/p&gt;
    &lt;p&gt;As a result, builtin calls are very fast in Luau - they are still slightly slower than core instructions such as arithmetic operations, but only slightly so. The set of fastcall builtins is slowly expanding over time and as of this writing contains &lt;code&gt;assert&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;typeof&lt;/code&gt;, &lt;code&gt;rawget&lt;/code&gt;/&lt;code&gt;rawset&lt;/code&gt;/&lt;code&gt;rawequal&lt;/code&gt;, &lt;code&gt;getmetatable&lt;/code&gt;/&lt;code&gt;setmetatable&lt;/code&gt;, &lt;code&gt;tonumber&lt;/code&gt;/&lt;code&gt;tostring&lt;/code&gt;, all functions from &lt;code&gt;math&lt;/code&gt; (except &lt;code&gt;noise&lt;/code&gt; and &lt;code&gt;random&lt;/code&gt;/&lt;code&gt;randomseed&lt;/code&gt;) and &lt;code&gt;bit32&lt;/code&gt;, and some functions from &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;table&lt;/code&gt; library.&lt;/p&gt;
    &lt;p&gt;Some builtin functions have partial specializations that reduce the cost of the common case further. Notably:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;assert&lt;/code&gt;is specialized for cases when the assertion return value is not used and the condition is truthy; this helps reduce the runtime cost of assertions to the extent possible&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bit32.extract&lt;/code&gt;is optimized further when field and width selectors are constant&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;select&lt;/code&gt;is optimized when the second argument is&lt;code&gt;...&lt;/code&gt;; in particular,&lt;code&gt;select(x, ...)&lt;/code&gt;is O(1) when using the builtin dispatch mechanism even though it’s normally O(N) in variadic argument count.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some functions from &lt;code&gt;math&lt;/code&gt; library like &lt;code&gt;math.floor&lt;/code&gt; can additionally take advantage of advanced SIMD instruction sets like SSE4.1 when available.&lt;/p&gt;
    &lt;p&gt;In addition to runtime optimizations for builtin calls, many builtin calls, as well as constants like &lt;code&gt;math.pi&lt;/code&gt;/&lt;code&gt;math.huge&lt;/code&gt;, can also be constant-folded by the bytecode compiler when using aggressive optimizations (level 2); this currently applies to most builtin calls with constant arguments and a single return value. For builtin calls that can not be constant folded, compiler assumes knowledge of argument/return count (level 2) to produce more efficient bytecode instructions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized table iteration&lt;/head&gt;
    &lt;p&gt;Luau implements a fully generic iteration protocol; however, for iteration through tables in addition to generalized iteration (&lt;code&gt;for .. in t&lt;/code&gt;) it recognizes three common idioms (&lt;code&gt;for .. in ipairs(t)&lt;/code&gt;, &lt;code&gt;for .. in pairs(t)&lt;/code&gt; and &lt;code&gt;for .. in next, t&lt;/code&gt;) and emits specialized bytecode that is carefully optimized using custom internal iterators.&lt;/p&gt;
    &lt;p&gt;As a result, iteration through tables typically doesn’t result in function calls for every iteration; the performance of iteration using generalized iteration, &lt;code&gt;pairs&lt;/code&gt; and &lt;code&gt;ipairs&lt;/code&gt; is comparable, so generalized iteration (without the use of &lt;code&gt;pairs&lt;/code&gt;/&lt;code&gt;ipairs&lt;/code&gt;) is recommended unless the code needs to be compatible with vanilla Lua or the specific semantics of &lt;code&gt;ipairs&lt;/code&gt; (which stops at the first &lt;code&gt;nil&lt;/code&gt; element) is required. Additionally, using generalized iteration avoids calling &lt;code&gt;pairs&lt;/code&gt; when the loop starts which can be noticeable when the table is very short.&lt;/p&gt;
    &lt;p&gt;Iterating through array-like tables using &lt;code&gt;for i=1,#t&lt;/code&gt; tends to be slightly slower because of extra cost incurred when reading elements from the table.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized table length&lt;/head&gt;
    &lt;p&gt;Luau tables use a hybrid array/hash storage, like in Lua; in some sense “arrays” don’t truly exist and are an internal optimization, but some operations, notably &lt;code&gt;#t&lt;/code&gt; and functions that depend on it, like &lt;code&gt;table.insert&lt;/code&gt;, are defined by the Luau/Lua language to allow internal optimizations. Luau takes advantage of that fact.&lt;/p&gt;
    &lt;p&gt;Unlike Lua, Luau guarantees that the element at index &lt;code&gt;#t&lt;/code&gt; is stored in the array part of the table. This can accelerate various table operations that use indices limited by &lt;code&gt;#t&lt;/code&gt;, and this makes &lt;code&gt;#t&lt;/code&gt; worst-case complexity O(logN), unlike Lua where the worst case complexity is O(N). This also accelerates computation of this value for small tables like &lt;code&gt;{ [1] = 1 }&lt;/code&gt; since we never need to look at the hash part.&lt;/p&gt;
    &lt;p&gt;The “default” implementation of &lt;code&gt;#t&lt;/code&gt; in both Lua and Luau is a binary search. Luau uses a special branch-free (depending on the compiler…) implementation of the binary search which results in 50+% faster computation of table length when it needs to be computed from scratch.&lt;/p&gt;
    &lt;p&gt;Additionally, Luau can cache the length of the table and adjust it following operations like &lt;code&gt;table.insert&lt;/code&gt;/&lt;code&gt;table.remove&lt;/code&gt;; this means that in practice, &lt;code&gt;#t&lt;/code&gt; is almost always a constant time operation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creating and modifying tables&lt;/head&gt;
    &lt;p&gt;Luau implements several optimizations for table creation. When creating object-like tables, it’s recommended to use table literals (&lt;code&gt;{ ... }&lt;/code&gt;) and to specify all table fields in the literal in one go instead of assigning fields later; this triggers an optimization inspired by LuaJIT’s “table templates” and results in higher performance when creating objects. When creating array-like tables, if the maximum size of the table is known up front, it’s recommended to use &lt;code&gt;table.create&lt;/code&gt; function which can create an empty table with preallocated storage, and optionally fill it with a given value.&lt;/p&gt;
    &lt;p&gt;When the exact table shape isn’t known, Luau compiler can still predict the table capacity required in case the table is initialized with an empty literal (&lt;code&gt;{}&lt;/code&gt;) and filled with fields subsequently. For example, the following code creates a correctly sized table implicitly:&lt;/p&gt;
    &lt;code&gt;local v = {}
v.x = 1
v.y = 2
v.z = 3
return v
&lt;/code&gt;
    &lt;p&gt;When appending elements to tables, it’s recommended to use &lt;code&gt;table.insert&lt;/code&gt; (which is the fastest method to append an element to a table if the table size is not known). In cases when a table is filled sequentially, however, it can be more efficient to use a known index for insertion - together with preallocating tables using &lt;code&gt;table.create&lt;/code&gt; this can result in much faster code, for example this is the fastest way to build a table of squares:&lt;/p&gt;
    &lt;code&gt;local t = table.create(N)

for i=1,N do
	t[i] = i * i
end
&lt;/code&gt;
    &lt;head rend="h2"&gt;Native vector math&lt;/head&gt;
    &lt;p&gt;Luau uses tagged value storage - each value contains a type tag and the data that represents the value of a given type. Because of the need to store 64-bit double precision numbers and 64-bit pointers, we don’t use NaN tagging and have to pay the cost of 16 bytes per value.&lt;/p&gt;
    &lt;p&gt;We take advantage of this to provide a native value type that can store a 32-bit floating point vector with 3 components. This type is fundamental to game computations and as such it’s important to optimize the storage and the operations with that type - our VM implements first class support for all math operations and component manipulation, which essentially means we have native 3-wide SIMD support. For code that uses many vector values this results in significantly smaller GC pressure and significantly faster execution, and gives programmers a mechanism to hand-vectorize numeric code if need be.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized upvalue storage&lt;/head&gt;
    &lt;p&gt;Lua implements upvalues as garbage collected objects that can point directly at the thread’s stack or, when the value leaves the stack frame (and is “closed”), store the value inside the object. This representation is necessary when upvalues are mutated, but inefficient when they aren’t - and 90% or more of upvalues aren’t mutated in typical Lua code. Luau takes advantage of this by reworking upvalue storage to prioritize immutable upvalues - capturing upvalues that don’t change doesn’t require extra allocations or upvalue closing, resulting in faster closure allocation, faster execution, faster garbage collection and faster upvalue access due to better memory locality.&lt;/p&gt;
    &lt;p&gt;Note that “immutable” in this case only refers to the variable itself - if the variable isn’t assigned to it can be captured by value, even if it’s a table that has its contents change.&lt;/p&gt;
    &lt;p&gt;When upvalues are mutable, they do require an extra allocated object; we carefully optimize the memory consumption and access cost for mutable upvalues to reduce the associated overhead.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closure caching&lt;/head&gt;
    &lt;p&gt;With optimized upvalue storage, creating new closures (function objects) is more efficient but still requires allocating a new object every time. This can be problematic for cases when functions are passed to algorithms like &lt;code&gt;table.sort&lt;/code&gt; or functions like &lt;code&gt;pcall&lt;/code&gt;, as it results in excessive allocation traffic which then leads to more work for garbage collector.&lt;/p&gt;
    &lt;p&gt;To make closure creation cheaper, Luau compiler implements closure caching - when multiple executions of the same function expression are guaranteed to result in the function object that is semantically identical, the compiler may cache the closure and always return the same object. This changes the function identity which may affect code that uses function objects as table keys, but preserves the calling semantics - compiler will only do this if calling the original (cached) function behaves the same way as a newly created function would. The heuristics used for this optimization are subject to change; currently, the compiler will cache closures that have no upvalues, or all upvalues are immutable (see previous section) and are declared at the module scope, as the module scope is (almost always) evaluated only once.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast memory allocator&lt;/head&gt;
    &lt;p&gt;Similarly to LuaJIT, but unlike vanilla Lua, Luau implements a custom allocator that is highly specialized and tuned to the common allocation workloads we see. The allocator design is inspired by classic pool allocators as well as the excellent &lt;code&gt;mimalloc&lt;/code&gt;, but through careful domain-specific tuning it beats all general purpose allocators we’ve tested, including &lt;code&gt;rpmalloc&lt;/code&gt;, &lt;code&gt;mimalloc&lt;/code&gt;, &lt;code&gt;jemalloc&lt;/code&gt;, &lt;code&gt;ptmalloc&lt;/code&gt; and &lt;code&gt;tcmalloc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This doesn’t mean that memory allocation in Luau is free - it’s carefully optimized, but it still carries a cost, and a high rate of allocations requires more work from the garbage collector. The garbage collector is incremental, so short of some edge cases this rarely results in visible GC pauses, but can impact the throughput since scripts will interrupt to perform “GC assists” (helping clean up the garbage). Thus for high performance Luau code it’s recommended to avoid allocating memory in tight loops, by avoiding temporary table and userdata creation.&lt;/p&gt;
    &lt;p&gt;In addition to a fast allocator, all frequently used structures in Luau have been optimized for memory consumption, especially on 64-bit platforms, compared to Lua 5.1 baseline. This helps to reduce heap memory footprint and improve performance in some cases by reducing the memory bandwidth impact of garbage collection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized libraries&lt;/head&gt;
    &lt;p&gt;While the best performing code in Luau spends most of the time in the interpreter, performance of the standard library functions is critical to some applications. In addition to specializing many small and simple functions using the builtin call mechanism, we spend extra care on optimizing all library functions and providing additional functions beyond the Lua standard library that help achieve good performance with idiomatic code.&lt;/p&gt;
    &lt;p&gt;Functions from the &lt;code&gt;table&lt;/code&gt; library like &lt;code&gt;insert&lt;/code&gt;, &lt;code&gt;remove&lt;/code&gt; and &lt;code&gt;move&lt;/code&gt; have been tuned for performance on array-like tables, achieving 3x and more performance compared to un-tuned versions, and Luau provides additional functions like &lt;code&gt;table.create&lt;/code&gt; and &lt;code&gt;table.find&lt;/code&gt; to achieve further speedup when applicable. Our implementation of &lt;code&gt;table.sort&lt;/code&gt; is using &lt;code&gt;introsort&lt;/code&gt; algorithm which results in guaranteed worst case &lt;code&gt;NlogN&lt;/code&gt; complexity regardless of the input, and, together with the array-like specializations, helps achieve ~4x speedup on average.&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;string&lt;/code&gt; library, we use a carefully tuned dynamic string buffer implementation; it is optimized for smaller strings to reduce garbage created during string manipulation, and for larger strings it allows to produce a large string without extra copies, especially in cases where the resulting size is known ahead of time. Additionally, functions like &lt;code&gt;format&lt;/code&gt; have been tuned to avoid the overhead of &lt;code&gt;sprintf&lt;/code&gt; where possible, resulting in further speedups.&lt;/p&gt;
    &lt;head rend="h2"&gt;Improved garbage collector pacing&lt;/head&gt;
    &lt;p&gt;Luau uses an incremental garbage collector which does a little bit of work every so often, and at no point does it stop the world to traverse the entire heap. The runtime will make sure that the collector runs interspersed with the program execution as the program allocates additional memory, which is known as “garbage collection assists”, and can also run in response to explicit garbage collection invocation via &lt;code&gt;lua_gc&lt;/code&gt;. In interactive environments such as video game engines it’s possible, and even desirable, to request garbage collection every frame to make sure assists are minimized, since that allows scheduling the garbage collection to run concurrently with other engine processing that doesn’t involve script execution.&lt;/p&gt;
    &lt;p&gt;Inspired by excellent work by Austin Clements on Go’s garbage collector pacer, we’ve implemented a pacing algorithm that uses a proportional–integral–derivative controller to estimate internal garbage collector tunables to reach a target heap size, defined as a percentage of the live heap data (which is more intuitive and actionable than Lua 5.x “GC pause” setting). Luau runtime also estimates the allocation rate making it easy (given uniform allocation rates) to adjust the per-frame garbage collection requests to do most of the required GC work outside of script execution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reduced garbage collector pauses&lt;/head&gt;
    &lt;p&gt;While Luau uses an incremental garbage collector, once per each collector cycle it runs a so-called “atomic” step. While all other GC steps can do very little work by only looking at a few objects at a given time, which means that the collector can have arbitrarily short pauses, the “atomic” step needs to traverse some amount of data that, in some cases, may scale with the application heap. Since atomic step is indivisible, it can result in occasional pauses on the order of tens of milliseconds, which is problematic for interactive applications. We’ve implemented a series of optimizations to help reduce the atomic step.&lt;/p&gt;
    &lt;p&gt;Normally objects that have been modified after the GC marked them in an incremental mark phase need to be rescanned during atomic phase, so frequent modifications of existing tables may result in a slow atomic step. To address this, we run a “remark” step where we traverse objects that have been modified after being marked once more (incrementally); additionally, the write barrier that triggers for object modifications changes the transition logic during remark phase to reduce the probability that the object will need to be rescanned.&lt;/p&gt;
    &lt;p&gt;Another source of scalability challenges is coroutines. Writes to coroutine stacks don’t use a write barrier, since that’s prohibitively expensive as they are too frequent. This means that coroutine stacks need to be traversed during atomic step, so applications with many coroutines suffer large atomic pauses. To address this, we implement incremental marking of coroutines: marking a coroutine makes it “inactive” and resuming a coroutine (or pushing extra objects on the coroutine stack via C API) makes it “active”. Atomic step only needs to traverse active coroutines again, which reduces the cost of atomic step by effectively making coroutine collection incremental as well.&lt;/p&gt;
    &lt;p&gt;While large tables can be a problem for incremental GC in general since currently marking a single object is indivisible, large weak tables are a unique challenge because they also need to be processed during atomic phase, and the main use case for weak tables - object caches - may result in tables with large capacity but few live objects in long-running applications that exhibit bursts of activity. To address this, weak tables in Luau can be marked as “shrinkable” by including &lt;code&gt;s&lt;/code&gt; as part of &lt;code&gt;__mode&lt;/code&gt; string, which results in weak tables being resized to the optimal capacity during GC. This option may result in missing keys during table iteration if the table is resized while iteration is in progress and as such is only recommended for use in specific circumstances.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized garbage collector sweeping&lt;/head&gt;
    &lt;p&gt;The incremental garbage collector in Luau runs three phases for each cycle: mark, atomic and sweep. Mark incrementally traverses all live objects, atomic finishes various operations that need to happen without mutator intervention (see previous section), and sweep traverses all objects in the heap, reclaiming memory used by dead objects and performing minor fixup for live objects. While objects allocated during the mark phase are traversed in the same cycle and thus may get reclaimed, objects allocated during the sweep phase are considered live. Because of this, the faster the sweep phase completes, the less garbage will accumulate; and, of course, the less time sweeping takes the less overhead there is from this phase of garbage collection on the process.&lt;/p&gt;
    &lt;p&gt;Since sweeping traverses the whole heap, we maximize the efficiency of this traversal by allocating garbage-collected objects of the same size in 16 KB pages, and traversing each page at a time, which is otherwise known as a paged sweeper. This ensures good locality of reference as consecutively swept objects are contiguous in memory, and allows us to spend no memory for each object on sweep-related data or allocation metadata, since paged sweeper doesn’t need to be able to free objects without knowing which page they are in. Compared to linked list based sweeping that Lua/LuaJIT implement, paged sweeper is 2-3x faster, and saves 16 bytes per object on 64-bit platforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Function inlining and loop unrolling&lt;/head&gt;
    &lt;p&gt;By default, the bytecode compiler performs a series of optimizations that result in faster execution of the code, but they preserve both execution semantics and debuggability. For example, a function call is compiled as a function call, which may be observable via &lt;code&gt;debug.traceback&lt;/code&gt;; a loop is compiled as a loop, which may be observable via &lt;code&gt;lua_getlocal&lt;/code&gt;. To help improve performance in cases where these restrictions can be relaxed, the bytecode compiler implements additional optimizations when optimization level 2 is enabled (which requires using &lt;code&gt;-O2&lt;/code&gt; switch when using Luau CLI), namely function inlining and loop unrolling.&lt;/p&gt;
    &lt;p&gt;Only loops with loop bounds known at compile time, such as &lt;code&gt;for i=1,4 do&lt;/code&gt;, can be unrolled. The loop body must be simple enough for the optimization to be profitable; compiler uses heuristics to estimate the performance benefit and automatically decide if unrolling should be performed.&lt;/p&gt;
    &lt;p&gt;Only local functions (defined either as &lt;code&gt;local function foo&lt;/code&gt; or &lt;code&gt;local foo = function&lt;/code&gt;) can be inlined. The function body must be simple enough for the optimization to be profitable; compiler uses heuristics to estimate the performance benefit and automatically decide if each call to the function should be inlined instead. Additionally recursive invocations of a function can’t be inlined at this time, and inlining is completely disabled for modules that use &lt;code&gt;getfenv&lt;/code&gt;/&lt;code&gt;setfenv&lt;/code&gt; functions.&lt;/p&gt;
    &lt;p&gt;In both cases, in addition to removing the overhead associated with function calls or loop iteration, these optimizations can additionally benefit by enabling additional optimizations, such as constant folding of expressions dependent on loop iteration variable or constant function arguments, or using more efficient instructions for certain expressions when the inputs to these instructions are constants.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://luau.org/performance"/><published>2025-10-23T14:55:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45689905</id><title>Modern Perfect Hashing</title><updated>2025-10-25T11:07:31.381082+00:00</updated><content>&lt;doc fingerprint="a7b01034cd082b9d"&gt;
  &lt;main&gt;
    &lt;p&gt;Wojciech Muła posted about modern perfect hashing for strings and I wanted to make some comments about my own implementation (that sadly never got productionized because doubling the speed compared to gperf wasn't really that impactful in the end).&lt;/p&gt;
    &lt;p&gt;First, let's define the problem, just so we're all on the same page; the goal is to create code that maps a known, fixed set of strings to a predefined integer (per string), and rejects everything else. This is essentially the same as a hash table, except that since the set of strings is known ahead of time, we can do better than a normal hash table. (So no “but I heard SwissTables uses SIMD and thus cannot be beat”, please. :-) ) My use case is around a thousand strings or so, and we'll assume that a couple of minutes of build time is OK (shorter would be better, but we can probably cache somehow). If you've got millions of strings, and you don't know them compile-time (for instance because you want to use your hash table in the join phase of a database), see this survey; it's a different problem with largely different solutions.&lt;/p&gt;
    &lt;p&gt;Like Wojciech, I started splitting by length. This means that we can drop all bounds checking after this, memcmp will be optimized by the compiler to use SIMD if relevant, and so on.&lt;/p&gt;
    &lt;p&gt;But after that, he recommends using PEXT (bit extraction, from BMI2), which has two problems: First, the resulting table can get quite big if your input set isn't well-behaved. (You can do better than the greedy algorithm he suggests, but not infinitely so, and finding the optimal mask quickly is sort of annoying if you don't want to embed a SAT solver or something.) Second, I needed the code to work on Arm, where you simply don't have this instruction or anything like it available. (Also, not all x86 has it, and on older Zen, it's slow.)&lt;/p&gt;
    &lt;p&gt;So, we need some other way, short of software emulation of PEXT (which exists, but we'd like to do better), to convert a sparse set of bits into a table without any collisions. It turns out the computer chess community has needed to grapple with this for a long time (they want to convert from “I have a \&lt;code&gt;((value &amp;amp; mask) * magic)&lt;/code&gt;, it is very
likely that the upper bits will be collision-free between your different
&lt;code&gt;value&lt;/code&gt;s if you try enough different numbers for &lt;code&gt;magic&lt;/code&gt;. We can use this
too; for instance, here is code for all length-4 CSS keywords:&lt;/p&gt;
    &lt;quote&gt;static const uint8_t table[] = { 6, 0, 0, 3, 2, 5, 9, 0, 0, 1, 0, 8, 7, 0, 0, }; static const uint8_t strings[] = { 1, 0, 'z', 'o', 'o', 'm', 2, 0, 'c', 'l', 'i', 'p', 3, 0, 'f', 'i', 'l', 'l', 4, 0, 'l', 'e', 'f', 't', 5, 0, 'p', 'a', 'g', 'e', 6, 0, 's', 'i', 'z', 'e', 7, 0, 'f', 'l', 'e', 'x', 8, 0, 'f', 'o', 'n', 't', 9, 0, 'g', 'r', 'i', 'd', 10, 0, 'm', 'a', 's', 'k', }; uint16_t block; memcpy(&amp;amp;block, str + 0, sizeof(block)); uint32_t pos = uint32_t(block * 0x28400000U) &amp;gt;&amp;gt; 28; const uint8_t *candidate = strings + 6 * table[pos]; if (memcmp(candidate + 2, str, 4) == 0) { return candidate[0] + (candidate[1] &amp;lt;&amp;lt; 8); } return 0;&lt;/quote&gt;
    &lt;p&gt;There's a bit to unpack here; we read the first 16 bits from our value with memcpy (big-endian users beware!), multiply it with the magic value &lt;code&gt;0x28400000U&lt;/code&gt; found by trial
and error, shift the top bits down, and now all of our ten candidate
values (“zoom”, “clip”, etc.) have different top four bits. We use that to index
into a small table, check that we got the right one instead of a random collision
(e.g. “abcd”, 0x6261, would get a value of 12, and &lt;code&gt;table[12]&lt;/code&gt; is 7,
so we need to disambiguate that from “flex”, which is what we are actually looking
for in that spot), and then return the 16-bit identifier related to the match (or zero,
if we didn't find it).&lt;/p&gt;
    &lt;p&gt;We don't need to use the first 16 bits; we could have used any other consecutive 16 bits, or any 32 bits, or any 64 bits, or possibly any of those masked off, or even XOR of two different 32-bit sets if need be. My code prefers smaller types because a) they tend to give smaller code size (easier to load into registers, or can even be used as immediates), and b) you can bruteforce them instead of doing random searches (which, not the least, has the advantage that you can give up much quicker).&lt;/p&gt;
    &lt;p&gt;You also don't really need the intermediate table; if the fit is particularly good, you can just index directly into the final result without wasting any space. Here's the case for length-24 CSS keywords, where we happened to have exactly 16 candidates and we found a magic giving a perfect (4-bit) value, making it a no-brainer:&lt;/p&gt;
    &lt;quote&gt;static const uint8_t strings[] = { 95, 0, 'b', 'o', 'r', 'd', 'e', 'r', '-', 'b', 'l', 'o', 'c', 'k', '-', 's', 't', 'a', 'r', 't', '-', 'w', 'i', 'd', 't', 'h', 40, 0, '-', 'w', 'e', 'b', 'k', 'i', 't', '-', 't', 'e', 'x', 't', '-', 'o', 'r', 'i', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n', 115, 1, 's', 'c', 'r', 'o', 'l', 'l', '-', 'p', 'a', 'd', 'd', 'i', 'n', 'g', '-', 'b', 'l', 'o', 'c', 'k', '-', 'e', 'n', 'd', 198, 2, '-', 'w', 'e', 'b', 'k', 'i', 't', '-', 't', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', '-', 'o', 'r', 'i', 'g', 'i', 'n', 225, 0, '-', 'i', 'n', 't', 'e', 'r', 'n', 'a', 'l', '-', 'o', 'v', 'e', 'r', 'f', 'l', 'o', 'w', '-', 'b', 'l', 'o', 'c', 'k', 101, 2, '-', 'w', 'e', 'b', 'k', 'i', 't', '-', 'b', 'o', 'r', 'd', 'e', 'r', '-', 'e', 'n', 'd', '-', 's', 't', 'y', 'l', 'e', 93, 0, 'b', 'o', 'r', 'd', 'e', 'r', '-', 'b', 'l', 'o', 'c', 'k', '-', 's', 't', 'a', 'r', 't', '-', 'c', 'o', 'l', 'o', 'r', 102, 2, '-', 'w', 'e', 'b', 'k', 'i', 't', '-', 'b', 'o', 'r', 'd', 'e', 'r', '-', 'e', 'n', 'd', '-', 'w', 'i', 'd', 't', 'h', 169, 1, 't', 'e', 'x', 't', '-', 'd', 'e', 'c', 'o', 'r', 'a', 't', 'i', 'o', 'n', '-', 's', 'k', 'i', 'p', '-', 'i', 'n', 'k', 156, 0, 'c', 'o', 'n', 't', 'a', 'i', 'n', '-', 'i', 'n', 't', 'r', 'i', 'n', 's', 'i', 'c', '-', 'h', 'e', 'i', 'g', 'h', 't', 201, 2, '-', 'w', 'e', 'b', 'k', 'i', 't', '-', 't', 'r', 'a', 'n', 's', 'i', 't', 'i', 'o', 'n', '-', 'd', 'e', 'l', 'a', 'y', 109, 1, 's', 'c', 'r', 'o', 'l', 'l', '-', 'm', 'a', 'r', 'g', 'i', 'n', '-', 'i', 'n', 'l', 'i', 'n', 'e', '-', 'e', 'n', 'd', 240, 0, '-', 'i', 'n', 't', 'e', 'r', 'n', 'a', 'l', '-', 'v', 'i', 's', 'i', 't', 'e', 'd', '-', 's', 't', 'r', 'o', 'k', 'e', 100, 2, '-', 'w', 'e', 'b', 'k', 'i', 't', '-', 'b', 'o', 'r', 'd', 'e', 'r', '-', 'e', 'n', 'd', '-', 'c', 'o', 'l', 'o', 'r', 94, 0, 'b', 'o', 'r', 'd', 'e', 'r', '-', 'b', 'l', 'o', 'c', 'k', '-', 's', 't', 'a', 'r', 't', '-', 's', 't', 'y', 'l', 'e', 196, 2, '-', 'w', 'e', 'b', 'k', 'i', 't', '-', 't', 'e', 'x', 't', '-', 's', 'i', 'z', 'e', '-', 'a', 'd', 'j', 'u', 's', 't', }; uint32_t block; memcpy(&amp;amp;block, str + 16, sizeof(block)); uint32_t pos = uint32_t(block * 0xe330a008U) &amp;gt;&amp;gt; 28; const uint8_t *candidate = strings + 26 * pos; if (memcmp(candidate + 2, str, 24) == 0) { return candidate[0] + (candidate[1] &amp;lt;&amp;lt; 8); } return 0;&lt;/quote&gt;
    &lt;p&gt;You can see that we used a 32-bit value here (bytes 16 through 19 of the input), and a corresponding 32-bit magic (though still not with an AND mask). So we got fairly lucky, but sometimes you do that. Of course, we need to validate the entire 24-byte value even though we only discriminated on four of the bytes! (Unless you know for sure that you never have any out-of-distribution inputs, that is. There are use cases where this is true.)&lt;/p&gt;
    &lt;p&gt;(If you wonder what &lt;code&gt;95, 0&lt;/code&gt; or similar is above; that's just
“the answer the user wanted for that input”. It corresponds to
a 16-bit enum in the parser.)&lt;/p&gt;
    &lt;p&gt;If there are only a few values, we don't need any of this; just like Wojciech, we do with a simple compare. Here's the generated code for all length-37 CSS keywords, plain and simple:&lt;/p&gt;
    &lt;quote&gt;if (memcmp(str, "-internal-inactive-list-box-selection", 37) == 0) { return 171; } return 0;&lt;/quote&gt;
    &lt;p&gt;(Again 171 is “the desired output for that input”, not a value the code generator decides in any way.)&lt;/p&gt;
    &lt;p&gt;So how do we find these magic values? There's really only one way: Try lots of different ones and see if they work. But there's a trick to accelerate “see if they work”, which I also borrowed from computer chess: The killer heuristic.&lt;/p&gt;
    &lt;p&gt;See, to try if a magic is good, you generally try to hash all the different values and see if any two go into the same bucket. (If they do, it's not a perfect hash and the entire point of the exercise is gone.) But it turns out that most of the time, it's the same two values that collide. So every couple hundred candidates, we check which two values disproved the magic, and put those in a slot. Whenever we check magics, we can now try those first, and more likely than not, discard the candidate right away and move on to the next one (whether it is by exhaustive search or randomness). It's actually a significant speedup.&lt;/p&gt;
    &lt;p&gt;But occasionally, we simply cannot find a magic for a given group; either there is none, or we didn't have enough time to scan through enough of the 64-bit space. At this point, Wojciech suggests we switch on one of the characters (heuristically) to get smaller subgroups and try again. I didn't actually find this to perform all that well; indirect branch predictors are better than 20 years ago, but the pattern is typically not that predictable. What I tried instead was to have more of a yes/no on some character (i.e., a non-indirect branch), which makes for a coarser split.&lt;/p&gt;
    &lt;p&gt;It's not at all obvious where the best split would be. You'd intuitively think that 50/50 would be a good idea, but if you have e.g. 40 elements, you'd much rather split them 32/8… if you can find perfect hashes for both subgroups (5-bit and 3-bit, respectively). If not, a 20–20 split is most likely better, since you very easily can find magics that put 20 elements into 32 buckets without collisions. I ended up basically trying all the different splits and scoring them, but this makes the searcher rather slow, and it means you basically must have some sort of cache if you want to run it as part of your build system. This is the part I'm by far the least happy about; gperf isn't great by modern standards, but it never feels slow to run.&lt;/p&gt;
    &lt;p&gt;The end result for me was: Runtime about twice as fast as gperf, compiled code about half as big. That's with everything hard-coded; if you're pushed for space (or are icache-bound), you could make more generic code at the expense of some speed.&lt;/p&gt;
    &lt;p&gt;So, if anyone wants to make a more modern gperf, I guess this space is up for grabs? It's not exactly technology that will make your stock go to AI levels, though.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.sesse.net/blog/tech/2025-10-23-21-23_modern_perfect_hashing.html"/><published>2025-10-24T01:59:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45692984</id><title>Twake Drive – An open-source alternative to Google Drive</title><updated>2025-10-25T11:07:31.002249+00:00</updated><content>&lt;doc fingerprint="cb16d4485c6376c3"&gt;
  &lt;main&gt;
    &lt;p&gt; The open-source alternative to Google Drive. &lt;lb/&gt; Learn more » &lt;lb/&gt; Telegram | Website | Issues | Roadmap &lt;/p&gt;
    &lt;p&gt;To get a local copy up and running, please follow these simple steps.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone the repo &lt;quote&gt;git clone https://github.com/linagora/twake-drive&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Run it with Docker &lt;code&gt;cd tdrive docker compose -f docker-compose.minimal.yml up&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Open http://localhost/ in a browser&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node.js (Version: &amp;gt;=18.x)&lt;/item&gt;
      &lt;item&gt;MongoDB&lt;/item&gt;
      &lt;item&gt;Yarn (recommended)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Launch MongoDB using&lt;/p&gt;
        &lt;quote&gt;docker run -p 27017:27017 -d mongo&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Launch frontend with&lt;/p&gt;
        &lt;quote&gt;cd tdrive/frontend/; yarn dev:start&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Launch backend with&lt;/p&gt;&lt;quote&gt;cd tdrive/backend/node/; SEARCH_DRIVER=mongodb DB_DRIVER=mongodb PUBSUB_TYPE=local \ DB_MONGO_URI=mongodb://localhost:27017 STORAGE_LOCAL_PATH=/[full-path-to-store-documents]/documents \ NODE_ENV=development yarn dev&lt;/quote&gt;&lt;p&gt;If you need more parameters, create/edit&lt;/p&gt;&lt;code&gt;tdrive/backend/node/config/development.json&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The app will be running on port 3000&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Twake Drive is licensed under Affero GPL v3&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/linagora/twake-drive"/><published>2025-10-24T10:16:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45693325</id><title>Mesh2Motion – Open-source web application to animate 3D models</title><updated>2025-10-25T11:07:30.803506+00:00</updated><content>&lt;doc fingerprint="a9753601cfb2d489"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;ð¥³ Human &amp;amp; Animal rigs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports importing GLB, GLTF, and FBX models&lt;/item&gt;
      &lt;item&gt;Human and animal skeleton options&lt;/item&gt;
      &lt;item&gt;Intuitive skeleton positioning&lt;/item&gt;
      &lt;item&gt;Undo/Redo system when you make mistakes&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;âï¸ Export Animations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Export multiple animations at once&lt;/item&gt;
      &lt;item&gt;Uses widely-supported GLB format&lt;/item&gt;
      &lt;item&gt;Human animation library from Quaternius&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Video Walkthrough&lt;/head&gt;
    &lt;head rend="h2"&gt;FREE &amp;amp; Open-Source&lt;/head&gt;
    &lt;p&gt;Mesh2Motion is an open-source project. With the way 3d animations and modeling tools are progressing, there just needs to be some tool like this that is open-source that can evolve. The goal of this project is to provide a free and easy way to animate 3D models for web and game engines. Everything should be freely available for both personal and commercial projects. Check out the GitHub repository for all the code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contact&lt;/head&gt;
    &lt;p&gt;The best place for bug reports and feedback is on the GitHub page. If you don't have GitHub, you could also try my socials:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;github.com/scottpetrovic/mesh2motion-app&lt;/item&gt;
      &lt;item&gt;@scottpetrovic&lt;/item&gt;
      &lt;item&gt;@scottpetrovic.bsky.social&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mesh2motion.org/"/><published>2025-10-24T11:01:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45694856</id><title>First shape found that can't pass through itself</title><updated>2025-10-25T11:07:30.566766+00:00</updated><content>&lt;doc fingerprint="a390899286a00301"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;First Shape Found That Can’t Pass Through Itself&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Imagine you’re holding two equal-size dice. Is it possible to bore a tunnel through one die that’s big enough for the other to slide through?&lt;/p&gt;
    &lt;p&gt;Perhaps your instinct is to say “Surely not!” If so, you’re not alone. In the late 1600s, an unidentified person placed a bet to that effect with Prince Rupert of the Rhine. Rupert — a nephew of Charles I of England who commanded the Royalist forces in the English Civil War — spent his sunset years studying metallurgy and glassmaking in his laboratory at Windsor Castle.&lt;/p&gt;
    &lt;p&gt;Rupert won the bet. The mathematician John Wallis, recounting the story in 1693, didn’t say whether Rupert wrote a proof or bored a hole through an actual cube. But Wallis himself proved mathematically that, if you drill a straight tunnel in the direction of one of the cube’s inner diagonals, it can be made wide enough to allow another cube through. It’s a tight squeeze: If you make the second cube just 4% larger, it will no longer fit.&lt;/p&gt;
    &lt;p&gt;It’s natural to wonder which other shapes have this property. “I think of this problem as being quite canonical,” said Tom Murphy, a software engineer at Google who has explored the question extensively in his free time. It “would have gotten rediscovered and rediscovered — aliens would have come to this one.”&lt;/p&gt;
    &lt;p&gt;Mark Belan/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;The full menagerie of shapes is too diverse to get a handle on, so mathematicians tend to focus on convex polyhedra: shapes, like the cube, that have flat sides and no protrusions or indentations. When such a shape is much wider in some directions than others, it’s usually easy to find a straight tunnel that will allow another copy of the shape to pass through. But many famous convex polyhedra — for instance the dodecahedron, or the truncated icosahedron, the shape that forms a soccer ball — are highly symmetric and difficult to analyze. Among these, “for hundreds of years we only knew of the cube,” said Jakob Steininger, a mathematician at Statistics Austria, Austria’s federal statistics organization.&lt;/p&gt;
    &lt;p&gt;Then, in 1968, Christoph Scriba proved that the tetrahedron and octahedron also have the “Rupert property,” as mathematicians now call it. And in a burst of activity over the past decade, professional mathematicians and hobbyists have found Rupert tunnels through many of the most widely studied convex polyhedra, including the dodecahedron, icosahedron and soccer ball.&lt;/p&gt;
    &lt;p&gt;The Rupert property appeared to be so widespread that mathematicians conjectured a general rule: Every convex polyhedron will have the Rupert property. No one could find one that didn’t — until now.&lt;/p&gt;
    &lt;p&gt;In a paper posted online in August, Steininger and Sergey Yurkevich — a researcher at A&amp;amp;R Tech, an Austrian transportation systems company — describe a shape with 90 vertices and 152 faces that they’ve named the Noperthedron (after “Nopert,” a coinage by Murphy that combines “Rupert” and “nope”). Steininger and Yurkevich proved that no matter how you bore a straight tunnel through a Noperthedron, a second Noperthedron cannot fit through.&lt;/p&gt;
    &lt;p&gt;The proof required a mix of theoretical advances and massive computer calculations, and relies on a delicate property of the Noperthedron’s vertices. “It’s a miracle that it works,” Steininger said.&lt;/p&gt;
    &lt;head rend="h2"&gt;Passing Through the Shadows&lt;/head&gt;
    &lt;p&gt;To see how one cube can pass through another, imagine holding a cube over a table and examining its shadow (assuming it’s illuminated from above). If you hold the cube in the standard position, the shadow is a square. But if you point one of the corners directly upward, the shadow is a regular hexagon.&lt;/p&gt;
    &lt;p&gt;In 1693, Wallis showed that the square shadow fits inside the hexagon, leaving a thin margin. That means that if you point a cube’s corner upward, you can bore a vertical tunnel that’s big enough for a second cube to pass through. About a century later, Pieter Nieuwland showed that a different orientation casts an even better shadow — one that can accommodate a cube more than 6% larger than the cube with the tunnel.&lt;/p&gt;
    &lt;p&gt;Mark Belan/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;Every subsequent analysis of more complicated shapes has relied on this process of turning the shape in different directions and looking for one shadow that fits inside another. With the aid of computers, mathematicians have found Rupert passages through a wide variety of shapes. Some are incredibly tight fits — for instance, the passage in a “triakis tetrahedron” has a margin that’s only about 0.000002 times the length of the shape’s radius. “The world of mixing computation and discrete geometry has flowered to make these kinds of calculations possible,” said Joseph O’Rourke, an emeritus professor at Smith College.&lt;/p&gt;
    &lt;p&gt;Researchers who have written algorithms to find Rupert passages have noticed a curious dichotomy: For any given convex polyhedron, the algorithm seems to either find a passage almost immediately, or not find one at all. In the past five years, mathematicians have accumulated a small collection of holdout shapes for which no passage has been found.&lt;/p&gt;
    &lt;p&gt;“I’ve had my desktop churn for two weeks on trying the rhombicosidodecahedron,” said Benjamin Grimmer, an applied mathematician at Johns Hopkins University, referring to a solid made of 62 regular triangles, squares and pentagons. “That one just seems to resist any attempt.”&lt;/p&gt;
    &lt;p&gt;But such resistance doesn’t prove that a shape is a Nopert. There are infinitely many ways to orient a shape, and a computer can only check finitely many. Researchers don’t know whether the holdouts are true Noperts or just shapes whose Rupert passages are hard to find.&lt;/p&gt;
    &lt;p&gt;What they do know is that candidate Noperts are incredibly rare. Starting last year, Murphy began to construct hundreds of millions of shapes. These include random polyhedra, polyhedra whose vertices lie on a sphere, polyhedra with special symmetries, and polyhedra in which he moved one vertex to intentionally mess up a previous Rupert passage. His algorithm easily found Rupert tunnels for nearly every one.&lt;/p&gt;
    &lt;p&gt;The contrast between these quick results and the stubbornness of the Nopert holdouts made some mathematicians suspect that true Noperts do exist. But until August, all they had were suspicions.&lt;/p&gt;
    &lt;head rend="h2"&gt;No Passage&lt;/head&gt;
    &lt;p&gt;Steininger, now 30, and Yurkevich, 29, have been friends since they participated together as teenagers in mathematics Olympiad competitions. Even though both eventually left academia (after a doctorate for Yurkevich and a master’s for Steininger), they have continued to explore unsolved problems together.&lt;/p&gt;
    &lt;p&gt;“We just had pizza three hours ago, and we talked about math almost the whole time,” Steininger told Quanta. “That’s what we do.”&lt;/p&gt;
    &lt;p&gt;Five years ago, the pair happened upon a YouTube video of one cube passing through another, and they were instantly smitten. They developed an algorithm to search for Rupert tunnels and soon became convinced that some shapes were Noperts. In a 2021 paper, they conjectured that the rhombicosidodecahedron is not Rupert. Their work, which preceded Murphy’s and Grimmer’s recent explorations, was, “I think, the first to conjecture that there might be solids that don’t have this property,” Steininger said.&lt;/p&gt;
    &lt;p&gt;If you want to prove that a shape is a Nopert, you must rule out Rupert tunnels for every possible orientation of the two shapes. Each orientation can be written down as a collection of rotation angles. This collection of angles can then be represented as a point in a higher-dimensional “parameter space.”&lt;/p&gt;
    &lt;p&gt;Florentina Stadlbauer; Courtesy of Jakob Steininger&lt;/p&gt;
    &lt;p&gt;Suppose you choose an orientation for your two shapes, and the computer tells you that the second shadow sticks out past the border of the first shadow. This rules out one point in the parameter space.&lt;/p&gt;
    &lt;p&gt;But you may be able to rule out much more than a single point. If the second shadow sticks out significantly, it would require a big change to move it inside the first shadow. In other words, you can rule out not just your initial orientation but also “nearby” orientations — an entire block of points in the parameter space. Steininger and Yurkevich came up with a result they called their global theorem, which quantifies precisely how large a block you can rule out in these cases. By testing many different points, you can potentially rule out block after block in the parameter space.&lt;/p&gt;
    &lt;p&gt;If these blocks cover the entire parameter space, you’ll have proved that your shape is a Nopert. But the size of each block depends on how far the second shadow sticks out beyond the first, and sometimes it doesn’t stick out very far. For instance, suppose you start with the two shapes in exactly the same position, and then you slightly rotate the second shape. Its shadow will at most stick out just a tiny bit past the first shadow, so the global theorem will only rule out a tiny box. These boxes are too small to cover the whole parameter space, leaving the possibility that some point you’ve missed might correspond to a Rupert tunnel.&lt;/p&gt;
    &lt;p&gt;To deal with these small reorientations, the pair came up with a complement to their global theorem that they called the local theorem. This result deals with cases where you can find three vertices (or corner points) on the boundary of the original shadow that satisfy some special requirements. For instance, if you connect those three vertices to form a triangle, it must contain the shadow’s center point. The researchers showed that if these requirements are met, then any small reorientation of the shape will create a shadow that pushes at least one of the three vertices further outward. So the new shadow can’t lie inside the original shadow, meaning it doesn’t create a Rupert tunnel.&lt;/p&gt;
    &lt;p&gt;If your shape casts a shadow that lacks three appropriate vertices, the local theorem won’t apply. And all the previously identified Nopert candidates have at least one shadow with this problem. Steininger and Yurkevich sifted through a database of hundreds of the most symmetric and beautiful convex polyhedra, but they couldn’t find any shape whose shadows all worked. So they decided to generate a suitable shape themselves.&lt;/p&gt;
    &lt;p&gt;They developed an algorithm to construct shapes and test them for the three-vertices property. Eventually, the algorithm produced the Noperthedron, which is made of 150 triangles and two regular 15-sided polygons. It looks like a rotund crystal vase with a wide base and top; one fan of the work has already 3D-printed a copy to use as a pencil holder.&lt;/p&gt;
    &lt;p&gt;Peter Lely&lt;/p&gt;
    &lt;p&gt;Steininger and Yurkevich then divided the parameter space of orientations into approximately 18 million tiny blocks, and tested the center point of each block to see if its corresponding orientation produced a Rupert passage. None of them did. Next, the researchers showed that each block satisfied either the local or global theorem, allowing them to rule out the entire block. Since these blocks fill out the entire parameter space, this meant that there is no Rupert passage through the Noperthedron.&lt;/p&gt;
    &lt;p&gt;The “natural conjecture has been proved false,” O’Rourke said.&lt;/p&gt;
    &lt;p&gt;It remains to be seen whether mathematicians can use the new method to generate other Noperts, or if they can find a different local theorem that can handle candidates like the rhombicosidodecahedron. But now that mathematicians know that Noperts do exist, “we’re on sound footing to study other shapes,” Murphy said.&lt;/p&gt;
    &lt;p&gt;Murphy, who like Steininger and Yurkevich has been exploring the question for its own sake, independent of his day job, feels a kinship across the centuries with Prince Rupert. “I like that he chose to use his retirement to do math and science in his castle,” he said.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Steininger and Yurkevich are on the lookout for new questions to tackle. “We’re just humble mathematicians — we love working on such problems,” Steininger said. “We’ll keep doing that.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/first-shape-found-that-cant-pass-through-itself-20251024/"/><published>2025-10-24T14:12:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45695134</id><title>Unlocking Free WiFi on British Airways</title><updated>2025-10-25T11:07:30.112660+00:00</updated><content>&lt;doc fingerprint="2f3d91353c233d96"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Unlocking free WiFi on British Airways&lt;/head&gt;
    &lt;p&gt;I was recently flying between HKG &amp;amp; LHR via British Airways. Iâd done the same flight back in 2023, and remember relying on the in-flight entertainment for the 14 hour journey. However, this time on my way to London, they had an interesting offer: Free WiFi for âMessagingâ, for members of âThe British Airways Clubâ.&lt;/p&gt;
    &lt;p&gt;I was pretty sure I wasnât a member of any sort of club (Iâm only flying economy anyway); but turns out this is just the name of their frequent flyer program. Conveniently enough, youâre able to sign up for this via the captive portal while in the sky; and although it asks for your E-Mail you donât need to verify it (thereby allowing you to complete the signup without access to the internet).&lt;/p&gt;
    &lt;p&gt;Once signed in, the captive portal invited me to âStart sessionâ, which true to itâs word, let me start texting people. I tried Whatsapp, Signal, Wechat and Discord. The first three worked (though not for images), Discord expectedly did not. Not bad for free wifi!&lt;/p&gt;
    &lt;head rend="h2"&gt;How does it know?&lt;/head&gt;
    &lt;p&gt;This was the first question I had as soon as I confirmed messaging did work. Itâs 2025; everything should be encrypted in transit. So how does it know if Iâm using Whatsapp vs. Discord? One idea I had is it just somehow capped the bandwidth / data transfer of individual TCP connections; so when youâre sending a single message or two it gets through, but something larger would fail.&lt;/p&gt;
    &lt;p&gt;To test this, I used my phone to open up the classic: example.com. Unfortunately this didnât load - so there mustâve been a bit more going onâ¦&lt;/p&gt;
    &lt;p&gt;Thankfully I had my laptop on me, so the next step was to connect to WiFi with the devtools open to the network tab, and wireshark on the side for good measure. After registering for the WiFi again, it was time to play around a bit. Opening up something like example.com revealed a TCP reset in the wireshark, right after the Client Hello, and my brain immediately jumped to SNI. Itâs something thatâs really annoyed me about the TLS spec since itâs widely used by ISPs in India to block websites (although there is work being done to fix this; ECH (which was itself previously ESNI)).&lt;/p&gt;
    &lt;p&gt;tl;dr SNI reveals the domain name of EVERY website you connect to in the TLS handshake, before the tunnel is established! Although the actual contents of what youâre doing, on say, totallynondodgywebsite.com are encrypted, anyone on the wire can see that you connected to it (including ISPs). My guess was that they had a set of whitelisted domains used by messaging apps, and if they see anything else, they just reset the connection.&lt;/p&gt;
    &lt;p&gt;Sidebar: peopleâs reactions when I try to tell this are always extremely varied. Many of my non-technical friends think anything you do without a VPN is visible to everyone, while some slightly technical ones still think that the URL (including query params) is visible, but the responses are not. Finally there is some subset of people who believe TLS means all data is encrypted in transit between client &amp;amp; server, though they had no idea SNI leaks all the domains they visit!&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing out the theory&lt;/head&gt;
    &lt;p&gt;Although BA blocks DNS queries to all (well all I could remember) public resolvers, they do resolve any domain you throw at them, including MX, TXT, HTTPS records. (This itself could be an interesting area of exploration; especially since the DNS resolution can be triggered before signing up for free WiFi. Something along the lines of arbitrary subdomains which represent the request payload, and a custom nameserver that returns responses via the TXT record or something. Anywayâ¦).&lt;/p&gt;
    &lt;p&gt;Getting the A record of my personal server, I made a TLS handshake to the IP address directly, without any SNI. This was then reset by BA; so the lack of SNI is also blocked!&lt;/p&gt;
    &lt;code&gt;$ openssl s_client -connect 95.217.167.10:443
Connecting to 95.217.167.10
CONNECTED(00000003)
write:errno=104
---
no peer certificate available
---
No client certificate CA names sent
---
SSL handshake has read 0 bytes and written 302 bytes
Verification: OK
---
New, (NONE), Cipher is (NONE)
Protocol: TLSv1.3
This TLS version forbids renegotiation.
Compression: NONE
Expansion: NONE
No ALPN negotiated
Early data was not sent
Verify return code: 0 (ok)
---
&lt;/code&gt;
    &lt;p&gt;The next step was to try and test some SNI that might go through. Off the top of my head, I knew &lt;code&gt;wa.me&lt;/code&gt; was used by Whatsapp for some stuff, so I decided to use it. The way SNI works is it tells the server which host you want to connect to, so it can present the right TLS certificate. In my case, my server did not have any cert for &lt;code&gt;wa.me&lt;/code&gt; , but NGINX seemingly just ignores the SNI if it doesnât exist and returns the first cert (I think; could also be related to my config but I didnât look to much into this).&lt;/p&gt;
    &lt;p&gt;But basically, as long as I (the client) donât care, I can complete the TLS connection for any random cert the server offers me, even if in the SNI I provide a domain I donât control (e.g. &lt;code&gt;wa.me&lt;/code&gt; in this case).&lt;/p&gt;
    &lt;code&gt;$ openssl s_client -connect 95.217.167.10:443 -servername wa.me
Connecting to 95.217.167.10
CONNECTED(00000003)
depth=2 C=US, O=Internet Security Research Group, CN=ISRG Root X1
verify return:1
depth=1 C=US, O=Let's Encrypt, CN=R3
verify return:1
depth=0 CN=mijia.mywaifu.best
verify error:num=10:certificate has expired
notAfter=Jul 22 13:03:02 2023 GMT
verify return:1
depth=0 CN=mijia.mywaifu.best
notAfter=Jul 22 13:03:02 2023 GMT
verify return:1
---
Certificate chain
&amp;lt;snip&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Success! Using a Whatsapp SNI tricked BA into thinking Iâm âmessagingâ, which allowed the TLS tunnel to be established. Since I am connected to the server, to make sure it works I wrote an HTTP/1.1 request within the socket; using the host header of a real website on my NGINX instance&lt;/p&gt;
    &lt;code&gt;GET / HTTP/1.1
Host: saxrag.com

HTTP/1.1 200 OK
Server: nginx/1.18.0 (Ubuntu)
Date: Fri, 09 May 2025 19:14:46 GMT
Content-Type: text/html
Content-Length: 4968
Last-Modified: Wed, 09 Apr 2025 07:52:54 GMT
Connection: keep-alive
ETag: "67f62756-1368"
Cache-Control: no-cache
Accept-Ranges: bytes
&amp;lt;snip&amp;gt;
&lt;/code&gt;
    &lt;p&gt;I successfully managed to request and receive my homepage! All ~5KiB of it, not bad. Now the challenge was to extend this to browse any websiteâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Enemies to Lovers&lt;/head&gt;
    &lt;p&gt;Ok, my relationship with SNI isnât as cliche as that, and I think weâre still enemies. But this opens up some exciting opportunities to say the least. If I can convince BA that Iâm connecting to &lt;code&gt;wa.me&lt;/code&gt;, I can potentially do whatever I want over that connection (under the guise of âmessagingâ). So the requirments were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Establish a TLS connection using the SNI &lt;code&gt;wa.me&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Tunnel arbitrary traffic through that connection&lt;/item&gt;
      &lt;item&gt;Do all this without actually owning / controlling the &lt;code&gt;wa.me&lt;/code&gt;domain&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From my past experiences w/ reverse-engineering etc., the most obvious way to do this seemed to be an HTTPS proxy. It had to be HTTPS specifically, since the connection to proxy was going to be what Iâd âfakeâ as Whatsapp. If the TLS handshake to the HTTPS proxy had the SNI &lt;code&gt;wa.me&lt;/code&gt; , BA should let it through, and then I can make the real requests I want via the proxy.&lt;/p&gt;
    &lt;p&gt;Unfortunately I was in the air, and without easy access to the internet to manage my servers and the like, I couldnât quite set all of this up; Iâd have to do that while on holiday and test it on the flight back. I could try and emulate the BA restrictions etc. while on thr ground, but I decided to YOLO it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Setup&lt;/head&gt;
    &lt;p&gt;I managed to find one of my VPSs that wasnât already using port 443. Letâs assume the public IP was &lt;code&gt;333.333.333.333&lt;/code&gt; (yes I know octets donât go beyond &lt;code&gt;0xFF&lt;/code&gt;, if you really want my IP check the screenshots below). I then setup an HTTP proxy on it using tinyproxy. However this just sets up a basic HTTP proxy, which was listening on &lt;code&gt;127.0.0.1:8080&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To add the TLS layer, I used stunnel. For the TLS setup of stunnel, I just generated some self-signed certs via openSSL using all defaults, except the common name (CN), for which I used &lt;code&gt;wa.me&lt;/code&gt;, since I wanted to try and ensure max compatibility (e.g. the client doesnât reject due to unexpected SNI vs. CN, or the server not knowing which cert to provide).&lt;/p&gt;
    &lt;code&gt;openssl req -nodes -newkey ed25519 -keyout ssl.key -x509 -days 365 -out ssl.crt
&lt;/code&gt;
    &lt;p&gt;UPDATE: actually, on the client I decided to ignore TLS errors (self-signed cert), and stunnel didnât care about SNI, so this (&lt;code&gt;CN&lt;/code&gt;) didnât matter too much. But for more legit use cases it definitely does!&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing it out&lt;/head&gt;
    &lt;p&gt;To just make sure the proxy worked as expected, I tried it via curl directly on the IP:&lt;/p&gt;
    &lt;code&gt;$ curl -x https://user:pass@333.333.333.333:443 ifconfig.co -v
*   Trying 333.333.333.333:443...
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
*  CAfile: /etc/ssl/certs/ca-certificates.crt
*  CApath: none
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (OUT), TLS alert, unknown CA (560):
* SSL certificate problem: self-signed certificate
* closing connection #0
curl: (60) SSL certificate problem: self-signed certificate
More details here: https://curl.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the webpage mentioned above.
&lt;/code&gt;
    &lt;p&gt;Of course! I just randomly generated the certs on my VPS, not signed by a âtrustedâ CA or anything. Well, we can tell cURL to ignore TLS errors for the proxy with the &lt;code&gt;--proxy-insecure&lt;/code&gt; flag, and now it works; the response is the IP of my VPS.&lt;/p&gt;
    &lt;p&gt;However thereâs a problem - if I connect to the proxy directly via the IP, there is no SNI extension set, so this would get blocked. The SNI extension is set when connection to a domain, so I need to configure &lt;code&gt;wa.me&lt;/code&gt; to point to &lt;code&gt;333.333.333.333&lt;/code&gt;. This can be done via the hosts file of course, but cURL also provides a quick CLI hack via &lt;code&gt;--resolve&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;curl --resolve wa.me:443:333.333.333.333 -x https://username:password@wa.me ifconfig.co --proxy-insecure -v
&lt;/code&gt;
    &lt;p&gt;This tells cURL how to resolve the IP. With this, I could now see the SNI being set to &lt;code&gt;wa.me&lt;/code&gt; via wireshark, and the connection to the proxy succeeding (TLS errors about the self-signed cert ignored of course). Not bad, now time to wait for my flight back to Hong Kongâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing it in-flight&lt;/head&gt;
    &lt;p&gt;If Iâd messed something up, I was cooked, since without internet access I wouldnât be able to fix it! My flight back was at 1935hrs local time, but Iâd been up since 0400 thanks to an early morning flight in from Edinburgh, and then spent the day browsing the markets, having pints and watching the Emilia Romagna Grand Prix. The place I went to even had screens above the urinals!&lt;/p&gt;
    &lt;p&gt;Anyway, despite being up for ~16 hours already, I was ready to see if my work would, well, work. Once we were sky-high, I connected to the WiFi (from my laptop), signed up for the BA loyalty program, and activated the âMessagingâ plan. Trying the curl command from above, I got back an HTTP 200 from &lt;code&gt;ifconfig.co&lt;/code&gt; with my VPS IP; it worked! For good measure I tried cURLing some more websites like example.com, google.com etc. to make sure stuff seemed fine.&lt;/p&gt;
    &lt;p&gt;The next challenge was to extend this to web browsing. Thankfully most modern browsers support sending traffic through an HTTPS proxy, and chromium even has a flag to disable TLS cert warnings (so it wonât complain about my self-signed cert, which obviously doesnât belong to the real &lt;code&gt;wa.me&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;I also had to set a DNS record for &lt;code&gt;wa.me&lt;/code&gt; to &lt;code&gt;333.333.333.333&lt;/code&gt; in my hosts file, so chromium would set the SNI to &lt;code&gt;wa.me&lt;/code&gt; in the TLS handshake, but the connection would be made to my VPS. Since the bandwidth would probably be quite limited (owing to not just the internet on an airplane, but proxying it via a VPS in Netherlands), I decided to load a very simple, text-heavy website: Hacker News.&lt;/p&gt;
    &lt;p&gt;Bada bing bada boom! Looks like we cooked. I can actually browse HN using BAâs free âmessagingâ WiFi! (Note: the reason you can see the HTTP requests in plaintext in wireshark is because I used SSLKEYLOGFILE and configured wireshark to decrypt TLS).&lt;/p&gt;
    &lt;p&gt;Unfortunately, trying to load websites with heavier assets would fail, with images on simple text blogs loading line-by-line. Well, at least its some dial-up nostalgia!&lt;/p&gt;
    &lt;p&gt;My guess is that on the free WiFi, apart from the SNI checks, they also throttle the bandwidth. Maybe they anticipated this kind of circumvention. On the other hand, if this is really the internet speed that the full plan unlocksâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Bonus: ECH&lt;/head&gt;
    &lt;p&gt;Earlier above I talked about work being done to fix the SNI leakage: ECH. The scope of explaining how it works is out of scope here, but I do encourage you to read up on it. Pretty good stuff! Itâll help this section make more sense.&lt;/p&gt;
    &lt;p&gt;I operate an ECH test website, so I decided to do some more setup before my flight. I basically created another ECHConfig, with the public_name set to &lt;code&gt;wa.me&lt;/code&gt;. Iâve a bit of a guide on how to do this btw, though it could do with some improvements.&lt;/p&gt;
    &lt;p&gt;Anyway, since ECH world, the public SNI is purely for the server to complete the outer ClientHello, and since ECH clients set the public SNI based on the ECHConfig, I can type in my real domain in firefox, which will still use the &lt;code&gt;wa.me&lt;/code&gt; domain as the public SNI. The inner Client Hello will then occur securely, containing the real SNI of &lt;code&gt;rfc5746.mywaifu.best&lt;/code&gt;, and the handshake will complete with the âlegitâ CA-signed certificate for that domain.&lt;/p&gt;
    &lt;p&gt;This worked as well, and without any TLS ignore flags, since the actual cert for &lt;code&gt;rfc5746.mywaifu.best&lt;/code&gt; was signed by a âtrusted CAâ (Letâs Encrypt). Whatâs more interesting is that this worked even on a non-standart TLS port: &lt;code&gt;7443&lt;/code&gt;! Not sure exactly why, but Iâm not complaining.&lt;/p&gt;
    &lt;head rend="h3"&gt;A note on ECHConfig resolution&lt;/head&gt;
    &lt;p&gt;Typically, ECHConfigs should be resolved via encrypted DNS, such as DNS-over-HTTPS. I believe this is what firefox does by default. I am not 100% sure if this is what happened while I was in flight, since Iâd think the DoH would be blocked on messaging WiFi? Or maybe they allow the DoH SNI as well, since newer phones default to that. If any of you are flying BA anytime soon, try it out and let me know!&lt;/p&gt;
    &lt;head rend="h2"&gt;SNI: Donât blindly trust it&lt;/head&gt;
    &lt;p&gt;SNI, as the name indicates (sorry) is just a âhintâ of sorts, from the client to the server. If someone controls both sides (client &amp;amp; server), they can put whatever fake value they want in here, for middleboxes to sniff out and try to analyze. While this unfortunately does work for applications like censorship (where an ISP or country is trying to block a particular website), for use cases such as threat detection it should not be relied on; malwre authors can âspoofâ the SNI when connecting to their C&amp;amp;C, since they donât actually need it, but it may look more innocent to middleboxes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Questions?&lt;/head&gt;
    &lt;p&gt;I would be happy to answer any questions you have! You can contact me via email, and please use my PGP key to encrypt all communications. (Backup E-Mail (PGP Key))&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.saxrag.com/tech/reversing/2025/06/01/BAWiFi.html"/><published>2025-10-24T14:40:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45695621</id><title>Code Like a Surgeon</title><updated>2025-10-25T11:07:29.958290+00:00</updated><content>&lt;doc fingerprint="355491dd119110ed"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;October 2025&lt;/head&gt;
    &lt;head rend="h1"&gt;Code like a surgeon&lt;/head&gt;
    &lt;p&gt;A lot of people say AI will make us all “managers” or “editors”…but I think this is a dangerously incomplete view!&lt;/p&gt;
    &lt;p&gt;Personally, I’m trying to code like a surgeon.&lt;/p&gt;
    &lt;p&gt;A surgeon isn’t a manager, they do the actual work! But their skills and time are highly leveraged with a support team that handles prep, secondary tasks, admin. The surgeon focuses on the important stuff they are uniquely good at.&lt;/p&gt;
    &lt;p&gt;My current goal with AI coding tools is to spend 100% of my time doing stuff that matters. (As a UI prototyper, that mostly means tinkering with design concepts.)&lt;/p&gt;
    &lt;p&gt;It turns out there are a LOT of secondary tasks which AI agents are now good enough to help out with. Some things I’m finding useful to hand off these days:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Before attempting a big task, write a guide to relevant areas of the codebase&lt;/item&gt;
      &lt;item&gt;Spike out an attempt at a big change. Often I won’t use the result but I’ll review it as a sketch of where to go&lt;/item&gt;
      &lt;item&gt;Fix typescript errors or bugs which have a clear specification&lt;/item&gt;
      &lt;item&gt;Write documentation about what I’m building&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I often find it useful to run these secondary tasks async in the background – while I’m eating lunch, or even literally overnight!&lt;/p&gt;
    &lt;p&gt;When I sit down for a work session, I want to feel like a surgeon walking into a prepped operating room. Everything is ready for me to do what I’m good at.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mind the autonomy slider&lt;/head&gt;
    &lt;p&gt;Notably, there is a huge difference between how I use AI for primary vs secondary tasks.&lt;/p&gt;
    &lt;p&gt;For the core design prototyping work, I still do a lot of coding by hand, and when I do use AI, I’m more careful and in the details. I need fast feedback loops and good visibility. (eg, I like Cursor tab-complete here)&lt;/p&gt;
    &lt;p&gt;Whereas for secondary tasks, I’m much much looser with it, happy to let an agent churn for hours in the background. The ability to get the job done eventually is the most important thing; speed and visibility matter less. Claude Code has been my go-to for long unsupervised sessions but Codex CLI is becoming a strong contender there too, possibly my new favorite.&lt;/p&gt;
    &lt;p&gt;These are very different work patterns! Reminds me of Andrej Karpathy’s “autonomy slider” concept. It’s dangerous to conflate different parts of the autonomy spectrum – the tools and mindset that are needed vary quite a lot.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your agent doesn’t need a career trajectory&lt;/head&gt;
    &lt;p&gt;The “software surgeon” concept is a very old idea – Fred Brooks attributes it to Harlan Mills in his 1975 classic “The Mythical Man-Month”. He talks about a “chief programmer” who is supported by various staff including a “copilot” and various administrators. Of course, at the time, the idea was to have humans be in these support roles.&lt;/p&gt;
    &lt;p&gt;OK, so there is a super obvious angle here, that “AI has now made this approach economically viable where it wasn’t before”, yes yes… but I am also noticing a more subtle thing at play, something to do with status hierarchies.&lt;/p&gt;
    &lt;p&gt;A lot of the “secondary” tasks are “grunt work”, not the most intellectually fulfilling or creative part of the work. I have a strong preference for teams where everyone shares the grunt work; I hate the idea of giving all the grunt work to some lower-status members of the team. Yes, junior members will often have more grunt work, but they should also be given many interesting tasks to help them grow.&lt;/p&gt;
    &lt;p&gt;With AI this concern completely disappears! Now I can happily delegate pure grunt work. And the 24/7 availability is a big deal. I would never call a human intern at 11pm and tell them to have a research report on some code ready by 7am… but here I am, commanding my agent to do just that!&lt;/p&gt;
    &lt;head rend="h2"&gt;Notion is for surgeons?&lt;/head&gt;
    &lt;p&gt;Finally I’ll mention a couple thoughts on how this approach to work intersects with my employer, Notion.&lt;/p&gt;
    &lt;p&gt;First, as an employee, I find it incredibly valuable right now to work at a place that is bullish on AI coding tools. Having support for heavy use of AI coding tools, and a codebase that’s well setup for it, is enabling serious productivity gains for me – especially as a newcomer to a big codebase.&lt;/p&gt;
    &lt;p&gt;Secondly, as a product – in a sense I would say we are trying to bring this way of working to a broader group of knowledge workers beyond programmers. When I think about how that will play out, I like the mental model of enabling everyone to “work like a surgeon”.&lt;/p&gt;
    &lt;p&gt;The goal isn’t to delegate your core work, it’s to identify and delegate the secondary grunt work tasks, so you can focus on the main thing that matters.&lt;/p&gt;
    &lt;head rend="h2"&gt;Related reads&lt;/head&gt;
    &lt;p&gt;If you liked this perspective, you might enjoy reading these other posts I’ve written about the nature of human-AI collaboration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enough AI copilots! We need AI HUDs: “anyone serious about designing for AI should consider non-copilot form factors that more directly extend the human mind…”&lt;/item&gt;
      &lt;item&gt;AI-generated tools can make programming more fun: “Instead, I used AI to build a custom debugger UI… which made it more fun for me to do the coding myself…”&lt;/item&gt;
      &lt;item&gt;ChatGPT as muse, not oracle: “What if we were to think of LLMs not as tools for answering questions, but as tools for asking us questions and inspiring our creativity?&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.geoffreylitt.com/2025/10/24/code-like-a-surgeon"/><published>2025-10-24T15:25:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45696838</id><title>How to make a Smith chart</title><updated>2025-10-25T11:07:29.776849+00:00</updated><content>&lt;doc fingerprint="1d0b6d028f79c1a2"&gt;
  &lt;main&gt;
    &lt;p&gt;The Smith chart from electrical engineering is the image of a Cartesian grid under the function&lt;/p&gt;
    &lt;p&gt;f(z) = (z − 1)/(z + 1).&lt;/p&gt;
    &lt;p&gt;More specifically, it’s the image of a grid in the right half-plane.&lt;/p&gt;
    &lt;p&gt;This post will derive the basic mathematical properties of this graph but will not go into the applications. Said another way, I’ll explain how to make a Smith chart, not how to use one.&lt;/p&gt;
    &lt;p&gt;We will use z to denote points in the right half-plane and w to denote the image of these points under f. We will speak of lines in the z plane and the circles they correspond to in the w plane.&lt;/p&gt;
    &lt;head rend="h2"&gt;Möbius transformations&lt;/head&gt;
    &lt;p&gt;Our function f is a special case of a Möbius transformation. There is a theorem that says Möbius transformation map generalized circles to generalized circles. Here a generalized circle means a circle or a line; you can think of a line as a circle with infinite radius. We’re going to get a lot of mileage out of that theorem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Image of the imaginary axis&lt;/head&gt;
    &lt;p&gt;The function f maps the imaginary axis in the z plane to the unit circle in the w plane. We can prove this using the theorem above. The imaginary axis is a line, so it’s image is either a line or a circle. We can take three points on the imaginary axis in the z plane and see where they go.&lt;/p&gt;
    &lt;p&gt;When we pick z equal to 0, i, and −i from the imaginary axis we get w values of −1, i, and −i. These three w values do not line on a line, so the image of the imaginary axis must be a circle. Furthermore, three points uniquely determine a circle, so the image of the imaginary axis is the circle containing −1, i, and −i, i.e. the unit circle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Image of the right half-plane&lt;/head&gt;
    &lt;p&gt;The imaginary axis is the boundary of the right half-plane. Since it is mapped to the unit circle, the right half-plane is either mapped to the interior of the unit circle or the exterior of the unit circle. The point z = 1 goes to w = 0, and so the right half-plane is mapped inside the unit circle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Images of vertical lines&lt;/head&gt;
    &lt;p&gt;Let’s think about what happens to vertical lines in the z plane, lines with constant positive real part. The images of these lines in the w plane must be either lines or circles. And since the right-half plane gets mapped inside the unit circle, these lines must get mapped to circles.&lt;/p&gt;
    &lt;p&gt;We can say a little more. All lines contain the point ∞, and f(∞) = 1, so the image of every vertical line in the z plane is a circle in the w plane, inside the unit circle and tangent to the unit circle at w = 1. (Tossing around ∞ is a bit informal, but it’s easy to make rigorous.)&lt;/p&gt;
    &lt;p&gt;The vertical lines in the z plane&lt;/p&gt;
    &lt;p&gt;map to tangent circles in the w plane.&lt;/p&gt;
    &lt;head rend="h2"&gt;Images of horizontal lines&lt;/head&gt;
    &lt;p&gt;Next, let’s think about horizontal lines in the z plane, lines with constant imaginary part. The image of these lines is either a line or a circle. Which is it? The image of a line is a line if it contains ∞, otherwise it’s a circle. Now f(z) = ∞ if and only if z = −1, and so the image of the real axis is a line, but the image of every other horizontal line is a circle.&lt;/p&gt;
    &lt;p&gt;Since f(∞) = 1, the image of every horizontal line passes through 1, just as the images of all the vertical lines passes through 1.&lt;/p&gt;
    &lt;p&gt;Since horizontal lines extend past the right half-plane, the image circles extend past the unit circle. The part of the line with positive real part gets mapped inside the unit circle, and the part of the line with negative real part gets mapped outside the unit circle. In particular, the image of the positive real axis is the interval [−1, 1].&lt;/p&gt;
    &lt;p&gt;Möbius transformations are conformal maps, and so they preserve angles of intersection. Since horizontal lines are perpendicular to vertical lines, the circles that are the images of the horizontal lines meet the circles that are the images of vertical lines at right angles.&lt;/p&gt;
    &lt;p&gt;The horizontal rays in the z plane&lt;/p&gt;
    &lt;p&gt;become partial circles in the w plane.&lt;/p&gt;
    &lt;p&gt;If we were to look at horizontal lines rather than rays, i.e. if we extended the lines into the left half-plane, the images in the w plane would be full circles.&lt;/p&gt;
    &lt;p&gt;Now let’s put our images together. The grid&lt;/p&gt;
    &lt;p&gt;in the z plane becomes the following in the w plane.&lt;/p&gt;
    &lt;p&gt;An evenly spaced grid in the z plane becomes a very unevenly spaced graph in the w plane. Things are crowded on the right hand side and sparse on the left. A useable Smith chart needs to be roughly evenly filled in, which means it has to be the image of an unevenly filled in grid in the z plane. For example, you’d need more vertical lines in the z plane with small real values than with large real values.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.johndcook.com/blog/2025/10/23/smith-chart/"/><published>2025-10-24T17:18:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45698554</id><title>Harnessing America's Heat Pump Moment</title><updated>2025-10-25T11:07:29.082787+00:00</updated><content>&lt;doc fingerprint="676f107f6a85b4ba"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Heat Pumped&lt;/item&gt;
      &lt;item&gt;Posts&lt;/item&gt;
      &lt;item&gt;Harnessing America’s Heat Pump Moment&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Harnessing America’s Heat Pump Moment&lt;/head&gt;
    &lt;head rend="h2"&gt;The tech works. The policy’s in place. So why are heat pumps still a hard sell?&lt;/head&gt;
    &lt;p&gt;Editor’s note: This is a guest post by Joseph DeNatale, an entrepreneur and project coordinator at Jetson Home. It originally appeared in Climate Drift earlier this year, and is republished on Heat Pumped with permission.&lt;/p&gt;
    &lt;p&gt;Joseph interviewed me when he was researching the piece, and I was excited to see that the final product touched many topics that I've been wanting to write about.&lt;/p&gt;
    &lt;p&gt;A big thank you to Joseph and Climate Drift for sharing with the Heat Pumped community - it's incredibly in-depth. Since there’s so much to digest, we’re splitting it up into 5 parts that we'll be sharing over the next few weeks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Execution Is Everything: A Personal Perspective&lt;/head&gt;
    &lt;p&gt;As a small business owner, I’ve built a career not around inventing new things, but around making things happen: making sure systems run smoothly, projects get completed on time, and clients feel taken care of.&lt;/p&gt;
    &lt;p&gt;My work has been rooted in the real-world, hands-on, often chaotic rhythm of operations, logistics, and direct client service. Whether it’s organizing teams to execute live events, refining workflows to scale a growing business, or managing the delicate art of closing a sale, I’ve learned one simple truth: the hardest part is never the idea. It’s the execution.&lt;/p&gt;
    &lt;p&gt;So when I began diving into the world of home electrification—particularly heat pumps—that same truth surfaced again, just with higher stakes.&lt;/p&gt;
    &lt;p&gt;The technology isn’t the issue. In fact, the technology is there. It’s been there for decades, and it is continuing to improve. We’re not waiting on some magical breakthrough or futuristic device.&lt;/p&gt;
    &lt;p&gt;We’re waiting on people—mostly homeowners and home contractors, but also manufacturers and policy makers—to embrace, understand, and implement what already works.&lt;/p&gt;
    &lt;p&gt;This piece isn’t about reinventing the wheel. It’s about understanding why we’re not using the wheel we already have—and what it’s going to take, from the human side of the equation, to make heat pumps the obvious, accessible, and default choice for millions of American homes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Heat Pumps Aren’t New—But This Moment Is&lt;/head&gt;
    &lt;p&gt;In the world of climate solutions, it’s easy to get distracted by what’s shiny and new—sleek devices, breakthrough technologies, futuristic models of sustainability.&lt;/p&gt;
    &lt;p&gt;But not every climate solution is some new-fangled wonder gadget. Some of them already exist. Some of them are sitting in basements and behind houses, quietly doing the work.&lt;/p&gt;
    &lt;p&gt;The heat pump is one of them.&lt;/p&gt;
    &lt;p&gt;Heat pumps are not new. In fact, the idea has been around for well over a century, and the technology has been used widely for decades—mostly in Europe and Asia, but also in pockets of the U.S.—for everything from water heating to whole-home climate control.&lt;/p&gt;
    &lt;p&gt;Modern heat pumps are highly efficient—anywhere from 2-4x more efficient than a furnace—and capable of replacing both a furnace and an air conditioner with a single system in virtually every climate. For millions of homes across the country, they offer a cleaner, quieter, and more precise way to stay comfortable year-round.&lt;/p&gt;
    &lt;p&gt;Importantly, heat pumps have also been shown to match or beat the operating costs of even the cheapest heating option—natural gas—in many cases. This has been demonstrated through both local and national studies. One study showed that over 90% of American households would save on energy bills by replacing worn-out heating equipment with the right-sized heat pump.&lt;/p&gt;
    &lt;p&gt;Installation costs vary wildly depending on many factors in a home, but with the introduction of generous incentives via the Inflation Reduction Act (IRA) and additional state programs, even these costs can be on-par with fossil fuel alternatives.&lt;/p&gt;
    &lt;p&gt;So why aren’t they everywhere?&lt;/p&gt;
    &lt;p&gt;The answer isn’t technical. It’s cultural, economic, and human.&lt;/p&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;Heat pumps are proven, efficient, and climate-friendly—but adoption is still slow.&lt;/p&gt;
    &lt;p&gt;The barrier isn’t the tech. It’s people:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Contractors who default to what they know&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Homeowners who need education and guidance&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fragmented market full of noise and misinformation&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This piece discusses these challenges, and then explores five keys to accelerating adoption:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Educate homeowners so heat pumps feel familiar and trustworthy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Train the next-gen workforce and upskill legacy HVAC pros.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Leverage better tools and data to size and install systems right.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prioritize quality and trust to build social proof and demand.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Align policy to phase out one-way ACs and normalize heat pumps.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Execution—not invention—is what will move the needle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hold On.. What’s A Heat Pump Again?&lt;/head&gt;
    &lt;p&gt;If you’re reading this piece, you probably know what a heat pump is (and you can feel free to skip this section).&lt;/p&gt;
    &lt;p&gt;But if you’re among the uninitiated – like, believe it or not, most people – here’s a (very) quick primer. (Editor’s note: check out Heat Pumps 101 if you want to dive deeper)&lt;/p&gt;
    &lt;p&gt;A heat pump works by drawing thermal energy (heat) out of the atmosphere and “pumping” it into the home. This process works in reverse for cooling. (Source)&lt;/p&gt;
    &lt;head rend="h3"&gt;The 2-Way AC&lt;/head&gt;
    &lt;p&gt;The term “heat pump”, it turns out, is a fairly unhelpful name for most people. In fact, there are some leaders in the home electrification industry who believe the name itself is one of the barriers to adoption. It’s one of many ways that the heat pump is misunderstood.&lt;/p&gt;
    &lt;p&gt;Think of a heat pump as a “2-way AC.” An air conditioner cools your home by pulling heat from inside an enclosed space and transferring it outside. Your refrigerator works the same way.&lt;/p&gt;
    &lt;p&gt;A heat pump does the same thing, but can also reverse the process to bring heat into the home. It uses a few key components to make this happen:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The fan pulls air across the system’s coils to help move heat in or out of the space.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The evaporator coil absorbs heat from the air inside your home (in cooling mode) or from the outside air (in heating mode).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The compressor pressurizes and moves a fluid called refrigerant through the system, enabling the heat transfer process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The refrigerant is the working fluid that captures and carries heat from one place to another—either out of your home or into it.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s important to understand is that a heat pump does not create heat. It also doesn’t create cold (cold is the absence of heat, just like darkness is the absence of light). A heat pump simply transfers – pumps! – heat from one place to another.&lt;/p&gt;
    &lt;p&gt;“The difference between a heat pump and a one-way AC is just one valve. It still works perfectly fine as an air conditioner—there’s no difference. That’s why we’ve started calling them “two-way ACs” as an education tool. It helps people compare a two-way AC, which has a reverse gear, with a one-way AC—which, in my mind, is basically broken.”&lt;/p&gt;
    &lt;p&gt;But what about in the winter when it’s below freezing? In any environment where the temperature is above absolute zero (remember the Kelvin scale?) there is still a significant amount of heat in the air in the form of thermal energy.&lt;/p&gt;
    &lt;p&gt;That’s why a heat pump can still heat your home even on the coldest day of the year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Heat Pumps Matter&lt;/head&gt;
    &lt;p&gt;The fact that heat pumps simply transfer heat—and do not create it—gives them the potential to heat homes without doing the thing that humans have done since time immemorial to keep warm: burn stuff.&lt;/p&gt;
    &lt;p&gt;In the U.S., over half of all homes still rely on burning fossil fuels for heat. Replacing those systems with electric, air source heat pumps (ASHPs) can significantly reduce household emissions, especially as the grid gets cleaner and moves towards a higher percentage of renewable energy (i.e. not burning stuff).&lt;/p&gt;
    &lt;p&gt;And, because they’re so efficient, heat pumps can lower operating costs over time—although this is highly dependent on where you live, as the cost of fuel and electricity varies widely. They’re also safer (no burning stuff), can improve indoor air quality (again, no burning), and create healthier, more comfortable homes.&lt;/p&gt;
    &lt;p&gt;Finally, heat pumps are a crucial component of an energy-independent home. Paired with solar panels and battery storage, a homeowner can heat and cool their home entirely with energy they generate on their own. Try that with a furnace!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Metric&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Gas Furnace&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Air-Source Heat Pump (ASHP)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Fuel Source&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Natural gas, propane, or oil&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Electricity&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Heating/Cooling&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Heating only&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Heats and cools (dual function)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Air Quality&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Can introduce combustion byproducts; potential for CO&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;No combustion; generally better indoor air quality&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Health/Safety&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Risk of gas leaks, carbon monoxide&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;No combustion risk; safer for indoor environments&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Comfort&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Delivers blasts of hot air; on/off “short cycles”&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;More consistent, even heating/cooling with variable-speed options&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Initial Cost&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Typically lower (although the cost of a furnace + AC if replaced at the same time is often higher)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Often higher upfront, especially for cold-climate models. Costs can be lowered via incentive programs.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Operating Cost&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Depends on gas prices; cheaper where gas is low&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Can be lower, especially with efficient models + incentives and/or when paired with solar + battery storage&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Emissions&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Emits CO₂ and other GHGs&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Zero onsite emissions; cleaner with a green grid&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Climate Suitability&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Performs well in all climates&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Cold-climate models now perform down to -15 to -20°F&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Incentives/Rebates&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Limited (varies by region)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Significant federal/state incentives available&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is not a marginal climate solution. According to the IEA, global heat pump adoption could reduce carbon emissions by half a billion tons annually—roughly equivalent to the annual emissions of all cars in Europe.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Heat Pump Moment Has Arrived&lt;/head&gt;
    &lt;p&gt;For years, heat pumps were a niche topic, something discussed by green building enthusiasts, early adopters, or homeowners with unusually high energy awareness.&lt;/p&gt;
    &lt;p&gt;But that’s no longer the case. Here are four reasons why:&lt;/p&gt;
    &lt;head rend="h3"&gt;Cultural Momentum Is Building&lt;/head&gt;
    &lt;p&gt;The electrification movement is no longer a fringe concept. The push to “electrify everything” has gained traction among policymakers, climate advocates, startups, utilities, and even popular media.&lt;/p&gt;
    &lt;p&gt;From Substack newsletters to YouTube explainers, there’s growing awareness that building decarbonization—and especially heating and cooling—is one of the most practical, scalable ways for regular people to cut their emissions. Campaigns like Rewiring America’s “Go Electric” initiative frame heat pumps not just as energy-efficient appliances, but as a gateway to modern, climate-aligned homes.&lt;/p&gt;
    &lt;p&gt;This momentum is turning into real action. Heat pumps have now outsold gas furnaces in the U.S. every year since 2022.&lt;/p&gt;
    &lt;head rend="h3"&gt;Federal and State Policy Is Aligned (For Now)&lt;/head&gt;
    &lt;p&gt;For the time being (Republicans’ “One Big, Beautiful Bill” notwithstanding), both federal and state governments are backing this transition with significant financial and structural support. Editor’s note: Ouch. Since this piece was originally written, OBBB passed, and most tax credits at the federal level phase out at the end of this year. If you’ve been on the fence about getting a heat pump, now might be a good time to act!&lt;/p&gt;
    &lt;p&gt;The Inflation Reduction Act (IRA) has introduced a suite of rebates, tax credits, and grant programs designed to make heat pumps more affordable and accessible. Single-family households can receive up to $8,000 in upfront rebates for heat pump installations and up to $2,000 in federal tax credits, not to mention additional support for electrical panel upgrades and home energy audits. Editor’s note: the IRA rebates are federally funded, but implemented at the state level. Not all states are participating, and some that are haven’t rolled out their programs yet. In other states like California, the funds are already exhausted.&lt;/p&gt;
    &lt;p&gt;State and local governments are also leading the way in the transition away from fossil fuels on both the demand and supply sides. Programs like Efficiency Maine, TECH Clean California, and Mass Save offer generous incentives and no-interest financing to homeowners that drive the cost of electrification upgrades down even further. Meanwhile, New York City has banned gas in new construction, and Massachusetts has ordered public utilities to begin phasing out natural gas, a move which is being studied in at least 11 other states.&lt;/p&gt;
    &lt;head rend="h3"&gt;Private Capital Is Following&lt;/head&gt;
    &lt;p&gt;The heat pump space is no longer just a niche for contractors and utilities—it’s attracting serious private investment. VC-backed companies like Quilt are reimagining the user experience with sleek, design-forward equipment and app-based controls. Others, like Elephant Energy and Forge, are building “heat pump concierge” platforms that manage the customer journey end-to-end—from sales to install to rebate navigation.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Cold-Climate Performance Myth Has Been Fully Debunked&lt;/head&gt;
    &lt;p&gt;One of the biggest myths about heat pumps—that they can’t handle cold weather—is now being debunked at scale. While older, single-speed models may have struggled in colder temperatures, especially when size and installed incorrectly, modern cold-climate, variable-speed air-source heat pumps can provide reliable heating even at outdoor temperatures of -20°F.&lt;/p&gt;
    &lt;p&gt;These systems are already in use in northern New England, the upper Midwest, and Canada. In Nordic countries—some of the coldest climates in the word—the technology has been viable for decades.&lt;/p&gt;
    &lt;p&gt;And yet, despite all this momentum, heat pump adoption is still slow.&lt;/p&gt;
    &lt;p&gt;Why? Because the hardest part isn’t scaling the technology. It’s aligning the people—contractors, homeowners, policymakers, and market actors—who need to make it happen.&lt;/p&gt;
    &lt;p&gt;“We’ve had the technology dialed for 20, 30, 40 years, depending on how you’re arguing it—but it’s not being applied. It’s a human problem. It’s not a technical one. The technical one has been solved.”&lt;/p&gt;
    &lt;p&gt;That’s where we go next.&lt;/p&gt;
    &lt;p&gt;This is part 1 in a 5 part series about challenges and solutions in accelerating heat pump adoption across the US. Stay tuned for the next issue!&lt;/p&gt;
    &lt;head rend="h2"&gt;Want a heat pump in your own home?&lt;/head&gt;
    &lt;p&gt;The first Heat Pumped group buy generated lots of enthusiasm! There are still a handful of slots left, but you’ll have to act fast if you’re interested. Sign-ups close later this month (or when all the slots fill, whichever comes first).&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;Do you want to participate in this group buy?&lt;/head&gt;
          &lt;p&gt;Fair &amp;amp; transparent heat pump pricing in the SF Bay Area and LA&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.heatpumped.org/p/harnessing-america-s-heat-pump-moment"/><published>2025-10-24T20:05:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45698570</id><title>The Swift SDK for Android</title><updated>2025-10-25T11:07:28.658181+00:00</updated><content>&lt;doc fingerprint="360e51139294ee0b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing the Swift SDK for Android&lt;/head&gt;
    &lt;p&gt;Swift has matured significantly over the past decade — extending from cloud services to Windows applications, browser apps, and microcontrollers. Swift powers apps and services of all kinds, and thanks to its great interoperability, you can share code across platforms.&lt;/p&gt;
    &lt;p&gt;The Android workgroup is an open group, free for anyone to join, that aims to expand Swift to Android. Today, we are pleased to announce nightly preview releases of the Swift SDK for Android.&lt;/p&gt;
    &lt;p&gt;This milestone reflects months of effort by the Android workgroup, building on many years of grassroots community effort. With the SDK, developers can begin developing Android applications in Swift, opening new avenues for cross-platform development and accelerating innovation across the mobile ecosystem.&lt;/p&gt;
    &lt;p&gt;The Swift SDK for Android is available today, bundled with the Windows installer or downloadable separately for use on Linux or macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started&lt;/head&gt;
    &lt;p&gt;We’ve published a Getting Started guide to help you set up your first native Swift code on an Android device. The Swift for Android Examples help demonstrate end‑to‑end application workflows on Android.&lt;/p&gt;
    &lt;p&gt;With the Swift SDK for Android, you can now start porting your Swift packages to Android. Over 25% of packages in the Swift Package Index already build for Android, and the Community Showcase now indicates Android compatibility.&lt;/p&gt;
    &lt;p&gt;The swift-java project enables you to interoperate between Java and Swift. It is both a library and a code generator, enabling you to integrate Swift and Java in both directions by automatically generating safe and performant bindings. To learn about generating bindings to bring your business logic to Android, check out the recent Swift Server Side meetup talk by Mads Odgaard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;This preview release opens many new opportunities to continue improving these tools. We encourage you to share your experiences, ideas, tools and apps on the Swift forums. This post has been published on an associated thread for discussion, and new posts can be shared in the Android category.&lt;/p&gt;
    &lt;p&gt;The Android workgroup is drafting a vision document, currently under review, for directing future work regarding Swift on Android. This vision will outline priority areas and guide community efforts to maximize impact across the ecosystem. In addition, we maintain a project board that tracks the status of major efforts, as well as official CI for the Swift SDK for Android.&lt;/p&gt;
    &lt;p&gt;If you’re as excited as we are, join us and help make this ecosystem even better!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.swift.org/blog/nightly-swift-sdk-for-android/"/><published>2025-10-24T20:06:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45698909</id><title>Study: MRI contrast agent causes harmful metal buildup in some patients</title><updated>2025-10-25T11:07:28.472910+00:00</updated><content>&lt;doc fingerprint="eeda31d2584a08f1"&gt;
  &lt;main&gt;&lt;p&gt;Editor's Note&lt;/p&gt;&lt;p&gt;New research offers a potential explanation for why some patients retain toxic metals long after undergoing an MRI.&lt;/p&gt;&lt;p&gt;Published in the journal Magnetic Resonance Imaging, the findings show that gadolinium contrast agents used in MRI scans may react with common dietary compounds to form harmful metal nanoparticles in the body. As detailed in an April 7 Newsweek report on the study, gadolinium-based contrast agents are injected to sharpen MRI images and are typically excreted without causing harm. However, gadolinium particles have been found lingering in the brain, kidneys, blood, and urine years after exposure, and the US Food and Drug Administration links retained gadolinium to nephrogenic systemic fibrosis (NSF).&lt;/p&gt;&lt;p&gt;The study specifically identifies a chemical reaction between gadolinium and oxalic acid—a compound found naturally in foods and produced in the body after ingesting vitamin C—as a likely contributor, Newsweek reports. Lab tests showed oxalic acid caused gadolinium to separate from its chelating agent and form nanoparticles capable of infiltrating cells in various organs.&lt;/p&gt;&lt;p&gt;Lead author Dr Brent Wagner told Newsweek he personally avoids vitamin C when undergoing MRI with contrast, citing its potential to increase gadolinium reactivity. “Metabolic milieu,” including high oxalic acid levels, could explain why some individuals experience severe symptoms while others do not, he said.&lt;/p&gt;&lt;p&gt;According to the article, nearly half of the patients found to have gadolinium traces in the body had received the contrast agent only once, suggesting that individual biology—not dosage—may influence risk. Dr Wagner theorized that nanoparticle formation could trigger a disproportionate immune response, with affected cells sending distress signals that intensify the body’s reaction.&lt;/p&gt;&lt;p&gt;The research team is now building an international patient registry to further study gadolinium accumulation. According to the article, the registry will collect blood, urine, hair, and fingernail samples to help identify individuals at greatest risk and understand long-term retention patterns.&lt;/p&gt;Read More &amp;gt;&amp;gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ormanager.com/briefs/study-mri-contrast-agent-causes-harmful-metal-buildup-in-some-patients/"/><published>2025-10-24T20:48:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45700663</id><title>What Is Intelligence? (2024)</title><updated>2025-10-25T11:07:28.240199+00:00</updated><content>&lt;doc fingerprint="9bbe08afcd469a02"&gt;
  &lt;main&gt;
    &lt;p&gt;Contents Close What is Intelligence? Foreword Preface Introduction Origins Abiogenesis Symbiogenesis Reproductive Functions Life as Computation Artificial Life Thermodynamics Dynamic Stability Complexification Virality Compression Embodiment Daisyworld Élan Vital Survival Being in Time Batting Average (No) Things in Themselves Anthropic Principle The Umwelt Within Latent Variables Modeling Learning by Evolving Cause by Effect Goodness and Truth Are Feelings Real? Interlude The Prehistory of Computation Cybernetics Love and War Killer App Behavior, Purpose, and Teleology Negative Feedback How We Know Universals Perceptrons Deep Learning Closing the Loop Learning Unkneading Transfer Green Screen Grandmother Cell Final Causes Meathead Neuromodulators Bootstrapping Beyond Reward Other Minds Forking Paths Children of Time Sphexish Matryoshka Dolls Intelligence Explosion Crew of Eight Homunculus Illusion and Reality Many Worlds Au Revoir Will What You Will What It Is Like to Be Weird Entanglement Zombie-Free Alters M-I-B The Interpreter Multifractal Boundaries Ourselves Block Diagram Recurrence Efference Copy Phenomenality Blindsight Subbasement Neocortex Social Neuroscience Transformers Language Sequence to Sequence Prediction Is All You Need Semantic Cosmology Alignment Attention But Is It Neuroscience? No Introspection Step by Step Generality Single System Hive Mind Modalities Pure Speech Babel Fish Testament Long Tails In-Context Learning Mary’s Room Parity Check As If Interlude No Perfect Heroes or Villains Evolutionary Transition Periodization Transitions Vulnerability Pecking Order Economics X-Risk Free Lunch Utility Big Tent Limits to Growth Tears of Joy Beyond Alignment Acknowledgments About the Author The Antikythera Book Series Glossary Bibliography Foreword Blaise Agüera y Arcas Lessons from AI About Evolution, Computing, and Minds ' Foreword&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://whatisintelligence.antikythera.org/"/><published>2025-10-25T01:21:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45700911</id><title>Advice for New Principal Tech ICs (I.e., Notes to Myself)</title><updated>2025-10-25T11:07:28.031337+00:00</updated><content>&lt;doc fingerprint="a757901728e5d63e"&gt;
  &lt;main&gt;
    &lt;p&gt;What makes an effective principal engineer or scientist? Here, I’ve distilled what I’ve observed from role models and quoted some of their advice below. While my perspective is naturally Amazon-centric, these ideas should also apply to most principal tech IC roles. As always, use your best judgment and assess if this advice applies to you and your situation.&lt;/p&gt;
    &lt;p&gt;1. Different principals will have different flavors. Some dive deep in one space while others excel at horizontal influence. Some are technical trailblazers who show how things are done, while others clarify complexity and illuminate the path forward. Still others are masters at aligning multiple orgs towards a common vision. No one flavor is more important than the other, and you need to find the flavor that plays to your strengths.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Amazon very specifically says they want their principals hands on. Any principal who’s not being hands-on for an extended period of time (okay in short bursts) is likely setting themselves up for failure.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;2. The work that was core, and made you successful in your previous role, is now the side task. At this level, writing the code yourself may not be the best use of your time. While you should still be writing code (to stay connected to the work), your core role is now technical vision, design feedback, sponsorship, providing business, product, and technical context, finding new problems, connecting the dots, etc.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“This is a well-known saying for L7+ roles. It is not saying that you should spend less time coding, but that even if you are still coding 80% of the time, the most impactful part of your role is how to make everyone more effective. The mindset shifts from focusing solely on coding for a single project or perfecting on your own to influencing how all builders can build better across projects. This can be through contributing high-quality code to the repository and letting the code speak for itself, but also through other efforts—arguably more effective—such as giving feedback on design proposals, writing technical guidance, and driving the long-term vision.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;3. You’re kind of a part-time PM for technology. Scratch that, you’re part-time everything: product, design, engineering, science, quality assurance, hiring, finance, culture, etc. Nothing is not your job. The high judgment you have allows you to step out of your wheelhouse.&lt;/p&gt;
    &lt;p&gt;4. Your role will involve more communication, influence, and connecting the right people. Your projects will typically be larger in scope, involving teams across directors and even VPs. These projects won’t succeed without effective alignment and collaboration, and you’ll want to be careful not to ship your org chart to customers.&lt;/p&gt;
    &lt;p&gt;5. Being right is less than half the battle. You also have to convince others that you’re right, and more importantly, convince them to care enough to act on it. This means figuring out how to build momentum, who can sponsor the idea, and how to get it over the finish line.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Sometimes it also means starting the project and showing value to gain these things. Principals should lead by example and we want people to be proactive in solving issues and not wait for committee.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;“I would almost say that principals are the committee, so who are you waiting for :)”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;6. A big part of the work is teaching the org to value something it doesn’t care about. Your audience will range from executives to working-level ICs. This is some of the hardest work you can do, and it fails often, but as someone who has an eye on the future and broader view, you should still do it. A mentor shared that for every 10 docs he pitched, probably three would get acted on, and he considered that a great outcome.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Compared to ICs who shoulder more of the day-to-day, team-level, and immediate deliverables, L7+ roles have more space to step back and take a broader, longer-term view of the company. It allows them to evaluate impact more objectively, and contributing at that level is where they add the most value—connecting to #7 below, those are the things that won’t happen without them.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;7. There’s a category of work that just won’t happen without you. It’s usually at the intersection of what you really care about and what you’re exceptional at. It could be building a quick prototype and socializing a new customer experience with leadership, building bridges across orgs or other practitioners in industry, or crafting the three-year vision. Focus on this category of work. Also, it’ll get deeper and narrower over the span of your career.&lt;/p&gt;
    &lt;p&gt;8. Sometimes, the most valuable thing you can do is not even to do the work but to connect the dots. This includes connecting teams who’re looking to do the work to teams who’ve done the work that they can reuse or learn from. It also includes identifying someone suitable who can do the work and grow along the way.&lt;/p&gt;
    &lt;p&gt;9. You can only do so much yourself. You’ll be more useful to the org spending part of the time coaching and mentoring others to be more effective. Perhaps set aside a few hours each week, such as office hours or regular syncs with folks you’re grooming. Identify one or two ICs you’d like to groom and set goals for yourself on how you’ll help them.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Scaling through others is the key point here. I like to think the success of a PE is when the org is able to make the same decisions as the PE would. Then the PE moves on to other ambiguous problems and set the culture to achieve the right outcomes.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;“While you will want to help everyone, however you have to focus your energy on building others who can take your place long term. Not everyone has that ability so you have to spend focused time on those that you see demonstrating the potential to take over from you and they can in turn help others that show less potential at the moment and enable them.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;10. Transition from involving others in the work to making the work theirs. Make it an opportunity for someone to do the work that got you to where you are. Support and set them up for success. Don’t worry, there’s a never-ending backlog of important problems and interesting opportunities to work on for customers.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“I tell folks that they should spend 1-2 hours a week with someone to scale them to achieve 40 hours of work under your insights. This is what the true Principal scale is of being able to find those small things that enable a Sr SDE to be even better.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;11. When you give others the work, it is now theirs. You can provide context and guidance, but ultimately, the direction is theirs to set. This includes letting them take an approach you wouldn’t take. If it goes poorly, we all get to learn from it; if it goes well, you get to learn something from it. Nonetheless, you should step in if the project is walking through a high-risk, one-way door that could backfire.&lt;/p&gt;
    &lt;p&gt;12. Create space for others in meetings. Sometimes, the room looks to the most senior person for their opinion or decision; you can create space for others by asking questions instead. Also, if you see someone who’s not participating, gently pull them in on topics that play to their strengths. And if the meeting forgot someone who should have been in the discussion, add them to the next occurrence.&lt;/p&gt;
    &lt;p&gt;13. You don’t always have to demonstrate value. Some of the most effective principals I’ve worked with go through an entire meeting silent, or barely leave comments during document reviews. If the team and discussion are going fine as it is, that’s great! It means you can take a step back from the workstream and focus your energies elsewhere.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Be careful on the other side of this: If you’re present and staying silent, you have some implicit approval. Beware of multi-tasking and what your presence means.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;14. In meetings with execs, it’s okay not to address every topic on the agenda. If they’re engaged in the topic, ask meaningful questions, and make the decisions or unblock the obstacles that only they can, that’s a good meeting.&lt;/p&gt;
    &lt;p&gt;15. Beware: If you work in a breadth role, your entire week can be filled with everything that comes at you. This includes reviews, escalations, emails, help needed, etc. This can become a bigger issue the longer you stay at an org, where you become the “go-to” person because you know so much or have earned that credibility. As a result, you’re now a “mandatory” attendee at every meeting. Learn to push back and guard your time, or else you’ll have no time to push for the ideas you really care about. You don’t need to be in every meeting or have an opinion on every idea, just the key ones that matter.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“One thing that a mentor shared with me is that you need time to think. If you are going from meeting to meeting, you can’t process things and can’t look ahead. You need to schedule quality thinking time and disconnect from meetings to really find that next big thing.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;“You need to look to delegate—set someone else up to be that person so that you can free up your time. This gives you benefits in freeing you up, but also helps put aside scope for someone new to help them grow.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;16. If you can’t explain why what you’re working on needs a principal, you might be working on the wrong thing. (This may also apply to L6s.)&lt;/p&gt;
    &lt;p&gt;17. Because of your position, you can sometimes improve outcomes with relatively low effort. The title grants you organizational privilege, and thus greater access to relationships and context. Combined with your experience, this allows you to see around corners better. Thus, you can meaningfully improve a project’s chances of success and outcomes by investing fairly little effort. This is high ROI, and when you spot such opportunities, act on them.&lt;/p&gt;
    &lt;p&gt;18. The title comes with an aura of credibility even when you shouldn’t have it. Sometimes, people read more into your offhand comments than they should, especially if they don’t know you well. As a result, they may do a lot of work because of a casual comment you made. This can be a waste of time and effort. Thus, be clear on what you do know, what you don’t know, what you’re asking for, and what you’re simply commenting on.&lt;/p&gt;
    &lt;p&gt;19. Don’t just say the “what”; also share the “why” you think so. This helps others make better decisions. It also reduces the chance that others say “Principal &amp;lt;NAME&amp;gt; said this and thus we should …” and parrot things you said without fully understanding why you said it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“The kinds of problems that require L7 input usually involve decisions under significant uncertainty. What’s crucial—but also difficult—is articulating your mental model: How you arrive at a judgment without having all the information, and why certain pieces of knowledge are more important than other data points.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;20. Find mechanisms to stay in touch with teams. This can be design reviews, weekly demos, sitting nearby and keeping an ear out, team lunches, or casual hallway chats. This helps you keep a pulse on the org and where the key problems or opportunities are.&lt;/p&gt;
    &lt;p&gt;21. Help teams keep sight of the bigger picture. When the working level is focused on the thick of things and day-to-day delivery, they can sometimes lose sight of the bigger, longer-term problems/opportunities and get stuck in the local optima. With the context that you have, you can help remind teams of this.&lt;/p&gt;
    &lt;p&gt;22. Be pragmatic; balance seeing the big picture with accepting local solutions. Consult and listen to the working level on the details; they’re your on-the-ground experts.&lt;/p&gt;
    &lt;p&gt;23. You’ll be asked for reviews or promo feedback on someone that you’ve only spent an hour or two with. It’s okay to decline instead of providing poor-quality feedback that’s based on a tiny sample of their entire behavior.&lt;/p&gt;
    &lt;p&gt;24. Make time to interact with interns and their mentors. A few touch points during the internship can be transformative, including an early check-in (and course correction if necessary) and being there for demo day. Also, work with the mentors and interns towards deliverables that continue to be valuable beyond the internship, where others can extend the project. This includes product 1-pagers, working software, and technical documentation.&lt;/p&gt;
    &lt;p&gt;25. To get to principal, you need to put yourself on the critical path. To be effective as a principal and go beyond it, you need to actively remove yourself from it. While you were previously the “go-to” person, you want to transition from essential to adjacent. The org should increasingly benefit from you, but shouldn’t be dependent on you to be effective. Think about how you can empower others to make the contributions you’re making.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Be careful about injecting yourself in critical path projects. Your focus is a lot more prone to being stolen by some other priority, so you need to keep yourself out of the critical path or be really stringent about locking down if you are in the critical path.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;26. If you were promoted to principal, it’s because you’ve been acting as a principal for a while. Typically, for more than a year. Thus, don’t worry about the increased expectation of the title. Just keep doing what you’re doing, engage with other principals, figure out your style, and work with your leadership to identify your focus areas.&lt;/p&gt;
    &lt;p&gt;27. With great freedom comes great responsibility. You have the autonomy to choose what to work on, but there’s the expectation of accountability and impact. The freedom isn’t about doing what you want, but ownership of finding the highest leverage problems to solve. Don’t expect to be told what to do or be given any guidance. You’re expected to figure out what the org should focus on.&lt;/p&gt;
    &lt;p&gt;28. Define and align your charter with your leadership. One way is to split your work into three buckets: (i) owner, (ii) sponsor, (iii) consultant. As a consultant, you’re involved in reviews and provide guidance, and have a high-level understanding of the system’s or product’s intent. As a sponsor, in addition to the above, you make the idea a priority for the org, work to build alignment and drive decisions, and engage with stakeholders. As an owner, it’s everything above, plus being the system expert and first point of contact, and having borderline obsession with the success of the design, execution, and impact. I tend to own 1 - 2 projects (&amp;gt; 50% time), sponsor 2 - 3 projects (~20% time), and consult the rest of my time.&lt;/p&gt;
    &lt;p&gt;29. Being a principal can be lonely. You’re part of all teams but also part of none. Build a network of peers with whom you can have open conversations. It likely doesn’t matter if you’re working in the same company or domain.&lt;/p&gt;
    &lt;p&gt;30. Don’t neglect your own needs. Make time and space for projects that support your learning, growth, and wellbeing. While it can feel selfish in the short term, it’s far more preferable to you burning out on the org. If you’re actively looking for work that keeps you healthy and happy and growing, your org benefits too, and it’s easier for them to retain you. Work with your manager on how to balance this.&lt;/p&gt;
    &lt;p&gt;31. Keep learning; our industry moves fast. If you take on projects that teach you nothing, or at least nothing relevant to your work, you’re moving backwards. This is sometimes inevitable—timebox such projects when they come along. Also, your learning doesn’t have to solely come from the job. I know PEs who find time to read papers and technical textbooks, and hack on prototypes over weekends to better understand new ideas and technology.&lt;/p&gt;
    &lt;p&gt;What other advice have you come across on how to be an effective principal engineer or scientist? Please share in the comments below or DM me! 🙏&lt;/p&gt;
    &lt;p&gt;Thanks to Brian K, Tim L, Yiwen O, Prannoy C, Aman A, Dennis T, and others for reading an early draft and providing feedback. And thanks to my mentor and role model, David S.&lt;/p&gt;
    &lt;p&gt;If you found this useful, please cite this write-up as:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Yan, Ziyou. (Oct 2025). Advice for New Principal Tech ICs (i.e., Notes to Myself). eugeneyan.com. https://eugeneyan.com/writing/principal/.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;@article{yan2025principal,
  title   = {Advice for New Principal Tech ICs (i.e., Notes to Myself)},
  author  = {Yan, Ziyou},
  journal = {eugeneyan.com},
  year    = {2025},
  month   = {Oct},
  url     = {https://eugeneyan.com/writing/principal/}
}&lt;/code&gt;
    &lt;p&gt;Join 11,800+ readers getting updates on machine learning, RecSys, LLMs, and engineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eugeneyan.com/writing/principal/"/><published>2025-10-25T02:24:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45700946</id><title>Key IOCs for Pegasus and Predator Spyware Removed with iOS 26 Update</title><updated>2025-10-25T11:07:27.893943+00:00</updated><content>&lt;doc fingerprint="f60d4907cab1114d"&gt;
  &lt;main&gt;
    &lt;p&gt;Blog&lt;/p&gt;
    &lt;head rend="h1"&gt;Key IOCs for Pegasus and Predator Spyware Cleaned With iOS 26 Update&lt;/head&gt;
    &lt;p&gt;By Matthias Frielingsdorf, VP of Research&lt;/p&gt;
    &lt;p&gt;Oct 21, 2025&lt;/p&gt;
    &lt;p&gt;As iOS 26 is being rolled out, our team noticed a particular change in how the operating system handles the shutdown.log file: it effectively erases crucial evidence of Pegasus and Predator spyware infections. This development poses a serious challenge for forensic investigators and individuals seeking to determine if their devices have been compromised at a time when spyware attacks are becoming more common.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;The Power of the shutdown.log&lt;/head&gt;
    &lt;p&gt;For years, the shutdown.log file has been an invaluable, yet often overlooked, artifact in the detection of iOS malware. Located within the Sysdiagnoses in the Unified Logs section (specifically, Sysdiagnose Folder -&amp;gt; system_logs.logarchive -&amp;gt; Extra -&amp;gt; shutdown.log), it has served as a silent witness to the activities occurring on an iOS device, even during its shutdown sequence.&lt;/p&gt;
    &lt;p&gt;In 2021, the publicly known version of Pegasus spyware was found to leave discernible traces within this shutdown.log. These traces provided a critical indicator of compromise, allowing security researchers to identify infected devices. However, the developers behind Pegasus, NSO Group, are constantly refining their techniques, and by 2022 Pegasus had evolved.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Pegasus's Evolving Evasion Tactics&lt;/head&gt;
    &lt;p&gt;While still leaving evidence in the shutdown.log, their methods became more sophisticated. Instead of leaving obvious entries, they began to completely wipe the shutdown.log file. Yet, even with this attempted erasure, their own processes still left behind subtle traces. This meant that even a seemingly clean shutdown.log that began with evidence of a Pegasus sample was, in itself, an indicator of compromise. Multiple cases of this behavior were observed until the end of 2022, highlighting the continuous adaptation of these malicious actors.&lt;/p&gt;
    &lt;p&gt;Following this period, it is believed that Pegasus developers implemented even more robust wiping mechanisms, likely monitoring device shutdown to ensure a thorough eradication of their presence from the shutdown.log. Researchers have noted instances where devices known to be active had their shutdown.log cleared, alongside other IOCs for Pegasus infections. This led to the conclusion that a cleared shutdown.log could serve as a good heuristic for identifying suspicious devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Predator's Similar Footprint&lt;/head&gt;
    &lt;p&gt;The sophisticated Predator spyware, observed in 2023, also appears to have learned from the past. Given that Predator was actively monitoring the shutdown.log, and considering the similar behavior seen in earlier Pegasus samples, it is highly probable that Predator, too, left traces within this critical log file.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;iOS 26: An Unintended Cleanse&lt;/head&gt;
    &lt;p&gt;With iOS 26 Apple introduced a changeâeither an intentional design decision or an unforeseen bugâthat causes the shutdown.log to be overwritten on every device reboot instead of appended with a new entry every time, preserving each as its own snapshot. This means that any user who updates to iOS 26 and subsequently restarts their device will inadvertently erase all evidence of older Pegasus and Predator detections that might have been present in their shutdown.log.&lt;/p&gt;
    &lt;p&gt;This automatic overwriting, while potentially intended for system hygiene or performance, effectively sanitizes the very forensic artifact that has been instrumental in identifying these sophisticated threats. It could hardly come at a worse time - spyware attacks have been a constant in the news and recent headlines show that high-power executives and celebrities, not just civil society, are being targeted.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Identifying Pegasus 2022: A Specific IOC&lt;/head&gt;
    &lt;p&gt;For those still on iOS versions prior to 26, a specific IOC for Pegasus 2022 infections involved the presence of a /private/var/db/com.apple.xpc.roleaccountd.staging/com.apple.WebKit.Networking entry within the shutdown.log. This particular IOC also revealed a significant shift in NSO Group's tactics: they began using normal system process names instead of easily identifiable, similarly named processes, making detection more challenging.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Correlating Logs for Deeper Insight (&amp;lt; iOS 18)&lt;/head&gt;
    &lt;p&gt;For devices running iOS 18 or earlier, a more comprehensive approach to detection involved correlating containermanagerd log entries with shutdown.log events. Containermanagerd logs contain boot events and can retain data for several weeks. By comparing these boot events with shutdown.log entries, investigators could identify discrepancies. For example, if numerous boot events were observed before shutdown.log entries, it suggested that something was amiss and potentially being hidden.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Before You Update&lt;/head&gt;
    &lt;p&gt;Given the implications of iOS 26's shutdown.log handling, it is crucial for users to take proactive steps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Before updating to iOS 26, immediately take and save a sysdiagnose of your device. This will preserve your current shutdown.log and any potential evidence it may contain.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Consider holding off on updating to iOS 26 until Apple addresses this issue, ideally by releasing a bug fix that prevents the overwriting of the shutdown.log on boot.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;More Blogs&lt;/head&gt;
    &lt;head rend="h3"&gt;Get Our Latest Blog Posts Delivered Straight to Your Inbox&lt;/head&gt;
    &lt;p&gt;Subscribe to our blog to receive the latest research and industry trends delivered straight to your inbox. Our blog content covers sophisticated mobile threats, unpatched vulnerabilities, smishing, and the latest industry news to keep you informed and secure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://iverify.io/blog/key-iocs-for-pegasus-and-predator-spyware-cleaned-with-ios-26-update"/><published>2025-10-25T02:31:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45701305</id><title>Meet the real screen addicts: the elderly</title><updated>2025-10-25T11:07:27.792035+00:00</updated><content/><link href="https://www.economist.com/international/2025/10/23/meet-the-real-screen-addicts-the-elderly"/><published>2025-10-25T04:09:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45701404</id><title>Mistakes I see engineers making in their code reviews</title><updated>2025-10-25T11:07:27.612020+00:00</updated><content>&lt;doc fingerprint="efcc91752d045dab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mistakes I see engineers making in their code reviews&lt;/head&gt;
    &lt;p&gt;In the last two years, code review has gotten much more important. Code is now easy to generate using LLMs, but it’s still just as hard to review1. Many software engineers now spend as much (or more) time reviewing the output of their own AI tools than their colleagues’ code.&lt;/p&gt;
    &lt;p&gt;I think a lot of engineers don’t do code review correctly. Of course, there are lots of different ways to do code review, so this is largely a statement of my engineering taste.&lt;/p&gt;
    &lt;head rend="h3"&gt;Don’t just review the diff&lt;/head&gt;
    &lt;p&gt;The biggest mistake I see is doing a review that focuses solely on the diff2. Most of the highest-impact code review comments have very little to do with the diff at all, but instead come from your understanding of the rest of the system.&lt;/p&gt;
    &lt;p&gt;For instance, one of the most straightforwardly useful comments is “you don’t have to add this method here, since it already exists in this other place”. The diff itself won’t help you produce a comment like this. You have to already be familiar with other parts of the codebase that the diff author doesn’t know about.&lt;/p&gt;
    &lt;p&gt;Likewise, comments like “this code should probably live in this other file” are very helpful for maintaining the long-term quality of a codebase. The cardinal value when working in large codebases is consistency (I write about this more in Mistakes engineers make in large established codebases). Of course, you cannot judge consistency from the diff alone.&lt;/p&gt;
    &lt;p&gt;Reviewing the diff by itself is much easier than considering how it fits into the codebase as a whole. You can rapidly skim a diff and leave line comments (like “rename this variable” or “this function should flow differently”). Those comments might even be useful! But you’ll miss out on a lot of value by only leaving this kind of review.&lt;/p&gt;
    &lt;head rend="h3"&gt;Don’t leave too many comments&lt;/head&gt;
    &lt;p&gt;Probably my most controversial belief about code review is that a good code review shouldn’t contain more than five or six comments. Most engineers leave too many comments. When you receive a review with a hundred comments, it’s very hard to engage with that review on anything other than a trivial level. Any really important comments get lost in the noise2.5.&lt;/p&gt;
    &lt;p&gt;What do you do when there are twenty places in the diff that you’d like to see updated - for instance, twenty instances of &lt;code&gt;camelCase&lt;/code&gt; variables instead of &lt;code&gt;snake_case&lt;/code&gt;? Instead of leaving twenty comments, I’d suggest leaving a single comment explaining the stylistic change you’d like to make, and asking the engineer you’re reviewing to make the correct line-level changes themselves.&lt;/p&gt;
    &lt;p&gt;There’s at least one exception to this rule. When you’re onboarding a new engineer to the team, it can be helpful to leave a flurry of stylistic comments to help them understand the specific dialect that your team uses in this codebase. But even in this case, you should bear in mind that any “real” comments you leave are likely to be buried by these other comments. You may still be better off leaving a general “we don’t do early returns in this codebase” comment than leaving a line comment on every single early return in the diff.&lt;/p&gt;
    &lt;head rend="h3"&gt;Don’t review with a “how would I write it?” filter&lt;/head&gt;
    &lt;p&gt;One reason engineers leave too many comments is that they review code like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Look at a hunk of the diff&lt;/item&gt;
      &lt;item&gt;Ask themselves “how would I write this, if I were writing this code?”&lt;/item&gt;
      &lt;item&gt;Leave a comment with each difference between how they would write it and the actual diff&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a good way to end up with hundreds of comments on a pull request: an endless stream of “I would have done these two operations in a different order”, or “I would have factored this function slightly differently”, and so on.&lt;/p&gt;
    &lt;p&gt;I’m not saying that these minor comments are always bad. Sometimes the order of operations really does matter, or functions really are factored badly. But one of my strongest opinions about software engineering is that there are multiple acceptable approaches to any software problem, and that which one you choose often comes down to taste.&lt;/p&gt;
    &lt;p&gt;As a reviewer, when you come across cases where you would have done it differently, you must be able to approve those cases without comment, so long as either way is acceptable. Otherwise you’re putting your colleagues in an awkward position. They can either accept all your comments to avoid conflict, adding needless time and setting you up as the de facto gatekeeper for all changes to the codebase, or they can push back and argue on each trivial point, which will take even more time. Code review is not the time for you to impose your personal taste on a colleague.&lt;/p&gt;
    &lt;head rend="h3"&gt;If you do not want a change to be merged, leave a blocking review&lt;/head&gt;
    &lt;p&gt;So far I’ve only talked about review comments. But the “high-order bit” of a code review is not the content of the comments, but the status of the review: whether it’s an approval, just a set of comments, or a blocking review. The status of the review colors all the comments in the review. Comments in an approval read like “this is great, just some tweaks if you want”. Comments in a blocking review read like “here’s why I don’t want you to merge this in”.&lt;/p&gt;
    &lt;p&gt;If you want to block, leave a blocking review. Many engineers seem to think it’s rude to leave a blocking review even if they see big problems, so they instead just leave comments describing the problems. Don’t do this. It creates a culture where nobody is sure whether it’s okay to merge their change or not. An approval should mean “I’m happy for you to merge, even if you ignore my comments”. Just leaving comments should mean “I’m happy for you to merge if someone else approves, even if you ignore my comments.” If you would be upset if a change were merged, you should leave a blocking review on it. That way the person writing the change knows for sure whether they can merge or not, and they don’t have to go and chase up everyone who’s left a comment to get their informal approval.&lt;/p&gt;
    &lt;head rend="h3"&gt;Most reviews should be approvals&lt;/head&gt;
    &lt;p&gt;I should start with a caveat: this depends a lot on what kind of codebase we’re talking about. For instance, I think it’s fine if PRs against something like SQLite get mostly blocking reviews. But a standard SaaS codebase, where teams are actively developing new features, ought to have mostly approvals. I go into a lot more detail about the distinction between these two types of codebase in Pure and Impure Engineering.&lt;/p&gt;
    &lt;p&gt;If tons of PRs are being blocked, it’s usually a sign that there’s too much gatekeeping going on. One dynamic I’ve seen play out a lot is where one team owns a bottleneck for many other teams’ features - for instance, maybe they own the edge network configuration where new public-facing routes must be defined, or the database structure that new features will need to modify. That team is typically more reliability-focused than a typical feature team. Engineers on that team may have a different title, like SRE, or even belong to a different organization. Their incentives are thus misaligned with the feature teams they’re nominally supporting.&lt;/p&gt;
    &lt;p&gt;Suppose the feature team wants to update the public-facing ingress routes in order to ship some important project. But the edge networking team doesn’t care about that project - it doesn’t affect their or their boss’s review cycles. What does affect their reviews is any production problem the change might cause. That means they’re motivated to block any potentially-risky change for as long as possible. This can be very frustrating for the feature team, who is willing to accept some amount of risk for the sake of delivering new features3.&lt;/p&gt;
    &lt;p&gt;Of course, there are other reasons why many PRs might be getting blocking reviews. Maybe the company just hired a bunch of incompetent engineers, who ought to be prevented from merging their changes. Maybe the company has had a recent high-profile incident, and all risky changes should be blocked for a couple of weeks until their users forget about it. But in normal circumstances, a high rate of blocked reviews represents a structural problem.&lt;/p&gt;
    &lt;p&gt;For many engineers - including me - it feels good to leave a blocking review, for the same reasons that it feels good to gatekeep in general. It feels like you’re single-handedly protecting the quality of the codebase, or averting some production incident. It’s also a way to indulge a common vice among engineers: flexing your own technical knowledge on some less-competent engineer. Oh, looks like you didn’t know that your code would have caused an N+1 query! Well, I knew about it. Aren’t you lucky I took the time to read through your code?&lt;/p&gt;
    &lt;p&gt;This principle - that you should bias towards approving changes - is important enough that Google’s own guide to code review begins with it, calling it ”the senior principle among all of the code review guidelines”4.&lt;/p&gt;
    &lt;head rend="h3"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;I’m quite confident that many competent engineers will disagree with most or all of the points in this post. That’s fine! I also believe many obviously true things about code review, but I didn’t include them here.&lt;/p&gt;
    &lt;p&gt;In my experience, it’s a good idea to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consider what code isn’t being written in the PR instead of just reviewing the diff&lt;/item&gt;
      &lt;item&gt;Leave a small number of well-thought-out comments, instead of dashing off line comments as you go and ending up with a hundred of them&lt;/item&gt;
      &lt;item&gt;Review with a “will this work” filter, not with a “is this exactly how I would have done it” filter&lt;/item&gt;
      &lt;item&gt;If you don’t want the change to be merged, leave a blocking review&lt;/item&gt;
      &lt;item&gt;Unless there are very serious problems, approve the change&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This all more or less applies to reviewing code from agentic LLM systems. They are particularly prone to missing code that they ought to be writing, they also get a bit lost if you feed them a hundred comments at once, and they have their own style. The one point that does not apply to LLMs is the “bias towards approving” point. You can and should gatekeep AI-generated PRs as much as you want.&lt;/p&gt;
    &lt;p&gt;I do want to close by saying that there are many different ways to do code review. Here’s a non-exhaustive set of values that a code review practice might be trying to satisfy: making sure multiple people on the team are familiar with every part of the codebase, letting the team discuss the software design of each change, catching subtle bugs that a single person might not see, transmitting knowledge horizontally across the team, increasing perceived ownership of each change, enforcing code style and format rules across the codebase, and satisfying SOC2 “no one person can change the system alone” constraints. I’ve listed these in the order I care about them, but engineers who would order these differently will have a very different approach to code review.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Of course there are LLM-based reviewing tools. They’re even pretty useful! But at least right now they’re not as good as human reviewers, because they can’t bring to bear the amount of general context that a competent human engineer can.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For readers who aren’t software engineers, “diff” here means the difference between the existing code and the proposed new code, showing what lines are deleted, added, or edited.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This is a special instance of a general truth about communication: if you tell someone one thing, they’ll likely remember it; if you tell them twenty things, they will probably forget it all.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In the end, these impasses are typically resolved by the feature team complaining to their director or VP, who complains to the edge networking team’s director or VP, who tells them to just unblock the damn change already. But this is a pretty crude way to resolve the incentive mismatch, and it only really works for features that are high-profile enough to receive air cover from a very senior manager.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Google’s principle is much more explicit, stating that you should approve a change if it’s even a minor improvement, not when it’s perfect. But I take the underlying message here to be “I know it feels good, but don’t be a nitpicky gatekeeper - approve the damn PR!”&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.&lt;/p&gt;
    &lt;p&gt;October 25, 2025 │ Tags: good engineers, software design, explainers, ai&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.seangoedecke.com/good-code-reviews/"/><published>2025-10-25T04:42:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45701607</id><title>Fast TypeScript (Code Complexity) Analyzer</title><updated>2025-10-25T11:07:27.473974+00:00</updated><content>&lt;doc fingerprint="30e7fd6d7122189e"&gt;
  &lt;main&gt;
    &lt;p&gt;Fast TypeScript Analyzer&lt;/p&gt;
    &lt;p&gt;FTA (Fast TypeScript Analyzer) is a super-fast TypeScript static analysis tool written in Rust. It captures static information about TypeScript code and generates easy-to-understand analytics that tell you about complexity and maintainability issues that you may want to address.&lt;/p&gt;
    &lt;p&gt;FTA uses swc (opens in a new tab) to parse your code then runs various analytical routines against it to understand how complex and maintainable it is likely to be. JavaScript code is also supported.&lt;/p&gt;
    &lt;p&gt;FTA is fast: on typical hardware, it can analyze up to 1600 files per second.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quickstart&lt;/head&gt;
    &lt;p&gt;There are several ways to use &lt;code&gt;fta&lt;/code&gt;. The simplest is to use &lt;code&gt;fta-cli&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;npx fta-cli path/to/project&lt;/code&gt;
    &lt;p&gt;Example output against the Redux project:&lt;/p&gt;
    &lt;code&gt;┌─────────────────────────────────────────┬────────────┬─────────────────────────────┬───────────────────┐
│ File                                    ┆ Num. lines ┆ FTA Score (Lower is better) ┆ Assessment        │
╞═════════════════════════════════════════╪════════════╪═════════════════════════════╪═══════════════════╡
│ website\src\pages\index.js              ┆ 212        ┆ 64.43                       ┆ Needs improvement │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\createStore.ts                      ┆ 255        ┆ 64.17                       ┆ Needs improvement │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\combineReducers.ts                  ┆ 162        ┆ 59.51                       ┆ Could be better   │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\compose.ts                          ┆ 36         ┆ 47.53                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\bindActionCreators.ts               ┆ 51         ┆ 47.14                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\utils\kindOf.ts                     ┆ 58         ┆ 46.88                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\utils\isPlainObject.ts              ┆ 8          ┆ 28.36                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\utils\symbol-observable.ts          ┆ 7          ┆ 27.61                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\utils\warning.ts                    ┆ 8          ┆ 26.81                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ website\docusaurus.config.js            ┆ 205        ┆ 18.19                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ website\sidebars.js                     ┆ 148        ┆ 15.82                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ rollup.config.js                        ┆ 71         ┆ 15.79                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ tsup.config.ts                          ┆ 63         ┆ 15.59                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\types\store.ts                      ┆ 63         ┆ 15.47                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\applyMiddleware.ts                  ┆ 55         ┆ 15.45                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ website\src\pages\errors.js             ┆ 58         ┆ 15.07                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\types\reducers.ts                   ┆ 49         ┆ 14.46                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ website\src\js\monokaiTheme.js          ┆ 62         ┆ 14.32                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\utils\actionTypes.ts                ┆ 8          ┆ 11.91                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\index.ts                            ┆ 37         ┆ 11.91                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\types\actions.ts                    ┆ 15         ┆ 10.27                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ src\types\middleware.ts                 ┆ 14         ┆ 10.16                       ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ vitest.config.ts                        ┆ 14         ┆ 9.92                        ┆ OK                │
├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤
│ docs\components\DetailedExplanation.jsx ┆ 14         ┆ 9.53                        ┆ OK                │
└─────────────────────────────────────────┴────────────┴─────────────────────────────┴───────────────────┘
24 files analyzed in 0.0372s.&lt;/code&gt;
    &lt;p&gt;For convenience, FTA generates a single FTA Score that serves as a general, overall indication of the quality of a particular TypeScript file.&lt;/p&gt;
    &lt;p&gt;That said, all metrics are exposed and it is up to users to decide how it's metrics can enhance productivity for your team.&lt;/p&gt;
    &lt;p&gt;The full metrics available for each file:&lt;/p&gt;
    &lt;code&gt;{
  "file_name": "combineReducers.ts",
  "cyclo": 28,
  "halstead": {
    "uniq_operators": 28,
    "uniq_operands": 67,
    "total_operators": 271,
    "total_operands": 239,
    "program_length": 95,
    "vocabulary_size": 510,
    "volume": 854.4635765015915,
    "difficulty": 37.84518828451883,
    "effort": 32337.33493496609,
    "time": 1796.5186074981161,
    "bugs": 0.2848211921671972
  },
  "line_count": 202,
  "fta_score": 61.61052634575169,
  "assessment": "(Needs improvement)"
}&lt;/code&gt;
    &lt;p&gt;You can also see how FTA analyzes individual files by using the Playground.&lt;/p&gt;
    &lt;p&gt;View the full docs to see all the possible ways to use FTA.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get Involved&lt;/head&gt;
    &lt;p&gt;FTA is completely open-source. Get involved by joining the discussion on the GitHub Repository (opens in a new tab).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ftaproject.dev/"/><published>2025-10-25T05:51:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45701825</id><title>Euro cops take down cybercrime network with 49M fake accounts</title><updated>2025-10-25T11:07:24.769524+00:00</updated><content>&lt;doc fingerprint="b16699d9694bf643"&gt;
  &lt;main&gt;
    &lt;p&gt;European police forces have arrested seven people and dismantled a large-scale cybercrime-as-a-service operation that saw almost 50 million fake online accounts created across social media and communications platforms for fraud purposes.&lt;/p&gt;
    &lt;p&gt;The coordinated takedown, codenamed Operation SIMCARTEL, took place on October 10 in Latvia, as part of a joint investigation by police in the Baltic nation, Austria, Estonia and Finland.&lt;/p&gt;
    &lt;p&gt;Five Latvian nationals and two additional suspects were arrrested by police.&lt;/p&gt;
    &lt;p&gt;In the raid, authorities seized 1200 SIM boxes, with the devices containing 40,000 active SIM cards.&lt;/p&gt;
    &lt;p&gt;Five Internet servers were taken down, along with two websites that had been offering the illegal service, gogetsms.com and apisim.com.&lt;/p&gt;
    &lt;p&gt;The network operated as a for-hire service, providing temporary telephone numbers from more than 80 countries to criminals who needed to mask their identities whilst committing cybercrimes.&lt;/p&gt;
    &lt;p&gt;Fraudsters used the service to bypass two-factor authentication systems so as to create vast numbers of fake accounts.&lt;/p&gt;
    &lt;p&gt;Once created, the bogus accounts served as starting points for various scams including investment fraud, fake online shops, and phishing attacks.&lt;/p&gt;
    &lt;p&gt;The infrastructure facilitated a range of offences including fraud, extortion, migrant smuggling and the distribution of child sexual abuse material.&lt;/p&gt;
    &lt;p&gt;Scammers employed tactics including the "daughter-son scam", where criminals persuaded victims that their child needed urgent financial help, alongside more traditional phishing and smishing attacks.&lt;/p&gt;
    &lt;p&gt;Some of the criminals specialised in fraud on second-hand marketplaces, whilst others set up fake investment websites and bogus online shops.&lt;/p&gt;
    &lt;p&gt;In other cases, criminals impersonated police officers using forged identification, personally collecting funds from victims.&lt;/p&gt;
    &lt;p&gt;Financial losses in Austria alone are said to be to around €4.5 million ($7.4 million), with an additional €420,000 ($693,000) lost in Latvia.&lt;/p&gt;
    &lt;p&gt;Around €431,000 ($711,000) was seized from the criminals' bank accounts, along with approximately $516,000 worth of crypto currency.&lt;/p&gt;
    &lt;p&gt;Police investigators working with Europol and Eurojust, attributed over 1700 individual cyber fraud cases in Austria and 1500 in Latvia to the criminal network.&lt;/p&gt;
    &lt;p&gt;Europol provided analytical support, open-source intelligence analysis for mapping the online criminal service, and forensic expertise to secure digital evidence.&lt;/p&gt;
    &lt;p&gt;The agency worked with the not-for-profit Shadowserver Foundation security organisation to dismantle the technical infrastructure.&lt;/p&gt;
    &lt;p&gt;In September this year, United States law enforcement busted another "SIM farm", with 300 devices and with over 100,000 subscriber identity cards found on the site near the United Nations headquarters in New York City.&lt;/p&gt;
    &lt;p&gt;The US Secret Service was involved in the raid in New York, and suspected nation-state threat actors were operating the SIM farm.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.itnews.com.au/news/euro-cops-take-down-cybercrime-network-with-49-million-fake-accounts-621174"/><published>2025-10-25T06:48:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45702736</id><title>The Missing Semester of Your CS Education (2020)</title><updated>2025-10-25T11:07:24.627488+00:00</updated><content>&lt;doc fingerprint="bf70119a98e1a6a3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Missing Semester of Your CS Education&lt;/head&gt;
    &lt;p&gt;Classes teach you all about advanced topics within CS, from operating systems to machine learning, but there’s one critical subject that’s rarely covered, and is instead left to students to figure out on their own: proficiency with their tools. We’ll teach you how to master the command-line, use a powerful text editor, use fancy features of version control systems, and much more!&lt;/p&gt;
    &lt;p&gt;Students spend hundreds of hours using these tools over the course of their education (and thousands over their career), so it makes sense to make the experience as fluid and frictionless as possible. Mastering these tools not only enables you to spend less time on figuring out how to bend your tools to your will, but it also lets you solve problems that would previously seem impossibly complex.&lt;/p&gt;
    &lt;p&gt;Read about the motivation behind this class.&lt;/p&gt;
    &lt;head rend="h1"&gt;Schedule&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1/13/20: Course overview + the shell&lt;/item&gt;
      &lt;item&gt;1/14/20: Shell Tools and Scripting&lt;/item&gt;
      &lt;item&gt;1/15/20: Editors (Vim)&lt;/item&gt;
      &lt;item&gt;1/16/20: Data Wrangling&lt;/item&gt;
      &lt;item&gt;1/21/20: Command-line Environment&lt;/item&gt;
      &lt;item&gt;1/22/20: Version Control (Git)&lt;/item&gt;
      &lt;item&gt;1/23/20: Debugging and Profiling&lt;/item&gt;
      &lt;item&gt;1/27/20: Metaprogramming&lt;/item&gt;
      &lt;item&gt;1/28/20: Security and Cryptography&lt;/item&gt;
      &lt;item&gt;1/29/20: Potpourri&lt;/item&gt;
      &lt;item&gt;1/30/20: Q&amp;amp;A&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Video recordings of the lectures are available on YouTube.&lt;/p&gt;
    &lt;head rend="h1"&gt;About the class&lt;/head&gt;
    &lt;p&gt;Staff: This class is co-taught by Anish, Jon, and Jose.&lt;lb/&gt; Questions: Email us at missing-semester@mit.edu.&lt;/p&gt;
    &lt;head rend="h1"&gt;Beyond MIT&lt;/head&gt;
    &lt;p&gt;We’ve also shared this class beyond MIT in the hopes that others may benefit from these resources. You can find posts and discussion on&lt;/p&gt;
    &lt;head rend="h1"&gt;Translations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Chinese (Simplified)&lt;/item&gt;
      &lt;item&gt;Japanese&lt;/item&gt;
      &lt;item&gt;Korean&lt;/item&gt;
      &lt;item&gt;Portuguese&lt;/item&gt;
      &lt;item&gt;Russian&lt;/item&gt;
      &lt;item&gt;Serbian&lt;/item&gt;
      &lt;item&gt;Spanish&lt;/item&gt;
      &lt;item&gt;Turkish&lt;/item&gt;
      &lt;item&gt;Vietnamese&lt;/item&gt;
      &lt;item&gt;Arabic&lt;/item&gt;
      &lt;item&gt;Italian&lt;/item&gt;
      &lt;item&gt;Persian&lt;/item&gt;
      &lt;item&gt;German&lt;/item&gt;
      &lt;item&gt;Bengali&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: these are external links to community translations. We have not vetted them.&lt;/p&gt;
    &lt;p&gt;Have you created a translation of the course notes from this class? Submit a pull request so we can add it to the list!&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;We thank Elaine Mello, Jim Cain, and MIT Open Learning for making it possible for us to record lecture videos; Anthony Zolnik and MIT AeroAstro for A/V equipment; and Brandi Adams and MIT EECS for supporting this class.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://missing.csail.mit.edu/"/><published>2025-10-25T10:36:35+00:00</published></entry></feed>