<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-01T11:36:39.384721+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46096556</id><title>Windows drive letters are not limited to A-Z</title><updated>2025-12-01T11:36:51.599237+00:00</updated><content>&lt;doc fingerprint="c2841e11e92da09d"&gt;
  &lt;main&gt;&lt;p&gt;On its own, the title of this post is just a true piece of trivia, verifiable with the built-in &lt;code&gt;subst&lt;/code&gt; tool (among other methods).&lt;/p&gt;&lt;p&gt;Here's an example creating the drive &lt;code&gt;+:\&lt;/code&gt; as an alias for a directory at &lt;code&gt;C:\foo&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;subst +: C:\foo
&lt;/code&gt;&lt;p&gt;The &lt;code&gt;+:\&lt;/code&gt; drive then works as normal (at least in cmd.exe, this will be discussed more later):&lt;/p&gt;&lt;code&gt;&amp;gt; cd /D +:\

+:\&amp;gt; tree .
Folder PATH listing
Volume serial number is 00000001 12AB:23BC
+:\
√¢√¢√¢√¢bar
&lt;/code&gt;&lt;p&gt;However, understanding why it's true elucidates a lot about how Windows works under the hood, and turns up a few curious behaviors.&lt;/p&gt;&lt;p&gt;The paths that most people are familiar with are Win32 namespace paths, e.g. something like &lt;code&gt;C:\foo&lt;/code&gt; which is a drive-absolute Win32 path. However, the high-level APIs that take Win32 paths like &lt;code&gt;CreateFileW&lt;/code&gt; ultimately will convert a path like &lt;code&gt;C:\foo&lt;/code&gt; into a NT namespace path before calling into a lower level API within &lt;code&gt;ntdll.dll&lt;/code&gt; like &lt;code&gt;NtCreateFile&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;This can be confirmed with NtTrace, where a call to &lt;code&gt;CreateFileW&lt;/code&gt; with &lt;code&gt;C:\foo&lt;/code&gt; ultimately leads to a call of &lt;code&gt;NtCreateFile&lt;/code&gt; with &lt;code&gt;\??\C:\foo&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;NtCreateFile( FileHandle=0x40c07ff640 [0xb8], DesiredAccess=SYNCHRONIZE|GENERIC_READ|0x80, ObjectAttributes="\??\C:\foo", IoStatusBlock=0x40c07ff648 [0/1], AllocationSize=null, FileAttributes=0, ShareAccess=7, CreateDisposition=1, CreateOptions=0x4000, EaBuffer=null, EaLength=0 ) =&amp;gt; 0
NtClose( Handle=0xb8 ) =&amp;gt; 0
&lt;/code&gt;&lt;p&gt;&lt;code&gt;createfilew.zig&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;const std = @import("std");
const windows = std.os.windows;
const L = std.unicode.wtf8ToWtf16LeStringLiteral;

pub extern "kernel32" fn CreateFileW(
    lpFileName: windows.LPCWSTR,
    dwDesiredAccess: windows.DWORD,
    dwShareMode: windows.DWORD,
    lpSecurityAttributes: ?*windows.SECURITY_ATTRIBUTES,
    dwCreationDisposition: windows.DWORD,
    dwFlagsAndAttributes: windows.DWORD,
    hTemplateFile: ?windows.HANDLE,
) callconv(.winapi) windows.HANDLE;

pub fn main() !void {
    const path = L("C:\\foo");
    const dir_handle = CreateFileW(
        path,
        windows.GENERIC_READ,
        windows.FILE_SHARE_DELETE | windows.FILE_SHARE_READ | windows.FILE_SHARE_WRITE,
        null,
        windows.OPEN_EXISTING,
        windows.FILE_FLAG_BACKUP_SEMANTICS | windows.FILE_FLAG_OVERLAPPED,
        null,
    );
    if (dir_handle == windows.INVALID_HANDLE_VALUE) return error.FailedToOpenDir;
    defer windows.CloseHandle(dir_handle);
}
&lt;/code&gt;&lt;p&gt;Built with:&lt;/p&gt;&lt;code&gt;zig build-exe createfilew.zig
&lt;/code&gt;&lt;p&gt;To run with NtTrace:&lt;/p&gt;&lt;code&gt;nttrace createfilew.exe &amp;gt; createfilew.log
&lt;/code&gt;&lt;p&gt;That &lt;code&gt;\??\C:\foo&lt;/code&gt; is a NT namespace path, which is what &lt;code&gt;NtCreateFile&lt;/code&gt; expects. To understand this path, though, we need to talk about the Object Manager, which is responsible for handling NT paths.&lt;/p&gt;&lt;p&gt;The Object Manager is responsible for keeping track of named objects, which we can explore using the WinObj tool. The &lt;code&gt;\??&lt;/code&gt; part of the &lt;code&gt;\??\C:\foo&lt;/code&gt; path is actually a special virtual folder within the Object Manager that combines the &lt;code&gt;\GLOBAL??&lt;/code&gt; folder and a per-user &lt;code&gt;DosDevices&lt;/code&gt; folder together.&lt;/p&gt;&lt;p&gt;For me, the object &lt;code&gt;C:&lt;/code&gt; is within &lt;code&gt;\GLOBAL??&lt;/code&gt;, and is actually a symbolic link to &lt;code&gt;\Device\HarddiskVolume4&lt;/code&gt;:&lt;/p&gt;&lt;p&gt;So, &lt;code&gt;\??\C:\foo&lt;/code&gt; ultimately resolves to &lt;code&gt;\Device\HarddiskVolume4\foo&lt;/code&gt;, and then it's up to the actual device to deal with the &lt;code&gt;foo&lt;/code&gt; part of the path.&lt;/p&gt;&lt;p&gt;The important thing here, though, is that &lt;code&gt;\??\C:\foo&lt;/code&gt; is just one way of referring to the device path &lt;code&gt;\Device\HarddiskVolume4\foo&lt;/code&gt;. For example, volumes will also get a named object created using their GUID with the format &lt;code&gt;Volume{18123456-abcd-efab-cdef-1234abcdabcd}&lt;/code&gt; that is also a symlink to something like &lt;code&gt;\Device\HarddiskVolume4&lt;/code&gt;, so a path like &lt;code&gt;\??\Volume{18123456-abcd-efab-cdef-1234abcdabcd}\foo&lt;/code&gt; is effectively equivalent to &lt;code&gt;\??\C:\foo&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;All this is to say that there's nothing innately special about the named object &lt;code&gt;C:&lt;/code&gt;; the Object Manager treats it just like any other symbolic link and resolves it accordingly.&lt;/p&gt;&lt;p&gt;How I see it, drive letters are essentially just a convention borne out of the conversion of a Win32 path into a NT path. In particular, that would be down to the implementation of &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In other words, since &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; converts &lt;code&gt;C:\foo&lt;/code&gt; to &lt;code&gt;\??\C:\foo&lt;/code&gt;, then an object named &lt;code&gt;C:&lt;/code&gt; will behave like a drive letter. To give an example of what I mean by that: in an alternate universe, &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; could convert the path &lt;code&gt;FOO:\bar&lt;/code&gt; to &lt;code&gt;\??\FOO:\bar&lt;/code&gt; and then &lt;code&gt;FOO:&lt;/code&gt; could behave like a drive letter.&lt;/p&gt;&lt;p&gt;So, getting back to the title, how does &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; treat something like &lt;code&gt;+:\foo&lt;/code&gt;? Well, exactly the same as &lt;code&gt;C:\foo&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;&amp;gt; paths.exe C:\foo
path type: .DriveAbsolute
  nt path: \??\C:\foo

&amp;gt; paths.exe +:\foo
path type: .DriveAbsolute
  nt path: \??\+:\foo
&lt;/code&gt;&lt;p&gt;&lt;code&gt;paths.zig&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;const std = @import("std");
const windows = std.os.windows;

pub fn main() !void {
    var arena_state = std.heap.ArenaAllocator.init(std.heap.page_allocator);
    defer arena_state.deinit();
    const arena = arena_state.allocator();

    const args = try std.process.argsAlloc(arena);
    if (args.len &amp;lt;= 1) return error.ExpectedArg;

    const path = try std.unicode.wtf8ToWtf16LeAllocZ(arena, args[1]);

    const path_type = RtlDetermineDosPathNameType_U(path);
    std.debug.print("path type: {}\n", .{path_type});
    const nt_path = try RtlDosPathNameToNtPathName_U(path);
    std.debug.print("  nt path: {f}\n", .{std.unicode.fmtUtf16Le(nt_path.span())});
}

const RTL_PATH_TYPE = enum(c_int) {
    Unknown,
    UncAbsolute,
    DriveAbsolute,
    DriveRelative,
    Rooted,
    Relative,
    LocalDevice,
    RootLocalDevice,
};

pub extern "ntdll" fn RtlDetermineDosPathNameType_U(
    Path: [*:0]const u16,
) callconv(.winapi) RTL_PATH_TYPE;

fn RtlDosPathNameToNtPathName_U(path: [:0]const u16) !windows.PathSpace {
    var out: windows.UNICODE_STRING = undefined;
    const rc = windows.ntdll.RtlDosPathNameToNtPathName_U(path, &amp;amp;out, null, null);
    if (rc != windows.TRUE) return error.BadPathName;
    defer windows.ntdll.RtlFreeUnicodeString(&amp;amp;out);

    var path_space: windows.PathSpace = undefined;
    const out_path = out.Buffer.?[0 .. out.Length / 2];
    @memcpy(path_space.data[0..out_path.len], out_path);
    path_space.len = out.Length / 2;
    path_space.data[path_space.len] = 0;

    return path_space;
}
&lt;/code&gt;&lt;p&gt;Therefore, if an object with the name &lt;code&gt;+:&lt;/code&gt; is within the virtual folder &lt;code&gt;\??&lt;/code&gt;, we can expect the Win32 path &lt;code&gt;+:\&lt;/code&gt; to behave like any other drive-absolute path, which is exactly what we see.&lt;/p&gt;&lt;p&gt;This section only focuses on a few things that were relevant to what I was working on. I encourage others to investigate the implications of this further if they feel so inclined.&lt;/p&gt;&lt;code&gt;explorer.exe&lt;/code&gt; doesn't play ball√∞&lt;p&gt;Drives with a drive-letter other than A-Z do not appear in File Explorer, and cannot be navigated to in File Explorer.&lt;/p&gt;&lt;code&gt;+:\&lt;/code&gt; in File Explorer
&lt;p&gt;For the "do not appear" part, my guess as to what's happening is that &lt;code&gt;explorer.exe&lt;/code&gt; is walking &lt;code&gt;\??&lt;/code&gt; and looking specifically for objects named &lt;code&gt;A:&lt;/code&gt; through &lt;code&gt;Z:&lt;/code&gt;. For the "cannot be navigated to" part, that's a bit more mysterious, but my guess is that &lt;code&gt;explorer.exe&lt;/code&gt; has a lot of special logic around handling paths typed into the location bar, and part of that restricts drive letters to &lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; (i.e. it's short-circuiting before it ever tries to actually open the path).&lt;/p&gt;&lt;p&gt;PowerShell seems to reject non-&lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; drives as well:&lt;/p&gt;&lt;code&gt;PS C:\&amp;gt; cd +:\
cd : Cannot find drive. A drive with the name '+' does not exist.
At line:1 char:1
+ cd +:\
+ ~~~~~~
    + CategoryInfo          : ObjectNotFound: (+:String) [Set-Location], DriveNotFoundException
    + FullyQualifiedErrorId : DriveNotFound,Microsoft.PowerShell.Commands.SetLocationCommand
&lt;/code&gt;
&lt;p&gt;Drive letters don't have to be within the ASCII range at all; they can also be non-ASCII characters.&lt;/p&gt;&lt;code&gt;&amp;gt; subst √¢¬¨: C:\foo

&amp;gt; cd /D √¢¬¨:\

√¢¬¨:\&amp;gt; tree .
Folder PATH listing
Volume serial number is 000000DE 12AB:23BC
√¢¬¨:\
√¢√¢√¢√¢bar
&lt;/code&gt;
&lt;p&gt;Non-ASCII drive letters are even case-insensitive like &lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; are:&lt;/p&gt;&lt;code&gt;&amp;gt; subst √é: C:\foo

&amp;gt; cd /D √é¬ª:\

√é¬ª:\&amp;gt; tree .
Folder PATH listing
Volume serial number is 000000DE 12AB:23BC
√é¬ª:\
√¢√¢√¢√¢bar
&lt;/code&gt;
&lt;p&gt;However, drive-letters cannot be arbitrary Unicode graphemes or even arbitrary code points; they are restricted to a single WTF-16 code unit (a &lt;code&gt;u16&lt;/code&gt;, so &amp;lt;= &lt;code&gt;U+FFFF&lt;/code&gt;). The tool that we've been using so far (&lt;code&gt;subst.exe&lt;/code&gt;) errors with &lt;code&gt;Invalid parameter&lt;/code&gt; if you try to use a drive letter with a code point larger than &lt;code&gt;U+FFFF&lt;/code&gt;, but you can get around that by going through the &lt;code&gt;MountPointManager&lt;/code&gt; directly:&lt;/p&gt;&lt;code&gt;√∞¬§¬¢:&lt;/code&gt; symlink&lt;code&gt;const std = @import("std");
const windows = std.os.windows;
const L = std.unicode.wtf8ToWtf16LeStringLiteral;

const MOUNTMGR_CREATE_POINT_INPUT = extern struct {
    SymbolicLinkNameOffset: windows.USHORT,
    SymbolicLinkNameLength: windows.USHORT,
    DeviceNameOffset: windows.USHORT,
    DeviceNameLength: windows.USHORT,
};

pub fn main() !void {
    const mgmt_handle = try windows.OpenFile(L("\\??\\MountPointManager"), .{
        .access_mask = windows.SYNCHRONIZE | windows.GENERIC_READ | windows.GENERIC_WRITE,
        .share_access = windows.FILE_SHARE_READ | windows.FILE_SHARE_WRITE | windows.FILE_SHARE_DELETE,
        .creation = windows.FILE_OPEN,
    });
    defer windows.CloseHandle(mgmt_handle);

    const volume_name = L("\\Device\\HarddiskVolume4");
    const mount_point = L("\\DosDevices\\√∞¬§¬¢:");

    const buf_size = @sizeOf(MOUNTMGR_CREATE_POINT_INPUT) + windows.MAX_PATH * 2 + windows.MAX_PATH * 2;
    var input_buf: [buf_size]u8 align(@alignOf(MOUNTMGR_CREATE_POINT_INPUT)) = [_]u8{0} ** buf_size;

    var input_struct: *MOUNTMGR_CREATE_POINT_INPUT = @ptrCast(&amp;amp;input_buf[0]);
    input_struct.SymbolicLinkNameOffset = @sizeOf(MOUNTMGR_CREATE_POINT_INPUT);
    input_struct.SymbolicLinkNameLength = mount_point.len * 2;
    input_struct.DeviceNameOffset = input_struct.SymbolicLinkNameOffset + input_struct.SymbolicLinkNameLength;
    input_struct.DeviceNameLength = volume_name.len * 2;

    @memcpy(input_buf[input_struct.SymbolicLinkNameOffset..][0..input_struct.SymbolicLinkNameLength], @as([*]const u8, @ptrCast(mount_point)));
    @memcpy(input_buf[input_struct.DeviceNameOffset..][0..input_struct.DeviceNameLength], @as([*]const u8, @ptrCast(volume_name)));

    const IOCTL_MOUNTMGR_CREATE_POINT = windows.CTL_CODE(windows.MOUNTMGRCONTROLTYPE, 0, .METHOD_BUFFERED, windows.FILE_READ_ACCESS | windows.FILE_WRITE_ACCESS);
    try windows.DeviceIoControl(mgmt_handle, IOCTL_MOUNTMGR_CREATE_POINT, &amp;amp;input_buf, null);
}
&lt;/code&gt;
&lt;p&gt;(the compiled executable must be run as administrator)&lt;/p&gt;&lt;p&gt;However, having the symlink in place doesn't solve anything on its own:&lt;/p&gt;&lt;code&gt;&amp;gt; cd /D √∞¬§¬¢:\
The filename, directory name, or volume label syntax is incorrect.
&lt;/code&gt;
&lt;p&gt;This is because there's no way to get the drive-absolute Win32 path &lt;code&gt;√∞¬§¬¢:\&lt;/code&gt; to end up as the relevant NT path. As mentioned earlier, the behavior of &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; is what matters, and we can verify that it will not convert a drive-absolute path with a drive letter bigger than &lt;code&gt;U+FFFF&lt;/code&gt; to the relevant NT path:&lt;/p&gt;&lt;code&gt;C:\foo&amp;gt; paths.exe √∞¬§¬¢:\foo
path type: .Relative
  nt path: \??\C:\foo\√∞¬§¬¢:\foo
&lt;/code&gt;

&lt;p&gt;It's very common for path-related functions to be written without the use of system-specific APIs, which means that there's high potential for a mismatch between how &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; treats a file path and how something like a particular implementation of &lt;code&gt;path.isAbsolute&lt;/code&gt; treats a file path.&lt;/p&gt;&lt;p&gt;As a random example, Rust only considers paths with &lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; drive letters as absolute:&lt;/p&gt;&lt;code&gt;use std::path::Path;

fn main() {
    println!("C:\\ {}", Path::new("C:\\foo").is_absolute());
    println!("+:\\ {}", Path::new("+:\\foo").is_absolute());
    println!("√¢¬¨:\\ {}", Path::new("√¢¬¨:\\foo").is_absolute());
}
&lt;/code&gt;
&lt;code&gt;&amp;gt; rustc test.rs

&amp;gt; test.exe
C:\ true
+:\ false
√¢¬¨:\ false
&lt;/code&gt;
&lt;p&gt;Whether or not this represents a problem worth fixing is left as an exercise for the reader (I genuinely don't know if it is a problem), but there's a second wrinkle (hinted at previously) involving text encoding that can make something like an &lt;code&gt;isAbsolute&lt;/code&gt; implementation return different results for the same path. This wrinkle is the reason I looked into this whole thing in the first place, as when I was doing some work on Zig's path-related functions recently I realized that looking at &lt;code&gt;path[0]&lt;/code&gt;, &lt;code&gt;path[1]&lt;/code&gt;, and &lt;code&gt;path[2]&lt;/code&gt; for a pattern like &lt;code&gt;C:\&lt;/code&gt; will look at different parts of the path depending on the encoding. That is, for something like &lt;code&gt;√¢¬¨:\&lt;/code&gt; (which is made up of the code points &lt;code&gt;&amp;lt;U+20AC&amp;gt;&amp;lt;U+003A&amp;gt;&amp;lt;U+005C&amp;gt;&lt;/code&gt;):&lt;/p&gt;&lt;code&gt;U+20AC&lt;/code&gt; can be encoded as the single &lt;code&gt;u16&lt;/code&gt; code unit &lt;code&gt;0x20AC&lt;/code&gt;, that'd mean &lt;code&gt;path[0]&lt;/code&gt; will be &lt;code&gt;0x20AC&lt;/code&gt;, &lt;code&gt;path[1]&lt;/code&gt; will be &lt;code&gt;0x3A&lt;/code&gt; (&lt;code&gt;:&lt;/code&gt;), and &lt;code&gt;path[2]&lt;/code&gt; will be &lt;code&gt;0x5C&lt;/code&gt; (&lt;code&gt;\&lt;/code&gt;), which looks like a drive-absolute path&lt;code&gt;U+20AC&lt;/code&gt; is encoded as three &lt;code&gt;u8&lt;/code&gt; code units (&lt;code&gt;0xE2 0x82 0xAC&lt;/code&gt;), that'd mean &lt;code&gt;path[0]&lt;/code&gt; will be &lt;code&gt;0xE2&lt;/code&gt;, &lt;code&gt;path[1]&lt;/code&gt; will be &lt;code&gt;0x82&lt;/code&gt;, and &lt;code&gt;path[2]&lt;/code&gt; will be &lt;code&gt;0xAC&lt;/code&gt;, meaning it will look nothing like a drive-absolute path&lt;p&gt;So, to write an implementation that treats paths the same regardless of encoding, some decision has to be made:&lt;/p&gt;&lt;code&gt;RtlDetermineDosPathNameType_U&lt;/code&gt;/&lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; is desired, decode the first code point and check for &lt;code&gt;&amp;lt;= 0xFFFF&lt;/code&gt; when dealing with WTF-8 (this is the option I went with for the Zig standard library, but I'm not super happy about it)&lt;code&gt;path[0]&lt;/code&gt;/&lt;code&gt;path[1]&lt;/code&gt;/&lt;code&gt;path[2]&lt;/code&gt; and don't care about non-ASCII drive letters, check for &lt;code&gt;path[0] &amp;lt;= 0x7F&lt;/code&gt; regardless of encoding&lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; drive letters, then check for that explicitly (this is what Rust does)&lt;p&gt;Something bizarre that I found with this whole thing is that the &lt;code&gt;kernel32.dll&lt;/code&gt; API &lt;code&gt;SetVolumeMountPointW&lt;/code&gt; has it's own unique quirk when dealing with non-ASCII drive letters. Specifically, this code (attempting to create the drive &lt;code&gt;√¢¬¨:\&lt;/code&gt;) will succeed:&lt;/p&gt;&lt;code&gt;const std = @import("std");
const windows = std.os.windows;
const L = std.unicode.wtf8ToWtf16LeStringLiteral;

extern "kernel32" fn SetVolumeMountPointW(
    VolumeMountPoint: windows.LPCWSTR,
    VolumeName: windows.LPCWSTR,
) callconv(.winapi) windows.BOOL;

pub fn main() !void {
    const volume_name = L("\\\\?\\Volume{18123456-abcd-efab-cdef-1234abcdabcd}\\");
    const mount_point = L("√¢¬¨:\\");
    if (SetVolumeMountPointW(mount_point, volume_name) == 0) {
        const err = windows.GetLastError();
        std.debug.print("{any}\n", .{err});
        return error.Failed;
    }
}
&lt;/code&gt;
&lt;p&gt;However, when we look at the Object Manager, the &lt;code&gt;√¢¬¨:&lt;/code&gt; symlink won't exist... but &lt;code&gt;√Ç¬¨:&lt;/code&gt; will:&lt;/p&gt;&lt;p&gt;My time dealing extensively with Windows quirks made me recognize what might be happening here: &lt;code&gt;0x20AC&lt;/code&gt; is likely being truncated to &lt;code&gt;0xAC&lt;/code&gt; by &lt;code&gt;SetVolumeMountPointW&lt;/code&gt;, and &lt;code&gt;U+00AC&lt;/code&gt; happens to be &lt;code&gt;√Ç¬¨&lt;/code&gt;. If that is indeed what's going on, it seems pretty strange to truncate the drive letter instead of reject the path, but it also makes sense that non-ASCII drive letters are an edge case that no one has really thought about at all.&lt;/p&gt;&lt;p&gt;I have no idea if anything I wrote about here is novel, although my cursory searches didn't turn up much. The only mention of non-&lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; drive letters I'm currently aware of is from the article The Definitive Guide on Win32 to NT Path Conversion which says:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;it's natural to assume that drive "letters" can only be A through Z. It turns out the&lt;/p&gt;&lt;code&gt;RtlGetFullPathName_U&lt;/code&gt;API does not enforce this requirement, although the Explorer shell and command prompt almost certainly do. Therefore as long as the second character of a path is a colon, the conversion will treat it as a Drive Absolute or Drive Relative path. Of course if the DosDevices object directory doesn't have an appropriate symbolic link it's not going to do you much good.&lt;/quote&gt;&lt;p&gt;Well, it turns out that the command prompt also doesn't enforce the requirement, and I'd guess that there's at least some more weirdness around this quirk that's waiting to be discovered.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ryanliptak.com/blog/windows-drive-letters-are-not-limited-to-a-z/"/><published>2025-11-30T13:40:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46096800</id><title>Migrating Dillo from GitHub</title><updated>2025-12-01T11:36:50.981536+00:00</updated><content>&lt;doc fingerprint="af008dd98a923a96"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Migrating Dillo from GitHub&lt;/head&gt;Written on 2025-11-30 by Rodrigo Arias Mallo&lt;p&gt;I would like to migrate the Dillo project away from GitHub into a new home which is more friendly to be used with Dillo and solves some of its problems. This page summarizes the current situation with GitHub and why I decided to move away from it into a self-hosted server with multiple mirrors in other forges.&lt;/p&gt;&lt;head rend="h2"&gt;Background&lt;/head&gt;&lt;p&gt;Before we dive into the details, I would like to briefly mention what happened with the old site. The original Dillo website was at dillo.org, which also had the source code of Dillo in a mercurial repository at hg.dillo.org. But it also included the mail server used to reach the developers, a bug tracker and archives for the mailing list. However, in 2022 the domain was lost and someone else decided to buy it to put a similar site but plaged with AI generated ads. The original developers are no longer active, but luckily I had a copy of the mercurial repository and with some help I was able to recover a lot of material from the original server (some parts are still missing to this day).&lt;/p&gt;&lt;p&gt;I want to avoid this situation as much as possible, so we cannot rely on a single site that can go down and the whole project become lost. Initially, I uploaded the Dillo source and website to git repositories on GitHub, but I no longer think this is a good idea.&lt;/p&gt;&lt;head rend="h2"&gt;The situation with GitHub&lt;/head&gt;&lt;p&gt;GitHub has been useful to store all repositories of the Dillo project, as well as to run the CI workflows for platforms in which I don't have a machine available (like Windows, Mac OS or some BSDs).&lt;/p&gt;&lt;p&gt;However, it has several problems that make it less suitable to develop Dillo anymore. The most annoying problem is that the frontend barely works without JavaScript, so we cannot open issues, pull requests, source code or CI logs in Dillo itself, despite them being mostly plain HTML, which I don't think is acceptable. In the past, it used to gracefully degrade without enforcing JavaScript, but now it doesn't. Additionally, the page is very resource hungry, which I don't think is needed to render mostly static text.&lt;/p&gt;&lt;p&gt;Another big problem is that it is a single point of failure. I don't mean that GitHub is stored in a single machine, but it is controlled by a single entity which can unilateraly ban our repository or account and we would lose the ability to notify in that URL what happened. This can cause data loss if we don't have a local copy of all the data.&lt;/p&gt;&lt;p&gt;On the usability side, the platform has become more and more slow over time, which is affecting the development process. It also requires you to have a fast Internet connection at all times, which is not the case for me sometimes. Additionally, GitHub seems to encourage a "push model" in which you are notified when a new event occurs in your project(s), but I don't want to work with that model. Instead, I prefer it to work as a "pull model", so I only get updates when I specifically look for them. This model would also allow me to easily work offline. Unfortunately, I see that the same push model has been copied to alternative forges.&lt;/p&gt;&lt;p&gt;On the social side, I feel that it doesn't have the right tools to moderate users, specially for projects where the ratio of non-technical users to developers is high. This is specially problematic when active issues with developer notes begin to be filled with comments from users that have never contributed to the project and usually do more harm than good. This situation ends up causing burnout in developers.&lt;/p&gt;&lt;p&gt;Lastly, GitHub seem to follow the current trend of over-focusing on LLMs and generative AI, which are destroying the open web (or what remains of it) among other problems. It has a direct impact on us because sites protect themseves with a JavaScript wall (or worse, browser fingerprinting) to prevent aggresive LLM crawler bots from overloading the site, but they also leave Dillo users out. So I would prefer not to encourage this trend. Despite my intentions, moving Dillo away won't change much their capability to train their model with our code, but at least I won't be actively helping.&lt;/p&gt;&lt;head rend="h2"&gt;Self-hosting Dillo&lt;/head&gt;&lt;p&gt;After researching the available options, it seems that none of the current forges would allow us to have a redundant system that can prevent the forge from becoming a single point of failure and solve the rest of the problems with GitHub. Therefore, I decided to self-host Dillo myself, move all important data to git repositories and keep them synchronized in multiple git mirrors.&lt;/p&gt;&lt;p&gt;I decided to buy the dillo-browser.org domain name and setup a very small VPS. Initially, I was very skeptical that it would be able to survive on today's web, but it seems to be doing an acceptable job at handling it (mostly AI bot traffic masquerading as users). The Dillo website is available here:&lt;/p&gt;&lt;p&gt;I researched which git frontends may suit our needs, and I discovered that most options are very complicated to self-host and require a lot of server resources and JavaScript on the frontend. I ended up testing cgit, which is written in C and it seems to be very lightweight both on RAM and CPU usage. Furthermore, the web frontend doesn't require JS, so I can use it from Dillo (I modified cgit CSS slightly to work well on Dillo). It is available on this URL:&lt;/p&gt;&lt;p&gt;https://git.dillo-browser.org/&lt;/p&gt;&lt;p&gt;Regarding the bug tracker, I also took a look at the available options. They are all too complicated for what I would like to have and they seem to centralize the data into a database that can get lost. This is precisely the case that happened with the old dillo bug tracker and we are still unable to recover the original bug entries.&lt;/p&gt;&lt;p&gt;To avoid this problem, I created my own bug tracker software, buggy, which is a very simple C tool that parses plain Markdown files and creates a single HTML page for each bug. All bugs are stored in a git repository and a git hook regenerates the bug pages and the index on each new commit. As it is simply plain text, I can edit the bugs locally and only push them to the remote when I have Internet back, so it works nice offline. Also, as the output is just an static HTML site, I don't need to worry about having any vulnerabilities in my code, as it will only run at build time. You can see it live here, with the exported issues from GitHub:&lt;/p&gt;&lt;p&gt;https://bug.dillo-browser.org/&lt;/p&gt;&lt;p&gt;The mailing list archives are stored by three independent external services, but I might include a copy with our own archives in the future.&lt;/p&gt;&lt;head rend="h2"&gt;Setting up mirrors&lt;/head&gt;&lt;p&gt;As all the important data is now stored in git repositories, we can mirror them in any forge, without having to rely on their custom storage format for the issues or other data. If a forge goes down (or goes rogue) we can simply switch to another site with low switching cost. To this end, I have created git mirrors in Codeberg and Sourcehut that are synced with our git server:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Codeberg: https://codeberg.org/dillo/&lt;/item&gt;&lt;item&gt;Sourcehut: https://git.sr.ht/~dillo/&lt;/item&gt;&lt;/list&gt;&lt;p&gt;However, we still have a single point of failure: the DNS entry of the dillo-browser.org domain. If we lose the DNS entry (like with dillo.org) it would cause a problem as all services will be unreachable. We could recover from such situation by relying on alternative ways to reach users, by the mailing list, fediverse or IRC, as well as updating the mirrors to reflect the current situation. It is not ideal, but I don't think it would cause a catastrophic data loss (like it happened before) as all the data is now stored in git and replicated across independent locations.&lt;/p&gt;&lt;head rend="h2"&gt;OpenPGP signature&lt;/head&gt;&lt;p&gt;In order for this page to have some authority, the HTML file is signed with my GPG key (32E65EC501A1B6FDF8190D293EE6BA977EB2A253), which is the same that I use to sign the last releases of Dillo and is also listed in my GitHub user. The signature is available here and is linked to the page with the &lt;code&gt;&amp;lt;link&amp;gt;&lt;/code&gt; tag using the &lt;code&gt;rel=signature&lt;/code&gt;
relation. You can find more information and how to verify the signature in the
Dillo RFC-006.

&lt;/p&gt;&lt;p&gt;Using OpenPGP signatures is robust against losing the DNS entry, as the authority is not given by the TLS certificate chain but by the trust in the OpenPGP signature, so we could move the site elsewhere and still claim that is owned by us. Additionally, as we can store the signatures inside all git mirrors, they are also resilient against data loss.&lt;/p&gt;&lt;head rend="h2"&gt;Closing remarks&lt;/head&gt;&lt;p&gt;Keep in mind that the migration process requires several moving parts and it will take a while for it to stabilize (switching costs). The GitHub repositories won't be removed at any point in time and they will continue to be updated until we finish the migration. When the migration process is completed, I will mark the Dillo repositories as archived and properly comunicate it in our site. It is important that we don't remove any commit or tarball release to avoid breaking downstream builds that still rely on the GitHub URL.&lt;/p&gt;&lt;p&gt;Lastly, I'm glad that we can have our own fully independent and self-hosted site with relatively low expenses and very little energy cost (which is good for the environment, but probably not even noticeable at large scale). With the current DNS and server costs and our current donations I consider that it is likely that we can continue covering the expenses for at least the next 3 years in the worst case scenario. If you are interested in keeping us afloat, you can help via Liberapay.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dillo-browser.org/news/migration-from-github/"/><published>2025-11-30T14:11:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46097829</id><title>GitHub to Codeberg: my experience</title><updated>2025-12-01T11:36:50.542741+00:00</updated><content>&lt;doc fingerprint="9a9c101d0be75ac8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GitHub ‚Üí Codeberg: my experience&lt;/head&gt;
    &lt;p&gt;Published . Estimated reading time: 11 minutes.&lt;/p&gt;
    &lt;p&gt;In which I talk about the process involved in switching forges, and how well that went.&lt;/p&gt;
    &lt;p&gt;Spoiler alert: this very site that you‚Äôre reading this on is not served from GitHub Pages anymore! At this point, I‚Äôd call my migration successful. But it took more than clicking a single button, so let‚Äôs talk about the steps involved, at least for me. I‚Äôm hoping that it can help be an example for other people, and show that it‚Äôs actually not that complicated.&lt;/p&gt;
    &lt;head rend="h2"&gt;(My) migration process&lt;/head&gt;
    &lt;p&gt;First, I took an hour or so to set up my profile picture, email address(es), SSH keys‚Ä¶&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 1: migrating the repos&lt;/head&gt;
    &lt;p&gt;This wasn‚Äôt difficult, because Forgejo (the forge software that powers Codeberg) offers a ‚Äúmigrate from GitHub‚Äù functionality. You need to generate a PAT on GitHub to import things like issues (which is awesome!), and as a bonus it also speeds up the process.&lt;/p&gt;
    &lt;p&gt;It was, however, tedious, because the process was entirely manual (perhaps there‚Äôs a way to automate it, like by using some Forgejo CLI tool, but I didn‚Äôt bother looking into that). And, due to GitHub API rate limits, whenever I tried importing two repos at the same time, one or both would fail. (It wasn‚Äôt too bad, though, since I could fill out the migration page for the next while one was in progress; and generally, it took me roughly as long to fill it out as it took Codeberg to perform the import.)&lt;/p&gt;
    &lt;p&gt;I‚Äôm really happy that issues, PRs, wikis, and releases can be imported flawlessly: this makes it possible to not have to refer to GitHub anymore!&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 2: repointing links to Codeberg&lt;/head&gt;
    &lt;p&gt;Of course I don‚Äôt control all links that point to my stuff, but I could at least run &lt;code&gt;rg -F github.com/ISSOtm&lt;/code&gt; in my home directory, to catch those within my own repos. It‚Äôs possible to automate the replacing process:&lt;/p&gt;
    &lt;p&gt;‚Ä¶and if you‚Äôre feeling like bulk-replacing all files in a directory:&lt;/p&gt;
    &lt;p&gt;Repositories, however, may still be pointing to GitHub:&lt;/p&gt;
    &lt;code&gt; 
 )
 )
&lt;/code&gt;
    &lt;p&gt;You can either manually &lt;code&gt;git remote set-url origin git@codeberg.org:ISSOtm/rsgbds.git&lt;/code&gt; (or the equivalent if you‚Äôre using HTTPS), or use one of the replace commands above, since remote URLs are stored textually:&lt;/p&gt;
    &lt;code&gt;# Within a single repo:

# For all repos within the current directory: (requires `shopt -s globstar` if using Bash)
&lt;/code&gt;
    &lt;p&gt;‚Ä¶then it‚Äôs a matter of pushing the changes to all of the repos.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 3: stubbing out the GitHub repos&lt;/head&gt;
    &lt;p&gt;I also wanted to make it clear that my repos were now living on Codeberg; so, I created a little script in an empty directory:&lt;/p&gt;
    &lt;code&gt;#!/bin/bash
 

 
 
 
 
 
 
 
&lt;/code&gt;
    &lt;p&gt;Then, to run it:&lt;/p&gt;
    &lt;code&gt; 
 
 
 
 
# ...etc.
&lt;/code&gt;
    &lt;p&gt;The automation made it not painful, so this went pretty well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 4: porting CI&lt;/head&gt;
    &lt;p&gt;Now, onto the harder stuff :)&lt;/p&gt;
    &lt;p&gt;The first interesting thing that I noticed is this section of Codeberg‚Äôs CI documentation:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Running CI/CD pipelines can use significant amounts of energy. As much as it is tempting to have green checkmarks everywhere, running the jobs costs real money and has environmental costs.&lt;/p&gt;
      &lt;p&gt;Unlike other giant platforms, we do not encourage you to write ‚Äúheavy‚Äù pipelines and charge you for the cost later. We expect you to carefully consider the costs and benefits from your pipelines and reduce CI/CD usage to a minimum amount necessary to guarantee consistent quality for your projects.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That got me to think about which projects of mine really need CI, and ultimately, I decided that I would only need CI for publishing my website, and the documentation of gb-starter-kit and fortISSimO; the rest of my projects don‚Äôt get contributions anyway, so I can live without CI on them, at least for now.&lt;/p&gt;
    &lt;p&gt;Anyway, Codeberg actually has two different CI solutions: Woodpecker, and Forgejo Actions; the former seems to be more powerful, but you need to apply for access, and the latter is very close to GitHub Actions, which should facilitate the migration. So I picked Forgejo Actions, even though it‚Äôs marked as being in beta.&lt;/p&gt;
    &lt;p&gt;It‚Äôs not very difficult to port a YAML file from GHA to Forgejo Actions; for example, look at the commit porting gb-starter-kit‚Äôs publishing CI. (This doesn‚Äôt really appear as a diff, since I‚Äôve moved the file; but it‚Äôs small, so it‚Äôs easy to compare manually.)&lt;/p&gt;
    &lt;p&gt;Here are some salient points:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Actions are normally just referred to as &lt;code&gt;owner/repo&lt;/code&gt;, but Forgejo supports cloning any Git repo, especially across forges. It‚Äôs actually recommended to use full URLs always, so you don‚Äôt rely on the default prefix, which is configurable by the instance admin and thus not necessarily portable.&lt;/item&gt;
      &lt;item&gt;I could have kept the files in &lt;code&gt;.github/workflows&lt;/code&gt;, since Forgejo picks up that directory automatically if&lt;code&gt;.forgejo/workflows&lt;/code&gt;doesn‚Äôt exist; however, I think it‚Äôs more convenient to keep un-migrated scripts in&lt;code&gt;.github&lt;/code&gt;and migrated ones in&lt;code&gt;.forgejo&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Most Actions (the individual steps, not the workflow files) actually work out of the box on Forgejo Actions. Nice!&lt;/item&gt;
      &lt;item&gt;Codeberg‚Äôs runners differ from GitHub‚Äôs significantly: they have way less software installed by default, fewer resources, and only Linux runners are provided (Ubuntu by default, but you can use any Docker container image). macOS and Windows being non-free OSes, Codeberg has no plans to offer either of those! For both philosophical and financial reasons. If this is a deal-breaker for you, consider cross-compiling, or bringing your own runner.&lt;/item&gt;
      &lt;item&gt;Unless low latency is crucial, consider using the lazy runners for better load balancing and possibly greener energy consumption. In practice I haven‚Äôt seen delays beyond a few minutes, which is acceptable to me.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I actually spent some extra time trying to use less compute to perform my CI jobs, somewhat motivated by the small size of the runners, and because I‚Äôm guessing that the smaller the runner you‚Äôre picking, the faster your job will be able to be scheduled. Here is one such commit; note in particular line 50, where I tried1 using a Docker image with LaTeX preinstalled, which saves the time taken by &lt;code&gt;apt install&lt;/code&gt; and requires fewer writes to the filesystem, freeing up RAM.&lt;/p&gt;
    &lt;p&gt;Unfortunately, due to a version discrepancy with &lt;code&gt;noweb&lt;/code&gt;, I had to revert to the base Ubuntu image; but a ‚Äúregular‚Äù LaTeX workflow would have had no problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 5: re-hosting my website&lt;/head&gt;
    &lt;p&gt;All of the previous steps were done within the span of a few days; however, since my website (this very website) was hosted using GitHub Pages, I couldn‚Äôt migrate its repos (yes, plural: you can configure individual repos to be published separately, which is how e.g. https://eldred.fr/fortISSimO is published, despite not being in the website‚Äôs main repo).&lt;/p&gt;
    &lt;p&gt;Nominally, Codeberg has an equivalent, Codeberg Pages; however, as mentioned on that page, &lt;quote&gt;the software behind this feature is currently in maintenance mode&lt;/quote&gt;, because of complexity and performance issues2. So I left it at that for roughly a month, hoping there‚Äôll eventually be an update. Also, subprojects are published as subdomains instead of subdirectories, which would have broken links (e.g. &lt;code&gt;http://eldred.fr/fortISSimO&lt;/code&gt; would have become &lt;code&gt;http://fortISSimO.eldred.fr&lt;/code&gt;). Meh‚Ä¶&lt;/p&gt;
    &lt;p&gt;And then (by chance lol) I discovered git-pages and its public instance Grebedoc3! It functions much like GitHub Pages, though with a bit more setup since it‚Äôs not integrated within the forge itself.&lt;/p&gt;
    &lt;p&gt;git-pages actually has several niceties:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;My website had zero downtime during the entire migration, as git-pages supports uploading your website before updating your DNS records!&lt;/item&gt;
      &lt;item&gt;It also supports server-side redirects, which lets me redirect people who still go to http://eldred.fr/gb-asm-tutorial/* to its new home, for example. People have been getting 404s because of incomplete client-side coverage on my side, but no more!&lt;/item&gt;
      &lt;item&gt;It also also supports custom headers; I‚Äôm not particularly interested in CORS, but I‚Äôve used that file to pay my respects4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Oh, and also, Codeberg‚Äôs November 2025 newsletter mentions that &lt;quote&gt;Codeberg is planning to gradually migrate to [git-pages]&lt;/quote&gt;. Exciting!&lt;/p&gt;
    &lt;p&gt;I‚Äôm actually much happier using this than GitHub Pages; so, I‚Äôve joined Catherine‚Äôs Patreon, because I want to see this go far.&lt;/p&gt;
    &lt;p&gt;To quote Catherine‚Äôs motivation for creating git-pages: &lt;quote&gt;I started out wanting to just use Codeberg Pages and then I found out that Codeberg Pages is in maintenance mode and has such poor architecture that not only does it have rather low uptime but it also regularly crashes Codeberg‚Äôs Forgejo instance itself.&lt;/quote&gt;&lt;/p&gt;
    &lt;p&gt;ü¶É. It is intensely looking at you‚Ä¶‚Ä¶.&lt;/p&gt;
    &lt;p&gt;Here is some context as to what this means.&lt;/p&gt;
    &lt;head rend="h3"&gt;Time tracking&lt;/head&gt;
    &lt;p&gt;Steps 1 through 3 (migrating the repos) took me the better part of an afternoon; step 4 (porting CI) took me another afternoon, mostly to learn the new CI system; and step 5 (the website) took me‚Ä¶ well, it should have taken an afternoon, but I used the opportunity to also pay down some tech debt (merging my slides repo into my main website), which took a few days due to required rearchitecting.&lt;/p&gt;
    &lt;p&gt;All in all, even with 45 repos migrated, this basically took a weekend. And I didn‚Äôt find it annoying!&lt;/p&gt;
    &lt;p&gt;Since the task seemed really daunting, my anxiety caused me to procrastinate this a lot, but in the end it was little work. One of the reasons I‚Äôm writing this is to let other people know that, so they can overcome their own anxiety. Maybe. :P&lt;/p&gt;
    &lt;head rend="h2"&gt;What now?&lt;/head&gt;
    &lt;p&gt;All in all, I‚Äôm very happy with this migration! As far as I can tell, nothing on this website has broken, and I‚Äôve tried reasonably containing the breakage over on GitHub: I have truncated the &lt;code&gt;master&lt;/code&gt; branches, but all other branches and tags remain in place (mostly due to laziness lol), permalinks (e.g. &lt;code&gt;https://github.com/ISSOtm/gb-bootroms/blob/c8ed9e106e0ab1193a57071820e46358006c79d0/src/dmg.asm&lt;/code&gt;) still work, only non-perma links (e.g. &lt;code&gt;https://github.com/ISSOtm/gb-bootroms/blob/master/src/dmg.asm&lt;/code&gt;) are broken, but those are unreliable in the first place anyway.&lt;/p&gt;
    &lt;p&gt;Since that means that all of my code is still on GitHub, I want to delete my repos; but that would be a bad idea at this point, due to leaving no redirects or anything. I‚Äôll consider that again in‚Ä¶ idk, a year or something. I would also like to delete my GitHub account (like I have deleted my Twitter account when‚Ä¶ *gestures vaguely*), but not only do I need my repos to be up, I also need my account to contribute to projects that are still on GitHub.&lt;/p&gt;
    &lt;p&gt;One downside of this migration is that since I‚Äôm moving off of The Main Forge, my projects are likely to get fewer contributions‚Ä¶ But I wasn‚Äôt getting many in the first place, and some people have already made accounts on Codeberg to keep contributing to my stuff. Likewise, I‚Äôm not really worried about discoverability. We‚Äôll see I guess lol ü§∑‚ôÇÔ∏è&lt;/p&gt;
    &lt;p&gt;Lastly, I‚Äôm writing this after the migration, and I haven‚Äôt really taken notes during it; so, if I‚Äôve forgotten any steps, feel free to let me know in the comments below or by opening an issue, and I‚Äôll edit this article.&lt;/p&gt;
    &lt;p&gt;Cheers!&lt;/p&gt;
    &lt;head rend="h2"&gt;Special thanks&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Catherine ‚Äòwhitequark‚Äô for her work on git-pages and for being part of the ops team for Grebedoc&lt;/item&gt;
      &lt;item&gt;SERVFAIL network (domi, Merlin, famfo, aprl, and all of #servfail) for being my awesome DNS providers&lt;/item&gt;
      &lt;item&gt;Codeberg team and Forgejo contributors for making all of this possible in the first place&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eldred.fr/blog/forge-migration/"/><published>2025-11-30T16:12:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46098673</id><title>ESA Sentinel-1D delivers first high-resolution images</title><updated>2025-12-01T11:36:49.759525+00:00</updated><content>&lt;doc fingerprint="a0002ffb95f93d86"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sentinel-1D delivers first images: from Antarctica to Bremen&lt;/head&gt;
    &lt;p&gt;The first high-resolution images have been received from Copernicus Sentinel-1D and were shared publicly for the first time at the European Space Agency‚Äôs Ministerial Council, held today in Bremen, Germany. Glaciers in Antarctica, the tip of South America, as well as the city of Bremen, are visible in these stunning radar images.&lt;/p&gt;
    &lt;p&gt;The groundbreaking Copernicus Sentinel-1 mission saw the arrival in orbit of its latest satellite earlier in November: Sentinel-1D was launched on 4 November, on board an Ariane 6 launcher from Europe‚Äôs Spaceport in French Guiana.&lt;/p&gt;
    &lt;p&gt;Once in orbit, the satellite and its instruments ‚Äì it carries a 12 m-long synthetic aperture radar (SAR) instrument ‚Äì were switched on, ready to capture images during a pass over the Antarctic and South America two days after launch. On the night of 6 November (European time), the first images were captured over the Antarctic Peninsula, the Tierra del Fuego and the Thwaites Glacier. Some six hours later, on the morning of 7 November, Sentinel-1D also captured images over Bremen, in Germany. The data was then transmitted, or ‚Äòdownlinked‚Äô, from the satellite to the ground station, in Matera (Italy), which is part of the Copernicus Ground Segment. All this was done within 50 hours of launch, which is likely to be the shortest time from launch to data delivery for a radar-based Earth observation satellite.&lt;/p&gt;
    &lt;p&gt;According to Nuno Miranda, ESA‚Äôs Sentinel-1 Mission Manager, the images are of unprecedented data quality for a ‚Äòfirst light‚Äô acquisition. They are very similar to the images captured not so long ago by Sentinel-1C, which, according to Nuno, is very promising for the commissioning phase. He noted, ‚ÄúThese images have been downlinked and processed within an exceptionally short timeframe. Some of us remember that when Sentinel-1B was launched, it delivered its first radar images within two hours of activation. Sentinel-1D achieved this in an even faster time, setting what we believe is a new record for space radar. This remarkable performance reflects the dedication and exceptional preparation of all the teams involved.‚Äù&lt;/p&gt;
    &lt;p&gt;Radar instruments can image Earth‚Äôs surface through clouds, precipitation, regardless of sunlight, making them particularly well suited for monitoring polar regions. The Sentinel-1C and -1D satellites also carry an Automatic Identification System (AIS) instrument ‚Äì improving the mission capacity to detect ships and sea pollution. The Sentinel-1D AIS was also activated as the satellite passed over Antarctica capturing the presence of ships in these extreme areas.&lt;/p&gt;
    &lt;head rend="h2"&gt;First images show fragility of glaciers&lt;/head&gt;
    &lt;p&gt;The Antarctic Peninsula (above) is part of the larger peninsula of West Antarctica, protruding 1300 km. It is an ice sheet resting on a string of rocky islands and its tip is just 1000 km from the southern tip of South America. The Antarctic Peninsula ice sheet is one of the smallest ice sheets in Antarctica but is perhaps the most vulnerable to climate change as its glaciers are small and in a region of rapid warming. Observable changes such as collapsing ice shelves, thinning and accelerating glaciers are all key indicators of climate change in the region.&lt;/p&gt;
    &lt;p&gt;This image is in black and white, showing the contrast between the ocean and the peninsula‚Äôs icy landscape.&lt;/p&gt;
    &lt;p&gt;Tierra del Fuego (above) is an archipelago off the southern tip of the South American continent. It covers territory in both Argentina to the east and in Chile to the west and is separated from the mainland by the Magellan Strait. The most southerly point of Tierra del Fuego is Cape Horn.&lt;/p&gt;
    &lt;p&gt;The bright contrasting colours in this image are created by using multiple types of radar wave, known polarisations. In this image the ocean and snowy peaks are shown in shades of blue, while the land appears yellow.&lt;/p&gt;
    &lt;p&gt;The Thwaites Glacier, and the adjacent Pine Island Glacier (above), are located west of the Antarctic Peninsula. Both are vulnerable to climate change. Thwaites is one of the most unstable glaciers in Antarctica and is at risk of rapid retreat. The details shown in this image from Sentinel-1D remind us of the fragility of glaciers in the Antarctic. And since 2025 is the United Nation‚Äôs International Year of Glaciers' Preservation, it is timely to see this image, captured on 6 November 2025.&lt;/p&gt;
    &lt;p&gt;This image also uses multiple radar polarisations to capture enhanced data on the landscape. In this image, the sea ice in the water is visible in tones of purple or violet, while the glacier appears white.&lt;/p&gt;
    &lt;p&gt;The publication of these images also follows the 30th meeting of the Conference of Parties, or COP30, where the consequences of climate change and the mitigating actions needed were discussed. The World Meteorological Organization‚Äôs State of the Climate Update for COP30 notes that glaciers, from October 2023 to September 2024, lost the largest amount of ice on record back to 1950. The report states this is equivalent to 1.2 mm of global mean sea-level rise. The report also notes that, on 24 February 2025, the extent of Antarctic sea-ice reached its third lowest extent since satellite records began in 1978, the lowest being in 2023.&lt;/p&gt;
    &lt;p&gt;Simonetta Cheli, Director of ESA‚Äôs Earth Observation Programmes, said, ‚ÄúThis is a great achievement and I am so pleased to see these results from Sentinel-1D. It really places the data we receive from our innovative missions in the spotlight ‚Äì it is data that we as a society rely upon as we continue to discuss and take action on climate change, and also data that we need in applications for understanding and studying our planet.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe Sentinel-1 team has done an amazing job and I would like to thank everyone within ESA, together with our partners in the space industry and European institutions, for delivering work of such high quality. It‚Äôs an honour to deliver this mission for the Copernicus Earth observation programme, and we thank the Commission for their support and collaboration. We look forward also to developing the Sentinel missions of the future, to further extend the capacity and potential of Copernicus for Europe.‚Äù&lt;/p&gt;
    &lt;p&gt;ESA‚Äôs Sentinel-1 Project Manager, Ram√≥n Torres, expressed the whole team‚Äôs pride, ‚ÄúUnveiling the first images from Sentinel-1D is an incredibly emotional milestone for all of us. The sense of awe and fulfilment goes beyond the thrill of liftoff itself, because seeing those breathtaking images from the SAR instrument brings our hard work to life. They are not just pictures ‚Äì they are proof of our vision becoming reality, underlining how cutting-edge this mission truly is. The fact that these stunning images also confirm the satellite‚Äôs health and flawless operation fills us with relief and joy. And to have achieved all of this within an astonishingly short time ‚Äì just over two days after launch ‚Äì makes this moment even more unforgettable for our team.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-1/Sentinel-1D_delivers_first_images_from_Antarctica_to_Bremen"/><published>2025-11-30T17:37:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46098747</id><title>ETH-Zurich: Digital Design and Computer Architecture; 227-0003-10L, Spring, 2025</title><updated>2025-12-01T11:36:49.222641+00:00</updated><content>&lt;doc fingerprint="5c71730e58600a24"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Table of Contents&lt;/head&gt;
    &lt;head rend="h1"&gt;Digital Design and Computer Architecture&lt;/head&gt;
    &lt;head rend="h1"&gt;Spring 2025 (227-0003-10L)&lt;/head&gt;
    &lt;p&gt;Welcome to the wiki for Digital Design and Computer Architecture for Spring 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Announcements&lt;/head&gt;
    &lt;head rend="h2"&gt;Course Information&lt;/head&gt;
    &lt;head rend="h3"&gt;Description&lt;/head&gt;
    &lt;p&gt;The class provides a first introduction to the design of digital circuits and computer architecture. It covers technical foundations of how a computing platform is designed from the bottom up. It introduces various execution paradigms, hardware description languages, and principles in digital design and computer architecture. The focus is on fundamental techniques employed in the design of modern microprocessors and their hardware/software interface.&lt;/p&gt;
    &lt;head rend="h3"&gt;Objectives&lt;/head&gt;
    &lt;p&gt;This class provides a first approach to Computer Architecture. The students learn the design of digital circuits in order to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;understand the basics,&lt;/item&gt;
      &lt;item&gt;understand the principles (of design),&lt;/item&gt;
      &lt;item&gt;understand the precedents (in computer architecture).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Based on such understanding, the students are expected to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;learn how a modern computer works underneath, from the bottom up,&lt;/item&gt;
      &lt;item&gt;evaluate tradeoffs of different designs and ideas,&lt;/item&gt;
      &lt;item&gt;implement a principled design (a simple microprocessor),&lt;/item&gt;
      &lt;item&gt;learn to systematically debug increasingly complex systems,&lt;/item&gt;
      &lt;item&gt;hopefully be prepared to develop novel, out-of-the-box designs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The focus is on basics, principles, precedents, and how to use them to create/implement good designs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lectures&lt;/head&gt;
    &lt;p&gt; Thursday, 14:15-16:00, in HG F7 (Overflow room: HG F5) &lt;lb/&gt; Friday, 14:15-16:00, in HG F7 (Overflow room: HG F5) &lt;/p&gt;
    &lt;p&gt;Watch the lectures in YouTube livestream:&lt;/p&gt;
    &lt;head rend="h3"&gt;Lab sessions&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;When?&lt;/cell&gt;
        &lt;cell role="head"&gt;Where?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Tuesday, 16:15-18:00&lt;/cell&gt;
        &lt;cell&gt;labs in HG E19, HG E26.1, HG E26.3, HG E27&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Wednesday, 16:15-18:00&lt;/cell&gt;
        &lt;cell&gt;labs in HG E19, HG E26.1, HG E26.3, HG E27&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Friday, 08:15-10:00&lt;/cell&gt;
        &lt;cell&gt;labs in HG D11, HG D12, HG E26.3, HG E27&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Friday, 10:15-12:00&lt;/cell&gt;
        &lt;cell&gt;labs in HG E19, HG E26.1, HG E26.3, HG E27&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Prerequisites: None.&lt;/p&gt;
    &lt;head rend="h2"&gt;Staff Information&lt;/head&gt;
    &lt;head rend="h3"&gt;Contact&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mailing List: digitaltechnik@lists.inf.ethz.ch (sent to instructor and TAs)&lt;/item&gt;
      &lt;item&gt;Office Hours: TBD&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Office&lt;/cell&gt;
        &lt;cell role="head"&gt;Phone&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Instructor&lt;/cell&gt;
        &lt;cell&gt;Onur Mutlu&lt;/cell&gt;
        &lt;cell&gt;onur.mutlu@safari.ethz.ch&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Instructor&lt;/cell&gt;
        &lt;cell&gt;Mohammad Sadrosadati&lt;/cell&gt;
        &lt;cell&gt;mohammad.sadrosadati@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ F76&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Head Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Ataberk Olgun&lt;/cell&gt;
        &lt;cell&gt;ataberk.olgun@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ H61.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Giray Yaglikci&lt;/cell&gt;
        &lt;cell&gt;giray.yaglikci@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ H61.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Can Firtina&lt;/cell&gt;
        &lt;cell&gt;can.firtina@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Geraldo De Oliveira Junior&lt;/cell&gt;
        &lt;cell&gt;geraldod@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ 61.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Rahul Bera&lt;/cell&gt;
        &lt;cell&gt;rahbera@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ H64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Konstantinos Kanellopoulos&lt;/cell&gt;
        &lt;cell&gt;kanellok@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ H61.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Nika Mansouri Ghiasi&lt;/cell&gt;
        &lt;cell&gt;mnika@ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ 61.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Nisa Bostancƒ±&lt;/cell&gt;
        &lt;cell&gt;nisa.bostanci@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ 61.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Rakesh Nadig&lt;/cell&gt;
        &lt;cell&gt;rakesh.nadig@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ H64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;ƒ∞smail Emir Y√ºksel&lt;/cell&gt;
        &lt;cell&gt;ETZ H61.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Haocong Luo&lt;/cell&gt;
        &lt;cell&gt;ETZ H61.2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://safari.ethz.ch/ddca/spring2025/doku.php?id=start"/><published>2025-11-30T17:45:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46098838</id><title>Writing a good Claude.md</title><updated>2025-12-01T11:36:48.984625+00:00</updated><content>&lt;doc fingerprint="474c0a5f0dbfd7fe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;# Writing a good CLAUDE.md&lt;/head&gt;
    &lt;p&gt;Kyle Mistele ¬∑ November 25, 2025 ¬∑ &amp;lt; 10 min read&lt;/p&gt;
    &lt;p&gt;Note: this post is also applicable to &lt;code&gt;AGENTS.md&lt;/code&gt;, the open-source equivalent of &lt;code&gt;CLAUDE.md&lt;/code&gt; for agents and harnesses like OpenCode, Zed, Cursor and Codex.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Principle: LLMs are (mostly) stateless&lt;/head&gt;
    &lt;p&gt;LLMs are stateless functions. Their weights are frozen by the time they're used for inference, so they don't learn over time. The only thing that the model knows about your codebase is the tokens you put into it.&lt;/p&gt;
    &lt;p&gt;Similarly, coding agent harnesses such as Claude Code usually require you to manage agents' memory explicitly. &lt;code&gt;CLAUDE.md&lt;/code&gt; (or &lt;code&gt;AGENTS.md&lt;/code&gt;) is the only file that by default goes into every single conversation you have with the agent.&lt;/p&gt;
    &lt;p&gt;This has three important implications:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Coding agents know absolutely nothing about your codebase at the beginning of each session.&lt;/item&gt;
      &lt;item&gt;The agent must be told anything that's important to know about your codebase each time you start a session.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;is the preferred way of doing this.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;## &lt;code&gt;CLAUDE.md&lt;/code&gt; onboards Claude to your codebase&lt;/head&gt;
    &lt;p&gt;Since Claude doesn't know anything about your codebase at the beginning of each session, you should use &lt;code&gt;CLAUDE.md&lt;/code&gt; to onboard Claude into your codebase. At a high level, this means it should cover:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WHAT: tell Claude about the tech, your stack, the project structure. Give Claude a map of the codebase. This is especially important in monorepos! Tell Claude what the apps are, what the shared packages are, and what everything is for so that it knows where to look for things&lt;/item&gt;
      &lt;item&gt;WHY: tell Claude the purpose of the project and what everything is doing in the repository. What are the purpose and function of the different parts of the project?&lt;/item&gt;
      &lt;item&gt;HOW: tell Claude how it should work on the project. For example, do you use &lt;code&gt;bun&lt;/code&gt;instead of&lt;code&gt;node&lt;/code&gt;? You want to include all the information it needs to actually do meaningful work on the project. How can Claude verify Claude's changes? How can it run tests, typechecks, and compilation steps?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the way you do this is important! Don't try to stuff every command Claude could possibly need to run in your &lt;code&gt;CLAUDE.md&lt;/code&gt; file - you will get sub-optimal results.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Claude often ignores &lt;code&gt;CLAUDE.md&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Regardless of which model you're using, you may notice that Claude frequently ignores your &lt;code&gt;CLAUDE.md&lt;/code&gt; file's contents.&lt;/p&gt;
    &lt;p&gt;You can investigate this yourself by putting a logging proxy between the claude code CLI and the Anthropic API using &lt;code&gt;ANTHROPIC_BASE_URL&lt;/code&gt;. Claude code injects the following system reminder with your &lt;code&gt;CLAUDE.md&lt;/code&gt; file in the user message to the agent:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;&amp;lt;system-reminder&amp;gt; IMPORTANT: this context may or may not be relevant to your tasks. You should not respond to this context unless it is highly relevant to your task. &amp;lt;/system-reminder&amp;gt;&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;As a result, Claude will ignore the contents of your &lt;code&gt;CLAUDE.md&lt;/code&gt; if it decides that it is not relevant to its current task. The more information you have in the file that's not universally applicable to the tasks you have it working on, the more likely it is that Claude will ignore your instructions in the file.&lt;/p&gt;
    &lt;p&gt;Why did Anthropic add this? It's hard to say for sure, but we can speculate a bit. Most &lt;code&gt;CLAUDE.md&lt;/code&gt; files we come across include a bunch of instructions in the file that aren't broadly applicable. Many users treat the file as a way to add "hotfixes" to behavior they didn't like by appending lots of instructions that weren't necessarily broadly applicable.&lt;/p&gt;
    &lt;p&gt;We can only assume that the Claude Code team found that by telling Claude to ignore the bad instructions, the harness actually produced better results.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Creating a good &lt;code&gt;CLAUDE.md&lt;/code&gt; file&lt;/head&gt;
    &lt;p&gt;The following section provides a number of recommendations on how to write a good &lt;code&gt;CLAUDE.md&lt;/code&gt; file following context engineering best practices.&lt;/p&gt;
    &lt;p&gt;Your mileage may vary. Not all of these rules are necessarily optimal for every setup. Like anything else, feel free to break the rules once...&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;you understand when &amp;amp; why it's okay to break them&lt;/item&gt;
      &lt;item&gt;you have a good reason to do so&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;### Less (instructions) is more&lt;/head&gt;
    &lt;p&gt;It can be tempting to try and stuff every single command that claude could possibly need to run, as well as your code standards and style guidelines into &lt;code&gt;CLAUDE.md&lt;/code&gt;. We recommend against this.&lt;/p&gt;
    &lt;p&gt;Though the topic hasn't been investigated in an incredibly rigorous manner, some research has been done which indicates the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Frontier thinking LLMs can follow ~ 150-200 instructions with reasonable consistency. Smaller models can attend to fewer instructions than larger models, and non-thinking models can attend to fewer instructions than thinking models.&lt;/item&gt;
      &lt;item&gt;Smaller models get MUCH worse, MUCH more quickly. Specifically, smaller models tend to exhibit an expotential decay in instruction-following performance as the number of instructions increase, whereas larger frontier thinking models exhibit a linear decay (see below). For this reason, we recommend against using smaller models for multi-step tasks or complicated implementation plans.&lt;/item&gt;
      &lt;item&gt;LLMs bias towards instructions that are on the peripheries of the prompt: at the very beginning (the Claude Code system message and &lt;code&gt;CLAUDE.md&lt;/code&gt;), and at the very end (the most-recent user messages)&lt;/item&gt;
      &lt;item&gt;As instruction count increases, instruction-following quality decreases uniformly. This means that as you give the LLM more instructions, it doesn't simply ignore the newer ("further down in the file") instructions - it begins to ignore all of them uniformly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our analysis of the Claude Code harness indicates that Claude Code's system prompt contains ~50 individual instructions. Depending on the model you're using, that's nearly a third of the instructions your agent can reliably follow already - and that's before rules, plugins, skills, or user messages.&lt;/p&gt;
    &lt;p&gt;This implies that your &lt;code&gt;CLAUDE.md&lt;/code&gt; file should contain as few instructions as possible - ideally only ones which are universally applicable to your task.&lt;/p&gt;
    &lt;head rend="h3"&gt;### &lt;code&gt;CLAUDE.md&lt;/code&gt; file length &amp;amp; applicability&lt;/head&gt;
    &lt;p&gt;All else being equal, an LLM will perform better on a task when its' context window is full of focused, relevant context including examples, related files, tool calls, and tool results compared to when its context window has a lot of irrelevant context.&lt;/p&gt;
    &lt;p&gt;Since &lt;code&gt;CLAUDE.md&lt;/code&gt; goes into every single session, you should ensure that its contents are as universally applicable as possible.&lt;/p&gt;
    &lt;p&gt;For example, avoid including instructions about (for example) how to structure a new database schema - this won't matter and will distract the model when you're working on something else that's unrelated!&lt;/p&gt;
    &lt;p&gt;Length-wise, the less is more principle applies as well. While Anthropic does not have an official recommendation on how long your &lt;code&gt;CLAUDE.md&lt;/code&gt; file should be, general consensus is that &amp;lt; 300 lines is best, and shorter is even better.&lt;/p&gt;
    &lt;p&gt;At HumanLayer, our root &lt;code&gt;CLAUDE.md&lt;/code&gt; file is less than sixty lines.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Progressive Disclosure&lt;/head&gt;
    &lt;p&gt;Writing a concise &lt;code&gt;CLAUDE.md&lt;/code&gt; file that covers everything you want Claude to know can be challenging, especially in larger projects.&lt;/p&gt;
    &lt;p&gt;To address this, we can leverage the principle of Progressive Disclosure to ensure that claude only sees task- or project-specific instructions when it needs them.&lt;/p&gt;
    &lt;p&gt;Instead of including all your different instructions about building your project, running tests, code conventions, or other important context in your &lt;code&gt;CLAUDE.md&lt;/code&gt; file, we recommend keeping task-specific instructions in separate markdown files with self-descriptive names somewhere in your project.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;agent_docs/ |- building_the_project.md |- running_tests.md |- code_conventions.md |- service_architecture.md |- database_schema.md |- service_communication_patterns.md&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Then, in your &lt;code&gt;CLAUDE.md&lt;/code&gt; file, you can include a list of these files with a brief description of each, and instruct Claude to decide which (if any) are relevant and to read them before it starts working. Or, ask Claude to present you with the files it wants to read for aproval first before reading them.&lt;/p&gt;
    &lt;p&gt;Prefer pointers to copies. Don't include code snippets in these files if possible - they will become out-of-date quickly. Instead, include &lt;code&gt;file:line&lt;/code&gt; references to point Claude to the authoritative context.&lt;/p&gt;
    &lt;p&gt;Conceptually, this is very similar to how Claude Skills are intended to work, although skills are more focused on tool use than instructions.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Claude is (not) an expensive linter&lt;/head&gt;
    &lt;p&gt;One of the most common things that we see people put in their &lt;code&gt;CLAUDE.md&lt;/code&gt; file is code style guidelines. Never send an LLM to do a linter's job. LLMs are comparably expensive and incredibly slow compared to traditional linters and formatters. We think you should always use deterministic tools whenever you can.&lt;/p&gt;
    &lt;p&gt;Code style guidelines will inevitably add a bunch of instructions and mostly-irrelevant code snippets into your context window, degrading your LLM's performance and instruction-following and eating up your context window.&lt;/p&gt;
    &lt;p&gt;LLMs are in-context learners! If your code follows a certain set of style guidelines or patterns, you should find that armed with a few searches of your codebase (or a good research document!) your agent should tend to follow existing code patterns and conventions without being told to.&lt;/p&gt;
    &lt;p&gt;If you feel very stronly about this, you might even consider setting up a Claude Code &lt;code&gt;Stop&lt;/code&gt; hook that runs your formatter &amp;amp; linter and presents errors to Claude for it to fix. Don't make Claude find the formatting issues itself.&lt;/p&gt;
    &lt;p&gt;Bonus points: use a linter that can automatically fix issues (we like Biome), and carefully tune your rules about what can safely be auto-fixed for maximum (safe) coverage.&lt;/p&gt;
    &lt;p&gt;You could also create a Slash Command that includes your code guidelines and which points claude at the changes in version control, or at your &lt;code&gt;git status&lt;/code&gt;, or similar. This way, you can handle implementation and formatting separately. You will see better results with both as a result.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Don't use &lt;code&gt;/init&lt;/code&gt; or auto-generate your &lt;code&gt;CLAUDE.md&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Both Claude Code and other harnesses with OpenCode come with ways to auto-generate your &lt;code&gt;CLAUDE.md&lt;/code&gt; file (or &lt;code&gt;AGENTS.md&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Because &lt;code&gt;CLAUDE.md&lt;/code&gt; goes into every single session with Claude code, it is one of the highest leverage points of the harness - for better or for worse, depending on how you use it.&lt;/p&gt;
    &lt;p&gt;A bad line of code is a bad line of code. A bad line of an implementation plan has the potential to create a lot of bad lines of code. A bad line of a research that misunderstands how the system works has the potential to result in a lot of bad lines in the plan, and therefore a lot more bad lines of code as a result.&lt;/p&gt;
    &lt;p&gt;But the &lt;code&gt;CLAUDE.md&lt;/code&gt; file affects every single phase of your workflow and every single artifact produced by it. As a result, we think you should spend some time thinking very carefully about every single line that goes into it:&lt;/p&gt;
    &lt;head rend="h2"&gt;## In Conclusion&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;is for onboarding Claude into your codebase. It should define your project's WHY, WHAT, and HOW.&lt;/item&gt;
      &lt;item&gt;Less (instructions) is more. While you shouldn't omit necessary instructions, you should include as few instructions as reasonably possible in the file.&lt;/item&gt;
      &lt;item&gt;Keep the contents of your &lt;code&gt;CLAUDE.md&lt;/code&gt;concise and universally applicable.&lt;/item&gt;
      &lt;item&gt;Use Progressive Disclosure - don't tell Claude all the information you could possibly want it to know. Rather, tell it how to find important information so that it can find and use it, but only when it needs to to avoid bloating your context window or instruction count.&lt;/item&gt;
      &lt;item&gt;Claude is not a linter. Use linters and code formatters, and use other features like Hooks and Slash Commands as necessary.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;is the highest leverage point of the harness, so avoid auto-generating it. You should carefully craft its contents for best results.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.humanlayer.dev/blog/writing-a-good-claude-md"/><published>2025-11-30T17:56:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46099108</id><title>Program-of-Thought Prompting Outperforms Chain-of-Thought by 15% (2022)</title><updated>2025-12-01T11:36:48.685850+00:00</updated><content>&lt;doc fingerprint="e8bcc40accf7908a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computation and Language&lt;/head&gt;&lt;p&gt; [Submitted on 22 Nov 2022 (v1), last revised 23 Oct 2023 (this version, v4)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks&lt;/head&gt;View PDF&lt;quote&gt;Abstract:Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github this https URL&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Wenhu Chen [view email]&lt;p&gt;[v1] Tue, 22 Nov 2022 21:06:00 UTC (8,689 KB)&lt;/p&gt;&lt;p&gt;[v2] Fri, 25 Nov 2022 01:49:50 UTC (8,689 KB)&lt;/p&gt;&lt;p&gt;[v3] Tue, 29 Nov 2022 03:46:29 UTC (8,689 KB)&lt;/p&gt;&lt;p&gt;[v4] Mon, 23 Oct 2023 01:27:38 UTC (4,047 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2211.12588"/><published>2025-11-30T18:34:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46100892</id><title>A Love Letter to FreeBSD</title><updated>2025-12-01T11:36:48.236915+00:00</updated><content>&lt;doc fingerprint="378a913fe1694f6e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Love Letter to FreeBSD&lt;/head&gt;
    &lt;p&gt;Dear FreeBSD,&lt;/p&gt;
    &lt;p&gt;I‚Äôm still the new person here, learning your ways, stumbling over the occasional quirk, smiling when I find the small touches that make you different. You remind me of what computing felt like before the noise. Before hype cycles and performance theatre. Before every tool needed a plugin system and a logo. You are coherent. You are deliberate. You are the kind of system that doesn‚Äôt have to shout to belong.&lt;/p&gt;
    &lt;p&gt;You carry the quiet strength of the greats, like a mainframe humming in a locked room, not chasing attention, just doing its work, year after year. Your base system feels like it was built by people who cared about the whole picture, not just the pieces. Your boot environments are like an old IBM i‚Äôs ‚Äúside A / side B‚Äù IPL, a built-in escape hatch that says, we‚Äôve thought ahead for you. You could be, you should be, the open-source mainframe: aligned with hardware lifecycles of three to five years or more, built for long-term trust, a platform people bet their uptime on. Your core design reminds me of Solaris in its best days: a stable base that commercial and community software could rely on without fear of shifting foundations.&lt;/p&gt;
    &lt;p&gt;And make uptime a design goal: a thousand-day uptime shouldn‚Äôt be folklore, it should be normal. Not a party trick, not a screenshot to boast about, but simply the natural consequence of a system built to endure. Mainframes never apologised for uptime measured in years, and neither should you. Apply updates without fear, reboot only when the kernel truly demands it, and let administrators see longevity as a feature, not a gamble.&lt;/p&gt;
    &lt;p&gt;I know you are reaching further into the desktop now. I understand why, and I can see how it might widen your reach. But here I find myself wondering: how do you keep the heartbeat of a rock-solid server while also embracing the quicker pulse of a modern desktop? I don‚Äôt pretend to have all the answers, I‚Äôm too new to you for that, but my first instinct is to lean on what you already have: the natural separation between CURRENT and RELEASE. Let those worlds move at their own pace, without asking one to carry the other‚Äôs compromises.&lt;/p&gt;
    &lt;p&gt;And now, with pkgbase in play, the stability of packages matters as much as the base system itself. The base must remain untouchable in its reliability, but I dream of a world where the package ecosystem is available in clear stability channels: from a rock-solid ‚Äúproduction tier‚Äù you can stake a business on, to faster-moving streams where new features can flow without fear of breaking mission-critical systems. Too many times in the past, packages vanished or broke unexpectedly. I understand the core is sacred, but I wouldn‚Äôt mind if some of the wider ecosystem inherited that same level of care.&lt;/p&gt;
    &lt;p&gt;Culture matters too. One reason I stepped away from Linux was the noise, the debates that drowned out the joy of building. Please keep FreeBSD the kind of place where thoughtful engineering is welcome without ego battles, where enterprise focus and technical curiosity can sit at the same table. That spirit, the calm, shared purpose that carried Unix from the PDP-11 labs to the backbone of the Internet, is worth protecting.&lt;/p&gt;
    &lt;p&gt;There‚Äôs also the practical side: keep the doors open with hardware vendors like Dell and HPE, so FreeBSD remains a first-class citizen. Give me the tools to flash firmware without having to borrow Linux or Windows. Make hardware lifecycle alignment part of your story, major releases paced with the real world, point releases treated as refinement rather than disruption.&lt;/p&gt;
    &lt;p&gt;My hope is simple: that you stay different. Not in the way that shouts for attention, but in the way that earns trust. If someone wants hype or the latest shiny thing every month, they have Linux. If they want a platform that feels like it could simply run, and keep running, the way the best of Unix always did, they should know they can find it here. And I still dream of a future where a purpose-built ‚Äúopen-source mainframe‚Äù exists: a modern, reliable hardware system running FreeBSD with the same quiet presence as Sun‚Äôs Enterprise 10k once did.&lt;/p&gt;
    &lt;p&gt;And maybe, one day, someone will walk past a rack of servers, hear the steady, unhurried rhythm of a FreeBSD system still running, and smile, knowing that in a world that burns through trends, there is still something built to last.&lt;/p&gt;
    &lt;p&gt;With gratitude,&lt;lb/&gt; and with the wish to stay for the long run,&lt;lb/&gt; A newcomer who finally feels at home.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tara.sh/posts/2025/2025-11-25_freebsd_letter/"/><published>2025-11-30T22:05:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46101263</id><title>Bricklink suspends Marketplace operations in 35 countries</title><updated>2025-12-01T11:36:47.145867+00:00</updated><content>&lt;doc fingerprint="f9211993d3ce0ac8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bricklink suspends Marketplace operations in 35 countries (developing story)&lt;/head&gt;
    &lt;p&gt;Bricklink, LEGO‚Äôs biggest online marketplace and lifeblood of the LEGO community has made quite an alarming announcement. From 12 December 2025, BrickLink Marketplace will not be available for members in 35 countries around the world, which means that LEGO and Bricklink members will no longer be able to buy or sell on the Marketplace.&lt;/p&gt;
    &lt;p&gt;See Bricklink‚Äôs statement and list of countries impacted.&lt;/p&gt;
    &lt;p&gt;This is very surprising as the list of countries (listed below) are pretty established, and are home to thousands of AFOLs and LEGO fans, and are pretty mature markets for LEGO all things considered. These include South Africa, Turkey, Brazil, Indonesia, Egypt, Israel, Taiwan and even huge swathes of the Middle East.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Indonesia&lt;/item&gt;
      &lt;item&gt;Turkey&lt;/item&gt;
      &lt;item&gt;South Africa&lt;/item&gt;
      &lt;item&gt;Taiwan&lt;/item&gt;
      &lt;item&gt;Ukraine&lt;/item&gt;
      &lt;item&gt;Brazil&lt;/item&gt;
      &lt;item&gt;Serbia&lt;/item&gt;
      &lt;item&gt;United Arab Emirates&lt;/item&gt;
      &lt;item&gt;Kazakhstan&lt;/item&gt;
      &lt;item&gt;Peru&lt;/item&gt;
      &lt;item&gt;Israel&lt;/item&gt;
      &lt;item&gt;India&lt;/item&gt;
      &lt;item&gt;Morocco&lt;/item&gt;
      &lt;item&gt;Chile&lt;/item&gt;
      &lt;item&gt;Vietnam&lt;/item&gt;
      &lt;item&gt;Georgia&lt;/item&gt;
      &lt;item&gt;Lebanon&lt;/item&gt;
      &lt;item&gt;Saudi Arabia&lt;/item&gt;
      &lt;item&gt;Qatar&lt;/item&gt;
      &lt;item&gt;Oman&lt;/item&gt;
      &lt;item&gt;El Salvador&lt;/item&gt;
      &lt;item&gt;Bahrain&lt;/item&gt;
      &lt;item&gt;Azerbaijan&lt;/item&gt;
      &lt;item&gt;Armenia&lt;/item&gt;
      &lt;item&gt;Pakistan&lt;/item&gt;
      &lt;item&gt;Egypt&lt;/item&gt;
      &lt;item&gt;Moldova&lt;/item&gt;
      &lt;item&gt;Ecuador&lt;/item&gt;
      &lt;item&gt;Argentina&lt;/item&gt;
      &lt;item&gt;Costa Rica&lt;/item&gt;
      &lt;item&gt;Colombia&lt;/item&gt;
      &lt;item&gt;Bosnia and Herzegovina&lt;/item&gt;
      &lt;item&gt;Turkmenistan&lt;/item&gt;
      &lt;item&gt;Greenland&lt;/item&gt;
      &lt;item&gt;San Marino&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To put this into perspective, the total combined population of these countries exceed 2.5 billion, or just about 30% of Earth‚Äôs population which is wild.&lt;/p&gt;
    &lt;p&gt;This is a massive shift, and while I understand that Bricklink might have issues operating in some of these countries which are located in remote locations (Greenland), or in a state of war (Ukraine), a blanket ban on these 35 countries with less than 2 weeks notice is crazy.&lt;/p&gt;
    &lt;p&gt;There are plenty of Bricklink sellers in these countries who run their Bricklink stores as businesses, and rely on the income to support their livelihoods and families, so this really should‚Äôve been communicated far ahead of time. I can‚Äôt imagine how stressful this might be to read for sellers in those countries.&lt;/p&gt;
    &lt;p&gt;Then there are LEGO fans in those countries, who will also be barred from buying on Bricklink, which is a whole other level of insane. LEGO fans, and LUGs rely deeply on Bricklink to source elements to express their creativity, as not every country in the world has official LEGO Stores, or access to Pick a Brick online, and Bricklink in some cases are the only way fans can buy LEGO sets too.&lt;/p&gt;
    &lt;p&gt;And these aren‚Äôt just any other countries, but countries where LEGO has an official presence in with LEGO Certified Stores, 3rd party retailers, and in the case of the UAE, even a Legoland Park. Oh, and Greenland‚Äôs on the list. That Greenland, a Danish territory.&lt;/p&gt;
    &lt;p&gt;There are also recognised LUG communities in these countries as well, so it‚Äôs not like The LEGO Group have no interest in nurturing LEGO fans and creativity in these countries.&lt;/p&gt;
    &lt;p&gt;This will absolutely smother and stifle LEGO fans and communities in these countries, and I cannot for the life of me understand why such drastic action is required.&lt;/p&gt;
    &lt;p&gt;It‚Äôs also ironic as Bricklink celebrated their 25th Anniversary this year, with quite a bit of fanfare, and to close out the year by barring 30% of the world‚Äôs population from using the service is in my opinion, an absolute slap in the face of Bricklink founder Dan Jezek, whose vision was to create a global LEGO marketplace and connect fans and elements from around the world.&lt;/p&gt;
    &lt;p&gt;Six years ago, I wrote that it was a terrible idea for LEGO to acquire Bricklink and revisiting some of my thoughts I expressed then, it sure seems like there‚Äôs some dodgy stuff happening behind the scenes.&lt;/p&gt;
    &lt;p&gt;To be fair, I acknowledge that there may be compliance challenges operating in some of these countries, where things like local laws, logistics, import restrictions etc may make it difficult for LEGO/Bricklink to do their business there, but surely there could‚Äôve been a better way to communicate this, or invite community feedback instead of turning the whole site off in 2 weeks.&lt;/p&gt;
    &lt;p&gt;Apart from the sudden forum message, there has been no other comment or statement from LEGO or Bricklink about this decision, and with the Bricklink head office based in California, they likely won‚Äôt issue more communication on this issue until after the Thanksgiving weekend.&lt;/p&gt;
    &lt;p&gt;If this news upsets you in any way, please share your thoughts by leaving a comment on this post, or if you can manage navigating the extremely archaic Bricklink forums, leave a comment on the Bricklink post announcing this shocking news.&lt;/p&gt;
    &lt;p&gt;See full text below from Bricklink below:&lt;/p&gt;
    &lt;p&gt;Dear BrickLink members,&lt;/p&gt;
    &lt;p&gt;We‚Äôve made the difficult decision to stop Marketplace operations in some areas:&lt;lb/&gt;https://www.bricklink.com/help.asp?helpID=2687&lt;/p&gt;
    &lt;p&gt;Starting December 12, 2025, BrickLink members in these countries will no longer&lt;lb/&gt;be able to buy or sell on the Marketplace.&lt;/p&gt;
    &lt;p&gt;We will review this decision regularly, and we hope to be able to reopen the&lt;lb/&gt;BrickLink Marketplace to LEGO¬Æ fans in these countries in the future.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs an FAQ for members in these countries:&lt;/p&gt;
    &lt;p&gt;I have pending orders. What will happen to them?&lt;/p&gt;
    &lt;p&gt;Starting December 12, 2025 you will no longer be able to place new orders. Any&lt;lb/&gt;orders placed before that will be processed as usual.&lt;/p&gt;
    &lt;p&gt;What happens to my wanted lists?&lt;/p&gt;
    &lt;p&gt;You will still be able to access and manage your wanted lists. If you would like&lt;lb/&gt;to export any of your wanted list, use the ‚ÄòDownload‚Äô button at the top of the&lt;lb/&gt;page.&lt;/p&gt;
    &lt;p&gt;Will I keep my store inventory?&lt;/p&gt;
    &lt;p&gt;You will still be able to access your store inventory, but it‚Äôs always a good&lt;lb/&gt;idea to have a copy of your inventory. You can download it using this page: https://www.bricklink.com/invExcel.asp&lt;/p&gt;
    &lt;p&gt;We appreciate your understanding,&lt;lb/&gt;The BrickLink Team&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jaysbrickblog.com/news/bricklink-suspends-marketplace-operations-in-35-countries/"/><published>2025-11-30T22:53:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46101492</id><title>Algorithms for Optimization [pdf]</title><updated>2025-12-01T11:36:46.529371+00:00</updated><content/><link href="https://algorithmsbook.com/optimization/files/optimization.pdf"/><published>2025-11-30T23:21:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46101996</id><title>Ly ‚Äì A lightweight TUI (ncurses-like) display manager for Linux and BSD</title><updated>2025-12-01T11:36:45.412086+00:00</updated><content>&lt;doc fingerprint="688ed55b0b05e3a1"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"/&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;.github&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;res&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;src&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;.gitignore&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;build.zig&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;build.zig.zon&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;license.md&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;readme.md&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h1"&gt;The Ly display manager&lt;/head&gt;
    &lt;p&gt;Ly is a lightweight TUI (ncurses-like) display manager for Linux and BSD, designed with portability in mind (e.g. it does not require systemd to run).&lt;/p&gt;
    &lt;p&gt;Join us on Matrix over at #ly:envs.net!&lt;/p&gt;
    &lt;p&gt;Note: Development happens on Codeberg with a mirror on GitHub.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dependencies&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compile-time: &lt;list rend="ul"&gt;&lt;item&gt;zig 0.15.x&lt;/item&gt;&lt;item&gt;libc&lt;/item&gt;&lt;item&gt;pam&lt;/item&gt;&lt;item&gt;xcb (optional, required by default; needed for X11 support)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Runtime (with default config): &lt;list rend="ul"&gt;&lt;item&gt;xorg&lt;/item&gt;&lt;item&gt;xorg-xauth&lt;/item&gt;&lt;item&gt;shutdown&lt;/item&gt;&lt;item&gt;brightnessctl&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Debian&lt;/head&gt;
    &lt;code&gt;# apt install build-essential libpam0g-dev libxcb-xkb-dev xauth xserver-xorg brightnessctl
&lt;/code&gt;
    &lt;head rend="h3"&gt;Fedora&lt;/head&gt;
    &lt;p&gt;Warning: You may encounter issues with SELinux on Fedora. It is recommended to add a rule for Ly as it currently does not ship one.&lt;/p&gt;
    &lt;code&gt;# dnf install kernel-devel pam-devel libxcb-devel zig xorg-x11-xauth xorg-x11-server brightnessctl
&lt;/code&gt;
    &lt;head rend="h3"&gt;FreeBSD&lt;/head&gt;
    &lt;code&gt;# pkg install ca_root_nss libxcb git xorg xauth
&lt;/code&gt;
    &lt;head rend="h2"&gt;Availability&lt;/head&gt;
    &lt;head rend="h2"&gt;Support&lt;/head&gt;
    &lt;p&gt;Ly has been tested with a wide variety of desktop environments and window managers, all of which you can find in the sections below:&lt;/p&gt;
    &lt;p&gt;Logs are defined by &lt;code&gt;/etc/ly/config.ini&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The session log is located at &lt;code&gt;~/.local/state/ly-session.log&lt;/code&gt;by default.&lt;/item&gt;
      &lt;item&gt;The system log is located at &lt;code&gt;/var/log/ly.log&lt;/code&gt;by default.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Manually building&lt;/head&gt;
    &lt;p&gt;The procedure for manually building Ly is pretty standard:&lt;/p&gt;
    &lt;code&gt;$ git clone https://codeberg.org/fairyglade/ly.git
$ cd ly
$ zig build
&lt;/code&gt;
    &lt;p&gt;After building, you can (optionally) test Ly in a terminal emulator, although authentication will not work:&lt;/p&gt;
    &lt;code&gt;$ zig build run
&lt;/code&gt;
    &lt;p&gt;Important: While you can also run Ly in a terminal emulator as root, it is not recommended either. If you want to properly test Ly, please enable its service (as described below) and reboot your machine.&lt;/p&gt;
    &lt;p&gt;The following sections show how to install Ly for a particular init system. Because the procedure is very similar for all of them, the commands will only be detailed for the first section (which is about systemd).&lt;/p&gt;
    &lt;p&gt;Note: All following sections will assume you are using LightDM for convenience sake.&lt;/p&gt;
    &lt;head rend="h3"&gt;systemd&lt;/head&gt;
    &lt;p&gt;Now, you can install Ly on your system:&lt;/p&gt;
    &lt;code&gt;# zig build installexe -Dinit_system=systemd
&lt;/code&gt;
    &lt;p&gt;Note: The &lt;code&gt;init_system&lt;/code&gt; parameter is optional and defaults to &lt;code&gt;systemd&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Note that you also need to disable your current display manager. For example, if LightDM is the current display manager, you can execute the following command:&lt;/p&gt;
    &lt;code&gt;# systemctl disable lightdm.service
&lt;/code&gt;
    &lt;p&gt;Then, similarly to the previous command, you need to enable the Ly service:&lt;/p&gt;
    &lt;code&gt;# systemctl enable ly.service
&lt;/code&gt;
    &lt;p&gt;Important: Because Ly runs in a TTY, you must disable the TTY service that Ly will run on, otherwise bad things will happen. For example, to disable &lt;code&gt;getty&lt;/code&gt; spawning on TTY 2 (the default TTY on which Ly spawns), you need to
execute the following command:&lt;/p&gt;
    &lt;code&gt;# systemctl disable getty@tty2.service
&lt;/code&gt;
    &lt;p&gt;You can change the TTY Ly will run on by editing the corresponding service file for your platform.&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenRC&lt;/head&gt;
    &lt;code&gt;# zig build installexe -Dinit_system=openrc
# rc-update del lightdm
# rc-update add ly
# rc-update del agetty.tty2
&lt;/code&gt;
    &lt;p&gt;Note: On Gentoo specifically, you also must comment out the appropriate line for the TTY in /etc/inittab.&lt;/p&gt;
    &lt;head rend="h3"&gt;runit&lt;/head&gt;
    &lt;code&gt;# zig build installexe -Dinit_system=runit
# rm /var/service/lightdm
# ln -s /etc/sv/ly /var/service/
# rm /var/service/agetty-tty2
&lt;/code&gt;
    &lt;head rend="h3"&gt;s6&lt;/head&gt;
    &lt;code&gt;# zig build installexe -Dinit_system=s6
# s6-rc -d change lightdm
# s6-service add default ly-srv
# s6-db-reload
# s6-rc -u change ly-srv
&lt;/code&gt;
    &lt;p&gt;To disable TTY 2, edit &lt;code&gt;/etc/s6/config/tty2.conf&lt;/code&gt; and set &lt;code&gt;SPAWN="no"&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;dinit&lt;/head&gt;
    &lt;code&gt;# zig build installexe -Dinit_system=dinit
# dinitctl disable lightdm
# dinitctl enable ly
&lt;/code&gt;
    &lt;p&gt;To disable TTY 2, go to &lt;code&gt;/etc/dinit.d/config/console.conf&lt;/code&gt; and modify
&lt;code&gt;ACTIVE_CONSOLES&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;sysvinit&lt;/head&gt;
    &lt;code&gt;# zig build installexe -Dinit_system=sysvinit
# update-rc.d lightdm disable
# update-rc.d ly defaults
&lt;/code&gt;
    &lt;p&gt;To disable TTY 2, go to &lt;code&gt;/etc/inittab&lt;/code&gt; and comment out the line containing &lt;code&gt;tty2&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;FreeBSD&lt;/head&gt;
    &lt;code&gt;# zig build installexe -Dprefix_directory=/usr/local -Dconfig_directory=/usr/local/etc -Dinit_system=freebsd
# sysrc lightdm_enable="NO"
&lt;/code&gt;
    &lt;p&gt;To enable Ly, add the following entry to &lt;code&gt;/etc/gettytab&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;Ly:\
	:lo=/usr/local/bin/ly_wrapper:\
	:al=root:
&lt;/code&gt;
    &lt;p&gt;Then, modify the command field of the &lt;code&gt;ttyv1&lt;/code&gt; terminal entry in &lt;code&gt;/etc/ttys&lt;/code&gt;
(TTYs in FreeBSD start at 0):&lt;/p&gt;
    &lt;code&gt;ttyv1 "/usr/libexec/getty Ly" xterm on secure
&lt;/code&gt;
    &lt;head rend="h3"&gt;Updating&lt;/head&gt;
    &lt;p&gt;You can also install Ly without overrding the current configuration file. This is called updating. To update, simply run:&lt;/p&gt;
    &lt;code&gt;# zig build installnoconf
&lt;/code&gt;
    &lt;p&gt;You can, of course, still select the init system of your choice when using this command.&lt;/p&gt;
    &lt;head rend="h2"&gt;Configuration&lt;/head&gt;
    &lt;p&gt;You can find all the configuration in &lt;code&gt;/etc/ly/config.ini&lt;/code&gt;. The file is fully
commented, and includes the default values.&lt;/p&gt;
    &lt;head rend="h2"&gt;Controls&lt;/head&gt;
    &lt;p&gt;Use the Up/Down arrow keys to change the current field, and the Left/Right arrow keys to scroll through the different fields (whether it be the info line, the desktop environment, or the username). The info line is where messages and errors are displayed.&lt;/p&gt;
    &lt;head rend="h2"&gt;A note on .xinitrc&lt;/head&gt;
    &lt;p&gt;If your &lt;code&gt;.xinitrc&lt;/code&gt; file doesn't work ,make sure it is executable and includes a
shebang. This file is supposed to be a shell script! Quoting from &lt;code&gt;xinit&lt;/code&gt;'s man
page:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If no specific client program is given on the command line, xinit will look for a file in the user's home directory called .xinitrc to run as a shell script to start up client programs.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A typical shebang for a shell script looks like this:&lt;/p&gt;
    &lt;code&gt;#!/bin/sh
&lt;/code&gt;
    &lt;head rend="h2"&gt;Tips&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The numlock and capslock state is printed in the top-right corner.&lt;/item&gt;
      &lt;item&gt;Use the F1 and F2 keys to respectively shutdown and reboot.&lt;/item&gt;
      &lt;item&gt;Take a look at your &lt;code&gt;.xsession&lt;/code&gt;file if X doesn't start, as it can interfere (this file is launched with X to configure the display properly).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Supported Wayland environments&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;budgie&lt;/item&gt;
      &lt;item&gt;cosmic&lt;/item&gt;
      &lt;item&gt;deepin&lt;/item&gt;
      &lt;item&gt;enlightenment&lt;/item&gt;
      &lt;item&gt;gnome&lt;/item&gt;
      &lt;item&gt;hyprland&lt;/item&gt;
      &lt;item&gt;kde&lt;/item&gt;
      &lt;item&gt;labwc&lt;/item&gt;
      &lt;item&gt;niri&lt;/item&gt;
      &lt;item&gt;pantheon&lt;/item&gt;
      &lt;item&gt;sway&lt;/item&gt;
      &lt;item&gt;weston&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Supported X11 environments&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;awesome&lt;/item&gt;
      &lt;item&gt;bspwm&lt;/item&gt;
      &lt;item&gt;budgie&lt;/item&gt;
      &lt;item&gt;cinnamon&lt;/item&gt;
      &lt;item&gt;dwm&lt;/item&gt;
      &lt;item&gt;enlightenment&lt;/item&gt;
      &lt;item&gt;gnome&lt;/item&gt;
      &lt;item&gt;kde&lt;/item&gt;
      &lt;item&gt;leftwm&lt;/item&gt;
      &lt;item&gt;lxde&lt;/item&gt;
      &lt;item&gt;mate&lt;/item&gt;
      &lt;item&gt;maxx&lt;/item&gt;
      &lt;item&gt;pantheon&lt;/item&gt;
      &lt;item&gt;qwm&lt;/item&gt;
      &lt;item&gt;spectrwm&lt;/item&gt;
      &lt;item&gt;windowmaker&lt;/item&gt;
      &lt;item&gt;xfce&lt;/item&gt;
      &lt;item&gt;xmonad&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;A final note&lt;/head&gt;
    &lt;p&gt;The name "Ly" is a tribute to the fairy from the game Rayman. Ly was tested by oxodao, who is some seriously awesome dude.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://codeberg.org/fairyglade/ly"/><published>2025-12-01T00:28:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46102202</id><title>Is America's jobs market nearing a cliff?</title><updated>2025-12-01T11:36:45.176413+00:00</updated><content/><link href="https://www.economist.com/finance-and-economics/2025/11/30/is-americas-jobs-market-nearing-a-cliff"/><published>2025-12-01T00:58:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46102347</id><title>Advent of Sysadmin 2025</title><updated>2025-12-01T11:36:44.984063+00:00</updated><content>&lt;doc fingerprint="d370ffcfa78f27d3"&gt;
  &lt;main&gt;
    &lt;p&gt;The Advent of Sysadmin is a 12-day Advent calendar of Linux and DevOps challenges of different difficulties that runs from December 1st to December 12th.&lt;/p&gt;
    &lt;p&gt;Each day there will be an Advent of Sysadmin scenario.&lt;/p&gt;
    &lt;p&gt;Sign up for a free account (needed to keep track of your progress) and start solving the scenarios!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Day&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Level&lt;/cell&gt;
        &lt;cell role="head"&gt;Time Limit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-12-01&lt;/cell&gt;
        &lt;cell&gt;"Auderghem": containers miscommunication&lt;/cell&gt;
        &lt;cell&gt;Easy&lt;/cell&gt;
        &lt;cell&gt;15 mins&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sadservers.com/advent"/><published>2025-12-01T01:17:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46103097</id><title>X210Ai is a new motherboard to upgrade ThinkPad X201/200</title><updated>2025-12-01T11:36:43.845153+00:00</updated><content>&lt;doc fingerprint="ee8d8858f8eff1e4"&gt;
  &lt;main&gt;
    &lt;p&gt;X210Ai is a new motherboard to upgrade Thinkpad X201/200, The details as below:&lt;/p&gt;
    &lt;p&gt;1, CPU: Ultra 7 165H, Ultra 9 185H;&lt;/p&gt;
    &lt;p&gt;2, Storage support: Two of M.2 SSD PCIE 4.0 (one 2280, one 2242), the original 2.5inch SATA;&lt;/p&gt;
    &lt;p&gt;3, Memory: DDR5 5600MHz max to 128G (64G +64G);&lt;/p&gt;
    &lt;p&gt;4, Two type-c: one is support thunderbolt 4.0, other one is full function type-c;&lt;/p&gt;
    &lt;p&gt;5, Support Output HDMI 2.1;&lt;/p&gt;
    &lt;p&gt;6, Display: support screens same as X2100, such as the original X201/200‚Äôs display, 13inch 3000√ó2000, 13.3inch 1920√ó1200 and 13.3 inch 2560√ó1600;&lt;/p&gt;
    &lt;p&gt;7, WWAN 4/5G;&lt;/p&gt;
    &lt;p&gt;8, The postion of SATA can install the 2rd fan;&lt;/p&gt;
    &lt;p&gt;9, A new daughterboard;&lt;/p&gt;
    &lt;p&gt;10, The X210Ai project currently cannot guarantee coreboot support, but we remain committed to exploring possible solutions.&lt;/p&gt;
    &lt;p&gt;X210Ai time line please click here&lt;/p&gt;
    &lt;p&gt;X210Ai related file &amp;amp; software download&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tpart.net/about-x210ai/"/><published>2025-12-01T03:12:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46103376</id><title>Search tool that only returns content created before ChatGPT's public release</title><updated>2025-12-01T11:36:43.503879+00:00</updated><content>&lt;doc fingerprint="d94012a3200d538b"&gt;
  &lt;main&gt;
    &lt;p&gt;A browser extension for avoiding AI slop. Download it for Chrome or Firefox.&lt;/p&gt;
    &lt;p&gt;This is a search tool that will only return content created before ChatGPT's first public release on November 30, 2022.&lt;/p&gt;
    &lt;p&gt;Since the public release of ChatGTPT and other large language models, the internet is being increasingly polluted by AI generated text, images and video. This browser extension uses the Google search API to only return content published before Nov 30th, 2022 so you can be sure that it was written or produced by the human hand.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tegabrain.com/Slop-Evader"/><published>2025-12-01T04:06:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46103532</id><title>Google Antigravity just deleted the contents of whole drive</title><updated>2025-12-01T11:36:43.387599+00:00</updated><content/><link href="https://old.reddit.com/r/google_antigravity/comments/1p82or6/google_antigravity_just_deleted_the_contents_of/"/><published>2025-12-01T04:39:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46103657</id><title>SmartTube Compromised</title><updated>2025-12-01T11:36:40.943567+00:00</updated><content>&lt;doc fingerprint="ad2c930509fb3ed1"&gt;
  &lt;main&gt;
    &lt;p&gt;Earlier this week, the developer of SmartTube, the most popular alternative YouTube app for Android TV and Fire TV devices, announced that his app‚Äôs digital signature had been exposed. A new version of the app using a new digital signature has since been released. While everyone is encouraged to switch to the new app, SmartTube‚Äôs developer has shared more information with me about what happened that may make you want to take additional precautions if you‚Äôve installed or updated the app recently.&lt;/p&gt;
    &lt;p&gt;SmartTube‚Äôs developer told me that the computer used to create the APKs for the project‚Äôs official GitHub page was compromised by malware. As a result, some official SmartTube releases were unintentionally released with malware. It‚Äôs unclear which version was first affected, but the compromise seems to have first occurred earlier this month. SmartTube versions 30.43 and 30.47 from APKMirror are both being flagged as infected by malware scanners.&lt;/p&gt;
    &lt;p&gt;It is likely the presence of this malware that caused Google and Amazon to forcibly uninstall SmartTube on some devices, not the exposed digital signature as first suspected. SmartTube‚Äôs developer says the compromised machine has been wiped and is confident that both the new SmartTube releases and the machine that created them are malware-free.&lt;/p&gt;
    &lt;p&gt;All older versions of SmartTube have been removed from the project‚Äôs GitHub in an abundance of caution. While there does not appear to be any evidence that the app‚Äôs digital signature was actually stolen or used by malicious actors, that too has been abandoned and replaced with a new one.&lt;/p&gt;
    &lt;p&gt;SmartTube version 30.56 is the first release built by the uncompromised machine and with the new digital signature. It can be installed using my Downloader app by entering code &lt;/p&gt;
    &lt;p&gt;It remains unknown what the malware that found its way into the official SmartTube APK files can actually do. Thankfully, SmartTube is programmed to only request minimal account permissions and does not ask for any login information directly. Even if you granted the app access to your Google Drive for backup purposes, your Google account and general Google Drive files remain out of the app‚Äôs scope of permissions. Permissions regarding control of your YouTube account seem like the only thing that could have easily been exposed to the malware, as far as account access is concerned.&lt;/p&gt;
    &lt;p&gt;That said, since very little is know about the malware, you should assume the worst. If you use SmartTube and are concerned about your exposure to this malware, you should factory reset any device that had the app installed, especially if you installed or updated the app in November. It would also be a good idea to audit your Google account permissions and your YouTube account activity for anything unusual. Once your devices and account are in order, if you wish to reinstall SmartTube, be sure to only install the latest version through the codes/links above.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.aftvnews.com/smarttubes-official-apk-was-compromised-with-malware-what-you-should-do-if-you-use-it/"/><published>2025-12-01T05:01:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46104396</id><title>Games using anti-cheats and their compatibility with GNU/Linux or Wine/Proton</title><updated>2025-12-01T11:36:40.556448+00:00</updated><content>&lt;doc fingerprint="d422fc810f425674"&gt;
  &lt;main&gt;
    &lt;p&gt;Light theme&lt;/p&gt;
    &lt;p&gt;A comprehensive and crowd-sourced list of games using anti-cheats and their compatibility with GNU/Linux or Wine/Proton.- Starz0r&lt;/p&gt;
    &lt;p&gt;1136&lt;/p&gt;
    &lt;p&gt;194 Supported (17%)&lt;/p&gt;
    &lt;p&gt;258 Running (23%)&lt;/p&gt;
    &lt;p&gt;2 Planned (0%)&lt;/p&gt;
    &lt;p&gt;635 Broken (56%)&lt;/p&gt;
    &lt;p&gt;47 Denied (4%)&lt;/p&gt;
    &lt;p&gt;Search&lt;/p&gt;
    &lt;p&gt;Sort By&lt;/p&gt;
    &lt;p&gt;Sort Order&lt;/p&gt;
    &lt;p&gt;Halo: The Master Chief Collection&lt;/p&gt;
    &lt;p&gt;Supported&lt;/p&gt;
    &lt;p&gt;Denied&lt;/p&gt;
    &lt;p&gt;Battlefield‚Ñ¢ 2042&lt;/p&gt;
    &lt;p&gt;Paladins&lt;/p&gt;
    &lt;p&gt;Running&lt;/p&gt;
    &lt;p&gt;Make sure to check recent updates, the game is known to break often (And the status may not update to reflect that)&lt;/p&gt;
    &lt;p&gt;Broken&lt;/p&gt;
    &lt;p&gt;Black Desert Online&lt;/p&gt;
    &lt;p&gt;Requires Proton GE or Proton Experimental&lt;/p&gt;
    &lt;p&gt;Epic Games Store is broken on Linux. Steam Recommended.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://areweanticheatyet.com/"/><published>2025-12-01T07:05:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46105079</id><title>DeepSeek releases open-weights math model with IMO gold medal performance</title><updated>2025-12-01T11:36:40.332990+00:00</updated><content>&lt;doc fingerprint="cd4a93503ddb8ad8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning&lt;/head&gt;
    &lt;head rend="h2"&gt;1. Introduction&lt;/head&gt;
    &lt;p&gt;Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute. While much work remains, these results suggest that self-verifiable mathematical reasoning is a feasible research direction that may help develop more capable mathematical AI systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Evaluation Results&lt;/head&gt;
    &lt;p&gt;Below are evaluation results on IMO-ProofBench (developed by the DeepMind team behind DeepThink IMO-Gold) and recent mathematics competitions including IMO 2025, CMO 2024, and Putnam 2024.&lt;/p&gt;
    &lt;p&gt;IMO-ProofBench&lt;/p&gt;
    &lt;p&gt;Mathematics Competitions&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Quick Start&lt;/head&gt;
    &lt;p&gt;DeepSeekMath-V2 is built on top of DeepSeek-V3.2-Exp-Base. For inference support, please refer to the DeepSeek-V3.2-Exp github repository.&lt;/p&gt;
    &lt;head rend="h2"&gt;6. License&lt;/head&gt;
    &lt;p&gt;This repository and the model weights are licensed under the Apache License, Version 2.0 (Apache 2.0).&lt;/p&gt;
    &lt;head rend="h2"&gt;7. Citation&lt;/head&gt;
    &lt;code&gt;@misc{deepseek-math-v2,
  author = {Zhihong Shao, Yuxiang Luo, Chengda Lu, Z.Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, Xiaokang Zhang},
  title = {DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning},
  year = {2025},
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;8. Contact&lt;/head&gt;
    &lt;p&gt;If you have any questions, please raise an issue or contact us at service@deepseek.com.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Downloads last month&lt;/item&gt;
      &lt;item rend="dd-1"&gt;4,434&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Model tree for deepseek-ai/DeepSeek-Math-V2&lt;/head&gt;
    &lt;p&gt;Unable to build the model tree, the base model loops to the model itself. Learn more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://huggingface.co/deepseek-ai/DeepSeek-Math-V2"/><published>2025-12-01T08:54:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46105803</id><title>AWS data centers' water use tied to spike in cancer and miscarriages in Oregon</title><updated>2025-12-01T11:36:40.028272+00:00</updated><content>&lt;doc fingerprint="e2c2639846c44c2a"&gt;
  &lt;main&gt;
    &lt;p&gt;Morrow County, Oregon, has recorded nitrate readings as high as 73 parts per million (ppm) in household wells‚Äîmore than ten times the state‚Äôs legal ceiling of 7ppm‚Äîfollowing reports that local data centres are intensifying aquifer contamination. According to an investigation by Rolling Stone, the cooling systems used by Amazon Web Services (AWS) are concentrating existing pollutants in the water supply, a phenomenon experts are linking to a surge in miscarriages and rare cancers.&lt;/p&gt;
    &lt;p&gt;The Lower Umatilla Basin aquifer, the region‚Äôs primary source of drinking water, has historically suffered from nitrate runoff caused by local mega-farms and food-processing plants. But engineers and public-health experts now warn that AWS‚Äôs heavy water use has ‚Äòsupercharged‚Äò the problem by concentrating nitrates during the cooling cycle.&lt;/p&gt;
    &lt;p&gt;The company‚Äôs data centres draw tens of millions of gallons from the same aquifer each year to cool its servers. Water leaves the centres hotter and, after partial evaporation, carries up to 56 ppm of nitrates when it is pumped back to the Port of Morrow‚Äôs treatment lagoons and then sprayed onto nearby agricultural fields. The porous soil saturates quickly, allowing the enriched wastewater to percolate back into the aquifer.&lt;/p&gt;
    &lt;p&gt;The health implications for the county‚Äôs residents are severe. State and federal guidelines set the nitrate limit at 10 ppm (with Oregon‚Äôs specific ceiling at 7 ppm) to prevent ‚Äúblue-baby‚Äù syndrome, specific cancers such as non-Hodgkin lymphoma, and reproductive issues. With local wells now testing above 70 ppm, area clinicians have reported an unusual rise in both pregnancy loss and rare cancer diagnoses.&lt;/p&gt;
    &lt;p&gt;Amazon has pushed back against these findings. Spokesperson Lisa Levandowski stated that the company‚Äôs water usage is ‚Äúonly a very small fraction‚Äù of the basin‚Äôs total and described the groundwater issues as long predating AWS operations. She dismissed the claims in the Rolling Stone report as ‚Äúmisleading and inaccurate.‚Äù&lt;/p&gt;
    &lt;p&gt;Kristin Ostrom, executive director of the advocacy group Oregon Rural Action, said 40 per cent of county residents live below the poverty line and lack the political leverage to demand alternative water supplies. State agencies have delivered bottled water to a handful of households but have not committed to a comprehensive clean-water project.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://techoreon.com/oregon-data-centers-water-use-nitrates-cancer-miscarriage/"/><published>2025-12-01T10:37:39+00:00</published></entry></feed>