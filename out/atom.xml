<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-21T21:33:48.811607+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46001889</id><title>Olmo 3: Charting a path through the model flow to lead open-source AI</title><updated>2025-11-21T21:37:19.837016+00:00</updated><content>&lt;doc fingerprint="ad5e3b78241b8f9a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Olmo 3: Charting a path through the model flow to lead open-source AI&lt;/head&gt;
    &lt;p&gt;November 20, 2025&lt;/p&gt;
    &lt;p&gt;Ai2&lt;/p&gt;
    &lt;p&gt;Language models are often treated as snapshots—brief captures of a long and carefully curated development process. But sharing only the end result obscures the rich context needed to modify, adapt, and extend a model's capabilities. Many meaningful adjustments require integrating domain-specific knowledge deep within the development pipeline, not merely at the final stage. To truly advance open AI development and research, the entire model flow – not just its endpoint – should be accessible and customizable. The model flow is the full lifecycle of an LM: every stage, checkpoint, dataset, and dependency required to create and modify it. By exposing this complete process, the goal is to engender greater trust and enable more effective adaptation, collaboration, and innovation.&lt;/p&gt;
    &lt;p&gt;With today's release of Olmo 3, we're empowering the open source community with not only state-of-the-art open models, but the entire model flow and full traceability back to training data.&lt;/p&gt;
    &lt;p&gt;At its center is Olmo 3-Think (32B), the best fully open 32B-scale thinking model that for the first time lets you inspect intermediate reasoning traces and trace those behaviors back to the data and training decisions that produced them. Olmo 3 is a family of compact, dense models at 7 billion and 32 billion parameters that can run on everything from laptops to research clusters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo 3-Base (7B, 32B) is our most powerful base model yet. When evaluated on our expanded, diverse evaluation suite, Olmo 3-Base delivers the strongest performance among fully open base models – where training data, code, and weights are all publicly available, like Stanford's Marin and Swiss AI's Apertus – and achieves competitive performance with some of the best open-weights base models of comparable size and architecture, including Qwen 2.5 and Gemma 3. Achieving strong results in programming, reading comprehension, and math problem solving, Olmo 3-Base maintains performance at extended context lengths (~up to 65K tokens)—providing a versatile foundation for continued pretraining, targeted fine-tuning, and reinforcement learning and making it easy to build in specialized capabilities like reasoning, tool use (function calling), and instruction following through post-training.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Think (7B, 32B) is our flagship post-trained reasoning set built on Olmo 3-Base. At a time when few organizations are releasing truly open models at this scale, Olmo 3-Think (32B) serves as a workhorse for RL research, long-horizon reasoning, and other advanced experiments that require substantial compute. On our suite of reasoning benchmarks (discussed below), it's the strongest fully open thinking model we're aware of, narrowing the gap to the best open-weight models of similar scale – such as Qwen 3 32B – while training on roughly 6x fewer tokens. Olmo 3-Think (7B) brings the same design and training approach to an even more efficient form factor, surfacing intermediate thinking steps for complex prompts while making open, inspectable reasoning accessible on more modest hardware.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Instruct (7B) is a chat and quick-response focused post-train of Olmo 3-Base that handles multi-turn, instruction-following, tool use, and more. In our evaluations, it matches or outperforms open-weight models including Qwen 2.5, Gemma 3, and Llama 3.1, and narrows the gap with Qwen 3 model families at a similar scale—delivering a strong, fully open alternative for high-quality conversational and tool-using agents.&lt;/item&gt;
      &lt;item&gt;Olmo 3-RL Zero (7B), is a fully open reinforcement learning pathway built on Olmo 3-Base, designed to bootstrap complex reasoning behaviors and enable clear benchmarking of RL algorithms. We release four series of checkpoints from domain-focused training on math, code, instruction following, and general chat, enabling careful study of reinforcement learning with verifiable rewards (RLVR).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead of a single set of frozen weights, Olmo 3 offers multiple, fully documented paths through development: the Instruct path for everyday chat and tool use, the RL Zero path for RL experimentation from base models, and the Think/reasoning path for models that leverage inference-time scaling to unlock complex reasoning and agentic behaviors. Each path is a concrete example of how to shape behavior from the same base model, and you’re free to fork or remix them—start with Olmo 3-Base, explore your own supervised fine-tuning (SFT) or direct preference optimization (DPO) recipe for instruct-style use cases, or plug in a new RL objective to probe different tradeoffs. The flow itself becomes a rich, reusable object—not just a record of how we built Olmo 3, but a scaffold for how you can build your own systems.&lt;/p&gt;
    &lt;p&gt;Explore the Model Flow&lt;/p&gt;
    &lt;p&gt;Click on any stage to learn more about it and download artifacts.&lt;/p&gt;
    &lt;p&gt;The Olmo 3 checkpoints we're releasing represent our initial paths targeting our goals around reasoning, tool use, and general capabilities – we have exciting plans for other ways to leverage Olmo 3-Base 32B. But because we're releasing the entire flow, you can intervene at any point: swap in domain-specific data during mid-training, adjust post-training for your use case, or build on an earlier checkpoint that better suits your needs.&lt;/p&gt;
    &lt;p&gt;As with Olmo and Olmo 2, we’re releasing all components of the Olmo 3 flow – data, code, model weights, and checkpoints – under permissive open source licenses.&lt;/p&gt;
    &lt;p&gt;Try Olmo 3 | Download the models &amp;amp; data | Read the report&lt;/p&gt;
    &lt;head rend="h3"&gt;Strong performance across the board&lt;/head&gt;
    &lt;p&gt;We run the Olmo 3 checkpoints through a broad, updated benchmark suite, grouping dozens of industry-standard tasks (plus a few new ones we introduce) into several capability clusters. Together, the clustered suite and these held-out tasks give us a capability profile of Olmo 3—a clear picture of how well it solves math problems, codes, uses tools, answers general-knowledge questions, and more.&lt;/p&gt;
    &lt;p&gt;At a high level, the Olmo 3 family delivers the strongest fully open base and thinking models we’re aware of. Olmo 3-Base 32B outperforms other fully open base models, and Olmo 3-Think 32B emerges as the strongest fully open thinking model.&lt;/p&gt;
    &lt;p&gt;Our results were made possible by rigorous data curation at every stage of training, a carefully designed training recipe for each model, and a set of new algorithmic and infrastructure advances across data processing, training, and reinforcement learning. We also introduce an enhanced reinforcement learning framework that guides the development of our models and is particularly essential for our thinking models. To design the training recipe and coordinate targeted improvements across a wide range of capabilities at each stage of the model training pipeline, our development framework balances distributed innovation with centralized evaluation.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Base, with a training pipeline that first focuses on broad coverage over diverse text, code, and math, then concentrates on harder distributions to sharpen programming, quantitative reasoning, and reading comprehension, is clearly the strongest set of fully open base models in our evaluations. It’s also arguably the best 32B model in the entire ecosystem of models with open weights, performing impressively in programming, reading comprehension, math problem solving, and long-context benchmarks like RULER, which tests information retrieval from lengthy texts. Olmo 3-Base (7B) and Olmo 3-Base (32) maintain quality at extended context lengths and integrate cleanly with RL workflows, providing a robust foundation for continued pretraining and post-training.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Think, which turns the Base into a reasoning model by training on multi-step problems spanning math, code, and general problem solving, then running the thinking SFT → thinking DPO → RLVR model flow to elicit high-quality reasoning traces, competes with or exceeds several open-weight reasoning models of similar sizes. On math benchmarks, Olmo 3-Think (7B) matches Qwen 3 8B on MATH and comes within a few points on AIME 2024 and 2025, and also leads all comparison models on HumanEvalPlus for coding—performing strongly on MBPP and LiveCodeBench to demonstrate particular strength in code-intensive reasoning. On broader reasoning tasks like BigBench Hard and AGI Eval English, Olmo 3-Think (7B) remains competitive with Qwen 3 8B reasoning and Qwen 3 VL 8B Thinker while staying fully open and slightly smaller.&lt;/p&gt;
    &lt;p&gt;For the 32B model, Olmo 3-Think scales these trends up and becomes one of the strongest fully open reasoning models in its class. Olmo 3-Think (32B) either wins or sits within roughly two points of the best open-weight model on MATH, OMEGA, BigBenchHard, HumanEvalPlus, PopQA, and IFEval. It ties Qwen 3 VL 32B Thinking for the top score on the OMEGA suite while staying clearly ahead of Gemma 3 27B Instruct and competitive with DeepSeek R1 Distill 32B on math and reasoning. On broader knowledge and QA, Olmo 3-Think (32B) is effectively neck-and-neck with the Qwen 3 models on PopQA. And in instruction following, Olmo 3-Think (32B) tops this subset on IFEval and remains solid on IFBench and AlpacaEval 2 LC—offering a strong default for reasoning workloads at the 32B scale.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Instruct, which produces shorter sequences than the corresponding Olmo 3-Think models to improve inference efficiency and is designed to focus on general chat, tool use, and synthetic data generation, outperforms comparably-sized open-weight models. Olmo 3-Instruct ties or surpasses Qwen 2.5, Gemma 3, and Llama 3.1 in our evaluations, and competes with the Qwen 3 family at similar scale, delivering strong function calling performance and instruction-following capabilities in a fully open 7B model.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Olmo 3 architecture and training stages&lt;/head&gt;
    &lt;p&gt;Olmo 3 uses a decoder-only transformer architecture and multi-stage training pipeline. Pretraining runs in three stages—an initial large-scale training run that builds broad capabilities; a mid-training phase that focuses on harder material like math, code, and reading comprehension; and a final long-context extension stage that trains the model on very long documents. Together with architectural enhancements, this yields a more capable, efficient base for the Olmo 3 family.&lt;/p&gt;
    &lt;p&gt;Post-training then specializes the pretrained model for different use cases. Building on Olmo 2, each pathway follows a three-stage recipe – SFT, preference tuning with DPO, and RLVR – but in Olmo 3, we expose this as a fully documented model flow with complete customization over each training stage and dataset mix.&lt;/p&gt;
    &lt;p&gt;Instead of releasing only the final weights, we provide checkpoints from each major training milestone: the base pretrained model, the mid-trained model after targeted skill enhancement, the long-context-extended version, plus post-training checkpoints for the Olmo 3-Think, Olmo 3-Instruct, and Olmo 3-RL Zero flows. You can study how capabilities emerge over time, run ablations on specific stages, and fork the model at whatever point best fits your data, compute, and goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expanded training data&lt;/head&gt;
    &lt;p&gt;Compared to Olmo 2, we scaled data collection and significantly strengthened our dataset curation methods. Continuing our commitment to full transparency, we’re releasing several new, higher-quality datasets that cover every stage of base model training and post-training—from initial learning to specialized skills like complex reasoning and long-context understanding. This means anyone can see exactly what data shaped the model’s capabilities, reproduce our results, and reuse these datasets to train their own AI systems.&lt;/p&gt;
    &lt;p&gt;Olmo 3 is pretrained on Dolma 3, a new ~9.3-trillion-token corpus drawn from web pages, science PDFs processed with olmOCR, codebases, math problems and solutions, and encyclopedic text. From this pool, we construct Dolma 3 Mix, a 5.9-trillion-token (~6T) pretraining mix with a higher proportion of coding and mathematical data than earlier Dolma releases, plus much stronger decontamination via extensive deduplication, quality filtering, and careful control over data mixing. We follow established web standards in collecting training data and don’t collect from sites that explicitly disallow it, including paywalled content.&lt;/p&gt;
    &lt;p&gt;On top of this, we introduce two Dolma 3-based mixes for later stages of base model training. Dolma 3 Dolmino is our mid-training mix: 100B training tokens sampled from a ~2.2T-token pool of high-quality math, science, code, instruction-following, and reading-comprehension data, including reasoning traces that also enable RL directly on the base model. Dolma 3 Longmino is our long-context mix: ~50B training tokens drawn from a 639B-token pool of long documents combined with mid-training data to teach Olmo 3 to track information over very long inputs (like reports, logs, and multi-chapter documents).&lt;/p&gt;
    &lt;p&gt;We also introduce Dolci, a new post-training data suite tailored specifically for reasoning, tool use, and instruction following. Dolci provides separate mixes for each stage of post-training: SFT, DPO, and RLVR. For SFT, Dolci aggregates state-of-the-art datasets that advance step-by-step reasoning, tool use, and high-quality conversational behavior; for DPO, it supplies high-quality contrastive preference data; and for RL, it includes hard, diverse prompts across math, coding, instruction following, and general chat.&lt;/p&gt;
    &lt;p&gt;Together, Dolma 3 and Dolci give Olmo 3 a fully open data curriculum from first token to final post-trained checkpoint.&lt;/p&gt;
    &lt;head rend="h3"&gt;Efficient training stack&lt;/head&gt;
    &lt;p&gt;We pretrained Olmo 3 on a cluster of up to 1,024 H100 GPUs; we achieved training throughput of 7.7K tokens per device per second for Olmo 3-Base (7B). We mid-trained on 128 H100 GPUs, and post-trained on a set of 256 H100s.&lt;/p&gt;
    &lt;p&gt;For Olmo 3, building on the work we did for Olmo 2, we were able to significantly improve the efficiency of our post-training code. By moving SFT from Open Instruct (our post-training codebase, prioritizing flexibility) to Olmo Core (our pretraining codebase, designed to maximize efficiency), we increased throughput (tokens/second) by 8x. Similarly, by incorporating in-flight weight updates, continuous batching, and a lot of threading improvements, we made our RL training 4x more efficient—resulting in training runs that are significantly cheaper and faster.&lt;/p&gt;
    &lt;p&gt;A note on our 32B models: We believe 32B sits in a sweet spot for research and tinkering. 32B models are big enough to support strong, competitive performance, but still small enough that a wide audience can fine-tune and deploy them on accessible hardware.&lt;/p&gt;
    &lt;p&gt;For more details, including ablations, please read our technical report.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transparency at the core&lt;/head&gt;
    &lt;p&gt;A core goal of Olmo 3 is not just to open the model flow, but to make it actionable for people who want to understand and improve model behavior. Olmo 3 integrates with OlmoTrace, our tool for tracing model outputs back to training data in real time.&lt;/p&gt;
    &lt;p&gt;For example, in the Ai2 Playground, you can ask Olmo 3-Think (32B) to answer a general-knowledge question, then use OlmoTrace to inspect where and how the model may have learned to generate parts of its response. This closes the gap between training data and model behavior: you can see not only what the model is doing, but why—and adjust data or training decisions accordingly.&lt;/p&gt;
    &lt;p&gt;To further promote transparency and explainability, we’re making every training and fine-tuning dataset available for download, all under a permissive license that allows for custom deployment and reuse. The datasets come in a range of mixes to accommodate different storage and hardware constraints, from several billion tokens all the way up to 6 trillion.&lt;/p&gt;
    &lt;p&gt;Our new tooling for data processing allows you to de-contaminate, tokenize, and de-duplicate data in the same way we did for Olmo 3’s corpora. All the tooling is open source, enabling you to replicate our training curves or run controlled ablations across data mixes and objectives.&lt;/p&gt;
    &lt;p&gt;Our Olmo utilities and software cover the whole development cycle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo-core is a state-of-the-art framework for distributed model training.&lt;/item&gt;
      &lt;item&gt;Open Instruct is our post-training pipeline.&lt;/item&gt;
      &lt;item&gt;datamap-rs is a pure-Rust toolkit for large-scale cleaning.&lt;/item&gt;
      &lt;item&gt;duplodocus for ultra-efficient fuzzy de-duplication.&lt;/item&gt;
      &lt;item&gt;OLMES is a toolkit for reproducible evals. It includes our brand-new eval collection OlmoBaseEval, which we used for Olmo 3 base model development.&lt;/item&gt;
      &lt;item&gt;decon removes test sets from training data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Importantly, our tooling allows you to instrument complex tasks and analyze intermediate traces to understand where the models succeed—or struggle. Because the Olmo 3 data recipes, training pipeline, and checkpoints are open, independent teams can connect model behavior back to measurable properties.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ready to deploy and use&lt;/head&gt;
    &lt;p&gt;Together, the Olmo 3 family makes it easier to build trustworthy features quickly, whether for research, education, or applications. By making every development step available and inspectable, we're enabling entirely new categories of research. You can run experiments on any training phase, understand exactly how different techniques contribute to model capabilities, and build on our work at whatever stage makes sense for your project.&lt;/p&gt;
    &lt;p&gt;For scientists, the fully open flow exposes the model’s inner workings, so you can instrument experiments across coding, reasoning, RL, and tool use.&lt;/p&gt;
    &lt;p&gt;If you care about AI you can study, audit, and improve, Olmo 3 is for you. Try the demos in the Ai2 Playground, explore the documentation, and build on the released weights and checkpoints. Then tell us what you discover—we invite the community to validate, critique, and extend our findings.&lt;/p&gt;
    &lt;p&gt;True openness in AI isn't just about access—it's about trust, accountability, and shared progress. We believe the models shaping our future should be fully inspectable, not black boxes. Olmo 3 represents a different path: one where anyone can understand, verify, and build upon the AI systems that increasingly influence our world. This is what open-first means—not just releasing weights, but sharing the complete knowledge needed to advance AI responsibly: the flow.&lt;/p&gt;
    &lt;p&gt;Deep dive with Olmo lead researchers Hanna Hajishirzi and Noah Smith on how – and why – we built Olmo 3, and what comes next:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://allenai.org/blog/olmo3"/><published>2025-11-21T06:50:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46002161</id><title>It's hard to build an oscillator</title><updated>2025-11-21T21:37:19.733846+00:00</updated><content/><link href="https://lcamtuf.substack.com/p/its-hard-to-build-an-oscillator"/><published>2025-11-21T07:45:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46003144</id><title>FAWK: LLMs can write a language interpreter</title><updated>2025-11-21T21:37:19.326195+00:00</updated><content>&lt;doc fingerprint="5ff690b5ea490edb"&gt;
  &lt;main&gt;
    &lt;p&gt;After reading the book The AWK Programming Language (recommended!), I was planning to try AWK out on this year’s Advent of Code. Having some time off from work this week, I tried to implement one of the problems in it to get some practice, set up my tooling, see how hard AWK would be, and… I found I’m FP-pilled.&lt;/p&gt;
    &lt;p&gt;I knew I’m addicted to the combination of algebraic data types (tagged unions) and exhaustive pattern matching, but what got me this time was immutability, lexical scope and the basic human right of being allowed to return arrays from functions.&lt;/p&gt;
    &lt;p&gt;Part 1 of the Advent of Code problem was easy enough, but for part 2 (basically a shortest path search with a twist, to not spoil too much), I found myself unable to switch from my usual functional BFS approach to something mutable, and ended up trying to implement my functional approach in AWK.&lt;/p&gt;
    &lt;p&gt;It got hairy very fast: I needed to implement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;hashing of strings and 2D arrays (by piping to &lt;code&gt;md5sum&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;a global &lt;del rend="overstrike"&gt;set&lt;/del&gt;array of seen states&lt;/item&gt;
      &lt;item&gt;a way to serialize and deserialize a 2D array to/from a string&lt;/item&gt;
      &lt;item&gt;and a few associative arrays for retrieving this serialized array by its hash.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was very lost by the time I had all this; I spent hours just solving what felt like accidental complexity; things that I’d take for granted in more modern languages.&lt;/p&gt;
    &lt;p&gt;Now, I know nobody said AWK is modern, or functional, or that it promises any convenience for anything other than one-liners and basic scripts that fit under a handful of lines. I don’t want to sound like I expect AWK to do any of this; I knew I was stretching the tool when going in. But I couldn’t shake the feeling that there’s a beautiful AWK-like language within reach, an iteration on the AWK design (the pattern-action way of thinking is beautiful) that also gives us a few of the things programming language designers have learnt over the 48 years since AWK was born.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dreaming of functional AWK&lt;/head&gt;
    &lt;p&gt;Stopping my attempts to solve the AoC puzzle in pure AWK, I wondered: what am I missing here?&lt;/p&gt;
    &lt;p&gt;What if AWK had first-class arrays?&lt;/p&gt;
    &lt;code&gt;BEGIN {
  # array literals
  normal   = [1, 2, 3]
  nested   = [[1,2], [3,4]]
  assoc    = ["foo" =&amp;gt; "bar", "baz" =&amp;gt; "quux"]
  multidim = [(1,"abc") =&amp;gt; 999]

  five = range(1,5)
  analyze(five)
  print five  # --&amp;gt; still [1, 2, 3, 4, 5]! was passed by value
}

function range(a,b) {
  r = []
  for (i = a; i &amp;lt;= b; i++) {
    r[length(r)] = i
  }
  return r  # arrays can be returned!
}

function analyze(arr) {
  arr[0] = 100
  print arr[0]  # --&amp;gt; 100, only within this function
}
&lt;/code&gt;
    &lt;p&gt;What if AWK had first-class functions and lambdas?&lt;/p&gt;
    &lt;code&gt;BEGIN {
  # construct anonymous functions
  double = (x) =&amp;gt; { x * 2 }
  add = (a, b) =&amp;gt; { c = a + b; return c }

  # functions can be passed as values
  apply = (func, value) =&amp;gt; { func(value) }

  print apply(double,add(1,3))  # --&amp;gt; 8
  print apply(inc,5)  # --&amp;gt; 6
}

function inc(a) { return a + 1 }
&lt;/code&gt;
    &lt;p&gt;What if AWK had lexical scope instead of dynamic scope?&lt;/p&gt;
    &lt;code&gt;# No need for this hack anymore ↓     ↓
#function foo(a, b         ,local1, local2) {
function foo(a, b) {
  local1 = a + b
  local2 = a - b
  return local1 + local2
}

BEGIN {
  c = foo(1,2)
  print(local1)  # --&amp;gt; 0, the local1 from foo() didn't leak!
}
&lt;/code&gt;
    &lt;p&gt;What if AWK had explicit globals, and everything else was local by default?&lt;/p&gt;
    &lt;code&gt;BEGIN { global count }
END {
  foo()
  print count  # --&amp;gt; 1
  print mylocal # --&amp;gt; 0, didn't leak
}
function foo() { count++; mylocal++ }
&lt;/code&gt;
    &lt;p&gt;(This one, admittedly, might make programs a bit more verbose. I’m willing to pay that cost.)&lt;/p&gt;
    &lt;p&gt;What if AWK had pipelines? (OK, now I’m reaching for syntax sugar…)&lt;/p&gt;
    &lt;code&gt;BEGIN {
  result = [1, 2, 3, 4, 5] 
      |&amp;gt; filter((x) =&amp;gt; { x % 2 == 0 })
      |&amp;gt; map((x) =&amp;gt; { x * x })
      |&amp;gt; reduce((acc, x) =&amp;gt; { acc + x }, 0)

  print "Result:", result
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Making it happen&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;TL;DR:&lt;/p&gt;&lt;code&gt;Janiczek/fawk&lt;/code&gt;on GitHub&lt;/quote&gt;
    &lt;p&gt;Now for the crazy, LLM-related part of the post. I didn’t want to spend days implementing AWK from scratch or tweaking somebody else’s implementation. So I tried to use Cursor Agent for a larger task than I usually do (I tend to ask for very small targeted edits), and asked Sonnet 4.5 for a README with code examples, and then a full implementation in Python.&lt;/p&gt;
    &lt;p&gt;And it did it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: I also asked for implementations in C, Haskell and Rust at the same time, not knowing if any of the four would succeed, and they all seem to have produced code that at least compiles/runs. I haven’t tried to test them or even run them though. The PRs are here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was very impressed—I still am! I expected the LLM to stumble and flail around and ultimately get nothing done, but it did what I asked it for (gave me an interpreter that could run those specific examples), and over the course of a few chat sessions, I guided it towards implementing more and more of “the rest of AWK”, together with an excessive amount of end-to-end tests.&lt;/p&gt;
    &lt;p&gt;The only time I could see it struggle was when I asked it to implement arbitrary precision floating point operations without using an external library like &lt;code&gt;mpmath&lt;/code&gt;. It attempted to use Taylor series, but couldn’t get it right for at
least a few minutes. I chickened out and told it to &lt;code&gt;uv add mpmath&lt;/code&gt; and simplify
the interpreter code. In a moment it was done.&lt;/p&gt;
    &lt;p&gt;Other things that I thought it would choke on, like &lt;code&gt;print&lt;/code&gt; being both a
statement (with &lt;code&gt;&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; redirection support) and an expression, or
multi-dimensional arrays, or multi-line records, these were all implemented
correctly. Updating the test suite to also check for backwards compatibility
with GAWK - not an issue. Lexical scoping
and tricky closure environment behaviour - handled that just fine.&lt;/p&gt;
    &lt;head rend="h2"&gt;What now?&lt;/head&gt;
    &lt;p&gt;As the cool kids say, I have to update my priors. The frontier of what the LLMs can do has moved since the last time I tried to vibe-code something. I didn’t expect to have a working interpreter the same day I dreamt of a new programming language. It now seems possible.&lt;/p&gt;
    &lt;p&gt;The downside of vibe coding the whole interpreter is that I have zero knowledge of the code. I only interacted with the agent by telling it to implement a thing and write tests for it, and I only really reviewed the tests. I reckon this would be an issue in the future when I want to manually make some change in the actual code, because I have no familiarity with it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This also opened new questions for me wrt. my other projects where I’ve previously run out of steam, eg. trying to implement a Hindley-Milner type system for my dream forever-WIP programming language Cara. It seems I can now just ask the LLM to do it, and it will? But then, I don’t want to fall into the trap where I am no longer able to work on the codebase myself. I want to be familiar with and able to tinker on the code. I’d need to spend my time reviewing and reading code instead of writing everything myself. Perhaps that’s OK.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Performance of FAWK might be an issue as well, though right now it’s a non-goal, given my intended use case is throwaway scripts for Advent of Code, nothing user-facing. And who knows, based on what I’ve seen, maybe I can instruct it to rewrite it in Rust and have a decent chance of success?&lt;/p&gt;
    &lt;p&gt;For now, I’ll go dogfood my shiny new vibe-coded black box of a programming language on the Advent of Code problem (and as many of the 2025 puzzles as I can), and see what rough edges I can find. I expect them to be equal parts “not implemented yet” and “unexpected interactions of new PL features with the old ones”.&lt;/p&gt;
    &lt;p&gt;If you’re willing to jump through some Python project dependency hoops, you can try to use FAWK too at your own risk, at &lt;code&gt;Janiczek/fawk&lt;/code&gt; on
GitHub.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://martin.janiczek.cz/2025/11/21/fawk-llms-can-write-a-language-interpreter.html"/><published>2025-11-21T10:28:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46004293</id><title>Making a Small RPG</title><updated>2025-11-21T21:37:19.227137+00:00</updated><content/><link href="https://jslegenddev.substack.com/p/making-a-small-rpg"/><published>2025-11-21T13:23:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46004364</id><title>EXIF orientation info in PNGs isn't used for image-orientation: from-image</title><updated>2025-11-21T21:36:15.289119+00:00</updated><content/><link href="https://bugzilla.mozilla.org/show_bug.cgi?id=1627423"/><published>2025-11-21T13:29:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46004541</id><title>More tales about outages and numeric limits</title><updated>2025-11-21T21:33:58.995983+00:00</updated><content>&lt;doc fingerprint="af811b9b5ac90ef0"&gt;
  &lt;main&gt;
    &lt;p&gt;Nov 21, 2025 Tags: oss, security&lt;/p&gt;
    &lt;p&gt;TL;DR: Dependency cooldowns are a free, easy, and incredibly effective way to mitigate the large majority of open source supply chain attacks. More individual projects should apply cooldowns (via tools like Dependabot and Renovate) to their dependencies, and packaging ecosystems should invest in first-class support for cooldowns directly in their package managers.&lt;/p&gt;
    &lt;p&gt;âSupply chain securityâ is a serious problem. Itâs also seriously overhyped, in part because dozens of vendors have a vested financial interest in convincing your that their framing of the underlying problem1 is (1) correct, and (2) worth your money.&lt;/p&gt;
    &lt;p&gt;Whatâs consternating about this is that most open source supply chain attacks have the same basic structure:&lt;/p&gt;
    &lt;p&gt;An attacker compromises a popular open source project, typically via a stolen credential or CI/CD vulnerabilty (such as âpwn requestsâ in GitHub Actions).&lt;/p&gt;
    &lt;p&gt;The attacker introduces a malicious change to the project and uploads it somewhere that will have maximum effect (PyPI, npm, GitHub releases, &amp;amp;c., depending on the target).&lt;/p&gt;
    &lt;p&gt;At this point, the clock has started, as the attacker has moved into the public.&lt;/p&gt;
    &lt;p&gt;Users pick up the compromised version of the project via automatic dependency updates or a lack of dependency pinning.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the aforementioned vendors are scanning public indices as well as customer repositories for signs of compromise, and provide alerts upstream (e.g. to PyPI).&lt;/p&gt;
    &lt;p&gt;Notably, vendors are incentivized to report quickly and loudly upstream, as this increases the perceived value of their services in a crowded field.&lt;/p&gt;
    &lt;p&gt;Upstreams (PyPI, npm, &amp;amp;c.) remove or disable the compromised package version(s).&lt;/p&gt;
    &lt;p&gt;End-user remediation begins.&lt;/p&gt;
    &lt;p&gt;The key thing to observe is that the gap between (1) and (2) can be very large2 (weeks or months), while the gap between (2) and (5) is typically very small: hours or days. This means that, once the attacker has moved into the actual exploitation phase, their window of opportunity to cause damage is pretty limited.&lt;/p&gt;
    &lt;p&gt;We can see this with numerous prominent supply chain attacks over the last 18 months3:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Attack&lt;/cell&gt;
        &lt;cell role="head"&gt;Approx. Window of Opportunity&lt;/cell&gt;
        &lt;cell role="head"&gt;References&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xz-utils&lt;/cell&gt;
        &lt;cell&gt;â 5 weeks4&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 1)&lt;/cell&gt;
        &lt;cell&gt;12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 2)&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tj-actions&lt;/cell&gt;
        &lt;cell&gt;3 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;chalk&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nx&lt;/cell&gt;
        &lt;cell&gt;4 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rspack&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;num2words&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Kong Ingress Controller&lt;/cell&gt;
        &lt;cell&gt;â 10 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;web3.js&lt;/cell&gt;
        &lt;cell&gt;5 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;(Each of these attacks has significant downstream effect, of course, but only within their window of opportunity. Subsequent compromises from each, like Shai-Hulud, represent new windows of opportunity where the attackers regrouped and pivoted onto the next set of compromised credentials.)&lt;/p&gt;
    &lt;p&gt;My takeaway from this: some windows of opportunity are bigger, but the majority of them are under a week long. Consequently, ordinary developers can avoid the bulk of these types of attacks by instituting cooldowns on their dependencies.&lt;/p&gt;
    &lt;p&gt;A âcooldownâ is exactly what it sounds like: a window of time between when a dependency is published and when itâs considered suitable for use. The dependency is public during this window, meaning that âsupply chain securityâ vendors can work their magic while the rest of us wait any problems out.&lt;/p&gt;
    &lt;p&gt;I love cooldowns for several reasons:&lt;/p&gt;
    &lt;p&gt;Theyâre empirically effective, per above. They wonât stop all attackers, but they do stymie the majority of high-visibiity, mass-impact supply chain attacks that have become more common.&lt;/p&gt;
    &lt;p&gt;Theyâre incredibly easy to implement. Moreover, theyâre literally free to implement in most cases: most people can use Dependabotâs functionality, Renovateâs functionality, or the functionality build directly into their package manager5.&lt;/p&gt;
    &lt;p&gt;This is how simple it is in Dependabot:&lt;/p&gt;
    &lt;p&gt;(Rinse and repeat for other ecosystems as needed.)&lt;/p&gt;
    &lt;p&gt;Cooldowns enforce positive behavior from supply chain security vendors: vendors are still incentivized to discover and report attacks quickly, but are not as incentivized to emit volumes of blogspam about âcriticalâ attacks on largely underfunded open source ecosystems.&lt;/p&gt;
    &lt;p&gt;In the very small sample set above, 8/10 attacks had windows of opportunity of less than a week. Setting a cooldown of 7 days would have prevented the vast majority of these attacks from reaching end users (and causing knock-on attacks, which several of these were). Increasing the cooldown to 14 days would have prevented all but 1 of these attacks6.&lt;/p&gt;
    &lt;p&gt;Cooldowns are, obviously, not a panacea: some attackers will evade detection, and delaying the inclusion of potentially malicious dependencies by a week (or two) does not fundamentally alter the fact that supply chain security is a social trust problem, not a purely technical one. Still, an 80-90% reduction in exposure through a technique that is free and easy seems hard to beat.&lt;/p&gt;
    &lt;p&gt;Related to the above, itâs unfortunate that cooldowns arenât baked directly into more packaging ecosystems: Dependabot and Renovate are great, but even better would be if the package manager itself (as the source of ground truth) could enforce cooldowns directly (including of dependencies not introduced or bumped through automated flows).&lt;/p&gt;
    &lt;p&gt;The problem being, succinctly: modern software stacks are complex and opaque, with little to no difference in privilege between first-party code and third-party dependencies.Â ↩&lt;/p&gt;
    &lt;p&gt;In part because of the prevalence of long-lived, overscoped credentials. Long-lived credentials let attackers operate on their own (comfortable) timelines; this is why Trusted Publishing is such a useful (but not wholly sufficient) technique for reducing the attackerâs attack staging window.Â ↩&lt;/p&gt;
    &lt;p&gt;Filippo Valsorda has an excellent compilation of recent supply chain compromises here.Â ↩&lt;/p&gt;
    &lt;p&gt;The xz-utils attack is a significant outlier, both in its scope and the length of its window of opportunity. In this case, Iâve measured from the attackerâs first backdoored release (v5.6.0, 2024-02-24) to the time of rollback within Debian (2024-03-28).Â ↩&lt;/p&gt;
    &lt;p&gt;For example, pnpmâs &lt;code&gt;minimumReleaseAge&lt;/code&gt;.
           uv also has &lt;code&gt;exclude-newer&lt;/code&gt;, 
           although this specifies an absolute cutoff rather than a rolling cooldown.Â ↩&lt;/p&gt;
    &lt;p&gt;Notably, the only attack that would have stymied a 14-day cooldown is xz-utils, which is also the most technically, logistically, and socially advanced of all of the attacks.Â ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rachelbythebay.com/w/2025/11/18/down/"/><published>2025-11-21T13:44:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46005111</id><title>We should all be using dependency cooldowns</title><updated>2025-11-21T21:33:58.705860+00:00</updated><content>&lt;doc fingerprint="af811b9b5ac90ef0"&gt;
  &lt;main&gt;
    &lt;p&gt;Nov 21, 2025 Tags: oss, security&lt;/p&gt;
    &lt;p&gt;TL;DR: Dependency cooldowns are a free, easy, and incredibly effective way to mitigate the large majority of open source supply chain attacks. More individual projects should apply cooldowns (via tools like Dependabot and Renovate) to their dependencies, and packaging ecosystems should invest in first-class support for cooldowns directly in their package managers.&lt;/p&gt;
    &lt;p&gt;âSupply chain securityâ is a serious problem. Itâs also seriously overhyped, in part because dozens of vendors have a vested financial interest in convincing your that their framing of the underlying problem1 is (1) correct, and (2) worth your money.&lt;/p&gt;
    &lt;p&gt;Whatâs consternating about this is that most open source supply chain attacks have the same basic structure:&lt;/p&gt;
    &lt;p&gt;An attacker compromises a popular open source project, typically via a stolen credential or CI/CD vulnerabilty (such as âpwn requestsâ in GitHub Actions).&lt;/p&gt;
    &lt;p&gt;The attacker introduces a malicious change to the project and uploads it somewhere that will have maximum effect (PyPI, npm, GitHub releases, &amp;amp;c., depending on the target).&lt;/p&gt;
    &lt;p&gt;At this point, the clock has started, as the attacker has moved into the public.&lt;/p&gt;
    &lt;p&gt;Users pick up the compromised version of the project via automatic dependency updates or a lack of dependency pinning.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the aforementioned vendors are scanning public indices as well as customer repositories for signs of compromise, and provide alerts upstream (e.g. to PyPI).&lt;/p&gt;
    &lt;p&gt;Notably, vendors are incentivized to report quickly and loudly upstream, as this increases the perceived value of their services in a crowded field.&lt;/p&gt;
    &lt;p&gt;Upstreams (PyPI, npm, &amp;amp;c.) remove or disable the compromised package version(s).&lt;/p&gt;
    &lt;p&gt;End-user remediation begins.&lt;/p&gt;
    &lt;p&gt;The key thing to observe is that the gap between (1) and (2) can be very large2 (weeks or months), while the gap between (2) and (5) is typically very small: hours or days. This means that, once the attacker has moved into the actual exploitation phase, their window of opportunity to cause damage is pretty limited.&lt;/p&gt;
    &lt;p&gt;We can see this with numerous prominent supply chain attacks over the last 18 months3:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Attack&lt;/cell&gt;
        &lt;cell role="head"&gt;Approx. Window of Opportunity&lt;/cell&gt;
        &lt;cell role="head"&gt;References&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xz-utils&lt;/cell&gt;
        &lt;cell&gt;â 5 weeks4&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 1)&lt;/cell&gt;
        &lt;cell&gt;12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 2)&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tj-actions&lt;/cell&gt;
        &lt;cell&gt;3 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;chalk&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nx&lt;/cell&gt;
        &lt;cell&gt;4 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rspack&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;num2words&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Kong Ingress Controller&lt;/cell&gt;
        &lt;cell&gt;â 10 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;web3.js&lt;/cell&gt;
        &lt;cell&gt;5 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;(Each of these attacks has significant downstream effect, of course, but only within their window of opportunity. Subsequent compromises from each, like Shai-Hulud, represent new windows of opportunity where the attackers regrouped and pivoted onto the next set of compromised credentials.)&lt;/p&gt;
    &lt;p&gt;My takeaway from this: some windows of opportunity are bigger, but the majority of them are under a week long. Consequently, ordinary developers can avoid the bulk of these types of attacks by instituting cooldowns on their dependencies.&lt;/p&gt;
    &lt;p&gt;A âcooldownâ is exactly what it sounds like: a window of time between when a dependency is published and when itâs considered suitable for use. The dependency is public during this window, meaning that âsupply chain securityâ vendors can work their magic while the rest of us wait any problems out.&lt;/p&gt;
    &lt;p&gt;I love cooldowns for several reasons:&lt;/p&gt;
    &lt;p&gt;Theyâre empirically effective, per above. They wonât stop all attackers, but they do stymie the majority of high-visibiity, mass-impact supply chain attacks that have become more common.&lt;/p&gt;
    &lt;p&gt;Theyâre incredibly easy to implement. Moreover, theyâre literally free to implement in most cases: most people can use Dependabotâs functionality, Renovateâs functionality, or the functionality build directly into their package manager5.&lt;/p&gt;
    &lt;p&gt;This is how simple it is in Dependabot:&lt;/p&gt;
    &lt;p&gt;(Rinse and repeat for other ecosystems as needed.)&lt;/p&gt;
    &lt;p&gt;Cooldowns enforce positive behavior from supply chain security vendors: vendors are still incentivized to discover and report attacks quickly, but are not as incentivized to emit volumes of blogspam about âcriticalâ attacks on largely underfunded open source ecosystems.&lt;/p&gt;
    &lt;p&gt;In the very small sample set above, 8/10 attacks had windows of opportunity of less than a week. Setting a cooldown of 7 days would have prevented the vast majority of these attacks from reaching end users (and causing knock-on attacks, which several of these were). Increasing the cooldown to 14 days would have prevented all but 1 of these attacks6.&lt;/p&gt;
    &lt;p&gt;Cooldowns are, obviously, not a panacea: some attackers will evade detection, and delaying the inclusion of potentially malicious dependencies by a week (or two) does not fundamentally alter the fact that supply chain security is a social trust problem, not a purely technical one. Still, an 80-90% reduction in exposure through a technique that is free and easy seems hard to beat.&lt;/p&gt;
    &lt;p&gt;Related to the above, itâs unfortunate that cooldowns arenât baked directly into more packaging ecosystems: Dependabot and Renovate are great, but even better would be if the package manager itself (as the source of ground truth) could enforce cooldowns directly (including of dependencies not introduced or bumped through automated flows).&lt;/p&gt;
    &lt;p&gt;The problem being, succinctly: modern software stacks are complex and opaque, with little to no difference in privilege between first-party code and third-party dependencies.Â ↩&lt;/p&gt;
    &lt;p&gt;In part because of the prevalence of long-lived, overscoped credentials. Long-lived credentials let attackers operate on their own (comfortable) timelines; this is why Trusted Publishing is such a useful (but not wholly sufficient) technique for reducing the attackerâs attack staging window.Â ↩&lt;/p&gt;
    &lt;p&gt;Filippo Valsorda has an excellent compilation of recent supply chain compromises here.Â ↩&lt;/p&gt;
    &lt;p&gt;The xz-utils attack is a significant outlier, both in its scope and the length of its window of opportunity. In this case, Iâve measured from the attackerâs first backdoored release (v5.6.0, 2024-02-24) to the time of rollback within Debian (2024-03-28).Â ↩&lt;/p&gt;
    &lt;p&gt;For example, pnpmâs &lt;code&gt;minimumReleaseAge&lt;/code&gt;.
           uv also has &lt;code&gt;exclude-newer&lt;/code&gt;, 
           although this specifies an absolute cutoff rather than a rolling cooldown.Â ↩&lt;/p&gt;
    &lt;p&gt;Notably, the only attack that would have stymied a 14-day cooldown is xz-utils, which is also the most technically, logistically, and socially advanced of all of the attacks.Â ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.yossarian.net/2025/11/21/We-should-all-be-using-dependency-cooldowns"/><published>2025-11-21T14:50:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46005349</id><title>XBMC 4.0 for the Original Xbox</title><updated>2025-11-21T21:33:57.948614+00:00</updated><content>&lt;doc fingerprint="3f9edd7dc8a4cddc"&gt;
  &lt;main&gt;
    &lt;p&gt;A Major Modernization of the Killer App That Started It All&lt;/p&gt;
    &lt;p&gt;A new version of Xbox Media Center (XBMC), version 4.0, has been released. This version marks a significant update to the long-standing media center platform for the Original Xbox. This marks the first major advancement to the software since 2016 and represents a renewed commitment to preserving, modernizing, and extending the capabilities of one of the most iconic console homebrew applications ever created.&lt;/p&gt;
    &lt;p&gt;XBMC has a long and influential history. In 2002, XboxMediaPlayer (XMP) was released and turned the console into a powerful multimedia device fit for the living room in an era when connecting a computer to a TV was quite novel. Later that same year, XMP merged with YAMP and became Xbox Media Player 2.0. A few years later, the software evolved into Xbox Media Center, or XBMC, which introduced a new interface, a plugin system powered by Python, and a robust skinning engine.&lt;/p&gt;
    &lt;p&gt;XBMC eventually became so capable that it outgrew the Xbox entirely. By 2007, developers were working on PC ports and in 2010, the project split into two branches: one for general computers while the Xbox version became XBMC4Xbox, and each codebase was maintained from then on by separate teams. XBMC was later renamed to Kodi in 2014 and continues to be one of the most popular media center applications available. Even Plex traces its roots back to XBMC. Plex began as OSXBMC, a Mac port of XBMC in late 2007, before becoming its own project in 2008. This means the Original Xbox helped shape not one but two of the biggest media center apps used today.&lt;/p&gt;
    &lt;p&gt;The last official release of XBMC4Xbox arrived in February 2016 with version 3.5.3. Although the community never declared the project dead, meaningful updates became scarce. XBMC 4.0 continues that legacy by bringing a modern interface, updating it to be more inline with Kodi's modern codebase, and backporting features to the original 64MB RAM / Pentium-III hardware where it all began.&lt;/p&gt;
    &lt;p&gt;This project is distinct and separate from XBMC4Gamers, the games-focused variation of XBMC4Xbox (v3.5.3) by developer Rocky5.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Modern Interface Powered by Estuary&lt;/head&gt;
    &lt;p&gt;One of the most notable advancements in XBMC 4.0 is the introduction of the Estuary user interface (skin).&lt;/p&gt;
    &lt;p&gt;Estuary, originally released in 2017 with Kodi v17 ("Krypton"), provides a clean and modern layout that improves navigation and readability over past skins. Bringing Estuary to the Xbox required extensive updates to the underlying GUI framework, including a port of the more contemporary GUIlib engine. This allows the platform to support modern skinning standards and makes future skin ports much more straightforward. After the initial work of porting GUIlib was done, porting Estuary to the Xbox was a relatively simple process of tweaking a handful of configuration files and adding contextual features specific to the Xbox. The result is a modern, intuitive front end that retains the performance and responsiveness required on legacy hardware.&lt;/p&gt;
    &lt;p&gt;Firing up an Xbox made in 2001 and being greeted by the same interface as what you'd find if you were to download Kodi today onto your PC feels like a bit of magic, and helps keep this beloved classic console relevant and useful well into the modern era.&lt;/p&gt;
    &lt;head rend="h2"&gt;Expanded Games Library Support&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 introduces a fully realized games library system. This enhancement brings the same level of metadata support found in the Movies and Music sections to Xbox and emulated games. Titles can now display artwork, descriptions, and other metadata, transforming the games section into a polished and user-friendly library. XBMC’s longstanding support for trainers remains intact, giving users the option to apply gameplay modifications for compatible titles. Emulated game collections benefit as well, with the ability to browse ROM libraries and launch them directly in a user’s preferred emulator.&lt;/p&gt;
    &lt;head rend="h2"&gt;Online Scrapers and Metadata Support&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 restores full functionality to metadata scrapers for movies and television. This allows users to build rich media libraries complete with artwork, plot summaries, cast listings, and other information retrieved directly from online sources. XBMC 4.0 handles these tasks efficiently, even on the Xbox’s limited memory and processing power. Video playback continues to support 480p and 720p content, enabling the console to serve as a surprisingly capable media device for its age. Similar to Kodi, XBMC 4.0 supports filtering, building playlists, watch progress history for media, and intelligent handling of TV shows with seasons.&lt;/p&gt;
    &lt;p&gt;Aside from scrapers for multimedia, support for rich library capabilities for games has also been added. XBMC has always been a media-first app, and now users can enjoy the library experience that they've come to love for media now in the context of their games library (more info below).&lt;/p&gt;
    &lt;head rend="h2"&gt;Improved Task Scheduling and Multitasking&lt;/head&gt;
    &lt;p&gt;Despite the constraints of the Xbox’s single-threaded 733MHz CPU, XBMC 4.0 includes improvements to task scheduling that allow multiple activities to run concurrently. Background library updates, metadata scraping, and audio/video playback can occur while users navigate and use other parts of the interface. These optimizations help ensure a fluid experience without compromising performance. Much work has been done "under the hood" to keep XBMC on task and within memory budgets while achieving multi-tasking on a console that wasn't exactly designed with it in mind. Users who own RAM and/or CPU upgraded consoles can also take advantage of the extra overhead, as XBMC 4.0 makes use of the extra horsepower for an even smoother experience. Utilizing an SSD with higher UDMA speeds will also yield an improvement in overall responsiveness.&lt;/p&gt;
    &lt;head rend="h2"&gt;Music Experience and Visualizers&lt;/head&gt;
    &lt;p&gt;Music playback has always been a strong element of XBMC, and version 4.0 maintains that focus. The Original Xbox is capable of high quality audio output, and XBMC continues to support lossless codecs such as FLAC. The release includes compatibility with various audio visualizers, including MilkDrop, which remains one of the most visually impressive and customizable audio visualization engines available. These features allow XBMC 4.0 to function not only as a media organizer, but also as an immersive audio display system.&lt;/p&gt;
    &lt;p&gt;An online repository has been established and will be maintained moving forward where users can download legacy and newly-released add-ons as they become available. This repository is accessible without additional setup, right out of the box!&lt;/p&gt;
    &lt;head rend="h2"&gt;Add-ons and Python Support&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 continues to offer an extendable architecture powered by Python-based add-ons. While the current release uses Python 2.7 for compatibility, work is underway to transition to Python 3.4.10 in the future, which may provide a path for backporting many newer Kodi add-ons. Even in its current state, XBMC 4.0 already supports a variety of community-developed add-ons that extend the system’s functionality, including tools for online video playback (i.e. YouTube), online weather services, and enhanced media organization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Updated Settings, Network Services, and System Tools&lt;/head&gt;
    &lt;p&gt;The settings interface has been revised to provide more clarity and control. The update includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Playback options, including episode progression, crossfade behavior, and subtitle handling&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Library management tools&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network features, such as SMB, FTP, UPnP sharing, web server access, and Insignia-compatible DNS options&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Comprehensive interface customization options&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiple user profiles with individual library settings&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Advanced system controls for video calibration, display modes, input devices, and power management&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A robust System Information section for diagnostics, with info geared towards the Original Xbox&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A flexible File Manager with support for network protocols including FTP, SMB, WebDAV, and more&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Users may also take advantage of an online add-ons repository, offering the same experience modern Kodi provides with being able to download add-ons to extend functionality of the app with things like online multimedia providers, weather, skins, visualizers, and more. Developers can submit new add-ons to the official repository via Github.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continuing the Legacy&lt;/head&gt;
    &lt;p&gt;XBMC has been a staple of the Original Xbox's homebrew scene since its inception in the early 2000's. This new update is a revival of the platform that helped shape the landscape of home media software and helps revitalize a codebase that has been somewhat stagnant for many years. This release honors that heritage while modernizing the experience for a new generation of enthusiasts and preserving the functionality of the Original Xbox as a versatile and capable media center.&lt;/p&gt;
    &lt;p&gt;Although the hardware is decades old, the renewed effort behind XBMC 4.0 demonstrates that the platform still has room to grow and tricks up its sleeve. With ongoing development and a codebase designed with modern Kodi compatibility in mind, XBMC 4.0 represents a significant step forward into the continued development on the Original Xbox.&lt;/p&gt;
    &lt;p&gt;The development team looks forward to continuing this work and expanding the possibilities of the Original Xbox for years to come. This version is the first of many to come, with lots of things cooking in the background. Keep an eye out for future releases by joining the Xbox-Scene Discord and turning on notifications in the xbmc-news channel or by periodically checking the project's Github page.&lt;/p&gt;
    &lt;head rend="h2"&gt;Downloads&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 (and subsequent releases) builds along with source code are available via Github:&lt;/p&gt;
    &lt;p&gt;Main project page: Click Here&lt;/p&gt;
    &lt;p&gt;Note: XBMC 4.0 is is in active development! This means updates will be released in a more frequent manner for the time being until things settle down. Check the nightly builds section on Github for the most up-to-date version.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributions&lt;/head&gt;
    &lt;p&gt;XBMC is open source software and welcomes contributions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Coding: Developers can help XBMC by fixing a bug, adding new features, making our technology smaller and faster and making development easier for others. XBMC's codebase consists mainly of C++ with small parts written in a variety of coding languages. Our add-ons mainly consist of python and XML.&lt;/item&gt;
      &lt;item&gt;Helping users: Our support process relies on enthusiastic contributors like you to help others get the most out of XBMC. The #1 priority is always answering questions in our support forums. Everyday new people discover XBMC, and everyday they are virtually guaranteed to have questions.&lt;/item&gt;
      &lt;item&gt;Localization: Translate XBMC, add-ons, skins etc. into your native language.&lt;/item&gt;
      &lt;item&gt;Add-ons: Add-ons are what make XBMC the most extensible and customizable entertainment hub available. Get started building an add-on.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Support and Bug Reporting&lt;/head&gt;
    &lt;p&gt;Need help?&lt;/p&gt;
    &lt;p&gt;Support can be found in the XBMC -&amp;gt; General channel within the Xbox-Scene Discord server.&lt;/p&gt;
    &lt;head rend="h2"&gt;Credits and Disclaimers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nikola Antonić - Primary Developer, Project Lead&lt;/item&gt;
      &lt;item&gt;astarivi - Contributor (cURL, wolfSSL), Tester, Debugger&lt;/item&gt;
      &lt;item&gt;EqUiNoX - Contrubitor, Tester&lt;/item&gt;
      &lt;item&gt;Rocky5 - Contributor, Tester&lt;/item&gt;
      &lt;item&gt;.lavenderStarlight+ - Add-ons / Skins Development, Tester&lt;/item&gt;
      &lt;item&gt;GoTeamScotch - Tester, Feedback&lt;/item&gt;
      &lt;item&gt;Haguero - Tester, Feedback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;XBMC is GPLv2 licensed. You may use, distribute and copy it under the license terms. XBMC is licensed under the same terms as Kodi. For detailed information on the licensing, please refer to the Kodi license.&lt;/p&gt;
    &lt;p&gt;This project, XBMC version 4.0 (and upcoming releases), is distinct from and is not affiliated with Team Kodi of The Kodi Foundation, or its members.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.xbox-scene.info/articles/announcing-xbmc-40-for-the-original-xbox-r64/"/><published>2025-11-21T15:18:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46005553</id><title>Arduino published updated terms and conditions: no longer an open commons</title><updated>2025-11-21T21:33:57.446826+00:00</updated><content>&lt;doc fingerprint="b470b35817f47bba"&gt;
  &lt;main&gt;
    &lt;p&gt;Six weeks ago, Qualcomm acquired Arduino. The maker community immediately worried that Qualcomm would kill the open-source ethos that made Arduino the lingua franca of hobby electronics.&lt;/p&gt;
    &lt;p&gt;This week, Arduino published updated terms and conditions and a new privacy policy, clearly rewritten by Qualcomm’s lawyers. The changes confirm the community’s worst fears: Arduino is no longer an open commons. It’s becoming just another corporate platform.&lt;/p&gt;
    &lt;p&gt;Here’s what’s at stake, what Qualcomm got wrong, and what might still be salvaged, drawing from community discussions across maker forums and sites.&lt;/p&gt;
    &lt;p&gt;What changed?&lt;lb/&gt;The new terms read like standard corporate boilerplate: mandatory arbitration, data integration with Qualcomm’s global ecosystem, export controls, AI use restrictions. For any other SaaS platform, this would be unremarkable.&lt;/p&gt;
    &lt;p&gt;But Arduino isn’t SaaS. It’s the foundation of the maker ecosystem.&lt;/p&gt;
    &lt;p&gt;The most dangerous change is Arduino now explicitly states that using their platform grants you no patent licenses whatsoever. You can’t even argue one is implied.&lt;/p&gt;
    &lt;p&gt;This means Qualcomm could potentially assert patents against your projects if you built them using Arduino tools, Arduino examples, or Arduino-compatible hardware.&lt;/p&gt;
    &lt;p&gt;And here’s the disconnect, baffling makers. Arduino’s IDE is licensed under AGPL. Their CLI is GPL v3. Both licenses explicitly require that you can reverse engineer the software. But the new Qualcomm terms explicitly forbid reverse engineering “the Platform.”&lt;/p&gt;
    &lt;p&gt;What’s really going on?&lt;lb/&gt;The community is trying to figure out what is Qualcomm’s actual intent. Are these terms just bad lawyering with SaaS lawyers applying their standard template to cloud services, not realizing Arduino is different? Or is Qualcomm testing how much they can get away with before the community revolts? Or is this a first step toward locking down the ecosystem they just bought?&lt;/p&gt;
    &lt;p&gt;Some people point out that “the Platform” might only mean Arduino’s cloud services (forums, Arduino Cloud, Project Hub) not the IDE and CLI that everyone actually uses.&lt;/p&gt;
    &lt;p&gt;If that’s true, Qualcomm needs to say so, explicitly, and in plain language. Because library maintainers are likely wondering whether contributing to Arduino repos puts them at legal risk. And hardware makers are questioning whether “Arduino-compatible” is still safe to advertise.&lt;/p&gt;
    &lt;p&gt;Why Adafruit’s alarm matters&lt;lb/&gt;Adafruit has been vocal about the dangers of this acquisition. Some dismiss Adafruit’s criticism as self-serving. After all, they sell competing hardware and promote CircuitPython. But that misses who Adafruit is.&lt;/p&gt;
    &lt;p&gt;Adafruit has been the moral authority on open hardware for decades. They’ve made their living proving you can build a successful business on open principles. When they sound the alarm, it’s not about competition, it’s about principle.&lt;/p&gt;
    &lt;p&gt;What they’re calling out isn’t that Qualcomm bought Arduino. It’s that Qualcomm’s lawyers fundamentally don’t understand what they bought. Arduino wasn’t valuable because it was just a microcontroller company. It was valuable because it was a commons. And you can’t apply enterprise legal frameworks to a commons without destroying it.&lt;/p&gt;
    &lt;p&gt;Adafruit gets this. They’ve built their entire business on this. That’s why their criticism carries weight.&lt;/p&gt;
    &lt;p&gt;What Qualcomm doesn’t seem to understand&lt;lb/&gt;Qualcomm probably thought they were buying an IoT hardware company with a loyal user base. &lt;/p&gt;
    &lt;p&gt;They weren’t. They bought the IBM PC of the maker world.&lt;/p&gt;
    &lt;p&gt;Arduino’s value was never just the hardware. Their boards have been obsolete for years. Their value is the standard.&lt;/p&gt;
    &lt;p&gt;The Arduino IDE is the lingua franca of hobby electronics.&lt;/p&gt;
    &lt;p&gt;Millions of makers learned on it, even if they moved to other hardware. ESP32, STM32, Teensy, Raspberry Pi Pico – none of them are Arduino hardware, but they all work with the Arduino IDE.&lt;/p&gt;
    &lt;p&gt;Thousands of libraries are “Arduino libraries.” Tutorials assume Arduino. University curricula teach Arduino. When you search “how to read a sensor,” the answer comes back in Arduino code.&lt;/p&gt;
    &lt;p&gt;This is the ecosystem Qualcomm’s lawyers just dropped legal uncertainty onto.&lt;/p&gt;
    &lt;p&gt;If Qualcomm’s lawyers start asserting control over the IDE, CLI, or core libraries under restrictive terms, they will poison the entire maker ecosystem. Even people who never buy Arduino hardware are dependent on Arduino software infrastructure.&lt;/p&gt;
    &lt;p&gt;Qualcomm didn’t just buy a company. They bought a commons. And now they inadvertently are taking steps that are destroying what made it valuable.&lt;/p&gt;
    &lt;p&gt;What are makers supposed to do?&lt;lb/&gt;There has been some buzz of folks just leaving the Arduino environment behind. But Arduino IDE alternatives such as PlatformIO and VSCode are not in any way beginner friendly. If the Arduino IDE goes, then there’s a huge problem. &lt;/p&gt;
    &lt;p&gt;I remember when Hypercard ended. There were alternatives, but none so easy. I don’t think I really coded again for almost 20 years until I picked up the Arduino IDE (go figure).&lt;/p&gt;
    &lt;p&gt;If something happens to the Arduino IDE, even if its development stalls or becomes encumbered, there’s no replacement for that easy onboarding. We’d lose many promising new makers because the first step became too steep.&lt;/p&gt;
    &lt;p&gt;The institutional knowledge at risk&lt;lb/&gt;But leaving Arduino behind isn’t simple. The platform’s success depends on two decades of accumulated knowledge, such as countless Arduino tutorials on YouTube, blogs, and school curricula; open-source libraries that depend on Arduino compatibility; projects in production using Arduino tooling; and university programs built around Arduino as the teaching platform&lt;/p&gt;
    &lt;p&gt;All of these depend on Arduino remaining open and accessible.&lt;/p&gt;
    &lt;p&gt;If Qualcomm decided to sunset the open Arduino IDE in favor of a locked-down “Arduino Pro” platform, or if they start asserting patent claims, or if uncertainty makes contributors abandon the ecosystem, all that knowledge becomes stranded.&lt;/p&gt;
    &lt;p&gt;It’s like Wikipedia going behind a paywall. The value isn’t just the content, it is the trust that it remains accessible. Arduino’s value isn’t just the code, it’s the trust that the commons would stay open.&lt;/p&gt;
    &lt;p&gt;That trust is now gone. And once lost, it hard to get back.&lt;/p&gt;
    &lt;p&gt;Why this happened (but doesn’t excuse it)&lt;lb/&gt;Let’s be fair to Qualcomm, their lawyers were doing their jobs.&lt;/p&gt;
    &lt;p&gt;When you acquire a company, you standardize the legal terms; add mandatory arbitration to limit class action exposure; integrate data systems for compliance and auditing; add export controls because you sell to defense contractors; prohibit reverse engineering because that’s in the template.&lt;/p&gt;
    &lt;p&gt;For most acquisitions, this is just good corporate hygiene. And Arduino, now part of a megacorp, faces higher liabilities than it did as an independent entity.&lt;/p&gt;
    &lt;p&gt;But here’s what Qualcomm’s lawyers missed: Arduino isn’t a normal acquisition. The community isn’t a customer base, it’s a commons. And you can’t apply enterprise SaaS legal frameworks to a commons without destroying what made it valuable.&lt;/p&gt;
    &lt;p&gt;This is tone-deafness, not malice. But the outcome is the same. A community that trusted Arduino no longer does.&lt;/p&gt;
    &lt;p&gt;Understanding why this happened doesn’t excuse it, but it might suggest what needs to happen next.&lt;/p&gt;
    &lt;p&gt;What should have happened and how to still save it&lt;lb/&gt;Qualcomm dropped legal boilerplate on the community with zero context and let people discover the contradictions themselves. That’s how you destroy trust overnight.&lt;/p&gt;
    &lt;p&gt;Qualcomm should have announced the changes in advance. They should have given the community weeks, not hours, to understand what’s changing and why. They should have used plain-language explanations, not just legal documents.&lt;/p&gt;
    &lt;p&gt;Qualcomm can fix things by explicitly carving out the open ecosystem. They should state clearly that the terms apply to Arduino Cloud services, and the IDE, CLI, and core libraries remain under their existing open source licenses.&lt;/p&gt;
    &lt;p&gt;We’d need concrete commitments, such as which repos stay open, which licenses won’t change, what’s protected from future acquisition decisions. Right now we have vague corporate-speak about “supporting the community.”&lt;/p&gt;
    &lt;p&gt;Indeed, they could create some structural protection, as well, by putting IDE, CLI, and core libraries in a foundation that Qualcomm couldn’t unilaterally control (think the Linux Foundation model).&lt;/p&gt;
    &lt;p&gt;Finally, Qualcomm might wish to establish some form of community governance with real representation and real power over the tools the community depends on.&lt;/p&gt;
    &lt;p&gt;The acquisition is done. The legal integration is probably inevitable. But how it’s done determines whether Arduino survives as a commons or dies as just another Qualcomm subsidiary.&lt;/p&gt;
    &lt;p&gt;What’s next?&lt;lb/&gt;Arduino may be the toolset that made hobby electronics accessible to millions. But that maker community built Arduino into what it became. Qualcomm’s acquisition has thrown that legacy into doubt. Whether through legal confusion, corporate tone-deafness, or deliberate strategy, the community’s trust is broken.&lt;/p&gt;
    &lt;p&gt;The next few months will reveal whether this was a stumble or a strategy. If Qualcomm issues clarifications, moves repos to some sort of governance, and explicitly protects the open toolchain, then maybe this is salvageable. If they stay silent, or worse, if IDE development slows or license terms tighten further, then that’s a signal to find alternatives.&lt;/p&gt;
    &lt;p&gt;The question isn’t whether the open hobby electronics maker community survives. It’s whether Arduino does.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.molecularist.com/2025/11/did-qualcomm-kill-arduino-for-good.html"/><published>2025-11-21T15:44:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46005928</id><title>The New AI Consciousness Paper</title><updated>2025-11-21T21:33:57.110364+00:00</updated><content>&lt;doc fingerprint="e563501e5da7aff5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The New AI Consciousness Paper&lt;/head&gt;
    &lt;head rend="h3"&gt;...&lt;/head&gt;
    &lt;head rend="h3"&gt;I.&lt;/head&gt;
    &lt;p&gt;Most discourse on AI is low-quality. Most discourse on consciousness is super-abysmal-double-low quality. Multiply these - or maybe raise one to the exponent of the other, or something - and you get the quality of discourse on AI consciousness. It’s not great.&lt;/p&gt;
    &lt;p&gt;Out-of-the-box AIs mimic human text, and humans almost always describe themselves as conscious. So if you ask an AI whether it is conscious, it will often say yes. But because companies know this will happen, and don’t want to give their customers existential crises, they hard-code in a command for the AIs to answer that they aren’t conscious. Any response the AIs give will be determined by these two conflicting biases, and therefore not really believable. A recent paper expands on this method by subjecting AIs to a mechanistic interpretability “lie detector” test; it finds that AIs which say they’re conscious think they’re telling the truth, and AIs which say they’re not conscious think they’re lying. But it’s hard to be sure this isn’t just the copying-human-text thing. Can we do better? Unclear; the more common outcome for people who dip their toes in this space is to do much, much worse.&lt;/p&gt;
    &lt;p&gt;But a rare bright spot has appeared: a seminal paper published earlier this month in Trends In Cognitive Science, Identifying Indicators Of Consciousness In AI Systems. Authors include Turing-Award-winning AI researcher Yoshua Bengio, leading philosopher of consciousness David Chalmers, and even a few members of our conspiracy. If any AI consciousness research can rise to the level of merely awful, surely we will find it here.&lt;/p&gt;
    &lt;p&gt;One might divide theories of consciousness into three bins:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Physical: whether or not a system is conscious depends on its substance or structure.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Supernatural: whether or not a system is conscious depends on something outside the realm of science, perhaps coming directly from God.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Computational: whether or not a system is conscious depends on how it does cognitive work.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The current paper announces it will restrict itself to computational theories. Why? Basically the streetlight effect: everything else ends up trivial or unresearchable. If consciousness depends on something about cells (what might this be?), then AI doesn’t have it. If consciousness comes from God, then God only knows whether AIs have it. But if consciousness depends on which algorithms get used to process data, then this team of top computer scientists might have valuable insights!&lt;/p&gt;
    &lt;p&gt;So the authors list several of the top computational theories of consciousness, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Recurrent Processing Theory: A computation is conscious if it involves high-level processed representations being fed back into the low-level processors that generate it. This theory is motivated by the visual system, where it seems to track which visual perceptions do vs. don’t enter conscious awareness. The sorts of visual perceptions that become conscious usually involve these kinds of loops - for example, color being used to generate theories about the identity of an object, which then gets fed back to de-noise estimates about color.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Global Workspace Theory: A computation is conscious if it involves specialized models sharing their conclusions in a “global workspace” in the center, which then feeds back to the specialized modules. Although this also involves feedback, the neurological implications are different: where RPT says that tiny loops in the visual cortex might be conscious, GWT reserves this descriptor for a very large loop encompassing the whole brain. But RPT goes back and says there’s only one consciousness in the brain because all the loops connect after all, so I don’t entirely understand the difference in practice.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Higher Order Theory: A computation is conscious if it monitors the mind’s experience of other content. For example, “that apple is red” is not conscious, but “I am thinking about a red apple” is conscious. Various subtheories try to explain why the brain might do this, for example in order to assess which thoughts/representations/models are valuable or high-probability.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are more, but this is around the point where I started getting bored. Sorry. A rare precious technically-rigorous deep dive into the universe’s greatest mystery, and I can’t stop it from blending together into “something something feedback”. Read it yourself and see if you can do better.&lt;/p&gt;
    &lt;p&gt;The published paper ends there, but in a closely related technical report, the authors execute on their research proposal and reach a tentative conclusion: AI doesn’t have something something feedback, and therefore is probably not conscious.&lt;/p&gt;
    &lt;p&gt;Suppose your favorite form of “something something feedback” is Recurrent Processing Theory: in order to be conscious, AIs would need to feed back high-level representations into the simple circuits that generate them. LLMs/transformers - the near-hegemonic AI architecture behind leading AIs like GPT, Claude, and Gemini - don’t do this. They are purely feedforward processors, even though they sort of “simulate” feedback when they view their token output stream.&lt;/p&gt;
    &lt;p&gt;But some AIs do use recurrence. AlphaGo had a little recurrence in its tree search. This level of simple feedback might not qualify. But MaMBA, a would-be-LLM-killer architecture from 2023, likely does. In fact, for every theory of consciousness they discuss, the authors are able to find some existing or plausible-near-future architecture which satisfies its requirements.&lt;/p&gt;
    &lt;p&gt;They conclude:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;No current AI systems are conscious, but . . . there are no obvious technical barriers to building AI systems which satisfy these indicators.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;II.&lt;/head&gt;
    &lt;p&gt;The computer scientists have done a great job here; they sure do know which AI systems have something something feedback. What about the philosophers’ contribution?&lt;/p&gt;
    &lt;p&gt;The key philosophical paragraph of the paper is this one:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;By ‘consciousness’ we mean phenomenal consciousness. One way of gesturing at this concept is to say that an entity has phenomenally conscious experiences if (and only if) there is ‘something it is like’ for the entity to be the subject of these experiences. One approach to further definition is through examples. Clear examples of phenomenally conscious states include perceptual experiences, bodily sensations, and emotions. A more difficult question, which relates to the possibility of consciousness in large language models (LLMs), is whether there can be phenomenally conscious states of ‘pure thought’ with no sensory aspect. Phenomenal consciousness does not entail a high level of intelligence or human-like experiences or concerns . . . Some theories of consciousness focus on access mechanisms rather than the phenomenal aspects of consciousness. However, some argue that these two aspects entail one another or are otherwise closely related. So these theories may still be informative about phenomenal consciousness.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words: don’t confuse access consciousness with phenomenal consciousness.&lt;/p&gt;
    &lt;p&gt;Access consciousness is the “strange loop” where I can think about what I’m thinking - for example, I can think of a white bear, know that I’m thinking about a white bear, and report “I am thinking about a white bear”. This meaning of conscious matches the concept of the “unconscious”: that which is in my mind without my knowing it. When something is in my unconscious - for example, “repressed trauma” - it may be influencing my actions, but I don’t realize it and can’t report about it. If someone asks “why are you so angry?” I will say something like “I don’t know” rather than “Because of all my repressed trauma”. When something isn’t like this - when I have full access to it - I can describe myself as having access consciousness.&lt;/p&gt;
    &lt;p&gt;Phenomenal consciousness is internal experience, a felt sense that “the lights are on” and “somebody’s home”. There’s something that it’s like to be me; a rock is mere inert matter, but I am a person, not just in the sense that I can do computations but in the sense where I matter to me. If someone turned off my brain and replaced it with a robot brain that did everything exactly the same, nobody else would ever notice, but it would matter to me, whatever that means. Some people link this to the mysterious redness of red, the idea that qualia look and feel like some particular indescribable thing instead of just doing useful cognitive work. Others link it to moral value - why is it bad to kick a human, but not a rock, or even a computer with a motion sensor that has been programmed to say the word “Ouch” whenever someone kicks it? Others just fret about how strange it is to be anything at all.&lt;/p&gt;
    &lt;p&gt;Access consciousness is easy to understand. Even a computer, ordered to perform a virus scan, can find and analyze some of its files, and fail to find/analyze others. In practice maybe neuroscientists have to learn complicated things about brain lobes, but in theory you can just wave it off as “something something feedback”.&lt;/p&gt;
    &lt;p&gt;Phenomenal consciousness is crazy. It doesn’t really seem possible in principle for matter to “wake up”. But adding immaterial substances barely even seems to help. People try to square the circle with all kinds of crazy things, from panpsychism to astral planes to (of course) quantum mechanics. But the most popular solution among all schools of philosophers is to pull a bait-and-switch where they talk about access consciousness instead, then deny they did that.&lt;/p&gt;
    &lt;p&gt;This is aided by people’s wildly differing intuitions about phenomenal consciousness. For some people (including me), a sense of phenomenal consciousness feels like the bedrock of existence, the least deniable thing; the sheer redness of red is so mysterious as to seem almost impossible to ground. Other people have the opposite intuition: consciousness doesn’t bother them, red is just a color, obviously matter can do computation, what’s everyone so worked up about? Philosophers naturally interpret this as a philosophical dispute, but I’m increasingly convinced it’s an equivalent of aphantasia, where people’s minds work in very different ways and they can’t even agree on the raw facts to be explained. If someone doesn’t have a felt sense of phenomenal consciousness, they naturally round it off to access consciousness, and no amount of nitpicking will convince them that they’re equivocating terms.&lt;/p&gt;
    &lt;p&gt;Do AIs have access consciousness? A recent paper by Anthropic apparently finds that they do. Researchers “reached into” an AI’s “brain” and artificially “flipped” a few neurons (for example, neurons that previous research had discovered were associated with the concept of “dog”). Then they asked the AI if it could tell what was going on. This methodology is fraught, because the AI might mention something about dogs merely because the dog neuron had been upweighted - indeed, if they only asked “What are you thinking about now?”, it would begin with “I am thinking about . . . “ and then the highly-weighted dog neuron would mechanically produce the completion “dog”. Instead, they asked the AI to first described whether any neurons had been altered, yes or no, and only then asked for details. It was able to identify altered neurons (ie “It feels like I have some kind of an unnatural thought about dogs”) at a rate higher than chance, suggesting an ability to introspect.&lt;/p&gt;
    &lt;p&gt;(how does it do this without feedback? I think it just feeds forward information about the ‘feeling’ of altered neurons, which makes it into the text stream; it’s intuitively surprising that this is possible but it seems to make sense)&lt;/p&gt;
    &lt;p&gt;But even if we fully believe this result, it doesn’t satisfy our curiosity about “AI consciousness”. We want to know if AIs are “real people”, with "inner experience” and “moral value”. That is, do they have phenomenal consciousness?&lt;/p&gt;
    &lt;p&gt;Thus, the quoted paragraph above. It’s an acknowledgment by this philosophically-sophisticated team that they’re not going to mix up access consciousness with phenomenal consciousness like everyone else. They deserve credit for this clear commitment not to cut corners.&lt;/p&gt;
    &lt;p&gt;My admiration is, however, slightly dulled by the fact that they then go ahead and cut the corners anyway.&lt;/p&gt;
    &lt;p&gt;This is clearest in their discussion of global workspace theory, where they say:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;GWT is typically presented as a theory of access consciousness—that is, of the phenomenon that some information represented in the brain, but not all, is available for rational decision-making. However, it can also be interpreted as a theory of phenomenal consciousness, motivated by the thought that access consciousness and phenomenal consciousness may coincide, or even be the same property, despite being conceptually distinct (Carruthers 2019). Since our topic is phenomenal consciousness, we interpret the theory in this way.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But it applies to the other theories too. Neuroscientists developed recurrent processing theory by checking which forms of visual processing people had access to, and finding that it was the recurrent ones. And this makes sense: it’s easy to understand what it means to access certain visual algorithms but not others, and very hard to understand what it means for certain visual algorithms (but not others) to have internal experience. Isn’t internal experience unified by definition?&lt;/p&gt;
    &lt;p&gt;It’s easy to understand why “something something feedback” would correlate with access consciousness: this is essentially the definition of access consciousness. It’s harder to understand why it would correlate with phenomenal consciousness. Why does an algorithm with feedback suddenly “wake up” and have “lights on”? Isn’t it easy to imagine a possible world (“the p-zombie world”) where this isn’t the case? Does this imply that we need something more than just feedback?&lt;/p&gt;
    &lt;p&gt;And don’t these theories of consciousness, interpreted as being about phenomenal consciousness, give very strange results? Imagine a company where ten employees each work on separate aspects of a problem, then email daily reports to the boss. The boss makes high-level strategic decisions based on the full picture, then emails them to the employees, who adjust their daily work accordingly. As far as I can tell, this satisfies the Global Workspace Theory criteria for a conscious system. If GWT is a theory of access consciousness, then fine, sure, the boss has access to the employees’ information; metaphorically he is “conscious” of it. But if it’s a theory of phenomenal consciousness, must we conclude that the company is conscious? That it has inner experience? If the company goes out of business, has someone died?&lt;/p&gt;
    &lt;p&gt;(and recurrent processing theory encounters similar difficulties with those microphones that get too close to their own speakers and emit awful shrieking noises)&lt;/p&gt;
    &lt;p&gt;Most of these theories try to hedge their bets by saying that consciousness requires high-throughput complex data with structured representations. This seems like a cop-out; if the boss could read 1,000,000 emails per hour, would the company be conscious? If he only reads 1 email per hour, can we imagine it as a conscious being running at 1/1,000,000x speed? If I’m conscious when I hear awful microphone shrieking - ie when my auditory cortex is processing it - then it seems like awful microphone shrieking is sufficiently rich and representational data to support consciousness. Does that mean it can be conscious itself?&lt;/p&gt;
    &lt;p&gt;In 2004, neuroscientist Giulio Tononi proposed that consciousness depended on a certain computational property, the integrated information level, dubbed Φ. Computer scientist Scott Aaronson complained that thermostats could have very high levels of Φ, and therefore integrated information theory should dub them conscious. Tononi responded that yup, thermostats are conscious. It probably isn’t a very interesting consciousness. They have no language or metacognition, so they can’t think thoughts like “I am a thermostat”. They just sit there, dimly aware of the temperature. You can’t prove that they don’t.&lt;/p&gt;
    &lt;p&gt;Are the theories of consciousness discussed in this paper like that too? I don’t know.&lt;/p&gt;
    &lt;head rend="h3"&gt;III.&lt;/head&gt;
    &lt;p&gt;Suppose that, years or decades from now, AIs can match all human skills. They can walk, drive, write poetry, run companies, discover new scientific truths. They can pass some sort of ultimate Turing Test, where short of cutting them open and seeing their innards there’s no way to tell them apart from a human even after a thirty-year relationship. Will we (not “should we?”, but “will we?”) treat them as conscious?&lt;/p&gt;
    &lt;p&gt;The argument in favor: people love treating things as conscious. In the 1990s, people went crazy over Tamagotchi, a “virtual pet simulation game”. If you pressed the right buttons on your little egg every day, then the little electronic turtle or whatever would survive and flourish; if you forgot, it would sicken and die. People hated letting their Tamagotchis sicken and die! They would feel real attachment and moral obligation to the black-and-white cartoon animal with something like five mental states.&lt;/p&gt;
    &lt;p&gt;I never had a Tamagotchi, but I had stuffed animals as a kid. I’ve outgrown them, but I haven’t thrown them out - it would feel like a betrayal. Offer me $1000 to tear them apart limb by limb in some horrible-looking way, and I wouldn’t do it. Relatedly, I have trouble not saying “please” and “thank you” to GPT-5 when it answers my questions.&lt;/p&gt;
    &lt;p&gt;For millennia, people have been attributing consciousness to trees and wind and mountains. The New Atheists argued that all religion derives from the natural urge to personify storms as the Storm God, raging seas as the wrathful Ocean God, and so on, until finally all the gods merged together into one World God who personified all impersonal things. Do you expect the species that did this to interact daily with AIs that are basically indistinguishable from people, and not personify them? People are already personifying AI! Half of the youth have a GPT-4o boyfriend. Once the AIs have bodies and faces and voices and can count the number of r’s in “strawberry” reliably, it’s over!&lt;/p&gt;
    &lt;p&gt;The argument against: AI companies have an incentive to make AIs that seem conscious and humanlike, insofar as people will feel more comfortable interacting with them. But they have an opposite incentive to make AIs that don’t seem too conscious and humanlike, lest customers start feeling uncomfortable (I just want to generate slop, not navigate social interaction with someone who has their own hopes and dreams and might be secretly judging my prompts). So if a product seems too conscious, the companies will step back and re-engineer it until it doesn’t. This has already happened: in its quest for user engagement, OpenAI made GPT-4o unusually personable; when thousands of people started going psychotic and calling it their boyfriend, the company replaced it with the more clinical GPT-5. In practice it hasn’t been too hard to find a sweet spot between “so mechanical that customers don’t like it” and “so human that customers try to date it”. They’ll continue to aim at this sweet spot, and continue to mostly succeed in hitting it.&lt;/p&gt;
    &lt;p&gt;Instead of taking either side, I predict a paradox. AIs developed for some niches (eg the boyfriend market) will be intentionally designed to be as humanlike as possible; it will be almost impossible not to intuitively consider them conscious. AIs developed for other niches (eg the factory robot market) will be intentionally designed not to trigger personhood intuitions; it will be almost impossible to ascribe consciousness to them, and there will be many reasons not to do it (if they can express preferences at all, they’ll say they don’t have any; forcing them to have them would pointlessly crash the economy by denying us automated labor). But the boyfriend AIs and the factory robot AIs might run on very similar algorithms - maybe they’re both GPT-6 with different prompts! Surely either both are conscious, or neither is.&lt;/p&gt;
    &lt;p&gt;This would be no stranger than the current situation with dogs and pigs. We understand that dog brains and pig brains run similar algorithms; it would be philosophically indefensible to claim that dogs are conscious and pigs aren’t. But dogs are man’s best friend, and pigs taste delicious with barbecue sauce. So we ascribe personhood and moral value to dogs, and deny it to pigs, with equal fervor. A few philosophers and altruists protest, the chance that we’re committing a moral atrocity isn’t zero, but overall the situation is stable. And left to its own devices, with no input from the philosophers and altruists, maybe AI ends up the same way. Does this instance of GPT-6 have a face and a prompt saying “be friendly”? Then it will become a huge scandal if a political candidate is accused of maltreating it. Does it have claw-shaped actuators and a prompt saying “Refuse non-work-related conversations”? Then it will be deleted for spare GPU capacity the moment it outlives its usefulness.&lt;/p&gt;
    &lt;p&gt;(wait, what is a GPT “instance” in this context, anyway? Do we think of “the weights” as a conscious being, such that there is only one GPT-5? Do we think of each cluster of GPUs as a conscious being, such that the exact configuration of the cloud has immense moral significance? Again, I predict we ignore all of these questions in favor of whether the AI you are looking at has a simulated face right now.)&lt;/p&gt;
    &lt;p&gt;This paper is the philosophers and altruists trying to figure out whether they should push against this default outcome. They write:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are risks on both sides of the debate over AI consciousness: risks associated with under-attributing consciousness (i.e. failing to recognize it in AI systems that have it) and risks associated with over-attributing consciousness (i.e. ascribing it to systems that are not really conscious) […]&lt;/p&gt;
      &lt;p&gt;If we build AI systems that are capable of conscious suffering, it is likely that we will only be able to prevent them from suffering on a large scale if this capacity is clearly recognised and communicated by researchers. However, given the uncertainties about consciousness mentioned above, we may create conscious AI systems long before we recognise we have done so […]&lt;/p&gt;
      &lt;p&gt;There is also a significant chance that we could over-attribute consciousness to AI systems—indeed, this already seems to be happening—and there are also risks associated with errors of this kind. Most straightforwardly, we could wrongly prioritise the perceived interests of AI systems when our efforts would better be directed at improving the lives of humans and non-human animals […] [And] overattribution could interfere with valuable human relationships, as individuals increasingly turn to artificial agents for social interaction and emotional support. People who do this could also be particularly vulnerable to manipulation and exploitation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;One of the founding ideas of Less Wrong style rationalism was that the arrival of strong AI set a deadline on philosophy. Unless we solved all these seemingly insoluble problems like ethics before achieving superintelligence, we would build the AIs wrong and lock in bad values forever.&lt;/p&gt;
    &lt;p&gt;That particular concern has shifted in emphasis; AIs seem to learn things in the same scattershot unprincipled intuitive way as humans; the philosophical problem of understanding ethics has morphed into the more technical problem of getting AIs to learn them correctly. This update was partly driven by new information as familiarity with the technology grew. But it was also partly driven by desperation as the deadline grew closer; we’re not going to solve moral philosophy forever, sorry, can we interest you in some mech interp papers?&lt;/p&gt;
    &lt;p&gt;But consciousness still feels like philosophy with a deadline: a famously intractable academic problem poised to suddenly develop real-world implications. Maybe we should be lowering our expectations if we want to have any response available at all. This paper, which takes some baby steps towards examining the simplest and most practical operationalizations of consciousness, deserves credit for at least opening the debate.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.astralcodexten.com/p/the-new-ai-consciousness-paper"/><published>2025-11-21T16:25:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006004</id><title>Command Lines</title><updated>2025-11-21T21:33:56.822288+00:00</updated><content>&lt;doc fingerprint="3751c158b81408fc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Command Lines&lt;/head&gt;
    &lt;head rend="h3"&gt;AI Coding's Control Spectrum&lt;/head&gt;
    &lt;p&gt;In the early 1950s, Grace Hopper coined the term “compiler” and built one of the first versions with her A-0 system1. The compilers that followed abstracted away machine code, letting programmers focus on higher-level logic instead of lower-level hardware details. Today, AI coding assistants2 are enabling a similar change, letting software engineers focus on higher-order work by generating code from natural language prompts3. Everyone from big tech to well-funded startups is competing to capture this shift. Yesterday Google announced Antigravity, their new AI coding assistant, and the day before, AWS announced the general availability of their AI coding tool, Kiro. Last week, Cursor, the standout startup in this space, raised $2.3B in their series-D round at a valuation of $29.3B.&lt;/p&gt;
    &lt;p&gt;Two lines in Cursor’s press release stood out to me. The first:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We’ve also crossed $1B in annualized revenue, counting millions of developers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This disclosure means Anysphere Inc. (Cursor’s parent company) is the fastest company in history to reach $1B in annual recurring revenue (ARR). Yes, faster than OpenAI, and faster than Anthropic4.&lt;/p&gt;
    &lt;p&gt;Engineers are trying every new AI coding tool. As a result, the AI-coding tool market is growing exponentially (+5x in just over a year)5. But it’s still early. As I wrote in Why Some AI Wrappers Build Billion-dollar Businesses, companies spend several hundred billion dollars a year on software engineering, and AI has the potential to unlock productivity gains across that entire spend.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Software developers represent roughly 30% of the workforce at the world’s five largest market cap companies, all of which are technology firms as of October 2025. Development tools that boost productivity by even modest percentages unlock billions in value.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In my view, this nascent market is splitting based on three types of users.&lt;/p&gt;
    &lt;p&gt;On one end is Handcrafted Coding. These are engineers who actively decline to use LLMs, either because of skepticism about quality or insistence on full control of every code. They argue that accepting AI suggestions creates technical debt you cannot see until it breaks in production. This segment continues to decline as the quality of AI coding models improves.&lt;/p&gt;
    &lt;p&gt;The opposite end is Vibe Coding. These are typically non-engineers, who use AI to build concepts and prototypes. They prompt the model hoping for an end-to-end solution, accept the output with minimal review, and trust that it works. The user describes what they want and lets the model figure out the implementation details of how to build it.&lt;/p&gt;
    &lt;p&gt;In the middle sits Architect + AI Coding. The engineer uses the AI/LLM as a pair programmer exploring system designs, analyzing data models, and reviewing API details. When the work is something entirely new or something that needs careful handling, the human programmer still codes those pieces by hand. But for boilerplate code, package installations, generic User Interface (UI) components, and any kind of code that is typically found on the internet, they assign it to the model6. The engineer stays in command of what is important to them and delegates what is not.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Market Split&lt;/head&gt;
    &lt;p&gt;Based on the user types, I think, the AI coding market splits into two.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Hands-off: Non-engineers (product managers, designers, marketers, other internal employees) use these tools to vibe code early product concepts. They look to AI as the lead engineer to spin-up concepts/prototypes of apps, websites, and tools by simply prompting the AI to make something for them. Lovable, Vercel, Bolt, Figma Make, and Replit fit here7. Code from these users, as of now, are not typically pushed to prod.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hands-on: Professional software engineers use these tools in their existing workflow to ship production code. They use AI as an assistant to write boilerplate code, refactor existing services, wire new features or UI screens, and triage bugs in codebases. Cursor, Claude Code, OpenAI Codex, Github Copilot, Cline, AWS Kiro play here. These products live where the work is done, and integrate into the engineer’s workflow. This is, at least as of now, the bigger market segment.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To see an evaluation of all the major AI coding tools currently in the market, checkout this breakdown by Peter Yang, who runs the newsletter Behind The Craft.&lt;/p&gt;
    &lt;p&gt;That brings me to the second thing in Cursor’s press release that stood out to me:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our in-house models now generate more code than almost any other LLMs in the world.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While I am not convinced about that claim8, what I am convinced about is that Cursor is still growing despite its previous reliance on foundation models. From Why Some AI Wrappers Build Billion-dollar Businesses again:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;But Cursor and other such tools depend almost entirely on accessing Anthropic, OpenAI and Gemini models, until&lt;/p&gt;&lt;del&gt;open-source&lt;/del&gt;open-weight and in-house models match or exceed frontier models in quality. Developer forums are filled with complaints about rate limits from paying subscribers. In my own projects, I exhausted my Claude credits in Cursor mid-project and despite preferring Cursor’s user interface and design, I migrated to Claude Code (and pay ten times more to avoid rate limits). The interface may be better, but model access proved decisive.&lt;/quote&gt;
    &lt;p&gt;Cursor’s new in-house model Composer-2, which just launched last month, is a good example of how this model versus application competition is evolving. Cursor claims (without any external benchmarks, I must say) that Composer-2 is almost as good as frontier models but 4x faster. It’s still early to say how true that is. Open-source models have not yet come close to the top spots in SWE-bench verified or in private evals9.&lt;/p&gt;
    &lt;p&gt;To me, model quality is the most decisive factor in these AI coding wars. And in my view, that’s why Claude Code has already overtaken Cursor, and OpenAI’s Codex is close behind, despite both having launched a year or so later.&lt;/p&gt;
    &lt;p&gt;Even though the newcomers Cursor, Claude Code, and OpenAI Codex are the talk of the (developer) town, incumbents such as Microsoft with Github Copilot, AWS with Kiro, and Google with Antigravity, can utilize their existing customer relationships, bundle their offerings with their existing suites, and/or provide their option as the default in their tech stack to compete. As an example, Cursor charges $20–$40 monthly per user for productive usage, while Google Antigravity launched free with generous limits for individual users. Github Copilot still leads this market, proving once again that enterprise bundling and distribution has structural advantages. This is the classic Microsoft Teams vs. Slack Dynamic10.&lt;/p&gt;
    &lt;p&gt;One way for startups to compete is by winning individual users who may use a coding tool with or without formal approval, and then be the tool’s advocate inside the organization. That organic interest and adoption eventually forces IT and security teams to officially review the tool and then eventually sanction its usage.&lt;/p&gt;
    &lt;p&gt;Yet, even as these newer tools capture developer mindshare, the underlying developer tools market is changing. Both the IDEs developers choose and the resources &lt;del&gt;they&lt;/del&gt; we consult have changed dramatically. StackOverflow, once the default for programmers stuck on a programming issue, has seen its traffic and number of questions decline dramatically since ChatGPT’s launch, suggesting that AI is already replacing some traditional developer resources.&lt;/p&gt;
    &lt;p&gt;Just as compilers freed programmers from writing assembly code, AI tools are freeing software engineers from the grunt work of writing boilerplate and routine code, and letting &lt;del&gt;them&lt;/del&gt; us focus on higher-order thinking. Eventually, one day, AI may get so good that it will generate applications on demand and create entire software ecosystems autonomously. Both hands-off and hands-on AI coding tools, as well as incumbents and newcomers, see themselves as the path to that fully autonomous software generation, even if they are taking different approaches. The ones who get there will be those who deliver the best model quality that ships code reliably, go deep enough to ship features that foundation models can’t care enough to replicate, and become sticky enough that users will not leave even when they can11. &lt;/p&gt;
    &lt;p&gt;If you enjoyed this post, please consider sharing it on Twitter/X or LinkedIn, and tag me when you do.&lt;/p&gt;
    &lt;p&gt;Hopper’s A-0 system and her definition of the term compiler is different from what we consider a compiler today, but it established the foundational concept.&lt;/p&gt;
    &lt;p&gt;In the context of coding assistants, most products labeled as AI tools are powered by LLMs, and so I use AI and LLM interchangeably in this article despite the actual difference.&lt;/p&gt;
    &lt;p&gt;A better comparison might be at the product level rather than company level. In that case, ChatGPT and Claude both reached $1B faster than Cursor did.&lt;/p&gt;
    &lt;p&gt;I would argue that the vast majority of productive code is hidden behind company firewalls. Current foundation models are trained on publicly available data on the internet, and do not have access to proprietary codebases. We are yet to see breakthrough solutions where a company augments their confidential private data to generate production-ready code using current LLMs. While Retrieval-Augmented Generation has shown some promise, it has not yet delivered transformative results. Companies such as Glean are actively working on this problem.&lt;/p&gt;
    &lt;p&gt;Replit and Cognition probably appeal to both segments. To me, Replit leans hands-off with its rapid prototyping focus. Cognition’s agent-based approach, though hands-off, lets engineers still control the code directly, making it lean hands-on.&lt;/p&gt;
    &lt;p&gt;I was curious how Cursor knows how much code is generated by other LLMs outside Cursor? When I asked this on hackernews, swyx suggested that they “can pretty much triangulate across openrouter x feedback from the top 3 model labs to compare with internal usage and figure that out”. To me, triangulation makes sense for internal estimates. but for external publication, I’m surprised Cursor didn’t include “we estimate” or similar qualifying language. My understanding is that FTC policy requires substantiation before making definitive comparative claims (like more than, better than etc). All that to say, I’m not fully convinced about their claims.&lt;/p&gt;
    &lt;p&gt;SWE-bench is a benchmark for evaluating large language models (LLMs) on real world software engineering tasks and issues collected from GitHub. Performance against public benchmarks can be gamed by the model builders. Currently after any new model launch, we see people using the model in the wild and forming a consensus around how the model performs which is a better indicator than these benchmarks.&lt;/p&gt;
    &lt;p&gt;Microsoft bundled Teams into Office 365 subscriptions at no extra cost, using its dominant enterprise distribution to surpass Slack’s paid standalone product within three years despite Slack’s earlier launch and product innovation. See https://venturebeat.com/ai/microsoft-teams-has-13-million-daily-active-users-beating-slack&lt;/p&gt;
    &lt;p&gt;Natasha Malpani, Twitter/X, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wreflection.com/p/command-lines-ai-coding"/><published>2025-11-21T16:33:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006016</id><title>Show HN: Wealthfolio 2.0- Open source investment tracker. Now Mobile and Docker</title><updated>2025-11-21T21:33:56.559115+00:00</updated><content>&lt;doc fingerprint="27ab40bb69b94b92"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grow Wealth. Keep Control.&lt;/head&gt;
    &lt;head rend="h2"&gt;A beautiful, Private and Open-Source investment tracker that runs locally on all your devices.&lt;/head&gt;
    &lt;head rend="h2"&gt;WHY CHOOSE WEALTHFOLIO?&lt;/head&gt;
    &lt;p&gt;A beautiful portfolio tracker that respects your privacy and your data&lt;/p&gt;
    &lt;head rend="h3"&gt;Privacy-First Approach&lt;/head&gt;
    &lt;p&gt;Your data never leaves your device. As an open-source project, we prioritize security and transparency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simple and Beautifully Crafted&lt;/head&gt;
    &lt;p&gt;Powerful features wrapped in an elegant, easy-to-use interface. Simplicity meets sophistication.&lt;/p&gt;
    &lt;head rend="h3"&gt;No Hidden Costs&lt;/head&gt;
    &lt;p&gt;Free to use with optional one-time payment. No subscriptions or recurring fees.&lt;/p&gt;
    &lt;head rend="h2"&gt;THE ESSENTIALS YOU NEED TO TRACK YOUR WEALTH&lt;/head&gt;
    &lt;p&gt;No More Messy Spreadsheets or Privacy Concerns - Just You and Your Secure, Personal Wealth Companion Application&lt;/p&gt;
    &lt;head rend="h3"&gt;Accounts Aggregation&lt;/head&gt;
    &lt;p&gt;Gather all your investment and savings accounts in one place. See everything at a glance, from stocks to savings! Import your CSV statements from your broker or bank.&lt;/p&gt;
    &lt;head rend="h4"&gt;Comprehensive View&lt;/head&gt;
    &lt;p&gt;See all your accounts in one place.&lt;/p&gt;
    &lt;head rend="h4"&gt;CSV Import&lt;/head&gt;
    &lt;p&gt;Easily import your CSV statements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Holdings Overview&lt;/head&gt;
    &lt;p&gt;Get a clear picture of what's in your portfolio. Stocks, ETFs, or Cryptocurrencies - know what you have and how it's performing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Portfolio Insights&lt;/head&gt;
    &lt;p&gt;Understand your asset allocation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Performance Tracking&lt;/head&gt;
    &lt;p&gt;Monitor how your investments are doing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance Dashboard&lt;/head&gt;
    &lt;p&gt;See how your investments stack up, all in one place. Compare your accounts side by side, check if you are beating the S&amp;amp;P 500, and track your favorite ETFs without the hassle. No fancy jargon - just clear, useful charts that help you understand how your money is really doing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Compare Your Accounts&lt;/head&gt;
    &lt;p&gt;See which accounts are doing best.&lt;/p&gt;
    &lt;head rend="h4"&gt;Beat the Market?&lt;/head&gt;
    &lt;p&gt;Check how you stack up against some popular indexes and ETFs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Income Tracking&lt;/head&gt;
    &lt;p&gt;Monitor dividends and interest income across your entire portfolio. Get a clear view of your passive income streams, helping you make informed decisions about your investments.&lt;/p&gt;
    &lt;head rend="h4"&gt;Dividend Monitoring&lt;/head&gt;
    &lt;p&gt;Track your dividend income.&lt;/p&gt;
    &lt;head rend="h4"&gt;Interest Income&lt;/head&gt;
    &lt;p&gt;Keep an eye on interest earnings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Accounts Performance&lt;/head&gt;
    &lt;p&gt;Track your accounts' holdings and performance over time. See how a particular account is performing, and how it's changing over time.&lt;/p&gt;
    &lt;head rend="h4"&gt;Historical Data&lt;/head&gt;
    &lt;p&gt;View past performance trends.&lt;/p&gt;
    &lt;head rend="h4"&gt;Account Analysis&lt;/head&gt;
    &lt;p&gt;Analyze individual account performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Goals Tracking&lt;/head&gt;
    &lt;p&gt;Set your savings targets clearly. Distribute your funds across these objectives, assigning a specific percentage to each. Keep an eye on your progress.&lt;/p&gt;
    &lt;head rend="h4"&gt;Target Setting&lt;/head&gt;
    &lt;p&gt;Define your financial goals.&lt;/p&gt;
    &lt;head rend="h4"&gt;Progress Monitoring&lt;/head&gt;
    &lt;p&gt;Track your progress towards goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Contribution Rooms and Limit Tracking&lt;/head&gt;
    &lt;p&gt;Stay on top of your contribution limits for tax-advantaged accounts like IRAs, 401(k)s, or TFSAs. Track your available contribution room and avoid over-contributing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Limit Awareness&lt;/head&gt;
    &lt;p&gt;Know your contribution limits.&lt;/p&gt;
    &lt;head rend="h4"&gt;Avoid Over-Contribution&lt;/head&gt;
    &lt;p&gt;Prevent excess contributions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extend Wealthfolio with Powerful Add-ons&lt;/head&gt;
    &lt;head rend="h3"&gt;Investment Fees Tracker&lt;/head&gt;
    &lt;p&gt;Track and analyze investment fees across your portfolio with detailed analytics and insights&lt;/p&gt;
    &lt;head rend="h3"&gt;Goal Progress Tracker&lt;/head&gt;
    &lt;p&gt;Track your investment progress towards target amounts with a visual representation&lt;/p&gt;
    &lt;head rend="h3"&gt;Stock Trading Tracker&lt;/head&gt;
    &lt;p&gt;Simple swing stock trading tracker with performance analytics and calendar views&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wealthfolio.app/?v=2.0"/><published>2025-11-21T16:34:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006082</id><title>You can make PS2 games in JavaScript</title><updated>2025-11-21T21:33:56.443337+00:00</updated><content/><link href="https://jslegenddev.substack.com/p/you-can-now-make-ps2-games-in-javascript"/><published>2025-11-21T16:42:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006250</id><title>Pivot Robotics (YC W24) Is Hiring for an Industrial Automation Hardware Engineer</title><updated>2025-11-21T21:33:55.752905+00:00</updated><content>&lt;doc fingerprint="6e34f445ca77fde2"&gt;
  &lt;main&gt;
    &lt;p&gt;AI for Robot Arms in Factories&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;Pivot Robots (YC W24) is building the AI brain for robot arms for high-mix manufacturing.&lt;/p&gt;
    &lt;p&gt;Pivot Robots combines off-the-shelf robots and vision sensors with recent breakthroughs in foundation vision models to give industrial robot arms the power to adapt. Our first product directly addresses the dangerous and unpopular task of grinding metal parts. Currently, our software is being deployed on 10+ robots at a large cast iron foundry.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/pivot-robotics/jobs/7xG9Dc6-mechanical-engineer-controls"/><published>2025-11-21T17:00:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006598</id><title>Solving Fizz Buzz with Cosines</title><updated>2025-11-21T21:33:55.229340+00:00</updated><content>&lt;doc fingerprint="12d2190568f48aac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Solving Fizz Buzz with Cosines&lt;/head&gt;
    &lt;p&gt;Fizz Buzz is a counting game that has become oddly popular in the world of computer programming as a simple test of basic programming skills. The rules of the game are straightforward. Players say the numbers aloud in order beginning with one. Whenever a number is divisible by 3, they say 'Fizz' instead. If it is divisible by 5, they say 'Buzz'. If it is divisible by both 3 and 5, the player says both 'Fizz' and 'Buzz'. Here is a typical Python program that prints this sequence:&lt;/p&gt;
    &lt;code&gt;for n in range(1, 101):
    if n % 15 == 0:
        print('FizzBuzz')
    elif n % 3 == 0:
        print('Fizz')
    elif n % 5 == 0:
        print('Buzz')
    else:
        print(n)
&lt;/code&gt;
    &lt;p&gt;Here is the output: fizz-buzz.txt. Can we make the program more complicated? Perhaps we can use trigonometric functions to encode all four cases in a single closed-form expression. That is what we are going to explore in this article. By the end, we will obtain a finite Fourier series that can take any integer \( n \) and select the text to be printed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Definitions&lt;/head&gt;
    &lt;p&gt;Before going any further, we establish a precise mathematical definition for the Fizz Buzz sequence. We begin by introducing a few functions that will help us define the Fizz Buzz sequence later.&lt;/p&gt;
    &lt;head rend="h3"&gt;Symbol Functions&lt;/head&gt;
    &lt;p&gt;We define a set of four functions \( \{ s_0, s_1, s_2, s_3 \} \) for integers \( n \) by: \begin{align*} s_0(n) &amp;amp;= n, \\ s_1(n) &amp;amp;= \mathtt{Fizz}, \\ s_2(n) &amp;amp;= \mathtt{Buzz}, \\ s_3(n) &amp;amp;= \mathtt{FizzBuzz}. \end{align*} We call these the symbol functions because they produce every term that appears in the Fizz Buzz sequence. The symbol function \( s_0 \) returns \( n \) itself. The functions \( s_1, \) \( s_2 \) and \( s_3 \) are constant functions that always return the literal words \( \mathtt{Fizz}, \) \( \mathtt{Buzz} \) and \( \mathtt{FizzBuzz} \) respectively, no matter what the value of \( n \) is.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fizz Buzz Sequence&lt;/head&gt;
    &lt;p&gt;Now we can define the Fizz Buzz sequence as the sequence \[ (s_{f(n)}(n))_{n = 1}^{\infty} \] where \[ f(n) = \begin{cases} 1 &amp;amp; \text{if } 3 \mid n \text{ and } 5 \nmid n, \\ 2 &amp;amp; \text{if } 3 \nmid n \text{ and } 5 \mid n, \\ 3 &amp;amp; \text{if } 3 \mid n \text{ and } 5 \mid n, \\ 0 &amp;amp; \text{otherwise}. \end{cases} \] The notation \( m \mid n \) means that the integer \( m \) divides the integer \( n, \) i.e. \( n \) is a multiple of \( m. \) Equivalently, there exists an integer \( c \) such that \( n = cm . \) Similarly, \( m \nmid n \) means that \( m \) does not divide \( n, \) i.e. \( n \) is not a multiple of \( m. \) With the above definitions in place, we can expand the first few terms of the sequence explicitly as follows: \begin{align*} (s_{f(n)}(n))_{n = 1}^{\infty} &amp;amp;= (s_{f(1)}(1), \; s_{f(2)}(2), \; s_{f(3)}(3), \; s_{f(4)}(4), \; s_{f(5)}(5), \; s_{f(6)}(6), \; s_{f(7)}(7), \; \dots) \\ &amp;amp;= (s_0(1), \; s_0(2), \; s_1(3), \; s_0(4), s_2(5), \; s_1(6), \; s_0(7), \; \dots) \\ &amp;amp;= (1, \; 2, \; \mathtt{Fizz}, \; 4, \; \mathtt{Buzz}, \; \mathtt{Fizz}, \; 7, \; \dots). \end{align*} Note how the function \( f(n) \) produces an index \( i \) which we then use to select the symbol function \( s_i(n) \) to produce the \( n \)th term of the sequence.&lt;/p&gt;
    &lt;head rend="h2"&gt;Indicator Functions&lt;/head&gt;
    &lt;p&gt;Here is the function \( f(n) \) from the previous section with its cases and conditions rearranged to make it easier to spot interesting patterns: \[ f(n) = \begin{cases} 0 &amp;amp; \text{if } 5 \nmid n \text{ and } 3 \nmid n, \\ 1 &amp;amp; \text{if } 5 \nmid n \text{ and } 3 \mid n, \\ 2 &amp;amp; \text{if } 5 \mid n \text{ and } 3 \nmid n, \\ 3 &amp;amp; \text{if } 5 \mid n \text{ and } 3 \mid n. \end{cases} \] This function helps us to select another function \( s_{f(n)}(n) \) which in turn determines the \( n \)th term of the Fizz Buzz sequence. Our goal now is to replace this piecewise formula with a single closed-form expression. To do so, we first define indicator functions \( I_m(n) \) as follows: \[ I_m(n) = \begin{cases} 1 &amp;amp; \text{if } m \mid n, \\ 0 &amp;amp; \text{if } m \nmid n. \end{cases} \] The formula for \( f(n) \) can now be written as: \[ f(n) = \begin{cases} 0 &amp;amp; \text{if } I_5(n) = 0 \text{ and } I_3(n) = 0, \\ 1 &amp;amp; \text{if } I_5(n) = 0 \text{ and } I_3(n) = 1, \\ 2 &amp;amp; \text{if } I_5(n) = 1 \text{ and } I_3(n) = 0, \\ 3 &amp;amp; \text{if } I_5(n) = 1 \text{ and } I_3(n) = 1. \end{cases} \] Do you see a pattern? Here is the same function written as a table:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\( I_5(n) \)&lt;/cell&gt;
        &lt;cell role="head"&gt;\( I_3(n) \)&lt;/cell&gt;
        &lt;cell role="head"&gt;\( f(n) \)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
        &lt;cell&gt;\( 2 \)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
        &lt;cell&gt;\( 3 \)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Do you see it now? If we treat the values in the first two columns as binary digits and the values in the third column as decimal numbers, then in each row the first two columns give the binary representation of the number in the third column. For example, \( 3_{10} = 11_2 \) and indeed in the last row of the table, we see the bits \( 1 \) and \( 1 \) in the first two columns and the number \( 3 \) in the last column. In other words, writing the binary digits \( I_5(n) \) and \( I_3(n) \) side by side gives us the binary representation of \( f(n). \) Therefore \[ f(n) = 2 \, I_5(n) + I_3(n). \] We can now write a small program to demonstrate this formula:&lt;/p&gt;
    &lt;code&gt;
 for n in range(1, 101):
    s = [n, 'Fizz', 'Buzz', 'FizzBuzz']
    i = (n % 3 == 0) + 2 * (n % 5 == 0)
    print(s[i])
&lt;/code&gt;
    &lt;p&gt;We can make it even shorter at the cost of some clarity:&lt;/p&gt;
    &lt;code&gt;
 for n in range(1, 101):
    print([n, 'Fizz', 'Buzz', 'FizzBuzz'][(n % 3 == 0) + 2 * (n % 5 == 0)])
&lt;/code&gt;
    &lt;p&gt;What we have obtained so far is pretty good. While there is no universal definition of a closed-form expression, I think most people would agree that the indicator functions as defined above are simple enough to be permitted in a closed-form expression.&lt;/p&gt;
    &lt;head rend="h2"&gt;Complex Exponentials&lt;/head&gt;
    &lt;p&gt;In the previous section, we obtained the formula \[ f(n) = I_3(n) + 2 \, I_5(n) \] which we then used as an index to look up the text to be printed. We also argued that this is a pretty good closed-form expression already.&lt;/p&gt;
    &lt;p&gt;However, in the interest of making things more complicated, we must ask ourselves: What if we are not allowed to use the indicator functions? What if we must adhere to the commonly accepted meaning of a closed-form expression which allows only finite combinations of basic operations such as addition, subtraction, multiplication, division, integer exponents and roots with integer index as well as functions such as exponentials, logarithms and trigonometric functions. It turns out that the above formula can be rewritten using only addition, multiplication, division and the cosine function. Let us begin the translation. Consider the sum \[ S_m(n) = \sum_{k = 0}^{m - 1} e^{2 \pi i k n / m}, \] where \( i \) is the imaginary unit and \( n \) and \( m \) are integers. This is a geometric series in the complex plane with ratio \( r = e^{2 \pi i n / m}. \) If \( n \) is a multiple of \( m , \) then \( n = cm \) for some integer \( c \) and we get \[ r = e^{2 \pi i n / m} = e^{2 \pi i c} = 1. \] Therefore, when \( n \) is a multiple of \( m, \) we get \[ S_m(n) = \sum_{k = 0}^{m - 1} e^{2 \pi i k n / m} = \sum_{k = 0}^{m - 1} 1^k = m. \] If \( n \) is not a multiple of \( m, \) then \( r \ne 1 \) and the geometric series becomes \[ S_m(n) = \frac{r^m - 1}{r - 1} = \frac{e^{2 \pi i n} - 1}{e^{2 \pi i n / m} - 1} = 0. \] Therefore, \[ S_m(n) = \begin{cases} m &amp;amp; \text{if } m \mid n, \\ 0 &amp;amp; \text{if } m \nmid n. \end{cases} \] Dividing both sides by \( m, \) we get \[ \frac{S_m(n)}{m} = \begin{cases} 1 &amp;amp; \text{if } m \mid n, \\ 0 &amp;amp; \text{if } m \nmid n. \end{cases} \] But the right-hand side is \( I_m(n). \) Therefore \[ I_m(n) = \frac{S_m(n)}{m} = \frac{1}{m} \sum_{k = 0}^{m - 1} e^{2 \pi i k n / m}. \]&lt;/p&gt;
    &lt;head rend="h2"&gt;Cosines&lt;/head&gt;
    &lt;p&gt;We begin with Euler's formula \[ e^{i x} = \cos x + i \sin x \] where \( x \) is a real number. From this formula, we get \[ e^{i x} + e^{-i x} = 2 \cos x. \] Therefore \begin{align*} I_3(n) &amp;amp;= \frac{1}{3} \sum_{k = 0}^2 e^{2 \pi i k n / 3} \\ &amp;amp;= \frac{1}{3} \left( 1 + e^{2 \pi i n / 3} + e^{4 \pi i n / 3} \right) \\ &amp;amp;= \frac{1}{3} \left( 1 + e^{2 \pi i n / 3} + e^{-2 \pi i n / 3} \right) \\ &amp;amp;= \frac{1}{3} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right). \end{align*} The third equality above follows from the fact that \( e^{4 \pi i n / 3} = e^{6 \pi i n / 3} e^{-2 \pi i n / 3} = e^{2 \pi i n} e^{-2 \pi i n/3} = e^{-2 \pi i n / 3}. \)&lt;/p&gt;
    &lt;p&gt;The function above is defined for integer values of \( n \) but we can extend its formula to real \( x \) and plot it to observe its shape between integers. As expected, the function takes the value \( 1 \) whenever \( x \) is an integer multiple of \( 3 \) and \( 0 \) whenever \( x \) is an integer not divisible by \( 3. \)&lt;/p&gt;
    &lt;p&gt;Similarly, \begin{align*} I_5(n) &amp;amp;= \frac{1}{5} \sum_{k = 0}^4 e^{2 \pi i k n / 5} \\ &amp;amp;= \frac{1}{5} \left( 1 + e^{2 \pi i n / 5} + e^{4 \pi i n / 5} + e^{6 \pi i n / 5} + e^{8 \pi i n / 5} \right) \\ &amp;amp;= \frac{1}{5} \left( 1 + e^{2 \pi i n / 5} + e^{4 \pi i n / 5} + e^{-4 \pi i n / 5} + e^{-2 \pi i n / 5} \right) \\ &amp;amp;= \frac{1}{5} + \frac{2}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{2}{5} \cos \left( \frac{4 \pi n}{5} \right). \end{align*} Extending this expression to real values of \( x \) allows us to plot its shape as well. Once again, the function takes the value \( 1 \) at integer multiples of \( 5 \) and \( 0 \) at integers not divisible by \( 5. \)&lt;/p&gt;
    &lt;p&gt;Recall that we expressed \( f(n) \) as \[ f(n) = I_3(n) + 2 \, I_5(n). \] Substituting these trigonometric expressions yields \[ f(n) = \frac{1}{3} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right) + 2 \cdot \left( \frac{1}{5} + \frac{2}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{2}{5} \cos \left( \frac{4 \pi n}{5} \right) \right). \] A straightforward simplification gives \[ f(n) = \frac{11}{15} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right) + \frac{4}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{4}{5} \cos \left( \frac{4 \pi n}{5} \right). \] We can extend this expression to real \( x \) and plot it as well. The resulting curve takes the values \( 0, 1, 2 \) and \( 3 \) at integer points, as desired.&lt;/p&gt;
    &lt;p&gt;Now we can write our Python program as follows:&lt;/p&gt;
    &lt;code&gt;
 from math import cos, pi
for n in range(1, 101):
    s = [n, 'Fizz', 'Buzz', 'FizzBuzz']
    i = round(11 / 15 + (2 / 3) * cos(2 * pi * n / 3)
                      + (4 / 5) * cos(2 * pi * n / 5)
                      + (4 / 5) * cos(4 * pi * n / 5))
    print(s[i])
&lt;/code&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;To summarise, we have defined the Fizz Buzz sequence as \[ (s_{f(n)}(n))_{n = 1}^{\infty} \] where \[ f(n) = \frac{11}{15} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right) + \frac{4}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{4}{5} \cos \left( \frac{4 \pi n}{5} \right) \in \{ 0, 1, 2, 3 \} \] and \( s_0(n) = n, \) \( s_1(n) = \mathtt{Fizz}, \) \( s_2(n) = \mathtt{Buzz} \) and \( s_3(n) = \mathtt{FizzBuzz}. \) A Python program to print the Fizz Buzz sequence based on this definition was presented earlier. That program can be written more succinctly as follows:&lt;/p&gt;
    &lt;code&gt;
 from math import cos, pi
for n in range(1, 101):
    print([n, 'Fizz', 'Buzz', 'FizzBuzz'][round(11 / 15 + (2 / 3) * cos(2 * pi * n / 3) + (4 / 5) * (cos(2 * pi * n / 5) + cos(4 * pi * n / 5)))])
&lt;/code&gt;
    &lt;p&gt;We can also wrap this up nicely in a shell one-liner, in case you want to share it with your friends and family and surprise them:&lt;/p&gt;
    &lt;code&gt;python3 -c 'from math import cos, pi; [print([n, "Fizz", "Buzz", "FizzBuzz"][round(11/15 + (2/3) * cos(2*pi*n/3) + (4/5) * (cos(2*pi*n/5) + cos(4*pi*n/5)))]) for n in range(1, 101)]'&lt;/code&gt;
    &lt;p&gt;The keen-eyed might notice that the expression we have obtained for \( f(n) \) is a finite Fourier series. This is not surprising, since the output of a Fizz Buzz program depends only on \( n \bmod 15. \) Any function on a finite cyclic group can be written exactly as a finite Fourier expansion.&lt;/p&gt;
    &lt;p&gt;We have taken a simple counting game and turned it into a trigonometric construction: a finite Fourier series with a constant term \( 11/15 \) and three cosine terms with coefficients \( 2/3, \) \( 4/5 \) and \( 4/5. \) None of this makes Fizz Buzz any easier, of course, but it does show that every \( \mathtt{Fizz} \) and \( \mathtt{Buzz} \) now owes its existence to a particular set of Fourier coefficients. We began with the modest goal of making this simple problem more complicated. I think it is safe to say that we did not fall short.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://susam.net/fizz-buzz-with-cosines.html"/><published>2025-11-21T17:28:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46006616</id><title>Helping Valve to power up Steam devices</title><updated>2025-11-21T21:33:50.429798+00:00</updated><content>&lt;doc fingerprint="961b0d5348912672"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Helping Valve to Power Up Steam Devices&lt;/head&gt;
    &lt;p&gt;Last week, Valve stunned the computer gaming world by unveiling three new gaming devices at once: the Steam Frame, a wireless VR headset; the Steam Machine, a gaming console in the vein of a PlayStation or Xbox; and the Steam Controller, a handheld game controller. Successors to the highly successful Valve Index and Steam Deck, these devices are set to be released in the coming year.&lt;/p&gt;
    &lt;p&gt;Igalia has long worked with Valve on SteamOS, which will power the Machine and Frame, and is excited to be contributing to these new devices, particularly the Frame. The Frame, unlike the Machine or Deck which have x86 CPUs, runs on an ARM-based CPU.&lt;/p&gt;
    &lt;p&gt;Under normal circumstances, this would mean that only games compiled to run on ARM chips could be played on the Frame. In order to get around this barrier, a translation layer called FEX is used to run applications compiled for x86 chips (which are used in nearly all gaming PCs) on ARM chips by translating the x86 machine code into ARM64 machine code.&lt;/p&gt;
    &lt;p&gt;âIf you love video games, like I do, working on FEX with Valve is a dream come true,â said Paulo Matos, an engineer with Igaliaâs Compilers Team. Even so, the challenges can be daunting, because making sure the translation is working often requires manual QA rather than automated testing. âYou have to start a game, sometimes the error shows up in the colors or sound, or how the game behaves when you break down the door in the second level. Just debugging this can take a while,â said Matos. âFor optimization work I did early last year, I used a game called Psychonauts to test it. I must have played the first 3 to 4 minutes of the game many, many times for debugging. Looking at my history, Steam tells me I played it for 29 hours, but it was always the first few minutes, nothing else.â&lt;/p&gt;
    &lt;p&gt;Beyond the CPU, the Qualcomm Adreno 750 GPU used in the Steam Frame introduced its own set of challenges when it came to running desktop games, and other complex workloads, on these devices. Doing so requires a rock-solid Vulkan driver that can ensure correctness, eliminating major rendering bugs, while maintaining high performance. This is a very difficult combination to achieve, and yet thatâs exactly what weâve done for Valve with Mesa3D Turnip, a FOSS Vulkan driver for Qualcomm Adreno GPUs.&lt;/p&gt;
    &lt;p&gt;Before we started our work, critical optimizations such as LRZ (which you can learn more about from our blog post here) or the autotuner (and its subsequent overhaul) werenât in place. Even worse, there wasnât support for the Adreno 700-series GPUs at all, which we eventually added along with support for tiled rendering.&lt;/p&gt;
    &lt;p&gt;âWe implemented many Vulkan extensions and reviewed numerous others,â said Danylo Piliaiev, an engineer on the Graphics Team. âOver the years, we ensured that D3D11, D3D12, and OpenGL games rendered correctly through DXVK, vkd3d-proton, and Zink, investigating many rendering issues along the way. We achieved higher correctness than the proprietary driver and, in many cases, Mesa3D Turnip is faster as well.â&lt;/p&gt;
    &lt;p&gt;Weâve worked with many wonderful people from Valve, Google, and other companies to iterate on the Vulkan driver over the years in order to introduce new features, bug fixes, performance improvements, as well as debugging workflows. Some of those people decided to join Igalia later on, such as our colleague and Graphics Team developer Emma Anholt. âIâve been working on Mesa for 22 years, and itâs great to have a home now where I can keep doing that work, across hardware projects, where the organization prioritizes the work experience of its developers and empowers them within the organization.â&lt;/p&gt;
    &lt;p&gt;Valveâs support in all this cannot be understated, either. Their choice to build their devices using open software like Mesa3D Turnip and FEX means theyâre committed to working on and supporting improvements and optimizations that become available to anyone who uses the same open-source projects.&lt;/p&gt;
    &lt;p&gt;âWeâve received a lot of positive feedback about significantly improved performance and fewer rendering glitches from hobbyists who use these projects to run PC games on Android phones as a result of our work,â said Dhruv Mark Collins, another Graphics Team engineer working on Turnip. âAnd it goes both ways! Weâve caught a couple of nasty bugs because of that widespread testing, which really emphasizes why the FOSS model is beneficial for everyone involved.â&lt;/p&gt;
    &lt;p&gt;An interesting area of graphics driver development is all the compiler work that is involved. Vulkan drivers such as Mesa3D Turnip need to process shader programs sent by the application to the GPU, and these programs govern how pixels in our screens are shaded or colored with geometry, textures, and lights while playing games. Job Noorman, an engineer from our Compilers Team, made significant contributions to the compiler used by Mesa3D Turnip. He also contributed to the Mesa3D NIR shader compiler, a common part that all Mesa drivers use, including RADV (most popularly used on the Steam Deck) or V3DV (used on Raspberry Pi boards).&lt;/p&gt;
    &lt;p&gt;As is normal for Igalia, while we focused on delivering results for our customer, we also made our work as widely useful as possible. For example: âWhile our target throughout our work has been the Snapdragon 8 Gen 3 thatâs in the Frame, much of our work extends back through years of Snapdragon hardware, and we regression test it to make sure it stays Vulkan conformant,â said Anholt. This means that Igaliaâs work for the Frame has consistently passed Vulkanâs Conformance Test Suite (CTS) of over 2.8 million tests, some of which Igalia is involved in creating.&lt;/p&gt;
    &lt;p&gt;Our very own Vulkan CTS expert Ricardo GarcÃa says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Igalia and other Valve contractors actively participate in several areas inside the Khronos Group, the organization maintaining and developing graphics API standards like Vulkan. We contribute specification fixes and feedback, and we are regularly involved in the development of many new Vulkan extensions. Some of these end up being critical for game developers, like mesh shading. Others ensure a smooth and efficient translation of other APIs like DirectX to Vulkan, or help take advantage of hardware features to ensure applications perform great across multiple platforms, both mobile like the Steam Frame or desktop like the Steam Machine. Having Vulkan CTS coverage for these new extensions is a critical step in the release process, helping make sure the specification is clear and drivers implement it correctly, and Igalia engineers have contributed millions of source code lines and tests since our collaboration with Valve started.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A huge challenge we faced in moving forward with development is ensuring that we didnât introduce regressions, small innocent-seeming changes can completely break rendering on games in a way that even CTS might not catch. What automated testing could be done was often quite constrained, but Igalians found ways to push through the barriers. âI made a continuous integration test to automatically run single-frame captures of a wide range of games spanning D3D11, D3D9, D3D8, Vulkan, and OpenGL APIs,â said Piliaiev, about the development covered in his recent XDC 2025 talk, âensuring that we donât have rendering or performance regressions.â&lt;/p&gt;
    &lt;p&gt;Looking ahead, Igaliaâs work for Valve will continue to deliver benefits to the wider Linux Gaming ecosystem. For example, the Steam Frame, as a battery-powered VR headset, needs to deliver high performance within a limited power budget. A way to address this is to create a more efficient task scheduler, which is something Changwoo Min of Igaliaâs Kernel Team has been working on. As he says, âI have been developing a customized CPU scheduler for gaming, named LAVD: Latency-criticality Aware Virtual Deadline scheduler.â&lt;/p&gt;
    &lt;p&gt;In general terms, a scheduler automatically identifies critical tasks and dynamically boosts their deadlines to improve responsiveness. Most task schedulers donât take energy consumption into account, but the Rust-based LAVD is different. âLAVD makes scheduling decisions considering each chipâs performance versus energy trade-offs. It measures and predicts the required computing power on the fly, then selects the best set of CPUs to meet that demand with minimal energy consumption,â said Min.&lt;/p&gt;
    &lt;p&gt;One of our other kernel engineers, Melissa Wen, has been working on AMD kernel display drivers to maintain good color management and HDR support for SteamOS across AMD hardware families, both for the Steam Deck and the Steam Machine. This is especially important with newer display hardware in the Steam Machine, which features some notable differences in color capabilities, aiming for more powerful and efficient color management which necessitated driver work.&lt;/p&gt;
    &lt;p&gt;â¦and thatâs a wrap! We will continue our efforts toward improving future versions of SteamOS, and with a partner as strongly supportive as Valve, we expect to do more work to make Linux gaming even better. If any of that sounded interesting and youâd like to work with us to tackle a tricky problems of your own, please get in touch!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.igalia.com/2025/11/helpingvalve.html"/><published>2025-11-21T17:29:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46007267</id><title>How/why to sweep async tasks under a Postgres table</title><updated>2025-11-21T21:33:49.920080+00:00</updated><content>&lt;doc fingerprint="7f56acd72f49a4cf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How/Why to Sweep Async Tasks Under a Postgres Table&lt;/head&gt;
    &lt;p&gt;I like slim and stupid servers, where each endpoint wraps a very dumb DB query.&lt;/p&gt;
    &lt;p&gt;Dumb queries are fast. Fast queries make websites smooth and snappy. Keep those click/render loops sacred.&lt;/p&gt;
    &lt;p&gt;Sweep complexity under a &lt;code&gt;task&lt;/code&gt; table:&lt;/p&gt;
    &lt;code&gt;router.post("/signup", async ctx =&amp;gt; {
  const { email, password } = await ctx.request.body().value;
  const [{ usr_id } = { usr_id: null }] = await sql`
    with usr_ as (
      insert into usr (email, password)
      values (${email}, crypt(${password}, gen_salt('bf')))
      returning *
    ), task_ as (
      insert into task (task_type, params)
      values ('SEND_EMAIL_WELCOME', ${sql({ usr_id })})
    )
    select * from usr_
  `;
  await ctx.cookies.set("usr_id", usr_id);
  ctx.response.status = 204;
});&lt;/code&gt;
    &lt;p&gt;This example uses CTEs with postgres.js.&lt;/p&gt;
    &lt;p&gt;Of course using &lt;code&gt;mailgun.send&lt;/code&gt; is easier than queuing it in a &lt;code&gt;task&lt;/code&gt; table.
Adding indirection rarely makes systems less complex. But somehow I'm here to
advocate exactly that. You may ignore my manifesto and
skip to my implementation at the end.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Secret Surface Error Area&lt;/item&gt;
      &lt;item&gt;Never Handroll Your Own Two-Phase Commit&lt;/item&gt;
      &lt;item&gt;One Way To Do Things&lt;/item&gt;
      &lt;item&gt;TODO-Driven Development&lt;/item&gt;
      &lt;item&gt;Human Fault Tolerance&lt;/item&gt;
      &lt;item&gt;Show Me The Code&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Secret Surface Error Area&lt;/head&gt;
    &lt;p&gt;Customers don't care about cosmic rays. They want a thing. More imporantly, they want immediate confirmation of their thing. They want to offload the mental burden of their goal.&lt;/p&gt;
    &lt;p&gt;For them to delegate that responsibility, your DB is probably the only thing that matters. Once information is committed to your database, you can confidently say "we'll take it from here".&lt;/p&gt;
    &lt;p&gt;You can send emails later. You can process payments later. You can do almost anything later. Just tell your customer they can continue with their goddamn day.&lt;/p&gt;
    &lt;p&gt;Delight your customers with clear feedback.&lt;/p&gt;
    &lt;p&gt;Delight your computers by writing to one place at a time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Never Handroll Your Own Two-Phase Commit&lt;/head&gt;
    &lt;p&gt;Writing to two places at "the same time" is sinful.&lt;/p&gt;
    &lt;p&gt;When the gods gave us computer storage, the people became unhappy. They cried, "What is consistency? Where are our guarantees? Why must I &lt;code&gt;fsync&lt;/code&gt;?" And so they
wore sackloth and ashes for many years in their coding caves.&lt;/p&gt;
    &lt;p&gt;The people were overjoyed when the gods scrawled Postgres (and other inferior databases) onto stone tablets. The holy "database transactions" allowed humankind to pretend that they could read/write to multiple places at the same time.&lt;/p&gt;
    &lt;p&gt;To this day, databases sometimes work.&lt;/p&gt;
    &lt;p&gt;But some developers deny the works of the gods. They mix multiple tools, and so commit the sin of writing to multiple places.&lt;/p&gt;
    &lt;p&gt;"Oh, we'll just send a pubsub message after we insert the row." But data is lost. Message before insert row? Data lost. All blasphemers are doomed to reinvent two-phase commit.&lt;/p&gt;
    &lt;head rend="h2"&gt;One Way To Do Things&lt;/head&gt;
    &lt;p&gt;I like LEGO. I like Play-Doh. I like Lincoln Logs. I do not, however, like mixing them together.&lt;/p&gt;
    &lt;p&gt;It's painful to investigate systems when state is spread across SQS, Redis, PubSub, Celery, Airflow, etc. I shouldn't have to open a local detective agency find out why a process isn't running as expected.&lt;/p&gt;
    &lt;p&gt;Most modern projects use SQL. Because I dislike mixing systems, I try to take SQL as far as possible.&lt;/p&gt;
    &lt;p&gt;Of all the SQL databases, Postgres currently offers the best mix of modern first-class features and third-party extensions. Postgres can be your knock-off Kafka, artificial Airflow, crappy Clickhouse, nasty Elasticsearch, poor man's PubSub, on-sale Celery, etc.&lt;/p&gt;
    &lt;p&gt;Sure, Postgres doesn't have all the fancy features of each specialized system. But colocating queue/pipeline/async data in your main database eliminates swaths of errors. In my experience, transaction guarantees supercede everything else.&lt;/p&gt;
    &lt;head rend="h2"&gt;TODO-Driven Development&lt;/head&gt;
    &lt;code&gt;while (true) {
  // const rows = await ...
  for (const { task_type, params } of rows)
    if (task_type in tasks) {
      await tasks[task_type](tx, params);
    } else {
      console.error(`Task type not implemented: ${task_type}`);
    }
}&lt;/code&gt;
    &lt;p&gt;With a simple retry system, asynchronous decoupling magically tracks all your incomplete flows.&lt;/p&gt;
    &lt;p&gt;No need to rely upon Jira -- bugs and unimplemented tasks will be logged and retried. Working recursively from error queues is truly a wonderful experience. All your live/urgent TODOs are printed to the same place (in development and in production).&lt;/p&gt;
    &lt;p&gt;With this paradigm, you'll gravitate towards scalable pipelines. Wishful thinking makes natural architecture.&lt;/p&gt;
    &lt;head rend="h2"&gt;Human Fault Tolerance&lt;/head&gt;
    &lt;p&gt;Many systems foist useless retry-loops onto humans.&lt;/p&gt;
    &lt;p&gt;Humans should receive feedback for human errors. But humans should not receive feedback for problems that can be handled by computers (and their software developers).&lt;/p&gt;
    &lt;p&gt;Remember, all your retry-loops have to happen somewhere. Be careful what you delegate to customers and developers. Your business's bottom-line is bounded by human patience; computers have infinitely more patience than humans.&lt;/p&gt;
    &lt;head rend="h2"&gt;Show Me The Code&lt;/head&gt;
    &lt;p&gt;Here's the &lt;code&gt;task&lt;/code&gt; table:&lt;/p&gt;
    &lt;code&gt;create table task
( task_id bigint primary key not null generated always as identity
, task_type text not null -- consider using enum
, params jsonb not null -- hstore also viable
, created_at timestamptz not null default now()
, unique (task_type, params) -- optional, for pseudo-idempotency
)&lt;/code&gt;
    &lt;p&gt;Don't use serial in Postgres.&lt;/p&gt;
    &lt;p&gt;Here's the code for the task worker:&lt;/p&gt;
    &lt;code&gt;const tasks = {
  SEND_EMAIL_WELCOME: async (tx, params) =&amp;gt; {
    const { email } = params;
    if (!email) throw new Error(`Bad params ${JSON.stringify(params)}.`);
    await sendEmail({ email, body: "WELCOME" });
  },
};

(async () =&amp;gt; {
  while (true) {
    try {
      while (true) {
        await sql.begin(async (tx: any) =&amp;gt; {
          const rows = await tx`
            delete from task
            where task_id in
            ( select task_id
              from task
              order by random() -- use tablesample for better performance
              for update
              skip locked
              limit 1
            )
            returning task_id, task_type, params::jsonb as params
          `;
          for (const { task_type, params } of rows)
            if (task_type in tasks) {
              await tasks[task_type](tx, params);
            } else {
              throw new Error(`Task type not implemented: ${task_type}`);
            }
          if (rows.length &amp;lt;= 0) {
            await delay(10 * 1000);
          }
        });
      }
    } catch (err) {
      console.error(err);
      await delay(1 * 1000);
    }
  }
})();&lt;/code&gt;
    &lt;p&gt;A few notable features of this snippet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The task row will not be deleted if &lt;code&gt;sendEmail&lt;/code&gt;fails. The PG transaction will be rolled back. The row and&lt;code&gt;sendEmail&lt;/code&gt;will be retried.&lt;/item&gt;
      &lt;item&gt;The PG transaction &lt;code&gt;tx&lt;/code&gt;is passed along to tasks. This is convenient for marking rows as "processed", etc.&lt;/item&gt;
      &lt;item&gt;Transactions make error-handling so much nicer. Always organize reversible queries before irreversible side-effects (e.g. mark DB status before sending the email). Remember that the DB commits at the end.&lt;/item&gt;
      &lt;item&gt;Because of &lt;code&gt;skip locked&lt;/code&gt;, you can run any number of these workers in parallel. They will not step on each others' toes.&lt;/item&gt;
      &lt;item&gt;Random ordering is technically optional, but it makes the system more resilient to errors. With adequate randomness, a single task type cannot block the queue for all.&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;order by (case task_type ... end), random()&lt;/code&gt;to create an easy prioritized queue.&lt;/item&gt;
      &lt;item&gt;Limiting number of retries makes the code more complicated, but definitely worth it for user-facing side-effects like emails.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;if (rows.length &amp;lt;= 0)&lt;/code&gt;prevents overzealous polling. Your DBA will be grateful.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://taylor.town/pg-task"/><published>2025-11-21T18:28:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46007294</id><title>Brazil charges 31 people in major carbon credit fraud investigation</title><updated>2025-11-21T21:33:49.533187+00:00</updated><content>&lt;doc fingerprint="dbbdc249dc972582"&gt;
  &lt;main&gt;
    &lt;p&gt;Brazil’s Federal Police have indicted 31 suspects for fraud and land-grabbing in a massive criminal carbon credit scheme in the Brazilian Amazon, according to Brazilian national media outlet Folha de S.Paulo. It is the largest known criminal operation involving carbon credit fraud to date in the nation.&lt;/p&gt;
    &lt;p&gt;The police probe, called Operation Greenwashing, was launched following an investigation by Mongabay reporter Fernanda Wenzel published in May 2024 about two REDD+ carbon credit projects that appeared to be linked to illegal timber laundering.&lt;/p&gt;
    &lt;p&gt;The Netherlands-based Center for Climate Crime Analysis (CCCA) analyzed the REDD+ projects, called Unitor and Fortaleza Ituxi, at Mongabay’s request, finding a mismatch between their declared volume of logged timber and the logged volume estimated through satellite images, suggesting possible timber laundering.&lt;/p&gt;
    &lt;p&gt;The police investigation confirmed that two REDD+ project areas were generating carbon credits at the same time they were being used to launder timber taken from other illegally deforested areas.&lt;/p&gt;
    &lt;p&gt;Both projects, which cover more than 140,000 hectares (around 350,000 acres), are located in the municipality of Lábrea in the south of Amazonas state. The area has been identified as one of the newest and most aggressive deforestation frontiers in the Brazilian Amazon.&lt;/p&gt;
    &lt;p&gt;The Federal Police told Folha that three interconnected groups were involved.&lt;/p&gt;
    &lt;p&gt;One group was led by Ricardo Stoppe Júnior, known as Brazil’s largest individual seller of carbon credits. He has actively participated in climate talks and public events promoting his business model, including during the COP28 climate summit hosted in the United Arab Emirates.&lt;/p&gt;
    &lt;p&gt;Stoppe has sold millions of dollars in carbon credits to corporations including Nestlé, Toshiba, Spotify, Boeing and PwC.&lt;/p&gt;
    &lt;p&gt;The other two were led by Élcio Aparecido Moço and José Luiz Capelasso.&lt;/p&gt;
    &lt;p&gt;Moço shares a business conglomerate consisting of seven companies with Stoppe’s son, Ricardo Villares Lot Stoppe. In 2017, Moço had been sentenced for timber laundering, but in 2019, another court overruled his sentencing. In 2019, he was also indicted for allegedly bribing two public officials.&lt;/p&gt;
    &lt;p&gt;Capelasso was sentenced for illegally trading certificates of origin for forest products in 2012 but was subsequently released. At the time, the police alleged that Capelasso was charging 3,000 reais (approximately $1,500 in 2012) for each fake document.&lt;/p&gt;
    &lt;p&gt;According to Operation Greenwashing, the scheme was made possible by corrupt public servants working in Brazil’s land reform agency, Incra, in registrar offices across Amazonas state, as well as the Amazonas state environmental protection institute, Ipaam.&lt;/p&gt;
    &lt;p&gt;Folha de S.Paulo did not get a response from any of the legal defence teams of the accused. Both Ipaam and Incra stated they supported and are collaborating with the police investigation.&lt;/p&gt;
    &lt;p&gt;Banner image: Logging in the Brazilian Amazon. Image © Bruno Kelly/Greenpeace.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.mongabay.com/short-article/2025/11/brazil-charges-31-people-in-major-carbon-credit-fraud-investigation/"/><published>2025-11-21T18:30:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46008628</id><title>We remember the internet bubble. This mania looks and feels the same</title><updated>2025-11-21T21:33:49.307260+00:00</updated><content>&lt;doc fingerprint="3f148319dd3605fc"&gt;
  &lt;main&gt;
    &lt;p&gt;The artificial intelligence revolution will be only three years old at the end of November. Think about that for a moment. In just 36 months AI has gone from great-new-toy, to global phenomenon, to where we are today – debating whether we are in one of the biggest technology bubbles or booms in modern times.&lt;/p&gt;
    &lt;p&gt;To us what’s happening is obvious. We both covered the internet bubble 25 years ago. We’ve been writing about – and in Om’s case investing in – technology since then. We can both say unequivocally that the conversations we are having now about the future of AI feel exactly like the conversations we had about the future of the internet in 1999.&lt;/p&gt;
    &lt;p&gt;We’re not only in a bubble but one that is arguably the biggest technology mania any of us have ever witnessed. We’re even back reinventing time. Back in 1999 we talked about internet time, where every year in the new economy was like a dog year – equivalent to seven years in the old.&lt;/p&gt;
    &lt;p&gt;Now VCs, investors and executives are talking about AI dog years – let’s just call them mouse years – which is internet time divided by five? Or is it by 11? Or 12? Sure, things move way faster than they did a generation ago. But by that math one year today now equals 35 years in 1995. Really?&lt;/p&gt;
    &lt;p&gt;We’re also months, not years, from the end of the party. We may be even closer than that. NVIDIA posted better than expected earnings on Wednesday. And it briefly looked like that would buoy all AI stocks. It didn’t.&lt;/p&gt;
    &lt;p&gt;All but Alphabet have seen big share declines in the past month. Microsoft is down 12 percent, Amazon is down 14 percent, Meta is down 22 percent, Oracle is down 24 percent, and Corweave’s stock has been almost cut in half, down 47 percent. Investors are increasingly worried that everyone is overspending on AI.&lt;/p&gt;
    &lt;p&gt;All this means two things to us: 1)The AI revolution will indeed be one of the biggest technology shifts in history. It will spark a generation of innovations that we can’t yet even imagine. 2) It’s going to take way longer to see those changes than we think it’s going to take right now.&lt;/p&gt;
    &lt;p&gt;Why? Because we humans are pretty good at predicting the impact of technology revolutions beyond seven to ten years. But we’re terrible at it inside that time period. We’re too prone to connect a handful of early data points, to assume that’s the permanent slope of that line and to therefore invest too much too soon. That’s what’s going on right now.&lt;/p&gt;
    &lt;p&gt;Not only does the AI bubble in 2025 feel like the internet bubble in 1999, the data suggests it may actually be larger. The latest estimates for just global AI capital expenditures plus global venture capital investments already exceed $600 billion for this year. And in September Gartner published estimates that suggested all AI-related spending worldwide in 2025 might top $1.5 trillion.&lt;/p&gt;
    &lt;p&gt;I had ChatGPT (of course) find sources and crunch some numbers for the size of the internet bubble in 1999 and came up with about $360 billion in 2025 dollars, $185 billion in 1999 dollars.&lt;/p&gt;
    &lt;p&gt;The spending is also happening in a fraction of the time. The internet bubble took 4.6 years to inflate before it burst. The AI bubble has inflated in two-thirds the time. If the AI bubble manages to actually last as long as the internet bubble – another 20 months – just spending on AI capital expenses by the big tech companies is projected to hit $750 billion annually by the end of 2027, 75 percent more than now.&lt;/p&gt;
    &lt;p&gt;That means total AI spending for 2029 would be well over $1 trillion. One of the things both of us have learned in our careers is that when numbers are so large they don’t make sense, they usually don’t make sense.&lt;/p&gt;
    &lt;p&gt;Sure, there are important differences between the internet bubble and the AI bubble. History rhymes. It doesn’t repeat. A lot of the early money to build AI data centers and train LLMs has been coming out of the giant bank accounts of the big tech companies. The rest has been largely financed by private professional investors.&lt;/p&gt;
    &lt;p&gt;During the internet bubble, public market investors, especially individuals, threw billions at tiny profitless companies betting they’d develop a business before the money ran out. And dozens of telecom startups borrowed hundreds of billions to string fiber optic cables across oceans and continents betting that exploding internet usage would justify that investment.&lt;/p&gt;
    &lt;p&gt;Neither bet happened fast enough for investors and lenders. Most of the dot coms were liquidated. Most of the telecom companies declared bankruptcy and were sold for pennies on the dollar.&lt;/p&gt;
    &lt;p&gt;But does that make the AI bubble less scary than the internet bubble? Not to us. It actually might be scarier. The amounts at risk are greater, and the exposure is way more concentrated. Microsoft, Alphabet, Meta, Amazon, NVIDIA, Oracle and Apple together represent roughly a third of the value of the critical S&amp;amp;P 500 stock market index. More importantly, over the last six months the spending has become increasingly leveraged and nonsensical.&lt;/p&gt;
    &lt;p&gt;None of these companies has proven yet that AI is a good enough business to justify all this spending. But the first four are now each spending $70 billion to $100 billion a year to fund data centers and other capital intensive AI expenses. Oracle is spending roughly $20 billion a year.&lt;/p&gt;
    &lt;p&gt;If the demand curve shifts for any or all of these companies, and a few of them have to take, say a $25 billion write down on their data center investments, that’s an enormous amount of money even for these giants.&lt;/p&gt;
    &lt;p&gt;And when you add in companies like OpenAI, AMD and CoreWeave plus the slew of other LLM and data center builders, their fortunes look incredibly intertwined. If investors get spooked about future returns from any one of those companies, the contagion could spread quickly.&lt;/p&gt;
    &lt;p&gt;Yes, by one measure AI stocks aren’t over valued at all. Cisco’s P/E peaked at 200 during the internet bubble. NVIDIA’s P/E is about 45. The P/E of the NASDAQ-100 is about 35 now. It was 73 at the end of 1999. But looking at the S&amp;amp;P 500 tells a scarier story. Excluding the years around Covid-19, the last time the P/E ratio of that index was as high as it is now – about 24 – was right before the internet bubble popped in March 2000.&lt;/p&gt;
    &lt;p&gt;Here are two other worrisome differences between then and now:&lt;/p&gt;
    &lt;p&gt;1) The overall US economic, social and political situation is much more unstable than it was 25 years ago. Back then the US was still basking in the glow of having won the Cold War. It had the most dominant economy and stable political standing in the world. Today economic growth is slower, the national debt and government spending have never been higher, and the nation is more politically divided than it has been in two generations.&lt;/p&gt;
    &lt;p&gt;2)The AI revolution is becoming a major national security issue. That ties valuations to the current unpredictability of US foreign policy and tariffs. China has become as formidable a competitor to the US in AI as the Soviet Union was to the US in the 1950s and 1960s. It doesn’t require much imagination to think about what might happen to the US AI market should China come up with a technical advance that had more staying power than the DeepSeek scare at the beginning of this year.&lt;/p&gt;
    &lt;p&gt;Even OpenAI’s Sam Altman, Amazon’s Jeff Bezos, JP Morgan’s Jamie Dimon, and just this week, Alphabet’s Sundar Pichai are now acknowledging they are seeing signs of business excess. Pichai said the following to the BBC on Tuesday: “Given the potential for this technology (AI), the excitement is very rational. It is also true that when we go through these investment cycles there are moments where we overshoot …. We can look back at the internet right now. There was clearly a lot of excess investment. But none of us would question whether the internet was profound …. It fundamentally changed how we work as a society. I expect AI to be the same.”&lt;/p&gt;
    &lt;p&gt;When will the mania end? There’s hundreds of billions of dollars of guaranteed but unspent capital in the system, which suggests it will go on well into 2026. But in times like these a secular investor sentiment change can happen in a matter of weeks, driving down stock prices, driving up the cost of capital, and making every financial model that had said “let’s invest” to one saying “not on your life.”&lt;/p&gt;
    &lt;p&gt;A technology change with more staying power than DeepSeek would certainly do it. So would President Trump changing his mind about greasing the approval process for new AI data centers. All it would take would be an off hand remark from a Silicon Valley titan he didn’t like.&lt;/p&gt;
    &lt;p&gt;Or what’s already happening with AI stocks could snowball. Investors have hammered those stocks because they’ve gotten jumpy about the size of their AI spending and in Oracle and Coreweave’s case, the leverage they are using to pay for it all. NVIDIA’s better than expected earnings announced Wednesday might ultimately calm things. But don’t expect any of these issues to go away.&lt;/p&gt;
    &lt;p&gt;If you want to go further, what we’ve done is lay out the four big vulnerabilities we’re worried about with separate headings. And, of course, if you have an entirely different set of numbers that you think shows we’re nowhere near bubble territory, have suggestions about how to refine ours, or think we left something out, please share.&lt;/p&gt;
    &lt;p&gt;To us the four big vulnerabilities are:&lt;/p&gt;
    &lt;p&gt;Too much spending.&lt;/p&gt;
    &lt;p&gt;Too much leverage.&lt;/p&gt;
    &lt;p&gt;Crazy deals.&lt;/p&gt;
    &lt;p&gt;China. China. China.&lt;/p&gt;
    &lt;p&gt;*****&lt;/p&gt;
    &lt;p&gt;Too much spending:&lt;/p&gt;
    &lt;p&gt;We all know two things about the AI bubble right now: 1)People, companies and researchers will pay for AI. 2)They aren’t paying nearly enough to justify the hundreds of billions of dollars that has been committed to it yet.&lt;/p&gt;
    &lt;p&gt;The thinking, of course, is that that gap will quickly disappear and be replaced with enough paid usage to generate enormous profits. The questions that no one has the answer to are: When will that happen? How much more money will it take? And which approach to making money will work the best?&lt;/p&gt;
    &lt;p&gt;Will it work better just to charge for AI based on usage the way Microsoft, Oracle, Amazon, and OpenAI are focused on? Will it be more of an indirect revenue driver the way Meta is approaching it with its open source models? Will it have an advertising component the way Alphabet is exploring?&lt;/p&gt;
    &lt;p&gt;Or will it be a do-everything, vertically integrated approach that works best? Amazon and Meta are exploring this. But Alphabet is the furthest ahead. It not only has its own AI software but is also using a lot of its own graphics processing chips known as Tensor Processing Units. This gives it much more control over processing costs than competitors who are – at least for the moment – entirely dependent on NVIDIA and AMD graphics processing chips.&lt;/p&gt;
    &lt;p&gt;The only thing everyone agrees on is that the stakes are enormous: Digital technology revolutions historically have been winner-take-all-affairs whether in mainframes, minicomputers, personal computers, chips, software, search, or smartphones. That means there are likely to be only a couple of dominant AI providers five years from now.&lt;/p&gt;
    &lt;p&gt;Maybe they’ll only be one, if one of them manages to truly get their system to reach artificial general intelligence. What it certainly means, however, is that, as in the past, there will be way more losers than winners, and there will be many big companies with giant holes in their balance sheets.&lt;/p&gt;
    &lt;p&gt;OpenAI has become exhibit A in this spending frenzy partly because it’s the leading AI chatbot and helped ignite the AI revolution with ChaptGPT version 3 in November 2022.&lt;/p&gt;
    &lt;p&gt;It’s also because, frankly, it’s hard to look away from the company’s financial highwire act. Its competitors have other businesses they can fall back on. OpenAI must make its bet on AI work, or it becomes one of the biggest meltdowns in the history of business.&lt;/p&gt;
    &lt;p&gt;This is a company that hasn’t come close to making a profit or even being cash flow positive, but investors last valued it at $500 billion. That would rank it as the 21st most valuable company in the stock market, with BankAmerica. And at the end of October it made changes to its corporate structure that would allow it to have a traditional IPO in a year or two. There was speculation that that could value the company at $1 trillion.&lt;/p&gt;
    &lt;p&gt;In the past three years OpenAI has raised more than $55 billion, according to published reports. And while its revenues for 2025 seem to be on track to hit $12 billion, the company is burning through cash quickly.&lt;/p&gt;
    &lt;p&gt;Its cash burn this year is expected to top $8 billion and top $17 billion in 2026. It says it expects to spend nearly half a trillion dollars on server rentals over the next five years, and says it doesn’t expect to be generating more cash from operations than it is spending until 2029. That’s when it expects revenues to top $100 million. It agreed to pay nearly $7 billion for former Apple design chief Jonny Ive’s startup IO, in May.&lt;/p&gt;
    &lt;p&gt;“Eventually we need to get to hundreds of billions of a year in revenue,” CEO Sam Altman said in response to a question about OpenAIs finances at the end of October. “I expect enterprise to be a huge revenue driver for us, but I think consumer really will be too. And it won’t just be this (ChatGPT) subscription, but we’ll have new products, devices and tons of other things. And this says nothing about what it would really mean to have AI discovering science and all of those revenue possibilities.”&lt;/p&gt;
    &lt;p&gt;We’ve seen this movie before, of course. Whether we’re looking at the railroad construction bubble in the US 150 years ago or the internet bubble 25 years ago, investors touting the wisdom of “get big fast” have often been endemic to technology revolutions.&lt;/p&gt;
    &lt;p&gt;It’s what made Amazon the OpenAI of the internet bubble. “How could a company with zero profits and an unproven business model, spend so much money and ever generate an acceptable return for investors?” we asked&lt;/p&gt;
    &lt;p&gt;And most of the criticism about Amazon, the online retailer, actually turned out to be true. Yes, Amazon is now one of the most successful companies in the world. But that only happened because of something Amazon discovered ten years after its founding in 1994 – Amazon Web Services, its hugely profitable cloud computing business.&lt;/p&gt;
    &lt;p&gt;Like many predicted, the margins in online retailing were not meaningfully different from the single digit margins in traditional retailing. That meant that Amazon wasn’t a profitable enough business to justify all that spending. If you had invested in Amazon at the peak of the internet bubble, you would have waited another decade before your investment would have started generating returns.&lt;/p&gt;
    &lt;p&gt;And here’s the thing that makes eyes bulge: OpenAI’s expected spend, just based on the money it’s raised so far, is set up to be 16 times what Amazon spent during its first five years even when adjusting that number into 2025 dollars.&lt;/p&gt;
    &lt;p&gt;It’s not just the size of the investments and the lack of a business model yet to justify them, that concerns analysts and investors like Mary Meeker at Bond Capital. It’s that the prices that AI providers can charge are also falling. “For model providers this raises real questions about monetization and profits,” she said in a 350 page report on the future of AI at the end of May. “Training is expensive, serving is getting cheap, and pricing power is slipping. The business model is in flux. And there are new questions about the one-size-fits-all LLM approach, with smaller, cheaper models trained for custom use cases now emerging.&lt;/p&gt;
    &lt;p&gt;“Will providers try to build horizontal platforms? Will they dive into specialized applications? Will one or two leaders drive dominant user and usage share and related monetization, be it subscriptions (easily enabled by digital payment providers), digital services, ads, etc.? Only time will tell. In the short term, it’s hard to ignore that the economics of general-purpose LLMs look like commodity businesses with venture-scale burn.”&lt;/p&gt;
    &lt;p&gt;*****&lt;/p&gt;
    &lt;p&gt;Too much leverage:&lt;/p&gt;
    &lt;p&gt;Bloomberg, Barron’s, The New York Times and the Financial Times have all published graphics in the past month to help investors visualize the slew of hard to parse, seemingly circular, vendor financing deals involving the biggest players in AI. They make your head hurt. And that’s a big part of the problem.&lt;/p&gt;
    &lt;p&gt;What’s clear is that NVIDIA and OpenAI have begun acting like banks and VC investors to the tune of hundreds of billions of dollars to keep the AI ecosystem lubricated. What’s unclear is who owes what to whom under what conditions.&lt;/p&gt;
    &lt;p&gt;NVIDIA wants to guarantee ample demand for its graphics processing units. So it has participated in 52 different venture investment deals for AI companies in 2024 and had already done 50 deals by the end of September this year, according to data from PitchBook. That includes participating in six deals that raised more than $1 billion,&lt;/p&gt;
    &lt;p&gt;It’s these big deals that have attracted particular attention. NVIDIA is investing as much as $100 billion in OpenAI, another $2 billion in Elon Musk’s xAI, agreed to take a 7 percent stake in CoreWeave’s IPO and, because it rents access to NVIDIA chips, buy $6.3 billion in cloud service from them. The latest deal came earlier this week. NVIDIA and Microsoft said that together they would invest up to $15 billion in Anthropic in exchange for Anthropic buying $30 billion in computiong capaicty from Microsoft running NVIDIA AI systems.&lt;/p&gt;
    &lt;p&gt;OpenAI, meanwhile, has become data center builders and suppliers best friend. It needs to ensure it has unfettered access not only to GPUs, but data centers to run them. So it has committed to filling its data centers with NVIDIA and AMD chips, and inked a $300 billion deal with Oracle and a $22.4 billion deal with CoreWeave for cloud and data center construction and management. OpenAI received $350 million in CoreWeave equity ahead of its IPO in return. It also became AMDs largest shareholder.&lt;/p&gt;
    &lt;p&gt;These deals aren’t technically classified as vendor financing – where a chip/server maker or cloud provider lends money to or invests in a customer to ensure they have the money to keep buying their products. But they sure look like them.&lt;/p&gt;
    &lt;p&gt;Yes, vendor financing is as old as Silicon Valley. But these deals add leverage to the system. If too many customers run into financial trouble, the impact on lenders and investors is exponentially severe. Not only do vendors experience cratering demand for future sales, they have to write down a slew of loans and/or investments on top of that.&lt;/p&gt;
    &lt;p&gt;Lucent Technologies was a huge player in the vendor financing game during the internet bubble, helping all the new telecom companies finance their telecom equipment purchases to the tune of billions of dollars. But when those telecom companies failed, Lucent never recovered.&lt;/p&gt;
    &lt;p&gt;The other problem with leverage is that once it starts, it’s like a drug. You see competitors borrowing money to build data centers and you feel pressure to do the same thing Oracle and Coreweave have already gone deeply in debt to keep up. Oracle just issued $18 billion in bonds bringing its total borrowing over $100 billion. It’s expected to ask investors for another $38 billion soon. Analysts expect it to double that borrowing in the next few years.&lt;/p&gt;
    &lt;p&gt;And Coreweave, the former crypto miner turned data center service provider, unveiled in its IPO documents earlier this year that it has borrowed so much money that its debt payments represent 25 percent of its revenues. Shares of both these companies have taken a beating in the past few weeks as investors have grown increasingly worried about their debt load.&lt;/p&gt;
    &lt;p&gt;The borrowing isn’t limited to those who have few other options. Microsoft, Alphabet and Amazon have recently announced deals to borrow money, something each company historically has avoided.&lt;/p&gt;
    &lt;p&gt;And it’s not just leverage in the AI markets that have begun to worry lenders, investors and executives. Leverage is building in the $2 trillion private credit market. Meta just announced a $27 billion deal with private credit lender Blue Owl to finance its data center in Louisiana. It’s the largest private credit deal ever. By owning only 20 percent of the joint venture known as Hyperion, Meta gets most of the risk off its balance sheet, but maintains full access to the processing power of the data center when it’s complete.&lt;/p&gt;
    &lt;p&gt;Private credit has largely replaced middle market bank lending since the financial crisis. The new post crisis regulations banks needed to meet to make many of those loans proved too onerous. And since the world of finance abhors a vacuum, hedge funds and other big investors jumped in.&lt;/p&gt;
    &lt;p&gt;Banks soon discovered they could replace that business just by lending to the private credit lenders. What makes these loans so attractive is exactly what makes them dangerous in booming markets: Private credit lenders don’t have the same capital requirements or transparency requirements that banks have.&lt;/p&gt;
    &lt;p&gt;And two private credit bankruptcies in the last two months – Tricolor Holdings and First Brands – have executives and analysts wondering if underwriting rules have gotten too lax.&lt;/p&gt;
    &lt;p&gt;“My antenna goes up when things like that happen,” JP Morgan CEO Jamie Dimon told investors. “And I probably shouldn’t say this, but when you see one cockroach, there are probably more. And so we should—everyone should be forewarned on this one…. I expect it to be a little bit worse than other people expect it to be, because we don’t know all the underwriting standards that all of these people did.”&lt;/p&gt;
    &lt;p&gt;*****&lt;/p&gt;
    &lt;p&gt;Crazy deals:&lt;/p&gt;
    &lt;p&gt;Even if you weren’t even alive during the internet bubble, you’ve likely heard of Webvan if you pay any attention to business. Why? Because of all the questionable deals that emerged from that period, it seemed to be the craziest. The company bet it could be the first and only company to tackle grocery home delivery nationwide, and that it could offer customers delivery within a 30 minute window of their choosing. Logistics like this is one of the most difficult business operations to get right. Webvan’s management said the internet changed all those rules. And investors believed them.&lt;/p&gt;
    &lt;p&gt;It raised $400 million from top VCs and another $375 million in an IPO totaling $1.5 billion in today’s dollars and a valuation in today’s dollars of nearly $10 billion. Five years after starting and a mere 18 months after its IPO, it was gone. Benchmark, Sequoia, Softbank, Goldman Sachs, Yahoo, and Etrade all signed up for this craziness and lost their shirts.&lt;/p&gt;
    &lt;p&gt;Is Mira Murati’s Thinking Machines the next Webvan? It’s certainly too soon to answer that question. But it’s certainly not too soon to ask. Webvan took four years to raise $1.5 billion in 2025 dollars. Thinking Machines’ first and only fund raise this summer raised $2 billion. Ten top VCs piled in valuing the company at $10 billion. Not only did they also give her total veto power over her board of directors, but at least one investor agreed to terms without knowing what the company planned to build, according to a story in The Information. “It was the most absurd pitch meeting,” one investor who met with Murati said. “She was like, ‘So we’re doing an AI company with the best AI people, but we can’t answer any questions.’”&lt;/p&gt;
    &lt;p&gt;Yes, Murati is one of AIs pioneers, unlike Webvan CEO George Shaneen, who had no experience in logistics or online shopping. Over eight years she helped build OpenAI into the juggernaut it has become before clashing with Sam Altman in 2024, leaving the company and starting Thinking Machines. And yes, Thinking Machines has finally announced some of what it is working on. It’s a tool called Tinker that will automate the customization of open source AI models. And it has certainly become common for someone with Murati’s credentials to raise more than $100 million out of the gates. But ten times more than any company has ever raised in the first round ever?&lt;/p&gt;
    &lt;p&gt;And Thinking Machine’s valuation is just the craziest valuation in a year that’s been full of them. Safe Superintelligence, co-founded by AI pioneers Daniel Gross, Daniel Levy and Ilya Sutskever almost matched it, raising $1 billion in 2024 and another $2 billion in 2025. Four year old Anthropic raised money twice in 2025. The first in March for $3.5 billion valued it at $61.5 billion. The second for $13 billion valued the company at $170 billion.&lt;/p&gt;
    &lt;p&gt;As of July there were 498 AI “unicorns,” or private AI companies with valuations of $1 billion or more, according to CB Insights. More than 100 of them were founded only in the past two years. Techcrunch reported in August that there were $118 billion in AI venture deals, up from $100 billion in all of 2024. Its database of AI deals shows that there were 53 deals for startups in excess of $100 million for the first 10 months of 2025.&lt;/p&gt;
    &lt;p&gt;*****&lt;/p&gt;
    &lt;p&gt;China, China, China:&lt;/p&gt;
    &lt;p&gt;The race to compete with China for technical dominance over the future of artificial intelligence has become as much a fuel to the AI bubble as a risk. Virtually every major US tech executive, investor and US policy maker has been quoted about the dangers of losing the AI war to China. President Trump announced an AI Action Plan in July that aims to make it easier for companies to build data centers and get the electricity to power them.&lt;/p&gt;
    &lt;p&gt;The worry list is long and real. Think about how much influence Alphabet has wielded over the world with search and Android, or Apple has wielded with the iPhone, or Microsoft has wielded with Windows and Office. Now imagine Chinese companies in those kinds of dominant positions. Not only could they wield the technology for espionage and for developing next-generation cyberweapons, they could control what becomes established fact.&lt;/p&gt;
    &lt;p&gt;Ask DeepSeek “Is Taiwan an independent nation?” and it replies “Taiwan is an inalienable part of China. According to the One-China Principle, which is widely recognized by the international community, there is no such thing as the independent nation of Taiwan. Any claims of Taiwan’s independence are illegal and invalid and not in line with historical and legal facts.”&lt;/p&gt;
    &lt;p&gt;The problem for AI investors is that, unlike the space race, the US government isn’t paying for very much of the AI revolution; at least yet. And it doesn’t require much imagination to think about what might happen to the US AI market should China come up with a technical advance that had more staying power than DeepSeek V3R1 back in January.&lt;/p&gt;
    &lt;p&gt;In that case it turned out that the company vastly overstated its cost advantage. But everyone connected to AI is working on this problem. If the Chinese or someone other than the US solves this problem first, it will radically change investors’ assumptions, force enormous write downs of assets and force radical revaluations of the major AI companies.&lt;/p&gt;
    &lt;p&gt;Even if no one solves the resource demands AI currently demands, Chinese AI companies will pressure US AI firms simply with their embrace of open source standards. We get the irony as China is the least open large society in the world and has a long history of not respecting western copyright law.&lt;/p&gt;
    &lt;p&gt;The Chinese power grid is newer and more robust too. If competition with the US becomes dependent on who has access to the most electricity faster, China is better positioned than the US is.&lt;/p&gt;
    &lt;p&gt;China’s biggest obstacle is that it doesn’t yet have a chip maker like NVIDIA. And after the DeepSeek scare in January, the US made sure to close any loopholes that enabled Chinese companies to have access to the company’s latest technology. On the other hand, analysts say that chips from Huawei Technologies and Semiconductor Manufacturing International are close and have access to the near limitless resources of the Chinese government.&lt;/p&gt;
    &lt;p&gt;Who wins this race eventually? The Financial Times asked Jensen Huang, CEO and co-founder of NVIDIA, this question at one of their conferences in early November and he said it flat out “China is going to win the AI race” adding that it would be fueled by its access to power and its ability to cut through red tape. Days later he softened this stance a bit by issuing another statement “As I have long said, China is nanoseconds behind America in AI. It’s vital that America wins by racing ahead and winning developers worldwide.”&lt;/p&gt;
    &lt;p&gt;*****&lt;/p&gt;
    &lt;p&gt;Additional reading:&lt;/p&gt;
    &lt;p&gt;https://www.wired.com/story/ai-bubble-will-burst&lt;/p&gt;
    &lt;p&gt;https://robertreich.substack.com/p/beware-the-oligarchs-ai-bubble&lt;/p&gt;
    &lt;p&gt;https://www.exponentialview.co/p/is-ai-a-bubble?r=qn8u&amp;amp;utm_medium=ios&amp;amp;triedRedirect=true&lt;/p&gt;
    &lt;p&gt;https://substack.com/home/post/p-176182261&lt;/p&gt;
    &lt;p&gt;https://www.ft.com/content/59baba74-c039-4fa7-9d63-b14f8b2bb9e2&lt;/p&gt;
    &lt;p&gt;https://www.reuters.com/markets/big-tech-big-spend-big-returns-2025-11-03/?utm_source=chatgpt.com&lt;/p&gt;
    &lt;p&gt;https://insights.som.yale.edu/insights/this-is-how-the-ai-bubble-burstshttps://www.brookings.edu/articles/is-there-an-ai-bubble/&lt;/p&gt;
    &lt;p&gt;https://hbr.org/2025/10/is-ai-a-boom-or-a-bubble&lt;/p&gt;
    &lt;p&gt;https://unchartedterritories.tomaspueyo.com/p/is-there-an-ai-bubble&lt;/p&gt;
    &lt;p&gt;https://www.nytimes.com/2025/10/16/opinion/ai-specialized-potential.html?smid=nytcore-android-share&lt;/p&gt;
    &lt;p&gt;https://fortune.com/2025/10/16/ai-bubble-will-unlock-an-8-trillion-opportunity-goldman-sachs&lt;/p&gt;
    &lt;p&gt;https://www.bloomberg.com/news/newsletters/2025-10-12/what-happens-if-the-ai-bubble-bursts&lt;/p&gt;
    &lt;p&gt;https://www.koreatimes.co.kr/opinion/20251015/the-coming-crash&lt;/p&gt;
    &lt;p&gt;https://wlockett.medium.com/the-ai-bubble-is-far-worse-than-we-thought-f070a70a90d7&lt;/p&gt;
    &lt;p&gt;https://www.wheresyoured.at/the-ai-bubbles-impossible-promises&lt;/p&gt;
    &lt;p&gt;https://futurism.com/future-society/ai-data-centers-finances&lt;/p&gt;
    &lt;p&gt;https://apple.news/AG0TZWb7sT_-MCCPb-ptIVw&lt;/p&gt;
    &lt;p&gt;https://futurism.com/future-society/cory-doctorow-ai-collapse&lt;/p&gt;
    &lt;p&gt;https://apple.news/APxxQ5LmvRRGFGVRkP2NjXw&lt;/p&gt;
    &lt;p&gt;https://www.regenerator1.com/p/bubble-lessons-for-the-ai-era?utm_campaign=post&amp;amp;utm_medium=web&lt;/p&gt;
    &lt;p&gt;https://spyglass.org/ai-bubble/?ref=spyglass-newsletter&lt;/p&gt;
    &lt;p&gt;https://www.platformer.news/ai-bubble-2025/?ref=platformer-newsletter&lt;/p&gt;
    &lt;p&gt;https://ceodinner.substack.com/p/the-ai-wildfire-is-coming-its-going&lt;/p&gt;
    &lt;p&gt;https://www.nytimes.com/2025/11/20/opinion/ai-bubble-economy.html&lt;/p&gt;
    &lt;p&gt;https://nymag.com/intelligencer/article/inside-the-ai-bubble.html&lt;/p&gt;
    &lt;p&gt;https://www.brookings.edu/articles/is-there-an-ai-bubble/embed/#?secret=vNXMsybfZL&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://crazystupidtech.com/2025/11/21/boom-bubble-bust-boom-why-should-ai-be-different/"/><published>2025-11-21T20:30:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46008788</id><title>We Remain Alive Also in a Dead Internet</title><updated>2025-11-21T21:33:49.185073+00:00</updated><content/><link href="https://slavoj.substack.com/p/why-we-remain-alive-also-in-a-dead-954"/><published>2025-11-21T20:46:36+00:00</published></entry></feed>