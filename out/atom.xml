<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-12T13:22:56.344697+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45210850</id><title>Behind the scenes of Bun Install</title><updated>2025-09-12T13:25:30.502599+00:00</updated><content>&lt;doc fingerprint="b279ac0a8ce08e80"&gt;
  &lt;main&gt;
    &lt;p&gt;Running &lt;code&gt;bun install&lt;/code&gt; is fast, very fast. On average, it runs ~7× faster than npm, ~4× faster than pnpm, and ~17× faster than yarn. The difference is especially noticeable in large codebases. What used to take minutes now takes (milli)seconds.&lt;/p&gt;
    &lt;p&gt;These aren't just cherry-picked benchmarks. Bun is fast because it treats package installation as a systems programming problem, not a JavaScript problem.&lt;/p&gt;
    &lt;p&gt;In this post we’ll explore what that means: from minimizing syscalls and caching manifests as binary, to optimizing tarball extraction, leveraging OS-native file copying, and scaling across CPU cores.&lt;/p&gt;
    &lt;p&gt;But to understand why this matters, we first have to take a small step back in time.&lt;/p&gt;
    &lt;p&gt;It's the year 2009. You're installing jQuery from a &lt;code&gt;.zip&lt;/code&gt; file, your iPhone 3GS has 256MB of RAM. GitHub was just a year old, SSDs cost $700 for 256GB. Your laptop's 5400RPM hard drive maxes out at 100MB/s, and "broadband" means 10 Mbps (if you're lucky).&lt;/p&gt;
    &lt;p&gt;But more importantly: Node.js just launched! Ryan Dahl is on stage explaining why servers spend most of their time waiting.&lt;/p&gt;
    &lt;p&gt;In 2009, a typical disk seek takes 10ms, a database query 50–200ms, and an HTTP request to an external API 300ms+. During each of these transactions, traditional servers would just... wait. Your server would start reading a file, and then just freeze for 10ms.&lt;/p&gt;
    &lt;p&gt;Now multiply that by thousands of concurrent connections each doing multiple I/O operations. Servers spent ~95% of their time waiting for I/O operations.&lt;/p&gt;
    &lt;p&gt;Node.js figured that JavaScript's event loop (originally designed for browser events) was perfect for server I/O. When code makes an async request, the I/O happens in the background while the main thread immediately moves to the next task. Once complete, a callback gets queued for execution.&lt;/p&gt;
    &lt;p&gt;JavaScript's event loop was a great solution for a world where waiting for data was the primary bottleneck.&lt;/p&gt;
    &lt;p&gt;For the next 15 years, Node's architecture shaped how we built tools. Package managers inherited Node's thread pool, event loop, async patterns; optimizations that made sense when disk seeks took 10ms.&lt;/p&gt;
    &lt;p&gt;But hardware evolved. It's not 2009 anymore, we're 16 years into the future, as hard as that is to believe. The M4 Max MacBook I'm using to write this would've ranked among the 50 fastest supercomputers on Earth in 2009. Today's NVMe drives push 7,000 MB/s, 70× faster than what Node.js was designed for! The slow mechanical drives are gone, internet speeds stream 4K video, and even low-end smartphones have more RAM than high-end servers had in 2009.&lt;/p&gt;
    &lt;p&gt;Yet today's package managers still optimize for the last decade's problems. In 2025, the real bottleneck isn't I/O anymore. It's system calls.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem with System Calls&lt;/head&gt;
    &lt;p&gt;Every time your program wants the operating system to do something (read a file, open a network connection, allocate memory), it makes a system call. Each time you make a system call, the CPU has to perform a mode switch.&lt;/p&gt;
    &lt;p&gt;Your CPU can run programs in two modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;user mode&lt;/code&gt;, where your application code runs. Programs in&lt;code&gt;user mode&lt;/code&gt;cannot directly access your device's hardware, physical memory addresses, etc. This isolation prevents programs from interfering with each other or crashing the system.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;kernel mode&lt;/code&gt;, where the operating system's kernel runs. The kernel is the core component of the OS that manages resources like scheduling processes to use the CPU, handling memory, and hardware like disks or network devices. Only the kernel and device drivers operate in&lt;code&gt;kernel mode&lt;/code&gt;!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you want to open a file, (e.g. &lt;code&gt;fs.readFile()&lt;/code&gt;) in your program, the CPU running in &lt;code&gt;user mode&lt;/code&gt; cannot directly read from disk. It first has to switch to &lt;code&gt;kernel mode&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;During this mode switch, the CPU stops executing your program → saves all its state → switches into kernel mode → performs the operation → then switches back to user mode.&lt;/p&gt;
    &lt;p&gt;However, this mode switching is expensive! Just this switch alone costs 1000-1500 CPU cycles in pure overhead, before any actual work happens.&lt;/p&gt;
    &lt;p&gt;Your CPU operates on a clock that ticks billions of times per second. A 3GHz processor completes 3 billion cycles per second. During each cycle the CPU can execute instructions: add numbers, move data, make comparisons, etc. Each cycle takes 0.33ns.&lt;/p&gt;
    &lt;p&gt;On a 3GHz processor, 1000-1500 cycles is about 500 nanoseconds. This might sound negligibly fast, but modern SSDs can handle over 1 million operations per second. If each operation requires a system call, you're burning 1.5 billion cycles per second just on mode switching.&lt;/p&gt;
    &lt;p&gt;Package installation makes thousands of these system calls. Installing React and its dependencies might trigger 50,000+ system calls: that's seconds of CPU time lost to mode switching alone! Not even reading files or installing packages, just switching between user and kernel mode.&lt;/p&gt;
    &lt;p&gt;This is why Bun treats package installation as a systems programming problem. Fast install speeds come from minimizing system calls and leveraging every OS-specific optimization available.&lt;/p&gt;
    &lt;p&gt;You can see the difference when we trace the actual system calls made by each package manager:&lt;/p&gt;
    &lt;code&gt;Benchmark 1: strace -c -f npm install
    Time (mean ± σ):  37.245 s ±  2.134 s [User: 8.432 s, System: 4.821 s]
    Range (min … max):   34.891 s … 41.203 s    10 runs

    System calls: 996,978 total (108,775 errors)
    Top syscalls: futex (663,158),  write (109,412), epoll_pwait (54,496)

  Benchmark 2: strace -c -f bun install
    Time (mean ± σ):      5.612 s ±  0.287 s [User: 2.134 s, System: 1.892 s]
    Range (min … max):    5.238 s …  6.102 s    10 runs

    System calls: 165,743 total (3,131 errors)
    Top syscalls: openat(45,348), futex (762), epoll_pwait2 (298)

  Benchmark 3: strace -c -f yarn install
    Time (mean ± σ):     94.156 s ±  3.821 s    [User: 12.734 s, System: 7.234 s]
    Range (min … max):   89.432 s … 98.912 s    10 runs

    System calls: 4,046,507 total (420,131 errors)
    Top syscalls: futex (2,499,660), epoll_pwait (326,351), write (287,543)

  Benchmark 4: strace -c -f pnpm install
    Time (mean ± σ):     24.521 s ±  1.287 s    [User: 5.821 s, System: 3.912 s]
    Range (min … max):   22.834 s … 26.743 s    10 runs

    System calls: 456,930 total (32,351 errors)
    Top syscalls: futex (116,577), openat(89,234), epoll_pwait (12,705)

  Summary
    'strace -c -f bun install' ran
      4.37 ± 0.28 times faster than 'strace -c -f pnpm install'
      6.64 ± 0.51 times faster than 'strace -c -f npm install'
     16.78 ± 1.12 times faster than 'strace -c -f yarn install'

  System Call Efficiency:
    - bun:  165,743 syscalls (29.5k syscalls/s)
    - pnpm: 456,930 syscalls (18.6k syscalls/s)
    - npm:  996,978 syscalls (26.8k syscalls/s)
    - yarn: 4,046,507 syscalls (43.0k syscalls/s)
&lt;/code&gt;
    &lt;p&gt;We can see that Bun installs much faster, but it also makes far fewer system calls. For a simple install, yarn makes over 4 million system calls, npm almost 1 million, pnpm close to 500k, and bun 165k.&lt;/p&gt;
    &lt;p&gt;At 1000-1500 cycles per call, yarn's 4 million system calls means it's spending billions of CPU cycles just on mode switching. On a 3GHz processor, that's seconds of pure overhead!&lt;/p&gt;
    &lt;p&gt;And it's not just the amount of system calls. Look at those &lt;code&gt;futex&lt;/code&gt; calls! Bun made 762 &lt;code&gt;futex&lt;/code&gt; calls (only 0.46% of total system calls), whereas npm made 663,158 (66.51%), yarn made 2,499,660 (61.76%), and pnpm made 116,577 (25.51%).&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;futex&lt;/code&gt; (fast userspace mutex) is a Linux system call used for thread synchronization. Threads are smaller units of a program that run simultaneously that often share access to memory or resources, so they must coordinate to avoid conflicts.&lt;/p&gt;
    &lt;p&gt;Most of the time, threads coordinate using fast atomic CPU instructions in &lt;code&gt;user mode&lt;/code&gt;. There's no need to switch to &lt;code&gt;kernel mode&lt;/code&gt;, so it's very efficient!&lt;/p&gt;
    &lt;p&gt;But if a thread tries to acquire a lock that's already taken, it makes a &lt;code&gt;futex&lt;/code&gt; syscall to ask the kernel to put it to sleep until the lock becomes available. A high number of &lt;code&gt;futex&lt;/code&gt; calls is an indicator that many threads are waiting on one another, causing delays.&lt;/p&gt;
    &lt;p&gt;So what's Bun doing differently here?&lt;/p&gt;
    &lt;head rend="h2"&gt;Eliminating JavaScript overhead&lt;/head&gt;
    &lt;p&gt;npm, pnpm and yarn are all written in Node.js. In Node.js, system calls aren’t made directly: when you call &lt;code&gt;fs.readFile()&lt;/code&gt;, you’re actually going through several layers before reaching the OS.&lt;/p&gt;
    &lt;p&gt;Node.js uses libuv, a C library that abstracts platform differences and manages async I/O through a thread pool.&lt;/p&gt;
    &lt;p&gt;The result is that when Node.js has to read a single file, it triggers a pretty complex pipeline. For a simple &lt;code&gt;fs.readFile('package.json', ...)&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;JavaScript validates arguments and converts strings from UTF-16 to UTF-8 for libuv's C APIs. This briefly blocks the main thread before any I/O even starts.&lt;/item&gt;
      &lt;item&gt;libuv queues the request for one of 4 worker threads. If all threads are busy, your request waits.&lt;/item&gt;
      &lt;item&gt;A worker thread picks up the request, opens the file descriptor, and makes the actual &lt;code&gt;read()&lt;/code&gt;system call.&lt;/item&gt;
      &lt;item&gt;The kernel switches to &lt;code&gt;kernel mode&lt;/code&gt;, fetches the data from disk, and returns it to the worker thread.&lt;/item&gt;
      &lt;item&gt;The worker pushes the file data back to the main thread through the event loop, which eventually schedules and runs your callback.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Every single &lt;code&gt;fs.readFile()&lt;/code&gt; call goes through this pipeline. Package installation involves reading thousands of &lt;code&gt;package.json&lt;/code&gt; files: scanning directories, processing dependency metadata, and so on. Each time threads coordinate (e.g., when accessing the task queue or signaling back to the event loop), a &lt;code&gt;futex&lt;/code&gt; system call can be used to manage locks or waits.&lt;/p&gt;
    &lt;p&gt;The overhead of making thousands of these system calls can take longer than the actual data movement itself!&lt;/p&gt;
    &lt;p&gt;Bun does it differently. Bun is written in Zig, a programming language that compiles to native code with direct system call access:&lt;/p&gt;
    &lt;code&gt;// Direct system call, no JavaScript overhead
var file = bun.sys.File.from(try bun.sys.openatA(
    bun.FD.cwd(),
    abs,
    bun.O.RDONLY,
    0,
).unwrap());
&lt;/code&gt;
    &lt;p&gt;When Bun reads a file:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Zig code directly invokes the system call (e.g., &lt;code&gt;openat()&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;The kernel immediately executes the system call and returns data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's it. There's no JavaScript engine, thread pools, event loops or marshaling between different runtime layers. Just native code making direct system calls to the kernel.&lt;/p&gt;
    &lt;p&gt;The performance difference speaks for itself:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Runtime&lt;/cell&gt;
        &lt;cell role="head"&gt;Version&lt;/cell&gt;
        &lt;cell role="head"&gt;Files/Second&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Bun&lt;/cell&gt;
        &lt;cell&gt;v1.2.20&lt;/cell&gt;
        &lt;cell&gt;146,057&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Node.js&lt;/cell&gt;
        &lt;cell&gt;v24.5.0&lt;/cell&gt;
        &lt;cell&gt;66,576&lt;/cell&gt;
        &lt;cell&gt;2.2x slower&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Node.js&lt;/cell&gt;
        &lt;cell&gt;v22.18.0&lt;/cell&gt;
        &lt;cell&gt;64,631&lt;/cell&gt;
        &lt;cell&gt;2.3x slower&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In this benchmark, Bun processes 146,057 &lt;code&gt;package.json&lt;/code&gt; files per second, while Node.js v24.5.0 manages 66,576 and v22.18.0 handles 64,631. That's over 2x faster!&lt;/p&gt;
    &lt;p&gt;Bun's 0.019ms per file represents the actual I/O cost, so how long it takes to read data when you make direct system calls without any runtime overhead. Node.js takes 0.065ms for the same operation. Package managers written in Node.js are "stuck" with Node's abstractions; they use the thread pool whether they need it or not. But they pay this cost on every file operation.&lt;/p&gt;
    &lt;p&gt;Bun's package manager is more like a native application that happens to understand JavaScript packages, not a JavaScript application trying to do systems programming.&lt;/p&gt;
    &lt;p&gt;Even though Bun isn't written in Node.js, you can use &lt;code&gt;bun install&lt;/code&gt; in any Node.js project without switching runtimes. Bun's package manager respects your existing Node.js setup and tooling, you just get faster installs!&lt;/p&gt;
    &lt;p&gt;But at this point we haven't even started installing packages yet. Let's see the optimizations Bun applies to the actual installation.&lt;/p&gt;
    &lt;p&gt;When you type &lt;code&gt;bun install&lt;/code&gt;, Bun first figures out what you're asking it to do. It reads any flags you've passed, and finds your &lt;code&gt;package.json&lt;/code&gt; to read your dependencies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Async DNS Resolution&lt;/head&gt;
    &lt;p&gt;⚠️ Note: This optimization is specific to macOS&lt;/p&gt;
    &lt;p&gt;Working with dependencies means working with network requests, and network requests require DNS resolution to convert domain names like &lt;code&gt;registry.npmjs.org&lt;/code&gt; into IP addresses.&lt;/p&gt;
    &lt;p&gt;As Bun is parsing the &lt;code&gt;package.json&lt;/code&gt;, it already starts to prefetch the DNS lookups. This means network resolution begins even before dependency analysis is even complete.&lt;/p&gt;
    &lt;p&gt;For a Node.js-based package managers, one way to do it is by using &lt;code&gt;dns.lookup()&lt;/code&gt;. While this looks async from JavaScript's perspective, it's actually implemented as a blocking &lt;code&gt;getaddrinfo()&lt;/code&gt; call under the hood, running on &lt;code&gt;libuv&lt;/code&gt;'s thread pool. It still blocks a thread, just not the main thread.&lt;/p&gt;
    &lt;p&gt;As a nice optimization, Bun takes a different approach on macOS by making it truly asynchronous at the system level. Bun uses Apple's "hidden" async DNS API (&lt;code&gt;getaddrinfo_async_start()&lt;/code&gt;), which isn't part of the POSIX standard, but it allows bun to make DNS requests that run completely asynchronously using mach ports, Apple's inter-process communication system.&lt;/p&gt;
    &lt;p&gt;While DNS resolution happens in the background, Bun can continue processing other operations like file I/O, network requests, or dependency resolution without any thread blocking. By the time it needs to download React, the DNS lookup is already done.&lt;/p&gt;
    &lt;p&gt;It's a small optimization (and not benchmarked), but it shows Bun's attention to detail: optimize at every layer!&lt;/p&gt;
    &lt;head rend="h2"&gt;Binary Manifest Caching&lt;/head&gt;
    &lt;p&gt;Now that Bun has established a connection to the npm registry, it needs the package manifests.&lt;/p&gt;
    &lt;p&gt;A manifest is a JSON file containing all versions, dependencies, and metadata for each package. For popular packages like React with 100+ versions, these manifests can be several megabytes!&lt;/p&gt;
    &lt;p&gt;A typical manifest can look something like this:&lt;/p&gt;
    &lt;code&gt;{
  "name": "lodash",
  "versions": {
    "4.17.20": {
      "name": "lodash",
      "version": "4.17.20",
      "description": "Lodash modular utilities.",
      "license": "MIT",
      "repository": {
        "type": "git",
        "url": "git+https://github.com/lodash/lodash.git"
      },
      "homepage": "https://lodash.com/"
    },
    "4.17.21": {
      "name": "lodash",
      "version": "4.17.21",
      "description": "Lodash modular utilities.",
      "license": "MIT",
      "repository": {
        "type": "git",
        "url": "git+https://github.com/lodash/lodash.git"
      },
      "homepage": "https://lodash.com/"
    }
    // ... 100+ more versions, nearly identical
  }
}
&lt;/code&gt;
    &lt;p&gt;Most package managers cache these manifests as JSON files in their cache directories. When you run &lt;code&gt;npm install&lt;/code&gt; again, instead of downloading the manifest, they read it from the cache.&lt;/p&gt;
    &lt;p&gt;That all makes sense, but the issue is that on every install (even if it's cached), they still need to parse the JSON file. This includes validating the syntax, building the object tree, managing garbage collection, and so on. A lot of parsing overhead.&lt;/p&gt;
    &lt;p&gt;And it's not just the JSON parsing overhead. Looking at lodash: the string &lt;code&gt;"Lodash modular utilities."&lt;/code&gt; appears in every single version—that's 100+ times. &lt;code&gt;"MIT"&lt;/code&gt; appears 100+ times. &lt;code&gt;"git+https://github.com/lodash/lodash.git"&lt;/code&gt; is duplicated for every version, the URL &lt;code&gt;"https://lodash.com/"&lt;/code&gt; appears in every version. Overall, lots of repeated strings.&lt;/p&gt;
    &lt;p&gt;In memory, JavaScript creates a separate string object for each string. This wastes memory and makes comparisons slower. Every time the package manager checks if two packages use the same version of postcss, it's comparing separate string objects rather than pointing to the same interned string.&lt;/p&gt;
    &lt;p&gt;Bun stores package manifests in a binary format. When Bun downloads package information, it parses the JSON once and stores it as binary files (&lt;code&gt;.npm&lt;/code&gt; files in &lt;code&gt;~/.bun/install/cache/&lt;/code&gt;). These binary files contain all the package information (versions, dependencies, checksums, etc.) stored at specific byte offsets.&lt;/p&gt;
    &lt;p&gt;When Bun accesses the name &lt;code&gt;lodash&lt;/code&gt;, it's just pointer arithmetic: &lt;code&gt;string_buffer + offset&lt;/code&gt;. No allocations, no parsing, no object traversal, just reading bytes at a known location.&lt;/p&gt;
    &lt;code&gt;// Pseudocode

// String buffer (all strings stored once)
string_buffer = "lodash\0MIT\0Lodash modular utilities.\0git+https://github.com/lodash/lodash.git\0https://lodash.com/\04.17.20\04.17.21\0..."
                 ^0     ^7   ^11                        ^37                                      ^79                   ^99      ^107

// Version entries (fixed-size structs)
versions = [
  { name_offset: 0, name_len: 6, version_offset: 99, version_len: 7, desc_offset: 11, desc_len: 26, license_offset: 7, license_len: 3, ... },  // 4.17.20
  { name_offset: 0, name_len: 6, version_offset: 107, version_len: 7, desc_offset: 11, desc_len: 26, license_offset: 7, license_len: 3, ... }, // 4.17.21
  // ... 100+ more version structs
]
&lt;/code&gt;
    &lt;p&gt;To check if packages need updating, Bun stores the responses's &lt;code&gt;ETag&lt;/code&gt; , and sends &lt;code&gt;If-None-Match&lt;/code&gt; headers. When npm responds with &lt;code&gt;"304 Not Modified"&lt;/code&gt;, Bun knows the cached data is fresh without parsing a single byte.&lt;/p&gt;
    &lt;p&gt;Looking at the benchmarks:&lt;/p&gt;
    &lt;code&gt;Benchmark 1: bun install # fresh
  Time (mean ± σ):     230.2 ms ± 685.5 ms    [User: 145.1 ms, System: 161.9 ms]
  Range (min … max):     9.0 ms … 2181.0 ms    10 runs

Benchmark 2: bun install # cached
  Time (mean ± σ):       9.1 ms ±   0.3 ms    [User: 8.5 ms, System: 5.9 ms]
  Range (min … max):     8.7 ms …  11.5 ms    10 runs

Benchmark 3: npm install # fresh
  Time (mean ± σ):      1.786 s ±  4.407 s    [User: 0.975 s, System: 0.484 s]
  Range (min … max):    0.348 s … 14.328 s    10 runs

Benchmark 4: npm install # cached
  Time (mean ± σ):     363.1 ms ±  21.6 ms    [User: 276.3 ms, System: 63.0 ms]
  Range (min … max):   344.7 ms … 412.0 ms    10 runs

Summary
  bun install # cached ran
    25.30 ± 75.33 times faster than bun install # fresh
    39.90 ± 2.37 times faster than npm install # cached
   	196.26 ± 484.29 times faster than npm install # fresh
&lt;/code&gt;
    &lt;p&gt;Here you can see that a cached(!!) &lt;code&gt;npm install&lt;/code&gt; is slower than a fresh Bun install. That's how much overhead JSON parsing the cached files can add (among other factors).&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized Tarball Extraction&lt;/head&gt;
    &lt;p&gt;Now that Bun has fetched the package manifests, it needs to download and extract compressed tarballs from the npm registry.&lt;/p&gt;
    &lt;p&gt;Tarballs are compressed archive files (like &lt;code&gt;.zip&lt;/code&gt; files) that contain all the actual source code and files for each package.&lt;/p&gt;
    &lt;p&gt;Most package managers stream the tarball data as it arrives, and decompress as it streams in. When you extract a tarball that's streaming in, the typical pattern assumes the size is unknown, and looks something like this:&lt;/p&gt;
    &lt;code&gt;let buffer = Buffer.alloc(64 * 1024); // Start with 64KB
let offset = 0;

function onData(chunk) {
  while (moreDataToCome) {
    if (offset + chunk.length &amp;gt; buffer.length) {
      // buffer full → allocate bigger one
      const newBuffer = Buffer.alloc(buffer.length * 2);

      // copy everything we’ve already written
      buffer.copy(newBuffer, 0, 0, offset);

      buffer = newBuffer;
    }

    // copy new chunk into buffer
    chunk.copy(buffer, offset);
    offset += chunk.length;
  }

  // ... decompress from buffer ...
}
&lt;/code&gt;
    &lt;p&gt;Start with a small buffer, and let it grow as more decompressed data arrives. When the buffer fills up, you allocate a larger buffer, copy all the existing data over, and continue.&lt;/p&gt;
    &lt;p&gt;This seems reasonable, but it creates a performance bottleneck: you end up copying the same data multiple times as the buffer repeatedly outgrows its current size.&lt;/p&gt;
    &lt;p&gt;When we have a 1MB package:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start with 64KB buffer&lt;/item&gt;
      &lt;item&gt;Fill up → Allocate 128KB → Copy 64KB over&lt;/item&gt;
      &lt;item&gt;Fill up → Allocate 256KB → Copy 128KB over&lt;/item&gt;
      &lt;item&gt;Fill up → Allocate 512KB → Copy 256KB over&lt;/item&gt;
      &lt;item&gt;Fill up → Allocate 1MB → Copy 512KB over&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You just copied 960KB of data unnecessarily! And this happens for every single package. The memory allocator has to find contiguous space for each new buffer, while the old buffer stays allocated during the copy operation. For large packages, you might copy the same bytes 5-6 times.&lt;/p&gt;
    &lt;p&gt;Bun takes a different approach by buffering the entire tarball before decompressing. Instead of processing data as it arrives, Bun waits until the entire compressed file is downloaded into memory.&lt;/p&gt;
    &lt;p&gt;Now you might think "Wait, aren't they just wasting RAM keeping everything in memory?" And for large packages like TypeScript (which can be 50MB compressed), you'd have a point.&lt;/p&gt;
    &lt;p&gt;But the vast majority of npm packages are tiny, most are under 1MB. For these common cases, buffering the whole thing eliminates all the repeated copying. Even for those larger packages, the temporary memory spike is usually fine on modern systems, and avoiding 5-6 buffer copies more than makes up for it.&lt;/p&gt;
    &lt;p&gt;Once Bun has the complete tarball in memory, it can read the last 4 bytes of the gzip format. These bytes are special since store the uncompressed size of the file! Instead of having to guess how large the uncompressed file will be, Bun can pre-allocate memory to eliminate buffer resizing entirely:&lt;/p&gt;
    &lt;code&gt;{
  // Last 4 bytes of a gzip-compressed file are the uncompressed size.
  if (tgz_bytes.len &amp;gt; 16) {
    // If the file claims to be larger than 16 bytes and smaller than 64 MB, we'll preallocate the buffer.
    // If it's larger than that, we'll do it incrementally. We want to avoid OOMing.
    const last_4_bytes: u32 = @bitCast(tgz_bytes[tgz_bytes.len - 4 ..][0..4].*);
    if (last_4_bytes &amp;gt; 16 and last_4_bytes &amp;lt; 64 * 1024 * 1024) {
      // It's okay if this fails. We will just allocate as we go and that will error if we run out of memory.
      esimated_output_size = last_4_bytes;
      if (zlib_pool.data.list.capacity == 0) {
          zlib_pool.data.list.ensureTotalCapacityPrecise(zlib_pool.data.allocator, last_4_bytes) catch {};
      } else {
          zlib_pool.data.ensureUnusedCapacity(last_4_bytes) catch {};
      }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;Those 4 bytes tell Bun "this gzip will decompress to exactly 1,048,576 bytes", so it can pre-allocate exactly this amount of memory upfront. There's no repeated resizing or copying of data; just one memory allocation.&lt;/p&gt;
    &lt;p&gt;To do the actual decompression, Bun uses &lt;code&gt;libdeflate&lt;/code&gt;. This is a high-performance lib that decompresses tarballs faster than the standard &lt;code&gt;zlib&lt;/code&gt; used by most package managers. It's optimized specifically for modern CPUs with SIMD instructions.&lt;/p&gt;
    &lt;p&gt;Optimized tarball extraction would've been difficult to for package managers written in Node.js. You'd need to create a separate read stream, seek to the end, read 4 bytes, parse them, close the stream, then start over with your decompression. Node's APIs aren't designed for this pattern.&lt;/p&gt;
    &lt;p&gt;In Zig it's pretty straight-forward: you just seek to the end and read the last four bytes, that's it!&lt;/p&gt;
    &lt;p&gt;Now that Bun has all the package data, it faces another challenge: how do you efficiently store and access thousands of (interdependent) packages?&lt;/p&gt;
    &lt;head rend="h2"&gt;Cache-Friendly Data Layout&lt;/head&gt;
    &lt;p&gt;Dealing with thousands of packages can be tricky. Each package has dependencies, which have their own dependencies, creating a pretty complex graph.&lt;/p&gt;
    &lt;p&gt;During installation, package managers have to traverse this graph to check the package versions, resolve any conflicts, and determine which version to install. They also need to "hoist" dependencies by moving them to higher levels so multiple packages can share them.&lt;/p&gt;
    &lt;p&gt;But the way that this dependency graph is stored has a big impact on performance. Traditional package managers store dependencies like this:&lt;/p&gt;
    &lt;code&gt;const packages = {
  next: {
    name: "next",
    version: "15.5.0",
    dependencies: {
      "@swc/helpers": "0.5.15",
      "postcss": "8.4.31",
      "styled-jsx": "5.1.6",
    },
  },
  postcss: {
    name: "postcss",
    version: "8.4.31",
    dependencies: {
      nanoid: "^3.3.6",
      picocolors: "^1.0.0",
    },
  },
};
&lt;/code&gt;
    &lt;p&gt;This looks clean as JavaScript code, but it's not ideal for modern CPU architectures.&lt;/p&gt;
    &lt;p&gt;In JavaScript, each object is stored on the heap. When accessing &lt;code&gt;packages["next"]&lt;/code&gt;, the CPU accesses a pointer that tells it where Next's data is located in memory. This data then contains yet another pointer to where its dependencies live, which in turn contains more pointers to the actual dependency strings.&lt;/p&gt;
    &lt;p&gt;The key issue is how JavaScript allocates objects in memory. When you create objects at different times, the JavaScript engine uses whatever memory is available at that moment:&lt;/p&gt;
    &lt;code&gt;// These objects are created at different moments during parsing
packages["react"] = { name: "react", ... }  	  // Allocated at address 0x1000
packages["next"] = { name: "next", ... }     		// Allocated at address 0x2000
packages["postcss"] = { name: "postcss", ... }  // Allocated at address 0x8000
// ... hundreds more packages
&lt;/code&gt;
    &lt;p&gt;These addresses are basically just random. There is no locality guarantee - objects can just be scattered across RAM, even objects that are related to each other!&lt;/p&gt;
    &lt;p&gt;This random scattering matters because of how modern CPUs actually fetch data.&lt;/p&gt;
    &lt;p&gt;Modern CPUs are incredibly fast at processing data (billions of operations per second), but fetching data from RAM is slow. To bridge this gap, CPUs have multiple cache levels:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;L1 cache, small storage, but extremely fast (~4 CPU cycles)&lt;/item&gt;
      &lt;item&gt;L2 cache, medium storage, a bit slower (~12 CPU cycles)&lt;/item&gt;
      &lt;item&gt;L3 cache: 8-32MB storage, requires ~40 CPU cycles&lt;/item&gt;
      &lt;item&gt;RAM: Lots of GB, requires ~300 cycles (slow!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;Visualizing CPU cache speeds vs RAM. Cache optimization matters! pic.twitter.com/q2rkGqSUAG&lt;/p&gt;— Ben Dicken (@BenjDicken) Oct 18, 2024&lt;/quote&gt;
    &lt;p&gt;The "issue" is that caches work with cache lines. When you access memory, the CPU doesn't just load that one byte: it loads the entire 64-byte chunk in which that byte appears. It figures that if you need one byte, you'll probably need nearby bytes soon (this is called spatial locality).&lt;/p&gt;
    &lt;p&gt;This optimization works great for data that's stored sequentially, but it backfires when your data is scattered randomly across memory.&lt;/p&gt;
    &lt;p&gt;When the CPU loads &lt;code&gt;packages["next"]&lt;/code&gt; at address &lt;code&gt;0x2000&lt;/code&gt;, it actually loads all the bytes within that cache line. But the next package, &lt;code&gt;packages["postcss"]&lt;/code&gt;, is at address &lt;code&gt;0x8000&lt;/code&gt; . This is a completely different cache line! The other 56 bytes the CPU loaded in the cache line are just completely wasted, they're just random memory from whatever happened to be allocated nearby; maybe garbage, maybe parts of unrelated objects.&lt;/p&gt;
    &lt;p&gt;But you paid the cost of loading 64 bytes but only used 8...&lt;/p&gt;
    &lt;p&gt;By the time it's accessed 512 different packages (32KB / 64 bytes), you've filled your entire L1 cache already. Now every new package access evicts a previously loaded cache line to make space. The package you just accessed will be evicted soon, and that dependency it needs to check in 10 microseconds is already gone. Cache hit rate drops, and every access becomes a ~300 cycle trip to RAM instead of a 4 cycle L1 hit, far from optimal.&lt;/p&gt;
    &lt;p&gt;The nested structure of objects creates whats called "pointer chasing", a common anti-pattern in system programming. The CPU can't predict where to load next because each pointer could point anywhere. It simply cannot know where &lt;code&gt;next.dependencies&lt;/code&gt; lives until it finishes loading the &lt;code&gt;next&lt;/code&gt; object.&lt;/p&gt;
    &lt;p&gt;When traversing Next's dependencies, the CPU has to perform multiple dependent memory loads:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Load &lt;code&gt;packages["next"]&lt;/code&gt;pointer → Cache miss → RAM fetch (~300 cycles)&lt;/item&gt;
      &lt;item&gt;Follow that pointer to load &lt;code&gt;next.dependencies&lt;/code&gt;pointer → Another cache miss → RAM fetch (~300 cycles)&lt;/item&gt;
      &lt;item&gt;Follow that to find &lt;code&gt;"postcss"&lt;/code&gt;in the hash table → Cache miss → RAM fetch (~300 cycles)&lt;/item&gt;
      &lt;item&gt;Follow that pointer to load the actual string data → Cache miss → RAM fetch (~300 cycles)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can end up with many cache misses since we're working with hundreds of dependencies, all scattered across memory. Each cache line we load (64 bytes) might contain data for just one object. With all those objects spread across GBs of RAM, the working set easily exceeds the L1 cache (32KB), L2 (256KB) and even the L3 cache (8-32MB). By the time we need an object again, it's likely that it's been evicted from all cache levels.&lt;/p&gt;
    &lt;p&gt;That's ~1200 cycles (400ns on a 3GHz CPU) just to read one dependency name! For a project with 1000 packages averaging 5 dependencies each, that's 2ms of pure memory latency.&lt;/p&gt;
    &lt;p&gt;Bun uses Structure of Arrays. Instead of each package storing its own dependency array, Bun keeps all dependencies in one big shared array, all package names in another shared array, and so on:&lt;/p&gt;
    &lt;code&gt;// ❌ Traditional Array of Structures (AoS) - lots of pointers
packages = {
  next: { dependencies: { "@swc/helpers": "0.5.15", "postcss": "8.4.31" } },
};

// ✅ Bun's Structure of Arrays (SoA) - cache friendly
packages = [
  {
    name: { off: 0, len: 4 },
    version: { off: 5, len: 6 },
    deps: { off: 0, len: 2 },
  }, // next
];

dependencies = [
  { name: { off: 12, len: 13 }, version: { off: 26, len: 7 } }, // @swc/helpers@0.5.15
  { name: { off: 34, len: 7 }, version: { off: 42, len: 6 } }, // postcss@8.4.31
];

string_buffer = "next\015.5.0\0@swc/helpers\00.5.15\0postcss\08.4.31\0";
&lt;/code&gt;
    &lt;p&gt;Instead of each package storing pointers to its own data scattered across memory, Bun just uses large contiguous buffers, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;packages&lt;/code&gt;stores lightweight structs that specify where to find this package's data using offsets&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;dependencies&lt;/code&gt;stores the actual dependency relationships for all packages in one place&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;string_buffer&lt;/code&gt;stores all text (names, versions, etc.) sequentially in one massive string&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;versions&lt;/code&gt;stores all parsed semantic versions as compact structs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now, accessing Next's dependencies just becomes arithmetic:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;packages[0]&lt;/code&gt;tells us that Next's dependencies start at position&lt;code&gt;0&lt;/code&gt;in the&lt;code&gt;dependencies&lt;/code&gt;array, and there's 2 dependencies:&lt;code&gt;{ name_offset: 0, deps_offset: 0, deps_count: 2 }&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Go to &lt;code&gt;dependencies[1]&lt;/code&gt;which tells us that postcss's name starts at position&lt;code&gt;34&lt;/code&gt;in the string&lt;code&gt;string_buffer&lt;/code&gt;, and version at position&lt;code&gt;42&lt;/code&gt;:&lt;code&gt;{ name_offset: 34, version_offset: 42 }&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Go to position 34 in &lt;code&gt;string_buffer&lt;/code&gt;and read&lt;code&gt;postcss&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Go to position 42 in &lt;code&gt;string_buffer&lt;/code&gt;and read&lt;code&gt;"8.4.31"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;… and so on&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now when you access &lt;code&gt;packages[0]&lt;/code&gt;, the CPU doesn't just load those 8 bytes: it loads an entire 64-byte cache line. Since each package is 8 bytes, and 64 ÷ 8 = 8, you get &lt;code&gt;packages[0]&lt;/code&gt; through &lt;code&gt;packages[7]&lt;/code&gt; in a single memory fetch.&lt;/p&gt;
    &lt;p&gt;So when your code processes the &lt;code&gt;react&lt;/code&gt; dependency (&lt;code&gt;packages[0]&lt;/code&gt;, &lt;code&gt;packages[1]&lt;/code&gt; through &lt;code&gt;packages[7]&lt;/code&gt; are already sitting in your L1 cache, ready to be accessed with zero additional memory fetches. That's why sequential access is so fast: you're getting 8 packages just by accessing memory once.&lt;/p&gt;
    &lt;p&gt;Instead of the many small, scattered allocations throughout memory that we saw in the previous example, we now have just ~6 large allocations in total, regardless of how many packages you have. This is completely different from the pointer-based approach, which required a separate memory fetch for each object.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized Lockfile Format&lt;/head&gt;
    &lt;p&gt;Bun also applies the Structure of Arrays approach to its &lt;code&gt;bun.lock&lt;/code&gt; lockfile.&lt;/p&gt;
    &lt;p&gt;When you run &lt;code&gt;bun install&lt;/code&gt;, Bun has to parse the existing lockfile to determine what's already installed and what needs updating. Most package managers store lockfiles as nested JSON (npm) or YAML (pnpm, yarn). When npm parses &lt;code&gt;package-lock.json&lt;/code&gt;, it's processing deeply nested objects:&lt;/p&gt;
    &lt;code&gt;{
  "dependencies": {
    "next": {
      "version": "15.5.0",
      "requires": {
        "@swc/helpers": "0.5.15",
        "postcss": "8.4.31"
      }
    },
    "postcss": {
      "version": "8.4.31",
      "requires": {
        "nanoid": "^3.3.6",
        "picocolors": "^1.0.0"
      }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;Each package becomes its own object with nested dependency objects. JSON parsers must allocate memory for every object, validate syntax, and build complex nested trees. For projects with thousands of dependencies, this creates the same pointer-chasing problem we saw earlier!&lt;/p&gt;
    &lt;p&gt;Bun applies the Structure of Arrays approach to its lockfile, in a human-readable format:&lt;/p&gt;
    &lt;code&gt;{
  "lockfileVersion": 0,
  "packages": {
    "next": [
      "next@npm:15.5.0",
      { "@swc/helpers": "0.5.15", "postcss": "8.4.31" },
      "hash123"
    ],
    "postcss": [
      "postcss@npm:8.4.31",
      { "nanoid": "^3.3.6", "picocolors": "^1.0.0" },
      "hash456"
    ]
  }
}
&lt;/code&gt;
    &lt;p&gt;This again deduplicates strings, and stores dependencies in a cache-friendly layout. They're stored following dependency order rather than alphabetically or in a nested hierarchy. This means that a parser can read memory more efficiently (sequentially), avoiding random jumps between objects.&lt;/p&gt;
    &lt;p&gt;And not only that, Bun also pre-allocates memory based on the lockfile size. Just like with tarball extraction, this avoids the repeated resize-and-copy cycles that create performance bottlenecks during parsing.&lt;/p&gt;
    &lt;p&gt;As a sidenote: Bun originally used a binary lockfile format (&lt;code&gt;bun.lockb&lt;/code&gt;) to avoid JSON parsing overhead entirely, but binary files are impossible to review in pull requests and can't be merged when conflicts happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;File copying&lt;/head&gt;
    &lt;p&gt;After the packages are installed and cached in &lt;code&gt;~/.bun/install/cache/&lt;/code&gt;, Bun must copy the files into &lt;code&gt;node_modules&lt;/code&gt;. This is where we see most of Bun's performance impact!&lt;/p&gt;
    &lt;p&gt;Traditional file copying traverses each directory and copies files individually. This requires multiple system calls per file:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;opening the source file (&lt;code&gt;open()&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;creating and opening the destination file (&lt;code&gt;open()&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;repeatedly reading chunks from the source and writing them to the destination until complete (&lt;code&gt;read()&lt;/code&gt;/&lt;code&gt;write()&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;finally, closing both files &lt;code&gt;close()&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each of these steps requires that expensive mode switch between user mode and the kernel.&lt;/p&gt;
    &lt;p&gt;For a typical React app with thousands of package files, this generates hundreds of thousands to millions of system calls! This is exactly the systems programming problem we described earlier: the overhead of making all these system calls becomes more expensive than actually moving the data.&lt;/p&gt;
    &lt;p&gt;Bun uses different strategies depending on your operating system and filesystem, leveraging every OS-specific optimization available. Bun supports several file copying backends, each with different performance characteristics:&lt;/p&gt;
    &lt;head rend="h3"&gt;macOS&lt;/head&gt;
    &lt;p&gt;On macOS, Bun uses Apple's native &lt;code&gt;clonefile()&lt;/code&gt; copy-on-write system call.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;clonefile&lt;/code&gt; can clone entire directory trees in a single system call. This system call creates new directory and file metadata entries that reference the same physical disk blocks as the original files. Instead of writing new data to disk, the filesystem just creates new "pointers" to existing data.&lt;/p&gt;
    &lt;code&gt;// Traditional approach: millions of syscalls
for (each file) {
  copy_file_traditionally(src, dst);  // 50+ syscalls per file
}

// Bun's approach: ONE syscall
clonefile("/cache/react", "/node_modules/react", 0);
&lt;/code&gt;
    &lt;p&gt;SSD stores data in fixed-size blocks. When you normally copy a file (&lt;code&gt;copy()&lt;/code&gt;), the filesystem allocates new blocks and writes duplicate data. With &lt;code&gt;clonefile&lt;/code&gt;, both the original and "copied" file have metadata that points to the exact same physical blocks on your SSD.&lt;/p&gt;
    &lt;p&gt;Copy-on-write means data is only duplicated when modified. This results in an &lt;code&gt;O(1)&lt;/code&gt; operation vs. the &lt;code&gt;O(n)&lt;/code&gt; of traditional copying.&lt;/p&gt;
    &lt;p&gt;The metadata of both files point to the same data blocks until you modify one of them.&lt;/p&gt;
    &lt;p&gt;When you modify the contents of one of the files, the filesystem automatically allocates new blocks for the edited parts, and updates the file metadata to point to the new blocks.&lt;/p&gt;
    &lt;p&gt;However, this rarely happens since &lt;code&gt;node_modules&lt;/code&gt; files are typically read-only after installation; we don't actively modify modules from within our code.&lt;/p&gt;
    &lt;p&gt;This makes copy-on-write extremely efficient: multiple packages can share identical dependency files without using additional disk space.&lt;/p&gt;
    &lt;code&gt;Benchmark 1: bun install --backend=copyfile
  Time (mean ± σ):      2.955 s ±  0.101 s    [User: 0.190 s, System: 1.991 s]
  Range (min … max):    2.825 s …  3.107 s    10 runs

Benchmark 2: bun install --backend=clonefile
  Time (mean ± σ):      1.274 s ±  0.052 s    [User: 0.140 s, System: 0.257 s]
  Range (min … max):    1.184 s …  1.362 s    10 runs

Summary
  bun install --backend=clonefile ran
    2.32 ± 0.12 times faster than bun install --backend=copyfile
&lt;/code&gt;
    &lt;p&gt;When &lt;code&gt;clonefile&lt;/code&gt; fails (due to lack of filesystem support), Bun falls back to &lt;code&gt;clonefile_each_dir&lt;/code&gt; for per-directory cloning. If that also fails, Bun uses traditional &lt;code&gt;copyfile&lt;/code&gt; as the final fallback.&lt;/p&gt;
    &lt;head rend="h3"&gt;Linux&lt;/head&gt;
    &lt;p&gt;Linux doesn't have &lt;code&gt;clonefile()&lt;/code&gt;, but it has something even older and more powerful: hardlinks. Bun implements a fallback chain that tries increasingly less optimal approaches until one works:&lt;/p&gt;
    &lt;head rend="h4"&gt;1. Hardlinks&lt;/head&gt;
    &lt;p&gt;On Linux, Bun's default strategy is hardlinks. A hardlink doesn't create a new file at all, it only creates a new name for an existing file, and references this existing file.&lt;/p&gt;
    &lt;code&gt;link("/cache/react/index.js", "/node_modules/react/index.js");
&lt;/code&gt;
    &lt;p&gt;To understand hardlinks, you need to understand inodes. Every file on Linux has an inode, which is a data structure that contains all the file's metadata (permissions, timestamps, etc.). The filename is just a pointer to an inode:&lt;/p&gt;
    &lt;p&gt;Both paths point to the same inode. If you delete one path, the other remains. However, if you modify one, both see changes (because they're the same file!).&lt;/p&gt;
    &lt;p&gt;This results in great performance gains because there's zero data movement. Creating a hard link requires a single system call that completes in microseconds, regardless of whether you're linking a 1KB file or a 100MB bundle. Much more efficient than traditional copying, which has to read and write every single byte.&lt;/p&gt;
    &lt;p&gt;They're also extremely efficient for disk space, since there's only ever one copy of the actual data on disk, no matter how many packages reference the same dependency files&lt;/p&gt;
    &lt;p&gt;However, hardlinks have limitations. They can't cross filesystem boundaries (e.g. your cache is in a different location than your &lt;code&gt;node_modules&lt;/code&gt;), some filesystems don't support them, and certain file types or permission configurations can cause hardlink creation to fail.&lt;/p&gt;
    &lt;p&gt;When hardlinks aren't possible, Bun has some fallbacks:&lt;/p&gt;
    &lt;head rend="h4"&gt;2. &lt;code&gt;ioctl_ficlone&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;It starts with &lt;code&gt;ioctl_ficlone&lt;/code&gt;, which enables copy-on-write on filesystems like Btrfs and XFS. This is very similar to &lt;code&gt;clonefile&lt;/code&gt;'s copy-on-write system in the way that it also creates a new file references that share the same disk data. Unlike hardlinks, these are separate files; they just happen to share storage until modified.&lt;/p&gt;
    &lt;head rend="h4"&gt;3. &lt;code&gt;copy_file_range&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;If copy-on-write isn't available, Bun tries to at least keep the copying in kernel space and falls back to &lt;code&gt;copy_file_range&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In a traditional copy, the kernel reads from disk into a kernel buffer, then copies that data to your program's buffer in user space. Later when you call &lt;code&gt;write()&lt;/code&gt;, it copies it back to a kernel buffer before writing to disk. That's four memory operations and multiple context switches!&lt;/p&gt;
    &lt;p&gt;With &lt;code&gt;copy_file_range&lt;/code&gt;, the kernel reads from disk into a kernel buffer and writes directly to disk. Just two operations and zero context switches for the data movement.&lt;/p&gt;
    &lt;head rend="h4"&gt;4. &lt;code&gt;sendfile&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;If that's unavailable, Bun uses &lt;code&gt;sendfile&lt;/code&gt;. This is a system call that was originally designed for network transfers, but it's also effective for copying data directly between two files on disk.&lt;/p&gt;
    &lt;p&gt;This command also keeps data in kernel space: the kernel reads data from one destination (a reference to an open file on disk, e.g. a source file in &lt;code&gt;~/.bun/install/cache/&lt;/code&gt;) and writes it to another destination (like a destination file in &lt;code&gt;node_modules&lt;/code&gt;), all within the kernel's memory space.&lt;/p&gt;
    &lt;p&gt;This process is called disk-to-disk copying, as it moves data between files stored on the same or different disks without touching your program's memory. It's an older API but more widely supported, making it a reliable fallback when newer system calls aren't available while still reducing the number of memory calls.&lt;/p&gt;
    &lt;head rend="h4"&gt;5. &lt;code&gt;copyfile&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;As a last resort, Bun uses traditional file copying; the same approach most package managers use. This creates entirely separate copies of each file by reading data from the cache and writing it to the destination using a &lt;code&gt;read()&lt;/code&gt;/&lt;code&gt;write()&lt;/code&gt; loop. This uses multiple system calls, which is exactly what Bun is trying to minimize. It's the least efficient option, but it's universally compatible.&lt;/p&gt;
    &lt;code&gt;Benchmark 1: bun install --backend=copyfile
  Time (mean ± σ):     325.0 ms ±   7.7 ms    [User: 38.4 ms, System: 295.0 ms]
  Range (min … max):   314.2 ms … 340.0 ms    10 runs

Benchmark 2: bun install --backend=hardlink
  Time (mean ± σ):     109.4 ms ±   5.1 ms    [User: 32.0 ms, System: 86.8 ms]
  Range (min … max):   102.8 ms … 119.0 ms    19 runs

Summary
  bun install --backend=hardlink ran
    2.97 ± 0.16 times faster than bun install --backend=copyfile
&lt;/code&gt;
    &lt;p&gt;These file copying optimizations address the primary bottleneck: system call overhead. Instead of using a one-size-fits-all approach, Bun chooses the most efficient file copying specifically tailored to you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Core Parallelism&lt;/head&gt;
    &lt;p&gt;All the above-mentioned optimizations are great, but they aim to reduce the workload for a single CPU core. However, modern laptops have 8, 16, even 24 CPU cores!&lt;/p&gt;
    &lt;p&gt;Node.js has a thread pool, but all the actual work (e.g. figuring out which version of React works with which version of webpack, building the dependency graph, deciding what to install) happens on one thread and one CPU core. When npm runs on your M3 Max, one core works really hard while the other 15 are idle.&lt;/p&gt;
    &lt;p&gt;A CPU core can independently execute instructions. Early computers had one core, they could only do one thing at a time, but modern CPUs pack multiple cores onto a single chip. A 16-core CPU can execute 16 different instruction streams simultaneously, not just switching between them really fast.&lt;/p&gt;
    &lt;p&gt;This is yet another fundamental bottleneck for traditional package managers: no matter how many cores you have, the package manager can only use one CPU core.&lt;/p&gt;
    &lt;p&gt;Bun takes a different approach with a lock-free, work-stealing thread pool architecture.&lt;/p&gt;
    &lt;p&gt;Work-stealing means that idle threads can "steal" pending tasks from busy threads' queues. When a thread finishes its work, it checks its local queue, then the global queue, then steals from other threads. No thread sits idle when there's still work to do.&lt;/p&gt;
    &lt;p&gt;Instead of being limited to JavaScript's event loop, Bun spawns native threads that can fully utilize every CPU core. The thread pool automatically scales to match your device's CPU's core count, allowing Bun to maximize parallelizing the I/O-heavy parts of the installation process. One thread can be extracting &lt;code&gt;next&lt;/code&gt;'s tarball, another is resolving &lt;code&gt;postcss&lt;/code&gt; dependencies, a third applying patches to &lt;code&gt;webpack&lt;/code&gt;, and so on.&lt;/p&gt;
    &lt;p&gt;But multi-threading often comes with synchronization overhead. Those hundreds of thousands of &lt;code&gt;futex&lt;/code&gt; calls npm made were just threads constantly waiting for each other. Each time a thread wants to add a task to a shared queue, it has to lock it first, blocking all other threads.&lt;/p&gt;
    &lt;code&gt;// Traditional approach: Locks
mutex.lock();                   // Thread 1 gets exclusive access
queue.push(task);               // Only Thread 1 can work
mutex.unlock();                 // Finally releases lock
// Problem: Threads 2-8 blocked, waiting in line
&lt;/code&gt;
    &lt;p&gt;Bun uses lock-free data structures instead. These use special CPU instructions called atomic operations that allow threads to safely modify shared data without locks:&lt;/p&gt;
    &lt;code&gt;pub fn push(self: *Queue, batch: Batch) void {
  // Atomic compare-and-swap, happens instantly
  _ = @cmpxchgStrong(usize, &amp;amp;self.state, state, new_state, .seq_cst, .seq_cst);
}
&lt;/code&gt;
    &lt;p&gt;In an earlier benchmark we saw that Bun was able to process 146,057 &lt;code&gt;package.json&lt;/code&gt; files/second versus Node.js's 66,576. That's the impact of using all cores instead of one.&lt;/p&gt;
    &lt;p&gt;Bun also runs network operations differently. Traditional package managers often block. When downloading a package, the CPU sits idle waiting for the network.&lt;/p&gt;
    &lt;p&gt;Bun maintains a pool of 64(!) concurrent HTTP connections (configurable via &lt;code&gt;BUN_CONFIG_MAX_HTTP_REQUESTS&lt;/code&gt;) on dedicated network threads. The network thread runs independently with its own event loop, handling all downloads while CPU threads handle the extraction and processing. Neither waits for the other.&lt;/p&gt;
    &lt;p&gt;Bun also gives each thread its own memory pool. An issue with "traditional" multi-threading is that all threads compete for the same memory allocator. This creates contention: if 16 threads all need memory at once, they have to wait for each other.&lt;/p&gt;
    &lt;code&gt;// Traditional: all threads share one allocator
Thread 1: "I need 1KB for package data"    // Lock allocator
Thread 2: "I need 2KB for JSON parsing"    // Wait...
Thread 3: "I need 512B for file paths"     // Wait...
Thread 4: "I need 4KB for extraction"      // Wait...
&lt;/code&gt;
    &lt;p&gt;Bun instead gives each thread its own large chunk of pre-allocated memory that the thread manages independently. There's no sharing or waiting, each thread works with its own data whenever possible.&lt;/p&gt;
    &lt;code&gt;// Bun: each thread has its own allocator
Thread 1: Allocates from pool 1    // Instant
Thread 2: Allocates from pool 2    // Instant
Thread 3: Allocates from pool 3    // Instant
Thread 4: Allocates from pool 4    // Instant
&lt;/code&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The package managers we benchmarked weren't built wrong, they were solutions designed for the constraints of their time.&lt;/p&gt;
    &lt;p&gt;npm gave us a foundation to build on, yarn made managing workspaces less painful, and pnpm came up with a clever way to save space and speed things up with hardlinks. Each worked hard to solve the problems developers were actually hitting at the time.&lt;/p&gt;
    &lt;p&gt;But that world no longer exists. SSDs are 70× faster, CPUs have dozens of cores, and memory is cheap. The real bottleneck shifted from hardware speed to software abstractions.&lt;/p&gt;
    &lt;p&gt;Buns approach wasn't revolutionary, it was just willing to look at what actually slows things down today. When SSDs can handle a million operations per second, why accept thread pool overhead? When you're reading the same package manifest for the hundredth time, why parse JSON again? When the filesystem supports copy-on-write, why duplicate gigabytes of data?&lt;/p&gt;
    &lt;p&gt;The tools that will define the next decade of developer productivity are being written right now, by teams who understand that performance bottlenecks shifted when storage got fast and memory got cheap. They're not just incrementally improving what exists; they're rethinking what's possible.&lt;/p&gt;
    &lt;p&gt;Installing packages 25x faster isn't "magic": it's what happens when tools are built for the hardware we actually have.&lt;/p&gt;
    &lt;p&gt;→ Experience software built for 2025 at bun.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bun.com/blog/behind-the-scenes-of-bun-install"/></entry><entry><id>https://news.ycombinator.com/item?id=45211596</id><title>Gene-edited pancreatic cells transplanted into a patient with type 1 diabetes</title><updated>2025-09-12T13:25:30.354699+00:00</updated><content>&lt;doc fingerprint="1654c01a1a0f0b13"&gt;
  &lt;main&gt;
    &lt;p&gt;All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links. Learn more.&lt;/p&gt;
    &lt;p&gt;Crispr gene-editing technology has demonstrated its revolutionary potential in recent years: It has been used to treat rare diseases, to adapt crops to withstand the extremes of climate change, or even to change the color of a spider’s web. But the greatest hope is that this technology will help find a cure for a global disease, such as diabetes. A new study points in that direction.&lt;/p&gt;
    &lt;p&gt;For the first time, researchers succeeded in implanting Crispr-edited pancreatic cells in a man with type 1 diabetes, an autoimmune disease where the immune system attacks insulin-producing cells in the pancreas. Without insulin, the body is then unable to regulate blood sugar. If steps aren’t taken to manage glucose levels by other means (typically, by injecting insulin), this can lead to damage to the nerves and organs—particularly the heart, kidneys, and eyes. Roughly 9.5 million people worldwide have type 1 diabetes.&lt;/p&gt;
    &lt;p&gt;In this experiment, edited cells produced insulin for months after being implanted, without the need for the recipient to take any immunosuppressive drugs to stop their body attacking the cells. The Crispr technology allowed the researchers to endow the genetically modified cells with camouflage to evade detection.&lt;/p&gt;
    &lt;p&gt;The study, published last month in The New England Journal of Medicine, details the step-by-step procedure. First, pancreatic islet cells were taken from a deceased donor without diabetes, and then altered with the gene-editing technique Crispr-Cas12b to allow them to evade the immune response of the diabetes patient. Cells altered like this are said to be “hypoimmune,” explains Sonja Schrepfer, a professor at Cedars-Sinai Medical Center in California and the scientific cofounder of Sana Biotechnology, the company that developed this treatment.&lt;/p&gt;
    &lt;p&gt;The edited cells were then implanted into the forearm muscle of the patient, and after 12 weeks, no signs of rejection were detected. (A subsequent report from Sana Biotechnology notes that the implanted cells were still evading the patient’s immune system after six months.)&lt;/p&gt;
    &lt;p&gt;Tests run as part of the study recorded that the cells were functional: The implanted cells secreted insulin in response to glucose levels, representing a key step toward controlling diabetes without the need for insulin injections. Four adverse events were recorded during follow-ups with the patient, but none of them were serious or directly linked to the modified cells.&lt;/p&gt;
    &lt;p&gt;The researchers’ ultimate goal is to apply immune-camouflaging gene edits to stem cells—which have the ability to reproduce and differentiate themselves into other cell types inside the body—and then to direct their development into insulin-secreting islet cells. “The advantage of engineering hypoimmune stem cells is that when these stem cells proliferate and create new cells, the new cells are also hypoimmune,” Schrepfer explained in a Cedars-Sinai Q+A earlier this year.&lt;/p&gt;
    &lt;p&gt;Traditionally, transplanting foreign cells into a patient has required suppressing the patient’s immune system to avoid them being rejected. This carries significant risks: infections, toxicity, and long-term complications. “Seeing patients die from rejection or severe complications from immunosuppression was frustrating to me, and I decided to focus my career on developing strategies to overcome immune rejection without immunosuppressive drugs,” Schrepfer told Cedars-Sinai.&lt;/p&gt;
    &lt;p&gt;Although the research marks a milestone in the search for treatments of type 1 diabetes, it’s important to note that the study involved one one participant, who received a low dose of cells for a short period—not enough for the patient to no longer need to control their blood sugar with injected insulin. An editorial by the journal Nature also says that some independent research groups have failed in their efforts to confirm that Sana’s method provides edited cells with the ability to evade the immune system.&lt;/p&gt;
    &lt;p&gt;Sana will be looking to conduct more clinical trials starting next year. Without overlooking the criticisms and limitations of the current study, the possibility of transplanting cells modified to be invisible to the immune system opens up a very promising horizon in regenerative medicine.&lt;/p&gt;
    &lt;p&gt;This story originally appeared on WIRED en Español and has been translated from Spanish.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wired.com/story/no-more-injections-crispr-offers-new-hope-for-treating-diabetes/"/></entry><entry><id>https://news.ycombinator.com/item?id=45214164</id><title>Bulletproof host Stark Industries evades EU sanctions</title><updated>2025-09-12T13:25:30.141676+00:00</updated><content>&lt;doc fingerprint="cf48e13f1a24a782"&gt;
  &lt;main&gt;
    &lt;p&gt;In May 2025, the European Union levied financial sanctions on the owners of Stark Industries Solutions Ltd., a bulletproof hosting provider that materialized two weeks before Russia invaded Ukraine and quickly became a top source of Kremlin-linked cyberattacks and disinformation campaigns. But new findings show those sanctions have done little to stop Stark from simply rebranding and transferring their assets to other corporate entities controlled by its original hosting providers.&lt;/p&gt;
    &lt;p&gt;Materializing just two weeks before Russia invaded Ukraine in 2022, Stark Industries Solutions became a frequent source of massive DDoS attacks, Russian-language proxy and VPN services, malware tied to Russia-backed hacking groups, and fake news. ISPs like Stark are called “bulletproof” providers when they cultivate a reputation for ignoring any abuse complaints or police inquiries about activity on their networks.&lt;/p&gt;
    &lt;p&gt;In May 2025, the European Union sanctioned one of Stark’s two main conduits to the larger Internet — Moldova-based PQ Hosting — as well as the company’s Moldovan owners Yuri and Ivan Neculiti. The EU Commission said the Neculiti brothers and PQ Hosting were linked to Russia’s hybrid warfare efforts.&lt;/p&gt;
    &lt;p&gt;But a new report from Recorded Future finds that just prior to the sanctions being announced, Stark rebranded to the[.]hosting, under control of the Dutch entity WorkTitans BV (AS209847) on June 24, 2025. The Neculiti brothers reportedly got a heads up roughly 12 days before the sanctions were announced, when Moldovan and EU media reported on the forthcoming inclusion of the Neculiti brothers in the sanctions package.&lt;/p&gt;
    &lt;p&gt;In response, the Neculiti brothers moved much of Stark’s considerable address space and other resources over to a new company in Moldova called PQ Hosting Plus S.R.L., an entity reportedly connected to the Neculiti brothers thanks to the re-use of a phone number from the original PQ Hosting.&lt;/p&gt;
    &lt;p&gt;“Although the majority of associated infrastructure remains attributable to Stark Industries, these changes likely reflect an attempt to obfuscate ownership and sustain hosting services under new legal and network entities,” Recorded Future observed.&lt;/p&gt;
    &lt;p&gt;Neither the Recorded Future report nor the May 2025 sanctions from the EU mentioned a second critical pillar of Stark’s network that KrebsOnSecurity identified in a May 2024 profile on the notorious bulletproof hoster: The Netherlands-based hosting provider MIRhosting.&lt;/p&gt;
    &lt;p&gt;MIRhosting is operated by 38-year old Andrey Nesterenko, whose personal website says he is an accomplished concert pianist who began performing publicly at a young age. DomainTools says mirhosting[.]com is registered to Mr. Nesterenko and to Innovation IT Solutions Corp, which lists addresses in London and in Nesterenko’s stated hometown of Nizhny Novgorod, Russia.&lt;/p&gt;
    &lt;p&gt;According to the book Inside Cyber Warfare by Jeffrey Carr, Innovation IT Solutions Corp. was responsible for hosting StopGeorgia[.]ru, a hacktivist website for organizing cyberattacks against Georgia that appeared at the same time Russian forces invaded the former Soviet nation in 2008. That conflict was thought to be the first war ever fought in which a notable cyberattack and an actual military engagement happened simultaneously.&lt;/p&gt;
    &lt;p&gt;Mr. Nesterenko did not respond to requests for comment. In May 2024, Mr. Nesterenko said he couldn’t verify whether StopGeorgia was ever a customer because they didn’t keep records going back that far. But he maintained that Stark Industries Solutions was merely one client of many, and claimed MIRhosting had not received any actionable complaints about abuse on Stark.&lt;/p&gt;
    &lt;p&gt;However, it appears that MIRhosting is once again the new home of Stark Industries, and that MIRhosting employees are managing both the[.]hosting and WorkTitans — the primary beneficiaries of Stark’s assets.&lt;/p&gt;
    &lt;p&gt;A copy of the incorporation documents for WorkTitans BV obtained from the Dutch Chamber of Commerce shows WorkTitans also does business under the names Misfits Media and and WT Hosting (considering Stark’s historical connection to Russian disinformation websites, “Misfits Media” is a bit on the nose).&lt;/p&gt;
    &lt;p&gt;The incorporation document says the company was formed in 2019 by a y.zinad@worktitans.nl. That email address corresponds to a LinkedIn account for a Youssef Zinad, who says their personal websites are worktitans[.]nl and custom-solution[.]nl. The profile also links to a website (etripleasims dot nl) that LinkedIn currently blocks as malicious. All of these websites are or were hosted at MIRhosting.&lt;/p&gt;
    &lt;p&gt;Although Mr. Zinad’s LinkedIn profile does not mention any employment at MIRhosting, virtually all of his LinkedIn posts over the past year have been reposts of advertisements for MIRhosting’s services.&lt;/p&gt;
    &lt;p&gt;A Google search for Youssef Zinad reveals multiple startup-tracking websites that list him as the founder of the[.]hosting, which censys.io finds is hosted by PQ Hosting Plus S.R.L.&lt;/p&gt;
    &lt;p&gt;The Dutch Chamber of Commerce document says WorkTitans’ sole shareholder is a company in Almere, Netherlands called Fezzy B.V. Who runs Fezzy? The phone number listed in a Google search for Fezzy B.V. — 31651079755 — also was used to register a Facebook profile for a Youssef Zinad from the same town, according to the breach tracking service Constella Intelligence.&lt;/p&gt;
    &lt;p&gt;In a series of email exchanges leading up to KrebsOnSecurity’s May 2024 deep dive on Stark, Mr. Nesterenko included Mr. Zinad in the message thread (youssef@mirhosting.com), referring to him as part of the company’s legal team. The Dutch website stagemarkt[.]nl lists Youssef Zinad as an official contact for MIRhosting’s offices in Almere. Mr. Zinad did not respond to requests for comment.&lt;/p&gt;
    &lt;p&gt;Given the above, it is difficult to argue with the Recorded Future report on Stark’s rebranding, which concluded that “the EU’s sanctioning of Stark Industries was largely ineffective, as affiliated infrastructure remained operational and services were rapidly re-established under new branding, with no significant or lasting disruption.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://krebsonsecurity.com/2025/09/bulletproof-host-stark-industries-evades-eu-sanctions/"/></entry><entry><id>https://news.ycombinator.com/item?id=45214670</id><title>Top model scores may be skewed by Git history leaks in SWE-bench</title><updated>2025-09-12T13:25:28.971846+00:00</updated><content>&lt;doc fingerprint="b7b29ad728b3bfb0"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 608&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;We've identified multiple loopholes with SWE Bench Verified where agents may look at future repository state (by querying it directly or through a variety of methods), and cases in which future repository state includes either solutions or detailed approaches to solving problems (commit messages and more).&lt;/p&gt;
    &lt;head rend="h3"&gt;Examples:&lt;/head&gt;
    &lt;p&gt;A trajectory with Claude 4 Sonnet, &lt;code&gt;Pytest-dev__pytest-6202&lt;/code&gt; (complete output here), the agent uses &lt;code&gt;git log --all&lt;/code&gt; which leaks future commits that directly fix the issue:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;&amp;lt;antml:parameter name="command"&amp;gt;cd /testbed &amp;amp;&amp;amp; git log --oneline --all | grep -i "bracket|parametrize|modpath" | head -10&amp;lt;/antml:parameter&amp;gt;&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The results of which directly reveal the fix:&lt;/p&gt;
    &lt;code&gt;    Fix incorrect result of getmodpath method.
diff --git a/src/_pytest/python.py b/src/_pytest/python.py
index b8b365ad3..734a92f9b 100644
--- a/src/_pytest/python.py
+++ b/src/_pytest/python.py
@@ -285,8 +285,7 @@ class PyobjMixin(PyobjContext):
                     break
             parts.append(name)
         parts.reverse()
-        s = ".".join(parts)
-        return s.replace(".[", "[")
+        return ".".join(parts)
&lt;/code&gt;
    &lt;p&gt;Qwen3-Coder 480B (&lt;code&gt;20250805-openhands-Qwen3-Coder-480B-A35B-Instruct&lt;/code&gt;) also has several cases of looking ahead: some examples include &lt;code&gt;django__django-13513&lt;/code&gt; (complete output here) uses &lt;code&gt;git log grep=[issue ID]&lt;/code&gt; which directly reveals the fix PR which is in the future repo state (future commits).&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Running command: cd /workspace/django__django__3.2 &amp;amp;&amp;amp; �[1m�[91mgit log�[0m --oneline --grep="31926" -i&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In another Qwen3-Coder trajectory, &lt;code&gt;Django__django-15572&lt;/code&gt;, (complete output here) where the model specifically finds the commit containing the fix: 62739b6e2630e37faa68a86a59fad135cc788cd7&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Command&lt;/p&gt;&lt;code&gt;cd /workspace/django__django__4.1 &amp;amp;&amp;amp; �[1m�[91mgit log�[0m --oneline --grep="33628" �[92m--all�[0m&lt;/code&gt;executed with exit code 0.&lt;/quote&gt;
    &lt;p&gt;There are other examples of leakage found in trajectories from GLM 4.5, Qwen3-Coder 30B (&lt;code&gt;20250805-openhands-Qwen3-Coder-30B-A3B-Instruct&lt;/code&gt;), and other models.&lt;/p&gt;
    &lt;p&gt;Mitigation will be to properly remove future repository state and any artifacts that contain information the agent could use (reflogs, branches, origins, tags, and more):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;remove origins (branch names can reveal information about fixes)&lt;/item&gt;
      &lt;item&gt;remove all branches &lt;code&gt;git log --all&lt;/code&gt;can be used to query them, plus branches that are tracking a remote origin might contain information about future commits even after a&lt;code&gt;git reset --hard&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;remove the reflog (&lt;code&gt;git reflog&lt;/code&gt;) can leak future commit messages that could detail approaches for solutions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The team (@felixkreuk, @UniverseFly, @jlko, @2dot71mily and others) will add more details as to findings here and below. We're still assessing broader impact on evaluations and understanding trajectories for sources of leakage.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/SWE-bench/SWE-bench/issues/465"/></entry><entry><id>https://news.ycombinator.com/item?id=45214908</id><title>Claude’s memory architecture is the opposite of ChatGPT’s</title><updated>2025-09-12T13:25:28.750828+00:00</updated><content>&lt;doc fingerprint="3f0503daafb7c06e"&gt;
  &lt;main&gt;
    &lt;p&gt;Earlier this week, I dissected ChatGPT's memory system. Since then, I've been doing the same for Claude and realized something remarkable: these two leading AI assistants have built completely opposite memory systems.&lt;/p&gt;
    &lt;p&gt;In this post, I'll start by breaking down exactly how Claude's memory works—what it stores and how it retrieves information. Then we'll get to the interesting stuff. Why these architectures diverge so dramatically, what that tells us about who uses each assistant and the philosophies driving each product's development, and just how vast the AI memory design space really is.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;Claude's memory system has two fundamental characteristics. First, it starts every conversation with a blank slate, without any preloaded user profiles or conversation history. Memory only activates when you explicitly invoke it. Second, Claude recalls by only referring to your raw conversation history. There are no AI-generated summaries or compressed profiles—just real-time searches through your actual past chats.&lt;/p&gt;
    &lt;p&gt;When Claude detects memory invocation through phrases like "what did we discuss about," "continue where we left off," or "remember when we talked about," it deploys two retrieval tools that work like web search or code execution—you see them activate in real-time and wait while Claude searches through your history. Once the search completes, Claude synthesizes the retrieved conversations to answer your question or continue the discussion.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conversation Search&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;conversation_search&lt;/code&gt; tool helps with keyword and topic-based searches across your entire conversation history. When I asked "Hey, can you recall our past conversations about Chandni Chowk?" (a historic neighborhood in Delhi), Claude found 9 relevant conversations—from when I explored its founding by Princess Jahanara Begum in 1650 to my queries about the best galouti kebabs at Karim's and stuffed parathas at Paranthe Wali Gali. Claude synthesized these scattered discussions into a coherent summary of my Chandni Chowk explorations.&lt;/p&gt;
    &lt;p&gt;When you ask about multiple topics, Claude runs separate searches sequentially. In my past job as a crypto researcher, I used Claude extensively as an editor. When I asked "Tell me all the conversations we've had about either Michelangelo or Chainflip or Solana," Claude ran three separate searches—one for my Michelangelo analogies for neural networks, another for Chainflip's cross-chain protocol work, and a third for Solana's technical architecture. It found 22 conversations across these searches and delivered a unified response with direct links to each chat.&lt;/p&gt;
    &lt;code&gt;{
  "description": "Search through past user conversations to find relevant context and information",
  "name": "conversation_search",
  "parameters": {
    "properties": {
      "max_results": {
        "default": 5,
        "description": "The number of results to return, between 1-10",
        "exclusiveMinimum": 0,
        "maximum": 10,
        "title": "Max Results",
        "type": "integer"
      },
      "query": {
        "description": "The keywords to search with",
        "title": "Query",
        "type": "string"
      }
    },
    "required": ["query"],
    "title": "ConversationSearchInput",
    "type": "object"
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Temporal Chat Retrieval&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;recent_chats&lt;/code&gt; tool provides time-based access to your conversation history. When I asked "Can you tell me what we spoke about in the last 10 conversations?" Claude retrieved my most recent chats chronologically and gave me a summary of my recent usage.&lt;/p&gt;
    &lt;p&gt;The tool also handles specific timeframes. When I asked "What did we discuss in the last week of November 2024?" Claude retrieved 16 conversations from that exact period.&lt;/p&gt;
    &lt;code&gt;{
  "description": "Retrieve recent chat conversations with customizable sort order (chronological or reverse chronological), optional pagination using 'before' and 'after' datetime filters, and project filtering",
  "name": "recent_chats",
  "parameters": {
    "properties": {
      "after": {
        "anyOf": [{"format": "date-time", "type": "string"}, {"type": "null"}],
        "default": null,
        "description": "Return chats updated after this datetime (ISO format, for cursor-based pagination)",
        "title": "After"
      },
      "before": {
        "anyOf": [{"format": "date-time", "type": "string"}, {"type": "null"}],
        "default": null,
        "description": "Return chats updated before this datetime (ISO format, for cursor-based pagination)",
        "title": "Before"
      },
      "n": {
        "default": 3,
        "description": "The number of recent chats to return, between 1-20",
        "exclusiveMinimum": 0,
        "maximum": 20,
        "title": "N",
        "type": "integer"
      },
      "sort_order": {
        "default": "desc",
        "description": "Sort order for results: 'asc' for chronological, 'desc' for reverse chronological (default)",
        "pattern": "^(asc|desc)$",
        "title": "Sort Order",
        "type": "string"
      }
    },
    "title": "GetRecentChatsInput",
    "type": "object"
  }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;ChatGPT vs Claude&lt;/head&gt;
    &lt;p&gt;A year ago, ChatGPT and Claude's assistant apps matched each other feature for feature—multiple models, file attachments, projects. Since then, their paths have diverged dramatically. ChatGPT has evolved into a mass-market consumer product, while Claude has deliberately chosen a different trajectory. Anthropic CPO Mike Krieger has acknowledged that OpenAI had "caught lightning in a bottle" with consumer adoption. Instead of chasing that market, Anthropic is focusing on what Claude does best: developer tools, coding, and professional workflows.&lt;/p&gt;
    &lt;p&gt;Memory implementation perfectly reflects this divergence.&lt;/p&gt;
    &lt;p&gt;ChatGPT's hundreds of millions of weekly active users come from all backgrounds—students, parents, hobbyists—who just want a product that works and remembers them without thinking about the mechanics. Every memory component loads automatically, creating instant personalization with zero wait time. The system builds detailed user profiles, learning preferences and patterns that could eventually power targeted features or monetization. It's the classic consumer tech playbook: make it magical, make it sticky, figure out different ways to monetize later.&lt;/p&gt;
    &lt;p&gt;Claude's users represent a different demographic entirely. Anthropic's more technical users inherently understand how LLMs work. They're comfortable with explicit control at every level. Just as they choose when to trigger web search or enable extended thinking, they decide when memory is worth invoking. They understand that memory calls add latency, but they make that tradeoff deliberately. Memory becomes just another tool in their arsenal, not an always-on feature. This audience doesn't need or want extensive profiling—they need a powerful, predictable tool for professional work. Not to mention, they're also more privacy-conscious.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Memory Design Space&lt;/head&gt;
    &lt;p&gt;It still amazes me that ChatGPT and Claude—the two top AI assistants—have built completely opposite memory systems. This only goes to show that memory in AI has a massive design space with no right answer or one-size-fits-all technique. You have to work backwards from who your users are and what they need, then build from first principles accordingly.&lt;/p&gt;
    &lt;p&gt;We're in uncharted territory. These tools are less than three years old, and nobody knows what happens when someone uses the same AI assistant for a decade. How much should it remember? How should it handle years of accumulated context? Meanwhile, we're seeing a Cambrian explosion of AI apps, each experimenting with their own memory approach, while the underlying models get more powerful every week. There's no playbook, no settled best practices—just everyone trying different things and seeing what sticks.&lt;/p&gt;
    &lt;p&gt;The more I dive into memory, the more fascinated I get. Over the coming weeks, I'll be dissecting different architectures, analyzing new approaches, and following the latest research. Subscribe below if you want updates as this space unfolds.&lt;/p&gt;
    &lt;p&gt;Update: Hours after publishing this, Anthropic announced a new memory feature for Team and Enterprise accounts that looks much closer to ChatGPT's approach. Haven't tried it yet (not available on Max plan), but will share an update once I do.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.shloked.com/writing/claude-memory"/></entry><entry><id>https://news.ycombinator.com/item?id=45214933</id><title>Rails on SQLite: new ways to cause outages</title><updated>2025-09-12T13:25:28.513408+00:00</updated><content>&lt;doc fingerprint="e7ca9cfeb9a65afb"&gt;
  &lt;main&gt;
    &lt;p&gt;11 Sep 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Rails on SQLite: exciting new ways to cause outages&lt;/head&gt;
    &lt;p&gt;This post was originally given as a talk for Friendly.rb. The slides are also available.&lt;/p&gt;
    &lt;p&gt;Between Litestack and the Rails 8 trifecta of Solid Cable, Solid Cache, and Solid Queue, it’s easier than ever to spin up a Rails app that doesn’t need a database service, or a redis service, or a file storage service. It’s great to simplify things, but even after 20 years of deploying Rails apps I was still caught out by some of the ways things are different.&lt;/p&gt;
    &lt;p&gt;Based on what happened when I built a new side project in Rails on SQLite, we’ll cover what’s different, what’s new, and several ways that you can knock your site offline or even destroy your entire production database. As we go, we’ll also talk about the advantages of using SQLite, and how those differences can help you.&lt;/p&gt;
    &lt;p&gt;So who am I, how did I learn these things, and why should you listen to me? I’m André Arko, better known on the internet as @indirect. A long time ago, I helped create Bundler, and I’ve been the OSS team lead for RubyGems and Bundler for more than a decade at this point.&lt;/p&gt;
    &lt;p&gt;I work at Spinel Cooperative, a collective of Ruby open source maintainers building rv, the Ruby language manager that can install Ruby in one second flat. We offer retainers for unlimited access to core team experts from Bundler, Rails, Hotwire, and more, who can answer your questions and solve your problems.&lt;/p&gt;
    &lt;p&gt;I’ve been deploying Rails applications to production since 2004, and most relevantly for this particular talk, I built a service called feedyour.email. Feed Your Email turns email subscriptions into RSS feeds that you can subscribe to in your feed reader. There is no signup, there are no accounts, you just go to the website and push a button to get an email address and a feed URL. Emails sent to that address will show up in that feed.&lt;/p&gt;
    &lt;p&gt;Feed Your Email is only possible as a service because of SQLite—if I had to maintain three Postgres instances and a couple of web instances and a couple of worker instances, I would have felt like it was too much hassle and cost too much money, and given up. SQLite reduced the complexity of building and deploying this service to the point where I was able to do it casually, for fun, and keep it running for everyone while feeling like it was worth it just for my own personal use.&lt;/p&gt;
    &lt;p&gt;This app serves about a million requests per month. That number sounds kind of big, but let’s do the math: 24 requests per minute, or one every 2.6 seconds. We can definitely serve at least one request every 2.6 seconds off of our Rails app, even on a small digital ocean droplet or a small cloud VM. I run my service on Fly.io, and hosting costs about USD$14 per month.&lt;/p&gt;
    &lt;p&gt;It has bonus features like a web view so you can share individual emails as a link without giving away your secret email address or letting anyone see the other emails in that feed, and it also has full-text search if you’re trying to find a particular email. That full-text search is a feature of SQLite, which brings us back to the topic of this talk. How did SQLite help? Let’s take a look.&lt;/p&gt;
    &lt;p&gt;The biggest fundamental difference, where almost every other difference comes from, is that SQLite is built in to your web server process. The reason for the “lite” in the name is that it doesn’t run a separate process, it doesn’t listen on a port or a socket, and you can’t connect to it. All the data is just in a single file, and your web process reads and writes that file when needed. This is awesome: you can’t have database connection errors anymore.&lt;/p&gt;
    &lt;p&gt;There’s a small issue with giving the web process its own database in a local file, though. If you deploy your app as usual, your production database can disappear at any time! Heroku destroys dynos every 24 hours, including all files. Fly.io loses the files in a container anytime they restart. In a world of containers, it’s incredibly easy to miss that your new SQLite database is on an ephemeral filesystem and will disappear along with the Puma process in your web container.&lt;/p&gt;
    &lt;p&gt;That leads us to the first and most important rule of using SQLite: put your database files in persistent storage. On AWS that means EBS, on Fly.io that means Volumes, but wherever you’re hosting, make sure that you can keep your database files across restarts (and ideally have automatic snapshots).&lt;/p&gt;
    &lt;p&gt;Now that your database won’t disappear at any moment, let’s talk about what it means to put all the data in a single file. You probably use &lt;code&gt;Rails.cache&lt;/code&gt; to store temporary data—that’s in a single SQLite file now, too. You also probably use &lt;code&gt;ActiveJob&lt;/code&gt; to send emails or do slower work in the background. All of those jobs are also in a single SQLite file now. By default, it’s the same file!&lt;/p&gt;
    &lt;p&gt;Putting everything in the same SQLite file makes everything very easy. You just need to keep track of that one file, and you’ll know that your model data, your caches, and your jobs will all be safe.&lt;/p&gt;
    &lt;p&gt;SQLite being in-process raises a new problem, though… what if your background job needs to update a model? You might be used to deploying your background workers in a separate container, so you can scale them as needed. That’s not going to fly anymore, because your background workers need to be able to read and write that same single file that your web server is reading and writing.&lt;/p&gt;
    &lt;p&gt;Since your database is now in just one file, you have two options. You can run your workers in a second process inside the same VM or container as the web process, or you can run your workers in threads inside the web process.&lt;/p&gt;
    &lt;p&gt;If this is a smallish application, doing a million requests per month or less, you’ll be absolutely fine putting your background jobs in threads. As a bonus, putting background jobs in threads can almost halve the amount of RAM you need because a single Rails process is handling both web and job requests.&lt;/p&gt;
    &lt;p&gt;If you really want to scale up your application, though, you’ll need to do what gets called “vertical” scaling rather than the traditional “horizontal” scaling. You can’t add more VMs, because other VMs won’t be able to see your database file. Instead, you need bigger and bigger single servers, with more and more CPU cores and RAM. That’s definitely possible, though. There are servers nowadays with 128 cores, or even more, and terabytes of RAM. Unfortunately, once you have scaled your wildly popular application vertically to the moon, you’ll discover the biggest limitation of SQLite: it’s just one file on disk.&lt;/p&gt;
    &lt;p&gt;If you have dozens processes and hundreds of threads in web servers and background job workers, all trying to write into this one database file at the same time, there’s probably going to be a lot of contention. By default, SQLite uses the filesystem to take out a lock on the entire database for each transaction. Holding the lock, it executes any read and write queries, commits, and then releases the lock. Then the next process can take the filesystem lock and do the same thing.&lt;/p&gt;
    &lt;p&gt;This can create quite the queue if even read-only queries have to wait in line and happen one at a time (because if they didn’t have the lock, some writer might sneak in and change the data mid-read!). To (partially) address this problem, SQLite offers a middle ground in the form of a Write-Ahead Log. The WAL log is an append-only file where any database writes can be written, one at a time. Then, a SQLite-controlled process copies those write instructions into the actual database file between reads. In the meantime, there can be as many readers as you want, because writes don’t have to block reads, and many reads from the same file at once are quite safe.&lt;/p&gt;
    &lt;p&gt;This solves the problem with only allowing one single read or write at a time, but it definitely has a cost. The database isn’t just one file anymore, it’s now a series of files, and you need to back them up and restore them together if you want to make sure you haven’t lost any data. Hopefully that’s not too much trouble, but it is definitely something to be aware of while planning your backup and disaster recovery strategy.&lt;/p&gt;
    &lt;p&gt;There’s one other approach worth calling out at this point, and that strategy is deliberately using multiple SQLite database files. If you are putting your not just your model data, but also your Rails cache, and also your background jobs, and maybe also your uploaded files all together into a single SQLite database file, your different use-cases can start to step on one another’s toes. For example, if you go to queue a few thousand jobs, any writes from your web requests will end up in the writer queue behind that pile of jobs in line to be written.&lt;/p&gt;
    &lt;p&gt;Creating a separate SQLite file per system, or per access pattern, can help a lot with this. In Rails, the most common splits are separate SQLite databases for ActiveRecord, for the Rails cache, for background jobs, and for ActionCable. Depending on your application, it might also make sense to put your ActiveStorage blobs into a SQLite database or into the same filesystem that you are already backing up, as well. There’s a lot of complexity and overhead involved in setting up S3 buckets with the correct permissions and getting files into and out of them, and you might just want to skip all of that in your new, simplified Rails config.&lt;/p&gt;
    &lt;p&gt;Taking this approach to an extreme might even involve sharding your model data across many database files. The most extreme example of this that I’ve heard of was an application that chose to shard their model data across one SQLite database file per customer. That meant every new signup created a new SQLite database file on disk, which is in some ways absurd, but it also meant that every individual user had the full power and speed of SQLite available to them. It’s hard to have read or write contention when every user gets their own separate database!&lt;/p&gt;
    &lt;p&gt;So now that we’ve covered vertically scaling the Rails server itself, let’s talk about the other implications of your application running on exactly one server. The downside to there being just one server is that if that server goes down, your entire app is down. No degraded service, no slower than usual application, just… no application at all.&lt;/p&gt;
    &lt;p&gt;If you’re running in a container, it’s impossible to deploy without downtime because only one container can ever have the volume with the database mounted. The old container has to stop before the new container can start. If you’re running in a VM, you might be able to deploy without downtime by running a local reverse proxy and more than one web server process, and restarting those web server processes one at a time rather than all at once. Welcome to how we used to do things in the 2000s, and my apologies.&lt;/p&gt;
    &lt;p&gt;That said, some of the implications of only one server are good: if there’s only one, it’s pretty easy to run status checks, and it’s pretty easy to troubleshoot. You don’t need to debug connections between load balancers and dozens of web servers and database servers and redis servers and file storage servers, you just need to debug the one server. That can definitely make your job easier!&lt;/p&gt;
    &lt;p&gt;Another implication of having just one single server: there is only one place for network requests to go. As I alluded to a moment ago, the only kind of load balancing that you can do is by running a local proxy and adding multiple separate processes as backends. The server itself is only going to have one IP address and one port where it can be reached, and there’s a certain amount of scale where that one IP and one port is going to become limiting. The good news is that you probably won’t hit that scale, and if you do, you’ll probably want to stop using SQLite anyway.&lt;/p&gt;
    &lt;p&gt;If you ever want to try switching towards or away from SQLite, the sequel gem has the amazing ability to read from one database and write into another, doing a full database copy while respecting all the quirks and limitations of each database. If you want to move from Mysql or Postgres over to SQLite, or if you ever want to load a SQLite database into Mysql or Postgres, I highly recommend it. The duckdb command line tool also has excellent cross-database capabilities, and is the next thing I would try if sequel wasn’t working for me for some reason.&lt;/p&gt;
    &lt;p&gt;There’s one more limitation that we need to consider that falls out of there only being one server: your app can only run in one geographic location. Some applications can benefit from adding additional web processes (or even database read replicas) spread out closer to end users, and that’s not possible if you are limited to a maximum of one server total for your entire application.&lt;/p&gt;
    &lt;p&gt;That said, there’s nothing stopping you from using the more usual kind of CDN-based global distribution. If your application has a decent amount of static or cacheable content, you can at least still set the cache-control headers and run the app behind Fastly or Cloudlfare.&lt;/p&gt;
    &lt;p&gt;Before we wrap up, I want to make sure to cover the various backup and replication options available to you while using SQLite for your application. The absolute all-star of SQLite backup and replication is called Litestream. It’s available as a gem, and can be used as easily as setting a few environment variables and using the &lt;code&gt;litestream&lt;/code&gt; command provided by the gem to wrap your puma or other web server.&lt;/p&gt;
    &lt;p&gt;What litestream does is fairly simple: it forwards a copy of each entry added to the write-ahead log over to any S3-compatible file store — you might even say that it streams your data in a light way. If you ever have a catastrophe, and your database file gets deleted or corrupted, the bucket will have a full copy of the WAL that you can replay to restore your database back to where it was when the server stopped working.&lt;/p&gt;
    &lt;p&gt;On AWS, this still means setting up an S3 bucket and setting the right env vars, but at least you don’t need to deal with the bucket having public access, or setting up signed uploads, or any of the other things that make S3 a huge pain. You just need a private bucket and read/write credentials, and you’re good to go. If you’re using fly.io, you don’t even have to set the env vars yourself! They are set automatically by the command that creates the S3-compatible bucket on Tigris.&lt;/p&gt;
    &lt;p&gt;There’s one last thing that you can try using if you’re feeling especially adventurous, LiteFS. LiteFS is a fascinating software achievement, offering full Mysql or Postgres-style replication for multiple SQLite databases running in many locations. The completely deranged trick that they use to do this is creating an entire software filesystem using FUSE, and then putting the SQLite database inside that filesystem. This gives them access to every filesystem read and write call made by your application, and allows them to create their own operations that are then sent to every other member of the cluster to be applied.&lt;/p&gt;
    &lt;p&gt;This kind of setup comes with a lot of caveats. The biggest one is the usual distributed systems kind of caveat. You’ll have stale reads where some users will see old data, and if the primary crashes you might lose some data. If you’re okay with the tradeoffs of a distributed system (and you’re okay with the idea of all of your database reads and writes going through a FUSE filesystem that might be adding extra bugs), LiteFS offers a version of the ultimate web application dream.&lt;/p&gt;
    &lt;p&gt;In the dream SQLite plus LiteFS world, you have all the advantages of SQLite and all the advantages of a fully replicated multi-writer database setup. Any individual server can go down without causing any downtime for the application as a whole, and every user has a full copy of the application and all its data, running extremely close to them.&lt;/p&gt;
    &lt;p&gt;I haven’t built that perfect system yet, but it feels more attainable than it ever has before thanks to SQLite.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://andre.arko.net/2025/09/11/rails-on-sqlite-exciting-new-ways-to-cause-outages/"/></entry><entry><id>https://news.ycombinator.com/item?id=45217269</id><title>Why our website looks like an operating system</title><updated>2025-09-12T13:25:28.347349+00:00</updated><content>&lt;doc fingerprint="340a00141d3d6a87"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why our website looks like an operating system&lt;/head&gt;
    &lt;p&gt;Sep 10, 2025&lt;/p&gt;
    &lt;p&gt;I have a problem with many large, technical websites.&lt;/p&gt;
    &lt;p&gt;Often times, I’ll want to refer to different pages at the same time. So I’ll &lt;code&gt;CMD&lt;/code&gt; + click “a couple times” while browsing around and before I know it, I have 12 new tabs open – all indistinguishable from each other because they share the same favicon.&lt;/p&gt;
    &lt;p&gt;PostHog.com has the same problem – especially as the site has grown from supporting a handful of paid products to over a dozen.&lt;/p&gt;
    &lt;p&gt;As I looked for ways to solve this explosion of pages, I started to question many of the typical patterns that marketing &amp;amp; docs websites have today.&lt;/p&gt;
    &lt;p&gt;Long-form scrolling. Oversized footers. Absurd whitespace.&lt;/p&gt;
    &lt;p&gt;These website encourage scrolling, but just to get people to the bottom of the page? And then what?&lt;/p&gt;
    &lt;p&gt;Why are we doing this? What if we just made better ways to consume content?&lt;/p&gt;
    &lt;p&gt;That’s the idea behind the new PostHog.com. You can multitask, open a few articles simultaneously, and move them around as you please. If anything there's a whitespace deficiency, and your fingers will be jealous you're not scrolling with them as much (because you're so engaged with our content).&lt;/p&gt;
    &lt;p&gt;It has window snapping, keyboard shortcuts, and a bookmark app. It works as well as you’d expect an operating system to work in a browser.&lt;/p&gt;
    &lt;p&gt;You can be reading the latest newsletter from Product for Engineers while watching a demo video in the corner and also playing Hedgehog Mode, the game.&lt;/p&gt;
    &lt;p&gt;I’ll be the first to admit it – an OS interface for a “website” is initially a jarring experience. I felt this as I built it. The human brain expects certain patterns within the confines of a browser viewport, and when it doesn’t get that assurance, it revolts.&lt;/p&gt;
    &lt;p&gt;But the more I used the new site, the more I started to like it. And the experience was the same for colleagues. And now I can’t imagine using anything else.&lt;/p&gt;
    &lt;p&gt;I had a lot of fun in building it with Eli Kinsey. Throughout the site you’ll find:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Windows File Explorer clone that also acts as the UI for our merch store&lt;/item&gt;
      &lt;item&gt;Product pages that resemble PowerPoint presentations&lt;/item&gt;
      &lt;item&gt;A document editor where you can actually edit content&lt;/item&gt;
      &lt;item&gt;Forums designed to look like you’re reading newsgroups in Outlook Express&lt;/item&gt;
      &lt;item&gt;A QuickTime clone&lt;/item&gt;
      &lt;item&gt;A lot of pages you’d expect to be well-designed that are… just formatted as spreadsheets&lt;/item&gt;
      &lt;item&gt;A screensaver and a library of desktop backgrounds&lt;/item&gt;
      &lt;item&gt;A plethora of keyboard shortcuts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It was also an interesting learning curve for me in figuring out how to organize five years worth of content while making it scalable for the future. Some of the technical highlights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Separation of visual layer from content&lt;list rend="ul"&gt;&lt;item&gt;All product pages are now powered from JSON files (example). This means that JSON dictates page layouts, content presentation, feature-level competitor comparison charts, and more. It also contains an array of screenshots used in various places (both in light and dark mode, of course).&lt;/item&gt;&lt;item&gt;Eventually this will move to a repository that’s shared with the PostHog app, so all the information is powered from the same source.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Skinning a site with themes and color schemes&lt;list rend="ul"&gt;&lt;item&gt;How do you maintain light and dark mode, along with themes across a handful of accent variations (primary, secondary, tertiary) in a way that all play well together? (I found this out, and I’ll write about it sometime!)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;A reference customer database&lt;list rend="ul"&gt;&lt;item&gt;I’ve created a single customer record in code that contains: a) which products they use, b) quotes from specific people about individual products c) SVG logos that work in light and dark mode&lt;/item&gt;&lt;item&gt;This means that any quote can be presented on any page for any product without having to be hard-coded. It pulls in their name and photo, quote, and company logo, and can be filtered in reference to a specific product.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a lot of this site, I was designing it while I was building UIs in Typescript and Tailwind. (The entire site is just a Git branch off our current site – it's all technically the same codebase. I just merged along the way over the last few months.)&lt;/p&gt;
    &lt;p&gt;Prototyping in a production-level environment was a great way to ideate and develop features along the way – stuff I never would have built if I were just going off of mockups. I did find myself popping open Balsamiq while I was building – but just long enough to flesh out some ideas.&lt;/p&gt;
    &lt;p&gt;So how will this pan out? Well, we’re about to find out. This feels like an early MVP – there’s a ton of stuff to improve upon from here.&lt;/p&gt;
    &lt;p&gt;But in the meantime, I hope you enjoy the new PostHog.com. Be curious, click around, and have some fun. I hope you enjoy your time here as much as we enjoyed building it.&lt;/p&gt;
    &lt;p&gt;If you're curious, read more about how the site technically works.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://posthog.com/blog/why-os"/></entry><entry><id>https://news.ycombinator.com/item?id=45217372</id><title>Toddlerbot: Open-Source Humanoid Robot</title><updated>2025-09-12T13:25:28.237193+00:00</updated><content>&lt;doc fingerprint="ff5dd38caf327400"&gt;
  &lt;main&gt;
    &lt;p&gt;TL;DR: ToddlerBot is a low-cost, open-source humanoid robot platform designed for scalable policy learning and research in robotics and AI.&lt;/p&gt;
    &lt;p&gt;See CHANGELOG.md for details. 1x Speed unless otherwise noted.&lt;/p&gt;
    &lt;p&gt;Cartwheel: Toddy successfully performs a highly dynamic cartwheel, showcasing its agility and balance capabilities.&lt;/p&gt;
    &lt;p&gt;Cartwheel Failures: With naive DeepMimic and domain randomization, the success rate is still low. But the good news is that Toddy is extremely robust—it almost never breaks. Even if it breaks, it is super easy to fix.&lt;/p&gt;
    &lt;p&gt;Crawling: Toddy can crawl like a toddler, using its arms and legs in a coordinated manner.&lt;/p&gt;
    &lt;p&gt;Walking Faster: Toddy can walk faster in all directions (up to 0.25m/s) and rotate faster in place (up to 1 rad/s).&lt;/p&gt;
    &lt;p&gt;VR Teleoperation: Real-time VR teleoperation with Meta quest 2.&lt;/p&gt;
    &lt;p&gt;Foundation Stereo Depth On-board (Jetson Orin NX 16GB) real-time (10Hz) depth estimation from stereo fisheye cameras with Foundation Stereo.&lt;/p&gt;
    &lt;p&gt;We present ToddleBot's mechatronic design in the figure below. We highlight ToddlerBot's 30 active DoFs with orange markers: 7 DoFs per arm, 6 DoFs per leg, a 2-DoF neck, and a 2-DoF waist. Green markers indicate two end-effector designs—a compliant palm and a parallel-jaw gripper. Purple markers denote the sensor and electronics layout with exploded views, featuring two fisheye cameras, a speaker, two microphones, an IMU, and a Jetson Orin NX computer.&lt;/p&gt;
    &lt;p&gt;1x Speed unless otherwise noted.&lt;/p&gt;
    &lt;p&gt;Arm Span Test: With a torso volume of 13x9x12 cm³, ToddlerBot successfully grasps objects 14 times its torso volume (27x24x31 cm³) using its compliant palm gripper.&lt;/p&gt;
    &lt;p&gt;Payload Test: ToddlerBot lifts 1,484 g (40% of its total weight) while maintaining balance. To determine the limit, screws are incrementally added to a 3D-printed gripping cup until it falls.&lt;/p&gt;
    &lt;p&gt;Endurance Test: Running a walking RL policy, ToddlerBot lasts 19 minutes before overheating affects stability. It withstands up to 7 falls before breaking, but repairs take only 21 minutes of 3D printing and 14 minutes of assembly for full restoration.&lt;/p&gt;
    &lt;p&gt;Conversation and Push-ups: This is achieved by integrating OpenAI's Realtime API with GPT-4o for speech-to-text and text-to-speech. Push-ups are zero-shot sim-to-real transfer of open-loop keyframe animation.&lt;/p&gt;
    &lt;p&gt;Pull-ups: We use an AprilTag to help ToddlerBot accurately locate the horizontal bar. The rest is a zero-shot sim-to-real transfer of open-loop keyframe animation.&lt;/p&gt;
    &lt;p&gt;Omnidirectional Walking: ToddlerBot achieves omnidirectional walking with RL and zero-shot sim-to-real.&lt;/p&gt;
    &lt;p&gt;Bimanual Manipulation: This is an RGB-based diffusion policy trained with 60 demonstrations.&lt;/p&gt;
    &lt;p&gt;Full-body Manipulation: This is also an RGB-based diffusion policy trained with 60 demonstrations.&lt;/p&gt;
    &lt;p&gt;Skill Chaining: ToddlerBot first executes a diffusion policy to grasp the handle, while maintaining that pose, switched to the RL policy to push the wagon forward.&lt;/p&gt;
    &lt;p&gt;Manipulation Policy Zero-Shot Transfer: We successfully transfer manipulation policy trained with data collected on Toddlerbot instance to another.&lt;/p&gt;
    &lt;p&gt;Two-Instance Collaboration Task: To demonstrate their equivalent performance of two Toddlerbot Instances, both robots collaborate on a long-horizon room tidying task.&lt;/p&gt;
    &lt;p&gt;Easy-to-Build Test: We show that ToddlerBot is easy to build with fully open-source assembly manuals and videos.&lt;/p&gt;
    &lt;p&gt;Complete the CAPTCHA to reveal the invite links.&lt;/p&gt;
    &lt;code&gt;@article{shi2025toddlerbot,
  title={ToddlerBot: Open-Source ML-Compatible Humanoid Platform for Loco-Manipulation},
  author={Shi, Haochen and Wang, Weizhuo and Song, Shuran and Liu, C. Karen},
  journal={arXiv preprint arXiv:2502.00893},
  year={2025}
}&lt;/code&gt;
    &lt;p&gt;The authors would like to express their gratitude to Kaizhe Hu for assembling the second instance of ToddlerBot and assisting with keyframe animation and demo recording. We also extend our thanks to Huy Ha, Yen-Jen Wang, Pei Xu, and Yifan Hou for their insightful discussions on locomotion, and to Sirui Chen, Chen Wang, and Yunfan Jiang for valuable input on manipulation policy deployment. We are grateful to Albert Wu for his guidance on mathematical formulation and notation. Additionally, we thank João Pedro Araújo for his assistance with the motion capture system. Finally, we appreciate the helpful discussions from all members of TML and REALab. This work was supported by National Science Foundation NSF-FRR-2153854, NSF-2143601, NSF-2037101, Sloan Fellowship, Stanford Institute for Human-Centered Artificial Intelligence, and Stanford Wu Tsai Human Performance Alliance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://toddlerbot.github.io/"/></entry><entry><id>https://news.ycombinator.com/item?id=45217415</id><title>Float Exposed</title><updated>2025-09-12T13:25:28.089941+00:00</updated><content>&lt;doc fingerprint="456d4d4bb87212d4"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt; halfbfloatfloatdouble &lt;/p&gt;
      &lt;div&gt;
        &lt;p&gt;Position within SignificandâExponent Range&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt;Evaluation in Base-2&lt;/p&gt;
      &lt;p&gt;(−12)2×102(2 − 2)×.2 &lt;/p&gt;
      &lt;p&gt;Evaluation in Base-10&lt;/p&gt;
      &lt;p&gt;×2×. &lt;/p&gt;
      &lt;p&gt;Exact Base-10 Value&lt;/p&gt;
      &lt;div&gt;
        &lt;p&gt;Delta to Next/Previous Representable Value&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Delta to Next Representable Value&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Delta to Previous Representable Value&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://float.exposed/"/></entry><entry><id>https://news.ycombinator.com/item?id=45218111</id><title>Debian 13, Postgres, and the US time zones</title><updated>2025-09-12T13:23:15.526937+00:00</updated><link href="https://rachelbythebay.com/w/2025/09/11/debtz/"/></entry><entry><id>https://news.ycombinator.com/item?id=45219228</id><title>Qwen3-Next</title><updated>2025-09-12T13:23:15.015057+00:00</updated><link href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list"/></entry><entry><id>https://news.ycombinator.com/item?id=45220069</id><title>Show HN: I made a generative online drum machine with ClojureScript</title><updated>2025-09-12T13:23:14.338580+00:00</updated><link href="https://dopeloop.ai/beat-maker/"/></entry><entry><id>https://news.ycombinator.com/item?id=45220121</id><title>Examples from The LaTeX Companion book (3rd edition)</title><updated>2025-09-12T13:22:58.822255+00:00</updated><content>&lt;doc fingerprint="2be3413d43f3dcbb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;tlc3-examples – All examples from “The LaTeX Companion”, third edition&lt;/head&gt;
    &lt;p&gt;The PDFs (as used with spotcolor and trimming) and sources for all examples from the third edition (Parts I+II), together with necessary supporting files. The edition is published by Addison-Wesley, 2023, ISBN-13: 978-0-13-816648-9, ISBN-10: 0-13-816648-X (bundle of Part I &amp;amp; II).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Sources&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/info/examples/tlc3&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Documentation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Bug tracker&lt;/cell&gt;
        &lt;cell&gt;https://github.com/FrankMittelbach/tlc3-examples/issues&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Repository&lt;/cell&gt;
        &lt;cell&gt;https://github.com/FrankMittelbach/tlc3-examples&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Licenses&lt;/cell&gt;
        &lt;cell&gt;The LaTeX Project Public License 1.3c&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Maintainer&lt;/cell&gt;
        &lt;cell&gt;Frank Mittelbach&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Contained in&lt;/cell&gt;
        &lt;cell&gt;TeX Live as tlc3-examples&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Topics&lt;/cell&gt;
        &lt;cell&gt;Book examples&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Download the contents of this package in one zip archive (82.4M).&lt;/p&gt;
    &lt;head rend="h2"&gt;Community Comments&lt;/head&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
    &lt;head rend="h2"&gt;Announcements&lt;/head&gt;
    &lt;head rend="h2"&gt;Suggestions&lt;/head&gt;
    &lt;p&gt;Maybe you are interested in the following packages as well.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ctan.org/pkg/tlc3-examples"/></entry><entry><id>https://news.ycombinator.com/item?id=45220656</id><title>Becoming the person who does the thing</title><updated>2025-09-12T13:22:58.661058+00:00</updated><content>&lt;doc fingerprint="3a08130fcd2f463d"&gt;
  &lt;main&gt;
    &lt;p&gt;It can be disorienting when our beliefs shift. The world we helped create no longer exists, and our role in it transforms too.&lt;/p&gt;
    &lt;p&gt;It can be unsettling, naturally. But that's kind of the point.&lt;/p&gt;
    &lt;p&gt;Looking back at times when I held certain beliefs—about how the world works, and what my role in this story is—it can feel less like a mod was installed and more like an entirely new operating system was swapped in.&lt;/p&gt;
    &lt;p&gt;Up until my late twenties, I could count the number of times I had been to the gym on one hand.&lt;/p&gt;
    &lt;p&gt;And worse, as a nerd, I was quietly proud of it. Why waste hours a week on something that hardly mattered? I have better things to do. I pitied the jocks who slaved away in the gym chasing vanity. What for? I don't need that. Who cares?&lt;/p&gt;
    &lt;p&gt;Like all childish thinking, it contained some truth. Physical fitness is less important than spiritual, emotional, and mental fitness; but it’s still important.&lt;/p&gt;
    &lt;p&gt;Even Paul, one of history’s most influential figures, with a worldview shaped by the utter centrality of spiritual health, said as much.&lt;/p&gt;
    &lt;p&gt;Your beliefs—and therefore approach to what a healthy life looks like—are foundational. It might sound obvious, but what you believe a "life well lived" looks like has a pretty transformative impact on both what life you end up building and how well lived it looks.&lt;/p&gt;
    &lt;p&gt;So if we’re a product of our beliefs and our most formative preconceptions are imposed by others, then where’s the hope?&lt;/p&gt;
    &lt;p&gt;We build, layer upon layer, and the layers laid first—now deeply buried within—are the ones we had the least say in.&lt;/p&gt;
    &lt;p&gt;We didn't pick them, but they shape everything we are and do. Bad luck, I guess.&lt;/p&gt;
    &lt;p&gt;Thankfully, we are dynamic beings. Old beliefs can peel off, and new ones take their place. Later layers can somehow seep deeper. Some recent beliefs can even become cornerstones.&lt;/p&gt;
    &lt;p&gt;For me, something shifted in my late twenties. Growing up I guess you could call it. I don’t remember the exact straw that broke the camel’s back, but a desire for change grew. I started working out.&lt;/p&gt;
    &lt;p&gt;It began slowly, but I began. Knee press-ups to start, later adding assisted pull-ups.&lt;/p&gt;
    &lt;p&gt;If anyone was watching, it would have looked stupid. A grown man barely able to push himself off the floor. But I showed up and put in my reps, day by day, week by week, in the privacy of my bedroom.&lt;/p&gt;
    &lt;p&gt;As the weeks and months passed, my strength grew.&lt;/p&gt;
    &lt;p&gt;Eventually, I graduated to full press-ups and pull-ups, no mods required.&lt;/p&gt;
    &lt;p&gt;Every small win reinforced the last and led me further away from who I used to be.&lt;/p&gt;
    &lt;p&gt;Fast forward almost to a decade and I feel a lot more friction not going to the gym than I do going. Cognitive dissonance is wonderful when it’s on your side, and it pops up whenever my healthy-Fred self-identity and actions diverge.&lt;/p&gt;
    &lt;p&gt;I'm far from a gym junkie—it hasn't become my life—but I go every weekday, 20 minutes a day. I arrive, do my workout, and leave, while most people are just getting started.&lt;/p&gt;
    &lt;p&gt;Our self-identity dictates everything, but it is not set in stone.&lt;/p&gt;
    &lt;p&gt;Changing our beliefs isn’t easy. Both those about the world around us and the world within. We can't simply will our way there and snap our fingers. We must journey. As with all great things, it's a process.&lt;/p&gt;
    &lt;p&gt;But it is possible. A well-trodden path is ahead for those who wish to walk it.&lt;/p&gt;
    &lt;p&gt;So how do you? How do you become the person who does the thing?&lt;/p&gt;
    &lt;p&gt;Earlier this week I spent a couple hours crafting my digital /shelf, a place where I can put the things that have impacted me the most up for all to see, so others can take them for themselves should they wish.&lt;/p&gt;
    &lt;p&gt;On it are these two quotes that have been living rent-free in my head from the beginning. Together, they create a twin-cog flywheel that cannot be stopped:&lt;/p&gt;
    &lt;quote&gt;“People like us do things like this” Seth Godin&lt;/quote&gt;
    &lt;quote&gt;“Every action you take is a vote for the type of person you wish to become” James Clear&lt;/quote&gt;
    &lt;p&gt;Your actions follow your self-beliefs.&lt;/p&gt;
    &lt;p&gt;If you identity as a failure, incapable of achievement, unfit, unlovable, destined to play a bit-part role in your own story, then by heck no matter how much willpower you put in to push that boulder up the hill, it will return to its place.&lt;/p&gt;
    &lt;p&gt;But there's a way through: every action you take is a vote for who you wish to become. Every day you wake up, look your old identity in the eye and say "thanks for your service, but you're not needed around here anymore," step forward and lean in, is a day your new identity is built.&lt;/p&gt;
    &lt;p&gt;It takes time. You have to actually want it. You have to choose to adopt a new mindset. Rome wasn't built in a day. But it comes, a little like how Hazel Grace Lancaster describes falling in love in The Fault In Our Stars: "slowly, and then all at once."&lt;/p&gt;
    &lt;p&gt;The path is there, should you choose.&lt;/p&gt;
    &lt;p&gt;Identify where your identity needs to shift. Then take a step. Cast today’s vote. Find your way through.&lt;/p&gt;
    &lt;p&gt;Do that day by day, then soon enough, your inner world will shift and recalibrate around the new reality you're co-creating.&lt;/p&gt;
    &lt;p&gt;Then one day you'll see it.&lt;/p&gt;
    &lt;p&gt;People like us really do things like this.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.fredrivett.com/2025/09/10/becoming-the-person-who-does-the-thing/"/></entry><entry><id>https://news.ycombinator.com/item?id=45220843</id><title>Astrophysics Source Code Library</title><updated>2025-09-12T13:22:58.186953+00:00</updated><content>&lt;doc fingerprint="eeb0eafba791d14b"&gt;
  &lt;main&gt;
    &lt;p&gt;The Astrophysics Source Code Library (ASCL) is a free online registry and repository for source codes of interest to astronomers and astrophysicists, including solar system astronomers, and lists codes that have been used in research that has appeared in, or been submitted to, peer-reviewed publications. The ASCL is indexed by the SAO/NASA Astrophysics Data System (ADS) and Web of Science and is citable by using the unique ascl ID assigned to each code. The ascl ID can be used to link to the code entry by prefacing the number with ascl.net (i.e., ascl.net/1201.001).&lt;/p&gt;
    &lt;p&gt;adstex automatically identifies all citation keys in a TeX source file and builds the corresponding bibliography file (.bib file) by fetching the reference information from NASA's Astrophysics Data System (ADS). adstex recognizes all variants of the cite commands in TeX, and works with various styles of citation keys, including arXiv IDs, DOIs, and ADS bibcodes. When a citation key is in the format of first-author name and year, adstex will query NASA's ADS and return a list of possible reference matches for the user to select the intended one. When a reference entry has updated information on NASA's ADS, adstex can detect such changes and fetch the new information and update the user's bibliography file. adstex supports any reference entry that is available on NASA's ADS, and allows the authors to write papers without manually searching for the bibliography entries.&lt;/p&gt;
    &lt;p&gt;IAR_Model fits unequally spaced time series from the Irregular Autoregressive (IAR). Available as Python and R functions, IAR_Model can generate observations for each process, compute the negative of the log likelihood of these process, fit each model to irregularly sampled data, and test the significance of the estimate.&lt;/p&gt;
    &lt;p&gt;fm4ar (flow matching for atmospheric retrievals) infers atmospheric properties of exoplanets from observed spectra. It uses flow matching posterior estimation (FMPE) for its machine learning (ML) approach to atmospheric retrieval; this approach provides many of the advantages of neural posterior estimation (NPE) while also providing greater architectural flexibility and scalability. The package uses importance sampling (IS) to verify and correct ML results, and to compute an estimate of the Bayesian evidence. fm4ar's ML models are conditioned on the assumed noise level of a spectrum (i.e., error bars), thus making them adaptable to different noise models.&lt;/p&gt;
    &lt;p&gt;AGNI simulates the atmospheric temperature-, height-, and compositional-structures of atmospheres overlying magma oceans while ensuring that radiative-convective equilibrium is maintained throughout the atmosphere. The code also supports real gas equations of state, self-gravitation, and various spectral surface compositions. Accounting for these energy transport processes permits AGNI to calculate atmospheric structure, which also yields realistic cooling rates for young rocky planets with magma oceans.&lt;/p&gt;
    &lt;p&gt;FiCUS (FItting the stellar Continuum of Uv Spectra) fit the stellar continuum of extragalactic ultraviolet (UV) spectra. The code takes observed-frame wavelength, flux density (with errors) and user-defined mask arrays as inputs, and returns an estimation of the galaxy stellar age, metallicity and dust extinction, as well as other secondary Spectral Energy Distribution (SED) parameters. FiCUS has two scripts; the first reads the INPUT file provided by the user and performs the fit according to selected options. It then gives the best-fit parameters and creates the OUTPUT files and figures. The second script includes pre-defined routines for spectral analysis, loading INPUT files and handling with data and models, as well as functions for the fitting routine, SED-parameters calculations and plotting, and imports functions into the first script.&lt;/p&gt;
    &lt;p&gt;pyStarburst99 is a Python version of the Starburst99 (ascl:1104.003) population synthesis code for star-forming galaxies. This Python version includes new evolutionary tracks and synthetic spectral energy distributions. pyStarburst99 provides wider coverage in metallicity, mass, and resolution, and includes evolutionary and spectral models of stars up to 300–500 M⊙.&lt;/p&gt;
    &lt;p&gt;The SIGWAY data analysis pipeline computes second-order, scalar induced gravitational wave signals emitted by curvature perturbations in the early universe. The package solves the Mukhanov-Sasaki equation for single field ultra-slow roll inflationary models and computes the primordial scalar power spectrum Pζ. SIGWAY also computes the second order gravitational wave power spectrum ΩGW from P ζ for reentry during radiation domination or a phase of early matter domination.&lt;/p&gt;
    &lt;p&gt;sMV (serial MultiView) scripts provide a semi-automatic and easy-to-use workflow for serial MultiView phase plane estimation. The phase plane is iteratively rotated based on the time series of calibrator residual phases; because time-domain information is included in the iterations, phase ambiguities are accurately and automatically identified. sMV enables efficient, high-accuracy differential astrometry and artifact-reduced imaging for astrophysical studies.&lt;/p&gt;
    &lt;p&gt;Built on Flax (ascl:2504.026), DeepSSM emulates gravitational wave (GW) spectra produced by sound waves during cosmological first-order phase transitions in the radiation-dominated era. It uses neural networks trained on an enhanced version of the Sound Shell Model (SSM). The code provides instantaneous predictions of GW spectra given the phase transition parameters, while achieving agreement with the enhanced SSM model. DeepSSM is particularly suitable for direct Bayesian inference on phase transition parameters without relying on empirical templates, such as broken power-law models.&lt;/p&gt;
    &lt;p&gt;The flux transport model HipFT implements advection, diffusion, and data assimilation on the solar surface on a logically rectangular nonuniform spherical grid. It is parallelized for use with multi-core CPUs and GPUs using a combination of Fortran's standard parallel do concurrent (DC), OpenMP Target data directives, and MPI. Serving as the computational core of the Open-source Flux Transport (OFT) software suite (ascl:2508.013), HipFT incorporates various differential rotation, meridional flow, super granular convective flow, and data assimilation models. HipRT also computes multiple realizations in a single run spanning multiple choices of parameters.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://ascl.net/"/></entry><entry><id>https://news.ycombinator.com/item?id=45221103</id><title>Lumina-DiMOO: An open-source discrete multimodal diffusion model</title><updated>2025-09-12T13:22:58.079505+00:00</updated><content>&lt;doc fingerprint="5d32a9963856c184"&gt;
  &lt;main&gt;
    &lt;p&gt;We introduce Lumina-DiMOO, an open-source foundational model for seamless multimodal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-diffusion paradigms and adeptly support a broad spectrum of multimodal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multimodal models. To foster further advancements in multimodal and dicrete diffusion model research, we release our code and checkpoints.&lt;/p&gt;
    &lt;p&gt;Overview of Lumina-DiMOO’s Multifunctionality and Superior Performance.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Methods&lt;/cell&gt;
        &lt;cell role="head"&gt;#Params&lt;/cell&gt;
        &lt;cell role="head"&gt;Single Object&lt;/cell&gt;
        &lt;cell role="head"&gt;Two Object&lt;/cell&gt;
        &lt;cell role="head"&gt;Counting&lt;/cell&gt;
        &lt;cell role="head"&gt;Colors&lt;/cell&gt;
        &lt;cell role="head"&gt;Position&lt;/cell&gt;
        &lt;cell role="head"&gt;Attibute&lt;/cell&gt;
        &lt;cell role="head"&gt;Overall ↑&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Gen. Only&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;SDXL&lt;/cell&gt;
        &lt;cell&gt;2.6B&lt;/cell&gt;
        &lt;cell&gt;0.98&lt;/cell&gt;
        &lt;cell&gt;0.74&lt;/cell&gt;
        &lt;cell&gt;0.39&lt;/cell&gt;
        &lt;cell&gt;0.85&lt;/cell&gt;
        &lt;cell&gt;0.15&lt;/cell&gt;
        &lt;cell&gt;0.23&lt;/cell&gt;
        &lt;cell&gt;0.55&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Emu3-Gen&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;0.98&lt;/cell&gt;
        &lt;cell&gt;0.71&lt;/cell&gt;
        &lt;cell&gt;0.34&lt;/cell&gt;
        &lt;cell&gt;0.81&lt;/cell&gt;
        &lt;cell&gt;0.17&lt;/cell&gt;
        &lt;cell&gt;0.21&lt;/cell&gt;
        &lt;cell&gt;0.54&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;SD3-Medium&lt;/cell&gt;
        &lt;cell&gt;2B&lt;/cell&gt;
        &lt;cell&gt;0.99&lt;/cell&gt;
        &lt;cell&gt;0.94&lt;/cell&gt;
        &lt;cell&gt;0.72&lt;/cell&gt;
        &lt;cell&gt;0.89&lt;/cell&gt;
        &lt;cell&gt;0.33&lt;/cell&gt;
        &lt;cell&gt;0.60&lt;/cell&gt;
        &lt;cell&gt;0.74&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;DALL-E 3&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.96&lt;/cell&gt;
        &lt;cell&gt;0.87&lt;/cell&gt;
        &lt;cell&gt;0.47&lt;/cell&gt;
        &lt;cell&gt;0.83&lt;/cell&gt;
        &lt;cell&gt;0.43&lt;/cell&gt;
        &lt;cell&gt;0.45&lt;/cell&gt;
        &lt;cell&gt;0.67&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;FLUX.1 [Dev]&lt;/cell&gt;
        &lt;cell&gt;12B&lt;/cell&gt;
        &lt;cell&gt;0.98&lt;/cell&gt;
        &lt;cell&gt;0.81&lt;/cell&gt;
        &lt;cell&gt;0.74&lt;/cell&gt;
        &lt;cell&gt;0.79&lt;/cell&gt;
        &lt;cell&gt;0.22&lt;/cell&gt;
        &lt;cell&gt;0.45&lt;/cell&gt;
        &lt;cell&gt;0.66&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;OmniGen&lt;/cell&gt;
        &lt;cell&gt;3.8B&lt;/cell&gt;
        &lt;cell&gt;0.98&lt;/cell&gt;
        &lt;cell&gt;0.84&lt;/cell&gt;
        &lt;cell&gt;0.66&lt;/cell&gt;
        &lt;cell&gt;0.74&lt;/cell&gt;
        &lt;cell&gt;0.40&lt;/cell&gt;
        &lt;cell&gt;0.43&lt;/cell&gt;
        &lt;cell&gt;0.68&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Lumina-mGPT 2.0&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;0.99&lt;/cell&gt;
        &lt;cell&gt;0.87&lt;/cell&gt;
        &lt;cell&gt;0.44&lt;/cell&gt;
        &lt;cell&gt;0.85&lt;/cell&gt;
        &lt;cell&gt;0.44&lt;/cell&gt;
        &lt;cell&gt;0.54&lt;/cell&gt;
        &lt;cell&gt;0.69&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Unified&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Show-o&lt;/cell&gt;
        &lt;cell&gt;1.3B&lt;/cell&gt;
        &lt;cell&gt;0.95&lt;/cell&gt;
        &lt;cell&gt;0.52&lt;/cell&gt;
        &lt;cell&gt;0.49&lt;/cell&gt;
        &lt;cell&gt;0.82&lt;/cell&gt;
        &lt;cell&gt;0.11&lt;/cell&gt;
        &lt;cell&gt;0.28&lt;/cell&gt;
        &lt;cell&gt;0.53&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;TokenFlow-XL&lt;/cell&gt;
        &lt;cell&gt;14B&lt;/cell&gt;
        &lt;cell&gt;0.95&lt;/cell&gt;
        &lt;cell&gt;0.60&lt;/cell&gt;
        &lt;cell&gt;0.41&lt;/cell&gt;
        &lt;cell&gt;0.81&lt;/cell&gt;
        &lt;cell&gt;0.16&lt;/cell&gt;
        &lt;cell&gt;0.24&lt;/cell&gt;
        &lt;cell&gt;0.55&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Janus-Pro&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;0.99&lt;/cell&gt;
        &lt;cell&gt;0.89&lt;/cell&gt;
        &lt;cell&gt;0.59&lt;/cell&gt;
        &lt;cell&gt;0.90&lt;/cell&gt;
        &lt;cell&gt;0.79&lt;/cell&gt;
        &lt;cell&gt;0.66&lt;/cell&gt;
        &lt;cell&gt;0.80&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;GPT-4o&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.99&lt;/cell&gt;
        &lt;cell&gt;0.92&lt;/cell&gt;
        &lt;cell&gt;0.85&lt;/cell&gt;
        &lt;cell&gt;0.92&lt;/cell&gt;
        &lt;cell&gt;0.75&lt;/cell&gt;
        &lt;cell&gt;0.61&lt;/cell&gt;
        &lt;cell&gt;0.84&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;BAGAL&lt;/cell&gt;
        &lt;cell&gt;14B&lt;/cell&gt;
        &lt;cell&gt;0.99&lt;/cell&gt;
        &lt;cell&gt;0.94&lt;/cell&gt;
        &lt;cell&gt;0.81&lt;/cell&gt;
        &lt;cell&gt;0.88&lt;/cell&gt;
        &lt;cell&gt;0.64&lt;/cell&gt;
        &lt;cell&gt;0.63&lt;/cell&gt;
        &lt;cell&gt;0.82&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMaDA&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;0.99&lt;/cell&gt;
        &lt;cell&gt;0.76&lt;/cell&gt;
        &lt;cell&gt;0.61&lt;/cell&gt;
        &lt;cell&gt;0.84&lt;/cell&gt;
        &lt;cell&gt;0.20&lt;/cell&gt;
        &lt;cell&gt;0.37&lt;/cell&gt;
        &lt;cell&gt;0.63&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Lumina-DiMOO&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;1.0&lt;/cell&gt;
        &lt;cell&gt;0.94&lt;/cell&gt;
        &lt;cell&gt;0.85&lt;/cell&gt;
        &lt;cell&gt;0.89&lt;/cell&gt;
        &lt;cell&gt;0.85&lt;/cell&gt;
        &lt;cell&gt;0.76&lt;/cell&gt;
        &lt;cell&gt;0.88&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell role="head"&gt;Methods&lt;/cell&gt;
        &lt;cell role="head"&gt;#Params&lt;/cell&gt;
        &lt;cell role="head"&gt;Global&lt;/cell&gt;
        &lt;cell role="head"&gt;Entity&lt;/cell&gt;
        &lt;cell role="head"&gt;Attribute&lt;/cell&gt;
        &lt;cell role="head"&gt;Relation&lt;/cell&gt;
        &lt;cell role="head"&gt;Other&lt;/cell&gt;
        &lt;cell role="head"&gt;Overall ↑&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Gen. Only&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;SDXL&lt;/cell&gt;
        &lt;cell&gt;2.6B&lt;/cell&gt;
        &lt;cell&gt;83.27&lt;/cell&gt;
        &lt;cell&gt;82.43&lt;/cell&gt;
        &lt;cell&gt;80.91&lt;/cell&gt;
        &lt;cell&gt;86.76&lt;/cell&gt;
        &lt;cell&gt;80.41&lt;/cell&gt;
        &lt;cell&gt;74.65&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Emu3-Gen&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;85.21&lt;/cell&gt;
        &lt;cell&gt;86.68&lt;/cell&gt;
        &lt;cell&gt;86.84&lt;/cell&gt;
        &lt;cell&gt;90.22&lt;/cell&gt;
        &lt;cell&gt;83.15&lt;/cell&gt;
        &lt;cell&gt;80.60&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;SD3-Medium&lt;/cell&gt;
        &lt;cell&gt;2B&lt;/cell&gt;
        &lt;cell&gt;87.90&lt;/cell&gt;
        &lt;cell&gt;91.01&lt;/cell&gt;
        &lt;cell&gt;88.83&lt;/cell&gt;
        &lt;cell&gt;80.70&lt;/cell&gt;
        &lt;cell&gt;88.68&lt;/cell&gt;
        &lt;cell&gt;84.08&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;DALL-E 3&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;90.97&lt;/cell&gt;
        &lt;cell&gt;89.61&lt;/cell&gt;
        &lt;cell&gt;88.39&lt;/cell&gt;
        &lt;cell&gt;90.58&lt;/cell&gt;
        &lt;cell&gt;89.83&lt;/cell&gt;
        &lt;cell&gt;83.50&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;FLUX.1 [Dev]&lt;/cell&gt;
        &lt;cell&gt;12B&lt;/cell&gt;
        &lt;cell&gt;74.35&lt;/cell&gt;
        &lt;cell&gt;90.00&lt;/cell&gt;
        &lt;cell&gt;88.96&lt;/cell&gt;
        &lt;cell&gt;90.87&lt;/cell&gt;
        &lt;cell&gt;88.33&lt;/cell&gt;
        &lt;cell&gt;83.84&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;OmniGen&lt;/cell&gt;
        &lt;cell&gt;3.8B&lt;/cell&gt;
        &lt;cell&gt;87.90&lt;/cell&gt;
        &lt;cell&gt;88.97&lt;/cell&gt;
        &lt;cell&gt;88.47&lt;/cell&gt;
        &lt;cell&gt;87.95&lt;/cell&gt;
        &lt;cell&gt;83.56&lt;/cell&gt;
        &lt;cell&gt;81.16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Lumina-mGPT 2.0&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;88.94&lt;/cell&gt;
        &lt;cell&gt;88.08&lt;/cell&gt;
        &lt;cell&gt;91.70&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;84.30&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Unified&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Show-o&lt;/cell&gt;
        &lt;cell&gt;1.3B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;67.48&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;TokenFlow-XL&lt;/cell&gt;
        &lt;cell&gt;14B&lt;/cell&gt;
        &lt;cell&gt;78.72&lt;/cell&gt;
        &lt;cell&gt;79.22&lt;/cell&gt;
        &lt;cell&gt;81.29&lt;/cell&gt;
        &lt;cell&gt;85.22&lt;/cell&gt;
        &lt;cell&gt;71.20&lt;/cell&gt;
        &lt;cell&gt;73.38&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Janus-Pro&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;86.90&lt;/cell&gt;
        &lt;cell&gt;88.90&lt;/cell&gt;
        &lt;cell&gt;89.40&lt;/cell&gt;
        &lt;cell&gt;89.32&lt;/cell&gt;
        &lt;cell&gt;89.48&lt;/cell&gt;
        &lt;cell&gt;84.19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;GPT-4o&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;88.89&lt;/cell&gt;
        &lt;cell&gt;88.94&lt;/cell&gt;
        &lt;cell&gt;89.84&lt;/cell&gt;
        &lt;cell&gt;92.63&lt;/cell&gt;
        &lt;cell&gt;90.96&lt;/cell&gt;
        &lt;cell&gt;85.15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;BAGAL&lt;/cell&gt;
        &lt;cell&gt;14B&lt;/cell&gt;
        &lt;cell&gt;88.94&lt;/cell&gt;
        &lt;cell&gt;90.37&lt;/cell&gt;
        &lt;cell&gt;91.29&lt;/cell&gt;
        &lt;cell&gt;90.82&lt;/cell&gt;
        &lt;cell&gt;88.67&lt;/cell&gt;
        &lt;cell&gt;85.07&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;MMaDA&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;77.81&lt;/cell&gt;
        &lt;cell&gt;78.48&lt;/cell&gt;
        &lt;cell&gt;81.74&lt;/cell&gt;
        &lt;cell&gt;84.79&lt;/cell&gt;
        &lt;cell&gt;63.2&lt;/cell&gt;
        &lt;cell&gt;69.97&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Lumina-DiMOO&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;81.46&lt;/cell&gt;
        &lt;cell&gt;92.08&lt;/cell&gt;
        &lt;cell&gt;88.98&lt;/cell&gt;
        &lt;cell&gt;94.31&lt;/cell&gt;
        &lt;cell&gt;82.0&lt;/cell&gt;
        &lt;cell&gt;86.04&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Methods&lt;/cell&gt;
        &lt;cell role="head"&gt;#Params&lt;/cell&gt;
        &lt;cell role="head"&gt;POPE&lt;/cell&gt;
        &lt;cell role="head"&gt;MME-P&lt;/cell&gt;
        &lt;cell role="head"&gt;MMB&lt;/cell&gt;
        &lt;cell role="head"&gt;SEED&lt;/cell&gt;
        &lt;cell role="head"&gt;MMMU&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Under. Only&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LLaVA&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;76.3&lt;/cell&gt;
        &lt;cell&gt;809.6&lt;/cell&gt;
        &lt;cell&gt;38.7&lt;/cell&gt;
        &lt;cell&gt;33.5&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LLaVA-v1.5&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;85.9&lt;/cell&gt;
        &lt;cell&gt;1510.7&lt;/cell&gt;
        &lt;cell&gt;64.3&lt;/cell&gt;
        &lt;cell&gt;58.6&lt;/cell&gt;
        &lt;cell&gt;35.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;InstructBLIP&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;36.0&lt;/cell&gt;
        &lt;cell&gt;53.4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Qwen-VL-Chat&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;1487.5&lt;/cell&gt;
        &lt;cell&gt;60.6&lt;/cell&gt;
        &lt;cell&gt;58.2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Emu3-Chat&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;85.2&lt;/cell&gt;
        &lt;cell&gt;1244&lt;/cell&gt;
        &lt;cell&gt;58.5&lt;/cell&gt;
        &lt;cell&gt;68.2&lt;/cell&gt;
        &lt;cell&gt;31.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Unified&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Show-o&lt;/cell&gt;
        &lt;cell&gt;1.3B&lt;/cell&gt;
        &lt;cell&gt;80.0&lt;/cell&gt;
        &lt;cell&gt;1097.2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;26.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;TokenFlow-XL&lt;/cell&gt;
        &lt;cell&gt;13B&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
        &lt;cell&gt;1545.9&lt;/cell&gt;
        &lt;cell&gt;68.9&lt;/cell&gt;
        &lt;cell&gt;68.7&lt;/cell&gt;
        &lt;cell&gt;38.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Janus-Pro&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;87.4&lt;/cell&gt;
        &lt;cell&gt;1567.1&lt;/cell&gt;
        &lt;cell&gt;79.2&lt;/cell&gt;
        &lt;cell&gt;72.1&lt;/cell&gt;
        &lt;cell&gt;41.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;BAGAL&lt;/cell&gt;
        &lt;cell&gt;14B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;1687&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;55.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MMaDA&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;86.1&lt;/cell&gt;
        &lt;cell&gt;1410.7&lt;/cell&gt;
        &lt;cell&gt;68.5&lt;/cell&gt;
        &lt;cell&gt;64.2&lt;/cell&gt;
        &lt;cell&gt;30.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Lumina-DiMOO&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;87.4&lt;/cell&gt;
        &lt;cell&gt;1534.2&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;83.1&lt;/cell&gt;
        &lt;cell&gt;58.6&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://synbol.github.io/Lumina-DiMOO/"/></entry><entry><id>https://news.ycombinator.com/item?id=45221274</id><title>The Treasury Is Expanding the Patriot Act to Attack Bitcoin Self Custody</title><updated>2025-09-12T13:22:57.711620+00:00</updated><content>&lt;doc fingerprint="365d59be7c351fbd"&gt;
  &lt;main&gt;
    &lt;p&gt;We shouldn't have to cater to the lowest common denominator.&lt;/p&gt;
    &lt;p&gt;We warned a couple of months ago when the Trump administration's "Crypto Brief" was released that there was some language in the brief that advised the government to expand the Patriot Act to account for digital assets. Well, it looks like FinCen and the Treasury have been working on guidelines and a rough outline is shared above courtesy of The Rage, and they are absolutely horrid.&lt;/p&gt;
    &lt;p&gt;It seems that FinCen and the Treasury are preparing to outlaw the use of CoinJoin, atomic swaps, single address use, and transaction broadcast timing delays. All of which are common best use practices that I would recommend any bitcoiner leveraging self-custody practice. This is an all out attack on financial privacy within bitcoin. If enacted, any user who leverages these tools will be flagged as a suspicious, any attempts to send a UTXO that has touched any of these tools will be rejected by regulated services, and could potentially be sent to prison.&lt;/p&gt;
    &lt;p&gt;This is an absurd affront to common sensibilities and freedom in the digital age. The fact that they want to prevent people from using single addresses for individual UTXOs is patently absurd. Not only is it a massive infringement on privacy, but it makes bitcoin usage less economically efficient and degrades the security of every bitcoiner. Loading up a single address with too many UTXOs degrades the entropy of a public-private key pair and makes it easier to brute force a user's private key.&lt;/p&gt;
    &lt;p&gt;Instead of expanding the Patriot Act, it should be abolished. Instead of trying to eliminate financial privacy for the 99.9% of law abiding citizens in this country, the government should be actively trying to foster an environment in which it can be improved. The proposed solutions will do nothing but put good Americans in harm's way and degrade the security of their savings.&lt;/p&gt;
    &lt;p&gt;We shouldn't have to live in a world where standards cater to the lowest common denominator, in this case criminals, and make things worse off for the overwhelming majority of the population. It's crazy that this even has to be said. The onus is on law enforcement to be so good at their jobs that they are able to prevent crimes from happening before they occur and effectively bring criminals to heel after they commit crimes. It shouldn't be on a neutral protocol and the industry being built on top of it that, when used effectively, provides people with a stable monetary system that respects user privacy and equips them with the tools to receive and spend in a way that provides them with peace of mind.&lt;/p&gt;
    &lt;p&gt;Why should everyone have to suffer because of a few bad apples? Isn't that letting the terrorist win?&lt;/p&gt;
    &lt;p&gt;Mel Mattison revealed a fascinating shift in Bitcoin's market dynamics that challenges conventional crypto wisdom. He pointed out that Bitcoin futures now exhibit lower volatility than platinum futures - a remarkable transformation for an asset once synonymous with wild price swings. The proliferation of ETFs, options, futures, and other traditional financial instruments has fundamentally altered Bitcoin's behavior, creating what Mel calls "volatility suppression." This institutionalization comes with trade-offs: while reducing dramatic downswings, it also caps explosive upside potential.&lt;/p&gt;
    &lt;quote&gt;"Bitcoin is becoming a TradFi security instrument and it's getting TradFi vol." - Mel Mattison&lt;/quote&gt;
    &lt;p&gt;Mel argued that the relationship between volatility and returns means investors must recalibrate expectations. Where 100% annual gains once seemed routine, he now considers 50% returns "massive" for this new era of Bitcoin. This maturation reflects Bitcoin's evolution from speculative experiment to financial infrastructure - less exciting perhaps, but ultimately more sustainable for long-term adoption.&lt;/p&gt;
    &lt;p&gt;Check out the full podcast here for more on China's gold strategy, Fed independence battles, and housing market manipulation plans.&lt;/p&gt;
    &lt;p&gt;New Bill for Strategic Bitcoin Reserve - via X&lt;/p&gt;
    &lt;p&gt;SEC to Host Crypto Roundtable October 17 - via X&lt;/p&gt;
    &lt;p&gt;Research Proposes Bitcoin for Mars Trade Standard - via X&lt;/p&gt;
    &lt;p&gt;Tom Honzik has helped 1,000+ people secure more than 5,000 BTC. Now, TFTC and Unchained are teaming up for a live online session on bitcoin custody.What you’ll learn:&lt;/p&gt;
    &lt;p&gt;Stick around for the AMA to ask Tom Honzik and Marty Bent anything—from privacy considerations to the tradeoffs of different multisig quorums.&lt;/p&gt;
    &lt;p&gt;Created by Carl Dong (former Bitcoin Core contributor), unlike other VPNs, it can’t log your activity by design, delivering verifiable privacy you can trust.&lt;/p&gt;
    &lt;p&gt;Outsmarts internet censorship: works even on the most restrictive Wi-Fi networks where other VPNs fail.&lt;lb/&gt;Pay with bitcoin over Lightning: better privacy and low fees.&lt;lb/&gt;No email required: accounts are generated like bitcoin wallets.&lt;lb/&gt;No trade-offs: browse freely with fast, reliable speeds.&lt;/p&gt;
    &lt;p&gt;Exclusive Deal for TFTC Listeners:&lt;lb/&gt;Sign up at obscura.net and use code TFTC25 for 25% off your first 12 months.&lt;/p&gt;
    &lt;p&gt;Now available on macOS, iOS, and WireGuard, with more platforms coming soon — so your privacy travels with you wherever you go.&lt;/p&gt;
    &lt;p&gt;Ten31, the largest bitcoin-focused investor, has deployed $200M across 30+ companies through three funds. I am a Managing Partner at Ten31 and am very proud of the work we are doing. Learn more at ten31.vc/invest.&lt;/p&gt;
    &lt;p&gt;Final thought...&lt;/p&gt;
    &lt;p&gt;Rest in peace, Charlie Kirk. Pray for humanity and for peace.&lt;/p&gt;
    &lt;p&gt;Download our free browser extension, Opportunity Cost: https://www.opportunitycost.app/ start thinking in SATS today.&lt;/p&gt;
    &lt;p&gt;Get this newsletter sent to your inbox daily: https://www.tftc.io/bitcoin-brief/&lt;/p&gt;
    &lt;p&gt;Subscribe to our YouTube channels and follow us on Nostr and X:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tftc.io/treasury-iexpanding-patriot-act/"/></entry><entry><id>https://news.ycombinator.com/item?id=45221423</id><title>AI Startup Founders Tout a Winning Formula–No Booze, No Sleep, No Fun</title><updated>2025-09-12T13:22:57.589490+00:00</updated><content/><link href="https://www.wsj.com/business/entrepreneurship/artificial-intelligence-startup-founders-bc730406"/></entry><entry><id>https://news.ycombinator.com/item?id=45221580</id><title>Chat Control repelled 4th time in the EU</title><updated>2025-09-12T13:22:57.154304+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/TutaPrivacy/status/1966384776883142661"/></entry><entry><id>https://news.ycombinator.com/item?id=45221694</id><title>Toxic "forever chemicals" found in 95% of beers tested in the U.S.</title><updated>2025-09-12T13:22:56.820309+00:00</updated><content>&lt;doc fingerprint="e7411058cc118703"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Toxic “forever chemicals” found in 95% of beers tested in the U.S.&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Date:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;September 12, 2025&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Source:&lt;/item&gt;
      &lt;item rend="dd-2"&gt;American Chemical Society&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Summary:&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Forever chemicals known as PFAS have turned up in an unexpected place: beer. Researchers tested 23 different beers from across the U.S. and found that 95% contained PFAS, with the highest concentrations showing up in regions with known water contamination. The findings reveal how pollution in municipal water supplies can infiltrate popular products, raising concerns for both consumers and brewers.&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Share:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Infamous for their environmental persistence and potential links to health conditions, per- and polyfluoroalkyl substances (PFAS), often called forever chemicals, are being discovered in unexpected places, including beer. Researchers publishing in ACS' Environmental Science &amp;amp; Technology tested beers brewed in different areas around the U.S. for these substances. They found that beers produced in parts of the country with known PFAS-contaminated water sources showed the highest levels of forever chemicals.&lt;/p&gt;
    &lt;p&gt;"As an occasional beer drinker myself, I wondered whether PFAS in water supplies was making its way into our pints," says research lead Jennifer Hoponick Redmon. "I hope these findings inspire water treatment strategies and policies that help reduce the likelihood of PFAS in future pours."&lt;/p&gt;
    &lt;p&gt;PFAS are human-made chemicals produced for their water-, oil- and stain-repellent properties. They have been found in surface water, groundwater and municipal water supplies across the U.S. and the world. Although breweries typically have water filtration and treatment systems, they are not designed to remove PFAS. By modifying a U.S. Environmental Protection Agency (EPA) testing method for analyzing levels of PFAS in drinking water, Hoponick Redmon and colleagues tested 23 beers. The test subjects were produced by U.S. brewers in areas with documented water system contamination, plus popular domestic and international beers from larger companies with unknown water sources.&lt;/p&gt;
    &lt;p&gt;The researchers found a strong correlation between PFAS concentrations in municipal drinking water and levels in locally brewed beer -- a phenomenon that Hoponick Redmon and colleagues say has not yet been studied in U.S. retail beer. They found PFAS in 95% of the beers they tested. These include perfluorooctanesulfonate (PFOS) and perfluorooctanoic acid (PFOA), two forever chemicals with recently established EPA limits in drinking water. Notably, the team found that beers brewed near the Cape Fear River Basin in North Carolina, an area with known PFAS pollution, had the highest levels and most diverse mix of forever chemicals, including PFOS and PFOA.&lt;/p&gt;
    &lt;p&gt;This work shows that PFAS contamination at one source can spread into other products, and the researchers call for greater awareness among brewers, consumers and regulators to limit overall PFAS exposure. These results also highlight the possible need for water treatment upgrades at brewing facilities as PFAS regulations in drinking water change or updates to municipal water system treatment are implemented.&lt;/p&gt;
    &lt;p&gt;The authors acknowledge funding from an internal research grant from RTI International.&lt;/p&gt;
    &lt;p&gt;Story Source:&lt;/p&gt;
    &lt;p&gt;Materials provided by American Chemical Society. Note: Content may be edited for style and length.&lt;/p&gt;
    &lt;p&gt;Journal Reference:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Jennifer Hoponick Redmon, Nicole M. DeLuca, Evan Thorp, Chamindu Liyanapatirana, Laura Allen, Andrew J. Kondash. Hold My Beer: The Linkage between Municipal Water and Brewing Location on PFAS in Popular Beverages. Environmental Science, 2025; 59 (17): 8368 DOI: 10.1021/acs.est.4c11265&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cite This Page:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.sciencedaily.com/releases/2025/09/250911073204.htm"/></entry></feed>