<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-02T22:35:46.928856+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45789424</id><title>Using FreeBSD to make self-hosting fun again</title><updated>2025-11-02T22:35:52.502146+00:00</updated><content>&lt;doc fingerprint="3f1431577be6acd1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Using FreeBSD to make self-hosting fun again&lt;/head&gt;
    &lt;p&gt;2025-11-01 - Feeling like a kid in a candy store, once more&lt;/p&gt;
    &lt;p&gt;As evident by my last blog post "A prison of my own making", I needed to change something about my relationship with technology. How I was doing things didn't work anymore, but I also felt unable to change anything about it, as the way I was doing things seemed like the way that I was supposed to use.&lt;/p&gt;
    &lt;p&gt;What I needed was a fresh start. And I managed to find that fresh start in the BSD family of operating systems.&lt;/p&gt;
    &lt;p&gt;I had already given FreeBSD and OpenBSD a try at the time and I liked what I saw. OpenBSD had already established itself in my workflow as an easy to use and reliable router and general OS for single-purpose VMs. But it isn't able to fullfill my needs for a multi-purpose system, where I'd want to run multiple separated workloads in something like a container or VM. But FreeBSD could.&lt;/p&gt;
    &lt;p&gt;I know that I generally operate best by just committing to using a thing and then figuring out what I need, as I need it. So I committed to using FreeBSD and found a really nice server to do just that on the Hetzner server auction.&lt;/p&gt;
    &lt;p&gt;I started setting it up with BastilleBSD for jails and vm-bhyve for VMs. I didn't know how to do most things and felt kinda lost. But there it was again, that feeling of excitement to learn something new, which got my into self-hosting in the first place.&lt;/p&gt;
    &lt;p&gt;After some trial and error I managed to find a setup that works for me. As per usual, it deviates a bit from what might be the most common setup, but it's undoubtedly me (I'll probably explain more about it in the future, when things have settled).&lt;/p&gt;
    &lt;p&gt;What I've come to appreciate about FreeBSD, and the BSD operating systems in general, is their simplicity and good documentation. Most tasks are just a few commands to run via SSH and if that isn't the case, someone has probably written a decent wrapper around it. If I need to find a piece of information, I still instinctively search online for it, just to be greeted by an online version of the corresponding man page. So I could also have just gathered that information on the CLI, oh well.&lt;/p&gt;
    &lt;p&gt;I also love the focus on long-term compatibility. I can find a solution to a problem in a forum post from 2008 and not even for a second do I have to doubt whether it will work, because it always does. At the same time, that doesn't mean there are no new features. The system doesn't feel old.&lt;/p&gt;
    &lt;p&gt;Sure, not everything was all roses and some of that was probably due to my way of just jumping into a problem and digging myself through it one step at a time, instead of reading up on it a lot beforehand. For example I was confused for a long time about the release cycle of the base system and whether that somehow related to pkg and ports (It does not). And I was not able to properly phrase the question in a way that would result in a helpful result while searching. Luckily the BSD community has been nothing but kind and helpful so far. I've had multiple people on the Fediverse offer their help and when I had a specific question, I would always get multiple solid answers explaining it to me. Thanks to everyone that replied, it's genuinely a blast to feel like a newbie again!&lt;/p&gt;
    &lt;p&gt;I don't know whether I will actually stick with all of what I'm doing right now, in the long term. But that's not important. What is important is that I'm having fun, learning a new thing, right now. I'll see what sticks long-term.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;@Joel: See? I wrote a blog post! :D&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jsteuernagel.de/posts/using-freebsd-to-make-self-hosting-fun-again/"/><published>2025-11-02T11:01:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45789474</id><title>URLs are state containers</title><updated>2025-11-02T22:35:52.090933+00:00</updated><content>&lt;doc fingerprint="abc21e28a1a26d89"&gt;
  &lt;main&gt;
    &lt;p&gt;Couple of weeks ago when I was publishing The Hidden Cost of URL Design I needed to add SQL syntax highlighting. I headed to PrismJS website trying to remember if it should be added as a plugin or what. I was overwhelmed with the amount of options in the download page so I headed back to my code. I checked the file for PrismJS and at the top of the file, I found a comment containing a URL:&lt;/p&gt;
    &lt;code&gt;/* https://prismjs.com/download.html#themes=prism&amp;amp;languages=markup+css+clike+javascript+bash+css-extras+markdown+scss+sql&amp;amp;plugins=line-highlight+line-numbers+autolinker */
&lt;/code&gt;
    &lt;p&gt;I had completely forgotten about this. I clicked the URL, and it was the PrismJS download page with every checkbox, dropdown, and option pre-selected to match my exact configuration. Themes chosen. Languages selected. Plugins enabled. Everything, perfectly reconstructed from that single URL.&lt;/p&gt;
    &lt;p&gt;It was one of those moments where something you once knew suddenly clicks again with fresh significance. Here was a URL doing far more than just pointing to a page. It was storing state, encoding intent, and making my entire setup shareable and recoverable. No database. No cookies. No localStorage. Just a URL.&lt;/p&gt;
    &lt;p&gt;This got me thinking: how often do we, as frontend engineers, overlook the URL as a state management tool? We reach for all sorts of abstractions to manage state such as global stores, contexts, and caches while ignoring one of the webâs most elegant and oldest features: the humble URL.&lt;/p&gt;
    &lt;p&gt;In my previous article, I wrote about the hidden costs of bad URL design. Today, I want to flip that perspective and talk about the immense value of good URL design. Specifically, how URLs can be treated as first-class state containers in modern web applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Overlooked Power of URLs&lt;/head&gt;
    &lt;p&gt;Scott Hanselman famously said âURLs are UIâ and heâs absolutely right. URLs arenât just technical addresses that browsers use to fetch resources. Theyâre interfaces. Theyâre part of the user experience.&lt;/p&gt;
    &lt;p&gt;But URLs are more than UI. Theyâre state containers. Every time you craft a URL, youâre making decisions about what information to preserve, what to make shareable, and what to make bookmarkable.&lt;/p&gt;
    &lt;p&gt;Think about what URLs give us for free:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shareability: Send someone a link, and they see exactly what you see&lt;/item&gt;
      &lt;item&gt;Bookmarkability: Save a URL, and youâve saved a moment in time&lt;/item&gt;
      &lt;item&gt;Browser history: The back button just works&lt;/item&gt;
      &lt;item&gt;Deep linking: Jump directly into a specific application state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;URLs make web applications resilient and predictable. Theyâre the webâs original state management solution, and theyâve been working reliably since 1991. The question isnât whether URLs can store state. Itâs whether weâre using them to their full potential.&lt;/p&gt;
    &lt;p&gt;Before we dive into examples, letâs break down how URLs encode state. Hereâs a typical stateful URL:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For many years, these were considered the only components of a URL. That changed with the introduction of Text Fragments, a feature that allows linking directly to a specific piece of text within a page. You can read more about it in my article Smarter than âCtrl+Fâ: Linking Directly to Web Page Content.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Different parts of the URL encode different types of state:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Path Segments (&lt;code&gt;/path/to/myfile.html&lt;/code&gt;). Best used for hierarchical resource navigation:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;/users/123/posts&lt;/code&gt;- User 123âs posts&lt;/item&gt;&lt;item&gt;&lt;code&gt;/docs/api/authentication&lt;/code&gt;- Documentation structure&lt;/item&gt;&lt;item&gt;&lt;code&gt;/dashboard/analytics&lt;/code&gt;- Application sections&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Query Parameters (&lt;code&gt;?key1=value1&amp;amp;key2=value2&lt;/code&gt;). Perfect for filters, options, and configuration:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;?theme=dark&amp;amp;lang=en&lt;/code&gt;- UI preferences&lt;/item&gt;&lt;item&gt;&lt;code&gt;?page=2&amp;amp;limit=20&lt;/code&gt;- Pagination&lt;/item&gt;&lt;item&gt;&lt;code&gt;?status=active&amp;amp;sort=date&lt;/code&gt;- Data filtering&lt;/item&gt;&lt;item&gt;&lt;code&gt;?from=2025-01-01&amp;amp;to=2025-12-31&lt;/code&gt;- Date ranges&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Anchor&lt;/del&gt;Fragment (&lt;code&gt;#SomewhereInTheDocument&lt;/code&gt;). Ideal for client-side navigation and page sections:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;#L20-L35&lt;/code&gt;- GitHub line highlighting&lt;/item&gt;&lt;item&gt;&lt;code&gt;#features&lt;/code&gt;- Scroll to section&lt;/item&gt;&lt;item&gt;&lt;code&gt;#/dashboard&lt;/code&gt;- Single-page app routing (though itâs rarely used these days)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Common Patterns That Work for Query Parameters&lt;/head&gt;
    &lt;head rend="h4"&gt;Multiple values with delimiters&lt;/head&gt;
    &lt;p&gt;Sometimes youâll see multiple values packed into a single key using delimiters like commas or plus signs. Itâs compact and human-readable, though it requires manual parsing on the server side.&lt;/p&gt;
    &lt;code&gt;?languages=javascript+typescript+python
?tags=frontend,react,hooks
&lt;/code&gt;
    &lt;head rend="h4"&gt;Nested or structured data&lt;/head&gt;
    &lt;p&gt;Developers often encode complex filters or configuration objects into a single query string. A simple convention uses keyâvalue pairs separated by commas, while others serialize JSON or even Base64-encode it for safety.&lt;/p&gt;
    &lt;code&gt;?filters=status:active,owner:me,priority:high
?config=eyJyaWNrIjoicm9sbCJ9==  (base64-encoded JSON)
&lt;/code&gt;
    &lt;head rend="h4"&gt;Boolean flags&lt;/head&gt;
    &lt;p&gt;For flags or toggles, itâs common to pass booleans explicitly or to rely on the keyâs presence as truthy. This keeps URLs shorter and makes toggling features easy.&lt;/p&gt;
    &lt;code&gt;?debug=true&amp;amp;analytics=false
?mobile  (presence = true)
&lt;/code&gt;
    &lt;head rend="h4"&gt;Arrays (Bracket notation)&lt;/head&gt;
    &lt;code&gt;?tags[]=frontend&amp;amp;tags[]=react&amp;amp;tags[]=hooks
&lt;/code&gt;
    &lt;p&gt;Another old pattern is bracket notation, which represents arrays in query parameters. It originated from early web frameworks like PHP where appending &lt;code&gt;[]&lt;/code&gt; to a parameter name signals that multiple values should be grouped together.&lt;/p&gt;
    &lt;code&gt;?tags[]=frontend&amp;amp;tags[]=react&amp;amp;tags[]=hooks
?ids[0]=42&amp;amp;ids[1]=73
&lt;/code&gt;
    &lt;p&gt;Many modern frameworks and parsers (like Nodeâs &lt;code&gt;qs&lt;/code&gt; library or Express middleware) still recognize this pattern automatically. However, itâs not officially standardized in the URL specification, so behavior can vary depending on the server or client implementation. Notice how it even breaks the syntax highlighting on my website.&lt;/p&gt;
    &lt;p&gt;The key is consistency. Pick patterns that make sense for your application and stick with them.&lt;/p&gt;
    &lt;head rend="h2"&gt;State via URL Parameters&lt;/head&gt;
    &lt;p&gt;Letâs look at real-world examples of URLs as state containers:&lt;/p&gt;
    &lt;p&gt;PrismJS Configuration&lt;/p&gt;
    &lt;code&gt;https://prismjs.com/download.html#themes=prism&amp;amp;languages=markup+css+clike+javascript&amp;amp;plugins=line-numbers
&lt;/code&gt;
    &lt;p&gt;The entire syntax highlighter configuration encoded in the URL. Change anything in the UI, and the URL updates. Share the URL, and someone else gets your exact setup. This one uses anchor and not query parameters, but the concept is the same.&lt;/p&gt;
    &lt;p&gt;GitHub Line Highlighting&lt;/p&gt;
    &lt;code&gt;https://github.com/zepouet/Xee-xCode-4.5/blob/master/XeePhotoshopLoader.m#L108-L136
&lt;/code&gt;
    &lt;p&gt;It links to a specific file while highlighting lines 108 through 136. Click this link anywhere, and youâll land on the exact code section being discussed.&lt;/p&gt;
    &lt;p&gt;Google Maps&lt;/p&gt;
    &lt;code&gt;https://www.google.com/maps/@22.443842,-74.220744,19z
&lt;/code&gt;
    &lt;p&gt;Coordinates, zoom level, and map type all in the URL. Share this link, and anyone can see the exact same view of the map.&lt;/p&gt;
    &lt;p&gt;Figma and Design Tools&lt;/p&gt;
    &lt;code&gt;https://www.figma.com/file/abc123/MyDesign?node-id=123:456&amp;amp;viewport=100,200,0.5
&lt;/code&gt;
    &lt;p&gt;Before shareable design links, finding an updated screen or component in a large file was a chore. Someone had to literally show you where it lived, scrolling and zooming across layers. Today, a Figma link carries all that context like canvas position, zoom level, selected element. Literally everything needed to drop you right into the workspace.&lt;/p&gt;
    &lt;p&gt;E-commerce Filters&lt;/p&gt;
    &lt;code&gt;https://store.com/laptops?brand=dell+hp&amp;amp;price=500-1500&amp;amp;rating=4&amp;amp;sort=price-asc
&lt;/code&gt;
    &lt;p&gt;This is one of the most common real-world patterns youâll encounter. Every filter, sort option, and price range preserved. Users can bookmark their exact search criteria and return to it anytime. Most importantly, they can come back to it after navigating away or refreshing the page.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frontend Engineering Patterns&lt;/head&gt;
    &lt;p&gt;Before we discuss implementation details, we need to establish a clear guideline for what should go into the URL. Not all state belongs in URLs. Hereâs a simple heuristic:&lt;/p&gt;
    &lt;p&gt;Good candidates for URL state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search queries and filters&lt;/item&gt;
      &lt;item&gt;Pagination and sorting&lt;/item&gt;
      &lt;item&gt;View modes (list/grid, dark/light)&lt;/item&gt;
      &lt;item&gt;Date ranges and time periods&lt;/item&gt;
      &lt;item&gt;Selected items or active tabs&lt;/item&gt;
      &lt;item&gt;UI configuration that affects content&lt;/item&gt;
      &lt;item&gt;Feature flags and A/B test variants&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Poor candidates for URL state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sensitive information (passwords, tokens, PII)&lt;/item&gt;
      &lt;item&gt;Temporary UI states (modal open/closed, dropdown expanded)&lt;/item&gt;
      &lt;item&gt;Form input in progress (unsaved changes)&lt;/item&gt;
      &lt;item&gt;Extremely large or complex nested data&lt;/item&gt;
      &lt;item&gt;High-frequency transient states (mouse position, scroll position)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are not sure if a piece of state belongs in the URL, ask yourself: If someone else clicking this URL, should they see the same state? If so, it belongs in the URL. If not, use a different state management approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation using Plain JavaScript&lt;/head&gt;
    &lt;p&gt;The modern &lt;code&gt;URLSearchParams&lt;/code&gt; API makes URL state management straightforward:&lt;/p&gt;
    &lt;code&gt;// Reading URL parameters
const params = new URLSearchParams(window.location.search);
const view = params.get('view') || 'grid';
const page = params.get('page') || 1;

// Updating URL parameters
function updateFilters(filters) {
  const params = new URLSearchParams(window.location.search);

  // Update individual parameters
  params.set('status', filters.status);
  params.set('sort', filters.sort);

  // Update URL without page reload
  const newUrl = `${window.location.pathname}?${params.toString()}`;
  window.history.pushState({}, '', newUrl);

  // Now update your UI based on the new filters
  renderContent(filters);
}

// Handling back/forward buttons
window.addEventListener('popstate', () =&amp;gt; {
  const params = new URLSearchParams(window.location.search);
  const filters = {
    status: params.get('status') || 'all',
    sort: params.get('sort') || 'date'
  };
  renderContent(filters);
});
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;popstate&lt;/code&gt; event fires when the user navigates with the browserâs Back or Forward buttons. It lets you restore the UI to match the URL, which is essential for keeping your appâs state and history in sync. Usually your frameworkâs router handles this for you, but itâs good to know how it works under the hood.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation using React&lt;/head&gt;
    &lt;p&gt;React Router and Next.js provide hooks that make this even cleaner:&lt;/p&gt;
    &lt;code&gt;import { useSearchParams } from 'react-router-dom';
// or for Next.js 13+: import { useSearchParams } from 'next/navigation';

function ProductList() {
  const [searchParams, setSearchParams] = useSearchParams();

  // Read from URL (with defaults)
  const color = searchParams.get('color') || 'all';
  const sort = searchParams.get('sort') || 'price';

  // Update URL
  const handleColorChange = (newColor) =&amp;gt; {
    setSearchParams(prev =&amp;gt; {
      const params = new URLSearchParams(prev);
      params.set('color', newColor);
      return params;
    });
  };

  return (
    &amp;lt;div&amp;gt;
      &amp;lt;select value={color} onChange={e =&amp;gt; handleColorChange(e.target.value)}&amp;gt;
        &amp;lt;option value="all"&amp;gt;All Colors&amp;lt;/option&amp;gt;
        &amp;lt;option value="silver"&amp;gt;Silver&amp;lt;/option&amp;gt;
        &amp;lt;option value="black"&amp;gt;Black&amp;lt;/option&amp;gt;
      &amp;lt;/select&amp;gt;

      {/* Your filtered products render here */}
    &amp;lt;/div&amp;gt;
  );
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Best Practices for URL State Management&lt;/head&gt;
    &lt;p&gt;Now that weâve seen how URLs can hold application state, letâs look at a few best practices that keep them clean, predictable, and user-friendly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Handling Defaults Gracefully&lt;/head&gt;
    &lt;p&gt;Donât pollute URLs with default values:&lt;/p&gt;
    &lt;code&gt;// Bad: URL gets cluttered with defaults
?theme=light&amp;amp;lang=en&amp;amp;page=1&amp;amp;sort=date

// Good: Only non-default values in URL
?theme=dark  // light is default, so omit it
&lt;/code&gt;
    &lt;p&gt;Use defaults in your code when reading parameters:&lt;/p&gt;
    &lt;code&gt;function getTheme(params) {
  return params.get('theme') || 'light'; // Default handled in code
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Debouncing URL Updates&lt;/head&gt;
    &lt;p&gt;For high-frequency updates (like search-as-you-type), debounce URL changes:&lt;/p&gt;
    &lt;code&gt;import { debounce } from 'lodash';

const updateSearchParam = debounce((value) =&amp;gt; {
  const params = new URLSearchParams(window.location.search);
  if (value) {
    params.set('q', value);
  } else {
    params.delete('q');
  }
  window.history.replaceState({}, '', `?${params.toString()}`);
}, 300);

// Use replaceState instead of pushState to avoid flooding history
&lt;/code&gt;
    &lt;head rend="h4"&gt;pushState vs. replaceState&lt;/head&gt;
    &lt;p&gt;When deciding between &lt;code&gt;pushState&lt;/code&gt; and &lt;code&gt;replaceState&lt;/code&gt;, think about how you want the browser history to behave. &lt;code&gt;pushState&lt;/code&gt; creates a new history entry, which makes sense for distinct navigation actions like changing filters, pagination, or navigating to a new view â users can then use the Back button to return to the previous state. On the other hand, &lt;code&gt;replaceState&lt;/code&gt; updates the current entry without adding a new one, making it ideal for refinements such as search-as-you-type or minor UI adjustments where you donât want to flood the history with every keystroke.&lt;/p&gt;
    &lt;head rend="h2"&gt;URLs as Contracts&lt;/head&gt;
    &lt;p&gt;When designed thoughtfully, URLs become more than just state containers. They become contracts between your application and its consumers. A good URL defines expectations for humans, developers, and machines alike&lt;/p&gt;
    &lt;head rend="h3"&gt;Clear Boundaries&lt;/head&gt;
    &lt;p&gt;A well-structured URL draws the line between whatâs public and whatâs private, client and server, shareable and session-specific. It clarifies where state lives and how it should behave. Developers know whatâs safe to persist, users know what they can bookmark, and machines know whats worth indexing.&lt;/p&gt;
    &lt;p&gt;URLs, in that sense, act as interfaces: visible, predictable, and stable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Communicating Meaning&lt;/head&gt;
    &lt;p&gt;Readable URLs explain themselves. Consider the difference between the two URLs below.&lt;/p&gt;
    &lt;code&gt;https://example.com/p?id=x7f2k&amp;amp;v=3
https://example.com/products/laptop?color=silver&amp;amp;sort=price
&lt;/code&gt;
    &lt;p&gt;The first one hides intent. The second tells a story. A human can read it and understand what theyâre looking at. A machine can parse it and extract meaningful structure.&lt;/p&gt;
    &lt;p&gt;Jim Nielsen calls these âexamples of great URLsâ. URLs that explain themselves.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caching and Performance&lt;/head&gt;
    &lt;p&gt;URLs are cache keys. Well-designed URLs enable better caching strategies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Same URL = same resource = cache hit&lt;/item&gt;
      &lt;item&gt;Query params define cache variations&lt;/item&gt;
      &lt;item&gt;CDNs can cache intelligently based on URL patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can even visualize a userâs journey without any extra tracking code:&lt;/p&gt;
    &lt;quote&gt;graph LR A["/products"] --&amp;gt; |selects category| B["/products?category=laptops"] B --&amp;gt; |adds price filter| C["/products?category=laptops&amp;amp;price=500-1000"] style A fill:#e9edf7,stroke:#455d8d,stroke-width:2px; style B fill:#e9edf7,stroke:#455d8d,stroke-width:2px; style C fill:#e9edf7,stroke:#455d8d,stroke-width:2px;&lt;/quote&gt;
    &lt;p&gt;Your analytics tools can track this flow without additional instrumentation. Every URL parameter becomes a dimension you can analyze.&lt;/p&gt;
    &lt;head rend="h3"&gt;Versioning and Evolution&lt;/head&gt;
    &lt;p&gt;URLs can communicate API versions, feature flags, and experiments:&lt;/p&gt;
    &lt;code&gt;?v=2                   // API version
?beta=true             // Beta features
?experiment=new-ui     // A/B test variant
&lt;/code&gt;
    &lt;p&gt;This makes gradual rollouts and backwards compatibility much more manageable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anti-Patterns to Avoid&lt;/head&gt;
    &lt;p&gt;Even with the best intentions, itâs easy to misuse URL state. Here are common pitfalls:&lt;/p&gt;
    &lt;head rend="h3"&gt;âState Only in Memoryâ SPAs&lt;/head&gt;
    &lt;p&gt;The classic single-page app mistake:&lt;/p&gt;
    &lt;code&gt;// User hits refresh and loses everything
const [filters, setFilters] = useState({});
&lt;/code&gt;
    &lt;p&gt;If your app forgets its state on refresh, youâre breaking one of the webâs fundamental features. Users expect URLs to preserve context. I remember a viral video from years ago where a Reddit user vented about an e-commerce site: every time she hit âBack,â all her filters disappeared. Her frustration summed it up perfectly. If users lose context, they lose patience.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sensitive Data in URLs&lt;/head&gt;
    &lt;p&gt;This one seems obvious, but itâs worth repeating:&lt;/p&gt;
    &lt;code&gt;// NEVER DO THIS
?password=secret123
&lt;/code&gt;
    &lt;p&gt;URLs are logged everywhere: browser history, server logs, analytics, referrer headers. Treat them as public.&lt;/p&gt;
    &lt;head rend="h3"&gt;Inconsistent or Opaque Naming&lt;/head&gt;
    &lt;code&gt;// Unclear and inconsistent
?foo=true&amp;amp;bar=2&amp;amp;x=dark

// Self-documenting and consistent
?mobile=true&amp;amp;page=2&amp;amp;theme=dark
&lt;/code&gt;
    &lt;p&gt;Choose parameter names that make sense. Future you (and your team) will thank you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Overloading URLs with Complex State&lt;/head&gt;
    &lt;code&gt;?config=eyJtZXNzYWdlIjoiZGlkIHlvdSByZWFsbHkgdHJpZWQgdG8gZGVjb2RlIHRoYXQ_IiwiZmlsdGVycyI6eyJzdGF0dXMiOlsiYWN0aXZlIiwicGVuZGluZyJdLCJwcmlvcml0eSI6WyJoaWdoIiwibWVkaXVtIl0sInRhZ3MiOlsiZnJvbnRlbmQiLCJyZWFjdCIsImhvb2tzIl0sInJhbmdlIjp7ImZyb20iOiIyMDI0LTAxLTAxIiwidG8iOiIyMDI0LTEyLTMxIn19LCJzb3J0Ijp7ImZpZWxkIjoiY3JlYXRlZEF0Iiwib3JkZXIiOiJkZXNjIn0sInBhZ2luYXRpb24iOnsicGFnZSI6MSwibGltaXQiOjIwfX0==
&lt;/code&gt;
    &lt;p&gt;If you need to base64-encode a massive JSON object, the URL probably isnât the right place for that state.&lt;/p&gt;
    &lt;head rend="h3"&gt;URL Length Limits&lt;/head&gt;
    &lt;p&gt;Browsers and servers impose practical limits on URL length (usually between 2,000 and 8,000 characters) but the reality is more nuanced. As this detailed Stack Overflow answer explains, limits come from a mix of browser behavior, server configurations, CDNs, and even search engine constraints. If youâre bumping against them, itâs a sign you need to rethink your approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Breaking the Back Button&lt;/head&gt;
    &lt;code&gt;// Replacing state incorrectly
history.replaceState({}, '', newUrl); // Used when pushState was needed
&lt;/code&gt;
    &lt;p&gt;Respect browser history. If a user action should be âundoableâ via the back button, use &lt;code&gt;pushState&lt;/code&gt;. If itâs a refinement, use &lt;code&gt;replaceState&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Thought&lt;/head&gt;
    &lt;p&gt;That PrismJS URL reminded me of something important: good URLs donât just point to content. They describe a conversation between the user and the application. They capture intent, preserve context, and enable sharing in ways that no other state management solution can match.&lt;/p&gt;
    &lt;p&gt;Weâve built increasingly sophisticated state management libraries like Redux, MobX, Zustand, Recoil and others. They all have their place but sometimes the best solution is the one thatâs been there all along.&lt;/p&gt;
    &lt;p&gt;In my previous article, I wrote about the hidden costs of bad URL design. Today, weâve explored the flip side: the immense value of good URL design. URLs arenât just addresses. Theyâre state containers, user interfaces, and contracts all rolled into one.&lt;/p&gt;
    &lt;p&gt;If your app forgets its state when you hit refresh, youâre missing one of the webâs oldest and most elegant features.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alfy.blog/2025/10/31/your-url-is-your-state.html"/><published>2025-11-02T11:12:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45789556</id><title>Mock – An API creation and testing utility: Examples</title><updated>2025-11-02T22:35:51.952269+00:00</updated><content>&lt;doc fingerprint="8c9aec89ac00d802"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How-tos &amp;amp; Examples¶&lt;/head&gt;
    &lt;head rend="h2"&gt;Delaying specific endpoints¶&lt;/head&gt;
    &lt;p&gt;Making an existing API slow can be easily accomplished combining mock’s Base APIs and the delay option.&lt;/p&gt;
    &lt;code&gt;$ mock serve -p 8000 --base example.com --delay 2000
&lt;/code&gt;
    &lt;p&gt;You may want however to make a specific endpoint slow instead of the whole API. This can be achieved using middlewares:&lt;/p&gt;
    &lt;code&gt;$ mock serve -p 8000 --base example.com --middleware '
if [ "${MOCK_REQUEST_ENDPOINT}" = "some/endpoint" ]
then
    sleep 2 # wait two seconds
fi
'
&lt;/code&gt;
    &lt;p&gt;With that last example, our API at &lt;code&gt;localhost:8000&lt;/code&gt; will act as a proxy to
&lt;code&gt;example.com&lt;/code&gt;. All requests will be responded immediately except
&lt;code&gt;some/endpoint&lt;/code&gt; which will have a delay of 2 seconds.&lt;/p&gt;
    &lt;head rend="h2"&gt;An API powered by multiple languages¶&lt;/head&gt;
    &lt;code&gt;$ mock serve -p 3000 \
    --route js \
    --exec '
node &amp;lt;&amp;lt;EOF | mock write
console.log("Hello from Node.js!")
EOF
' \
    --route python \
    --exec '
python3 &amp;lt;&amp;lt;EOF | mock write
print("Hello from Python!")
EOF
' \
    --route php \
    --exec '
php &amp;lt;&amp;lt;EOF | mock write
&amp;lt;?php
echo "Hello from PHP!\n";
?&amp;gt;
EOF
'
&lt;/code&gt;
    &lt;p&gt;Let’s test it:&lt;/p&gt;
    &lt;code&gt;$ curl localhost:3000/js
# Prints out: Hello from Node.js!
$ curl localhost:3000/python
# Prints out: Hello from Python!
$ curl localhost:3000/php
# Prints out: Hello from PHP!
&lt;/code&gt;
    &lt;head rend="h2"&gt;A stateful API¶&lt;/head&gt;
    &lt;code&gt;$ export TMP=$(mktemp)
$ printf "0" &amp;gt; "${TMP}"

$ mock serve -p 3000 \
    --route '/hello' \
    --exec '
printf "%s + 1\n" "$(cat ${TMP})" | bc | sponge "${TMP}"

printf "This server has received %s request(s) so far." "$(cat '"${TMP}"')" | mock write
'
&lt;/code&gt;
    &lt;p&gt;Let’s test it:&lt;/p&gt;
    &lt;code&gt;$ curl localhost:3000/hello
# Prints out: This server has received 1 request(s) so far.
$ curl localhost:3000/hello
# Prints out: This server has received 2 request(s) so far.
$ curl localhost:3000/hello
# Prints out: This server has received 3 request(s) so far.
&lt;/code&gt;
    &lt;head rend="h2"&gt;A CRUD API with a simple data storage¶&lt;/head&gt;
    &lt;p&gt;The following API does two tasks: add users and fetch users.&lt;/p&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;To run this example, you’ll need to have jq installed, as the “/users” endpoint uses it for parsing.&lt;/p&gt;
    &lt;code&gt;$ export DATA_DIR=$(mktemp -d)

$ mock serve -p 3000 \
    --route 'user' \
    --method POST \
    --exec '
# Insert a new user

USER_NAME=$(mock get-payload name)
USER_EMAIL=$(mock get-payload email)
USER_COUNT=$(ls $DATA_DIR | wc -l)
NEW_USER_ID="$(printf "%s + 1\n" "${USER_COUNT}" | bc)"

printf "New user ID generated: %s\n" "${NEW_USER_ID}"

printf '"'"'{"name":"%s","email":"%s"}'"'"' "${USER_NAME}" "${USER_EMAIL}" &amp;gt; "${DATA_DIR}/${NEW_USER_ID}.json"

printf '"'"'{"id":"%s"}'"'"' "${NEW_USER_ID}" | mock write
' \
    --route 'user/{user_id}' \
    --exec '
# Get an existing user

USER_ID="$(mock get-route-param user_id)"
USER_FILE="${DATA_DIR}/${USER_ID}.json"

if [ ! -f "${USER_FILE}" ]
then
    mock set-status 400

    exit 0
fi

mock write &amp;lt; "${USER_FILE}"
' \
    --route 'users' \
    --exec '
# Gets ALL users

cat $DATA_DIR/*.json | jq -s | mock write
'
&lt;/code&gt;
    &lt;p&gt;Let’s now test it:&lt;/p&gt;
    &lt;code&gt;$ curl -X POST localhost:3000/user \
    -H 'Content-Type: application/json' \
    -d @- &amp;lt;&amp;lt;EOF
{"name":"John Doe","email":"john.doe@example.com"}
EOF
# Prints out: {"id":"1"}

$ curl -X POST localhost:3000/user \
    -H 'Content-Type: application/json' \
    -d @- &amp;lt;&amp;lt;EOF
{"name":"Jane Doe","email":"jane.doe@example.com"}
EOF
# Prints out: {"id":"2"}

$ curl -v localhost:3000/user/1
# Prints out: {"name":"John Doe","email":"john.doe@example.com"}
$ curl -v localhost:3000/user/2
# Prints out: {"name":"Jane Doe","email":"jane.doe@example.com"}
$ curl -v localhost:3000/users
# Prints out: [
#  {"name":"John Doe","email":"john.doe@example.com"},
#  {"name":"Jane Doe","email":"jane.doe@example.com"}
# ]
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dhuan.github.io/mock/latest/examples.html"/><published>2025-11-02T11:30:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45789602</id><title>Tongyi DeepResearch – open-source 30B MoE Model that rivals OpenAI DeepResearch</title><updated>2025-11-02T22:35:51.781824+00:00</updated><content>&lt;doc fingerprint="efd86393ad1b861a"&gt;
  &lt;main&gt;
    &lt;p&gt;GITHUB HUGGINGFACE MODELSCOPE SHOWCASE&lt;/p&gt;
    &lt;head rend="h2"&gt;From Chatbot to Autonomous Agent&lt;/head&gt;
    &lt;p&gt;We are proud to present Tongyi DeepResearch, the first fully open‑source Web Agent to achieve performance on par with OpenAI’s DeepResearch across a comprehensive suite of benchmarks. Tongyi DeepResearch demonstrates state‑of‑the‑art results, scoring 32.9 on the academic reasoning task Humanity’s Last Exam (HLE), 43.4 on BrowseComp and 46.7 on BrowseComp‑ZH in extremely complex information‑seeking tasks, and achieving a score of 75 on the user‑centric xbench‑DeepSearch benchmark, systematically outperforming all existing proprietary and open‑source Deep Research agents.&lt;/p&gt;
    &lt;p&gt;Beyond the model, we share a complete and battle‑tested methodology for creating such advanced agents. Our contribution details a novel data synthesis solution applied across the entire training pipeline, from Agentic Continual Pre‑training (CPT) and Supervised Fine‑Tuning (SFT) for cold‑starting, to the final Reinforcement Learning (RL) stage. For RL, we provide a full‑stack solution, including algorithmic innovations, automated data curation, and robust infrastructure. For inference, the vanilla ReAct framework showcases the model’s powerful intrinsic capabilities without any prompt engineering, while the advanced Heavy Mode (test‑time‑scaling) demonstrates the upper limits of its complex reasoning and planning potential.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continual Pre‑training and Post‑training Empowered by Fully Synthetic Data&lt;/head&gt;
    &lt;head rend="h3"&gt;Continual Pre‑training Data&lt;/head&gt;
    &lt;p&gt;We introduce Agentic CPT to deep research agent training, creating powerful agentic foundation models for post‑training. We propose AgentFounder, a systematic and scalable solution for large‑scale data synthesis that creates a data flywheel with data from the post‑training pipeline.&lt;/p&gt;
    &lt;p&gt;Data Reorganization and Question Construction. We continuously collect data from various sources, including documents, publicly available crawled data, knowledge graphs, and historical trajectories and tool invocation records (e.g., search results with links). As shown in the figure, these diverse data sources are restructured into an entity‑anchored open‑world knowledge memory. Based on randomly sampled entities and their corresponding knowledge, we generate multi‑style (question,answer) pairs.&lt;/p&gt;
    &lt;p&gt;Action Synthesis. Based on diverse problems and historical trajectories, we construct first‑order action synthesis data and higher‑order action synthesis data. Our method enables large‑scale and comprehensive exploration of the potential reasoning‑action space within offline environments, thereby thereby eliminating the need for additional commercial tool API calls. Specifically, for the higher‑order action synthesis, we remodel trajectories as multi‑step decision‑making processes to enhance the model’s decision‑making capabilities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post-training Data&lt;/head&gt;
    &lt;p&gt;High-quality synthetic QA pairs&lt;/p&gt;
    &lt;p&gt;We develop an end‑to‑end solution for synthetic data generation. This fully automated process requires no human intervention to construct super‑human quality datasets, designed to push the boundaries of AI agent performance. Through long‑term exploration and iteration‑from early methods like reverse‑engineering QA pairs from clickstreams (WebWalker) to the more systematic graph‑based synthesis (WebSailor and WebSailor‑V2), then the formalized task modeling (WebShaper)‑our approach ensures both exceptional data quality and massive scalability, breaking through the upper limits of model capabilities.&lt;/p&gt;
    &lt;p&gt;To address complex, high‑uncertainty questions, we synthesize web‑based QA data through a novel pipeline. The process begins by constructing a highly interconnected knowledge graph via random walks and isomorphic tables towards tabular data fusion from real‑world websites , ensuring a realistic information structure. We then sample subgraphs and subtables to generate initial questions and answers. The crucial step involves intentionally increasing difficulty by strategically obfuscating or blurring information within the question. This practical approach is grounded in a complete theoretical framework, where we formally model QA difficulty as a series of controllable “atomic operations” (e.g., merging entities with similar attributes) on entity relationships, allowing us to systematically increase complexity.&lt;/p&gt;
    &lt;p&gt;To further reduce inconsistencies between the organized information structure and the reasoning structure of QA, enable more controllable difficulty and structure scaling of reasoning, we proposed a formal modeling of the information‑seeking problem based on set theory. With this formalization, we developed agents that expands the problem in a controlled manner, and minimizes reasoning shortcuts and structural redundancy, leading to further improved QA quality. Moreover, this formal modeling also allows for efficient verification of QA correctness, effectively addressing the challenge of validating synthetic information‑seeking data for post‑training.&lt;/p&gt;
    &lt;p&gt;Furthermore, we have developed an automated data engine to scale up the creation of PhD‑level research questions. This engine begins with a multi‑disciplinary knowledge base, generating “seed” QA pairs that require multi‑source reasoning. Each seed then enters a self‑guided loop of “iterative complexity upgrades”, where a question‑crafting agent is equipped with a powerful toolset including web search, academic retrieval, and a Python execution environment. In each iteration, the agent expands knowledge boundaries, deepens conceptual abstraction, and even constructs computational tasks, creating a virtuous cycle where the output of one round becomes the more complex input for the next, ensuring a controllable and systematic escalation of task difficulty.&lt;/p&gt;
    &lt;p&gt;Unleashing Agent Capabilities with Diverse Reasoning Pattern&lt;/p&gt;
    &lt;p&gt;To bootstrap the model’s initial capabilities, we constructed a set of trajectories via rejection sampling, based on the ReAct and IterResearch frameworks (for details, see below). On one hand, ReAct, as a classic and foundational multi-turn reasoning format, instills rich reasoning behaviors and reinforces the model’s ability to adhere to structured formats.&lt;/p&gt;
    &lt;p&gt;On the other hand, we introduce IterResearch, an innovative agent paradigm (detailed below). It unleashes the model’s full reasoning potential by dynamically reconstructing a streamlined workspace in each turn, ensuring that every decision is deliberate and well-considered. Leveraging IterResearch, we constructed a set of trajectories that integrate reasoning, planning, and tool-use, thereby strengthening the model’s capacity for sustained planning when confronted with Long-Horizon tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rollout Mode&lt;/head&gt;
    &lt;p&gt;We have conducted extensive exploration into the rollout paradigms for DeepResearch‑type agents. As a result, our final model supports multiple rollout formats, including the native ReAct Mode and the context‑managing Heavy Mode.&lt;/p&gt;
    &lt;head rend="h3"&gt;Native ReAct Mode&lt;/head&gt;
    &lt;p&gt;Our model demonstrates excellent performance using the native ReAct reasoning paradigm without any prompt engineering. It strictly adheres to the Thought‑Action‑Observation cycle, performing multiple iterations to solve problems. With a model context length of 128K, it can handle a large number of interaction rounds, fully achieving scaling in its interaction with the environment. ReAct’s simplicity and universality provide the clearest benchmark for a model’s intrinsic capabilities and the efficacy of our training pipeline.&lt;/p&gt;
    &lt;p&gt;Our choice of ReAct is heavily informed by “The Bitter Lesson”, which posits that general methods leveraging scalable computation ultimately outperform approaches that rely on complex, human‑engineered knowledge and intricate designs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Heavy Mode&lt;/head&gt;
    &lt;p&gt;In addition to the native ReAct mode, we have developed a “Heavy Mode” for complex, multi‑step research tasks. This mode is built on our new IterResearch paradigm, designed to push the agent’s capabilities to their limit.&lt;/p&gt;
    &lt;p&gt;The IterResearch paradigm was created to solve the “cognitive suffocation” and “noise pollution” that occurs when agents accumulate all information into a single, ever‑expanding context. Instead, IterResearch deconstructs a task into a series of “research rounds”.&lt;/p&gt;
    &lt;p&gt;In each round, the agent reconstructs a streamlined workspace using only the most essential outputs from the previous round. Within this focused workspace, the agent analyzes the problem, integrates key findings into a continuously evolving central report, and then decides its next action‑either gathering more information or providing a final answer. This iterative process of “synthesis and reconstruction” allows the agent to maintain a clear “cognitive focus” and high reasoning quality throughout long tasks.&lt;/p&gt;
    &lt;p&gt;Building on this, we propose the Research‑Synthesis framework. In this model, multiple Research Agents use the IterResearch process to explore a problem in parallel. A final Synthesis Agent then integrates their refined reports and conclusions to produce a more comprehensive final answer. This parallel structure enables the model to consider a wider range of research paths within a limited context window, pushing its performance to the limit.&lt;/p&gt;
    &lt;head rend="h2"&gt;End-to‑End Agent Training Pipeline&lt;/head&gt;
    &lt;p&gt;Training an agentic model like this required us to rethink the entire model training pipeline, from pre‑training to fine‑tuning to reinforcement learning. We established a new paradigm for agent model training that connects Agentic CPT → Agentic SFT → Agentic RL, creating a seamless end‑to‑end training loop for an AI agent. Here’s how we tackled the final stage with reinforcement learning, which was crucial for aligning the agent’s behavior with high‑level goals:&lt;/p&gt;
    &lt;head rend="h3"&gt;On‑Policy Agent Reinforcement Learning (RL)&lt;/head&gt;
    &lt;p&gt;Constructing a high‑quality agent through RL is a complex system engineering challenge; if this entire development process is viewed as a “reinforcement learning” loop, any instability or lack of robustness in its components can lead to erroneous “reward” signals. We will now share our practices in RL, covering both the algorithmic and infrastructure sides.&lt;/p&gt;
    &lt;p&gt;For RL algorithm, we made several algorithmic breakthroughs, using a customized on‑policy Group Relative Policy Optimization (GRPO). We employ a strictly on‑policy training regimen, ensuring that the learning signal is always relevant to the model’s current capabilities. The training objective is optimized using a token‑level policy gradient loss. Second, to further reduce variance in the advantage estimation, we adopt a leave‑one‑out strategy. Furthermore, we employ a conservative strategy for negative samples, having observed that an unfiltered set of negative trajectories significantly degrades training stability. This can manifest as a “format collapse” phenomenon after extended training. To mitigate this, we selectively exclude certain negative samples from the loss calculation, for instance, those that do not yield a final answer because they exceed a length limit. For the sake of efficiency, we do not employ dynamic sampling. We instead leverage larger batch and group sizes, which serve to maintain smaller variance and provide adequate supervision.&lt;/p&gt;
    &lt;p&gt;The training dynamics demonstrate effective learning, with a consistent upward trend in reward. Meanwhile, policy entropy remains consistently high, indicating sustained exploration and preventing premature convergence. We attribute this to the non‑stationary nature of the web environment, which naturally fosters a robust, adaptive policy and obviates the need for explicit entropy regularization.&lt;/p&gt;
    &lt;p&gt;We consider that the algorithm is important but not the only decisive factor in the success of Agentic RL. We have experimented with many different algorithms and tricks, and find that data and stability of the training environment are likely the more critical components in determining whether the RL works. Interestingly, we have tested to train the model directly on the BrowseComp testing set, but the results are substantially poorer than when using our synthetic data. We hypothesize that this disparity arises because the synthetic data offers a more consistent distribution, which allows the model to be more effectively tailored. Conversely, the human‑annotated data (such as BrowseComp) is inherently noisier. Given its limited scale, it is difficult to approximate a learnable underlying distribution, which consequently hinders the model to learn and generalize from it.&lt;/p&gt;
    &lt;p&gt;On the infrastructure side, training an agent with tools required us to develop a highly stable and efficient environment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Synthetic Training Environment: Relying on live web APIs for development is expensive, slow, and inconsistent. We addressed this by creating a simulated training environment using an offline Wikipedia database and a custom tool suite. By adapting our data pipeline to generate high‑quality, complex tasks for this environment, we created a cost‑effective, fast, and controllable platform that dramatically accelerates our research and iteration.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stable &amp;amp; Efficient Tool Sandbox: To ensure reliable tool use during agent training and evaluation, we developed a unified sandbox. The sandbox handles concurrency and failure gracefully by caching results, retrying failed calls, and using redundant providers as fallbacks (e.g., a backup search API). This provides the agent with a fast and deterministic experience, which is crucial for preventing tool errors from corrupting its learning trajectory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automatic Data Curation: Data is the core driver of model capability enhancement; its importance even surpasses that of the algorithm. The quality of the data directly determines the upper bound on the model’s ability to generalize to out‑of‑distribution scenarios through self‑exploration. To address this challenge, we optimize data in real time, guided by training dynamics. This optimization is achieved through a fully automated data synthesis and filtering pipeline that dynamically adjusts the training set. By closing the loop between data generation and model training, this approach not only ensures training stability but also delivers substantial performance gains.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On‑Policy Asynchronous Framework: We implemented a custom step‑level asynchronous RL training loop on top of rLLM. Multiple agent instances interact with the (simulated or real) environment in parallel, each producing trajectories.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Through these measures, we “closed the loop” on agent training. Starting from a raw model, we did Agentic pre‑training to initialize tool‑use skills, then supervised finetuning on expert‑like data to cold start, and finally on‑policy RL to let the model conduct self‑evolution. This full‑stack approach ‑ now proven with Tongyi DeepResearch ‑ presents a new paradigm for training AI agents that can robustly solve complex tasks in dynamic environments.&lt;/p&gt;
    &lt;p&gt;(Our RL approach is inspired by several past work from Agentica. We adapt their rLLM framework and extend it to train our web agents.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Real‑World Applications and Impact&lt;/head&gt;
    &lt;p&gt;Tongyi DeepResearch is not just a research showcase; it’s already powering real applications within Alibaba and beyond, demonstrating its value in practical scenarios:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gaode Mate (Map &amp;amp; Navigation Agent): Collaborating with Amap (Gaode) Team, we co‑developed “Xiao Gao,” an AI copilot that leverages the app’s rich toolset. It can execute complex travel planning commands, like creating a multi‑day driving tour that includes specific scenic spots and pet‑friendly hotels. Through multi‑step reasoning, Xiao Gao autonomously researches and integrates information to produce a detailed, personalized itinerary, offering an intelligent planning experience that far surpasses standard navigation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tongyi FaRui (Legal Research Agent): Empowered by our DeepResearch architecture, FaRui now functions as a true legal agent. It autonomously executes complex, multi‑step research tasks that mirror a junior attorney’s workflow‑systematically retrieving case law, cross‑referencing statutes, and synthesizing analysis. Crucially, all conclusions are grounded in verifiable judicial sources and delivered with precise case and statute citations, ensuring professional‑grade accuracy and credibility.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;Our future work will address three key limitations. First, the current 128k context length is still insufficient for the most complex long‑horizon tasks, requiring us to explore expanded context windows and more sophisticated information management. Second, our training pipeline’s scalability remains unproven on foundation models significantly larger than our 30B‑scale MoE, and we plan to validate our methods on larger‑scale models. Lastly, we aim to improve the efficiency of our reinforcement learning framework by investigating techniques like partial rollouts, which will necessitate solving the challenges of off‑policy training, such as distributional shift.&lt;/p&gt;
    &lt;head rend="h2"&gt;Series Work&lt;/head&gt;
    &lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following papers:&lt;/p&gt;
    &lt;p&gt;[1] WebWalker: Benchmarking LLMs in Web Traversal&lt;/p&gt;
    &lt;p&gt;[2] WebDancer: Towards Autonomous Information Seeking Agency&lt;/p&gt;
    &lt;p&gt;[3] WebSailor: Navigating Super‑human Reasoning for Web Agent&lt;/p&gt;
    &lt;p&gt;[4] WebShaper: Agentically Data Synthesizing via Information‑Seeking Formalization&lt;/p&gt;
    &lt;p&gt;[5] WebWatcher: Breaking New Frontier of Vision‑Language Deep Research Agent&lt;/p&gt;
    &lt;p&gt;[6] WebResearch: Unleashing reasoning capability in Long‑Horizon Agents&lt;/p&gt;
    &lt;p&gt;[7] ReSum: Unlocking Long‑Horizon Search Intelligence via Context Summarization&lt;/p&gt;
    &lt;p&gt;[8] WebWeaver: Structuring Web‑Scale Evidence with Dynamic Outlines for Open‑Ended Deep Research&lt;/p&gt;
    &lt;p&gt;[10] Scaling Agents via Continual Pre‑training&lt;/p&gt;
    &lt;p&gt;[11] Towards General Agentic Intelligence via Environment Scaling&lt;/p&gt;
    &lt;p&gt;Our team has a long‑standing commitment to the research and development of deep research agents. Over the past six months, we have consistently published one technical report per month, totaling five to date. Today, we are excited to simultaneously release six new reports and share our Tongyi DeepResearch‑30B‑A3B model with the community.&lt;/p&gt;
    &lt;p&gt;Stay tuned for our next generation of agentic models.&lt;/p&gt;
    &lt;code&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi DeepResearch: A New Era of Open-Source AI Researchers},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/"/><published>2025-11-02T11:43:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45790015</id><title>X.org Security Advisory: multiple security issues X.Org X server and Xwayland</title><updated>2025-11-02T22:35:51.393946+00:00</updated><content>&lt;doc fingerprint="981a9199600a0298"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;X.Org Security Advisory: multiple security issues X.Org X server and Xwayland&lt;/head&gt; Olivier Fourdan ofourdan at redhat.com &lt;lb/&gt;Tue Oct 28 13:22:18 UTC 2025&lt;quote&gt;====================================================================== X.Org Security Advisory: October 28, 2025 Issues in X.Org X server prior to 21.1.18 and Xwayland prior to 24.1.8 ====================================================================== Multiple issues have been found in the X server and Xwayland implementations published by X.Org for which we are releasing security fixes for in xorg-server-21.1.19 and xwayland-24.1.9. 1) CVE-2025-62229: Use-after-free in XPresentNotify structures creation Using the X11 Present extension, when processing and adding the notifications after presenting a pixmap, if an error occurs, a dangling pointer may be left in the error code path of the function causing a use-after-free when eventually destroying the notification structures later. Introduced in: Xorg 1.15 Fixed in: xorg-server-21.1.19 and xwayland-24.1.9 Fix: https://gitlab.freedesktop.org/xorg/xserver/-/commit/5a4286b1 Found by: Jan-Niklas Sohn working with Trend Micro Zero Day Initiative. 2) CVE-2025-62230: Use-after-free in Xkb client resource removal When removing the Xkb resources for a client, the function XkbRemoveResourceClient() will free the XkbInterest data associated with the device, but not the resource associated with it. As a result, when the client terminates, the resource delete function triggers a use-after-free. Introduced in: X11R6 Fixed in: xorg-server-21.1.19 and xwayland-24.1.9 Fix: https://gitlab.freedesktop.org/xorg/xserver/-/commit/99790a2c https://gitlab.freedesktop.org/xorg/xserver/-/commit/10c94238 Found by: Jan-Niklas Sohn working with Trend Micro Zero Day Initiative. 3) CVE-2025-62231: Value overflow in Xkb extension XkbSetCompatMap() The XkbCompatMap structure stores some of its values using an unsigned short, but fails to check whether the sum of the input data might overflow the maximum unsigned short value. Introduced in: X11R6 Fixed in: xorg-server-21.1.19 and xwayland-24.1.9 Fix: https://gitlab.freedesktop.org/xorg/xserver/-/commit/475d9f49 Found by: Jan-Niklas Sohn working with Trend Micro Zero Day Initiative. ------------------------------------------------------------------------ X.Org thanks all of those who reported and fixed these issues, and those who helped with the review and release of this advisory and these fixes. -------------- next part -------------- A non-text attachment was scrubbed... Name: OpenPGP_0x14706DBE1E4B4540.asc Type: application/pgp-keys Size: 2988 bytes Desc: OpenPGP public key URL: &amp;lt;https://lists.x.org/archives/xorg-announce/attachments/20251028/ff11c77e/attachment.key&amp;gt; -------------- next part -------------- A non-text attachment was scrubbed... Name: OpenPGP_signature.asc Type: application/pgp-signature Size: 203 bytes Desc: OpenPGP digital signature URL: &amp;lt;https://lists.x.org/archives/xorg-announce/attachments/20251028/ff11c77e/attachment.sig&amp;gt; &lt;/quote&gt;&lt;lb/&gt;More information about the xorg-announce
mailing list&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lists.x.org/archives/xorg-announce/2025-October/003635.html"/><published>2025-11-02T13:07:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45790293</id><title>Writing FreeDOS Programs in C</title><updated>2025-11-02T22:35:51.237972+00:00</updated><content>&lt;doc fingerprint="3ceffed571b967d9"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;This project was backed by Patreon supporters&lt;/head&gt;
    &lt;p&gt;This web programming guide started out as a video series on YouTube, supported through Patreon. Patrons at the "C programming" level and above (Patreon) got access to these extras:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Early access to the "C programming" videos&lt;/item&gt;
      &lt;item&gt;Exclusive access to the rest of the "programming guide" with more detail and information that didn't make it into the videos&lt;/item&gt;
      &lt;item&gt;A weekly Patreon forum to ask questions about that week's "C programming" topics (if you were following along with the videos and need help, this was the place to ask)&lt;/item&gt;
      &lt;item&gt;After the video series was finished, I edited the programming guide into a "teach yourself programming" book, via publishing partner Lulu. Patrons could purchase the book at cost.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.freedos.org/books/cprogramming/"/><published>2025-11-02T13:43:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45790827</id><title>Why don't you use dependent types?</title><updated>2025-11-02T22:35:51.138874+00:00</updated><content>&lt;doc fingerprint="a748029f0d0bb7f0"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;"Why don't you use dependent types?"&lt;/head&gt;[&lt;code&gt;&lt;nobr&gt;memories&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;AUTOMATH&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;LCF&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;type theory&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;Martin-Löf type theory&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;NG de Bruijn&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;ALEXANDRIA&lt;/nobr&gt;&lt;/code&gt; 
  
]

&lt;p&gt;To be fair, nobody asks me this exact question. But people have regularly asked why Isabelle dispenses with proof objects. The two questions are essentially the same, because proof objects are intrinsic to all the usual type theories. They are also completely unnecessary and a huge waste of space. As described in an earlier post, type checking in the implementation language (rather than in the logic) can ensure that only legitimate proof steps are executed. Robin Milner had this fundamental insight 50 years ago, giving us the LCF architecture with its proof kernel. But the best answer to the original question is simply this: I did use dependent types, for years.&lt;/p&gt;&lt;head rend="h3"&gt;My time with AUTOMATH&lt;/head&gt;&lt;p&gt;I was lucky enough to get some personal time with N G de Bruijn when he came to Caltech in 1977 to lecture about AUTOMATH. I never actually got to use this system. Back then, researchers used the nascent Internet (the ARPAnet) not to download software so much as to run software directly on the host computer, since most software was not portable. But Eindhoven University was not on the ARPAnet, and AUTOMATH was configured to run on a computer we did not have:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Until September 1973, the computer was the Electrologica X8, after that Burroughs 6700. In both cases the available multiprogranming systems required the use of ALGOL 60.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I did however read many of the research reports, including the PhD dissertation by LS Jutting, where he presents his translation of Landau’s text Grundlagen der Analysis (described last time) from German into AUTOMATH. It is no coincidence that many of my papers, from the earliest to the latest, copied the idea of formalising a text and attempting to be faithful to it, if possible line by line.&lt;/p&gt;&lt;p&gt;As an aside, note that while AUTOMATH was a system of dependent types, it did not embody the Curry–Howard correspondence (sometimes wrongly called the Curry–Howard–de Bruijn correspondence). That correspondence involves having a type theory strong enough to represent the predicate calculus directly in the form of types. In AUTOMATH you had to introduce the symbols and inference rules of your desired calculus in the form of axioms, much as you do with Isabelle. In short, AUTOMATH was a logical framework:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;like a big restaurant that serves all sorts of food: vegetarian, kosher, or anything else the customer wants&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;De Bruijn did not approve of the increasingly powerful type theories being developed in the 1990s. AUTOMATH was a weak language, a form of λ-calculus including a general product construction just powerful enough to express the inference rules of a variety of formalisms and to make simple definitions, again clearly the inspiration for Isabelle. Isabelle aims to be generic, like the big AUTOMATH restaurant. Only these days everybody prefers the same cuisine, higher-order logic, so Isabelle/HOL has become dominant. Unfortunately, I last spoke to Dick (as he was known to friends) when I was putting all my effort into Isabelle/ZF. He simply loathed set theory and saw mathematics as essentially typed. He never lived to see the enormous amount of advanced mathematics that would be formalised using types in Isabelle/HOL.&lt;/p&gt;&lt;p&gt;I annoyed him in another way. I kept asking, AUTOMATH looks natural, but how do we know that it is right? He eventually sent me a 300 page volume entitled The Language Theory of Automath. It describes AUTOMATH’s formal properties such as strong normalisation and Church–Rosser properties, but this was not the answer I wanted at all. I got that answer for a quite different type theory.&lt;/p&gt;&lt;head rend="h3"&gt;Martin-Löf type theory&lt;/head&gt;&lt;p&gt;In response to kind invitations from Bengt Nordström and Kent Petersson, I paid a number of visits to Chalmers University in Gothenburg to learn about Martin-Löf type theory. I was particularly impressed by its promise of a systematic and formal approach to program synthesis. I had already encountered intuitionism through a course on the philosophy of mathematics at Stanford University, as I recall taught by Ian Hacking. The “rightness” of Martin-Löf type theory was obvious, because it directly embodied the principles of intuition truth as outlined by Heyting: for example, that a proof of $A\land B$ consists of a proof of $A$ paired with a proof of $B$.&lt;/p&gt;&lt;p&gt;I devoted several years of research to Martin-Löf type theory. This included a whole year of intricate hand derivations to produce a paper that I once thought would be important, and the very first version of Isabelle. Yes: Isabelle began as an implementation of Martin-Löf type theory, which is still included in the distribution even today as Isabelle/CTT. But eventually I tired of what seemed to me a doctrinaire attitude bordering on a cult of personality around Per Martin-Löf. The sudden switch to intensional equality (everyone was expected to adopt the new approach) wrecked most of my work. Screw that.&lt;/p&gt;&lt;p&gt;You might ask, what about the calculus of constructions, which arose during that time and eventually gave us Rocq and Lean? (Not to mention LEGO.) To me they raised, and continue to raise, the same question I had put to de Bruijn. Gérard Huet said something like “it is nothing but function application”, which did not convince me. It’s clear that I am being fussy,1 because thousands of people find these formalisms perfectly natural and believable. But it is also true that the calculus of constructions underwent numerous changes over the past four decades. There seem to be several optional axioms that people sometimes adopt while attempting to minimise their use, like dieters enjoying an occasional croissant.&lt;/p&gt;&lt;head rend="h3"&gt;Decisions, decisions&lt;/head&gt;&lt;p&gt;We can see all this as an example of the choices we make in research. People were developing new formalisms. This specific fact was the impetus for making Isabelle generic in the first place. But we have to choose whether to spend our time developing formalisms or instead to choose a fixed formalism and see how far you can push it. Both are legitimate research goals.&lt;/p&gt;&lt;p&gt;For example, already in 1985, Mike Gordon was using higher-order logic to verify hardware. He was not distracted by the idea that some dependent type theory might work better for n-bit words and the like. The formalism that he implemented was essentially the same as the simple theory of types outlined by Alonzo Church in 1940. He made verification history using this venerable formalism, and John Harrison later found a clever way to encode the dimension of vector types including words. Isabelle/HOL also implements Church’s simple type theory, with one extension: axiomatic type classes. Isabella users also derive much power from the locale concept, a kind of module sysstem that lies outside any particular logic.&lt;/p&gt;&lt;p&gt;During all this time, both Martin-Löf type theory and the calculus of constructions went through several stages of evolution. It’s remarkable how the Lean community, by running with a certain version of the calculus, quickly formalised a vast amount of mathematics.&lt;/p&gt;&lt;head rend="h3"&gt;Pushing higher-order logic to its limit&lt;/head&gt;&lt;p&gt;I felt exceptionally lucky to win funding from the European Research Council for the advanced grant ALEXANDRIA. When I applied, homotopy type theory was still all the rage, so the proposal emphasised Isabelle’s specific advantages: its automation, its huge libraries and the legibility of its proofs.&lt;/p&gt;&lt;p&gt;The team started work with enthusiasm. Nevertheless, I fully expected that we would hit a wall, reaching mathematical material that could not easily be formalised in higher-order logic. Too much of Isabelle’s analysis library identified topological spaces with types. Isabelle’s abstract algebra library was old and crufty. A number of my research colleagues were convinced that higher-logic was not adequate for serious mathematics. But Anthony Bordg took up the challenge, leading a subproject to formalise Grothendieck schemes.&lt;/p&gt;&lt;p&gt;For some reason I had a particular fear of the field extension $F[a]$, which extends the field $F$ with some $a$ postulated to be a root of some polynomial over $F$. (For example, the field of complex numbers is precisely $\mathbb{R}[i]$, where $i$ is postulated to be a root of $x^2+1=0$.) And yet an early outcome of ALEXANDRIA was a proof, by Paulo Emílio de Vilhena and Martin Baillon, that every field admits an algebraically closed extension. This was the first proof of that theorem in any proof assistant, and its proof involves an infinite series of field extensions.&lt;/p&gt;&lt;p&gt;We never hit any wall. As our group went on to formalise more and more advanced results, such as the Balog–Szemerédi–Gowers theorem, people stopped saying “you can’t formalise mathematics without dependent types” and switched to saying “dependent types give you nicer proofs”. But they never proved this claim.&lt;/p&gt;&lt;p&gt;Now that dependent type theory has attained maturity and has an excellent tool in the form of Lean, shall I go back to dependent types? I am not tempted. The only aspects of Lean that I envy are its huge community and the Blueprint tool. I hear too many complaints about Lean’s performance. I’ve heard of too many cases where dependent types played badly with intensional equality – I sat through an entire talk on this topic – or otherwise made life difficult. Quite a few people have told me that the secret of dependent types is knowing when not to use them. And so, to me, they have too much in common with Tesla’s Full Self-Driving.&lt;/p&gt;&lt;p&gt;Addendum: somebody commented on Hacker News that higher-order logic is too weak (in terms of proof-theoretic strength) to formalise post-WWII mathematics. This is not quite right. It is true that higher-order logic is much, much weaker than ZF set theory. But one of the most striking findings of ALEXANDRIA is that this is no obstacle to doing advanced mathematics, say to formalise Grothendieck schemes. Such elaborate towers of definitions do not seem to ascend especially high in the set-theoretic hierarchy. I can only recall a couple of proofs (this one, and that one) that required strengthening higher-order logic with the ZF axioms (which is easily done). These were theorems that referred to ZF entities in their very statements.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Especially as regards constructive mathematics. To its founders, intuitionism is a philosophy suspicious of language, which it relegates to the purpose of recording and communicating mathematical thoughts. This is the opposite of today’s “constructive mathematics”, which refers the use of a formalism satisfying certain syntactic properties. ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lawrencecpaulson.github.io//2025/11/02/Why-not-dependent.html"/><published>2025-11-02T15:06:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45790867</id><title>New South Korean national law will turn large parking lots into solar farms</title><updated>2025-11-02T22:35:51.072213+00:00</updated><content>&lt;doc fingerprint="6803b15ebfb795ba"&gt;
  &lt;main&gt;
    &lt;p&gt;Starting this month, parking lots in South Korea with more than 80 spaces will be required to install solar canopies and carports. But, unlike similar laws that have been proposed in the US, this new law doesn’t just apply to new construction – existing lots will have to comply as well!&lt;/p&gt;
    &lt;p&gt;South Korea’s Ministry of Trade, Industry and Energy announced in August that it has prepared an amendment to the Enforcement Decree of the Act on the Promotion of the Development, Use, and Diffusion of New and Renewable Energy to the effect that all publicly- and privately-owned parking lots in the Asian country with room for more than 80 vehicles will be compelled to add solar panels to their lots in a move designed to proactively expand renewable energy and create more solar and construction jobs.&lt;/p&gt;
    &lt;p&gt;In addition to creating jobs and working to stabilize the local grid with more renewable energy, the proposed solar canopies will offer a number of practical, day-to-day benefits for Korean drivers, as well.&lt;/p&gt;
    &lt;p&gt;The shaded structures will protect vehicles from heavy rain, snow, and the blistering summer sun — keeping interiors cooler, extending the life of plastics and upholstery, and even helping to preserve battery range in EVs and PHEVs by reducing their AC loads (and, of course, provide charging while the cars are parked).&lt;/p&gt;
    &lt;p&gt;To their credit, Ministry officials absolutely get it. “Through this mandatory installation,” one unnamed official told Asia Business Daily, “we expect to expand the distribution of eco-friendly renewable energy generation facilities while providing tangible benefits to the public. By utilizing idle land such as parking lots, we can maximize land use efficiency. In addition, installing canopy-type solar panels can provide shade underneath, offering noticeable comfort to people using parking lots during hot weather.”&lt;/p&gt;
    &lt;p&gt;The new rule was approved in late September, and is expected to go into effect later this month, with new installation projects set to begin immediately.&lt;/p&gt;
    &lt;head rend="h2"&gt;It could work here&lt;/head&gt;
    &lt;p&gt;South Korea is proving that an idea like is practical. Here in the US, we’re proving that out, too – the Northwest Fire District in Arizona partnered with Standard Solar to build a conceptually similar, 657 kW solar carport system across 12 parking lots (shown, above) that delivers more than 1.23 million kWh of clean, emissions-free power annually and offsets the equivalent of 185,000 vehicles’ worth of harmful carbon emissions.&lt;/p&gt;
    &lt;p&gt;That’s just Arizona. In New York, a new initiative to help expand solar into parking lots has more than doubled commercially zoned land where EV charging stations can be sited, “freeing up” an additional 400 million square feet of space throughout the city.&lt;/p&gt;
    &lt;p&gt;Sun-rich states like Texas, New Mexico, and Florida could also benefit, and even if we’re “just” adding fresh energy sources to municipal parking, dealer lots, and public schools, we could do a lot to reduce the cost of energy generation for the entire community. And, for what it’s worth, that seems to be right in line with the big reasons why people are choosing to add solar to their homes today.&lt;/p&gt;
    &lt;head rend="h2"&gt;Top comment by Pedro&lt;/head&gt;
    &lt;p&gt;Germany has a broader incentive for large commercial roofs to add solar. It has also been successful. I think the underlying goal here is that the more energy generated by renewables, means stronger national sovereignty: No coal, oil. or natural gas reliance. That takes a lot of coercive cards off the table.&lt;/p&gt;
    &lt;p&gt;What do you guys think – would something like this work in the US, or are we too far gone down the sophomoric, pseudo-libertarian rabbit hole to ever dig our way out? Let us know your take in the comments.&lt;/p&gt;
    &lt;p&gt;SOURCE | IMAGES: Asia Business Daily, via LinkedIn; Standard Solar.&lt;/p&gt;
    &lt;p&gt;If you’re considering going solar, it’s always a good idea to get quotes from a few installers. To make sure you find a trusted, reliable solar installer near you that offers competitive pricing, check out EnergySage, a free service that makes it easy for you to go solar. It has hundreds of pre-vetted solar installers competing for your business, ensuring you get high-quality solutions and save 20-30% compared to going it alone. Plus, it’s free to use, and you won’t get sales calls until you select an installer and share your phone number with them.&lt;/p&gt;
    &lt;p&gt;Your personalized solar quotes are easy to compare online and you’ll get access to unbiased Energy Advisors to help you every step of the way. Get started here.&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://electrek.co/2025/11/02/new-national-law-will-turn-large-parking-lots-into-solar-power-farms/"/><published>2025-11-02T15:12:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45792166</id><title>Is Your Bluetooth Chip Leaking Secrets via RF Signals?</title><updated>2025-11-02T22:35:50.465413+00:00</updated><content>&lt;doc fingerprint="9fb01c14c0e0095a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Your Bluetooth Chip Leaking Secrets via RF Signals?&lt;/head&gt;
    &lt;quote&gt;@article{Ji2025IsYB, title={Is Your Bluetooth Chip Leaking Secrets via RF Signals?}, author={Yanning Ji and Elena Dubrova and Ruize Wang}, journal={IACR Cryptol. ePrint Arch.}, year={2025}, volume={2025}, pages={559}, url={https://api.semanticscholar.org/CorpusID:278151312} }&lt;/quote&gt;
    &lt;p&gt;A machine learning-assisted side-channel attack on the hardware AES accelerator of a Bluetooth chip used in millions of devices worldwide, ranging from wearables and smart home products to industrial IoT, can recover the full encryption key from 90,000 traces captured at a one-meter distance from the target device.&lt;/p&gt;
    &lt;head rend="h2"&gt;One Citation&lt;/head&gt;
    &lt;head rend="h3"&gt;Probabilistic Skipping-Based Data Structures with Robust Efficiency Guarantees&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science&lt;/p&gt;
    &lt;p&gt;This work presents adaptive attacks on all three aforementioned data structures that, in the case of hash tables and skip lists, cause exponential degradation compared to the input-independent setting, and proposes simple and efficient modifications to the original designs of these data structures to provide provable security against adaptive adversaries.&lt;/p&gt;
    &lt;head rend="h2"&gt;27 References&lt;/head&gt;
    &lt;head rend="h3"&gt;Screaming Channels: When Electromagnetic Side Channels Meet Radio Transceivers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2018&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Engineering, Physics&lt;/p&gt;
    &lt;p&gt;This paper presents a new side channel that affects mixed-signal chips used in widespread wireless communication protocols, such as Bluetooth and WiFi and argues that protections against side channels (such as masking or hiding) need to be used on this class of devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Understanding Screaming Channels: From a Detailed Analysis to Improved Attacks&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2020&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This work conducts a thorough experimental analysis of the peculiar properties of Screaming Channels, and provides a broader security evaluation of the leaks, helping the defender and radio designers to evaluate risk, and the need of countermeasures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Far Field EM Side-Channel Attack on AES Using Deep Learning&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2020&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This work presents the first deep learning-based side-channel attack on AES-128 using far field electromagnetic emissions as a side channel and can recover the key from less than 10K traces captured in an office environment at 15 m distance to target even if the measurement for each encryption is taken only once.&lt;/p&gt;
    &lt;head rend="h3"&gt;Advanced Far Field EM Side-Channel Attack on AES&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2021&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;Deep learning models are trained on "clean" traces, captured through a coaxial cable and the resulting models can extract the AES key from less than 500 traces on average captured at 15 m from the victim device without repeating each encryption more than once.&lt;/p&gt;
    &lt;head rend="h3"&gt;Attacking at non-harmonic frequencies in screaming-channel attacks&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2023&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Engineering, Physics&lt;/p&gt;
    &lt;p&gt;This work demonstrates that compromising signals appear not only at the harmonics and that leakage at non-harmonics can be exploited for successful attacks, and proposes two methodologies to locate frequencies that contain leakage and demonstrates that it appears atNon-harmonic frequencies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Machine learning in side-channel analysis: a first study&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2011&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This work comprehensively investigates the application of a machine learning technique in SCA, a powerful kernel-based learning algorithm: the Least Squares Support Vector Machine (LS-SVM) and the target is a software implementation of the Advanced Encryption Standard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power Analysis Attacks Against IEEE 802.15.4 Nodes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2016&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This work measures the leakage characteristics of the AES accelerator on the Atmel ATMega128RFA1, and demonstrates how this allows recovery of the encryption key from nodes running an IEEE 802.15.4 stack.&lt;/p&gt;
    &lt;head rend="h3"&gt;Screaming Channels Revisited: Encryption Key Recovery from AES-CCM Accelerator&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science, Engineering&lt;/p&gt;
    &lt;p&gt;This paper demonstrates the first successful extraction of the encryption key from the hardware AES accelerator in the nRF52832 Bluetooth Low Energy system-on-chip operating in Counter with CBC-MAC (CCM) mode using side-channel information recovered from RF signals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power analysis attacks - revealing the secrets of smart cards&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2007&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science&lt;/p&gt;
    &lt;p&gt;This volume explains how power analysis attacks work and provides an extensive discussion of countermeasures like shuffling, masking, and DPA-resistant logic styles to decide how to protect smart cards.&lt;/p&gt;
    &lt;head rend="h3"&gt;Non-Profiled Deep Learning-Based Side-Channel Attacks&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2018&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Computer Science&lt;/p&gt;
    &lt;p&gt;This paper introduces a new method to apply Deep Learning techniques in a Non-Profiled context, where an attacker can only collect a limited number of side-channel traces for a fixed unknown key value from a closed device and introduces metrics based on Sensitivity Analysis that can reveal both the secret key value and points of interest.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.semanticscholar.org/paper/Is-Your-Bluetooth-Chip-Leaking-Secrets-via-RF-Ji-Dubrova/c1d3ceb47ea6f9cc4f29929e2f97d36862a260a2"/><published>2025-11-02T18:06:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45792209</id><title>Anti-cybercrime laws are being weaponized to repress journalism</title><updated>2025-11-02T22:35:50.127734+00:00</updated><content>&lt;doc fingerprint="1480d19f3aa57a8d"&gt;
  &lt;main&gt;
    &lt;p&gt;Sign up for the daily CJR newsletter.&lt;/p&gt;
    &lt;p&gt;In May 2024, Daniel Ojukwu, a twenty-six-year-old reporter for the Foundation for Investigative Journalism, a Nigerian nonprofit, was grabbed off the streets of Lagos by armed police and bundled into a vehicle. For the next several days, he was held in a cell incommunicado—first in Lagos, and later in the federal capital, Abuja—without being told exactly what he’d been arrested for. “It was more of an abduction,” Ojukwu recalled recently, via WhatsApp. Finally, on the fourth day, the authorities informed him that he was being accused of breaching a 2015 law known as the Cybercrime Act. His violation: an article he wrote about alleged corruption in the office of the president.&lt;/p&gt;
    &lt;p&gt;The Cybercrime Act was introduced to combat a growing trend of internet fraud and other criminal activity within Nigeria, but it has instead frequently been used to suppress journalism published online. One provision in particular—Section 24, which made it illegal to publish false information online that was deemed to be “grossly offensive,” “indecent,” or even merely an “annoyance”—has been especially ripe for abuse. In 2019, for instance, Agba Jalingo, a journalist and publisher of CrossRiverWatch, in Nigeria’s Cross River State, was arrested and charged under Section 24 after he published articles accusing the state’s governor of corruption. (He was later acquitted.) In February 2024, Nigerian lawmakers amended Section 24 to remove some of its most egregious elements, but the new language still makes it illegal, and punishable by up to three years in jail, to “knowingly or intentionally” communicate online anything that is “false, for the purpose of causing a breakdown of law and order [or] posing a threat to life.”&lt;/p&gt;
    &lt;p&gt;“This vague text is still used to unfairly prosecute journalists, particularly those who regularly publish investigative reports implicating political or institutional forces,” said Sadibou Marong, the sub-Saharan Africa bureau director of Reporters Without Borders. “Authorities are intent on gagging investigative journalism uncovering corruption and governance issues in the country. The continued implementation of this law constitutes a real threat.”&lt;/p&gt;
    &lt;p&gt;Nigeria is not the only country using laws designed to legitimately combat online misbehavior to instead repress journalism. In neighboring Niger, Abdourahamane Tchiani, who seized power in a coup in 2023, signed an order amending three articles of the country’s cybercrime law to reinstate prison sentences for “defamation,” “insults,” and the “dissemination of data likely to disturb public order or undermine human dignity” when these offenses are committed electronically. The law, originally enacted in 2019, had previously been softened to remove prison sentences for such offenses, in part owing to how the law had been abused to repress journalists. In Pakistan, Georgia, and Turkey, among others, recent laws meant to limit nefarious activity online, or the spread of misinformation, have been used to restrict acts of journalism. According to Amnesty International, at least fifteen people in Jordan have been prosecuted under a 2023 expansion of the country’s Cybercrimes Law, for offenses ranging from “spreading fake news” to “threatening society peace.”&lt;/p&gt;
    &lt;p&gt;“Unfortunately, most of the laws being passed will have little effect in actually curbing misinformation, but instead may give governments far more authority to control content they deem false or misleading,” said Gabrielle Lim, a doctoral fellow at the Citizen Lab at the University of Toronto, who recently coauthored a paper tracking the misuse of “fake news” laws around the world. “For some governments, the threat of misinformation provides a convenient justification for censorship. This is compounded by the fact that liberal democracies are also considering or passing similar laws, which can give cover to authoritarian regimes who want to do the same.”&lt;/p&gt;
    &lt;p&gt;In Nigeria, more than two dozen journalists have faced prosecution under the Cybercrime Act, according to the Committee to Protect Journalists. In most cases, the journalists have been accused of cyberbullying, cyberstalking, or attempting to overthrow the government. On February 16, 2024—two weeks before the amended Cybercrime Act was signed into law—four journalists of The Informant247, an independent online newspaper based in Nigeria’s Kwara State, were arrested and briefly detained after they published a two-part investigative series that alleged a corrupt atmosphere at a state-run polytechnic institute. “The experience was profoundly disturbing,” said Salihu Ayatullahi, the publication’s editor in chief and one of the arrested journalists. “We were locked in a dark, cramped cell with hardened criminals. The psychological impact was heavier than the physical discomfort: I couldn’t sleep, not because of the poor conditions, but because I couldn’t stop thinking about how broken our system had become and how the corrupt could illegally summon the police to punish those who expose them.” The case was dismissed eleven months later without any evidence presented against the reporters.&lt;/p&gt;
    &lt;p&gt;Solomon Okedara, a Nigerian digital rights lawyer and researcher, notes that the use of the Cybercrime Act has created a chilling effect in the nation’s civic space. “It is even more worrisome that most of the time, the prosecution cannot establish ingredients of the offense to the point of conviction,” Okedara said. “Knowing a fellow journalist has faced arrest, harassment, and detention, or endless trials, can force others to drop an investigative story idea.”&lt;/p&gt;
    &lt;p&gt;Despite their ordeals, both Ojukwu and Ayatullahi say they are more determined than ever to use their craft to hold public officials accountable. “As a journalist, the whole experience has made me understand that there is more work to do,” Ojukwu said. “And since there is no limit to which the corrupt are willing to go, there is also none for me. The Cybercrime Act remains a thorn in the flesh of journalists in Nigeria.”&lt;/p&gt;
    &lt;p&gt;Has America ever needed a media defender more than now? Help us by joining CJR today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cjr.org/analysis/nigeria-pakistan-jordan-cybercrime-laws-journalism.php"/><published>2025-11-02T18:12:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45792373</id><title>Reproducing the AWS Outage Race Condition with a Model Checker</title><updated>2025-11-02T22:35:49.994902+00:00</updated><content>&lt;doc fingerprint="2d18da0b79028601"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Reproducing the AWS Outage Race Condition with a Model Checker&lt;/head&gt;
    &lt;p&gt;Oct 30, 2025&lt;/p&gt;
    &lt;p&gt;AWS published a post-mortem about a recent outage [1]. Big systems like theirs are complex, and when you operate at that scale, things sometimes go wrong. Still, AWS has an impressive record of reliability.&lt;/p&gt;
    &lt;p&gt;The post-mortem mentioned a race condition, which caught my eye. I don’t know all the details of AWS’s internal setup, but using the information in the post-mortem and a few assumptions, we can try to reproduce a simplified version of the problem.&lt;/p&gt;
    &lt;p&gt;As a small experiment, we’ll use a model checker to see how such a race could happen. Formal verification can’t prevent every failure, but it helps us think more clearly about correctness and reason about subtle concurrency bugs. For this, we’ll use the Spin model checker, which uses the Promela language.&lt;/p&gt;
    &lt;p&gt;There’s a lot of detail in the post-mortem, but for simplicity we’ll focus only on the race-condition aspect. The incident was triggered by a defect in DynamoDB’s automated DNS management system. The components of this system involved in the incident were the DNS Planner, DNS Enactor, and Amazon Route 53 service.&lt;/p&gt;
    &lt;p&gt;The DNS Planner creates DNS plans, and the DNS Enactors look for new DNS plans and apply them to the Amazon Route 53 service. Three Enactors operate independently in three different availability zones.&lt;/p&gt;
    &lt;p&gt;Here is an illustration showing these components (if the images appear small, please open them in a new browser tab):&lt;/p&gt;
    &lt;p&gt;My understanding of how the DNS Enactor works is as follows: it picks up the latest plan and, before applying it, performs a one-time check to ensure the plan is newer than the previously applied one. It then applies the plan and invokes a clean-up process. During the clean-up, it identifies plans significantly older than the one it just applied and deletes them.&lt;/p&gt;
    &lt;p&gt;Using the details from the incident report, we could sketch an interleaving that could explain the race condition. Two Enactors running side by side: Enactor 2 applies a new plan and starts cleaning up, while the other, running just a little behind, applies an older plan, making it an active one. When the Enactor 2 finishes its cleanup, it deletes that plan, and the DNS entries disappear. Here’s what that sequence looks like:&lt;/p&gt;
    &lt;p&gt;Let’s try to uncover this interleaving using a model checker.&lt;/p&gt;
    &lt;p&gt;In Promela, you can model each part of the system as its own process. Spin then takes those processes, starts from the initial state, and systematically applies every possible transition, exploring all interleavings to build the set of reachable states [2]. It checks that your invariants hold in each one, and if it finds a violation, it reports a counterexample.&lt;/p&gt;
    &lt;p&gt;We’ll create a DNS Planner process that produces plans, and DNS Enactor processes that pick them up. The Enactor will check whether the plan it’s about to apply is newer than the previous one, update the state of certain variables to simulate changes in Route 53, and finally clean up the older plans.&lt;/p&gt;
    &lt;p&gt;In our simplified model, we’ll run one DNS Planner process and two concurrent DNS Enactor processes. (AWS appears to run three across zones; we abstract that detail here.) The Planner generates plans, and through Promela channels, these plans are sent to the Enactors for processing.&lt;/p&gt;
    &lt;p&gt;Inside each DNS Enactor, we track the key aspects of system state. The Enactor keeps the current plan in current_plan, and it represents DNS health using dns_valid. It also records the highest plan applied so far in highest_plan_applied. The incident report also notes that the clean-up process deletes plans that are “significantly older than the one it just applied.” In our model, we capture this by allowing an Enactor to remove only those plans that are much older than its current plan. To simulate the deletion of an active plan, the Enactor’s clean-up process checks whether current_plan equals the plan being deleted. If it does, we simulate the resulting DNS failure by setting dns_valid to false.&lt;/p&gt;
    &lt;p&gt;Here’s the code for the DNS Planner:&lt;/p&gt;
    &lt;code&gt;active proctype Planner() {
    byte plan = 1;
    
    do
    :: (plan &amp;lt;= MAX_PLAN) -&amp;gt;
        latest_plan = plan;
        plan_channel ! plan; 
        printf("Planner: Generated Plan v%d\n", plan);
        plan++;
    :: (plan &amp;gt; MAX_PLAN) -&amp;gt; break;
    od;
    
    printf("Planner: Completed\n");
}
&lt;/code&gt;
    &lt;p&gt;It creates plans and sends them over a channel (plan is being sent to the channel plan_channel) to be picked up later by the DNS Enactor.&lt;/p&gt;
    &lt;p&gt;We start two concurrent DNS Enactor processes by specifying the number of enactors after the active keyword.&lt;/p&gt;
    &lt;code&gt;active [NUM_ENACTORS] proctype Enactor() 
&lt;/code&gt;
    &lt;p&gt;The DNS Enactor waits for plans and receives them (? opertaor receives a plan from the channel plan_channel). It then performs a staleness check, updates the state of certain variables to simulate changes in Route 53, and finally cleans up the older plans.&lt;/p&gt;
    &lt;code&gt;:: plan_channel ? my_plan -&amp;gt;
    snapshot_current = current_plan;

    // staleness check    
    if
    :: (my_plan &amp;gt; snapshot_current || snapshot_current == 0) -&amp;gt;

        if
            :: !plan_deleted[my_plan] -&amp;gt;
                /* Apply the plan to Route53 */
                
                current_plan = my_plan;
                dns_valid = true;
                initialized = true;
               /* Track highest plan applied for regression detection */
                if 
                :: (my_plan &amp;gt; highest_plan_applied) -&amp;gt;
                    highest_plan_applied = my_plan;
                fi 
            
            // runs the clean-up process (omitted for brevity, included in the 
            // code linked below)
        fi
    fi

&lt;/code&gt;
    &lt;p&gt;How do we discover the race condition? The idea is this: we express as an invariant what must always be true of the system, and then ask the model checker to confirm that it holds in every possible state. In this case, we can set up an invariant stating that the DNS should never be deleted once a newer plan has been applied. (With more information about the real system, we could simplify or refine this rule further.)&lt;/p&gt;
    &lt;p&gt;We specify this invariant formally as follows:&lt;/p&gt;
    &lt;code&gt;/*

A quick note on some of the keywords used in the invariant below:

ltl - keyword that declares a temporal property to verify (ltl: linear temporal logic lets you specify properties about all possible executions of your program.)

[] - "always" operator (this must be true at every step forever)

-&amp;gt; - "implies" (if left side is true, then right side must be true)

*/

ltl no_dns_deletion_on_regression {
    [] ( (initialized &amp;amp;&amp;amp; highest_plan_applied &amp;gt; current_plan 
            &amp;amp;&amp;amp; current_plan &amp;gt; 0) -&amp;gt; dns_valid )
}



&lt;/code&gt;
    &lt;p&gt;When we start the model checker, one DNS Planner process begins generating plans and sending them through channels to the DNS Enactors. Two Enactors receive these plans, perform their checks, apply updates, and run their cleanup routines. As these processes interleave, the model checker systematically builds the set of reachable states, allowing the invariant to be checked in each one.&lt;/p&gt;
    &lt;p&gt;When we run the model with this invariant in the model checker, it reports a violation. Spin reports one error and writes a trail file that shows, step by step, how the system reached the bad state.&lt;/p&gt;
    &lt;code&gt;
$ spin -a aws-dns-race.pml
$ gcc -O2 -o pan pan.c                                                       
$ ./pan -a -N no_dns_deletion_on_regression  

pan: wrote aws-dns-race.pml.trail

(Spin Version 6.5.2 -- 6 December 2019)

State-vector 64 byte, depth reached 285, errors: 1
    23201 states, stored
    11239 states, matched
    34440 transitions (= stored+matched)
  (truncated for brevity....)

&lt;/code&gt;
    &lt;p&gt;The trail file in the repository below shows how the race happens. The trail file shows that two Enactors operate side by side: the faster one applies plan 4 and starts cleaning up. Because cleanup only removes plans much older than the one just applied, it deletes 1 and 2 but skips 3. The slower Enactor then applies plan 3 and makes it active, and when the faster Enactor picks up cleanup again, it deletes 3 and the DNS goes down.&lt;/p&gt;
    &lt;p&gt;Here’s an illustration of the interleaving reconstructed from the trail:&lt;/p&gt;
    &lt;p&gt;Before publishing, I reread the incident report and noted: “Additionally, because the active plan was deleted, the system was left in an inconsistent state…”. This suggests a direct invariant: the active plan must never be deleted.&lt;/p&gt;
    &lt;code&gt;ltl never_delete_active {
    [] ( current_plan &amp;gt; 0 -&amp;gt; !plan_deleted[current_plan] )
}
&lt;/code&gt;
    &lt;p&gt;Running the model checker with this invariant produces essentially the same counterexample as before: one Enactor advances to newer plans while the other lags and applies an older plan, thereby making it active. When control returns to the faster Enactor, its cleanup deletes that now-active plan, violating the invariant.&lt;/p&gt;
    &lt;p&gt;Invariants are invaluable for establishing correctness. If we can show that an invariant holds in the initial state, in every state reachable from it, and in the final state as well, we gain confidence that the system’s logic is sound.&lt;/p&gt;
    &lt;p&gt;To fix the code, we execute the problematic statements atomically. You can find both versions of the code, the one with the race and the fixed one, along with the interleaving trail in the accompanying repository [3]. I’ve included detailed comments to make it self-explanatory, as well as instructions on how to run the model and explore the trail.&lt;/p&gt;
    &lt;p&gt;Some of the assumptions in this model are necessarily simplified, since I don’t have access to AWS’s internal design details. Without that context, there will naturally be gaps between this abstraction and the real system. This model was created in a short time frame for experimental purposes. With more time and context, one could certainly build a more accurate and refined version.&lt;/p&gt;
    &lt;p&gt;Please keep in mind that I’m only human, and there’s a chance this post contains errors. If you notice anything off, I’d appreciate a correction. Please feel free to send me an email.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;AWS Post-Incident Summary — October 2025 Outage&lt;/item&gt;
      &lt;item&gt;How concurrency works: A visual guide&lt;/item&gt;
      &lt;item&gt;Source code repository&lt;/item&gt;
      &lt;item&gt;Spin model checker&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wyounas.github.io/aws/concurrency/2025/10/30/reproducing-the-aws-outage-race-condition-with-model-checker/"/><published>2025-11-02T18:37:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45792429</id><title>React-Native-Godot</title><updated>2025-11-02T22:35:49.431755+00:00</updated><content>&lt;doc fingerprint="650eb84a5f6f86d5"&gt;
  &lt;main&gt;
    &lt;p&gt;React Native Godot allows embedding the Godot Engine into React Native applications.&lt;/p&gt;
    &lt;p&gt;Born React Native Godot was created by Born and developed by Migeran, in close collaboration between the two teams.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports Android and iOS, built on LibGodot.&lt;/item&gt;
      &lt;item&gt;Stable implementation serving millions of users in Born's applications.&lt;/item&gt;
      &lt;item&gt;Supports starting, stopping and restarting the Godot Engine. (docs)&lt;/item&gt;
      &lt;item&gt;When restarting, the engine can be reconfigured, so a different Godot app may be loaded each time. (docs)&lt;/item&gt;
      &lt;item&gt;It is also possible to pause and resume the running Godot instance. (docs)&lt;/item&gt;
      &lt;item&gt;Godot is running on a separate thread, so it does not affect the main thread of the application nor the React Native JavaScript thread. (docs)&lt;/item&gt;
      &lt;item&gt;The Godot main window and any subwindows created by the Godot app may be embedded into the React Native application either on the same screen, or on separate screens (see example app).&lt;/item&gt;
      &lt;item&gt;The whole Godot API is accessible from TypeScript / JavaScript. It is possible to instantiate objects, call methods, get and set properties, attach JS functions to signals, provide JS functions as callables to Godot methods ... etc. (docs)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="px-3 py-2"&gt;Example.mp4&lt;/head&gt;
    &lt;p&gt;The example app shows the main features of React Native Godot in action.&lt;/p&gt;
    &lt;p&gt;During development we use ASDF to manage most external dependencies required for React Native development, like node, java, gradle or ruby. If you also use ASDF, just run:&lt;/p&gt;
    &lt;code&gt;asdf install&lt;/code&gt;
    &lt;p&gt;This will make sure that all the dependencies are the same like in our environment. Otherwise you may also install React Native prerequisites using any other method.&lt;/p&gt;
    &lt;p&gt;Run the following scripts for either platform you plan to test (or both):&lt;/p&gt;
    &lt;code&gt;cd example
./export_godot_GodotTest.sh android
./export_godot_GodotTest.sh ios
./export_godot_GodotTest2.sh android
./export_godot_GodotTest2.sh ios&lt;/code&gt;
    &lt;p&gt;The script is configured to look for Godot in the standard system wide installation folder on MacOS. If your Godot is installed elsewhere, or you are on Linux, just point the GODOT_EDITOR environment variable to your GODOT EDITOR prior to running the above scripts:&lt;/p&gt;
    &lt;code&gt;export GODOT_EDITOR=/path/to/godot_editor&lt;/code&gt;
    &lt;code&gt;cd example
yarn
yarn download-prebuilt&lt;/code&gt;
    &lt;p&gt;These commands will resolve all the React Native and other dependencies from NPM. The second one will download the prebuilt LibGodot release from GitHub.&lt;/p&gt;
    &lt;code&gt;cd example/ios
bundle install
bundle exec pod install
cd ..
yarn ios&lt;/code&gt;
    &lt;code&gt;cd example
yarn android&lt;/code&gt;
    &lt;p&gt;You may use Xcode and Android Studio the same way as with any other project. Just open:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;ios/GodotTest.xcworkspace&lt;/code&gt;from Xcode&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;android&lt;/code&gt;from Android Studio&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: If you are using ASDF to manage your java and Node dependencies, you should start Android Studio from under the react-native-godot (or example) folder, so it can find these tools. For example on MacOS:&lt;/p&gt;
    &lt;code&gt;cd example
open -a "Android Studio"&lt;/code&gt;
    &lt;p&gt;There is an update_deps.sh script included in the example app's folder. It will execute all the setup commands for both iOS and Android in one step, so you may start your work immediately.&lt;/p&gt;
    &lt;code&gt;cd example
./update_deps.sh
yarn ios # or yarn android&lt;/code&gt;
    &lt;p&gt;Born React Native Godot is distributed on NPM.&lt;/p&gt;
    &lt;p&gt;Just follow these steps to add it to your React Native application:&lt;/p&gt;
    &lt;code&gt;yarn add @borndotcom/react-native-godot&lt;/code&gt;
    &lt;p&gt;The LibGodot packages used by React Native Godot are not distributed on NPM. Instead, they are downloaded separately by issuing the following command:&lt;/p&gt;
    &lt;code&gt;yarn download-prebuilt&lt;/code&gt;
    &lt;p&gt;This way React Native Godot can be updated independently from LibGodot, and also local, customized builds of LibGodot are supported.&lt;/p&gt;
    &lt;code&gt;import { RTNGodot, RTNGodotView, runOnGodotThread } from "@borndotcom/react-native-godot";&lt;/code&gt;
    &lt;code&gt;const App = () =&amp;gt; {
	return (
        &amp;lt;View&amp;gt;
            &amp;lt;RTNGodotView style={...}/&amp;gt;
        &amp;lt;/View&amp;gt;
	)
};&lt;/code&gt;
    &lt;p&gt;If no windowName property is specified, that view is for the main window of Godot.&lt;/p&gt;
    &lt;p&gt;We will also add Expo Filesystem module for handling file system paths from React Native easily.&lt;/p&gt;
    &lt;code&gt;yarn add expo-file-system&lt;/code&gt;
    &lt;code&gt;import * as FileSystem from 'expo-file-system/legacy';

function initGodot() {
  runOnGodotThread(() =&amp;gt; {
    'worklet';
    console.log("Initializing Godot");

    if (Platform.OS === 'android') {
        RTNGodot.createInstance(
            ["--verbose", "--path",
            "/main",
            "--rendering-driver", "opengl3",
            "--rendering-method", "gl_compatibility",
            "--display-driver", "embedded"]
        );  
    } else {
        RTNGodot.createInstance(
            ["--verbose", "--main-pack",
            FileSystem.bundleDirectory + "main.pck",
            "--rendering-driver", "opengl3",
            "--rendering-method", "gl_compatibility",
            "--display-driver", "embedded"]
        );  
    }
}&lt;/code&gt;
    &lt;p&gt;A couple of things to note here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The usual Godot command line parameters can be passed to the initialization function.&lt;/item&gt;
      &lt;item&gt;It is key to use the "embedded" display driver, which is required to embed Godot into the React Native application.&lt;/item&gt;
      &lt;item&gt;It is possible to specify both a directory or a pack file. &lt;list rend="ul"&gt;&lt;item&gt;On Android, inside the main package the access of the pack file's contents is much slower than accessing pack files stored in the private area of the application. If the Godot app is stored inside the main package, then it should be stored as a folder of files in the asset folder.&lt;/item&gt;&lt;item&gt;On iOS, there is no such limitation, so we use a pack file there.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;In many cases the best way is to download the Godot apps at runtime, which has many advantages, including: &lt;list rend="ul"&gt;&lt;item&gt;Smaller initial application size&lt;/item&gt;&lt;item&gt;Deliver specialized app builds for different devices&lt;/item&gt;&lt;item&gt;Update the Godot app without going through the whole app review process&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;const App = () =&amp;gt; {
    useEffect(() =&amp;gt; {
      initGodot()
      return () =&amp;gt; {
      }
    }, [])

    return (
        &amp;lt;View&amp;gt;
            &amp;lt;RTNGodotView style={...}/&amp;gt;
        &amp;lt;/View&amp;gt;
	)
};&lt;/code&gt;
    &lt;p&gt;In this Hello World app, we just initialize Godot when the App view is displayed. Check out the example app in this repository for a more elaborate example.&lt;/p&gt;
    &lt;p&gt;To stop a running Godot instance, call RTNGodot.destroyInstance() on the Godot thread:&lt;/p&gt;
    &lt;code&gt;function destroyGodot() {
  runOnGodotThread(() =&amp;gt; {
    "worklet";
    RTNGodot.destroyInstance();
  });
}&lt;/code&gt;
    &lt;p&gt;NOTE: After stopping an instance, you can start a new one by calling RTNGodot.createInstance again. The new instance can be started with different parameters including other Godot projects.&lt;/p&gt;
    &lt;p&gt;To pause a running Godot instance, call RTNGodot.pause() on the Javascript main thread:&lt;/p&gt;
    &lt;code&gt;RTNGodot.pause();&lt;/code&gt;
    &lt;p&gt;This won't shut down the running instance, just halt it's execution until further notice.&lt;/p&gt;
    &lt;p&gt;To resume a paused Godot instance, call RTNGodot.resume() on the Javascript main thread:&lt;/p&gt;
    &lt;code&gt;RTNGodot.resume();&lt;/code&gt;
    &lt;p&gt;You may use the usual export functionality of Godot Engine, just make sure to export to PCK or ZIP and not the whole application.&lt;/p&gt;
    &lt;p&gt;In the React Native Godot example application an &lt;code&gt;export_godot.sh&lt;/code&gt; and an associated &lt;code&gt;export_godot_GodotTest.sh&lt;/code&gt; script is provided, which can make the export process easier.&lt;/p&gt;
    &lt;p&gt;To export your project to iOS or Android, use our included export_godot.sh script with the following parameters:&lt;/p&gt;
    &lt;code&gt;--target: Base directory where the project will be exported. Depending on the platform a pck file or a folder will be created here.
--project: The directory of the project that will be exported.
--name: The name of the exported project.
--preset: The export preset to be used.
--platform: The platform to export the project to (iOS or Android).
&lt;/code&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Android projects will be exported into project folders while iOS project will be exported as PCK files.&lt;/item&gt;
      &lt;item&gt;By default, export_godot.sh will look for an official Godot installation under the Applications folder. If your installation is somewhere else or you would like to use a custom Godot build, specify it's location in the GODOT_EDITOR environment variable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After importing React Native Godot, you can access the Godot API through the RTNGodot class.&lt;/p&gt;
    &lt;p&gt;To access the Godot API, first call RTNGodot.API():&lt;/p&gt;
    &lt;code&gt;let Godot = RTNGodot.API();&lt;/code&gt;
    &lt;p&gt;From the entry point, you can get the engine's singletons:&lt;/p&gt;
    &lt;code&gt;// Get Godot's Engine singleton.
let Godot = RTNGodot.API();
var engine = Godot.Engine;&lt;/code&gt;
    &lt;p&gt;You can instantiate Godot API objects using their defined constructors:&lt;/p&gt;
    &lt;code&gt;var vector = Godot.Vector2();&lt;/code&gt;
    &lt;p&gt;Object properties can be accessed directly:&lt;/p&gt;
    &lt;code&gt;var vector = Godot.Vector2();
vector.x = 1.0;
vector.y = 2.0;&lt;/code&gt;
    &lt;p&gt;Godot API methods can also be called:&lt;/p&gt;
    &lt;code&gt;let Godot = RTNGodot.API();
var engine = Godot.Engine;

// Call Godot API methods:
var sceneTree = engine.get_main_loop();
var root = sceneTree.get_root();
// NOTE: From here you can access nodes of the scene tree and manipulate them from TypeScript.&lt;/code&gt;
    &lt;p&gt;JS functions can be connected to Godot signals:&lt;/p&gt;
    &lt;code&gt;let Godot = RTNGodot.API();

var button = Godot.Button();
button.set_text("Button");

button.pressed.connect(function() {
  console.log("Button pressed.")
})&lt;/code&gt;
    &lt;p&gt;JS functions can be passed to Godot methods to be used as Callables:&lt;/p&gt;
    &lt;p&gt;Let's create a custom RNInterface Node, and add it to the Godot project's scene tree. It's GDScript should read:&lt;/p&gt;
    &lt;code&gt;extends Node
class_name RNInterface

func test_callable(c: Callable) -&amp;gt; void:
	c.call("Hello from Godot")
&lt;/code&gt;
    &lt;p&gt;From TypeScript we may access it with the following code snippet:&lt;/p&gt;
    &lt;code&gt;let Godot = RTNGodot.API();
var engine = Godot.Engine;
var sceneTree = engine.get_main_loop();
var root = sceneTree.get_root();

var iface = root.find_child("RNInterface", true, false)

iface.test_callable(function(s: string) {
  console.log("Received text from Godot: " + s)
}); &lt;/code&gt;
    &lt;p&gt;In a React Native app, the main JavaScript thread, where the bulk of the JavaScript code of the application runs is separate from the Android or iOS apps's main thread.&lt;/p&gt;
    &lt;p&gt;This way the JS Thread's processing does not affect the main application UI. Following the same pattern, the Godot Engine is also running on its own thread that is separate from both the application's and React Native's main JavaScript thread. As JavaScript is single-threaded by design, to be able to communicate with the Godot thread from JavaScript, we use the well-known react-native-worklets-core library, which allows us running JS code in the Godot thread using worklets.&lt;/p&gt;
    &lt;p&gt;Worklets are JavaScript functions designated with a 'worklet' keyword.&lt;/p&gt;
    &lt;code&gt;function worklet() {
    'worklet'
}&lt;/code&gt;
    &lt;p&gt;These functions and all their external dependencies are transpiled into self contained JS bundles so they can be executed in separate JS contexts associated with separate threads. For more information on how this works, please refer to the React Native Worklets Core documentation.&lt;/p&gt;
    &lt;p&gt;React Native Godot provides a helper function called &lt;code&gt;runOnGodotThread()&lt;/code&gt; which will allow you to execute such workletized JS functions on the Godot thread.&lt;/p&gt;
    &lt;p&gt;In general, this is the recommended way of interacting with the Godot Engine, as shown in our example app.&lt;/p&gt;
    &lt;p&gt;While it is possible to interact with the Godot Engine directly from the main React Native thread, there are some caveats and it is generally not recommended:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;From the Godot Engine's view, such invocations will be executed on a "background" thread, which means, that for example the Scene Tree cannot be fully accessed from this thread directly. For more information on how background threads in Godot work, please refer to its documentation.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Godot object references that were obtained in the Main JS thread and in worklets are not interchangeable, because they are associated with separate JS contexts. It is possible to create object references that can work in multiple JS contexts, but these would add runtime overhead and are currently not supported in React Native Godot. If you would require this feature in your application, then please contact Migeran to discuss your requirements in detail.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;To use a custom LibGodot build, you first need to clone the LibGodot project on the branch&lt;/p&gt;&lt;code&gt;libgodot_migeran_45&lt;/code&gt;, including all its submodules.&lt;quote&gt;git clone --recursive https://github.com/migeran/libgodot -b libgodot_migeran_45&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Then build LibGodot from source:&lt;/p&gt;&lt;quote&gt;cd libgodot ./build_prebuilt_release.sh # for release libraries # OR ./build_prebuilt_dev.sh # for development libraries&lt;/quote&gt;&lt;p&gt;NOTE: You may comment out the unnecessary lines in the build script if you're only building for one platform.&lt;/p&gt;&lt;p&gt;At the end of the process, the necessary files will be produced in the&lt;/p&gt;&lt;code&gt;build/prebuilt/release&lt;/code&gt;or the&lt;code&gt;build/prebuilt/dev&lt;/code&gt;folder.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Once the build finished, set these environment variables:&lt;/p&gt;
        &lt;quote&gt;export LIBGODOT_XCFRAMEWORK_PATH=path/to/libgodot/build/prebuilt/release or dev/libgodot.xcframework.zip export LIBGODOT_CPP_XCFRAMEWORK_PATH=path/to/libgodot/build/prebuilt/release or dev/libgodot-cpp.xcframework.zip export LIBGODOT_ANDROID_PATH=path/to/libgodot/build/prebuilt/release or dev/libgodot-android.zip export LIBGODOT_CPP_ANDROID_PATH=path/to/libgodot/build/prebuilt/release or dev/godot-cpp-android.zip export SHASUM_CHECK=false export REPLACE_EXISTING=true&lt;/quote&gt;
        &lt;p&gt;These environment variables override the download logic of the download-prebuilt script of react-native-godot:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Use locally present files instead of the URLs specified in the package.json of react-native-godot.&lt;/item&gt;
          &lt;item&gt;Do not check the provided SHA-256 hash.&lt;/item&gt;
          &lt;item&gt;Force replacing the installed library, even if the same version was already installed.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Then run the download-prebuilts script to install the custom built libraries:&lt;/p&gt;
        &lt;quote&gt;yarn download-prebuilt&lt;/quote&gt;
        &lt;p&gt;NOTE: An example usage of this script can be found in our example/update_deps.sh script.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;On Android, if using the development builds, in the react-native-godot/android/build.gradle the following line needs to be updated to include&lt;/p&gt;&lt;code&gt;godot-dev&lt;/code&gt;as the artifactId:&lt;code&gt;api "com.migeran.libgodot:godot:${libGodotVersion}-SNAPSHOT"&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To debug native engine code (C++), you first need to build and install a development version of LibGodot as described above.&lt;/p&gt;
    &lt;p&gt;On iOS this allows you to set breakpoints into the Godot Engine itself in Xcode and debug it as easily as your own app.&lt;/p&gt;
    &lt;p&gt;Depending on the Xcode version, you might have to open these files separately in Xcode, and then move the editor from the new window into the window your application's Xcode project.&lt;/p&gt;
    &lt;p&gt;On Android a couple more steps will be required:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;In the Android Studio Run/Debug configurations you also need to add a Symbol directory, where libgodot_android.so with the included debug symbols is located. One good place is:&lt;/p&gt;&lt;code&gt;path/to/libgodot/godot/platform/android/java/lib/libs/dev/arm64-v8a&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Depending on the Android Studio version, you may have to include the Godot source tree under the Android Studio project to be able to set breakpoints. A way to do this is to create a symbolic link under the React Native Godot library as follows:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd /path/to/your/app/node_modules/@borndotcom/react-native-godot/android/src/main/cpp
ln -s /path/to/libgodot/godot godot&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Then reimport the project / resync the Gradle project files into Android Studio.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;For remote debugging, you first need to build and install a development version of LibGodot as described above.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Then add these 2 extra parameters to your RTNGodot.createInstance call:&lt;/p&gt;
        &lt;quote&gt;'--remote-debug', 'tcp://127.0.0.1:6007'&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To expose TCP port 6007 on your Android device via TCP port 6007 on your computer, open a terminal and call:&lt;/p&gt;
        &lt;quote&gt;adb reverse tcp:6007 tcp:6007&lt;/quote&gt;
        &lt;p&gt;NOTE: This needs to be called again every time you disconnect and reconnect your device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Now you can open your project in the Godot Editor. First enable 'Debug' -&amp;gt; 'Keep Debug Server Open' from the top menu then you can set your breakpoints. You can also set breakpoints while the app is running.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Once your breakpoints are set, you can start debugging by launching your app from Android Studio.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;For remote debugging, you first need to build and install a development version of LibGodot as described above.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Open the Godot Editor, then go to 'godot' -&amp;gt; 'Editor Settings' -&amp;gt; 'Network' -&amp;gt; 'Debug' -&amp;gt; 'Remote Host' and set your Mac's IP address.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Also enable 'Debug' -&amp;gt; 'Keep Debug Server Open'.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Then add these 2 extra parameters to your RTNGodot.createInstance call:&lt;/p&gt;
        &lt;quote&gt;'--remote-debug', 'tcp://&amp;lt;your_macs_ip_address&amp;gt;:6007'&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Now you can start debugging by launching your app from Xcode. You can set your breakpoints both before running the app or at runtime.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;NOTE: Your Mac and your iOS device must be connected to the same network/router in order to establish the connection.&lt;/p&gt;
    &lt;p&gt;We’ve opened new positions in Berlin and New York! We’re looking for the best React Native Engineers who want to reach millions of people every day and work at the frontier of technology.&lt;/p&gt;
    &lt;p&gt;Check out our openings at: born.com&lt;/p&gt;
    &lt;p&gt;Migeran offers commercial support and development services around React Native Godot, LibGodot and the Godot Engine. Start Here with describing your requirements.&lt;/p&gt;
    &lt;p&gt;Born React Native Godot is released under the MIT license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/borndotcom/react-native-godot"/><published>2025-11-02T18:45:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45792503</id><title>Linux gamers on Steam cross over the 3% mark</title><updated>2025-11-02T22:35:49.233118+00:00</updated><content>&lt;doc fingerprint="9e09189f1897341d"&gt;
  &lt;main&gt;
    &lt;p&gt;It finally happened. Linux gamers on Steam as of the Steam Hardware &amp;amp; Software Survey for October 2025 have crossed over the elusive 3% mark. The trend has been clear for sometime, and with Windows 10 ending support, it was quite likely this was going to be the time for it to happen as more people try out Linux.&lt;/p&gt;
    &lt;p&gt;As of the October 2025 survey the operating system details:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows 94.84% -0.75%&lt;/item&gt;
      &lt;item&gt;Linux 3.05% +0.41%&lt;/item&gt;
      &lt;item&gt;macOS 2.11% +0.34%&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The snapshot chart from our dedicated Steam Tracker page shows the clear trend:&lt;/p&gt;
    &lt;p&gt;Overall, 3% might not seem like much to some, but again - that trend is very clear and equates to millions of people. The last time Valve officially gave a proper monthly active user count was in 2022, and we know Steam has grown a lot since then, but even going by that original number would put monthly active Linux users at well over 4 million. Sadly, Valve have not given out a more recent monthly active user number but it's likely a few million higher, especially with the Steam Deck selling millions.&lt;/p&gt;
    &lt;p&gt;And if we look at the distribution breakdown chart from our page:&lt;/p&gt;
    &lt;p&gt;The overall distribution numbers for October 2025:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SteamOS Holo 64 bit - 27.18% (-0.47%)&lt;/item&gt;
      &lt;item&gt;Arch Linux 64 bit - 10.32% (-0.66%)&lt;/item&gt;
      &lt;item&gt;Linux Mint 22.2 64 bit - 6.65% (+6.65%)&lt;/item&gt;
      &lt;item&gt;CachyOS 64 bit - 6.01% (+1.32%)&lt;/item&gt;
      &lt;item&gt;Ubuntu Core 22 64 bit - 4.55% (+0.55%)&lt;/item&gt;
      &lt;item&gt;Freedesktop SDK 25.08 (Flatpak runtime) 64 bit - 4.29% (+4.29%)&lt;/item&gt;
      &lt;item&gt;Bazzite 64 bit - 4.24% (+4.24%)&lt;/item&gt;
      &lt;item&gt;Ubuntu 24.04.3 LTS 64 bit - 3.70% (+3.70%)&lt;/item&gt;
      &lt;item&gt;Linux Mint 22.1 64 bit - 2.56% (-5.65%)&lt;/item&gt;
      &lt;item&gt;EndeavourOS Linux 64 bit - 2.32% (-0.08%)&lt;/item&gt;
      &lt;item&gt;Freedesktop SDK 24.08 (Flatpak runtime) 64 bit - 2.31% (-3.98%)&lt;/item&gt;
      &lt;item&gt;Fedora Linux 42 (KDE Plasma Desktop Edition) 64 bit - 2.12% (+0.19%)&lt;/item&gt;
      &lt;item&gt;Manjaro Linux 64 bit - 2.04% (-0.31%)&lt;/item&gt;
      &lt;item&gt;Pop!_OS 22.04 LTS 64 bit - 1.93% (-0.04%)&lt;/item&gt;
      &lt;item&gt;Fedora Linux 42 (Workstation Edition) 64 bit - 1.75% (-0.43%)&lt;/item&gt;
      &lt;item&gt;Other - 18.04% (-4.28%)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The numbers are still being massively pumped up by the Steam Deck with SteamOS Linux, which is not surprising considering that the Steam Deck is still in the top 10 of the global top sellers on Steam constantly. And with all the rumours and leaks surrounding the upcoming Steam Frame, which will hopefully be a SteamOS Linux powered VR kit, we could see the numbers just continue to jump higher.&lt;/p&gt;
    &lt;p&gt;Source: Valve&lt;/p&gt;
    &lt;p&gt;Also please split the linear approximation in pieces (Steam Deck release is a good mark)&lt;/p&gt;
    &lt;p&gt;Last edited by lucinos on 2 Nov 2025 at 1:18 pm UTC&lt;/p&gt;
    &lt;quote&gt;Interesting that MacOS usage went up as well, been a while since that's happened.It's been trending back up since around the start of the year.&lt;/quote&gt;
    &lt;p&gt;https://i.ibb.co/tT3g0WT7/Combined.png [External Link]&lt;/p&gt;
    &lt;p&gt;It's a smidge behind non-Deck Linux.&lt;/p&gt;
    &lt;p&gt;A bigger non-Windows share (above 5% for the first time since 2013) is a good thing for encouraging multi-platform development and support, although Apple makes it harder than it needs to be with their refusal of Vulkan, and harder than it used to be when OpenGL would work on Windows, Linux and Mac.&lt;/p&gt;
    &lt;p&gt;Once marketshare is high enough for more multiplayer games, it makes limiting support just to the Steam Deck hardware much less viable. That is definitely a risk as seen with games like Delta Force.&lt;/p&gt;
    &lt;p&gt;But if the trend continues, they'll have to support a wide range of hardware and distros.&lt;/p&gt;
    &lt;p&gt;Let's keep climbing.&lt;/p&gt;
    &lt;quote&gt;I'm surprised in general that Linux Mint is ahead of Debian (testing / unstable)&lt;/quote&gt;
    &lt;p&gt;Mint user here. I think that's because for gaming, Mint is a great compromise. Debian's ultimate focus is stability, which makes it a fantastic choice for servers, but in gaming, you often want components that aren't quite that old. It still doesn't randomly break your stuff, unlike rolling release distros.&lt;/p&gt;
    &lt;quote&gt;I'm surprised in general that Linux Mint is ahead of Debian (testing / unstable)I would be surprised if something that is explicitly unstable was significantly popular. Iirc every more or less official Debian related place tells you not to use those unless you really know what you're doing in a way that makes Arch or Fedora much more appealing if you want fresh packages&lt;/quote&gt;
    &lt;quote&gt;Mint user here. I think that's because for gaming, Mint is a great compromise. Debian's ultimate focus is stability, which makes it a fantastic choice for servers, but in gaming, you often want components that aren't quite that old. It still doesn't randomly break your stuff, unlike rolling release distros&lt;/quote&gt;
    &lt;p&gt;That's why I said Debian testing / unstable, not Debian stable. Such kind of approach (whether in Mint or Debian stable itself) can cause problems too unless people understand its limitations.&lt;/p&gt;
    &lt;p&gt;I periodically see a bunch of people complaining that their hardware doesn't work, which ends up being them using Mint which doesn't ship recent kernel and Mesa.&lt;/p&gt;
    &lt;p&gt;Rolling flavors of Debian are a better fit in my opinion.&lt;/p&gt;
    &lt;p&gt;Also, I think KDE is a better fit for modern gaming features, due to Cinnamon being way slower in supporting Wayland. Having focus on its own DE and not keeping up with the times is a downside for Mint. Even Ubuntu stopped its own DE efforts for that reason.&lt;/p&gt;
    &lt;quote&gt;irc every more or less official Debian related place tells you not to use those unless you really know what you're doing&lt;/quote&gt;
    &lt;p&gt;You should know what you are doing no matter what you are using. That's my experience. I'd say Debian testing/unstable isn't any worse than a bunch of other rolling distros, like Arch or what not. If anything, it's more stable than Arch. Those who say not to use it are doing a disservice.&lt;/p&gt;
    &lt;p&gt;Last edited by Shmerl on 2 Nov 2025 at 5:05 pm UTC&lt;/p&gt;
    &lt;quote&gt;That's why I said Debian testing / unstable, not Debian stable.&lt;/quote&gt;
    &lt;p&gt;My bad! :)&lt;/p&gt;
    &lt;quote&gt;Mint which doesn't ship recent kernel&lt;/quote&gt;
    &lt;p&gt;The version numbers might seem dated, by mind that Ubuntu based distros maintain these kernels for a longer time and backport newer features.&lt;/p&gt;
    &lt;quote&gt;Also, I think KDE is a better fit for modern gaming features&lt;/quote&gt;
    &lt;p&gt;I love KDE Plasma, really. Only reason why I didn't switch is because Cinnamon is "good enough" for the time being, and my requirements of DE features aren't all that high. Wayland is not required in any shape or fashion for gaming as of today. I'd notice if it were (still not using it). ;)&lt;/p&gt;
    &lt;quote&gt;it's more stable than Arch&lt;/quote&gt;
    &lt;p&gt;Anything is. ;)&lt;/p&gt;
    &lt;p&gt;Not everyone uses computers only for gaming, I'd imagine majority actually uses them for everything, and gaming is just one use case among many.&lt;/p&gt;
    &lt;p&gt;Last edited by Shmerl on 2 Nov 2025 at 8:40 pm UTC&lt;/p&gt;
    &lt;quote&gt;I periodically see a bunch of people complaining that their hardware doesn't work, which ends up being them using Mint which doesn't ship recent kernel and Mesa.This can happen if you've got very recent hardware. You don't get anything newer than Ubuntu's HWE kernels via the kernel manager UI, and Mesa is whatever Ubuntu LTS ships. My own solution is to install latest Mesa from Kisak's PPA and the kernel from Xanmod. Only takes a couple of minutes to set these up, but it's not something I'd expect a complete Linux newbie to do, obviously.&lt;/quote&gt;
    &lt;quote&gt;this can happen if you've got very recent hardware.&lt;/quote&gt;
    &lt;p&gt;Or simply recent enough, say latest generation of AMD GPUs that have minimum requirements that distros like Mint often don't supply by default. My point is that I find it a bit counter productive to recommend such distros for newcomers from Windows, since it results in them having problems.&lt;/p&gt;
    &lt;p&gt;On the other side of it, the trade off of rolling distros is the need to learn more stuff, but I think such trade off is worth it and that's not time wasted.&lt;/p&gt;
    &lt;p&gt;Last edited by Shmerl on 2 Nov 2025 at 9:38 pm UTC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.gamingonlinux.com/2025/11/linux-gamers-on-steam-finally-cross-over-the-3-mark/"/><published>2025-11-02T18:54:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45792579</id><title>Lisp: Notes on its Past and Future (1980)</title><updated>2025-11-02T22:35:48.821592+00:00</updated><content>&lt;doc fingerprint="9449101996ab135c"&gt;
  &lt;main&gt;
    &lt;p&gt; John McCarthy &lt;lb/&gt; Computer Science Department &lt;lb/&gt; Stanford University &lt;lb/&gt; Stanford, CA 94305 &lt;lb/&gt; jmc@cs.stanford.edu &lt;lb/&gt; http://www-formal.stanford.edu/jmc/&lt;/p&gt;
    &lt;p&gt; JanFebMarAprMayJun JulAugSepOctNovDec , :&amp;lt; 10 0 &lt;/p&gt;
    &lt;p&gt;LISP has survived for 21 years because it is an approximate local optimum in the space of programming languages. However, it has accumulated some barnacles that should be scraped off, and some long-standing opportunities for improvement have been neglected. It would benefit from some co-operative maintenance especially in creating and maintaining program libraries. Computer checked proofs of program correctness are now possible for pure LISP and some extensions, but more theory and some smoothing of the language itself are required before we can take full advantage of LISP's mathematical basis.&lt;/p&gt;
    &lt;p&gt;1999 note: This article was included in the 1980 Lisp conference held at Stanford. Since it almost entirely corresponds to my present opinions, I should have asked to have it reprinted in the 1998 Lisp users conference proceedings at which I gave a talk with the same title.&lt;/p&gt;
    &lt;p/&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www-formal.stanford.edu/jmc/lisp20th/lisp20th.html"/><published>2025-11-02T19:05:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45792951</id><title>MTurk is 20 years old today – what did you create with it?</title><updated>2025-11-02T22:35:48.408323+00:00</updated><content>&lt;doc fingerprint="17aa4d192c3126df"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;MTurk was built by two two-pizza teams at AWS over the course of a year and launched on Nov 2, 2005. It took a few days for people to find it and catch on, but then things got busy.&lt;/p&gt;
      &lt;p&gt;At the time, AWS was about 100 people (when you were on call, you were on call for all of AWS), Amazon had just hit 10,000, S3 was still in private beta, and EC2 was a whitepaper.&lt;/p&gt;
      &lt;p&gt;What did you create with MTurk and the incredibly patient hard-working workforce behind it?&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45792951"/><published>2025-11-02T20:02:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45793087</id><title>Amazon Rivian Electric Delivery Vans Arrive in Canada</title><updated>2025-11-02T22:35:48.166668+00:00</updated><content>&lt;doc fingerprint="f86212f2aeae28e9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Amazon Rivian Electric Delivery Vans Arrive in Canada&lt;/head&gt;
    &lt;p&gt;Support CleanTechnica's work through a Substack subscription or on Stripe.&lt;/p&gt;
    &lt;p&gt;Amazon has deployed Rivian’s electric delivery vans in Canada for the first time, the companies announced this week. Deployed in the Vancouver area, there are now 50 Rivian EDVs roaming the streets in their Amazon wrapping.&lt;/p&gt;
    &lt;p&gt;As someone living in Florida who has seen these electric delivery vans all over the place for quite a while now, it’s surprising to me that they are just launching in Canada. Without a doubt, though, those of you living there will be excited to see them popping up in your neighborhoods, easily identifiable by their funny round headlights.&lt;/p&gt;
    &lt;p&gt;“Decarbonized transportation is a huge part of Amazon’s goal for net-zero carbon emissions across operations by 2040. It’s partners like Rivian that make this goal feasible,” noted Emily Barber, Director of Global Fleet and Products at Amazon. “We are bringing 100,000 Rivian Electric Delivery Vans on the road by 2030. Today, we have more than 35,000 Electric Delivery Vans globally that have delivered more than 1.5 billion packages. Our mission is to build the safest, most sustainable and advanced fleet in the world.”&lt;/p&gt;
    &lt;p&gt;“We built this state-of-the-art Electric Delivery Van from the ground up, and it’s unlike anything else in the market. Launching our vans in Canada is a proud moment for our entire team,” added Erica Tsypin, Director, B2B Sales &amp;amp; Partnerships, Rivian. “Since our first production vans hit the road in 2022, Rivian and Amazon have learned quite a lot together, which has allowed us to continuously improve the vehicle’s performance, safety and durability as Amazon has scaled these vans across many climates and geographies.”&lt;/p&gt;
    &lt;p&gt;Rivian notes that the company worked closely with Amazon delivery drivers in developing the vehicle. Some highlights of the EDV are “360-degree cameras for driver safety, and patented, energy-saving microclimate seats for all-weather comfort.”&lt;/p&gt;
    &lt;p&gt;Rivian has been operating in Canada for 5 years. It has the following facilities in place there:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A unique space in downtown Vancouver.&lt;/item&gt;
      &lt;item&gt;4 service &amp;amp; delivery centers in Toronto, Ontario; Richmond, British Columbia; Montreal, Quebec; and Calgary, Alberta.&lt;/item&gt;
      &lt;item&gt;Software engineering hubs in downtown Vancouver and Toronto, which is also where engineers from the company’s joint venture with Volkswagen, Rivian Volkswagen Group Technologies, work.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Canada continues to advance its electrification and decarbonization efforts. In the wake of extreme, unnecessary conflict with the United States due to the decisions of President Donald Trump, rumor is that it’s possible another big step forward could be coming to The Great White North. Canada may drop its 100% tariffs on China-produced electric vehicles. If that was to happen, the country could get access to all manner of high-tech, unbelievably affordable, ridiculously compelling electric vehicles from brands like BYD, XPENG, Leapmotor, Zeekr, Avatr, and NIO, among others. For now, though, this is only a rumor and it could just be a negotiating tactic to push back on some of Trump’s demands and behavior.&lt;/p&gt;
    &lt;p&gt;In any case, Canada will have more electric vehicles on the roads this Christmas season thanks to the Amazon and Rivian EDV deployment, and I imagine it won’t be long before that fleet of 50 doubles, triples, and quadruples.&lt;/p&gt;
    &lt;p&gt;Sign up for CleanTechnica's Weekly Substack for Zach and Scott's in-depth analyses and high level summaries, sign up for our daily newsletter, and follow us on Google News!&lt;/p&gt;
    &lt;p&gt;Have a tip for CleanTechnica? Want to advertise? Want to suggest a guest for our CleanTech Talk podcast? Contact us here.&lt;/p&gt;
    &lt;p&gt;Sign up for our daily newsletter for 15 new cleantech stories a day. Or sign up for our weekly one on top stories of the week if daily is too frequent.&lt;/p&gt;
    &lt;p&gt;CleanTechnica uses affiliate links. See our policy here.&lt;/p&gt;
    &lt;p&gt;CleanTechnica's Comment Policy&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cleantechnica.com/2025/10/30/rivian-electric-delivery-vans-arrive-in-canada/"/><published>2025-11-02T20:22:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45793216</id><title>"You Don't Need Kafka, Just Use Postgres" Considered Harmful</title><updated>2025-11-02T22:35:48.009062+00:00</updated><content>&lt;doc fingerprint="734890b55de30186"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;"You Don't Need Kafka, Just Use Postgres" Considered Harmful&lt;/head&gt;
    &lt;p&gt;Looking to make it to the front page of HackerNews? Then writing a post arguing that "Postgres is enough", or why "you don’t need Kafka at your scale" is a pretty failsafe way of achieving exactly that. No matter how often it has been discussed before, this topic is always doing well. And sure, what’s not to love about that? I mean, it has it all: Postgres, everybody’s most favorite RDBMS—check! Keeping things lean and easy—sure, count me in! A somewhat spicy take—bring it on!&lt;/p&gt;
    &lt;p&gt;The thing is, I feel all these articles kinda miss the point; Postgres and Kafka are tools designed for very different purposes, and naturally, which tool to use depends very much on the problem you actually want to solve. To me, the advice "You Don’t Need Kafka, Just Use Postgres" is doing more harm than good, leading to systems built in a less than ideal way, and I’d like to discuss why this is in more detail in this post. Before getting started though, let me get one thing out of the way really quick: this is not an anti-Postgres post. I enjoy working with Postgres as much as the next person (for those use cases it is meant for). I’ve used it in past jobs, and I’ve written about it on this blog before. No, this is a pro-"use the right tool for the job" post.&lt;/p&gt;
    &lt;p&gt;So what’s the argument of the "You Don’t Need Kafka, Just Use Postgres" posts? Typically, they argue that Kafka is hard to run or expensive to run, or a combination thereof. When you don’t have "big data", this cost may not be justified. And if you already have Postgres as a database in your tech stack, why not keep using this, instead of adding yet another technology?&lt;/p&gt;
    &lt;p&gt;Usually, these posts then go on to show how to use &lt;code&gt;SELECT ... FOR UPDATE SKIP LOCKED&lt;/code&gt; for building a… job queue. Which is where things already start to make a bit less sense to me. The reason being that queuing just is not a typical use case for Kafka to begin with. It requires message-level consumer parallelism, as well as the ability to acknowledge individual messages, something Kafka historically has not supported. Now, the Kafka community actually is working towards queue support via KIP-932, but this is not quite ready for primetime yet. Until then, the argument boils down to not use Kafka for something it has not been designed for in the first place. Hm, yeah, ok?&lt;/p&gt;
    &lt;p&gt;That being said, building a robust queue on top of Postgres is actually harder than it may sound. Long-running transactions by queue consumers can cause MVCC bloat and WAL pile-up; Postgres' vacuum process not being able to keep up with the rate of changes can quickly become a problem for this use case. So if you want to go down that path, make sure to run representative performance tests, for a sustained period of time. You won’t find out about issues like this by running two minute tests.&lt;/p&gt;
    &lt;p&gt;So let’s actually take a closer look at the "small scale" argument, as in "with such a low data volume, you just can use Postgres". But to use it for what exactly? What is the problem you are trying to solve? After all, Postgres and Kafka are tools designed for addressing specific use cases. One is a database, the other is an event streaming platform. Without knowing and talking about what one actually wants to achieve, the conversation boils down to "I like this tool better than that" and is pretty meaningless.&lt;/p&gt;
    &lt;p&gt;Kafka enables a wide range of use cases such as microservices communication and data exchange, ingesting IoT sensor data, click streams, or metrics, log processing and aggregation, low-latency data pipelines between operational databases and data lakes/warehouses, or realtime stream processing, for instance for fraud detection and recommendation systems.&lt;/p&gt;
    &lt;p&gt;So if you have one of those use cases, but at a small scale (low volume of data), could you then use Postgres instead of Kafka? And if so, does it make sense? To answer this, you need to consider the capabilities and features you get from Kafka which make it such a good fit for these applications. And while scalability indeed is one of Kafka’s core characteristics, it has many other traits which make it very attractive for event streaming applications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Log semantics: At its core, Kafka is a persistent ordered event log. Records are not deleted after processing, instead they are subject to time-based retention policies or key-based compaction, or they could be retained indefinitely. Consumers can replay a topic from a given offset, or from the very beginning. If needed, consumers can work with exactly-once semantics. This goes way beyond simple queue semantics and replicating it on top of Postgres will be a substantial undertaking.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Fault tolerance and high availability (HA): Kafka workloads are scaled out in clusters running on multiple compute nodes. This is done for two reasons: increasing the throughput the system can handle (not relevant at small scale) and increasing reliability (very much relevant also at small scale). By replicating the data to multiple nodes, instance failures can be easily tolerated. Each node in the cluster can be a leader for a topic partition (i.e., receive writes), with another node taking over if the previous leader becomes unavailable.&lt;/p&gt;&lt;lb/&gt;With Postgres in contrast, all writes go to a single node, while replicas only support read requests. A broker failover in Kafka will affect (in the form of increased latencies) only those partitions it is the leader for, whereas the failure of the Postgres primary node in a cluster is going to affect all writers. While Kafka broker failovers happen automatically, manual intervention is required in order to promote a Postgres replica to primary, or an external coordinator such as Patroni must be used. Alternatively, you might consider Postgres-compatible distributed databases such as CockroachDB, but then the conversation shifts quite a bit away from "Just use Postgres".&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Consumer groups: One of the strengths of the Kafka protocol is its support for organizing consumers in groups. Multiple clients can distribute the load of reading the messages from a given topic, making sure that each message is processed by exactly one member of the group. Also when handling only a low volume of messages, this is very useful. For instance, consider a microservice which receives messages from another service. For the purposes of fault-tolerance, the service is scaled out to multiple instances. By configuring a Kafka consumer group for all the service instances, the incoming messages will be distributed amongst them.&lt;/p&gt;&lt;lb/&gt;How would the same look when using Postgres? Considering the "small scale" scenario, you could decide that only one of the service instances should read all the messages. But which one do you select? What happens if that node fails? Some kind of leader election would be required. Ok, so let’s make each member of the application cluster consume from the topic then? For this you need to think about how to distribute the messages from the Postgres-based topic, how to handle client failures, etc. So your job now essentially is to re-implement Kafka’s consumer rebalance protocol. This is far from trivial and it certainly goes against the initial goal of keeping things simple.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Low latency: Let’s talk about latency, i.e. the time it takes from sending a message to a topic until it gets processed by a consumer. Having a low data volume doesn’t necessarily imply that you do not want low latency. Think about fraud detection, for example. Also when processing only a handful of transactions per second, you want to be able to spot fraudulent patterns very quickly and take action accordingly. Or a data pipeline from your operational data store to a search index. For a good user experience, search results should be based on the latest data as much as possible. With Kafka, latencies in the milli-second range can be achieved for use cases like this. Trying to do the same with Postgres would be really tough, if possible at all. You don’t want to hammer your database with queries from a herd of poll-based queue clients too often, while LISTEN/NOTIFY is known to suffer from heavy lock contention problems.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Connectors: One important aspect which is usually omitted from all the "Just use Postgres" posts is connectivity. When implementing data pipelines and ETL use cases, you need to get data out of your data source and put it into Kafka. From there, it needs to be propagated into all kinds of data sinks, with the same dataset oftentimes flowing into multiple sinks at once, such as a search index and a data lake. Via Kafka Connect, Kafka has a vast ecosystem of source and sink connectors, which can be combined, mix-and-match style. Taking data from MySQL into Iceberg? Easy. Going from Salesforce to Snowflake? Sure. There’s ready-made connectors for pretty much every data system under the sun.&lt;/p&gt;&lt;lb/&gt;Now, what would this look like when using Postgres instead? There’s no connector ecosystem for Postgres like there is for Kafka. This makes sense, as Postgres never has been meant to be a data integration platform, but it means you’ll have to implement bespoke source and sink connectors for all the systems you want to integrate with.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Clients, schemas, developer experience: One last thing I want to address is the general programming model of a "Just use Postgres" event streaming solution. You might think of using SQL as the primary interface for producing and consuming messages. That sounds easy enough, but it’s also very low level. Building some sort of client will probably make sense. You may need consumer group support, as discussed above. You’ll need support for metrics and observability ("What’s my consumer lag?"). How do you actually go about converting your events into a persistent format? Some kind of serializer/deserializer infrastructure will be needed, and while at it, you probably should have support for schema management and evolution, too. What about DLQ support? With Kafka and its ecosystem, you get battle-proven clients and tooling, which will help you with all that, for all kinds of programming languages. You could rebuild all this, of course, but it would take a long time and essentially equate to recreating large parts of Kafka and its ecosystem.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So where does all that leave us? Should you use Postgres as a job queue then? I mean, why not, if it fits the bill for you, go for it. Don’t build it yourself though, use an existing extension like pgmq. And make sure to understand the potential implications on MVCC bloat and vacuuming discussed above.&lt;/p&gt;
    &lt;p&gt;Now, when it comes to using Postgres instead of Kafka as an event streaming platform, this proposition just doesn’t make an awful lot of sense to me, no matter what the volume of the data is going to be. There’s so much more to event streaming than what’s typically discussed in the "Just use Postgres" posts; while you might be able to punt some of the challenges for some time, you’ll eventually find yourself in the business of rebuilding your own version of Kafka, on top of Postgres. But what’s the point of recreating and maintaining the work already done by hundreds of contributors in the course of many years? What starts as an effort to "keep things simple" actually creates a substantial amount of unnecessary complexity. Solving this challenge might sound like a lot of fun purely from an engineering perspective, but for most organizations out there, it’s probably just not the right problem they should focus on.&lt;/p&gt;
    &lt;p&gt;Another problem of the "small scale" argument is that what’s a low data volume today may be a much bigger volume next week. This is a trade-off, of course, but a common piece of advice is to build your systems for the current and the next order of magnitude of load: you should be able to sustain 10x of your current load and data volume as your business grows. This will be easily doable with Kafka which has been designed with scalability at its core, but it may be much harder for a queue implementation based on Postgres. It is single-writer as discussed above, so you’d have to look at scaling up, which becomes really expensive really quickly. So you might decide to migrate to Kafka eventually, which will be a substantial effort when thinking of migrating data, moving your applications from your home-grown clients to Kafka, etc.&lt;/p&gt;
    &lt;p&gt;In the end, it all comes down to choosing the right tool for the job. Use Postgres if you want to manage and query a relational data set. Use Kafka if you need to implement realtime event streaming use cases. Which means, yes, oftentimes, it actually makes sense to work with both tools as part of your overall solution: Postgres for managing a service’s internal state, and Kafka for exchanging data and events with other services. Rather than trying to emulate one with the other, use each one for its specific strengths. How to keep both Postgres and Kafka in sync in this scenario? Change data capture, and in particular the outbox pattern can help there. So if there is a place for "Postgres over Kafka", it is actually here: for many cases it makes sense to write to Kafka not directly, but through your database, and then to emit events to Kafka via CDC, using tools such as Debezium. That way, both resources are (eventually) consistent, keeping things very simple from an application developer perspective.&lt;/p&gt;
    &lt;p&gt;This approach also has the benefit of decoupling (and protecting) your operational datastore from the potential impact of downstream event consumers. You probably don’t want to be at the risk of increased tail latencies of your operational REST API because there’s a data lake ingest process, perhaps owned by another team, which happens to reread an entire topic from a table in your service’s database at the wrong time. Adhering to the idea of the synchrony budget, it makes sense to separate the systems for addressing these different concerns.&lt;/p&gt;
    &lt;p&gt;What about the operational overhead then? While this definitely warrants consideration, I believe that oftentimes that concern is overblown. Running Kafka for small data sets really isn’t that hard. With the move from ZooKeeper to KRaft mode, running a single Kafka instance is trivial for scenarios not requiring fault tolerance. Managed services make running Kafka a very uneventful experience (pun intended) and should be the first choice, in particular when setting out with low scale use cases. Cost will be manageable kinda by definition by virtue of having a low volume of data. Plus, the time and effort for solving all the issues with a custom implementation discussed above should be part of the TCO consideration to be useful.&lt;/p&gt;
    &lt;p&gt;So yes, if you want to make it to the front page of HackerNews, arguing that "Postgres is enough" may get you there; but if you actually want to solve your real-world problems in an effective and robust way, use the right tool for the job.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.morling.dev/blog/you-dont-need-kafka-just-use-postgres-considered-harmful/"/><published>2025-11-02T20:37:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45793244</id><title>Alleged Jabber Zeus Coder 'MrICQ' in U.S. Custody</title><updated>2025-11-02T22:35:47.908452+00:00</updated><content>&lt;doc fingerprint="62a459f43823868d"&gt;
  &lt;main&gt;
    &lt;p&gt;A Ukrainian man indicted in 2012 for conspiring with a prolific hacking group to steal tens of millions of dollars from U.S. businesses was arrested in Italy and is now in custody in the United States, KrebsOnSecurity has learned.&lt;/p&gt;
    &lt;p&gt;Sources close to the investigation say Yuriy Igorevich Rybtsov, a 41-year-old from the Russia-controlled city of Donetsk, Ukraine, was previously referenced in U.S. federal charging documents only by his online handle “MrICQ.” According to a 13-year-old indictment (PDF) filed by prosecutors in Nebraska, MrICQ was a developer for a cybercrime group known as “Jabber Zeus.”&lt;/p&gt;
    &lt;p&gt;The Jabber Zeus name is derived from the malware they used — a custom version of the ZeuS banking trojan — that stole banking login credentials and would send the group a Jabber instant message each time a new victim entered a one-time passcode at a financial institution website. The gang targeted mostly small to mid-sized businesses, and they were an early pioneer of so-called “man-in-the-browser” attacks, malware that can silently intercept any data that victims submit in a web-based form.&lt;/p&gt;
    &lt;p&gt;Once inside a victim company’s accounts, the Jabber Zeus crew would modify the firm’s payroll to add dozens of “money mules,” people recruited through elaborate work-at-home schemes to handle bank transfers. The mules in turn would forward any stolen payroll deposits — minus their commissions — via wire transfers to other mules in Ukraine and the United Kingdom.&lt;/p&gt;
    &lt;p&gt;The 2012 indictment targeting the Jabber Zeus crew named MrICQ as “John Doe #3,” and said this person handled incoming notifications of newly compromised victims. The Department of Justice (DOJ) said MrICQ also helped the group launder the proceeds of their heists through electronic currency exchange services.&lt;/p&gt;
    &lt;p&gt;Two sources familiar with the Jabber Zeus investigation said Rybtsov was arrested in Italy, although the exact date and circumstances of his arrest remain unclear. A summary of recent decisions (PDF) published by the Italian Supreme Court states that in April 2025, Rybtsov lost a final appeal to avoid extradition to the United States.&lt;/p&gt;
    &lt;p&gt;According to the mugshot website lockedup[.]wtf, Rybtsov arrived in Nebraska on October 9, and was being held under an arrest warrant from the U.S. Federal Bureau of Investigation (FBI).&lt;/p&gt;
    &lt;p&gt;The data breach tracking service Constella Intelligence found breached records from the business profiling site bvdinfo[.]com showing that a 41-year-old Yuriy Igorevich Rybtsov worked in a building at 59 Barnaulska St. in Donetsk. Further searching on this address in Constella finds the same apartment building was shared by a business registered to Vyacheslav “Tank” Penchukov, the leader of the Jabber Zeus crew in Ukraine.&lt;/p&gt;
    &lt;p&gt;Penchukov was arrested in 2022 while traveling to meet his wife in Switzerland. Last year, a federal court in Nebraska sentenced Penchukov to 18 years in prison and ordered him to pay more than $73 million in restitution.&lt;/p&gt;
    &lt;p&gt;Lawrence Baldwin is founder of myNetWatchman, a threat intelligence company based in Georgia that began tracking and disrupting the Jabber Zeus gang in 2009. myNetWatchman had secretly gained access to the Jabber chat server used by the Ukrainian hackers, allowing Baldwin to eavesdrop on the daily conversations between MrICQ and other Jabber Zeus members.&lt;/p&gt;
    &lt;p&gt;Baldwin shared those real-time chat records with multiple state and federal law enforcement agencies, and with this reporter. Between 2010 and 2013, I spent several hours each day alerting small businesses across the country that their payroll accounts were about to be drained by these cybercriminals.&lt;/p&gt;
    &lt;p&gt;Those notifications, and Baldwin’s tireless efforts, saved countless would-be victims a great deal of money. In most cases, however, we were already too late. Nevertheless, the pilfered Jabber Zeus group chats provided the basis for dozens of stories published here about small businesses fighting their banks in court over six- and seven-figure financial losses.&lt;/p&gt;
    &lt;p&gt;Baldwin said the Jabber Zeus crew was far ahead of its peers in several respects. For starters, their intercepted chats showed they worked to create a highly customized botnet directly with the author of the original Zeus Trojan — Evgeniy Mikhailovich Bogachev, a Russian man who has long been on the FBI’s “Most Wanted” list. The feds have a standing $3 million reward for information leading to Bogachev’s arrest.&lt;/p&gt;
    &lt;p&gt;The core innovation of Jabber Zeus was an alert that MrICQ would receive each time a new victim entered a one-time password code into a phishing page mimicking their financial institution. The gang’s internal name for this component was “Leprechaun,” (the video below from myNetWatchman shows it in action). Jabber Zeus would actually re-write the HTML code as displayed in the victim’s browser, allowing them to intercept any passcodes sent by the victim’s bank for multi-factor authentication.&lt;/p&gt;
    &lt;p&gt;“These guys had compromised such a large number of victims that they were getting buried in a tsunami of stolen banking credentials,” Baldwin told KrebsOnSecurity. “But the whole point of Leprechaun was to isolate the highest-value credentials — the commercial bank accounts with two-factor authentication turned on. They knew these were far juicier targets because they clearly had a lot more money to protect.”&lt;/p&gt;
    &lt;p&gt;Baldwin said the Jabber Zeus trojan also included a custom “backconnect” component that allowed the hackers to relay their pilfering of commercial bank accounts through the victim’s own infected PC.&lt;/p&gt;
    &lt;p&gt;“The Jabber Zeus crew were literally connecting to the victim’s bank account from the victim’s IP address, or from the remote control function and by fully emulating the device,” he said. “That trojan was like a hot knife through butter of what everyone thought was state-of-the-art secure online banking at the time.”&lt;/p&gt;
    &lt;p&gt;Although the Jabber Zeus crew was in direct contact with the Zeus author, the chats intercepted by myNetWatchman show Bogachev frequently ignored the group’s pleas for help. The government says the real leader of the Jabber Zeus crew was Maksim Yakubets, a 38-year Ukrainian man with Russian citizenship who went by the hacker handle “Aqua.”&lt;/p&gt;
    &lt;p&gt;The Jabber chats intercepted by Baldwin show that Aqua interacted almost daily with MrICQ, Tank and other members of the hacking team, often facilitating the group’s money mule and cashout activities remotely from Russia.&lt;/p&gt;
    &lt;p&gt;The government says Yakubets/Aqua would later emerge as the leader of an elite cybercrime ring of at least 17 hackers that referred to themselves internally as “Evil Corp.” Members of Evil Corp developed and used the Dridex (a.k.a. Bugat) trojan, which helped them siphon more than $100 million from hundreds of victim companies in the United States and Europe.&lt;/p&gt;
    &lt;p&gt;This 2019 story about the government’s $5 million bounty for information leading to Yakubets’s arrest includes excerpts of conversations between Aqua, Tank, Bogachev and other Jabber Zeus crew members discussing stories I’d written about their victims. Both Baldwin and I were interviewed at length for a new weekly six-part podcast by the BBC that delves deep into the history of Evil Corp. Episode One focuses on the evolution of Zeus, while the second episode centers on an investigation into the group by former FBI agent Jim Craig.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://krebsonsecurity.com/2025/11/alleged-jabber-zeus-coder-mricq-in-u-s-custody/"/><published>2025-11-02T20:40:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45793466</id><title>Paris Had a Moving Sidewalk in 1900, and a Thomas Edison Film Captured It (2020)</title><updated>2025-11-02T22:35:47.811707+00:00</updated><content>&lt;doc fingerprint="6f8a17565f3f99af"&gt;
  &lt;main&gt;
    &lt;p&gt;It’s fair to say that few of us now marvel at moving walkways, those standard infrastructural elements of such utilitarian spaces as airport terminals, subway stations, and big-box stores. But there was a time when they astounded even residents of one of the most cosmopolitan cities in the world. The innovation of the moving sidewalk demonstrated at the Paris Exposition of 1900 (previously seen here on Open Culture when we featured Lumière Brothers footage of that period) commanded even Thomas Edison’s attention. As Paleofuture’s Matt Novak tells it at Smithsonian magazine, “Thomas Edison sent one of his producers, James Henry White, to the Exposition and Mr. White shot at least 16 movies,” a clip of which footage you can see above.&lt;/p&gt;
    &lt;p&gt;White “had brought along a new panning-head tripod that gave his films a newfound sense of freedom and flow. Watching the film, you can see children jumping into frame and even a man doffing his cap to the camera, possibly aware that he was being captured by an exciting new technology while a fun novelty of the future chugs along under his feet.”&lt;/p&gt;
    &lt;p&gt;Novak also includes hand-colored photographs from the Paris Exhibition and quotes a New York Observer correspondent describing the moving sidewalk as a “novelty” consisting of “three elevated platforms, the first being stationary, the second moving at a moderate rate of speed, and the third at the rate of about six miles an hour.” Thus “the circuit of the Exposition can be made with rapidity and ease by this contrivance. It also affords a good deal of fun, for most of the visitors are unfamiliar with this mode of transit, and are awkward in its use.”&lt;/p&gt;
    &lt;p&gt;Novak features contemporary images of the Paris Exhibition’s moving sidewalk at Paleofuture, found in the book Paris Exposition Reproduced From the Official Photographs. Its authors describe the trottoir roulant as “a detached structure like a railway train, arriving at and passing certain points at stated times” without a break. “In engineers’ language, it is an ‘endless floor’ raised thirty feet above the level of the ground, ever and ever gliding along the four sides of the square — a wooden serpent with its tail in its mouth.” But the history of the moving walkway didn’t start in Paris: “In 1871 inventor Alfred Speer patented a system of moving sidewalks that he thought would revolutionize pedestrian travel in New York City,” as Novak notes, and the first one actually built was built for Chicago’s 1893 Columbian Exposition — but it cost a nickel to ride and “was undependable and prone to breaking down,” making Paris’ version the more impressive spectacle.&lt;/p&gt;
    &lt;p&gt;Still, the Columbian Exposition’s visitors must have got a kick out of gliding down the pier without having to do the walking themselves. You can learn more about this first moving walkway and its successors, the one at the Paris Exhibition included, from the Little Car video above. However much these early models may look like quaint turn-of-the century novelties, some still see in the technology genuine promise for the future of public transit. Moving walkways work well, writes Treehugger’s Lloyd Alter, “when the walking distance and time is just a bit too long.” And they remind us that “transportation should be about more than just getting from A to B; it should be a pleasure as well.” Parisians “kept the Eiffel Tower from the exhibition” — it had been built for the 1889 World’s Fair — but “it is too bad they didn’t keep this, a sort of moving High Line that is both transportation and entertainment.”&lt;/p&gt;
    &lt;p&gt;Related Content:&lt;/p&gt;
    &lt;p&gt;How French Artists in 1899 Envisioned Life in the Year 2000: Drawing the Future&lt;/p&gt;
    &lt;p&gt;Based in Seoul, Colin Marshall writes and broadcasts on cities, language, and culture. His projects include the book The Stateless City: a Walk through 21st-Century Los Angeles and the video series The City in Cinema. Follow him on Twitter at @colinmarshall or on Facebook.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.openculture.com/2020/03/paris-had-a-moving-sidewalk-in-1900.html"/><published>2025-11-02T21:08:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45793652</id><title>FurtherAI (Series A – A16Z, YC) Is Hiring Across Software and AI</title><updated>2025-11-02T22:35:47.413830+00:00</updated><content>&lt;doc fingerprint="ed5e509c2efe2c46"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;FurtherAI (Series A, a16z + YC) is hiring Software Engineers, AI Engineers, and Forward-Deployed Engineers.&lt;/p&gt;
      &lt;p&gt;We're building AI Agents for the insurance industry and are already post-PMF with strong enterprise adoption.&lt;/p&gt;
      &lt;p&gt;Highlights:&lt;/p&gt;
      &lt;p&gt;- $25M Series A led by Andreessen Horowitz (a16z) - &amp;gt; 10× revenue growth this year - Seed -&amp;gt; Series A in under a year - Small, talent-dense team - 6/15 are founders (incl. 4 YC founders) - Team backgrounds include staff engineers and researchers from Apple, Microsoft, Amazon&lt;/p&gt;
      &lt;p&gt;Looking for strong engineers based in SF who want high ownership, fast shipping, and real impact.&lt;/p&gt;
      &lt;p&gt;If you/someone you know is interested, feel free to reach out directly to Sashank (CTO) - sg+hn@furtherai.com&lt;/p&gt;
      &lt;p&gt;PS: We also have a $10k referral bonus per hire, so plz share it across!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45793652"/><published>2025-11-02T21:35:02+00:00</published></entry></feed>