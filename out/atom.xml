<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-01T16:10:02.992932+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45758392</id><title>Introducing architecture variants</title><updated>2025-11-01T16:10:10.685975+00:00</updated><content>&lt;doc fingerprint="7175513321d03c02"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Is there any way I can check if ubuntu-drivers isn’t working because amd64v3 or not?&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;liviu@bobdenaut:~$ sudo ubuntu-drivers devices
[sudo: authenticate] Password: 
liviu@bobdenaut:~$ sudo ubuntu-drivers list
liviu@bobdenaut:~$ sudo ubuntu-drivers debug
=== log messages from detection ===
DEBUG:root:package_get_nv_allowing_driver(): unable to read /etc/custom_supported_gpus.json
DEBUG:root:package_get_nv_allowing_driver(): unable to read /etc/custom_supported_gpus.json
DEBUG:root:Loading custom detection plugin /usr/share/ubuntu-drivers-common/detect/arm-gles.py
DEBUG:root:plugin /usr/share/ubuntu-drivers-common/detect/arm-gles.py return value: None
DEBUG:root:Loading custom detection plugin /usr/share/ubuntu-drivers-common/detect/sl-modem.py
DEBUG:root:plugin /usr/share/ubuntu-drivers-common/detect/sl-modem.py return value: None
=== modaliases in the system ===
platform:acpi-cpufreq
acpi:INT33A1:PNP0D80:
platform:intel_rapl_msr
acpi:INTC1085:
acpi:USBC000:PNP0CA0:
acpi:PNP0C14:
wmi:05901221-D566-11D1-B2F0-00A0C9062910
wmi:0B3CBB35-E3C2-45ED-91C2-4C5A6D195D1C
wmi:97845ED0-4E6D-11DE-8A39-0800200C9A66
acpi:PNP0C0C:
acpi:ACPI000C:
platform:rtc-efi
acpi:ASUS2018:
wmi:A6FEA33E-DABF-46F5-BFC8-460D961BEC9F
wmi:2BC49DEF-7B15-4F05-8BB7-EE37B9547C0B
acpi:ASUS9001:
platform:coretemp
acpi:INTC1041:
acpi:PNP0C0E:
acpi:MSFT0101:
acpi:ACPI000E:
platform:efivars
wmi:88765ADC-4FFE-4824-A48C-C68BBD71821B
wmi:FEF38251-8214-4D1F-A98A-71A010EBFF7B
serio:ty01pr00id00ex00
input:b0011v0001p0002eAB41-e0,1,4,11,14,k71,72,73,74,75,76,77,79,7A,7B,7C,7D,7E,7F,80,8C,8E,8F,9B,9C,9D,9E,9F,A3,A4,A5,A6,AC,AD,B7,B8,B9,BA,BB,BC,BD,BE,BF,C0,C1,C2,D9,E2,ram4,l0,1,2,sfw
platform:asus-nb-wmi
input:b0019v0000p0000e0000-e0,1,4,14,k71,72,73,8C,94,95,96,98,A3,A4,A5,A6,A9,B7,B8,B9,CA,CB,D4,D7,E0,E1,E2,E3,E5,E6,ED,EE,F0,F7,F8,1D6,1E2,212,213,230,27A,ram4,lsfw
acpi:NVDA0820:
wmi:1F13AB7F-6220-4210-8F8E-8BB5E71EE969
platform:pcspkr
pci:v00008086d00007A3Dsv00000000sd00000000bc06sc04i00
pci:v000010ECd00008168sv00001043sd0000205Fbc02sc00i00
wmi:603E9613-EF25-4338-A3D0-C46177516DB7
pci:v00008086d0000A74Fsv00000000sd00000000bc08sc80i00
pci:v00008086d00007A0Csv00001043sd000014D3bc06sc01i00
acpi:PNP0C0A:
acpi:PNP0C0D:
acpi:PNP0103:
acpi:INTC1046:
pci:v00008086d0000A70Dsv00001043sd000014D3bc06sc04i00
pci:v000010DEd00002860sv00001043sd000014D3bc03sc00i00
pci:v000010DEd000022BDsv00001043sd000014D3bc04sc03i00
hdaudio:v10DE00A6r00100100a01
input:b0000v0000p0000e0000-e0,5,kramlsfw6,8,
pci:v00008086d00007A79sv00000000sd00000000bc0Csc80i00
acpi:CSC3551:
platform:pxa2xx-spi
spi:cs35l41-hda
platform:idma64
pci:v00008086d0000A71Dsv00001043sd000014D3bc11sc80i00
pci:v00008086d00007A70sv00008086sd00000094bc02sc80i00
pci:v00008086d00007A68sv00001043sd000014D3bc07sc80i00
mei::6861ec7b-d07a-4673-856c-7f22b4d55769:02:
mei::3c4852d6-d47b-4f46-b05e-b5edc1aa440e:01:
mei::dba4d603-d7ed-4931-8823-17ad585705d5:01:
mei::dd17041c-09ea-4b17-a271-5b989867ec65:02:
mei::082ee5a7-7c25-470a-9643-0c06f0466ea1:00:
mei::42b3ce2f-bd9f-485a-96ae-26406230b1ff:01:
mei::309dcde8-ccb1-4062-8f78-600115a34327:01:
mei::55213584-9a29-4916-badf-0fb7ed682aeb:02:
mei::8e6a6715-9abc-4043-88ef-9e39c6f63e0f:02:
mei::8c2f4425-77d6-4755-aca3-891fdbc66a58:01:
mei::fbf6fcf1-96cf-4e2e-a6a6-1bab8cbe36b1:01:
mei::b638ab7e-94e2-4ea2-a552-d1c54b627f04:01:
mei::5565a099-7fe2-45c1-a22b-d7e9dfea9a2e:01:
pci:v00008086d00007A44sv00000000sd00000000bc06sc04i00
pci:v00002646d00005017sv00002646sd00005017bc01sc08i02
pci:v00008086d00007A24sv00001043sd000014D3bc0Csc80i00
spi:spi-nor
pci:v00008086d00007A50sv00001043sd00001C9Fbc04sc03i80
hdaudio:v80862818r00100000a01
hdaudio:v10EC0294r00100004a01
input:b0000v0000p0000e0000-e0,5,kramlsfw4,
input:b0000v0000p0000e0000-e0,5,kramlsfw2,
pci:v00008086d0000A702sv00001043sd000014D3bc06sc00i00
pci:v00008086d00007A34sv00000000sd00000000bc06sc04i00
pci:v00001344d00005413sv00001344sd00002100bc01sc08i02
pci:v00008086d00007A4Csv00001043sd000014D3bc0Csc80i00
acpi:ASUF1205:PNP0C50:
hid:b0018g0004v00002808p00000106
input:b0018v2808p0106e0100-e0,1,2,4,k110,111,r0,1,8,B,am4,lsfw
input:b0018v2808p0106e0100-e0,1,3,4,k110,145,148,14A,14D,14E,14F,ra0,1,2F,35,36,37,39,m5,lsfw
pci:v00008086d00007A48sv00000000sd00000000bc06sc04i00
pci:v00008086d00001133sv00001043sd00001F02bc06sc04i00
pci:v00008086d00001134sv00001043sd00001F02bc0Csc03i40
pci:v00008086d00001135sv00000000sd00000000bc0Csc03i30
usb:v1D6Bp0002d0617dc09dsc00dp01ic09isc00ip00in00
usb:v1D6Bp0003d0617dc09dsc00dp03ic09isc00ip00in00
pci:v00008086d00007A30sv00000000sd00000000bc06sc04i00
pci:v00008086d00007A27sv00000000sd00000000bc05sc00i00
pci:v00008086d0000A788sv00001043sd000014D3bc03sc80i00
pci:v00008086d00007A60sv00001043sd0000201Fbc0Csc03i30
usb:v8087p0033d0000dcE0dsc01dp01icE0isc01ip01in00
usb:v8087p0033d0000dcE0dsc01dp01icE0isc01ip01in01
usb:v0B05p19B6d0316dc00dsc00dp00ic03isc01ip01in00
hid:b0003g0001v00000B05p000019B6
input:b0003v0B05p19B6e0110-e0,1,2,3,4,11,14,k71,72,73,74,75,77,79,7A,7B,7C,7D,7E,7F,80,81,82,83,84,85,86,87,88,89,8A,8B,8C,8E,8F,90,94,95,96,98,9B,9C,9E,9F,A1,A3,A4,A5,A6,A7,A8,A9,AB,AC,AD,AE,B0,B1,B2,B3,B4,B5,B7,B8,B9,BA,BB,BC,BD,BE,BF,C0,C1,C2,CA,CB,CE,CF,D0,D1,D2,D4,D8,D9,DB,E0,E1,E4,E5,E6,EA,EB,F0,F1,F4,F5,F7,F8,100,110,111,112,113,114,161,162,166,16A,16E,172,174,176,177,178,179,17A,17B,17C,17D,17F,180,182,183,185,188,189,18C,18D,18E,18F,190,191,192,193,195,197,198,199,19A,19C,1A0,1A1,1A2,1A3,1A4,1A5,1A6,1A7,1A8,1A9,1AA,1AB,1AC,1AD,1AE,1AF,1B0,1B1,1B7,1BA,1D1,240,241,242,243,244,245,246,247,249,24A,24B,24C,24D,250,251,r0,1,6,8,B,C,a20,28,m4,l0,1,2,3,4,sfw
usb:v322Ep2122d1118dcEFdsc02dp01ic0Eisc02ip00in01
usb:v322Ep2122d1118dcEFdsc02dp01ic0Eisc01ip00in00
usb:v17EFp60D1d0100dc00dsc00dp00ic03isc01ip02in00
hid:b0003g0001v000017EFp000060D1
input:b0003v17EFp60D1e0110-e0,1,2,4,k110,111,112,113,114,r0,1,6,8,B,C,am4,lsfw
input:b0003v17EFp60D1e0110-e0,1,4,k74,8E,8F,ram4,lsfw
input:b0003v17EFp60D1e0110-e0,14,kramlsfw
input:b0003v17EFp60D1e0110-e0,1,2,3,4,k71,72,73,74,77,80,82,83,85,86,87,88,89,8A,8B,8C,8E,90,96,98,9B,9C,9E,9F,A1,A3,A4,A5,A6,A7,A8,A9,AB,AC,AD,AE,B1,B2,B5,CE,CF,D0,D1,D2,D4,D8,D9,DB,E0,E1,E4,E5,E6,EA,EB,F0,F1,F4,100,161,162,166,16A,16E,172,174,176,177,178,179,17A,17B,17C,17D,17F,180,182,183,185,188,189,18C,18D,18E,18F,190,191,192,193,195,197,198,199,19A,19C,1A0,1A1,1A2,1A3,1A4,1A5,1A6,1A7,1A8,1A9,1AA,1AB,1AC,1AD,1AE,1AF,1B0,1B1,1B7,1BA,240,241,242,243,244,245,246,247,249,24A,24B,24C,24D,250,251,r6,C,a20,m4,lsfw
pci:v00008086d00007A23sv00001043sd000014D3bc0Csc05i00
platform:iTCO_wdt
cpu:type:x86,ven0000fam0006mod00B7:feature:,0000,0001,0002,0003,0004,0005,0006,0007,0008,0009,000B,000C,000D,000E,000F,0010,0011,0013,0015,0016,0017,0018,0019,001A,001B,001C,001D,001F,002B,0034,003A,003B,003D,0068,006A,006B,006C,006D,006F,0070,0072,0074,0075,0076,0078,0079,007C,007F,0080,0081,0082,0083,0084,0085,0087,0088,0089,008B,008C,008D,008E,008F,0091,0093,0094,0095,0096,0097,0098,0099,009A,009B,009C,009D,009E,00C0,00C5,00C8,00E1,00E3,00EA,00F0,00F1,00F9,00FA,00FB,00FE,00FF,0100,0101,0102,0103,0111,0120,0121,0123,0125,0126,0127,0128,0129,012A,012D,0132,0133,0134,0137,0138,0139,013D,0140,0141,0142,0143,0164,0165,0166,016B,0171,0177,017B,0184,018B,0196,01C0,01C1,01C2,01C4,01C5,01C6,01C7,01C8,01C9,01CA,01CB,01CE,01CF,01D0,01D1,01D2,01D3,01D4,01D6,01D7,01F9,0202,0203,0204,0205,0207,0208,0209,020A,0216,0217,021B,021C,021F,0244,024A,024E,024F,0253,0254,025A,025B,025C,025D,025E,025F,0282,02A2,02A3,02AE
dmi:bvnAmericanMegatrendsInternational,LLC.:bvrG614JI.331:bd11/11/2024:br5.27:efr0.81:svnASUSTeKCOMPUTERINC.:pnROGStrixG614JI_G614JI:pvr1.0:rvnASUSTeKCOMPUTERINC.:rnG614JI:rvr1.0:cvnASUSTeKCOMPUTERINC.:ct10:cvr1.0:sku:
acpi:LNXSYSTM:
acpi:LNXSYBUS:
acpi:ACPI0007:
acpi:PNP0C0F:
acpi:PNP0C02:
input:b0019v0000p0001e0000-e0,1,k74,8F,ramlsfw
acpi:INT340E:PNP0C02:
input:b0019v0000p0003e0000-e0,1,k8E,ramlsfw
acpi:PNP0A08:PNP0A03:
acpi:LNXPOWER:
acpi:LNXVIDEO:
input:b0019v0000p0006e0000-e0,1,kE0,E1,E3,F1,F2,F3,F4,F5,ramlsfw
acpi:ACPI0003:
acpi:ATK3001:PNP030B:
input:b0019v0000p0005e0000-e0,5,kramlsfw0,
acpi:INTC109C:PNP0C02:
acpi:PNP0100:
acpi:PNP0000:
acpi:PRP00001:PNP0A05:
acpi:PNP0C0B:
=== matching driver packages ===

&lt;/code&gt;
      &lt;/quote&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://discourse.ubuntu.com/t/introducing-architecture-variants-amd64v3-now-available-in-ubuntu-25-10/71312"/><published>2025-10-30T10:35:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45763359</id><title>How We Found 7 TiB of Memory Just Sitting Around</title><updated>2025-11-01T16:10:10.442647+00:00</updated><content>&lt;doc fingerprint="aec895875436e7e0"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Inside the hypercube of bad vibes: the namespace dimension&lt;/head&gt;
    &lt;p&gt;Credits: Hyperkube from gregegan.net, diagram (modified) from Kubernetes community repo&lt;/p&gt;
    &lt;p&gt;Plenty of teams run Kubernetes clusters bigger than ours. More nodes, more pods, more ingresses, you name it. In most dimensions, someone out there has us beat.&lt;/p&gt;
    &lt;p&gt;There's one dimension where I suspect we might be near the very top: namespaces. I say that because we keep running into odd behavior in any process that has to keep track of them. In particular, anything that listwatches them ends up using a surprising amount of memory and puts real pressure on the apiserver. This has become one of those scaling quirks you only really notice once you hit a certain threshold. As this memory overhead adds up, efficiency decreases: each byte we have to use for management is a byte we can't put towards user services.&lt;/p&gt;
    &lt;p&gt;The problem gets significantly worse when a daemonset needs to listwatch namespaces or network policies (netpols, which we define per namespace). Since daemonsets run a pod on every node, each of those pods independently performs a listwatch on the same resources. As a result, memory usage increases with the number of nodes.&lt;/p&gt;
    &lt;p&gt;Even worse, these listwatch calls can put significant load on the apiserver. If many daemonset pods restart at once, such as during a rollout, they can overwhelm the server with requests and cause real disruption.&lt;/p&gt;
    &lt;head rend="h2"&gt;Following the memory trail&lt;/head&gt;
    &lt;p&gt;A few months ago, if you looked at our nodes, the largest memory consumers were often daemonsets. In particular, Calico and Vector which handle configuring networking and log collection respectively.&lt;/p&gt;
    &lt;p&gt;We had already done some work to reduce Calico’s memory usage, working closely with the project’s maintainers to make it scale more efficiently. That optimization effort was a big win for us, and it gave us useful insight into how memory behaves when namespaces scale up.&lt;/p&gt;
    &lt;p&gt;To support that work, we set up a staging cluster with several hundred thousand namespaces. We knew that per-namespace network policies (netpols) were the scaling factor that stressed Calico, so we reproduced those conditions to validate our changes.&lt;/p&gt;
    &lt;p&gt;While running those tests, we noticed something strange. Vector, another daemonset, also started consuming large amounts of memory.&lt;/p&gt;
    &lt;p&gt;The pattern looked familiar, and we knew we had another problem to dig into. Vector obviously wasn’t looking at netpols but after poking around a bit we found it was listwatching namespaces from every node in order to allow referencing namespace labels per-pod in the kubernetes logs source.&lt;/p&gt;
    &lt;head rend="h2"&gt;Do we really need these labels?&lt;/head&gt;
    &lt;p&gt;That gave us an idea: what if Vector didn’t need to use namespaces at all? Was that even possible?&lt;/p&gt;
    &lt;p&gt;As it turns out, yes, they were in use in our configuration, but only to check whether a pod belonged to a user namespace.&lt;/p&gt;
    &lt;p&gt;Conveniently, we realized we could hackily describe that condition in another way, and the memory savings were absolutely worth it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building the fix (and breaking the logs)&lt;/head&gt;
    &lt;p&gt;At that point, we were feeling a bit too lucky. We reached out to the Vector maintainers to ask whether disabling this behavior would actually work, and whether they would be open to accepting a contribution if we made it happen.&lt;/p&gt;
    &lt;p&gt;From there, all that was left was to try it. The code change was straightforward. We added a new config option and threaded it through the relevant parts of the codebase.&lt;/p&gt;
    &lt;p&gt;After a few hours of flailing at rustc, a Docker image finally built and we were ready to test the theory. The container ran cleanly with no errors in the logs, which seemed promising.&lt;/p&gt;
    &lt;p&gt;But then we hit a snag. Nothing was being emitted. No logs at all. I couldn’t figure out why.&lt;/p&gt;
    &lt;p&gt;Thankfully, our pal Claude came to the rescue:&lt;/p&gt;
    &lt;p&gt;I rebuilt it (which took like 73 hours because Rust), generated a new image, updating staging, and watched nervously. This time, logs were flowing like normal and…&lt;/p&gt;
    &lt;head rend="h2"&gt;The numbers don’t add up&lt;/head&gt;
    &lt;p&gt;The change saved 50 percent of memory. A huge win. We were ready to wrap it up and ship to production.&lt;/p&gt;
    &lt;p&gt;But then Hieu, one of our teammates, asked a very good question.&lt;/p&gt;
    &lt;p&gt;He was right, something didn’t add up.&lt;/p&gt;
    &lt;p&gt;A few hours later, after repeatedly running my head into a wall, I still hadn’t found anything. There was still a full gibibyte of memory unaccounted for. My whole theory about how this worked was starting to fall apart.&lt;/p&gt;
    &lt;p&gt;I even dropped into the channel to see if anyone had Valgrind experience:&lt;/p&gt;
    &lt;p&gt;Me (later in channel): anybody got a background in valgrind? seems pretty straightforward to get working so far but it won’t end up interfacing with pyroscope. we’ll have to exec in and gdb manually.&lt;lb/&gt;The answer was no.&lt;/p&gt;
    &lt;p&gt;In a last-ditch effort to profile it again, I finally saw the answer. It had been staring me in the face the whole time.&lt;/p&gt;
    &lt;p&gt;We actually had two kubernetes_logs sources on user nodes. I had only set the flag on one of them. Once I applied it to both, memory usage dropped to the level we had seen in staging before the extra namespaces were added.&lt;/p&gt;
    &lt;head rend="h2"&gt;Shipping it&lt;/head&gt;
    &lt;p&gt;I put together a full pull request, and after waiting a little while, it shipped!&lt;/p&gt;
    &lt;p&gt;Around the same time, our colleague Mark happened to be on-call. He did his usual magic — pulled everything together, tested the rollout in staging, and got it shipped to production.&lt;/p&gt;
    &lt;p&gt;I’ll let the results speak for themselves.&lt;/p&gt;
    &lt;p&gt;Our largest cluster saw a 1 TiB memory drop, with savings across our other clusters adding up to a total of just over 7 TiB.&lt;/p&gt;
    &lt;head rend="h2"&gt;7 TiB later&lt;/head&gt;
    &lt;p&gt;Debugging infrastructure at scale is rarely about one big “aha” moment. It’s often the result of many small questions, small changes, and small wins stacked up until something clicks.&lt;/p&gt;
    &lt;p&gt;In this case, it started with a memory chart that didn’t look quite right, a teammate asking the right question at the right time, and a bit of persistence. When applied to our whole infrastructure, that simple fix freed up 7 TiB of memory, reduced risk during rollouts, and made the system easier to reason about.&lt;/p&gt;
    &lt;p&gt;Huge thanks to Hieu for pushing the investigation forward, Mark for shipping it smoothly, and the Vector maintainers for being responsive and open to the change.&lt;/p&gt;
    &lt;p&gt;If you’re running daemonsets at scale and seeing unexplained memory pressure, it might be worth asking:&lt;/p&gt;
    &lt;p&gt;Do you really need those namespace labels?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://render.com/blog/how-we-found-7-tib-of-memory-just-sitting-around"/><published>2025-10-30T18:25:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45766501</id><title>Leaker reveals which Pixels are vulnerable to Cellebrite phone hacking</title><updated>2025-11-01T16:10:10.036757+00:00</updated><content>&lt;doc fingerprint="27564c1580bdbb20"&gt;
  &lt;main&gt;
    &lt;p&gt;Despite being a vast repository of personal information, smartphones used to have little by way of security. That has thankfully changed, but companies like Cellebrite offer law enforcement tools that can bypass security on some devices. The company keeps the specifics quiet, but an anonymous individual recently logged in to a Cellebrite briefing and came away with a list of which of Google’s Pixel phones are vulnerable to Cellebrite phone hacking.&lt;/p&gt;
    &lt;p&gt;This person, who goes by the handle rogueFed, posted screenshots from the recent Microsoft Teams meeting to the GrapheneOS forums (spotted by 404 Media). GrapheneOS is an Android-based operating system that can be installed on select phones, including Pixels. It ships with enhanced security features and no Google services. Because of its popularity among the security-conscious, Cellebrite apparently felt the need to include it in its matrix of Pixel phone support.&lt;/p&gt;
    &lt;p&gt;The screenshot includes data on the Pixel 6, Pixel 7, Pixel 8, and Pixel 9 family. It does not list the Pixel 10 series, which launched just a few months ago. The phone support is split up into three different conditions: before first unlock, after first unlock, and unlocked. The before first unlock (BFU) state means the phone has not been unlocked since restarting, so all data is encrypted. This is traditionally the most secure state for a phone. In the after first unlock (AFU) state, data extraction is easier. And naturally, an unlocked phone is open season on your data.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/gadgets/2025/10/leaker-reveals-which-pixels-are-vulnerable-to-cellebrite-phone-hacking/"/><published>2025-10-30T23:12:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45767178</id><title>Myths Programmers Believe about CPU Caches (2018)</title><updated>2025-11-01T16:10:09.895580+00:00</updated><content>&lt;doc fingerprint="e2561c9267702f31"&gt;
  &lt;main&gt;
    &lt;p&gt;As a computer engineer who has spent half a decade working with caches at Intel and Sun, I’ve learnt a thing or two about cache-coherency. This was one of the hardest concepts to learn back in college – but once you’ve truly understood it, it gives you a great appreciation for system design principles.&lt;/p&gt;
    &lt;p&gt;You might be wondering why you as a software developer should care about CPU cache-design. For one thing, many of the concepts learnt in cache-coherency are directly applicable to distributed-system-architecture and database-isolation-levels as well. For instance, understanding how coherency is implemented in hardware caches, can help in better understanding strong-vs-eventual consistency. It can spur ideas on how to better enforce consistency in distributed systems, using the same research and principles applied in hardware.&lt;/p&gt;
    &lt;p&gt;For another thing, misconceptions about caches often lead to false assertions, especially when it comes to concurrency and race conditions. For example, the common refrain that concurrent programming is hard because “different cores can have different/stale values in their individual caches”. Or that the reason we need volatiles in languages like Java, is to “prevent shared-data from being cached locally”, and force them to be “read/written all the way to main memory”.&lt;/p&gt;
    &lt;p&gt;Such misconceptions are mostly harmless (and maybe even helpful), but can also lead to bad design decisions. For instance, developers can start to believe that they are insulated from the above concurrency bugs, when working with single-core-systems. In reality, even single-core systems are at risk of concurrency bugs, if the appropriate concurrency constructs aren’t used.&lt;/p&gt;
    &lt;p&gt;For another, if volatile variables were truly written/read from main-memory every single time, they would be horrendously slow – main-memory references are 200x slower than L1 cache references. In reality, volatile-reads (in Java) can often be just as cheap as a L1 cache reference, putting to rest the notion that volatile forces reads/writes all the way to main memory. If you’ve been avoiding the use of volatiles because of performance concerns, you might have been a victim of the above misconceptions.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Importance of Being Coherent&lt;/head&gt;
    &lt;p&gt;But if different cores each have their own private cache, storing copies of the same data, wouldn’t that naturally lead to data mismatches as they start issuing writes? The answer: hardware caches on modern x86 CPUs like Intel’s, are kept in-sync with one another. These caches aren’t just dumb memory storage units, as many developers seem to think. Rather, there are very intricate protocols and logics, embedded in every cache, communicating with other caches, enforcing coherency across all threads. And all this is happening at the hardware level, meaning that we as software/compiler/systems developers don’t have to deal with it.&lt;/p&gt;
    &lt;p&gt;A quick word about what I mean when I say that caches are “in sync”. There is a great wealth of nuance in this topic, but to simplify greatly, we mean the following: If 2 different threads, anywhere in the system, read from the same memory address, they should never simultaneously read different values.&lt;/p&gt;
    &lt;p&gt;For a quick example of how non-coherent caches can violate the above rule, simply refer to the first section of this tutorial. No modern x86 CPU behaves the way the tutorial describes it, but a buggy processor certainly can. Everything discussed here is a means towards one simple end: preventing such data-mismatches from happening.&lt;/p&gt;
    &lt;p&gt;A widely used protocol used to enforce coherency amongst caches, is known as the MESI protocol. The details of this protocol are entirely abstracted away from software, which gives CPU architects tremendous flexibility to experiment and innovate on its nuances with every new product or iteration. If you peek under the covers you’ll find that every CPU has its own variant of MESI, with its own unique benefits, tradeoffs and potential for unique bugs. However, these variants all share a great deal in common. And that’s the following: each line of data sitting in a cache, is tagged with one of the following states:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Modified (M) &lt;list rend="ol"&gt;&lt;item&gt;This data has been modified, and differs from main memory&lt;/item&gt;&lt;item&gt;This data is the source-of-truth, and all other data elsewhere is stale&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Exclusive (E) &lt;list rend="ol"&gt;&lt;item&gt;This data has not been modified, and is in sync with the data in main memory&lt;/item&gt;&lt;item&gt;No other sibling cache has this data&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Shared (S) &lt;list rend="ol"&gt;&lt;item&gt;This data has not been modified, and is in sync with the data elsewhere&lt;/item&gt;&lt;item&gt;There are other sibling caches that (may) also have this same data&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Invalid (I) &lt;list rend="ol"&gt;&lt;item&gt;This data is stale, and should never ever be used&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cache coherency can now be accomplished as long as we enforce and update the above states. Let’s look at a few examples for a CPU with 4 cores, each of which has its own L1 cache, along with a global on-chip L2 cache.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Write&lt;/head&gt;
    &lt;p&gt;Suppose a thread on core-1 wants to write to address 0xabcd. The following are some possible sequence of events.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cache Hit&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;L1-1 has the data in E or M state&lt;/item&gt;
      &lt;item&gt;L1-1 performs the write. All done &lt;list rend="ol"&gt;&lt;item&gt;No other cache has the data, so it is safe to write to it immediately&lt;/item&gt;&lt;item&gt;The state of the cache-line is set to M, since it is now modified&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Local Cache Miss, Sibling Cache Hit&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;L1-1 has the data in S state &lt;list rend="ol"&gt;&lt;item&gt;This implies that another sibling cache might have the data&lt;/item&gt;&lt;item&gt;This same flow is also used if L1-1 doesn’t have the data at all&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;L1-1 sends a Request-For-Ownership to the L2 cache&lt;/item&gt;
      &lt;item&gt;L2 looks up its directory and sees that L1-2 currently has the data in S state&lt;/item&gt;
      &lt;item&gt;L2 sends a snoop-invalidate to L1-2&lt;/item&gt;
      &lt;item&gt;L1-2 marks its data as being Invalid (I)&lt;/item&gt;
      &lt;item&gt;L1-2 sends an Ack to L2&lt;/item&gt;
      &lt;item&gt;L2 sends an Ack, along with the latest data, to L1-1 &lt;list rend="ol"&gt;&lt;item&gt;L2 keeps track of the fact that L1-1 has the data for this address in E state&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;L1-1 now has the latest data, as well as permission to enter E state&lt;/item&gt;
      &lt;item&gt;L1-1 performs the write, and changes the state of that data to M&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Memory Read&lt;/head&gt;
    &lt;p&gt;Now suppose a thread on core-2 wants to read from address 0xabcd. The following are some possible sequences of events.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cache Hit&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;L1-2 has the data in S or E or M state&lt;/item&gt;
      &lt;item&gt;L1-2 reads the data and returns it to the thread. All done&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Local Cache Miss, Parent Cache Miss&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;L1-2 has the data in I (invalid) state, meaning it’s not allowed to use it&lt;/item&gt;
      &lt;item&gt;L1-2 sends a Request-for-Share to the L2 cache&lt;/item&gt;
      &lt;item&gt;L2 does not have the data either. It reads the data from memory&lt;/item&gt;
      &lt;item&gt;L2 gets back the data from memory&lt;/item&gt;
      &lt;item&gt;L2 sends this data to L1-2, along with permission to enter S state &lt;list rend="ol"&gt;&lt;item&gt;L2 keeps track of the fact that L1-2 has this data in S-state&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;L1-2 gets the data, stores it in its cache, and sends it to the thread&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Local Cache Miss, Parent Cache Hit&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;L1-2 has the data in I state&lt;/item&gt;
      &lt;item&gt;L1-2 sends a Request-for-S to the L2 cache&lt;/item&gt;
      &lt;item&gt;L2 sees that L1-1 has the data in S state&lt;/item&gt;
      &lt;item&gt;L2 sends an Ack to L1-2, along with the data, and permission to enter S state&lt;/item&gt;
      &lt;item&gt;L1-2 gets the data, stores it in its cache, and sends it to the thread&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Local Cache Miss, Sibling Cache Hit&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;L1-2 has the data in I state&lt;/item&gt;
      &lt;item&gt;L1-2 sends a Request-for-S to the L2 cache&lt;/item&gt;
      &lt;item&gt;L2 sees that L1-1 has the data in E (or M) state&lt;/item&gt;
      &lt;item&gt;L2 sends a snoop-share to L1-1&lt;/item&gt;
      &lt;item&gt;L1-1 downgrades its state to an S&lt;/item&gt;
      &lt;item&gt;L1-1 sends an Ack to L2, along with the modified data if applicable&lt;/item&gt;
      &lt;item&gt;L2 sends an Ack to L1-2, along with the data, and permission to enter S state&lt;/item&gt;
      &lt;item&gt;L1-2 gets the data, stores it in its cache, and sends it to the thread&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Variations&lt;/head&gt;
    &lt;p&gt;The above are just some of the possible scenarios that can occur. In reality, there are numerous variations of the above design, and no 2 implementations are the same. For example, some designs have an O/F state. Some have write-back caches, whereas others use write-through. Some use snoop-broadcasts, while others use a snoop-filter. Some have inclusive caches and others have exclusive caches. The variations are endless, and we haven’t even discussed store-buffers!&lt;/p&gt;
    &lt;p&gt;The above example also considers a simple processor with only 2 levels of caching, but note that this same protocol can also be applied recursively. You could easily add an L3 cache, which in turn coordinates multiple L2s, using the exact same protocol as above. You can also have a multi-processor system, with “Home Agents” that coordinate across multiple L3 caches on completely different chips.&lt;/p&gt;
    &lt;p&gt;In each scenario, each cache only needs to communicate with its parent (to get data/permissions), and its children (to grant/revoke data/permissions). And all this can be accomplished in a manner that’s invisible to the software thread. From the perspective of the software application, the memory subsystem appears to be a single, coherent, monolith … with very variable latencies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Synchronization Still Matters&lt;/head&gt;
    &lt;p&gt;One final word, now that we’ve discussed the awesome power and coherency of your computer’s memory system. If caches are so in-sync with one another, why do we need volatiles at all in languages like Java?&lt;/p&gt;
    &lt;p&gt;That’s a very complicated question that’s better answered elsewhere, but let me just drop one partial hint. Data that’s read into CPU registers, is not kept in sync with data in cache/memory. The software compiler makes all sorts of optimizations when it comes to loading data into registers, writing it back to the cache, and even reordering of instructions. This is all done assuming that the code will be run single-threaded. Hence why any data that is at risk of race-conditions, needs to be manually protected through concurrency algorithms and language constructs such as atomics and volatiles.&lt;/p&gt;
    &lt;p&gt;In the case of Java volatiles, part of the solution is to force all reads/writes to bypass the local registers, and immediately trigger cache reads/writes instead. As soon as the data is read/written to the L1 cache, the hardware-coherency protocol takes over and provides guaranteed coherency across all global threads. Thus ensuring that if multiple threads are reading/writing to the same variable, they are all kept in sync with one another. And this is how you can achieve inter-thread coordination in as little as 1ns.&lt;/p&gt;
    &lt;p&gt;Hacker News – 2018/08&lt;lb/&gt;Hacker News – 2019/11&lt;lb/&gt;/r/programming – 2019/11&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://software.rajivprab.com/2018/04/29/myths-programmers-believe-about-cpu-caches/"/><published>2025-10-31T00:46:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45774086</id><title>Futurelock: A subtle risk in async Rust</title><updated>2025-11-01T16:10:08.626064+00:00</updated><content>&lt;doc fingerprint="4f04f8462db13f84"&gt;
  &lt;main&gt;
    &lt;p&gt;This RFD describes futurelock: a type of deadlock where a resource owned by Future &lt;code&gt;A&lt;/code&gt; is required for another Future &lt;code&gt;B&lt;/code&gt; to proceed, while the Task responsible for both Futures is no longer polling &lt;code&gt;A&lt;/code&gt;.  Futurelock is a particularly subtle risk in writing asynchronous Rust.&lt;/p&gt;
    &lt;p&gt;Oxide initially saw this problem in oxidecomputer/omicron#9259.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example of the problem&lt;/head&gt;
    &lt;p&gt;Consider the following program (in the playground):&lt;/p&gt;
    &lt;code&gt;use std::sync::Arc;&lt;/code&gt;
    &lt;p&gt;This program reliably deadlocks. This surprises a lot of people! A background Task takes a lock, waits 5s, drops the lock and exits. In the meantime, we &lt;code&gt;do_stuff&lt;/code&gt;.  That stuff consists of waiting for two Futures concurrently via &lt;code&gt;select!&lt;/code&gt;. One future waits for the lock while the other sleeps for 0.5s and waits for the lock. So there’s just one lock and all logical streams of execution seem to execute concurrently.  How could this possibly hang?&lt;/p&gt;
    &lt;p&gt;The interesting bits are all in &lt;code&gt;do_stuff()&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;async fn do_stuff(lock: Arc&amp;lt;Mutex&amp;lt;()&amp;gt;&amp;gt;) {&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;future1&lt;/code&gt; is the (boxed) future returned by &lt;code&gt;do_async_thing()&lt;/code&gt;, an async function.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;Weâll call the future returned by &lt;code&gt;sleep&lt;/code&gt;: &lt;code&gt;future2&lt;/code&gt; (or, the "sleep" future).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;The second branch of the &lt;code&gt;select!&lt;/code&gt; is its own future.  Weâll call this &lt;code&gt;future3&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It’s really important to understand what’s happening here so let’s be clear about the sequence.&lt;/p&gt;
    &lt;p&gt;First:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;background task takes&lt;/p&gt;&lt;code&gt;lock&lt;/code&gt;, begins holding it for 5 seconds&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tokio::select!&lt;/code&gt;begins polling&lt;code&gt;&amp;amp;mut future1&lt;/code&gt;.[1] This future attempts to take the lock, blocks, returns&lt;code&gt;Poll::Pending&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tokio::select!&lt;/code&gt;begins polling&lt;code&gt;future2&lt;/code&gt;(the sleep future) and blocks, returning&lt;code&gt;Poll::Pending&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At this point:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;the background task holds the lock&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;the main task is blocked in&lt;/p&gt;&lt;code&gt;tokio::select!&lt;/code&gt;on two different futures:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;future1&lt;/code&gt;is blocked on taking the lock&lt;/item&gt;&lt;item&gt;&lt;code&gt;future2&lt;/code&gt;(the&lt;code&gt;sleep&lt;/code&gt;future) waiting for 500ms&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;500ms later, &lt;code&gt;tokio&lt;/code&gt; wakes up the main task because &lt;code&gt;future2&lt;/code&gt; (the sleep future) is ready.  Inside &lt;code&gt;tokio::select!&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The task polls both futures.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;future1&lt;/code&gt;is still blocked on the lock and returns&lt;code&gt;Pending&lt;/code&gt;.[2]&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;future2&lt;/code&gt;(the sleep future) is ready and returns&lt;code&gt;Ready&lt;/code&gt;.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tokio::select!&lt;/code&gt;chooses the second branch&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;&amp;amp;mut future1&lt;/code&gt;is dropped, but this is just a reference and so has no effect. Importantly, the future itself (&lt;code&gt;future1&lt;/code&gt;) is not dropped.&lt;/item&gt;&lt;item&gt;&lt;p&gt;the second branch is entered.&lt;/p&gt;&lt;code&gt;do_async_thing("op2", …)&lt;/code&gt;is called, creating a new future&lt;code&gt;future3&lt;/code&gt;. This future immediately blocks trying to take the lock, which is still held by the background task.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At this point, we have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;the lock (still) held by the background task&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;the lock’s wait queue contains two waiting futures:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;code&gt;future1&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;code&gt;future3&lt;/code&gt;(the second arm of the&lt;code&gt;tokio::select!&lt;/code&gt;)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are two key points here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;The lockâs wait queue is literally a queue: only&lt;/p&gt;&lt;code&gt;future1&lt;/code&gt;can take the lock once it is released by the background task (unless&lt;code&gt;future1&lt;/code&gt;is dropped).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The behavior of&lt;/p&gt;&lt;code&gt;tokio::select!&lt;/code&gt;is to poll all branches' futures only until one of them returns `Ready`. At that point, it drops the other branches' futures and only runs the body of the branch thatâs ready.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Critically: the same task is responsible for both of the futures waiting on the lock. But that task is currently only polling on one of them. Unfortunately, it’s the wrong one.&lt;/p&gt;
    &lt;p&gt;About 4.5 seconds later:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The background task drops the lock.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;p&gt;The lock is given to&lt;/p&gt;&lt;code&gt;future1&lt;/code&gt;. (See below for more on why.)&lt;/item&gt;
          &lt;item&gt;&lt;p&gt;The task that polled&lt;/p&gt;&lt;code&gt;future1&lt;/code&gt;(the main task) is woken up.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;However, that task is not polling&lt;/p&gt;&lt;code&gt;future1&lt;/code&gt;.&lt;code&gt;future1&lt;/code&gt;is polled at the top-level&lt;code&gt;tokio::select!&lt;/code&gt;. But the&lt;code&gt;tokio::select!&lt;/code&gt;has already chosen the other branch. It’s now only polling&lt;code&gt;future3&lt;/code&gt;. (In fact, even absent the imminent hang,&lt;code&gt;future1&lt;/code&gt;would never be polled again. It would be cancelled without having completed when it got dropped at the end of&lt;code&gt;do_stuff&lt;/code&gt;.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;There is only one task left. It’s blocked on&lt;/p&gt;&lt;code&gt;future3&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;future3&lt;/code&gt;is blocked on a Mutex that’s owned by&lt;code&gt;future1&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;future1&lt;/code&gt;cannot run (and therefore cannot drop the Mutex) until the task starts running it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We call this specific kind of deadlock futurelock. The program is stuck in this state forever.&lt;/p&gt;
    &lt;head rend="h3"&gt;FAQ: why doesnât the Mutex wake up the other future?&lt;/head&gt;
    &lt;p&gt;This particular example uses &lt;code&gt;tokio::sync::Mutex&lt;/code&gt;, which is a fair Mutex.  That means that the lock is given to waiters in the order that they started waiting.  It has to give it to &lt;code&gt;future1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;An unfair Mutex would not fix things. The problem wouldn’t be guaranteed to happen with an unfair Mutex, but it wouldn’t be guaranteed not to, either. The Mutex does not (and cannot) know which future would be "better" to wake up, or which one is being polled. You could imagine an unfair Mutex that always woke up all waiters and let them race to grab the lock again. That would not suffer from risk of futurelock, but it would have the thundering herd problem plus all the liveness issues associated with unfair synchronization primitives. And it’s not how many synchronization primitives work.&lt;/p&gt;
    &lt;p&gt;It’s helpful to view this in terms of responsibilities: the Mutex’s job here is to wake up the next task waiting for the lock. And it’s doing that. It’s that task’s responsibility to check on all the futures that it’s responsible for. The Mutex cannot do that.&lt;/p&gt;
    &lt;head rend="h3"&gt;FAQ: why isnât the &lt;code&gt;tokio::select!&lt;/code&gt; polling on &lt;code&gt;future1&lt;/code&gt;?  Isnât that the whole idea of &lt;code&gt;tokio::select!&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The idea of &lt;code&gt;tokio::select!&lt;/code&gt; is to poll on multiple futures concurrently and enter the branch for whichever one finishes first.  Once one of the futures does finish (as the &lt;code&gt;sleep&lt;/code&gt; one has in our case), control enters that specific branch.  It essentially commits to that branch and it’s only running that branch at that point.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;tokio::select!&lt;/code&gt; docs are explicit about this:&lt;/p&gt;
    &lt;p&gt;By running all async expressions on the current task, the expressions are able to run concurrently but not in parallel. This means all expressions are run on the same thread and if one branch blocks the thread, all other expressions will be unable to continue. If parallelism is required, spawn each async expression using tokio::spawn and pass the join handle to select!.&lt;/p&gt;
    &lt;head rend="h3"&gt;FAQ: doesnât &lt;code&gt;future1&lt;/code&gt; get cancelled?&lt;/head&gt;
    &lt;p&gt;When one of the futures that &lt;code&gt;tokio::select!&lt;/code&gt; is polling on completes, the others get dropped.  In this case, what’s dropped is &lt;code&gt;&amp;amp;mut future1&lt;/code&gt;.  But &lt;code&gt;future1&lt;/code&gt; is not dropped, so the actual future is not cancelled.&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;future1&lt;/code&gt; did get cancelled, you’d get no deadlock.  Try it: change the above to wait on &lt;code&gt;future1&lt;/code&gt; instead of &lt;code&gt;&amp;amp;mut future1&lt;/code&gt;.  Alternatively, you can add an explicit &lt;code&gt;drop(future1);&lt;/code&gt; at line 51 between the &lt;code&gt;sleep&lt;/code&gt; and the &lt;code&gt;do_async_thing&lt;/code&gt;.  This mimics what &lt;code&gt;select!&lt;/code&gt; does if we use &lt;code&gt;future1&lt;/code&gt; rather than &lt;code&gt;&amp;amp;mut future1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When first learning async Rust, it’s common to think of tasks and futures almost interchangeably. When you want parallelism, you spawn a new task and give it the future that you want to run. If you want to do 10 things in parallel, you spawn 10 tasks and then wait for them all to finish.&lt;/p&gt;
    &lt;p&gt;You can have concurrency without tasks (and without parallelism) using something like &lt;code&gt;tokio::select!&lt;/code&gt;.  Within a single task, you can do 10 things concurrently (not in parallel) using &lt;code&gt;tokio::select!&lt;/code&gt; or &lt;code&gt;FuturesUnordered&lt;/code&gt; or the like.  In this case, your one task is polling on all these futures and getting woken up when any of them might be ready.&lt;/p&gt;
    &lt;p&gt;Tasks are the top-level entities that the runtime executes. Each task runs one top-level future. That future can choose to do only do one thing at a time (as in the case of sequential code using &lt;code&gt;await&lt;/code&gt;), or it can choose to do things concurrently by polling many futures, using &lt;code&gt;tokio::select!&lt;/code&gt; or &lt;code&gt;FuturesUnordered&lt;/code&gt; or the like.&lt;/p&gt;
    &lt;head rend="h2"&gt;What causes futurelock?&lt;/head&gt;
    &lt;p&gt;The general problem here is that you have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;task&lt;/p&gt;&lt;code&gt;T&lt;/code&gt;is blocked on future&lt;code&gt;F1&lt;/code&gt;completing (and&lt;code&gt;T&lt;/code&gt;is directly awaiting&lt;code&gt;F1&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;future&lt;/p&gt;&lt;code&gt;F1&lt;/code&gt;is blocked on future&lt;code&gt;F2&lt;/code&gt;in some way (e.g., acquiring a shared Mutex)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;future&lt;/p&gt;&lt;code&gt;F2&lt;/code&gt;is blocked on task&lt;code&gt;T&lt;/code&gt;polling it, but&lt;code&gt;T&lt;/code&gt;isn’t polling it because it’s only polling&lt;code&gt;F1&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most commonly this happens after &lt;code&gt;T&lt;/code&gt; started polling &lt;code&gt;F2&lt;/code&gt; earlier, but then switched to &lt;code&gt;F1&lt;/code&gt;.  This can happen in a bunch of different cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;using&lt;/p&gt;&lt;code&gt;tokio::select!&lt;/code&gt;with a&lt;code&gt;&amp;amp;mut future&lt;/code&gt;and awaiting in one of the other branches (our example above)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;polling futures from a&lt;/p&gt;&lt;code&gt;FuturesOrdered&lt;/code&gt;/&lt;code&gt;FuturesUnordered&lt;/code&gt;(e.g., by calling&lt;code&gt;next()&lt;/code&gt;) and then awaiting on any other future (e.g., each time one of the futures completes from the set, you do some async activity)&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;in a hand-written&lt;/p&gt;&lt;code&gt;Future&lt;/code&gt;impl that behaves analogously&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can hit futurelock even if you never start polling one of the futures. Consider this example:&lt;/p&gt;
    &lt;code&gt;use futures::FutureExt;&lt;/code&gt;
    &lt;p&gt;This deadlocks, too. And for the same reason: this task is waiting on a future that itself depends on a future that this task is responsible for running. This is possible but feels contrived. This RFD focuses on cases where the dependency between futures relates to a shared resource. That generally requires that the futures all start running so they can get in line for the resource.&lt;/p&gt;
    &lt;head rend="h3"&gt;How you can hit this with &lt;code&gt;tokio::select!&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Hitting this problem with &lt;code&gt;tokio::select!&lt;/code&gt; (as in the example above) requires two things to be true:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;You must be passing a&lt;/p&gt;&lt;code&gt;&amp;amp;mut future&lt;/code&gt;to one of the branches. If you’re passing an owned future, then it will get dropped when the&lt;code&gt;tokio::select!&lt;/code&gt;enters a different branch. That generally releases the resources that might have been blocking other futures.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You must be using&lt;/p&gt;&lt;code&gt;await&lt;/code&gt;in one of the branches' handlers. If you’re not doing this, then the task does not get blocked on any particular future at the expense of the others.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That said, it’s just as problematic to have an owned future across a &lt;code&gt;tokio::select!&lt;/code&gt; and await after it (full example):&lt;/p&gt;
    &lt;code&gt;async fn do_stuff(lock: Arc&amp;lt;Mutex&amp;lt;()&amp;gt;&amp;gt;) {&lt;/code&gt;
    &lt;p&gt;This results in exactly the same behavior.&lt;/p&gt;
    &lt;head rend="h3"&gt;How you can hit this with Streams&lt;/head&gt;
    &lt;p&gt;If you pull a future from a &lt;code&gt;Stream&lt;/code&gt; and then await a future that somehow depends on another Future in the stream, you can wind up with futurelock.  Here’s what it looks like with FuturesOrdered (full example):&lt;/p&gt;
    &lt;code&gt;async fn do_stuff(lock: Arc&amp;lt;Mutex&amp;lt;()&amp;gt;&amp;gt;) {&lt;/code&gt;
    &lt;p&gt;These are often used in a loop, so it may tend to look more like this (full example):&lt;/p&gt;
    &lt;code&gt;async fn do_stuff(lock: Arc&amp;lt;Mutex&amp;lt;()&amp;gt;&amp;gt;) {&lt;/code&gt;
    &lt;p&gt;It seems likely that futurelock is a risk when using many other Stream functions.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about &lt;code&gt;join_all&lt;/code&gt;?&lt;/head&gt;
    &lt;p&gt;You can’t hit this with &lt;code&gt;futures::future::join_all&lt;/code&gt;.  That’s because it polls all of its futures and does not stop polling any of the pending futures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Failure mode, debugging&lt;/head&gt;
    &lt;p&gt;Futurelock is a type of deadlock and tends to manifest as a hang of part or all of the program. When we saw this in omicron#9259, every future attempting to access the database became part of the futurelock. Since authorization uses the database, essentially every incoming HTTP request hung indefiniteily.&lt;/p&gt;
    &lt;p&gt;Debugging this problem from direct observation can be next to impossible. Typically, you’d only start looking at data long after the problem happened. At that point, it’s not clear what evidence you’d find even if you could peer into the executor state. The problem looks like a &lt;code&gt;pending&lt;/code&gt; future whose task has been woken up because of that future, but the task has not polled the future.  (Maybe &lt;code&gt;tokio-console&lt;/code&gt; could help?)&lt;/p&gt;
    &lt;p&gt;In omicron#9259, we were able to determine (by tracing individual function calls with DTrace) that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;all incoming requests were blocking on attempts to send on an&lt;/p&gt;&lt;code&gt;mpsc&lt;/code&gt;channel with capacity 1&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;the receiving end of this channel was regularly checking it and finding no messages queued&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This confused us for quite a while. Why are senders blocking if there’s nothing in the channel? In hindsight, the answer’s implied by the documentation for &lt;code&gt;Sender&lt;/code&gt;, which says:&lt;/p&gt;
    &lt;p&gt;Sends a value, waiting until there is capacity.&lt;/p&gt;
    &lt;p&gt;…&lt;/p&gt;
    &lt;p&gt;This channel uses a queue to ensure that calls to send and reserve complete in the order they were requested.&lt;/p&gt;
    &lt;p&gt;One can infer that a given call to &lt;code&gt;send&lt;/code&gt; may block either because there is no capacity or because another sender’s &lt;code&gt;send()&lt;/code&gt; is not completing.  That could be because the channel is full, but in our case it’s because the future for that sender had run into futurelock.&lt;/p&gt;
    &lt;p&gt;It’s hard to give useful advice for debugging this sort of problem aside from advising that you consider futurelock as a possibility if you’re debugging a hang and some future appears blocked when other evidence suggests that it shouldn’t be.&lt;/p&gt;
    &lt;head rend="h2"&gt;Determinations: avoiding this problem&lt;/head&gt;
    &lt;p&gt;Like async cancellation (see [rfd397]), futurelock defeats Rust’s goal of being able to reason locally about correctness. If we look at the pieces involved in our example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Using&lt;/p&gt;&lt;code&gt;tokio::select!&lt;/code&gt;to wait for any of a few things to happen&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Using&lt;/p&gt;&lt;code&gt;await&lt;/code&gt;in a&lt;code&gt;tokio::select!&lt;/code&gt;branch&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Using a&lt;/p&gt;&lt;code&gt;&amp;amp;mut future&lt;/code&gt;with&lt;code&gt;tokio::select!&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Using a Mutex[3]&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;None of these by itself is wrong, but combining them results in futurelock. Remember too that the Mutex could be buried beneath several layers of function calls in different modules or packages. It could require looking across many layers of the stack at once to be able to see the problem.&lt;/p&gt;
    &lt;p&gt;There’s no one abstraction, construct, or programming pattern we can point to here and say "never do this". Still, we can provide some guidelines.&lt;/p&gt;
    &lt;head rend="h3"&gt;In general&lt;/head&gt;
    &lt;p&gt;The most specific general advice we can give is: any time you have a single task polling multiple futures concurrently, be extremely careful that the task never stops polling a future that it previously started polling.&lt;/p&gt;
    &lt;p&gt;One way to avoid this situation is to bias towards spawning futures in new tasks instead. There are other considerations with this approach: futures would be cancelled when they’re dropped, but tasks won’t be aborted when their JoinHandle is dropped. If you want this, see &lt;code&gt;AbortOnDropHandle&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;When using &lt;code&gt;tokio::select!&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;If you find yourself writing or reviewing code that does either of these:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Uses a&lt;/p&gt;&lt;code&gt;&amp;amp;mut future&lt;/code&gt;as one of the async expressions in the&lt;code&gt;tokio::select!&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Awaits inside the handler of a&lt;/p&gt;&lt;code&gt;tokio::select!&lt;/code&gt;branch or after the&lt;code&gt;tokio::select!&lt;/code&gt;before the&lt;code&gt;future&lt;/code&gt;has been dropped&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;then look for the other as well. If both are present, pay close attention to the risk of futurelock. To avoid it, you either need to avoid doing both of these things in the same &lt;code&gt;tokio::select!&lt;/code&gt; call or else be very sure that &lt;code&gt;future&lt;/code&gt; never blocks with shared resources held that could block other futures.&lt;/p&gt;
    &lt;p&gt;Let’s consider a variation of our original example:&lt;/p&gt;
    &lt;code&gt;    let mut future1 = do_async_thing("op1", lock.clone()).boxed();&lt;/code&gt;
    &lt;p&gt;Here, we’ve wrapped the &lt;code&gt;tokio::select!&lt;/code&gt; in a loop.  This is a common pattern.  The idea here is mainly to run &lt;code&gt;future1&lt;/code&gt;, but every 500ms we do something related (like report progress or check if we should cancel the like).&lt;/p&gt;
    &lt;p&gt;The easiest way to make this safer is to spawn &lt;code&gt;future&lt;/code&gt; in a new task.  Then use the &lt;code&gt;JoinHandle&lt;/code&gt; in the &lt;code&gt;tokio::select!&lt;/code&gt;, like this version:&lt;/p&gt;
    &lt;code&gt;    let future1 = do_async_thing("op1", lock.clone());&lt;/code&gt;
    &lt;p&gt;This has the same desired effect of keeping &lt;code&gt;future1&lt;/code&gt; running, but now &lt;code&gt;future1_task&lt;/code&gt; is a separate future.  It’s cancellable, and cancelling it won’t cancel &lt;code&gt;future1&lt;/code&gt;.  (If you want that, you can still use &lt;code&gt;future1_task.abort()&lt;/code&gt;.)  This construction cannot result in futurelock.&lt;/p&gt;
    &lt;p&gt;If you’re not using a loop, this approach is even better: then you can just pass &lt;code&gt;future1_task&lt;/code&gt; to &lt;code&gt;tokio::select!&lt;/code&gt; (rather than &lt;code&gt;&amp;amp;mut future1_task&lt;/code&gt;) and it’ll be more obvious that this is safe.&lt;/p&gt;
    &lt;p&gt;In the end, you should always be extremely careful with &lt;code&gt;tokio::select!&lt;/code&gt;.  That’s because:&lt;/p&gt;
    &lt;p&gt;So either way you’ve got a subtle, non-locally-reasonable, undebuggable problem to worry about that the compiler can’t really help with.&lt;/p&gt;
    &lt;head rend="h3"&gt;When using &lt;code&gt;Stream&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;When using a &lt;code&gt;FuturesOrdered&lt;/code&gt; or &lt;code&gt;FuturesUnordered&lt;/code&gt;, consider instead using tokio’s &lt;code&gt;JoinSet&lt;/code&gt;.  This provides a similar interface, but the futures you’re waiting for are all running in separate tasks.&lt;/p&gt;
    &lt;p&gt;If for whatever reason that’s not appropriate (e.g., you’re not using &lt;code&gt;tokio&lt;/code&gt;, or you really need a &lt;code&gt;Stream&lt;/code&gt; interface), then in the body of a loop that pulls completed futures from the &lt;code&gt;Stream&lt;/code&gt;, do not await any other futures.  If you’re working with a &lt;code&gt;FuturesUnordered&lt;/code&gt;, consider putting those futures into the set instead.&lt;/p&gt;
    &lt;head rend="h3"&gt;When using bounded channels&lt;/head&gt;
    &lt;p&gt;Bounded channels are not really the issue here. Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect. It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property. However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.&lt;/p&gt;
    &lt;p&gt;In Omicron, we commonly use bounded channels with &lt;code&gt;send(msg).await&lt;/code&gt;.  The bound is intended to cap memory usage and provide backpressure, but using the blocking &lt;code&gt;send&lt;/code&gt; creates a second unbounded queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus &lt;code&gt;try_send()&lt;/code&gt; and propagate failure from &lt;code&gt;try_send()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages in the channel. So we use &lt;code&gt;capacity = 1&lt;/code&gt; and let clients block in &lt;code&gt;send().await&lt;/code&gt;.  But we could instead have &lt;code&gt;capacity = 16&lt;/code&gt; and have clients use &lt;code&gt;try_send()&lt;/code&gt; and propagate failure if they’re unable to send the message.  The value &lt;code&gt;16&lt;/code&gt; here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:&lt;/p&gt;
    &lt;p&gt;Channel limits, channel limits: always wrong!&lt;/p&gt;
    &lt;p&gt;Some too short and some too long!&lt;/p&gt;
    &lt;p&gt;But as with timeouts, it’s often possible to find values that work in practice.&lt;/p&gt;
    &lt;p&gt;Using &lt;code&gt;send_timeout()&lt;/code&gt; is not a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.&lt;/p&gt;
    &lt;head rend="h3"&gt;Anti-pattern: just make the channel bigger&lt;/head&gt;
    &lt;p&gt;In our initial encounter with this problem, we had a bounded &lt;code&gt;tokio::sync::mpsc&lt;/code&gt; channel of capacity 1.  Why not bump the capacity up?&lt;/p&gt;
    &lt;p&gt;To avoid futurelock, the channel would have to have capacity big enough that nobody in the call stack could possibly have that many futures that they’ve started and aren’t polling. There is of course no way to know how big this needs to be, and it could change over time as the program evolves. Further, there are big side effects to having big channels like this in terms of latency, backpressure, and memory usage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Anti-pattern: try to avoid dependencies between futures&lt;/head&gt;
    &lt;p&gt;In principle, you could avoid this problem if you avoid dependencies between futures. Aside from using &lt;code&gt;spawn&lt;/code&gt; to do this, we do not recommend this in general because it’s brittle and risky.&lt;/p&gt;
    &lt;p&gt;First, it’s hard to know there are no dependencies. Any shared resource can be such a dependency: a bounded channel of any kind, a Mutex, a request to a remote service, etc. And it can be anywhere in the stack, including several dependency packages down the call chain.&lt;/p&gt;
    &lt;p&gt;Even if there’s no such dependency now, one could be added later. You could imagine &lt;code&gt;future1&lt;/code&gt; calling &lt;code&gt;some_crate::func1()&lt;/code&gt; and &lt;code&gt;future2&lt;/code&gt; calling &lt;code&gt;other_crate::func2()&lt;/code&gt; that seem like simple functions.  &lt;code&gt;some_crate&lt;/code&gt; could decide to add a global Mutex that is otherwise safe and correct, but this would now break your &lt;code&gt;tokio::select!&lt;/code&gt; that was previously assuming these futures shared no dependencies.&lt;/p&gt;
    &lt;p&gt;The exception to this is that using &lt;code&gt;tokio::spawn&lt;/code&gt; is a good way to replace one or more futures that could be subject to futurelock with ones that can’t.  The returned &lt;code&gt;JoinHandle&lt;/code&gt; is a future that becomes ready under the same conditions as the underlying one, but it does not hold shared resources and it’s very unlikely that that would ever change as tokio evolves.  (Such a change would almost certainly break lots of correctly-written programs.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Questions&lt;/head&gt;
    &lt;p&gt;Can we write clippy lints to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Warn when passing&lt;/p&gt;&lt;code&gt;&amp;amp;mut future&lt;/code&gt;to a&lt;code&gt;tokio::select!&lt;/code&gt;arm and suggest that&lt;code&gt;tokio::spawn&lt;/code&gt;be used instead, and&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Warn when using&lt;/p&gt;&lt;code&gt;await&lt;/code&gt;in a&lt;code&gt;tokio::select!&lt;/code&gt;arm? (This is problematic for other reasons anyway when&lt;code&gt;select!&lt;/code&gt;is used in a loop.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are certainly cases to do this and it’s okay to override the warning, but it’d be nice to have that guard rail.&lt;/p&gt;
    &lt;head rend="h2"&gt;Security Considerations&lt;/head&gt;
    &lt;p&gt;None actionable. Futurelock is a potential vector for denial of service, but it’s bad anyway, and we know we want to avoid it.&lt;/p&gt;
    &lt;head rend="h2"&gt;External References&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;[rfd397] Oxide Computer Co. RFD 397 Challenges with async/await in the control plane. 2023.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;[rfd400] Oxide Computer Co. RFD 400 Dealing with cancel safety in async Rust&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rfd.shared.oxide.computer/rfd/0609"/><published>2025-10-31T16:49:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45774571</id><title>Use DuckDB-WASM to query TB of data in browser</title><updated>2025-11-01T16:10:08.366268+00:00</updated><content>&lt;doc fingerprint="8b33bb57b32f478c"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Authors:&lt;/head&gt;
    &lt;p&gt;Published:&lt;/p&gt;
    &lt;p&gt;As part of our Public Data Project, LIL recently launched Data.gov Archive Search. In this post, we look under the hood and reflect on how and why we built this project the way we did.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rethinking the Old Trade-Off: Cost, Complexity, and Access&lt;/head&gt;
    &lt;p&gt;Libraries, digital humanities projects, and cultural heritage organizations have long had to perform a balancing act when sharing their collections online, negotiating between access and affordability. Providing robust features for data discovery, such as browsing, filtering, and search, has traditionally required dedicated computing infrastructure such as servers and databases. Ongoing server hosting, regular security and software updates, and consistent operational oversight are expensive and require skilled staff. Over years or decades, budget changes and staff turnover often strand these projects in an unmaintained or nonfunctioning state.&lt;/p&gt;
    &lt;p&gt;The alternative, static file hosting, requires minimal maintenance and reduces expenses dramatically. For example, storing gigabytes of data on Amazon S3 may cost $1/month or less. However, static hosting often diminishes the capacity for rich data discovery. Without a dynamic computing layer between the userâs web browser and the source files, data access may be restricted to brittle pre-rendered browsing hierarchies or search functionality that is impeded by client memory limits. Under such barriers, the collectionâs discoverability suffers.&lt;/p&gt;
    &lt;p&gt;For years, online collection discovery has been stuck between a rock and a hard place: accept the complexity and expense required for a good user experience, or opt for simplicity and leave users to contend with the blunt limitations of a static discovery layer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why We Explored a New Approach&lt;/head&gt;
    &lt;p&gt;When LIL began thinking about how to provide discovery for the Data.gov Archive, we decided that building a lightweight and easily maintained access point from the beginning would be worth our teamâs effort. We wanted to provide low-effort discovery with minimal impact on our resources. We also wanted to ensure that whatever path we chose would encourage, rather than impede, long-term access.&lt;/p&gt;
    &lt;p&gt;This approach builds on our recent experience when the Caselaw Access Project (CAP) hit a transition moment. At that time, we elected to switch case.law to a static site and to partner with others dedicated to open legal data to provide more feature-rich access.&lt;/p&gt;
    &lt;p&gt;CAP includes some 11 TB of data; the Data.gov Archive represents nearly 18 TB, with the catalog metadata alone accounting for about 1 GB. Manually browsing the archive data in its repository, even for a user who knows what sheâs looking for, is laborious and time-consuming. Thus we faced a challenge. Could we enable dynamic, scalable discovery of the Data.gov Archive while enjoying the frugality, simplicity, and maintainability of static hosting?&lt;/p&gt;
    &lt;head rend="h2"&gt;Our Experiment: Rich Discovery, No Server Required&lt;/head&gt;
    &lt;p&gt;Recent advancements in client-side data analysis led us to try something new. Tools like DuckDB-Wasm, sql.js-httpvfs, and Protomaps, powered by standards such as WebAssembly, web workers, and HTTP range requests, allow users to efficiently query large remote datasets in the browser. Rather than downloading a 2 GB data file into memory, these tools can incrementally retrieve only the relevant parts of the file and process query results locally.&lt;/p&gt;
    &lt;p&gt;We developed Data.gov Archive Search on the same model. Hereâs how it works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data storage: We store Data.gov Archive catalog metadata as sorted, compressed Parquet files on Source.coop, taking advantage of performant static file hosting.&lt;/item&gt;
      &lt;item&gt;In-browser query engine: Our client-side web application loads DuckDB-Wasm, a fully functional database engine running inside the userâs browser.&lt;/item&gt;
      &lt;item&gt;On-demand data access: When a user navigates to a resource or submits a search, our DuckDB-Wasm client executes a targeted retrieval of the data needed to fulfill the request. No dedicated server is required; queries run entirely in the browser.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This experiment has not been without obstacles. Getting good performance out of this model demands careful data engineering, and the large DuckDB-Wasm binary imposes a considerable latency cost. As of this writing, weâre continuing to explore speedy alternatives like hyparquet and Arquero to further improve performance.&lt;/p&gt;
    &lt;p&gt;Still, weâre pleased with the result: an inexpensive, low-maintenance static discovery platform that allows users to browse, search, and filter Data.gov Archive records entirely in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for Libraries, Digital Humanities Projects, and Beyond&lt;/head&gt;
    &lt;p&gt;This new pattern offers a compelling model for libraries, academic archives, and DH projects of all sizes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lower operating costs: By shifting from an expensive server to lower cost static storage, projects can sustainably offer their users access to data.&lt;/item&gt;
      &lt;item&gt;Reduced technical overhead: With no dedicated backend server, security risks are reduced, no patching or upgrades are needed, and crashing servers are not a concern.&lt;/item&gt;
      &lt;item&gt;Sustained access: Projects can be set up with care, but without demanding constant attention. Organizations can be more confident that their archive and discovery interfaces remain usable and accessible, even as staffing or funding changes over time.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Knowing that we are not the only group interested in approaching access in this way, weâre sharing our generalized learnings. We see a few ways forward for others in the knowledge and information world:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Prototype or pilot: If your organization has large, relatively static datasets, consider experimenting with a browser-based search tool using static hosting.&lt;/item&gt;
      &lt;item&gt;Share and collaborate: Template applications, workflows, and lessons learned can help this new pattern gain adoption and maturity across the community.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is still evolving, and we invite othersâparticularly those in libraries and digital cultural heritageâto explore these possibilities with us. Weâre committed to open sharing as we refine our tools, and we welcome collaboration or feedback at lil@law.harvard.edu.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lil.law.harvard.edu/blog/2025/10/24/rethinking-data-discovery-for-libraries-and-digital-humanities/"/><published>2025-10-31T17:37:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45774640</id><title>Addiction Markets</title><updated>2025-11-01T16:10:08.250456+00:00</updated><content/><link href="https://www.thebignewsletter.com/p/addiction-markets-abolish-corporate"/><published>2025-10-31T17:42:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45776269</id><title>A theoretical way to circumvent Android developer verification</title><updated>2025-11-01T16:10:08.181907+00:00</updated><content>&lt;doc fingerprint="af20594190b6d770"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A theoretical way to circumvent Android developer verification&lt;/head&gt;
    &lt;p&gt;As you all know, Google has introduced developer verification as a way to prevent users from installing “unregistered” APKs. This measure was taken as a security feature to link every APK in existence to its developer, as in Play Store.&lt;/p&gt;
    &lt;p&gt;Link to the Android documentation, link to FAQ&lt;/p&gt;
    &lt;head rend="h2"&gt;Why this is bad&lt;/head&gt;
    &lt;p&gt;This has already been discussed by ArsTechnica and on some threads (some cherry-picked ones): reddit, ycombinator, hackaday.&lt;/p&gt;
    &lt;p&gt;A quick recap of the main points (as of 30 Oct 2025):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The base tier costs $25, as in Play Market. Requires an ID&lt;/item&gt;
      &lt;item&gt;There will be a limited “hobbyist” unpaid license. Google claims that they won’t require an ID&lt;/item&gt;
      &lt;item&gt;Legal info is told to be private, unlike with Play Market&lt;/item&gt;
      &lt;item&gt;The verification code is supposed to be located in Play Services, but Google hasn’t published the source code yet&lt;/item&gt;
      &lt;item&gt;Google assures that it would be possible to install applications locally using ADB, but there are no details on this&lt;/item&gt;
      &lt;item&gt;Hobbyist license restrictions are unknown&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A few months prior Google has decided to make Android development private, which seems to be a preparation for the upcoming changes (another article). Due to this change in AOSP release format, it is no longer possible to track what exactly Google is doing.&lt;/p&gt;
    &lt;p&gt;My answer to this question is that it would simply prevent small developers from distributing their apps, including myself. If we take the legal route, a hobbyist license is supposed to have some limit on the number of installs by design. If we take, say, 10K installs, this is not enough in my case. Another question is how exactly the process of verification is going to happen, what if Google adopts the same rules as in Play Store? Taking my fork of the old VN engine port, this apk would not pass security checks, as the old codebase relies on legacy external storage permissions, which are banned in Play Store. If we take the adb route, there are no guarantees that this method is going to work in the future in the form you expect. For instance, Google mentions that this method is meant for on-device tests during development, and nothing prevents them from reporting the install to their servers and checking if a self-signed apk has been installed on other devices. Another way to put it, this is problematic for an average Android user to perform these steps, and this is going to be the developer’s problem.&lt;/p&gt;
    &lt;p&gt;The situation links pretty well with Samsung removing bootloader unlocking with the One UI 8 update. Great, duh…&lt;/p&gt;
    &lt;head rend="h2"&gt;The concept&lt;/head&gt;
    &lt;p&gt;My vision of the hack is to distribute a verified loader apk, which in turn dynamically loads any apk the user wants. A user obtains the loader apk once and loads apps without installing as much as they want.&lt;/p&gt;
    &lt;p&gt;The Java virtual machine in Android is the ART/Dalvik runtime (I will refer to it as Dalvik, it seems that Google hates cool names). Did you know that Dalvik natively allows dynamic code execution using PathClassLoader? So an apk may just load some zip/apk/dex code from external storage and execute it in current context. Essentially, this means that we can natively load the apk into memory and execute any code inside of the target apk, and we are not altering the original code signature of the loader.&lt;/p&gt;
    &lt;p&gt;In order to actually run the apk, the loader needs to properly initialize the main activity (aka the main screen, or the entrypoint) of the target apk. So, the main activity needs to be initialized and somehow placed inside of the Android’s activity cycle with the loader acting as a wrapper. Then, the loader apk should handle other aspects like local files handling and names conflict resolution. This can be achieved by patching the target apk bytecode: .odex/.dex classes may be dynamically decompiled into .smali, analyzed and compiled back into a modified apk. Furthermore, the loader would have to parse AndroidManifest options of the target (main activity location, screen options).&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation&lt;/head&gt;
    &lt;p&gt;Developing such wrapper in a straightforward way has proven to be rather difficult, as Android activity management logic is extremely complicated and differs from version to version. In short, it was problematic to perform the initialization process the right way. Some people suggested to avoid the initialization step completely, and use Unsafe Dalvik api to register the target’s activity as the loader apk activity stub, which is declared in the loader’s manifest without class. I couldn’t find exact methods in the Unsafe documentation, but this actually may be a way to go.&lt;/p&gt;
    &lt;p&gt;Due to this particular issue I couldn’t bring the proof of concept to a working state in a reasonable time, and because of this I was considering to not publish this article at all. The purpose of this post is not to give a somewhat ready solution, but get some feedback on the concept, as I was not ready to devote lots of time on a potentially broken solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;The logistics&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Information provided in this section is for educational use only, all scenarios discussed below are hypothetical.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In order to install the loader apk on the device, it would require, well, some form of verification. Hobbyist license is the only choice here, as paying $25 for each attempt is not optimal. Since the hobbyist license has a limited number of installs, there should be multiple instances of the apk with separate licences. In this hypothetical scenario there may either be a pool of volunteers who sign the code, or completely random users who are willing to help. In the second case, the loader code would somehow need to be verified or scanned, since such distribution system would be vulnerable to malware.&lt;/p&gt;
    &lt;p&gt;The final and the most important issue in this process is the verification process itself, as the loader code may (and likely will) be flagged by Google. So, the code would require some form of obfuscation like code flow modification and implementing double functionality (for instance, registering it as a file manager). If Google decides to ban dynamic code loading altogether, the final solution would be to pack the Dalvik runtime into the loader as a native library. This of course would have extremely low performance, but it should be technically possible.&lt;/p&gt;
    &lt;p&gt;Overall, the hypothetical plan has lots of assumptions, with which I’m not happy with. First of all, it requires lots of manual work by the volunteers or random people, and this work also includes the apk obfuscation, which was not discussed in detail. Then, the verification process itself should be somewhat permissive to allow potentially suspicious apps (I would like to hear how does this happen with current Play Store verification).&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The project described in this article by no means is a finished solution, and if you have started to think what else could work, it means that the article has reached its original goal. I believe that we would eventually come up with a proper solution in the future. Thank you for reading!&lt;/p&gt;
    &lt;p&gt;You may find the source code here. Feel free to create an issue if you wish to discuss&lt;/p&gt;
    &lt;head rend="h1"&gt;Update 1&lt;/head&gt;
    &lt;p&gt;Linking the ycombinator thread here.&lt;/p&gt;
    &lt;p&gt;The most common reaction to this post was “why bother, there exists adb and Shinzuku”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why we should bother&lt;/head&gt;
    &lt;p&gt;We can only hope that Google allows to install distributed APKs over adb in the future, as they explicitly stated that they will allow this for developers to test their own APKs:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As a developer, you are free to install apps without verification with ADB. This is designed to support developers’ need to develop, test apps that are not intended or not yet ready to distribute to the wider consumer population&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It is possible for them to do the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Limit the number of installs of an unverified APK, limit it to a single device or go the Apple way (uninstall app after a time limit). This technically won’t prevent developers from testing their apps&lt;/item&gt;
      &lt;item&gt;Make it harder to unlock the developer options by requiring some kind of verification that you are a developer. This seems to be less likely, since it’s quite tricky to implement.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There is no reason to dismiss this scenario, as it aligns with Google’s recent actions against power users like making AOSP development private, not publishing Pixel device trees and irrationally trying to ban ad blockers on Youtube (though the last point is partially related to Android through Vanced). If the subset of users who utilize adb is going to be large enough (and it will be), what exactly is stopping them from doing something about it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Efficiency&lt;/head&gt;
    &lt;p&gt;Another opinion was that it is “not a good idea to try and find a technical solution to a people/organisation problem”. Furthermore, if such solution is implemented, it is going to require too much effort to maintain, as each upload is going to be almost immediately banned by Google. I absolutely agree with both of these takes, since the apk loader developers are likely going to give up sooner or later.&lt;/p&gt;
    &lt;p&gt;In this post I’ve made a critical mistake of not putting the emphasis on the removal of bootloader unlock, since in fact it’s indeed awful, as it makes people stuck with Google’s restrictions in the first place. While the first move was done by Samsung, it’s still very alarming as a precedent. Right now the situation with AOSP-based ROMs is not great, as you have to own a specific device like a Pixel or OnePlus model, and Google keeps messing with AOSP. The true hope here are root-based solutions, as they work on stock Android - I hope that they are not going to go anywhere in the future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://enaix.github.io/2025/10/30/developer-verification.html"/><published>2025-10-31T20:20:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45777682</id><title>S.A.R.C.A.S.M: Slightly Annoying Rubik's Cube Automatic Solving Machine</title><updated>2025-11-01T16:10:07.692978+00:00</updated><content>&lt;doc fingerprint="46cfb39d4df2f5f0"&gt;
  &lt;main&gt;
    &lt;p&gt;Slightly Annoying Rubik's Cube Automatic Solving Machine&lt;/p&gt;
    &lt;p&gt;S.A.R.C.A.S.M is a 3D-printed, Teensy-powered robot that scans, solves, and sassily comments on a Rubik’s Cube.&lt;/p&gt;
    &lt;p&gt;This repository contains the code and schematics of the build.&lt;/p&gt;
    &lt;p&gt;Additional details may be found on the Teensy forum thread:&lt;/p&gt;
    &lt;p&gt;https://forum.pjrc.com/index.php?threads/sarcasm-an-over-engineered-rubiks-cube-solving-robot.77338/&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Short clip: https://youtube.com/shorts/Xer4mPZZH8E&lt;/item&gt;
      &lt;item&gt;Full demo: https://youtu.be/WV52RtuWXk0&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Teensy 4.1 main controller + ESP32-CAM for image capture.&lt;/item&gt;
      &lt;item&gt;ILI9341 display with custom 2D&amp;amp;3D graphics, animations, and lip-sync.&lt;/item&gt;
      &lt;item&gt;Stepper + servos for cube handling with position sensors to detect faults.&lt;/item&gt;
      &lt;item&gt;RGBW lighting synchronized to audio.&lt;/item&gt;
      &lt;item&gt;On-device TTS (espeak-ng) with a collection of sarcastic lines.&lt;/item&gt;
      &lt;item&gt;...&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A slight modification of Teensy's core is required in order for the whole code to fit in RAM.&lt;/p&gt;
    &lt;p&gt;Edit &lt;code&gt;cores/teensy4/usb_serial.c&lt;/code&gt; and &lt;code&gt;cores/teensy4/usb_serial2.c&lt;/code&gt; and, in each file, remove the DMAMEM attribute in front of the definition of the &lt;code&gt;txbuffer[]&lt;/code&gt; and &lt;code&gt;rx_buffer[]&lt;/code&gt; arrays.&lt;/p&gt;
    &lt;p&gt;This is a work in progress. The repo is currently in a very messy and incomplete state (and will most likely stay that way until I find some time to work on it...). Sorry !&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/vindar/SARCASM"/><published>2025-10-31T23:03:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45777810</id><title>Show HN: Strange Attractors</title><updated>2025-11-01T16:10:07.350130+00:00</updated><content>&lt;doc fingerprint="a3ac5b1ecf85872a"&gt;
  &lt;main&gt;
    &lt;p&gt;A few months back, while playing around with Three.js, I came across something that completely derailed my plans. Strange attractors - fancy math that creates beautiful patterns. At first I thought I'd just render one and move on, but then soon I realized that this is too much fun. When complexity emerges from three simple equations, when you see something chaotic emerge into beautiful, it's hard not to waste some time. I've spent countless hours, maybe more than I'd care to admit, watching these patterns form. I realized there's something deeply satisfying about seeing order emerge from randomness. Let me show you what kept me hooked.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Basics: Dynamical Systems and Chaos Theory&lt;/head&gt;
    &lt;p&gt;Dynamical Systems are a mathematical way to understand how things change over time. Imagine you have a system, which could be anything from the movement of planets to the growth of a population. In this system, there are rules that determine how it evolves from one moment to the next. These rules tell you what will happen next based on what is happening now. Some examples are, a pendulum, the weather patterns, a flock of birds, the spread of a virus in a population (we are all too familiar with this one), and stock market.&lt;/p&gt;
    &lt;p&gt;There are two primary things to understand about this system:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Phase Space: This is like a big collection of all the possible states the system can be in. Each state is like a snapshot of the system at a specific time. This is also called the state space or the world state.&lt;/item&gt;
      &lt;item&gt;Dynamics: These are the rules that takes one state of the system and moves it to the next state. It can be represented as a function that transforms the system from now to later.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For instance, when studying population growth, a phase-space (world-state) might consist of the current population size and the rate of growth or decline at a specific time. The dynamics would then be derived from models of population dynamics, which, considering factors like birth rates, death rates, and carrying capacity of the environment, dictate the changes in population size over time.&lt;/p&gt;
    &lt;p&gt;Another way of saying this is that the dynamical systems describe how things change over time, in a space of possibilities, governed by a set of rules. Numerous fields such as biology, physics, economics, and applied mathematics, study systems like these, focusing on the specific rules that dictate their evolution. These rules are grounded in relevant theories, such as Newtonian mechanics, fluid dynamics, and mathematics of economics, among others.&lt;/p&gt;
    &lt;head rend="h3"&gt;Chaos Theory&lt;/head&gt;
    &lt;p&gt;There are different ways of classifying dynamical systems, and one of the most interesting is the classification into chaotic and non-chaotic systems. The change over time in non-chaotic systems is more deterministic as compared to chaotic systems which exhibit randomness and unpredictability.&lt;/p&gt;
    &lt;p&gt;Chaos Theory is the sub branch of dynamical systems that studies chaotic systems and challenges the traditional deterministic views of causality. Most of the natural systems we observe are chaotic in nature, like the weather, a drop of ink dissolving in water, social and economic behaviours etc. In contrast, systems like the movement of planets, pendulums, and simple harmonic oscillators are extremely predictable and non-chaotic.&lt;/p&gt;
    &lt;p&gt;Chaos Theory deals with systems that exhibit irregular and unpredictable behavior over time, even though they follow deterministic rules. Having a set of rules that govern the system, and yet exhibit randomness and unpredictability, might seem a bit contradictory, but it is because the rules do not always represent the whole system. In fact, most of the time, these rules are an approximation of the system and that is what leads to the unpredictability. In complex systems, we do not have enough information to come up with a perfect set of rules. And by using incomplete information to make predictions, we introduce uncertainty, which amplifies over time, leading to the chaotic behaviour.&lt;/p&gt;
    &lt;p&gt;Chaotic systems generally have many non-linear interacting components, which we partially understand (or can partially observe) and which are very sensitive to small changes. A small change in the initial conditions can lead to a completely different outcome, a phenomenon known as the butterfly effect. In this post, we will try to see the butterfly effect in action but before that, let's talk about Strange Attractors.&lt;/p&gt;
    &lt;head rend="h2"&gt;Strange Attractors&lt;/head&gt;
    &lt;p&gt;To understand Strange Attractors, let's first understand what an attractor is. As discussed earlier, dynamical systems are all about change over time. During this change, the system moves through different possible states (remember the phase space jargon?). An attractor is a set of states towards which a system tends to settle over time, or you can say, towards which it is attracted. It's like a magnet that pulls the system towards it.&lt;/p&gt;
    &lt;p&gt;For example, think of a pendulum. When you release it, it swings back and forth, but eventually, it comes to rest at the bottom. The bottom is the attractor in this case. It's the state towards which the pendulum is attracted.&lt;/p&gt;
    &lt;p&gt;This happens due to the system's inherent dynamics, which govern how states in the phase space change. Here are some of the reasons why different states get attracted towards attractors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stability: Attractors are stable states of the system, meaning that once the system reaches them, it tends to stay there. This stability arises from the system's dynamics, which push it towards the attractor and keep it there.&lt;/item&gt;
      &lt;item&gt;Dissipation: Many dynamical systems have dissipative forces, which cause the system to lose energy over time. This loss of energy leads the system to settle into a lower-energy state, which often corresponds to an attractor. This is what happens in the case of the pendulum.&lt;/item&gt;
      &lt;item&gt;Contraction: In some regions of the phase space, the system's dynamics cause trajectories to converge. This contraction effect means that nearby states will tend to come closer together over time, eventually being drawn towards the attractor.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some attractors have complex governing equations that can create unpredictable trajectories or behaviours. These nonlinear interactions can result in multiple stable states or periodic orbits, towards which the system evolves. These complex attractors are categorised as strange attractors. They are called "strange" due to their unique characteristics.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fractal Structure: Strange attractors often have a fractal-like structure, meaning they display intricate patterns that repeat at different scales. This complexity sets them apart from simpler, regular attractors.&lt;/item&gt;
      &lt;item&gt;Sensitive Dependence on Initial Conditions: Systems with strange attractors are highly sensitive to their initial conditions. Small changes in the starting point can lead to vastly different long-term behaviors, a phenomenon known as the "butterfly effect".&lt;/item&gt;
      &lt;item&gt;Unpredictable Trajectories: The trajectories on a strange attractor never repeat themselves, exhibiting non-periodic motion. The system's behavior appears random and unpredictable, even though it is governed by deterministic rules.&lt;/item&gt;
      &lt;item&gt;Emergent Order from Chaos: Despite their chaotic nature, strange attractors exhibit a form of underlying order. Patterns and structures emerge from the seemingly random behavior, revealing the complex dynamics at play.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can observe most of these characteristics in the visualisation. The one which is most fascinating to observe is the butterfly effect.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Butterfly Effect&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;A butterfly can flutter its wings over a flower in China and cause a hurricane in the Caribbean.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;One of the defining features of strange attractors is their sensitivity to initial conditions. This means that small changes in the starting state of the system can lead to vastly different long-term behaviors, a phenomenon known as the butterfly effect. In chaotic systems, tiny variations in the initial conditions can amplify over time, leading to drastically different outcomes.&lt;/p&gt;
    &lt;p&gt;In our visualisation, let's observe this behavior on Thomas Attractor. It is governed by the following equations:&lt;/p&gt;
    &lt;head rend="h3"&gt;Thomas Attractor Equation&lt;/head&gt;
    &lt;quote&gt;1float a = 0.19;23dx = (-a*x + sin(y)) * dt;4dy = (-a*y + sin(z)) * dt;5dz = (-a*z + sin(x)) * dt;&lt;/quote&gt;
    &lt;quote&gt;1float a = 0.19;23dx = (-a*x + sin(y)) * dt;4dy = (-a*y + sin(z)) * dt;5dz = (-a*z + sin(x)) * dt;&lt;/quote&gt;
    &lt;p&gt;A small change in the parameter &lt;code&gt;a&lt;/code&gt; can lead to vastly different particle trajectories and the overall shape of the
attractor. Change this value in the control panel and observe the butterfly effect in action.&lt;/p&gt;
    &lt;p&gt;There is another way of observing the butterfly effect in this visualisation. Change the &lt;code&gt;Initial State&lt;/code&gt; from &lt;code&gt;cube&lt;/code&gt; to
&lt;code&gt;sphere surface&lt;/code&gt; in the control panel and observe how the particles move differently in the two cases. The particles
eventually get attracted to the same states but have different trajectories.&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation Details&lt;/head&gt;
    &lt;p&gt;This visualization required rendering a large number of particles using Three.js. To achieve this efficiently, we used a technique called ping-pong rendering 2. This method handles iterative updates of particle systems directly on the GPU, minimizing data transfers between the CPU and GPU. It utilizes two frame buffer objects (FBOs) that alternate roles: One stores the current state of particles and render them on the screen, while the other calculates the next state.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation Workflow&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Setting Up Frame Buffer Objects (FBOs): We start by creating two FBOs,&lt;/p&gt;&lt;code&gt;ping&lt;/code&gt;and&lt;code&gt;pong&lt;/code&gt;, to hold the current and next state of particles. These buffers store data such as particle positions in RGBA channels, making efficient use of GPU resources.typescript1const ping = new THREE.WebGLRenderTarget(size, size, {2minFilter: THREE.NearestFilter,3magFilter: THREE.NearestFilter,4format: THREE.RGBAFormat,5stencilBuffer: false,6type: THREE.FloatType,7});89const pong = new THREE.WebGLRenderTarget(size, size, {10minFilter: THREE.NearestFilter,11magFilter: THREE.NearestFilter,12format: THREE.RGBAFormat,13stencilBuffer: false,14type: THREE.FloatType,15});1const ping = new THREE.WebGLRenderTarget(size, size, {2minFilter: THREE.NearestFilter,3magFilter: THREE.NearestFilter,4format: THREE.RGBAFormat,5stencilBuffer: false,6type: THREE.FloatType,7});89const pong = new THREE.WebGLRenderTarget(size, size, {10minFilter: THREE.NearestFilter,11magFilter: THREE.NearestFilter,12format: THREE.RGBAFormat,13stencilBuffer: false,14type: THREE.FloatType,15});&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Shader Programs for Particle Dynamics: The shader programs execute on the GPU and apply attractor dynamics to each particle. Following is the attractor function which update the particle positions based on the attractor equation.&lt;/p&gt;glsl1vec3 attractor(vec3 pos) {2float a = 0.16;3float x = pos.x, y = pos.y, z = pos.z;4float dt = 0.015;56float dx, dy, dz;7dx = (-a*x + sin(y)) * dt;8dy = (-a*y + sin(z)) * dt;9dz = (-a*z + sin(x)) * dt;10return vec3(dx, dy, dz);11}1vec3 attractor(vec3 pos) {2float a = 0.16;3float x = pos.x, y = pos.y, z = pos.z;4float dt = 0.015;56float dx, dy, dz;7dx = (-a*x + sin(y)) * dt;8dy = (-a*y + sin(z)) * dt;9dz = (-a*z + sin(x)) * dt;10return vec3(dx, dy, dz);11}&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Rendering and Buffer Swapping: In each frame, the shader computes the new positions based on the attractor's equations and stores them in the inactive buffer. After updating, the roles of the FBOs are swapped: The previously inactive buffer becomes active, and vice versa.&lt;/p&gt;typescript1const currentTarget = flip ? ping : pong;2const nextTarget = flip ? pong : ping;34// Use current positions for calculations in shader5uniforms.positions.value = currentTarget.texture;67// Render the other on the screen8gl.setRenderTarget(nextTarget);9gl.clear();10gl.render(scene, camera);11gl.setRenderTarget(null);1213flip = !flip;1const currentTarget = flip ? ping : pong;2const nextTarget = flip ? pong : ping;34// Use current positions for calculations in shader5uniforms.positions.value = currentTarget.texture;67// Render the other on the screen8gl.setRenderTarget(nextTarget);9gl.clear();10gl.render(scene, camera);11gl.setRenderTarget(null);1213flip = !flip;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This combination of efficient shader calculations and the ping-pong technique allows us to render the particle system.&lt;/p&gt;
    &lt;p&gt;If you have any comments, please leave them on this GitHub discussions topic. Sooner or later, I will integrate it with the blog. The hacker news discussion can be found here.&lt;/p&gt;
    &lt;head rend="h4"&gt;Footnotes&lt;/head&gt;
    &lt;head rend="h4"&gt;References&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inspired by the work of Maxim&lt;/item&gt;
      &lt;item&gt;Wikipedia: Attractor&lt;/item&gt;
      &lt;item&gt;Wikipedia: List Of Chaotic Maps&lt;/item&gt;
      &lt;item&gt;Dynamical Systems Theory: What in the World is it?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Related Links&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://fusefactory.github.io/openfuse/strange%20attractors/particle%20system/Strange-Attractors-GPU/&lt;/item&gt;
      &lt;item&gt;https://chaoticatmospheres.com/mathrules-strange-attractors&lt;/item&gt;
      &lt;item&gt;https://www.dynamicmath.xyz/strange-attractors/&lt;/item&gt;
      &lt;item&gt;https://www.reddit.com/r/math/comments/z0dmms/visualization_of_3d_strange_attractors&lt;/item&gt;
      &lt;item&gt;https://www.clicktorelease.com/code/codevember-2016/3&lt;/item&gt;
      &lt;item&gt;https://discourse.mcneel.com/t/strange-attractors/120053&lt;/item&gt;
      &lt;item&gt;https://www.reddit.com/r/generative/comments/191fkkv/genuary_day_8_chaotic_system/&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.shashanktomar.com/posts/strange-attractors"/><published>2025-10-31T23:23:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45779860</id><title>Hard Rust requirements from May onward</title><updated>2025-11-01T16:10:06.821504+00:00</updated><content>&lt;doc fingerprint="87995b15cc375c90"&gt;
  &lt;main&gt;
    &lt;quote&gt;Hi all, I plan to introduce hard Rust dependencies and Rust code into APT, no earlier than May 2026. This extends at first to the Rust compiler and standard library, and the Sequoia ecosystem. In particular, our code to parse .deb, .ar, .tar, and the HTTP signature verification code would strongly benefit from memory safe languages and a stronger approach to unit testing. If you maintain a port without a working Rust toolchain, please ensure it has one within the next 6 months, or sunset the port. It's important for the project as whole to be able to move forward and rely on modern tools and technologies and not be held back by trying to shoehorn modern software on retro computing devices. Thank you for your understanding. -- debian developer - deb.li/jak | jak-linux.org - free software dev ubuntu core developer i speak de, en&lt;/quote&gt;
    &lt;p&gt;Attachment: signature.asc&lt;lb/&gt; Description: PGP signature&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lists.debian.org/debian-devel/2025/10/msg00285.html"/><published>2025-11-01T07:31:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45780228</id><title>You can't refuse to be scanned by ICE's facial recognition app, DHS document say</title><updated>2025-11-01T16:10:06.763436+00:00</updated><content>&lt;doc fingerprint="d4998b5510e20338"&gt;
  &lt;main&gt;
    &lt;p&gt;Immigration and Customs Enforcement (ICE) does not let people decline to be scanned by its new facial recognition app, which the agency uses to verify a person’s identity and their immigration status, according to an internal Department of Homeland Security (DHS) document obtained by 404 Media. The document also says any face photos taken by the app, called Mobile Fortify, will be stored for 15 years, including those of U.S. citizens.&lt;/p&gt;
    &lt;p&gt;The document provides new details about the technology behind Mobile Fortify, how the data it collects is processed and stored, and DHS’s rationale for using it. On Wednesday 404 Media reported that both ICE and Customs and Border Protection (CBP) are scanning peoples’ faces in the streets to verify citizenship.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.404media.co/you-cant-refuse-to-be-scanned-by-ices-facial-recognition-app-dhs-document-says/"/><published>2025-11-01T08:58:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45780490</id><title>.arpa, rDNS and a few magical ICMP hacks</title><updated>2025-11-01T16:10:05.894355+00:00</updated><content>&lt;doc fingerprint="a60861d098323395"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;.arpa, rDNS and a few magical ICMP hacks&lt;/head&gt;&lt;p&gt;Through Project SERVFAIL, I became aware that there are a few individuals, not just ISPs, who host their own in-addr.arpa. and ip6.arpa. zones. It never occurred to me that I could ask bgp.wtf, my beloved ISP, to delegate me a zone like this - until one faithful late-night chat.&lt;lb/&gt;ARPA zones are usually totally out of reach for individuals, so I was absolutely hyped when one of our netadmins agreed to delegate me the ip6.arpa. zone for my whole /48 IPv6 range. Thank you so much &amp;lt;3!!&lt;/p&gt;&lt;head rend="h3"&gt;Haruhi says...&lt;/head&gt;"This post includes an incredible amount of hacks, questions the RFCs, and generally has fun in very abstract, weird ways.&lt;p&gt;But that's what you came here for, right?"&lt;/p&gt;&lt;head rend="h3"&gt;What even is .arpa?&lt;/head&gt;&lt;p&gt;The story begins in the late 1960s, a long time before the Internet itself. Several experimental networks existed back then, ARPANET being one of the most prominent ones.&lt;/p&gt;Picture 1 - ARPANET logical map circa 1977. Mirrored from Wikimedia Commons, Public Domain&lt;p&gt;In its original form, ARPANET connected four universities in the US; By mid 1970s, that expanded to every major uni in the US, plus a few participants from abroad, several connected by satellite links. ARPANET is relevant to the history of the internet because a lot of the core protocols in use today have originated within it - most notably IP (RFC760, RFC791), ICMP (RFC777) and the concept of Name Servers (RFC883 et al). ARPANET also pioneered dynamic routing, a technology without which Internet as we know it couldn't exist.&lt;/p&gt;&lt;p&gt;Looking at those old RFCs with the power of hindsight is quite an experience: Some concepts stuck, others were complete misses that look hilariously wrong today. ARPANET was a period of rapid evolution - there were virtually no legacy systems, and thanks to the academic deployment base, backwards compatibility wasn't a major concern. Protocols changed rapidly, things were tried, applied and ratified. This is a stark contrast to how the commercial internet ended up being, with many things set in stone purely out of fear of breaking compat.&lt;/p&gt;&lt;p&gt;That state didn't last for long, as ARPANET was already scheduled for closure in 1985. It was finally shut down in 1990, being superseded by NSFNET and the slowly forming commercial Internet3.&lt;/p&gt;&lt;p&gt;The .arpa zone has a somewhat convoluted history; In RFC920 "Domain Requirements", it's defined as one of the first non-country domains, and the only one in the "Temporary" category. At that point, all old ARPANET domains got moved under .arpa as a "legacy" stopgap until everyone reconfigured their servers. However, there also was one new subdomain used for a contact mail address (see the same RFC). Wikipedia cites that mailserver as the reason why .arpa didn't get decommissioned; That statement, along with early .arpa history in general, is hard to verify. IANA was actively pushing for deprecating the zone, instead placing all metadata services within the .int domain (RFC1886, 1995). Later on, they seemed to revert that (RFC3152, 2001), dedicating .int for international organizations only, and .arpa to all the &amp;lt;meta&amp;gt; services, like reverse DNS.&lt;/p&gt;&lt;p&gt;Temporary solutions truly are the most permanent ones.&lt;/p&gt;&lt;head rend="h3"&gt;How does it .arpa?&lt;/head&gt;&lt;p&gt;Perhaps the biggest uses of .arpa are the in-addr.arpa. and ip6.arpa. domains, used for IPv4 and IPv6 reverse DNS, respectively. If you're not familiar with reverse DNS - it's used to map IPs to domain names (as opposed to "forwards" DNS, which maps domains to IPs). Given an IP address, you can look up its corresponding PTR record, which - if it exists - should return you a domain name.&lt;/p&gt;Picture 2 - pinging sdomi.pl, you'll get replies from an IP with rDNS sakamoto.pl.&lt;p&gt;Specifically, rDNS works by querying the PTR record for a specific subdomain (in this case, 103.240.236.185.in-addr.arpa.), which returns a string, usually matching the "main" domain used on that IP. However, a lot of software is quite lentient at accepting weird responses - a thought we'll come back to later :3&lt;/p&gt;&lt;p&gt;Having never seen a domain like the one above, it may be a bit hard to grasp what's the meaning of all the numbers; The following things are important to know:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Reverse DNS uses the same underlying protocols and systems as forward DNS&lt;/item&gt;&lt;item&gt;DNS is a hierarchical system. This means that it can be divided into smaller and smaller zones, with different entities administering each one&lt;/item&gt;&lt;item&gt;IP is also a hierarchy: someone owns an address range, leases it to an ISP who then divides the range into multiple smaller ranges for their users. Each step ends up with a smaller chunk.&lt;/item&gt;&lt;item&gt;DNS hierarchy goes left, with subdomains. IP hierarchy goes right, with subnets.&lt;/item&gt;&lt;item&gt;To make them fit together, something had to be reversed, so they both go in one direction. Hence, the last part of the IP from the right will become the first subdomain from the left.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The same rules apply to IPv6, except that for technical reasons each subdomain is just one character.&lt;/p&gt;&lt;p&gt;Of note, while you can use domains and raw IPs interchangeably with the vast majority of tools, an application needs to be explicitly made revDNS-aware to use it. A lot of tools aren't, by negligence or design. This is why I consider those zones to be both least and most used; technically all IPs have it, yet it isn't actually used that much.&lt;/p&gt;&lt;head rend="h3"&gt;But what fun can you use it for?&lt;/head&gt;&lt;p&gt;See, normally you'd only see PTR records being used within revDNS zones. There's also the odd CNAME, if someone wanted to copy another entry verbatim. However, what if I told you that there's nothing stopping you from adding other record types to it? RFCs suggest a specific use for those zones (revDNS), but they don't explicitly forbid any other uses; And even if they did, it's not like most software would care about rules like this!&lt;/p&gt;Picture 5 - oh gawd. it's not even an AAAA record!&lt;p&gt;So we can use reverse DNS for forward DNS, which then can also do reverse DNS for (proper) name resolution. Cursed, right? It's about to get much better!&lt;/p&gt;&lt;code&gt;$ curl 'http://1023.1023.0.6.0.0.8.0.0.b.e.d.0.a.2.ip6.arpa/'
i opened my nginx config for this
&lt;/code&gt;


Figure 1 - There's nothing limiting us to just pings.&lt;p&gt;This can be considered a regular domain... almost.&lt;/p&gt;&lt;p&gt;As nginx (the web server hosting this page) doesn't do any specific validations on vhosts (because why would it?), we can set the server_name to serve an arpa domain under our zone. Check this out: the post you're reading can be also accessed through http://meow.6.0.0.8.0.0.b.e.d.0.a.2.ip6.arpa/weblog?id=24 - cool, right?&lt;/p&gt;&lt;p&gt;Unfortunately, getting a TLS certificate issued has proven to be more difficult than anticipated. Let's Encrypt, which I use for all my other HTTPS needs, has a rule prohibiting domains with more than 10 parts (subdomains). Even if I got around that, they blanket-forbid all .arpa zones anyways. I've tried all other free TLS providers, and all I got was errors, and weird hangs within the flow. ZeroDNS even got to a point when they were somehow stating that I need to wait for verification to conclude, and that they succeeded, and will generate me a certificate shortly (both at the same time, on one webpage). Ultimately, nothing came of it after over a few days of waiting. I was even considering paying for a certificate, but I didn't trust that a provider wouldn't just eat my money and run away, proudly saying that they don't serve arpa domains.&lt;/p&gt;&lt;p&gt;I had one last idea, which was to leverage cloudflare's "DDoS protection" (which is in fact just a global MitM attack - but I digress). If you didn't know - when using any domain with clownflare, they take the liberty to generate TLS certificates for themselves, even if you're not "securing" your domain with their proxy. CF was on my radar, because they issue the vast majority of certs for various arpa subdomains. And I mean vast majority: it's around 100:1 cloudflare to all other issuers. See the crt.sh reports for in-addr.arpa and ip6.arpa (IA links to not accidentally DoS crt.sh ^^). As previously stated, this just means that cloudflare has generated certs for themselves, not that anyone is actually using them.&lt;/p&gt;&lt;head rend="h3"&gt;But what more fun can a TLS certificate get us?&lt;/head&gt;Picture 6 - one of my first ideas which strictly required a certificate: a fedi instance&lt;p&gt;Contrary to other TLS providers, CF just worked - at the cost of me not actually having the certificate itself. Anyhoo, I setup a GoToSocial instance, because cursed GTS setups became somewhat of a custom, at least around my parts. For the unaware: GoToSocial is one of the smaller Fediverse servers, filling a particular niche, often used for single-user instances.&lt;/p&gt;&lt;p&gt;I stood up, looked around to see if anyone would try to stop me. As expected, there was nobody else in the flat, nor anyone screaming from the ouside. I hesitated a bit, but finally clicked Post. I copied the full URL to boost it from Akkoma. It worked.&lt;/p&gt;Picture 7 - first post on the instance&lt;p&gt;Registrations are closed, plus this is not a forever instance. Please don't ask if you can get let in. I don't have enough hands to moderate and care for the instance as it already stands - sorry :/&lt;/p&gt;&lt;p&gt;Important thing to mention: while CF worked just fine for me, I've seen others struggling to reproduce my results and needing to pay CF to change the default TLS provider. My account went through either by pure luck, or because it had an old provider option grandfathered in. Just, know that it may be harder for you to pull this off, should you attempt so.&lt;/p&gt;&lt;head rend="h3"&gt;MX server&lt;/head&gt;&lt;p&gt;For the GTS server, I also needed a mail account, and hosting it on .arpa seemed fitting. Since I already had a mail server for all my other domains, this wasn't a particularly involved operation.&lt;/p&gt;&lt;p&gt;Once again, my expectations were very low: I thought it would just explode upon the first interaction with any major e-mail provider, as is common.&lt;/p&gt;Picture 8 - thanks to ari for the gmail screenshot&lt;p&gt;But the e-mails, of course, arrived anyways - because mail also doesn't care what humans think is impossible. Some of it lands in spam (as expected, tbh), but it generally arrives just fine - a welcome surprise, for once.&lt;/p&gt;&lt;head rend="h3"&gt;illegal ICMP hacks&lt;/head&gt;&lt;p&gt;As a network nerd, the place where I see revDNS used the most is perhaps ICMP in various flavors: ping, traceroute, the like. Some nicer tools resolve the revDNS to make the output more readable. Is this something?&lt;/p&gt;&lt;p&gt;Consider this: ICMP Echo requests (colloquially known as "pings") contain a few unexpected fields, most notably the Sequence number (icmp_seq) which increases with every consequent ping, and an Identifier, a random per-session 16-bit value. Some time ago I wondered whether one could use those two to send different mock responses, simulating a fake traceroute. Identifier could match a specific user running a traceroute, while the sequence number could be used as to customize the response based on how much time has elapsed since the start.&lt;/p&gt;&lt;p&gt;Unfortunately, I arrived at the lack of tools:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;ICMP is largely stateless. There isn't a concept of "port" and "listening", your kernel just gets a packet, and responds to it with another packet&lt;/item&gt;&lt;item&gt;If I wanted to reply with different data to multiple clients at once, I couldn't just use a funky static IP route, as that introduces state.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;To make this stateless, I'd need to write an entire IP stack2, which is a lot of effort for a shitpost.&lt;/p&gt;So let's do it.&lt;head rend="h3"&gt;Writing an IPv6 stack&lt;/head&gt;&lt;p&gt;Fortunately, our IP stack will be relatively simple: we only need ICMP(v6), no TCP or UDP in sight. Unfortunately, this means I can't name it Trumpet Bashsock (see Trumpet Winsock, but this is a sacrifice I'm willing to live with (...FOR NOW).&lt;/p&gt;&lt;p&gt;First roadblock: Picking a way to interface with the network. I thought of TUN/TAP virtual interfaces; Sadly, those require the use of ioctls, which are simply impossible to send out with pure Bash (and, for now, I drew the line at adding builtins).&lt;/p&gt;&lt;p&gt;Mentioning my problem in a conversation with Wanda, she suggested I use SLIP, and I loved the idea. Initially, I couldn't get slattach(8) to bind to any ptys I made with mknod(1). Finally, I found out that socat(1) can make me a proper PTY, then connect it to a program's stdio on the other end. Poking around, figuring out the "protocol" (or lack thereof, it's just raw IPv4 packets) and scribbling out a minimum implementation, I got hit with a brick wall: apparently there's no way to transmit IPv6 over SLIP, even though the first nibble in each packet is the IP version. It seems like an oversight and I don't buy that it can't be done. However, I accepted defeat rather quickly, as I noticed something much, much more interesting while reading through socat -h.&lt;/p&gt;&lt;head rend="h3"&gt;spooky scary socat TUN&lt;/head&gt;&lt;p&gt;Remember when I was complaining about ioctls? Apparently you can just use socat to make up tun interfaces!? All sources I've seen describe the suggested serving as L3 bridging over TCP/UDP, some even going as far as stringing SSH into the mix. Regardless, the beauty of socat is that it doesn't guard me from my own terrible ideas:&lt;/p&gt;&lt;code&gt;doas socat -d -d EXEC:./ip.sh TUN:127.21.37.1/32,up&lt;/code&gt;

Figure 2 - because why not. why shouldn't I make my stdio into a TUN?

&lt;p&gt;Through some manual experimentation around the interface, I arrived at the following facts:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;even if we never intend to use IPv4, socat requires some dummy address. Sadly, IPv6 routes have to be added manually.&lt;/item&gt;&lt;item&gt;each frame starts with 2 nullbytes, folowed by another 2 bytes for the protocol (standard EtherType: 0x0800 for IPv4, 0x86dd for IPv6)&lt;/item&gt;&lt;item&gt;a standard IP packet is transmitted afterwards&lt;/item&gt;&lt;item&gt;there appears to be no way to discern bounds between packets, so you have to look for a length field inside, and only read so many bytes at once&lt;/item&gt;&lt;item&gt;the interface needs to be primed; it only starts transferring data after you transmit something yourself&lt;/item&gt;&lt;item&gt;... to not mess up the framing, we should actually send a proper (but dummy) packet&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I already had some experience with IPv6 from my networking adventures with SerenityOS; I decided to skip IPv4 altogether for simplicity, as what I planned didn't exactly need it, and it's a bit more painful to interface.&lt;/p&gt;&lt;p&gt;For working on raw data, I used my messy, but slowly shaping up binary manipulation library. This got me from an idea to a basic implementation within maybe 30 minutes. My code reads the IPv6 header, populates some variables, reads the payload, and calls the handle_icmp6 function if the next_header value matches 0x3a.&lt;/p&gt;&lt;p&gt;handle_icmp6 was surprisingly simple to write. I initially expected to need to respond to Neighbor/Router Advertisements (outside of standard Echo Requests), but socat's TUN interface doesn't expose MAC addresses, so I got to skip that. My function parses some of the header data, then calls icmp6_resp, which glues together a packet, computes a checksum and actually sends it off.&lt;/p&gt;Picture 9 - checksum functions. they look kinda similar if you squint.&lt;p&gt;C++ "inspiration" on the left, Bash on the right&lt;/p&gt;&lt;p&gt;ICMP uses the "Internet Checksum" algorithm, which is a one's complement of all payload bytes, plus some extra steps. I got a bit lucky, because I've reworked exactly that algo within SerenityOS last year. My Bash rewrite matches C++ surprisingly closely.&lt;/p&gt;&lt;p&gt;After struggling a bit with some embarrasing bugs, I got my first matching checksums (and therefore, first accepted responses). Now, I could easily add more logic for whatever I wanted.&lt;/p&gt;Picture 10 - "hey it works! i'm replying to pings from bash! :D"&lt;p&gt;I added a check, returning a different IP when the Hop Limit (TTL) field is lower than 12. That results in 12 fake hops along the way; If that doesn't make any sense, I suspect you may not be familiar with how traceroute works:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;traceroute is a Hack. It's not a separate protocol, it works on top of ICMP (at least traditionally). Gravis has a wonderful blogpost about it (see link).&lt;/item&gt;&lt;item&gt;In IP networks, each packet has a Hop Limit (also named TTL, Time-To-Live) field in the header&lt;/item&gt;&lt;item&gt;Every host ("hop") along the way decreases it by 1&lt;/item&gt;&lt;item&gt;If it ever reaches 0, packet is dropped, and a response is generated with the hop's IP address as source&lt;/item&gt;&lt;item&gt;We can leverage this behavior: by checking if that value is n or lower and responding with a different source IP, traceroute will see n bogus hosts along the way.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;As seen above, the route doesn't have to make logical sense; repetitions are OK, as long as only the last hop replies with the "correct" address (traceroute uses that as a marker to end the trace).&lt;/p&gt;&lt;head rend="h4"&gt;OK, Now what?&lt;/head&gt;&lt;p&gt;A few sections ago I've firmly buried the lede as to why I would even want to have a custom per-client response. My plan? Play an animation stored within PTR records, through careful manipulation of ICMP Echo Reply packets!&lt;/p&gt;Picture 12 - same screenshot as above, but with revDNS resolution enabled&lt;p&gt;As outlined before, PTR responses can contain almost anything. Two largest limitations1 include absolutely no Unicode, and no spaces - but you can use characters which are otherwise forbidden in domains, like underscore, or an octothorp.&lt;/p&gt;&lt;p&gt;I opted for automating the conversion (instead of manually making an ASCII animation, however much fun doing a conversion akin to towel.blinkenlights.nl might have been). The pipeline has three parts:&lt;/p&gt;&lt;code&gt;ffmpeg -i input.mkv -vf scale=24x12 -r 1 'out/%06d.bmp'
# intentional bad scaling to account for non-square fonts

# dec.sh from https://git.sakamoto.pl/domi/88x31/src/branch/meow/dec.sh
for i in out/*; do
    ./dec.sh &amp;lt; $i \
        | sed -E 's/(.).{5}/\1/g;s/f/#/g;s/0/_/g;s/[12]/,/g;s/[45]/\!/g;s/[67]/~/g;s/[89]/=/g;s/[abc]/%/g;s/[de]/#/g' \
        | tac
done &amp;gt; frames
&lt;/code&gt;


Figure 3 - first part of the conversion process, from MKV to ASCII

&lt;p&gt;This snippet takes any video file, scales it down, outputs raw BMP frames @ 1fps, decodes those one by one, samples the most significant nibble (out of a 24bit RGB value), and finally outputs ASCII characters roughly corresponding to the pixel brightness. Result is a list of frames, each separated by an empty newline.&lt;/p&gt;&lt;p&gt;This intermediate representation is only really useful as a preview. The next snippets both work on a lines file which is generated by cat frames | sort | uniq &amp;gt; lines.&lt;/p&gt;&lt;code&gt;#!/bin/bash
source ~/projects/libsh/bin/bin.sh
IFS=$'\n'
lines=($(cat lines))

while read line; do
	if [[ "$line" == "$prev" ]]; then
		echo "$(u16 "$i")"
		continue
	fi
	for (( i=0; i&amp;lt;${#lines[@]}; i++ )); do
		if [[ "$line" == "${lines[i]}" ]]; then
			echo "$(u16 "$i")"
			break
		fi
	done
	prev="$line"
done &amp;lt; frames
&lt;/code&gt;


Figure 4 - second part of the conversion, dumping a sequential list of indexes

&lt;p&gt;This one iterates over frames, and outputs an index of each line into the deduplicated file. This will be used later on by the IP stack to determine the fake hop order.&lt;/p&gt;&lt;code&gt;#!/bin/bash
source ~/projects/libsh/bin/bin.sh
IFS=$'\n'
lines=($(cat lines))

for (( i=0; i&amp;lt;${#lines[@]}; i++ )); do
	echo "$(u16 "$i").0.0.0.0.0.0.0.0.b.a.c.a.f.0.0.0.6.0.0.8.0.0.b.e.d.0.a.2.ip6.arpa. 3600 IN PTR ${lines[i]}."
done | sed -E 's/(.)(.)(.)(.)/\4.\3.\2.\1/'
&lt;/code&gt;


Figure 5 - final part. outputs the animation in a BIND-compatible format

&lt;p&gt;This outputs a list which can be easily imported by BIND, or a million other compatible DNS servers (in my case, servfail-web's raw record view). And that's all there is to it!&lt;/p&gt;&lt;p&gt;...Well, okay, that's a lie. That's all there is to it if you want just video; Audio is a separate issue, but I'll handle it in a way equivalent to how we did it two posts ago: lots of TXT records. I also thought about making it into a series of PTR records, resolved with a similar traceroute, but this has proven to be infeasible, as PTRs are limited to somewhere between 60 and 99 characters by PowerDNS. In contrast, TXT records are quite flexible: Each one can store up to around 42000 characters (theoretical limit being a few short of 65535, but resolvers tend to explode before we get there); Even counting base64 overhead, that's still around 30-32K per record. Through some quick napkin math, this is about 21 seconds of audio per entry, given that we use a heavily compressed OPUS @ 12kbps. Not audiophile grade by any means, but enough for this demo :p&lt;/p&gt;&lt;head rend="h3"&gt;Suddenly: Firewalls&lt;/head&gt;&lt;p&gt;My traceroute worked quite well locally, as well as from some servers with public IPv6 addressing. But then I tested it on a residential connection, and I received no responses. What gives?&lt;/p&gt;Picture 13 - wireshark packet dump, showing some Unforeseen Behavior&lt;p&gt;I assumed that ICMPv6 packets can be returned for TTL-exceeded reasons with a regular ICMP Echo Reply, just by modifying the source address. Investigating this issue, I learnt about ICMPv6 Time Exceeded, error code 3. Turns out that my previous version only worked incidentally: routers on the way will reject the packet if they're set up to only let "related" packets go through (which is the case for the majority of end users).&lt;/p&gt;&lt;p&gt;Anyhoo, I added a branch which would send the ICMPv6 Time Exceeded message for all mock hops, yet answer with a proper Echo Reply for the last hop. It's rather ugly, but who cares! it works!&lt;/p&gt;Picture 14 - Packet losses are unfortunately beyond my control.&lt;p&gt;However, those patterns are quite interesting.&lt;/p&gt;&lt;head rend="h2"&gt;Presenting BadMTR:&lt;lb/&gt;Bad Apple over traceroute&lt;/head&gt;&lt;code&gt;(sleep 1.5; dig +short +tcp @2a0d:eb00:8006::acab TXT {0..10}.0.0.f.0.0.0.0.0.0.0.0.b.a.c.a.f.0.0.0.6.0.0.8.0.0.b.e.d.0.a.2.ip6.arpa. | sed 's/[" ]//g' | base64 -d | mpv - 2&amp;gt;/dev/null &amp;gt;/dev/null) &amp;amp; mtr --displaymode 1 -i 1 -m 30 -s 28 2a0d:eb00:8006:dead::acab
&lt;/code&gt;
Figure 6 - The command invocation. Click once to select all.


&lt;p&gt;The cool thing is that this just works with mtr, all you need is a specific switch (and a local IPv6 address, which is still surprisingly hard to come by).&lt;/p&gt;&lt;p&gt;For transparency: I ended up rewriting the Bash IPv6 stack into Raku. The bash stack is functional, but much slower; On my laptop, frames would start dropping after ~3 clients connected at once, while the Raku version was able to handle 15+ clients (likely much more), hitting barely 24% CPU load.&lt;/p&gt;Picture 15 - MikroTiks hate them: ONE SIMPLE TRICK against the DNS cache&lt;head rend="h3"&gt;Addendum #1: Please, sanitize your inputs!&lt;/head&gt;&lt;p&gt;Working on this post got me thinking - since in reality domains are only a serving suggestion for contents of a PTR, can they be used as an "unexpected" attack vector? Thinking of the numerous webpages which offer various DNS resolution tools, maybe some of those made bad design assumptions?&lt;/p&gt;&lt;code&gt;3.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.@ 3600 IN PTR &amp;lt;script&amp;gt;alert(1)&amp;lt;/script&amp;gt;sakamoto.pl.
4.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.@ 3600 IN PTR $(sleep${IFS}15).
&lt;/code&gt;


Figure 7 - Those may look fake, but it's completely real:&lt;p&gt;XSS payload, and a timing shell injection payload&lt;/p&gt;&lt;p&gt;I went through the first 3 pages of duckduckgo and google results for "online reverse DNS lookup". In the sea of SEO garbage, I found roughly 10 webpages offering actual lookups; Four of them have turned out vulnerable, but only to the XSS payload. Thankfully, no page actually executed the shell injection (nor variants of it).&lt;/p&gt;&lt;p&gt;Interestingly, almost all of those pages can be divided into two categories:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;displays the record verbatim, is vulnerable to this attack or a variant&lt;/item&gt;&lt;item&gt;refuses to display a record if it isn't a valid domain, isn't vulnerable&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The odd-one-out was the googleapps dig page, which mangled the record, removing the &amp;lt;script&amp;gt; tag - from both sides, even. Turns out that they're doing sanitizations client-side, using a slightly inapropriate library for the job. It even lets through some basic tags like &amp;lt;b&amp;gt; and &amp;lt;br&amp;gt;. It is unknown to me why they didn't just use innerText instead of innerHTML. TXT record lookups worked just the same, so I doubt I'm the first to look at this. Anyhoo, beyond visual oddities, NOTABUG.&lt;/p&gt;&lt;p&gt;All the actual vulnerabilities that I discovered have been reported, and this post has been delayed to give them time to patch. To my best knowledge, all of the pages are patched by now. Also, no hate to any parties involved: vulnerable code happens, and it's hard to write something that's secure against all kinds of attacks. Everyone who I wrote to was really responsive, with the quickest ACKs getting to me within 15 minutes. A pleasant surprise!&lt;/p&gt;&lt;head rend="h3"&gt;Addendum #2: Other ARPA subdomains&lt;/head&gt;&lt;p&gt;There are several other subdomains outside in-addr and ip6; Wikipedia even has a nice list. Of those, perhaps the most interesting one is e164.arpa., which relates to the E.164 public numbering plan for the public telephone network. In very short terms, the dream was to have a subdomain with a reversed phone number (same reason as revDNS), pointing to an IP address on which one could receive a telephone call. This telephone-to-IP mapping could be used in various scenarios, such as roaming between physical locations.&lt;/p&gt;&lt;p&gt;Unfortunately, e164 never caught on, and multiple regional registrars have already deprecated the supporting infra. Still, if after reading this article you're in a dire need of an ARPA subdomain, &lt;del&gt;this may be your easiest way of getting ahold one.&lt;/del&gt; See below for an easier method!&lt;/p&gt;&lt;head rend="h3"&gt;Addendum #3: Do It Yourself (no please don't)&lt;/head&gt;&lt;p&gt;Through the adventures of yet another entity hosting an ARPA fedi instance I found out that Hurricane Electric's tunnelbroker.net, besides giving out free IPv6 ranges, also gives out free ip6.arpa zones! HE 💚&lt;/p&gt;&lt;p&gt;Greetz to paige, and thanks for sharing your method! Also greetz to caramel for their writeup (linked earlier)!&lt;/p&gt;&lt;p&gt;Should you welcome this into your own home, I published all the source under a BSD license - feel free to steal any part. also click here for a free ipv6 shirt.&lt;/p&gt;&lt;p&gt;Notes:&lt;/p&gt;&lt;p&gt;1. Technically, you're supposed to put a domain in there. Practically, the protocol is very malleable: with a custom DNS server, you could likely put anything in a PTR record and see how clients reject your response, or burn in undefined behavior. However! For simplicity, I wanted to keep with PowerDNS, so for the purpose of this article I'm considering limits of pdns as limits of the protocol.&lt;/p&gt;&lt;p&gt;2. Yes, yes, I know, SOCK_RAW is a thing, and you can handle ICMP with it. Bash doesn't have a single way to open RAW sockets tho, so this was out of the question - and even if it wasn't, I find it not much less painful than what I ended up doing.&lt;/p&gt;&lt;p&gt;3. The "superseded" part is a bit vague. In reality, links joining ARPANET and the internet existed for some time before the final closure. For a lack of a better word, this shall do.&lt;/p&gt;&lt;p&gt;Big thanks to kleines Filmröllchen, Lili, famfo and Linus for proofreading this post!&lt;/p&gt;&lt;head rend="h3"&gt;Comments:&lt;/head&gt;&lt;p&gt;what.&lt;/p&gt;🐢 at 30.03.2025, 08:02:59&lt;p&gt;🐢 That's crazy🐢 That's actually crazy🐢That's messed up🐢&lt;/p&gt;liuzhen932 at 30.03.2025, 23:32:47&lt;p&gt;Displaying images or even animations in a traceroute is a really crazy idea!&lt;/p&gt;paige at 04.04.2025, 05:26:12&lt;p&gt;greetz! bad apple in traceroute is crazy&lt;/p&gt;tufo at 08.04.2025, 21:05:45&lt;p&gt;very cool, i love reading longer blogposts such as this.&lt;/p&gt;Ymity at 19.04.2025, 11:22:14&lt;p&gt;Thanks for the great read! &amp;lt;3&lt;/p&gt;bud at 27.04.2025, 09:01:15&lt;p&gt;finally!!!, we got the blogpost!!! :DDDD&lt;/p&gt;utahorange at 15.06.2025, 06:09:46&lt;p&gt;so awesome!!!!!!&lt;/p&gt;vvv at 09.10.2025, 03:46:18&lt;p&gt;Incredible stuff!&lt;/p&gt;Jay Jay at 01.11.2025, 14:27:54&lt;p&gt;I put up a website in my in-addr.arpa zone. But Let's Encrypt wouldn't let me generate a certificate. 😅 We might need a petition to enable in-addr.arpa certificates.&lt;/p&gt;By commenting, you agree for the session cookie to be stored on your device ;p&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sdomi.pl/weblog/24-arpa-hacks/"/><published>2025-11-01T10:01:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45781293</id><title>Abandonware of the web: do you know that there is an HTML tables API?</title><updated>2025-11-01T16:10:05.620548+00:00</updated><content>&lt;doc fingerprint="3220185a99e5836b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Abandonware of the web: do you know that there is an HTML tables API?&lt;/head&gt;Wednesday, October 8th, 2025 at 3:08 pm&lt;p&gt;When people turn data into HTML tables using JavaScript, they either use the DOM methods (createElement() and the likes), but most of the time just append a huge string and use innerHTML, which always is a security concern. However, did you know that HTML tables also have an old, forgotten API ? Using this one, you can loop over tables, create bodies, rows, cells, heads, footers, captions an summaries (yes, HTML tables have all of those) and access the table cells. Without having to re-render the whole table on each change. Check out the Codepen to see how you can create a table from a nested array:&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;quote&gt;let table = [ ['one','two','three'], ['four','five','six'] ]; let b = document.body; let t = document.createElement('table'); b.appendChild(t); table.forEach((row,ri) =&amp;gt; { let r = t.insertRow(ri); row.forEach((l,i) =&amp;gt; { let c = r.insertCell(i); c.innerText = l; }) });&lt;/quote&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;You can then access each table cell with an index (with t being a reference to the table):&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;quote&gt;console.log(t.rows[1].cells[1]); // =&amp;gt; &amp;lt;td&amp;gt;five&amp;lt;/td&amp;gt;&lt;/quote&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;You can also delete and create cells and rows, if you want to add a row to the end of the table with a cell, all you need to do is:&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;quote&gt;t.insertRow(-1); t.rows[2].insertCell(0); t.rows[2].cells[0].innerText = 'foo';&lt;/quote&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;There are a few things here that are odd – adding a -1 to add a row at the end for example – and there seems to be no way to create a TH element instead of a TD. All table cells are just cells.&lt;/p&gt;&lt;p&gt;However, seeing how much of a pain it is to create tables, it would be fun to re-visit this API and add more functionality to it. We did add a lot of things to HTML forms, like formData and the change event, so why not add events and other features to tables. That way they’d finally get the status as data structures and not a hack to layout content on the web.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://christianheilmann.com/2025/10/08/abandonware-of-the-web-do-you-know-that-there-is-an-html-tables-api/"/><published>2025-11-01T12:58:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45781298</id><title>SQLite concurrency and why you should care about it</title><updated>2025-11-01T16:10:05.244878+00:00</updated><content>&lt;doc fingerprint="7b4e765eeca27f9a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SQLite concurrency and why you should care about it&lt;/head&gt;
    &lt;p&gt;SQLite is a powerful database engine, but due to its design, it has limitations that should not be overlooked.&lt;/p&gt;
    &lt;p&gt;Jellyfin has used a SQLite-based database for storing most of its data for years, but it has also encountered issues on many systems. In this blog post, I will explain how we address these limitations and how developers using SQLite can apply the same solutions.&lt;/p&gt;
    &lt;p&gt;This will be a technical blog post intended for developers and everyone wanting to learn about concurrency.&lt;/p&gt;
    &lt;p&gt;Also Jellyfin's implementation of locking for SQLite should be fairly easy to be implemented into another EF Core application if you are facing the same issue.&lt;/p&gt;
    &lt;p&gt;- JPVenson&lt;/p&gt;
    &lt;head rend="h2"&gt;The Premise&lt;/head&gt;
    &lt;p&gt;SQLite is a file-based database engine running within your application and allows you to store data in a relational structure. Overall it gives your application the means of storing structured data as a single file and without having to depend on another application to do so. Naturally this also comes at a price. If your application fully manages this file, the assumption must be made that your application is the sole owner of this file, and nobody else will tinker with it while you are writing data to it.&lt;/p&gt;
    &lt;p&gt;So an application that wants to use SQLite as its database needs to be the only one accessing it. Having established this fact, an important thought arises: if only a single write operation should be performed on a single file at a time, this rule must also apply to operations within the same application.&lt;/p&gt;
    &lt;head rend="h2"&gt;The W-A-L mode&lt;/head&gt;
    &lt;p&gt;SQLite has a feature that tries to get around this limitation: the Write-Ahead-Log (WAL). The WAL is a separate file that acts as a journal of operations that should be applied to an SQLite file. This allows multiple parallel writes to take place and get enqueued into the WAL. When another part of the application wants to read data, it reads from the actual database, then scans the WAL for modifications and applies them on the fly. This is not a foolproof solution; there are still scenarios where WAL does not prevent locking conflicts.&lt;/p&gt;
    &lt;head rend="h2"&gt;SQLite transactions&lt;/head&gt;
    &lt;p&gt;A transaction is supposed to ensure two things. Modifications made within a transaction can be reverted, either when something goes wrong or when the application decides it should and optionally a transaction may also block other readers from reading data that is modified within a transaction. This is where it gets spicy and we come to the real reason why I am writing this blog post. For some reason on some systems that run Jellyfin when a transaction takes place the SQLite engine reports the database is locked and instead of waiting for the transaction to be resolved the engine refuses to wait and just crashes. This seems to be a not uncommon issue and there are many reports to be found on the issue.&lt;/p&gt;
    &lt;p&gt;The factor that makes this issue so bad is that it does not happen reliably. So far we only have one team member where this can be (somewhat) reliably be reproduced which makes this an even worse a bug. From the reports this issue happens across all operating systems, drive speeds and with or without virtualization. So we do not have any deciding factor identified that even contributes to the likelihood of the issue happening.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Jellyfin factor&lt;/head&gt;
    &lt;p&gt;Having established the general theory on how SQLite behaves, we also have to look at the specifics of Jellyfins usage of SQLite. During normal operations on a recommended setup (Non-Networked Storage and preferably SSD) its unusual for any problems to arise, however the way Jellyfin utilises the SQLite db up to 10.11 is very suboptimal. In versions prior to 10.11 Jellyfin had a bug in its parallel task limit which resulted in exponential overscheduling of library scan operations which hammered the database engine with thousands of parallel write requests that an SQLite engine is simply not able to handle. While most SQLite engine implementations have retry behavior, they also have timeouts and checks in place to prevent limitless waiting so if we stress the engine enough, it just fails with an error. That and very long running and frankly unoptimized transactions could lead to the database just being overloaded with requests and flaking out.&lt;/p&gt;
    &lt;head rend="h2"&gt;The solution&lt;/head&gt;
    &lt;p&gt;Since we moved the codebase over to EF Core proper, we have the tools to actually do something about this as EF Core gives us a structured abstraction level. EF Core supports a way of hooking into every command execution or transaction by creating Interceptors. With an interceptor we can finally do the straight forward idea of just "not" writing to the database in parallel in a transparent way to the caller. The overall idea is to have multiple strategies of locking. Because all levels of synchronization will inevitably come at the cost of performance, we only want to do it when it is really necessary. So, I decided on three locking strategies:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;No-Lock&lt;/item&gt;
      &lt;item&gt;Optimistic locking&lt;/item&gt;
      &lt;item&gt;Pessimistic locking&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a default, the no-lock behavior does exactly what the name implies. Nothing. This is the default because my research shows that for 99% all of this is not an issue and every interaction at this level will slow down the whole application.&lt;/p&gt;
    &lt;p&gt;Both the optimistic and pessimistic behaviors use two interceptors—one for transactions and one for commands—and override &lt;code&gt;SaveChanges&lt;/code&gt; in &lt;code&gt;JellyfinDbContext&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Optimistic locking behavior&lt;/head&gt;
    &lt;p&gt;Optimistic locking means to assume the operation in question will succeed and only handle issues afterwards. In essence this can be boiled down to "Try and Retry and Retry ..." for a set number of times until either we succeed with the operation or fail entirely. This still leaves the possibility that we will not actually be able to perform a write, but the introduced overhead is far less than the Pessimistic locking behavior.&lt;/p&gt;
    &lt;p&gt;The idea behind how this works is simple: every time two operations try to write to the database, one will always win. The other will fail, wait some time, then retry a few times.&lt;/p&gt;
    &lt;p&gt;Jellyfin uses the &lt;code&gt;Polly&lt;/code&gt; library perform the retry behavior and will only retry operations it will find have been locked due to this exact issue.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pessimistic locking behavior&lt;/head&gt;
    &lt;p&gt;Pessimistic locking always locks when a write to SQLite should be performed. Essentially every time an transaction is started or a write operation on the database is done though EF Core, Jellyfin will wait until all other read operations are finished and then block all other operations may they be read or write until the write in question has been performed. This however means, that Jellyfin can only ever perform a single write to the database, even if it would technically does not need to.&lt;/p&gt;
    &lt;p&gt;In theory, an application should have no issue reading from table "Alice" while writing to table "Bob" however to eliminate all possible sources of concurrency related locking, Jellyfin will only ever allow a single write performed on its database in this mode. While this will absolutely result in the most stable operation, it will undoubtedly also be the slowest.&lt;/p&gt;
    &lt;p&gt;Jellyfin uses a ReaderWriterLockSlim to lock the operations, that means we allow an unlimited number of reads to happen concurrently while only one write may ever be done on the database.&lt;/p&gt;
    &lt;head rend="h3"&gt;The future Smart locking behavior&lt;/head&gt;
    &lt;p&gt;In the future we might also consider combining both modes, to get the best of both worlds.&lt;/p&gt;
    &lt;head rend="h1"&gt;The result&lt;/head&gt;
    &lt;p&gt;Initial testing showed that with both modes, we had great success in handling the underlying issue. While we are not yet sure why this happens only on some systems when others work, we at least now have an option for users previously left out of using Jellyfin.&lt;/p&gt;
    &lt;p&gt;When I was researching this topic, I found many reports all over the internet facing the same error but nobody was able to provide a conclusive explanation whats really happening here. There have been similar proposals made to handle it but there wasn't a "ready to drop in" solution that handles all the different cases or only code that required massive modifications to every EF Core query. Jellyfin's implementation of the locking behaviors should be a copy-paste solution for everyone having the same issues as its using interceptors and the caller has no idea of the actual locking behavior.&lt;/p&gt;
    &lt;p&gt;Best of luck,&lt;/p&gt;
    &lt;p&gt;- JPVenson&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jellyfin.org/posts/SQLite-locking/"/><published>2025-11-01T12:59:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45781397</id><title>CharlotteOS – An Experimental Modern Operating System</title><updated>2025-11-01T16:10:04.722429+00:00</updated><content>&lt;doc fingerprint="25a44afe193d139f"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;catten&lt;/code&gt; is an operating system kernel developed as a key component of the CharlotteOS project but it is designed to be flexible enough that we hope it can also find use in many other places. It seeks to be a monolithic kernel with low-level system call interfaces that borrows ideas from exokernels and other novel systems like Plan 9 and Fuchsia. Its design allows for almost any higher level interface to be layered on top and also includes a typesafe system namespace (akin to the namespaces found in Fuschsia and Plan 9 but more flexible and typesafe) with URIs as paths which has the added benefit of allowing access to the namespace of another host over a network without having to mount anything locally all while being secured by granular capabilities and a persistent mandatory access control policy.&lt;/p&gt;
    &lt;p&gt;catten is still in early development, and core subsystems are actively being built. We welcome contributions—feel free to grab an issue from the tracker, suggest features, or participate in discussions on our repository, Discord server or Matrix instance.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;catten&lt;/code&gt;is written in Rust and ISA specific assembly languages&lt;/item&gt;
      &lt;item&gt;x86_64 assembly should use Intel syntax as implemented by &lt;code&gt;rustc&lt;/code&gt;and&lt;code&gt;llvm-mc&lt;/code&gt;exclusively&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;C language dependencies are allowed if vetted by maintainers.&lt;/item&gt;
      &lt;item&gt;Any dependencies in languages other than Rust, C, and assembly are strictly forbidden.&lt;/item&gt;
      &lt;item&gt;Always prefer a high-quality Rust equivalent over an external C library unless there is good reason to do otherwise&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Processor: &lt;list rend="ul"&gt;&lt;item&gt;x86_64 (Primary ISA) &lt;list rend="ul"&gt;&lt;item&gt;x2APIC LAPIC operating mode&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;x86_64 (Primary ISA) &lt;/item&gt;
      &lt;item&gt;Firmware: &lt;list rend="ul"&gt;&lt;item&gt;Unified Extensible Firmware Interface (UEFI)&lt;/item&gt;&lt;item&gt;Advanced Configuration and Power Interface (ACPI)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Memory: &lt;list rend="ul"&gt;&lt;item&gt;Recommended: &amp;gt;= 1 GiB&lt;/item&gt;&lt;item&gt;Required: 128 MiB&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Storage: &lt;list rend="ul"&gt;&lt;item&gt;Recommended: &amp;gt;= 64 GiB&lt;/item&gt;&lt;item&gt;Required: 4 GiB&lt;/item&gt;&lt;item&gt;Device Types: &lt;list rend="ul"&gt;&lt;item&gt;Non-Volatile Memory Express (NVMe)&lt;/item&gt;&lt;item&gt;USB Mass Storage Device Class&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Output: &lt;list rend="ul"&gt;&lt;item&gt;Display Adapter: Any adapter capable of providing framebuffers via the UEFI Graphics Output Protocol&lt;/item&gt;&lt;item&gt;Serial: &lt;list rend="ul"&gt;&lt;item&gt;NS16550 compatible UART&lt;/item&gt;&lt;item&gt;USB CDC ACM (Virtual UART)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Input: &lt;list rend="ul"&gt;&lt;item&gt;Keyboard &lt;list rend="ul"&gt;&lt;item&gt;PS/2&lt;/item&gt;&lt;item&gt;USB HID&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Serial &lt;list rend="ul"&gt;&lt;item&gt;NS16550 compatible UART&lt;/item&gt;&lt;item&gt;USB CDC ACM (Virtual UART)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Keyboard &lt;/item&gt;
      &lt;item&gt;Networking: &lt;list rend="ul"&gt;&lt;item&gt;USB CDC Network Control Model&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please reach out to us on Matrix or Discord if you are interested in contributing.&lt;/p&gt;
    &lt;p&gt;This kernel is licensed under the GNU General Public License version 3.0 (or at your option, any later version). By contributing to this project you agree to license your contributions under those same terms exclusively.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/charlotte-os/Catten"/><published>2025-11-01T13:12:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45781792</id><title>Async/Await is finally back in Zig</title><updated>2025-11-01T16:10:04.661034+00:00</updated><content/><link href="https://charlesfonseca.substack.com/p/asyncawait-is-finally-back-in-zig"/><published>2025-11-01T14:14:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45781843</id><title>Tigerbeetle Simulator</title><updated>2025-11-01T16:10:04.326565+00:00</updated><link href="https://sim.tigerbeetle.com/"/><published>2025-11-01T14:20:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45782136</id><title>arXiv No Longer Accepts Computer Science Position or Review Papers Due to LLMs</title><updated>2025-11-01T16:10:03.687887+00:00</updated><content>&lt;doc fingerprint="ff55b99b0494c981"&gt;
  &lt;main&gt;
    &lt;p&gt;arXiv’s computer science (CS) category has updated its moderation practice with respect to review (or survey) articles and position papers. Before being considered for submission to arXiv’s CS category, review articles and position papers must now be accepted at a journal or a conference and complete successful peer review. When submitting review articles or position papers, authors must include documentation of successful peer review to receive full consideration. Review/survey articles or position papers submitted to arXiv without this documentation will be likely to be rejected and not appear on arXiv.&lt;lb/&gt; This change is being implemented due to the unmanageable influx of review articles and position papers to arXiv CS.&lt;/p&gt;
    &lt;p&gt;Is this a policy change?&lt;/p&gt;
    &lt;p&gt;Technically, no! If you take a look at arXiv’s policies for specific content types you’ll notice that review articles and position papers are not (and have never been) listed as part of the accepted content types. Review articles and position papers have, in the past, only been accepted at moderator discretion, because the few we received were of high quality and of interest to arXiv readers and the scientific community at large.&lt;/p&gt;
    &lt;p&gt;Why is the arXiv CS category making this change?&lt;/p&gt;
    &lt;p&gt;In the past few years, arXiv has been flooded with papers. Generative AI / large language models have added to this flood by making papers – especially papers not introducing new research results – fast and easy to write. While categories across arXiv have all seen a major increase in submissions, it’s particularly pronounced in arXiv’s CS category.&lt;/p&gt;
    &lt;p&gt;The goal of this change of practice is to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Help arXiv readers more easily find valuable review articles and position papers written by subject matter experts&lt;/item&gt;
      &lt;item&gt;Free up moderators to focus on the content types officially accepted by arXiv, reduce submission hold times, and keep the pace of scientific discovery going!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Above all, the core purpose of arXiv is to share research papers and facilitate scientific discovery quickly and freely. We are making this change in support of that mission.&lt;/p&gt;
    &lt;p&gt;In the past, arXiv CS received a relatively small amount of review or survey articles, and those we did receive were of extremely high quality, written by senior researchers at the request of publications like Annual Reviews, Proceedings of the IEEE, and Computing Surveys. Position paper submissions to arXiv were similarly rare, and usually produced by scientific societies or government study groups (for example,the Computing Research Association of the National Academies of Science, Engineering, and Medicine). While, as now, these papers were not content types officially accepted by arXiv, the arXiv moderators accepted them because of their scholarly value to the research community.&lt;/p&gt;
    &lt;p&gt;Fast forward to present day – submissions to arXiv in general have risen dramatically, and we now receive hundreds of review articles every month. The advent of large language models have made this type of content relatively easy to churn out on demand, and the majority of the review articles we receive are little more than annotated bibliographies, with no substantial discussion of open research issues.&lt;/p&gt;
    &lt;p&gt;arXiv believes that there are position papers and review articles that are of value to the scientific community, and we would like to be able to share them on arXiv. However, our team of volunteer moderators do not have the time or bandwidth to review the hundreds of these articles we receive without taking time away from our core purpose, which is to share research articles.&lt;/p&gt;
    &lt;p&gt;Reasonable and trusted outside refereed venues already exist (conferences and journals) which solicit position papers and review articles on subjects of concern or interest to our readers (such as concerns over privacy, ethics, safety, and security of recent CS technologies, particularly applications of artificial intelligence) and as part of that process, they conduct in-depth review to assure quality, evidential support of opinions, and scholarly value. Since arXiv does not have the resources to conduct this quality-control in-house for content types that we do not officially accept, this change of practice is allowing us to rely on these refereed venues to do so for us so that we can still share position papers and review articles of value on arXiv.&lt;/p&gt;
    &lt;p&gt;How do I submit my review article or position paper to arXiv? Before submission to arXiv, have your review article or position paper accepted to a refereed venue with peer review like a journal or a conference. Review articles or position papers must be accepted to a journal or conference before being submitted to arXiv and you must have documentation of complete and successful peer review.&lt;/p&gt;
    &lt;p&gt;Please note: the review conducted at conference workshops generally does not meet the same standard of rigor of traditional peer review and is not enough to have your review article or position paper accepted to arXiv.&lt;/p&gt;
    &lt;p&gt;How do I show my review article or position paper has successfully completed peer review? When you submit to arXiv, please include the peer reviewed journal reference and DOI metadata. If you do not provide this, your review article or position paper will likely be rejected.&lt;/p&gt;
    &lt;p&gt;Can I resubmit my position paper or review article after being rejected? If your position paper or review article was rejected because it did not complete a successful peer review process, you can submit an appeal request to resubmit if your article has since completed a successful peer review process. Do not resubmit your position paper or review article without an accepted appeal. Here are the instructions for how to appeal.&lt;/p&gt;
    &lt;p&gt;I have a scientific paper studying the impact of science and technology in society. Can I submit this to arXiv without peer review? Yes, arXiv has always released these types of scientific papers, for example in cs.CY or physics.soc-ph. These are scientific research papers and are not subject to this moderation practice change.&lt;/p&gt;
    &lt;p&gt;Will other categories on arXiv also change their practice re: review articles and position papers? Each category of arXiv has different moderators, who are subject matter experts with a terminal degree in their particular subject, to best serve the scholarly pursuits, goals, and standards of their category. While all moderators adhere to arXiv policy, the only policy arXiv has in place with regard to review articles and position papers is that they are not a generally accepted content type. The goal of the moderators of each category is to make sure the work being submitted is actually science, and that it is of potential interest to the scientific community. If other categories see a similar rise in LLM-written review articles and position papers, they may choose to change their moderation practices in a similar manner to better serve arXiv authors and readers. We will make these updates public if and when they do occur.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/"/><published>2025-11-01T14:58:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45782267</id><title>Tech companies are firing everyone to "fund AI", spending money on each other</title><updated>2025-11-01T16:10:03.621052+00:00</updated><content/><link href="https://old.reddit.com/r/ArtificialInteligence/comments/1oj52xx/tech_companies_are_firing_everyone_to_fund_ai_but/"/><published>2025-11-01T15:12:00+00:00</published></entry></feed>