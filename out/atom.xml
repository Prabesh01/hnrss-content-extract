<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-18T16:10:45.046465+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45615237</id><title>EVs are depreciating faster than gas-powered cars</title><updated>2025-10-18T16:10:53.778066+00:00</updated><content>&lt;doc fingerprint="28f21016809a21c8"&gt;
  &lt;main&gt;
    &lt;p&gt;The resale value of electric vehicles is collapsing worldwide, hurting private owners and fleet operators.&lt;/p&gt;
    &lt;p&gt;The crisis became especially apparent when BluSmart, India’s pioneering all-electric ride-hailing service, collapsed in April amid financial fraud allegations. The Delhi-based company’s fleet of thousands of cars, originally worth over $12,000 each, suddenly flooded the market at about $3,000.&lt;/p&gt;
    &lt;p&gt;For Tesla owners in the U.S., their 2023 Model Ys are worth 42% less than what they paid two years ago, while a Ford F-150 truck bought the same year depreciated just 20%. Older EV models depreciate even faster than newer ones.&lt;/p&gt;
    &lt;p&gt;The crisis exposes the fundamental problem that nobody really knows what electric cars are worth in the secondhand market, as their value is largely tied to batteries with uncertain lifespans.&lt;/p&gt;
    &lt;p&gt;“For gas cars, there’s a 100-year process behind them based on odometer and major maintenance schedules and the expected life of combustion engine parts,” said Andrew Garberson, head of growth and research at Recurrent, a Seattle-based startup that evaluates the wear and tear on used EVs to bring transparency to the secondhand market. “Electric cars have fewer moving parts, and a lot of the value of the car is tied up in one component — the battery.”&lt;/p&gt;
    &lt;p&gt;A U.K.-based study found 3-year-old EVs lost more than half of their value compared with 39% for gas cars. Another study conducted by Boucar Diouf, an EV researcher and professor at Kyung Hee University in Seoul, found EVs in the U.S. can lose as much as 60% of their value over three to five years, compared with less than half for traditional vehicles.&lt;/p&gt;
    &lt;p&gt;For fleet owners and operators — across sectors from ride-hailing to rentals to logistics — pledging to go green, this value disaster is threatening to derail the sustainability movement.&lt;/p&gt;
    &lt;p&gt;Florida-based car rental company Hertz, which bought 100,000 Teslas in 2021, reported a $2.9 billion loss in 2024, driven largely by plummeting EV value, according to its February 2025 earnings call.&lt;/p&gt;
    &lt;p&gt;The company was hemorrhaging more than $530 a car monthly by late 2024, due to high upfront costs, steep insurance premiums, and long repair and restoration lead times, according to EV news publication InsideEVs. Hertz dumped 30,000 EVs — Teslas bought for more than $40,000 ended up on its website for resale at prices under $20,000. As of October 8, it listed a Model Y for $27,000. A new Model Y cost $45,000 in the U.S. until earlier this month, when Tesla launched a more affordable version at just under $40,000.&lt;/p&gt;
    &lt;p&gt;“Fleets feel EV residual risk the most because they model total cost of ownership to the decimal and have to remarket thousands of units,” Jack Carlson, CEO of Carvai.ai, an AI-powered vehicle research and shopping platform, told Rest of World. “Retail buyers worry too, but mostly about battery health and charging. Fleets worry about the exit price.”&lt;/p&gt;
    &lt;p&gt;Ironically, Tesla still represents the best-case scenario, holding on to resale value better given its years of experience and brand-building. Chinese newcomers like BYD, Nio, and XPeng tend to have lower resale value in comparison, Mariusz Sawula, CEO of Poland-based autoDNA, a provider of vehicle history reports, told Rest of World.&lt;/p&gt;
    &lt;p&gt;“Premium brands consistently retain higher resale value than mass-market brands, for both ICE [internal combustion engine] vehicles and EVs. No one knows what entirely new Chinese marques will cost in the secondary market — for instance, what a 5-year-old Omoda will fetch,” Sawula said, comparing the likes of Tesla to a sub-brand of Chinese carmaker Chery.&lt;/p&gt;
    &lt;p&gt;The depreciation crisis also varies by region. While Japanese and U.S. consumers remain skeptical, more EV-friendly markets support strong resale prices, according to Justin Fischer, an automotive expert at vehicle-trading company CarEdge.&lt;/p&gt;
    &lt;p&gt;“In markets where consumers are open to the idea of going electric, like China, Norway, and Costa Rica, resale values are supported by this higher demand,” Fischer told Rest of World.&lt;/p&gt;
    &lt;p&gt;North America’s vast highways and sprawling distances create a tough environment for used EVs, Karl Brauer, who has reviewed thousands of vehicles since the mid-1990s, told Rest of World. Europe, with its denser cities and shorter commutes, offers more stability in the used EV market compared to the U.S. or Canada. Distance is just one of several limitations, which also include climate sensitivity and charging downtime.&lt;/p&gt;
    &lt;p&gt;“While an electric vehicle is ideal for shorter trips in urban areas with moderate temperatures, they aren’t as effective as traditional cars for long-distance travel or when the temperature is very high or very low,” Brauer said. “Even electric vehicles with larger batteries and longer ranges still require more time to replenish their energy compared to gasoline and hybrid vehicles.”&lt;/p&gt;
    &lt;p&gt;Consumer confidence is also buoyed by favourable policies and extensive public fast-charging networks.&lt;/p&gt;
    &lt;p&gt;“Greater demand and a more secure supply side helps stabilize resale values there, compared to what we see here in North America,” Johnny T. Beckett, vice president of sales at Canadian multibrand electric car dealer EVNet, told Rest of World.&lt;/p&gt;
    &lt;p&gt;Uber walked away from buying 5,000 used BluSmart vehicles, while local rivals like Evara Cabs wouldn’t touch them because of battery and warranty concerns. BluSmart’s fire-sales in the cities of New Delhi and Bengaluru showed exactly how fast EVs can become worthless.&lt;/p&gt;
    &lt;p&gt;A family car drives modest distances yearly, but Indian fleet vehicles rack up three to four times their mileage, destroying resale value faster, according to Anirudh Damani, managing director at Artha Venture Fund, which backs Uber partner Everest Fleet in India.&lt;/p&gt;
    &lt;p&gt;“For individual consumers, the resale value is an inconvenience; for fleet operators, it’s an existential problem,” Damani told Rest of World. “Their vehicles are financial assets, not lifestyle products. Without a predictable residual value, even a profitable fleet can become unviable once replacement cycles begin.”&lt;/p&gt;
    &lt;p&gt;Battery-as-a-service models are emerging as a potential lifeline as they give fleet operators the predictable costs and stable values they desperately need, offering a path forward where battery risk no longer threatens entire business models, Damani said.&lt;/p&gt;
    &lt;p&gt;An April 2025 McKinsey report shows one in five Europeans and just one in 10 U.S. consumers are considering going electric. Fleet operators, though, are making massive bets globally, with companies such as Uber, Bolt, and Lyft pledging full electrification — despite vehicles potentially becoming worthless before the loans are even paid off.&lt;/p&gt;
    &lt;p&gt;Battery tech is also proving more resilient than expected. Research from Recurrent shows batteries deteriorate just 1%–2% annually, with only 1% of cars built after 2016 needing replacements versus 13% for older EVs. Most of the recent replacements are also covered by warranties.&lt;/p&gt;
    &lt;p&gt;“The data is helping people have confidence in used EV batteries,” Garberson said. “As confidence rises, so will resale prices.”&lt;/p&gt;
    &lt;p&gt;On the back of these quantifiable indicators, there’s been a rise in certified pre-owned EV programs and state-of-health reports that aid informed decision-making. It’s now also easier to keep up with these cars. The pace of innovation that initially scared buyers has slowed, with manufacturers prioritizing longevity over rapid changes, according to EVNet’s Beckett, who is also the co-founder of the Electric Vehicle Association of Atlantic Canada.&lt;/p&gt;
    &lt;p&gt;According to Beckett, 2026 “will be a year of industry and market readjustments to both lower supply and demand.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://restofworld.org/2025/ev-depreciation-blusmart-collapse/"/><published>2025-10-17T10:56:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45615931</id><title>Live Stream from the Namib Desert</title><updated>2025-10-18T16:10:53.644037+00:00</updated><content>&lt;doc fingerprint="b301111443ec52cd"&gt;
  &lt;main&gt;
    &lt;p&gt;A couple weeks ago I happened on this live streaming camera near a water hole in the Namib Desert and it's now my first stop every morning.&lt;/p&gt;
    &lt;p&gt;Namibia is 6 hours ahead of my U.S. Eastern Time Zone so it's really hot there when I visit and almost always there are lots of creatures hydrating.&lt;/p&gt;
    &lt;p&gt;Zebras, ostriches (which wait as a family at a safe distance), oryx, warthogs, wildebeest, jackal, bat-eared fox, spotted hyena, cape hare, red hartebeest, giraffes, springbok, elephants, etc.&lt;/p&gt;
    &lt;p&gt;If I were an elementary school teacher I'd place a big-screen TV in my classroom and turn it on to this camera's feed on a regular basis.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bookofjoe2.blogspot.com/2025/10/live-stream-from-namib-desert.html"/><published>2025-10-17T12:23:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45619329</id><title>Andrej Karpathy – It will take a decade to work through the issues with agents</title><updated>2025-10-18T16:10:53.133166+00:00</updated><content>&lt;doc fingerprint="2771005f5ee40ddf"&gt;
  &lt;main&gt;
    &lt;p&gt;The Andrej Karpathy episode.&lt;/p&gt;
    &lt;p&gt;Andrej explains why reinforcement learning is terrible (but everything else is much worse), why model collapse prevents LLMs from learning the way humans do, why AGI will just blend into the previous ~2.5 centuries of 2% GDP growth, why self driving took so long to crack, and what he sees as the future of education.&lt;/p&gt;
    &lt;p&gt;Watch on YouTube; listen on Apple Podcasts or Spotify.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sponsors&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Labelbox helps you get data that is more detailed, more accurate, and higher signal than you could get by default, no matter your domain or training paradigm. Reach out today at labelbox.com/dwarkesh&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mercury helps you run your business better. It’s the banking platform we use for the podcast — we love that we can see our accounts, cash flows, AR, and AP all in one place. Apply online in minutes at mercury.com&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Google’s Veo 3.1 update is a notable improvement to an already great model. Veo 3.1’s generations are more coherent and the audio is even higher-quality. If you have a Google AI Pro or Ultra plan, you can try it in Gemini today by visiting https://gemini.google&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Timestamps&lt;/head&gt;
    &lt;p&gt;(00:00:00) – AGI is still a decade away&lt;/p&gt;
    &lt;p&gt;(00:29:45) – LLM cognitive deficits&lt;/p&gt;
    &lt;p&gt;(00:49:38) – How do humans learn?&lt;/p&gt;
    &lt;p&gt;(01:06:25) – AGI will blend into 2% GDP growth&lt;/p&gt;
    &lt;p&gt;(01:32:50) – Evolution of intelligence &amp;amp; culture&lt;/p&gt;
    &lt;p&gt;(01:42:55) - Why self driving took so long&lt;/p&gt;
    &lt;p&gt;(01:56:20) - Future of education&lt;/p&gt;
    &lt;head rend="h3"&gt;Transcript&lt;/head&gt;
    &lt;head rend="h3"&gt;00:00:00 – AGI is still a decade away&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 00:00:00&lt;/p&gt;
    &lt;p&gt;Today I’m speaking with Andrej Karpathy. Andrej, why do you say that this will be the decade of agents and not the year of agents?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:00:07&lt;/p&gt;
    &lt;p&gt;First of all, thank you for having me here. I’m excited to be here.&lt;/p&gt;
    &lt;p&gt;The quote you’ve just mentioned, “It’s the decade of agents,” is actually a reaction to a pre-existing quote. I’m not actually sure who said this but they were alluding to this being the year of agents with respect to LLMs and how they were going to evolve. I was triggered by that because there’s some over-prediction going on in the industry. In my mind, this is more accurately described as the decade of agents.&lt;/p&gt;
    &lt;p&gt;We have some very early agents that are extremely impressive and that I use daily—Claude and Codex and so on—but I still feel there’s so much work to be done. My reaction is we’ll be working with these things for a decade. They’re going to get better, and it’s going to be wonderful. I was just reacting to the timelines of the implication.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:00:58&lt;/p&gt;
    &lt;p&gt;What do you think will take a decade to accomplish? What are the bottlenecks?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:01:02&lt;/p&gt;
    &lt;p&gt;Actually making it work. When you’re talking about an agent, or what the labs have in mind and maybe what I have in mind as well, you should think of it almost like an employee or an intern that you would hire to work with you. For example, you work with some employees here. When would you prefer to have an agent like Claude or Codex do that work?&lt;/p&gt;
    &lt;p&gt;Currently, of course they can’t. What would it take for them to be able to do that? Why don’t you do it today? The reason you don’t do it today is because they just don’t work. They don’t have enough intelligence, they’re not multimodal enough, they can’t do computer use and all this stuff.&lt;/p&gt;
    &lt;p&gt;They don’t do a lot of the things you’ve alluded to earlier. They don’t have continual learning. You can’t just tell them something and they’ll remember it. They’re cognitively lacking and it’s just not working. It will take about a decade to work through all of those issues.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:01:44&lt;/p&gt;
    &lt;p&gt;Interesting. As a professional podcaster and a viewer of AI from afar, it’s easy for me to identify what’s lacking: continual learning is lacking, or multimodality is lacking. But I don’t really have a good way of trying to put a timeline on it. If somebody asks how long continual learning will take, I have no prior about whether this is a project that should take 5 years, 10 years, or 50 years. Why a decade? Why not one year? Why not 50 years?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:02:16&lt;/p&gt;
    &lt;p&gt;This is where you get into a bit of my own intuition, and doing a bit of an extrapolation with respect to my own experience in the field. I’ve been in AI for almost two decades. It’s going to be 15 years or so, not that long. You had Richard Sutton here, who was around for much longer. I do have about 15 years of experience of people making predictions, of seeing how they turned out. Also I was in the industry for a while, I was in research, and I’ve worked in the industry for a while. I have a general intuition that I have left from that.&lt;/p&gt;
    &lt;p&gt;I feel like the problems are tractable, they’re surmountable, but they’re still difficult. If I just average it out, it just feels like a decade to me.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:02:57&lt;/p&gt;
    &lt;p&gt;This is quite interesting. I want to hear not only the history, but what people in the room felt was about to happen at various different breakthrough moments. What were the ways in which their feelings were either overly pessimistic or overly optimistic? Should we just go through each of them one by one?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:03:16&lt;/p&gt;
    &lt;p&gt;That’s a giant question because you’re talking about 15 years of stuff that happened. AI is so wonderful because there have been a number of seismic shifts where the entire field has suddenly looked a different way. I’ve maybe lived through two or three of those. I still think there will continue to be some because they come with almost surprising regularity.&lt;/p&gt;
    &lt;p&gt;When my career began, when I started to work on deep learning, when I became interested in deep learning, this was by chance of being right next to Geoff Hinton at the University of Toronto. Geoff Hinton, of course, is the godfather figure of AI. He was training all these neural networks. I thought it was incredible and interesting. This was not the main thing that everyone in AI was doing by far. This was a niche little subject on the side. That’s maybe the first dramatic seismic shift that came with the AlexNet and so on.&lt;/p&gt;
    &lt;p&gt;AlexNet reoriented everyone, and everyone started to train neural networks, but it was still very per-task, per specific task. Maybe I have an image classifier or I have a neural machine translator or something like that. People became very slowly interested in agents. People started to think, “Okay, maybe we have a check mark next to the visual cortex or something like that, but what about the other parts of the brain, and how can we get a full agent or a full entity that can interact in the world?”&lt;/p&gt;
    &lt;p&gt;The Atari deep reinforcement learning shift in 2013 or so was part of that early effort of agents, in my mind, because it was an attempt to try to get agents that not just perceive the world, but also take actions and interact and get rewards from environments. At the time, this was Atari games.&lt;/p&gt;
    &lt;p&gt;I feel that was a misstep. It was a misstep that even the early OpenAI that I was a part of adopted because at that time, the zeitgeist was reinforcement learning environments, games, game playing, beat games, get lots of different types of games, and OpenAI was doing a lot of that. That was another prominent part of AI where maybe for two or three or four years, everyone was doing reinforcement learning on games. That was all a bit of a misstep.&lt;/p&gt;
    &lt;p&gt;What I was trying to do at OpenAI is I was always a bit suspicious of games as being this thing that would lead to AGI. Because in my mind, you want something like an accountant or something that’s interacting with the real world. I just didn’t see how games add up to it. My project at OpenAI, for example, was within the scope of the Universe project, on an agent that was using keyboard and mouse to operate web pages. I really wanted to have something that interacts with the actual digital world that can do knowledge work.&lt;/p&gt;
    &lt;p&gt;It just so turns out that this was extremely early, way too early, so early that we shouldn’t have been working on that. Because if you’re just stumbling your way around and keyboard mashing and mouse clicking and trying to get rewards in these environments, your reward is too sparse and you just won’t learn. You’re going to burn a forest computing, and you’re never going to get something off the ground. What you’re missing is this power of representation in the neural network.&lt;/p&gt;
    &lt;p&gt;For example, today people are training those computer-using agents, but they’re doing it on top of a large language model. You have to get the language model first, you have to get the representations first, and you have to do that by all the pre-training and all the LLM stuff.&lt;/p&gt;
    &lt;p&gt;I feel maybe loosely speaking, people kept trying to get the full thing too early a few times, where people really try to go after agents too early, I would say. That was Atari and Universe and even my own experience. You actually have to do some things first before you get to those agents. Now the agents are a lot more competent, but maybe we’re still missing some parts of that stack.&lt;/p&gt;
    &lt;p&gt;I would say those are the three major buckets of what people were doing: training neural nets per-tasks, trying the first round of agents, and then maybe the LLMs and seeking the representation power of the neural networks before you tack on everything else on top.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:07:02&lt;/p&gt;
    &lt;p&gt;Interesting. If I were to steelman the Sutton perspective, it would be that humans can just take on everything at once, or even animals can take on everything at once. Animals are maybe a better example because they don’t even have the scaffold of language. They just get thrown out into the world, and they just have to make sense of everything without any labels.&lt;/p&gt;
    &lt;p&gt;The vision for AGI then should just be something which looks at sensory data, looks at the computer screen, and it just figures out what’s going on from scratch. If a human were put in a similar situation and had to be trained from scratch… This is like a human growing up or an animal growing up. Why shouldn’t that be the vision for AI, rather than this thing where we’re doing millions of years of training?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:07:41&lt;/p&gt;
    &lt;p&gt;That’s a really good question. Sutton was on your podcast and I saw the podcast and I had a write-up about that podcast that gets into a bit of how I see things. I’m very careful to make analogies to animals because they came about by a very different optimization process. Animals are evolved, and they come with a huge amount of hardware that’s built in.&lt;/p&gt;
    &lt;p&gt;For example, my example in the post was the zebra. A zebra gets born, and a few minutes later it’s running around and following its mother. That’s an extremely complicated thing to do. That’s not reinforcement learning. That’s something that’s baked in. Evolution obviously has some way of encoding the weights of our neural nets in ATCGs, and I have no idea how that works, but it apparently works.&lt;/p&gt;
    &lt;p&gt;Brains just came from a very different process, and I’m very hesitant to take inspiration from it because we’re not actually running that process. In my post, I said we’re not building animals. We’re building ghosts or spirits or whatever people want to call it, because we’re not doing training by evolution. We’re doing training by imitation of humans and the data that they’ve put on the Internet.&lt;/p&gt;
    &lt;p&gt;You end up with these ethereal spirit entities because they’re fully digital and they’re mimicking humans. It’s a different kind of intelligence. If you imagine a space of intelligences, we’re starting off at a different point almost. We’re not really building animals. But it’s also possible to make them a bit more animal-like over time, and I think we should be doing that.&lt;/p&gt;
    &lt;p&gt;One more point. I do feel Sutton has a very... His framework is, “We want to build animals.” I think that would be wonderful if we can get that to work. That would be amazing. If there were a single algorithm that you can just run on the Internet and it learns everything, that would be incredible. I’m not sure that it exists and that’s certainly not what animals do, because animals have this outer loop of evolution.&lt;/p&gt;
    &lt;p&gt;A lot of what looks like learning is more like maturation of the brain. I think there’s very little reinforcement learning for animals. A lot of the reinforcement learning is more like motor tasks; it’s not intelligence tasks. So I actually kind of think humans don’t really use RL, roughly speaking.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:09:52&lt;/p&gt;
    &lt;p&gt;Can you repeat the last sentence? A lot of that intelligence is not motor task…it’s what, sorry?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:09:54&lt;/p&gt;
    &lt;p&gt;A lot of the reinforcement learning, in my perspective, would be things that are a lot more motor-like, simple tasks like throwing a hoop. But I don’t think that humans use reinforcement learning for a lot of intelligence tasks like problem-solving and so on. That doesn’t mean we shouldn’t do that for research, but I just feel like that’s what animals do or don’t.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:10:17&lt;/p&gt;
    &lt;p&gt;I’m going to take a second to digest that because there are a lot of different ideas. Here’s one clarifying question I can ask to understand the perspective. You suggest that evolution is doing the kind of thing that pre-training does in the sense of building something which can then understand the world.&lt;/p&gt;
    &lt;p&gt;The difference is that evolution has to be titrated in the case of humans through three gigabytes of DNA. That’s very unlike the weights of a model. Literally, the weights of the model are a brain, which obviously does not exist in the sperm and the egg. So it has to be grown. Also, the information for every single synapse in the brain simply cannot exist in the three gigabytes that exist in the DNA.&lt;/p&gt;
    &lt;p&gt;Evolution seems closer to finding the algorithm which then does the lifetime learning. Now, maybe the lifetime learning is not analogous to RL, to your point. Is that compatible with the thing you were saying, or would you disagree with that?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:11:17&lt;/p&gt;
    &lt;p&gt;I think so. I would agree with you that there’s some miraculous compression going on because obviously, the weights of the neural net are not stored in ATCGs. There’s some dramatic compression. There are some learning algorithms encoded that take over and do some of the learning online. I definitely agree with you on that. I would say I’m a lot more practically minded. I don’t come at it from the perspective of, let’s build animals. I come from it from the perspective of, let’s build useful things. I have a hard hat on, and I’m just observing that we’re not going to do evolution, because I don’t know how to do that.&lt;/p&gt;
    &lt;p&gt;But it does turn out we can build these ghosts, spirit-like entities, by imitating internet documents. This works. It’s a way to bring you up to something that has a lot of built-in knowledge and intelligence in some way, similar to maybe what evolution has done. That’s why I call pre-training this crappy evolution. It’s the practically possible version with our technology and what we have available to us to get to a starting point where we can do things like reinforcement learning and so on.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:12:15&lt;/p&gt;
    &lt;p&gt;Just to steelman the other perspective, after doing this Sutton interview and thinking about it a bit, he has an important point here. Evolution does not give us the knowledge, really. It gives us the algorithm to find the knowledge, and that seems different from pre-training.&lt;/p&gt;
    &lt;p&gt;Perhaps the perspective is that pre-training helps build the kind of entity which can learn better. It teaches meta-learning, and therefore it is similar to finding an algorithm. But if it’s “Evolution gives us knowledge, pre-training gives us knowledge,” that analogy seems to break down.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:12:42&lt;/p&gt;
    &lt;p&gt;It’s subtle and I think you’re right to push back on it, but basically the thing that pre-training is doing, you’re getting the next-token predictor over the internet, and you’re training that into a neural net. It’s doing two things that are unrelated. Number one, it’s picking up all this knowledge, as I call it. Number two, it’s actually becoming intelligent.&lt;/p&gt;
    &lt;p&gt;By observing the algorithmic patterns in the internet, it boots up all these little circuits and algorithms inside the neural net to do things like in-context learning and all this stuff. You don’t need or want the knowledge. I think that’s probably holding back the neural networks overall because it’s getting them to rely on the knowledge a little too much sometimes.&lt;/p&gt;
    &lt;p&gt;For example, I feel agents, one thing they’re not very good at, is going off the data manifold of what exists on the internet. If they had less knowledge or less memory, maybe they would be better. What I think we have to do going forward—and this would be part of the research paradigms—is figure out ways to remove some of the knowledge and to keep what I call this cognitive core. It’s this intelligent entity that is stripped from knowledge but contains the algorithms and contains the magic of intelligence and problem-solving and the strategies of it and all this stuff.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:13:50&lt;/p&gt;
    &lt;p&gt;There’s so much interesting stuff there. Let’s start with in-context learning. This is an obvious point, but I think it’s worth just saying it explicitly and meditating on it. The situation in which these models seem the most intelligent—in which I talk to them and I’m like, “Wow, there’s really something on the other end that’s responding to me thinking about things—is if it makes a mistake it’s like, “Oh wait, that’s the wrong way to think about it. I’m backing up.” All that is happening in context. That’s where I feel like the real intelligence is that you can visibly see.&lt;/p&gt;
    &lt;p&gt;That in-context learning process is developed by gradient descent on pre-training. It spontaneously meta-learns in-context learning, but the in-context learning itself is not gradient descent, in the same way that our lifetime intelligence as humans to be able to do things is conditioned by evolution but our learning during our lifetime is happening through some other process.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:14:42&lt;/p&gt;
    &lt;p&gt;I don’t fully agree with that, but you should continue your thought.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:14:44&lt;/p&gt;
    &lt;p&gt;Well, I’m very curious to understand how that analogy breaks down.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:14:48&lt;/p&gt;
    &lt;p&gt;I’m hesitant to say that in-context learning is not doing gradient descent. It’s not doing explicit gradient descent. In-context learning is pattern completion within a token window. It just turns out that there’s a huge amount of patterns on the internet. You’re right, the model learns to complete the pattern, and that’s inside the weights. The weights of the neural network are trying to discover patterns and complete the pattern. There’s some adaptation that happens inside the neural network, which is magical and just falls out from the internet just because there’s a lot of patterns.&lt;/p&gt;
    &lt;p&gt;I will say that there have been some papers that I thought were interesting that look at the mechanisms behind in-context learning. I do think it’s possible that in-context learning runs a small gradient descent loop internally in the layers of the neural network. I recall one paper in particular where they were doing linear regression using in-context learning. Your inputs into the neural network are XY pairs, XY, XY, XY that happen to be on the line. Then you do X and you expect Y. The neural network, when you train it in this way, does linear regression.&lt;/p&gt;
    &lt;p&gt;Normally when you would run linear regression, you have a small gradient descent optimizer that looks at XY, looks at an error, calculates the gradient of the weights and does the update a few times. It just turns out that when they looked at the weights of that in-context learning algorithm, they found some analogies to gradient descent mechanics. In fact, I think the paper was even stronger because they hardcoded the weights of a neural network to do gradient descent through attention and all the internals of the neural network.&lt;/p&gt;
    &lt;p&gt;That’s just my only pushback. Who knows how in-context learning works, but I think that it’s probably doing a bit of some funky gradient descent internally. I think that that’s possible. I was only pushing back on your saying that it’s not doing in-context learning. Who knows what it’s doing, but it’s probably maybe doing something similar to it, but we don’t know.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:16:39&lt;/p&gt;
    &lt;p&gt;So then it’s worth thinking okay, if in-context learning and pre-training are both implementing something like gradient descent, why does it feel like with in-context learning we’re getting to this continual learning, real intelligence-like thing? Whereas you don’t get the analogous feeling just from pre-training. You could argue that.&lt;/p&gt;
    &lt;p&gt;If it’s the same algorithm, what could be different? One way you could think about it is, how much information does the model store per information it receives from training? If you look at pre-training, if you look at Llama 3 for example, I think it’s trained on 15 trillion tokens. If you look at the 70B model, that would be the equivalent of 0.07 bits per token that it sees in pre-training, in terms of the information in the weights of the model compared to the tokens it reads. Whereas if you look at the KV cache and how it grows per additional token in in-context learning, it’s like 320 kilobytes. So that’s a 35 million-fold difference in how much information per token is assimilated by the model. I wonder if that’s relevant at all.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:17:46&lt;/p&gt;
    &lt;p&gt;I kind of agree. The way I usually put this is that anything that happens during the training of the neural network, the knowledge is only a hazy recollection of what happened in training time. That’s because the compression is dramatic. You’re taking 15 trillion tokens and you’re compressing it to just your final neural network of a few billion parameters. Obviously it’s a massive amount of compression going on. So I refer to it as a hazy recollection of the internet documents.&lt;/p&gt;
    &lt;p&gt;Whereas anything that happens in the context window of the neural network—you’re plugging in all the tokens and building up all those KV cache representations—is very directly accessible to the neural net. So I compare the KV cache and the stuff that happens at test time to more like a working memory. All the stuff that’s in the context window is very directly accessible to the neural net.&lt;/p&gt;
    &lt;p&gt;There’s always these almost surprising analogies between LLMs and humans. I find them surprising because we’re not trying to build a human brain directly. We’re just finding that this works and we’re doing it. But I do think that anything that’s in the weights, it’s a hazy recollection of what you read a year ago. Anything that you give it as a context at test time is directly in the working memory. That’s a very powerful analogy to think through things.&lt;/p&gt;
    &lt;p&gt;When you, for example, go to an LLM and you ask it about some book and what happened in it, like Nick Lane’s book or something like that, the LLM will often give you some stuff which is roughly correct. But if you give it the full chapter and ask it questions, you’re going to get much better results because it’s now loaded in the working memory of the model. So a very long way of saying I agree and that’s why.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:19:11&lt;/p&gt;
    &lt;p&gt;Stepping back, what is the part about human intelligence that we have most failed to replicate with these models?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:19:20&lt;/p&gt;
    &lt;p&gt;Just a lot of it. So maybe one way to think about it, I don’t know if this is the best way, but I almost feel like — again, making these analogies imperfect as they are — we’ve stumbled by with the transformer neural network, which is extremely powerful, very general. You can train transformers on audio, or video, or text, or whatever you want, and it just learns patterns and they’re very powerful, and it works really well. That to me almost indicates that this is some piece of cortical tissue. It’s something like that, because the cortex is famously very plastic as well. You can rewire parts of brains. There were the slightly gruesome experiments with rewiring the visual cortex to the auditory cortex, and this animal learned fine, et cetera.&lt;/p&gt;
    &lt;p&gt;So I think that this is cortical tissue. I think when we’re doing reasoning and planning inside the neural networks, doing reasoning traces for thinking models, that’s kind of like the prefrontal cortex. Maybe those are like little checkmarks, but I still think there are many brain parts and nuclei that are not explored. For example, there’s a basal ganglia doing a bit of reinforcement learning when we fine-tune the models on reinforcement learning. But where’s the hippocampus? Not obvious what that would be. Some parts are probably not important. Maybe the cerebellum is not important to cognition, its thoughts, so maybe we can skip some of it. But I still think there’s, for example, the amygdala, all the emotions and instincts. There’s probably a bunch of other nuclei in the brain that are very ancient that I don’t think we’ve really replicated.&lt;/p&gt;
    &lt;p&gt;I don’t know that we should be pursuing the building of an analog of a human brain. I’m an engineer mostly at heart. Maybe another way to answer the question is that you’re not going to hire this thing as an intern. It’s missing a lot of it because it comes with a lot of these cognitive deficits that we all intuitively feel when we talk to the models. So it’s not fully there yet. You can look at it as not all the brain parts are checked off yet.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:21:16&lt;/p&gt;
    &lt;p&gt;This is maybe relevant to the question of thinking about how fast these issues will be solved. Sometimes people will say about continual learning, “Look, you could easily replicate this capability. Just as in-context learning emerged spontaneously as a result of pre-training, continual learning over longer horizons will emerge spontaneously if the model is incentivized to recollect information over longer horizons, or horizons longer than one session.” So if there’s some outer loop RL which has many sessions within that outer loop, then this continual learning where it fine-tunes itself, or it writes to an external memory or something, will just emerge spontaneously. Do you think things like that are plausible? I just don’t have a prior over how plausible that is. How likely is that to happen?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:22:07&lt;/p&gt;
    &lt;p&gt;I don’t know that I fully resonate with that. These models, when you boot them up and they have zero tokens in the window, they’re always restarting from scratch where they were. So I don’t know in that worldview what it looks like. Maybe making some analogies to humans—just because I think it’s roughly concrete and interesting to think through—I feel like when I’m awake, I’m building up a context window of stuff that’s happening during the day. But when I go to sleep, something magical happens where I don’t think that context window stays around. There’s some process of distillation into the weights of my brain. This happens during sleep and all this stuff.&lt;/p&gt;
    &lt;p&gt;We don’t have an equivalent of that in large language models. That’s to me more adjacent to when you talk about continual learning and so on as absent. These models don’t really have a distillation phase of taking what happened, analyzing it obsessively, thinking through it, doing some synthetic data generation process and distilling it back into the weights, and maybe having a specific neural net per person. Maybe it’s a LoRA. It’s not a full-weight neural network. It’s just some small sparse subset of the weights that are changed.&lt;/p&gt;
    &lt;p&gt;But we do want to create ways of creating these individuals that have very long context. It’s not only remaining in the context window because the context windows grow very, very long. Maybe we have some very elaborate, sparse attention over it. But I still think that humans obviously have some process for distilling some of that knowledge into the weights. We’re missing it. I do also think that humans have some very elaborate, sparse attention scheme, which I think we’re starting to see some early hints of. DeepSeek v3.2 just came out and I saw that they have sparse attention as an example, and this is one way to have very, very long context windows. So I feel like we are redoing a lot of the cognitive tricks that evolution came up with through a very different process. But we’re going to converge on a similar architecture cognitively.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:24:02&lt;/p&gt;
    &lt;p&gt;In 10 years, do you think it’ll still be something like a transformer, but with much more modified attention and more sparse MLPs and so forth?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:24:10&lt;/p&gt;
    &lt;p&gt;The way I like to think about it is translation invariance in time. So 10 years ago, where were we? 2015. In 2015, we had convolutional neural networks primarily, residual networks just came out. So remarkably similar, I guess, but quite a bit different still. The transformer was not around. All these more modern tweaks on the transformer were not around. Maybe some of the things that we can bet on, I think in 10 years by translational equivariance, is that we’re still training giant neural networks with a forward backward pass and update through gradient descent, but maybe it looks a bit different, and it’s just that everything is much bigger.&lt;/p&gt;
    &lt;p&gt;Recently I went back all the way to 1989 which was a fun exercise for me, a few years ago, because I was reproducing Yann LeCun’s 1989 convolutional network, which was the first neural network I’m aware of trained via gradient descent, like modern neural network trained gradient descent on digit recognition. I was just interested in how I could modernize this. How much of this is algorithms? How much of this is data? How much of this progress is compute and systems? I was able to very quickly halve the learning just by time traveling by 33 years.&lt;/p&gt;
    &lt;p&gt;So if I time travel by algorithms 33 years, I could adjust what Yann LeCun did in 1989, and I could halve the error. But to get further gains, I had to add a lot more data, I had to 10x the training set, and then I had to add more computational optimizations. I had to train for much longer with dropout and other regularization techniques.&lt;/p&gt;
    &lt;p&gt;So all these things have to improve simultaneously. We’re probably going to have a lot more data, we’re probably going to have a lot better hardware, probably going to have a lot better kernels and software, we’re probably going to have better algorithms. All of those, it’s almost like no one of them is winning too much. All of them are surprisingly equal. This has been the trend for a while.&lt;/p&gt;
    &lt;p&gt;So to answer your question, I expect differences algorithmically to what’s happening today. But I do also expect that some of the things that have stuck around for a very long time will probably still be there. It’s probably still a giant neural network trained with gradient descent. That would be my guess.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:26:16&lt;/p&gt;
    &lt;p&gt;It’s surprising that all of those things together only halved the error, 30 years of progress…. Maybe half is a lot. Because if you halve the error, that actually means that…&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:26:30&lt;/p&gt;
    &lt;p&gt;Half is a lot. But I guess what was shocking to me is everything needs to improve across the board: architecture, optimizer, loss function. It also has improved across the board forever. So I expect all those changes to be alive and well.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:26:43&lt;/p&gt;
    &lt;p&gt;Yeah. I was about to ask you a very similar question about nanochat. Since you just coded it up recently, every single step in the process of building a chatbot is fresh in your RAM. I’m curious if you had similar thoughts about, “Oh, there was no one thing that was relevant to going from GPT-2 to nanochat.” What are some surprising takeaways from the experience?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:27:08&lt;/p&gt;
    &lt;p&gt;Of building nanochat? So nanochat is a repository I released. Was it yesterday or the day before? I can’t remember.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:27:15&lt;/p&gt;
    &lt;p&gt;We can see the sleep deprivation that went into the…&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:27:18&lt;/p&gt;
    &lt;p&gt;It’s trying to be the simplest complete repository that covers the whole pipeline end-to-end of building a ChatGPT clone. So you have all of the steps, not just any individual step, which is a bunch. I worked on all the individual steps in the past and released small pieces of code that show you how that’s done in an algorithmic sense, in simple code. But this handles the entire pipeline. In terms of learning, I don’t know that I necessarily found something that I learned from it. I already had in my mind how you build it. This is just the process of mechanically building it and making it clean enough so that people can learn from it and that they find it useful.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:28:04&lt;/p&gt;
    &lt;p&gt;What is the best way for somebody to learn from it? Is it to just delete all the code and try to reimplement from scratch, try to add modifications to it?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:28:10&lt;/p&gt;
    &lt;p&gt;That’s a great question. Basically it’s about 8,000 lines of code that takes you through the entire pipeline. I would probably put it on the right monitor. If you have two monitors, you put it on the right. You want to build it from scratch, you build it from the start. You’re not allowed to copy-paste, you’re allowed to reference, you’re not allowed to copy-paste. Maybe that’s how I would do it.&lt;/p&gt;
    &lt;p&gt;But I also think the repository by itself is a pretty large beast. When you write this code, you don’t go from top to bottom, you go from chunks and you grow the chunks, and that information is absent. You wouldn’t know where to start. So it’s not just a final repository that’s needed, it’s the building of the repository, which is a complicated chunk-growing process. So that part is not there yet. I would love to add that probably later this week. It’s probably a video or something like that. Roughly speaking, that’s what I would try to do. Build the stuff yourself, but don’t allow yourself copy-paste.&lt;/p&gt;
    &lt;p&gt;I do think that there’s two types of knowledge, almost. There’s the high-level surface knowledge, but when you build something from scratch, you’re forced to come to terms with what you don’t understand and you don’t know that you don’t understand it.&lt;/p&gt;
    &lt;p&gt;It always leads to a deeper understanding. It’s the only way to build. If I can’t build it, I don’t understand it. That’s a Feynman quote, I believe. I 100% have always believed this very strongly, because there are all these micro things that are just not properly arranged and you don’t really have the knowledge. You just think you have the knowledge. So don’t write blog posts, don’t do slides, don’t do any of that. Build the code, arrange it, get it to work. It’s the only way to go. Otherwise, you’re missing knowledge.&lt;/p&gt;
    &lt;head rend="h3"&gt;00:29:45 – LLM cognitive deficits&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 00:29:45&lt;/p&gt;
    &lt;p&gt;You tweeted out that coding models were of very little help to you in assembling this repository. I’m curious why that was.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:29:53&lt;/p&gt;
    &lt;p&gt;I guess I built the repository over a period of a bit more than a month. I would say there are three major classes of how people interact with code right now. Some people completely reject all of LLMs and they are just writing by scratch. This is probably not the right thing to do anymore.&lt;/p&gt;
    &lt;p&gt;The intermediate part, which is where I am, is you still write a lot of things from scratch, but you use the autocomplete that’s available now from these models. So when you start writing out a little piece of it, it will autocomplete for you and you can just tap through. Most of the time it’s correct, sometimes it’s not, and you edit it. But you’re still very much the architect of what you’re writing. Then there’s the vibe coding: “Hi, please implement this or that,” enter, and then let the model do it. That’s the agents.&lt;/p&gt;
    &lt;p&gt;I do feel like the agents work in very specific settings, and I would use them in specific settings. But these are all tools available to you and you have to learn what they’re good at, what they’re not good at, and when to use them. So the agents are pretty good, for example, if you’re doing boilerplate stuff. Boilerplate code that’s just copy-paste stuff, they’re very good at that. They’re very good at stuff that occurs very often on the Internet because there are lots of examples of it in the training sets of these models. There are features of things where the models will do very well.&lt;/p&gt;
    &lt;p&gt;I would say nanochat is not an example of those because it’s a fairly unique repository. There’s not that much code in the way that I’ve structured it. It’s not boilerplate code. It’s intellectually intense code almost, and everything has to be very precisely arranged. The models have so many cognitive deficits. One example, they kept misunderstanding the code because they have too much memory from all the typical ways of doing things on the Internet that I just wasn’t adopting. The models, for example—I don’t know if I want to get into the full details—but they kept thinking I’m writing normal code, and I’m not.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:31:49&lt;/p&gt;
    &lt;p&gt;Maybe one example?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:31:51&lt;/p&gt;
    &lt;p&gt;You have eight GPUs that are all doing forward, backwards. The way to synchronize gradients between them is to use a Distributed Data Parallel container of PyTorch, which automatically as you’re doing the backward, it will start communicating and synchronizing gradients. I didn’t use DDP because I didn’t want to use it, because it’s not necessary. I threw it out and wrote my own synchronization routine that’s inside the step of the optimizer. The models were trying to get me to use the DDP container. They were very concerned. This gets way too technical, but I wasn’t using that container because I don’t need it and I have a custom implementation of something like it.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:32:26&lt;/p&gt;
    &lt;p&gt;They just couldn’t internalize that you had your own.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:32:28&lt;/p&gt;
    &lt;p&gt;They couldn’t get past that. They kept trying to mess up the style. They’re way too over-defensive. They make all these try-catch statements. They keep trying to make a production code base, and I have a bunch of assumptions in my code, and it’s okay. I don’t need all this extra stuff in there. So I feel like they’re bloating the code base, bloating the complexity, they keep misunderstanding, they’re using deprecated APIs a bunch of times. It’s a total mess. It’s just not net useful. I can go in, I can clean it up, but it’s not net useful.&lt;/p&gt;
    &lt;p&gt;I also feel like it’s annoying to have to type out what I want in English because it’s too much typing. If I just navigate to the part of the code that I want, and I go where I know the code has to appear and I start typing out the first few letters, autocomplete gets it and just gives you the code. This is a very high information bandwidth to specify what you want. You point to the code where you want it, you type out the first few pieces, and the model will complete it.&lt;/p&gt;
    &lt;p&gt;So what I mean is, these models are good in certain parts of the stack. There are two examples where I use the models that I think are illustrative. One was when I generated the report. That’s more boilerplate-y, so I partially vibe-coded some of that stuff. That was fine because it’s not mission-critical stuff, and it works fine.&lt;/p&gt;
    &lt;p&gt;The other part is when I was rewriting the tokenizer in Rust. I’m not as good at Rust because I’m fairly new to Rust. So there’s a bit of vibe coding going on when I was writing some of the Rust code. But I had a Python implementation that I fully understand, and I’m just making sure I’m making a more efficient version of it, and I have tests so I feel safer doing that stuff. They increase accessibility to languages or paradigms that you might not be as familiar with. I think they’re very helpful there as well. There’s a ton of Rust code out there, the models are pretty good at it. I happen to not know that much about it, so the models are very useful there.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:34:23&lt;/p&gt;
    &lt;p&gt;The reason this question is so interesting is because the main story people have about AI exploding and getting to superintelligence pretty rapidly is AI automating AI engineering and AI research. They’ll look at the fact that you can have Claude Code and make entire applications, CRUD applications, from scratch and think, “If you had this same capability inside of OpenAI and DeepMind and everything, just imagine a thousand of you or a million of you in parallel, finding little architectural tweaks.”&lt;/p&gt;
    &lt;p&gt;It’s quite interesting to hear you say that this is the thing they’re asymmetrically worse at. It’s quite relevant to forecasting whether the AI 2027-type explosion is likely to happen anytime soon.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:35:05&lt;/p&gt;
    &lt;p&gt;That’s a good way of putting it, and you’re getting at why my timelines are a bit longer. You’re right. They’re not very good at code that has never been written before, maybe it’s one way to put it, which is what we’re trying to achieve when we’re building these models.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:35:19&lt;/p&gt;
    &lt;p&gt;Very naive question, but the architectural tweaks that you’re adding to nanochat, they’re in a paper somewhere, right? They might even be in a repo somewhere. Is it surprising that they aren’t able to integrate that into whenever you’re like, “Add RoPE embeddings” or something, they do that in the wrong way?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:35:42&lt;/p&gt;
    &lt;p&gt;It’s tough. They know, but they don’t fully know. They don’t know how to fully integrate it into the repo and your style and your code and your place, and some of the custom things that you’re doing and how it fits with all the assumptions of the repository. They do have some knowledge, but they haven’t gotten to the place where they can integrate it and make sense of it.&lt;/p&gt;
    &lt;p&gt;A lot of the stuff continues to improve. Currently, the state-of-the-art model that I go to is the GPT-5 Pro, and that’s a very powerful model. If I have 20 minutes, I will copy-paste my entire repo and I go to GPT-5 Pro, the oracle, for some questions. Often it’s not too bad and surprisingly good compared to what existed a year ago.&lt;/p&gt;
    &lt;p&gt;Overall, the models are not there. I feel like the industry is making too big of a jump and is trying to pretend like this is amazing, and it’s not. It’s slop. They’re not coming to terms with it, and maybe they’re trying to fundraise or something like that. I’m not sure what’s going on, but we’re at this intermediate stage. The models are amazing. They still need a lot of work. For now, autocomplete is my sweet spot. But sometimes, for some types of code, I will go to an LLM agent.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:36:53&lt;/p&gt;
    &lt;p&gt;Here’s another reason this is really interesting. Through the history of programming, there have been many productivity improvements—compilers, linting, better programming languages—which have increased programmer productivity but have not led to an explosion. That sounds very much like the autocomplete tab, and this other category is just automation of the programmer. It’s interesting you’re seeing more in the category of the historical analogies of better compilers or something.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:37:26&lt;/p&gt;
    &lt;p&gt;Maybe this gets to one other thought. I have a hard time differentiating where AI begins and stops because I see AI as fundamentally an extension of computing in a pretty fundamental way. I see a continuum of this recursive self-improvement or speeding up programmers all the way from the beginning: code editors, syntax highlighting, or checking even of the types, like data type checking—all these tools that we’ve built for each other.&lt;/p&gt;
    &lt;p&gt;Even search engines. Why aren’t search engines part of AI? Ranking is AI. At some point, Google, even early on, was thinking of themselves as an AI company doing Google Search engine, which is totally fair.&lt;/p&gt;
    &lt;p&gt;I see it as a lot more of a continuum than other people do, and it’s hard for me to draw the line. I feel like we’re now getting a much better autocomplete, and now we’re also getting some agents which are these loopy things, but they go off-rails sometimes. What’s going on is that the human is progressively doing a bit less and less of the low-level stuff. We’re not writing the assembly code because we have compilers. Compilers will take my high-level language in C and write the assembly code.&lt;/p&gt;
    &lt;p&gt;We’re abstracting ourselves very, very slowly. There’s this what I call “autonomy slider,” where more and more stuff is automated—of the stuff that can be automated at any point in time—and we’re doing a bit less and less and raising ourselves in the layer of abstraction over the automation.&lt;/p&gt;
    &lt;head rend="h3"&gt;00:40:05 – RL is terrible&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 00:40:05&lt;/p&gt;
    &lt;p&gt;Let’s talk about RL a bit. You tweeted some very interesting things about this. Conceptually, how should we think about the way that humans are able to build a rich world model just from interacting with our environment, and in ways that seem almost irrespective of the final reward at the end of the episode?&lt;/p&gt;
    &lt;p&gt;If somebody is starting a business, and at the end of 10 years, she finds out whether the business succeeded or failed, we say that she’s earned a bunch of wisdom and experience. But it’s not because the log probs of every single thing that happened over the last 10 years are up-weighted or down-weighted. Something much more deliberate and rich is happening. What is the ML analogy, and how does that compare to what we’re doing with LLMs right now?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:40:47&lt;/p&gt;
    &lt;p&gt;Maybe the way I would put it is that humans don’t use reinforcement learning, as I said. I think they do something different. Reinforcement learning is a lot worse than I think the average person thinks. Reinforcement learning is terrible. It just so happens that everything that we had before it is much worse because previously we were just imitating people, so it has all these issues.&lt;/p&gt;
    &lt;p&gt;In reinforcement learning, say you’re solving a math problem, because it’s very simple. You’re given a math problem and you’re trying to find the solution. In reinforcement learning, you will try lots of things in parallel first. You’re given a problem, you try hundreds of different attempts. These attempts can be complex. They can be like, “Oh, let me try this, let me try that, this didn’t work, that didn’t work,” etc. Then maybe you get an answer. Now you check the back of the book and you see, “Okay, the correct answer is this.” You can see that this one, this one, and that one got the correct answer, but these other 97 of them didn’t. Literally what reinforcement learning does is it goes to the ones that worked really well and every single thing you did along the way, every single token gets upweighted like, “Do more of this.”&lt;/p&gt;
    &lt;p&gt;The problem with that is people will say that your estimator has high variance, but it’s just noisy. It’s noisy. It almost assumes that every single little piece of the solution that you made that arrived at the right answer was the correct thing to do, which is not true. You may have gone down the wrong alleys until you arrived at the right solution. Every single one of those incorrect things you did, as long as you got to the correct solution, will be upweighted as, “Do more of this.” It’s terrible. It’s noise.&lt;/p&gt;
    &lt;p&gt;You’ve done all this work only to find, at the end, you get a single number of like, “Oh, you did correct.” Based on that, you weigh that entire trajectory as like, upweight or downweight. The way I like to put it is you’re sucking supervision through a straw. You’ve done all this work that could be a minute of rollout, and you’re sucking the bits of supervision of the final reward signal through a straw and you’re broadcasting that across the entire trajectory and using that to upweight or downweight that trajectory. It’s just stupid and crazy.&lt;/p&gt;
    &lt;p&gt;A human would never do this. Number one, a human would never do hundreds of rollouts. Number two, when a person finds a solution, they will have a pretty complicated process of review of, “Okay, I think these parts I did well, these parts I did not do that well. I should probably do this or that.” They think through things. There’s nothing in current LLMs that does this. There’s no equivalent of it. But I do see papers popping out that are trying to do this because it’s obvious to everyone in the field.&lt;/p&gt;
    &lt;p&gt;The first imitation learning, by the way, was extremely surprising and miraculous and amazing, that we can fine-tune by imitation on humans. That was incredible. Because in the beginning, all we had was base models. Base models are autocomplete. It wasn’t obvious to me at the time, and I had to learn this. The paper that blew my mind was InstructGPT, because it pointed out that you can take the pretrained model, which is autocomplete, and if you just fine-tune it on text that looks like conversations, the model will very rapidly adapt to become very conversational, and it keeps all the knowledge from pre-training. This blew my mind because I didn’t understand that stylistically, it can adjust so quickly and become an assistant to a user through just a few loops of fine-tuning on that kind of data. It was very miraculous to me that that worked. So incredible. That was two to three years of work.&lt;/p&gt;
    &lt;p&gt;Now came RL. And RL allows you to do a bit better than just imitation learning because you can have these reward functions and you can hill-climb on the reward functions. Some problems have just correct answers, you can hill-climb on that without getting expert trajectories to imitate. So that’s amazing. The model can also discover solutions that a human might never come up with. This is incredible. Yet, it’s still stupid.&lt;/p&gt;
    &lt;p&gt;We need more. I saw a paper from Google yesterday that tried to have this reflect &amp;amp; review idea in mind. Was it the memory bank paper or something? I don’t know. I’ve seen a few papers along these lines. So I expect there to be some major update to how we do algorithms for LLMs coming in that realm. I think we need three or four or five more, something like that.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:44:54&lt;/p&gt;
    &lt;p&gt;You’re so good at coming up with evocative phrases. “Sucking supervision through a straw.” It’s so good.&lt;/p&gt;
    &lt;p&gt;You’re saying the problem with outcome-based reward is that you have this huge trajectory, and then at the end, you’re trying to learn every single possible thing about what you should do and what you should learn about the world from that one final bit. Given the fact that this is obvious, why hasn’t process-based supervision as an alternative been a successful way to make models more capable? What has been preventing us from using this alternative paradigm?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:45:29&lt;/p&gt;
    &lt;p&gt;Process-based supervision just refers to the fact that we’re not going to have a reward function only at the very end. After you’ve done 10 minutes of work, I’m not going to tell you you did well or not well. I’m going to tell you at every single step of the way how well you’re doing. The reason we don’t have that is it’s tricky how you do that properly. You have partial solutions and you don’t know how to assign credit. So when you get the right answer, it’s just an equality match to the answer. It’s very simple to implement. If you’re doing process supervision, how do you assign in an automatable way, a partial credit assignment? It’s not obvious how you do it.&lt;/p&gt;
    &lt;p&gt;Lots of labs are trying to do it with these LLM judges. You get LLMs to try to do it. You prompt an LLM, “Hey, look at a partial solution of a student. How well do you think they’re doing if the answer is this?” and they try to tune the prompt.&lt;/p&gt;
    &lt;p&gt;The reason that this is tricky is quite subtle. It’s the fact that anytime you use an LLM to assign a reward, those LLMs are giant things with billions of parameters, and they’re gameable. If you’re reinforcement learning with respect to them, you will find adversarial examples for your LLM judges, almost guaranteed. So you can’t do this for too long. You do maybe 10 steps or 20 steps, and maybe it will work, but you can’t do 100 or 1,000. I understand it’s not obvious, but basically the model will find little cracks. It will find all these spurious things in the nooks and crannies of the giant model and find a way to cheat it.&lt;/p&gt;
    &lt;p&gt;One example that’s prominently in my mind, this was probably public, if you’re using an LLM judge for a reward, you just give it a solution from a student and ask it if the student did well or not. We were training with reinforcement learning against that reward function, and it worked really well. Then, suddenly, the reward became extremely large. It was a massive jump, and it did perfect. You’re looking at it like, “Wow, this means the student is perfect in all these problems. It’s fully solved math.”&lt;/p&gt;
    &lt;p&gt;But when you look at the completions that you’re getting from the model, they are complete nonsense. They start out okay, and then they change to “dhdhdhdh.” It’s just like, “Oh, okay, let’s take two plus three and we do this and this, and then dhdhdhdh.” You’re looking at it, and it’s like, this is crazy. How is it getting a reward of one or 100%? You look at the LLM judge, and it turns out that “dhdhdhdh” is an adversarial example for the model, and it assigns 100% probability to it.&lt;/p&gt;
    &lt;p&gt;It’s just because this is an out-of-sample example to the LLM. It’s never seen it during training, and you’re in pure generalization land. It’s never seen it during training, and in the pure generalization land, you can find these examples that break it.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:47:52&lt;/p&gt;
    &lt;p&gt;You’re basically training the LLM to be a prompt injection model.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:47:56&lt;/p&gt;
    &lt;p&gt;Not even that. Prompt injection is way too fancy. You’re finding adversarial examples, as they’re called. These are nonsensical solutions that are obviously wrong, but the model thinks they are amazing.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:48:07&lt;/p&gt;
    &lt;p&gt;To the extent you think this is the bottleneck to making RL more functional, then that will require making LLMs better judges, if you want to do this in an automated way. Is it just going to be some sort of GAN-like approach where you have to train models to be more robust?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:48:22&lt;/p&gt;
    &lt;p&gt;The labs are probably doing all that. The obvious thing is, “dhdhdhdh” should not get 100% reward. Okay, well, take “dhdhdhdh,” put it in the training set of the LLM judge, and say this is not 100%, this is 0%. You can do this, but every time you do this, you get a new LLM, and it still has adversarial examples. There’s an infinity of adversarial examples.&lt;/p&gt;
    &lt;p&gt;Probably if you iterate this a few times, it’ll probably be harder and harder to find adversarial examples, but I’m not 100% sure because this thing has a trillion parameters or whatnot. I bet you the labs are trying. I still think we need other ideas.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:48:57&lt;/p&gt;
    &lt;p&gt;Interesting. Do you have some shape of what the other idea could be?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:49:02&lt;/p&gt;
    &lt;p&gt;This idea of a review solution encompassing synthetic examples such that when you train on them, you get better, and meta-learn it in some way. I think there are some papers that I’m starting to see pop out. I am only at a stage of reading abstracts because a lot of these papers are just ideas. Someone has to make it work on a frontier LLM lab scale in full generality because when you see these papers, they pop up, and it’s just a bit noisy. They’re cool ideas, but I haven’t seen anyone convincingly show that this is possible. That said, the LLM labs are fairly closed, so who knows what they’re doing now.&lt;/p&gt;
    &lt;head rend="h3"&gt;00:49:38 – How do humans learn?&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 00:49:38&lt;/p&gt;
    &lt;p&gt;I can conceptualize how you would be able to train on synthetic examples or synthetic problems that you have made for yourself. But there seems to be another thing humans do—maybe sleep is this, maybe daydreaming is this—which is not necessarily to come up with fake problems, but just to reflect.&lt;/p&gt;
    &lt;p&gt;I’m not sure what the ML analogy is for daydreaming or sleeping, or just reflecting. I haven’t come up with a new problem. Obviously, the very basic analogy would just be fine-tuning on reflection bits, but I feel like in practice that probably wouldn’t work that well. Do you have some take on what the analogy of this thing is?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:50:17&lt;/p&gt;
    &lt;p&gt;I do think that we’re missing some aspects there. As an example, let’s take reading a book. Currently when LLMs are reading a book, what that means is we stretch out the sequence of text, and the model is predicting the next token, and it’s getting some knowledge from that. That’s not really what humans do. When you’re reading a book, I don’t even feel like the book is exposition I’m supposed to be attending to and training on. The book is a set of prompts for me to do synthetic data generation, or for you to get to a book club and talk about it with your friends. It’s by manipulating that information that you actually gain that knowledge. We have no equivalent of that with LLMs. They don’t really do that. I’d love to see during pre-training some stage that thinks through the material and tries to reconcile it with what it already knows, and thinks through it for some amount of time and gets that to work. There’s no equivalence of any of this. This is all research.&lt;/p&gt;
    &lt;p&gt;There are some subtle—very subtle that I think are very hard to understand—reasons why it’s not trivial. If I can just describe one: why can’t we just synthetically generate and train on it? Because every synthetic example, if I just give synthetic generation of the model thinking about a book, you look at it and you’re like, “This looks great. Why can’t I train on it?” You could try, but the model will get much worse if you continue trying. That’s because all of the samples you get from models are silently collapsed. Silently—it is not obvious if you look at any individual example of it—they occupy a very tiny manifold of the possible space of thoughts about content. The LLMs, when they come off, they’re what we call “collapsed.” They have a collapsed data distribution. One easy way to see it is to go to ChatGPT and ask it, “Tell me a joke.” It only has like three jokes. It’s not giving you the whole breadth of possible jokes. It knows like three jokes. They’re silently collapsed.&lt;/p&gt;
    &lt;p&gt;You’re not getting the richness and the diversity and the entropy from these models as you would get from humans. Humans are a lot noisier, but at least they’re not biased, in a statistical sense. They’re not silently collapsed. They maintain a huge amount of entropy. So how do you get synthetic data generation to work despite the collapse and while maintaining the entropy? That’s a research problem.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:52:20&lt;/p&gt;
    &lt;p&gt;Just to make sure I understood, the reason that the collapse is relevant to synthetic data generation is because you want to be able to come up with synthetic problems or reflections which are not already in your data distribution?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:52:32&lt;/p&gt;
    &lt;p&gt;I guess what I’m saying is, say we have a chapter of a book and I ask an LLM to think about it, it will give you something that looks very reasonable. But if I ask it 10 times, you’ll notice that all of them are the same.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:52:44&lt;/p&gt;
    &lt;p&gt;You can’t just keep scaling “reflection” on the same amount of prompt information and then get returns from that.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:52:54&lt;/p&gt;
    &lt;p&gt;Any individual sample will look okay, but the distribution of it is quite terrible. It’s quite terrible in such a way that if you continue training on too much of your own stuff, you actually collapse.&lt;/p&gt;
    &lt;p&gt;I think that there’s possibly no fundamental solution to this. I also think humans collapse over time. These analogies are surprisingly good. Humans collapse during the course of their lives. This is why children, they haven’t overfit yet. They will say stuff that will shock you because you can see where they’re coming from, but it’s just not the thing people say, because they’re not yet collapsed. But we’re collapsed. We end up revisiting the same thoughts. We end up saying more and more of the same stuff, and the learning rates go down, and the collapse continues to get worse, and then everything deteriorates.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:53:39&lt;/p&gt;
    &lt;p&gt;Have you seen this super interesting paper that dreaming is a way of preventing this kind of overfitting and collapse? The reason dreaming is evolutionary adaptive is to put you in weird situations that are very unlike your day-to-day reality, so as to prevent this kind of overfitting.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:53:55&lt;/p&gt;
    &lt;p&gt;It’s an interesting idea. I do think that when you’re generating things in your head and then you’re attending to it, you’re training on your own samples, you’re training on your synthetic data. If you do it for too long, you go off-rails and you collapse way too much. You always have to seek entropy in your life. Talking to other people is a great source of entropy, and things like that. So maybe the brain has also built some internal mechanisms for increasing the amount of entropy in that process. That’s an interesting idea.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:54:25&lt;/p&gt;
    &lt;p&gt;This is a very ill-formed thought so I’ll just put it out and let you react to it. The best learners that we are aware of, which are children, are extremely bad at recollecting information. In fact, at the very earliest stages of childhood, you will forget everything. You’re just an amnesiac about everything that happens before a certain year date. But you’re extremely good at picking up new languages and learning from the world. Maybe there’s some element of being able to see the forest for the trees.&lt;/p&gt;
    &lt;p&gt;Whereas if you compare it to the opposite end of the spectrum, you have LLM pre-training, where these models will literally be able to regurgitate word-for-word what is the next thing in a Wikipedia page. But their ability to learn abstract concepts really quickly, the way a child can, is much more limited. Then adults are somewhere in between, where they don’t have the flexibility of childhood learning, but they can memorize facts and information in a way that is harder for kids. I don’t know if there’s something interesting about that spectrum.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:55:19&lt;/p&gt;
    &lt;p&gt;I think there’s something very interesting about that, 100%. I do think that humans have a lot more of an element, compared to LLMs, of seeing the forest for the trees. We’re not actually that good at memorization, which is actually a feature. Because we’re not that good at memorization, we’re forced to find patterns in a more general sense.&lt;/p&gt;
    &lt;p&gt;LLMs in comparison are extremely good at memorization. They will recite passages from all these training sources. You can give them completely nonsensical data. You can hash some amount of text or something like that, you get a completely random sequence. If you train on it, even just for a single iteration or two, it can suddenly regurgitate the entire thing. It will memorize it. There’s no way a person can read a single sequence of random numbers and recite it to you.&lt;/p&gt;
    &lt;p&gt;That’s a feature, not a bug, because it forces you to only learn the generalizable components. Whereas LLMs are distracted by all the memory that they have of the pre-training documents, and it’s probably very distracting to them in a certain sense. So that’s why when I talk about the cognitive core, I want to remove the memory, which is what we talked about. I’d love to have them have less memory so that they have to look things up, and they only maintain the algorithms for thought, and the idea of an experiment, and all this cognitive glue of acting.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:56:36&lt;/p&gt;
    &lt;p&gt;And this is also relevant to preventing model collapse?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:56:41&lt;/p&gt;
    &lt;p&gt;Let me think. I’m not sure. It’s almost like a separate axis. The models are way too good at memorization, and somehow we should remove that. People are much worse, but it’s a good thing.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:56:57&lt;/p&gt;
    &lt;p&gt;What is a solution to model collapse? There are very naive things you could attempt. The distribution over logits should be wider or something. There are many naive things you could try. What ends up being the problem with the naive approaches?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:57:11&lt;/p&gt;
    &lt;p&gt;That’s a great question. You can imagine having a regularization for entropy and things like that. I guess they just don’t work as well empirically because right now the models are collapsed. But I will say most of the tasks that we want from them don’t actually demand diversity. That’s probably the answer to what’s going on.&lt;/p&gt;
    &lt;p&gt;The frontier labs are trying to make the models useful. I feel like the diversity of the outputs is not so much... Number one, it’s much harder to work with and evaluate and all this stuff, but maybe it’s not what’s capturing most of the value.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:57:42&lt;/p&gt;
    &lt;p&gt;In fact, it’s actively penalized. If you’re super creative in RL, it’s not good.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:57:48&lt;/p&gt;
    &lt;p&gt;Yeah. Or maybe if you’re doing a lot of writing, help from LLMs and stuff like that, it’s probably bad because the models will silently give you all the same stuff. They won’t explore lots of different ways of answering a question.&lt;/p&gt;
    &lt;p&gt;Maybe this diversity, not as many applications need it so the models don’t have it. But then it’s a problem at synthetic data generation time, et cetera. So we’re shooting ourselves in the foot by not allowing this entropy to maintain in the model. Possibly the labs should try harder.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:58:17&lt;/p&gt;
    &lt;p&gt;I think you hinted that it’s a very fundamental problem, it won’t be easy to solve. What’s your intuition for that?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:58:24&lt;/p&gt;
    &lt;p&gt;I don’t know if it’s super fundamental. I don’t know if I intended to say that. I do think that I haven’t done these experiments, but I do think that you could probably regularize the entropy to be higher. So you’re encouraging the model to give you more and more solutions, but you don’t want it to start deviating too much from the training data. It’s going to start making up its own language. It’s going to start using words that are extremely rare, so it’s going to drift too much from the distribution.&lt;/p&gt;
    &lt;p&gt;So I think controlling the distribution is just tricky. It’s probably not trivial in that sense.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:58:58&lt;/p&gt;
    &lt;p&gt;How many bits should the optimal core of intelligence end up being if you just had to make a guess? The thing we put on the von Neumann probes, how big does it have to be?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:59:10&lt;/p&gt;
    &lt;p&gt;It’s really interesting in the history of the field because at one point everything was very scaling-pilled in terms of like, “Oh, we’re gonna make much bigger models, trillions of parameter models.” What the models have done in size is they’ve gone up and now they’ve come down. State-of-the-art models are smaller. Even then, I think they memorized way too much. So I had a prediction a while back that I almost feel like we can get cognitive cores that are very good at even a billion parameters.&lt;/p&gt;
    &lt;p&gt;If you talk to a billion parameter model, I think in 20 years, you can have a very productive conversation. It thinks and it’s a lot more like a human. But if you ask it some factual question, it might have to look it up, but it knows that it doesn’t know and it might have to look it up and it will just do all the reasonable things.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:59:54&lt;/p&gt;
    &lt;p&gt;That’s surprising that you think it’ll take a billion parameters. Because already we have billion parameter models or a couple billion parameter models that are very intelligent.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:00:02&lt;/p&gt;
    &lt;p&gt;Well, state-of-the-art models are like a trillion parameters. But they remember so much stuff.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:00:06&lt;/p&gt;
    &lt;p&gt;Yeah, but I’m surprised that in 10 years, given the pace… We have gpt-oss-20b. That’s way better than GPT-4 original, which was a trillion plus parameters. Given that trend, I’m surprised you think in 10 years the cognitive core is still a billion parameters. I’m surprised you’re not like, “Oh it’s gonna be like tens of millions or millions.”&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:00:30&lt;/p&gt;
    &lt;p&gt;Here’s the issue, the training data is the internet, which is really terrible. There’s a huge amount of gains to be made because the internet is terrible. Even the internet, when you and I think of the internet, you’re thinking of like The Wall Street Journal. That’s not what this is. When you’re looking at a pre-training dataset in the frontier lab and you look at a random internet document, it’s total garbage. I don’t even know how this works at all. It’s some like stock tickers, symbols, it’s a huge amount of slop and garbage from like all the corners of the internet. It’s not like your Wall Street Journal article, that’s extremely rare. So because the internet is so terrible, we have to build really big models to compress all that. Most of that compression is memory work instead of cognitive work.&lt;/p&gt;
    &lt;p&gt;But what we really want is the cognitive part, delete the memory. I guess what I’m saying is that we need intelligent models to help us refine even the pre-training set to just narrow it down to the cognitive components. Then I think you get away with a much smaller model because it’s a much better dataset and you could train it on it. But probably it’s not trained directly on it, it’s probably distilled from a much better model still.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:01:35&lt;/p&gt;
    &lt;p&gt;But why is the distilled version still a billion?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:01:39&lt;/p&gt;
    &lt;p&gt;I just feel like distillation works extremely well. So almost every small model, if you have a small model, it’s almost certainly distilled.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:01:46&lt;/p&gt;
    &lt;p&gt;Right, but why is the distillation in 10 years not getting below 1 billion?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:01:50&lt;/p&gt;
    &lt;p&gt;Oh, you think it should be smaller than a billion? I mean, come on, right? I don’t know. At some point it should take at least a billion knobs to do something interesting. You’re thinking it should be even smaller?&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:02:01&lt;/p&gt;
    &lt;p&gt;Yeah. If you look at the trend over the last few years of just finding low-hanging fruit and going from trillion plus models to models that are literally two orders of magnitude smaller in a matter of two years and having better performance, it makes me think the sort of core of intelligence might be even way, way smaller. Plenty of room at the bottom, to paraphrase Feynman.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:02:22&lt;/p&gt;
    &lt;p&gt;I feel like I’m already contrarian by talking about a billion parameter cognitive core and you’re outdoing me. Maybe we could get a little bit smaller. I do think that practically speaking, you want the model to have some knowledge. You don’t want it to be looking up everything because then you can’t think in your head. You’re looking up way too much stuff all the time. Some basic curriculum needs to be there for knowledge, but it doesn’t have esoteric knowledge.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:02:48&lt;/p&gt;
    &lt;p&gt;We’re discussing what plausibly could be the cognitive core. There’s a separate question which is what will be the size of frontier models over time? I’m curious if you have predictions. We had increasing scale up to maybe GPT 4.5 and now we’re seeing decreasing or plateauing scale. There are many reasons this could be going on. Do you have a prediction going forward? Will the biggest models be bigger, will they be smaller, will they be the same?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:03:14&lt;/p&gt;
    &lt;p&gt;I don’t have a super strong prediction. The labs are just being practical. They have a flops budget and a cost budget. It just turns out that pre-training is not where you want to put most of your flops or your cost. That’s why the models have gotten smaller. They are a bit smaller, the pre-training stage is smaller, but they make it up in reinforcement learning, mid-training, and all this stuff that follows. They’re just being practical in terms of all the stages and how you get the most bang for the buck.&lt;/p&gt;
    &lt;p&gt;Forecasting that trend is quite hard. I do still expect that there’s so much low-hanging fruit. That’s my basic expectation. I have a very wide distribution here.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:03:51&lt;/p&gt;
    &lt;p&gt;Do you expect the low-hanging fruit to be similar in kind to the kinds of things that have been happening over the last two to five years? If I look at nanochat versus nanoGPT and the architectural tweaks you made, is that the flavor of things you expect to continue to keep happening? You’re not expecting any giant paradigm shifts.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:04:11&lt;/p&gt;
    &lt;p&gt;For the most part, yeah. I expect the datasets to get much, much better. When you look at the average datasets, they’re extremely terrible. They’re so bad that I don’t even know how anything works. Look at the average example in the training set: factual mistakes, errors, nonsensical things. Somehow when you do it at scale, the noise washes away and you’re left with some of the signal. Datasets will improve a ton.&lt;/p&gt;
    &lt;p&gt;Everything gets better. Our hardware, all the kernels for running the hardware and maximizing what you get with the hardware. Nvidia is slowly tuning the hardware itself, Tensor Cores, all that needs to happen and will continue to happen. All the kernels will get better and utilize the chip to the max extent. All the algorithms will probably improve over optimization, architecture, and all the modeling components of how everything is done and what the algorithms are that we’re even training with. I do expect that nothing dominates. Everything plus 20%. This is roughly what I’ve seen.&lt;/p&gt;
    &lt;head rend="h3"&gt;01:06:25 – AGI will blend into 2% GDP growth&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 01:06:25&lt;/p&gt;
    &lt;p&gt;People have proposed different ways of charting how much progress we’ve made towards full AGI. If you can come up with some line, then you can see where that line intersects with AGI and where that would happen on the x-axis. People have proposed it’s the education level. We had a high schooler, and then they went to college with RL, and they’re going to get a Ph.D.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:06:44&lt;/p&gt;
    &lt;p&gt;I don’t like that one.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:06:45&lt;/p&gt;
    &lt;p&gt;Or they’ll propose horizon length. Maybe they can do tasks that take a minute, they can do those autonomously. Then they can autonomously do tasks that take an hour, a human an hour, a human a week. How do you think about the relevant y-axis here? How should we think about how AI is making progress?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:07:05&lt;/p&gt;
    &lt;p&gt;I have two answers to that. Number one, I’m almost tempted to reject the question entirely because I see this as an extension of computing. Have we talked about how to chart progress in computing, or how do you chart progress in computing since the 1970s or whatever? What is the y-axis? The whole question is funny from that perspective a little bit.&lt;/p&gt;
    &lt;p&gt;When people talk about AI and the original AGI and how we spoke about it when OpenAI started, AGI was a system you could go to that can do any economically valuable task at human performance or better. That was the definition. I was pretty happy with that at the time. I’ve stuck to that definition forever, and then people have made up all kinds of other definitions. But I like that definition.&lt;/p&gt;
    &lt;p&gt;The first concession that people make all the time is they just take out all the physical stuff because we’re just talking about digital knowledge work. That’s a pretty major concession compared to the original definition, which was any task a human can do. I can lift things, etc. AI can’t do that, obviously, but we’ll take it. What fraction of the economy are we taking away by saying, “Oh, only knowledge work?” I don’t know the numbers. I feel about 10% to 20%, if I had to guess, is only knowledge work, someone could work from home and perform tasks, something like that. It’s still a really large market. What is the size of the economy, and what is 10% or 20%? We’re still talking about a few trillion dollars, even in the US, of market share or work. So it’s still a very massive bucket.&lt;/p&gt;
    &lt;p&gt;Going back to the definition, what I would be looking for is to what extent is that definition true? Are there jobs or lots of tasks? If we think of tasks as not jobs but tasks. It’s difficult because the problem is society will refactor based on the tasks that make up jobs, based on what’s automatable or not. Today, what jobs are replaceable by AI? A good example recently was Geoff Hinton’s prediction that radiologists would not be a job anymore, and this turned out to be very wrong in a bunch of ways. Radiologists are alive and well and growing, even though computer vision is really, really good at recognizing all the different things that they have to recognize in images. It’s just a messy, complicated job with a lot of surfaces and dealing with patients and all this stuff in the context of it.&lt;/p&gt;
    &lt;p&gt;I don’t know that by that definition AI has made a huge dent yet. Some of the jobs that I would be looking for have some features that make it very amenable to automation earlier than later. As an example, call center employees often come up, and I think rightly so. Call center employees have a number of simplifying properties with respect to what’s automatable today. Their jobs are pretty simple. It’s a sequence of tasks, and every task looks similar. You take a phone call with a person, it’s 10 minutes of interaction or whatever it is, probably a bit longer. In my experience, a lot longer. You complete some task in some scheme, and you change some database entries around or something like that. So you keep repeating something over and over again, and that’s your job.&lt;/p&gt;
    &lt;p&gt;You do want to bring in the task horizon—how long it takes to perform a task—and then you want to also remove context. You’re not dealing with different parts of services of companies or other customers. It’s just the database, you, and a person you’re serving. It’s more closed, it’s more understandable, it’s purely digital. So I would be looking for those things.&lt;/p&gt;
    &lt;p&gt;But even there, I’m not looking at full automation yet. I’m looking for an autonomy slider. I expect that we are not going to instantly replace people. We’re going to be swapping in AIs that do 80% of the volume. They delegate 20% of the volume to humans, and humans are supervising teams of five AIs doing the call center work that’s more rote. I would be looking for new interfaces or new companies that provide some layer that allows you to manage some of these AIs that are not yet perfect. Then I would expect that across the economy. A lot of jobs are a lot harder than a call center employee.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:11:02&lt;/p&gt;
    &lt;p&gt;With radiologists, I’m totally speculating and I have no idea what the actual workflow of a radiologist involves. But one analogy that might be applicable is when Waymos were first being rolled out, there’d be a person sitting in the front seat, and you just had to have them there to make sure that if something went really wrong, they’re there to monitor. Even today, people are still watching to make sure things are going well. Robotaxi, which was just deployed, still has a person inside it.&lt;/p&gt;
    &lt;p&gt;Now we could be in a similar situation where if you automate 99% of a job, that last 1% the human has to do is incredibly valuable because it’s bottlenecking everything else. If it were the case with radiologists, where the person sitting in the front of Waymo has to be specially trained for years in order to provide the last 1%, their wages should go up tremendously because they’re the one thing bottlenecking wide deployment. Radiologists, I think their wages have gone up for similar reasons, if you’re the last bottleneck and you’re not fungible. A Waymo driver might be fungible with others. So you might see this thing where your wages go up until you get to 99% and then fall just like that when the last 1% is gone. And I wonder if we’re seeing similar things with radiology or salaries of call center workers or anything like that.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:12:17&lt;/p&gt;
    &lt;p&gt;That’s an interesting question. I don’t think we’re currently seeing that with radiology. I think radiology is not a good example. I don’t know why Geoff Hinton picked on radiology because I think it’s an extremely messy, complicated profession.&lt;/p&gt;
    &lt;p&gt;I would be a lot more interested in what’s happening with call center employees today, for example, because I would expect a lot of the rote stuff to be automatable today. I don’t have first-level access to it but I would be looking for trends of what’s happening with the call center employees. Some of the things I would also expect is that maybe they are swapping in AI, but then I would still wait for a year or two because I would potentially expect them to pull back and rehire some of the people.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:13:00&lt;/p&gt;
    &lt;p&gt;There’s been evidence that that’s already been happening generally in companies that have been adopting AI, which I think is quite surprising.&lt;/p&gt;
    &lt;p&gt;I also found what was really surprising. AGI, right? A thing which would do everything. We’ll take out physical work, but it should be able to do all knowledge work. What you would have naively anticipated is that the way this progression would happen is that you take a little task that a consultant is doing, you take that out of the bucket. You take a little task that an accountant is doing, you take that out of the bucket. Then you’re just doing this across all knowledge work.&lt;/p&gt;
    &lt;p&gt;But instead, if we do believe we’re on the path of AGI with the current paradigm, the progression is very much not like that. It does not seem like consultants and accountants are getting huge productivity improvements. It’s very much like programmers are getting more and more chiseled away at their work. If you look at the revenues of these companies, discounting normal chat revenue—which is similar to Google or something—just looking at API revenues, it’s dominated by coding. So this thing which is “general”, which should be able to do any knowledge work, is just overwhelmingly doing only coding. It’s a surprising way that you would expect the AGI to be deployed.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:14:13&lt;/p&gt;
    &lt;p&gt;There’s an interesting point here. I do believe coding is the perfect first thing for these LLMs and agents. That’s because coding has always fundamentally worked around text. It’s computer terminals and text, and everything is based around text. LLMs, the way they’re trained on the Internet, love text. They’re perfect text processors, and there’s all this data out there. It’s a perfect fit.&lt;/p&gt;
    &lt;p&gt;We also have a lot of infrastructure pre-built for handling code and text. For example, we have Visual Studio Code or your favorite IDE showing you code, and an agent can plug into that. If an agent has a diff where it made some change, we suddenly have all this code already that shows all the differences to a code base using a diff. It’s almost like we’ve pre-built a lot of the infrastructure for code.&lt;/p&gt;
    &lt;p&gt;Contrast that with some of the things that don’t enjoy that at all. As an example, there are people trying to build automation not for coding, but for slides. I saw a company doing slides. That’s much, much harder. The reason it’s much harder is because slides are not text. Slides are little graphics, they’re arranged spatially, and there’s a visual component to it. Slides don’t have this pre-built infrastructure. For example, if an agent is to make a change to your slides, how does a thing show you the diff? How do you see the diff? There’s nothing that shows diffs for slides. Someone has to build it. Some of these things are not amenable to AIs as they are, which are text processors, and code surprisingly is.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:15:48&lt;/p&gt;
    &lt;p&gt;I’m not sure that alone explains it. I personally have tried to get LLMs to be useful in domains which are just pure language-in, language-out, like rewriting transcripts, coming up with clips based on transcripts. It’s very plausible that I didn’t do every single possible thing I could do. I put a bunch of good examples in context, but maybe I should have done some kind of fine-tuning.&lt;/p&gt;
    &lt;p&gt;Our mutual friend, Andy Matuschak, told me that he tried 50 billion things to try to get models to be good at writing spaced repetition prompts. Again, very much language-in, language-out tasks, the kind of thing that should be dead center in the repertoire of these LLMs. He tried in-context learning with a few-shot examples. He tried supervised fine-tuning and retrieval. He could not get them to make cards to his satisfaction.&lt;/p&gt;
    &lt;p&gt;So I find it striking that even in language-out domains, it’s very hard to get a lot of economic value out of these models separate from coding. I don’t know what explains it.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:16:57&lt;/p&gt;
    &lt;p&gt;That makes sense. I’m not saying that anything text is trivial. I do think that code is pretty structured. Text is maybe a lot more flowery, and there’s a lot more entropy in text, I would say. I don’t know how else to put it. Also code is hard, and so people feel quite empowered by LLMs, even from simple knowledge. I don’t know that I have a very good answer. Obviously, text makes it much, much easier, but it doesn’t mean that all text is trivial.&lt;/p&gt;
    &lt;head rend="h3"&gt;01:17:36 – ASI&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 01:17:36&lt;/p&gt;
    &lt;p&gt;How do you think about superintelligence? Do you expect it to feel qualitatively different from normal humans or human companies?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:17:45&lt;/p&gt;
    &lt;p&gt;I see it as a progression of automation in society. Extrapolating the trend of computing, there will be a gradual automation of a lot of things, and superintelligence will an extrapolation of that. We expect more and more autonomous entities over time that are doing a lot of the digital work and then eventually even the physical work some amount of time later. Basically I see it as just automation, roughly speaking.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:18:10&lt;/p&gt;
    &lt;p&gt;But automation includes the things humans can already do, and superintelligence implies things humans can’t do.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:18:16&lt;/p&gt;
    &lt;p&gt;But one of the things that people do is invent new things, which I would just put into the automation if that makes sense.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:18:20&lt;/p&gt;
    &lt;p&gt;But I guess, less abstractly and more qualitatively, do you expect something to feel like… Because this thing can either think so fast, or has so many copies, or the copies can merge back into themselves, or is much smarter, any number of advantages an AI might have, will the civilization in which these AIs exist will just feel qualitatively different from humans?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:18:51&lt;/p&gt;
    &lt;p&gt;I think it will. It is fundamentally automation, but it will be extremely foreign. It will look really strange. Like you mentioned, we can run all of this on a computer cluster and much faster.&lt;/p&gt;
    &lt;p&gt;Some of the scenarios that I start to get nervous about when the world looks like that is this gradual loss of control and understanding of what’s happening. I think that’s the most likely outcome, that there will be a gradual loss of understanding. We’ll gradually layer all this stuff everywhere, and there will be fewer and fewer people who understand it. Then there will be a gradual loss of control and understanding of what’s happening. That to me seems the most likely outcome of how all this stuff will go down.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:19:31&lt;/p&gt;
    &lt;p&gt;Let me probe on that a bit. It’s not clear to me that loss of control and loss of understanding are the same things. A board of directors at TSMC, Intel—name a random company—they’re just prestigious 80-year-olds. They have very little understanding, and maybe they don’t practically actually have control.&lt;/p&gt;
    &lt;p&gt;A better example is the President of the United States. The President has a lot of fucking power. I’m not trying to make a good statement about the current operant, or maybe I am, but the actual level of understanding is very different from the level of control.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:20:06&lt;/p&gt;
    &lt;p&gt;I think that’s fair. That’s a good pushback. I think I expect loss of both.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:20:15&lt;/p&gt;
    &lt;p&gt;How come? Loss of understanding is obvious, but why loss of control?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:20:20&lt;/p&gt;
    &lt;p&gt;We’re really far into a territory where I don’t know what this looks like, but if I were to write sci-fi novels, they would look along the lines of not even a single entity that takes over everything, but multiple competing entities that gradually become more and more autonomous. Some of them go rogue and the others fight them off. It’s this hot pot of completely autonomous activity that we’ve delegated to. I feel it would have that flavor.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:20:52&lt;/p&gt;
    &lt;p&gt;It is not the fact that they are smarter than us that is resulting in the loss of control. It’s the fact that they are competing with each other, and whatever arises out of that competition leads to the loss of control.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:21:06&lt;/p&gt;
    &lt;p&gt;A lot of these things, they will be tools to people, they’re acting on behalf of people or something like that. So maybe those people are in control, but maybe it’s a loss of control overall for society in the sense of outcomes we want. You have entities acting on behalf of individuals that are still roughly seen as out of control.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:21:30&lt;/p&gt;
    &lt;p&gt;This is a question I should have asked earlier. We were talking about how currently it feels like when you’re doing AI engineering or AI research, these models are more in the category of compiler rather than in the category of a replacement.&lt;/p&gt;
    &lt;p&gt;At some point, if you have AGI, it should be able to do what you do. Do you feel like having a million copies of you in parallel results in some huge speed-up of AI progress? If that does happen, do you expect to see an intelligence explosion once we have a true AGI? I’m not talking about LLMs today.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:22:01&lt;/p&gt;
    &lt;p&gt;I do, but it’s business as usual because we’re in an intelligence explosion already and have been for decades. It’s basically the GDP curve that is an exponential weighted sum over so many aspects of the industry. Everything is gradually being automated and has been for hundreds of years. The Industrial Revolution is automation and some of the physical components and tool building and all this stuff. Compilers are early software automation, et cetera. We’ve been recursively self-improving and exploding for a long time.&lt;/p&gt;
    &lt;p&gt;Another way to see it is that Earth was a pretty boring place if you don’t look at the biomechanics and so on, and looked very similar. If you look from space, we’re in the middle of this firecracker event, but we’re seeing it in slow motion. I definitely feel like this has already happened for a very long time. Again, I don’t see AI as a distinct technology with respect to what has already been happening for a long time.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:23:00&lt;/p&gt;
    &lt;p&gt;You think it’s continuous with this hyper-exponential trend?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:23:03&lt;/p&gt;
    &lt;p&gt;Yes. That’s why this was very interesting to me, because I was trying to find AI in the GDP for a while. I thought that GDP should go up. But then I looked at some of the other technologies that I thought were very transformative, like computers or mobile phones or et cetera. You can’t find them in GDP. GDP is the same exponential.&lt;/p&gt;
    &lt;p&gt;Even the early iPhone didn’t have the App Store, and it didn’t have a lot of the bells and whistles that the modern iPhone has. So even though we think of 2008, when the iPhone came out, as this major seismic change, it’s actually not. Everything is so spread out and it so slowly diffuses that everything ends up being averaged up into the same exponential. It’s the exact same thing with computers. You can’t find them in the GDP like, “Oh, we have computers now.” That’s not what happened, because it’s such slow progression.&lt;/p&gt;
    &lt;p&gt;With AI we’re going to see the exact same thing. It’s just more automation. It allows us to write different kinds of programs that we couldn’t write before, but AI is still fundamentally a program. It’s a new kind of computer and a new kind of computing system. But it has all these problems, it’s going to diffuse over time, and it’s still going to add up to the same exponential. We’re still going to have an exponential that’s going to get extremely vertical. It’s going to be very foreign to live in that kind of an environment.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:24:10&lt;/p&gt;
    &lt;p&gt;Are you saying that, if you look at the trend before the Industrial Revolution to now, you have a hyper-exponential where you go from 0% growth to then 10,000 years ago, 0.02% growth, and to now when we’re at 2% growth. That’s a hyper-exponential. Are you saying if you’re charting AI on there, then AI takes you to 20% growth or 200% growth?&lt;/p&gt;
    &lt;p&gt;Or are you saying that if you look at the last 300 years, what you’ve been seeing is that you have technology after technology—computers, electrification, steam engines, railways, et cetera—but the rate of growth is the exact same, it’s 2%. Are you saying the rate of growth will go up?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:24:46&lt;/p&gt;
    &lt;p&gt;The rate of growth has also stayed roughly constant, right?&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:24:49&lt;/p&gt;
    &lt;p&gt;Only over the last 200, 300 years. But over the course of human history it’s exploded. It’s gone from 0% to faster, faster, faster. Industrial explosion, 2%.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:25:01&lt;/p&gt;
    &lt;p&gt;For a while I tried to find AI or look for AI in the GDP curve, and I’ve convinced myself that this is false. Even when people talk about recursive self-improvement and labs and stuff like that, this is business as usual. Of course it’s going to recursively self-improve, and it’s been recursively self-improving.&lt;/p&gt;
    &lt;p&gt;LLMs allow the engineers to work much more efficiently to build the next round of LLM, and a lot more of the components are being automated and tuned and et cetera. All the engineers having access to Google Search is part of it. All the engineers having an IDE, all of them having autocomplete or having Claude code, et cetera, it’s all just part of the same speed-up of the whole thing. It’s just so smooth.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:25:41&lt;/p&gt;
    &lt;p&gt;Just to clarify, you’re saying that the rate of growth will not change. The intelligence explosion will show up as it just enabled us to continue staying on the 2% growth trajectory, just as the Internet helped us stay on the 2% growth trajectory.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:25:53&lt;/p&gt;
    &lt;p&gt;Yes, my expectation is that it stays in the same pattern.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:25:58&lt;/p&gt;
    &lt;p&gt;Just to throw the opposite argument against you, my expectation is that it blows up because I think true AGI—and I’m not talking about LLM coding bots, I’m talking about actual replacement of a human in a server—is qualitatively different from these other productivity-improving technologies because it’s labor itself.&lt;/p&gt;
    &lt;p&gt;I think we live in a very labor-constrained world. If you talk to any startup founder or any person, you can be like, what do you need more of? You need really talented people. And if you have billions of extra people who are inventing stuff, integrating themselves, making companies bottom start to finish, that feels qualitatively different from a single technology. It’s as if you get 10 billion extra people on the planet.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:26:44&lt;/p&gt;
    &lt;p&gt;Maybe a counterpoint. I’m pretty willing to be convinced one way or another on this point. But I will say, for example, computing is labor. Computing was labor. Computers, a lot of jobs disappeared because computers are automating a bunch of digital information processing that you now don’t need a human for. So computers are labor, and that has played out.&lt;/p&gt;
    &lt;p&gt;Self-driving as an example is also computers doing labor. That’s already been playing out. It’s still business as usual.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:27:13&lt;/p&gt;
    &lt;p&gt;You have a machine which is spitting out more things like that at potentially faster pace. Historically, we have examples of the growth regime changing where you went from 0.2% growth to 2% growth. It seems very plausible to me that a machine which is then spitting out the next self-driving car and the next Internet and whatever…&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:27:33&lt;/p&gt;
    &lt;p&gt;I see where it’s coming from. At the same time, I do feel like people make this assumption of, “We have God in a box, and now it can do everything,” and it just won’t look like that. It’s going to be able to do some of the things. It’s going to fail at some other things. It’s going to be gradually put into society, and we’ll end up with the same pattern. That is my prediction.&lt;/p&gt;
    &lt;p&gt;This assumption of suddenly having a completely intelligent, fully flexible, fully general human in a box, and we can dispense it at arbitrary problems in society, I don’t think that we will have this discrete change. I think we’ll arrive at the same kind of gradual diffusion of this across the industry.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:28:14&lt;/p&gt;
    &lt;p&gt;It often ends up being misleading in these conversations. I don’t like to use the word intelligence in this context because intelligence implies you think there’ll be a single superintelligence sitting in a server and it’ll divine how to come up with new technologies and inventions that cause this explosion. That’s not what I’m imagining when I’m imagining 20% growth. I’m imagining that there are billions of very smart human-like minds, potentially, or that’s all that’s required.&lt;/p&gt;
    &lt;p&gt;But the fact that there’s hundreds of millions of them, billions of them, each individually making new products, figuring out how to integrate themselves into the economy. If a highly experienced smart immigrant came to the country, you wouldn’t need to figure out how we integrate them in the economy. They figure it out. They could start a company, they could make inventions, or increase productivity in the world.&lt;/p&gt;
    &lt;p&gt;We have examples, even in the current regime, of places that have had 10-20% economic growth. If you just have a lot of people and less capital in comparison to the people, you can have Hong Kong or Shenzhen or whatever with decades of 10% plus growth. There’s a lot of really smart people who are ready to make use of the resources and do this period of catch-up because we’ve had this discontinuity, and I think AI might be similar.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:29:33&lt;/p&gt;
    &lt;p&gt;I understand, but I still think that you’re presupposing some discrete jump. There’s some unlock that we’re waiting to claim. And suddenly we’re going to have geniuses in data centers. I still think you’re presupposing some discrete jump that has no historical precedent that I can’t find in any of the statistics and that I think probably won’t happen.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:29:52&lt;/p&gt;
    &lt;p&gt;I mean, the Industrial Revolution is such a jump. You went from 0.2% growth to 2% growth. I’m just saying you’ll see another jump like that.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:30:00&lt;/p&gt;
    &lt;p&gt;I’m a little bit suspicious, I would have to take a look. For example, some of the logs are not very good from before the Industrial Revolution. I’m a bit suspicious of it but I don’t have strong opinions. You’re saying that this was a singular event that was extremely magical. You’re saying that maybe there’s going to be another event that’s going to be just like that, extremely magical. It will break the paradigm, and so on.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:30:23&lt;/p&gt;
    &lt;p&gt;I actually don’t think… The crucial thing with the Industrial Revolution was that it was not magical. If you just zoomed in, what you would see in 1770 or 1870 is not that there was some key invention. But at the same time, you did move the economy to a regime where the progress was much faster and the exponential 10x’d. I expect a similar thing from AI where it’s not like there’s going to be a single moment where we’ve made the crucial invention.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:30:51&lt;/p&gt;
    &lt;p&gt;It’s an overhang that’s being unlocked. Like maybe there’s a new energy source. There’s some unlock—in this case, some kind of a cognitive capacity—and there’s an overhang of cognitive work to do.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:31:02&lt;/p&gt;
    &lt;p&gt;That’s right.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:31:03&lt;/p&gt;
    &lt;p&gt;You’re expecting that overhang to be filled by this new technology when it crosses the threshold.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:31:06&lt;/p&gt;
    &lt;p&gt;Maybe one way to think about it is throughout history, a lot of growth comes because people come up with ideas, and then people are out there doing stuff to execute those ideas and make valuable output. Through most of this time, the population has been exploding. That has been driving growth.&lt;/p&gt;
    &lt;p&gt;For the last 50 years, people have argued that growth has stagnated. The population in frontier countries has also stagnated. I think we go back to the exponential growth in population that causes hyper-exponential growth in output.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:31:37&lt;/p&gt;
    &lt;p&gt;It’s really hard to tell. I understand that viewpoint. I don’t intuitively feel that viewpoint.&lt;/p&gt;
    &lt;head rend="h3"&gt;01:32:50 – Evolution of intelligence &amp;amp; culture&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 01:32:50&lt;/p&gt;
    &lt;p&gt;You recommended Nick Lane’s book to me. On that basis, I also found it super interesting and I interviewed him. I have some questions about thinking about intelligence and evolutionary history.&lt;/p&gt;
    &lt;p&gt;Now that you, over the last 20 years of doing AI research, you maybe have a more tangible sense of what intelligence is, what it takes to develop it. Are you more or less surprised as a result that evolution just spontaneously stumbled upon it?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:33:19&lt;/p&gt;
    &lt;p&gt;I love Nick Lane’s books. I was just listening to his podcast on the way up here. With respect to intelligence and its evolution, it’s very, very recent. I am surprised that it evolved.&lt;/p&gt;
    &lt;p&gt;I find it fascinating to think about all the worlds out there. Say there’s a thousand planets like Earth and what they look like. I think Nick Lane was here talking about some of the earliest parts. He expects very similar life forms, roughly speaking, and bacteria-like things in most of them. There are a few breaks in there. The evolution of intelligence intuitively feels to me like it should be a fairly rare event.&lt;/p&gt;
    &lt;p&gt;Maybe you should base it on how long something has existed. If bacteria were around for 2 billion years and nothing happened, then going to eukaryote is probably pretty hard because bacteria came up quite early in Earth’s evolution or history. How long have we had animals? Maybe a couple hundred million years, multicellular animals that run around, crawl, et cetera. That’s maybe 10% of Earth’s lifespan. Maybe on that timescale it’s not too tricky. It’s still surprising to me, intuitively, that it developed. I would maybe expect just a lot of animal-like life forms doing animal-like things. The fact that you can get something that creates culture and knowledge and accumulates it is surprising to me.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:34:42&lt;/p&gt;
    &lt;p&gt;There’s a couple of interesting follow-ups. If you buy the Sutton perspective that the crux of intelligence is animal intelligence… The quote he said is “If you got to the squirrel, you’d be most of the way to AGI.”&lt;/p&gt;
    &lt;p&gt;We got to squirrel intelligence right after the Cambrian explosion 600 million years ago. It seems like what instigated that was the oxygenation event 600 million years ago. But immediately the intelligence algorithm was there to make the squirrel intelligence. It’s suggestive that animal intelligence was like that. As soon as you had the oxygen in the environment, you had the eukaryote, you could just get the algorithm. Maybe it was an accident that evolution stumbled upon it so fast, but I don’t know if that suggests that at the end it’s going to be quite simple.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:35:31&lt;/p&gt;
    &lt;p&gt;It’s so hard to tell with any of this stuff. You can base it a bit on how long something has existed or how long it feels like something has been bottlenecked. Nick Lane is very good about describing this very apparent bottleneck in bacteria and archaea. For two billion years, nothing happened. There’s extreme diversity of biochemistry, and yet nothing grows to become animals. Two billion years.&lt;/p&gt;
    &lt;p&gt;I don’t know that we’ve seen exactly that kind of an equivalent with animals and intelligence, to your point. We could also look at it with respect to how many times we think certain intelligence has individually sprung up.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:36:07&lt;/p&gt;
    &lt;p&gt;That’s a really good thing to investigate.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:36:09&lt;/p&gt;
    &lt;p&gt;One thought on that. There’s hominid intelligence, and then there’s bird intelligence. Ravens, etc., are extremely clever, but their brain parts are quite distinct, and we don’t have that much in common. That’s a slight indication of maybe intelligence springing up a few times. In that case, you’d expect it more frequently.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:36:32&lt;/p&gt;
    &lt;p&gt;A former guest, Gwern, and Carl Shulman, they’ve made a really interesting point about that. Their perspective is that the scalable algorithm which humans have and primates have, arose in birds as well, and maybe other times as well. But humans found an evolutionary niche which rewarded marginal increases in intelligence and also had a scalable brain algorithm that could achieve those increases in intelligence.&lt;/p&gt;
    &lt;p&gt;For example, if a bird had a bigger brain, it would just collapse out of the air. It’s very smart for the size of its brain, but it’s not in a niche which rewards the brain getting bigger. It’s maybe similar to some really smart…&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy&lt;/p&gt;
    &lt;p&gt;Like dolphins?&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel&lt;/p&gt;
    &lt;p&gt;Exaclty, humans, we have hands that reward being able to learn how to do tool use. We can externalize digestion, more energy to the brain, and that kicks off the flywheel.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:37:28&lt;/p&gt;
    &lt;p&gt;Also stuff to work with. I’m guessing it would be harder if I were a dolphin. How do you have fire? The universe of things you can do in water, inside water, is probably lower than what you can do on land, just chemically.&lt;/p&gt;
    &lt;p&gt;I do agree with this viewpoint of these niches and what’s being incentivized. I still find it miraculous. I would have expected things to get stuck on animals with bigger muscles. Going through intelligence is a really fascinating breaking point.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:38:02&lt;/p&gt;
    &lt;p&gt;The way Gwern put it is the reason it was so hard is that it’s a very tight line between being in a situation where something is so important to learn that it’s not worth distilling the exact right circuits directly back into your DNA, versus it’s not important enough to learn at all. It has to be something that incentivizes building the algorithm to learn in a lifetime.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:38:28&lt;/p&gt;
    &lt;p&gt;You have to incentivize some kind of adaptability. You want environments that are unpredictable so evolution can’t bake your algorithms into your weights. A lot of animals are pre-baked in this sense. Humans have to figure it out at test time when they get born. You want these environments that change really rapidly, where you can’t foresee what will work well. You create intelligence to figure it out at test time.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:38:55&lt;/p&gt;
    &lt;p&gt;Quintin Pope had this interesting blog post where he’s saying the reason he doesn’t expect a sharp takeoff is that humans had the sharp takeoff where 60,000 years ago we seem to have had the cognitive architectures that we have today. 10,000 years ago, agricultural revolution, modernity. What was happening in that 50,000 years? You had to build this cultural scaffold where you can accumulate knowledge over generations.&lt;/p&gt;
    &lt;p&gt;This is an ability that exists for free in the way we do AI training. In many cases they are literally distilled. If you retrain a model, they can be trained on each other, they can be trained on the same pre-training corpus, they don’t literally have to start from scratch. There’s a sense in which it took humans a long time to get this cultural loop going, but it just comes for free with the way we do LLM training.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:39:45&lt;/p&gt;
    &lt;p&gt;Yes and no. Because LLMs don’t really have the equivalent of culture. Maybe we’re giving them way too much and incentivizing not to create it or something like that. But the invention of culture and of written record and of passing down notes between each other, I don’t think there’s an equivalent of that with LLMs right now. LLMs don’t really have culture right now and it’s one of the impediments I would say.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:40:05&lt;/p&gt;
    &lt;p&gt;Can you give me some sense of what LLM culture might look like?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:40:09&lt;/p&gt;
    &lt;p&gt;In the simplest case it would be a giant scratchpad that the LLM can edit and as it’s reading stuff or as it’s helping out with work, it’s editing the scratchpad for itself. Why can’t an LLM write a book for the other LLMs? That would be cool. Why can’t other LLMs read this LLM’s book and be inspired by it or shocked by it or something like that? There’s no equivalence for any of this stuff.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:40:29&lt;/p&gt;
    &lt;p&gt;Interesting. When would you expect that kind of thing to start happening? Also, multi-agent systems and a sort of independent AI civilization and culture?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:40:40&lt;/p&gt;
    &lt;p&gt;There are two powerful ideas in the realm of multi-agent that have both not been really claimed or so on. The first one I would say is culture and LLMs having a growing repertoire of knowledge for their own purposes.&lt;/p&gt;
    &lt;p&gt;The second one looks a lot more like the powerful idea of self-play. In my mind it’s extremely powerful. Evolution has a lot of competition driving intelligence and evolution. In AlphaGo more algorithmically, AlphaGo is playing against itself and that’s how it learns to get really good at Go. There’s no equivalent of self-playing LLMs, but I would expect that to also exist. No one has done it yet. Why can’t an LLM for example, create a bunch of problems that another LLM is learning to solve? Then the LLM is always trying to serve more and more difficult problems, stuff like that.&lt;/p&gt;
    &lt;p&gt;There’s a bunch of ways to organize it. It’s a realm of research, but I haven’t seen anything that convincingly claims both of those multi-agent improvements. We’re mostly in the realm of a single individual agent, but that will change. In the realm of culture also, I would also bucket organizations. We haven’t seen anything like that convincingly either. That’s why we’re still early.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:41:53&lt;/p&gt;
    &lt;p&gt;Can you identify the key bottleneck that’s preventing this kind of collaboration between LLMs?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:41:59&lt;/p&gt;
    &lt;p&gt;Maybe the way I would put it is, some of these analogies work and they shouldn’t, but somehow, remarkably, they do. A lot of the smaller models, or the dumber models, remarkably resemble a kindergarten student, or an elementary school student or high school student. Somehow, we still haven’t graduated enough where this stuff can take over. My Claude Code or Codex, they still feel like this elementary-grade student. I know that they can take PhD quizzes, but they still cognitively feel like a kindergarten or an elementary school student.&lt;/p&gt;
    &lt;p&gt;I don’t think they can create culture because they’re still kids. They’re savant kids. They have perfect memory of all this stuff. They can convincingly create all kinds of slop that looks really good. But I still think they don’t really know what they’re doing and they don’t really have the cognition across all these little checkboxes that we still have to collect.&lt;/p&gt;
    &lt;head rend="h3"&gt;01:42:55 - Why self driving took so long&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 01:42:55&lt;/p&gt;
    &lt;p&gt;You’ve talked about how you were at Tesla leading self-driving from 2017 to 2022. And you firsthand saw this progress from cool demos to now thousands of cars out there actually autonomously doing drives. Why did that take a decade? What was happening through that time?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:43:11&lt;/p&gt;
    &lt;p&gt;One thing I will almost instantly push back on is that this is not even near done, in a bunch of ways that I’m going to get to. Self-driving is very interesting because it’s definitely where I get a lot of my intuitions because I spent five years on it. It has this entire history where the first demos of self-driving go all the way to the 1980s. You can see a demo from CMU in 1986. There’s a truck that’s driving itself on roads.&lt;/p&gt;
    &lt;p&gt;Fast forward. When I was joining Tesla, I had a very early demo of Waymo. It basically gave me a perfect drive in 2014 or something like that, so a perfect Waymo drive a decade ago. It took us around Palo Alto and so on because I had a friend who worked there. I thought it was very close and then it still took a long time.&lt;/p&gt;
    &lt;p&gt;For some kinds of tasks and jobs and so on, there’s a very large demo-to-product gap where the demo is very easy, but the product is very hard. It’s especially the case in cases like self-driving where the cost of failure is too high. Many industries, tasks, and jobs maybe don’t have that property, but when you do have that property, that definitely increases the timelines.&lt;/p&gt;
    &lt;p&gt;For example, in software engineering, I do think that property does exist. For a lot of vibe coding, it doesn’t. But if you’re writing actual production-grade code, that property should exist, because any kind of mistake leads to a security vulnerability or something like that. Millions and hundreds of millions of people’s personal Social Security numbers get leaked or something like that. So in software, people should be careful, kind of like in self-driving. In self-driving, if things go wrong, you might get injured. There are worse outcomes. But in software, it’s almost unbounded how terrible something could be.&lt;/p&gt;
    &lt;p&gt;I do think that they share that property. What takes the long amount of time and the way to think about it is that it’s a march of nines. Every single nine is a constant amount of work. Every single nine is the same amount of work. When you get a demo and something works 90% of the time, that’s just the first nine. Then you need the second nine, a third nine, a fourth nine, a fifth nine. While I was at Tesla for five years or so, we went through maybe three nines or two nines. I don’t know what it is, but multiple nines of iteration. There are still more nines to go.&lt;/p&gt;
    &lt;p&gt;That’s why these things take so long. It’s definitely formative for me, seeing something that was a demo. I’m very unimpressed by demos. Whenever I see demos of anything, I’m extremely unimpressed by that. If it’s a demo that someone cooked up as a showing, it’s worse. If you can interact with it, it’s a bit better. But even then, you’re not done. You need the actual product. It’s going to face all these challenges when it comes in contact with reality and all these different pockets of behavior that need patching.&lt;/p&gt;
    &lt;p&gt;We’re going to see all this stuff play out. It’s a march of nines. Each nine is constant. Demos are encouraging. It’s still a huge amount of work to do. It is a critical safety domain, unless you’re doing vibe coding, which is all nice and fun and so on. That’s why this also enforced my timelines from that perspective.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:46:25&lt;/p&gt;
    &lt;p&gt;It’s very interesting to hear you say that, that the safety guarantees you need from software are not dissimilar to self-driving. What people will often say is that self-driving took so long because the cost of failure is so high. A human makes a mistake on average every 400,000 miles or every seven years. If you had to release a coding agent that couldn’t make a mistake for at least seven years, it would be much harder to deploy.&lt;/p&gt;
    &lt;p&gt;But your point is that if you made a catastrophic coding mistake, like breaking some important system every seven years...&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:46:56&lt;/p&gt;
    &lt;p&gt;Very easy to do.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:46:57&lt;/p&gt;
    &lt;p&gt;In fact, in terms of wall clock time, it would be much less than seven years because you’re constantly outputting code like that. In terms of tokens, it would be seven years. But in terms of wall clock time...&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:47:09&lt;/p&gt;
    &lt;p&gt;In some ways, it’s a much harder problem. Self-driving is just one of thousands of things that people do. It’s almost like a single vertical, I suppose. Whereas when we’re talking about general software engineering, it’s even more... There’s more surface area.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:47:20&lt;/p&gt;
    &lt;p&gt;There’s another objection people make to that analogy, which is that with self-driving, what took a big fraction of that time was solving the problem of having basic perception that’s robust, building representations, and having a model that has some common sense so it can generalize to when it sees something that’s slightly out of distribution. If somebody’s waving down the road this way, you don’t need to train for it. The thing will have some understanding of how to respond to something like that.&lt;/p&gt;
    &lt;p&gt;These are things we’re getting for free with LLMs or VLMs today, so we don’t have to solve these very basic representation problems. So now deploying AIs across different domains will sort of be like deploying a self-driving car with current models to a different city, which is hard but not like a 10-year-long task.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:48:07&lt;/p&gt;
    &lt;p&gt;I’m not 100% sure if I fully agree with that. I don’t know how much we’re getting for free. There’s still a lot of gaps in understanding what we are getting. We’re definitely getting more generalizable intelligence in a single entity, whereas self-driving is a very special-purpose task that requires. In some sense building a special-purpose task is maybe even harder in a certain sense because it doesn’t fall out from a more general thing that you’re doing at scale, if that makes sense.&lt;/p&gt;
    &lt;p&gt;But the analogy still doesn’t fully resonate because the LLMs are still pretty fallible and they have a lot of gaps that still need to be filled in. I don’t think that we’re getting magical generalization completely out of the box, in a certain sense.&lt;/p&gt;
    &lt;p&gt;The other aspect that I wanted to return to is that self-driving cars are nowhere near done still. The deployments are pretty minimal. Even Waymo and so on has very few cars. They’re doing that roughly speaking because they’re not economical. They’ve built something that lives in the future. They’ve had to pull back the future, but they had to make it uneconomical. There are all these costs, not just marginal costs for those cars and their operation and maintenance, but also the capex of the entire thing. Making it economical is still going to be a slog for them.&lt;/p&gt;
    &lt;p&gt;Also, when you look at these cars and there’s no one driving, I actually think it’s a little bit deceiving because there are very elaborate teleoperation centers of people kind of in a loop with these cars. I don’t have the full extent of it, but there’s more human-in-the-loop than you might expect. There are people somewhere out there beaming in from the sky. I don’t know if they’re fully in the loop with the driving. Some of the time they are, but they’re certainly involved and there are people. In some sense, we haven’t actually removed the person, we’ve moved them to somewhere where you can’t see them.&lt;/p&gt;
    &lt;p&gt;I still think there will be some work, as you mentioned, going from environment to environment. There are still challenges to make self-driving real. But I do agree that it’s definitely crossed a threshold where it kind of feels real, unless it’s really teleoperated. For example, Waymo can’t go to all the different parts of the city. My suspicion is that it’s parts of the city where you don’t get good signal. Anyway, I don’t know anything about the stack. I’m just making stuff up.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:50:23&lt;/p&gt;
    &lt;p&gt;You led self-driving for five years at Tesla.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:50:27&lt;/p&gt;
    &lt;p&gt;Sorry, I don’t know anything about the specifics of Waymo. By the way, I love Waymo and I take it all the time. I just think that people are sometimes a little bit too naive about some of the progress and there’s still a huge amount of work. Tesla took in my mind a much more scalable approach and the team is doing extremely well. I’m kind of on the record for predicting how this thing will go. Waymo had an early start because you can package up so many sensors. But I do think Tesla is taking the more scalable strategy and it’s going to look a lot more like that. So this will still have to play out and hasn’t. But I don’t want to talk about self-driving as something that took a decade because it didn’t take it yet, if that makes sense.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:51:08&lt;/p&gt;
    &lt;p&gt;Because one, the start is at 1980 and not 10 years ago, and then two, the end is not here yet.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:51:14&lt;/p&gt;
    &lt;p&gt;The end is not near yet because when we’re talking about self-driving, usually in my mind it’s self-driving at scale. People don’t have to get a driver’s license, etc.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:51:22&lt;/p&gt;
    &lt;p&gt;I’m curious to bounce two other ways in which the analogy might be different. The reason I’m especially curious about this is because the question of how fast AI is deployed, how valuable it is when it’s early on is potentially the most important question in the world right now. If you’re trying to model what the year 2030 looks like, this is the question you ought to have some understanding of.&lt;/p&gt;
    &lt;p&gt;Another thing you might think is one, you have this latency requirement with self-driving. I have no idea what the actual models are, but I assume it’s like tens of millions of parameters or something, which is not the necessary constraint for knowledge work with LLMs. Maybe it might be with computer use and stuff.&lt;/p&gt;
    &lt;p&gt;But the other big one is, maybe more importantly, on this capex question. Yes, there is additional cost to serving up an additional copy of a model, but the opex of a session is quite low and you can amortize the cost of AI into the training run itself, depending on how inference scaling goes and stuff. But it’s certainly not as much as building a whole new car to serve another instance of a model. So the economics of deploying more widely are much more favorable.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:52:37&lt;/p&gt;
    &lt;p&gt;I think that’s right. If you’re sticking to the realm of bits, bits are a million times easier than anything that touches the physical world. I definitely grant that. Bits are completely changeable, arbitrarily reshuffleable at a very rapid speed. You would expect a much faster adaptation also in the industry and so on. What was the first one?&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:52:59&lt;/p&gt;
    &lt;p&gt;The latency requirements and its implications for model size?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:53:02&lt;/p&gt;
    &lt;p&gt;I think that’s roughly right. I also think that if we are talking about knowledge work at scale, there will be some latency requirements, practically speaking, because we’re going to have to create a huge amount of compute and serve that.&lt;/p&gt;
    &lt;p&gt;The last aspect that I very briefly want to also talk about is all the rest of it. What does society think about it? What are the legal ramifications? How is it working legally? How is it working insurance-wise? What are those layers of it and aspects of it? What is the equivalent of people putting a cone on a Waymo? There are going to be equivalents of all that. So I feel like self-driving is a very nice analogy that you can borrow things from. What is the equivalent of a cone in the car? What is the equivalent of a teleoperating worker who’s hidden away and all the aspects of it.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:53:53&lt;/p&gt;
    &lt;p&gt;Do you have any opinions on what this implies about the current AI buildout, which would 10x the amount of available compute in the world in a year or two and maybe more than 100x it by the end of the decade. If the use of AI will be lower than some people naively predict, does that mean that we’re overbuilding compute or is that a separate question?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:54:15&lt;/p&gt;
    &lt;p&gt;Kind of like what happened with railroads.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:54:18&lt;/p&gt;
    &lt;p&gt;With what, sorry?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:54:19&lt;/p&gt;
    &lt;p&gt;Was it railroads or?&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:54:20&lt;/p&gt;
    &lt;p&gt;Yeah, it was.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:54:21&lt;/p&gt;
    &lt;p&gt;Yeah. There’s historical precedent. Or was it with the telecommunication industry? Pre-paving the internet that only came a decade later and creating a whole bubble in the telecommunications industry in the late ‘90s.&lt;/p&gt;
    &lt;p&gt;I understand I’m sounding very pessimistic here. I’m actually optimistic. I think this will work. I think it’s tractable. I’m only sounding pessimistic because when I go on my Twitter timeline, I see all this stuff that makes no sense to me. There’s a lot of reasons for why that exists. A lot of it is honestly just fundraising. It’s just incentive structures. A lot of it may be fundraising. A lot of it is just attention, converting attention to money on the internet, stuff like that. There’s a lot of that going on, and I’m only reacting to that.&lt;/p&gt;
    &lt;p&gt;But I’m still overall very bullish on technology. We’re going to work through all this stuff. There’s been a rapid amount of progress. I don’t know that there’s overbuilding. I think we’re going to be able to gobble up what, in my understanding, is being built. For example, Claude Code or OpenAI Codex and stuff like that didn’t even exist a year ago. Is that right? This is a miraculous technology that didn’t exist. There’s going to be a huge amount of demand, as we see the demand in ChatGPT already and so on.&lt;/p&gt;
    &lt;p&gt;So I don’t know that there’s overbuilding. I’m just reacting to some of the very fast timelines that people continue to say incorrectly. I’ve heard many, many times over the course of my 15 years in AI where very reputable people keep getting this wrong all the time. I want this to be properly calibrated, and some of this also has geopolitical ramifications and things like that with some of these questions. I don’t want people to make mistakes in that sphere of things. I do want us to be grounded in the reality of what technology is and isn’t.&lt;/p&gt;
    &lt;head rend="h3"&gt;01:56:20 - Future of education&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 01:56:20&lt;/p&gt;
    &lt;p&gt;Let’s talk about education and Eureka. One thing you could do is start another AI lab and then try to solve those problems. I’m curious what you’re up to now, and why not AI research itself?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:56:33&lt;/p&gt;
    &lt;p&gt;I guess the way I would put it is I feel some amount of determinism around the things that AI labs are doing. I feel like I could help out there, but I don’t know that I would uniquely improve it. My personal big fear is that a lot of this stuff happens on the side of humanity, and that humanity gets disempowered by it. I care not just about all the Dyson spheres that we’re going to build and that AI is going to build in a fully autonomous way, I care about what happens to humans. I want humans to be well off in the future.&lt;/p&gt;
    &lt;p&gt;I feel like that’s where I can a lot more uniquely add value than an incremental improvement in the frontier lab. I’m most afraid of something depicted in movies like WALL-E or Idiocracy or something like that, where humanity is on the side of this stuff. I want humans to be much, much better in this future. To me, this is through education that you can achieve this.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:57:35&lt;/p&gt;
    &lt;p&gt;So what are you working on there?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:57:36&lt;/p&gt;
    &lt;p&gt;The easiest way I can describe it is we’re trying to build the Starfleet Academy. I don’t know if you’ve watched Star Trek.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:57:44&lt;/p&gt;
    &lt;p&gt;I haven’t.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:57:44&lt;/p&gt;
    &lt;p&gt;Starfleet Academy is this elite institution for frontier technology, building spaceships, and graduating cadets to be the pilots of these spaceships and whatnot. So I just imagine an elite institution for technical knowledge and a kind of school that’s very up-to-date and a premier institution.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:58:05&lt;/p&gt;
    &lt;p&gt;A category of questions I have for you is explaining how one teaches technical or scientific content well, because you are one of the world masters at it. I’m curious both about how you think about it for content you’ve already put out there on YouTube, but also, to the extent it’s any different, how you think about it for Eureka.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:58:25&lt;/p&gt;
    &lt;p&gt;With respect to Eureka, one thing that is very fascinating to me about education is that I do think education will pretty fundamentally change with AIs on the side. It has to be rewired and changed to some extent.&lt;/p&gt;
    &lt;p&gt;I still think that we’re pretty early. There’s going to be a lot of people who are going to try to do the obvious things. Have an LLM and ask it questions. Do all the basic things that you would do via prompting right now. It’s helpful, but it still feels to me a bit like slop. I’d like to do it properly, and I think the capability is not there for what I would want. What I’d want is an actual tutor experience.&lt;/p&gt;
    &lt;p&gt;A prominent example in my mind is I was recently learning Korean, so language learning. I went through a phase where I was learning Korean by myself on the internet. I went through a phase where I was part of a small class in Korea taking Korean with a bunch of other people, which was really funny. We had a teacher and 10 people or so taking Korean. Then I switched to a one-on-one tutor.&lt;/p&gt;
    &lt;p&gt;I guess what was fascinating to me was, I think I had a really good tutor, but just thinking through what this tutor was doing for me and how incredible that experience was and how high the bar is for what I want to build eventually. Instantly from a very short conversation, she understood where I am as a student, what I know and don’t know. She was able to probe exactly the kinds of questions or things to understand my world model. No LLM will do that for you 100% right now, not even close. But a tutor will do that if they’re good. Once she understands, she really served me all the things that I needed at my current sliver of capability. I need to be always appropriately challenged. I can’t be faced with something too hard or too trivial, and a tutor is really good at serving you just the right stuff.&lt;/p&gt;
    &lt;p&gt;I felt like I was the only constraint to learning. I was always given the perfect information. I’m the only constraint. I felt good because I’m the only impediment that exists. It’s not that I can’t find knowledge or that it’s not properly explained or etc. It’s just my ability to memorize and so on. This is what I want for people.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:00:27&lt;/p&gt;
    &lt;p&gt;How do you automate that?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:00:29&lt;/p&gt;
    &lt;p&gt;Very good question. At the current capability, you don’t. That’s why I think it’s not actually the right time to build this kind of an AI tutor. I still think it’s a useful product, and lots of people will build it, but the bar is so high and the capability is not there. Even today, I would say ChatGPT is an extremely valuable educational product. But for me, it was so fascinating to see how high the bar is. When I was with her, I almost felt like there’s no way I can build this.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:01:02&lt;/p&gt;
    &lt;p&gt;But you are building it, right?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:01:03&lt;/p&gt;
    &lt;p&gt;Anyone who’s had a really good tutor is like, “How are you going to build this?” I’m waiting for that capability.&lt;/p&gt;
    &lt;p&gt;I did some AI consulting for computer vision. A lot of times, the value that I brought to the company was telling them not to use AI. I was the AI expert, and they described the problem, and I said, “Don’t use AI.” This is my value add. I feel like it’s the same in education right now, where I feel like for what I have in mind, it’s not yet the time, but the time will come. For now, I’m building something that looks maybe a bit more conventional that has a physical and digital component and so on. But it’s obvious how this should look in the future.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:01:43&lt;/p&gt;
    &lt;p&gt;To the extent you’re willing to say, what is the thing you hope will be released this year or next year?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:01:49&lt;/p&gt;
    &lt;p&gt;I’m building the first course. I want to have a really, really good course, the obvious state-of-the-art destination you go to to learn, AI in this case. That’s just what I’m familiar with, so it’s a really good first product to get to be really good at it. So that’s what I’m building. Nanochat, which you briefly mentioned, is a capstone project of LLM101N, which is a class that I’m building. That’s a really big piece of it. But now I have to build out a lot of the intermediates, and then I have to hire a small team of TAs and so on and build the entire course.&lt;/p&gt;
    &lt;p&gt;One more thing that I would say is that many times, when people think about education, they think more about what I would say is a softer component of diffusing knowledge. I have something very hard and technical in mind. In my mind, education is the very difficult technical process of building ramps to knowledge. In my mind, nanochat is a ramp to knowledge because it’s very simple. It’s the super simplified full-stack thing. If you give this artifact to someone and they look through it, they’re learning a ton of stuff. It’s giving you a lot of what I call eurekas per second, which is understanding per second. That’s what I want, lots of eurekas per second. So to me, this is a technical problem of how do we build these ramps to knowledge.&lt;/p&gt;
    &lt;p&gt;So I almost think of Eureka as maybe not that different from some of the frontier labs or some of the work that’s going on there. I want to figure out how to build these ramps very efficiently so that people are never stuck and everything is always not too hard or not too trivial, and you have just the right material to progress.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:03:25&lt;/p&gt;
    &lt;p&gt;You’re imagining in the short term that instead of a tutor being able to probe your understanding, if you have enough self-awareness to be able to probe yourself, you’re never going to be stuck. You can find the right answer between talking to the TA or talking to an LLM and looking at the reference implementation. It sounds like automation or AI is not a significant part. So far, the big alpha here is your ability to explain AI codified in the source material of the class. That’s fundamentally what the course is.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:04:00&lt;/p&gt;
    &lt;p&gt;You always have to be calibrated to what capability exists in the industry. A lot of people are going to pursue just asking ChatGPT, etc. But I think right now, for example, if you go to ChatGPT and you say, teach me AI, there’s no way. It’s going to give you some slop. AI is never going to write nanochat right now. But nanochat is a really useful intermediate point. I’m collaborating with AI to create all this material, so AI is still fundamentally very helpful.&lt;/p&gt;
    &lt;p&gt;Earlier on, I built CS231n at Stanford, which I think was the first deep learning class at Stanford, which became very popular. The difference in building out 231n then and LLM101N now is quite stark. I feel really empowered by the LLMs as they exist right now, but I’m very much in the loop. They’re helping me build the materials, I go much faster. They’re doing a lot of the boring stuff, etc. I feel like I’m developing the course much faster, and it’s LLM-infused, but it’s not yet at a place where it can creatively create the content. I’m still there to do that. The trickiness is always calibrating yourself to what exists.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:05:04&lt;/p&gt;
    &lt;p&gt;When you imagine what is available through Eureka in a couple of years, it seems like the big bottleneck is going to be finding Karpathys in field after field who can convert their understanding into these ramps.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:05:18&lt;/p&gt;
    &lt;p&gt;It would change over time. Right now, it would be hiring faculty to help work hand-in-hand with AI and a team of people probably to build state-of-the-art courses. Over time maybe some of the TAs can become AIs. You just take all the course materials and then I think you could serve a very good automated TA for the student when they have more basic questions or something like that. But I think you’ll need faculty for the overall architecture of a course and making sure that it fits. So I see a progression of how this will evolve. Maybe at some future point I’m not even that useful and AI is doing most of the design much better than I could. But I still think that’s going to take some time to play out.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:05:59&lt;/p&gt;
    &lt;p&gt;Are you imagining that people who have expertise in other fields are then contributing courses, or do you feel like it’s quite essential to the vision that you, given your understanding of how you want to teach, are the one designing the content? Sal Khan is narrating all the videos on Khan Academy. Are you imagining something like that?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:06:20&lt;/p&gt;
    &lt;p&gt;No, I will hire faculty because there are domains in which I’m not an expert. That’s the only way to offer the state-of-the-art experience for the student ultimately. I do expect that I would hire faculty, but I will probably stick around in AI for some time. I do have something more conventional in mind for the current capability than what people would probably anticipate.&lt;/p&gt;
    &lt;p&gt;When I’m building Starfleet Academy, I do probably imagine a physical institution, and maybe a tier below that a digital offering that is not the state-of-the-art experience you would get when someone comes in physically full-time and we work through material from start to end and make sure you understand it. That’s the physical offering. The digital offering is a bunch of stuff on the internet and maybe some LLM assistant. It’s a bit more gimmicky in a tier below, but at least it’s accessible to 8 billion people.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:07:08&lt;/p&gt;
    &lt;p&gt;I think you’re basically inventing college from first principles for the tools that are available today and just selecting for people who have the motivation and the interest of really engaging with material.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:07:26&lt;/p&gt;
    &lt;p&gt;There’s going to have to be a lot of not just education but also re-education. I would love to help out there because the jobs will probably change quite a bit. For example, today a lot of people are trying to upskill in AI specifically. I think it’s a really good course to teach in this respect. Motivation-wise, before AGI motivation is very simple to solve because people want to make money. This is how you make money in the industry today. Post-AGI is a lot more interesting possibly because if everything is automated and there’s nothing to do for anyone, why would anyone go to a school?&lt;/p&gt;
    &lt;p&gt;I often say that pre-AGI education is useful. Post-AGI education is fun. In a similar way, people go to the gym today. We don’t need their physical strength to manipulate heavy objects because we have machines that do that. They still go to the gym. Why do they go to the gym? Because it’s fun, it’s healthy, and you look hot when you have a six-pack. It’s attractive for people to do that in a very deep, psychological, evolutionary sense for humanity. Education will play out in the same way. You’ll go to school like you go to the gym.&lt;/p&gt;
    &lt;p&gt;Right now, not that many people learn because learning is hard. You bounce from material. Some people overcome that barrier, but for most people, it’s hard. It’s a technical problem to solve. It’s a technical problem to do what my tutor did for me when I was learning Korean. It’s tractable and buildable, and someone should build it. It’s going to make learning anything trivial and desirable, and people will do it for fun because it’s trivial. If I had a tutor like that for any arbitrary piece of knowledge, it’s going to be so much easier to learn anything, and people will do it. They’ll do it for the same reasons they go to the gym.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:09:17&lt;/p&gt;
    &lt;p&gt;That sounds different from using… So post-AGI, you’re using this as entertainment or as self-betterment. But it sounded like you had a vision also that this education is relevant to keeping humanity in control of AI. That sounds different. Is it entertaining for some people, but then empowerment for some others? How do you think about that?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:09:41&lt;/p&gt;
    &lt;p&gt;I do think eventually it’s a bit of a losing game, if that makes sense. It is in the long term. In the long term, which is longer than maybe most people in the industry think about, it’s a losing game. I do think people can go so far and we’ve barely scratched the surface of how much a person can go. That’s just because people are bouncing off of material that’s too easy or too hard. People will be able to go much further. Anyone will speak five languages because why not? Because it’s so trivial. Anyone will know all the basic curriculum of undergrad, et cetera.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:10:18&lt;/p&gt;
    &lt;p&gt;Now that I’m understanding the vision, that’s very interesting. It has a perfect analog in gym culture. I don’t think 100 years ago anybody would be ripped. Nobody would have been able to just spontaneously bench two plates or three plates or something. It’s very common now because of this idea of systematically training and lifting weights in the gym, or systematically training to be able to run a marathon, which is a capability most humans would not spontaneously have. You’re imagining similar things for learning across many different domains, much more intensely, deeply, faster.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:10:54&lt;/p&gt;
    &lt;p&gt;Exactly. I am betting a bit implicitly on some of the timelessness of human nature. It will be desirable to do all these things, and I think people will look up to it as they have for millennia. This will continue to be true. There’s some evidence of that historically. If you look at, for example, aristocrats, or you look at ancient Greece or something like that, whenever you had little pocket environments that were post-AGI in a certain sense, people have spent a lot of their time flourishing in a certain way, either physically or cognitively. I feel okay about the prospects of that.&lt;/p&gt;
    &lt;p&gt;If this is false and I’m wrong and we end up in a WALL-E or Idiocracy future, then I don’t even care if there are Dyson spheres. This is a terrible outcome. I really do care about humanity. Everyone has to just be superhuman in a certain sense.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:11:52&lt;/p&gt;
    &lt;p&gt;It’s still a world in which that is not enabling us to… It’s like the culture world, right? You’re not fundamentally going to be able to transform the trajectory of technology or influence decisions by your own labor or cognition alone. Maybe you can influence decisions because the AI is asking for your approval, but it’s not because I’ve invented something or I’ve come up with a new design that I’m really influencing the future.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:12:21&lt;/p&gt;
    &lt;p&gt;Maybe. I think there will be a transitional period where we are going to be able to be in the loop and advance things if we understand a lot of stuff. In the long-term, that probably goes away. It might even become a sport. Right now you have powerlifters who go extreme in this direction. What is powerlifting in a cognitive era? Maybe it’s people who are really trying to make Olympics out of knowing stuff. If you have a perfect AI tutor, maybe you can get extremely far. I feel that the geniuses of today are barely scratching the surface of what a human mind can do, I think.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:12:59&lt;/p&gt;
    &lt;p&gt;I love this vision. I also feel like the person you have the most product-market fit with is me because my job involves having to learn different subjects every week, and I am very excited.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:13:17&lt;/p&gt;
    &lt;p&gt;I’m similar, for that matter. A lot of people, for example, hate school and want to get out of it. I really liked school. I loved learning things, et cetera. I wanted to stay in school. I stayed all the way until Ph.D. and then they wouldn’t let me stay longer, so I went to the industry. Roughly speaking, I love learning, even for the sake of learning, but I also love learning because it’s a form of empowerment and being useful and productive.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:13:39&lt;/p&gt;
    &lt;p&gt;You also made a point that was subtle and I want to spell it out. With what’s happened so far with online courses, why haven’t they already enabled us to enable every single human to know everything? They’re just so motivation-laden because there are no obvious on-ramps and it’s so easy to get stuck. If you had this thing instead—like a really good human tutor—it would just be such an unlock from a motivation perspective.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:14:10&lt;/p&gt;
    &lt;p&gt;I think so. It feels bad to bounce from material. It feels bad. You get negative reward from sinking an amount of time in something and it doesn’t pan out, or being completely bored because what you’re getting is too easy or too hard. When you do it properly, learning feels good. It’s a technical problem to get there. For a while, it’s going to be AI plus human collab, and at some point, maybe it’s just AI.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:14:36&lt;/p&gt;
    &lt;p&gt;Can I ask some questions about teaching well? If you had to give advice to another educator in another field that you’re curious about to make the kinds of YouTube tutorials you’ve made. Maybe it might be especially interesting to talk about domains where you can’t test someone’s technical understanding by having them code something up or something. What advice would you give them?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:14:58&lt;/p&gt;
    &lt;p&gt;That’s a pretty broad topic. There are 10–20 tips and tricks that I semi-consciously do probably. But a lot of this comes from my physics background. I really, really did enjoy my physics background. I have a whole rant on how everyone should learn physics in early school education because early school education is not about accumulating knowledge or memory for tasks later in the industry. It’s about booting up a brain. Physics uniquely boots up the brain the best because some of the things that they get you to do in your brain during physics is extremely valuable later.&lt;/p&gt;
    &lt;p&gt;The idea of building models and abstractions and understanding that there’s a first-order approximation that describes most of the system, but then there’re second-order, third-order, fourth-order terms that may or may not be present. The idea that you’re observing a very noisy system, but there are these fundamental frequencies that you can abstract away. When a physicist walks into the class and they say, “Assume there’s a spherical cow,” everyone laughs at that, but this is brilliant. It’s brilliant thinking that’s very generalizable across the industry because a cow can be approximated as a sphere in a bunch of ways.&lt;/p&gt;
    &lt;p&gt;There’s a really good book, for example, Scale. It’s from a physicist talking about biology. Maybe this is also a book I would recommend reading. You can get a lot of really interesting approximations and chart scaling laws of animals. You can look at their heartbeats and things like that, and they line up with the size of the animal and things like that. You can talk about an animal as a volume. You can talk about the heat dissipation of that, because your heat dissipation grows as the surface area, which is growing as a square. But your heat creation or generation is growing as a cube. So I just feel like physicists have all the right cognitive tools to approach problem solving in the world.&lt;/p&gt;
    &lt;p&gt;So because of that training, I always try to find the first-order terms or the second-order terms of everything. When I’m observing a system or a thing, I have a tangle of a web of ideas or knowledge in my mind. I’m trying to find, what is the thing that matters? What is the first-order component? How can I simplify it? How can I have a simplest thing that shows that thing, shows it in action, and then I can tack on the other terms?&lt;/p&gt;
    &lt;p&gt;Maybe an example from one of my repos that I think illustrates it well is called micrograd. I don’t know if you’re familiar with this. So micrograd is 100 lines of code that shows backpropagation. You can create neural networks out of simple operations like plus and times, et cetera. Lego blocks of neural networks. You build up a computational graph and you do a forward pass and a backward pass to get the gradients. Now, this is at the heart of all neural network learning.&lt;/p&gt;
    &lt;p&gt;So micrograd is a 100 lines of pretty interpretable Python code, and it can do forward and backward arbitrary neural networks, but not efficiently. So micrograd, these 100 lines of Python, are everything you need to understand how neural networks train. Everything else is just efficiency. Everything else is efficiency. There’s a huge amount of work to get efficiency. You need your tensors, you lay them out, you stride them, you make sure your kernels, orchestrating memory movement correctly, et cetera. It’s all just efficiency, roughly speaking. But the core intellectual piece of neural network training is micrograd. It’s 100 lines. You can easily understand it. It’s a recursive application of chain rule to derive the gradient, which allows you to optimize any arbitrary differentiable function.&lt;/p&gt;
    &lt;p&gt;So I love finding these small-order terms and serving them on a platter and discovering them. I feel like education is the most intellectually interesting thing because you have a tangle of understanding and you’re trying to lay it out in a way that creates a ramp where everything only depends on the thing before it. I find that this untangling of knowledge is just so intellectually interesting as a cognitive task. I love doing it personally, but I just have a fascination with trying to lay things out in a certain way. Maybe that helps me.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:18:41&lt;/p&gt;
    &lt;p&gt;It also makes the learning experience so much more motivated. Your tutorial on the transformer begins with bigrams, literally a lookup table from, “Here’s the word right now, or here’s the previous word, here’s the next word.” It’s literally just a lookup table.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:18:58&lt;/p&gt;
    &lt;p&gt;That’s the essence of it, yeah.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:18:59&lt;/p&gt;
    &lt;p&gt;It’s such a brilliant way, starting with a lookup table and then going to a transformer. Each piece is motivated. Why would you add that? Why would you add the next thing? You could memorize the attention formula, but having an understanding of why every single piece is relevant, what problem it solves.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:19:13&lt;/p&gt;
    &lt;p&gt;You’re presenting the pain before you present a solution, and how clever is that? You want to take the student through that progression. There are a lot of other small things that make it nice and engaging and interesting. Always prompting the student.&lt;/p&gt;
    &lt;p&gt;There’s a lot of small things like that are important and a lot of good educators will do this. How would you solve this? I’m not going to present the solution before you guess. That would be wasteful. That’s a little bit of a…I don’t want to swear but it’s a dick move towards you to present you with the solution before I give you a shot to try to come up with it yourself.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:19:51&lt;/p&gt;
    &lt;p&gt;Because if you try to come up with it yourself, you get a better understanding of what the action space is, what the objective is, and then why only this action fulfills that objective.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:20:03&lt;/p&gt;
    &lt;p&gt;You have a chance to try it yourself, and you have an appreciation when I give you the solution. It maximizes the amount of knowledge per new fact added.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:20:11&lt;/p&gt;
    &lt;p&gt;Why do you think, by default, people who are genuine experts in their field are often bad at explaining it to somebody ramping up?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:20:24&lt;/p&gt;
    &lt;p&gt;It’s the curse of knowledge and expertise. This is a real phenomenon, and I suffered from it myself as much as I try not to. But you take certain things for granted, and you can’t put yourself in the shoes of new people who are just starting out. This is pervasive and happens to me as well.&lt;/p&gt;
    &lt;p&gt;One thing that’s extremely helpful. As an example, someone was trying to show me a paper in biology recently, and I just instantly had so many terrible questions. What I did was I used ChatGPT to ask the questions with the paper in the context window. It worked through some of the simple things. Then I shared the thread to the person who wrote that paper or worked on that work. I felt like if they could see the dumb questions I had, it might help them explain better in the future.&lt;/p&gt;
    &lt;p&gt;For my material, I would love it if people shared their dumb conversations with ChatGPT about the stuff that I’ve created because it really helps me put myself again in the shoes of someone who’s starting out.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:21:19&lt;/p&gt;
    &lt;p&gt;Another trick that just works astoundingly well. If somebody writes a paper or a blog post or an announcement, it is in 100% of cases that just the narration or the transcription of how they would explain it to you over lunch is way more, not only understandable, but actually also more accurate and scientific, in the sense that people have a bias to explain things in the most abstract, jargon-filled way possible and to clear their throat for four paragraphs before they explain the central idea. But there’s something about communicating one-on-one with a person which compels you to just say the thing.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:22:07&lt;/p&gt;
    &lt;p&gt;Just say the thing. I saw that tweet, I thought it was really good. I shared it with a bunch of people. I noticed this many, many times.&lt;/p&gt;
    &lt;p&gt;The most prominent example is that I remember back in my PhD days doing research. You read someone’s paper, and you work to understand what it’s doing. Then you catch them, you’re having beers at the conference later, and you ask them, “So this paper, what were you doing? What is the paper about?”&lt;/p&gt;
    &lt;p&gt;They will just tell you these three sentences that perfectly captured the essence of that paper and totally give you the idea. And you didn’t have to read the paper. It’s only when you’re sitting at the table with a beer or something, and they’re like, “Oh yeah, the paper is just, you take this idea, you take that idea and try this experiment and you try out this thing.” They have a way of just putting it conversationally just perfectly. Why isn’t that the abstract?&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:22:51&lt;/p&gt;
    &lt;p&gt;Exactly. This is coming from the perspective of how somebody who’s trying to explain an idea should formulate it better. What is your advice as a student to other students, if you don’t have a Karpathy who is doing the exposition of an idea? If you’re reading a paper from somebody or reading a book, what strategies do you employ to learn material you’re interested in in fields you’re not an expert at?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:23:20&lt;/p&gt;
    &lt;p&gt;I don’t know that I have unique tips and tricks, to be honest. It’s a painful process. One thing that has always helped me quite a bit is—I had a small tweet about this—learning things on demand is pretty nice. Learning depth-wise. I do feel you need a bit of alternation of learning depth-wise, on demand—you’re trying to achieve a certain project that you’re going to get a reward from—and learning breadth-wise, which is just, “Oh, let’s do whatever 101, and here’s all the things you might need.” Which is a lot of school—does breadth-wise learning, like, “Oh, trust me, you’ll need this later,” that kind of stuff. Okay, I trust you. I’ll learn it because I guess I need it. But I love the kind of learning where you’ll get a reward out of doing something, and you’re learning on demand.&lt;/p&gt;
    &lt;p&gt;The other thing that I’ve found extremely helpful. This is an aspect where education is a bit more selfless, but explaining things to people is a beautiful way to learn something more deeply. This happens to me all the time. It probably happens to other people too because I realize if I don’t really understand something, I can’t explain it. I’m trying and I’m like, “Oh, I don’t understand this.” It’s so annoying to come to terms with that. You can go back and make sure you understood it. It fills these gaps of your understanding. It forces you to come to terms with them and to reconcile them.&lt;/p&gt;
    &lt;p&gt;I love to re-explain things and people should be doing that more as well. That forces you to manipulate the knowledge and make sure that you know what you’re talking about when you’re explaining it.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:24:48&lt;/p&gt;
    &lt;p&gt;That’s an excellent note to close on. Andrej, that was great.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:24:51&lt;/p&gt;
    &lt;p&gt;Thank you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dwarkesh.com/p/andrej-karpathy"/><published>2025-10-17T17:24:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45619537</id><title>Claude Skills are awesome, maybe a bigger deal than MCP</title><updated>2025-10-18T16:10:53.004433+00:00</updated><content>&lt;doc fingerprint="954715ff2fbb933d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Claude Skills are awesome, maybe a bigger deal than MCP&lt;/head&gt;
    &lt;p&gt;16th October 2025&lt;/p&gt;
    &lt;p&gt;Anthropic this morning introduced Claude Skills, a new pattern for making new abilities available to their models:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Claude can now use Skills to improve how it performs specific tasks. Skills are folders that include instructions, scripts, and resources that Claude can load when needed.&lt;/p&gt;
      &lt;p&gt;Claude will only access a skill when it’s relevant to the task at hand. When used, skills make Claude better at specialized tasks like working with Excel or following your organization’s brand guidelines.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Their engineering blog has a more detailed explanation. There’s also a new anthropics/skills GitHub repo.&lt;/p&gt;
    &lt;p&gt;(I inadvertently preempted their announcement of this feature when I reverse engineered and wrote about it last Friday!)&lt;/p&gt;
    &lt;p&gt;Skills are conceptually extremely simple: a skill is a Markdown file telling the model how to do something, optionally accompanied by extra documents and pre-written scripts that the model can run to help it accomplish the tasks described by the skill.&lt;/p&gt;
    &lt;p&gt;Claude’s new document creation abilities, which accompanied their new code interpreter feature in September, turned out to be entirely implemented using skills. Those are now available in Anthropic’s repo covering &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.xlsx&lt;/code&gt;, and &lt;code&gt;.pptx&lt;/code&gt; files.&lt;/p&gt;
    &lt;p&gt;There’s one extra detail that makes this a feature, not just a bunch of files on disk. At the start of a session Claude’s various harnesses can scan all available skill files and read a short explanation for each one from the frontmatter YAML in the Markdown file. This is very token efficient: each skill only takes up a few dozen extra tokens, with the full details only loaded in should the user request a task that the skill can help solve.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Trying out the slack-gif-creator skill&lt;/item&gt;
      &lt;item&gt;Skills depend on a coding environment&lt;/item&gt;
      &lt;item&gt;Claude Code as a General Agent&lt;/item&gt;
      &lt;item&gt;Skills compared to MCP&lt;/item&gt;
      &lt;item&gt;Here come the Skills&lt;/item&gt;
      &lt;item&gt;The simplicity is the point&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Trying out the slack-gif-creator skill&lt;/head&gt;
    &lt;p&gt;Here’s that metadata for an example slack-gif-creator skill that Anthropic published this morning:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Toolkit for creating animated GIFs optimized for Slack, with validators for size constraints and composable animation primitives. This skill applies when users request animated GIFs or emoji animations for Slack from descriptions like “make me a GIF for Slack of X doing Y”.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I just tried this skill out in the Claude mobile web app, against Sonnet 4.5. First I enabled the slack-gif-creator skill in the settings, then I prompted:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;Make me a gif for slack about how Skills are way cooler than MCPs&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;And Claude made me this GIF. Click to play (it’s almost epilepsy inducing, hence the click-to-play mechanism):&lt;/p&gt;
    &lt;p&gt;OK, this particular GIF is terrible, but the great thing about skills is that they’re very easy to iterate on to make them better.&lt;/p&gt;
    &lt;p&gt;Here are some noteworthy snippets from the Python script it wrote, comments mine:&lt;/p&gt;
    &lt;quote&gt;# Start by adding the skill's directory to the Python path import sys sys.path.insert(0, '/mnt/skills/examples/slack-gif-creator') from PIL import Image, ImageDraw, ImageFont # This class lives in the core/ directory for the skill from core.gif_builder import GIFBuilder # ... code that builds the GIF ... # Save it to disk: info = builder.save('/mnt/user-data/outputs/skills_vs_mcps.gif', num_colors=128, optimize_for_emoji=False) print(f"GIF created successfully!") print(f"Size: {info['size_kb']:.1f} KB ({info['size_mb']:.2f} MB)") print(f"Frames: {info['frame_count']}") print(f"Duration: {info['duration_seconds']:.1f}s") # Use the check_slack_size() function to confirm it's small enough for Slack: passes, check_info = check_slack_size('/mnt/user-data/outputs/skills_vs_mcps.gif', is_emoji=False) if passes: print("✓ Ready for Slack!") else: print(f"⚠ File size: {check_info['size_kb']:.1f} KB (limit: {check_info['limit_kb']} KB)")&lt;/quote&gt;
    &lt;p&gt;This is pretty neat. Slack GIFs need to be a maximum of 2MB, so the skill includes a validation function which the model can use to check the file size. If it’s too large the model can have another go at making it smaller.&lt;/p&gt;
    &lt;head rend="h4"&gt;Skills depend on a coding environment&lt;/head&gt;
    &lt;p&gt;The skills mechanism is entirely dependent on the model having access to a filesystem, tools to navigate it and the ability to execute commands in that environment.&lt;/p&gt;
    &lt;p&gt;This is a common pattern for LLM tooling these days—ChatGPT Code Interpreter was the first big example of this back in early 2023, and the pattern later extended to local machines via coding agent tools such as Cursor, Claude Code, Codex CLI and Gemini CLI.&lt;/p&gt;
    &lt;p&gt;This requirement is the biggest difference between skills and other previous attempts at expanding the abilities of LLMs, such as MCP and ChatGPT Plugins. It’s a significant dependency, but it’s somewhat bewildering how much new capability it unlocks.&lt;/p&gt;
    &lt;p&gt;The fact that skills are so powerful and simple to create is yet another argument in favor of making safe coding environments available to LLMs. The word safe there is doing a lot of work though! We really need to figure out how best to sandbox these environments such that attacks such as prompt injections are limited to an acceptable amount of damage.&lt;/p&gt;
    &lt;head rend="h4"&gt;Claude Code as a General Agent&lt;/head&gt;
    &lt;p&gt;Back in January I made some foolhardy predictions about AI/LLMs, including that “agents” would once again fail to happen:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I think we are going to see a lot more froth about agents in 2025, but I expect the results will be a great disappointment to most of the people who are excited about this term. I expect a lot of money will be lost chasing after several different poorly defined dreams that share that name.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was entirely wrong about that. 2025 really has been the year of “agents”, no matter which of the many conflicting definitions you decide to use (I eventually settled on "tools in a loop").&lt;/p&gt;
    &lt;p&gt;Claude Code is, with hindsight, poorly named. It’s not purely a coding tool: it’s a tool for general computer automation. Anything you can achieve by typing commands into a computer is something that can now be automated by Claude Code. It’s best described as a general agent. Skills make this a whole lot more obvious and explicit.&lt;/p&gt;
    &lt;p&gt;I find the potential applications of this trick somewhat dizzying. Just thinking about this with my data journalism hat on: imagine a folder full of skills that covers tasks like the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Where to get US census data from and how to understand its structure&lt;/item&gt;
      &lt;item&gt;How to load data from different formats into SQLite or DuckDB using appropriate Python libraries&lt;/item&gt;
      &lt;item&gt;How to publish data online, as Parquet files in S3 or pushed as tables to Datasette Cloud&lt;/item&gt;
      &lt;item&gt;A skill defined by an experienced data reporter talking about how best to find the interesting stories in a new set of data&lt;/item&gt;
      &lt;item&gt;A skill that describes how to build clean, readable data visualizations using D3&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Congratulations, you just built a “data journalism agent” that can discover and help publish stories against fresh drops of US census data. And you did it with a folder full of Markdown files and maybe a couple of example Python scripts.&lt;/p&gt;
    &lt;head rend="h4"&gt;Skills compared to MCP&lt;/head&gt;
    &lt;p&gt;Model Context Protocol has attracted an enormous amount of buzz since its initial release back in November last year. I like to joke that one of the reasons it took off is that every company knew they needed an “AI strategy”, and building (or announcing) an MCP implementation was an easy way to tick that box.&lt;/p&gt;
    &lt;p&gt;Over time the limitations of MCP have started to emerge. The most significant is in terms of token usage: GitHub’s official MCP on its own famously consumes tens of thousands of tokens of context, and once you’ve added a few more to that there’s precious little space left for the LLM to actually do useful work.&lt;/p&gt;
    &lt;p&gt;My own interest in MCPs has waned ever since I started taking coding agents seriously. Almost everything I might achieve with an MCP can be handled by a CLI tool instead. LLMs know how to call &lt;code&gt;cli-tool --help&lt;/code&gt;, which means you don’t have to spend many tokens describing how to use them—the model can figure it out later when it needs to.&lt;/p&gt;
    &lt;p&gt;Skills have exactly the same advantage, only now I don’t even need to implement a new CLI tool. I can drop a Markdown file in describing how to do a task instead, adding extra scripts only if they’ll help make things more reliable or efficient.&lt;/p&gt;
    &lt;head rend="h4"&gt;Here come the Skills&lt;/head&gt;
    &lt;p&gt;One of the most exciting things about Skills is how easy they are to share. I expect many skills will be implemented as a single file—more sophisticated ones will be a folder with a few more.&lt;/p&gt;
    &lt;p&gt;Anthropic have Agent Skills documentation and a Claude Skills Cookbook. I’m already thinking through ideas of skills I might build myself, like one on how to build Datasette plugins.&lt;/p&gt;
    &lt;p&gt;Something else I love about the design of skills is there is nothing at all preventing them from being used with other models.&lt;/p&gt;
    &lt;p&gt;You can grab a skills folder right now, point Codex CLI or Gemini CLI at it and say “read pdf/SKILL.md and then create me a PDF describing this project” and it will work, despite those tools and models having no baked in knowledge of the skills system.&lt;/p&gt;
    &lt;p&gt;I expect we’ll see a Cambrian explosion in Skills which will make this year’s MCP rush look pedestrian by comparison.&lt;/p&gt;
    &lt;head rend="h4"&gt;The simplicity is the point&lt;/head&gt;
    &lt;p&gt;I’ve seen a some push back against skills as being so simple they’re hardly a feature at all. Plenty of people have experimented with the trick of dropping extra instructions into a Markdown file and telling the coding agent to read that file before continuing with a task. AGENTS.md is a well established pattern, and that file can already include instructions to “Read PDF.md before attempting to create a PDF”.&lt;/p&gt;
    &lt;p&gt;The core simplicity of the skills design is why I’m so excited about it.&lt;/p&gt;
    &lt;p&gt;MCP is a whole protocol specification, covering hosts, clients, servers, resources, prompts, tools, sampling, roots, elicitation and three different transports (stdio, streamable HTTP and originally SSE).&lt;/p&gt;
    &lt;p&gt;Skills are Markdown with a tiny bit of YAML metadata and some optional scripts in whatever you can make executable in the environment. They feel a lot closer to the spirit of LLMs—throw in some text and let the model figure it out.&lt;/p&gt;
    &lt;p&gt;They outsource the hard parts to the LLM harness and the associated computer environment. Given everything we have learned about LLMs’ ability to run tools over the last couple of years I think that’s a very sensible strategy.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NVIDIA DGX Spark: great hardware, early days for the ecosystem - 14th October 2025&lt;/item&gt;
      &lt;item&gt;Claude can write complete Datasette plugins now - 8th October 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://simonwillison.net/2025/Oct/16/claude-skills/"/><published>2025-10-17T17:40:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45621074</id><title>The pivot</title><updated>2025-10-18T16:10:52.184612+00:00</updated><content>&lt;doc fingerprint="32709077d7a86c66"&gt;
  &lt;main&gt;
    &lt;p&gt;I don’t want to mislead anyone here. This corner of the website—“New Stuff”—is not a resurrection of The Far Side daily cartoons. (Well, not exactly, anyway—like the proverbial tiger and its stripes, I’m pretty much stuck with my sense of humor. Aren’t we all?) The thing is, I thoroughly enjoyed my career as a syndicated cartoonist, and I hope, in spirit at least, we had some laughs together. But after fifteen years of meeting deadlines, well, blah blah blah … you know the rest. The day after I retired from syndication, it felt good not to draw on a deadline. And after moving on to other interests, drawing just wasn’t on my to-do list. Things change. But then a few years ago—and returning to the subject at hand—something happened in my life, and it started with a clogged pen.&lt;/p&gt;
    &lt;p&gt;Despite my retirement, I still had intermittent connections to cartooning, including my wife’s and my personal Christmas card. Once a year, I’d sit myself down to take on Santa, and every year it began with the same ritual: me cursing at, and then cleaning out, my clogged pen. (Apparently, the concept of cleaning it before putting it away each year was just too elusive for me.) As problems go, this is admittedly not exactly on the scale of global warming, but in the small world of my studio, it was cataclysmic. Okay, highly annoying.&lt;/p&gt;
    &lt;p&gt;So a few years ago—finally fed up with my once-loyal but now reliably traitorous pen—I decided to try a digital tablet. I knew nothing about these devices but hoped it would just get me through my annual Christmas card ordeal. I got one, fired it up, and lo and behold, something totally unexpected happened: within moments, I was having fun drawing again. I was stunned at all the tools the thing offered, all the creative potential it contained. I simply had no idea how far these things had evolved. Perhaps fittingly, the first thing I drew was a caveman.&lt;/p&gt;
    &lt;p&gt;The “New Stuff” that you’ll see here is the result of my journey into the world of digital art. Believe me, this has been a bit of a learning curve for me. I hail from a world of pen and ink, and suddenly I was feeling like I was sitting at the controls of a 747. (True, I don’t get out much.) But as overwhelmed as I was, there was still something familiar there—a sense of adventure. That had always been at the core of what I enjoyed most when I was drawing The Far Side, that sense of exploring, reaching for something, taking some risks, sometimes hitting a home run and sometimes coming up with “Cow tools.” (Let’s not get into that.) But as a jazz teacher once said to me about improvisation, “You want to try and take people somewhere where they might not have been before.” I think that my approach to cartooning was similar—I’m just not sure if even I knew where I was going. But I was having fun.&lt;/p&gt;
    &lt;p&gt;So here goes. I’ve got my coffee, I’ve got this cool gizmo, and I’ve got no deadlines. And—to borrow from Sherlock Holmes—the game is afoot.&lt;/p&gt;
    &lt;p&gt;Again, please remember, I’m just exploring, experimenting, and trying stuff. New Stuff. I have just one last thing to say before I go: thank you, clogged pen.&lt;/p&gt;
    &lt;p&gt;—Gary Larson&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.antipope.org/charlie/blog-static/2025/10/the-pivot-1.html"/><published>2025-10-17T19:37:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45622365</id><title>New Work by Gary Larson</title><updated>2025-10-18T16:10:51.973983+00:00</updated><content>&lt;doc fingerprint="32709077d7a86c66"&gt;
  &lt;main&gt;
    &lt;p&gt;I don’t want to mislead anyone here. This corner of the website—“New Stuff”—is not a resurrection of The Far Side daily cartoons. (Well, not exactly, anyway—like the proverbial tiger and its stripes, I’m pretty much stuck with my sense of humor. Aren’t we all?) The thing is, I thoroughly enjoyed my career as a syndicated cartoonist, and I hope, in spirit at least, we had some laughs together. But after fifteen years of meeting deadlines, well, blah blah blah … you know the rest. The day after I retired from syndication, it felt good not to draw on a deadline. And after moving on to other interests, drawing just wasn’t on my to-do list. Things change. But then a few years ago—and returning to the subject at hand—something happened in my life, and it started with a clogged pen.&lt;/p&gt;
    &lt;p&gt;Despite my retirement, I still had intermittent connections to cartooning, including my wife’s and my personal Christmas card. Once a year, I’d sit myself down to take on Santa, and every year it began with the same ritual: me cursing at, and then cleaning out, my clogged pen. (Apparently, the concept of cleaning it before putting it away each year was just too elusive for me.) As problems go, this is admittedly not exactly on the scale of global warming, but in the small world of my studio, it was cataclysmic. Okay, highly annoying.&lt;/p&gt;
    &lt;p&gt;So a few years ago—finally fed up with my once-loyal but now reliably traitorous pen—I decided to try a digital tablet. I knew nothing about these devices but hoped it would just get me through my annual Christmas card ordeal. I got one, fired it up, and lo and behold, something totally unexpected happened: within moments, I was having fun drawing again. I was stunned at all the tools the thing offered, all the creative potential it contained. I simply had no idea how far these things had evolved. Perhaps fittingly, the first thing I drew was a caveman.&lt;/p&gt;
    &lt;p&gt;The “New Stuff” that you’ll see here is the result of my journey into the world of digital art. Believe me, this has been a bit of a learning curve for me. I hail from a world of pen and ink, and suddenly I was feeling like I was sitting at the controls of a 747. (True, I don’t get out much.) But as overwhelmed as I was, there was still something familiar there—a sense of adventure. That had always been at the core of what I enjoyed most when I was drawing The Far Side, that sense of exploring, reaching for something, taking some risks, sometimes hitting a home run and sometimes coming up with “Cow tools.” (Let’s not get into that.) But as a jazz teacher once said to me about improvisation, “You want to try and take people somewhere where they might not have been before.” I think that my approach to cartooning was similar—I’m just not sure if even I knew where I was going. But I was having fun.&lt;/p&gt;
    &lt;p&gt;So here goes. I’ve got my coffee, I’ve got this cool gizmo, and I’ve got no deadlines. And—to borrow from Sherlock Holmes—the game is afoot.&lt;/p&gt;
    &lt;p&gt;Again, please remember, I’m just exploring, experimenting, and trying stuff. New Stuff. I have just one last thing to say before I go: thank you, clogged pen.&lt;/p&gt;
    &lt;p&gt;—Gary Larson&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.thefarside.com/new-stuff"/><published>2025-10-17T21:34:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45622604</id><title>WebMCP</title><updated>2025-10-18T16:10:51.285975+00:00</updated><content>&lt;doc fingerprint="3519d4909a639e9c"&gt;
  &lt;main&gt;
    &lt;p&gt;A proposal and code for websites to support client side LLMs&lt;/p&gt;
    &lt;p&gt;WebMCP allows websites to share tools, resources, prompts, etc. to LLMs. In other words, WebMCP allows a website to be an MCP server. No sharing API Keys. Use any model you want.&lt;/p&gt;
    &lt;p&gt;Here's a simple website I built that is WebMCP-enabled&lt;/p&gt;
    &lt;p&gt;It comes in the form of a widget that a website owner can put on their site and expose tools to give client-side LLMs what they need to provide a great UX for the user or agent.&lt;/p&gt;
    &lt;p&gt;The look, feel, how it's used, and security are all absolutely open for contribution / constructive criticism. MCP Clients directly building WebMCP functionality seems like an ideal outcome.&lt;/p&gt;
    &lt;p&gt;An end-user can connect to any number of websites at a time - and tools are "scoped" (by name) based on the domain to simplify organization.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;webmcp.mp4&lt;/head&gt;
    &lt;p&gt;Just specify your MCP client (&lt;code&gt;claude&lt;/code&gt;, &lt;code&gt;cursor&lt;/code&gt;, &lt;code&gt;cline&lt;/code&gt;, &lt;code&gt;windsurf&lt;/code&gt;, or a path to json)&lt;/p&gt;
    &lt;code&gt;npx -y @jason.today/webmcp@latest --config claude&lt;/code&gt;
    &lt;p&gt;If you're interested in setting it up manually, use the command &lt;code&gt;npx -y @jason.today/webmcp@latest --mcp&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Auto-install was inspired by Smithery, but their code is AGPL so I wrote this myself. If it doesn't work for you or you don't see your mcp client, please file an issue.&lt;/p&gt;
    &lt;p&gt;When you're ready to connect to a website, you can ask your model to generate you an mcp token.&lt;/p&gt;
    &lt;p&gt;Copy the token and paste it to the website's input. As soon as the website registers with it, it's thrown away and cannot be used for subsequent registrations or anything else. The website will receive its own session token for making requests.&lt;/p&gt;
    &lt;p&gt;If you'd rather your model / service never see the token, you can manually execute &lt;code&gt;npx @jason.today/webmcp --new&lt;/code&gt; instead.&lt;/p&gt;
    &lt;p&gt;Some MCP clients, including Claude Desktop, need to be restarted to get access to new tools. (at least at time of writing)&lt;/p&gt;
    &lt;p&gt;To disconnect, you can close the browser tab, click "disconnect", or shut down the server with &lt;code&gt;npx @jason.today/webmcp -q&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;All configuration files are stored in &lt;code&gt;~/.webmcp&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;To use WebMCP, simply include &lt;code&gt;webmcp.js&lt;/code&gt; on your page (via src or directly):&lt;/p&gt;
    &lt;code&gt;&amp;lt;script src="webmcp.js"&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;
    &lt;p&gt;The WebMCP widget will automatically initialize and appear in the bottom right corner of your page. Clicking on it will ask for a webmcp token which the end-user will generate.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;webmcp.mp4&lt;/head&gt;
    &lt;p&gt;The bridge between the MCP client and the website is a localhost-only (not accessible to requests outside your computer) websocket server. Because it is configured to allow requests from your local web browser, authentication / token exchange is required, in case you visit a website attempting to abuse this.&lt;/p&gt;
    &lt;p&gt;Ideally the web browser itself would have an explicit permission for this, like webcam or microphone use.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The MCP client connects to the &lt;code&gt;/mcp&lt;/code&gt;path using the server token from&lt;code&gt;.env&lt;/code&gt;(auto-generated)&lt;/item&gt;
      &lt;item&gt;The server generates a registration token (instigated via the built-in mcp tool by a model or the &lt;code&gt;--new&lt;/code&gt;command)&lt;/item&gt;
      &lt;item&gt;Web clients connect to the &lt;code&gt;/register&lt;/code&gt;endpoint with this token and its domain.&lt;/item&gt;
      &lt;item&gt;Web pages connect to their assigned channel based on their domain.&lt;/item&gt;
      &lt;item&gt;When an LLM wants to use a tool / resource / prompt, the request flows from: &lt;list rend="ul"&gt;&lt;item&gt;MCP Client → MCP Server → WebSocket Server → Web Page with the tool / resource / prompt&lt;/item&gt;&lt;item&gt;(similar for requesting a list of tools / resources / prompts)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The web page performs the request (e.g. call tool) and sends the result back through the same path&lt;/item&gt;
      &lt;item&gt;Multiple web pages can be connected simultaneously, each with their own set of tools and tokens&lt;/item&gt;
      &lt;item&gt;The MCP client sees all tools as a unified list, with channel prefixes to avoid name collisions&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sequenceDiagram
    participant User
    participant MCP as MCP Client
    participant Server as MCP Server
    participant WS as WebSocket Server
    participant Web as Website
    
    %% Initial connection
    MCP-&amp;gt;&amp;gt;Server: Connect to /mcp with internal server token
    
    %% Website registration token
    User-&amp;gt;&amp;gt;MCP: Request registration token
    MCP-&amp;gt;&amp;gt;Server: Request registration token
    Server--&amp;gt;&amp;gt;MCP: Return registration token
    MCP--&amp;gt;&amp;gt;User: Display registration token
    
    %% Website registration
    User-&amp;gt;&amp;gt;Web: Paste registration token
    Web-&amp;gt;&amp;gt;WS: Connect to /register with token &amp;amp; domain (registration token deleted)
    WS--&amp;gt;&amp;gt;Web: Assign channel &amp;amp; session token
    Web-&amp;gt;&amp;gt;WS: Connect to assigned channel
    
    %% Tool interaction
    MCP-&amp;gt;&amp;gt;Server: Request tools list
    Server-&amp;gt;&amp;gt;WS: Forward request
    WS-&amp;gt;&amp;gt;Web: Request tools
    Web--&amp;gt;&amp;gt;WS: Return tools list
    WS--&amp;gt;&amp;gt;Server: Forward tools list
    Server--&amp;gt;&amp;gt;MCP: Return tools list
    
    %% Tool execution
    MCP-&amp;gt;&amp;gt;Server: Tool request
    Server-&amp;gt;&amp;gt;WS: Forward request
    WS-&amp;gt;&amp;gt;Web: Execute tool
    Web--&amp;gt;&amp;gt;WS: Return result
    WS--&amp;gt;&amp;gt;Server: Forward result
    Server--&amp;gt;&amp;gt;MCP: Return result
    
    %% Disconnection
    User-&amp;gt;&amp;gt;Web: Disconnect
    Web-&amp;gt;&amp;gt;WS: Close connection
&lt;/code&gt;
    &lt;p&gt;This is a super early project. I'm very interested in hardening security to prevent malicious extensions etc. from being able to perform prompt injection attacks and similar. If you have constructive ideas, please reach out or open an issue.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Token generator (for connecting to WebMCP websites)&lt;/item&gt;
      &lt;item&gt;MCP Tool Definer (to simplify building the schema of a tool for use with MCP) &lt;list rend="ul"&gt;&lt;item&gt;You can ask for the javascript (if relevant) in a follow-up message for use with WebMCP&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There is a &lt;code&gt;Dockerfile&lt;/code&gt; specifically for Smithery deployment.&lt;/p&gt;
    &lt;p&gt;If you'd like to use docker to run the websocket server, I've added a &lt;code&gt;docker-compose.yml&lt;/code&gt; for demonstration purposes.&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;--docker&lt;/code&gt; is provided to the mcp client config alongside &lt;code&gt;--mcp&lt;/code&gt;, it will assume the server is running. This will allow you to dockerize the main process (websocket server), and your mcp client will connect to your docker container via websocket. Similarly, websites will communicate with your docker container.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/jasonjmcghee/WebMCP"/><published>2025-10-17T21:57:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45623917</id><title>The Unix Executable as a Smalltalk Method [pdf]</title><updated>2025-10-18T16:10:51.046047+00:00</updated><content/><link href="https://programmingmadecomplicated.wordpress.com/wp-content/uploads/2025/10/onward25-jakubovic.pdf"/><published>2025-10-18T01:03:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45624888</id><title>AMD's Chiplet APU: An Overview of Strix Halo</title><updated>2025-10-18T16:10:50.916906+00:00</updated><content>&lt;doc fingerprint="aff85855953a28bd"&gt;
  &lt;main&gt;
    &lt;p&gt;Hello you fine Internet folks!&lt;/p&gt;
    &lt;p&gt;Today we are looking at AMD’s largest client APU to date, Strix Halo. This is an APU designed to be a true all-in-one mobile processor, able to handle high end CPU and GPU workloads without compromise. Offering a TDP range of 55W to 120W, the chip targets a far higher power envelope compared to standard Strix Point, but eschews the need for dedicated graphics.&lt;/p&gt;
    &lt;p&gt;To get y’all all caught up on the history and specifications of this APU, AMD first announced Strix Halo at CES 2025 earlier this year to much fanfare. Strix Halo is AMD’s first chiplet APU in the consumer market with AMD using Strix Halo as a bit of a show piece for what both CPU and GPU performance can look like with a sufficiently large APU.&lt;/p&gt;
    &lt;p&gt;AMD’s Strix Halo can be equipped with dual 8 core Zen 5 CCDs for a total of 16 cores that feature the same 512b FPU as the desktop parts. This is a change from the more mainstream and monolithic Strix Point APU which has “double-pumped” 256b FPUs similar to Zen 4 for use with AVX512 code. What is similar to the more mainstream Strix Point is the same 5.1GHz max boost clock which is a 600MHz deficit compared to the desktop flagship Zen 5 CPU, the Ryzen 9 9950X.&lt;/p&gt;
    &lt;p&gt;Moving to the 3rd die on a Strix Halo package, a RDNA 3.5 iGPU takes up the majority of the SoC die with 40 compute units, 32MB of Infinity Cache, and a boost clock of up to 2.9GHz placing raw compute capability somewhere between the RX 7600 XT and RX 7700.&lt;/p&gt;
    &lt;p&gt;To feed this chip, AMD has equipped Strix Halo with a 256b LPDDR5X-8000 memory bus, which provides up to 256GB/s shared between all of the components. This is slightly lower than the 288GB/s available to the RX 7600 XT but is much higher than any other APU we have tested.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;A massive thank you to both Asus and HP for sending over a ROG Flow Z13 (2025) and a ZBook Ultra G1a 14” for testing which were both equipped with an AMD Ryzen AI Max+ 395. All of the gaming tests were done on the Flow Z13 due to that being a more gaming focused device and all of the microbenchmarking was done on the ZBook Ultra G1a.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Subsystem from the CPU’s Perspective&lt;/head&gt;
    &lt;p&gt;Starting with the memory latency from Zen 5’s perspective, we see that the latency difference between Strix Point and Strix Halo is negligible with Strix Point at ~128ns of memory latency and Strix Halo at ~123ns of memory latency. However, as you can see the CPU does not have access to the 32MB of Infinity Cache on the IO die. This behavior was confirmed by Mahesh Subramony during our interview about Strix Halo at CES 2025.&lt;/p&gt;
    &lt;p&gt;While the 123ns DRAM latency seen here is quite good for a mobile part, desktop processors like our 9950X fare much better at 75-80ns.&lt;/p&gt;
    &lt;p&gt;Moving on to memory bandwidth, we see Strix Halo fall into a category of its own of the SoCs we have tested.&lt;/p&gt;
    &lt;p&gt;When doing read-modify-add operations across both CCDs, the 16 Zen 5 cores can pull over 175GB/s of bandwidth from the memory with reads being no slouch at 124GB/s across both CCDs.&lt;/p&gt;
    &lt;p&gt;However, looking at the bandwidth of a single CCD and just like the desktop CPUs a single Strix-Halo CCD only has a 32 byte per cycle read link to the IO die. And just like the desktop chips, the chip to chip link runs at ~2000MHz, which caps out the single CCD read at 64GB/s. Unlike the desktop chips, the write link is 32 bytes per cycle and we are seeing about 43GB/s for the write bandwidth. That brings the total theoretical single CCD bandwidth to 128GB/s and the observed bandwidth is just over 103GB/s.&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU’s Performance&lt;/head&gt;
    &lt;p&gt;The performance of Strix Halo’s CPU packs quite a bit more of a punch than Strix Point’s CPU.&lt;/p&gt;
    &lt;p&gt;Strix Halo’s CPU can match a last generation desktop flagship CPU, the 7950X, in Integer performance despite a 11.7% clock speed delta. And nearly matches AMD current desktop flagship CPU, the 9950X, in Floating Point performance again with a 11.7% clock speed deficit.&lt;/p&gt;
    &lt;p&gt;Looking at the SPEC CPU 2017 Integer subtests and while Strix Halo can’t quite match the desktop 9950X, likely due to the higher memory latency of Strix Halo’s LPDDR5X bus, it does get close in a number of subtests.&lt;/p&gt;
    &lt;p&gt;Moving to the FP subtests and the story is similar to the Integer subtests but Strix Halo can get even closer to the 9950X and even beat it in the fotonik3d subtest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory from the GPU’s Perspective&lt;/head&gt;
    &lt;p&gt;Moving to the GPU side of things and this is where Strix Halo really shines. The laptop we used as a comparison to Strix Halo was the HP Omen Transcend 14 2025 with a 5070M equipped which maxed out at about 75 Watts for the GPU.&lt;/p&gt;
    &lt;p&gt;Strix Halo has over double the memory bandwidth of any of the other mobile SoCs that we have tested. However, the RTX 5070 Mobile does have about 50% more memory bandwidth compared to Strix Halo.&lt;/p&gt;
    &lt;p&gt;Looking at the caches of Strix Halo, the Infinity Cache, AKA MALL, is able to deliver over 40% higher bandwidth compared to the 5070M’s L2 while having 33% more capacity. Plus Strix Halo has a 4MB L2 which is capable of providing 2.5TB/s of bandwidth to the GPU.&lt;/p&gt;
    &lt;p&gt;Moving to the latency, the more complex cache layout of Strix Halo does give it a latency advantage after the 128KB with Strix Halo’s L2 being significantly lower latency than the 5070M’s L2 and the larger 32MB MALL that Strix Halo has a similar latency to the 5070M’s L2. And Strix Halo’s memory latency is about 35% lower than the 5070M’s memory latency.&lt;/p&gt;
    &lt;head rend="h2"&gt;The GPU’s Compute Throughput&lt;/head&gt;
    &lt;p&gt;Looking at the floating point throughput, we see that Strix Halo unsurprisingly has about 2.5x the throughput of Strix Point considering it has about two and a half times the number of Compute Units. Strix Halo oftentimes can match or even pull ahead of the 5070 Mobile in terms of throughput. I will note that the FP16 results for the 5070 Mobile are half of what I would expect; the FP16:FP32 ratio for the 5070 Mobile should be 1:1 so I am not positive about what is going on there.&lt;/p&gt;
    &lt;p&gt;Moving to the integer throughput and we see the 5070 Mobile soundly pulling ahead of the Radeon 8060S.&lt;/p&gt;
    &lt;head rend="h2"&gt;GPU Performance&lt;/head&gt;
    &lt;p&gt;Looking at the GPU performance, we see Strix Halo once again shine, with a staggering level of performance available for an iGPU, courtesy of the large CU count paired with relatively high memory bandwidth. Our comparison suite includes several recent iGPU’s from Intel/AMD, along with the newest generation RTX 5070 Mobile @ 75W to act as a reference for mid to high-range laptop dedicated graphics, and the antiquated GTX 1050 as a reference for budget dedicated graphics.&lt;/p&gt;
    &lt;p&gt;Looking at Fluid X3D for a compute-heavy workload, we can see the Radeon 8060S absolutely obliterates the other iGPU’s from Intel/AMD, putting itself firmly in a class above. The 5070 is no slouch though, and still holds a substantial 64.1% lead largely due to the higher memory bandwidth afforded to the 5070M.&lt;/p&gt;
    &lt;p&gt;Switching to gaming workloads with Cyberpunk 2077, we start with a benchmark conducted while on battery power. The gap with other iGPU’s is still wide, but now the 5070M is limited to 55W and exhibits 7.5% worse performance at 1080p low settings when compared to the Radeon 8060S.&lt;/p&gt;
    &lt;p&gt;Finally, moving to wall power and allowing both the Radeon 8060S and 5070M to access the full power limit in CP2077, we can see that the 8060S still pulls ahead at 1080p low by 2.5%, while at 1440p medium we see a reversal, with the 5070M commanding an 8.3% lead. Overall the two provide a comparable experience in Cyberpunk 2077, with changes in settings or power limits adjusting the lead between the two. This is a seriously impressive turnaround for an iGPU working against dedicated graphics, and demonstrates the versatility of the chip in workloads like gaming, where iGPU’s have traditionally struggled.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Strix Halo follows in the footsteps of many other companies in the goal of designing an SoC for desktop and laptop usage that is truly all encompassing. The CPU and GPU performance is truly a class above standard low power laptop chips, and is even able to compete with larger systems boasting dedicated graphics. CPU performance is especially impressive with a comparable showing to the desktop Zen 5 CPUs. GPU performance is comparable to mid range dedicated graphics, while still offering the efficiency and integration of an iGPU. High end dedicated graphics still have a place above Strix Halo, but the versatility of this design for smaller form factor devices is class leading.&lt;/p&gt;
    &lt;p&gt;However, this is not to say that Strix Halo is perfect. I was hoping to have a section dedicated to the ML performance of Strix Halo in this article, however AMD only just released preview support for Strix Halo in the ROCm 7.0.2 release which came out about a week ago from time of publication. As a result of the long delay between the launch of Strix Halo and the release of ROCm 7.0.2, the ML performance will have to wait until a future article.&lt;/p&gt;
    &lt;p&gt;However, putting aside ROCm, Strix Halo is a very, very cool piece of technology and I would love to see successors to Strix Halo with newer CPU and GPU IP and possibly even larger memory buses similar to Apple’s Max and Ultra series of SoCs with 512b and 1024b memory respectively. AMD has a formula for building bigger APUs with Strix Halo, which opens the door to a lot of interesting hardware possibilities in the future.&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipsandcheese.com/p/amds-chiplet-apu-an-overview-of-strix"/><published>2025-10-18T04:26:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45625251</id><title>StageConnect: Behringer protocol is open source</title><updated>2025-10-18T16:10:50.474983+00:00</updated><content>&lt;doc fingerprint="2d848816fdf04a85"&gt;
  &lt;main&gt;
    &lt;p&gt;This repository contains an Arduino-Library to create a StageConnect device (subordinated node-device as well as controlling main-device) using AnalogDevices AD242x-ICs like the AD2428.&lt;/p&gt;
    &lt;p&gt;StageConnect is based on the automotive A²B and is used to connect the Behringer WING to the Midas DP48, for instance. Analog Devices A²B audio bus is used as the technology basis and uses simple XLR connectors between two devices to transmit 32 channels of uncompressed 48kHz 32bit audio.&lt;/p&gt;
    &lt;p&gt;A StageConnect/A2B main-device uses a virtual I2C-connection through the A2B-link that is routed via the AD242x-IC to the Arduino microcontroller.&lt;/p&gt;
    &lt;p&gt;Include the main-library as well as the I2C-Wrapper-Class, instantiate both classes and setup the Wire-library:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;StageConnect.h&amp;gt;
#include &amp;lt;ci2c_com.h&amp;gt;

Ci2c_com i2c_com;
StageConnect stageConnect(false, 1, 0xD0, &amp;amp;i2c_com);

void I2C_RxHandler(int numBytes) {...}
void I2C_TxHandler(void) {...}

void setup() {
    Wire.begin(0x3D);
	Wire.onReceive(I2C_RxHandler);
	Wire.onRequest(I2C_TxHandler);
}
&lt;/code&gt;
    &lt;p&gt;stageConnect.update() should be then called every 100ms.&lt;/p&gt;
    &lt;p&gt;Have a look into the example-sketch to learn how to use the callbacks and the mailbox-system to receive channel-names from the host-device.&lt;/p&gt;
    &lt;p&gt;For electrical connection have a look at the following picture showing the connection with the AD2428MINI evaluation board:&lt;/p&gt;
    &lt;p&gt;Analog Devices suggests cable-length of up to 15 meters, while Behringer allows longer cables. The filter-network of the AD242x-chips is quite tricky and needs caution on adjusting the hardware-parameters. Use the schematics of the AD2428MINI as a reference.&lt;/p&gt;
    &lt;p&gt;Read the files in the folder "Documentation" for more information about the configuration-options and technical details regarding the brand- and product-ID as well as the specific commands for the mailbox-system.&lt;/p&gt;
    &lt;p&gt;Please use brand-IDs above 0x80 to prevent interference with other products.&lt;/p&gt;
    &lt;p&gt;Many thanks to MusicTribe/Behringer for sharing detailed information about the used protocol and for the permission to share the code publicly.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/OpenMixerProject/StageConnect"/><published>2025-10-18T05:52:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45625848</id><title>Life, Work, Death and the Peasant, Part V: Life in Cycles</title><updated>2025-10-18T16:10:49.743780+00:00</updated><content>&lt;doc fingerprint="3e109b3373c107fb"&gt;
  &lt;main&gt;
    &lt;p&gt;This is the fifth and final part of our series (I, II, IIIa, IIIb, IVa, IVb, IVc, IVd, IVe) looking at the structures of life for pre-modern peasant farmers and showing how historical modeling can help us explore the experiences of people who rarely leave much evidence of their day-to-day personal lives. I’ve been stressing this over and over again, but it is worth repeating, peasant farmers make up a simply majority of all humans who have ever lived, and yet we generally have very little evidence for their lives, because they were rarely literate and thus do not typically write to us.&lt;/p&gt;
    &lt;p&gt;We’ve talked about the patterns of marriage, of birth, of death, of subsistence in farming and spinning and weaving, the innumerable maintenance tasks that keep the household running and the pressures that elite extraction – omnipresent for our peasants – exert on the system.&lt;/p&gt;
    &lt;p&gt;This week I want to try to put it all together, taking our models and transmuting them into a sense of what life in these communities was like, with its hardships and its joys. In particular, this is an effort to take our models – which exist mostly as numbers – and turn them into something approaching a narrative, a digital-to-analog conversion that I hope can capture a bit more of the nature of life for these people. That narrative is going to follow one of the dominant ways early agrarian societies thought about time: not as a linear progression, but as a sequence of cycles, from the smallest to the largest.&lt;/p&gt;
    &lt;p&gt;But first, if you like what you are reading, please share it and if you really like it, you can support this project on Patreon! While I do teach as the academic equivalent of a tenant farmer, tilling the Big Man’s classes, this project is my little plot of freeheld land which enables me to keep working as a writers and scholar. And if you want updates whenever a new post appears, you can click below for email updates or follow me on Twitter and Bluesky and (less frequently) Mastodon (@bretdevereaux@historians.social) for updates when posts go live and my general musings; I have largely shifted over to Bluesky (I maintain some de minimis presence on Twitter), given that it has become a much better place for historical discussion than Twitter.&lt;/p&gt;
    &lt;p&gt;Before we launch in, I am going to be referring to the members of our model households a bunch, so if you need the reference, here is the table showing who is who (by relationship to the householder, whose name is in bold):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;The Smalls (4 members)&lt;/cell&gt;
        &lt;cell&gt;The Middles (6 members)&lt;/cell&gt;
        &lt;cell&gt;The Biggs (10 members)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mr. Smalls (M. 40)&lt;p&gt;Householder&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Widow Middles (F. 46)&lt;p&gt;Mother&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Widow Biggs (F. 50)&lt;p&gt;Mother&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mrs. Smalls (F. 32)&lt;p&gt;Wife&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Mr. Middles Jr. (M. 27)&lt;p&gt;Householder&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Mr. Matt Biggs (M. 43)&lt;p&gt;Householder&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;John (M. 14)&lt;p&gt;Son&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Mrs. Middles Jr. (F. 22)&lt;p&gt;Wife&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Mrs. Maddie Biggs (F. 33)&lt;p&gt;Wife&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Jane (F. 6)&lt;p&gt;Daughter&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Fanny Middles (F. 4)&lt;p&gt;Daughter&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Mark Biggs (M. 16)&lt;p&gt;Son&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Freida Middles (F. newborn)&lt;p&gt;Daughter&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Matilda Biggs (F. 12)&lt;p&gt;Daughter&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Freddie Middles (M. 16)&lt;p&gt;Brother&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Mary Biggs (F. 8)&lt;p&gt;Daughter&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mr. Martin Biggs (M. 28)&lt;p&gt;Brother&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mrs. Martha Biggs (F. 22)&lt;p&gt;Sister-in-Law&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Michael Biggs (M. 4)&lt;p&gt;Nephew&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Melanie Biggs (F. 1)&lt;p&gt;Niece&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;The Shortest Cycles&lt;/head&gt;
    &lt;p&gt;As you may have already teased out of our models, the lives of these peasants work in a series of cycles. There’s a reason agrarian societies of these sort often do not think in terms of time as a linear progression, but instead as a set of ‘ages’ or ‘cycles,’ with the present, in a sense, endlessly repeating in a static sort of rhythm. For these societies technological and social progress, while real is often so slow as to be almost or entirely imperceptible on a normal human lifespan. For instance, we can see that between 1000 and 1800, changes in spinning and loom technology are going to radically change the labor efficiency of that task, but for a peasant in c. 1100, for whom the technology of spinning and weaving has been constant for centuries, that is not obvious. Indeed, consider the peasant woman in c. 1500 working a non-treadle spinning wheel – things might seem almost as static. For a society with limited literacy, she relies on ‘living memory’ to understand change and that technology, introduced in Europe in the 1200s, hasn’t changed massively in three centuries, so that woman’s mother, grandmother, great-grandmother (we’ve already well passed the limit of living memory here), great-great-grandmother, great-great-great-grandmother, and on for close to fifteen generations have been using basically the same device.&lt;/p&gt;
    &lt;p&gt;Change in these societies isn’t always that slow – though when it is faster, it is often traumatic for communities that simply are not built to handle rapid change – but it frequently is. So time doesn’t seem like a march endlessly into the future, but like a wheel spinning. Or more correctly, wheels within wheels, like a clock: there is the spin of seconds, minutes, hours, days and so on – smaller cycles within larger ones, with the largest cycle our peasants, as humans, can observe being their own lifetimes.&lt;/p&gt;
    &lt;p&gt;But let’s start with the smallest cycle: the day.&lt;/p&gt;
    &lt;p&gt;The day begins early, a bit before true sunrise. Contrary to what one might imagine from pop-culture, artificial light, such as is available, is provided by lamps and candles, not torches and is, in any event, expensive. Our peasants rely almost entirely on sunlight instead, so sunlight defines much of this daily cycle. The women are likely to be moving earliest, as they’re expected to prepare breakfast for the men before they set out into the fields or to other tasks. For Mrs. Smalls, the daily set of chores is going to be especially packed as she has to shoulder most of the burden for the household, but by this point little Jane Smalls is an extra helping hand – not old enough yet to do many tasks fully on her own, but already well into the task of learning them.&lt;/p&gt;
    &lt;p&gt;By contrast in the larger households, there’s likely a bit of a division of the morning’s labor between the grown women. Widow Middles might start preparing breakfast – something simple, like pottage – while Mrs. Middles Jr nurses tiny Freida Middles and prepares to head out to fetch water from the well. For The Biggs, water-fetching is one of Matilda Biggs’ chores, while Mary Biggs helps her mother stoke the hearthfire and start warming breakfast. Martha Biggs, of course, has two children, one nursing, to rouse and look after.&lt;/p&gt;
    &lt;p&gt;In all three households, the women work something like a team, directed typically by the eldest matron, dividing tasks. They are rarely alone, nor do they stay forever sedentary in the house. Instead, with children in tow, they are on the move quiet a lot, moving to different parts of the farmhouse – which, recall, includes working and storage spaces as well – as well as heading out to pick up water, heading into the village to check on neighbors and so on. Remember that horizontal social relations in this society aren’t just a nice bit of socializing – maintaining those ties is crucial to the household. If there’s a lot of field work distant from the house, it wouldn’t be unusual for them to also plan to bring lunch out to the men in the field (though equally the men might carry lunch out with them or, if working close to home, return for a hot lunch).&lt;/p&gt;
    &lt;p&gt;As the sun comes up, the men are dressing, gathering tools and eating breakfast. Their work during the day won’t be isolated either: they’ll head out in groups and work in the fields effectively as a team. My best sense is that around seven or eight is when we’d expect to start seeing these boys supporting the farming tasks of their fathers and be expected to function as adults, performing the full range of tasks around fourteen or so, though the exact ages here will vary culture to culture. Just as with the women of the household, the senior adult male is likely to direct the collective labor of the men and boys in his household. For the Smalls, that is simple enough: John is learning how to be a farmer by helping his father, but you can easily imagine these relationships being a bit more complex for Mr. Middles Jr. and Mr. Matt Biggs, both of whom have a grown brother still in the household who is likely to chafe to some degree at the control exerted by the householder (something we’ll come back to).&lt;/p&gt;
    &lt;p&gt;The work day in the fields is likely a full one – in farming, there is always more that could be done, but our farmers are setting their own pace and schedule, with rests as necessary (or at least, as deemed necessary by the head of household). Village farming would often be itself a cooperative effort not only within households but between them, so we might very well see multiple households working together on shared or adjoining plots as well. It is worth remembering that the topography of these villages is not spread out like homesteads: for the most part the farmhouses are nucleated in the village itself, with the fields spread out around them, so walking from one house to the next to visit, ask favors or coordinate group labor is quick enough.&lt;/p&gt;
    &lt;p&gt;In all of these activities – men working in the fields, women working in the home, getting water, moving through the village – our peasants are rarely alone, for better and for worse. For the better, the social aspect of these activities are very strong: they chat with each other while working, they might sing together, they certainly pray together. Horizontal ties are also important to survival – perhaps while going through the village to fetch water, Mrs. Smalls might stop by at the Biggs to speak to Maddie Biggs (a sister, or perhaps a cousin, in these close-knit communities) to see if she can’t get some extra spun thread, perhaps in exchange for some grain, vegetables or such.&lt;/p&gt;
    &lt;p&gt;On the other hand, eyes are always watching and these are societies which expect the individual to place the community first, with individuals valued to the degree that they fill a communal role. There’s very little space for self-expression here and throughout the day, every day our peasants exist within a hierarchy beneath the male head of household. The older women direct the other women and girls, the men direct the boys and the male householder, who by law is the one that owns (or has claim to) the productive asset (land) that enables the household is the final decider on basically everything. But those heads of household are hardly able to make all of their own decisions either, constrained by the need to remain ‘respectable’ in a community that demands conformity and by debt or peonage to the Big Man.&lt;/p&gt;
    &lt;p&gt;There might be a break in the field work around midday. While women in these societies work more hours overall, the physical demands in strength and endurance in field labor are very high, so that midday rest – siesta – is an important way to conserve strength. Artists looking to capture ‘pastoral simplicity’ often seem to depict these sorts of breaks, which can give a deceptive sense of what the farming day is like: periods of rest alternated with periods of quite intense physical labor.&lt;/p&gt;
    &lt;p&gt;While the men are in the fields, the women are working through the myriad maintenance tasks necessary to keep the farm running: storing and preparing food, watching children, cleaning living and work spaces, maintaining the hearthfire and such. The diet was heavily based on grains – wheat, barley and the like would be providing a majority of the calories for most peasants – but that doesn’t mean that other types of food (legumes, meat, vegetables, fruits) were entirely absent, simply that they existed within grain-based system. The proportion of calories coming from grains would have always been high, but varied based on time and place, with wealthier societies having marginally more varied diets; a good ballpark for the calorie proportion of grains is around 75%, higher in some places, lower in others.1&lt;/p&gt;
    &lt;p&gt;That grain might take the form of bread, of course, which once baked could be taken into the field and eaten cold, but equally it could be made into porridge or (thinner) gruel). In all of these societies I’ve investigated, the task of managing food stores fell to women, who had to figure out how to make a meal out of whatever was available. Many foods would thus naturally be seasonal, especially with limited options to preserve meats, fruits or vegetables.2 The frequent recourse was to stews, which could be made with essentially whatever was available, especially if anything was liable to spoil if not eaten soon – ‘perpetual stews‘ kept over relatively long periods with new ingredients added regularly as they became available were one option to ‘use up’ any odd bits of food.&lt;/p&gt;
    &lt;p&gt;It is easy to over-idealize ‘home cooking’ – some peasant women, doubtless, were clever and creative cooks, but when you make all women cook, naturally you are going to have some indifferent or ineffective cooks as well (the same way that when you make all men farm, chances are some of them are not very good farmers). Almost regardless of cooking skill, peasant cuisine would have been mostly remarkably bland by modern standards, based heavily in grains, with few options to add sweet or savory flavors. Alongside the actual cooking of the meals, there’s quite a bit of work involved in cleaning cooking and eating surfaces, as it all has to be done by hand and without the aid of modern cleaning supplies that can loosen things like grease.&lt;/p&gt;
    &lt;p&gt;Our peasant women are also engaged in a mix of childcare activities. Matilda Biggs, at 12 is likely working along side her mother or grandmother effective as an adult; childhood doesn’t last long in these societies and by her age Matilda is already likely a relatively proficient spinner and more than able to help with cooking and other household tasks. Our three households also have a number of teenage boys (John Smalls, 14, Freddie Middles and Mark Biggs, both 16). While they probably aren’t yet legal adults (ages of majority in these societies for young men range from 15 to 21), by those ages these boys are expected to work like adults, so they would be in the fields alongside their fathers and older brothers (and would have been helping out in the fields in some capacity probably since around age 7). Likewise, as noted above, little Jane Smalls is probably transitioning into this state and so while she is with her mother, Mrs. Smalls is increasingly expecting Jane to be an active worker alongside her.&lt;/p&gt;
    &lt;p&gt;For the younger children, the two nursing infants (Freida Middles and Melanie Biggs) are going to need to be with their mothers. However for children in the age 4-7 bracket – too young to really start working, but old enough not to need constant supervision – the parenting style was, somewhat necessarily substantially ‘free range.’ Fanny Middles and Michael Biggs (both 4) might be in this period of their life, which really was the last gasp of ‘childhood’ as we understand it, as a period of play and learning rather than labor. The women (and older girls) of their households will be keeping an eye on them, in between the necessary work tasks the day demands.&lt;/p&gt;
    &lt;p&gt;Throughout all of these activities – cooking, cleaning spaces in the home, cleaning cooking and eating surfaces, watching children – our women are working textiles. For much of Eurasia, that will mean primarily wool, though linen and other fabrics are certainly used. With more capacity for spinning than strictly required, the women of the Middles and the Biggs might focus some of their efforts on producing modestly nicer clothes, while poor harried Mrs. Smalls will have to struggle just to replace worn out clothing. Nothing spun and wove will be wasted: worn out clothes are turned into children’s clothes, patchworks, rags or whatever else they can be used for, until they basically disintegrate.&lt;/p&gt;
    &lt;p&gt;As the sun begins to set, the farming work parties in the fields will start heading back home, while their wives, sisters and daughters prepare the evening meal. The day has had its cycle, from morning tasks, to the main of the work day, to evening tasks and finally, as the sun sets and work is no longer possible without artificial light that is simply too expensive for our peasants to use in any kind of quantity, to sleep. There is almost no dedicated leisure time during the day. There is a regularity to the cycle, a monotony – each day more or less like the one before it and the one after – one imagines it was comforting to some peasants and deeply constricting to others, shaped by the continuing demands of peasant labor (itself structured by the heavy extraction regime they operate under, which consumes the leisure time they might otherwise have). The next day, they’ll rise and repeat the cycle.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Year&lt;/head&gt;
    &lt;p&gt;If that monotony was all there was, one might imagine most peasants would give into despair, but while most days were just like the ones before and after, the peasant calendar had all sorts of cyclical changes. The cyclical nature here is worth stressing: most of the events that broke up the yearly cycle happened every year at roughly the same time, so they too became part of the routine of life. Peasants wouldn’t go ‘on vacation’ (though they might go to war, more on that below) or do other things that we do to disrupt the cycle, but the cycle had its breaks.&lt;/p&gt;
    &lt;p&gt;The next smallest cycle was a more-or-less weekly cycle of days, with a day of rest or religious observance at the week’s end. The Abrahamic faiths all have a weekly day of religious observance on which work is to be avoided or at least limited by religious activity, while the Romans had the nundinae (‘ninth-days’) every eight days (they’re counting inclusively) which were days for rest or attending local markets. So while that tends to mean these societies had six work days out of seven (or seven out of eight) there was, for our peasants, a day that offered, if not a rest, at least a change of pace and a chance to gather for larger social events on a regular schedule.&lt;/p&gt;
    &lt;p&gt;Beyond this was the annual cycle, all-important for farmers whose crops had to be planted and harvested at the correct times. That cycle isn’t universal, but depends on the planting and harvest times for the local major cereal crop. Spring wheat is planted in spring (usually around April) and harvested in late summer or early fall (often in August) while winter wheat is typically planted in early Fall (sometime between late September and November) and harvested in early Summer (typically June or July). These dates shift around a bit depending on local climate as well, which is why I’m offering rough ranges.&lt;/p&gt;
    &lt;p&gt;Using a winter wheat schedule, since that’s what I’m a bit more familiar with, and I am going to follow roughly a Roman agricultural calendar, the Menologium Rusticum Colatianum, but note that the exact timings here would vary depending on local climate and such. Plowing would begin in September; this was hard, backbreaking work (even with a plow-team of animals) but requires relatively few hands and has a decent amount of time for it in the calendar. It’s worth noting that fallowed fields also need to be plowed, usually once each in the fall, spring and summer. October, just before the plants go in the ground, another plowing and manuring. November brings the first labor peak in the planting season (done at the last plowing) and in some cultures we’ll see women in the fields helping get the seed into the ground at this point.3 Beans – an important food in Roman crop rotation and also simply to provide a source of protein – are planted in December.&lt;/p&gt;
    &lt;p&gt;The calendar then lightens a little bit while the plants are in the ground, but this isn’t necessarily ‘spare time’ as months without major pressing agricultural jobs are when our peasants need to do all of the gathering, fixing, preparing and such they can’t do during the hard months. January is a good time for repairing tools, cutting trees, fixing buildings and such (note this is a warmer climate; in a colder one, you’d want your firewood gathering done before winter). Hoeing is done in February. March is a slower month (and not an accident then, that it is the traditional month for the Roman dilectus – if you must call everyone to Rome for the draft, do it in a month that doesn’t have heavy labor demands). April is when sheep are washed and shorn and also when weeding might begin, although on the Roman calendar weeding more properly belongs to May. The big task in June is haymaking (although that may begin in May according to Columella) and then in July comes the harvest, the highest labor-demand part of the year, running into August. As we’ve noted, the harvest brings everyone available out into the fields to reap, thresh and winnow the grain so that it is ready for storage.4 The rest of August is often a break from all of that work – another chance to repair tools, buildings, fences and so on – and then the cycle starts again.&lt;/p&gt;
    &lt;p&gt;That annual calendar, of course, structured agricultural work, determining the kind of labor that our peasants (mostly the men, in this case) but it was also a calendar of anxiety for our peasants. After all, roughly three-quarters of the households annual calories came in during a single month – July in in a winter-wheat region, August if you’ve planted spring wheat. Remember our quote of Theophrastus, “the year bears the harvest, not the field.”5 Meaning harvests were significantly variable one year to the next and as we’ve seen our peasants do not have large margins of error in terms of their production. Most of our model households were already falling short of their ‘respectability’ requirements, but a year where the harvest was, say, 75% (or worse yet, 50%) of its expected yield could put them in serious danger of shortage. The margin of error here is actually tighter than our model suggests: we haven’t accounted, for instance, for spoilage of grain in storage over the year.&lt;/p&gt;
    &lt;p&gt;So our peasants have Janus-like worries, looking forward and looking back. Looking forward, once the seed is in the ground, while our peasants can weed and watch as carefully as they can most of the process is out of their control. Too much rain or too little, weather too hot or too cold, can ruin the harvest (while ideal conditions might produce a ‘bumper crop’ and a good year), but the farmer can only sit and watch and work on other tasks and desperately hope.&lt;/p&gt;
    &lt;p&gt;Hope, because if the harvest is poor and food is short, someone needs to be underfed. The household has to survive and that means the working adults need to have enough strength to do the farming and the household cannot shortchange the seed set aside for the next year. So the first people to tighten their belts and go with less in a shortage are the very old and the very young. Reduced nutrition in turn renders them vulnerable to sickness or injuries that in another year might be easily overcome, with the greatest vulnerability for very young children (for whom sickness can be a significant concern even with modern medicine). The Smalls are perhaps the least vulnerable household here – a bad year might stunt Jane’s growth, leaving her a bit shorter or with other developmental problems, but it probably won’t kill her. But for the Middles and the Biggs, a poorly timed bad year is quite possibly a death sentence for little Freida Middles and Melanie Biggs; it has a distressing chance of claiming Widows Middles and Biggs too.&lt;/p&gt;
    &lt;p&gt;So the peasant farmers plow their fields and plant their seeds and hope.&lt;/p&gt;
    &lt;p&gt;Meanwhile, of course, there is the food from last year’s harvest. Grain is hard to keep in the conditions these peasants can create much longer than a year or two, but it wouldn’t matter too much if they had radically better storage given that they’re barely producing enough for their own needs anyway. The task of preserving, preparing and if necessary, rationing that food generally falls to the women of the household. And they don’t lack for worries either: because the grain all comes in at once, should anything happen to it, the result could still be shortage. And there are a lot of things that can happen to it: pests, spoilage, theft or fire, for instance. So our peasant woman are, each week, keeping track of how much food remains in storage and measuring it out accordingly. If they’ve got more food than necessary, that might allow them to banquet their neighbors or to sell some grain to get tastier foods (e.g. meats) they might not normally have. But if the harvest was poor, or there’s an unwelcome surprise (say, a sack of grain develops mold), then the job suddenly becomes trying desperately to stretch what is there to the next harvest, filling gaps as much as can be done with whatever crops and produce become available in the meantime. And if necessary, knowingly shortchanging the elderly or the little ones to make sure there is enough for the rest.&lt;/p&gt;
    &lt;p&gt;(Of course there is also the peasant woman’s own nutrition to think about. Remember, she is very often pregnant or nursing, which produces substantial nutritional demands on her body, which can endanger her health, potentially cause a pregnancy to miscarry or reduce the amount of nutrients she can pass through nursing to her child, endangering the child’s health. There’s no way out of the calorie calculus: someone has to go short and both our families and mother nature will tend to prioritize health adults over children and the elderly.)&lt;/p&gt;
    &lt;p&gt;So the calendar is a cycle of anxiety, relief and despair: anxiety as the family waits for the harvest, watching the skies for weather and the pantry for its steady depletion. Glorious relief if the harvest is good, the pantry restocked, another year survived and despair if it is poor, which at best likely means seeking aid – with many strings attached – from the Big Man with his Big Estate and at worst means the household loses some of its most vulnerable members, the “vacant seat…and a crutch without an owner.”6&lt;/p&gt;
    &lt;p&gt;But the yearly calendar is not just the harbinger of threat and anxiety: it is also the bringer of joy and society, because the year is studded with festivals, days of rest and social gathering, joy and merry-making. Of course we still have holidays too, but I wonder if we don’t miss their potency to pre-modern farmers because – with, at least for some of us, eight-hour-work-days, two-day weekends and built-in vacation days – they are not our only escape from labor.&lt;/p&gt;
    &lt;p&gt;In any case, for those long days in the fields or the long hours of spinning thread while keeping one eye on the large pot and the other on the tiny tot, our peasants would be looking forward to the next festival, the next feast day, the next major event which might have music and dance and special foods. The break in the monotony of food is a significant one: in addition to peasant families putting in extra effort and deploying their relatively limited access to tastier meats, dairy and such, festivals often served as an opportunity for the ‘Big Men’ to engage in conspicuous wealth display by providing finer foods. In ancient polytheistic cultures, religious festivals generally involved animal sacrifices at scale, paid for by either the state or the very wealthy (often a bit of both) with the meat from those sacrifices cooked and given out to the celebrants, a chance for the elite to cement their hold over the community. In the Middle Ages, certain holidays might include similar traditions, where the local lord or other big man might throw a feast for the commons.&lt;/p&gt;
    &lt;p&gt;Many of these festivals were single-day affairs, but some could be multi-day events, a period for rest, socializing, singing, story-telling, and general merry-making. One very common festival motif was a ‘fool’s feast,’ – a festival predicated around brief social inversion, like the Roman Saturnalia (celebrated in December) or various ‘Feasts of Fools‘ in the Middle Ages. These sorts of festivals often created a space for a kind of tongue-in-cheek parody and mockery of authority and thus potentially a space for the lower classes to ‘let off steam’ in a way that didn’t threaten elite power, reducing the strain created on a society with such tremendous and conspicuous inequality and functionally no social mobility.&lt;/p&gt;
    &lt;p&gt;So our peasants, doing their work, have a lot to look forward to, good and bad. Mrs. Smalls, terribly overworked, might be spinning at home while Jane sweeps out the kitchen and the stew slowly boils, thinking about a coming festival – her household has food to spare so perhaps she’ll sell or exchange some for some fancier foods (perhaps to bake some cakes) to impress her neighbors (though she worries her good kirtle – that’s the top layer of the typical European medieval dress – is getting a born worn, but she’s short on fabric and needs to replace her husband’s work tunic before she can work on a new kirtle). Meanwhile, Mr. Matt and Martin Biggs are nervously watching the sky: everyone in their household has worked as hard as they can, but they need so much land and the tenancy terms for some of it were not good, so they need a good harvest to avoid a shortfall. Everyone adores little Melanie but at just one year old, she’s certainly not out of danger yet for a bad year.&lt;/p&gt;
    &lt;p&gt;John Smalls and Mark Biggs, both young men, are looking forward to that coming festival too, but for the chance to play some games and attend some dances and perhaps try to catch the eyes of some of the eligible girls in the village. Of course they’ll also need to impress those girls’ families – it will be their fathers, in this society, who decide who they marry – but John and Mark both stand to eventually inherit their farms and so have a fair bit to recommend them as matches. Freddie Middles, the same age, is not so fortunate – his brother’s farm, on which he works, is likely to go to a son if Mr. Middles has one and in many of these societies, might still go to his daughters – and so he thoughts drift a little further into the future, as he has relatively little chance of being marriageable in the village (the same, in a decade’s time, will be true of little Michael Biggs, but right now he is still of the playing age).&lt;/p&gt;
    &lt;p&gt;Which leads us now to the larger cycles.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Family Cycles&lt;/head&gt;
    &lt;p&gt;The bigger cycle is the generational cycle. If we think of these households in the long term, they tend to go through broad cycles. The foundational work on this idea is by Alexander Chayanov from work on Russian peasants,7 but as we’ve seen the precise family formation and household structure do vary depending on when peasants marry, mortality rates and also when they form new households (in particular, extended kin-groups and social relationships across multiple households soften the sharp transitions in Chayanov’s basic model).8So the exact cycle is going to vary a bit, but the basic ideas are a good framework.&lt;/p&gt;
    &lt;p&gt;We can think about it very simply thinking in terms of our smallest, simplest household: the Smalls. We can start imagining the Smalls household perhaps 10 years ago when Mr. Smalls is 30 and Mrs. Smalls is 22. John is, at that point, 4 – too young to work – and Jane is not yet born. But as you’ll recall from our childrearing model, while John and Jane are Mrs. Smalls only surviving children in the present, they are unlikely to have been her only children. That age gap between John and Jane is likely to be ‘filled’ by a child – James Smalls – who died at age 2 but 10 years ago is still living (one and a half). Mr. Smalls’ father was still alive then too – 62 years of age – but increasingly limited in his ability to work. When Old Man Smalls died, that left a household with two adults, but 2 small children and thus quite vulnerable, with a low ratio of workers to dependents. That might explain why poor James Smalls didn’t survive and also Mrs. Smalls’ repeated miscarriages: in the bad years, with Mr. Smalls only able to work so much land, food ran short and the children suffered.&lt;/p&gt;
    &lt;p&gt;But now, now John is of working age and Jane will be as well soon. If we imagine the Smalls’ household over the next, say, five years, we might expect Mr. Smalls is likely to be alive and able to work all five years, and John the same, but John Smalls won’t marry and start having his own children for quite some time. Meanwhile, Jane will be more and more helpful for her mother until she hits marriageable age which, as we’ve seen, varies by culture. These are the ‘peak’ years for the household in terms of labor-per-dependent. And while Mrs. Smalls, remembering well how the generosity of neighbors got Jane through a bad year when she was young, is thinking about how to use their increased labor to pay back favors, Mr. Smalls is thinking that he has a chance, in these years, if he works hard, of pulling together just enough money (or other resources) to maybe buy an extra field or two and thereby make the family more secure for generations to come.&lt;/p&gt;
    &lt;p&gt;That aim is possible, but the odds are stacked against him. For one, remember: the yield is born by the year, not the land. So while Mr. Smalls might want a boom harvest year to get a lot of grain to sell to pull together the capital he needs to expand, that bumper year is likely to be a good year for everyone and to cause grain prices to fall accordingly. But of course in a bad year, even the Smalls’ don’t have the wide margin for error to benefit from rising grain prices (unlike the Big Man in his estate).&lt;/p&gt;
    &lt;p&gt;Moreover, most of these peasant societies are dowry societies, meaning that the father of the bride is expected to lay down money, resources or land when his daughter marries and of course Jane must – by the rules of these societies (which we are describing, not endorsing) – must eventually marry. That time is quite a ways off, but Mr. Smalls has to think that in the next 10-15 years he needs to have put together enough of a reserve to provide that dowry, otherwise his failure of foresight will harm her marriage prospects. At the same time, in a decade, John will also be looking to potentially marry. That might bring in a dowry, but it will also in short order mean more children and the family shifting back to that first phase again. Mr. Smalls might seek to delay that marriage, but he can hardly do so forever. So while his household right now is reasonably secure, as peasants go, he is scrimping and saving with the hope that over the next few years he can just barely thread that needle in order to get those extra fields while the family is at its peak productivity…so that John won’t need to suffer through losing four very much wanted children (two to miscarriage, two to infant mortality) like he and Mrs. Smalls did.&lt;/p&gt;
    &lt;p&gt;As an aside, there is a tendency for modern folks and especially modern popular culture to assume that people in the past dreaded the sort of frequently arranged marriages they had, but this isn’t the impression we get. People in the past, after all, tended to share their societies values, and in these societies a lot of value was placed on marriage and the legitimate children it produced. Marriage was, for women especially, but also for most men, a necessary step towards adulthood and status in the community and so in most cases seems to have been anticipated, yearned for and welcomed when it came.&lt;/p&gt;
    &lt;p&gt;So while the Biggs brothers look, worried at the sky and the lack of rain clouds, worried about this year, Mr. Smalls is right there with them, worrying about the future. As humans are wont.&lt;/p&gt;
    &lt;p&gt;All of these households are going through similar cycles: periods where they are more vulnerable and less vulnerable. Though they mostly don’t see it in those terms. Instead, they see it in terms of generational events: the celebration of marriages (always a good occasion for a party), of births and of course of deaths. Alongside the annual festivals, these too create a cycle of anticipations and moments (of both joy and grief). None of our households have an unmarried child of marriageable age (an oversight of mine in plotting them out), but we might well imagine that all three households know that one of the other village households is anticipating the marriage of their daughter to a boy from the next village over and everyone is looking forward to the party that will entail. And, of course, just a few months ago the Middles were celebrating the arrival of little Freida.&lt;/p&gt;
    &lt;p&gt;For most of these peasants, there is no real escape from these cycles: one generation of Smalls, Middles and Biggs after another, each mostly the same as the last. And that is often the cultural ideal of these societies: people are born into roles and the expectation is that they will fill those roles. One day after another, one year after another, one generation after another. And within the vision permitted by a single human life, that pattern mostly holds. John Smalls and Mark Biggs can expect to marry in another 10 or 15 years; that their fathers will pass away perhaps in another 15 to 25 years and that they will then absorb the role of head of household, their wives taking the place their aging mothers held, with a new generation on the way behind them.9&lt;/p&gt;
    &lt;p&gt;Students sometimes find it odd to the point of absurd that these societies often concieve of time generally as cyclical rather than progressive, imagining that time can kind of loop back on itself if you wait long enough. But from this vantage, it doesn’t seem so absurd: the technology and conditions that shape these farmer’s lives largely don’t appear to change without the benefit of longue durée history that they (and even the aristocrats who lord over them) lack. They are so incredibly distant from their pre-farming roots (far more distant from them than they are from us) that they do not remember a time before this kind of living and cannot imagine a time after it. Instead the cycles of their lives seem to stretch out endlessly into the past beyond living memory and into the future beyond imagination.&lt;/p&gt;
    &lt;p&gt;For most of them, at least. For Freddie Middles, the picture is different: his brother inherited the farm and now has children. In societies where girls cannot inherit, Freddie might hope that his brother and sister-in-law will remain without a son – and the odds on that aren’t terrible given the high infant mortality – but in societies where daughters do inherit, he is pretty much already likely out of luck. He can probably remain in his brother’s household and be reasonably safe from want – he’s a good source of labor, so unless his relationship with his brother is really bad Mr. Middles is going to want to keep him around – but as he looks at his future, he’ll never be head of this household and will struggle to marry.&lt;/p&gt;
    &lt;p&gt;A cautious fellow might stay in his place regardless. but Freddie isn’t a cautious fellow. Because the supply of men like Freddie is virtually guaranteed in societies that have such static structures – far too static for the fluid family formation dynamics underneath them – they have ‘release valves’ for this sort of thing – which these societies understand (not entirely unreasonably) as ‘land scarcity’. One very common release valve is military service. If these were families in the Roman Republic, we might expect Freddie to be the sort of fellow who volunteers for additional military service – which both removes his mouth-to-feed from the household, but also offers him the opportunity (however narrow) of getting enough loot and pay to be able to form his own household. Likewise in the Middle Ages and especially in the early modern period, Freddie is the sort of fellow we might see enlist as a mercenary or professional soldier. Alternately, Freddie is precisely the sort of fellow we might see showing up to do some Greek or Phoenician style colonization.&lt;/p&gt;
    &lt;p&gt;In a decade or so, Michael Biggs is likely to be in a similar spot: his Uncle Matt and father Martin had enough of an arrangement to try to squeeze two family units into a single household (which is part of why the Biggs’ are more tightly economically constrained than the other households), but that sort of thing isn’t likely to continue forever. Chances are Mark Biggs, when he becomes the head of the household, would be happy enough to keep Michael on as a worker, but probably not to play host to Michael forming a family with yet more dependents in a household that is already too big. Of course either of these young men might ‘luck out’ with a marriage to a young woman with no brothers who is thus set to inherit her father’s farm (in societies where that inheritance pattern can happen), but some significant number of young men are going to be ‘thrown off’ by this system, providing a small but meaningful ‘floating’ workforce of young men with dim futures in the countryside, to do day-labor in the cities or service in the armies.10&lt;/p&gt;
    &lt;p&gt;It is notable that of the peasants, if we hear about any of them, it is often the peasants shunted through these ‘release values’ to become something else that arrive in our history. But of course I feel the need to note that the future for most such members of the peasantry was pretty grim: most would die poor and frequently young, compared to the tiny handful of unusual successes that make it into our books.&lt;/p&gt;
    &lt;p&gt;But of course, even for Freddie Middles and Michael Biggs, those disappointments are in the future. In the present Freddie has work and Michael has play and both are looking forward to that festival, mostly because they heard tell that Mrs. Smalls is planning to make her village-famous cakes and they remember those fondly from last year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;This final post in the series is, of course, a bit more ‘made up’ than the rest: I am doing my best to ‘fill in’ the color between the black and white lines of our models. But I think there is value in that, so long as we are reminded that the color is not original, these are ‘colorized’ pictures, not color pictures, to get a sense of what it was like to be a human in these societies.&lt;/p&gt;
    &lt;p&gt;On the one hand you do not work more than a medieval peasant. These households – both the men and women – worked under conditions of extremely low (by modern standards) productivity, meaning they had to put in a ton of labor to get out a fairly small amount of production. We can see that in how limited the surplus they generate under ideal conditions is and even more so just how much labor it takes to keep these families clothed. But we can also see they are also pressed down under the weight of extraction, a regime that squeezes them as hard as it can without quite ever crushing them.&lt;/p&gt;
    &lt;p&gt;That extraction goes to provide for everything else these societies do. It is how the build monuments and temples, how they maintain the lifestyles of the elite, how they provide the time to develop literature, to invent philosophy and mathematics. It is how they feed soldiers, form armies, produce weapons. Everything else that these societies do is done on the backs of these peasants. And from that list, you can easily see great injustice – our peasants could be much better off if our aristocrats were modestly less comfortable. But of course then we also don’t get the Epic of Gilgamesh or innovations in mathematics or engineering, or Greek sculpture, or preserved Greek and Roman literature, painstakingly copied by hand over and over again (mostly by monks being fed off of the produce of – of course – our peasants).&lt;/p&gt;
    &lt;p&gt;And the brutal reality of competition in the pre-industrial world is that agrarian societies which do not find some way to extract resources from their peasants to fund warfare end up conquered by societies which do. There are better and worse ways to do that extraction: it is not the case that the most brutal extraction regimes are the most militarily effective (more on this in my book project; the Roman extraction regime in Italy in the Middle Republic was actually relatively light in terms of taxes, but expected citizen-farmers to support the cost of warfare themselves, which turned out to work really well). But it is the case that the sharp edge of military necessity meant ‘no extraction’ wasn’t a realistic option.&lt;/p&gt;
    &lt;p&gt;So you do not work as much or as hard or with as much difficulty as a medieval or ancient peasant, at least if you are a typical person reading this from an industrialized country.11 But that also doesn’t mean that these peasants lives were nothing but grim starvation.&lt;/p&gt;
    &lt;p&gt;They were humans. They loved and cared about people, they made plans (which often fall through), had hopes and desires tailored to the constraints of the situation they were born into. It is true that, for the most part, they weren’t going anywhere – at least, not on the time scale of a single generation – but that wasn’t typically what they or their society valued. Indeed, major disruptions – which we haven’t dealt with here – were generally of the bad kind. That lifestyle might feel to us to be stultifying and it certainly felt that way for many of our peasants: when factories and jobs in the cities at last did appear, peasants did not have to be chased into them, large numbers of them voluntarily quit the countryside for new horizons and opportunities (to the point that their landlord overseers sometimes scrambled to find ways to prevent them from leaving).&lt;/p&gt;
    &lt;p&gt;But like humans, within the confines of the structures we’ve laid out, they had their full share of joy and grief, of success and hardship, of anxiety and relief. And they liked to have nice things: a nice meal, a carefully made piece of clothing and so on. One of the most striking misconceptions about ancient and medieval peasants is how our popular culture dresses them in brown rags, when these folks made their own clothes and liked to look nice. Their clothes, tools, housewares and homes might have been a lot more worn than ours, perhaps a little thin in places, but they were made and kept with care. It is important not to overcorrect in either direction: the average American closet represents a concentration of wealth that would shock even a rich peasant, but that doesn’t mean peasants went around wearing rags.&lt;/p&gt;
    &lt;p&gt;Likewise, it seems abundantly clear from the evidence we do have that they felt the grief and anxiety every bit as keenly as we do and that they had more of it. But that doesn’t mean their lives were without joy. Indeed, the regular predictable cycles of life left them almost always with something – the next Sunday, the next festival, the next harvest, the next generation – to look forward to with joy and anticipation. If we are shocked by how poor their lives were, they might be shocked by how terrifyingly unstable our lives are – how many people work jobs that effectively didn’t exist a century ago? Or have grandparents who once worked jobs that effectively no longer exist? They might well wonder how we coped with the anxiety of that. At least they knew that their jobs would be there for them, one day after the next, one year after the next, one generation after the next.&lt;/p&gt;
    &lt;p&gt;Perhaps most of all, I hope this sort of project helps to reorient the way some of us think about the past. We’re used to thinking about the past in terms of kings and soldiers, because that is who write to us and thus whose history gets written, to the point that we need to engage in this kind of round-a-bout modeling sometimes to get a solid picture of the structures of pre-modern life. Yet these structures make up the largest part – the majority! – of the entire human experience. Most of humanity lived not like us, nor like our more distant hunter-gatherer ancestors, but as agrarian peasant communities, in societies with very low productivity and high extraction, which changed so slowly the effect was usually imperceptible within a human lifetime.&lt;/p&gt;
    &lt;p&gt;Because of course these peasants were people too, their place in the human story no smaller than yours or mine – but far more numerous.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We can, for instance, see meat, fish and such becoming a more prominent part of the rural Roman diet from the late Republic onwards, on this see Bowes et al. “Diet, Dining and Subsistence” in The Roman Peasant Project 2009-2014 (2021). My ballpark figure is extrapolated roughly from the estimated Roman military diet suggested by Roth, The Logistics of the Roman Army at War (2012), which is about 60% grain by calories and probably represents something like an upper-limit for non-grains in typical rural peasant diets, given how relatively well paid Roman soldiers in the imperial period were.&lt;/item&gt;
      &lt;item&gt;Of course, limited doesn’t mean none: fruit preserves are known since antiquity and meat can be dried or smoked to enable it to last longer.&lt;/item&gt;
      &lt;item&gt;November on the MRC, but it’s worth noting the agronomists have earlier dates – October in Columella and Palladius. For the full rundown of the Roman agricultural calendar, see K.D. White, Roman Farming (1970), 194-5, but note that White expects you to know some Latin to fully read his appendix (he does not translate the activity labels in the MRC and only summarizes some of them).&lt;/item&gt;
      &lt;item&gt;For heavy wine producing regions, the vintage – the bringing in of the grapes to make wine – is a similar ‘all hands on deck’ season. For the Romans, the vintage (vindemiae) came in October, or September for particularly warm regions.&lt;/item&gt;
      &lt;item&gt;Theophr. Caus. pl. 3.23.5&lt;/item&gt;
      &lt;item&gt;Of course I must note that the Cratchits of A Christmas Carol are not peasants, but the emotion evoked here is the same. Bob Cratchit is a clerk, in A Christmas Carol, a member, albeit quite a poor one, of the urban middle class (of, I should note, an industrializing society, not a pre-industrial one). But I find this story is one of the few ones modern readers in wealthy countries know that connects to this sort of parent’s grief, of realizing that unless economic conditions improve, your little one will wither away and die. What Dickens treats as a unique, striking example of poverty would have been the common experience of almost every peasant household at one time or another.&lt;/item&gt;
      &lt;item&gt;Chayanov, The Theory of Peasant Economy, ed. D. Thorner, et al. (1966)&lt;/item&gt;
      &lt;item&gt;For instance, note this study by E A Hammel.&lt;/item&gt;
      &lt;item&gt;A father dying early leaving a son inheriting the household early is one thing that could make these young fellows marriageable a lot sooner than the normal social convention, while in some societies the marriage of a son might be delayed while his father lives (which, as you might imagine, can create some tensions!).&lt;/item&gt;
      &lt;item&gt;Folks will sometimes ask here ‘what about the clergy’ and the answer for most of these societies is that if there is a professional clergy (the Greeks and Romans do not have one) that is where the ‘excess’ sons and daughters of the elite go, but those doors are often at least partially if not entirely closed to our peasants.&lt;/item&gt;
      &lt;item&gt;Of course there are still countries – a shrinking number – with subsistence economies and also people in wealthy countries who are nevertheless very poor. But ‘some (few) countries’ and ‘some (few) people’ is a far cry from ‘functionally everyone all the time everywhere.’ We’re talking averages and medians here.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://acoup.blog/2025/10/17/collections-life-work-death-and-the-peasant-part-v-life-in-cycles/"/><published>2025-10-18T08:31:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45626037</id><title>Fast calculation of the distance to cubic Bezier curves on the GPU</title><updated>2025-10-18T16:10:49.057579+00:00</updated><content>&lt;doc fingerprint="7ebb8b95b86f333a"&gt;
  &lt;main&gt;
    &lt;p&gt;Bézier curves are a core building block of text and 2D shapes rendering. There are several approaches to rendering them, but one especially challenging problem, both mathematically and technically, is computing the distance to a Bézier curve. For quadratic curves (one control point), this is fairly accessible, but for cubic (two control points) we're going to see why it is so hard.&lt;/p&gt;
    &lt;p&gt;Having this distance field opens up many rendering possibilities. It's hard, but it's possible; here is a live proof:&lt;/p&gt;
    &lt;p&gt;In this visualization, I'm borrowing your device resources to compute the distance to the curve for every single pixel. The yellow points are the control points of the curve (in white) and the blue zone is a representation of the distance field.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;All the demos and code in this article are self-contained GLSL fragment shaders. Most of the code can be found in the article, but feel free to inspect the source code of any of these WebGL demo for the complete code. They can be run verbatim using ShaderWorkshop.&lt;/p&gt;
    &lt;head rend="h2"&gt;The basic maths&lt;/head&gt;
    &lt;p&gt;In a previous article, we explained that a Bézier curve can be expressed as a polynomial. In our case, a cubic polynomial:&lt;/p&gt;
    &lt;p&gt;Where a, b, c and d are the vector coefficients derived from the start (P_0), end (P_3), and control points (P_1, P_2) using the following formulas (you can refer to the previous article for details):&lt;/p&gt;
    &lt;p&gt;For a given point p in 2D space, the distance to that Bézier curve can be expressed as a length between our curve and p:&lt;/p&gt;
    &lt;p&gt;Our goal is to find the t value where d(t) is the smallest.&lt;/p&gt;
    &lt;p&gt;The length formula has an annoying square root, so we start with the distance squared for simplicity, which we are going to unroll:&lt;/p&gt;
    &lt;p&gt;The derivative of that function will allow us to identify critical points: that is, points where the distance starts growing or reducing. Said differently, solving D'(t)=0 will identify all the maximums and minimums (we're interested in the latter) of D(t) (and thus d(t) as well).&lt;/p&gt;
    &lt;p&gt;It is a bit convoluted in our case but straightforward to compute:&lt;/p&gt;
    &lt;p&gt;A polynomial, this time of degree 5, emerges here. For conciseness, we can express D'(t) polynomial coefficients as a bunch of dot products:&lt;/p&gt;
    &lt;p&gt;Finally, we notice that solving D'(t)=0 is equivalent to solving D'(t)/2 = 0, so we simplify the expression:&lt;/p&gt;
    &lt;p&gt;Assuming we are able to solve this equation, we will get at most 5 values of t, among which we should find the shortest distance from p to the curve. Since t is bound within 0 and 1 (start and end of the curve), we will also have to test the distance at these locations.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;We could also compute the 2nd derivative in order to differentiate minimums from maximums, but simply evaluating the 5(+2) potential t values and keeping the smallest works just fine.&lt;/p&gt;
    &lt;p&gt;The red dot in the blue field is a random point in space. The red lines show which distances are evaluated (at most 5+2) to find the smallest one.&lt;/p&gt;
    &lt;head rend="h3"&gt;Translated to GLSL code&lt;/head&gt;
    &lt;p&gt;Transposing these formulas into code gives us this base template code:&lt;/p&gt;
    &lt;code&gt;float bezier_distance(vec2 p, vec2 p0, vec2 p1, vec2 p2, vec2 p3) {
    // Start by testing the distance to the boundary points at t=0 (p0) and t=1 (p3)
    vec2 dp0 = p0 - p,
         dp3 = p3 - p;
    float dist = min(dot(dp0, dp0), dot(dp3, dp3));

    // Bezier cubic points to polynomial coefficients
    vec2 a = -p0 + 3.0*(p1 - p2) + p3,
         b = 3.0 * (p0 - 2.0*p1 + p2),
         c = 3.0 * (p1 - p0),
         d = p0;

    // Solve D'(t)=0 where D(t) is the distance squared
    vec2 dmp = d - p;
    float da = 3.0 * dot(a, a),
          db = 5.0 * dot(a, b),
          dc = 4.0 * dot(a, c) + 2.0 * dot(b, b),
          dd = 3.0 * (dot(a, dmp) + dot(b, c)),
          de = 2.0 * dot(b, dmp) + dot(c, c),
          df = dot(c, dmp);

    float roots[5];
    int count = root_find5(roots, da, db, dc, dd, de, df);
    for (int i = 0; i &amp;lt; count; i++) {
        float t = roots[i];
        // Evaluate the distance to our point p and keep the smallest
        vec2 dp = ((a * t + b) * t + c) * t + dmp;
        dist = min(dist, dot(dp, dp));
    }

    // We've been working with the squared distance so far, it's time to get its
    // square root
    return sqrt(dist);
}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;dot(dp,dp)&lt;/code&gt; is a shorthand for the length squared, of course cheaper than
computing &lt;code&gt;length()&lt;/code&gt; which contains a square root.&lt;/p&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;We assume here the root finder only returns the roots that are within [0,1].&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;root_find5()&lt;/code&gt; is our 5th degree root finder, that is the function that gives us
all the t (at most 5) which satisfy:&lt;/p&gt;
    &lt;p&gt;But before we are able to solve that, we need to study the simpler 2nd degree polynomial solving:&lt;/p&gt;
    &lt;head rend="h2"&gt;Solving quadratic polynomial equations&lt;/head&gt;
    &lt;p&gt;Diving into the rabbit hole of solving polynomial numerically will lead you to insanity. But we still have to scratch the surface because superior degree solvers usually rely on it.&lt;/p&gt;
    &lt;p&gt;My favorite quadratic root finding formula is the super simple one introduced by 3Blue1Brown, which involves locating a mid point m from which you get the 2 surrounding roots r:&lt;/p&gt;
    &lt;p&gt;In GLSL, a code to cover most common corner cases would look like this:&lt;/p&gt;
    &lt;code&gt;// Return true if x is not a NaN nor an infinite
// highp is probably mandatory to force IEEE 754 compliance
bool isfinite(highp float x) { return (floatBitsToUint(x) &amp;amp; 0x7f800000u) != 0x7f800000u; }

// Quadratic: solve ax²+bx+c=0
int root_find2(out float r[5], float a, float b, float c) {
    int count = 0;
    float m = -b / (2.*a);
    float d = m*m - c/a;
    if (!isfinite(m) || !isfinite(d)) { // a is (probably) too small
        // Linear: solve bx+c=0
        float s = -c / b;
        if (isfinite(s))
            r[count++] = s;
        return count;
    }
    if (d &amp;lt; 0.) // no root
        return count;
    if (d == 0.) {
        r[count++] = m; // single root
        return count;
    }
    float z = sqrt(d);
    r[count++] = m - z;
    r[count++] = m + z;
    return count;
}
&lt;/code&gt;
    &lt;p&gt;Not quite as straightforward as the math formula, isn't it?&lt;/p&gt;
    &lt;p&gt;We cannot know in advance whether the division is going to succeed, so we do run divisions and only then check if they failed (and assume a reason for the failing). This is much more reliable than an arbitrary epsilon value. We also try to avoid duplicated roots.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;The roots are automatically sorted because z is always positive.&lt;/p&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;isfinite()&lt;/code&gt; may not be as reliable because in GLSL "NaNs are not required
to be generated", meaning some edge case may not be supported depending on
the hardware, drivers, and the current weather in Yokohama.&lt;/p&gt;
    &lt;p&gt;As much as I like it, this implementation might not be the most stable numerically (even though I don't have have strong data to back this claim). Instead, we may prefer the formula from Numerical Recipes:&lt;/p&gt;
    &lt;p&gt;Leading to the following alternative implementation:&lt;/p&gt;
    &lt;code&gt;int root_find2(out float r[5], float a, float b, float c) {
    int count = 0;
    float d = b*b - 4.*a*c;
    if (d &amp;lt; 0.)
        return count;
    if (d == 0.) {
        float s = -.5 * b / a;
        if (isfinite(s))
            r[count++] = s;
        return count;
    }
    float h = sqrt(d);
    float q = -.5 * (b + (b &amp;gt; 0. ? h : -h));
    float r0 = q/a, r1 = c/q;
    if (isfinite(r0)) r[count++] = r0;
    if (isfinite(r1)) r[count++] = r1;
    return count;
}
&lt;/code&gt;
    &lt;p&gt;This is not perfect at all (especially with the b²-4ac part). There are actually many other possible implementations, and this HAL CNRS paper shows how near impossible it is to make a correct one. It is an interesting but depressing read, especially since it "only" covers IEEE 754 floats, and we have no such guarantee on GPUs. We also don't have &lt;code&gt;fma()&lt;/code&gt; in
WebGL, which greatly limits improvements. For now, it will have to do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solving quintic polynomial equations: attempt 1&lt;/head&gt;
    &lt;p&gt;Solving polynomials of degree 5 cannot be solved analytically like quadratics. And even if they were, we probably wouldn't do it because of numerical instability. Typically, in my experience, analytical 3rd degree polynomials solver do not provide reliable results.&lt;/p&gt;
    &lt;p&gt;The first iterative algorithm I picked was the Aberth–Ehrlich method. Nowadays, more appropriate algorithms exist, but at the time I started messing up with these problems (several years ago), it was a fairly good contender. This video explores how it works.&lt;/p&gt;
    &lt;p&gt;The convergence to the roots is quick, and it's overall simple to implement. But it's not without flaws. The main problem is that it works in complex space. We can't ignore the complex roots because they all "respond" to each others. And filtering these roots out at the end implies some unreliable arbitrary threshold mechanism (we keep the root only when the imaginary part is close to 0).&lt;/p&gt;
    &lt;p&gt;The initialization process also annoyingly requires you to come up with a guess at what the roots are, and doesn't provide anything relevant. Aberth-Ehrlich works by refining these initial roots, similar to a more elaborate Newton iterations algorithm. Choosing better initial estimates leads to a faster convergence (meaning less iterations).&lt;/p&gt;
    &lt;p&gt;The Cauchy bound specifies a space by defining the radius of a disk (complex numbers are in 2D space) where all the roots of a polynomial should lie within. We are going to use it for the initial guess, and more specifically its "tight" version (which unfortunately relies on &lt;code&gt;pow()&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Since Aberth-Ehrlich is a refinement and not just a shrinking process, we define and use an inner disk that has half the area of Cauchy bound disk. That way, we're more likely to start with initial guesses spread in the "middle" of the roots; this is where the √2 comes from in the formula below.&lt;/p&gt;
    &lt;code&gt;#define K5_0 vec2( 0.951056516295154,  0.309016994374947)
#define K5_1 vec2( 0.000000000000000,  1.000000000000000)
#define K5_2 vec2(-0.951056516295154,  0.309016994374948)
#define K5_3 vec2(-0.587785252292473, -0.809016994374947)
#define K5_4 vec2( 0.587785252292473, -0.809016994374948)

int root_find5_aberth(out float roots[5], float a, float b, float c, float d, float e, float f) {
    // Initial candidates set mid-way of the tight Cauchy bound estimate
    float r = (1.0 + max_5(
        pow(abs(b/a), 1.0/5.0),
        pow(abs(c/a), 1.0/4.0),
        pow(abs(d/a), 1.0/3.0),
        pow(abs(e/a), 1.0/2.0),
            abs(f/a))) / sqrt(2.0);

    // Spread in a circle
    vec2 r0 = r * K5_0,
         r1 = r * K5_1,
         r2 = r * K5_2,
         r3 = r * K5_3,
         r4 = r * K5_4;
&lt;/code&gt;
    &lt;p&gt;The circle constants are generated with the following script:&lt;/p&gt;
    &lt;code&gt;import math
import sys

n = int(sys.argv[1])
for k in range(n):
    angle = 2 * math.pi / n
    off = math.pi / (2 * n)
    z = angle * k + off
    c, s = math.cos(z), math.sin(z)
    print(f"#define K{n}_{k} vec2({c:18.15f}, {s:18.15f})")
&lt;/code&gt;
    &lt;p&gt;Next, it's basically a simple iterative process. Unrolling everything for degree 5 looks like this:&lt;/p&gt;
    &lt;code&gt;#define close_to_zero(x) (abs(x) &amp;lt; eps)

// This also filters out roots out of the [0,1] range
#define ADD_ROOT_IF_REAL(r) if (close_to_zero(r.y) &amp;amp;&amp;amp; r.x &amp;gt;= 0. &amp;amp;&amp;amp; r.x &amp;lt;= 1.) roots[count++] = r.x

#define SMALL_OFF(off) (dot(off, off) &amp;lt;= eps*eps)

/* Complex multiply, divide, inverse */
vec2 c_mul(vec2 a, vec2 b) { return mat2(a, -a.y, a.x) * b; }
vec2 c_div(vec2 a, vec2 b) { return mat2(a, a.y, -a.x) * b / dot(b, b); }
vec2 c_inv(vec2 z)         { return vec2(z.x, -z.y) / dot(z, z); }

// Compute f(x)/f'(x): complex polynomial evaluation (y) divided by their
// derivatives (q) using Horner's method in one pass
vec2 c_poly5d4(float a, float b, float c, float d, float e, float f, vec2 x) {
    vec2 y =       a*x  + vec2(b, 0), q =       a*x  + y;
         y = c_mul(y,x) + vec2(c, 0); q = c_mul(q,x) + y;
         y = c_mul(y,x) + vec2(d, 0); q = c_mul(q,x) + y;
         y = c_mul(y,x) + vec2(e, 0); q = c_mul(q,x) + y;
         y = c_mul(y,x) + vec2(f, 0);
    return c_div(y, q);
}

vec2 sum_of_inv(vec2 z0, vec2 z1, vec2 z2, vec2 z3, vec2 z4) { return c_inv(z0 - z1) + c_inv(z0 - z2) + c_inv(z0 - z3) + c_inv(z0 - z4); }

int root_find5_aberth(out float roots[5], float a, float b, float c, float d, float e, float f) {
    if (close_to_zero(a))
        return root_find4_aberth(roots, b, c, d, e, f);

    // Code snip: see previous snippet
    // float r = ...
    // vec2 r0, r1, r2, ... 

    for (int m = 0; m &amp;lt; 16; m++) {
        vec2 d0 = c_poly5d4(a, b, c, d, e, f, r0),
             d1 = c_poly5d4(a, b, c, d, e, f, r1),
             d2 = c_poly5d4(a, b, c, d, e, f, r2),
             d3 = c_poly5d4(a, b, c, d, e, f, r3),
             d4 = c_poly5d4(a, b, c, d, e, f, r4);

        vec2 off0 = c_div(d0, vec2(1,0) - c_mul(d0, sum_of_inv(r0, r1, r2, r3, r4))),
             off1 = c_div(d1, vec2(1,0) - c_mul(d1, sum_of_inv(r1, r0, r2, r3, r4))),
             off2 = c_div(d2, vec2(1,0) - c_mul(d2, sum_of_inv(r2, r0, r1, r3, r4))),
             off3 = c_div(d3, vec2(1,0) - c_mul(d3, sum_of_inv(r3, r0, r1, r2, r4))),
             off4 = c_div(d4, vec2(1,0) - c_mul(d4, sum_of_inv(r4, r0, r1, r2, r3)));

        r0 -= off0;
        r1 -= off1;
        r2 -= off2;
        r3 -= off3;
        r4 -= off4;

        if (SMALL_OFF(off0) &amp;amp;&amp;amp; SMALL_OFF(off1) &amp;amp;&amp;amp; SMALL_OFF(off2) &amp;amp;&amp;amp; SMALL_OFF(off3) &amp;amp;&amp;amp; SMALL_OFF(off4))
            break;
    }

    int count = 0;
    ADD_ROOT_IF_REAL(r0);
    ADD_ROOT_IF_REAL(r1);
    ADD_ROOT_IF_REAL(r2);
    ADD_ROOT_IF_REAL(r3);
    ADD_ROOT_IF_REAL(r4);
    return count;
}
&lt;/code&gt;
    &lt;p&gt;When the main coefficient is too small, we fall back on the 4th degree (and so on until we reach the analytic quadratic). The 4th and 3rd degree versions of this function are easy to guess (they're pretty much identical, just removing one coefficient at each degree).&lt;/p&gt;
    &lt;p&gt;We're also hardcoding a maximum of 16 iterations here because it's usually enough. To have an idea of how many iterations are required in practice, following is a visualization of the heat map of the number of iterations for every pixel:&lt;/p&gt;
    &lt;p&gt;The big picture and the weaknesses of the algorithm should be pretty obvious by now. Among all drawbacks of this approach, there are also surprising pathological cases where the algorithm is not performing well. Fortunately, there were some progress on the state of the art in recent years.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solving quintic polynomial equations: the state of the art&lt;/head&gt;
    &lt;p&gt;In 2022, Cem Yuksel published a new algorithm for polynomial root solving. Initially I had my reservations because the official implementation had a few shortcomings on some edge cases, which made me question its reliability. It's also optimized for CPU computation and is, to my very personal taste, overly complex.&lt;/p&gt;
    &lt;p&gt;Fortunately, Christoph Peters showed that it was possible on the GPU by implementing it for very large degrees, and without any recursion. Inspired by that, I decided to unroll it myself for degree 5.&lt;/p&gt;
    &lt;p&gt;One core difference with Aberth approach is that it is designed for arbitrary ranges. In our case this is actually convenient because, due to how Bézier curves are defined, we are only interested in roots between 0 and 1. We will need to adjust the Quadratic function to work in this range, as well as keeping the roots ordered:&lt;/p&gt;
    &lt;code&gt;     }
     float h = sqrt(d);
     float q = -.5 * (b + (b &amp;gt; 0. ? h : -h));
-    float r0 = q/a, r1 = c/q;
-    if (isfinite(r0)) r[count++] = r0;
-    if (isfinite(r1)) r[count++] = r1;
+    vec2 v = vec2(q/a, c/q);
+    if (v.x &amp;gt; v.y) v.xy = v.yx; // keep them ordered
+    if (isfinite(v.x) &amp;amp;&amp;amp; v.x &amp;gt;= 0. &amp;amp;&amp;amp; v.x &amp;lt;= 1.) r[count++] = v.x;
+    if (isfinite(v.y) &amp;amp;&amp;amp; v.y &amp;gt;= 0. &amp;amp;&amp;amp; v.y &amp;lt;= 1.) r[count++] = v.y;
     return r;
 }
&lt;/code&gt;
    &lt;p&gt;The core logic of the algorithm relies on a cascade of derivatives for every degree. Christoph Peters provides an analytic formula to obtain the derivative for any degree. This is a huge helper when we need to work for an arbitrary degree, but in our case we can just differentiate manually:&lt;/p&gt;
    &lt;p&gt;Since we're only interested in the roots, similar to what we did to D(t), we can simplify some of these expressions:&lt;/p&gt;
    &lt;p&gt;The purpose of that cascade of derivatives is to cut the curve into monotonic segments. In practice, the core function looks like this:&lt;/p&gt;
    &lt;code&gt;int root_find5_cy(out float r[5], float a, float b, float c, float d, float e, float f) {
    float r2[5], r3[5], r4[5];
    int n = root_find2(r2,          10.*a, 4.*b,    c);            // degree 2
    n = cy_find5(r3, r2, n, 0. ,0., 10.*a, 6.*b, 3.*c,   d);       // degree 3
    n = cy_find5(r4, r3, n,     0.,  5.*a, 4.*b, 3.*c, d+d, e);    // degree 4
    n = cy_find5(r,  r4, n,             a,    b,    c,   d, e, f); // degree 5
    reutnr n;
}
&lt;/code&gt;
    &lt;p&gt;We could unroll &lt;code&gt;cy_find3&lt;/code&gt;, &lt;code&gt;cy_find4&lt;/code&gt;, and &lt;code&gt;cy_find5&lt;/code&gt;, but to keep the
code simple, the degree 3 to 5 will share the same function, with leading
coefficients set to 0 (hopefully the compiler does its job properly).&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cy_find5&lt;/code&gt; relies on roots found (at most 4) at previous stages to define
intervals of search:&lt;/p&gt;
    &lt;p&gt;Such an approach has the nice side effect of keeping the roots ordered.&lt;/p&gt;
    &lt;p&gt;The solver itself is not that complex either:&lt;/p&gt;
    &lt;code&gt;float poly5(float a, float b, float c, float d, float e, float f, float t) {
     return ((((a * t + b) * t + c) * t + d) * t + e) * t + f;
}

// Quintic: solve ax⁵+bx⁴+cx³+dx²+ex+f=0
iint cy_find5(out float r[5], float r4[5], int n, float a, float b, float c, float d, float e, float f) {
    int count = 0;
    vec2 p = vec2(0, poly5(a,b,c,d,e,f, 0.));
    for (int i = 0; i &amp;lt;= n; i++) {
        float x = i == n ? 1. : r4[i],
              y = poly5(a,b,c,d,e,f, x);
        if (p.y * y &amp;gt; 0.)
            continue;
        float v = bisect5(a,b,c,d,e,f, vec2(p.x,x), vec2(p.y,y));
        r[count++] = v;
        p = vec2(x, y);
    }
    return count;
}
&lt;/code&gt;
    &lt;p&gt;The last brick of the algorithm is the Newton bisection, the slowest part:&lt;/p&gt;
    &lt;code&gt;// Newton bisection
//
// a,b,c,d,e,f: 5th degree polynomial parameters
// t: x-axis boundaries
// v: respectively f(t.x) and f(t.y)
float bisect5(float a, float b, float c, float d, float e, float f, vec2 t, vec2 v) {
    float x = (t.x+t.y) * .5; // mid point
    float s = v.x &amp;lt; v.y ? 1. : -1.; // sign flip
    for (int i = 0; i &amp;lt; 32; i++) {
        // Evaluate polynomial (y) and its derivative (q) using Horner's method in one pass
        float y = a*x + b, q = a*x + y;
              y = y*x + c; q = q*x + y;
              y = y*x + d; q = q*x + y;
              y = y*x + e; q = q*x + y;
              y = y*x + f;

        t = s*y &amp;lt; 0. ? vec2(x, t.y) : vec2(t.x, x);
        float next = x - y/q; // Newton iteration
        next = next &amp;gt;= t.x &amp;amp;&amp;amp; next &amp;lt;= t.y ? next : (t.x+t.y) * .5;
        if (abs(next - x) &amp;lt; eps)
            return next;
        x = next;
    }
    return x;
}
&lt;/code&gt;
    &lt;p&gt;And that's pretty much it. Looking at its heat map, it has a completely different look than Aberth:&lt;/p&gt;
    &lt;p&gt;The number of iterations might be larger but it is much faster (I observed a factor 3 on my machine), the code is shorter, and actually more reliable.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;The scale used to represent the heat map is not the same as the one used in Aberth, but it is identical with the method presented in the next section.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploring ITP convergence&lt;/head&gt;
    &lt;p&gt;The bisection being the hot loop, it is interesting to ponder how to make this faster. A while back, Raph Levien hypothesized about how the ITP method could perform. Out of curiosity, I gave it a chance. The function is designed to work like a bisection, claiming to be as performant in the worst case.&lt;/p&gt;
    &lt;p&gt;There isn't a lot of code, and the paper provides a pseudo-code. But implementing it was actually challenging in many ways.&lt;/p&gt;
    &lt;p&gt;First of all, the authors didn't seem to find relevant to mention that it only works if f(a)&amp;lt;0&amp;lt;f(b). If f(a)&amp;gt;0&amp;gt;f(b), you're pretty much on your own. It requires just 2 lines of adjustments but figuring out this shortcoming of the algorithm was particularly unexpected.&lt;/p&gt;
    &lt;p&gt;Another bothering aspect concerns the parameters: K_1, K_2 and n_0. The paper proposes those:&lt;/p&gt;
    &lt;p&gt;I played with them for a while and couldn't find any set that would make a real difference, so I ended up with the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For performance reasons, reducing K_2 to a value of 2 saves a call to &lt;code&gt;pow()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;For K_1, CRAN seems to suggest \frac{0.2}{b-a} so I went along with it&lt;/item&gt;
      &lt;item&gt;And for n_0, well 1 or 2 seem to be the usual parameter.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the end, the function looks like this:&lt;/p&gt;
    &lt;code&gt;// ITP algorithm (2020) by Oliveira &amp;amp; Takahashi
// "An Enhancement of the Bisection Method Average Performance Preserving Minmax Optimality"
//
// a,b,c,d,e,f: 5th degree polynomial parameters
// t: x-axis boundaries (a and b in the paper)
// v: respectively f(a) and f(b) in the paper (evaluation of the function with t.x and t.y)
float itp5(float a, float b, float c, float d, float e, float f, vec2 t, vec2 v) {
    float diff = t.y-t.x;

    // K1 and n0 suggested by CRAN
    float K1 = .2 / diff;
    int n0 = 1;

    // The paper has the assumption that f(a)&amp;lt;0&amp;lt;f(b) but we want to
    // support f(a)&amp;gt;0&amp;gt;f(b) too, so we keep a sign flip
    float s = v.x &amp;lt; v.y ? 1. : -1.;

    // Using log(ab)=log(a)+log(b): log2(x/(2ε)) &amp;lt;=&amp;gt; log2(x/ε)-1
    int nh = int(ceil(log2(diff/eps)-1.)); // n_{1/2} (half point)
    int n_max = nh + n0;

    // ε 2^(n_max-k) = ε 2^n_max 2^-k = ε 2^n_max ½^k
    // ½^k is done iteratively in the loop, simplifying the arithmetic
    float q = eps * float(1&amp;lt;&amp;lt;n_max);

    while (diff &amp;gt; eps+eps) {
        // Interpolation
        float xf = (v.y*t.x - v.x*t.y) / (v.y-v.x); // Regula-Falsi

        // Truncation
        float xh = (t.x+t.y) * .5; // x half point
        float sigma = sign(xh - xf);
        float delta = K1*diff*diff; // save a pow() by forcing K2=2
        float xt = delta &amp;lt;= abs(xh - xf) ? xf + sigma*delta : xh; // xt: truncation of xf

        // Projection
        float r = q - diff*.5;
        float x = abs(xt-xh) &amp;lt;= r ? xt : xh-sigma*r;

        // Updating
        float y = poly5(a,b,c,d,e,f, x);
        float side = s*y;
        if      (side &amp;gt; 0.) t.y=x, v.y=y;
        else if (side &amp;lt; 0.) t.x=x, v.x=y;
        else                return x;

        diff = t.y-t.x;
        q *= .5;
    }
    return (t.x+t.y) * .5;
}
&lt;/code&gt;
    &lt;p&gt;This function can be used as a drop'in replacement for &lt;code&gt;bisect5&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;I had a lot of expectations about it, but in the end it requires more iterations than the bisection we implemented. The paper claims to perform at least as good as a bisection, but our &lt;code&gt;bisect5&lt;/code&gt; is driven by the derivatives so it converges
much faster. Here is the heat map with &lt;code&gt;itp5&lt;/code&gt; instead of &lt;code&gt;bisect5&lt;/code&gt;:&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The naive unrolled version of Cem Yuksel paper definitely is, so far, the best choice for our problem. I have still concerns about how to implement a good quadratic formula, and I have my reservations about various edge cases. There is also still room for improvements in the cubic solver (degree 3) because it's still a special case where analytical formulas exist, but in general this implementation is satisfying.&lt;/p&gt;
    &lt;p&gt;The next step is to work with chains of Bézier curves to make up complex shapes (such as font glyphs). It will lead us to build a signed distance field. This is not trivial at all and mandates one or several dedicated articles. We will hopefully study these subjects in the not-so-distant future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.pkh.me/p/46-fast-calculation-of-the-distance-to-cubic-bezier-curves-on-the-gpu.html"/><published>2025-10-18T09:25:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45626130</id><title>./watch</title><updated>2025-10-18T16:10:48.891034+00:00</updated><link href="https://dotslashwatch.com/"/><published>2025-10-18T09:55:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45626549</id><title>MD RAID or DRBD can be broken from userspace when using O_DIRECT</title><updated>2025-10-18T16:10:48.528210+00:00</updated><content>&lt;doc fingerprint="5f8f763c0c2747ac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Making sure you're not a bot!&lt;/head&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
    &lt;head&gt;Why am I seeing this?&lt;/head&gt;
    &lt;p&gt;You are seeing this because the administrator of this website has set up Anubis to protect the server against the scourge of AI companies aggressively scraping websites. This can and does cause downtime for the websites, which makes their resources inaccessible for everyone.&lt;/p&gt;
    &lt;p&gt;Anubis is a compromise. Anubis uses a Proof-of-Work scheme in the vein of Hashcash, a proposed proof-of-work scheme for reducing email spam. The idea is that at individual scales the additional load is ignorable, but at mass scraper levels it adds up and makes scraping much more expensive.&lt;/p&gt;
    &lt;p&gt;Ultimately, this is a hack whose real purpose is to give a "good enough" placeholder solution so that more time can be spent on fingerprinting and identifying headless browsers (EG: via how they do font rendering) so that the challenge proof of work page doesn't need to be presented to users that are much more likely to be legitimate.&lt;/p&gt;
    &lt;p&gt;Please note that Anubis requires the use of modern JavaScript features that plugins like JShelter will disable. Please disable JShelter or other such plugins for this domain.&lt;/p&gt;
    &lt;p&gt;This website is running Anubis version &lt;code&gt;1.19.1&lt;/code&gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bugzilla.kernel.org/show_bug.cgi?id=99171"/><published>2025-10-18T11:39:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45626910</id><title>The IDEs we had 30 years ago ... and we lost</title><updated>2025-10-18T16:10:48.441858+00:00</updated><content/><link href="https://blogsystem5.substack.com/p/the-ides-we-had-30-years-ago-and"/><published>2025-10-18T12:44:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45626961</id><title>Lux: A luxurious package manager for Lua</title><updated>2025-10-18T16:10:47.669060+00:00</updated><content>&lt;doc fingerprint="ac056e38d961f788"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;A luxurious package manager for Lua.&lt;/head&gt;
    &lt;p&gt;Key Features • How To Use • Comparison with Luarocks • Related Projects • Contributing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create and manage Lua projects &lt;list rend="ul"&gt;&lt;item&gt;Easily manage dependencies, build steps and more through the &lt;code&gt;lux.toml&lt;/code&gt;file.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Easily manage dependencies, build steps and more through the &lt;/item&gt;
      &lt;item&gt;Parallel builds and installs 🚀&lt;/item&gt;
      &lt;item&gt;Add/remove dependencies with simple CLI commands&lt;/item&gt;
      &lt;item&gt;Automatic generation of rockspecs &lt;list rend="ul"&gt;&lt;item&gt;Say goodbye to managing 10 different rockspec files in your source code 🎉&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Integrated code formatting via &lt;code&gt;lx fmt&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Powered by &lt;code&gt;stylua&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Powered by &lt;/item&gt;
      &lt;item&gt;Easily specify compatible Lua versions &lt;list rend="ul"&gt;&lt;item&gt;Lux will take care of Lua header installation automatically&lt;/item&gt;&lt;item&gt;Forget about users complaining they have the wrong Lua headers installed on their system&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Automatic EmmyLua/LuaCATS based type checking via &lt;code&gt;lx check&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Powered by &lt;code&gt;emmylua-analyzer-rust&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Powered by &lt;/item&gt;
      &lt;item&gt;Automatic code linting via &lt;code&gt;lx lint&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Powered by &lt;code&gt;luacheck&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Powered by &lt;/item&gt;
      &lt;item&gt;Powerful lockfile support &lt;list rend="ul"&gt;&lt;item&gt;Makes for fully reproducible developer environments.&lt;/item&gt;&lt;item&gt;Makes Lux easy to integrate with Nix!&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Fully compatible &lt;list rend="ul"&gt;&lt;item&gt;Works with existing luarocks packages.&lt;/item&gt;&lt;item&gt;Have a complex rockspec that you don't want to rewrite to TOML? No problem! Lux allows the creation of an &lt;code&gt;extra.rockspec&lt;/code&gt;file, everything just works.&lt;/item&gt;&lt;item&gt;Have a very complex build script? Lux can shell out to &lt;code&gt;luarocks&lt;/code&gt;if it knows it has to preserve maximum compatibility.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Automatically adds project dependencies to a &lt;code&gt;.luarc.json&lt;/code&gt;file so they can be picked up by&lt;code&gt;lua-language-server&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;Lux, while generally functional, is a work in progress and does not have a &lt;code&gt;1.0&lt;/code&gt; release yet.&lt;/p&gt;
    &lt;p&gt;Feel free to consult the documentation on how to get started with Lux!&lt;/p&gt;
    &lt;p&gt;It features a tutorial and several guides to make you good at managing Lua projects.&lt;/p&gt;
    &lt;p&gt;As this project is still a work in progress, some luarocks features have not been (fully) implemented yet. On the other hand, lux has some features that are not present in luarocks.&lt;/p&gt;
    &lt;p&gt;The following table provides a brief comparison:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;lux&lt;/cell&gt;
        &lt;cell role="head"&gt;luarocks v3.12.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;project format&lt;/cell&gt;
        &lt;cell&gt;TOML / Lua&lt;/cell&gt;
        &lt;cell&gt;Lua&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;add/remove dependencies&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;parallel builds/installs&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;proper lockfile support with integrity checks&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌ (basic, dependency versions only)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;run tests with busted&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;linting with luacheck&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;code formatting with stylua&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;automatic lua detection/installation&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;default build specs&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;custom build backends&lt;/cell&gt;
        &lt;cell&gt;✅1&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;rust-mlua&lt;/code&gt; build spec&lt;/cell&gt;
        &lt;cell&gt;✅ (builtin)&lt;/cell&gt;
        &lt;cell&gt;✅ (external build backend)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;treesitter-parser&lt;/code&gt; build spec&lt;/cell&gt;
        &lt;cell&gt;✅ (builtin)&lt;/cell&gt;
        &lt;cell&gt;✅ (external build backend)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;install pre-built binary rocks&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;install multiple packages with a single command&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;install packages using version constraints&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;auto-detect external dependencies and Lua headers with &lt;code&gt;pkg-config&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;resolve multiple versions of the same dependency at runtime&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;pack and upload pre-built binary rocks&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;luarocks.org manifest namespaces&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;luarocks.org dev packages&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;versioning&lt;/cell&gt;
        &lt;cell&gt;SemVer2&lt;/cell&gt;
        &lt;cell&gt;arbitrary&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rockspecs with CVS/Mercurial/SVN/SSCM sources&lt;/cell&gt;
        &lt;cell&gt;❌ (YAGNI3)&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;static type checking&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;generate a &lt;code&gt;.luarc&lt;/code&gt; file with dependencies&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;git dependencies in local projects&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Lux includes the following packages and libraries:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;lux-cli&lt;/code&gt;: The main CLI for interacting with projects and installing Lua packages from the command line.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua&lt;/code&gt;: The Lux Lua API, which provides:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;lux.loader&lt;/code&gt;for resolving dependencies on&lt;code&gt;require&lt;/code&gt;at runtime.&lt;/item&gt;&lt;item&gt;A work-in-progress API for embedding Lux into Lua applications. We provide builds of &lt;code&gt;lux-lua&lt;/code&gt;for Lua 5.1, 5.2, 5.3, 5.4 and Luajit.&lt;code&gt;lux-cli&lt;/code&gt;uses&lt;code&gt;lux-lua&lt;/code&gt;for commands like&lt;code&gt;lx lua&lt;/code&gt;,&lt;code&gt;lx run&lt;/code&gt;and&lt;code&gt;lx path&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lib&lt;/code&gt;: The Lux library for Rust. A dependency of&lt;code&gt;lux-cli&lt;/code&gt;and&lt;code&gt;lux-lua&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;We do not yet provide a way to install &lt;code&gt;lux-lua&lt;/code&gt; as a Lua library using Lux.
See #663.
Lux can detect a lux-lua installation using pkg-config
or via the &lt;code&gt;LUX_LIB_DIR&lt;/code&gt; environment variable.&lt;/p&gt;
    &lt;p&gt;Our pre-built binary release artifacts are bundled with &lt;code&gt;lux-lua&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Dependencies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;gnupg&lt;/code&gt;,&lt;code&gt;libgpg-error&lt;/code&gt;and&lt;code&gt;gpgme&lt;/code&gt;(*nix only)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If building without the &lt;code&gt;vendored&lt;/code&gt; feature:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;lua&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;libgit2&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;openssl&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If building with the &lt;code&gt;vendored&lt;/code&gt; feature:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;perl&lt;/code&gt;and&lt;code&gt;perl-core&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;make&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To link &lt;code&gt;gpgme&lt;/code&gt; statically on Linux and macOS, set the environment variable
&lt;code&gt;SYSTEM_DEPS_LINK=static&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We usually recommend building with the &lt;code&gt;vendored&lt;/code&gt; feature enabled,
to statically link &lt;code&gt;lua&lt;/code&gt;, &lt;code&gt;libgit2&lt;/code&gt; and &lt;code&gt;openssl&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;SYSTEM_DEPS_LINK="static" cargo build --locked --profile release --features vendored&lt;/code&gt;
    &lt;p&gt;Or, to build with dynamically linked libraries:&lt;/p&gt;
    &lt;code&gt;cargo build --locked --profile release&lt;/code&gt;
    &lt;p&gt;On Windows/MSVC, you must disable the &lt;code&gt;gpgme&lt;/code&gt; feature:&lt;/p&gt;
    &lt;code&gt;cargo build --locked --profile release --no-default-features --features lua54,vendored&lt;/code&gt;
    &lt;p&gt;You can build &lt;code&gt;lux-lua&lt;/code&gt; for a given Lua version with:&lt;/p&gt;
    &lt;code&gt;cargo xtask51 dist-lua # lux-lua for Lua 5.1
cargo xtask52 dist-lua # for Lua 5.2
cargo xtask53 dist-lua # ...
cargo xtask54 dist-lua
cargo xtaskjit dist-lua&lt;/code&gt;
    &lt;p&gt;This will install &lt;code&gt;lux-lua&lt;/code&gt; to &lt;code&gt;target/dist/share/lux-lua/&amp;lt;lua&amp;gt;/lux.so&lt;/code&gt;
and a pkg-config &lt;code&gt;.pc&lt;/code&gt; file to &lt;code&gt;target/dist/lib/lux-lua*.pc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To build completions:&lt;/p&gt;
    &lt;code&gt;cargo xtask dist-completions&lt;/code&gt;
    &lt;p&gt;To build man pages:&lt;/p&gt;
    &lt;code&gt;cargo xtask dist-man&lt;/code&gt;
    &lt;p&gt;To build the binary distributions for your platform, bundled with completions, man pages and &lt;code&gt;lux-lua&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;cargo xtask dist-package&lt;/code&gt;
    &lt;p&gt;If you would like to use the latest version of lux with Nix, you can import our flake. It provides an overlay and packages for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;lux-cli&lt;/code&gt;: The Lux CLI package.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua51&lt;/code&gt;The Lux Lua API for Lua 5.1&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua52&lt;/code&gt;The Lux Lua API for Lua 5.2&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua53&lt;/code&gt;The Lux Lua API for Lua 5.3&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua54&lt;/code&gt;The Lux Lua API for Lua 5.4&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-luajit&lt;/code&gt;The Lux Lua API for Luajit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have a &lt;code&gt;lux-lua&lt;/code&gt; build and &lt;code&gt;pkg-config&lt;/code&gt; in a Nix devShell,
Lux will auto-detect &lt;code&gt;lux-lua&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;luarocks - The original Lua package manager&lt;/item&gt;
      &lt;item&gt;rocks.nvim - A Neovim plugin manager that uses &lt;code&gt;luarocks&lt;/code&gt;under the hood, and will soon be undergoing a rewrite to use Lux instead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Credits go to the Luarocks team for maintaining luarocks and luarocks.org for as long as they have. Without their prior work Lux would not be possible.&lt;/p&gt;
    &lt;p&gt;Contributions are more than welcome! See CONTRIBUTING.md for a guide.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lux is licensed under LGPL-3.0+.&lt;/item&gt;
      &lt;item&gt;The Lux logo © 2025 by Kai Jakobi is licensed under CC BY-NC-SA 4.0.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Supported via a compatibility layer that uses luarocks as a backend. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mostly compatible with the luarocks version parser, which allows an arbitrary number of version components. To comply with SemVer, we treat anything after the third version component (except for the specrev) as a prerelease/build version. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/lumen-oss/lux"/><published>2025-10-18T12:53:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45626985</id><title>SQL Anti-Patterns You Should Avoid</title><updated>2025-10-18T16:10:47.615274+00:00</updated><content/><link href="https://datamethods.substack.com/p/sql-anti-patterns-you-should-avoid"/><published>2025-10-18T12:56:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45627324</id><title>Ripgrep 15.0.0</title><updated>2025-10-18T16:10:47.083011+00:00</updated><content>&lt;doc fingerprint="3cab1bfb2c6333d9"&gt;
  &lt;main&gt;
    &lt;p&gt;ripgrep 15 is a new major version release of ripgrep that mostly has bug fixes,&lt;lb/&gt; some minor performance improvements and minor new features.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;In case you haven't heard of it before, ripgrep is a line-oriented search&lt;/p&gt;&lt;lb/&gt;tool that recursively searches the current directory for a regex pattern.&lt;lb/&gt;By default, ripgrep will respect gitignore rules and automatically skip&lt;lb/&gt;hidden files/directories and binary files.&lt;/quote&gt;
    &lt;p&gt;Here are some highlights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Several bugs around gitignore matching have been fixed. This includes&lt;lb/&gt;a commonly reported bug related to applying gitignore rules from parent&lt;lb/&gt;directories.&lt;/item&gt;
      &lt;item&gt;A memory usage regression when handling very large gitignore files has been&lt;lb/&gt;fixed.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;rg -vf file&lt;/code&gt;, where&lt;code&gt;file&lt;/code&gt;is empty, now matches everything.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;-r/--replace&lt;/code&gt;flag now works with&lt;code&gt;--json&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;A subset of Jujutsu (&lt;code&gt;jj&lt;/code&gt;) repositories are now treated as if they were git&lt;lb/&gt;repositories. That is, ripgrep will respect&lt;code&gt;jj&lt;/code&gt;'s gitignores.&lt;/item&gt;
      &lt;item&gt;Globs can now use nested curly braces.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Platform support:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;aarch64&lt;/code&gt;for Windows now has release artifacts.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;powerpc64&lt;/code&gt;no longer has release artifacts generated for it. The CI&lt;lb/&gt;release workflow stopped working, and I didn't deem it worth my time to&lt;lb/&gt;debug it. If someone wants this and can test it, I'd be happy to add it&lt;lb/&gt;back.&lt;/item&gt;
      &lt;item&gt;ripgrep binaries are now compiled with full LTO enabled. You may notice&lt;lb/&gt;small performance improvements from this and a modest decrease in binary&lt;lb/&gt;size.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Performance improvements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PERF #2111:&lt;lb/&gt;Don't resolve helper binaries on Windows when&lt;code&gt;-z/--search-zip&lt;/code&gt;isn't used.&lt;/item&gt;
      &lt;item&gt;PERF #2865:&lt;lb/&gt;Avoid using path canonicalization on Windows when emitting hyperlinks.&lt;/item&gt;
      &lt;item&gt;PERF #3184:&lt;lb/&gt;Improve performance of large values with&lt;code&gt;-A/--after-context&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bug fixes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;BUG #829,&lt;lb/&gt;BUG #2731,&lt;lb/&gt;BUG #2747,&lt;lb/&gt;BUG #2770,&lt;lb/&gt;BUG #2778,&lt;lb/&gt;BUG #2836,&lt;lb/&gt;BUG #2933,&lt;lb/&gt;BUG #3067:&lt;lb/&gt;Fix bug related to gitignores from parent directories.&lt;/item&gt;
      &lt;item&gt;BUG #1332,&lt;lb/&gt;BUG #3001:&lt;lb/&gt;Make&lt;code&gt;rg -vf file&lt;/code&gt;where&lt;code&gt;file&lt;/code&gt;is empty match everything.&lt;/item&gt;
      &lt;item&gt;BUG #2177:&lt;lb/&gt;Ignore a UTF-8 BOM marker at the start of&lt;code&gt;.gitignore&lt;/code&gt;(and similar files).&lt;/item&gt;
      &lt;item&gt;BUG #2750:&lt;lb/&gt;Fix memory usage regression for some truly large gitignore files.&lt;/item&gt;
      &lt;item&gt;BUG #2944:&lt;lb/&gt;Fix a bug where the "bytes searched" in&lt;code&gt;--stats&lt;/code&gt;output could be incorrect.&lt;/item&gt;
      &lt;item&gt;BUG #2990:&lt;lb/&gt;Fix a bug where ripgrep would mishandle globs that ended with a&lt;code&gt;.&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;BUG #2094,&lt;lb/&gt;BUG #3076:&lt;lb/&gt;Fix bug with&lt;code&gt;-m/--max-count&lt;/code&gt;and&lt;code&gt;-U/--multiline&lt;/code&gt;showing too many matches.&lt;/item&gt;
      &lt;item&gt;BUG #3100:&lt;lb/&gt;Preserve line terminators when using&lt;code&gt;-r/--replace&lt;/code&gt;flag.&lt;/item&gt;
      &lt;item&gt;BUG #3108:&lt;lb/&gt;Fix a bug where&lt;code&gt;-q --files-without-match&lt;/code&gt;inverted the exit code.&lt;/item&gt;
      &lt;item&gt;BUG #3131:&lt;lb/&gt;Document inconsistency between&lt;code&gt;-c/--count&lt;/code&gt;and&lt;code&gt;--files-with-matches&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;BUG #3135:&lt;lb/&gt;Fix rare panic for some classes of large regexes on large haystacks.&lt;/item&gt;
      &lt;item&gt;BUG #3140:&lt;lb/&gt;Ensure hyphens in flag names are escaped in the roff text for the man page.&lt;/item&gt;
      &lt;item&gt;BUG #3155:&lt;lb/&gt;Statically compile PCRE2 into macOS release artifacts on&lt;code&gt;aarch64&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;BUG #3173:&lt;lb/&gt;Fix ancestor ignore filter bug when searching whitelisted hidden files.&lt;/item&gt;
      &lt;item&gt;BUG #3178:&lt;lb/&gt;Fix bug causing incorrect summary statistics with&lt;code&gt;--json&lt;/code&gt;flag.&lt;/item&gt;
      &lt;item&gt;BUG #3179:&lt;lb/&gt;Fix gitignore bug when searching absolute paths with global gitignores.&lt;/item&gt;
      &lt;item&gt;BUG #3180:&lt;lb/&gt;Fix a panicking bug when using&lt;code&gt;-U/--multiline&lt;/code&gt;and&lt;code&gt;-r/--replace&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feature enhancements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many enhancements to the default set of file types available for filtering.&lt;/item&gt;
      &lt;item&gt;FEATURE #1872:&lt;lb/&gt;Make&lt;code&gt;-r/--replace&lt;/code&gt;work with&lt;code&gt;--json&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;FEATURE #2708:&lt;lb/&gt;Completions for the fish shell take ripgrep's config file into account.&lt;/item&gt;
      &lt;item&gt;FEATURE #2841:&lt;lb/&gt;Add&lt;code&gt;italic&lt;/code&gt;to the list of available style attributes in&lt;code&gt;--color&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;FEATURE #2842:&lt;lb/&gt;Directories containing&lt;code&gt;.jj&lt;/code&gt;are now treated as git repositories.&lt;/item&gt;
      &lt;item&gt;FEATURE #2849:&lt;lb/&gt;When using multithreading, schedule files to search in order given on CLI.&lt;/item&gt;
      &lt;item&gt;FEATURE #2943:&lt;lb/&gt;Add&lt;code&gt;aarch64&lt;/code&gt;release artifacts for Windows.&lt;/item&gt;
      &lt;item&gt;FEATURE #3024:&lt;lb/&gt;Add&lt;code&gt;highlight&lt;/code&gt;color type, for styling non-matching text in a matching line.&lt;/item&gt;
      &lt;item&gt;FEATURE #3048:&lt;lb/&gt;Globs in ripgrep (and the&lt;code&gt;globset&lt;/code&gt;crate) now support nested alternates.&lt;/item&gt;
      &lt;item&gt;FEATURE #3096:&lt;lb/&gt;Improve completions for&lt;code&gt;--hyperlink-format&lt;/code&gt;in bash and fish.&lt;/item&gt;
      &lt;item&gt;FEATURE #3102:&lt;lb/&gt;Improve completions for&lt;code&gt;--hyperlink-format&lt;/code&gt;in zsh.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/BurntSushi/ripgrep/releases/tag/15.0.0"/><published>2025-10-18T13:44:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45627394</id><title>1,180 root system drawings</title><updated>2025-10-18T16:10:45.654499+00:00</updated><content>&lt;doc fingerprint="fbf286432d583069"&gt;
  &lt;main&gt;
    &lt;p&gt;Javascript Required To experience full interactivity, please enable Javascript in your browser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://images.wur.nl/digital/collection/coll13/search"/><published>2025-10-18T13:52:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45628186</id><title>Meta convinces Blue Owl to cut $30B check for its Hyperion AI super cluster</title><updated>2025-10-18T16:10:45.327985+00:00</updated><content>&lt;doc fingerprint="9b436e616b6c75b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Meta convinces Blue Owl to cut $30B check for its Hyperion AI super cluster&lt;/head&gt;
    &lt;head rend="h2"&gt;Cleverly concocked deal keeps debt off Social Media empire's books&lt;/head&gt;
    &lt;p&gt;Facebook parent Meta has managed to convince private equity firm Blue Owl Capital to finance its 2.2 gigawatt Hyperion datacenter project in Richland Parish, Louisiana.&lt;/p&gt;
    &lt;p&gt;Meta sealed the deal with the private equity firm on Thursday for roughly $27 billion in debt and 1.5 billion in equity financing, according to a Bloomberg report citing persons familiar with the matter.&lt;/p&gt;
    &lt;p&gt;The agreement, which was reportedly brokered by Morgan Stanley, will see Meta retain a 20 percent stake in the colossal datacenter project and has been structured in such a way as to keep the debt off of the Social Network's balance sheets.&lt;/p&gt;
    &lt;p&gt;The debt, which Bloomberg reports will be carried by the financier, is set to mature in 2049 and is said to be fully amortizing. Meta, meanwhile, will be responsible for building, operating, and ultimately leasing the facility when it comes online in 2029.&lt;/p&gt;
    &lt;p&gt;First announced in late December, the four million square-foot datacenter campus, since branded as Hyperion, was originally expected to cost $10 billion — a figure we suspect didn't include critical IT hardware like servers and networking.&lt;/p&gt;
    &lt;p&gt;Since then, CEO Mark Zuckerberg's ambitions have grown, and the site is now expected to top five gigawatts of total compute capacity making it one of the largest single datacenter projects of the AI boom so far.&lt;/p&gt;
    &lt;p&gt;However, achieving this will require a substantial expansion of power infrastructure. As we learned in December, Meta has commissioned a new natural gas generator plant to be built by local utility operator Entergy.&lt;/p&gt;
    &lt;p&gt;At least for the initial build out, the gas plant would employ three combined cycle combustion turbine generators with a total generative capacity of over 2.2 gigawatts.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Amazon spills plan to nuke Washington...with X-Energy mini-reactors&lt;/item&gt;
      &lt;item&gt;Hyperscalers try to beat the heat with larger racks, more air flow&lt;/item&gt;
      &lt;item&gt;Britain's AI gold rush hits a wall – not enough electricity&lt;/item&gt;
      &lt;item&gt;Decomposed dinosaurs make Texas a top destination for AI bit barns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Meta's Hyperion datacenter development is one of several already underway. Just this week, Meta announced a new datacenter complex in El Paso, Texas, that's expected to scale to a gigawatt of compute capacity.&lt;/p&gt;
    &lt;p&gt;The Instagram parent is also working on another gigawatt datacenter development in Ohio called Prometheus which is expected to become operational next year.&lt;/p&gt;
    &lt;p&gt;El Reg reached out to Meta and Blue Owl for comment, but had not heard back at the time of publication. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/10/17/meta_blue_owl_hyperion/"/><published>2025-10-18T15:39:29+00:00</published></entry></feed>