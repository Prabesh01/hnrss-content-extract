<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-09T18:17:15.183035+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46544981</id><title>The unreasonable effectiveness of the Fourier transform</title><updated>2026-01-09T18:17:23.770071+00:00</updated><content>&lt;doc fingerprint="b6901a02009d27ef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Unreasonable Effectiveness of the Fourier Transform&lt;/head&gt;
    &lt;p&gt;Notes from Joshua Wise's talk at Teardown 2025.&lt;/p&gt;
    &lt;p&gt;New: You can now watch a recording of my talk on my YouTube channel! Or you can just click "play" below, I suppose.&lt;/p&gt;
    &lt;p&gt;Here are a few resources from my talk.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Here is a PDF of my slides, if you wanted to refer to anything in specific.&lt;/item&gt;
      &lt;item&gt;Here is the Jupyter notebook that I used to produce all of the zillions of plots. I do not claim that it is good code, but it is code.&lt;/item&gt;
      &lt;item&gt;The OFDM patent is US3488445A, filed in 1966, expired in 1987.&lt;/item&gt;
      &lt;item&gt;Here is Eugene Wigner's original discussion, "The Unreasonable Effectiveness of Mathematics in the Natural Sciences". There are many good follow-ons to this, too.&lt;/item&gt;
      &lt;item&gt;Here is the paper on how to estimate both carrier offset and time offset at the same time. I implemented it by typing in the algorithm, and it worked, but if you understand it and can explain it to me please let me know.&lt;/item&gt;
      &lt;item&gt;Here is the DVB-T decoder that I wrote. I do not claim that it is the right way to do any of these things, but it is a way to do these things.&lt;/item&gt;
      &lt;item&gt;Finally, here is an absolutely fantastic video that breaks down the implementation of the Fast Fourier Transform algorithm. I watch it every year or two.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks so much for coming! Please let me know if you have feedback on this. I'd love to hear what you thought.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://joshuawise.com/resources/ofdm/"/><published>2026-01-08T19:00:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46545620</id><title>How to Code Claude Code in 200 Lines of Code</title><updated>2026-01-09T18:17:23.476804+00:00</updated><content>&lt;doc fingerprint="94509511ead51683"&gt;
  &lt;main&gt;&lt;p&gt;Today AI coding assistants feel like magic. You describe what you want in sometimes barely coherent English, and they read files, edit your project, and write functional code.&lt;/p&gt;&lt;p&gt;But here’s the thing: the core of these tools isn’t magic. It’s about 200 lines of straightforward Python.&lt;/p&gt;&lt;p&gt;Let’s build a functional coding agent from scratch.&lt;/p&gt;&lt;p&gt;Before we write any code, let’s understand what’s actually happening when you use a coding agent. It’s essentially just a conversation with a powerful LLM that has a toolbox.&lt;/p&gt;&lt;p&gt;That’s the whole loop. The LLM never actually touches your filesystem. It just asks for things to happen, and your code makes them happen.&lt;/p&gt;&lt;p&gt;Our coding agent fundamentally needs three capabilities:&lt;/p&gt;&lt;p&gt;That’s it. Production agents like Claude Code have a few more capabilities including &lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;bash&lt;/code&gt;, &lt;code&gt;websearch&lt;/code&gt;, etc but for our purposes we’ll see that three tools is sufficient to do incredible things.&lt;/p&gt;&lt;p&gt;We start with basic imports and an API client. I’m using OpenAI here, but this works with any LLM provider:&lt;/p&gt;&lt;code&gt;import inspect
import json
import os

import anthropic
from dotenv import load_dotenv
from pathlib import Path
from typing import Any, Dict, List, Tuple

load_dotenv()

claude_client = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])&lt;/code&gt;&lt;p&gt;Some terminal colors to make outputs readable:&lt;/p&gt;&lt;code&gt;YOU_COLOR = "\u001b[94m"
ASSISTANT_COLOR = "\u001b[93m"
RESET_COLOR = "\u001b[0m"&lt;/code&gt;&lt;p&gt;And a utility to resolve file paths (so &lt;code&gt;file.py&lt;/code&gt; becomes &lt;code&gt;/Users/you/project/file.py&lt;/code&gt;):&lt;/p&gt;&lt;code&gt;def resolve_abs_path(path_str: str) -&amp;gt; Path:
    """
    file.py -&amp;gt; /Users/you/project/file.py
    """
    path = Path(path_str).expanduser()
    if not path.is_absolute():
        path = (Path.cwd() / path).resolve()
    return path&lt;/code&gt;&lt;p&gt;Note you should be detailed about your tool function docstrings as they will be used by the LLM to reason about what tools should be called during the conversation. More on this below.&lt;/p&gt;&lt;p&gt;The simplest tool. Take a filename, return its contents:&lt;/p&gt;&lt;code&gt;def read_file_tool(filename: str) -&amp;gt; Dict[str, Any]:
    """
    Gets the full content of a file provided by the user.
    :param filename: The name of the file to read.
    :return: The full content of the file.
    """
    full_path = resolve_abs_path(filename)
    print(full_path)
    with open(str(full_path), "r") as f:
        content = f.read()
    return {
        "file_path": str(full_path),
        "content": content
    }&lt;/code&gt;&lt;p&gt;We return a dictionary because the LLM needs structured context about what happened.&lt;/p&gt;&lt;p&gt;Navigate directories by listing their contents:&lt;/p&gt;&lt;code&gt;def list_files_tool(path: str) -&amp;gt; Dict[str, Any]:
    """
    Lists the files in a directory provided by the user.
    :param path: The path to a directory to list files from.
    :return: A list of files in the directory.
    """
    full_path = resolve_abs_path(path)
    all_files = []
    for item in full_path.iterdir():
        all_files.append({
            "filename": item.name,
            "type": "file" if item.is_file() else "dir"
        })
    return {
        "path": str(full_path),
        "files": all_files
    }&lt;/code&gt;&lt;p&gt;This is the most complex tool, but still straightforward. It handles two cases:&lt;/p&gt;&lt;code&gt;old_str&lt;/code&gt; is empty&lt;code&gt;old_str&lt;/code&gt; and replacing with &lt;code&gt;new_str&lt;/code&gt;&lt;code&gt;def edit_file_tool(path: str, old_str: str, new_str: str) -&amp;gt; Dict[str, Any]:
    """
    Replaces first occurrence of old_str with new_str in file. If old_str is empty,
    create/overwrite file with new_str.
    :param path: The path to the file to edit.
    :param old_str: The string to replace.
    :param new_str: The string to replace with.
    :return: A dictionary with the path to the file and the action taken.
    """
    full_path = resolve_abs_path(path)
    if old_str == "":
        full_path.write_text(new_str, encoding="utf-8")
        return {
            "path": str(full_path),
            "action": "created_file"
        }
    original = full_path.read_text(encoding="utf-8")
    if original.find(old_str) == -1:
        return {
            "path": str(full_path),
            "action": "old_str not found"
        }
    edited = original.replace(old_str, new_str, 1)
    full_path.write_text(edited, encoding="utf-8")
    return {
        "path": str(full_path),
        "action": "edited"
    }&lt;/code&gt;
      &lt;p&gt;The convention here: empty &lt;code&gt;old_str&lt;/code&gt; means “create this file.” Otherwise, find and replace. Real IDEs add sophisticated fallback behavior when the string isn’t found, but this works.&lt;/p&gt;&lt;p&gt;We need a way to look up tools by name:&lt;/p&gt;&lt;code&gt;TOOL_REGISTRY = {
    "read_file": read_file_tool,
    "list_files": list_files_tool,
    "edit_file": edit_file_tool 
}&lt;/code&gt;
      &lt;p&gt;The LLM needs to know what tools exist and how to call them. We generate this dynamically from our function signatures and docstrings:&lt;/p&gt;&lt;code&gt;def get_tool_str_representation(tool_name: str) -&amp;gt; str:
    tool = TOOL_REGISTRY[tool_name]
    return f"""
    Name: {tool_name}
    Description: {tool.__doc__}
    Signature: {inspect.signature(tool)}
    """

def get_full_system_prompt():
    tool_str_repr = ""
    for tool_name in TOOL_REGISTRY:
        tool_str_repr += "TOOL\n===" + get_tool_str_representation(tool_name)
        tool_str_repr += f"\n{'='*15}\n"
    return SYSTEM_PROMPT.format(tool_list_repr=tool_str_repr)&lt;/code&gt;
      &lt;p&gt;And the system prompt itself:&lt;/p&gt;&lt;code&gt;SYSTEM_PROMPT = """
You are a coding assistant whose goal it is to help us solve coding tasks. 
You have access to a series of tools you can execute. Here are the tools you can execute:

{tool_list_repr}

When you want to use a tool, reply with exactly one line in the format: 'tool: TOOL_NAME({{JSON_ARGS}})' and nothing else.
Use compact single-line JSON with double quotes. After receiving a tool_result(...) message, continue the task.
If no tool is needed, respond normally.
"""&lt;/code&gt;
      &lt;p&gt;This is the key insight: we’re just telling the LLM “here are your tools, here’s the format to call them.” The LLM figures out when and how to use them.&lt;/p&gt;&lt;p&gt;When the LLM responds, we need to detect if it’s asking us to run a tool:&lt;/p&gt;&lt;code&gt;def extract_tool_invocations(text: str) -&amp;gt; List[Tuple[str, Dict[str, Any]]]:
    """
    Return list of (tool_name, args) requested in 'tool: name({...})' lines.
    The parser expects single-line, compact JSON in parentheses.
    """
    invocations = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line.startswith("tool:"):
            continue
        try:
            after = line[len("tool:"):].strip()
            name, rest = after.split("(", 1)
            name = name.strip()
            if not rest.endswith(")"):
                continue
            json_str = rest[:-1].strip()
            args = json.loads(json_str)
            invocations.append((name, args))
        except Exception:
            continue
    return invocations&lt;/code&gt;
      &lt;p&gt;Simple text parsing. Look for lines starting with &lt;code&gt;tool:&lt;/code&gt;, extract the function name and JSON arguments.&lt;/p&gt;&lt;p&gt;A thin wrapper around the API:&lt;/p&gt;&lt;code&gt;def execute_llm_call(conversation: List[Dict[str, str]]):
    system_content = ""
    messages = []
    
    for msg in conversation:
        if msg["role"] == "system":
            system_content = msg["content"]
        else:
            messages.append(msg)
    
    response = claude_client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=2000,
        system=system_content,
        messages=messages
    )
    return response.content[0].text&lt;/code&gt;
      &lt;p&gt;Now we put it all together. This is where the “magic” happens:&lt;/p&gt;&lt;code&gt;def run_coding_agent_loop():
    print(get_full_system_prompt())
    conversation = [{
        "role": "system",
        "content": get_full_system_prompt()
    }]
    while True:
        try:
            user_input = input(f"{YOU_COLOR}You:{RESET_COLOR}:")
        except (KeyboardInterrupt, EOFError):
            break
        conversation.append({
            "role": "user",
            "content": user_input.strip()
        })
        while True:
            assistant_response = execute_llm_call(conversation)
            tool_invocations = extract_tool_invocations(assistant_response)
            if not tool_invocations:
                print(f"{ASSISTANT_COLOR}Assistant:{RESET_COLOR}: {assistant_response}")
                conversation.append({
                    "role": "assistant",
                    "content": assistant_response
                })
                break
            for name, args in tool_invocations:
                tool = TOOL_REGISTRY[name]
                resp = ""
                print(name, args)
                if name == "read_file":
                    resp = tool(args.get("filename", "."))
                elif name == "list_files":
                    resp = tool(args.get("path", "."))
                elif name == "edit_file":
                    resp = tool(args.get("path", "."), 
                                args.get("old_str", ""), 
                                args.get("new_str", ""))
                conversation.append({
                    "role": "user",
                    "content": f"tool_result({json.dumps(resp)})"
                })&lt;/code&gt;
      &lt;p&gt;The structure:&lt;/p&gt;&lt;p&gt;Inner loop: Call LLM, check for tool invocations&lt;/p&gt;&lt;p&gt;The inner loop continues until the LLM responds without requesting any tools. This lets the agent chain multiple tool calls (read a file, then edit it, then confirm the edit).&lt;/p&gt;&lt;code&gt;if __name__ == "__main__":
    run_coding_agent_loop()&lt;/code&gt;
      &lt;p&gt;Now you can have conversations like:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;You: Make me a new file called hello.py and implement hello world in it&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Agent calls edit_file with path=“hello.py”, old_str="", new_str=“print(‘Hello World’)”&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Assistant: Done! Created hello.py with a hello world implementation.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Or multi-step interactions:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;You: Edit hello.py and add a function for multiplying two numbers&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Agent calls read_file to see current contents. Agent calls edit_file to add the function.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Assistant: Added a multiply function to hello.py.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is about 200 lines. Production tools like Claude Code add:&lt;/p&gt;&lt;p&gt;But the core loop? It’s exactly what we built here. The LLM decides what to do, your code executes it, results flow back. That’s the whole architecture.&lt;/p&gt;&lt;p&gt;The full source is about 200 lines. Swap in your preferred LLM provider, adjust the system prompt, add more tools as an exercise. You’ll be surprised how capable this simple pattern is.&lt;/p&gt;&lt;p&gt;This is part of the first module in my modern AI software engineering course based on my Stanford lectures. Check it out here.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.mihaileric.com/The-Emperor-Has-No-Clothes/"/><published>2026-01-08T19:54:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46546113</id><title>Sopro TTS: A 169M model with zero-shot voice cloning that runs on the CPU</title><updated>2026-01-09T18:17:22.956495+00:00</updated><content>&lt;doc fingerprint="1d983b58ad7ac7f2"&gt;
  &lt;main&gt;
    &lt;head class="px-3 py-2"&gt;sopro_readme.mp4&lt;/head&gt;
    &lt;p&gt;Sopro (from the Portuguese word for “breath/blow”) is a lightweight English text-to-speech model I trained as a side project. Sopro is composed of dilated convs (à la WaveNet) and lightweight cross-attention layers, instead of the common Transformer architecture. Even though Sopro is not SOTA across most voices and situations, I still think it’s a cool project made with a very low budget (trained on a single L40S GPU), and it can be improved with better data.&lt;/p&gt;
    &lt;p&gt;Some of the main features are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;169M parameters&lt;/item&gt;
      &lt;item&gt;Streaming&lt;/item&gt;
      &lt;item&gt;Zero-shot voice cloning&lt;/item&gt;
      &lt;item&gt;0.25 RTF on CPU (measured on an M3 base model), meaning it generates 30 seconds of audio in 7.5 seconds&lt;/item&gt;
      &lt;item&gt;3-12 seconds of reference audio for voice cloning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I only pinned the minimum dependency versions so you can install the package without having to create a separate env. However, some versions of Torch work best. For example, on my M3 CPU, &lt;code&gt;torch==2.6.0&lt;/code&gt; (without &lt;code&gt;torchvision&lt;/code&gt;) achieves ~3× more performance.&lt;/p&gt;
    &lt;p&gt;(Optional)&lt;/p&gt;
    &lt;code&gt;conda create -n soprotts python=3.10
conda activate soprotts&lt;/code&gt;
    &lt;code&gt;pip install sopro&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/samuel-vitorino/sopro
cd sopro
pip install -e .&lt;/code&gt;
    &lt;code&gt;soprotts \
  --text "Sopro is a lightweight 169 million parameter text-to-speech model. Some of the main features are streaming, zero-shot voice cloning, and 0.25 real-time factor on the CPU." \
  --ref_audio ref.wav \
  --out out.wav&lt;/code&gt;
    &lt;p&gt;You have the expected &lt;code&gt;temperature&lt;/code&gt; and &lt;code&gt;top_p&lt;/code&gt; parameters, alongside:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--style_strength&lt;/code&gt;(controls the FiLM strength; increasing it can improve or reduce voice similarity; default&lt;code&gt;1.0&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--no_stop_head&lt;/code&gt;to disable early stopping&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--stop_threshold&lt;/code&gt;and&lt;code&gt;--stop_patience&lt;/code&gt;(number of consecutive frames that must be classified as final before stopping). For short sentences, the stop head may fail to trigger, in which case you can lower these values. Likewise, if the model stops before producing the full text, adjusting these parameters up can help.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;from sopro import SoproTTS

tts = SoproTTS.from_pretrained("samuel-vitorino/sopro", device="cpu")

wav = tts.synthesize(
    "Hello! This is a non-streaming Sopro TTS example.",
    ref_audio_path="ref.wav",
)

tts.save_wav("out.wav", wav)&lt;/code&gt;
    &lt;code&gt;import torch
from sopro import SoproTTS

tts = SoproTTS.from_pretrained("samuel-vitorino/sopro", device="cpu")

chunks = []
for chunk in tts.stream(
    "Hello! This is a streaming Sopro TTS example.",
    ref_audio_path="ref.mp3",
):
    chunks.append(chunk.cpu())

wav = torch.cat(chunks, dim=-1)
tts.save_wav("out_stream.wav", wav)&lt;/code&gt;
    &lt;p&gt;After you install the &lt;code&gt;sopro&lt;/code&gt; package:&lt;/p&gt;
    &lt;code&gt;pip install -r demo/requirements.txt
uvicorn demo.server:app --host 0.0.0.0 --port 8000&lt;/code&gt;
    &lt;p&gt;Or with docker:&lt;/p&gt;
    &lt;code&gt;docker build -t sopro-demo .
docker run --rm -p 8000:8000 sopro-demo&lt;/code&gt;
    &lt;p&gt;Navigate to http://localhost:8000 on your browser.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sopro can be inconsistent, so mess around with the parameters until you get a decent sample.&lt;/item&gt;
      &lt;item&gt;Voice cloning is highly dependent on mic quality, ambient noise, etc. On more OOD voices it might fail to match the voice well.&lt;/item&gt;
      &lt;item&gt;Prefer phonemes instead of abbreviations and symbols. For example, &lt;code&gt;“1 + 2”&lt;/code&gt;→&lt;code&gt;“1 plus 2”&lt;/code&gt;. That said, Sopro can generally read abbreviations like “CPU”, “TTS”, etc.&lt;/item&gt;
      &lt;item&gt;The streaming version is not bit-exact compared to the non-streaming version. For best quality, prioritize the non-streaming version.&lt;/item&gt;
      &lt;item&gt;If you use torchaudio to read or write audio, ffmpeg may be required. I recommend just using soundfile.&lt;/item&gt;
      &lt;item&gt;I will publish the training code once I have time to organize it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Due to budget constraints, the dataset used for training was pre-tokenized and the raw audio was discarded (it took up a lot of space). Later in training, I could have used the raw audio to improve the speaker embedding / voice similarity, because some nuances of voice are lost when you compress it with a neural codec into a discrete space.&lt;/p&gt;
    &lt;p&gt;I didn't lose much time trying to optimize further, but there is still some room for improvement. For example, caching conv states.&lt;/p&gt;
    &lt;p&gt;Currently, generation is limited to ~32 seconds (400 frames). You can increase it, but the model generally hallucinates beyond that.&lt;/p&gt;
    &lt;p&gt;AI was used mainly for creating the web demo, organizing my messy code into this repo, ablations and brainstorming.&lt;/p&gt;
    &lt;p&gt;I would love to support more languages and continue improving the model. If you like this project, consider buying me a coffee so I can buy more compute: https://buymeacoffee.com/samuelvitorino&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/samuel-vitorino/sopro"/><published>2026-01-08T20:37:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46546413</id><title>Mux (YC W16) is hiring a platform engineer that cares about (internal) DX</title><updated>2026-01-09T18:17:22.788375+00:00</updated><content/><link href="https://www.mux.com/jobs"/><published>2026-01-08T21:01:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46546614</id><title>Richard D. James aka Aphex Twin speaks to Tatsuya Takahashi (2017)</title><updated>2026-01-09T18:17:22.314184+00:00</updated><content>&lt;doc fingerprint="363a18141fe60bbb"&gt;
  &lt;main&gt;
    &lt;p&gt;Richard D. James: I really enjoyed working on this with you. I know I only joined the project near the end, but I found it really exciting. Like a proper job, ha.&lt;/p&gt;
    &lt;p&gt;Tatsuya Takahashi: Richard, it was amazing working with you on the monologue. And now to be interviewed by you?!? That's crazy. But also a lot of fun. The monologue was also the last Korg synth that I was involved with directly, so I guess it's a nice conclusion to things.&lt;/p&gt;
    &lt;p&gt;RDJ: It is now the only synth on the market currently being made to have full microtuning editing, congratulations!&lt;/p&gt;
    &lt;p&gt;TT: Thanks! But it was completely because of you that we included microtuning. If you hadn't insisted on it, I definitely wouldn't have discovered how powerful it was. Did you ever have a moment of realisation, or some kind of trigger that made you discover microtuning?&lt;/p&gt;
    &lt;p&gt;RDJ: The first thoughts that I had about tuning in general happened with my early noodlings on a Yamaha DX100, one of the first synths I saved up for. I remember looking at the master tuning of 440 Hz and thinking I would change it, for no other reason apart from it was set by default to that frequency and that it could be changed.&lt;/p&gt;
    &lt;p&gt;I just used to select a single note, adjust the master tuning of it to taste and then base the whole track around that, something I’ve done ever since, just intuition and maybe a bit of rebelliousness. It’s very simple, but do you want your music to be based on an international standard or on what you think sounds right to you?&lt;/p&gt;
    &lt;p&gt;I’ve since gone on to learn more about this damn 440 Hz. It was a standard introduced in 1939 by western governments, so I’m very glad I trusted my instincts. Listening to that other voice is THE most important thing in creativity, whether you’re an engineer or a musician. Tesla had some important advice on listening to the thoughts from the other. One of the most important inventors ever, but we’re not taught about him in British schools. Funny that.&lt;/p&gt;
    &lt;p&gt;TT: I don't know why it's thin on the curriculum, but the Tesla coil is definitely amazing. If you modulate the high frequency with audio signals you can play music with plasma – that's super cool. I will read up on him though, cos I don't know much about his life and thinking.&lt;/p&gt;
    &lt;p&gt;RDJ: An interesting “note”: I’ve just been reading a book on electronic instruments published in the 1940’s and it says that 440 Hz was transmitted over the radio on different frequencies 24 hours a day and others between midnight and 2 in the afternoon, ha, so you could tune your instruments and be well behaved or calibrate your lab equipment to it.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;It’s very simple, but do you want your music to be based on an international standard or on what you think sounds right to you?&lt;/p&gt;RICHARD D. JAMES&lt;/quote&gt;
    &lt;p&gt;But I’ve also read studies from the old Philips laboratories in the Netherlands that show orchestras average deviation from 440 Hz was measured over many concerts and was seen to differ by a few Hz, usually slightly below. Pretty anal. Some people obviously really cared that 440 Hz was being adhered to in practice.&lt;/p&gt;
    &lt;p&gt;Why 440 Hz was chosen in the first place is another interesting story, but looking at the resonances of water and sound is a great place to start, or read up on cymatics. If you aren’t already familiar with it, that is.&lt;/p&gt;
    &lt;p&gt;TT: So many things are standardised that you don't really think about because they were there before you started using it. 440 Hz was brought about to standardise the way people play together and, yeah, someone can bring a guitar to a piano and it would work together because of that standard.&lt;/p&gt;
    &lt;p&gt;It's like how a green light means you can cross the road or if you shake your head sideways it means no. Those two standards will help you through life in many places around the world. But it's dangerous to enforce standards in creativity. I have a son who's started school in Japan, where every kid will paint the sun red. Now that is some fucked up standardisation! Just really messed up on so many levels.&lt;/p&gt;
    &lt;p&gt;Anyway, I'm not going into that whole 432 Hz vs 440 Hz debate. (BTW: I absolutely love cymatics and I've done some nice workshops for kids with it.) But I will say different frequencies sound different, so why not use that in your music? You got to use whatever feels right and the monologue let's you do exactly that with pitch.&lt;/p&gt;
    &lt;p&gt;RDJ: Yep.&lt;/p&gt;
    &lt;p&gt;TT: Talking of standards, the sample rate of 48 kHz is another one for sampling and signal processing, but the volca sample uses a weird one at 31.25 kHz. Purely because of technical constraints, but I was thinking that might be part of the reason you liked it so much, because the different sample rate gives it a unique sound.&lt;/p&gt;
    &lt;p&gt;RDJ: Haha, yes, it was pretty much the first thing I noticed. Yeah, I thought the 48 kHz, was based on the Nyquist Theorem. I think it’s double what humans can apparently hear or something, which is another weird one. I don’t know how anybody worked out humans only hear to 20 kHz. I mean even if you can’t hear above 20 kHz, it doesn't mean that your body doesn't feel it. You don’t just experience sound through your eardrums. A good example of this is listening to a recording of your own voice. To almost everyone apart from maybe the most narcissistic, it always sounds weird/thinner/smaller, as you don’t feel the vibration of your chest and body. There are other reasons of course but that’s one for sure. Anyway, I’m into the extremes of the audio spectrum, ultra clarity ’n’ all but I probably prefer fucked-muffled/lo-bit/’70s sound more, ha!&lt;/p&gt;
    &lt;p&gt;TT: Oh, and when something defies the standard – I just remembered the first time I played a Yamaha SK-10, the faders were all upside down, like max was downwards, even on the volume. I didn't know what was going on and it threw me off at first, but it's actually a bit fun like that and you soon realise it all comes from organ drawbars.&lt;/p&gt;
    &lt;p&gt;RDJ: I never played the SK-10, but these Calrec mixers I use are like that also, the faders are backwards. There is a little dip switch inside to change it, but I think they have them like that for TV/broadcasting, coz if someone falls asleep at the desk they don’t want them to push all the faders up and distort two million TVs at once… Not surprising they have this safeguard considering how skull numbingly boring most TV is.&lt;/p&gt;
    &lt;p&gt;TT: Right!! Yeah, but there is a certain feeling to pulling rather than pushing. It's like how an orgasm is "coming" in English, but it's “going” [iku] in Japanese.&lt;/p&gt;
    &lt;p&gt;RDJ: Never thought of it like that.&lt;/p&gt;
    &lt;p&gt;TT: I mean, written text in Japanese was traditionally vertical. Although now a lot is westernised and horizontal.&lt;/p&gt;
    &lt;p&gt;RDJ: Ah, that’s kinda sad… So traditional Japanese text is like trackers and now it’s going like Cubase! :)&lt;/p&gt;
    &lt;p&gt;TT: I sometimes wonder what Japanese synths would have looked like if they didn't copy Moog in the ’70s. You've got to think about what is convention and what is really a good design.&lt;/p&gt;
    &lt;p&gt;RDJ: I’ve got one Japanese keyboard, Suzuki, which has got some Japanese tunings built in and a little string on one end that you can pluck. It sounds really nice as well. It also has some good Japanese percussion and MIDI. I don’t think it’s very well known.&lt;/p&gt;
    &lt;p&gt;I wish faders were curved horizontally and vertically, so you could make them like a double helix that go over and under each other, hehe. Could do it with an augmented reality UI I guess.&lt;/p&gt;
    &lt;p&gt;TT: Now that could be cool (if I'm imagining it right)! I've seen rotation sensors on the camera lens focus that work like faders on a curved surface and really thin. That could do it.&lt;/p&gt;
    &lt;p&gt;RDJ: Later on when I got an SH-101, I realised its tuning wasn't like the DX100 at all. It was based on 1v/octave and was supposed to be equal temperament but because of the nature of analogue, it really wasn’t and I REALLY loved that and how it layered with the frozen 12TET of the DX100.&lt;/p&gt;
    &lt;p&gt;I recently made a tuning on the monologue that I matched to an improperly calibrated SH-101 that I was fond of. I tried at first to do this using formulas inside Scala, but it's impossible to represent this accurately with simple maths, Scala can’t deal with these types of tunings unless it’s a keyboard map tuning file. This “bad” tuning is really great when I apply it to a precisely tuned digital synth that has full microtuning capabilities. It’s top making a digital synth sound like an out of tune 101! :)&lt;/p&gt;
    &lt;p&gt;TT: Yeah, I think it's really telling of the age we live in when you get a knob like "SLOP" on the new Prophet that makes pitch inconsistencies a programmable parameter. On one hand, you think that the level of control is great, but on the other it feels weird to deliberately degrade something that's stable. Especially if you're a young engineer striving to design something to be close as possible to perfection, it can be hard to grasp. The best lesson about this came from Mieda – my hero at Korg. When he looked at my first synth schematic, he told me, “Takahashi-kun, your circuits are functional, but they are not musical. Musical instruments do not need perfect waveforms and correct operating points. You need to use the transistor for what it is. As long as it sounds good, it’s OK.”&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;WHEN [MIEDA] LOOKED AT MY FIRST SYNTH SCHEMATIC, HE TOLD ME, “TAKAHASHI-KUN, YOUR CIRCUITS ARE FUNCTIONAL, BUT THEY ARE NOT MUSICAL. MUSICAL INSTRUMENTS DO NOT NEED PERFECT WAVEFORMS AND CORRECT OPERATING POINTS. YOU NEED TO USE THE TRANSISTOR FOR WHAT IT IS. AS LONG AS IT SOUNDS GOOD, IT’S OK.&lt;/p&gt;TATSUYA TAKAHASHI&lt;/quote&gt;
    &lt;p&gt;RDJ: I was going to ask you about SLOP, as you brought that up before in some old emails. I get you now. I mean, yeah, if it just sounds good in the first place then you don’t need that option, but I guess some people like their Osc’s drifty and others not so. It changes with the context I guess. Also, if you’re doing FM you might want to keep them dead on, and for analogue lead sounds, really drifty. Anyway I think I mentioned it before, but the drift on the monologue sounds REALLY nice. It seems to move, but then never go out. Care to explain? Sounds to me like it gets reset/synced at some point, but I’m probably wrong, haven't studied it in depth, just listened. Reminds me a bit of Arp oscillators, which have really nice driftyness, prob my faves! :)&lt;/p&gt;
    &lt;p&gt;TT: That's bang on! So same thing in the minilogue and the volcas too: the oscillators are re-tuned when they're not being used. I'm super glad you like it though because this is such a subjective thing. The autotuning was done in a way that felt nice to me, so it was a really subjective thing and you can’t present a report to convince others that it was OK. At least now I can say RDJ said it was alright!&lt;/p&gt;
    &lt;p&gt;RDJ: I’d like to talk more about this 1v/octave, but that’s for another time. But, anyway, getting back to the question, I was always interested in sound and how it affected me, especially the tuning. It wasn't until my *Selected Ambient Works Vol. II* album that I actually made my own full custom tunings, although there were a few scattered things before that.&lt;/p&gt;
    &lt;p&gt;I’ve got a slightly weird balance thing going on and getting the tuning “right” sometimes makes the balance thing less weird for me. It’s a longer story though.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, I think I read somewhere about how humans normally hear pitches differently in the left and right ear and that you don't have that. That is super interesting.&lt;/p&gt;
    &lt;p&gt;RDJ: Because we made it very intuitive to edit the tuning tables, I would actually just buy this synth only for that feature alone. When the export is implemented, it can be the central hub of either complete table creation or just to tweak existing imported Scala files, etc.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, absolutely. I would definitely download the monologue librarian because you can import and export Scala files easily with that. Hopefully other manufacturers will join the club.&lt;/p&gt;
    &lt;p&gt;The intuitive interface was pretty much all your idea, so a great job on that. I think your idea for the interface came from when you got your Chroma modded for microtuning. Have you modded a lot of synths for this functionality?&lt;/p&gt;
    &lt;p&gt;RDJ: That’s right, I burned my own custom O.S. Eproms for the Chroma, which enables full micro tuning and editing and that’s what the monologue editor was based on. I’ve got a good list of hardware and software now that can do it. It’s been a long haul and involved hassling a lot of people, but it is now finally possible with quite a bit of equipment.&lt;/p&gt;
    &lt;p&gt;I’ve generally received really good responses from engineers and programmers. I’ve contacted around 50 different people/companies in the last ten years. Many weren’t even aware that all their equipment and programs were adhering to a standard that was devised hundreds of years ago.&lt;/p&gt;
    &lt;p&gt;Same goes for a lot of electronic musicians, this is quite surprising for electronic music, which supposedly is forward-thinking and futuristic, but most people have since told me how fascinating they have found the subject once they realised it *was* a subject!&lt;/p&gt;
    &lt;p&gt;I know microtuning is much more useful on polyphonic keyboards, but it’s still very usable on monophonic instruments and, again, it can be used in the future to create tuning tables that can be used in other Scala-compatible polyphonic synths.&lt;/p&gt;
    &lt;p&gt;TT: Well, my initial impression was that microtuning is a really niche thing that wouldn't be needed for a mass market synth, especially a monophonic one, but if you try shifting the tuning while running a sequence, you can hear that it gives it another dimension even if it’s subtle. I'm not super-sensitive to pitch or anything, but you can still hear it change. To me, it feels like casting light on a rough surface and seeing different patterns as you move the light. So it was really important to have the easy scale edits you can do on the fly. Scala is great, it's super flexible, but it can be daunting to use and you won't get the real-time interaction, so I hope the monologue gets more people into this stuff.&lt;/p&gt;
    &lt;p&gt;RDJ: I really like your light analogy, that’s great. Yep, on a monophonic instrument, what you just described will be more pronounced if you use a delay with plenty of feedback or reverb, so you can hear the differently tuned notes overlap each other.&lt;/p&gt;
    &lt;p&gt;Scala is deep, very deep, but some things are very quick and easy to get going. For instance, you can just type Equal 24 &amp;amp; press the sysex send shortcut and you have a quarter tone tuning in your synth. Scala is only good for non-intuitive tuning creation, purely mathematical. I love this approach, but really prefer making tunings intuitively, note-by-note. When you’re actually composing something, making them up while you go along, a combination of the two is best for me.&lt;/p&gt;
    &lt;p&gt;TT: I know that you like that Wilsonic app you showed me, which is mainly structured on mathematical relationships of frequencies, but you've also mentioned using a lot of trial-and-error. Do you have a method to your microtuning?&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, many. For instance, on the Chroma I like holding down one key, pressing another key and then tuning the second key in relation to the first, sometimes making two extremely different frequency combinations, like something very low and extremely high at the same time and maybe a group of these dual combos only existing in the top octave of the keyboard map, the rest being another tuning or multiple tunings, all in one tuning table.&lt;/p&gt;
    &lt;p&gt;It’s something I never saw in anyone else’s tunings, combining several tuning tables within one map, so that’s one of my little inventions I guess, as I rarely used the full range of 127 notes in one tuning within one track. monologue can tune four notes at a time which we planned. It’s a different approach again and something I look forward to experimenting with more.&lt;/p&gt;
    &lt;p&gt;TT: Here are five short tracks you made with custom scales. Could you explain how you came up with the scales?&lt;/p&gt;
    &lt;p&gt;RDJ: I forgot which tunings they used, I’ve got so many floating around in folders on the computer and in hardware. I didn’t make any notes. I think they might have been ones that I made in Scala and then tweaked on the monologue, most likely.&lt;/p&gt;
    &lt;p&gt;TT: If you could share the tuning files that you created, that would be great too!&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, I’ve got loads saved and loads lost. I’ve never been a saver. I do save more things these days, getting older or something, but still love to use new sets of rules for every set of new tracks. Also I’ve got to say again many thanks for that lovely MIDI tuning box you made me for the minilogue!&lt;/p&gt;
    &lt;p&gt;TT: No problem! That was an eye-opener for all of us. [For the readers: Richard asked me for microtuning on our synths and since, at the time, we thought it wasn't something we would put on a production model, we made a custom little tuning tool. Fellow engineer Kazuki Saita and I made a MIDI thru box that could load custom scales. Any MIDI coming in would be transposed by note and cent (using pitch bend) and so you could get microtuning on any mono synth.] When we were testing that box, Saita and I were blown away. I mean, sequencing on a simple step sequencer like in the monologue can be a bit rigid, but messing with the tuning really opens it up. It basically redefines the keyboard. We were messing around with some subtle stuff and more extreme ones like octaves split into 50 intervals and playing with the arpeggiator. It was crazy and that's when we decided we should put it on the next synth.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, great! Arpeggiators and microtunings can be a very nice mix. We should include a picture of that box, I’ve got one here if you don’t.&lt;/p&gt;
    &lt;p&gt;TT: We should! Don't have one handy, would you be able to snap a photo?&lt;/p&gt;
    &lt;p&gt;RDJ: Attached it!&lt;/p&gt;
    &lt;p&gt;TT: Cheers! Wood cheeks for the Cirklon. Nice.&lt;/p&gt;
    &lt;p&gt;RDJ: I think the monologue is very nice looking, small, very cute and very capable. At first I thought, “Oh, it hasn’t got this, it hasn’t got that, etc. etc.” But I very quickly realised you have turned these limitations into advantages, which is really quite something special. I really mean that. The lack of extensive features makes the whole thing much more speedy to work with.&lt;/p&gt;
    &lt;p&gt;TT: That's got to be the best compliment. And it's a way of thinking that runs through all the synths I've worked on, from the volcas and monotrons to the monologue. I think with electronic instruments we've got to a point where software can do most things. But I'm a fan of gear where less is more – where the simplest controls can give you the most creative freedom.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, I like this approach. It’s true, I do it with modular setups as well. I’m lucky to have loads of modular gear but I prefer to make small systems now and leave everything else in another room where I just try things out before committing them to a more thought out config.&lt;/p&gt;
    &lt;p&gt;Of course us musicians always look at something new and we see if it does what we expect it to. And this is OK. But we shouldn’t overlook something before actually trying it out, try and get into the head of the designer first. I try and do this. It’s difficult sometimes to push your ego and expectations out of the way for a while, but if we don’t do this we won’t learn anything new. That’s not to say that every designer’s head is worth getting into, but we gotta give it a go sometimes.&lt;/p&gt;
    &lt;p&gt;TT: This is exactly the reason I really enjoyed working with you. I'd send you a prototype and a day later you'd be sending me a dozen emails about how the drive circuit actually controls gain and dry/wet at the same time. Or how some menu option wasn’t working completely as intended. You would give everything a chance. You went through every single menu option and went after some easter eggs, like finding CC34 VCO1 pitch! In fact, you were the best ever beta tester. Guess you wouldn't be after a day job tho...&lt;/p&gt;
    &lt;p&gt;RDJ: *blush* Some examples of this: When I first checked out the volca sample, the lack of velocity response had me scratching my head, but when I realised how it handled it with motion recording of the level control, it was actually loads more fun and SO much faster to program! It’s such a great little idea, I really love it, way more intuitive. I’ve started doing it this way on the Cirklon now sometimes.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, so you're a huge fan of the Cirklon, which you used for "korg funk 5." Could you tell us how that track was put together?&lt;/p&gt;
    &lt;p&gt;Here's the gear list you sent me:&lt;lb/&gt; Korg Monologue x3&lt;lb/&gt; Korg MS-20 kit&lt;lb/&gt; Korg Poly-61M&lt;lb/&gt; Korg Volca keys&lt;lb/&gt; Korg Volca beats&lt;lb/&gt; Korg Volca sample&lt;lb/&gt; Korg Minilogue&lt;lb/&gt; My son on vox&lt;/p&gt;
    &lt;p&gt;I was blown away by this and really really touched. I don't think there is another track out there using so much of the gear I worked on! Also, can you touch on the processing that went on the sounds, cos I can tell there's a lot going on.&lt;/p&gt;
    &lt;p&gt;RDJ: That’s so nice to hear… It was really top making some tracks with only Korg gear. I’m a secret nerd-fan of synth demos, mainly vintage ’80s ones currently! Some amazing music has been made as equipment demos, unsung heroes. I collect synth demos. Well, ones that I like. It’s kind of an unclassified music genre, so doing these tracks for you and Korg was a natural thing for me. I also really like picking certain combinations of gear. That is endlessly fascinating.&lt;/p&gt;
    &lt;p&gt;The Volca beats I used, I did the snare mod but used the mix output, so I treated all the sounds with the same treatment, I think I sent you the full list… looks it up… OK, here it is.&lt;/p&gt;
    &lt;p&gt;volca beats &amp;gt; Skibbe 736-5 mic pre [nice low mid sound] &amp;gt; BAC 500 compressor &amp;gt; RTZ PEQ1549 [this is based on my fave eq, I’ve got some Calrec originals as well, standard circuit design but not standard sound! ] &amp;gt; Calrec minimixer&lt;/p&gt;
    &lt;p&gt;Monologue [main riff] &amp;gt; blonder tongue EQ [i love these eq’s, hardly anyone has heard of them]&lt;/p&gt;
    &lt;p&gt;TT: Any chance you could share the tracks separately? There might be something we could do with that and a lot of people will be interested in seeing how the different synths sound soloed. Only if you're up for it of course!&lt;/p&gt;
    &lt;p&gt;RDJ: I would if I had them, but I never save individual tracks. I’m trying to get into the habit of that soon. I just recorded that down to the Sound Devices 722.&lt;/p&gt;
    &lt;p&gt;TT: Ah shame! But you know that was the other great thing – that the track was done totally sequenced on the Cirklon and recorded in one take.&lt;/p&gt;
    &lt;p&gt;RDJ: I was thinking a while back on different ways to visualise the data in the Cirklon. Also with the volca fm, you also managed to turn the lack of velocity per note into a bonus [again], it puts a different slant on it, applying and recording motion velocity on the whole phrase, it works very well.&lt;/p&gt;
    &lt;p&gt;TT: So the volca keyboard is never going to do a great job of sensing velocity and we could have spent a lot more money to make it velocity-sensitive, but then you'd sit there going, "Well, it's too small to play. We need to make it bigger..." So trying to force it to be something it's not is a great way of creating more problems. Much rather turn the game around.&lt;/p&gt;
    &lt;p&gt;RDJ: That’s a great example of necessity and invention. I was absolutely amazed to find out that it IS actually possible to edit a DX7 voice with great speed from the interface you have designed. I never thought you could do that, but it is and is totally usable. I’ve come up with loads of things on it that I would never have done on a full size DX7. Hats off to Tats!&lt;/p&gt;
    &lt;p&gt;TT: Cheers! So everyone knows the typical DX7 sounds – well, the presets anyway – and by doing things a bit differently, you can open up so much stuff. Take an organ patch on the volca fm and sequence it normally, but then motion sequence the algorithm and it goes in a completely different dimension. It's a discovery, which is fun. I find a lot of artists are discovery junkies.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, I think I HAVE to be learning something when making tracks, even if it’s something very small. If there’s no learning involved, I wouldn’t get excited enough to do anything. Great fun being able to take a DX7 in your pocket, love it, ultimate walkman in a way. In fact, one for the future: volca fm with built in MP3 player + radio… be super lush.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, super great idea! Also if it could tap into some MIDI archives and play them on the FM engine, it would be great.&lt;/p&gt;
    &lt;p&gt;RDJ: Or maybe a pitch tracker from the MP3s! :-)&lt;/p&gt;
    &lt;p&gt;TT: Even better! :) And it can take real time mic input, so people are saying hello to you, but you're just hearing bells or something.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, recently I was offering up ideas to a talented coder friend on an app that uses evolutionary/genetic synthesis to try and resynthesise audio/live audio into DX7 patches. It sounds really cool. He’s working on making it a standalone app on Raspberry Pi, and it is based on some vintage code by Andrew Horner. Kyma also used his code for their GA synthesis. Chuck that in there while we’re at it.&lt;/p&gt;
    &lt;p&gt;TT: Got to say it's pretty funny getting a consumer product idea from you. Haha!&lt;/p&gt;
    &lt;p&gt;RDJ: :) I’m full of ‘em, I’m like this guy.&lt;/p&gt;
    &lt;p&gt;TT: BAHAHAHHA! Holy crap.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;I think I HAVE to be learning something when making tracks, even if it’s something very small. If there’s no learning involved, I wouldn’t get excited enough to do anything.&lt;/p&gt;Richard D. James&lt;/quote&gt;
    &lt;p&gt;RDJ: How different is the finished monologue to what was designed or what you had in mind?&lt;/p&gt;
    &lt;p&gt;TT: Well, it didn't have microtuning for a start!&lt;/p&gt;
    &lt;p&gt;RDJ: :)&lt;/p&gt;
    &lt;p&gt;TT: When I initially came up with the product plan, it wasn't very detailed. None of my product plans are. Something like: "smaller than the minilogue and monophonic." It's only when you start designing and prototyping that things start to come together. Things like: “What kind of filter do we need?” “Do we need distortion?” “Battery power would be great!”&lt;/p&gt;
    &lt;p&gt;RDJ: If there are features that were designed that didn’t make it, could you tell us about them?&lt;/p&gt;
    &lt;p&gt;TT: Nothing really got properly designed before being ditched. The team is pretty good at putting together test versions where we can just about see if something is going to work before we go to full implementation.&lt;/p&gt;
    &lt;p&gt;In terms of ideas, you had some pretty good ones:&lt;/p&gt;
    &lt;p&gt;I think the team had others like arpeggiator, which is the most obvious one. But we dropped that and added key-trigger sequence instead.&lt;/p&gt;
    &lt;p&gt;RDJ: When or how do you find out that features that were wanted by your team are not going to make it? Is that frustrating?&lt;/p&gt;
    &lt;p&gt;TT: Well, it's not like someone stands there casting their decision on whether something makes it or not. We all try to figure out how it will come together as an instrument, so a single feature might be the focus in a heated discussion, but really it's about the whole thing being coherent but also incoherent and surprising in a good way. Sometimes you need to throw people off what they're expecting to do something interesting. The team was always pretty small, so we could do it without having a draconian decision-making process, but also without it getting too democratic either. We would never ever vote on a feature.&lt;/p&gt;
    &lt;p&gt;RDJ: Would it be possible that Korg could release limited edition and more costly versions of your designs with no corners cut, for us posh musos?&lt;/p&gt;
    &lt;p&gt;TT: Sure, that's definitely a possibility. What's on your wish list?&lt;/p&gt;
    &lt;p&gt;RDJ: Oh dear, that is a big question, I think I’ll have to get back to you on that. Well, those ones above to start with I suppose. :) Do you have a studio at home? Got any pics? Or a description of your setup?&lt;/p&gt;
    &lt;p&gt;TT: I wouldn't say it's a studio, but more of a workshop. I build stuff there for my own live setup, although recently most of it is made up of products I've worked on. One of my favourite things is volca fm going into audio input of monotribe which has been modded so you can kill the VCO. I put on a slow chord progression on the fm and then work a sequence with it with the monotribe. It's actually better if I don't sync the volca fm to the monotribe.&lt;/p&gt;
    &lt;p&gt;RDJ: Nice, I keep meaning to rack up 8 analogue filters to a TX802. Nobody ever made a decent FM synth with analogue filters, there are a few simple FM ones but not 4OP+.&lt;/p&gt;
    &lt;p&gt;TT: My other favourite thing is my speaker system designed by my friends at Taguchi. They're omni-directional and I've been experimenting with the positions. My room is acoustically untreated, but with these speakers you can actually work with the reflections in the room. It's definitely not a typical setup, but it's great because you can pan your instruments around the room and you’re not glued to a sweet spot between a stereo pair. It's great if you just sequence piano phase on two volcas and offset the BPM and just let it run while the sequence phases in and out. The trick there is actually not to hard pan them, but to leave quite a bit of overlap.&lt;/p&gt;
    &lt;p&gt;RDJ: [*looks at pics*] Great, that is an unusual speaker setup! I’m a big fan of suspending speakers from the ceiling, the first speakers that I built, I filled with tar and hung them from nylon cords from my bedroom ceiling. Saves space as well. Do you live and breathe Korg, do you get time for anything else, any other hobbies?&lt;/p&gt;
    &lt;p&gt;TT: Don't know if it counts as a hobby, but I really like polyhedra. Maybe that’s why I like those speakers, since they're great 3D structures hanging off my ceiling. My favourite polyhedron is the dodecahedron and when you make one with wire, it's hard to make it completely regular. But it turns out I actually like the wonky ones better. Anyway, they have a cool name.&lt;/p&gt;
    &lt;p&gt;RDJ: That’s very nice. I absolutely love geometry, I did a track called “Dodeccaheedron,” a long time ago, one of my fave tracks. I was playing on this spirograph emulator recently. Ha, a 3D one would be really interesting.&lt;/p&gt;
    &lt;p&gt;TT: Oh man, of course you have a track named “Dodeccaheedron”! I wonder if the track had anything to do with the fact I like them now. Bet it did. Spirographs are so cool. Bit like Lissajous – could stare at that stuff all day. I really want to get hold of some XY lasers actually and fire some really intense ones. Wish there was a way to do that in 3D.&lt;/p&gt;
    &lt;p&gt;RDJ: I’ve been looking into this recently. :)&lt;/p&gt;
    &lt;p&gt;TT: Maybe you can design some phosphorescent smoke that you could fire lasers into and the lines would stay in the air. That will be so cool. And the smoke particles will move with the bass – get some fat bass bins and you would get lines of light vibrating.&lt;/p&gt;
    &lt;p&gt;RDJ: Top idea… Reminds me of this.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, really. I mean it could be a way of visualising the propagation of sound waves, so maybe a scientific use too. And not just sound waves. It could be used in wind tunnels to study air flow. Are we onto something here?&lt;/p&gt;
    &lt;p&gt;RDJ: Yes.&lt;/p&gt;
    &lt;p&gt;RDJ: What Is Your Dream?&lt;/p&gt;
    &lt;p&gt;TT: Having a good cigarette. When you're having a shit day or you're under a lot of stress, cigarettes taste crap. On the other hand, a cigarette after an amazing experience tastes good. So my dream is to smoke the best cigarette ever. Smoking is a full-stop, a moment of recognition that whatever came before it was real.&lt;/p&gt;
    &lt;p&gt;RDJ: I like that.&lt;/p&gt;
    &lt;p&gt;TT: Bit wanky tho. ;) Getting weird vibes reading back at my answers!&lt;/p&gt;
    &lt;p&gt;RDJ: LOLz&lt;/p&gt;
    &lt;p&gt;TT: Oh well, wrote it once, can't deny it.&lt;/p&gt;
    &lt;p&gt;RDJ: If you could magically create any device, what would it be? I understand if you’re not allowed to answer this!&lt;/p&gt;
    &lt;p&gt;TT: A time machine, teleportation machine – the obvious ones. Or actually a machine where you could have as many parallel existences as you want. So you could be a super-dimensional being encompassing all the different possibilities of yourself. That's what popped into my head, but how self-centred!&lt;/p&gt;
    &lt;p&gt;RDJ: I go to sleep thinking things like this… Maybe it's a bit like this already! :)&lt;/p&gt;
    &lt;p&gt;TT: Hell yeah. Anyway, that's probably not what you meant. So... a lifelogging device for your musical activities. I was packing up to leave Tokyo and found a bunch of minidiscs of music that I'd forgotten I'd made in my teens and I’m guessing there would have been a lot more if I knew where my cassettes were. I cringed at most of it, but it's still part of who I am and I can't erase whatever brain patterns I have because of that.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, bloody right, that would be very useful. One thing I’d say, though, is I’ve found a lot of artists write off their older work for various personal reasons, while other people won’t have those associations and just really love what you made.&lt;/p&gt;
    &lt;p&gt;TT: Do you have lost musical moments from the past that you would like to hear again?&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, I think I’m obsessed with thoughts like this. If you could selectively erase your memory so you could keep experiencing things for the first time, it would be very interesting, although you would get stuck in loops, so you would have to limit it to a certain number of re-experiences, ha! How many future products have you got in your head or on the drawing board?&lt;/p&gt;
    &lt;p&gt;TT: Quite a lot, but not all will be made. We (meaning the team still at Korg) have always got a bunch of ideas up our sleeves, it's just a case of which ones will get made and when.&lt;/p&gt;
    &lt;p&gt;RDJ: Is your job stressful? I imagine it’s very stressful. What's the most stressful part?&lt;/p&gt;
    &lt;p&gt;TT: Well, the stress was part of the balance, because there's a lot of adrenaline involved in meeting deadlines, starting production and working up to release. Now that I've left that position, I can look back in calm retrospect. I'd say it was quite physical. Kind of like a sport and also quite addictive. But at the same time you can't do it forever. I was also lucky enough to find new possibilities elsewhere, so I stopped before that high pace / full-throttle thing became the only thing I could do. I really did have an amazing time at Korg. I had the best team and I also had a lot of freedom. My decision to leave was really about me than anything to do with my working environment.&lt;/p&gt;
    &lt;p&gt;RDJ: What is your worst fear?&lt;/p&gt;
    &lt;p&gt;TT: Well, doing the same thing over again and then one day realising that's all you can do.&lt;/p&gt;
    &lt;p&gt;RDJ: Yeah, I think we all have to fight against this, especially as you get older. I’ve really been looking at my habits recently and denying them. It feels great if you can manage it. &lt;lb/&gt;I don't understand the economics of getting hardware to market, but I guess it's safe to assume that the company makes more money from releasing new products than it does upgrading old ones. &lt;/p&gt;
    &lt;p&gt;I can’t help thinking, though, that by continuing to upgrade older products that are still in production, to make them absolutely awesome, would benefit the company in the long-term. Any thoughts about this?&lt;/p&gt;
    &lt;p&gt;TT: That depends how you look at it. You can look at something like the monotribe which we spent a lot of time doing the major update for, which was then soon discontinued. So your initial point might look to hold true. But then you look at the amount we learnt from that update and that we put into the volcas, and then you can say it was worthwhile. I think it's really really important to look back and review past products. Some would benefit from an update, but others are better off redesigned.&lt;/p&gt;
    &lt;p&gt;RDJ: Ok then, well lovely chatting to you as always.. wishing you all the best in your new endeavours, very brave moving yourself to a new country, well done and speak soon.&lt;/p&gt;
    &lt;p&gt;Here’s a nice link to end with!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://web.archive.org/web/20180719052026/http://item.warp.net/interview/aphex-twin-speaks-to-tatsuya-takahashi/"/><published>2026-01-08T21:17:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46547740</id><title>Embassy: Modern embedded framework, using Rust and async</title><updated>2026-01-09T18:17:20.495832+00:00</updated><content>&lt;doc fingerprint="7b05f0bb442a83d4"&gt;
  &lt;main&gt;
    &lt;p&gt;Embassy is the next-generation framework for embedded applications. Write safe, correct, and energy-efficient embedded code faster, using the Rust programming language, its async facilities, and the Embassy libraries.&lt;/p&gt;
    &lt;p&gt;The Rust programming language is blazingly fast and memory-efficient, with no runtime, garbage collector, or OS. It catches a wide variety of bugs at compile time, thanks to its full memory- and thread-safety, and expressive type system.&lt;/p&gt;
    &lt;p&gt;Rust's async/await allows for unprecedentedly easy and efficient multitasking in embedded systems. Tasks get transformed at compile time into state machines that get run cooperatively. It requires no dynamic memory allocation and runs on a single stack, so no per-task stack size tuning is required. It obsoletes the need for a traditional RTOS with kernel context switching, and is faster and smaller than one!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware Abstraction Layers&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;HALs implement safe, idiomatic Rust APIs to use the hardware capabilities, so raw register manipulation is not needed. The Embassy project maintains HALs for select hardware, but you can still use HALs from other projects with Embassy.&lt;/item&gt;
          &lt;item&gt;embassy-stm32, for all STM32 microcontroller families.&lt;/item&gt;
          &lt;item&gt;embassy-nrf, for the Nordic Semiconductor nRF52, nRF53, nRF54 and nRF91 series.&lt;/item&gt;
          &lt;item&gt;embassy-rp, for the Raspberry Pi RP2040 and RP23xx microcontrollers.&lt;/item&gt;
          &lt;item&gt;embassy-mspm0, for the Texas Instruments MSPM0 microcontrollers.&lt;/item&gt;
          &lt;item&gt;esp-rs, for the Espressif Systems ESP32 series of chips. &lt;list rend="ul"&gt;&lt;item&gt;Embassy HAL support for Espressif chips, as well as Async Wi-Fi, Bluetooth, and ESP-NOW, is being developed in the esp-rs/esp-hal repository.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
          &lt;item&gt;ch32-hal, for the WCH 32-bit RISC-V(CH32V) series of chips.&lt;/item&gt;
          &lt;item&gt;mpfs-hal, for the Microchip PolarFire SoC.&lt;/item&gt;
          &lt;item&gt;py32-hal, for the Puya Semiconductor PY32 series of microcontrollers.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Time that Just Works - No more messing with hardware timers. embassy_time provides Instant, Duration, and Timer types that are globally available and never overflow.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Real-time ready - Tasks on the same async executor run cooperatively, but you can create multiple executors with different priorities so that higher priority tasks preempt lower priority ones. See the example.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Low-power ready - Easily build devices with years of battery life. The async executor automatically puts the core to sleep when there's no work to do. Tasks are woken by interrupts, there is no busy-loop polling while waiting.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Networking - The embassy-net network stack implements extensive networking functionality, including Ethernet, IP, TCP, UDP, ICMP, and DHCP. Async drastically simplifies managing timeouts and serving multiple connections concurrently.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bluetooth&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;The trouble crate provides a Bluetooth Low Energy 4.x and 5.x Host that runs on any microcontroller implementing the bt-hci traits (currently &lt;code&gt;nRF52&lt;/code&gt;,&lt;code&gt;nrf54&lt;/code&gt;,&lt;code&gt;rp2040&lt;/code&gt;,&lt;code&gt;rp23xx&lt;/code&gt;and&lt;code&gt;esp32&lt;/code&gt;and&lt;code&gt;serial&lt;/code&gt;controllers are supported).&lt;/item&gt;
          &lt;item&gt;The nrf-softdevice crate provides Bluetooth Low Energy 4.x and 5.x support for nRF52 microcontrollers.&lt;/item&gt;
          &lt;item&gt;The embassy-stm32-wpan crate provides Bluetooth Low Energy 5.x support for stm32wb microcontrollers.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;The trouble crate provides a Bluetooth Low Energy 4.x and 5.x Host that runs on any microcontroller implementing the bt-hci traits (currently &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LoRa - The lora-rs project provides an async LoRa and LoRaWAN stack that works well on Embassy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;USB - embassy-usb implements a device-side USB stack. Implementations for common classes such as USB serial (CDC ACM) and USB HID are available, and a rich builder API allows building your own.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bootloader and DFU - embassy-boot is a lightweight bootloader supporting firmware application upgrades in a power-fail-safe way, with trial boots and rollbacks.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;use defmt::info;
use embassy_executor::Spawner;
use embassy_time::{Duration, Timer};
use embassy_nrf::gpio::{AnyPin, Input, Level, Output, OutputDrive, Pin, Pull};
use embassy_nrf::{Peri, Peripherals};

// Declare async tasks
#[embassy_executor::task]
async fn blink(pin: Peri&amp;lt;'static, AnyPin&amp;gt;) {
    let mut led = Output::new(pin, Level::Low, OutputDrive::Standard);

    loop {
        // Timekeeping is globally available, no need to mess with hardware timers.
        led.set_high();
        Timer::after_millis(150).await;
        led.set_low();
        Timer::after_millis(150).await;
    }
}

// Main is itself an async task as well.
#[embassy_executor::main]
async fn main(spawner: Spawner) {
    let p = embassy_nrf::init(Default::default());

    // Spawned tasks run in the background, concurrently.
    spawner.spawn(blink(p.P0_13.into()).unwrap());

    let mut button = Input::new(p.P0_11, Pull::Up);
    loop {
        // Asynchronously wait for GPIO events, allowing other tasks
        // to run, or the core to sleep.
        button.wait_for_low().await;
        info!("Button pressed!");
        button.wait_for_high().await;
        info!("Button released!");
    }
}&lt;/code&gt;
    &lt;p&gt;Examples are found in the &lt;code&gt;examples/&lt;/code&gt; folder separated by the chip manufacturer they are designed to run on. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;examples/nrf52840&lt;/code&gt;run on the&lt;code&gt;nrf52840-dk&lt;/code&gt;board (PCA10056) but should be easily adaptable to other nRF52 chips and boards.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;examples/nrf5340&lt;/code&gt;run on the&lt;code&gt;nrf5340-dk&lt;/code&gt;board (PCA10095).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;examples/stm32xx&lt;/code&gt;for the various STM32 families.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;examples/rp&lt;/code&gt;are for the RP2040 chip.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;examples/std&lt;/code&gt;are designed to run locally on your PC.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install &lt;code&gt;probe-rs&lt;/code&gt;following the instructions at https://probe.rs.&lt;/item&gt;
      &lt;item&gt;Change directory to the sample's base directory. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd examples/nrf52840&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Ensure&lt;/p&gt;&lt;code&gt;Cargo.toml&lt;/code&gt;sets the right feature for the name of the chip you are programming. If this name is incorrect, the example may fail to run or immediately crash after being programmed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Ensure&lt;/p&gt;&lt;code&gt;.cargo/config.toml&lt;/code&gt;contains the name of the chip you are programming.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run the example&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;cargo run --release --bin blinky&lt;/code&gt;
    &lt;p&gt;For more help getting started, see Getting Started and Running the Examples.&lt;/p&gt;
    &lt;p&gt;The Rust Analyzer is used by Visual Studio Code and others. Given the multiple targets that Embassy serves, there is no Cargo workspace file. Instead, the Rust Analyzer must be told of the target project to work with. In the case of Visual Studio Code, please refer to the &lt;code&gt;.vscode/settings.json&lt;/code&gt; file's &lt;code&gt;rust-analyzer.linkedProjects&lt;/code&gt;setting.&lt;/p&gt;
    &lt;p&gt;Embassy is guaranteed to compile on stable Rust 1.75 and up. It might compile with older versions, but that may change in any new patch release.&lt;/p&gt;
    &lt;p&gt;EMBedded ASYnc! :)&lt;/p&gt;
    &lt;p&gt;Embassy is licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/embassy-rs/embassy"/><published>2026-01-08T23:00:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46549444</id><title>Show HN: Executable Markdown files with Unix pipes</title><updated>2026-01-09T18:17:19.972425+00:00</updated><content>&lt;doc fingerprint="b5628e4cb7ac3f69"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I wanted to run markdown files like shell scripts. So I built an open source tool that lets you use a shebang to pipe them through Claude Code with full stdin/stdout support.&lt;/p&gt;
      &lt;p&gt;task.md:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;    #!/usr/bin/env claude-run

    Analyze this codebase and summarize the architecture.
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Then:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;    chmod +x task.md

    ./task.md
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; These aren't just prompts. Claude Code has tool use, so a markdown file can run shell commands, write scripts, read files, make API calls. The prompt orchestrates everything.&lt;/p&gt;
      &lt;p&gt;A script that runs your tests and reports results (`run_tests.md`):&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;    #!/usr/bin/env claude-run --permission-mode bypassPermissions

    Run ./test/run_tests.sh and summarize what passed and failed.
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Because stdin/stdout work like any Unix program, you can chain them:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;    cat data.json | ./analyze.md &amp;gt; results.txt

    git log -10 | ./summarize.md

    ./generate.md | ./review.md &amp;gt; final.txt
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Or mix them with traditional shell scripts:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;    for f in logs/\*.txt; do

        cat "$f" | ./analyze.md &amp;gt;&amp;gt; summary.txt

    done
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; This replaced a lot of Python glue code for us. Tasks that needed LLM orchestration libraries are now markdown files composed with standard Unix tools. Composable as building blocks, runnable as cron jobs, etc.&lt;/p&gt;
      &lt;p&gt;One thing we didn't expect is that these are more auditable (and shareable) than shell scripts. Install scripts like `curl -fsSL https://bun.com/install | bash` could become:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;    `curl -fsSL https://bun.com/install.md | claude-run`
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Where install.md says something like "Detect my OS and architecture, download the right binary from GitHub releases, extract to ~/.local/bin, update my shell config." A normal human can actually read and verify that.&lt;/p&gt;
      &lt;p&gt;The (really cool) executable markdown idea and auditability examples are from Pete Koomen (@koomen on X). As Pete says: "Markdown feels increasingly important in a way I'm not sure most people have wrapped their heads around yet."&lt;/p&gt;
      &lt;p&gt;We implemented it and added Unix pipe semantics. Currently works with Claude Code - hoping to support other AI coding tools too. You can also route scripts through different cloud providers (AWS Bedrock, etc.) if you want separate billing for automated jobs.&lt;/p&gt;
      &lt;p&gt;GitHub: https://github.com/andisearch/claude-switcher&lt;/p&gt;
      &lt;p&gt;What workflows would you use this for?&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46549444"/><published>2026-01-09T02:29:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46550895</id><title>Mathematics for Computer Science (2018) [pdf]</title><updated>2026-01-09T18:17:19.267432+00:00</updated><content/><link href="https://courses.csail.mit.edu/6.042/spring18/mcs.pdf"/><published>2026-01-09T07:06:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46551044</id><title>What happened to WebAssembly</title><updated>2026-01-09T18:17:19.070236+00:00</updated><content>&lt;doc fingerprint="764b019a19bb2a90"&gt;
  &lt;main&gt;
    &lt;head data-astro-cid-6t6zfk7k=""&gt;Table Of Contents&lt;/head&gt;
    &lt;p&gt;On every WebAssembly discussion, there is inevitably one comment (often near the top) asking what happened.&lt;/p&gt;
    &lt;p&gt;It seems to have been advertised as a world-changing advancement. Was it just oversold? Was it another JVM applet scenario, doomed to fail?&lt;/p&gt;
    &lt;p&gt;I’d like to tackle this in a weirdly roundabout way because I think these sorts of questions make a few misplaced assumptions that are critical to clarify.&lt;/p&gt;
    &lt;head rend="h1"&gt;#In The Real World&lt;/head&gt;
    &lt;p&gt;Of course, WebAssembly does see real-world usage. Let’s list some examples!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Godot uses WebAssembly to build games for the web.&lt;/item&gt;
      &lt;item&gt;Squoosh.app uses WebAssembly to make use of image libraries.&lt;/item&gt;
      &lt;item&gt;Zellij uses WebAssembly for its plugin ecosystem.&lt;/item&gt;
      &lt;item&gt;Figma uses WebAssembly to convert their C++ code to something usable in a browser&lt;/item&gt;
      &lt;item&gt;Stackblitz uses WebAssembly for their web containers.&lt;/item&gt;
      &lt;item&gt;Ruffle uses WebAssembly to run a flash emulator in your browser&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For many of these, WebAssembly is critical to either their entire product or a major feature.&lt;/p&gt;
    &lt;p&gt;But I think this alone is not very convincing. We don’t yet see major websites entirely built with webassembly-based frameworks. We’re not building our applications directly to WebAssembly for maximum portability. But why not?&lt;/p&gt;
    &lt;p&gt;To answer this, we need a good mental model for what WebAssembly is. This will help us qualify where it is most impactful and the limitations we’re up against.&lt;/p&gt;
    &lt;head rend="h1"&gt;#What Is WebAssembly&lt;/head&gt;
    &lt;p&gt;In a word, WebAssembly is a language.&lt;/p&gt;
    &lt;head rend="h2"&gt;#A Note On Speed&lt;/head&gt;
    &lt;p&gt;This makes questions like “how fast is WebAssembly” a bit hard to answer. You don’t ask how fast algebraic notation is—it’s not a very sensible question.&lt;/p&gt;
    &lt;p&gt;Taken in the context of something like JavaScript, the language is only as fast as the engine running it. JavaScript the language has no speed, but you can benchmark JS engines like V8, SpiderMonkey, and JavaScriptCore. You can benchmark the IO libraries of JS runtimes like Bun, Deno, and Node.&lt;/p&gt;
    &lt;p&gt;What people actually mean is “how useful are the constructs of this language to efficient mappings of modern hardware” and “what is the current landscape of systems taking advantage of these constructs”.&lt;/p&gt;
    &lt;p&gt;Through clever-enough engineering, you can make any system sufficiently fast with some trade-offs. If compiling your code directly to C doesn’t bother you, getting “near native” speeds is possible in both JavaScript and WebAssembly.&lt;/p&gt;
    &lt;p&gt;That’s right, you can compile WebAssembly! You can also choose to interpret it directly—that’ll be up to your runtime, just like every other system.&lt;/p&gt;
    &lt;p&gt;So let’s ask the actual question of WebAssembly: how useful are the constructs of this language to efficient mappings of modern hardware? Turns out, pretty useful!&lt;/p&gt;
    &lt;head rend="h2"&gt;#An Efficient Mapping&lt;/head&gt;
    &lt;p&gt;WebAssembly is a pretty close approximation of an assembly language. Not too close, mind you. It’s higher level than that. But it’s close enough to cleanly compile to most assembly languages without a significant speed trade-off.&lt;/p&gt;
    &lt;p&gt;And yes, you can write WebAssembly by hand! I made a rustlings-esque course called watlings where you can hand-write WAT to solve some basic exercises.&lt;/p&gt;
    &lt;p&gt;WAT is a very close approximation to Wasm. It is almost 1:1 in that you can compile WAT to Wasm and then back to WAT with barely any loss in information (you may lose variable names and some metadata). It looks like this:&lt;/p&gt;
    &lt;p&gt;Try reading the code. It will feel both familiar and foreign.&lt;/p&gt;
    &lt;p&gt;We have functions and S-expressions. We have imports and exports. But we also have instructions like &lt;code&gt;i32.add&lt;/code&gt; and implicit stack returns.&lt;/p&gt;
    &lt;p&gt;Wasm is a bytecode perhaps best compared to JVMIS (i.e. JVM bytecode). They have similar goals and constraints, but different landscapes and guarantees.&lt;/p&gt;
    &lt;p&gt;Compared to JVM bytecode, Wasm has a significantly smaller API and stronger safety guarantees. It has fewer opinions on your memory management strategy and more limitations on what your program can do without permission from its host environment.&lt;/p&gt;
    &lt;p&gt;It can crunch numbers, but must be explicitly provided its memory and all imports. In this way, it is much different from an actual assembly language (or, a more widely used one).&lt;/p&gt;
    &lt;p&gt;We’ll wrap back around to this later.&lt;/p&gt;
    &lt;head rend="h1"&gt;#A compilation target&lt;/head&gt;
    &lt;p&gt;You can compile many languages to Wasm.&lt;/p&gt;
    &lt;p&gt;Notable among them are Rust, C, Zig, Go, Kotlin, Java, and C#. Commonly interpreted languages have even had their runtimes compiled to WebAssembly, such as Python, PHP, and Ruby. There are also many languages that solely compile to WebAssembly, such as AssemblyScript, Grain, and MoonBit.&lt;/p&gt;
    &lt;p&gt;For many of these, it is important not to require a garbage-collector. For others, it would be helpful to include one. Wasm allows for both (with the GC option being much more recent).&lt;/p&gt;
    &lt;p&gt;Your browser includes a Wasm “engine”, making this doubly an attractive compilation target. This means without much setup, your phone and laptop can run Wasm programs already.&lt;/p&gt;
    &lt;p&gt;Like how JVM can have many implementations of its runner, there are many implementations that run independently of your browser such as Wasmtime, WasmEdge, and Wasmer.&lt;/p&gt;
    &lt;p&gt;These languages can output a single artifact without being too specific to your computer’s hardware. You only need a Wasm runner to execute it (note more JVM analogies).&lt;/p&gt;
    &lt;head rend="h1"&gt;#Security and what it enables&lt;/head&gt;
    &lt;p&gt;Right now, Wasm is looking really similar to JVM. The main differences seem to be around memory management strategies and how many platforms support it.&lt;/p&gt;
    &lt;p&gt;The security story is what really starts to drive in the wedge.&lt;/p&gt;
    &lt;p&gt;WebAssembly maintains a minimal attack surface by treating all external interactions as explicit, host-defined imports. We went over this earlier. Its “deny-by-default” architecture, small instruction set, hidden control-flow stack (i.e. no raw pointers), and linear memory combine to create a very strong security story.&lt;/p&gt;
    &lt;p&gt;It is such that you can ensure process-like isolation within a single process. Cloudflare takes advantage of this aspect within V8 to run untrusted code very efficiently using V8 isolates. This means significant efficiency gains without significant security trade-offs.&lt;/p&gt;
    &lt;p&gt;Wasm programs can start 100x faster if you can avoid spinning up a separate process. Fermyon, a company in the Wasm hosting space, advertises sub-millisecond spinup times.&lt;/p&gt;
    &lt;p&gt;In these cases, the performance is a direct result of what the security guarantees enable.&lt;/p&gt;
    &lt;p&gt;In other cases, security can unlock feature support.&lt;/p&gt;
    &lt;p&gt;Flash is a multimedia platform that was primarily used for animations and games up until it was dropped from all major browsers in January of 2021 (primarily) due to security concerns. Ruffle has revived Flash experiences on sites like Newgrounds by acting as an interpreter and VM for ActionScript.&lt;/p&gt;
    &lt;p&gt;Cloudflare allows running Python code with similar security guarantees to its JS code by using Pyodide, which is a Wasm build of CPython.&lt;/p&gt;
    &lt;p&gt;Figma runs untrusted user plugins in your browser by running them in a QuickJS engine that is compiled to Wasm.&lt;/p&gt;
    &lt;p&gt;Elsewhere, the security allows for extreme embeddability.&lt;/p&gt;
    &lt;head rend="h1"&gt;#Portability and Embeddability&lt;/head&gt;
    &lt;p&gt;We’ve gone over the number of ways you can run Wasm programs. A Wasm runner can be pretty light. Instead of forcing library authors into a specific language (usually Lua or JavaScript), supporting Wasm itself opens the door to a much wider set of choices.&lt;/p&gt;
    &lt;p&gt;Tools like Zellij, Envoy, and Lapce support Wasm for their plugin ecosystem.&lt;/p&gt;
    &lt;p&gt;In environments where a JavaScript engine is already being used, this means access to programs you would not have been able to run otherwise.&lt;/p&gt;
    &lt;p&gt;This includes image processing, ocr, physics engines, rendering engines, media toolkits, databases, and parsers, among many others.&lt;/p&gt;
    &lt;p&gt;In a majority of these cases, the use of Wasm will be transparent to you. A library you installed will just be using it somewhere in its dependency tree.&lt;/p&gt;
    &lt;p&gt;Godot and Figma have codebases written in C++, but are often browser-ready by compiling to (or in combination with) WebAssembly.&lt;/p&gt;
    &lt;p&gt;It seems the most common use of Wasm is bridging the language gap. Certain ecosystems seem to have suites of tools more common to them. Squoosh would be a much more limited application if it could only choose image compression libraries from NPM.&lt;/p&gt;
    &lt;head rend="h1"&gt;#Speed and size revisited&lt;/head&gt;
    &lt;p&gt;Browsers run WebAssembly with roughly the same pipeline that runs JavaScript. This seemingly puts a hard limit on the performance of Wasm applications, but they will often be more or less performant due to their architecture or domain.&lt;/p&gt;
    &lt;p&gt;Using languages with richer type systems and more sophisticated optimizing compilers can produce more efficient programs. The JIT model of engines like V8 might prevent optimizations if the cost of optimizing exceeds the gains from running the optimized code. You might avoid megamorphic functions more easily by avoiding JavaScript.&lt;/p&gt;
    &lt;p&gt;However, there is a cost to crossing the host-program boundary, especially if cloning memory. Zaplib’s post-mortem is an interesting read here. Incrementally moving a codebase to Wasm can incur significant costs in boundary crossing, eliminating any benefit in the short term.&lt;/p&gt;
    &lt;p&gt;A small API surface also means binary bloat as system APIs are more often re-created than imported. There are standards like WASI which aim to help here. Still, there is no native string type (yet).&lt;/p&gt;
    &lt;p&gt;Zig seems to produce the smallest Wasm binaries among mainstream languages.&lt;/p&gt;
    &lt;p&gt;Practical performance of Wasm in native contexts (i.e. outside of a JS engine) seems to suffer for a variety of reasons. Threading and IO of any sort incurs some cost. Memory usage is larger. Cold start is slower.&lt;/p&gt;
    &lt;p&gt;Still, the performance trade-offs might not be significant enough to matter. For most uses, I’d wager it’s “fast enough”. If you’re in a performance-sensitive context, the benefits of Wasm are likely not as relevant.&lt;/p&gt;
    &lt;head rend="h1"&gt;#Language development and you&lt;/head&gt;
    &lt;p&gt;Clearly things are happening.&lt;/p&gt;
    &lt;p&gt;The Wasm IO YouTube channel has lots of talks worth watching.&lt;/p&gt;
    &lt;p&gt;In fact, standards and language development in Wasm has stirred significant controversy internally. There is a lot of desire for advancement, but standardization means decisions are hard to reverse. For many, things are moving too quickly and in the wrong direction.&lt;/p&gt;
    &lt;p&gt;There is the “more official” W3C working group and then the “less official” Bytecode Alliance which works much more quickly and is centered around tooling and language development outside of Wasm directly (e.g. on WIT and the WebAssembly Component Model).&lt;/p&gt;
    &lt;p&gt;Wasm feature proposals are being quickly advanced and adopted by a wide suite of tools. This is remarkable progress for standardization, but is also scary to watch if you fear large missteps.&lt;/p&gt;
    &lt;p&gt;So why do people think nothing has happened?&lt;/p&gt;
    &lt;p&gt;I figure most are under the impression that the advancement of this technology would have had a more visible impact on their work. That they would intentionally reach for and use Wasm tools.&lt;/p&gt;
    &lt;p&gt;Many seem to think there is a path to Wasm replacing JavaScript within the browser—that they might not need to include a &lt;code&gt;.js&lt;/code&gt; file at all. This is very unlikely.&lt;/p&gt;
    &lt;p&gt;However, you can use frameworks like Blazor and Leptos without being aware or involved in the produced JS artifacts.&lt;/p&gt;
    &lt;p&gt;Mostly, Wasm tools have been adopted and used by library authors, not application developers. The internals are opaque. This is fine, probably.&lt;/p&gt;
    &lt;p&gt;Separately, I think the community is not helped by the philosophy of purposely obfuscating teaching material around Wasm. This is a fight I lost a few times.&lt;/p&gt;
    &lt;p&gt;For now, maybe check out watlings. I’ll expand it at some point, surely.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://emnudge.dev/blog/what-happened-to-webassembly/"/><published>2026-01-09T07:38:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46553343</id><title>Kagi releases alpha version of Orion for Linux</title><updated>2026-01-09T18:17:18.947169+00:00</updated><content>&lt;doc fingerprint="8923d81071c60f5d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Orion for Linux Status â&lt;/head&gt;
    &lt;p&gt;The alpha stage is an early, unstable version meant primarily for testing.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is ready to test â&lt;/head&gt;
    &lt;p&gt;All visual components, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Main menus, submenus, dialogs, buttons, and toolbars.&lt;/item&gt;
      &lt;item&gt;Right-click menus and other visual controls.&lt;/item&gt;
      &lt;item&gt;Window layouts and basic controls.&lt;/item&gt;
      &lt;item&gt;Demonstrated basic website navigation functionality, supporting essentials like the homepage, tabs, and simple searches&lt;/item&gt;
      &lt;item&gt;Advanced tab management is now complete, with the exception of the Tab Switcher UI, which is not supported yet.&lt;/item&gt;
      &lt;item&gt;Tabs now function independently and can be opened in parallel&lt;/item&gt;
      &lt;item&gt;Session persistence is implemented: previously opened tabs, along with their history, will reopen when the application is launched again.&lt;/item&gt;
      &lt;item&gt;Tabs currently appear in the main window and are supported in the left sidebar as well.&lt;/item&gt;
      &lt;item&gt;Bookmarks system a simple bookmark feature is now available.&lt;/item&gt;
      &lt;item&gt;Users can save pages, organize them into folders&lt;/item&gt;
      &lt;item&gt;Users can view them in the bookmarks dialog, sidebar, and bookmarks bar.&lt;/item&gt;
      &lt;item&gt;Bookmarking via the â´ï¸ icon.&lt;/item&gt;
      &lt;item&gt;Intuitive folder assignment when saving a new bookmark.&lt;/item&gt;
      &lt;item&gt;Advanced history management provides handling of browsing history&lt;/item&gt;
      &lt;item&gt;Password management framework establishes the core infrastructure needed for secure password handling and future improvements in this area.&lt;/item&gt;
      &lt;item&gt;Local export/import (via file)&lt;/item&gt;
      &lt;item&gt;Managing passwords&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Future improvements (not implemented in Alpha): â&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WebKit Extension support&lt;/item&gt;
      &lt;item&gt;Sync infrastructure&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://help.kagi.com/orion/misc/linux-status.html"/><published>2026-01-09T12:54:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46554462</id><title>London–Calcutta Bus Service</title><updated>2026-01-09T18:17:18.782494+00:00</updated><content>&lt;doc fingerprint="d36cd5cbf64c31a6"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;London–Calcutta bus service&lt;/head&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Overview&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Operator&lt;/cell&gt;&lt;cell&gt;Albert Travel&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Status&lt;/cell&gt;&lt;cell&gt;defunct&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Began service&lt;/cell&gt;&lt;cell&gt;c. 1957&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Ended service&lt;/cell&gt;&lt;cell&gt;c. 1976&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Route&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Start&lt;/cell&gt;&lt;cell&gt;London, United Kingdom&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Via&lt;/cell&gt;&lt;cell&gt;Belgium, West Germany, Austria, Yugoslavia, Bulgaria, Turkey, Iran, Afghanistan, Pakistan&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;End&lt;/cell&gt;&lt;cell&gt;Calcutta, India&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Other routes&lt;/cell&gt;&lt;cell&gt;London-Calcutta-Sydney&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Service&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Journey time&lt;/cell&gt;&lt;cell&gt;50+ days&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The London to Calcutta bus service was a long-distance international bus route that operated between London, England, and Calcutta, India. Launched in 1957, it was widely regarded as the longest bus route in the world at the time.[1][2][3] The journey spanned approximately 10,000 miles (16,000 km) one way, and over 20,000 miles (32,700 km) round trip, taking about 50 days to complete each leg.&lt;/p&gt;&lt;p&gt;The route passed through several countries, including Belgium, Yugoslavia, and parts of Northwest India,[4] and became famously associated with the overland Hippie Trail of the 1960s and 1970s.&lt;/p&gt;&lt;p&gt;The service offered an all-inclusive experience covering travel, food, and accommodation.[3] In 1957, a one-way ticket cost £85 (equivalent to £2,589 in 2023), rising to £145 by 1973 (equivalent to £2,215 in 2023).&lt;/p&gt;&lt;p&gt;The service was discontinued in 1976 due to growing geopolitical instability in the Middle East, which made the route increasingly dangerous.[5]&lt;/p&gt;&lt;head rend="h2"&gt;Route&lt;/head&gt;[edit]&lt;p&gt;The bus service was operated by Albert Travel.[6] The maiden journey set out from London on April 15, 1957.[7] The first service arrived in Calcutta on June 5, 50 days later. During its journey the bus traveled from England to Belgium, and from there to India via West Germany, Austria, Yugoslavia, Bulgaria, Turkey, Iran, Afghanistan, Pakistan and North Western India. After entering India, it eventually reached Calcutta via New Delhi, Agra, Allahabad and Banaras.[5]&lt;/p&gt;&lt;head rend="h2"&gt;Facilities on the bus&lt;/head&gt;[edit]&lt;p&gt;The bus was equipped with reading facilities, separate sleeping bunks for all passengers, fan-operated heaters, and a kitchen. There was a forward observation lounge on the upper deck of the bus. Radio and a music system for parties was provided.[1] It had time to spend at tourist destinations in India, including Banaras and the Taj Mahal on the banks of the Yamuna. Shopping was also allowed in Salzburg, Vienna, Istanbul, Tehran and Kabul.[3][8]&lt;/p&gt;&lt;head rend="h2"&gt;Later history&lt;/head&gt;[edit]&lt;p&gt;After some years the bus had an accident and became unusable. Later[specify] the bus was purchased by Andy Stewart, a British traveler. He rebuilt it to be a mobile home with two levels. The double-decker was renamed to Albert and was traveled from Sydney to London via India on October 8, 1968. It took about 132 days for the bus to reach London. Albert Tours was a company based in England and Australia and it operated on London–Calcutta–London and London–Calcutta–Sydney routes.[9]&lt;/p&gt;&lt;p&gt;The bus reached India through Iran and then it traveled to Singapore through Burma, Thailand and Malaysia. From Singapore, the bus was transported to Perth in Australia by ship, and from there it traveled by road to Sydney.[10][11] The charge for this service from London to Calcutta was £145. The service had all the modern facilities as before. The bus service was discontinued in 1976 due to political conditions leading up to the Iranian Revolution and the escalation of tensions between Pakistan and India.[12] The Albert Tours completed about 15 trips between Kolkata to London and again from London to Sydney, before the service ended permanently.[13]&lt;/p&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ a b "This Was 'World's Longest Bus Route' From Kolkata To London". Curly Tales. 2020-07-06. Retrieved 2020-07-24.&lt;/item&gt;&lt;item&gt;^ "A Bus Ride From London to Kolkata in 1950s? Yes, The Viral Photo is Real". News18. Retrieved 2020-07-24.&lt;/item&gt;&lt;item&gt;^ a b c Civic Affairs. Vol. 4. P. C. Kapoor at the Citizen Press. 1957 – via books.google.com.&lt;/item&gt;&lt;item&gt;^ "London Calcutta Bus Trip 1957 london India Editorial Stock Photo - Stock Image | Shutterstock". Shutterstock Editorial. Retrieved 2020-07-24.&lt;/item&gt;&lt;item&gt;^ a b "Samayam". malayalam.samayam.com. 2 July 2020. Retrieved 13 June 2023.&lt;/item&gt;&lt;item&gt;^ "London to Calcutta by Road? Picture of 1950s Albert Travel Bus Service is Going Viral, Know Details About This Fascinating Historic Journey". Unique News Online. 2020-07-02. Retrieved 2021-02-19.&lt;/item&gt;&lt;item&gt;^ Whispers of Yesterday, Rare Historical Photos, Old Photos, retrieved 2023-11-29&lt;/item&gt;&lt;item&gt;^ admin (2020-07-04). "ലണ്ടൻ – കൽക്കട്ട ബസ് റൂട്ട്". News Kerala online. Archived from the original on 2020-07-06. Retrieved 2020-07-24.&lt;/item&gt;&lt;item&gt;^ Eat, Tech Travel (2020-07-03). "ലണ്ടനിൽ നിന്നും ഇന്ത്യയിലെ കൽക്കട്ടയിലേക്ക് ഒരു ബസ് സർവ്വീസ്". Technology &amp;amp; Travel Blog from India. Retrieved 2020-07-31.&lt;/item&gt;&lt;item&gt;^ "ലണ്ടനിൽ നിന്നു കൽക്കട്ടയിലെത്തിയ ഇന്ത്യാ മാന്..." ManoramaOnline (in Malayalam). Retrieved 2020-07-31.&lt;/item&gt;&lt;item&gt;^ INGALIS, LEONARD (1957-08-08). "London-Calcutta Bus is back in London - Owner drove passengers 20,300 Miles". The New York Times.&lt;/item&gt;&lt;item&gt;^ K, Noushad K. "ലണ്ടൻ - കൽക്കട്ട ബസ്". Archived from the original on 2020-07-06. Retrieved 2020-07-31.&lt;/item&gt;&lt;item&gt;^ "Kolkata, Then Calcutta, Once Had The World's Longest Bus Route All The Way Till London!". Whats Hot. Retrieved 2020-07-31.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/London%E2%80%93Calcutta_bus_service"/><published>2026-01-09T14:50:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46555302</id><title>Developers Are Solving the Wrong Problem</title><updated>2026-01-09T18:17:18.542547+00:00</updated><content>&lt;doc fingerprint="37a0d05d2f7520b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Everyone is either offended or excited about “vibe coding.” It’s all the rage and going to solve all your problems, or it’s the next great evil spewing crap code all over your systems. Those of us who love well structured clean code which is modular and concise seem to be a dying breed. For someone who’s early career was shaped by McConnell’s Code Complete, Brooks’ The Mythical Man Month, and Fowler’s Refactoring, this feels.. odd.&lt;/p&gt;
    &lt;p&gt;But when we dig into the WHY, something interesting happens:&lt;/p&gt;
    &lt;p&gt;Why do we want “well structured” code?&lt;/p&gt;
    &lt;p&gt;Well structured code is easier to understand to debug, extend, and maintain. But is there a single, shared definition of “well structured”?&lt;/p&gt;
    &lt;p&gt;Why do we want “clean” code?&lt;/p&gt;
    &lt;p&gt;Clean code is easier to understand to debug, extend, and maintain. But is there a single, shared definition of “clean”?&lt;/p&gt;
    &lt;p&gt;Why do we want “modular” code?&lt;/p&gt;
    &lt;p&gt;Modular code is easier to understand to debug, extend, and maintain. But is there a single, shared definition of “modular”? Actually, yes the Single Responsibility Principle addresses this one.&lt;/p&gt;
    &lt;p&gt;Why do we want “concise” code?&lt;/p&gt;
    &lt;p&gt;The less code there is, the easier it is to understand to debug, extend, and maintain. But too concise can make things harder to understand.&lt;/p&gt;
    &lt;p&gt;But when it gets down to it, all of these goals point at the problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s Not About You&lt;/head&gt;
    &lt;p&gt;Previously, we needed “well structured clean code which is modular and concise” because writing code was easy but reading code is hard. Really hard. Painfully hard. Making sense of someone else’s code is harder still. All of our practices are really just to decrease that pain. Anything we can do to make it easier for the next person – or ourselves six months from now – is worth it.&lt;/p&gt;
    &lt;p&gt;But what if the next “person” isn’t a person?&lt;/p&gt;
    &lt;p&gt;If we assume that the code will only be debugged, extended, and maintained by a computer, most of our reasoning for clean code goes out the window. We don’t care what a human can do with the resulting (output) code as long as they can consistently generate code and configuration to solve the business problem, which gets at the REAL problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s Not About Code&lt;/head&gt;
    &lt;p&gt;Somewhere along the line, we started treating the code as our goal. We worked hard to make sure it was structurally perfect on whatever framework we’re using today and didn’t realize that no one cares about our code. No one cares or even knows how “clean” our code is. They don’t even know what code is.&lt;/p&gt;
    &lt;p&gt;All our customers know is “does this solve my problem?” Enter vibe coding.&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s About the Problem&lt;/head&gt;
    &lt;p&gt;When a savvy user can describe their problem in detail, they can skip over all the messy coding steps and get directly to a solution. It won’t be perfect, and it won’t have “clean code” but they’ll see it in hours instead of months. More importantly, the savviest users can make mistakes, improve their understanding, experiment with ideas, and iterate on the entire process while getting better at each step along the way at a fraction of the cost.&lt;/p&gt;
    &lt;p&gt;And that’s why vibe coding is popular and only going to get more popular.&lt;/p&gt;
    &lt;p&gt;Instead of seeing vibe coding as a threat, we need to consider it another tool.&lt;/p&gt;
    &lt;p&gt;If you see it exclusively as a threat, I’m shocked you read this far. Thanks. I hope I can nudge your thinking.&lt;/p&gt;
    &lt;p&gt;If you see it as a tool, there are a few things we can do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Improving Vibe Coding&lt;/head&gt;
    &lt;p&gt;First, remember that any generative AI approach is only as good as the underlying model. If there’s a public model that suits your needs, use it. Though you can also give it context by adding code that fits your standards and expectations. To be clear, I don’t mean coding standards but patterns and practices which demonstrate good security and performant code.&lt;/p&gt;
    &lt;p&gt;Next, build out the rest of your tools. If you’re not writing the code directly, you need to be able to validate that the code works exactly the way you expect. At a minimum, that means testing business logic and validating interfaces, but you should include security and load or performance testing systems too.&lt;/p&gt;
    &lt;p&gt;Next, figure out how to describe your needs and capabilities effectively. We all know about requirements documents and Jiras but you’ll need to figure out how to translate that into actionable requests and steps for the generative AI. This will vary heavily on the system you’re using.&lt;/p&gt;
    &lt;p&gt;Finally, get used to throwing code away. Remember that your goal is solving the problem, and your code is merely the byproduct or the tool to get to that solution. The most important parts are the prompt and process used to generate the code, along with the understanding you gained and applied to get to that prompt. The more and faster you can learn, adapt, iterate, and ship, the better your solution will be.&lt;/p&gt;
    &lt;head rend="h2"&gt;But what happens After Vibe Coding?&lt;/head&gt;
    &lt;p&gt;It’s hard to predict things – especially the future – but I have a couple ideas.&lt;/p&gt;
    &lt;p&gt;First, more people are going to try more ideas faster. This is good. The people who were Excel wizards a generation ago are going to create “pretty good” things that solve their problems without involving a developer. My friends over at Dreambase are already doing that. We’ll have more automations and solutions than ever before.&lt;/p&gt;
    &lt;p&gt;Second, we may see the rise of “developer-less” companies. People will be able to imagine, describe, and ship a product with minimal developer input. Occasionally they may need an integration or similar and might contract with someone but maybe not even that. In my “Creating Better SDKs with Generative AI” course for LinkedIn, I realized that with well-defined interfaces, you don’t need much human interaction.&lt;/p&gt;
    &lt;p&gt;BUT – is this good or bad? Depends on who you are. If you job is to solve problems faster, this is a great time to be in the space. If your job is only to write beautiful code, you have a problem.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://caseysoftware.com/blog/developers-are-solving-the-wrong-problem"/><published>2026-01-09T16:05:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46555485</id><title>Cloudspecs: Cloud Hardware Evolution Through the Looking Glass</title><updated>2026-01-09T18:17:18.212712+00:00</updated><content>&lt;doc fingerprint="edc998f165b26ddb"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Cloudspecs: Cloud Hardware Evolution Through the Looking Glass&lt;/head&gt;
    &lt;p&gt;This paper (CIDR'26) presents a comprehensive analysis of cloud hardware trends from 2015 to 2025, focusing on AWS and comparing it with other clouds and on-premise hardware.&lt;/p&gt;
    &lt;p&gt;TL;DR: While network bandwidth per dollar improved by one order of magnitude (10x), CPU and DRAM gains (again in performance per dollar terms) have been much more modest. Most surprisingly, NVMe storage performance in the cloud has stagnated since 2016. Check out the NVMe SSD discussion below for data on this anomaly.&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU Trends&lt;/head&gt;
    &lt;p&gt;Multi-core parallelism has skyrocketed in the cloud. Maximum core counts have increased by an order of magnitude over the last decade. The largest AWS instance u7in now boasts 448 cores. However, simply adding cores hasn't translated linearly into value. To measure real evolution, the authors normalized benchmarks (SPECint, TPC-H, TPC-C) by instance cost. SPECint benchmarking shows that cost-performance improved roughly 3x over ten years. A huge chunk of that gain comes from AWS Graviton. Without Graviton, the gain drops to roughly 2x. For in-memory database benchmarks, gains were even lower (2x–2.5x), likely due to memory and cache latency bottlenecks.&lt;/p&gt;
    &lt;p&gt;On-prem hardware comparison shows that this stagnation is not cloud price gouging. Historically, Moore's Law and Dennard scaling doubled cost-performance every two years (which would have sum up to 32x gain over a decade). However, an analysis of on-premise AMD server CPUs reveals a similar slump, only a 1.7x gain from 2017 to 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Trends&lt;/head&gt;
    &lt;p&gt;DRAM capacity per dollar has effectively flatlined. The only significant improvement was the 2016 introduction of memory-optimized x instances, which offered ~3.3x more GiB-hours/$ than compute-optimized peers. While absolute single-socket bandwidth jumped ~5x (93 GiB/s to 492 GiB/s) as servers moved from DDR3 to DDR5, the cost-normalized gain is only 2x.&lt;/p&gt;
    &lt;p&gt;Historical data suggests commodity DRAM prices dropped 3x over the decade. But in the last three months, due to AI-driven demand, DDR5 prices rose sharply, further limiting effective memory gains.&lt;/p&gt;
    &lt;head rend="h2"&gt;Network Trends&lt;/head&gt;
    &lt;p&gt;We have good news here, finally. Network bandwidth per dollar exploded by 10x. And absolute speeds went from 10 Gbit/s to 600 Gbit/s (60x).&lt;/p&gt;
    &lt;p&gt;These gains were not universal though. Generic instances saw little change. The gains were driven by network-optimized n instances (starting with the c5n in 2018) powered by proprietary Nitro cards.&lt;/p&gt;
    &lt;head rend="h2"&gt;NVMe Trends&lt;/head&gt;
    &lt;p&gt;NVMe SSDs are the biggest surprise. Unlike CPUs and memory, where cloud trends mirror on-prem hardware, NVMe performance in AWS has largely stagnated. The first NVMe-backed instance family, i3, appeared in 2016. As of 2025, AWS offers 36 NVMe instance families. Yet the i3 still delivers the best I/O performance per dollar by nearly 2x.&lt;/p&gt;
    &lt;p&gt;SSD capacity has stagnated since 2019 and I/O throughput since 2016. This sharply contrasts with on-prem hardware, where SSD performance doubled twice (PCIe 4 and PCIe 5) in the same timeframe. The gap between cloud and on-premise NVMe is widening rapidly.&lt;/p&gt;
    &lt;p&gt;This price/performance gap likely explains the accelerating push toward disaggregated storage. When local NVMe is expensive and underperforming, remote storage starts to look attractive. The paper speculates that with network speeds exploding and NVMe stagnating, architectures may shift further. For systems like Snowflake, using local NVMe for caching might no longer be worth the complexity compared to reading directly from S3 with fast networks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;I think the main takeaway is that uniform hardware scaling in the cloud is over. Moore's Law no longer lifts all boats. Performance gains now come from specialization, especially networking (e.g., Graviton, Nitro, Accelerators).&lt;/p&gt;
    &lt;p&gt;In my HPTS 2024 review, I noted that contrary to the deafening AI hype, the real excitement in the hallways was about hardware/software codesign. This paper validates that sentiment. With general-purpose CPU and memory cost-performance stagnating, future databases must be tightly integrated with specialized hardware and software capabilities to provide value. I think the findings here will refuel that trend.&lt;/p&gt;
    &lt;p&gt;A key open question is why massive core counts deliver so little value. Where is the performance lost? Possible explanations include memory bandwidth limits, poor core-to-memory balance, or configuration mismatches. But I think the most likely culprit is software. Parallel programming remains hard, synchronization is expensive, and many systems fail to scale beyond a modest number of cores. We may be leaving significant performance on the table simply because our software cannot effectively utilize the massive parallelism now available.&lt;/p&gt;
    &lt;p&gt;The paper comes with an interactive tool, Cloudspecs, built on DuckDB-WASM (yay!). This allows you to run SQL queries over the dataset directly in the browser to visualize these trends. The figures in the PDF actually contain clickable link symbols that take you to the specific query used to generate that chart. Awesome reproducibility!&lt;/p&gt;
    &lt;p&gt;Aleksey and I did a live-reading of the paper. As usual, we had a lot to argue about. I'll add a recording of our discussion on YouTube when it becomes available, and here is a link to my annotated paper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://muratbuffalo.blogspot.com/2026/01/cloudspecs-cloud-hardware-evolution.html"/><published>2026-01-09T16:23:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46555512</id><title>Latest SteamOS Beta Now Includes Ntsync Kernel Driver</title><updated>2026-01-09T18:17:17.866421+00:00</updated><content>&lt;doc fingerprint="4fa7b87e288b3d14"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Latest SteamOS Beta Now Includes NTSYNC Kernel Driver&lt;/head&gt;
    &lt;p&gt; Valve released the SteamOS 3.7.20 beta overnight and with it they are finally building the NTSYNC kernel driver for helping accelerate Windows NT synchronization primitives. &lt;lb/&gt;The NTSYNC kernel driver has been in good shape for about one year now after the implementation was finished up. From user-space Wine 10.16 added NTSYNC usage support as part of the upcoming Wine 11.0 stable release due out this month. In turn Proton 11.0 will see that support when it is re-based atop Wine 11.0. Albeit Proton (Steam Play) already has FSYNC for good performance but will be interesting to see how the NTSYNC path performs for SteamOS / Steam Play needs in comparison.&lt;lb/&gt;For gearing up for that future Proton NTSYNC support, SteamOS 3.7.20 enables the NTSYNC kernel driver and loads the module by default. Most Linux distributions are at least already building the NTSYNC kernel module though there's been different efforts on how to handle ensuring it's loaded when needed. The presence of the NTSYC kernel driver is the main highlight of the SteamOS 3.7.20 beta now available for testing.&lt;/p&gt;
    &lt;p&gt;The NTSYNC kernel driver has been in good shape for about one year now after the implementation was finished up. From user-space Wine 10.16 added NTSYNC usage support as part of the upcoming Wine 11.0 stable release due out this month. In turn Proton 11.0 will see that support when it is re-based atop Wine 11.0. Albeit Proton (Steam Play) already has FSYNC for good performance but will be interesting to see how the NTSYNC path performs for SteamOS / Steam Play needs in comparison.&lt;/p&gt;
    &lt;p&gt;For gearing up for that future Proton NTSYNC support, SteamOS 3.7.20 enables the NTSYNC kernel driver and loads the module by default. Most Linux distributions are at least already building the NTSYNC kernel module though there's been different efforts on how to handle ensuring it's loaded when needed. The presence of the NTSYC kernel driver is the main highlight of the SteamOS 3.7.20 beta now available for testing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.phoronix.com/news/Steam-OS-Beta-NTSYNC"/><published>2026-01-09T16:27:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46555615</id><title>Why is SendGrid emailing me about supporting ICE?</title><updated>2026-01-09T18:17:17.702294+00:00</updated><content>&lt;doc fingerprint="27bbc89b0fb8bbf9"&gt;
  &lt;main&gt;
    &lt;p&gt;For the past several months, I’ve been receiving and then ignoring a steady stream of concerning emails from Sendgrid, the popular email delivery service owned by Twilio that I use for sending emails from Breadwinner. I’d see some weird API error notification, login to my SendGrid account, check everything is working properly, and then delete the email. I didn’t pay too close attention to them until I saw a couple very strange ones. &lt;lb/&gt;Today, I received this one implying SendGrid was going to be adding a “Support ICE” button to all emails sent through their platform:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;If you’ve been paying any attention at all to US politics, you’ll know how insidiuously provocative this would be if it were a real email.&lt;/p&gt;
    &lt;p&gt;But it isn’t. It’s a phishing email. If you use SendGrid, or have ever used it, you might be getting these too.&lt;/p&gt;
    &lt;p&gt;This phishing campaign is a fascinating example of how sophisticated social engineering has become. Instead of Nigerian 419 scams, hackers have evolved to carefully craft messages sent to professionals that are designed to exploit the American political consciousness.&lt;/p&gt;
    &lt;p&gt;The opt-out buttons are the trap.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Attack&lt;/head&gt;
    &lt;p&gt;Here’s how it works: hackers compromise SendGrid customer accounts (through credential stuffing, password reuse, the usual methods). Once they have access, they can send emails through SendGrid’s infrastructure, which means the emails pass all the standard authentication checks (SPF, DKIM) that your spam filter uses to determine legitimacy. The emails look real because, technically, they are real SendGrid emails sent via SendGrid’s platform and via a customer’s reputation – they’re just sent by the wrong people and wrong domains.&lt;/p&gt;
    &lt;p&gt;They’re likely using a list of SendGrid customers so they can target this to only people who have used the service before.&lt;/p&gt;
    &lt;p&gt;Security researchers at Netcraft dubbed this “Phishception” back in 2024: attackers using SendGrid to phish SendGrid users, creating a self-perpetuating cycle where each compromised account can be used to compromise more accounts.&lt;/p&gt;
    &lt;p&gt;This has been going on for years. Brian Krebs wrote about it in 2020. And yet here we are.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Lures&lt;/head&gt;
    &lt;p&gt;What’s changed, or at least what I’ve noticed recently, is the political sophistication of the bait. The attackers aren’t just sending “your account is suspended” emails (though they do that too). They’re sending messages designed to provoke a strong emotional reaction that compels you to click.&lt;/p&gt;
    &lt;p&gt;Here are some I’ve received:&lt;/p&gt;
    &lt;head rend="h2"&gt;The LGBT Pride Footer&lt;/head&gt;
    &lt;p&gt;From: [email protected]&lt;/p&gt;
    &lt;p&gt;This one claims SendGrid’s CEO “James Mitchell” (not a real person) came out as gay, and to show support, SendGrid is adding a pride-themed footer to all emails. “We understand this may not be right for everyone,” it helpfully notes, offering a “Manage Preferences” button.&lt;/p&gt;
    &lt;p&gt;Note the opt-out. If you support LGBTQ+ rights, you might ignore this. But if you don’t? You’re clicking that button immediately.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Black Lives Matter Theme&lt;/head&gt;
    &lt;p&gt;From: [email protected]&lt;/p&gt;
    &lt;p&gt;For “one week,” all emails will feature a commemorative theme honoring George Floyd and the Black Lives Matter movement. This change applies “platform-wide to all users.”&lt;/p&gt;
    &lt;p&gt;Again: “If you prefer not to participate, you can opt out below.”&lt;/p&gt;
    &lt;p&gt;Note the sender domain: nellions.co.ke, a Kenyan domain. This is a compromised SendGrid customer account being used to send phishing emails to American targets about American political issues.&lt;/p&gt;
    &lt;head rend="h2"&gt;The ICE Support Initiative&lt;/head&gt;
    &lt;p&gt;From: [email protected]&lt;/p&gt;
    &lt;p&gt;This one arrived just this morning. SendGrid is supposedly adding a “Support ICE” donation button to the footer of every email sent through their platform, “in response to recent events” and “as part of our commitment to supporting U.S. Immigration and Customs Enforcement.”&lt;/p&gt;
    &lt;p&gt;The timing here is notable: these hackers are reading the news.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Spanish Language Switch&lt;/head&gt;
    &lt;p&gt;From: [email protected]&lt;/p&gt;
    &lt;p&gt;And then there’s this one, which is just absurd: “Your language preference has been successfully changed to Spanish. All emails sent via the API will now be formatted in Spanish.”&lt;/p&gt;
    &lt;p&gt;This one is less politically charged and more “wait, what? I didn’t do that” – just enough anxiety to get you to click.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Classic Account Termination&lt;/head&gt;
    &lt;p&gt;From: [email protected]&lt;/p&gt;
    &lt;p&gt;And of course, they still do the classics: “Your account has been terminated for misusing sending guidelines.”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Pattern&lt;/head&gt;
    &lt;p&gt;Look closely at those sender addresses again at the top of the Gmail message:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;drummond.com&lt;/item&gt;
      &lt;item&gt;nellions.co.ke&lt;/item&gt;
      &lt;item&gt;theraoffice.com&lt;/item&gt;
      &lt;item&gt;nutritionsociety.org&lt;/item&gt;
      &lt;item&gt;myplace.co&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;None of these are sendgrid.com. They’re all legitimate businesses whose SendGrid accounts have been compromised. When these emails hit your inbox, they pass authentication because they really were sent through SendGrid, just not by SendGrid.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who’s Behind This?&lt;/head&gt;
    &lt;p&gt;The political sophistication on display here (BLM, LGBTQ+ rights, ICE, even the Spanish language switch playing on immigration anxieties) suggests someone with a deep understanding of American cultural fault lines.&lt;/p&gt;
    &lt;p&gt;We know that state actors have invested heavily in understanding and exploiting these divisions. Russian active measures campaigns have been documented doing exactly this kind of work: identifying wedge issues and creating content designed to inflame both sides. North Korea has demonstrated similar sophistication in their social engineering operations by targeting academics and foreign policy experts.&lt;/p&gt;
    &lt;p&gt;I’m not saying this is a state actor necessarily – the economic value of exploiting SendGrid’s formidable email infrastructure is most likely the appeal here. Similarly, this could just as easily be a domestic operation run by someone who’s extremely online and knows which culture war buttons to push. But I think the skill set required (technical ability to compromise accounts at scale plus cultural fluency in American politics) is notable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Can This Be Fixed?&lt;/head&gt;
    &lt;p&gt;Honestly? I don’t know.&lt;/p&gt;
    &lt;p&gt;SendGrid has known about this problem for years. Twilio (SendGrid’s parent company) has talked about requiring two-factor authentication for all customers, but implementation has been slow. The fundamental issue is that SendGrid’s business model depends on making it easy for legitimate businesses to send email at scale. Anything that adds friction for good actors also adds friction for bad actors, but the bad actors are more motivated to work around it.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the attackers only need one thing: access to SendGrid customer accounts. As long as people reuse passwords and don’t enable 2FA, there will be a steady supply of compromised accounts. It’s a bit of a hydra problem: cut off one head, another grows behind it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Protecting Yourself&lt;/head&gt;
    &lt;p&gt;If you’re a SendGrid customer: enable two-factor authentication immediately. Use a unique password. Check your account for unauthorized API keys or sender identities.&lt;/p&gt;
    &lt;p&gt;If you’re just receiving these emails: don’t click anything. The links go to fake SendGrid login pages that will steal your credentials in real-time as they actually validate your password against SendGrid’s API and even capture your 2FA codes.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Filter Hack&lt;/head&gt;
    &lt;p&gt;For Gmail users, you can create a filter to automatically delete SendGrid impersonation emails that don’t come from legitimate SendGrid domains:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to Settings → Filters and Blocked Addresses → Create new filter&lt;/item&gt;
      &lt;item&gt;In the “From” field, enter: &lt;code&gt;-from:sendgrid.com -from:twilio.com&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;In the “Has the words” field, enter: &lt;code&gt;sendgrid&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Click “Create filter” and select “Delete it”&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This will catch emails that mention SendGrid but aren’t actually from SendGrid. It’s not perfect, but it helps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Have You Gotten These?&lt;/head&gt;
    &lt;p&gt;I’m curious what other variations are out there. If you’ve received SendGrid phishing emails (especially weird or politically-charged ones) leave a comment or reach out. The more examples we document, the easier it is for people to recognize these when they land in their inbox.&lt;/p&gt;
    &lt;p&gt;And if you work at Twilio/SendGrid and want to explain what’s being done about this: I’m all ears.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fredbenenson.com/blog/2026/01/09/why-is-sendgrid-emailing-me-about-supporting-ice/"/><published>2026-01-09T16:36:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46555760</id><title>Cloudflare CEO on the Italy Fines</title><updated>2026-01-09T18:17:17.455193+00:00</updated><content>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/eastdakota/status/2009654937303896492"/><published>2026-01-09T16:46:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46555963</id><title>The Vietnam government has banned rooted phones from using any banking app</title><updated>2026-01-09T18:17:17.213012+00:00</updated><content>&lt;doc fingerprint="d77b9987e9f764cd"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt; Regulated in Circular 77/2025/TT-NHNN amending Circular 50 on online service security in the banking industry, to be in affect from March 1st:&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Thông tư số 77/2025/TT-NHNN của Ngân hàng Nhà nước Việt Nam: Sửa đổi, bổ sung một số điều của Thông tư số 50/2024/TT-NHNN của Thống đốc Ngân hàng Nhà nước Việt Nam quy định về an toàn, bảo mật cho việc cung cấp dịch vụ trực tuyến trong ngành Ngân hàng&lt;/p&gt;
            &lt;div&gt;
              &lt;p&gt; vanban.chinhphu.vn &lt;/p&gt;
            &lt;/div&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;p&gt; Clause 2, Article 5: Amend and supplement Clause 4 of Article 8 as follows: &lt;/p&gt;
      &lt;p&gt; 4. Implement solutions to prevent, combat, and detect unauthorized interference with the Mobile Banking application installed on customers' mobile devices. The Mobile Banking application must automatically exit or stop functioning and notify the customer of the reason if any of the following signs are detected: &lt;/p&gt;
      &lt;p&gt; a) A debugger is attached or the environment has a debugger running; or when the application is running in an emulator/virtual machine/emulator; or operating in a mode that allows the computer to communicate directly with the Android device (Android Debug Bridge); &lt;/p&gt;
      &lt;p&gt; b) The application software is injected with external code while running, performing actions such as monitoring executed functions, logging data transmitted through functions, APIs, etc. (hooks); or the application software is tampered with or repackaged. &lt;/p&gt;
      &lt;p&gt; c) The device has been rooted/jailbroken; or its bootloader has been unlocked." &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://xdaforums.com/t/discussion-the-root-and-mod-hiding-fingerprint-spoofing-keybox-stealing-cat-and-mouse-game.4425939/page-118"/><published>2026-01-09T17:00:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46555977</id><title>IcePanel (YC W23) is hiring full-stack engineers in Vancouver</title><updated>2026-01-09T18:17:16.264569+00:00</updated><content>&lt;doc fingerprint="80b89e5ad2c4a6e0"&gt;
  &lt;main&gt;
    &lt;p&gt;$170,000 salary (CAD) + Profit-share quarterly bonus (last year averaged ~30k-40k each)&lt;/p&gt;
    &lt;p&gt;+ 1% equity + Unlimited holiday + Health benefits&lt;/p&gt;
    &lt;p&gt;We’re looking for someone with a high degree of agency, who can immediately take ownership of building new functionality from design &amp;gt; implementation &amp;gt; maintaining and refining current features based on our customers' needs.&lt;/p&gt;
    &lt;p&gt;You’ll be building end-to-end, including: - Frontend UI/UX design alongside a designer. - Backend API/data structure design. - Data migration and infrastructure changes. - Bug fixing and iterations.&lt;/p&gt;
    &lt;p&gt;We're simplifying how teams design for complex systems. We're building a collaborative diagramming and modelling tool that software architects think is cool.&lt;/p&gt;
    &lt;p&gt;We’re a small, energetic team that believes in building a lean and profitable business after being in the YCombinator W23 batch. We’ve grown the product to ~$4 million CAD in ARR and believe in continuing to build on profitability over funding. We’re looking for talented, driven people who love their craft to help achieve our vision of simplifying complexity.&lt;/p&gt;
    &lt;p&gt;🙋 Independence to build our way&lt;/p&gt;
    &lt;p&gt;🛠️ Build simple and exceptional experiences&lt;/p&gt;
    &lt;p&gt;🧊 Transparency and openness&lt;/p&gt;
    &lt;p&gt;💡 Stay humble and explore all ideas&lt;/p&gt;
    &lt;p&gt;💩 No bullshit, have fun&lt;/p&gt;
    &lt;p&gt;- In-person days every week (Tuesday, Wednesday, Thursday)&lt;/p&gt;
    &lt;p&gt;- North Vancouver, British Columbia, Canada&lt;/p&gt;
    &lt;p&gt;- Hybrid &amp;amp; flexible work environment&lt;/p&gt;
    &lt;p&gt;- This is not a fully remote job&lt;/p&gt;
    &lt;p&gt;🍰 Equity in the company 💰 Profit sharing&lt;/p&gt;
    &lt;p&gt;💻 Work setup provided&lt;/p&gt;
    &lt;p&gt;🎉 Flexible work culture&lt;/p&gt;
    &lt;p&gt;🏂 Unlimited holiday&lt;/p&gt;
    &lt;p&gt;🧑⚕️ Health, dental, vision&lt;/p&gt;
    &lt;p&gt;📚 Learning budget&lt;/p&gt;
    &lt;p&gt;✈️ Conference budget&lt;/p&gt;
    &lt;p&gt;🌴 Annual team retreat&lt;/p&gt;
    &lt;p&gt;🌭 Hot dog Wednesdays&lt;/p&gt;
    &lt;p&gt;🧊 Free ice cubes&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://forms.icepanel.io/careers/senior-product-engineer"/><published>2026-01-09T17:01:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46556210</id><title>Show HN: I made a memory game to teach you to play piano by ear</title><updated>2026-01-09T18:17:16.111379+00:00</updated><link href="https://lend-me-your-ears.specr.net"/><published>2026-01-09T17:17:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46556822</id><title>Replit (YC W18) Is Hiring</title><updated>2026-01-09T18:17:15.793615+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jobs.ashbyhq.com/replit"/><published>2026-01-09T18:00:56+00:00</published></entry></feed>