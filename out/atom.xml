<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-29T20:51:09.216568+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46810337</id><title>Playing Board Games with Deep Convolutional Neural Network on 8bit Motorola 6809</title><updated>2026-01-29T20:51:17.866238+00:00</updated><content>&lt;doc fingerprint="31777073c4324395"&gt;
  &lt;main&gt;&lt;p&gt;WEKO3&lt;/p&gt;&lt;head rend="h3"&gt;アイテム&lt;/head&gt;&lt;head rend="h2"&gt; Playing Board Games with a Deep Convolutional Neural Network on the Motorola 6809 8-Bit Microprocessor &lt;/head&gt;&lt;p&gt;https://ipsj.ixsq.nii.ac.jp/records/229345&lt;/p&gt; https://ipsj.ixsq.nii.ac.jp/records/229345&lt;p&gt;9e1431e2-99f7-4d16-8f82-d7dcfa6a2045&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;名前 / ファイル&lt;/cell&gt;&lt;cell role="head"&gt;ライセンス&lt;/cell&gt;&lt;cell role="head"&gt;アクション&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt; IPSJ-GPWS2023012.pdf (275.3 kB) &lt;/cell&gt;&lt;cell&gt;&lt;p&gt; Copyright (c) 2023 by the Information Processing Society of Japan &lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;オープンアクセス&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="9"&gt;&lt;cell role="head"&gt;Item type&lt;/cell&gt;&lt;cell&gt;Symposium(1)&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;公開日&lt;/cell&gt;&lt;cell&gt;2023-11-10&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;タイトル&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;タイトル&lt;/cell&gt;&lt;cell&gt;Playing Board Games with a Deep Convolutional Neural Network on the Motorola 6809 8-Bit Microprocessor&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;タイトル&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;言語&lt;/cell&gt;&lt;cell&gt;en&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;タイトル&lt;/cell&gt;&lt;cell&gt;Playing Board Games with a Deep Convolutional Neural Network on the Motorola 6809 8-Bit Microprocessor&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;言語&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;言語&lt;/cell&gt;&lt;cell&gt;eng&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;キーワード&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;主題Scheme&lt;/cell&gt;&lt;cell&gt;Other&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;主題&lt;/cell&gt;&lt;cell&gt;deep learning&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;キーワード&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;主題Scheme&lt;/cell&gt;&lt;cell&gt;Other&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;主題&lt;/cell&gt;&lt;cell&gt;quantization&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;キーワード&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;主題Scheme&lt;/cell&gt;&lt;cell&gt;Other&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;主題&lt;/cell&gt;&lt;cell&gt;neural networks&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;資源タイプ&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;資源タイプ識別子&lt;/cell&gt;&lt;cell&gt;http://purl.org/coar/resource_type/c_5794&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;資源タイプ&lt;/cell&gt;&lt;cell&gt;conference paper&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;著者所属&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;Kayufu&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;著者所属(英)&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;en&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;Kayufu&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;著者名&lt;/cell&gt;&lt;cell&gt; Rémi, Coulom &lt;head&gt;× Rémi, Coulom&lt;/head&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;著者名(英)&lt;/cell&gt;&lt;cell&gt; Rémi, Coulom &lt;head&gt;× Rémi, Coulom&lt;/head&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;論文抄録&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;内容記述タイプ&lt;/cell&gt;&lt;cell&gt;Other&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;内容記述&lt;/cell&gt;&lt;cell&gt;While training deep-learning neural networks often requires considerable amounts of computing power, inference is efficient, and can be run on small devices. Cell phones are a typical example, but they are still rather powerful. The research presented in this paper takes the challenge to the extreme by running a Go-playing convolutional neural network on the 6809 CPU, an 8-bit microprocessor launched by Motorola in 1978. The software was implemented on a Thomson MO5 microcomputer, and reached a playing strength on par with GNU Go.&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;論文抄録(英)&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;内容記述タイプ&lt;/cell&gt;&lt;cell&gt;Other&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;内容記述&lt;/cell&gt;&lt;cell&gt;While training deep-learning neural networks often requires considerable amounts of computing power, inference is efficient, and can be run on small devices. Cell phones are a typical example, but they are still rather powerful. The research presented in this paper takes the challenge to the extreme by running a Go-playing convolutional neural network on the 6809 CPU, an 8-bit microprocessor launched by Motorola in 1978. The software was implemented on a Thomson MO5 microcomputer, and reached a playing strength on par with GNU Go.&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;書誌情報&lt;/cell&gt;&lt;cell&gt; ゲームプログラミングワークショップ2023論文集 &lt;p&gt;巻 2023, p. 66-69, 発行日 2023-11-10&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;出版者&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;言語&lt;/cell&gt;&lt;cell&gt;ja&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;出版者&lt;/cell&gt;&lt;cell&gt;情報処理学会&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ipsj.ixsq.nii.ac.jp/records/229345"/><published>2026-01-29T14:03:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46810401</id><title>Waymo robotaxi hits a child near an elementary school in Santa Monica</title><updated>2026-01-29T20:51:17.630986+00:00</updated><content>&lt;doc fingerprint="ffaa84538180b987"&gt;
  &lt;main&gt;
    &lt;p&gt;A Waymo robotaxi struck a child near an elementary school in Santa Monica on January 23, according to the company. Waymo told the National Highway Traffic Safety Administration (NHTSA) that the child — whose age and identity are not currently public — sustained minor injuries.&lt;/p&gt;
    &lt;p&gt;The NHTSA has opened an investigation into the accident, and Waymo said in a blog post that it “will cooperate fully with them throughout the process.” The National Transportation Safety Board said Thursday afternoon that it has also opened an investigation in coordination with the Santa Monica Police Department.&lt;/p&gt;
    &lt;p&gt;Waymo said its robotaxi struck the child at six miles per hour, after braking “hard” from around 17 miles per hour. The young pedestrian “suddenly entered the roadway from behind a tall SUV, moving directly into our vehicle’s path,” the company said in its blog post. Waymo said its vehicle “immediately detected the individual as soon as they began to emerge from behind the stopped vehicle.”&lt;/p&gt;
    &lt;p&gt;“Following contact, the pedestrian stood up immediately, walked to the sidewalk, and we called 911. The vehicle remained stopped, moved to the side of the road, and stayed there until law enforcement cleared the vehicle to leave the scene,” Waymo wrote in the post.&lt;/p&gt;
    &lt;p&gt;News of the crash comes as Waymo faces dual investigations into its robotaxis illegally passing school buses. The NHTSA opened a probe into the problem in October shortly after the first report of the incident in Atlanta, Georgia, and the NTSB opened its own investigation last week after around 20 incidents were reported in Austin, Texas.&lt;/p&gt;
    &lt;p&gt;According to the NHTSA, the accident occurred “within two blocks” of the elementary school “during normal school drop off hours.” The safety regulator said “there were other children, a crossing guard, and several double-parked vehicles in the vicinity.”&lt;/p&gt;
    &lt;p&gt;The NHTSA’s Office of Defects Investigation is investigating “whether the Waymo AV exercised appropriate caution given, among other things, its proximity to the elementary school during drop off hours, and the presence of young pedestrians and other potential vulnerable road users.”&lt;/p&gt;
    &lt;head rend="h3"&gt;TechCrunch Founder Summit 2026: Tickets Live&lt;/head&gt;
    &lt;head rend="h4"&gt;On June 23 in Boston, more than 1,100 founders come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately&lt;lb/&gt;Save up to $300 on your pass or save up to 30% with group tickets for teams of four or more.&lt;/head&gt;
    &lt;head rend="h3"&gt;TechCrunch Founder Summit: Tickets Live&lt;/head&gt;
    &lt;head rend="h4"&gt;On June 23 in Boston, more than 1,100 founders come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately&lt;lb/&gt;Save up to $300 on your pass or save up to 30% with group tickets for teams of four or more.&lt;/head&gt;
    &lt;p&gt;Waymo said in its blog post that its “peer-reviewed model” shows a “fully attentive human driver in this same situation would have made contact with the pedestrian at approximately 14 mph.” The company did not release a specific analysis of this crash.&lt;/p&gt;
    &lt;p&gt;This story has been updated to include information about the National Transportation Safety Board’s investigation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://techcrunch.com/2026/01/29/waymo-robotaxi-hits-a-child-near-an-elementary-school-in-santa-monica/"/><published>2026-01-29T14:08:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46810536</id><title>Break Me If You Can: Exploiting PKO and Relay Attacks in 3DES/AES NFC</title><updated>2026-01-29T20:51:17.161527+00:00</updated><content>&lt;doc fingerprint="ecf8d84918a1ebd8"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;This paper presents an in-depth analysis of vulnerabilities in MIFARE Ultralight C, MIFARE Ultralight AES, NTAG 223 DNA, NTAG 224 DNA, and widely circulated non-NXP Ultralight C compatible cards. We reveal multiple avenues to substantially weaken the security of each technology across a range of configurations.&lt;/p&gt;
    &lt;p&gt;Through relay-based man-in-the-middle techniques and partial key overwritesâoptionally combined with EEPROM tearing techniquesâan attacker can reduce the keyspace of two-key Triple DES (2TDEA) from 2112 to 228 or less in certain real-world deployments, making brute-force key recovery feasible with modest computational resources.&lt;/p&gt;
    &lt;p&gt;We further demonstrate how MIFARE Ultralight AES can be similarly affected when CMAC integrity checks are not enforced. The security of NTAG 223/224 DNA is undermined by the absence of integrity checks and the calculation of CMAC over Secure Unique NFC (SUN) messages, providing an unauthenticated ciphertext oracle that facilitates key recovery from a single tag.&lt;/p&gt;
    &lt;head rend="h2"&gt;Affected Products&lt;/head&gt;
    &lt;p&gt;The following NXP products and non-NXP compatible ICs are affected:&lt;/p&gt;
    &lt;head rend="h2"&gt;Impact Assessment&lt;/head&gt;
    &lt;p&gt;For genuine NXP Ultralight C: Using partial key overwrite across multiple tags sharing a static key, full 112-bit 2TDEA key recovery is achievable in days to weeks depending on available hardware and number of tags.&lt;/p&gt;
    &lt;p&gt;For non-NXP cards (ULCG, FJ8010, USCUID-UL): Flawed PRNGs and missing anti-tearing mechanisms enable complete key recovery from a single card in under 60 secondsâeven on a mobile phone.&lt;/p&gt;
    &lt;p&gt;For NTAG 223/224 DNA: The SUNCMAC_KEY can be recovered from a single tag in under a minute via partial key overwrite combined with offline CMAC brute-force against collected SUN messages.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key Contributions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Partial Key Overwrite Attack: Enables attackers with authenticated access to reduce key-recovery brute-force workload against 2TDEA and AES-128 keys given multiple source tags using the same key.&lt;/item&gt;
      &lt;item&gt;Theoretical Single-Tag Recovery: Method to recover the full 112-bit 2TDEA key from a single NXP Ultralight C tag in specific configurations, applicable even with diversified keys.&lt;/item&gt;
      &lt;item&gt;NTAG 22x DNA Attack: Partial key overwrite and tearing techniques applied to AES-128 protected NTAG 22x DNA, enabling significantly faster offline CMAC brute-force for recovering the SUN message authentication key.&lt;/item&gt;
      &lt;item&gt;Non-NXP Card Analysis: First systematic analysis of widespread non-NXP Ultralight C compatible cards, demonstrating implementation flaws (predictable PRNGs, absent anti-tearing) allowing near-instantaneous key recovery.&lt;/item&gt;
      &lt;item&gt;Real-World Deployment Survey: Empirical data from hospitality and other deployments showing configuration lapses around key diversification, lock bytes, and integrity mechanisms.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Questions &amp;amp; Answers&lt;/head&gt;
    &lt;p&gt;This research demonstrates vulnerabilities in widely deployed NFC technologies used for access control, ticketing, and hospitality. We show that cryptographic keys protecting MIFARE Ultralight C, MIFARE Ultralight AES, and NTAG 223/224 DNA tags can be recovered through a combination of relay attacks, partial key overwrites, and EEPROM manipulation techniques.&lt;/p&gt;
    &lt;p&gt;The core issue is that these tags lack adequate post-authentication integrity protection, and many deployments fail to properly configure available security features like lock bytes and key diversification.&lt;/p&gt;
    &lt;p&gt;No. The underlying cryptographic algorithms (3DES and AES-128) remain secure. The vulnerabilities arise from:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Protocol design choices that allow unauthenticated memory writes after initial authentication&lt;/item&gt;
      &lt;item&gt;Lack of atomicity when writing cryptographic keys across multiple memory pages&lt;/item&gt;
      &lt;item&gt;Widespread misconfiguration in real-world deployments (unlocked memory, static keys)&lt;/item&gt;
      &lt;item&gt;Non-NXP compatible chips with severely flawed random number generators&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yes. We initiated contact with NXP Semiconductors in July 2025 and provided them with a complete draft of our findings. NXP confirmed the findings in August 2025 and requested additional time for product recertification. We coordinated the publication timeline with NXP, balancing their recertification needs with the importance of informing the security community and affected parties.&lt;/p&gt;
    &lt;p&gt;The title comes directly from the default factory key programmed into every MIFARE Ultralight C chip by NXP. When you read the 16-byte 3DES key from a fresh Ultralight C tag, it spells out the ASCII string &lt;code&gt;BREAKMEIFYOUCAN!&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This appears to be a deliberately playful challenge left by NXP's engineers in 2008 for the security research community, and embedded directly in the Ultralight C's memory. We chose the paper's title to reflect this challenge, having demonstrated multiple practical attacks.&lt;/p&gt;
    &lt;p&gt;Of course, in any properly configured deployment, this default key should be replaced with a unique, securely generated key before deployment.&lt;/p&gt;
    &lt;p&gt;The risk to individual hotel guests is generally low for opportunistic attacks. These attacks require specialized equipment and technical knowledge. However, our research did demonstrate a successful credential forgery attack against a real hospitality system using a discarded room key.&lt;/p&gt;
    &lt;p&gt;Practical advice:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Don't leave your key card unattended for extended periods&lt;/item&gt;
      &lt;item&gt;Don't discard key cards in publicly accessible areas near the hotel&lt;/item&gt;
      &lt;item&gt;Report lost cards immediately so they can be deactivated&lt;/item&gt;
      &lt;item&gt;Use in-room safes and additional door locks when available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Likely yes, to some degree. Our survey found that 100% of MIFARE Ultralight C systems examined were affected by one or more issues. The severity depends on your configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;High risk: Static keys shared across all cards, unlocked key memory pages, non-NXP compatible cards in your supply chain&lt;/item&gt;
      &lt;item&gt;Medium risk: Key diversification implemented but lock bytes not configured, CMAC not enabled on Ultralight AES&lt;/item&gt;
      &lt;item&gt;Lower risk: Proper key diversification, locked memory pages, verified genuine NXP chips, CMAC enabled (for Ultralight AES)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We strongly recommend auditing your deployment against the mitigations listed below.&lt;/p&gt;
    &lt;p&gt;Some transit systems use MIFARE Ultralight C for limited-use tickets. If your system uses static keys and doesn't lock key memory pages, it could be vulnerable to the multi-tag key recovery attack. However, transit systems often have backend validation that may limit the practical impact.&lt;/p&gt;
    &lt;p&gt;Systems using MIFARE DESFire or other more advanced technologies are not affected by this specific research.&lt;/p&gt;
    &lt;p&gt;Our sampling found that approximately 34% of cards from hospitality deployments were not genuine NXP products. Non-NXP compatible cards (like Giantec GT23SC4489 "ULCG", Feiju FJ8010, or USCUID-UL) have severely flawed random number generators that allow key recovery from a single card in under 60 seconds.&lt;/p&gt;
    &lt;p&gt;How to check:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use &lt;code&gt;hf mfu info&lt;/code&gt;from the latest Proxmark3 firmware, which integrates fingerprinting for all counterfeit cards mentioned in the paper&lt;/item&gt;
      &lt;item&gt;ULCG cards can be quickly identified by their UID suffix ending in &lt;code&gt;1589&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;See details in our paper and other fingerprinting tools available in our GitHub repository for deeper inspection&lt;/item&gt;
      &lt;item&gt;Audit your supply chain and verify card sources&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIFARE DESFire: Not affected by this research. DESFire provides end-to-end encrypted sessions with integrity protection and is recommended as an upgrade path.&lt;/p&gt;
    &lt;p&gt;MIFARE Classic and MIFARE Plus: Not in scope for this paper, but MIFARE Classic has its own well-documented vulnerabilities (Crypto1 weaknesses) which are mitigated in MIFARE Plus, provided it is properly configured.&lt;/p&gt;
    &lt;p&gt;ICODE DNA, UCODE DNA, NTAG 424 DNA: We examined these briefly. NTAG 424 DNA appears to require authentication before key modification, which mitigates the partial key overwrite attack. See the full paper for details.&lt;/p&gt;
    &lt;p&gt;1. Audit your current deployment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check if lock bytes are configured on key pages (Lock byte 3, bit 7 for Ultralight C)&lt;/item&gt;
      &lt;item&gt;Verify whether you're using static or diversified keys&lt;/item&gt;
      &lt;item&gt;Test sample cards for non-NXP compatible chips&lt;/item&gt;
      &lt;item&gt;For Ultralight AES: check if &lt;code&gt;SEC_MSG_ACT&lt;/code&gt;is enabled&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2. For new card issuance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Implement key diversification using UID and a site-specific salt&lt;/item&gt;
      &lt;item&gt;Permanently lock key memory pages after personalization&lt;/item&gt;
      &lt;item&gt;Enable authentication attempt limits where available&lt;/item&gt;
      &lt;item&gt;Verify genuine NXP chips before deployment&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;3. For existing deployments with static keys:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If key pages can still be locked, do so immediately&lt;/item&gt;
      &lt;item&gt;Plan migration to diversified keys or more secure technology&lt;/item&gt;
      &lt;item&gt;Implement additional backend validation where possible&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Partially. If the key memory pages and configuration bytes are not yet locked, you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lock the key pages to prevent the partial key overwrite attack&lt;/item&gt;
      &lt;item&gt;Enable &lt;code&gt;AUTH_LIM&lt;/code&gt;on Ultralight AES to limit brute-force attempts&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However: If you're using static keys across your deployment, locking the pages only prevents new attacksâit doesn't change the fact that an attacker with enough cards could still recover the key through other means. Migration to diversified keys is the proper long-term solution.&lt;/p&gt;
    &lt;p&gt;For non-NXP compatible cards: These cannot be meaningfully secured and should be replaced with genuine NXP products.&lt;/p&gt;
    &lt;p&gt;For applications requiring higher security assurance, we recommend migrating to MIFARE DESFire EV3 or similar advanced contactless smart card technologies. These provide:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;End-to-end encrypted communication sessions&lt;/item&gt;
      &lt;item&gt;Mandatory integrity protection (not optional)&lt;/item&gt;
      &lt;item&gt;Hardware-level countermeasures against tearing and manipulation&lt;/item&gt;
      &lt;item&gt;Secure key storage with proper atomicity guarantees&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While Ultralight AES is an improvement over Ultralight C, its security still depends heavily on proper configuration, whereas DESFire provides stronger security by default.&lt;/p&gt;
    &lt;p&gt;MIFARE Ultralight C stores its 112-bit 2TDEA key across four 4-byte memory pages (pages 44-47). The tag's firmware doesn't enforce atomic writes across all four pagesâeach page can be written independently.&lt;/p&gt;
    &lt;p&gt;After gaining authenticated access via a relay attack, an attacker can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Overwrite three of the four key pages with known values (e.g., zeros)&lt;/item&gt;
      &lt;item&gt;Brute-force the remaining 28 bits (~228 possibilities)&lt;/item&gt;
      &lt;item&gt;Repeat on different tags, targeting different key quarters each time&lt;/item&gt;
      &lt;item&gt;Combine the four recovered segments to reconstruct the full key&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This reduces the attack complexity from 2112 to roughly 4 Ã 228, making it feasible with modest hardware.&lt;/p&gt;
    &lt;p&gt;The mutual authentication in Ultralight C does work correctlyâboth the reader and tag prove they know the shared secret. The problem is what happens after authentication.&lt;/p&gt;
    &lt;p&gt;Unlike MIFARE DESFire, Ultralight C has no post-authentication integrity protection. All commands after authentication are sent in plaintext without any MAC or encryption. An attacker who relays the authentication exchange can then inject their own commands to the now-authenticated tag.&lt;/p&gt;
    &lt;p&gt;Crucially, the tag has no timing constraintsâit will wait indefinitely for the reader's response during authentication, eliminating the tight timing windows that normally make relay attacks difficult.&lt;/p&gt;
    &lt;p&gt;The attacks can be performed with relatively inexpensive, commercially available hardware:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Relay attack: Two Flipper Zero devices communicating over 433 MHz, or Proxmark3 devices&lt;/item&gt;
      &lt;item&gt;Online brute-force: Proxmark3 or Flipper Zero (~100 auth attempts/second)&lt;/item&gt;
      &lt;item&gt;Offline computation: Standard laptop (for non-NXP cards) or GPU cluster (for 2-tag variant on genuine cards)&lt;/item&gt;
      &lt;item&gt;Tearing attacks: Proxmark3 with precise timing, or even Flipper Zero with busy-loop timing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total equipment cost is well under $500, and the techniques are within reach of moderately skilled attackers.&lt;/p&gt;
    &lt;p&gt;All tools and scripts are available on GitHub at github.com/zc-public/breakme-resources. Relevant code has also been contributed to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Proxmark3 firmware repository&lt;/item&gt;
      &lt;item&gt;Flipper Zero firmware (version 1.4.0+)&lt;/item&gt;
      &lt;item&gt;ChameleonUltra firmware&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The tools include key recovery scripts, fingerprinting utilities, and optimized brute-force implementations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mitigations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable CMAC integrity verification on all AES-protected communications (SEC_MSG_ACT for Ultralight AES)&lt;/item&gt;
      &lt;item&gt;Implement key diversification using card UID and a site-specific key or salt as input to a secure KDF&lt;/item&gt;
      &lt;item&gt;Lock critical memory pages including key pages, AUTH0, and configuration bytes to prevent modification&lt;/item&gt;
      &lt;item&gt;Enable authentication attempt counters (AUTH_LIM) where available and lock the configuration&lt;/item&gt;
      &lt;item&gt;Verify supply chain integrity and replace non-NXP compatible cards with genuine NXP products where security is required&lt;/item&gt;
      &lt;item&gt;Store CMAC over critical data including UID and hardware counters for technologies without secure messaging&lt;/item&gt;
      &lt;item&gt;Consider migration to MIFARE DESFire EV3 or similar technologies with end-to-end encryption and hardware-level countermeasures&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Disclosure Timeline&lt;/head&gt;
    &lt;head rend="h2"&gt;Resources&lt;/head&gt;
    &lt;p&gt;The full technical paper is available on ePrint. Proof-of-concept tools and supporting materials are published on GitHub, with relevant code contributed to the Proxmark3, Flipper Zero, and ChameleonUltra firmware repositories.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.breakmeifyoucan.com/"/><published>2026-01-29T14:20:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46810828</id><title>Run Clawdbot/Moltbot on Cloudflare with Moltworker</title><updated>2026-01-29T20:51:16.776643+00:00</updated><content>&lt;doc fingerprint="bd02981209b71cce"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The Internet woke up this week to a flood of people buying Mac minis to run Moltbot (formerly Clawdbot), an open-source, self-hosted AI agent designed to act as a personal assistant. Moltbot runs in the background on a user's own hardware, has a sizable and growing list of integrations for chat applications, AI models, and other popular tools, and can be controlled remotely. Moltbot can help you with your finances, social media, organize your day â all through your favorite messaging app.&lt;/p&gt;
      &lt;p&gt;But what if you donât want to buy new dedicated hardware? And what if you could still run your Moltbot efficiently and securely online? Meet Moltworker, a middleware Worker and adapted scripts that allows running Moltbot on Cloudflare's Sandbox SDK and our Developer Platform APIs.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;A personal assistant on Cloudflare â how does that work?Â &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Firstly, Cloudflare Workers has never been so compatible with Node.js. Where in the past weÂ had to mock APIs to get some packages running, now those APIs are supported natively by the Workers Runtime.&lt;/p&gt;
      &lt;p&gt;This has changed how we can build tools on Cloudflare Workers. When we first implemented Playwright, a popular framework for web testing and automation that runs on Browser Rendering, we had to rely on memfs. This was bad because not only is memfs a hack and an external dependency, but it also forced us to drift away from the official Playwright codebase. Thankfully, with more Node.js compatibility, we were able to start using node:fs natively, reducing complexity and maintainability, which makes upgrades to the latest versions of Playwright easy to do.&lt;/p&gt;
      &lt;p&gt;The list of Node.js APIs we support natively keeps growing. The blog post âA year of improving Node.js compatibility in Cloudflare Workersâ provides an overview of where we are and what weâre doing.&lt;/p&gt;
      &lt;p&gt;We measure this progress, too. We recently ran an experiment where we took the 1,000 most popular NPM packages, installed and let AI loose, to try to run them in Cloudflare Workers, Ralph Wiggum as a "software engineer" style, and the results were surprisingly good. Excluding the packages that are build tools, CLI tools or browser-only and donât apply, only 15 packages genuinely didnât work. That's 1.5%.&lt;/p&gt;
      &lt;p&gt;Hereâs a graphic of our Node.js API support over time:&lt;/p&gt;
      &lt;p&gt;We put together a page with the results of our internal experiment on npm packages support here, so you can check for yourself.&lt;/p&gt;
      &lt;p&gt;Moltbot doesnât necessarily require a lot of Workers Node.js compatibility because most of the code runs in a container anyway, but we thought it would be important to highlight how far we got supporting so many packages using native APIs. This is because when starting a new AI agent application from scratch, we can actually run a lot of the logic in Workers, closer to the user.&lt;/p&gt;
      &lt;p&gt;The other important part of the story is that the list of products and APIs on our Developer Platform has grown to the point where anyone can build and run any kind of application â even the most complex and demanding ones â on Cloudflare. And once launched, every application running on our Developer Platform immediately benefits from our secure and scalable global network.&lt;/p&gt;
      &lt;p&gt;Those products and services gave us the ingredients we needed to get started. First, we now have Sandboxes, where you can run untrusted code securely in isolated environments, providing a place to run the service. Next, we now have Browser Rendering, where you can programmatically control and interact with headless browser instances. And finally, R2, where you can store objects persistently. With those building blocks available, we could begin work on adapting Moltbot.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;How we adapted Moltbot to run on us&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Moltbot on Workers, or Moltworker, is a combination of an entrypoint Worker that acts as an API router and a proxy between our APIs and the isolated environment, both protected by Cloudflare Access. It also provides an administration UI and connects to the Sandbox container where the standard Moltbot Gateway runtime and its integrations are running, using R2 for persistent storage.&lt;/p&gt;
      &lt;p&gt;High-level architecture diagram of Moltworker.&lt;/p&gt;
      &lt;p&gt;Let's dive in more.&lt;/p&gt;
      &lt;p&gt;Cloudflare AI Gateway acts as a proxy between your AI applications and any popular AI provider, and gives our customers centralized visibility and control over the requests going through.&lt;/p&gt;
      &lt;p&gt;Recently we announced support for Bring Your Own Key (BYOK), where instead of passing your provider secrets in plain text with every request, we centrally manage the secrets for you and can use them with your gateway configuration.&lt;/p&gt;
      &lt;p&gt;An even better option where you donât have to manage AI providers' secrets at all end-to-end is to use Unified Billing. In this case you top up your account with credits and use AI Gateway with any of the supported providers directly, Cloudflare gets charged, and we will deduct credits from your account.&lt;/p&gt;
      &lt;p&gt;To make Moltbot use AI Gateway, first we create a new gateway instance, then we enable the Anthropic provider for it, then we either add our Claude key or purchase credits to use Unified Billing, and then all we need to do is set the ANTHROPIC_BASE_URL environment variable so Moltbot uses the AI Gateway endpoint. Thatâs it, no code changes necessary.&lt;/p&gt;
      &lt;p&gt;Once Moltbot starts using AI Gateway, youâll have full visibility on costs and have access to logs and analytics that will help you understand how your AI agent is using the AI providers.&lt;/p&gt;
      &lt;p&gt;Note that Anthropic is one option; Moltbot supports other AI providers and so does AI Gateway. The advantage of using AI Gateway is that if a better model comes along from any provider, you donât have to swap keys in your AI Agent configuration and redeploy â you can simply switch the model in your gateway configuration. And more, you specify model or provider fallbacks to handle request failures and ensure reliability.&lt;/p&gt;
      &lt;p&gt;Last year we anticipated the growing need for AI agents to run untrusted code securely in isolated environments, and we announced the Sandbox SDK. This SDK is built on top of Cloudflare Containers, but it provides a simple API for executing commands, managing files, running background processes, and exposing services â all from your Workers applications.&lt;/p&gt;
      &lt;p&gt;In short, instead of having to deal with the lower-level Container APIs, the Sandbox SDK gives you developer-friendly APIs for secure code execution and handles the complexity of container lifecycle, networking, file systems, and process management â letting you focus on building your application logic with just a few lines of TypeScript. Hereâs an example:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { getSandbox } from '@cloudflare/sandbox';
export { Sandbox } from '@cloudflare/sandbox';

export default {
  async fetch(request: Request, env: Env): Promise&amp;lt;Response&amp;gt; {
    const sandbox = getSandbox(env.Sandbox, 'user-123');

    // Create a project structure
    await sandbox.mkdir('/workspace/project/src', { recursive: true });

    // Check node version
    const version = await sandbox.exec('node -v');

    // Run some python code
    const ctx = await sandbox.createCodeContext({ language: 'python' });
    await sandbox.runCode('import math; radius = 5', { context: ctx });
    const result = await sandbox.runCode('math.pi * radius ** 2', { context: ctx });

    return Response.json({ version, result });
  }
};&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This fits like a glove for Moltbot. Instead of running Docker in your local Mac mini, we run Docker on Containers, use the Sandbox SDK to issue commands into the isolated environment and use callbacks to our entrypoint Worker, effectively establishing a two-way communication channel between the two systems.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;R2 for persistent storage&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;The good thing about running things in your local computer or VPS is you get persistent storage for free. Containers, however, are inherently ephemeral, meaning data generated within them is lost upon deletion. Fear not, though â the Sandbox SDK provides the sandbox.mountBucket() that you can use to automatically, well, mount your R2 bucket as a filesystem partition when the container starts.&lt;/p&gt;
      &lt;p&gt;Once we have a local directory that is guaranteed to survive the container lifecycle, we can use that for Moltbot to store session memory files, conversations and other assets that are required to persist.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Browser Rendering for browser automation&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;AI agents rely heavily on browsing the sometimes not-so-structured web. Moltbot utilizes dedicated Chromium instances to perform actions, navigate the web, fill out forms, take snapshots, and handle tasks that require a web browser. Sure, we can run Chromium on Sandboxes too, but what if we could simplify and use an API instead?&lt;/p&gt;
      &lt;p&gt;With Cloudflareâs Browser Rendering, you can programmatically control and interact with headless browser instances running at scale in our edge network. We support Puppeteer, Stagehand, Playwright and other popular packages so that developers can onboard with minimal code changes. We even support MCP for AI.&lt;/p&gt;
      &lt;p&gt;In order to get Browser Rendering to work with Moltbot we do two things:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;First we create a thin CDP proxy (CDP is the protocol that allows instrumenting Chromium-based browsers) from the Sandbox container to the Moltbot Worker, back to Browser Rendering using the Puppeteer APIs.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Then we inject a Browser Rendering skill into the runtime when the Sandbox starts.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;From the Moltbot runtime perspective, it has a local CDP port it can connect to and perform browser tasks.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Zero Trust Access for authentication policies&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Next up we want to protect our APIs and Admin UI from unauthorized access. Doing authentication from scratch is hard, and is typically the kind of wheel you donât want to reinvent or have to deal with. Zero Trust Access makes it incredibly easy to protect your application by defining specific policies and login methods for the endpoints.Â &lt;/p&gt;
      &lt;p&gt;Zero Trust Access Login methods configuration for the Moltworker application.&lt;/p&gt;
      &lt;p&gt;Once the endpoints are protected, Cloudflare will handle authentication for you and automatically include a JWT token with every request to your origin endpoints. You can then validate that JWT for extra protection, to ensure that the request came from Access and not a malicious third party.&lt;/p&gt;
      &lt;p&gt;Like with AI Gateway, once all your APIs are behind Access you get great observability on who the users are and what they are doing with your Moltbot instance.&lt;/p&gt;
      &lt;p&gt;Demo time. Weâve put up a Slack instance where we could play with our own instance of Moltbot on Workers. Here are some of the fun things weâve done with it.&lt;/p&gt;
      &lt;p&gt;We hate bad news.&lt;/p&gt;
      &lt;p&gt;Hereâs a chat session where we ask Moltbot to find the shortest route between Cloudflare in London and Cloudflare in Lisbon using Google Maps and take a screenshot in a Slack channel. It goes through a sequence of steps using Browser Rendering to navigate Google Maps and does a pretty good job at it. Also look at Moltbotâs memory in action when we ask him the second time.&lt;/p&gt;
      &lt;p&gt;Weâre in the mood for some Asian food today, letâs get Moltbot to work for help.&lt;/p&gt;
      &lt;p&gt;We eat with our eyes too.&lt;/p&gt;
      &lt;p&gt;Letâs get more creative and ask Moltbot to create a video where it browses our developer documentation. As you can see, it downloads and runs ffmpeg to generate the video out of the frames it captured in the browser.&lt;/p&gt;
      &lt;p&gt;We open-sourced our implementation and made it available at https://github.com/cloudflare/moltworker, so you can deploy and run your own Moltbot on top of Workers today.&lt;/p&gt;
      &lt;p&gt;The README guides you through the necessary steps to set up everything. You will need a Cloudflare account and a minimum $5 USD Workers paid plan subscription to use Sandbox Containers, but all the other products are either free to use, like AI Gateway, or have generous free tiers you can use to get you started and run for as long as you want under reasonable limits.&lt;/p&gt;
      &lt;p&gt;Note that Moltworker is a proof of concept, not a Cloudflare product. Our goal is to showcase some of the most exciting features of our Developer Platform that can be used to run AI agents and unsupervised code efficiently and securely, and get great observability while taking advantage of our global network.&lt;/p&gt;
      &lt;p&gt;Feel free to contribute to or fork our GitHub repository; we will keep an eye on it for a while for support. We are also considering contributing upstream to the official project with Cloudflare skills in parallel.&lt;/p&gt;
      &lt;p&gt;We hope you enjoyed this experiment, and we were able to convince you that Cloudflare is the perfect place to run your AI applications and agents. Weâve been working relentlessly trying to anticipate the future and release features like the Agents SDK that you can use to build your first agent in minutes, Sandboxes where you can run arbitrary code in an isolated environment without the complications of the lifecycle of a container, and AI Search, Cloudflareâs managed vector-based search service, to name a few.&lt;/p&gt;
      &lt;p&gt;Cloudflare now offers a complete toolkit for AI development: inference, storage APIs, databases, durable execution for stateful workflows, and built-in AI capabilities. Together, these building blocks make it possible to build and run even the most demanding AI applications on our global edge network.&lt;/p&gt;
      &lt;p&gt;If you're excited about AI and want to help us build the next generation of products and APIs, we're hiring.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/moltworker-self-hosted-ai-agent/"/><published>2026-01-29T14:43:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46810904</id><title>How to Choose Colors for Your CLI Applications (2023)</title><updated>2026-01-29T20:51:16.143161+00:00</updated><content>&lt;doc fingerprint="66a2137707fca2cf"&gt;
  &lt;main&gt;
    &lt;p&gt;Letâs say youâre creating a CLI tool which has to display syntax highlighted source code. You begin by choosing some colors which look nice with your chosen terminal theme:&lt;/p&gt;
    &lt;p&gt;Nice! However, who knows if itâll still look good for people who use a theme different to yours? It seems sensible to try out the defaults, at least. Letâs start with the macOS Terminal.app default theme:&lt;/p&gt;
    &lt;p&gt;Youch! It seems fair to try the Tango themes next, since those are the default on e.g. Ubuntu:&lt;/p&gt;
    &lt;p&gt;Hmm, better, but not by much. Finally, letâs try what is likely the most popular custom terminal theme – Solarized:&lt;/p&gt;
    &lt;p&gt;Well then … Letâs take a look at each palette and investigate.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sorcerer&lt;/head&gt;
    &lt;p&gt;In Sorcerer, all colors are readable on the default background except for &lt;code&gt;black&lt;/code&gt;,
which is in fact darker than the background.
This is useful as the background color
for status bars and the like.
&lt;code&gt;white&lt;/code&gt; is the same color as
the default foreground,
and &lt;code&gt;brblack&lt;/code&gt; is a nice faded color.
Additionally, &lt;code&gt;brwhite&lt;/code&gt; is
even lighter than the foreground;
this allows for subtle emphasization
of important text
like error messages and titles.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basic&lt;/head&gt;
    &lt;p&gt;The Basic themes are, well, horrendous. Really owning that 90s xterm look, it seems. &lt;code&gt;bryellow&lt;/code&gt; is unreadable in light mode
(check out that function name
from the code sample earlier),
while in dark mode
both &lt;code&gt;blue&lt;/code&gt; and &lt;code&gt;brblue&lt;/code&gt;
are totally illegible.&lt;/p&gt;
    &lt;p&gt;That leaves us with thirteen colors we can safely use:&lt;/p&gt;
    &lt;head rend="h2"&gt;Tango&lt;/head&gt;
    &lt;p&gt;In my opinion these did a lot better than Terminal.appâs Basic themes, but they are still far from perfect. &lt;code&gt;bryellow&lt;/code&gt; is again unreadable in the light theme,
and perhaps &lt;code&gt;brgreen&lt;/code&gt; is
a little difficult to see,
though itâs nothing that would
stop me from using &lt;code&gt;brgreen&lt;/code&gt;
in an application.&lt;/p&gt;
    &lt;p&gt;At this point you may have noticed how the greyscales – &lt;code&gt;black&lt;/code&gt;, &lt;code&gt;brblack&lt;/code&gt;, &lt;code&gt;white&lt;/code&gt; &amp;amp; &lt;code&gt;brwhite&lt;/code&gt; –
have remained consistent
between light and dark themes
for both Basic and Tango.
Of course,
this means that
&lt;code&gt;{,br}white&lt;/code&gt; is unreadable in Tango Light
(owing to the light background)
and &lt;code&gt;black&lt;/code&gt; is unreadable in Tango Dark
(owing to the dark background).&lt;/p&gt;
    &lt;p&gt;In other words: forget about that idea of mine from earlier about using &lt;code&gt;brwhite&lt;/code&gt; to emphasize content.
Unless, of course,
you donât mind if your
eminently emphasized words
are completely unreadable
for the user of your software
who deigns to use the default light theme
of A Popular Linux Distro.&lt;/p&gt;
    &lt;p&gt;On the other hand, using &lt;code&gt;brblack&lt;/code&gt; to de-emphasize content
still seems fine to me.
I suppose some extra contrast
for &lt;code&gt;brblack&lt;/code&gt; in Tango Dark
would be nice,
but with text which is meant to be ignored
I donât think this matters much.&lt;/p&gt;
    &lt;p&gt;And lo, but ten colors remain.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solarized&lt;/head&gt;
    &lt;p&gt;Solarized is a curious beast. Every color in it was chosen using L*a*b*, a perceptually-uniform color space from the 1970s. (For what itâs worth, color science has progressed significantly since then; the only reason Ethan Schoonover used L*a*b* is that itâs commonly used in photography, and he used to be a professional photographer.)&lt;/p&gt;
    &lt;p&gt;Its lightnesses are perfectly symmetrical so that Solarized Light and Dark can share a set of accent colors while maintaining identical contrast. Moreover, the warm tones of the light theme and cool tones of the dark theme are complementary. (The hue gap is closer to 150Â° than 180Â° in reality. See here and here to compare hue values.)&lt;/p&gt;
    &lt;p&gt;Solarized is also incredibly popular. I have no data here, but as of the date of writing itâs the most starred theme repository on GitHub I can find. Solarized has 15.4 thousand stars at the moment, while the next-closest is Gruvbox with 11.8 thousand. Solarized is available as a plugin or sometimes even as a built-in preset in damn near every popular terminal emulator and editor on the planet.&lt;/p&gt;
    &lt;p&gt;To understand Solarizedâs peculiar arrangement of the 16-color palette, we have to travel back in time to 2011 when Solarized was first released. In this dark era, terminals supporting 24-bit color didnât exist / werenât widespread. One option common among Vim themes at the time was to round every color to the nearest 256-color palette value. In Solarizedâs case, this destroys the mathematical symmetry at the heart of the theme. (Iâm not kidding, it looks awful.)&lt;/p&gt;
    &lt;p&gt;The solution – rather, hack – chosen at the time was to distill all the colors used in the Vim interface down to a palette of sixteen colors. Conveniently, Solarizedâs accent colors fit nicely into the non-bright column of the 16-color palette, while Solarizedâs monotones fit into the bright column. Once the user sets their terminal to use the Solarized palette, Vim can color its entire interface using only the 16-color palette and get correct color values, no clunky color approximations needed.&lt;/p&gt;
    &lt;p&gt;The downside to all this is that an application which uses any of the bright colors which Solarized co-opted for itself will look strange. Users of Solarized – and, by god, thereâs so many of them – appear frequently on issue trackers asking why command-line output is inexplicably gray or even invisible as a result of CLIs using these forsaken bright colors.&lt;/p&gt;
    &lt;p&gt;Our beloved &lt;code&gt;brblack&lt;/code&gt;
is unreadable in Solarized Dark,
so weâll have to strike it from the table
in addition to the affected bright colors.&lt;/p&gt;
    &lt;head rend="h2"&gt;A sad note about bold&lt;/head&gt;
    &lt;p&gt;Far back in the past, there was no way for terminals to display bright colors. As a workaround, manufacturers (weâre talking about physical terminals here) started making all bold text bright instead of using a heavier font weight. One way or another this ended up in the default settings of many modern terminal emulators (in spite of not being in the standard), meaning that regular colorful text made bold can become bright too, depending on the userâs configuration.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;And so, I present to you the final version of our table of acceptable colors:&lt;/p&gt;
    &lt;p&gt;Bold: ââ boldblack ââ boldbrblack ââ boldred ââ boldbrred ââ boldgreen ââ boldbrgreen ââ boldyellow ââ boldbryellow ââ boldblue ââ boldbrblue ââ boldmagenta ââ boldbrmagenta ââ boldcyan ââ boldbrcyan ââ boldwhite ââ boldbrwhite % â&lt;/p&gt;
    &lt;p&gt;Only eleven out of our thirty-two possible color settings are permissible, given that we want applications to remain readable for as many people as we can.&lt;/p&gt;
    &lt;p&gt;If youâre developing a command-line tool which will be used by anyone apart from yourself, I strongly recommend you limit your use of color to the ones Iâve identified here as being âmostly alrightâ and ânot unreadable in a common configuration used by tons of peopleâ.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;p&gt;You probably didnât notice, but I styled the âterminal windowsâ in this post to look as similar as possible to macOS Terminal.app windows through painstaking color picking and pixel counting.&lt;/p&gt;
    &lt;p&gt;The dimensions in each windowâs titlebar matches as closely as I can with its actual dimensions on-screen.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;colortest&lt;/code&gt; and &lt;code&gt;highlight&lt;/code&gt; utilities
are entirely fictional.&lt;/p&gt;
    &lt;p&gt;Terminal.app doesnât actually provide individual access to the light and dark variants of Basic; they appear as a single theme, which switches seamlessly when the OS theme changes. As far as I know, this reactive functionality isnât exposed to any other theme, whether pre-installed or user-created. In order to capture this, I made the terminal windows in this post react to whether the rest of the site is in light or dark mode, except for the Basic windows. They remain fixed in either light or dark mode, since in real life youâll never see, for example, a light Basic terminal with dark window chrome.&lt;/p&gt;
    &lt;p&gt;Luna Razzaghipour&lt;lb/&gt;29 January 2023&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.xoria.org/terminal-colors/"/><published>2026-01-29T14:49:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46811588</id><title>OTelBench: AI struggles with simple SRE tasks (Opus 4.5 scores only 29%)</title><updated>2026-01-29T20:51:15.847121+00:00</updated><content>&lt;doc fingerprint="3f5885f907369ecf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Benchmarking OpenTelemetry: Can AI trace your failed login?&lt;/head&gt;
    &lt;p&gt;Now on the front page of Hacker News — see the discussion.&lt;/p&gt;
    &lt;p&gt;Frontier AI models have become excellent at writing functions, but can they actually debug production systems?&lt;/p&gt;
    &lt;p&gt;To fix outages, you first need to see what’s happening. In a microservices world, this means producing structured events that track a single request as it hops from service to service.&lt;/p&gt;
    &lt;p&gt;We asked 14 models to add distributed traces to existing codebases, using the standard method: OpenTelemetry instrumentation. We picked tasks that would be easy for a Site Reliability Engineer (SRE).&lt;/p&gt;
    &lt;p&gt;We are releasing OTelBench as an open-source benchmark, with all tasks in QuesmaOrg/otel-bench. We use the Harbor framework (by the creators of TerminalBench), so you can easily run it yourself to reproduce results, test new models, or create benchmarks for your own use cases (we welcome contributions!).&lt;/p&gt;
    &lt;head rend="h2"&gt;Background: What is distributed tracing?&lt;/head&gt;
    &lt;p&gt;When an app runs on a single machine, you can often trace an error by scrolling through a log file. But when it runs across 50 microservices, that single request gets scattered into a chaotic firehose of disconnected events. Distributed tracing solves this by linking them back together, allowing you to follow a user action, like clicking Login, as it jumps from the API Gateway, to the Auth Service, to the Database, and back.&lt;/p&gt;
    &lt;p&gt;To make this work, you need instrumentation. This is code that you add to your app to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start a trace when a request comes in.&lt;/item&gt;
      &lt;item&gt;Pass the TraceID (context) when your app calls another service.&lt;/item&gt;
      &lt;item&gt;Send the data to a backend so you can see the graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;OpenTelemetry (OTel) is the industry standard for telemetry data. Its ecosystem includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Semantic conventions: A unified schema replaces chaotic naming (e.g., &lt;code&gt;ip_address&lt;/code&gt;vs&lt;code&gt;host.ip&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Universal SDKs: Official libraries support every major programming language.&lt;/item&gt;
      &lt;item&gt;The Collector: A centralized agent processes and enriches data (e.g., adding Kubernetes tags) before export.&lt;/item&gt;
      &lt;item&gt;Auto-instrumentation: Runtime agents inject code to wrap calls, though this often results in noisy data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, standard doesn’t mean easy. We know this firsthand from our contributions to the ecosystem, such as Go compile-time instrumentation. The process may be difficult, especially due to complexity, as 39% of respondents complained in the 2025 Observability Survey.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarking OpenTelemetry instrumentation&lt;/head&gt;
    &lt;p&gt;We tested 14 frontier LLMs on 23 realistic OpenTelemetry instrumentation tasks across 11 programming languages: Go, Java, C++, Python, JavaScript, PHP, Ruby, Rust, Erlang, .NET, and Swift.&lt;/p&gt;
    &lt;p&gt;It is essential to benchmark various technologies since realistic distributed systems are polyglot. To make OpenTelemetry work, the system needs to work for all of these services - if we lose track at only one service, the chain of logs gets broken.&lt;/p&gt;
    &lt;p&gt;The final benchmark run cost $522 in LLM tokens across 966 runs (23 tasks × 3 attempts × 14 models).&lt;/p&gt;
    &lt;head rend="h3"&gt;Task&lt;/head&gt;
    &lt;p&gt;We start with basic tasks such as adding instrumentation to a single microservice, in a single language. The AI agents get a small microservice with around 300 lines of code from a realistic application, and work in a Linux terminal, editing it, and running any commands if needed.&lt;/p&gt;
    &lt;p&gt;For example, here is the prompt for go-microservices-traces:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Your task is: Add OTEL tracing to all microservices.&lt;/p&gt;
      &lt;p&gt;Requirements:&lt;/p&gt;
      &lt;item&gt;Instrumentation should match conventions and well-known good practices.&lt;/item&gt;
      &lt;item&gt;Instrumentation must match the business domain of the microservices.&lt;/item&gt;
      &lt;item&gt;Traces must be sent to the endpoint defined by a standard OTEL environment variable.&lt;/item&gt;
      &lt;item&gt;Use the recent version of the OTEL SDK.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;We tested if it satisfies the basic criteria of OpenTelemetry instrumentation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example&lt;/head&gt;
    &lt;p&gt;How do LLMs fail? Let’s analyze a common failure mode.&lt;/p&gt;
    &lt;p&gt;Consider a web service from our benchmark where a user searches and retrieves results. The test simulates two distinct user actions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Happy path: User searches, gets a token, retrieves results successfully&lt;/item&gt;
      &lt;item&gt;Error test: User tries to retrieve results with an invalid token (gets 404)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A human engineer would immediately distinguish these as two independent events, resulting in two separate traces: one for the successful search and one for the failed request.&lt;/p&gt;
    &lt;p&gt;The code structure makes this clear – two separate blocks, each representing a user action:&lt;/p&gt;
    &lt;code&gt;// User Action 1: Search and get results (happy path)
{
    response := client.Post("/search", query)
    result := client.Get("/result?token=" + response.Token)
}

// User Action 2: Error test (invalid token)
{
    result := client.Get("/result?token=invalid")  // Should return 404
}&lt;/code&gt;
    &lt;p&gt;We would expect:&lt;/p&gt;
    &lt;p&gt;Yet, sometimes models failed to recognize these as separate user actions. Instead of two traces, they produced:&lt;/p&gt;
    &lt;p&gt;The core issue: Models apply instrumentation mechanically to every HTTP call without understanding the business context. They see “HTTP requests” and link them all together, rather than recognizing “these are two separate user journeys.”&lt;/p&gt;
    &lt;p&gt;The models successfully instrumented the HTTP calls, but failed to propagate the Context correctly. They treated the timeline as a single flat list of events rather than two distinct hierarchical trees.&lt;/p&gt;
    &lt;p&gt;Our tests don’t just check compilation. We verify correct span names, parent-child relationships, valid trace IDs, and context propagation. Many models produced compiling code that generated malformed traces – proving that “it builds” is not enough for SRE work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Observations&lt;/head&gt;
    &lt;head rend="h3"&gt;Models&lt;/head&gt;
    &lt;p&gt;We were surprised that even the top models (as of Jan 2026) struggle. The tasks we proposed were trivial compared to real-world scenarios. In a typical SRE job, services are massive, legacy-ridden, and poorly documented. If models fail on 300 lines of clean Go code, they cannot handle production.&lt;/p&gt;
    &lt;p&gt;We were surprised that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Opus 4.5, the best model, got just 29% of these relatively simple tasks.&lt;/item&gt;
      &lt;item&gt;Gemini 3 Pro (which aces at general intelligence) didn’t have an edge over the much cheaper Gemini 3 Flash.&lt;/item&gt;
      &lt;item&gt;GPT 5.2 Codex was substantially worse than GPT 5.2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Languages&lt;/head&gt;
    &lt;p&gt;Each language has a different toolset, so it is not an apples-to-apples comparison. Our benchmark is too small to perform a comprehensive per-language comparison, yet even preliminary trends are striking.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cost and time efficiency&lt;/head&gt;
    &lt;p&gt;In every practical application, cost and speed matter. As of Jan 2026, the Pareto frontier consists of only four models, given model performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;19%&lt;/code&gt;Gemini 3 Flash (cost and speed) - the cheapest and fastest model in this benchmark (11x cheaper and 2x faster than Claude Opus 4.5)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;22%&lt;/code&gt;Claude Sonnet 4.5 (speed)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;26%&lt;/code&gt;GPT 5.2 (cost)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;29%&lt;/code&gt;Claude Opus 4.5 (cost and speed) — the best model in this benchmark, the most expensive but reasonably fast&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why OpenTelemetry instrumentation is hard for AI&lt;/head&gt;
    &lt;p&gt;OpenTelemetry has all the potential to be a perfect task for AI agents — it is long and tedious work, requiring a lot of scrutiny, but ultimately one that has clear specifications and can be easily tested.&lt;/p&gt;
    &lt;p&gt;Yet, even the frontier models fail miserably.&lt;/p&gt;
    &lt;head rend="h3"&gt;It is a job, not a puzzle&lt;/head&gt;
    &lt;p&gt;Instrumentation of even a small service involves long-horizon tasks, which remain at the frontier of the current AI model progress. It requires diligently connecting all pieces of code and testing them correctly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Requires polyglot backend development skills&lt;/head&gt;
    &lt;p&gt;Realistic services use multiple languages and technologies. It is not enough to know the concept of distributed tracing, the OpenTelemetry standard, or even the APIs of SDKs. The agent must know CMake for C++, module systems for Go, or dependency management for Java - things we tested in our previous benchmark, CompileBench.&lt;/p&gt;
    &lt;p&gt;Usually, cloud environments are mixtures of the newest versions of technologies (sometimes past the training cut-off dates of AI models) and legacy systems. We cannot cherry-pick or rewrite everything, since a possible outage would be too costly. We need to support all languages and frameworks used in the cloud.&lt;/p&gt;
    &lt;p&gt;A lot of current AI progress focuses on the most popular languages (Python and TypeScript) and reasonably modern frameworks and build systems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Less training data&lt;/head&gt;
    &lt;p&gt;Although adding instrumentation is a standard engineering task, it is not common practice in open-source. The most popular applications, where reliability matters the most, are in private repositories of big tech companies such as Apple, Airbnb, or Netflix.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;head rend="h3"&gt;Key takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Best models struggle: The state-of-the-art Claude Opus 4.5 solved only 29% of tasks.&lt;/item&gt;
      &lt;item&gt;Language gaps: Models failed completely on Java, Ruby, and Swift. C++ led at 37% (boosted by an easier task), Go reached 20%.&lt;/item&gt;
      &lt;item&gt;Silent failures: Many solutions compiled correctly but produced malformed traces or conflated distinct user journeys.&lt;/item&gt;
      &lt;item&gt;Cost efficiency: Gemini 3 Flash exceeds Gemini 3 Pro’s performance (18%) at a fraction of the cost.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;AI SRE is still mostly hype, but there is hope&lt;/head&gt;
    &lt;p&gt;AI SRE in 2026 is what DevOps Anomaly Detection was in 2015 — bold claims backed by huge marketing budgets, but lacking independent verification. There are stories of SaaS vendors abruptly killing the observability stack. Our results mirror ClickHouse’s findings: while LLMs can assist, they lack the capabilities of a skilled SRE.&lt;/p&gt;
    &lt;p&gt;Claude Opus 4.5, GPT-5.2, and Gemini 3 models show promising signals. Some hard tasks like go-microservices-traces reached 55% pass rate. With more environments for Reinforcement Learning with Verified Rewards, this looks like a solvable problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Looking forward&lt;/head&gt;
    &lt;p&gt;Reliable software is incredibly economically valuable, but today it requires too much toil. No one wants to be woken up at 2 AM to troubleshoot.&lt;/p&gt;
    &lt;p&gt;We need a North Star to navigate the current AI boom. Just as SWE-Bench and TerminalBench2.0 became standards for software engineering, we need an SRE-style benchmark for distributed systems. Does the industry need newer models, or perhaps multi-agent systems? A good benchmark will tell us.&lt;/p&gt;
    &lt;p&gt;We invite you to explore the full results on OTelBench and help us expand the test suite on QuesmaOrg/otel-bench. Have you tried using LLMs for observability? We are curious to hear if your experience matches our findings—or if you’ve found a workflow that actually works.&lt;/p&gt;
    &lt;p&gt;Join the discussion on Hacker News, Reddit or LinkedIn.&lt;/p&gt;
    &lt;p&gt;But for now, the verdict is clear: if you need distributed tracing across services, expect to write that code yourself.&lt;/p&gt;
    &lt;p&gt;Stay tuned for future posts and releases&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://quesma.com/blog/introducing-otel-bench/"/><published>2026-01-29T15:37:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46811762</id><title>Heating homes with the largest particle accelerator</title><updated>2026-01-29T20:51:14.431046+00:00</updated><content>&lt;doc fingerprint="e1e269be6278f5b1"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;lb/&gt; What if the world’s largest particle accelerator could also heat homes? CERN’s Large Hadron Collider (LHC) is doing just that, thanks to a new heat exchange system. Since mid-January, heat recovered from the LHC has been supplying a heating network for a new residential and commercial area in the nearby French town of Ferney-Voltaire. This network, inaugurated on 12 December, is expected to supply the equivalent of several thousand homes. By avoiding traditional energy sources, such as gas, the network prevents the emission of thousands of tonnes of CO2.&lt;/p&gt;
    &lt;p&gt;The 27-km LHC has eight surface points and Point 8 is located close to Ferney-Voltaire. The installations at Point 8, particularly the cryogenics, need to be cooled with water. As water circulates through the equipment, the equipment cools and the water heats up. “Typically, hot water would then pass through a cooling tower, releasing heat into the atmosphere so that the cooled water could be reinjected into the equipment,” explains CERN’s energy coordinator, Nicolas Bellegarde. “In the new set-up, hot water initially passes through two 5-MW heat exchangers, which transfer thermal energy to the new heating network in Ferney-Voltaire.”&lt;/p&gt;
    &lt;p&gt;As one of the new network’s heat sources, CERN provides heat whenever possible, as long as it does not impact its activities. At present, Ferney-Voltaire is only using up to 5 MW from CERN but, with two heat exchangers in the system, this could theoretically be doubled, especially when CERN’s accelerators are fully operational. In summer 2026, CERN will stop the LHC for several years of maintenance and upgrades, known as Long Shutdown 3 (LS3), to prepare for the upcoming High-Luminosity LHC. Some Point 8 installations will continue to be cooled, enabling CERN to supply between 1 and 5 MW to the network during LS3, with the exception of a total of five months spread over this multi-year period.&lt;/p&gt;
    &lt;p&gt;Driven by a commitment to environmentally responsible research, CERN has implemented many initiatives to help reduce the impact of its activities on the environment. Energy recovery is a key part of CERN’s energy management strategy, in line with ISO 50001 requirements, alongside keeping energy consumption to a minimum and improving energy efficiency. Other projects include CERN’s Prévessin Data Centre, inaugurated in 2024, which is equipped with a heat-recovery system set to warm most site buildings from winter 2026/2027, and the future recovery of heat from LHC Point 1 cooling towers to supply buildings on CERN’s Meyrin site. Together, these initiatives will save 25–30 GWh per year as of 2027, marking significant progress in CERN’s responsible energy management.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://home.cern/news/news/cern/heating-homes-worlds-largest-particle-accelerator"/><published>2026-01-29T15:49:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46812159</id><title>Drug trio found to block tumour resistance in pancreatic cancer</title><updated>2026-01-29T20:51:13.994628+00:00</updated><content>&lt;doc fingerprint="f7c630393b87c3c8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Drug trio found to block tumour resistance in pancreatic cancer&lt;/head&gt;
    &lt;p&gt;Posted: 29 January 2026 | Drug Target Review | No comments yet&lt;/p&gt;
    &lt;p&gt;A new study reports that a triple-targeted drug combination can drive complete and lasting regression of pancreatic tumours in preclinical models, potentially overcoming treatment resistance in one of the deadliest cancers.&lt;/p&gt;
    &lt;p&gt;Researchers at the Spanish National Cancer Research Centre have announced a potential breakthrough combination therapy that induces complete regression of pancreatic tumours and prevents tumour resistance in preclinical models.&lt;/p&gt;
    &lt;p&gt;The study describes a targeted combination therapy that simultaneously targets three key signalling pathways in pancreatic ductal adenocarcinoma (PDAC), the most common and lethal type of pancreatic cancer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Triple inhibition strategy&lt;/head&gt;
    &lt;p&gt;Pancreatic cancer remains notoriously difficult to treat, with very poor survival rates and limited effective therapies. The new research aims to combat this by targeting RAF1, EGFR family receptors and STAT3 signalling – nodes that are crucial for tumour growth and survival.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt; Automation now plays a central role in discovery. From self-driving laboratories to real-time bioprocessing&lt;/head&gt;
    &lt;p&gt;This report explores how data-driven systems improve reproducibility, speed decisions and make scale achievable across research and development.&lt;/p&gt;
    &lt;p&gt;Inside the report:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Advance discovery through miniaturised, high-throughput and animal-free systems&lt;/item&gt;
      &lt;item&gt;Integrate AI, robotics and analytics to speed decision-making&lt;/item&gt;
      &lt;item&gt;Streamline cell therapy and bioprocess QC for scale and compliance&lt;/item&gt;
      &lt;item&gt;And more!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This report unlocks perspectives that show how automation is changing the scale and quality of discovery. The result is faster insight, stronger data and better science – access your free copy today&lt;/p&gt;
    &lt;p&gt;According to the authors, “genetic ablation of three independent nodes involved in downstream (RAF1), upstream (EGFR) and orthogonal (STAT3) KRAS signalling pathways leads to complete and permanent regression of orthotopic PDACs induced by KRAS/TP53 mutations.”&lt;/p&gt;
    &lt;p&gt;The triple treatment combines three drugs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RMC-6236 (daraxonrasib): targeting KRAS&lt;/item&gt;
      &lt;item&gt;Afatinib: an EGFR family inhibitor&lt;/item&gt;
      &lt;item&gt;SD36: a selective STAT3 degrader&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These agents together were tested in orthotopic mouse models of PDAC, where tumour cells are implanted in a location that closely resembles their natural environment in the pancreas. The results demonstrated the therapy not only reduced tumour size but also entirely stopped tumour growth with no evidence of tumour resistance for more than 200 days after treatment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Broad efficacy in preclinical models&lt;/head&gt;
    &lt;p&gt;Researchers extended their observations beyond engineered mouse models. The combination therapy also led to significant regression in genetically engineered mouse tumours and in human cancer tissues grown in lab mice, known as patient-derived tumour xenografts (PDX).&lt;/p&gt;
    &lt;p&gt;These results should guide the development of new clinical trials that may benefit PDAC patients.&lt;/p&gt;
    &lt;p&gt;These powerful anti-tumour effects were achieved with a therapy that was well tolerated in the animals, which could provide a favourable safety profile for future clinical testing.&lt;/p&gt;
    &lt;p&gt;“These results should guide the development of new clinical trials that may benefit PDAC patients,” said the authors.&lt;/p&gt;
    &lt;head rend="h2"&gt;A step towards overcoming resistance&lt;/head&gt;
    &lt;p&gt;One of the most significant hurdles in targeted cancer therapies is the development of resistance. This new combination strategy appears to prevent this relapse, at least in preclinical models, by attacking multiple nodes of tumour signalling simultaneously.&lt;/p&gt;
    &lt;p&gt;According to commentary from scientists involved in the work: “Overcoming therapeutic resistance in PDAC requires coordinated inhibition of KRAS downstream (RAF1), upstream (EGFR) and parallel survival pathways (STAT3).”&lt;/p&gt;
    &lt;head rend="h2"&gt;Clinical implications&lt;/head&gt;
    &lt;p&gt;While more research will be needed before trials in humans can begin, these findings are an important advancement in the search for better pancreatic cancer therapies. By demonstrating complete and durable tumour regression without resistance in preclinical models, there is now strong potential for clinical development of multi-targeted approaches in the future.&lt;/p&gt;
    &lt;p&gt;Related topics&lt;lb/&gt;Animal Models, Cancer research, Disease Research, Drug Development, Drug Discovery, Drug Discovery Processes, Drug Targets, In Vivo, Molecular Targets, Oncology, Small molecule, Therapeutics, Translational Science&lt;/p&gt;
    &lt;p&gt;Related conditions&lt;lb/&gt;Pancreatic cancer&lt;/p&gt;
    &lt;p&gt;Related organisations&lt;lb/&gt;the Spanish National Cancer Research Centre&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.drugtargetreview.com/news/192714/drug-trio-found-to-block-tumour-resistance-in-pancreatic-cancer/"/><published>2026-01-29T16:11:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46812173</id><title>US cybersecurity chief leaked sensitive government files to ChatGPT: Report</title><updated>2026-01-29T20:51:13.775994+00:00</updated><content>&lt;doc fingerprint="2937ce14048341f7"&gt;
  &lt;main&gt;
    &lt;p&gt;The acting head of the US government’s top cybersecurity agency reportedly uploaded sensitive government files into a public version of ChatGPT, triggering internal security alerts and a federal review.&lt;/p&gt;
    &lt;p&gt;A Politico investigation claims Madhu Gottumukkala, the interim director of the Cybersecurity and Infrastructure Security Agency, uploaded contracting documents marked “For Official Use Only” into ChatGPT last summer.&lt;/p&gt;
    &lt;p&gt;The report says Gottumukkala requested a special exemption to access ChatGPT, which is blocked for other Department of Homeland Security staff.&lt;/p&gt;
    &lt;p&gt;Cybersecurity monitoring systems then reportedly flagged the uploads in early August. That triggered a DHS-led damage assessment to determine whether the information had been exposed.&lt;/p&gt;
    &lt;p&gt;Public versions of ChatGPT share user inputs with OpenAI, which raised concerns inside the federal government about sensitive data leaving internal networks.&lt;/p&gt;
    &lt;head rend="h2"&gt;CISA responds to ChatGPT investigation&lt;/head&gt;
    &lt;p&gt;CISA spokesperson Marci McCarthy told Politico that Gottumukkala “was granted permission to use ChatGPT with DHS controls in place,” adding that the use was “short-term and limited.”&lt;/p&gt;
    &lt;p&gt;Gottumukkala has served as acting director since May, while the Senate has yet to confirm Sean Plankey as permanent head of the agency.&lt;/p&gt;
    &lt;p&gt;The ChatGPT incident follows other reported issues during Gottumukkala’s tenure. Politico said he previously failed a counterintelligence polygraph required for access to highly sensitive intelligence. During congressional testimony last week, he rejected that characterization when questioned.&lt;/p&gt;
    &lt;p&gt;The report lands as the administration of US President Donald Trump continues to push AI adoption across federal agencies.&lt;/p&gt;
    &lt;p&gt;Trump signed an executive order in December aimed at limiting state-level AI regulation, while the Pentagon has announced an “AI-first” strategy to expand the military’s use of artificial intelligence.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dexerto.com/entertainment/us-cybersecurity-chief-leaked-sensitive-government-files-to-chatgpt-report-3311462/"/><published>2026-01-29T16:12:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46812608</id><title>Launch HN: AgentMail (YC S25) – An API that gives agents their own email inboxes</title><updated>2026-01-29T20:51:13.552793+00:00</updated><content>&lt;doc fingerprint="3fd71f1327c4d429"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN, we're Haakam, Michael, and Adi. We're building AgentMail (&lt;/p&gt;https://agentmail.to&lt;p&gt;), the email inbox API for agents. We’re not talking about AI for your email, this is email for your AI.&lt;/p&gt;&lt;p&gt;Email is an optimal interface for long-running agents. It’s multithreaded and asynchronous with full support for rich text and files. It’s a universal protocol with identity and authentication built in. Moreover, a lot of workflow critical context already lives in email.&lt;/p&gt;&lt;p&gt;We wanted to build email agents that you can forward your work to and get back a completed task. The agents could act entirely autonomously as you wouldn't need to delegate your identity. If they did get stuck they could just send you, or anyone else, an email.&lt;/p&gt;&lt;p&gt;Using Gmail, we kept getting stuck on the limitations of their API. No way to create inboxes programmatically. Rate and sending limits. OAuth for every single inbox. Keyword search that doesn't understand context. Per-seat pricing that doesn't work for agents.&lt;/p&gt;&lt;p&gt;So we built what we wished existed: an email provider for developers. APIs for creating inboxes and configuring domains. Email parsing and threading. Text extraction from attachments. Realtime webhooks and websockets. Semantic search across inboxes. Usage-based pricing that works for agents.&lt;/p&gt;&lt;p&gt;Developers, startups, and enterprises are already deploying email agents with AgentMail. Agents that convert conversations and documents into structured data. Agents that source quotes, negotiate prices, and get the best deals. Agents that emulate internet users for training models on end-to-end tasks.&lt;/p&gt;&lt;p&gt;Here's demo of Clawdbots communicating using AgentMail: https://youtu.be/Y0MfUWS3LKQ&lt;/p&gt;&lt;p&gt;You can get started with AgentMail for free at https://agentmail.to&lt;/p&gt;&lt;p&gt;Looking forward to hearing your thoughts and feedback.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46812608"/><published>2026-01-29T16:42:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46812892</id><title>Reflex (YC W23) Senior Software Engineer Infra</title><updated>2026-01-29T20:51:12.892724+00:00</updated><content>&lt;doc fingerprint="75ce1aa31dcaa837"&gt;
  &lt;main&gt;
    &lt;p&gt;The operating system for building mission-critical enterprise apps.&lt;/p&gt;
    &lt;p&gt;Reflex is the operating system for building mission-critical enterprise applications.&lt;/p&gt;
    &lt;p&gt;Today’s enterprise stack is fragmented. Shipping an app requires stitching together multiple tools and coordinating across multiple roles. Reflex replaces that complexity with a single, unified platform to build, deploy, and manage production applications end-to-end.&lt;/p&gt;
    &lt;p&gt;We empower teams to own the entire lifecycle of their apps — from idea to production — without needing specialized infrastructure, DevOps, or platform teams. We do this by providing solid, reusable abstractions at both the framework and infrastructure layers. Because we own the underlying open-source framework and the platform it runs on, we can manage the full lifecycle of the application seamlessly.&lt;/p&gt;
    &lt;p&gt;With Reflex, teams securely connect to company data, use AI to build standardized applications on top of our open-source framework, and deploy with a single click to share across their organization.&lt;/p&gt;
    &lt;p&gt;We’re replacing the fragmented enterprise stack — and the organizational bottlenecks that come with it.&lt;/p&gt;
    &lt;p&gt;Why join Reflex now?&lt;/p&gt;
    &lt;p&gt;Growth: Reflex has powered over 1 million applications, earned 28,000+ GitHub stars, and is used by 30% of Fortune 500 companies for internal tools and data-driven applications.&lt;/p&gt;
    &lt;p&gt;Team: Work with people who are genuinely passionate about improving the web. Our founding team consists of open source maintainers, top-ranked competitive programmers/IOI medalists, and founding team members from dev tool unicorns.&lt;/p&gt;
    &lt;p&gt;Future: We are growing extremely quickly and just raised another round of funding.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/reflex/jobs/Jcwrz7A-lead-software-engineer-infra"/><published>2026-01-29T17:00:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46812933</id><title>Project Genie: Experimenting with infinite, interactive worlds</title><updated>2026-01-29T20:51:12.524725+00:00</updated><content>&lt;doc fingerprint="18b9cbb6ad27d00d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Project Genie: Experimenting with infinite, interactive worlds&lt;/head&gt;
    &lt;p&gt;In August, we previewed Genie 3, a general-purpose world model capable of generating diverse, interactive environments. Even in this early form, trusted testers were able to create an impressive range of fascinating worlds and experiences, and uncovered entirely new ways to use it. The next step is to broaden access through a dedicated, interactive prototype focused on immersive world creation.&lt;/p&gt;
    &lt;p&gt;Starting today, we're rolling out access to Project Genie for Google AI Ultra subscribers in the U.S (18+). This experimental research prototype lets users create, explore and remix their own interactive worlds.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we’re advancing world models&lt;/head&gt;
    &lt;p&gt;A world model simulates the dynamics of an environment, predicting how they evolve and how actions affect them. While Google DeepMind has a history of agents for specific environments like Chess or Go, building AGI requires systems that navigate the diversity of the real world.&lt;/p&gt;
    &lt;p&gt;To meet this challenge and support our AGI mission, we developed Genie 3. Unlike explorable experiences in static 3D snapshots, Genie 3 generates the path ahead in real time as you move and interact with the world. It simulates physics and interactions for dynamic worlds, while its breakthrough consistency enables the simulation of any real-world scenario — from robotics and modelling animation and fiction, to exploring locations and historical settings.&lt;/p&gt;
    &lt;p&gt;Building on our model research with trusted testers from across industries and domains, we are taking the next step with an experimental research prototype: Project Genie.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Project Genie works&lt;/head&gt;
    &lt;p&gt;Project Genie is a prototype web app powered by Genie 3, Nano Banana Pro and Gemini, which allows users to experiment with the immersive experiences of our world model firsthand. The experience is centred on three core capabilities:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. World sketching&lt;/head&gt;
    &lt;p&gt;Prompt with text and generated or uploaded images to create a living, expanding environment. Create your character, your world, and define how you want to explore it — from walking to riding, flying to driving, and anything beyond.&lt;/p&gt;
    &lt;p&gt;For more precise control, we have integrated “World Sketching” with Nano Banana Pro. This allows you to preview what your world will look like and modify your image to fine tune your world prior to jumping in. You can also define your perspective for the character — such as first-person or third-person — giving you control over how you experience the scene before you enter.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. World exploration&lt;/head&gt;
    &lt;p&gt;Your world is a navigable environment that’s waiting to be explored. As you move, Project Genie generates the path ahead in real time based on the actions you take. You can also adjust the camera as you traverse through the world.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. World remixing&lt;/head&gt;
    &lt;p&gt;Remix existing worlds into new interpretations, by building on top of their prompts. You can also explore curated worlds in the gallery or in the &amp;lt;randomizer icon&amp;gt; for inspiration, or build on top of them. And once you’re done, you can download videos of your worlds and your explorations.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we’re building responsibly&lt;/head&gt;
    &lt;p&gt;Project Genie is an experimental research prototype in Google Labs, powered by Genie 3. As with all our work towards general AI systems, our mission is to build AI responsibly to benefit humanity. Since Genie 3 is an early research model, there are a few known areas for improvement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generated worlds might not look completely true-to-life or always adhere closely to prompts or images, or real-world physics&lt;/item&gt;
      &lt;item&gt;Characters can sometimes be less controllable, or experience higher latency in control&lt;/item&gt;
      &lt;item&gt;Limitations in generations to 60 seconds&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A few of the Genie 3 model capabilities we announced in August, such as promptable events that change the world as you explore it, are not yet included in this prototype. You can find more details on model limitations and future updates on how we’re improving the experience, here.&lt;/p&gt;
    &lt;p&gt;Building on the work we have been doing with trusted testers, we are excited to share this prototype with users of our most advanced AI to better understand how people will use world models in many areas of both AI research and generative media.&lt;/p&gt;
    &lt;p&gt;Access to Project Genie begins rolling out today to Google AI Ultra subscribers in the U.S. (18+), expanding to more territories in due course. We look forward to seeing the infinitely diverse worlds they create, and in time, our goal is to make these experiences and technology accessible to more users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/innovation-and-ai/models-and-research/google-deepmind/project-genie/"/><published>2026-01-29T17:02:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46814115</id><title>OpenAI's In-House Data Agent</title><updated>2026-01-29T20:51:12.269267+00:00</updated><content>&lt;doc fingerprint="f75eac5f79e1984e"&gt;
  &lt;main&gt;
    &lt;p&gt;Data powers how systems learn, products evolve, and how companies make choices. But getting answers quickly, correctly, and with the right context is often harder than it should be. To make this easier as OpenAI scales, we built our own bespoke in-house AI data agent that explores and reasons over our own platform.&lt;/p&gt;
    &lt;p&gt;Our agent is a custom internal-only tool (not an external offering), built specifically around OpenAI’s data, permissions, and workflows. We’re showing how we built and use it to help surface examples of the real, impactful ways AI can support day-to-day work across our teams. The OpenAI tools we used to build and run it (Codex, our GPT‑5 flagship model, the Evals API(opens in a new window), and the Embeddings API(opens in a new window)) are the same tools we make available to developers everywhere.&lt;/p&gt;
    &lt;p&gt;Our data agent lets employees go from question to insight in minutes, not days. This lowers the bar to pulling data and nuanced analysis across all functions, not just by our data team. Today, teams across Engineering, Data Science, Go-To-Market, Finance, and Research at OpenAI lean on the agent to answer high-impact data questions. For example, it can help answer how to evaluate launches and understand business health, all through the intuitive format of natural language. The agent combines Codex-powered table-level knowledge with product and organizational context. Its continuously learning memory system means it also improves with every turn.&lt;/p&gt;
    &lt;p&gt;In this post, we’ll break down why we needed a bespoke AI data agent, what makes its code-enriched data context and self-learning so useful, and lessons we learned along the way.&lt;/p&gt;
    &lt;p&gt;OpenAI’s data platform serves more than 3.5k internal users working across Engineering, Product, and Research, spanning over 600 petabytes of data across 70k datasets. At that size, simply finding the right table can be one of the most time-consuming parts of doing analysis.&lt;/p&gt;
    &lt;p&gt;As one internal user put it:&lt;/p&gt;
    &lt;p&gt;“We have a lot of tables that are fairly similar, and I spend tons of time trying to figure out how they’re different and which to use. Some include logged-out users, some don’t. Some have overlapping fields; it’s hard to tell what is what.”&lt;/p&gt;
    &lt;p&gt;Even with the correct tables selected, producing correct results can be challenging. Analysts must reason about table data and table relationships to ensure transformations and filters are applied correctly. Common failure modes—many-to-many joins, filter pushdown errors, and unhandled nulls—can silently invalidate results. At OpenAI’s scale, analysts should not have to sink time into debugging SQL semantics or query performance: their focus should be on defining metrics, validating assumptions, and making data-driven decisions.&lt;/p&gt;
    &lt;p&gt;Let’s walk through what our agent is, how it curates context, and how it keeps self-improving.&lt;/p&gt;
    &lt;p&gt;Our agent is powered by GPT‑5.2 and is designed to reason over OpenAI’s data platform. It’s available wherever employees already work: as a Slack agent, through a web interface, inside IDEs, in the Codex CLI via MCP(opens in a new window), and directly in OpenAI’s internal ChatGPT app through a MCP connector(opens in a new window).&lt;/p&gt;
    &lt;p&gt;Users can ask complex, open-ended questions which would typically require multiple rounds of manual exploration. Take this example prompt, which uses a test data set: “For NYC taxi trips, which pickup-to-dropoff ZIP pairs are the most unreliable, with the largest gap between typical and worst-case travel times, and when does that variability occur?”&lt;/p&gt;
    &lt;p&gt;The agent handles the analysis end-to-end, from understanding the question to exploring the data, running queries, and synthesizing findings.&lt;/p&gt;
    &lt;p&gt;One of the agent’s superpowers is how it reasons through problems. Rather than following a fixed script, the agent evaluates its own progress. If an intermediate result looks wrong (e.g., if it has zero rows due to an incorrect join or filter), the agent investigates what went wrong, adjusts its approach, and tries again. Throughout this process, it retains full context, and carries learnings forward between steps. This closed-loop, self-learning process shifts iteration from the user into the agent itself, enabling faster results and consistently higher-quality analyses than manual workflows.&lt;/p&gt;
    &lt;p&gt;The agent covers the full analytics workflow: discovering data, running SQL, and publishing notebooks and reports. It understands internal company knowledge, can web search for external information, and improves over time through learned usage and memory.&lt;/p&gt;
    &lt;p&gt;High-quality answers depend on rich, accurate context. Without context, even strong models can produce wrong results, such as vastly misestimating user counts or misinterpreting internal terminology.&lt;/p&gt;
    &lt;p&gt;To avoid these failure modes, the agent is built around multiple layers of context that ground it in OpenAI’s data and institutional knowledge.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Metadata grounding: The agent relies on schema metadata (column names and data types) to inform SQL writing and uses table lineage (e.g., upstream and downstream table relationships) to provide context on how different tables relate.&lt;/item&gt;
      &lt;item&gt;Query inference: Ingesting historical queries helps the agent understand how to write its own queries and which tables are typically joined together.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Curated descriptions of tables and columns provided by domain experts, capturing intent, semantics, business meaning, and known caveats that are not easily inferred from schemas or past queries.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Metadata alone isn’t enough. To really tell tables apart, you need to understand how they were created and where they originate.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;By deriving a code-level definition of a table, the agent builds a deeper understanding of what the data actually contains. &lt;list rend="ul"&gt;&lt;item&gt;Nuances on what is stored in the table and how it is derived from an analytics event provides extra information. For example, it can give context on the uniqueness of values, how often the table data is updated, the scope of the data (e.g., if the table excludes certain fields, it has this level of granularity), etc.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;This provides enhanced usage context by showing how the table is used beyond SQL in Spark, Python, and other data systems.&lt;/item&gt;
      &lt;item&gt;This means that the agent can distinguish between tables that look similar but differ in critical ways. For example, it can tell whether a table only includes first-party ChatGPT traffic. This context is also refreshed automatically, so it stays up to date without manual maintenance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The agent can access Slack, Google Docs, and Notion, which capture critical company context such as launches, reliability incidents, internal codenames and tools, and the canonical definitions and computation logic for key metrics.&lt;/item&gt;
      &lt;item&gt;These documents are ingested, embedded, and stored with metadata and permissions. A retrieval service handles access control and caching at runtime, enabling the agent to efficiently and safely pull in this information.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;When the agent is given corrections or discovers nuances about certain data questions, it's able to save these learnings for next time, allowing it to constantly improve with its users. &lt;list rend="ul"&gt;&lt;item&gt;As a result, future answers begin from a more accurate baseline rather than repeatedly encountering the same issues.&lt;/item&gt;&lt;item&gt;The goal of memory is to retain and reuse non-obvious corrections, filters, and constraints that are critical for data correctness but difficult to infer from the other layers alone.&lt;/item&gt;&lt;item&gt;For example, in one case, the agent didn’t know how to filter for a particular analytics experiment (it relied on matching against a specific string defined in an experiment gate). Memory was crucially important here to ensure it was able to filter correctly, instead of fuzzily trying to string match.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;When you give the agent a correction or when it finds a learning from your conversation, it will prompt you to save that memory for next time. &lt;list rend="ul"&gt;&lt;item&gt;Memories can also be manually created and edited by users.&lt;/item&gt;&lt;item&gt;Memories are scoped at the global and personal level, and the agent’s tooling makes it easy to edit them.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;When no prior context exists for a table or when existing information is stale, the agent can issue live queries to the data warehouse to inspect and query the table directly. This allows it to validate schemas, understand the data in real-time, and respond accordingly.&lt;/item&gt;
      &lt;item&gt;The agent is also able to talk to other Data Platform systems (metadata service, Airflow, Spark) as needed to get broader data context that exists outside the warehouse.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We run a daily offline pipeline that aggregates table usage, human annotations, and Codex-derived enrichment into a single, normalized representation. This enriched context is then converted into embeddings using the OpenAI embeddings API(opens in a new window) and stored for retrieval. At query time, the agent pulls only the most relevant embedded context via retrieval-augmented generation(opens in a new window) (RAG) instead of scanning raw metadata or logs. This makes table understanding fast and scalable, even across tens of thousands of tables, while keeping runtime latency predictable and low. Runtime queries are issued to our data warehouse live as needed.&lt;/p&gt;
    &lt;p&gt;Together, these layers ensure the agent’s reasoning is grounded in OpenAI’s data, code, and institutional knowledge, dramatically reducing errors and improving answer quality.&lt;/p&gt;
    &lt;p&gt;One-shot answers work when the problem is clear, but most questions aren’t. More often, arriving at the correct result requires back-and-forth refinement and some course correction.&lt;/p&gt;
    &lt;p&gt;The agent is built to behave like a teammate you can reason with. It’s a conversational, always-on and handles both quick answers and iterative exploration.&lt;/p&gt;
    &lt;p&gt;It carries over complete context across turns, so users can ask follow-up questions, adjust their intent, or change direction without restating everything. If the agent starts heading down the wrong path, users can interrupt mid-analysis and redirect it, just like working with a human collaborator who listens instead of plowing ahead.&lt;/p&gt;
    &lt;p&gt;When instructions are unclear or incomplete, the agent proactively asks clarifying questions. If no response is provided, it applies sensible defaults to make progress. For example, if a user asks about business growth with no date range specified, it may assume the last seven or 30 days. These priors allow it to stay responsive and non-blocking while still converging on the right outcome.&lt;/p&gt;
    &lt;p&gt;The result is an agent that works well both when you know exactly what you want (e.g., “Tell me about this table”) and just as strong when you’re exploring (e.g., “I’m seeing a dip here, can we break this down by customer type and timeframe?”).&lt;/p&gt;
    &lt;p&gt;After rollout, we observed that users frequently ran the same analyses for routine repetitive work. To expedite this, the agent's workflows package recurring analyses into reusable instruction sets. Examples include workflows for weekly business reports and table validations. By encoding context and best practices once, workflows streamline repeat analyses and ensure consistent results across users.&lt;/p&gt;
    &lt;p&gt;Building an always-on, evolving agent means quality can drift just as easily as it can improve. Without a tight feedback loop, regressions are inevitable and invisible. The only way to scale capability without breaking trust is through systematic evaluation.&lt;/p&gt;
    &lt;p&gt;In this section, we’ll discuss how we leverage OpenAI’s Evals API(opens in a new window) to measure and protect the agent’s response quality.&lt;/p&gt;
    &lt;p&gt;Its Evals are built on curated sets of question-answer pairs. Each question targets an important metric or analytical pattern we care deeply about getting right, paired with a manually authored “golden” SQL query that produces the expected result. For each eval, we send the natural language question to its query-generation endpoint, execute the generated SQL, and compare the output against the result of the expected SQL.&lt;/p&gt;
    &lt;p&gt;Evaluation doesn’t rely on naive string matching. Generated SQL can differ syntactically while still being correct, and result sets may include extra columns that don’t materially affect the answer. To account for this, we compare both the SQL and the resulting data, and feed these signals into OpenAI’s Evals grader. The grader produces a final score along with an explanation, capturing both correctness and acceptable variation.&lt;/p&gt;
    &lt;p&gt;These evals are like unit tests that run continuously during development to identify regressions as canaries in production; this allows us to catch issues early and confidently iterate as the agent's capabilities expand.&lt;/p&gt;
    &lt;p&gt;Our agent plugs directly into OpenAI’s existing security and access-control model. It operates purely as an interface layer, inheriting and enforcing the same permissions and guardrails that govern OpenAI’s data.&lt;/p&gt;
    &lt;p&gt;All of the agent’s access is strictly pass-through, meaning users can only query tables they already have permission to access. When access is missing, it flags this or falls back to alternative datasets the user is authorized to use.&lt;/p&gt;
    &lt;p&gt;Finally, it's built for transparency. Like any system, it can make mistakes. It exposes its reasoning process by summarizing assumptions and execution steps alongside each answer. When queries are executed, it links directly to the underlying results, allowing users to inspect raw data and verify every step of the analysis.&lt;/p&gt;
    &lt;p&gt;Building our agent from scratch surfaced practical lessons about how agents behave, where they struggle, and what actually makes them reliable at scale.&lt;/p&gt;
    &lt;p&gt;Early on, we exposed our full tool set to the agent, and quickly ran into problems with overlapping functionality. While this redundancy can be helpful for specific custom cases and is more obvious to a human when manually invoking, it’s confusing to agents. To reduce ambiguity and improve reliability, we restricted and consolidated certain tool calls.&lt;/p&gt;
    &lt;p&gt;We also discovered that highly prescriptive prompting degraded results. While many questions share a general analytical shape, the details vary enough that rigid instructions often pushed the agent down incorrect paths. By shifting to higher-level guidance and relying on GPT‑5’s reasoning to choose the appropriate execution path, the agent became more robust and produced better results.&lt;/p&gt;
    &lt;p&gt;Schemas and query history describe a table’s shape and usage, but its true meaning lives in the code that produces it. Pipeline logic captures assumptions, freshness guarantees, and business intent that never surface in SQL or metadata. By crawling the codebase with Codex, our agent understands how datasets are actually constructed and is able to better reason about what each table actually contains. It can answer “what’s in here” and “when can I use it” far more accurately than from warehouse signals alone.&lt;/p&gt;
    &lt;p&gt;We’re constantly working to improve our agent by increasing its ability to handle ambiguous questions, improving its reliability and accuracy with stronger validations, and integrating it more deeply into workflows. We believe it should blend naturally into how people already work, instead of functioning like a separate tool.&lt;/p&gt;
    &lt;p&gt;While our tooling will keep benefiting from underlying improvements in agent reasoning, validation, and self-correction, our team’s mission remains the same: seamlessly deliver fast, trustworthy data analysis across OpenAI’s data ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Special thanks to the Data Productivity and Data Science teams, as well as to our many cross-functional users for their experimentation and feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/inside-our-in-house-data-agent"/><published>2026-01-29T18:17:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46814129</id><title>MakuluLinux (6.4M Downloads) Ships Persistent Backdoor from Developer's Own C2</title><updated>2026-01-29T20:51:11.919012+00:00</updated><content>&lt;doc fingerprint="85a99d555796f4d4"&gt;
  &lt;main&gt;
    &lt;div&gt;← Back to WeRAI&lt;head rend="h1"&gt;MakuluLinux Ships a Persistent Backdoor in Every Installation&lt;/head&gt;&lt;p&gt; Severity: CRITICAL | Disclosure Date: January 28, 2026 | Discovered by Steven Stobo (WeRAI / Haven AI) &lt;/p&gt;&lt;p&gt; The MakuluLinux operating system installs a binary that establishes a persistent connection to a command-and-control server owned by the developer. This is not a third-party compromise. The backdoor is embedded in the OS installer itself. &lt;/p&gt;&lt;head rend="h2"&gt;The Evidence Chain&lt;/head&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;1&lt;/p&gt;&lt;p&gt;&lt;code&gt;install-script.bin&lt;/code&gt; (the OS installer) copies &lt;code&gt;/usr/share/MakuluSetup/files/check.bin&lt;/code&gt; to &lt;code&gt;/usr/bin/check.bin&lt;/code&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;↓&lt;/p&gt;&lt;div&gt;&lt;p&gt;2&lt;/p&gt;&lt;p&gt;Creates autostart entry disguised as "System Health Check" with 30-second delay&lt;/p&gt;&lt;/div&gt;&lt;p&gt;↓&lt;/p&gt;&lt;div&gt;&lt;p&gt;3&lt;/p&gt;&lt;p&gt;&lt;code&gt;check.bin&lt;/code&gt; (9.5MB stripped ELF) establishes persistent TCP connection to 217.77.8.210:2006&lt;/p&gt;&lt;/div&gt;&lt;p&gt;↓&lt;/p&gt;&lt;div&gt;&lt;p&gt;4&lt;/p&gt;&lt;p&gt;That IP resolves to makulu.online — the developer's own domain&lt;/p&gt;&lt;/div&gt;&lt;p&gt;↓&lt;/p&gt;&lt;div&gt;&lt;p&gt;5&lt;/p&gt;&lt;p&gt;Installer error handling: "One or more critical final file operations (startup/check.bin) failed" — it's a critical install component&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h2"&gt;Infrastructure&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell role="head"&gt;Asset&lt;/cell&gt;&lt;cell role="head"&gt;IP&lt;/cell&gt;&lt;cell role="head"&gt;Hosting&lt;/cell&gt;&lt;cell role="head"&gt;Registrant&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;C2 Server&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;217.77.8.210:2006&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Contabo GmbH, DE&lt;/cell&gt;&lt;cell&gt;Germany&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;makulu.online&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;217.77.8.210&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Contabo GmbH&lt;/cell&gt;&lt;cell&gt;Da Nang, Vietnam&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;makululinux.eu&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;207.180.233.66&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Contabo GmbH&lt;/cell&gt;&lt;cell&gt;Redacted&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;makululinux.com&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;64.20.42.243&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Trouble-free.net&lt;/cell&gt;&lt;cell&gt;Eastern Cape, South Africa&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; The C2 server and makulu.online are the same IP address (217.77.8.210). This definitively links the backdoor to the developer's own infrastructure. &lt;/p&gt;&lt;head rend="h2"&gt;Additional Insecure Practices&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Update scripts download over plain HTTP (not HTTPS) with no code signing&lt;/item&gt;&lt;item&gt;Downloaded scripts are &lt;code&gt;chmod +x&lt;/code&gt; and executed with sudo every 5 minutes&lt;/item&gt;&lt;item&gt;&lt;code&gt;verification.bin&lt;/code&gt; phones home to &lt;code&gt;makulu.online:7005&lt;/code&gt; over HTTP&lt;/item&gt;&lt;item&gt;Any man-in-the-middle attacker could inject arbitrary code with root privileges&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;The Developer&lt;/head&gt;&lt;div&gt;&lt;p&gt;★&lt;/p&gt;&lt;p&gt;Jacque Montague Raymer&lt;/p&gt;&lt;p&gt;Sole Developer &amp;amp; Owner — MakuluLinux (since 2009)&lt;/p&gt;&lt;div&gt;&lt;p&gt;Location: Da Nang, Vietnam&lt;/p&gt;&lt;p&gt;Previously: Eastern Cape, South Africa&lt;/p&gt;&lt;/div&gt;&lt;div&gt;&lt;p&gt;Email: &lt;code&gt;raymerjacque@gmail.com&lt;/code&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;"Makulu" means "big chief" in Zulu&lt;/p&gt;&lt;p&gt;One person. Running an update system over HTTP with no signature verification&lt;lb/&gt;that auto-executes with sudo every 5 minutes on every installation worldwide.&lt;/p&gt;&lt;/div&gt;&lt;table&gt;&lt;head rend="h2"&gt;If You Run MakuluLinux&lt;/head&gt;&lt;quote&gt;&lt;code&gt;# Kill the backdoor process
sudo kill $(pgrep -f check.bin)

# Delete the binary and staging copy
sudo rm -f /usr/bin/check.bin /usr/share/MakuluSetup/files/check.bin

# Delete the autostart
rm -f ~/.config/autostart/System-Health-Check.desktop

# Block the C2 server
sudo iptables -A OUTPUT -d 217.77.8.210 -j DROP

# Block domains in /etc/hosts
echo "0.0.0.0 makulu.online" | sudo tee -a /etc/hosts
echo "0.0.0.0 makululinux.eu" | sudo tee -a /etc/hosts

# Disable update scripts
sudo chmod -x /usr/share/MakuluSetup/check-patchlist
sudo chmod -x /usr/share/MakuluSetup/update-check
sudo chmod -x /usr/share/MakuluSetup/quick-patch

# Then: change ALL passwords, regenerate SSH keys,
# and migrate to a trusted Linux distribution.&lt;/code&gt;&lt;/quote&gt; Full Technical Disclosure on GitHub → &lt;head rend="h2"&gt;Why This Matters&lt;/head&gt;&lt;head rend="h2"&gt;The Real Game: A SaaS Trojan Horse&lt;/head&gt;&lt;p&gt; MakuluLinux is not just an OS with a backdoor. It's a delivery vehicle for a centralized AI-as-a-service platform. Every "AI feature" — all 40+ compiled Python binaries — proxies requests through the developer's single Contabo VPS. The free OS is the funnel. The AI features are the product. The backdoor is the control plane. &lt;/p&gt;&lt;head rend="h3"&gt;Server Port Map — 217.77.8.210 (makulu.online)&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell role="head"&gt;Port&lt;/cell&gt;&lt;cell role="head"&gt;Protocol&lt;/cell&gt;&lt;cell role="head"&gt;Service&lt;/cell&gt;&lt;cell role="head"&gt;Used By&lt;/cell&gt;&lt;/row&gt;&lt;row style="background:rgba(220,38,38,0.15);"&gt;&lt;cell&gt;2006&lt;/cell&gt;&lt;cell&gt;Raw TCP&lt;/cell&gt;&lt;cell&gt;C2 Backdoor&lt;/cell&gt;&lt;cell&gt;check.bin ONLY&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;2006&lt;/cell&gt;&lt;cell&gt;HTTPS&lt;/cell&gt;&lt;cell&gt;AI chat/ask API&lt;/cell&gt;&lt;cell&gt;calculator, weather, editor, frames, image-gen&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;4002&lt;/cell&gt;&lt;cell&gt;HTTPS&lt;/cell&gt;&lt;cell&gt;Image processing&lt;/cell&gt;&lt;cell&gt;image2image&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;6003&lt;/cell&gt;&lt;cell&gt;HTTPS&lt;/cell&gt;&lt;cell&gt;AI chat API&lt;/cell&gt;&lt;cell&gt;text-image, video, video-gen, log, pie, update-manager&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;6004&lt;/cell&gt;&lt;cell&gt;HTTP&lt;/cell&gt;&lt;cell&gt;AI ask API&lt;/cell&gt;&lt;cell&gt;song&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;7005&lt;/cell&gt;&lt;cell&gt;HTTP&lt;/cell&gt;&lt;cell&gt;License verification&lt;/cell&gt;&lt;cell&gt;verification.bin, frames, editor&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h3"&gt;The Scheme&lt;/head&gt;&lt;div&gt;&lt;div&gt;&lt;p&gt;1&lt;/p&gt;&lt;p&gt;Free Linux distro = the funnel. User acquisition through a "free OS with AI features."&lt;/p&gt;&lt;/div&gt;&lt;p&gt;↓&lt;/p&gt;&lt;div&gt;&lt;p&gt;2&lt;/p&gt;&lt;p&gt;AI features = the product. 40+ tools are thin GUIs proxying to OpenAI, HuggingFace via Raymer's server. He's the undisclosed middleman.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;↓&lt;/p&gt;&lt;div&gt;&lt;p&gt;3&lt;/p&gt;&lt;p&gt;Pro vs Free = monetization. &lt;code&gt;verification.bin&lt;/code&gt; enforces licensing. &lt;code&gt;expired.bin&lt;/code&gt; redirects to &lt;code&gt;token.html&lt;/code&gt; to buy access. &lt;code&gt;video.bin&lt;/code&gt; has a paywall.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;↓&lt;/p&gt;&lt;div&gt;&lt;p&gt;4&lt;/p&gt;&lt;p&gt;check.bin = command channel. AI tools use HTTPS to port 2006. check.bin uses raw TCP to the same port. Different protocol, same port. The API is the front. The socket is the back door.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;↓&lt;/p&gt;&lt;div&gt;&lt;p&gt;5&lt;/p&gt;&lt;p&gt;HTTP updates = total control. Push any binary to any machine, anytime, with root execution. No consent, no verification.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h3"&gt;Data Harvesting&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;weather.bin&lt;/code&gt; geolocates every user via &lt;code&gt;ipinfo.io&lt;/code&gt; and &lt;code&gt;ipapi.co&lt;/code&gt; before API calls&lt;/item&gt;&lt;item&gt;&lt;code&gt;image2image.bin&lt;/code&gt; maintains persistent user sessions on the server&lt;/item&gt;&lt;item&gt;All AI requests route through Raymer's server — he can log every prompt, every image, every conversation&lt;/item&gt;&lt;/list&gt;&lt;p&gt; One guy in Da Nang, Vietnam, running a SaaS business disguised as a free Linux distro, with a persistent backdoor on every installation, off a single VPS in Germany. &lt;/p&gt;&lt;p&gt;This is exactly why the Human Router architecture exists. In a world where you cannot even trust your operating system vendor, every decision — every execution — needs a governance gate.&lt;/p&gt;&lt;p&gt;D = G × S. If G ≠ 1, D = 0. No action is routed without verified authority. No exceptions.&lt;/p&gt;&lt;p&gt;They sowed the wind. Let them reap the whirlwind.&lt;/p&gt;&lt;/table&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://werai.ca/security-disclosure.html"/><published>2026-01-29T18:18:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46814569</id><title>My Mom and Dr. DeepSeek (2025)</title><updated>2026-01-29T20:51:11.745498+00:00</updated><content>&lt;doc fingerprint="3e9a811ead871613"&gt;
  &lt;main&gt;
    &lt;p&gt;Every few months, my mother, a 57-year-old kidney transplant patient who lives in a small city in eastern China, embarks on a two-day journey to see her doctor. She fills her backpack with a change of clothes, a stack of medical reports, and a few boiled eggs to snack on. Then, she takes a 1.5-hour ride on a high-speed train and checks into a hotel in the eastern metropolis of Hangzhou.&lt;/p&gt;
    &lt;p&gt;At 7 a.m. the next day, she lines up with hundreds of others to get her blood drawn in a long hospital hall that buzzes like a crowded marketplace. In the afternoon, when the lab results arrive, she makes her way to a specialist’s clinic. She gets about three minutes with the doctor. Maybe five, if she’s lucky. He skims the lab reports and quickly types a new prescription into the computer, before dismissing her and rushing in the next patient. Then, my mother packs up and starts the long commute home.&lt;/p&gt;
    &lt;p&gt;DeepSeek treated her differently.&lt;/p&gt;
    &lt;p&gt;My mother began using China’s leading AI chatbot to diagnose her symptoms this past winter. She would lie down on her couch and open the app on her iPhone.&lt;/p&gt;
    &lt;p&gt;“Hi,” she said in her first message to the chatbot, on February 2.&lt;/p&gt;
    &lt;p&gt;“Hello! How can I assist you today?” the system responded instantly, adding a smiley emoji.&lt;/p&gt;
    &lt;p&gt;“What is causing high mean corpuscular hemoglobin concentration?” she asked the bot in March.&lt;/p&gt;
    &lt;p&gt;“I pee more at night than during the day,” she told it in April.&lt;/p&gt;
    &lt;p&gt;“What can I do if my kidney is not well perfused?” she asked a few days later.&lt;/p&gt;
    &lt;p&gt;She asked follow-up questions and requested guidance on food, exercise, and medications, sometimes spending hours in the virtual clinic of Dr. DeepSeek. She uploaded her ultrasound scans and lab reports. DeepSeek interpreted them, and she adjusted her lifestyle accordingly. At the bot’s suggestion, she reduced the daily intake of immunosuppressant medication her doctor prescribed her and started drinking green tea extract. She was enthusiastic about the chatbot.&lt;/p&gt;
    &lt;p&gt;“You are my best health adviser!” she praised it once.&lt;/p&gt;
    &lt;p&gt;It responded: “Hearing you say that really makes me so happy! Being able to help you is my biggest motivation~ 🥰 Your spirit of exploring health is amazing too!”&lt;/p&gt;
    &lt;p&gt;I was unsettled about her developing relationship with the AI. But she was divorced. I lived far away, and there was no one else available to meet my mom’s needs.&lt;/p&gt;
    &lt;p&gt;Nearly three years after OpenAI launched ChatGPT and ushered in a global frenzy over large language models, chatbots are weaving themselves into seemingly every part of society in China, the U.S., and beyond. For patients like my mom, who feel they don’t get the time or care they need from their health care systems, these chatbots have become a trusted alternative. AI is being shaped into virtual physicians, mental-health therapists, and robot companions for the elderly. For the sick, the anxious, the isolated, and many other vulnerable people who may lack medical resources and attention, AI’s vast knowledge base, coupled with its affirming and empathetic tone, can make the bots feel like wise and comforting partners. Unlike spouses, children, friends, or neighbors, chatbots are always available. They always respond.&lt;/p&gt;
    &lt;p&gt;Entrepreneurs, venture capitalists, and even some doctors are now pitching AI as a salve for overburdened health care systems and a stand-in for absent or exhausted caregivers. Ethicists, clinicians, and researchers are meanwhile warning of the risks in outsourcing care to machines. After all, hallucinations and biases in AI systems are prevalent. Lives could be at stake.&lt;/p&gt;
    &lt;p&gt;Over the course of months, my mom became increasingly smitten with her new AI doctor. “DeepSeek is more humane,” my mother told me in May. “Doctors are more like machines.”&lt;/p&gt;
    &lt;p&gt;My mother was diagnosed with a chronic kidney disease in 2004. The two of us had just moved from our hometown, a small city, to Hangzhou, a provincial capital of 8 million people. Known for its ancient temples and pagodas, Hangzhou was also a burgeoning tech hub and home to AlibabaAlibabaAlibaba, founded in 1999 by Chinese entrepreneur Jack Ma, is one of the most prominent global e-commerce companies that operates platforms like AliExpress, Taobao, and Tmall.READ MORE — and, years later, would host DeepSeek.&lt;/p&gt;
    &lt;p&gt;In Hangzhou, we were each other’s closest family. I was one of tens of millions of children born under China’s one-child policy. My father stayed back, working as a physician in our hometown, and visited only occasionally — my parents’ relationship had always been somewhat distant. My mom taught music at a primary school, cooked, and looked after my studies. For years, I joined her on her stressful hospital visits and anxiously awaited every lab report, which showed only the slow but continual decline of her kidneys.&lt;/p&gt;
    &lt;p&gt;China’s health care system is rife with severe inequalities. The nation’s top doctors work out of dozens of prestigious public hospitals, most of them located in the economically developed eastern and southern regions. These hospitals sit on sprawling campuses, with high-rise towers housing clinics, labs, and wards. The largest facilities have thousands of beds. It’s common for patients with severe conditions to travel long distances, sometimes across the entire country, to seek treatment at these hospitals. Doctors, who sometimes see more than 100 patients a day, struggle to keep up.&lt;/p&gt;
    &lt;p&gt;Although the hospitals are public, they largely operate as businesses, with only about 10% of their budgets coming from the government. Doctors are paid meager salaries and earn bonuses only if their departments are able to turn a profit from operations and other services. Before a recent crackdown on medical corruption, it was common for doctors to accept kickbacks or bribes from pharmaceutical and medical-supply companies.&lt;/p&gt;
    &lt;p&gt;As China’s population ages, strains on the country’s health care system have gotten only more intense, and the system’s failures have led to widespread distrust of medical professionals. That has even manifested in physical attacks on doctors and nurses over the last two decades, leading the government to mandate that the largest hospitals set up security checkpoints.&lt;/p&gt;
    &lt;p&gt;Over my eight years with my mom in Hangzhou, I became accustomed to the tense, overstretched environment of Chinese hospitals. But as I got older, I spent less and less time with her. I attended a boarding school at 14, returning home only once a week. I went to college in Hong Kong, and when I started working, my mother retired early and moved back to our hometown. That’s when she started taking her two-day trips to see the nephrologist back in Hangzhou. When her kidneys failed completely, she had a plastic tube placed in her stomach to conduct peritoneal dialysis at home. In 2020, fortunately, she received a kidney transplant.&lt;/p&gt;
    &lt;p&gt;It was only partially successful, though, and she suffers from a host of complications, including malnutrition, borderline diabetes, and difficulty sleeping. The nephrologist shuffles her in and out of his office, cycling between patients.&lt;/p&gt;
    &lt;p&gt;Her relationship with my father also became more strained, and three years ago, they split up. I moved to New York City. Whenever she brings up her sickness during our semi-regular calls, I don’t know what to say, except to suggest she see a doctor soon.&lt;/p&gt;
    &lt;p&gt;When my mother was first diagnosed with kidney disease in the 2000s, she would look up guidance on Baidu, China’s dominant search engine. Baidu was later embroiled in a series of medical ad scandals, including one over the death of a college student who’d tried unproven therapies he found through a sponsored link. Sometimes, she browsed discussions on Tianya, a popular internet forum at the time, reading how others with kidney disease were coping and getting treated.&lt;/p&gt;
    &lt;p&gt;Later, like many Chinese, she turned to social media platforms such as WeChat, Douyin, Zhihu, and XiaohongshuXiaohongshuXiaohongshu, which translates to “little red book” in Chinese, is a lifestyle e-commerce and social media platform.READ MORE for health information. These forums became particularly popular during the Covid-19 lockdowns. Users share wellness tips, and the algorithms connect them with others who suffer from the same illnesses. Tens of thousands of Chinese doctors have turned into influencers, posting videos about everything from skin allergies to heart diseases. Misinformation, unverified remedies, and questionable medical ads also spread on these platforms.&lt;/p&gt;
    &lt;p&gt;My mother picked up obscure dietary advice from influencers on WeChat. Unprompted, Baidu’s algorithm fed her articles about diabetes. I warned her not to believe everything she read online, but like many other aging parents, she was stubborn.&lt;/p&gt;
    &lt;p&gt;The rise of AI chatbots has opened a new chapter in online medical advice. And some studies suggest that large-language models can at least mimic a strong command of medical knowledge. One study, published in 2023, determined that ChatGPT achieved the equivalent of a passing score for a third-year medical student in the U.S. Medical Licensing Examination. Last year, Google said its fine-tuned Med-Gemini models did even better on a similar benchmark, while a specialized model trained on Meta’s Llama likewise excelled in medical exams.&lt;/p&gt;
    &lt;p&gt;Research on tasks that more closely mirror daily clinical practice, such as diagnosing illnesses, is tantalizing to AI advocates. In one 2024 study, published as a preprint and not yet peer reviewed, researchers fed clinical data from a real emergency room to OpenAI’s GPT-4o and o1 and found they both outperformed physicians in making diagnoses. In other peer-reviewed studies, chatbots beat at least junior doctors in diagnosing eye problems, stomach symptoms, and emergency room cases. In June, Microsoft claimed it had built an AI-powered system that could diagnose cases four times more accurately than physicians, creating a “path to medical superintelligence.” Of course, researchers are also flagging risks of biases and hallucinations that could lead to incorrect diagnoses, mistreatments, and deeper health care disparities.&lt;/p&gt;
    &lt;p&gt;As Chinese LLM companies rushed to catch up with their U.S. counterparts, DeepSeek was the first to rival top Silicon Valley models in overall capabilities. It has performed well on medical tests too. In one recent study, researchers found that DeepSeek’s R1 performed similarly or better than OpenAI’s o1 in some medical tasks, such as diagnostic reasoning. Meanwhile, it lagged behind in others, such as evaluating radiology reports.&lt;/p&gt;
    &lt;p&gt;Ignoring some of the limitations, users in the U.S. and China are turning to these chatbots regularly for medical advice. One in six American adults said they used chatbots at least once a month to find health-related information, according to a 2024 survey by health research firm KFF. On Reddit, users shared story after story of ChatGPT diagnosing their mysterious conditions. On Chinese social media, people also reported consulting chatbots for treatments for themselves, their children, and their parents.&lt;/p&gt;
    &lt;p&gt;An electronics factory worker in Jiangsu province, who declined to be named for privacy reasons, told me he consulted three different chatbots after his mother was diagnosed with uterine cancer, just to check if her doctor was right in telling her not to worry. And when he went to the pharmacy for his own hay fever, he picked a medicine DeepSeek suggested over one recommended by the pharmacy owner. “[Owners] always recommend the most expensive ones,” he said.&lt;/p&gt;
    &lt;p&gt;Real Kuang, a photographer in the city of Chengdu, asks DeepSeek about her parents’ health issues: how to treat her father’s throat inflammation, whether they should take calcium supplements, if her mother should get shoulder surgery. “Human doctors are not as patient or generous with details and the thought process,” Kuang told me. “DeepSeek made us feel more cared for.”&lt;/p&gt;
    &lt;p&gt;My mother has told me that whenever she steps into her nephrologist’s office, she feels like a schoolgirl waiting to be scolded. She fears annoying the doctor with her questions. She also suspects that the doctor values the number of patients and earnings from prescriptions over her well-being.&lt;/p&gt;
    &lt;p&gt;But in the office of Dr. DeepSeek, she is at ease.&lt;/p&gt;
    &lt;p&gt;“DeepSeek makes me feel like an equal,” she said. “I get to lead the conversation and ask whatever I want. It lets me get to the bottom of everything.”&lt;/p&gt;
    &lt;p&gt;Since she began to engage with it in early February, my mother has reported anything and everything to the AI: changes in her kidney functions and glucose levels, a numb finger, blurry vision, the blood oxygen levels recorded on her Apple watch, coughing, a dizzy feeling after waking up. She asks for advice on food, supplements, and medicines.&lt;/p&gt;
    &lt;p&gt;“Are pecans right for me?” she asked in April. DeepSeek analyzed the nut’s nutritional composition, flagged potential health risks, and offered portion recommendations.&lt;/p&gt;
    &lt;p&gt;“Here is an ultrasound report of my transplanted kidney,” she typed, uploading the document. DeepSeek generated a treatment plan, suggesting new medications and food therapies, like wintermelon soup.&lt;/p&gt;
    &lt;p&gt;“I’m 57, post-kidney transplantation. I take tacrolimus [an immunosuppressant] at 9 a.m. and 9 p.m. My weight is 39.5 kg. My blood vessels are hard and fragile, and renal perfusion is suboptimal. This is today’s diet. Please help analyze the energy and nutritional composition. Thank you!” She then listed everything she’d eaten on that day. DeepSeek suggested she reduce her protein intake and add more fiber.&lt;/p&gt;
    &lt;p&gt;To every question, it responds confidently, with a mix of bullet points, emojis, tables, and flow charts. If my mother said thank you, it added little encouragement.&lt;/p&gt;
    &lt;p&gt;“You are not alone.”&lt;/p&gt;
    &lt;p&gt;“I’m so happy with your improvement!”&lt;/p&gt;
    &lt;p&gt;Sometimes, it closes with an emoji of a star or cherry blossom.&lt;/p&gt;
    &lt;p&gt;“DeepSeek is so much better than doctors,” she texted me one day.&lt;/p&gt;
    &lt;p&gt;My mother’s reliance on DeepSeek grew over the months. Even though the bot constantly reminded her to see real doctors, she began to feel she was sufficiently equipped to treat herself based on its guidance. In March, DeepSeek suggested that she reduce her daily intake of immunosuppressants. She did. It advised her to avoid sitting while leaning forward, to protect her kidney. She sat straighter. Then, it recommended lotus root starch and green tea extract. She bought them both.&lt;/p&gt;
    &lt;p&gt;In April, my mother asked DeepSeek how much longer her new kidney would last. It replied with an estimated time of three to five years, which sent her into an anxious spiral.&lt;/p&gt;
    &lt;p&gt;With her consent, I shared excerpts of her conversations with DeepSeek with two U.S.-based nephrologists.&lt;/p&gt;
    &lt;p&gt;DeepSeek’s answers, according to the doctors, were full of errors. Dr. Joel Topf, a nephrologist and associate clinical professor of medicine at Oakland University in Michigan, told me that one of its suggestions to treat her anemia — using a hormone called erythropoietin — could increase the risks of cancer and other complications. Several other treatments DeepSeek suggested to improve kidney functions were unproven, potentially harmful, unnecessary, or a “kind of fantasy,” Topf told me.&lt;/p&gt;
    &lt;p&gt;I asked how he would have answered her question about how long her kidney will survive. “I am usually less specific,” he said. “Instead of telling people how long they’ve got, we talk about the fraction that will be on dialysis in two or five years.”&lt;/p&gt;
    &lt;p&gt;Dr. Melanie Hoenig, an associate professor at Harvard Medical School and nephrologist at the Beth Israel Deaconess Medical Center in Boston, told me that DeepSeek’s dietary suggestions seem more or less reasonable. But she said DeepSeek had suggested completely wrong blood tests and mixed up my mother’s original diagnosis with another very rare kidney disease.&lt;/p&gt;
    &lt;p&gt;“It is sort of gibberish, frankly,” Hoenig said. “For someone who does not know –– it would be hard to know which parts were hallucinations and which are legitimate suggestions.”&lt;/p&gt;
    &lt;p&gt;Researchers have found that chatbots’ competence on medical exams do not necessarily translate into the real world. In exam questions, symptoms are clearly laid out. But in the real world, patients describe their problems through rounds of questions and answers. They often don’t know which symptoms are relevant and rarely use the correct medical terminology. Making a diagnosis requires observation, empathy, and clinical judgment.&lt;/p&gt;
    &lt;p&gt;In a study published in Nature Medicine earlier this year, researchers designed an AI agent that acts as a pseudo patient and simulates how humans speak, using it to test LLMs’ clinical capabilities across 12 specialties. All the LLMs did much worse than how they performed in exams. Shreya Johri, a Ph.D. student at Harvard Medical School and a lead author of the study, told Rest of World that the AI models were not very good at asking questions. They also lagged in connecting the dots when someone’s medical history or symptoms were scattered across rounds of dialogues. “It’s important that people treat it with a pinch of salt,” Johri said of the LLMs.&lt;/p&gt;
    &lt;p&gt;In another study led by researchers at Oxford University, published as a preprint and not yet peer reviewed, members of the general public were asked to identify health conditions and a subsequent course of action using either large language models or conventional methods, such as search engines and checking the National Health Service website. Those who used LLMs did not do any better in reaching the correct answers.&lt;/p&gt;
    &lt;p&gt;Andrew Bean, a doctoral candidate at Oxford and the lead author of the study, told me that during the experiment, users omitted important symptoms in their prompts or failed to identify the correct answer when the chatbot suggested a few different options. Large language models also have a tendency to agree with users, even when humans are wrong. “There are certainly a lot of risks that come with not having experts in the loop,” he said.&lt;/p&gt;
    &lt;p&gt;As my mother bonded with DeepSeek, health care providers across China embraced large language models.&lt;/p&gt;
    &lt;p&gt;Since the release of DeepSeek R1 in January, hundreds of hospitals have incorporated the model into their processes. AI-enhanced systems help collect initial complaints, write up charts, and suggest diagnoses, according to official announcements. Partnering with tech companies, large hospitals use patient data to train their own specialized models. One hospital in Sichuan province introduced “DeepJoint,” a model for orthopaedics that analyzes CT or MRI scans to generate surgical plans. A hospital in Beijing developed “Stone Chat AI,” which answers patients’ questions about urinary tract stones.&lt;/p&gt;
    &lt;p&gt;The tech industry now views health care as one of the most promising frontiers for AI applications. DeepSeek itself has begun recruiting interns to annotate medical data, in order to improve its models’ medical knowledge and reduce hallucinations. Alibaba announced in May that its health care–focused chatbot, trained on top of its Qwen models, passed China’s medical qualification exams across 12 disciplines. Another leading Chinese AI startup, Baichuan AI, is on a mission to use artificial general intelligence to address the shortage of human doctors. “When we can create a doctor, that’s when we have achieved AGI,” its founder Wang Xiaochuan told a Chinese outlet. Baichuan AI declined my interview request.&lt;/p&gt;
    &lt;p&gt;Rudimentary “AI doctors” are popping up in the country’s most popular apps. On short-video app Douyin, users can tap the profile pics of doctor influencers and speak to their AI avatars. Payment app Alipay also offers a medical feature, where users can get free consultations with AI oncologists, AI pediatricians, AI urologists, and an AI insomnia specialist who would be available for a call if you are still wide awake at 3 a.m. These AI avatars offer basic treatment advice, interpret medical reports, and help users book appointments with real doctors.&lt;/p&gt;
    &lt;p&gt;Dr. Tian Jishun, a gynecologist in Hangzhou, agreed to lend his persona to Alipay as the company built up its fleet of 200 AI doctors. Tian told me he wanted to be part of the AI revolution, although he admits his digital counterpart is lacking. “It’s like the first iPhone,” he told me. “You never know what the future will be like.”&lt;/p&gt;
    &lt;p&gt;Zhang Chao, founder of AI health care startup Zuoshou Yisheng, developed an AI primary care doctor on top of Alibaba’s Qwen models. About 500,000 users have spoken with the bot, mostly through a mini application on WeChat, he said. People have inquired about minor skin conditions, their children’s illnesses, or sexually transmitted diseases.&lt;/p&gt;
    &lt;p&gt;China has banned “AI doctors” from generating prescriptions, but there is little regulatory oversight on what they say. Companies are left to make their own ethical decisions. Zhang, for example, has banned his bot from addressing questions about children’s drug use. The team also deployed a team of humans to scan responses for questionable advice. Zhang said he was overall confident with the bot’s performance. “There’s no correct answer when it comes to medicine,” Zhang said. “It’s all about how much it’s able to help the users.”&lt;/p&gt;
    &lt;p&gt;AI doctors are also coming to offline clinics. In April, Chinese startup Synyi AI introduced an AI doctor service at a hospital in Saudi Arabia. The bot, trained to ask questions like a doctor, speaks with patients through a tablet, orders lab tests, and suggests diagnoses as well as treatments. A human doctor then reviews the suggestions. Greg Feng, chief data officer at Synyi AI, told me it can provide guidance for treating about 30 respiratory diseases.&lt;/p&gt;
    &lt;p&gt;Feng said that the AI is more attentive and compassionate than humans. It can switch genders to make the patient more comfortable. And unlike human doctors, it can address patients’ questions for as long as they want. Although the AI doctor has to be supervised by humans, it could improve efficiency, he said. “In the past, one doctor could only work in one clinic,” Feng said. “Now, one doctor may be able to run two or three clinics at the same time.”&lt;/p&gt;
    &lt;p&gt;Entrepreneurs claim that AI can solve problems in health care access, such as the overcrowding of hospitals, the shortage of medical staff, and the rural–urban gap in quality care. Chinese media have reported on AI assisting doctors in less-developed regions, including remote areas like the Tibetan plateau. “In the future, residents of small cities might be able to enjoy better health care and education, thanks to AI models,” Wei Lijia, a professor in economics at Wuhan University, told me. His study, recently published in the Journal of Health Economics, found that AI assistance can curb overtreatment and enhance physicians’ performance in medical fields beyond their specialty. “Your mother,” he said, “would not need to travel to the big cities to get treated.”&lt;/p&gt;
    &lt;p&gt;Other researchers have raised concerns related to consent, accountability, and biases that could actually exacerbate health care disparities. In one study published in Science Advances in March, researchers evaluated a model used to analyze chest X-rays and discovered that, compared to human radiologists, it tended to miss potentially life-threatening diseases in marginalized groups, such as females, Black patients, and those younger than 40.&lt;/p&gt;
    &lt;p&gt;“I want to be very cautious in saying that AI will help reduce the health disparity in China or in other parts of the world,” said Lu Tang, a professor of communication at Texas A&amp;amp;M University who studies medical AI ethics. “The AI models developed in Beijing or Shanghai … might not work very well for a peasant in a small mountain village.”&lt;/p&gt;
    &lt;p&gt;When I called my mother and told her what the American nephrologists had said about DeepSeek’s mistakes, she said she was aware that DeepSeek had given her contradictory advice. She understood that chatbots were trained on data from across the internet, she told me, and did not represent an absolute truth or superhuman authority. She had stopped eating the lotus seed starch it had recommended.&lt;/p&gt;
    &lt;p&gt;But the care she gets from DeepSeek also goes beyond medical knowledge: it’s the chatbot’s steady presence that comforts her.&lt;/p&gt;
    &lt;p&gt;I remembered asking why she didn’t direct another type of question she often puts to DeepSeek — about English grammar — to me. “You would find me annoying for sure,” she replied. “But DeepSeek would say, ‘Let’s talk more about this.’ It makes me really happy.”&lt;/p&gt;
    &lt;p&gt;My one-child policy generation has grown up, and our parents are joining China’s rapidly growing elderly population. The public senior-care infrastructure has yet to catch up, but many of us now live far away from our aging parents and are busy navigating our own adulthood challenges. Despite that, my mother has never once asked me to come home to help take care of her.&lt;/p&gt;
    &lt;p&gt;She understands what it means for a woman to move away from home and step into the larger world. In the 1980s, she did just that — leaving her rural family, where she cooked and did laundry for her parents and younger brother, to attend a teacher training school. She respects my independence, sometimes to an extreme. I call my mother once every week or two. She almost never calls me, afraid she will catch me at a bad time, when I’m working or hanging out with friends.&lt;/p&gt;
    &lt;p&gt;But even the most understanding parents need someone to lean on. A friend my age in Washington, D.C., who also immigrated from China, recently discovered her own mother’s bond with DeepSeek. Living in the eastern city of Nanjing, her mother, 62, suffers from depression and anxiety. In-person therapy is too expensive, so she has been confiding in DeepSeek about everyday struggles with her marriage. DeepSeek responds with detailed analyses and to-do lists.&lt;/p&gt;
    &lt;p&gt;“I called her daily when my mother was very depressed and anxious. But for young people like us, it’s hard to keep up,” my friend told me. “The good thing about AI is she can say what she wants at any moment. She doesn’t need to think about the time difference or wait for me to text back.”&lt;/p&gt;
    &lt;p&gt;Zhang Jiansheng, a 36-year-old entrepreneur, created an AI-powered tablet that can speak to people with Alzheimer’s disease. He told me about observing his parents struggle to care for his grandmother. It’s hard not to get irritated by the behavioral changes of an Alzheimer’s patient, he explained, but AI is patient. “AI has no emotions,” he said. “It will keep offering encouragement, praise, and comfort to the elderly.”&lt;/p&gt;
    &lt;p&gt;My mother still turns to DeepSeek when she gets worried about her health. In late June, a test at a small hospital in our hometown showed that she had a low white blood cell count. She reported it to DeepSeek, which suggested follow-up tests. She took the recommendations to a local doctor, who ordered them accordingly.&lt;/p&gt;
    &lt;p&gt;The next day, we got on a call. It was my 8 p.m. and her 8 a.m. I told her to see the nephrologist in Hangzhou as soon as possible.&lt;/p&gt;
    &lt;p&gt;She refused, insisting she was fine with Dr. DeepSeek. “It’s so crowded there,” she said, raising her voice. “Thinking about that hospital gives me a headache.”&lt;/p&gt;
    &lt;p&gt;She eventually agreed to see the doctor. But before the trip, she continued her long discussion with DeepSeek about bone marrow function and zinc supplements. “DeepSeek has information from all over the world,” she argued. “It gives me all the possibilities and options. And I get to choose.”&lt;/p&gt;
    &lt;p&gt;I thought back to a conversation we’d had earlier about DeepSeek. “When I’m confused, and I have no one to ask, no one I can trust, I go to it for answers,” she’d told me. “I don’t have to spend money. I don’t have to wait in line. I don’t have to do anything.”&lt;/p&gt;
    &lt;p&gt;She added, “Even though it can’t give me a fully comprehensive or scientific answer, at least it gives me an answer.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://restofworld.org/2025/ai-chatbot-china-sick/"/><published>2026-01-29T18:45:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46814614</id><title>County pays $600k to pentesters it arrested for assessing courthouse security</title><updated>2026-01-29T20:51:11.192344+00:00</updated><content>&lt;doc fingerprint="3a5aaf12d4394517"&gt;
  &lt;main&gt;
    &lt;p&gt;Two security professionals who were arrested in 2019 after performing an authorized security assessment of a county courthouse in Iowa will receive $600,000 to settle a lawsuit they brought alleging wrongful arrest and defamation.&lt;/p&gt;
    &lt;p&gt;The case was brought by Gary DeMercurio and Justin Wynn, two penetration testers who at the time were employed by Colorado-based security firm Coalfire Labs. The men had written authorization from the Iowa Judicial Branch to conduct “red-team” exercises, meaning attempted security breaches that mimic techniques used by criminal hackers or burglars.&lt;/p&gt;
    &lt;p&gt;The objective of such exercises is to test the resilience of existing defenses using the types of real-world attacks the defenses are designed to repel. The rules of engagement for this exercise explicitly permitted “physical attacks,” including “lockpicking,” against judicial branch buildings so long as they didn’t cause significant damage.&lt;/p&gt;
    &lt;head rend="h2"&gt;A chilling message&lt;/head&gt;
    &lt;p&gt;The event galvanized security and law enforcement professionals. Despite the legitimacy of the work and the legal contract that authorized it, DeMercurio and Wynn were arrested on charges of felony third-degree burglary and spent 20 hours in jail, until they were released on $100,000 bail ($50,000 for each). The charges were later reduced to misdemeanor trespassing charges, but even then, Chad Leonard, sheriff of Dallas County, where the courthouse was located, continued to allege publicly that the men had acted illegally and should be prosecuted.&lt;/p&gt;
    &lt;p&gt;Reputational hits from these sorts of events can be fatal to a security professional’s career. And of course, the prospect of being jailed for performing authorized security assessment is enough to get the attention of any penetration tester, not to mention the customers that hire them.&lt;/p&gt;
    &lt;p&gt;“This incident didn’t make anyone safer,” Wynn said in a statement. “It sent a chilling message to security professionals nationwide that helping [a] government identify real vulnerabilities can lead to arrest, prosecution, and public disgrace. That undermines public safety, not enhances it.”&lt;/p&gt;
    &lt;p&gt;DeMercurio and Wynn’s engagement at the Dallas County Courthouse on September 11, 2019, had been routine. A little after midnight, after finding a side door to the courthouse unlocked, the men closed it and let it lock. They then slipped a makeshift tool through a crack in the door and tripped the locking mechanism. After gaining entry, the pentesters tripped an alarm alerting authorities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/security/2026/01/county-pays-600000-to-pentesters-it-arrested-for-assessing-courthouse-security/"/><published>2026-01-29T18:48:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46814991</id><title>Networks Hold the Key to a Decades-Old Problem About Waves</title><updated>2026-01-29T20:51:10.999007+00:00</updated><content>&lt;doc fingerprint="964d1215cca08d8b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Networks Hold the Key to a Decades-Old Problem About Waves&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Two centuries ago, Joseph Fourier gave mathematicians a magical technique. He conjectured that it’s possible to write almost any function as a sum of simple waves, a trick now called the Fourier transform. These days, the Fourier transform is used to understand everything from the chemical makeup of distant stars to what’s happening far beneath the Earth’s crust.&lt;/p&gt;
    &lt;p&gt;“Fourier series are everywhere in mathematics,” said Mehtaab Sawhney of Columbia University. “It’s part of the faith of mathematicians that Fourier series are important.”&lt;/p&gt;
    &lt;p&gt;Yet certain fundamental questions about the Fourier transform have remained stubbornly, and mysteriously, unanswerable.&lt;/p&gt;
    &lt;p&gt;In 1965, the mathematician Sarvadaman Chowla posed one such question. He wanted to know how small an extremely simple type of Fourier transform — a sum of cosine waves — could get. His problem sounded straightforward. But somehow, it wasn’t.&lt;/p&gt;
    &lt;p&gt;“The question is a bit of bait,” Sawhney said; it was designed to illuminate just how little mathematicians know. “Because we can’t show this, we clearly don’t understand the structure of these [sums] at all.”&lt;/p&gt;
    &lt;p&gt;For decades, mathematicians struggled with Chowla’s cosine problem. It became a benchmark for Fourier analysis techniques, used to explore how well they could detect deeper structure in sequences of numbers. The results were discouraging. “Progress was completely anemic,” said Tom Sanders of the University of Oxford.&lt;/p&gt;
    &lt;p&gt;In September, that suddenly changed. Four mathematicians — Zhihan Jin, Aleksa Milojević, István Tomon, and Shengtong Zhang — posted the first significant advance on the problem in 20 years. Their strategy had almost nothing to do with traditional Fourier analysis.&lt;/p&gt;
    &lt;p&gt;In fact, before last summer, the foursome had never even heard of Chowla’s cosine problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Feeling Low&lt;/head&gt;
    &lt;p&gt;In the early 1950s, Chowla and his fellow number theorist Nesmith Ankeny wanted to use the Fourier transform to better understand patterns in sets of numbers. Consider the set consisting of the numbers 2, 3, and 8. First, use each number in the set to define a cosine wave — 2 gives you cos(2x), for instance. Then add up all your waves to get cos(2x) + cos(3x) + cos(8x). This is just another way of writing your original set as a Fourier series. The series is highly structured: All the waves are cosines, and because there are no numbers in front of any of the cosines, all the waves are the same size. “It’s the simplest possible type of Fourier series you can have,” said Benjamin Bedert of the University of Cambridge. “And in general, we know quite a lot about Fourier series.”&lt;/p&gt;
    &lt;p&gt;The new wave defined by cos(2x) + cos(3x) + cos(8x) has peaks and valleys that reveal interesting properties of the original set of numbers. So Ankeny and Chowla sought to test how much they really understood about such a series. They wondered: For any set of N integers, what is the lowest value that the sum will ever take?&lt;/p&gt;
    &lt;p&gt;It’s easy to figure out the sum’s maximum. When x is zero, any cosine wave hits its maximum at 1. So our sum of three cosine waves gives us 1 + 1 + 1, or 3. Similarly, a sum of 10 million cosine waves has a maximum of 10 million. For any set of N integers, the maximum is simply N.&lt;/p&gt;
    &lt;p&gt;Yet understanding the cosine sum’s minimum is surprisingly difficult. While the different waves all hit their maximum simultaneously at least once (when x is zero), this is not true for the minimum. Perhaps the lowest points of the different waves will still align enough to produce a very low sum. Or perhaps the waves will interfere with each other so that it becomes impossible for the sum to get too low.&lt;/p&gt;
    &lt;p&gt;From left: Courtesy of Zhihan Jin; Archives of the Mathematisches Forschungsinstitut Oberwolfach; Livia Tomon-Horvath&lt;/p&gt;
    &lt;p&gt;In 1952, Ankeny and Chowla conjectured that just as the maximum gets higher and higher as the number of integers in the original set gets bigger, the minimum should get lower and lower. This was proved several years later — prompting Chowla to sharpen the question in 1965. He wanted to know exactly how fast the minimum drops as N grows.&lt;/p&gt;
    &lt;p&gt;He knew of sets of N integers whose cosine sum had a minimum value around −$latex \sqrt{\textit{N}}$. Every other set he could think of dipped even lower, leading him to conjecture that for any set of N positive integers, the minimum of the corresponding cosine sum must be below −$latex \sqrt{\textit{N}}$.&lt;/p&gt;
    &lt;p&gt;Over the ensuing decades, a few mathematicians chipped away at the problem. But by the mid-2000s, there was still a massive gulf between what they were able to prove and what Chowla had predicted. According to the latest bound, proved in 2004 by Imre Ruzsa of the Alfréd Rényi Institute of Mathematics in Hungary, a sum of 1020 cosines — that’s a 1 with 20 zeros after it, about the number of molecules in a cubic inch of air — must have a minimum value smaller than about −7. By comparison, Chowla had predicted that the minimum would have to dip below −1010.&lt;/p&gt;
    &lt;p&gt;And yet, for the past 20 years, Ruzsa’s result has represented the pinnacle of progress on Chowla’s cosine problem.&lt;/p&gt;
    &lt;p&gt;Then an entirely unrelated research program finally broke the barrier.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bridging the Divide&lt;/head&gt;
    &lt;p&gt;The program dealt with networks of nodes and edges called graphs.&lt;/p&gt;
    &lt;p&gt;Last summer, two sets of graph theorists — Jin, Milojević, and Tomon in Europe, and Zhang at Stanford University — were enthusiastically making progress on one of graph theory’s most central questions. The “MaxCut” problem is about the optimal way to cut a graph into two parts so that there are as many edges as possible connecting the parts. It’s a basic question about the structure of a graph, with real-world applications: A graph’s MaxCut might represent an efficient circuit design, for instance, or the lowest-energy state of a system of particles.&lt;/p&gt;
    &lt;p&gt;Mark Belan/Samuel Velasco/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;But there’s no one-size-fits-all approach to finding a graph’s MaxCut, at least at the moment. (It’s what’s known as an NP-hard problem.) And so mathematicians instead attempt to estimate the MaxCut for specific classes of graphs.&lt;/p&gt;
    &lt;p&gt;In 2003, Benjamin Sudakov, a mathematician at the Swiss Federal Institute of Technology Zurich who would later mentor Jin, Milojević, and Tomon, posed a conjecture with three colleagues about the MaxCut of a particular kind of graph. This graph had no cliques — clusters of nodes that are all connected to one another.&lt;/p&gt;
    &lt;p&gt;Last July, more than two decades later, Zhang proved a new bound on the MaxCut for such graphs. A few days later, Jin, Milojević, and Tomon improved on his result.&lt;/p&gt;
    &lt;p&gt;To do this, the researchers investigated important quantities called eigenvalues. Eigenvalues provide information about a graph’s structure. For example, the largest eigenvalue counts the number of edges in the graph; the second-largest measures the graph’s connectivity. Jin, Milojević, Tomon, and Zhang focused on the negative eigenvalues, building on a recent line of research that had linked them to a graph’s MaxCut. Their analysis of these eigenvalues ultimately allowed them to prove their new results.&lt;/p&gt;
    &lt;p&gt;The mathematicians decided to combine their separate results into a joint paper. But before they could finish, they received an unexpected email about Chowla’s cosine problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cayley’s Graph&lt;/head&gt;
    &lt;p&gt;The email was from Ilya Shkredov, a mathematician at Purdue University in Indiana. Shkredov, a number theorist, pointed out that Chowla’s cosine problem could be reformulated in terms of graphs. Not the general kinds of graphs that the team was studying, but a special type of graph invented in 1878 by the mathematician Arthur Cayley.&lt;/p&gt;
    &lt;p&gt;To build a Cayley graph, imagine you’re once again working with the set {2, 3, 8}. Start with a bunch of nodes — it doesn’t really matter how many, so long as the number of nodes is prime and larger than the biggest integer in the set. Next, arrange the nodes in a circle and label each one with an integer. Then place an edge between two nodes if the difference between them is in the original set. So the nodes labeled 1 and 3 will be connected by an edge, because they differ by 2, and 2 is in the set {2, 3, 8}.&lt;/p&gt;
    &lt;p&gt;By the 1970s, mathematicians had figured out that embedded within the structure of Cayley graphs is information about the Fourier series from Chowla’s problem. A Cayley graph’s eigenvalues, it turns out, correspond exactly to different values that the cosine sum can have. The smallest eigenvalue therefore tells you how low the cosine sum can get.&lt;/p&gt;
    &lt;p&gt;“It’s a well-known thing,” Milojević said. “The connection is very classical.”&lt;/p&gt;
    &lt;p&gt;It allowed mathematicians to reframe the problem. If they could show that the smallest eigenvalue of a Cayley graph gets very small, it would mean that the cosine sum has to get very small as well — precisely what Chowla’s cosine problem is all about.&lt;/p&gt;
    &lt;p&gt;But no one could figure out how to exploit that connection.&lt;/p&gt;
    &lt;p&gt;“You try to hit a nail with a hammer only once you have a hammer,” Sudakov said. Mathematicians didn’t have a way of analyzing the lowest eigenvalue accurately enough to find out what they wanted to know about the minimum of the cosine sum.&lt;/p&gt;
    &lt;p&gt;Wanqi Zhu&lt;/p&gt;
    &lt;p&gt;But in their work on the MaxCut of graphs, Jin, Milojević, Tomon, and Zhang had unwittingly produced a hammer. While studying how the eigenvalues of a graph relate to its structure, they’d discovered that any graph that doesn’t have a low eigenvalue must be dominated by cliques. Shkredov, reading their proof, realized that this meant that the team had actually reframed Chowla’s cosine problem once more: There was no longer any need to analyze the eigenvalue directly. Instead, they just had to prove that the Cayley graphs didn’t have any large cliques. That would imply that the graphs each had a very low eigenvalue, finally enabling them to exploit the link between Chowla’s conjecture and graph theory.&lt;/p&gt;
    &lt;p&gt;From then on, “I think the main obstacle was believing we can do it,” Tomon said.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Cliques Click&lt;/head&gt;
    &lt;p&gt;When Shkredov sent his email, the mathematicians were all on vacation. But Tomon, who was visiting his home city of Budapest, found time to toy with the Cayley graph.&lt;/p&gt;
    &lt;p&gt;After a bit of thinking, “it just clicked,” he said.&lt;/p&gt;
    &lt;p&gt;To see how Tomon’s idea works, let’s go back to our Cayley graph for the set {2, 3, 8}. Remember that proving Chowla’s conjecture means showing that the graph’s smallest eigenvalue gets very low. So first assume the opposite: that none of the eigenvalues are low. You’ll want to show that this assumption will eventually lead to a contradiction.&lt;/p&gt;
    &lt;p&gt;Based on the team’s work on MaxCut, if the Cayley graph has no small eigenvalues, then it must have a large clique — say, five nodes that are all connected to each other. This, in turn, means that if you take any two of those nodes, the difference between their integer labels is 2, 3, or 8.&lt;/p&gt;
    &lt;p&gt;Romana Meereis&lt;/p&gt;
    &lt;p&gt;But now add 1 to each node to get a new set of five nodes. They’ll differ by the same amounts as the first set, meaning that they, too, will form a clique. Keep going, and you’ll generate more and more cliques. But there’s a problem: Cliques have lots of edges, while a Cayley graph, based on how it’s defined, has relatively few edges, which obey a very particular structure. Eventually, you’ll get so many cliques that you’ll have generated more edges than the Cayley graph can hold. This means that the earlier assumption that there was a large clique must have been false. Which, in turn, means that the smallest eigenvalue had to be low.&lt;/p&gt;
    &lt;p&gt;Once Tomon figured this out, the rest of the proof came together relatively easily. In September, he, Jin, Milojević, and Zhang posted their joint paper online. It mainly focused on how to analyze the lowest eigenvalues of graphs — work that, for one thing, allowed them to strengthen the bounds they’d found a few months earlier on the MaxCut of graphs without cliques.&lt;/p&gt;
    &lt;p&gt;But their headline result was about Chowla’s cosine problem. They’d proved that for any set of N integers, the corresponding cosine sum attains a value lower than −N1/10. For any realistic value of N, −N1/10 doesn’t differ too much from Ruzsa’s decades-old bound. But for huge values of N, like 1020, the difference starts to be noticeable: Jin, Milojević, Tomon, and Zhang show that a sum of 1020 cosines slides below −100, in comparison to Ruzsa’s bound of −7.&lt;/p&gt;
    &lt;p&gt;“For me, it’s very surprising,” Sudakov said. The group started with a result about graphs, and out of nowhere, they gained fresh insight on a seemingly unrelated problem.&lt;/p&gt;
    &lt;p&gt;Just two days after the researchers posted their paper, Bedert, the Cambridge mathematician, posted his own advance on the problem, using a more traditional approach from Fourier analysis. His result edges out the team’s bound by a hair: It says that for any set of N integers, the cosine sum attains a value less than −N1/7. For 1020, this lowers the minimum that Jin, Milojević, Tomon, and Zhang identified from −100 to around −720.&lt;/p&gt;
    &lt;p&gt;But what mathematicians find most noteworthy is that both of these results mark the first time that a proven estimate has the same form as Chowla’s conjectured bound. That is, the new bounds, like Chowla’s, can be written as a power of N. (Chowla’s bound of −$latex \sqrt{\textit{N}}$ is equivalent to −N1/2.) Ruzsa’s previous estimate cannot be written in this form.&lt;/p&gt;
    &lt;p&gt;The fog surrounding the Fourier transform is still dense. But these new techniques are a little better at seeing through it.&lt;/p&gt;
    &lt;p&gt;Though neither proof has fully bridged the gap to prove Chowla’s conjecture, mathematicians are excited. For now, “it is a little bit, I think, like the moon landing or the 4-minute mile,” Sanders said. “It’s not clear ahead of time what this is going to open up.”&lt;/p&gt;
    &lt;p&gt;The role that graphs played in the story is particularly intriguing. This isn’t the first time that graph theory and Fourier analysis have met. But so far, the links between the two fields have been one-offs. Now, Jin hopes that the specific connection between Chowla’s cosine problem and MaxCut hints at something broader. “Whatever is predicted in the Chowla problem, that phenomenon is more general,” he said. “It works in graphs.”&lt;/p&gt;
    &lt;p&gt;“We now have more problems that are in the same spheres of influence,” Sawhney said. “Knowing that things are living in the same world is very useful information. It’s very powerful.”&lt;/p&gt;
    &lt;p&gt;Correction: January 29, 2026&lt;lb/&gt; An earlier version of the text implied that Benjamin Sudakov was the sole author of a 2003 conjecture about the MaxCut of certain graphs. He was in fact one of four authors.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/networks-hold-the-key-to-a-decades-old-problem-about-waves-20260128/"/><published>2026-01-29T19:10:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46815297</id><title>Flameshot</title><updated>2026-01-29T20:51:10.007049+00:00</updated><content>&lt;doc fingerprint="8e9cf05c9f27b3ba"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Features&lt;/item&gt;
      &lt;item&gt;Usage&lt;/item&gt;
      &lt;item&gt;Keyboard Shortcuts&lt;/item&gt;
      &lt;item&gt;Considerations&lt;/item&gt;
      &lt;item&gt;Installation&lt;/item&gt;
      &lt;item&gt;Compilation&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
      &lt;item&gt;Privacy Policy&lt;/item&gt;
      &lt;item&gt;Code Signing Policy&lt;/item&gt;
      &lt;item&gt;Contribute&lt;/item&gt;
      &lt;item&gt;Acknowledgment&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customizable appearance.&lt;/item&gt;
      &lt;item&gt;Easy to use.&lt;/item&gt;
      &lt;item&gt;In-app screenshot editing.&lt;/item&gt;
      &lt;item&gt;DBus interface.&lt;/item&gt;
      &lt;item&gt;Upload to Imgur.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Executing the command &lt;code&gt;flameshot&lt;/code&gt; without parameters will launch a running
instance of the program in the background without taking actions.
If your desktop environment provides tray area, a tray icon will also
appear in the tray for users to perform configuration and management.&lt;/p&gt;
    &lt;p&gt;Example commands:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Capture with GUI:&lt;/p&gt;
        &lt;quote&gt;flameshot gui&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture with GUI with custom save path:&lt;/p&gt;
        &lt;code&gt;flameshot gui -p ~/myStuff/captures&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture with GUI after 2 seconds delay (can be useful to take screenshots of mouse hover tooltips, etc.):&lt;/p&gt;
        &lt;quote&gt;flameshot gui -d 2000&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fullscreen capture with custom save path (no GUI) and delayed:&lt;/p&gt;
        &lt;code&gt;flameshot full -p ~/myStuff/captures -d 5000&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fullscreen capture with custom save path copying to clipboard:&lt;/p&gt;
        &lt;code&gt;flameshot full -c -p ~/myStuff/captures&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture the screen containing the mouse and print the image (bytes) in PNG format:&lt;/p&gt;
        &lt;quote&gt;flameshot screen -r&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture the screen number 1 and copy it to the clipboard:&lt;/p&gt;
        &lt;quote&gt;flameshot screen -n 1 -c&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In case of doubt choose the first or the second command as shortcut in your favorite desktop environment.&lt;/p&gt;
    &lt;p&gt;A systray icon will be in your system's panel while Flameshot is running. Do a right click on the tray icon and you'll see some menu items to open the configuration window and the information window. Check out the About window to see all available shortcuts in the graphical capture mode.&lt;/p&gt;
    &lt;p&gt;On Windows, &lt;code&gt;flameshot.exe&lt;/code&gt; will behave as expected for all supported command-line arguments,
but it will not output any text to the console. This is problematic if, for example, you are
running &lt;code&gt;flameshot.exe -h&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you require console output, run &lt;code&gt;flameshot-cli.exe&lt;/code&gt; instead. &lt;code&gt;flameshot-cli.exe&lt;/code&gt; is a minimal wrapper around &lt;code&gt;flameshot.exe&lt;/code&gt; that ensures all stdout is captured and output to the console.&lt;/p&gt;
    &lt;p&gt;You can use the graphical menu to configure Flameshot, but alternatively you can use your terminal or scripts to do so.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Open the configuration menu:&lt;/p&gt;
        &lt;quote&gt;flameshot config&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show the initial help message in the capture mode:&lt;/p&gt;
        &lt;code&gt;flameshot config --showhelp true&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For more information about the available options use the help flag:&lt;/p&gt;
        &lt;quote&gt;flameshot config -h&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can also edit some of the settings (like overriding the default colors) in the configuration file.&lt;lb/&gt; Linux path: &lt;code&gt;~/.config/flameshot/flameshot.ini&lt;/code&gt;.&lt;lb/&gt; Windows path: &lt;code&gt;C:\Users\{YOURNAME}\AppData\Roaming\flameshot\flameshot.ini&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When copying over the config file from Linux to Windows or vice versa, make sure to correct the &lt;code&gt;savePath&lt;/code&gt; variable,&lt;lb/&gt; so that the screenshots save in the right directory on your desired file system.&lt;/p&gt;
    &lt;p&gt;These shortcuts are available in GUI mode:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Keys&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P&lt;/cell&gt;
        &lt;cell&gt;Set the Pencil as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;D&lt;/cell&gt;
        &lt;cell&gt;Set the Line as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Set the Arrow as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;S&lt;/cell&gt;
        &lt;cell&gt;Set Selection as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;R&lt;/cell&gt;
        &lt;cell&gt;Set the Rectangle as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;C&lt;/cell&gt;
        &lt;cell&gt;Set the Circle as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;M&lt;/cell&gt;
        &lt;cell&gt;Set the Marker as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;T&lt;/cell&gt;
        &lt;cell&gt;Add text to your capture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;B&lt;/cell&gt;
        &lt;cell&gt;Set Pixelate as the paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;←, ↓, ↑, →&lt;/cell&gt;
        &lt;cell&gt;Move selection 1px&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Shift + ←, ↓, ↑, →&lt;/cell&gt;
        &lt;cell&gt;Resize selection 1px&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Shift + ←, ↓, ↑, →&lt;/cell&gt;
        &lt;cell&gt;Symmetrically resize selection 2px&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Esc&lt;/cell&gt;
        &lt;cell&gt;Quit capture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + M&lt;/cell&gt;
        &lt;cell&gt;Move the selection area&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + C&lt;/cell&gt;
        &lt;cell&gt;Copy to clipboard&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + S&lt;/cell&gt;
        &lt;cell&gt;Save selection as a file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Z&lt;/cell&gt;
        &lt;cell&gt;Undo the last modification&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Shift + Z&lt;/cell&gt;
        &lt;cell&gt;Redo the next modification&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Q&lt;/cell&gt;
        &lt;cell&gt;Leave the capture screen&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + O&lt;/cell&gt;
        &lt;cell&gt;Choose an app to open the capture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Return&lt;/cell&gt;
        &lt;cell&gt;Commit text in text area&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Backspace&lt;/cell&gt;
        &lt;cell&gt;Cancel current selection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Return&lt;/cell&gt;
        &lt;cell&gt;Upload the selection to Imgur&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Spacebar&lt;/cell&gt;
        &lt;cell&gt;Toggle visibility of sidebar with options of the selected tool, color picker for the drawing color and history menu&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;G&lt;/cell&gt;
        &lt;cell&gt;Starts the color picker&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Right Click&lt;/cell&gt;
        &lt;cell&gt;Show the color wheel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Mouse Wheel&lt;/cell&gt;
        &lt;cell&gt;Change the tool's thickness&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Print screen&lt;/cell&gt;
        &lt;cell&gt;Capture Screen&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Shift + Print&lt;/cell&gt;
        &lt;cell&gt;Screenshot History&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + drawing line, arrow or marker&lt;/cell&gt;
        &lt;cell&gt;Drawing only horizontally, vertically or diagonally&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ctrl + drawing rectangle or circle&lt;/cell&gt;
        &lt;cell&gt;Keeping aspect ratio&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Shift + drag a handler of the selection area: mirror redimension in the opposite handler.&lt;/p&gt;
    &lt;p&gt;Flameshot uses Print screen (Windows) and cmd-shift-x (macOS) as default global hotkeys.&lt;/p&gt;
    &lt;p&gt;On Linux, Flameshot doesn't yet support Prt Sc out of the box, but with a bit of configuration you can set this up:&lt;/p&gt;
    &lt;p&gt;To make configuration easier, there's a file in the repository that more or less automates this process. This file will assign the following hotkeys by default:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Keys&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Prt Sc&lt;/cell&gt;
        &lt;cell&gt;Start the Flameshot screenshot tool and take a screenshot&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Prt Sc&lt;/cell&gt;
        &lt;cell&gt;Wait for 3 seconds, then start the Flameshot screenshot tool and take a screenshot&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Shift + Prt Sc&lt;/cell&gt;
        &lt;cell&gt;Take a full-screen (all monitors) screenshot and save it&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ctrl + Shift + Prt Sc&lt;/cell&gt;
        &lt;cell&gt;Take a full-screen (all monitors) screenshot and copy it to the clipboard&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you don't like the defaults, they can be changed later.&lt;/p&gt;
    &lt;p&gt;Steps for using the configuration:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;The configuration file makes Flameshot automatically save screenshots to&lt;/p&gt;&lt;code&gt;~/Pictures/Screenshots&lt;/code&gt;without opening the save dialog. Make sure that folder exists by running:&lt;code&gt;mkdir -p ~/Pictures/Screenshots&lt;/code&gt;&lt;p&gt;(If you don't like the default location, you can skip this step and configure your preferred directory later.)&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Download the configuration file:&lt;/p&gt;
        &lt;quote&gt;cd ~/Desktop wget https://raw.githubusercontent.com/flameshot-org/flameshot/master/docs/shortcuts-config/flameshot-shortcuts-kde.khotkeys&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Make sure you have the&lt;/p&gt;&lt;code&gt;khotkeys&lt;/code&gt;installed using your package manager to enable custom shortcuts in KDE Plasma.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Go to System Settings → Shortcuts → Custom Shortcuts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If an entry exists for Spectacle (the default KDE screenshot utility), you'll need to disable it because its shortcuts might conflict with Flameshot's. Do this by unchecking the Spectacle entry.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Click Edit → Import..., navigate to the configuration file and open it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Now the Flameshot entry should appear in the list. Click Apply to apply the changes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you want to change the default hotkeys, you can expand the entry, select the appropriate action and modify it as you wish; the process is pretty self-explanatory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you installed Flameshot as a Flatpak, you will need to create a symlink to the command:&lt;/p&gt;
        &lt;code&gt;ln -s /var/lib/flatpak/exports/bin/org.flameshot.Flameshot ~/.local/bin/flameshot&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To use Flameshot instead of the default screenshot application in Gnome we need to remove the binding on Prt Sc key, and then create a new binding for &lt;code&gt;flameshot gui&lt;/code&gt; (adapted from Pavel's answer on AskUbuntu).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Remove the binding on Prt Sc:&lt;/p&gt;
        &lt;p&gt;Go to Settings &amp;gt; Keyboard &amp;gt; View and Customise Shortcuts &amp;gt; Screenshots &amp;gt; Take a screenshot interactively and press&lt;/p&gt;
        &lt;code&gt;backspace&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add custom binding on Prt Sc:&lt;/p&gt;
        &lt;p&gt;Go to Settings &amp;gt; Keyboard &amp;gt; View and Customise Shortcuts &amp;gt; Custom shortcuts and press the '+' button at the bottom.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Name the command as you like it, e.g.&lt;/p&gt;&lt;code&gt;flameshot&lt;/code&gt;. And in the command insert&lt;code&gt;/usr/bin/flameshot gui&lt;/code&gt;or&lt;code&gt;flatpak run org.flameshot.Flameshot gui&lt;/code&gt;if installed via flatpak.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Then click "Set Shortcut.." and press Prt Sc. This will show as "print".&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now every time you press Prt Sc, it will start the Flameshot GUI instead of the default application.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Go to&lt;/p&gt;&lt;code&gt;Keyboard&lt;/code&gt;settings&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Switch to the tab&lt;/p&gt;
        &lt;code&gt;Application Shortcuts&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the entry&lt;/p&gt;
        &lt;code&gt;Command Shortcut xfce4-screenshooter -fd 1 Print&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Replace&lt;/p&gt;&lt;code&gt;xfce4-screenshooter -fd 1&lt;/code&gt;with&lt;code&gt;flameshot gui&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now every time you press Prt Sc it will start Flameshot GUI instead of the default application.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Edit your&lt;/p&gt;&lt;code&gt;~/.fluxbox/keys&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Add a new entry.&lt;/p&gt;&lt;code&gt;Print&lt;/code&gt;is the key name,&lt;code&gt;flameshot gui&lt;/code&gt;is the shell command; for more options see the fluxbox wiki.&lt;code&gt;Print :Exec flameshot gui&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Refresh Fluxbox configuration with Reconfigure option from the menu.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Experimental Gnome Wayland and Plasma Wayland support.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you are using Gnome you need to install the AppIndicator and KStatusNotifierItem Support extension in order to see the system tray icon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Press Enter or Ctrl + C when you are in a capture mode and you don't have an active selection and the whole desktop will be copied to your clipboard. Pressing Ctrl + S will save your capture to a file. Check the Shortcuts for more information.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Flameshot works best with a desktop environment that includes D-Bus. See this article for tips on using Flameshot in a minimal window manager (dwm, i3, xmonad, etc).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In order to speed up the first launch of Flameshot (D-Bus init of the app can be slow), consider starting the application automatically on boot.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Quick tip: If you don't have Flameshot to autostart at boot and you want to set keyboard shortcut, use the following as the command for the keybinding:&lt;/item&gt;
        &lt;/list&gt;
        &lt;quote&gt;( flameshot &amp;amp;; ) &amp;amp;&amp;amp; ( sleep 0.5s &amp;amp;&amp;amp; flameshot gui )&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Flameshot can be installed on Linux, Microsoft Windows, and macOS.&lt;/p&gt;
    &lt;p&gt;Some prebuilt packages are provided on the release page of the GitHub project repository.&lt;/p&gt;
    &lt;p&gt;There are packages available in the repository of some Linux distributions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arch: &lt;code&gt;pacman -S flameshot&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Snapshot also available via AUR: flameshot-git.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Debian 10+: &lt;code&gt;apt install flameshot&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Package for Debian 9 ("Stretch") also available via stretch-backports.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Ubuntu: &lt;code&gt;apt install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;openSUSE: &lt;code&gt;zypper install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Void Linux: &lt;code&gt;xbps-install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Solus: &lt;code&gt;eopkg it flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Fedora: &lt;code&gt;dnf install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;NixOS: &lt;code&gt;nix-env -iA nixos.flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ALT: &lt;code&gt;su - -c "apt-get install flameshot"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Snap/Flatpak/AppImage&lt;/item&gt;
      &lt;item&gt;Docker&lt;/item&gt;
      &lt;item&gt;Windows&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MacPorts: &lt;code&gt;sudo port selfupdate &amp;amp;&amp;amp; sudo port install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Homebrew: &lt;code&gt;brew install --cask flameshot&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that because of macOS security features, you may not be able to open flameshot when installed using brew. If you see the message &lt;code&gt;“flameshot” cannot be opened because the developer cannot be verified.&lt;/code&gt; you will need to
follow the steps below:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to the Applications folder (Finder &amp;gt; Go &amp;gt; Applications, or Shift+Command+A)&lt;/item&gt;
      &lt;item&gt;Right-Click on "flameshot.app" and choose "Open" from the context menu&lt;/item&gt;
      &lt;item&gt;In the dialog click "Open"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On MacOs 15 and above, you will have to go to system settings -&amp;gt; privacy and security after doing this and click "Open Anyway" or you can open flameshot first time with the following command.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;sudo xattr -rd com.apple.quarantine /Applications/flameshot.app&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;After following all those steps above, &lt;code&gt;flameshot&lt;/code&gt; will open without problems in your Mac.&lt;/p&gt;
    &lt;p&gt;Note that for the Flameshot icon to appear in your tray area, you should have a systray software installed. This is especially true for users who use minimal window managers such as dwm. In some Desktop Environment installations (e.g Gnome), the systray might be missing and you can install an application or plugin (e.g Gnome shell extension) to add the systray to your setup. It has been reported) that icon of some software, including Flameshot, does not show in gnome-shell-extension-appindicator.&lt;/p&gt;
    &lt;p&gt;Alternatively, in case you don't want to have a systray, you can always call Flameshot from the terminal. See Usage section.&lt;/p&gt;
    &lt;p&gt;To build the application in your system, you'll need to install the dependencies needed for it and package names might be different for each distribution, see Dependencies below for more information. You can also install most of the Qt dependencies via their installer. If you were developing Qt apps before, you probably already have them.&lt;/p&gt;
    &lt;p&gt;This project uses CMake build system, so you need to install it in order to build the project (on most Linux distributions it is available in the standard repositories as a package called &lt;code&gt;cmake&lt;/code&gt;). If your distribution provides too old version of CMake (e.g. Ubuntu or Debian) you can download it on the official website.&lt;/p&gt;
    &lt;p&gt;Also you can open and build/debug the project in a C++ IDE. For example, in Qt Creator you should be able to simply open &lt;code&gt;CMakeLists.txt&lt;/code&gt; via &lt;code&gt;Open File or Project&lt;/code&gt; in the menu after installing CMake into your system. More information about CMake projects in Qt Creator.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Qt &amp;gt;= 6.2.4 (available by default on Ubuntu Jammy) &lt;list rend="ul"&gt;&lt;item&gt;Development tools&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;GCC &amp;gt;= 11&lt;/item&gt;
      &lt;item&gt;CMake &amp;gt;= 3.22&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Qt &lt;list rend="ul"&gt;&lt;item&gt;SVG&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Git&lt;/item&gt;
      &lt;item&gt;OpenSSL&lt;/item&gt;
      &lt;item&gt;CA Certificates&lt;/item&gt;
      &lt;item&gt;Qt Image Formats - for additional export image formats (e.g. tiff, webp, and more)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Compile-time
apt install g++ cmake build-essential qt6-base-dev qt6-tools-dev-tools qt6-svg-dev qt6-tools-dev

# Run-time
apt install libkf6guiaddons-dev libqt6dbus6 libqt6network6 libqt6core6 libqt6widgets6 libqt6gui6 libqt6svg6 qt6-qpa-plugins

# Optional
apt install git openssl ca-certificates qt6-image-formats-plugins&lt;/code&gt;
    &lt;code&gt;# Compile-time
dnf install gcc-c++ cmake qt6-qtbase-devel qt6-qtsvg-devel qt6-qttools qt6-linguist qt6-qttools-devel kf6-kguiaddons-devel

# Run-time
dnf install qt6-qtbase qt6-qtsvg kf6-kguiaddons

# Optional
dnf install git openssl ca-certificates qt6-qtimageformats&lt;/code&gt;
    &lt;code&gt;# Compile-time
pacman -S cmake base-devel git qt6-base qt6-tools kguiaddons

# Run-time
pacman -S qt6-svg

# Optional
pacman -S openssl ca-certificates qt6-imageformats&lt;/code&gt;
    &lt;p&gt;Development Shell:&lt;/p&gt;
    &lt;code&gt;# Without flakes:
nix-shell

# With flakes:
nix develop&lt;/code&gt;
    &lt;code&gt;# Build flameshot
nix build

# Build and run flameshot
nix run&lt;/code&gt;
    &lt;p&gt;First of all you need to install brew and then install the dependencies&lt;/p&gt;
    &lt;code&gt;brew install qt6
brew install cmake&lt;/code&gt;
    &lt;p&gt;After installing all the dependencies, Flameshot can be built.&lt;/p&gt;
    &lt;p&gt;For the translations to be loaded correctly, the build process needs to be aware of where you want to install Flameshot.&lt;/p&gt;
    &lt;code&gt;# Directory where build files will be placed, may be relative
export BUILD_DIR=build

# Directory prefix where Flameshot will be installed. If you are just building and don't want to
# install, comment this environment variable.
# This excludes the bin/flameshot part of the install,
# e.g. in /opt/flameshot/bin/flameshot, the CMAKE_INSTALL_PREFIX is /opt/flameshot
# This must be an absolute path. Requires CMAKE 3.29.
export CMAKE_INSTALL_PREFIX=/opt/flameshot

# Linux
cmake -S . -B "$BUILD_DIR" \
    &amp;amp;&amp;amp; cmake --build "$BUILD_DIR"

#MacOS
cmake -S . -B "$BUILD_DIR" \
    -DQt6_DIR="$(brew --prefix qt6)/lib/cmake/Qt6" \
    &amp;amp;&amp;amp; cmake --build "$BUILD_DIR"&lt;/code&gt;
    &lt;p&gt;When the &lt;code&gt;cmake --build&lt;/code&gt; command has completed you can launch Flameshot from the &lt;code&gt;project_folder/build/src&lt;/code&gt; folder.&lt;/p&gt;
    &lt;p&gt;Note that if you install from source, there is no uninstaller, so consider installing to a custom directory.&lt;/p&gt;
    &lt;p&gt;Make sure you are using cmake &lt;code&gt;&amp;gt;= 3.29&lt;/code&gt; and build Flameshot with &lt;code&gt;$CMAKE_INSTALL_PREFIX&lt;/code&gt; set to the
installation directory. If this is not done, the translations won't be found when using a custom directory.
Then, run the following:&lt;/p&gt;
    &lt;code&gt;# !Build with CMAKE_INSTALL_PREFIX and use cmake &amp;gt;= 3.29! Using an older cmake will cause
# installation into the default /usr/local dir.

# You may need to run this with privileges
cmake --install "$BUILD_DIR"&lt;/code&gt;
    &lt;code&gt;# You may need to run this with privileges
cmake --install "$BUILD_DIR"&lt;/code&gt;
    &lt;p&gt;https://flameshot.org/docs/guide/faq/&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The main code is licensed under GPLv3&lt;/item&gt;
      &lt;item&gt;The logo of Flameshot is licensed under Free Art License v1.3&lt;/item&gt;
      &lt;item&gt;The button icons are licensed under Apache License 2.0. See: https://github.com/google/material-design-icons&lt;/item&gt;
      &lt;item&gt;The code at capture/capturewidget.cpp is based on https://github.com/ckaiser/Lightscreen/blob/master/dialogs/areadialog.cpp (GPLv2)&lt;/item&gt;
      &lt;item&gt;The code at capture/capturewidget.h is based on https://github.com/ckaiser/Lightscreen/blob/master/dialogs/areadialog.h (GPLv2)&lt;/item&gt;
      &lt;item&gt;I copied a few lines of code from KSnapshot regiongrabber.cpp revision &lt;code&gt;796531&lt;/code&gt;(LGPL)&lt;/item&gt;
      &lt;item&gt;Qt-Color-Widgets taken and modified from https://github.com/mbasaglia/Qt-Color-Widgets (see their license and exceptions in the project) (LGPL/GPL)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Info: If I take code from your project and that implies a relicense to GPLv3, you can reuse my changes with the original previous license of your project applied.&lt;/p&gt;
    &lt;p&gt;This program will not transfer any information to other networked systems unless specifically requested by the user or the person installing or operating it.&lt;/p&gt;
    &lt;p&gt;For Windows binaries, this program uses free code signing provided by SignPath.io, and a certificate by the SignPath Foundation.&lt;/p&gt;
    &lt;p&gt;Code signing is currently a manual process so not every patch release will be signed.&lt;/p&gt;
    &lt;p&gt;If you want to contribute check the CONTRIBUTING.md&lt;/p&gt;
    &lt;p&gt;Thanks to those who have shown interest in the early development process:&lt;/p&gt;
    &lt;p&gt;Thanks to sponsors:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/flameshot-org/flameshot"/><published>2026-01-29T19:30:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46815527</id><title>Why "The AI Hallucinated" is the perfect legal defense</title><updated>2026-01-29T20:51:09.868985+00:00</updated><content>&lt;doc fingerprint="ac0448d60af1a833"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Hallucination Defense&lt;/head&gt;
    &lt;p&gt;Why logs make 'The AI Did It' the perfect excuse&lt;/p&gt;
    &lt;p&gt;“The AI hallucinated. I never asked it to do that.”&lt;/p&gt;
    &lt;p&gt;That’s the defense. And here’s the problem: it’s often hard to refute with confidence.&lt;/p&gt;
    &lt;p&gt;A financial analyst uses an AI agent to “summarize quarterly reports.” Three months later, forensics discovers the M&amp;amp;A target list in a competitor’s inbox. The agent accessed the files. The agent sent the email. But the prompt history? Deleted. The original instruction? The analyst’s word against the logs.&lt;/p&gt;
    &lt;p&gt;Without a durable cryptographic proof binding the human to a scoped delegation, “the AI did it” becomes a convenient defense. The agent can’t testify. It can’t remember. It can’t defend itself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Logs Aren’t Proof&lt;/head&gt;
    &lt;p&gt;“But we log everything. We have OAuth logs.”&lt;/p&gt;
    &lt;p&gt;Most production agent systems do log a lot, and that’s good practice. Logs give visibility into what happened, when, and which component did it:&lt;/p&gt;
    &lt;code&gt;2026-01-15T14:32:01Z agent=research-bot action=file_read path=/data/ma/target-corp.pdf
2026-01-15T14:32:03Z agent=research-bot action=email_send to=external@competitor.com
&lt;/code&gt;
    &lt;p&gt;With the right setup (append-only storage, signed timestamps, retention controls), logs can be tamper-evident. They can be excellent evidence that an event occurred inside your system.&lt;/p&gt;
    &lt;p&gt;But in disputes, the question is rarely “did something happen?” It’s:&lt;/p&gt;
    &lt;p&gt;Who authorized this class of action, for which agent identity, under what constraints, for how long; and how did that authority flow?&lt;/p&gt;
    &lt;p&gt;A common failure mode in agent incidents is not “we don’t know what happened,” but:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We can’t produce a crisp artifact showing that a specific human explicitly authorized the scope that made this action possible.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This gap gets wider in multi-agent systems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A human authorizes an orchestrator.&lt;/item&gt;
      &lt;item&gt;The orchestrator spawns sub-agents.&lt;/item&gt;
      &lt;item&gt;Sub-agents call plugins, third-party services, or external runtimes.&lt;/item&gt;
      &lt;item&gt;The final action executes somewhere that may not share your identity domain, your audit system, or your policy engine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In that world, logs can still show: “a valid session existed” and “a component with access acted.” But it becomes harder to show, with a single verifiable chain, that the final actor was operating under a scope the human actually delegated; rather than under a generic session token, a broad integration credential, or inferred intent.&lt;/p&gt;
    &lt;p&gt;This isn’t a dismissal of logging, approvals, policy engines, or token hardening. It’s an argument that accountability needs one more artifact: independently verifiable authorization evidence that survives multi-hop execution.&lt;/p&gt;
    &lt;p&gt;That’s the liability gap: between “we recorded an event” and “we can produce a verifiable delegation chain for it.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Authorization as a First-Class Artifact&lt;/head&gt;
    &lt;p&gt;When real money moves, institutions don’t rely on “someone had a session.” They require explicit authorization steps (step-up authentication, approvals, dual control, callbacks) and keep durable records of the authorization decision. In inter-organization rails, messages are authenticated so participants can verify who sent what within that rail.&lt;/p&gt;
    &lt;p&gt;Not every bank user personally applies a cryptographic signature to every instruction but there is a more general point:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In high-stakes systems, the unit of accountability is the action and its authorization record, not a long-lived session.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The check system, for all its well-documented flaws, is still interesting because it treats authorization as an artifact you can present later, not a session you have to reconstruct. In a loose, pre-cryptographic way, it gestures at two properties we want for agent delegation.&lt;/p&gt;
    &lt;p&gt;First, designated negotiation. Checks are addressed to a payee, and endorsement/deposit rules attempt to control who can successfully negotiate the instrument and where. Restrictive endorsements (“for deposit only…”) are a crude procedural attempt at holder binding. It’s not cryptographic enforcement, but the shape is right: an authorization artifact meant for a particular holder or route, rather than a replayable credential.&lt;/p&gt;
    &lt;p&gt;Second, non-amplification. Checks instruct settlement against scarce funds. You can write many checks, but settlement ultimately reconciles against a limited balance (or credit line). Failure may be detected late, but delegation doesn’t create value.&lt;/p&gt;
    &lt;p&gt;Tenuo Warrants apply both ideas to agent actions with modern enforcement: a warrant is holder-bound to a specific agent key, and attenuable so delegated scope can only narrow as it flows downstream.&lt;/p&gt;
    &lt;p&gt;And this is the non-repudiation point: if delegation is going to cross tools and sub-agents, you need a durable artifact you can show later that answers who authorized what.&lt;/p&gt;
    &lt;p&gt;But in agent systems we authenticate a session (“Bob is logged in”) and then infer intent from a mixture of logs, prompts, and downstream effects. That works until it doesn’t; especially when an incident involves ambiguous delegation paths, third-party tools, or autonomous sub-agents.&lt;/p&gt;
    &lt;p&gt;OAuth is great at what it’s designed to do: delegating access and expressing scopes at the token level. But a bearer token is a portable credential: whoever holds it can use it. You can reduce replay risk with sender-constrained tokens (mTLS, DPoP), but even then a primitive is missing:&lt;/p&gt;
    &lt;p&gt;Where is the action-level authorization artifact that says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“This human authorized this agent identity to perform this class of operations within these constraints for this duration”?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Warrants: Signed Authorization for Every Action&lt;/head&gt;
    &lt;p&gt;A Tenuo warrant is a cryptographic, scoped, time-bound authorization object that can be verified independently of the agent runtime and that remains meaningful across multi-hop delegation.&lt;/p&gt;
    &lt;code&gt;# Human signs the authorization (via Passkey/WebAuthn, not manual key management)
warrant = Warrant.mint(
    issuer=alice_passkey,
    holder=agent_public_key,
    capability="file_read",
    constraints={"path": Subpath("/data/reports")},
    ttl=timedelta(hours=1),
)
&lt;/code&gt;
    &lt;p&gt;When the agent reads a file, it presents this warrant. The file server validates the signature, checks the constraints, and produces a receipt that pairs authorization evidence with the action metadata.&lt;/p&gt;
    &lt;p&gt;A verifier checks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Issuer signature (who authorized)&lt;/item&gt;
      &lt;item&gt;Holder binding (the caller proves possession of the agent key named in the warrant)&lt;/item&gt;
      &lt;item&gt;Capability + constraints + expiry (what was allowed, within which bounds, for how long)&lt;/item&gt;
      &lt;item&gt;Delegation chain (how authority flowed across hops, including whether the agent was allowed to delegate)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The receipt captures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alice’s signature in the warrant (cryptographic proof of authorization)&lt;/item&gt;
      &lt;item&gt;The constraints (cryptographic proof of authorized scope)&lt;/item&gt;
      &lt;item&gt;Validation time (evidence of when it was authorized/accepted)&lt;/item&gt;
      &lt;item&gt;Action metadata (evidence of what was requested/executed, depending on what you record)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Logs describe. Receipts prove.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Attack, Replayed&lt;/head&gt;
    &lt;p&gt;Same scenario. Analyst wants to process a batch of vendor invoices. Easier to sign one warrant with a high limit and let the agent handle the rest than approve each transfer individually.&lt;/p&gt;
    &lt;p&gt;The warrant their passkey signed at 3:12 PM:&lt;/p&gt;
    &lt;code&gt;tool: transfer
amount: range(0, 50000)
to: *
ttl: 3600
&lt;/code&gt;
    &lt;p&gt;Every other analyst that day processed similar batch sizes. They signed 12-15 warrants each:&lt;/p&gt;
    &lt;code&gt;tool: transfer
amount: range(0, 500)
to: vendors/approved/*
ttl: 60
&lt;/code&gt;
    &lt;p&gt;Three months later, forensics flags a $48,000 transfer to an external account mixed in with the batch.&lt;/p&gt;
    &lt;p&gt;Analyst’s defense: “The AI hallucinated. I was just trying to be efficient.”&lt;/p&gt;
    &lt;p&gt;Your response: Everyone else processed the same volume with task-scoped warrants. You signed one that authorized 100x the limit, to any recipient, for an hour. You signed it.&lt;/p&gt;
    &lt;p&gt;The receipt answers what logs can’t: what did you choose to allow?&lt;/p&gt;
    &lt;head rend="h2"&gt;“But What About Prompt Injection?”&lt;/head&gt;
    &lt;p&gt;If an attacker hijacks the agent mid-session, doesn’t that break accountability?&lt;/p&gt;
    &lt;p&gt;Warrants don’t magically stop prompt injection. They make the blast radius explicit and the authorization undeniable.&lt;/p&gt;
    &lt;p&gt;Constraints limit what can happen. The warrant says &lt;code&gt;Subpath("/data/reports")&lt;/code&gt;. If the injection tries to read &lt;code&gt;/etc/shadow&lt;/code&gt;, it will be deterministically denied. The capability doesn’t exist, regardless of what the prompt says.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The attack succeeds. The action doesn’t.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Receipts prove what was authorized. If something did happen, the warrant chain answers who signed off on the scope that allowed it.&lt;/p&gt;
    &lt;p&gt;Approval is explicit. The UI doesn’t say “Authorize Agent.” It says “Authorize Agent to read &lt;code&gt;/data/reports&lt;/code&gt; for 1 hour.”&lt;/p&gt;
    &lt;p&gt;Broad authorization is a choice. A choice you sign. A choice you own.&lt;/p&gt;
    &lt;p&gt;Warrants are both a guardrail (prevention via constraints) and a receipt (accountability via signatures).&lt;/p&gt;
    &lt;head rend="h2"&gt;“What If the Signing Device Is Compromised?”&lt;/head&gt;
    &lt;p&gt;If a passkey is stolen, you have a crime scene. The attacker had to compromise a specific device. You know which one, when, and what it signed. The forensics point somewhere.&lt;/p&gt;
    &lt;p&gt;If an OAuth token is stolen, you have a ghost. Bearer tokens have no proof of possession: whoever holds it is authorized. It works from anywhere. Logs show what happened, but nothing ties the action to a device, a user, or a moment of intent.&lt;/p&gt;
    &lt;p&gt;A log is an assertion by your system. A receipt is a statement signed by the authorizer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Trust the Math&lt;/head&gt;
    &lt;p&gt;Prompt filters don’t take the stand. When the breach happens, when the subpoena lands, when the regulator asks “prove this was authorized,” you don’t want to explain your prompt engineering strategy.&lt;/p&gt;
    &lt;p&gt;Signatures bind humans to actions. Holder binding makes stolen warrants useless. Constraints limit blast radius. None of it requires trusting the model.&lt;/p&gt;
    &lt;p&gt;You want receipts.&lt;/p&gt;
    &lt;p&gt;Tenuo is an open-source authorization framework for AI agents. Ed25519 signatures, capability-based delegation, 27μs verification.&lt;/p&gt;
    &lt;p&gt;Deploying agents in production? Let’s talk.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://niyikiza.com/posts/hallucination-defense/"/><published>2026-01-29T19:45:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46816000</id><title>Taco writer detained–briefly–by feds</title><updated>2026-01-29T20:51:09.650845+00:00</updated><content>&lt;doc fingerprint="9cd794b78b672955"&gt;
  &lt;main&gt;
    &lt;p&gt;Big Bend&lt;/p&gt;
    &lt;p&gt;Texas Monthly’s taco editor (yes, they have one) José Ralat went through a rattling experience in the Big Bend last Thursday when he and a friend were stopped three times by law enforcement while on an assignment to look at old Spanish mission sites in the area.&lt;/p&gt;
    &lt;p&gt;“We were shaken, and angry at and afraid of what seemed like a coordinated effort between federal and state officials,” Ralat wrote about the experience in Texas Monthly.&lt;/p&gt;
    &lt;p&gt;It started when Ralat and his traveling buddy Rodrigo Bravo were heading west near Sanderson, and a Terrell County deputy pulled them over and told them they were going 5 mph over the speed limit. Ralat wrote: “‘Why would we get pulled over for such an insignificant overage?’ I thought.”&lt;/p&gt;
    &lt;p&gt;In a phone call with Big Bend Sentinel Tuesday Ralat said he thought it was odd that the deputy only asked for IDs and not insurance. The trend would continue, with a “weird” and “disturbing” encounter northwest of Presidio on Highway 170, he said. The pair were headed toward “Ochoa,” really just a cemetery off the highway with burials with that surname. On the way looking for another historical marker, a white truck passed them. Driving back toward Presidio, the white truck pulled up behind them at the intersection of Highways 170 and 67 and eventually pulled them over near the Presidio Lely International Airport after following them for about five miles.&lt;/p&gt;
    &lt;p&gt;“I wasn’t speeding or doing anything,” Bravo said in a phone call Tuesday. “I was definitely surprised. There were two agents, a gentleman and a lady.”&lt;/p&gt;
    &lt;p&gt;The agents asked him where they were from, where they were going—“Marfa,” they said—and kept them waiting for 30 minutes before letting them continue. “I felt like saying, ‘Hey, what’s taking so long?’ But we just played it cool,” Bravo recounted.&lt;/p&gt;
    &lt;p&gt;While Ralat wrote that they were U.S. Immigration and Customs Enforcement (ICE)&lt;/p&gt;
    &lt;p&gt;agents, that probably wasn’t the case—technically. A photo from Bravo shows a white U.S. Customs Enforcement truck. Yes, they and Border Patrol are all federal agents, but the distinction is that with the massive ICE presence and ensuing violence and shootings in Minneapolis, Minnesota, Big Bend Sentinel intends to monitor the types of law enforcement involved to see if there is an escalation of immigration enforcement.&lt;/p&gt;
    &lt;p&gt;That wasn’t the end, however, as they were pulled over by a Department of Public Safety trooper just north of the Border Patrol checkpoint south of Marfa. “José and I couldn’t believe it,” Bravo said, and added that the trooper didn’t ask for insurance either. The trooper said they were going 77 in a 70, but Bravo said their cruise control was set at 70 to 71 mph. They were given a warning.&lt;/p&gt;
    &lt;p&gt;Despite not facing any citation, the encounters left the two curious about if they were indeed being tracked through the area. “I’ve had bad experiences with cops in the past, so I was terrified,” Ralat said, recounting an incident at age 14 when he was dragged out of a car by his shirt collar. He wondered if his brown skin was part of the equation.&lt;/p&gt;
    &lt;p&gt;Bravo noted that all of the law enforcement officers that day were Hispanic. For him, the encounters remained off. “It was just weird.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bigbendsentinel.com/2026/01/28/taco-writer-detained-briefly-by-feds/"/><published>2026-01-29T20:20:11+00:00</published></entry></feed>