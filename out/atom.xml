<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-24T14:38:59.131190+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45352672</id><title>Qwen3-VL</title><updated>2025-09-24T14:39:08.177275+00:00</updated><link href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;from=research.latest-advancements-list"/><published>2025-09-23T20:59:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45352944</id><title>From Rust to reality: The hidden journey of fetch_max</title><updated>2025-09-24T14:39:07.908516+00:00</updated><content>&lt;doc fingerprint="1c018251a0ff3b2c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Rust to Reality: The Hidden Journey of fetch_max&lt;/head&gt;
    &lt;head rend="h2"&gt;How a Job Interview Sent Me Down a Compiler Rabbit Hole&lt;/head&gt;
    &lt;p&gt;I occasionally interview candidates for engineering roles. We need people who understand concurrent programming. One of our favorite questions involves keeping track of a maximum value across multiple producer threads - a classic pattern that appears in many real-world systems.&lt;/p&gt;
    &lt;p&gt;Candidates can use any language they want. In Java (the language I know best), you might write a CAS loop, or if you're feeling functional, use &lt;code&gt;updateAndGet()&lt;/code&gt; with a lambda:&lt;/p&gt;
    &lt;quote&gt;AtomicLong highScore = new AtomicLong(100);[...]highScore.updateAndGet(current -&amp;gt; Math.max(current, newScore));&lt;/quote&gt;
    &lt;p&gt;But that lambda is doing work - it's still looping under the hood, retrying if another thread interferes. You can see the loop right in AtomicLong's source code.&lt;/p&gt;
    &lt;p&gt;Then one candidate chose Rust.&lt;/p&gt;
    &lt;p&gt;I was following along as he started typing, expecting to see either an explicit CAS loop or some functional wrapper around one. But instead, he just wrote:&lt;/p&gt;
    &lt;quote&gt;high_score.fetch_max(new_score, Ordering::Relaxed);&lt;/quote&gt;
    &lt;p&gt;"Rust has fetch_max built in," he explained casually, moving on to the next part of the problem.&lt;/p&gt;
    &lt;p&gt;Hold on. This wasn't a wrapper around a loop pattern - this was a first-class atomic operation, sitting right there next to &lt;code&gt;fetch_add&lt;/code&gt; and &lt;code&gt;fetch_or&lt;/code&gt;. Java
doesn't have this. C++ doesn't have this. How could Rust just... have this?&lt;/p&gt;
    &lt;p&gt;After the interview, curiosity got the better of me. Why would Rust provide &lt;code&gt;fetch_max&lt;/code&gt; as a built-in intrinsic? Intrinsics usually exist to leverage
specific hardware instructions. But x86-64 doesn't have an &lt;code&gt;atomic max&lt;/code&gt;
instruction. So there had to be a CAS loop somewhere in the pipeline. Unless...
maybe some architectures do have this instruction natively? And if so, how
does the same Rust code work on both?&lt;/p&gt;
    &lt;p&gt;I had to find out. Was the loop in Rust's standard library? Was it in LLVM? Was it generated during code generation for x86-64?&lt;/p&gt;
    &lt;p&gt;So I started digging. What I found was a fascinating journey through five distinct layers of compiler transformations, each one peeling back another level of abstraction, until I found exactly where that loop materialized. Let me share what I discovered.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 1: The Rust Code&lt;/head&gt;
    &lt;p&gt;Let's start with what that candidate wrote - a simple high score tracker that can be safely updated from multiple threads:&lt;/p&gt;
    &lt;quote&gt;use std::sync::atomic::{AtomicU64, Ordering};fn main() {let high_score = AtomicU64::new(100);// [...]// Another thread reports a new score of 200let _old_score = high_score.fetch_max(200, Ordering::Relaxed);// [...]}// Save this snippet as `main.rs` we are going to use it later.&lt;/quote&gt;
    &lt;p&gt;This single line does exactly what it promises: atomically fetches the current value, compares it with the new one, updates it if the new value is greater, and returns the old value. It's safe, concise, and impossible to mess up. No explicit loops, no retry logic visible anywhere. But how does it actually work under the hood?&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 2: The Macro Expansion&lt;/head&gt;
    &lt;p&gt;Before our &lt;code&gt;fetch_max&lt;/code&gt; call even reaches anywhere close to machine code generation,
there's another layer of abstraction at work. The &lt;code&gt;fetch_max&lt;/code&gt; method isn't hand-written
for each atomic type - it's generated by a Rust macro called &lt;code&gt;atomic_int!&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If we peek into Rust's standard library source code, we find that &lt;code&gt;AtomicU64&lt;/code&gt;
and all its methods are actually created by
this macro:&lt;/p&gt;
    &lt;quote&gt;atomic_int! {cfg(target_has_atomic = "64"),// ... various configuration attributes ...atomic_umin, atomic_umax, // The intrinsics to use8, // Alignmentu64 AtomicU64 // The type to generate}&lt;/quote&gt;
    &lt;p&gt;Inside this macro, &lt;code&gt;fetch_max&lt;/code&gt; is defined as a
template
that works for any integer type:&lt;/p&gt;
    &lt;quote&gt;pub fn fetch_max(&amp;amp;self, val: $int_type, order: Ordering) -&amp;gt; $int_type {// SAFETY: data races are prevented by atomic intrinsics.unsafe { $max_fn(self.v.get(), val, order) }}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;$max_fn&lt;/code&gt; placeholder gets replaced with &lt;code&gt;atomic_umax&lt;/code&gt; for unsigned types
and &lt;code&gt;atomic_max&lt;/code&gt; for signed types. This single macro definition generates
&lt;code&gt;fetch_max&lt;/code&gt; methods for &lt;code&gt;AtomicI8&lt;/code&gt;, &lt;code&gt;AtomicU8&lt;/code&gt;, &lt;code&gt;AtomicI16&lt;/code&gt;, &lt;code&gt;AtomicU16&lt;/code&gt;, and so
on - all the way up to &lt;code&gt;AtomicU128&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So our simple &lt;code&gt;fetch_max&lt;/code&gt; call is actually invoking generated code. But what
does the &lt;code&gt;atomic_umax&lt;/code&gt; function actually do? To answer that, we need
to see what the Rust compiler produces next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 3: LLVM IR&lt;/head&gt;
    &lt;p&gt;Now that we know &lt;code&gt;fetch_max&lt;/code&gt; is macro-generated code calling &lt;code&gt;atomic_umax&lt;/code&gt;,
let's see what happens when the Rust compiler processes it. The compiler
doesn't go straight to assembly. First, it translates the code into an
intermediate representation. Rust uses the LLVM compiler project, so it
generates LLVM Intermediate Representation (IR).&lt;/p&gt;
    &lt;p&gt;If we peek at the LLVM IR for our &lt;code&gt;fetch_max&lt;/code&gt; call, we see something like this:&lt;/p&gt;
    &lt;quote&gt;; Before the transformationbb7:%0 = atomicrmw umax ptr %self, i64 %val monotonic, align 8...&lt;/quote&gt;
    &lt;p&gt;This is LLVM's language for saying: "I need an atomic read-modify-write operation. The modification I want to perform is an unsigned maximum."&lt;/p&gt;
    &lt;p&gt;This is a powerful, high-level instruction within the compiler itself. But it poses a critical question: does the CPU actually have a single instruction called &lt;code&gt;umax&lt;/code&gt;? For most architectures, the answer is no. So how does the
compiler bridge this gap?&lt;/p&gt;
    &lt;head rend="h3"&gt;How to See This Yourself&lt;/head&gt;
    &lt;p&gt;My goal is not to merely describe what is happening, but to give you the tools to see it for yourself. You can trace this transformation step-by-step on your own machine.&lt;/p&gt;
    &lt;p&gt;First, tell the Rust compiler to stop after generating the LLVM IR:&lt;/p&gt;
    &lt;quote&gt;rustc --emit=llvm-ir main.rs&lt;/quote&gt;
    &lt;p&gt;This creates a &lt;code&gt;main.ll&lt;/code&gt; file. This file contains the LLVM IR
representation of your Rust code, including our &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction.
Keep the file around; we'll use it in the next steps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Interlude: Compiler Intrinsics&lt;/head&gt;
    &lt;p&gt;We're missing something important. How does the Rust function &lt;code&gt;atomic_umax&lt;/code&gt;
actually become the LLVM instruction &lt;code&gt;atomicrmw umax&lt;/code&gt;? This is where compiler
intrinsics come into play.&lt;/p&gt;
    &lt;p&gt;If you dig into Rust's source code, you'll find that &lt;code&gt;atomic_umax&lt;/code&gt; is
defined like this:&lt;/p&gt;
    &lt;quote&gt;/// Updates `*dst` to the max value of `val` and the old value (unsigned comparison)#[inline]#[cfg(target_has_atomic)]#[cfg_attr(miri, track_caller)] // even without panics, this helps for Miri backtracesunsafe fn atomic_umax&amp;lt;T: Copy&amp;gt;(dst: *mut T, val: T, order: Ordering) -&amp;gt; T {// SAFETY: the caller must uphold the safety contract for `atomic_umax`unsafe {match order {Relaxed =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Relaxed }&amp;gt;(dst, val),Acquire =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Acquire }&amp;gt;(dst, val),Release =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Release }&amp;gt;(dst, val),AcqRel =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::AcqRel }&amp;gt;(dst, val),SeqCst =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::SeqCst }&amp;gt;(dst, val),}}}&lt;/quote&gt;
    &lt;p&gt;But what is this &lt;code&gt;intrinsics::atomic_umax&lt;/code&gt; function? If you look at its
definition,
you find something slightly unusual:&lt;/p&gt;
    &lt;quote&gt;/// Maximum with the current value using an unsigned comparison./// `T` must be an unsigned integer type.////// The stabilized version of this intrinsic is available on the/// [`atomic`] unsigned integer types via the `fetch_max` method. For example, [`AtomicU32::fetch_max`].#[rustc_intrinsic]#[rustc_nounwind]pub unsafe fn atomic_umax&amp;lt;T: Copy, const ORD: AtomicOrdering&amp;gt;(dst: *mut T, src: T) -&amp;gt; T;&lt;/quote&gt;
    &lt;p&gt;There is no body. This is a declaration, not a definition. The &lt;code&gt;#[rustc_intrinsic]&lt;/code&gt; attribute tells the Rust compiler that this function
maps directly to a low-level operation understood by the compiler
itself. When the Rust compiler sees a call to &lt;code&gt;intrinsics::atomic_umax&lt;/code&gt;, it
knows to
replace it
with the corresponding
LLVM intrinsic function.&lt;/p&gt;
    &lt;p&gt;So our journey actually looks like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;fetch_max&lt;/code&gt;method (user-facing API)&lt;/item&gt;
      &lt;item&gt;Macro expands to call &lt;code&gt;atomic_umax&lt;/code&gt;function&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;atomic_umax&lt;/code&gt;is a compiler intrinsic&lt;/item&gt;
      &lt;item&gt;Rustc replaces the intrinsic with LLVM's &lt;code&gt;atomicrmw umax&lt;/code&gt;← We are here&lt;/item&gt;
      &lt;item&gt;LLVM processes this instruction...&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Layer 4: The Transformation&lt;/head&gt;
    &lt;p&gt;LLVM runs a series of "passes" that analyze and transform the code. The one we're interested in is called the &lt;code&gt;AtomicExpandPass&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Its job is to look at high-level atomic operations like &lt;code&gt;atomicrmw umax&lt;/code&gt; and ask
the target architecture, "Can you do this natively?"&lt;/p&gt;
    &lt;p&gt;When the &lt;code&gt;x86-64&lt;/code&gt; backend says "No, I can't," this pass expands the single
instruction into a sequence of more fundamental ones that the CPU does
understand. The result is a
compare-and-swap (CAS) loop.&lt;/p&gt;
    &lt;p&gt;We can see this transformation in action by asking LLVM to emit the intermediate representation before and after this pass. To see the IR before the &lt;code&gt;AtomicExpandPass&lt;/code&gt;, run:&lt;/p&gt;
    &lt;quote&gt;llc -print-before=atomic-expand main.ll -o /dev/null&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;Tip: If you do not have&lt;/p&gt;&lt;code&gt;llc&lt;/code&gt;installed, you can ask&lt;code&gt;rustc&lt;/code&gt;to run the pass for you directly.&lt;code&gt;rustc -C llvm-args="-print-before=atomic-expand -print-after=atomic-expand" main.rs&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;The code will be printed to your terminal. The function containing our atomic max looks like this:&lt;/p&gt;
    &lt;quote&gt;*** IR Dump Before Expand Atomic instructions (atomic-expand) ***; Function Attrs: inlinehint nonlazybind uwtabledefine internal i64 @_ZN4core4sync6atomic9AtomicU649fetch_max17h6c42d6f2fc1a6124E(ptr align 8 %self, i64 %val, i8 %0) unnamed_addr #1 {start:%_0 = alloca [8 x i8], align 8%order = alloca [1 x i8], align 1store i8 %0, ptr %order, align 1%1 = load i8, ptr %order, align 1%_7 = zext i8 %1 to i64switch i64 %_7, label %bb2 [i64 0, label %bb7i64 1, label %bb5i64 2, label %bb6i64 3, label %bb4i64 4, label %bb3]bb2: ; preds = %startunreachablebb7: ; preds = %start%2 = atomicrmw umax ptr %self, i64 %val monotonic, align 8store i64 %2, ptr %_0, align 8br label %bb1bb5: ; preds = %start%3 = atomicrmw umax ptr %self, i64 %val release, align 8store i64 %3, ptr %_0, align 8br label %bb1bb6: ; preds = %start%4 = atomicrmw umax ptr %self, i64 %val acquire, align 8store i64 %4, ptr %_0, align 8br label %bb1bb4: ; preds = %start%5 = atomicrmw umax ptr %self, i64 %val acq_rel, align 8store i64 %5, ptr %_0, align 8br label %bb1bb3: ; preds = %start%6 = atomicrmw umax ptr %self, i64 %val seq_cst, align 8store i64 %6, ptr %_0, align 8br label %bb1bb1: ; preds = %bb3, %bb4, %bb6, %bb5, %bb7%7 = load i64, ptr %_0, align 8ret i64 %7}&lt;/quote&gt;
    &lt;p&gt;You can see the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction in multiple places, depending on
the memory ordering specified. This is the high-level atomic operation that the
compiler backend understands, but the CPU does not.&lt;/p&gt;
    &lt;quote&gt;llc -print-after=atomic-expand main.ll -o /dev/null&lt;/quote&gt;
    &lt;p&gt;This is the relevant part of the output:&lt;/p&gt;
    &lt;quote&gt;*** IR Dump After Expand Atomic instructions (atomic-expand) ***; Function Attrs: inlinehint nonlazybind uwtabledefine internal i64 @_ZN4core4sync6atomic9AtomicU649fetch_max17h6c42d6f2fc1a6124E(ptr align 8 %self, i64 %val, i8 %0) unnamed_addr #1 {start:%_0 = alloca [8 x i8], align 8%order = alloca [1 x i8], align 1store i8 %0, ptr %order, align 1%1 = load i8, ptr %order, align 1%_7 = zext i8 %1 to i64switch i64 %_7, label %bb2 [i64 0, label %bb7i64 1, label %bb5i64 2, label %bb6i64 3, label %bb4i64 4, label %bb3]bb2: ; preds = %startunreachablebb7: ; preds = %start%2 = load i64, ptr %self, align 8 ; seed expected valuebr label %atomicrmw.start ; enter CAS loopatomicrmw.start: ; preds = %atomicrmw.start, %bb7%loaded = phi i64 [ %2, %bb7 ], [ %newloaded, %atomicrmw.start ] ; on first iteration: use %2, on retries: use value observed by last cmpxchg%3 = icmp ugt i64 %loaded, %val ; unsigned compare (umax semantics)%new = select i1 %3, i64 %loaded, i64 %val ; desired = max(loaded, val)%4 = cmpxchg ptr %self, i64 %loaded, i64 %new monotonic monotonic, align 8 ; CAS: if *self==loaded, store new%success = extractvalue { i64, i1 } %4, 1 ; boolean: whether the swap happened%newloaded = extractvalue { i64, i1 } %4, 0 ; value seen in memory before the CASbr i1 %success, label %atomicrmw.end, label %atomicrmw.start ; loop until CAS succeedsatomicrmw.end: ; preds = %atomicrmw.startstore i64 %newloaded, ptr %_0, align 8br label %bb1[... MORE OF THE SAME, JUST FOR DIFFERENT ORDERING..]bb1: ; preds = %bb3, %bb4, %bb6, %bb5, %bb7%7 = load i64, ptr %_0, align 8ret i64 %7}&lt;/quote&gt;
    &lt;p&gt;We can see the pass did not change the first part - it still has the code to dispatch based on the memory ordering. But in the &lt;code&gt;bb7&lt;/code&gt; block, where we originally had the
&lt;code&gt;atomicrmw umax&lt;/code&gt; LLVM instruction, we now see a full compare-and-swap loop.
A compiler engineer would say that the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction has been
"lowered" into a sequence of more primitive operations, that are closer to what
the hardware can actually execute.&lt;/p&gt;
    &lt;p&gt;Here's the simplified logic:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read (seed): grab the current value (&lt;code&gt;expected&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Compute: &lt;code&gt;desired = umax(expected, val)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Attempt: &lt;code&gt;observed, success = cmpxchg(ptr, expected, desired, [...])&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;If success, return &lt;code&gt;observed&lt;/code&gt;(the old value). Otherwise&lt;code&gt;set expected = observed&lt;/code&gt;and loop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This CAS loop is a fundamental pattern in lock-free programming. The compiler just built it for us automatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 5: The Final Product (x86-64 Assembly)&lt;/head&gt;
    &lt;p&gt;We're at the final step. To see the final machine code, you can tell &lt;code&gt;rustc&lt;/code&gt; to
emit the assembly directly:&lt;/p&gt;
    &lt;quote&gt;rustc --emit=asm main.rs&lt;/quote&gt;
    &lt;p&gt;This will produce a &lt;code&gt;main.s&lt;/code&gt; file containing the final assembly code.
Inside, you'll find the result of the &lt;code&gt;cmpxchg&lt;/code&gt; loop:&lt;/p&gt;
    &lt;quote&gt;.LBB8_2:movq -32(%rsp), %rax # rax = &amp;amp;selfmovq (%rax), %rax # rax = *self (seed 'expected')movq %rax, -48(%rsp) # spill expected to stack.LBB8_3: # loop headmovq -48(%rsp), %rax # rax = expectedmovq -32(%rsp), %rcx # rcx = &amp;amp;selfmovq -40(%rsp), %rdx # rdx = valmovq %rax, %rsi # rsi = expected (scratch)subq %rdx, %rsi # set flags for unsigned compare: expected - valcmovaq %rax, %rdx # if (expected &amp;gt; val) rdx = expected; else rdx = val (compute max)lock cmpxchgq %rdx, (%rcx)# CAS: if *rcx==rax then *rcx=rdx; rax &amp;lt;- old *rcx; ZF=successsete %cl # cl = successmovq %rax, -56(%rsp) # spill observed to stacktestb $1, %cl # branch on successmovq %rax, -48(%rsp) # expected = observed (for retry)jne .LBB8_4 # success -&amp;gt; exitjmp .LBB8_3 # failure → retry&lt;/quote&gt;
    &lt;p&gt;The syntax might look a bit different from what you're used to, that's because it's in AT&amp;amp;T syntax, which is the default for &lt;code&gt;rustc&lt;/code&gt;. If you prefer Intel syntax, you can
use &lt;code&gt;rustc --emit=asm main.rs -C "llvm-args=-x86-asm-syntax=intel"&lt;/code&gt; to get that.&lt;/p&gt;
    &lt;p&gt;I'm not an assembly expert, but you can see the key parts of the CAS loop here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Seed read (first iteration): Load &lt;code&gt;*self&lt;/code&gt;once to initialize the expected value.&lt;/item&gt;
      &lt;item&gt;Compute umax without branching: The pair &lt;code&gt;sub&lt;/code&gt;+&lt;code&gt;cmova&lt;/code&gt;implements&lt;code&gt;desired = max_u(expected, val)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;CAS operation: On x86-64, &lt;code&gt;cmpxchg&lt;/code&gt;uses&lt;code&gt;RAX&lt;/code&gt;as the expected value and returns the observed value in&lt;code&gt;RAX&lt;/code&gt;;&lt;code&gt;ZF&lt;/code&gt;encodes success.&lt;/item&gt;
      &lt;item&gt;Retry or finish: If &lt;code&gt;ZF&lt;/code&gt;is clear, we failed and need to retry. Otherwise, we are done.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;Note we did not ask&lt;/p&gt;&lt;code&gt;rustc&lt;/code&gt;to optimize the code. If we did, the compiler would generate more efficient assembly: No spills to the stack, fewer jumps, no dispatch on memory ordering, etc. But I wanted to keep the output as close to the original IR as possible to make it easier to follow.&lt;/quote&gt;
    &lt;head rend="h2"&gt;The Beauty of Abstraction&lt;/head&gt;
    &lt;p&gt;And there we have it. Our journey is complete. We started with a safe, clear, single line of Rust and ended with a CAS loop written in assembly language.&lt;/p&gt;
    &lt;p&gt;Rust &lt;code&gt;fetch_max&lt;/code&gt; → Macro-generated &lt;code&gt;atomic_umax&lt;/code&gt; → LLVM
&lt;code&gt;atomicrmw umax&lt;/code&gt; → LLVM &lt;code&gt;cmpxchg&lt;/code&gt; loop → Assembly &lt;code&gt;lock cmpxchg&lt;/code&gt; loop&lt;/p&gt;
    &lt;p&gt;This journey is a perfect example of the power of modern compilers. We get to work at a high level of abstraction, focusing on safety and logic, while the compiler handles the messy, error-prone, and incredibly complex task of generating correct and efficient code for the hardware.&lt;/p&gt;
    &lt;p&gt;So, next time you use an atomic, take a moment to appreciate the incredible, hidden journey your code is about to take.&lt;/p&gt;
    &lt;p&gt;PS: After conducting this journey I learned that C++26 adds &lt;code&gt;fetch_max&lt;/code&gt;
too!&lt;/p&gt;
    &lt;p&gt;PPS: We are hiring!&lt;/p&gt;
    &lt;head rend="h2"&gt;Bonus: Apple Silicon (AArch64)&lt;/head&gt;
    &lt;p&gt;Out of curiosity, I also checked how this looks on Apple Silicon (AArch64). This architecture does have a native &lt;code&gt;atomic max&lt;/code&gt; instruction, so the
&lt;code&gt;AtomicExpandPass&lt;/code&gt; does not need to lower it into a CAS loop. The LLVM code before and after
the pass is identical, still containing the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;p&gt;The final assembly contains a variant of the &lt;code&gt;LDUMAX&lt;/code&gt; instruction. This is the relevant part of the assembly:&lt;/p&gt;
    &lt;quote&gt;ldr x8, [sp, #16] # x8 = value to compare withldr x9, [sp, #8] # x9 = pointer to the atomic variableldumax x8, x8, [x9] # atomic unsigned max (relaxed), [x9] = max(x8, [x9]), x8 = old valuestr x8, [sp, #40] # Store old valueb LBB8_11&lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that AArch64 uses Unified Assembler Language, when reading the snippet above, it's important to remember that the destination register comes first.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And that's really it. We could continue to dig into the microarchitecture, to see how instructions are executed at the hardware level, what are the effects of the &lt;code&gt;LOCK&lt;/code&gt; prefix, dive into differences in memory ordering, etc.
But we'll leave that for another day.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Alice: "Would you tell me, please, which way I ought to go from here?"&lt;/p&gt;&lt;lb/&gt;The Cat: "That depends a good deal on where you want to get to."&lt;lb/&gt;Alice: "I don't much care where."&lt;lb/&gt;The Cat: "Then it doesn't much matter which way you go."&lt;lb/&gt;Alice: "...So long as I get somewhere."&lt;lb/&gt;The Cat: "Oh, you're sure to do that, if only you walk long enough."&lt;p&gt;- Lewis Carroll, Alice's Adventures in Wonderland&lt;/p&gt;&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://questdb.com/blog/rust-fetch-max-compiler-journey/"/><published>2025-09-23T21:24:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45354314</id><title>Top Programming Languages 2025</title><updated>2025-09-24T14:39:07.117341+00:00</updated><content>&lt;doc fingerprint="ff481adb833c8cfd"&gt;
  &lt;main&gt;&lt;p&gt;Since 2013, we’ve been metaphorically peering over the shoulders of programmers to create our annual interactive rankings of the most popular programming languages. But fundamental shifts in how people are coding may not just make it harder to measure popularity, but could even make the concept itself irrelevant. And then things might get really weird. To see why, let’s start with this year’s rankings and a quick refresher of how we put this thing together.&lt;/p&gt;&lt;p&gt;In the “Spectrum” default ranking, which is weighted with the interests of IEEE members in mind, we see that once again Python has the top spot, with the biggest change in the top five being JavaScript’s drop from third place last year to sixth place this year. As JavaScript is often used to create web pages, and vibe coding is often used to create websites, this drop in the apparent popularity may be due to the effects of AI that we’ll dig into in a moment. But first to finish up with this year’s scores, in the “Jobs” ranking, which looks exclusively at what skills employers are looking for, we see that Python has also taken 1st place, up from second place last year, though SQL expertise remains an incredibly valuable skill to have on your resume.&lt;/p&gt;&lt;p&gt;Because we can’t literally look over the shoulders of everyone who codes, including kids hacking on Minecraft servers or academic researchers developing new architectures, we rely on proxies to measure popularity. We detail our methodology here, but the upshot is that we merge metrics from multiple sources to create our rankings. The metrics we choose publicly signal interest across a wide range of languages—Google search traffic, questions asked on Stack Exchange, mentions in research papers, activity on the GitHub open source code repository, and so on.&lt;/p&gt;&lt;p&gt;But programmers are turning away from many of these public expressions of interest. Rather than page through a book or search a website like Stack Exchange for answers to their questions, they’ll chat with an LLM like Claude or ChatGPT in a private conversation. And with an AI assistant like Cursor helping to write code, the need to pose questions in the first place is significantly decreased. For example, across the total set of languages evaluated in the TPL, the number of questions we saw posted per week on Stack Exchange in 2025 was just 22 percent of what it was in 2024.&lt;/p&gt;&lt;p&gt;With less signal in publicly available metrics, it becomes harder to track popularity across a broad range of languages. This existential problem for our rankings can be tackled by searching for new metrics, or trying to survey programmers—in all their variety—directly. However, an even more fundamental problem is looming in the wings.&lt;/p&gt;&lt;p&gt;Whether it’s a seasoned coder using an AI to handle the grunt work, or a neophyte vibe coding a complete web app, AI assistance means that programmers can concern themselves less and less with the particulars of any language. First details of syntax, then flow control and functions, and so on up the levels of how a program is put together—more and more is being left to the AI.&lt;/p&gt;&lt;p&gt;Although code-writing LLM’s are still very much a work in progress, as they take over an increasing share of the work, programmers inevitably shift from being the kind of people willing to fight religious wars over whether source code should be indented by typing tabs or spaces to people who care less and less about what language is used.&lt;/p&gt;&lt;p&gt;After all, the whole reason different computer languages exist is because given a particular challenge, it’s easier to express a solution in one language versus another. You wouldn’t control a washing machine using the R programming language, or conversely do a statistical analysis on large datasets using C.&lt;/p&gt;&lt;p&gt;But it is technically possible to do both. A human might tear their hair out doing it, but LLMs have about as much hair as they do sentience. As long as there’s enough training data, they’ll generate code for a given prompt in any language you want. In practical terms, this means using one—any one—of today’s most popular general purpose programming languages. In the same way most developers today don’t pay much attention to the instruction sets and other hardware idiosyncrasies of the CPUs that their code runs on, which language a program is vibe coded in ultimately becomes a minor detail.&lt;/p&gt;&lt;p&gt;Sure, there will always be some people who care, just as today there are nerds like me willing to debate the merits of writing for the Z80 versus the 6502 8-bit CPUs. But overall, the popularity of different computer languages could become as obscure a topic as the relative popularity of railway track gauges.&lt;/p&gt;&lt;p&gt;One obvious long-term consequence to this is that it will become harder for new languages to emerge. Previously, new languages could emerge from individuals or small teams evangelizing their approach to potential contributors and users. Presentations, papers, demos, sample code and tutorials seeded new developer ecosystems. A single well-written book, like Leo Brodie’s Starting Forth or Brian Kernighan and Dennis Ritchies’ The C Programming Language, could make an enormous difference to a language’s popularity.&lt;/p&gt;&lt;p&gt;But while a few samples and a tutorial can be enough material to jump-start adoption among programmers familiar with the ins and outs of hands-on coding, it’s not enough for today’s AIs. Humans build mental models that can extrapolate from relatively small amounts of data. LLMs rely on statistical probabilities, so the more data they can crunch, they better they are. Consequently programmers have noted that AIs give noticeably poorer results when trying to code in less-used languages.&lt;/p&gt;&lt;p&gt;There are research efforts to make LLMs more universal coders, but that doesn’t really help new languages get off the ground. Fundamentally new languages grow because they are scratching some itch a programmer has. That itch can be as small as being annoyed at semicolons having to be placed after every statement, or as large as a philosophical argument about the purpose of computation.&lt;/p&gt;&lt;p&gt;But if an AI is soothing our irritations with today’s languages, will any new ones ever reach the kind of critical mass needed to make an impact? Will the popularity of today’s languages remain frozen in time?&lt;/p&gt;&lt;head rend="h2"&gt;What’s the future of programming languages?&lt;/head&gt;&lt;p&gt;Before speculating further about the future, let’s touch base again where we are today. Modern high-level computer languages are really designed to do two things: create an abstraction layer that makes it easier to process data in a suitable fashion, and stop programmers from shooting themselves in the foot.&lt;/p&gt;&lt;p&gt;The first objective has been around since the days of Fortran and Cobol, aimed at processing scientific and business data respectively. The second objective emerged later, spurred in no small part by Edgar Dijkstra’s 1968 paper “Go To Statement Considered Harmful.” In this he argued for eliminating the ability for a programmer to make jumps to arbitrary points in their code. This restriction was to prevent so-called spaghetti code that makes it hard for a programmer to understand how a computer actually executes a given program. Instead, Dijkstra demanded that programmers bend to structural rules imposed by the language. Dijkstra’s argument ultimately won the day, and most modern languages do indeed minimize or eliminate Go Tos altogether in favor of structures like functions and other programmatic blocks.&lt;/p&gt;&lt;p&gt;These structures don’t exist at the level of the CPU. If you look at the instruction sets for Arm, x86, or RISC-V processors, the flow of a program is controlled by just three types of machine code instructions. These are conditional jumps, unconditional jumps, and jumps with a trace stored (so you can call a subroutine and return to where you started). In other words, it’s Go Tos all the way down. Similarly, strict data types designed to label and protect data from incorrect use dissolve into anonymous bits flowing in and out of memory.&lt;/p&gt;&lt;p&gt;So how much abstraction and anti-foot-shooting structure will a sufficiently-advanced coding AI really need? A hint comes from recent research in AI-assisted hardware design, such as Dall-EM, a generative AI developed at Princeton University used to create RF and electromagnetic filters. Designing these filters has always been something of a black art, involving the wrangling of complex electromagnetic fields as they swirl around little strips of metal. But Dall-EM can take in the desired inputs and outputs and spit out something that looks like a QR code. The results are something no human would ever design—but it works.&lt;/p&gt;&lt;p&gt;Similarly, could we get our AIs to go straight from prompt to an intermediate language that could be fed into the interpreter or compiler of our choice? Do we need high-level languages at all in that future? True, this would turn programs into inscrutable black boxes, but they could still be divided into modular testable units for sanity and quality checks. And instead of trying to read or maintain source code, programmers would just tweak their prompts and generate software afresh.&lt;/p&gt;&lt;p&gt;What’s the role of the programmer in a future without source code? Architecture design and algorithm selection would remain vital skills—for example, should a pathfinding program use a classic approach like the A* algorithm, or instead should it try to implement a new method? How should a piece of software be interfaced with a larger system? How should new hardware be exploited? In this scenario, computer science degrees, with their emphasis on fundamentals over the details of programming languages, rise in value over coding boot camps.&lt;/p&gt;Will there be a Top Programming Language in 2026? Right now, programming is going through the biggest transformation since compilers broke onto the scene in the early 1950s. Even if the predictions that much of AI is a bubble about to burst come true, the thing about tech bubbles is that there’s always some residual technology that survives. It’s likely that using LLMs to write and assist with code is something that’s going to stick. So we’re going to be spending the next 12 months figuring out what popularity means in this new age, and what metrics might be useful to measure. What do you think popularity should mean? What metrics do you think we should consider? Let us know in the comments below.&lt;list rend="ul"&gt;&lt;item&gt;AI Models Embrace Humanlike Reasoning ›&lt;/item&gt;&lt;item&gt;LLM Benchmarking Shows Capabilities Doubling Every 7 Months ›&lt;/item&gt;&lt;item&gt;Why Functional Programming Should Be the Future of Software Development ›&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Stephen Cass is the special projects editor at IEEE Spectrum. He currently helms Spectrum's Hands On column, and is also responsible for interactive projects such as the Top Programming Languages app. He has a bachelor's degree in experimental physics from Trinity College Dublin.&lt;/p&gt;&lt;p&gt;A programming language is a bridge between natural English and computer machine languages, allowing humans to tell the computer what to do.&lt;/p&gt;&lt;p&gt;The idea language will be human's natural languages, including English, Chinese and other popular human languages we learned after birth.&lt;/p&gt;&lt;p&gt;The point about why AI should not use high level languages is spot on. These languages are developed for humans. If the final code is only going to pass acceptance tests, without unit tests, then AI should be creating machine code directly.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/top-programming-languages-2025"/><published>2025-09-23T23:42:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45354644</id><title>Baldur's Gate 3 Steam Deck – Native Version</title><updated>2025-09-24T14:39:06.380923+00:00</updated><content>&lt;doc fingerprint="bcd4a8bd8df387c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Steam Deck - Native Version&lt;/head&gt;
    &lt;p&gt;Upon release of Hotfix #34 on your Steam Deck, your device will install the Native version.&lt;/p&gt;
    &lt;p&gt;If you are unsure whether the build has been installed correctly, you can do the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to the game’s Steam page. Click on the Settings button and select Properties.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p/&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Once in the Properties page, go to the Compatibility tab.&lt;/item&gt;
      &lt;item&gt;Tick the box for “Force the use of a specific Steam Play compatibility tool”.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Select any version that has Linux Runtime.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow the game to update if an update appears.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s the difference between the Steam Deck Native and Proton version?&lt;/p&gt;
    &lt;p&gt;Our Proton version runs on the Steam Deck via the Proton compatibility layer, which requires extra CPU processing power. Running the game natively on the Steam Deck requires less CPU usage and memory consumption overall!&lt;/p&gt;
    &lt;p&gt;Can I still switch back to the Proton version?&lt;/p&gt;
    &lt;p&gt;Yes. If you’re having issues with the Steam Deck Native build, you can revert to the Proton version. Take the following steps to do so:&lt;/p&gt;
    &lt;p&gt;Go to the game’s Steam page. Click on the Settings button and select Properties.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Once in the Properties page, go to the Compatibility tab.&lt;/item&gt;
      &lt;item&gt;Tick the box for “Force the use of a specific Steam Play compatibility tool”.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Select any Proton version 8 or higher.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow the game to update.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now that there is a Steam Deck Native build, is Baldur’s Gate 3 supported on Linux?&lt;/p&gt;
    &lt;p&gt;Larian does not provide support for the Linux platform. The Steam Deck Native build is only supported on Steam Deck.&lt;/p&gt;
    &lt;head rend="h2"&gt;Savegames&lt;/head&gt;
    &lt;p&gt;Where are my saves located currently (before using the Steam Deck Native version)?&lt;/p&gt;
    &lt;p&gt;Before the Steam Deck Native version becomes the primary version, your saves will be in the compatdata folder: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/p&gt;
    &lt;p&gt;Where are my saves located when I use the Steam Deck Native version?&lt;/p&gt;
    &lt;p&gt;After the Steam Deck Native version becomes the primary version, your saves will be in the following folder: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/p&gt;
    &lt;p&gt;Why are my saves in different folders?&lt;/p&gt;
    &lt;p&gt;When Baldur’s Gate 3 runs on the Proton compatibility layer, the Proton version will store the saves in the compatdata folder, which is a mirrored version of the Windows file storage system. On the Steam Deck Native version, the saves are stored natively on the SteamOS file storage system.&lt;/p&gt;
    &lt;p&gt;Will my savegames be transferred over to the new version when I use the Steam Deck Native version?&lt;/p&gt;
    &lt;p&gt;If your Steam Cloud saves are turned on, your most recent saves will be synced to the Steam Deck Native savegame folder automatically.&lt;/p&gt;
    &lt;p&gt;What if I don’t have Cloud saves turned on, or I want my older saves?&lt;/p&gt;
    &lt;p&gt;Your saves are still stored on the Steam Deck, but they will be stored in the compatdata folder.&lt;lb/&gt; You can manually transfer these files via the Desktop:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First, switch to Desktop Mode by clicking on the Steam button and selecting Power. Then click on Switch to Desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p/&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you have a mouse and keyboard to hand, plug them in to make your life a little easier, and click on the folder icon on the bar at the bottom.&lt;/item&gt;
      &lt;item&gt;In the explorer window, navigate to: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/item&gt;
      &lt;item&gt;Copy the Savegames folder.&lt;/item&gt;
      &lt;item&gt;Navigate to: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/item&gt;
      &lt;item&gt;Paste the copied folder in this location.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;lb/&gt; Will my old saves still take up storage space on my Steam Deck?&lt;/p&gt;
    &lt;p&gt;Yes, your old saves will still take up storage space. If you want to save some space and you don't plan on using the Proton version, you can delete the compatdata folder after you've copied over the folders.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mods&lt;/head&gt;
    &lt;p&gt;Will my mods be transferred over automatically?&lt;/p&gt;
    &lt;p&gt;If you are logged into your Larian Account and have it connected to mod.io, all mods you are subscribed to will be downloaded when the transition to Steam Deck Native occurs.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; What if I’m not logged into a Larian Account or connected to mod.io?&lt;/p&gt;
    &lt;p&gt;You can either manually download the mods from the Mod Manager or transfer them manually from the previous folder.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;To do so,switch to Desktop Mode by clicking on the Steam button and selecting Power. Then click on Switch to Desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Click on the folder icon on the bar at the bottom.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the explorer window, navigate to: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3&lt;/item&gt;
      &lt;item&gt;Copy the Mods folder.&lt;/item&gt;
      &lt;item&gt;Navigate to: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/&lt;/item&gt;
      &lt;item&gt;Paste the copied folder in this location.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://larian.com/support/faqs/steam-deck-native-version_121"/><published>2025-09-24T00:26:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45355965</id><title>New study shows plants and animals emit a visible light that expires at death</title><updated>2025-09-24T14:39:06.093063+00:00</updated><content/><link href="https://pubs.acs.org/doi/10.1021/acs.jpclett.4c03546"/><published>2025-09-24T03:27:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45357693</id><title>That Secret Service SIM farm story is bogus</title><updated>2025-09-24T14:39:05.937033+00:00</updated><content/><link href="https://cybersect.substack.com/p/that-secret-service-sim-farm-story"/><published>2025-09-24T08:24:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45358216</id><title>Exploring GrapheneOS secure allocator: Hardened Malloc</title><updated>2025-09-24T14:39:05.063402+00:00</updated><content>&lt;doc fingerprint="6e5acb5114a5060c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Exploring GrapheneOS secure allocator: Hardened Malloc&lt;/head&gt;- 22/09/2025 - in&lt;p&gt;GrapheneOS is a mobile operating system based on Android and focusing on privacy and security. To enhance further the security of their product, GrapheneOS developers introduced a new libc allocator : hardened malloc. This allocator has a security-focused design in mind to protect processes against common memory corruption vulnerabilities. This article will explain in details its internal architecture and how security mitigation are implemented from a security researcher point of view.&lt;/p&gt;&lt;p&gt;Looking to improve your skills? Discover our trainings sessions! Learn more.&lt;/p&gt;&lt;head rend="h2"&gt;Introduction&lt;/head&gt;&lt;p&gt;GrapheneOS is a security and privacy-focused mobile operating system based on a modified version of Android (AOSP). To enhance its protection, it integrates advanced security features, including its own memory allocator for libc: hardened malloc. Designed to be as robust as the operating system itself, this allocator specifically seeks to protect against memory corruption.&lt;/p&gt;&lt;p&gt;This technical article details the internal workings of hardened malloc and the protection mechanisms it implements to prevent common memory corruption vulnerabilities. It is intended for a technical audience, particularly security researchers or exploit developers, who wish to gain an in-depth understanding of this allocator's internals.&lt;/p&gt;&lt;p&gt;The analyses and tests in this article were performed on two devices running GrapheneOS:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Pixel 4a 5G: &lt;code&gt;google/bramble/bramble:14/UP1A.231105.001.B2/2025021000:user/release-keys&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Pixel 9a: &lt;code&gt;google/tegu/tegu:16/BP2A.250705.008/2025071900:user/release-keys&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The devices were rooted with Magisk 29 in order to use Frida to observe the internal state of hardened malloc within system processes. The study was based on the source code from the official GrapheneOS GitHub repository (commit &lt;code&gt;7481c8857faf5c6ed8666548d9e92837693de91b&lt;/code&gt;).&lt;/p&gt;&lt;head rend="h2"&gt;GrapheneOS&lt;/head&gt;&lt;p&gt;GrapheneOS is a hardened operating system based on Android. As an actively maintained open-source project, it benefits from frequent updates and the swift application of security patches. All information is available on GrapheneOS website.&lt;/p&gt;&lt;p&gt;To effectively protect the processes running on the device, GrapheneOS implements several security mechanisms. The following sections briefly describe the specific mechanisms that contribute to the hardening of its memory allocator.&lt;/p&gt;&lt;head rend="h3"&gt;Extended Address Space&lt;/head&gt;&lt;p&gt;On standard Android systems, the address space for userland processes is limited to 39 bits, ranging from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;0x8000000000&lt;/code&gt;. On GrapheneOS, this space is extended to 48 bits, and to take advantage of this extension, ASLR entropy has also been increased from 18 to 33 bits. This detail is important as hardened malloc relies heavily on &lt;code&gt;mmap&lt;/code&gt; for its internal structures and its allocations.&lt;/p&gt;&lt;code&gt;
 tegu:/ # cat /proc/self/maps
c727739a2000-c727739a9000 rw-p 00000000 00:00 0                          [anon:.bss]
c727739a9000-c727739ad000 r--p 00000000 00:00 0                          [anon:.bss]
c727739ad000-c727739b1000 rw-p 00000000 00:00 0                          [anon:.bss]
c727739b1000-c727739b5000 r--p 00000000 00:00 0                          [anon:.bss]
c727739b5000-c727739c1000 rw-p 00000000 00:00 0                          [anon:.bss]
e5af7fa30000-e5af7fa52000 rw-p 00000000 00:00 0                          [stack]
tegu:/ # cat /proc/self/maps
d112736be000-d112736c5000 rw-p 00000000 00:00 0                          [anon:.bss]
d112736c5000-d112736c9000 r--p 00000000 00:00 0                          [anon:.bss]
d112736c9000-d112736cd000 rw-p 00000000 00:00 0                          [anon:.bss]
d112736cd000-d112736d1000 r--p 00000000 00:00 0                          [anon:.bss]
d112736d1000-d112736dd000 rw-p 00000000 00:00 0                          [anon:.bss]
ea0de59be000-ea0de59e1000 rw-p 00000000 00:00 0                          [stack]
tegu:/ # cat /proc/self/maps
d71f87043000-d71f8704a000 rw-p 00000000 00:00 0                          [anon:.bss]
d71f8704a000-d71f8704e000 r--p 00000000 00:00 0                          [anon:.bss]
d71f8704e000-d71f87052000 rw-p 00000000 00:00 0                          [anon:.bss]
d71f87052000-d71f87056000 r--p 00000000 00:00 0                          [anon:.bss]
d71f87056000-d71f87062000 rw-p 00000000 00:00 0                          [anon:.bss]
f69f7c952000-f69f7c974000 rw-p 00000000 00:00 0                          [stack]
&lt;/code&gt;&lt;head rend="h3"&gt;Secure app spawning&lt;/head&gt;&lt;p&gt;On standard Android, each application is launched via a &lt;code&gt;fork&lt;/code&gt; of the zygote process. This mechanism, designed to speed up startup, has a major security consequence: all applications inherit the same address space as zygote. In practice, this means that pre-loaded libraries end up at identical addresses from one application to another. For an attacker, this predictability makes it easy to bypass ASLR protection without needing a prior information leak.&lt;/p&gt;&lt;p&gt;To overcome this limitation, GrapheneOS fundamentally changes this process. Instead of just a &lt;code&gt;fork&lt;/code&gt;, new applications are launched with &lt;code&gt;exec&lt;/code&gt;. This method creates an entirely new and randomized address space for each process, thereby restoring the full effectiveness of ASLR. It is no longer possible to predict the location of remote memory regions. This enhanced security does, however, come at a cost: a slight impact on launch performance and an increased memory footprint for each application.&lt;/p&gt;&lt;code&gt;
 tegu:/ # cat /proc/$(pidof zygote64)/maps | grep libc\.so
d6160aac0000-d6160ab19000 r--p 00000000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
d6160ab1c000-d6160abbe000 r-xp 0005c000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
d6160abc0000-d6160abc5000 r--p 00100000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
d6160abc8000-d6160abc9000 rw-p 00108000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
tegu:/ # cat /proc/$(pidof com.android.messaging)/maps | grep libc\.so
d5e4a9c68000-d5e4a9cc1000 r--p 00000000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
d5e4a9cc4000-d5e4a9d66000 r-xp 0005c000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
d5e4a9d68000-d5e4a9d6d000 r--p 00100000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
d5e4a9d70000-d5e4a9d71000 rw-p 00108000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
tegu:/ # cat /proc/$(pidof com.topjohnwu.magisk)/maps | grep libc\.so
dabc42ac5000-dabc42b1e000 r--p 00000000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
dabc42b21000-dabc42bc3000 r-xp 0005c000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
dabc42bc5000-dabc42bca000 r--p 00100000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
dabc42bcd000-dabc42bce000 rw-p 00108000 07:f0 24                         /apex/com.android.runtime/lib64/bionic/libc.so
&lt;/code&gt;&lt;head rend="h3"&gt;Memory Tagging Extension (MTE)&lt;/head&gt;&lt;p&gt;Memory Tagging Extension, or MTE, is an extension of the ARM architecture introduced with Armv8.5. MTE aims to prevent memory corruption vulnerabilities from being exploited by an attacker. This protection relies on a mechanism of tagging memory regions.&lt;/p&gt;&lt;p&gt;During an allocation, a 4-bit tag is associated with the allocated region and stored in the top bits of the pointer. To access the data, both the address and the tag must be correct. If the tag is wrong, an exception is raised. This mechanism allows for the detection and blocking of vulnerabilities such as out-of-bound reads/writes and use-after-free.&lt;/p&gt;&lt;p&gt;For example, the out-of-bounds write in the following C code could be detected, depending on the allocator's implementation with MTE:&lt;/p&gt;&lt;code&gt;
 char* ptr = malloc(8); 
ptr[16] = 12; // oob write, this tag is not valid for the area&lt;/code&gt;&lt;p&gt;Since MTE is a feature offered by the CPU, it is necessary for the hardware to be compatible. This is the case for all Google Pixel smartphones since the Pixel 8. For more information, refer to the ARM documentation.&lt;/p&gt;&lt;p&gt;Hardened malloc therefore uses MTE on compatible smartphones to prevent this type of memory corruption.&lt;/p&gt;&lt;p&gt;In order to benefit from MTE, a binary must be compiled with the appropriate flags. For the purposes of this article, the flags below were added to the &lt;code&gt;Application.mk&lt;/code&gt; file of our test binaries to enable MTE.&lt;/p&gt;&lt;code&gt;
 APP_CFLAGS := -fsanitize=memtag -fno-omit-frame-pointer -march=armv8-a+memtag
APP_LDFLAGS := -fsanitize=memtag -march=armv8-a+memtag&lt;/code&gt;&lt;p&gt;The Android documentation provides all the necessary information to create an MTE-compatible application.&lt;/p&gt;&lt;p&gt;Hardened malloc relies heavily on MTE by adding tags to its allocations. Please note that only small allocation (less than &lt;code&gt;0x20000&lt;/code&gt; bytes) are tagged.&lt;/p&gt;&lt;head rend="h2"&gt;Hardened malloc architecture&lt;/head&gt;&lt;p&gt;To enhance security, hardened malloc isolates metadata from user data in separate memory regions, holding it primarily within two main structures :&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;ro&lt;/code&gt;: the main structure in the&lt;code&gt;.bss&lt;/code&gt;section of libc.&lt;/item&gt;&lt;item&gt;&lt;code&gt;allocator_state&lt;/code&gt;: a large structure grouping all metadata for the different allocation types. Its memory region is reserved only once at initialization.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Similar to jemalloc, hardened malloc partitions threads into arenas, with each arena managing its own allocations. This implies that memory allocated in one arena cannot be managed or freed by another arena. However, there is no explicit data structure to define these arenas; their existence is implicit and primarily affects the size of certain internal arrays.&lt;/p&gt;&lt;p&gt;Although the arena concept is present in the source code, analysis of the libc binaries from the test devices revealed that hardened malloc was compiled to use only a single arena. As a result, all threads share the same pool of allocation metadata.&lt;/p&gt;&lt;head rend="h3"&gt;ro structure&lt;/head&gt;&lt;p&gt;The &lt;code&gt;ro&lt;/code&gt; structure is the allocator's main metadata structure. It is contained within the &lt;code&gt;.bss&lt;/code&gt; section of libc and consists of the following attributes:&lt;/p&gt;&lt;code&gt;
 static union {
    struct {
        void *slab_region_start;
        void *_Atomic slab_region_end;
        struct size_class *size_class_metadata[N_ARENA];
        struct region_allocator *region_allocator;
        struct region_metadata *regions[2];
#ifdef USE_PKEY
        int metadata_pkey;
#endif
#ifdef MEMTAG
        bool is_memtag_disabled;
#endif
    };
    char padding[PAGE_SIZE];
} ro __attribute__((aligned(PAGE_SIZE)));&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;slab_region_start&lt;/code&gt;: The start of the memory area containing the regions for small allocations.&lt;/item&gt;&lt;item&gt;&lt;code&gt;slab_region_end&lt;/code&gt;: The end of the memory area containing the regions for small allocations.&lt;/item&gt;&lt;item&gt;&lt;code&gt;size_class_metadata[N_ARENA]&lt;/code&gt;: An array of pointers to the metadata for small allocations, per arena.&lt;/item&gt;&lt;item&gt;&lt;code&gt;region_allocator&lt;/code&gt;: A pointer to the management structure for large allocations.&lt;/item&gt;&lt;item&gt;&lt;code&gt;regions[2]&lt;/code&gt;: A pointer to the hash tables that reference the large allocations.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;allocator_state&lt;/head&gt;&lt;p&gt;This structure contains all the metadata used for both small and large allocations. It is mapped only once when the allocator initializes and is isolated by guard pages. Its size is fixed and computed based on the maximum number of allocations the allocator can handle.&lt;/p&gt;&lt;code&gt;
 struct __attribute__((aligned(PAGE_SIZE))) allocator_state {
    struct size_class size_class_metadata[N_ARENA][N_SIZE_CLASSES];
    struct region_allocator region_allocator;
    // padding until next page boundary for mprotect
    struct region_metadata regions_a[MAX_REGION_TABLE_SIZE] __attribute__((aligned(PAGE_SIZE)));
    // padding until next page boundary for mprotect
    struct region_metadata regions_b[MAX_REGION_TABLE_SIZE] __attribute__((aligned(PAGE_SIZE)));
    // padding until next page boundary for mprotect
    struct slab_info_mapping slab_info_mapping[N_ARENA][N_SIZE_CLASSES];
    // padding until next page boundary for mprotect
};&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;size_class_metadata[N_ARENA][N_SIZE_CLASSES]&lt;/code&gt;: An array of&lt;code&gt;size_class&lt;/code&gt;structures containing the metadata for small allocations for each class.&lt;/item&gt;&lt;item&gt;&lt;code&gt;region_allocator&lt;/code&gt;: The metadata for theÂ large allocations regions.&lt;/item&gt;&lt;item&gt;&lt;code&gt;regions_a/b[MAX_REGION_TABLE_SIZE]&lt;/code&gt;: A hash table that groups information about the mappings of large allocations.&lt;/item&gt;&lt;item&gt;&lt;code&gt;slab_info_mapping&lt;/code&gt;: The metadata for the slabs of small allocations.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;User data&lt;/head&gt;&lt;p&gt;Hardened malloc stores user data in two types of regions, separate from its metadata:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Slabs region: a very large area reserved only once at initialization, which contains the slabs for small allocations. It is initialized in the&lt;/p&gt;&lt;code&gt;init_slow_path&lt;/code&gt;function and its starting address is stored in&lt;code&gt;ro.slab_region_start&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Large regions: dynamically reserved areas that hold the data for large allocations. Each such region contains only a single large allocation.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Allocations&lt;/head&gt;&lt;p&gt;There are two types of allocations in hardened malloc: small allocations and large allocations.&lt;/p&gt;&lt;head rend="h3"&gt;Small allocations&lt;/head&gt;&lt;head rend="h4"&gt;Size classes/bins&lt;/head&gt;&lt;p&gt;Small allocations are categorized by size into size classes, also known as bins. hardened malloc utilizes 49 such classes, which are indexed by increasing size and represented by the &lt;code&gt;size_class&lt;/code&gt; structure:&lt;/p&gt;&lt;table&gt;&lt;row span="7"&gt;&lt;cell role="head"&gt;Size Class&lt;/cell&gt;&lt;cell role="head"&gt;Total Bin Size&lt;/cell&gt;&lt;cell role="head"&gt;Available Size&lt;/cell&gt;&lt;cell role="head"&gt;Slots&lt;/cell&gt;&lt;cell role="head"&gt;Slab size&lt;/cell&gt;&lt;cell role="head"&gt;Max slabs&lt;/cell&gt;&lt;cell role="head"&gt;Quarantines Size (random / FIFO)&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0x10&lt;/cell&gt;&lt;cell&gt;0x10&lt;/cell&gt;&lt;cell&gt;256&lt;/cell&gt;&lt;cell&gt;0x1000&lt;/cell&gt;&lt;cell&gt;8388608&lt;/cell&gt;&lt;cell&gt;8192 / 8192&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0x10&lt;/cell&gt;&lt;cell&gt;0x8&lt;/cell&gt;&lt;cell&gt;256&lt;/cell&gt;&lt;cell&gt;0x1000&lt;/cell&gt;&lt;cell&gt;8388608&lt;/cell&gt;&lt;cell&gt;8192 / 8192&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;0x20&lt;/cell&gt;&lt;cell&gt;0x18&lt;/cell&gt;&lt;cell&gt;128&lt;/cell&gt;&lt;cell&gt;0x1000&lt;/cell&gt;&lt;cell&gt;8388608&lt;/cell&gt;&lt;cell&gt;4096 / 4096&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;cell&gt;0x30&lt;/cell&gt;&lt;cell&gt;0x28&lt;/cell&gt;&lt;cell&gt;85&lt;/cell&gt;&lt;cell&gt;0x1000&lt;/cell&gt;&lt;cell&gt;8388608&lt;/cell&gt;&lt;cell&gt;4096 / 4096&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;cell&gt;0x40&lt;/cell&gt;&lt;cell&gt;0x38&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0x1000&lt;/cell&gt;&lt;cell&gt;8388608&lt;/cell&gt;&lt;cell&gt;2048 / 2048&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;5&lt;/cell&gt;&lt;cell&gt;0x50&lt;/cell&gt;&lt;cell&gt;0x48&lt;/cell&gt;&lt;cell&gt;51&lt;/cell&gt;&lt;cell&gt;0x1000&lt;/cell&gt;&lt;cell&gt;8388608&lt;/cell&gt;&lt;cell&gt;2048 / 2048&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;6&lt;/cell&gt;&lt;cell&gt;0x60&lt;/cell&gt;&lt;cell&gt;0x58&lt;/cell&gt;&lt;cell&gt;42&lt;/cell&gt;&lt;cell&gt;0x1000&lt;/cell&gt;&lt;cell&gt;8388608&lt;/cell&gt;&lt;cell&gt;2048 / 2048&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;7&lt;/cell&gt;&lt;cell&gt;0x70&lt;/cell&gt;&lt;cell&gt;0x68&lt;/cell&gt;&lt;cell&gt;36&lt;/cell&gt;&lt;cell&gt;0x1000&lt;/cell&gt;&lt;cell&gt;8388608&lt;/cell&gt;&lt;cell&gt;2048 / 2048&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;0x80&lt;/cell&gt;&lt;cell&gt;0x78&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0x2000&lt;/cell&gt;&lt;cell&gt;4194304&lt;/cell&gt;&lt;cell&gt;1024 / 1024&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;9&lt;/cell&gt;&lt;cell&gt;0xa0&lt;/cell&gt;&lt;cell&gt;0x98&lt;/cell&gt;&lt;cell&gt;51&lt;/cell&gt;&lt;cell&gt;0x2000&lt;/cell&gt;&lt;cell&gt;4194304&lt;/cell&gt;&lt;cell&gt;1024 / 1024&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;10&lt;/cell&gt;&lt;cell&gt;0xc0&lt;/cell&gt;&lt;cell&gt;0xb8&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0x3000&lt;/cell&gt;&lt;cell&gt;2796202&lt;/cell&gt;&lt;cell&gt;1024 / 1024&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;11&lt;/cell&gt;&lt;cell&gt;0xe0&lt;/cell&gt;&lt;cell&gt;0xd8&lt;/cell&gt;&lt;cell&gt;54&lt;/cell&gt;&lt;cell&gt;0x3000&lt;/cell&gt;&lt;cell&gt;2796202&lt;/cell&gt;&lt;cell&gt;1024 / 1024&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;12&lt;/cell&gt;&lt;cell&gt;0x100&lt;/cell&gt;&lt;cell&gt;0xf8&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0x4000&lt;/cell&gt;&lt;cell&gt;2097152&lt;/cell&gt;&lt;cell&gt;512 / 512&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;13&lt;/cell&gt;&lt;cell&gt;0x140&lt;/cell&gt;&lt;cell&gt;0x138&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0x5000&lt;/cell&gt;&lt;cell&gt;1677721&lt;/cell&gt;&lt;cell&gt;512 / 512&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;14&lt;/cell&gt;&lt;cell&gt;0x180&lt;/cell&gt;&lt;cell&gt;0x178&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0x6000&lt;/cell&gt;&lt;cell&gt;1398101&lt;/cell&gt;&lt;cell&gt;512 / 512&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;15&lt;/cell&gt;&lt;cell&gt;0x1c0&lt;/cell&gt;&lt;cell&gt;0x1b8&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0x7000&lt;/cell&gt;&lt;cell&gt;1198372&lt;/cell&gt;&lt;cell&gt;512 / 512&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;16&lt;/cell&gt;&lt;cell&gt;0x200&lt;/cell&gt;&lt;cell&gt;0x1f8&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0x8000&lt;/cell&gt;&lt;cell&gt;1048576&lt;/cell&gt;&lt;cell&gt;256 / 256&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;17&lt;/cell&gt;&lt;cell&gt;0x280&lt;/cell&gt;&lt;cell&gt;0x278&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0xa000&lt;/cell&gt;&lt;cell&gt;838860&lt;/cell&gt;&lt;cell&gt;256 / 256&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;18&lt;/cell&gt;&lt;cell&gt;0x300&lt;/cell&gt;&lt;cell&gt;0x2f8&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0xc000&lt;/cell&gt;&lt;cell&gt;699050&lt;/cell&gt;&lt;cell&gt;256 / 256&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;19&lt;/cell&gt;&lt;cell&gt;0x380&lt;/cell&gt;&lt;cell&gt;0x378&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0xe000&lt;/cell&gt;&lt;cell&gt;599186&lt;/cell&gt;&lt;cell&gt;256 / 256&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;20&lt;/cell&gt;&lt;cell&gt;0x400&lt;/cell&gt;&lt;cell&gt;0x3f8&lt;/cell&gt;&lt;cell&gt;64&lt;/cell&gt;&lt;cell&gt;0x10000&lt;/cell&gt;&lt;cell&gt;524288&lt;/cell&gt;&lt;cell&gt;128 / 128&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;21&lt;/cell&gt;&lt;cell&gt;0x500&lt;/cell&gt;&lt;cell&gt;0x4f8&lt;/cell&gt;&lt;cell&gt;16&lt;/cell&gt;&lt;cell&gt;0x5000&lt;/cell&gt;&lt;cell&gt;1677721&lt;/cell&gt;&lt;cell&gt;128 / 128&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;22&lt;/cell&gt;&lt;cell&gt;0x600&lt;/cell&gt;&lt;cell&gt;0x5f8&lt;/cell&gt;&lt;cell&gt;16&lt;/cell&gt;&lt;cell&gt;0x6000&lt;/cell&gt;&lt;cell&gt;1398101&lt;/cell&gt;&lt;cell&gt;128 / 128&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;23&lt;/cell&gt;&lt;cell&gt;0x700&lt;/cell&gt;&lt;cell&gt;0x6f8&lt;/cell&gt;&lt;cell&gt;16&lt;/cell&gt;&lt;cell&gt;0x7000&lt;/cell&gt;&lt;cell&gt;1198372&lt;/cell&gt;&lt;cell&gt;128 / 128&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;24&lt;/cell&gt;&lt;cell&gt;0x800&lt;/cell&gt;&lt;cell&gt;0x7f8&lt;/cell&gt;&lt;cell&gt;16&lt;/cell&gt;&lt;cell&gt;0x8000&lt;/cell&gt;&lt;cell&gt;1048576&lt;/cell&gt;&lt;cell&gt;64 / 64&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;25&lt;/cell&gt;&lt;cell&gt;0xa00&lt;/cell&gt;&lt;cell&gt;0x9f8&lt;/cell&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;0x5000&lt;/cell&gt;&lt;cell&gt;1677721&lt;/cell&gt;&lt;cell&gt;64 / 64&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;26&lt;/cell&gt;&lt;cell&gt;0xc00&lt;/cell&gt;&lt;cell&gt;0xbf8&lt;/cell&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;0x6000&lt;/cell&gt;&lt;cell&gt;1398101&lt;/cell&gt;&lt;cell&gt;64 / 64&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;27&lt;/cell&gt;&lt;cell&gt;0xe00&lt;/cell&gt;&lt;cell&gt;0xdf8&lt;/cell&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;0x7000&lt;/cell&gt;&lt;cell&gt;1198372&lt;/cell&gt;&lt;cell&gt;64 / 64&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;28&lt;/cell&gt;&lt;cell&gt;0x1000&lt;/cell&gt;&lt;cell&gt;0xff8&lt;/cell&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;0x8000&lt;/cell&gt;&lt;cell&gt;1048576&lt;/cell&gt;&lt;cell&gt;32 / 32&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;29&lt;/cell&gt;&lt;cell&gt;0x1400&lt;/cell&gt;&lt;cell&gt;0x13f8&lt;/cell&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;0xa000&lt;/cell&gt;&lt;cell&gt;838860&lt;/cell&gt;&lt;cell&gt;32 / 32&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;30&lt;/cell&gt;&lt;cell&gt;0x1800&lt;/cell&gt;&lt;cell&gt;0x17f8&lt;/cell&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;0xc000&lt;/cell&gt;&lt;cell&gt;699050&lt;/cell&gt;&lt;cell&gt;32 / 32&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;31&lt;/cell&gt;&lt;cell&gt;0x1c00&lt;/cell&gt;&lt;cell&gt;0x1bf8&lt;/cell&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;0xe000&lt;/cell&gt;&lt;cell&gt;599186&lt;/cell&gt;&lt;cell&gt;32 / 32&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;32&lt;/cell&gt;&lt;cell&gt;0x2000&lt;/cell&gt;&lt;cell&gt;0x1ff8&lt;/cell&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;0x10000&lt;/cell&gt;&lt;cell&gt;524288&lt;/cell&gt;&lt;cell&gt;16 / 16&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;33&lt;/cell&gt;&lt;cell&gt;0x2800&lt;/cell&gt;&lt;cell&gt;0x27f8&lt;/cell&gt;&lt;cell&gt;6&lt;/cell&gt;&lt;cell&gt;0xf000&lt;/cell&gt;&lt;cell&gt;559240&lt;/cell&gt;&lt;cell&gt;16 / 16&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;34&lt;/cell&gt;&lt;cell&gt;0x3000&lt;/cell&gt;&lt;cell&gt;0x2ff8&lt;/cell&gt;&lt;cell&gt;5&lt;/cell&gt;&lt;cell&gt;0xf000&lt;/cell&gt;&lt;cell&gt;559240&lt;/cell&gt;&lt;cell&gt;16 / 16&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;35&lt;/cell&gt;&lt;cell&gt;0x3800&lt;/cell&gt;&lt;cell&gt;0x37f8&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;cell&gt;0xe000&lt;/cell&gt;&lt;cell&gt;599186&lt;/cell&gt;&lt;cell&gt;16 / 16&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;36&lt;/cell&gt;&lt;cell&gt;0x4000&lt;/cell&gt;&lt;cell&gt;0x3ff8&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;cell&gt;0x10000&lt;/cell&gt;&lt;cell&gt;524288&lt;/cell&gt;&lt;cell&gt;8 / 8&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;37&lt;/cell&gt;&lt;cell&gt;0x5000&lt;/cell&gt;&lt;cell&gt;0x4ff8&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0x5000&lt;/cell&gt;&lt;cell&gt;1677721&lt;/cell&gt;&lt;cell&gt;8 / 8&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;38&lt;/cell&gt;&lt;cell&gt;0x6000&lt;/cell&gt;&lt;cell&gt;0x5ff8&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0x6000&lt;/cell&gt;&lt;cell&gt;1398101&lt;/cell&gt;&lt;cell&gt;8 / 8&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;39&lt;/cell&gt;&lt;cell&gt;0x7000&lt;/cell&gt;&lt;cell&gt;0x6ff8&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0x7000&lt;/cell&gt;&lt;cell&gt;1198372&lt;/cell&gt;&lt;cell&gt;8 / 8&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;40&lt;/cell&gt;&lt;cell&gt;0x8000&lt;/cell&gt;&lt;cell&gt;0x7ff8&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0x8000&lt;/cell&gt;&lt;cell&gt;1048576&lt;/cell&gt;&lt;cell&gt;4 / 4&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;41&lt;/cell&gt;&lt;cell&gt;0xa000&lt;/cell&gt;&lt;cell&gt;0x9ff8&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0xa000&lt;/cell&gt;&lt;cell&gt;838860&lt;/cell&gt;&lt;cell&gt;4 / 4&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;42&lt;/cell&gt;&lt;cell&gt;0xc000&lt;/cell&gt;&lt;cell&gt;0xbff8&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0xc000&lt;/cell&gt;&lt;cell&gt;699050&lt;/cell&gt;&lt;cell&gt;4 / 4&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;43&lt;/cell&gt;&lt;cell&gt;0xe000&lt;/cell&gt;&lt;cell&gt;0xdff8&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0xe000&lt;/cell&gt;&lt;cell&gt;599186&lt;/cell&gt;&lt;cell&gt;4 / 4&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;44&lt;/cell&gt;&lt;cell&gt;0x10000&lt;/cell&gt;&lt;cell&gt;0xfff8&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0x10000&lt;/cell&gt;&lt;cell&gt;524288&lt;/cell&gt;&lt;cell&gt;2 / 2&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;45&lt;/cell&gt;&lt;cell&gt;0x14000&lt;/cell&gt;&lt;cell&gt;0x13ff8&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0x14000&lt;/cell&gt;&lt;cell&gt;419430&lt;/cell&gt;&lt;cell&gt;2 / 2&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;46&lt;/cell&gt;&lt;cell&gt;0x18000&lt;/cell&gt;&lt;cell&gt;0x17ff8&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0x18000&lt;/cell&gt;&lt;cell&gt;349525&lt;/cell&gt;&lt;cell&gt;2 / 2&lt;/cell&gt;&lt;/row&gt;&lt;row span="7"&gt;&lt;cell&gt;47&lt;/cell&gt;&lt;cell&gt;0x1c000&lt;/cell&gt;&lt;cell&gt;0x1bff8&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0x1c000&lt;/cell&gt;&lt;cell&gt;299593&lt;/cell&gt;&lt;cell&gt;2 / 2&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;48&lt;/cell&gt;&lt;cell&gt;0x20000&lt;/cell&gt;&lt;cell&gt;0x1fff8&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0x20000&lt;/cell&gt;&lt;cell&gt;262144&lt;/cell&gt;&lt;cell&gt;1 / 1&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Within each arena, an array of 49 &lt;code&gt;size_class&lt;/code&gt; entries maintains the metadata for every size class. For each class, the allocator reserves a dedicated memory region to hold its corresponding allocations. This region is segmented into slabs, which are in turn subdivided into slots. Each slot corresponds to a single memory chunk returned to the user.&lt;/p&gt;&lt;p&gt;The regions for all classes are reserved contiguously in memory when the allocator is initialized. Each region occupies 32 GiB of memory at a random offset within a 64 GiB area. The empty areas before and after the region act as page-aligned guards of a random size.&lt;/p&gt;&lt;p&gt;To summarize:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;A 32 GiB region is allocated per size class.&lt;/item&gt;&lt;item&gt;It is encapsulated at a random offset within a zone twice its size (64 GiB).&lt;/item&gt;&lt;item&gt;The 64 GiB zones are contiguous and ordered by increasing size class.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The size of the contiguous memory area reserved during initialization is &lt;code&gt;N_ARENA * 49 * 64 GiB&lt;/code&gt;. On the test devices, which use a single arena, this amounts to &lt;code&gt;0x31000000000&lt;/code&gt; bytes (~3 TB). By default, these pages are protected with &lt;code&gt;PROT_NONE&lt;/code&gt;, meaning they are not backed by physical memory. This protection is changed to Read/Write (RW) on demand for specific pages as allocations are needed.&lt;/p&gt;&lt;code&gt;
 // CONFIG_EXTENDED_SIZE_CLASSES := true
// CONFIG_LARGE_SIZE_CLASSES := true
// CONFIG_CLASS_REGION_SIZE := 34359738368 # 32GiB
// CONFIG_N_ARENA := 1

#define CLASS_REGION_SIZE (size_t)CONFIG_CLASS_REGION_SIZE
#define REAL_CLASS_REGION_SIZE (CLASS_REGION_SIZE * 2)
#define ARENA_SIZE (REAL_CLASS_REGION_SIZE * N_SIZE_CLASSES)
static const size_t slab_region_size = ARENA_SIZE * N_ARENA; // 0x31000000000 on Pixel 4a 5G and Pixel 9a

// ...

COLD static void init_slow_path(void) {
    // ...

    // Create a big mapping with MTE enabled
    ro.slab_region_start = memory_map_tagged(slab_region_size);
    if (unlikely(ro.slab_region_start == NULL)) {
        fatal_error("failed to allocate slab region");
    }
    void *slab_region_end = (char *)ro.slab_region_start + slab_region_size;
    memory_set_name(ro.slab_region_start, slab_region_size, "malloc slab region gap");
    // ...
}
&lt;/code&gt;&lt;p&gt;Each size class (or bin) is represented by the &lt;code&gt;size_class&lt;/code&gt; structure, a relatively large structure that holds all the relevant information for that class.&lt;/p&gt;&lt;code&gt;
 struct __attribute__((aligned(CACHELINE_SIZE))) size_class {
    struct mutex lock;

    void *class_region_start;
    struct slab_metadata *slab_info;
    struct libdivide_u32_t size_divisor;
    struct libdivide_u64_t slab_size_divisor;

#if SLAB_QUARANTINE_RANDOM_LENGTH &amp;gt; 0
    void *quarantine_random[SLAB_QUARANTINE_RANDOM_LENGTH &amp;lt;&amp;lt; (MAX_SLAB_SIZE_CLASS_SHIFT - MIN_SLAB_SIZE_CLASS_SHIFT)];
#endif

#if SLAB_QUARANTINE_QUEUE_LENGTH &amp;gt; 0
    void *quarantine_queue[SLAB_QUARANTINE_QUEUE_LENGTH &amp;lt;&amp;lt; (MAX_SLAB_SIZE_CLASS_SHIFT - MIN_SLAB_SIZE_CLASS_SHIFT)];
    size_t quarantine_queue_index;
#endif

    // slabs with at least one allocated slot and at least one free slot
    //
    // LIFO doubly-linked list
    struct slab_metadata *partial_slabs;

    // slabs without allocated slots that are cached for near-term usage
    //
    // LIFO singly-linked list
    struct slab_metadata *empty_slabs;
    size_t empty_slabs_total; // length * slab_size

    // slabs without allocated slots that are purged and memory protected
    //
    // FIFO singly-linked list
    struct slab_metadata *free_slabs_head;
    struct slab_metadata *free_slabs_tail;
    struct slab_metadata *free_slabs_quarantine[FREE_SLABS_QUARANTINE_RANDOM_LENGTH];

#if CONFIG_STATS
    u64 nmalloc; // may wrap (per jemalloc API)
    u64 ndalloc; // may wrap (per jemalloc API)
    size_t allocated;
    size_t slab_allocated;
#endif

    struct random_state rng;
    size_t metadata_allocated;
    size_t metadata_count;
    size_t metadata_count_unguarded;
};
&lt;/code&gt;&lt;p&gt;Its main members are:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;class_region_start&lt;/code&gt;: start address of the memory region for this class's slabs.&lt;/item&gt;&lt;item&gt;&lt;code&gt;slab_info&lt;/code&gt;: pointer to the beginning of the slab metadata array.&lt;/item&gt;&lt;item&gt;&lt;code&gt;quarantine_random&lt;/code&gt;,&lt;code&gt;quarantine_queue&lt;/code&gt;: arrays of pointers to allocations currently in quarantine (see the section on quarantines).&lt;/item&gt;&lt;item&gt;&lt;code&gt;partial_slabs&lt;/code&gt;: a stack of metadata for partially filled slabs.&lt;/item&gt;&lt;item&gt;&lt;code&gt;free_slabs_{head, tail}&lt;/code&gt;: a queue of metadata for empty slabs.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Slab metadata is held in the &lt;code&gt;slab_metadata&lt;/code&gt; structure. For any given size class, these structures form a contiguous array, accessible via the &lt;code&gt;size_class-&amp;gt;slab_info&lt;/code&gt; pointer. The layout of this metadata array directly mirrors the layout of the slabs in their memory region. This design allows for direct lookup: a slab's metadata can be found simply by using the slab's index to access the array.&lt;/p&gt;&lt;code&gt;
 struct slab_metadata {
    u64 bitmap[4];
    struct slab_metadata *next;
    struct slab_metadata *prev;
#if SLAB_CANARY
    u64 canary_value;
#endif
#ifdef SLAB_METADATA_COUNT
    u16 count;
#endif
#if SLAB_QUARANTINE
    u64 quarantine_bitmap[4];
#endif
#ifdef HAS_ARM_MTE
    // arm_mte_tags is used as a u4 array (MTE tags are 4-bit wide)
    //
    // Its size is calculated by the following formula:
    // (MAX_SLAB_SLOT_COUNT + 2) / 2
    // MAX_SLAB_SLOT_COUNT is currently 256, 2 extra slots are needed for branchless handling of
    // edge slots in tag_and_clear_slab_slot()
    //
    // It's intentionally placed at the end of struct to improve locality: for most size classes,
    // slot count is far lower than MAX_SLAB_SLOT_COUNT.
    u8 arm_mte_tags[129];
#endif
};
&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;bitmap[4]&lt;/code&gt;: bitmap tracking which slots in the slab are in use.&lt;/item&gt;&lt;item&gt;&lt;code&gt;next, prev&lt;/code&gt;: pointers to the next/previous elements when the structure belongs to a linked list (for example, in the stack of partially used slabs&lt;code&gt;size_class-&amp;gt;partial_slabs&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;&lt;code&gt;canary_value&lt;/code&gt;: canary value appended to the end of each slot within the slab (on non-MTE devices only). This value is verified upon&lt;code&gt;free&lt;/code&gt;to detect buffer overflows.&lt;/item&gt;&lt;item&gt;&lt;code&gt;arm_mte_tags[129]&lt;/code&gt;: MTE tags currently in use per slot&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Alloc&lt;/head&gt;&lt;p&gt;First, the actual size to be allocated is calculated by adding 8 bytes to the user's requested size. These extra bytes are filled with a canary and placed immediately after the data. An allocation is considered "small" only if this new size is less than &lt;code&gt;0x20000&lt;/code&gt; bytes (131,072 bytes) . Next, a free slot must be retrieved from a slab by following these steps:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Retrieve the arena: the current arena is fetched from the thread's local storage.&lt;/item&gt;&lt;item&gt;Get size class metadata: the metadata for the corresponding size class (the &lt;code&gt;size_class&lt;/code&gt;structure) is retrieved using&lt;code&gt;ro.size_class_metadata[arena][size_class]&lt;/code&gt;, where&lt;code&gt;arena&lt;/code&gt;is the arena number and&lt;code&gt;size_class&lt;/code&gt;is the index calculated from the allocation size.&lt;/item&gt;&lt;item&gt;Find a slab with a free slot: &lt;list rend="ul"&gt;&lt;item&gt;if a partially filled slab exists (&lt;code&gt;size_class-&amp;gt;partial_slabs != NULL&lt;/code&gt;), this slab is used.&lt;/item&gt;&lt;item&gt;otherwise, if at least one empty slab is available (&lt;code&gt;size_class-&amp;gt;empty_slabs != NULL&lt;/code&gt;), the first slab from this list is used.&lt;/item&gt;&lt;item&gt;if no slab is available, a new one is allocated (by allocating a &lt;code&gt;slab_metadata&lt;/code&gt;structure using the&lt;code&gt;alloc_metadata()&lt;/code&gt;function). A "guard" slab is reserved between each real slab.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;if a partially filled slab exists (&lt;/item&gt;&lt;item&gt;Select a random free slot: A free slot is chosen randomly from within the selected slab. Occupied slots are marked by &lt;code&gt;1&lt;/code&gt;s in the&lt;code&gt;slab_metadata-&amp;gt;bitmap&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;Select a MTE tag: A new MTE tag is chosen for the slot, ensuring it is different from adjacent tags to prevent simple linear overflows. The following tags are excluded: &lt;list rend="ul"&gt;&lt;item&gt;the previous slot's tag.&lt;/item&gt;&lt;item&gt;the next slot's tag.&lt;/item&gt;&lt;item&gt;the old tag of the currently selected slot.&lt;/item&gt;&lt;item&gt;the &lt;code&gt;RESERVED_TAG&lt;/code&gt;(0), which is used for freed allocations.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Set protections: &lt;list rend="ul"&gt;&lt;item&gt;on devices without MTE, the canary (which is common to all slots in the slab) is written into the last 8 bytes of the slot.&lt;/item&gt;&lt;item&gt;on MTE-enabled devices, these 8 bytes are set to 0.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Return the address of the slot, now tagged with the MTE tag.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;For a small allocation, the address returned by &lt;code&gt;malloc&lt;/code&gt; is a pointer to a slot with a 4-bit MTE tag encoded in its most significant bits. The pointers below, retrieved from successive calls to &lt;code&gt;malloc(8)&lt;/code&gt;, are located in the same slab but at random offsets and have different MTE tags.&lt;/p&gt;&lt;code&gt;
 ptr[0] = 0xa00cd70ad02a930
ptr[1] = 0xf00cd70ad02ac50
ptr[2] = 0x300cd70ad02a2f0
ptr[3] = 0x900cd70ad02a020
ptr[4] = 0x300cd70ad02ac90
ptr[5] = 0x700cd70ad02a410
ptr[6] = 0xc00cd70ad02a3c0
ptr[7] = 0x500cd70ad02a3d0
ptr[8] = 0xf00cd70ad02a860
ptr[9] = 0x600cd70ad02ad20&lt;/code&gt;&lt;p&gt;If an overflow occurs, a &lt;code&gt;SIGSEGV/SEGV_MTESERR&lt;/code&gt; exception is raised, indicating that an MTE-protected area was accessed with an incorrect tag. On GrapheneOS, this causes the application to terminate and sends a crash log to logcat.&lt;/p&gt;&lt;code&gt;
 07-23 11:32:19.948  4169  4169 F DEBUG   : Cmdline: /data/local/tmp/bin
07-23 11:32:19.948  4169  4169 F DEBUG   : pid: 4165, tid: 4165, name: bin  &amp;gt;&amp;gt;&amp;gt; /data/local/tmp/bin &amp;lt;&amp;lt;&amp;lt;
07-23 11:32:19.948  4169  4169 F DEBUG   : uid: 2000
07-23 11:32:19.949  4169  4169 F DEBUG   : tagged_addr_ctrl: 000000000007fff3 (PR_TAGGED_ADDR_ENABLE, PR_MTE_TCF_SYNC, mask 0xfffe)
07-23 11:32:19.949  4169  4169 F DEBUG   : pac_enabled_keys: 000000000000000f (PR_PAC_APIAKEY, PR_PAC_APIBKEY, PR_PAC_APDAKEY, PR_PAC_APDBKEY)
07-23 11:32:19.949  4169  4169 F DEBUG   : signal 11 (SIGSEGV), code 9 (SEGV_MTESERR), fault addr 0x0500d541414042c0
07-23 11:32:19.949  4169  4169 F DEBUG   :     x0  0800d541414042c0  x1  0000d84c01173140  x2  0000000000000015  x3  0000000000000014
07-23 11:32:19.949  4169  4169 F DEBUG   :     x4  0000b1492c0f16b5  x5  0300d6f2d01ea99b  x6  0000000000000029  x7  203d207972742029
07-23 11:32:19.949  4169  4169 F DEBUG   :     x8  5dde6df273e81100  x9  5dde6df273e81100  x10 0000000000001045  x11 0000000000001045
07-23 11:32:19.949  4169  4169 F DEBUG   :     x12 0000f2dbd10c1ca4  x13 0000000000000000  x14 0000000000000001  x15 0000000000000020
07-23 11:32:19.949  4169  4169 F DEBUG   :     x16 0000d84c0116e228  x17 0000d84c010faf50  x18 0000d84c1eb38000  x19 0500d541414042c0
07-23 11:32:19.949  4169  4169 F DEBUG   :     x20 0000000000001e03  x21 0000b1492c0f16e8  x22 0800d541414042c0  x23 0000000000000001
07-23 11:32:19.949  4169  4169 F DEBUG   :     x24 0000d541414042c0  x25 0000000000000000  x26 0000000000000000  x27 0000000000000000
07-23 11:32:19.949  4169  4169 F DEBUG   :     x28 0000000000000000  x29 0000f2dbd10c1f10
07-23 11:32:19.949  4169  4169 F DEBUG   :     lr  002bb1492c0f2ba0  sp  0000f2dbd10c1f10  pc  0000b1492c0f2ba4  pst 0000000060001000
&lt;/code&gt;&lt;p&gt;Â&lt;/p&gt;&lt;head rend="h4"&gt;Free&lt;/head&gt;&lt;p&gt;To free a small allocation, the allocator first determines its size class index from the pointer. This index allows it to locate the relevant metadata and the memory region where the data resides. The &lt;code&gt;slab_size_class&lt;/code&gt; function performs this initial calculation.&lt;/p&gt;&lt;code&gt;
 static struct slab_size_class_info slab_size_class(const void *p) {
    size_t offset = (const char *)p - (const char *)ro.slab_region_start;
    unsigned arena = 0;
    if (N_ARENA &amp;gt; 1) {
        arena = offset / ARENA_SIZE;
        offset -= arena * ARENA_SIZE;
    }
    return (struct slab_size_class_info){arena, offset / REAL_CLASS_REGION_SIZE};
}
&lt;/code&gt;&lt;p&gt;With this index, now referred to as &lt;code&gt;class_id&lt;/code&gt;, it is possible to gather various details about the slab containing the allocation:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;size_class&lt;/code&gt;structure:&lt;code&gt;size_class *c = &amp;amp;ro.size_class_metadata[size_class_info.arena][class_id]&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Allocation size: the size for this class is found using the &lt;code&gt;size_classes&lt;/code&gt;lookup table:&lt;code&gt;size_t size = size_classes[class_id]&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Slots per slab: the number of slots is found using the &lt;code&gt;size_class_slots&lt;/code&gt;lookup table:&lt;code&gt;slots = size_class_slots[class_id]&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Slab size: &lt;code&gt;slab_size = page_align(slots * size)&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Current slab metadata: &lt;code&gt;offset = (const char *)p - (const char *)c-&amp;gt;class_region_start&lt;/code&gt;&lt;code&gt;index = offset / slab_size&lt;/code&gt;&lt;code&gt;slab_metadata = c-&amp;gt;slab_info + index&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;With this information, the allocator can pinpoint the slab's base address and determine the specific slot's index and offset within that slab using the &lt;code&gt;get_slab()&lt;/code&gt; function.&lt;/p&gt;&lt;code&gt;
 static void *get_slab(const struct size_class *c, size_t slab_size, const struct slab_metadata *metadata) {
    size_t index = metadata - c-&amp;gt;slab_info;
    return (char *)c-&amp;gt;class_region_start + (index * slab_size);
}
&lt;/code&gt;&lt;p&gt;The slot's address is then deduced with the formula &lt;code&gt;slot = (const char*)slab - p&lt;/code&gt;, as is its index: &lt;code&gt;slot_index = ((const char*)slab - slot) / slots&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Once the slot is identified, a series of crucial security and integrity checks are performed to validate the &lt;code&gt;free&lt;/code&gt; operation:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Pointer alignment: the allocator verifies that the pointer is perfectly aligned with the start of a slot. Any misalignment indicates some kind of corruption, and the operation is immediately aborted.&lt;/item&gt;&lt;item&gt;Slot state: it then checks the slab's metadata to confirm the slot is currently marked as "in use."&lt;/item&gt;&lt;item&gt;Canary verification: the 8-byte canary at the end of the slot is checked for integrity. A key difference from scudo is that this canary is shared across the entire slab. This means a memory leak from one slot could theoretically allow an attacker to forge a valid canary for another slot and prevent a crash in case of a &lt;code&gt;free&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;MTE Tag Invalidation: the slot's MTE tag is reset to the reserved value (0), effectively invalidating the original pointer and preventing dangling pointer access.&lt;/item&gt;&lt;item&gt;Zero out: The slot's memory is completely wiped by zeroing it out.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If an invalid canary is detected, an &lt;code&gt;abort&lt;/code&gt; is called with the following message:&lt;/p&gt;&lt;code&gt;
 07-23 02:14:09.559  7610  7610 F libc    : hardened_malloc: fatal allocator error: canary corrupted
07-23 02:14:09.559  7610  7610 F libc    : Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 7610 (bin), pid 7610 (bin)
07-23 02:14:09.775  7614  7614 F DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
07-23 02:14:09.775  7614  7614 F DEBUG   : Build fingerprint: 'google/bramble/bramble:14/UP1A.231105.001.B2/2025021000:user/release-keys'
07-23 02:14:09.776  7614  7614 F DEBUG   : Revision: 'MP1.0'
07-23 02:14:09.776  7614  7614 F DEBUG   : ABI: 'arm64'
07-23 02:14:09.776  7614  7614 F DEBUG   : Timestamp: 2025-07-23 02:14:09.603643955+0200
07-23 02:14:09.776  7614  7614 F DEBUG   : Process uptime: 1s
07-23 02:14:09.776  7614  7614 F DEBUG   : Cmdline: /data/local/tmp/bin
07-23 02:14:09.776  7614  7614 F DEBUG   : pid: 7610, tid: 7610, name: bin  &amp;gt;&amp;gt;&amp;gt; /data/local/tmp/bin &amp;lt;&amp;lt;&amp;lt;
07-23 02:14:09.776  7614  7614 F DEBUG   : uid: 2000
07-23 02:14:09.776  7614  7614 F DEBUG   : signal 6 (SIGABRT), code -1 (SI_QUEUE), fault addr --------
07-23 02:14:09.776  7614  7614 F DEBUG   : Abort message: 'hardened_malloc: fatal allocator error: canary corrupted'
07-23 02:14:09.776  7614  7614 F DEBUG   :     x0  0000000000000000  x1  0000000000001dba  x2  0000000000000006  x3  0000ea4a84242960
07-23 02:14:09.776  7614  7614 F DEBUG   :     x4  716e7360626e6b6b  x5  716e7360626e6b6b  x6  716e7360626e6b6b  x7  7f7f7f7f7f7f7f7f
07-23 02:14:09.777  7614  7614 F DEBUG   :     x8  00000000000000f0  x9  0000cf1d482da2a0  x10 0000000000000001  x11 0000cf1d48331980
07-23 02:14:09.777  7614  7614 F DEBUG   :     x12 0000000000000004  x13 0000000000000033  x14 0000cf1d482da118  x15 0000cf1d482da050
07-23 02:14:09.777  7614  7614 F DEBUG   :     x16 0000cf1d483971e0  x17 0000cf1d48383650  x18 0000cf1d6fe40000  x19 0000000000001dba
07-23 02:14:09.777  7614  7614 F DEBUG   :     x20 0000000000001dba  x21 00000000ffffffff  x22 0000cc110ff0d150  x23 0000000000000000
07-23 02:14:09.777  7614  7614 F DEBUG   :     x24 0000000000000001  x25 0000cf0f4a421300  x26 0000000000000000  x27 0000cf0f4a421328
07-23 02:14:09.777  7614  7614 F DEBUG   :     x28 0000cf0f7ba30000  x29 0000ea4a842429e0
07-23 02:14:09.777  7614  7614 F DEBUG   :     lr  0000cf1d4831a9f8  sp  0000ea4a84242940  pc  0000cf1d4831aa24  pst 0000000000001000
&lt;/code&gt;&lt;p&gt;Finally, the slot is not immediately made available. Instead, it is placed into quarantine to delay its reuse, a key defense against use-after-free vulnerabilities.&lt;/p&gt;&lt;head rend="h4"&gt;Quarantines&lt;/head&gt;&lt;p&gt;Each allocation class uses a two-stage quarantine system for its freed slots. When an allocation is freed, it isn't immediately available for reuse but is passed instead through two distinct holding areas:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;A random quarantine: a fixed-size array where incoming slots replace a randomly chosen existing slot.&lt;/item&gt;&lt;item&gt;A queue quarantine: a First-In, First-Out queue that receives slots ejected from the random quarantine.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;When a slot enters the random quarantine, it overwrites a randomly selected entry. That ejected entry is then pushed into the queue quarantine. The queue then ejects its oldest element, which is finally made available for new allocations. This entire process is managed within each class's &lt;code&gt;size_class&lt;/code&gt; structure :&lt;/p&gt;&lt;code&gt;
 struct __attribute__((aligned(CACHELINE_SIZE))) size_class {
  // ...
  #if SLAB_QUARANTINE_RANDOM_LENGTH &amp;gt; 0
    void *quarantine_random[SLAB_QUARANTINE_RANDOM_LENGTH &amp;lt;&amp;lt; (MAX_SLAB_SIZE_CLASS_SHIFT - MIN_SLAB_SIZE_CLASS_SHIFT)];
#endif

#if SLAB_QUARANTINE_QUEUE_LENGTH &amp;gt; 0
    void *quarantine_queue[SLAB_QUARANTINE_QUEUE_LENGTH &amp;lt;&amp;lt; (MAX_SLAB_SIZE_CLASS_SHIFT - MIN_SLAB_SIZE_CLASS_SHIFT)];
    size_t quarantine_queue_index;
#endif
  // ...
}
&lt;/code&gt;&lt;p&gt;Â&lt;/p&gt;&lt;p&gt;This design is a significant shift from traditional allocators, which use a simple LIFO (Last-In, First-Out) freelist. In hardened malloc, the last item freed is never the first to be reallocated. To reclaim a specific slot, an attacker must trigger enough &lt;code&gt;free&lt;/code&gt; operations to successfully cycle their target slot through both the random quarantine and the queue quarantine. This adds a substantial layer of non-determinism and complexity to use-after-free exploits, providing a robust defense even on devices that lack MTE.&lt;/p&gt;&lt;p&gt;Since the allocator has no a freelist, the most straightforward way to force a reuse is to chain calls to &lt;code&gt;malloc&lt;/code&gt; and &lt;code&gt;free&lt;/code&gt;. The number of &lt;code&gt;free&lt;/code&gt; operations required depends on the quarantine sizes for that specific size class.&lt;/p&gt;&lt;code&gt;
 void reuse(void* target_ptr, size_t size) {
  free(target_ptr);
  for (int i = 0; ; i++) {
    void* new_ptr = malloc(size);
    if (untag(target_ptr) == untag(new_ptr)) {
      printf("REUSED [size = 0x%x] target_ptr @ %p (new_ptr == %p) try = %d\n", size, target_ptr, new_ptr, i);
      break;
    }
    free(new_ptr);
  }
}
&lt;/code&gt;&lt;p&gt;For an 8-byte allocation, both quarantines hold 8,192 elements. While this implies at least 8,192 frees are needed, the random nature of the first stage means the actual number is far greater. In testing, it required an average of ~19,000 &lt;code&gt;free&lt;/code&gt; operations to reliably reclaim a slot. The double quarantine turns predictable memory reuse into a costly and unreliable lottery, severely hindering a common exploit vector.&lt;/p&gt;&lt;head rend="h3"&gt;Large allocations&lt;/head&gt;&lt;head rend="h4"&gt;Alloc&lt;/head&gt;&lt;p&gt;Unlike small allocations, large allocations are not sorted by size into pre-reserved regions. Instead, the allocator maps them on demand. This mechanism is the only one in hardened malloc that dynamically creates memory mappings. The total size of the mapping depends on several factors:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Aligned Size: computed in &lt;code&gt;get_large_size_class&lt;/code&gt;by aligning the requested size to predefined classes, continuing from the small allocation sizes.&lt;/item&gt;&lt;item&gt;Guard Page Size: a random number of pages preceding and following the actual allocation.&lt;/item&gt;&lt;/list&gt;&lt;code&gt;
 static size_t get_large_size_class(size_t size) {
    if (CONFIG_LARGE_SIZE_CLASSES) {
        // Continue small size class growth pattern of power of 2 spacing classes:
        //
        // 4 KiB [20 KiB, 24 KiB, 28 KiB, 32 KiB]
        // 8 KiB [40 KiB, 48 KiB, 54 KiB, 64 KiB]
        // 16 KiB [80 KiB, 96 KiB, 112 KiB, 128 KiB]
        // 32 KiB [160 KiB, 192 KiB, 224 KiB, 256 KiB]
        // 512 KiB [2560 KiB, 3 MiB, 3584 KiB, 4 MiB]
        // 1 MiB [5 MiB, 6 MiB, 7 MiB, 8 MiB]
        // etc.
        return get_size_info(max(size, (size_t)PAGE_SIZE)).size;
    }
    return page_align(size);
}
&lt;/code&gt;&lt;p&gt;Once these sizes are determined, the allocator creates a mapping via &lt;code&gt;mmap()&lt;/code&gt; for their combined total. The guard areas before and after the data are mapped with &lt;code&gt;PROT_NONE&lt;/code&gt;, while the data region itself is mapped &lt;code&gt;PROT_READ|PROT_WRITE&lt;/code&gt;. This use of randomly sized guard pages means that two large allocations of the same requested size will occupy mapped areas of different total sizes, adding a layer of non-determinism.&lt;/p&gt;&lt;code&gt;
 void *allocate_pages_aligned(size_t usable_size, size_t alignment, size_t guard_size, const char *name) {
    //...

    // Compute real mapped size = alloc_size + 2 * guard_size
    size_t real_alloc_size;
    if (unlikely(add_guards(alloc_size, guard_size, &amp;amp;real_alloc_size))) {
        errno = ENOMEM;
        return NULL;
    }
    // Mapping whole region with PROT_NONE
    void *real = memory_map(real_alloc_size);
    if (unlikely(real == NULL)) {
        return NULL;
    }
    memory_set_name(real, real_alloc_size, name);

    void *usable = (char *)real + guard_size;

    size_t lead_size = align((uintptr_t)usable, alignment) - (uintptr_t)usable;
    size_t trail_size = alloc_size - lead_size - usable_size;
    void *base = (char *)usable + lead_size;

    // Change protection to usable data with PROT_RAD|PROT_WRITE
    if (unlikely(memory_protect_rw(base, usable_size))) {
        memory_unmap(real, real_alloc_size);
        return NULL;
    }

    //...
    return base;
}
&lt;/code&gt;&lt;p&gt;If the mapping is successful, a structure containing the address, usable size, and guard size is inserted into a hash table of regions. This hash table is implemented using two arrays of &lt;code&gt;region_metadata&lt;/code&gt; structs: &lt;code&gt;allocator_state.regions_a&lt;/code&gt; and &lt;code&gt;allocator_state.regions_b&lt;/code&gt; (referenced as &lt;code&gt;ro.regions[0]&lt;/code&gt; and &lt;code&gt;ro.regions[1]&lt;/code&gt;). These arrays have a static size and are reserved at initialization.&lt;/p&gt;&lt;p&gt;Initially, only a portion of these arrays is accessible (marked Read/Write); the rest is protected with &lt;code&gt;PROT_NONE&lt;/code&gt;. As the number of active large allocations grows and exceeds the available metadata slots, the accessible portion of the arrays is doubled. This expansion uses a two-table system: the current hash table is copied to the previously unused table, which then becomes the active one. The old table is re-mapped back to &lt;code&gt;PROT_NONE&lt;/code&gt; to render it inaccessible.&lt;/p&gt;&lt;code&gt;
 static int regions_grow(void) {
    struct region_allocator *ra = ro.region_allocator;

    if (ra-&amp;gt;total &amp;gt; SIZE_MAX / sizeof(struct region_metadata) / 2) {
        return 1;
    }

    // Compute new grown size
    size_t newtotal = ra-&amp;gt;total * 2;
    size_t newsize = newtotal * sizeof(struct region_metadata);
    size_t mask = newtotal - 1;

    if (newtotal &amp;gt; MAX_REGION_TABLE_SIZE) {
        return 1;
    }

    // Select new metadata array
    struct region_metadata *p = ra-&amp;gt;regions == ro.regions[0] ?
        ro.regions[1] : ro.regions[0];

    // Enlarge new metadata elements
    if (memory_protect_rw_metadata(p, newsize)) {
        return 1;
    }

    // Copy elements to the new array
    for (size_t i = 0; i &amp;lt; ra-&amp;gt;total; i++) {
        const void *q = ra-&amp;gt;regions[i].p;
        if (q != NULL) {
            size_t index = hash_page(q) &amp;amp; mask;
            while (p[index].p != NULL) {
                index = (index - 1) &amp;amp; mask;
            }
            p[index] = ra-&amp;gt;regions[i];
        }
    }

    memory_map_fixed(ra-&amp;gt;regions, ra-&amp;gt;total * sizeof(struct region_metadata));
    memory_set_name(ra-&amp;gt;regions, ra-&amp;gt;total * sizeof(struct region_metadata), "malloc allocator_state");
    ra-&amp;gt;free = ra-&amp;gt;free + ra-&amp;gt;total;
    ra-&amp;gt;total = newtotal;

    // Switch current metadata array/hash table
    ra-&amp;gt;regions = p;
    return 0;
}
&lt;/code&gt;&lt;p&gt;Eventually, allocation metadata, &lt;code&gt;address + size + guard size&lt;/code&gt;, is inserted in the current hash table &lt;code&gt;ro.region_allocator-&amp;gt;regions&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;For large allocations, which are not protected by MTE, the randomly sized guard pages are the primary defense against overflows. If an attacker can bypass this randomization and has an out-of-bounds read/write vulnerability with a precise offset, corrupting adjacent data remains a possible, though complex, scenario.&lt;/p&gt;&lt;p&gt;For example, a call to &lt;code&gt;malloc(0x28001)&lt;/code&gt; creates the following metadata. A random guard size of &lt;code&gt;0x18000&lt;/code&gt; bytes was chosen by the allocator.&lt;/p&gt;&lt;code&gt;
 large alloc @ 0xc184d36f4ac8
  ptr       : 0xbe6cadf4c000
  size      : 0x30000
  guard size: 0x18000&lt;/code&gt;&lt;p&gt;By inspecting the process's memory maps, we can see that the large allocation (which aligns to a size of &lt;code&gt;0x30000&lt;/code&gt;) is securely sandwiched between two &lt;code&gt;PROT_NONE&lt;/code&gt; guard regions, each &lt;code&gt;0x18000&lt;/code&gt; bytes in size.&lt;/p&gt;&lt;code&gt;
 be6cadf34000-be6cadf4c000 ---p 00000000 00:00 0
be6cadf4c000-be6cadf7c000 rw-p 00000000 00:00 0
be6cadf7c000-be6cadf94000 ---p 00000000 00:00 0&lt;/code&gt;&lt;head rend="h4"&gt;Free&lt;/head&gt;&lt;p&gt;Freeing a large allocation is a relatively simple process that uses the same quarantine mechanism as small allocations.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Calculate Pointer Hash: the hash of the pointer is calculated to locate its metadata.&lt;/item&gt;&lt;item&gt;Retrieve Metadata: the allocation's metadata structure is retrieved from the current hash table (&lt;code&gt;ro-&amp;gt;region_allocator.regions&lt;/code&gt;).&lt;/item&gt;&lt;item&gt;Quarantine or Unmap: the next step depends on the allocation's size. &lt;list rend="ul"&gt;&lt;item&gt;If the size is less than &lt;code&gt;0x2000000&lt;/code&gt;(32 MiB), the allocation is placed into a two-stage quarantine system identical to the one for small allocations (a random-replacement cache followed by a FIFO queue). This quarantine is global for all large allocations and is managed in&lt;code&gt;ro.region_allocator&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;If the size is &lt;code&gt;0x2000000&lt;/code&gt;or greater, or when an allocation is ejected from the quarantine, it is immediately unmapped from memory. The entire memory region, including the data area and its surrounding guard pages, is unmapped using&lt;code&gt;munmap()&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;munmap((char *)usable - guard_size, usable_size + guard_size * 2);&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;If the size is less than &lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Conclusion&lt;/head&gt;&lt;p&gt;Hardened Malloc is a security-hardened memory allocator that implements several advanced protection mechanisms, most notably leveraging the ARM Memory Tagging Extension (MTE) to detect and prevent memory corruption. While it offers an improvement over the standard scudo allocator, particularly against use-after-free vulnerabilities, its true strength lies in its integration with GrapheneOS. This combination achieves a higher level of security than a typical Android device that uses scudo.&lt;/p&gt;&lt;p&gt;Furthermore, the use of canaries and numerous guard pages complements its arsenal, especially on older devices without MTE, by quickly triggering exceptions in case of unwanted memory access.&lt;/p&gt;&lt;p&gt;From an attacker's perspective, hardened malloc significantly reduces opportunities to exploit memory corruption vulnerabilities:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Heap overflow: hardened malloc is relatively similar to scudo, yet it adds guard pages between slabs, which prevents an overflow from spreading from one slab to another. However, with MTE enabled, the protection becomes much more granular: even an overflow within the same slab (from one slot to another) is detected and blocked without the need to check canaries, making the exploitation of this type of vulnerability nearly impossible.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Use-after-free: the double quarantine mechanism complicates the reuse of a freed memory region but does not make it entirely impossible. However, MTE radically changes the deal. The pointer and its associated memory region are "tagged." Upon being freed, this tag is modified. Any subsequent attempt to use the old pointer (with its now-invalid tag) will very likely raise an exception, neutralizing the attack. For large allocations, which are not covered by MTE, the strategy is different: each allocation is isolated by guard pages and its location in memory is randomized. This combination of isolation and randomization makes any attempt to reuse these memory regions difficult and unreliable for an attacker.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Additionally, its implementation has proven to be particularly clear and concise, facilitating its audit and maintenance.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.synacktiv.com/en/publications/exploring-grapheneos-secure-allocator-hardened-malloc"/><published>2025-09-24T09:56:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45358280</id><title>S3 scales to petabytes a second on top of slow HDDs</title><updated>2025-09-24T14:39:04.836116+00:00</updated><content>&lt;doc fingerprint="365cf1f6358e8e8d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;how AWS S3 serves 1 petabyte per second on top of slow HDDs&lt;/head&gt;
    &lt;head rend="h3"&gt;Learn how Amazon built the backbone of the modern web that scales to 1 PB/s and 150M QPS on commodity hard drives&lt;/head&gt;
    &lt;p&gt;Everyone knows what AWS S3 is, but few comprehend the massive scale it operates at, nor what it took to get there.&lt;/p&gt;
    &lt;p&gt;In essence - it’s a scalable multi-tenant storage service with APIs to store and retrieve objects, offering extremely high availability1 and durability2 at a relatively low cost3.&lt;/p&gt;
    &lt;head rend="h1"&gt;Scale&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;400+ trillion4 objects&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;150 million requests a second (150,000,000/s)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;&amp;gt; 1 PB/s of peak traffic&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;tens of millions of disks&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Behind It All?&lt;/head&gt;
    &lt;p&gt;Hard drives.&lt;/p&gt;
    &lt;p&gt;How S3 achieves this scale is an engineering marvel. To understand and appreciate the system, we first must appreciate its core building block - the hard drive.&lt;/p&gt;
    &lt;p&gt;Hard Disk Drives (HDDs) are an old, somewhat out-of-favor technology largely superseded by SSDs. They are physically fragile, constrained for IOPS and high in latency.&lt;/p&gt;
    &lt;p&gt;But they nailed something flash still hasn’t: dirt cheap commodity economics:&lt;/p&gt;
    &lt;p&gt;Over their lifetime, HDDs have seen exponential improvement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;price: 6,000,000,000x cheaper per byte (inflation-adjusted)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;capacity: increased 7,200,000x&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;size: decreased 5,000x&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;weight: decreased 1,235x&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But one issue has consistently persisted - they’re constrained for IOPS. They have been stuck at 120 IOPS for the last 30 years.&lt;lb/&gt;Latency also hasn’t kept up in the same pace as the rest.&lt;/p&gt;
    &lt;p&gt;This means that per byte, HDDs are becoming slower.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why are HDDs slow?&lt;/head&gt;
    &lt;p&gt;HDDs are slow because of physics.&lt;/p&gt;
    &lt;p&gt;They require real-world mechanical movement to read data. (unlike SSDs, which use electricity travelling at ~50% the speed of light). Here is a good visualization:&lt;/p&gt;
    &lt;p&gt;The platter spins around the spindle at about 7200 rounds per minute (RPM)5.&lt;/p&gt;
    &lt;p&gt;The mechanical arm (actuator) with its read/write head physically moves across the platter and waits for it to rotate until it gets to the precise LBA address where the data resides.&lt;/p&gt;
    &lt;p&gt;Accessing data from the disk therefore involves two mechanical operations and one electrical.&lt;/p&gt;
    &lt;p&gt;That physical movements are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;seek - the act of the actuator moving left or right to the correct track on the platter&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;full-platter seek time: ~8ms&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;half-platter seek time (avg): ~4ms&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;rotation - waiting for the spindle to spin the disk until it matches the precise address on the platter’s track&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;full rotational latency: ~8.3ms6&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;half rotational latency (avg): ~4ms&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then the electrical one:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;transfer rate - the act of the head shoving bits off the platter across the bus into memory (the drive’s internal cache)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;reading 0.5MB: ~2.5ms on average7&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Sequential I/O&lt;/head&gt;
    &lt;p&gt;Hard Drives are optimized for sequential access patterns.&lt;/p&gt;
    &lt;p&gt;Reading/writing bytes that are laid out consecutively on the disk is fast. The natural rotation of the platter cycles through the block of bytes and no excessive seeks need to be performed (the actuator stays still).&lt;/p&gt;
    &lt;p&gt;The easiest and most popular data structure with sequential access patterns is the Log. Popular distributed systems like Apache Kafka are built on top of it and through sequential access patterns squeeze out great performance off cheap hardware.&lt;/p&gt;
    &lt;p&gt;It is no surprise that S3’s storage backend - ShardStore - is based on a log-structured merge tree (LSM) itself.&lt;/p&gt;
    &lt;p&gt;In essence, writes for S3 is easy. Because they write sequentially to the disk, they take advantage of the HDD’s performance. (similar to Kafka, I bet they batch pending PUTs so as to squeeze out more sequential throughput on disk via appends to the log)8&lt;/p&gt;
    &lt;p&gt;Reads, however, are trickier. AWS can’t control what files the user requests - so they have to jump around the drive when serving them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Random I/O&lt;/head&gt;
    &lt;p&gt;In the average case, a read on a random part of the drive would involve half of the full physical movement.&lt;/p&gt;
    &lt;p&gt;The average read latency is the sum of both average physical movements plus the transfer rate. Overall, you’re looking at ~11ms on average to read 0.5 MB of random I/O from a drive. That’s very slow.&lt;/p&gt;
    &lt;p&gt;Since a second has 1000 milliseconds, you’d only achieve ~45MB/s of random I/O from a single drive.&lt;/p&gt;
    &lt;p&gt;Because physical movements are a bottleneck - disks have been stuck at this same random I/O latency for the better part of 30 years.&lt;/p&gt;
    &lt;p&gt;They are simply not efficient under random access patterns. That’s when you’d opt for SSDs. But if you have to store massive amounts of data - SSDs become unaffordable.9&lt;/p&gt;
    &lt;p&gt;This becomes a pickle when you are S3 - a random access system10 that also stores massive amounts of data.&lt;/p&gt;
    &lt;p&gt;Yet, S3 found a way to do it - it delivers tolerable latency11 and outstanding12 throughput while working around the physical limitations.&lt;/p&gt;
    &lt;head rend="h1"&gt;Need for Parallelism&lt;/head&gt;
    &lt;p&gt;S3 solves this problem through massive parallelism.&lt;/p&gt;
    &lt;p&gt;They spread the data out in many (many!13) hard drives so they can achieve massive read throughput by utilizing each drive in parallel.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Storing a 1 TB file in a single HDD means limits your reading rate by that single drive’s max throughput (~300 MB/s14).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Splitting that same 1 TB file across 20,000 different HDDs means you can read it in parallel at the sum of all HDDs’ throughput (TB/s).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They do this via Erasure Coding.&lt;/p&gt;
    &lt;head rend="h2"&gt;Erasure Coding&lt;/head&gt;
    &lt;p&gt;Redundancy schemes are common practice in storage systems.&lt;/p&gt;
    &lt;p&gt;They are most often associated with data durability - protecting against data loss when hardware fails.&lt;/p&gt;
    &lt;p&gt;S3 uses Erasure Coding (EC). It breaks data into K shards with M redundant “parity” shards. EC allows you to reconstruct the data from any K shards out of the total K+M shards.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The S3 team shares they use a 5-of-9 scheme. They shard each object into 9 pieces - 5 Regular Shards (K) and 4 Parity Shards (M)&lt;/p&gt;
      &lt;p&gt;This approach tolerates up to 4 losses. To access the object, they need 5/9 shards.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This scheme helps S3 find a middle balance - it doesn’t take much extra disk capacity yet still provides flexible I/O.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;EC makes them store 1.8x the original data.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;A naive alternative like 3-way replication would result in 3x the data. That extra 1.2x starts to matter when we’re talking hundreds of exabytes.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;EC gives them 9 possible read sources - an ample hedge against bottlenecks&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;3-way replication would only give them 3 sources. If all 3 nodes are hot, performance would suffer.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;9 read sources also offer much more burst demand I/O due to parallelism&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;An under-appreciated aspect of EC is precisely its ability to distribute load. Such schemes spread the hot spots of a system out and give it the flexibility to steer read traffic in a balanced way. And since shards are small, firing off hedge requests15 to dodge stragglers is far cheaper than with full replicas.&lt;/p&gt;
    &lt;head rend="h2"&gt;Parallelism in Action&lt;/head&gt;
    &lt;p&gt;S3 leverages parallelism in three main ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;From the user’s perspective - upload/download the file in chunks.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;From the client’s perspective - send requests to multiple different front-end servers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;From the server’s perspective - store an object in multiple storage servers.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Any part of the end-to-end path can become a bottleneck, so it’s important to optimize everything.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Across Front-end Servers&lt;/head&gt;
    &lt;p&gt;Instead of requesting all the files through one connection to one S3 endpoint, users are encouraged to open as many connections as necessary. This happens behind the scenes in the library code through an internal HTTP connection pool.&lt;/p&gt;
    &lt;p&gt;This approach utilizes many different endpoints of the distributed system, ensuring no single point in the infrastructure becomes too hot (e.g. front-end proxies, caches, etc)&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Across Hard Drives&lt;/head&gt;
    &lt;p&gt;Instead of storing the data in a single hard-drive, the system breaks it into shards via EC and spreads it out across multiple storage back ends.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Across PUT/GET Operations&lt;/head&gt;
    &lt;p&gt;Instead of sending one request through a single thread and HTTP connection, the client chunks it into 10 parts and uploads each in parallel.16&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;PUT requests support multipart upload, which AWS recommends in order to maximize throughput by leveraging multiple threads.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;GET requests similarly support an HTTP header denoting you read only a particular range of the object (called byte-ranged GET). AWS again recommends this for achieving higher aggregate throughput instead of the single object read request.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Uploading 1 GB/s to a single server may be difficult, but uploading 100 chunks each at 10 MB/s chunks to 100 different servers is very practical.&lt;/p&gt;
    &lt;p&gt;This simple idea goes a long way.&lt;/p&gt;
    &lt;head rend="h1"&gt;Avoiding Hot Spots&lt;/head&gt;
    &lt;p&gt;S3 now finds itself with a difficult problem. They have tens of millions of drives, hundreds of millions of parallel requests per second and hundreds of millions of EC shards to persist per second.&lt;/p&gt;
    &lt;p&gt;How do they spread this load around effectively so as to avoid certain nodes/disks overheating?&lt;/p&gt;
    &lt;p&gt;As we said earlier - a single disk can do around ~45 MB/s of random IOs. It seems trivial to hit that bottleneck. Not to mention any additional system maintenance work like rebalancing data around for more efficient spreading would also take valuable IOs off the disk.&lt;/p&gt;
    &lt;p&gt;Forming hot spots in a distributed system is dangerous, because it can easily cause a domino-like spiral into system-wide degradation17.&lt;/p&gt;
    &lt;p&gt;Needless to say, S3 is very careful in trying to spread data around. Their solution is again deceptively simple:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;randomize where you place data on ingestion&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;continuously rebalance it&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;scale &amp;amp; chill&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Shuffle Sharding &amp;amp; Power of Two&lt;/head&gt;
    &lt;p&gt;Where you place data initially is key to performance. Moving it later is more expensive.&lt;/p&gt;
    &lt;p&gt;Unfortunately, at write time you have no good way of knowing whether the data you’re about to persist is going to be accessed frequently or not.&lt;/p&gt;
    &lt;p&gt;Knowing the perfect the least-loaded HDD to place new data in is also impossible at this scale. You can’t keep a synchronous globally-consistent view when you are serving hundreds of millions of requests per second across tens of millions of drives. This approach would also risk load correlation - placing similar workloads together and having them burst together at once.&lt;/p&gt;
    &lt;p&gt;A key realization is that picking at random works better in this scenario. 💡&lt;/p&gt;
    &lt;p&gt;It’s how AWS intentionally engineers decorrelation into their system:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;A given PUT picks a random set of drives&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The next PUT, even if it’s targetting the same key/bucket, picks a different set of near-random drives.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The way they do it is through the so-called Power of Two Random Choices:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Power of Two Random Choices: a well-studied phenomenon in load balancing that says choosing between the least-loaded of two completely random nodes yields much better results than choosing just one node at random.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Rebalancing&lt;/head&gt;
    &lt;p&gt;Another key realization is that newer data chunks are hotter than older ones. 💡&lt;/p&gt;
    &lt;p&gt;Fresh data is accessed more frequently. As it grows older, it gets accessed less.&lt;lb/&gt;All hard drives therefore eventually cool off in usage as they get filled with data and said data ages. The result is full storage capacity with ample I/O capacity.&lt;/p&gt;
    &lt;p&gt;AWS has to proactively rebalance the cold data out (so as to free up space) and rebalance cold data in (so as to make use of the free I/O).&lt;/p&gt;
    &lt;p&gt;Data rebalances are also needed when new racks of disks are added to S3. Each rack contains 20 PB of capacity18, and every disk in there is completely empty. The system needs to proactively spread the load around the new capacity.&lt;/p&gt;
    &lt;p&gt;Suffice to say - S3 constantly rebalances data around.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chill@Scale&lt;/head&gt;
    &lt;p&gt;The last realization is perhaps the least intuitive: the larger the system becomes, the more predictable it is. 💡&lt;/p&gt;
    &lt;p&gt;AWS experienced so-called workload decorrelation as S3 grew. That is the phenomenon of seeing a smoothening of load once it’s aggregated on a large enough scale. While their peak demand is growing in size, their peak-to-mean delta is collapsing.&lt;/p&gt;
    &lt;p&gt;This is because storage workloads are inherently very bursty - they demand a lot at once, and then may remain idle for a long time (months).&lt;/p&gt;
    &lt;p&gt;Because independent workloads do not burst together, the more workloads you cram together - the more those idle spots get filled up and the more predictable the system becomes in aggregate. 💡&lt;/p&gt;
    &lt;p&gt;copyright: AWS; from this re:Invent presentation.&lt;/p&gt;
    &lt;head rend="h1"&gt;Summary&lt;/head&gt;
    &lt;p&gt;AWS S3 is a massively multi-tenant storage service. It’s a gigantic distributed system consisting of many individually slow nodes that on aggregate allow you to access data faster than any single node can provide. S3 achieves this through:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;massive parallelization across the end-to-end path (user, client, server)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;neat load-balancing tricks like the power of two random&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;spreading out data via erasure coding&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;lowering tail latency via hedge requests&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;the economies of multi-tenancy at world scale&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It started as a service optimized for backups, video and image storage for e-commerce websites - but eventually grew support being the main storage system used for analytics and machine learning on massive data lakes.&lt;/p&gt;
    &lt;p&gt;Nowadays, the growing trend is for entire data infrastructure projects to be based on top of S3. This gives them the benefits of stateless nodes (easy scaling, less management) while outsourcing difficult durability, replication and load-balancing problems to S3. And get this - it also reduces cloud costs.19&lt;/p&gt;
    &lt;p&gt;Subscribe for more interesting dives in big data distributed systems (or the occassional small data gem).&lt;/p&gt;
    &lt;head rend="h1"&gt;References&lt;/head&gt;
    &lt;p&gt;S3 has a lot of other goodies up its bag, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;shuffle sharding at the DNS level&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;client library hedging requests by cancelling slow requests that pass the p95 threshold and sending new ones to a different host&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;software updates done erasure-coding-style, including rolling out their brand-new ShardStore storage system without any impact to their fleet&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;conway’s law and how it shapes S3’s architecture (consisting of 300+ microservices)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;their durability culture, including continuous detection, durable chain of custody, a design process that includes durability threat modelling and formal verification&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are generally shared in their annual S3 Deep Dive at re:Invent:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;2022 (video)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2023 (video)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2024 (video)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Building and operating a pretty big storage system called S3 (article)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you to the S3 team for sharing what they’ve built, and thank you for reading!&lt;/p&gt;
    &lt;head rend="h1"&gt;Side Quest Reads 👇&lt;/head&gt;
    &lt;p&gt;S3 has never been down for more than 5 hours in its entire existence. And that incident was 8 years ago, in just one region (out of 38) in AWS. It was considered one of AWS' most impactful outages of all time.&lt;/p&gt;
    &lt;p&gt;S3 markets itself as being designed for 11 nines of durability. Careful with the wording - they don’t legally promise 99.999999999% durability. In fact, Amazon does not legally provide any SLA for durability.&lt;/p&gt;
    &lt;p&gt;By relative low cost, I mean relative to the other storage you can buy on AWS. S3 is still ~$21.5-$23.5 per TB of storage. In fact, S3 hasn’t lowered its prices in 8 years despire HDD prices falling 60% since. When I ran back-of-the-napkin maths for what it’d cost for me to build my own S3 bare metal, the cost came out to $0.875 per TB of storage (25x cheaper). Alternatively, hosting it on Hetzner would be around $5.73 per TB.&lt;/p&gt;
    &lt;p&gt;400,000,000,000,000&lt;/p&gt;
    &lt;p&gt;The first 7200 rpm drive was the Seagate Barracuda released in 1992. Today, rpm has largely remained unchanged. Larger 15k-ish rpm drives exist, but aren’t super common.&lt;/p&gt;
    &lt;p&gt;7200 rotations per minute == 7200 rotations per 60000 milliseconds == 8.33ms per rotation&lt;/p&gt;
    &lt;p&gt;HDDs on average have ~170-200 MB/s transfer rate; 200mb / 1000ms == 0.2 mb/ms; 0.5 mb == ~2.5ms; They’re simply not optimized for random access like this.&lt;/p&gt;
    &lt;p&gt;Kafka loves batching entries. It batches on the client (by waiting), it batches in the protocol (by merging entries) and it batches on the server (by storing in page cache and utilizing the OS’ async flush). It’s such an obvious perf gain that S3 must do something similar in their back-end and storage system.&lt;/p&gt;
    &lt;p&gt;although this is slowly but surely beginning to change for certain data thresholds. SSDs have massively deflated in price in just the last 15 years.&lt;/p&gt;
    &lt;p&gt;In aggregate, S3 exhibits random access. You as a tenant can PUT/GET any blobs of any size. An average S3 disk would therefore consist of blobs from thousands of tenants. If they all attempt to access their data simultaneously, the drive simply cannot serve every request at once.&lt;/p&gt;
    &lt;p&gt;Not a lot of benchmark actually exist here. Testing 0.5MB files, I got writes at ~140ms p99 and 26ms p50, reads at 86ms p99. Larger files allegedly get larger p99, and they vary throughout the week.&lt;/p&gt;
    &lt;p&gt;At least one public number is Anthropic driving tens of terabytes per second. There are likely much larger single customer workloads out there - S3 does more than a petabyte a second!&lt;/p&gt;
    &lt;p&gt;AWS shares that tens of thousands of their customers have their data spread over 1,000,000 disks. This is a great example of how multi-tenancy at scale can convert the financially impossible into the affordable. It would be prohibitively expensive for any single tenant to deploy a million HDDs themselves, but when shared behind a multi-tenant system - it becomes surprisingly cheap.&lt;/p&gt;
    &lt;p&gt;e.g a modern cheap 20TB HDD maxes out at around 291 MB/s of data transfer: https://www.westerndigital.com/products/internal-drives/wd-gold-sata-hdd?sku=WD203KRYZ; note this is marketing numbers too&lt;/p&gt;
    &lt;p&gt;The concept of a hedge request was popularized by this Google paper “The Tail at Scale”. It essentially talks about how fanout requests (where a root request results in many sub-requests, e.g like S3’s GETs requesting multiple shards) can significantly reduce their tail latency by speculatively sending extra requests (i.e if you need 5 sub-requests to build an object - send 6). This extra request is sent only once one of the sub-requests surpasses the usual p95 latency. S3’s client libraries also utilize this concept.&lt;/p&gt;
    &lt;p&gt;An interesting detail is that each part of the multi-part upload must be getting Erasure Coded 5-of-9 too. So a single object uploaded through multipart upload can consist of hundreds of shards.&lt;/p&gt;
    &lt;p&gt;If too many requests hit the same disk at the same point in time, the disk starts to stall because its limited I/O is exhausted. This accumulates tail latency to requests that depend on the drive. This delay impacts other operations like writes. It also gets amplified up the stack in other components beyond the drive. If left unchecked, it can cause a cascade that significantly slows down the whole system.&lt;/p&gt;
    &lt;p&gt;As someone with no data center experience, I find it super cool when Amazon shares pictures of what the physical disks look like. Here is an example of one such rack of disks. It consists of 1000 drives - 20TB each. It’s said this rack weighs more than a car, and Amazon had to reinforce the flooring in their data centers to support it.&lt;/p&gt;
    &lt;p&gt;Apache Kafka (what I am most familiar with) has been seeing the so-called “Diskless” trend where the write path uses S3 instead of local disks. This trades off higher latency for lower costs (by 90% [!]). Similar projects exist - Turbopuffer (Vector DB built on S3), SlateDB (embedded LSM on S3), Nixiesearch (Lucene on S3). In general, every data infra project seems to be offloading as much as possible to object storage. (Clickhouse, OpenSearch, Elastic). Before Diskless, Kafka similarly used a two-tier approach where cold data was offloaded to S3 (for a 10x storage cost saving)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives"/><published>2025-09-24T10:05:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45358433</id><title>My game's server is blocked in Spain whenever there's a football match on</title><updated>2025-09-24T14:39:04.770163+00:00</updated><content/><link href="https://old.reddit.com/r/gamedev/comments/1np6kyn/my_games_server_is_blocked_in_spain_whenever/"/><published>2025-09-24T10:26:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45358527</id><title>Preparing for the .NET 10 GC</title><updated>2025-09-24T14:39:04.526302+00:00</updated><content>&lt;doc fingerprint="2e6b5a97a927d570"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Preparing for the .NET 10 GC&lt;/head&gt;
    &lt;p&gt;In .NET 9 we enabled DATAS by default. But .NET 9 is not an LTS release so for many people they will be getting DATAS for the first time when they upgrade to .NET 10. This was a tough decision because GC features are usually the kind that don’t require user intervention — but DATAS is a bit different. That’s why this post is titled “preparing for” instead of just “what’s new” 😊.&lt;/p&gt;
    &lt;p&gt;If you’re using Server GC, you might notice a performance profile that’s more noticeably different than what you saw in previous runtime upgrades. Memory usage may look drastically different (very likely smaller) — and that may or may not be desirable. It all depends on whether the tradeoff is noticeable, and if it is, whether it aligns with your optimization goals. I’d recommend taking at least a quick look at your application performance metrics to see if you are happy with the results of this change. Many people will absolutely welcome it — but if you are not one of them, don’t panic. I encourage you to read on to see whether it makes sense to simply turn DATAS off or if a bit of tuning could make it work in your favor.&lt;/p&gt;
    &lt;p&gt;I’ll talk about how we generally decide which performance features to add, why DATAS is so different from typical GC features, and the tuning changes introduced since my last DATAS blog post. I’ll also share two examples of how I tuned DATAS in first-party scenarios.&lt;/p&gt;
    &lt;p&gt;If you’re mainly here to see which scenarios DATAS isn’t designed for — to help decide whether to turn it off — feel free to skip ahead to this section.&lt;/p&gt;
    &lt;head rend="h2"&gt;General policies of adding GC performance features&lt;/head&gt;
    &lt;p&gt;Most GC performance features — whether it’s a new GC flavor, a new mechanism that enables the GC to do something it couldn’t before, or optimizations that improve an existing mechanism — are typically lit up automatically when you upgrade to a new runtime version. We don’t require users to take action because these features are designed to improve a wide range of scenarios. In fact, that’s often why we choose to implement them: we analyze many scenarios to understand the most common problems, figure out what it would take to solve them, and then prioritize which ones to design and implement.&lt;/p&gt;
    &lt;p&gt;Of course, with any performance changes, there’s always the risk of regressions — and for a framework used by millions, you’re guaranteed to regress someone. These regressions can be especially visible in microbenchmarks, where the behavior is so extreme that even small changes can cause wild swings in results.&lt;/p&gt;
    &lt;p&gt;A recent example being the change we made in how we handle the free regions for UOH (ie, LOH + POH) generations. We changed from the budget based trimming policy to an age based because it’s more robust in general (so we don’t either quickly decommit memory and have to recommit again, or keep extra free regions around even after a long time because we continue not consuming nearly all of the UOH budgets). But this can totally change a microbenchmark that used to observe the primary memory go down to a very low value after one GC.Collect() now requires 3 GC.Collect() calls (because we have to wait for the UOH free regions to age out in 2 gen2 GCs and the 3rd one will put it on the decommit list).&lt;/p&gt;
    &lt;p&gt;But for DATAS, we knew it was by definition not necessarily for a wide range of scenarios. As I mentioned in my last blog post, there were 2 specific kinds of scenarios that DATAS targeted. I’ll reiterate them here –&lt;/p&gt;
    &lt;p&gt;1. Bursty workloads running in memory constraint environments. DATAS aims to retract the heap size back when the application doesn’t require as much memory and grow it when the app requires more. This is especially important for apps running in containers with memory limits.&lt;/p&gt;
    &lt;p&gt;2. Small workloads using Server GC — for example, if someone wants to try out a small asp.net core app to see what the experience is like in .NET, DATAS aims provide a heap size much more inline with what the small app actually needs.&lt;/p&gt;
    &lt;p&gt;I should give more explanation about 1). It’s not uncommon to see bursty workloads at all. If you have an app that handles requests, which is completely common, naturally you could have many more users during a specific time of the day than the rest of the day. However, the key here is the action that follows it — if you have memory freed up during the non peak hours, what would you do with this memory? It turns out that sometimes folks don’t actually know — they want to see the memory go down when the workload lightens, but they have no plans to do anything with this memory. And for some teams, they don’t need to the memory usage to go down because they already budgeted all that memory to their apps. I was just talking to a customer recently and when I asked them “if DATAS frees up memory for you, what would you use for it?”. The answer was “that’s a good question, we never thought about it”.&lt;/p&gt;
    &lt;p&gt;For folks who do want to make use of the freed up memory, a common way is to use an orchestrated environment . DATAS makes this scenario more robust as heap sizes will be much more predictable, as I’ll explain below, which helps with setting sensible memory limits. For example, in k8s, you can determine appropriate request and limit values for both non-peak and peak workloads to better leverage HPA. I have also seen teams that schedule tasks to run when the machines/VMs have free memory — this is more involved (and these teams usually are equipped with a team of dedicated perf engineers) but gives them more control.&lt;/p&gt;
    &lt;p&gt;Then there are plenty of teams that have dedicated fleets of machines and want to maximize their throughput during peak hours as much as possible. They do not want to tolerate any type of slow down. They are definitely not the target of DATAS which will almost always regress their throughput — when it comes to perf it’s rarely an all or none situation and I will discuss below how to make a decision if you should turn DATAS off.&lt;/p&gt;
    &lt;p&gt;All these made it difficult to make DATAS the default because we know there are a lot of teams that don’t want to sacrifice throughput at all or don’t make use of freed up memory.&lt;/p&gt;
    &lt;p&gt;I will discuss in detail below if you do want to look at the perf differences and make a decision if DATAS is for you or not (maybe when you see the memory reduction you will have ideas of using the freed up memory).&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance differences between DATAS and the traditional Server GC&lt;/head&gt;
    &lt;p&gt;DATAS is a GC feature that I spent more time explaining to my coworkers than any other — being such a user visible feature, it naturally attracted more questions than pretty much any other GC features I added. And there were lots of misconceptions. Some thought that DATAS only affected startup; some assumed it would just “reduce memory by x% and throughput by y%”; some expected it to “magically reducing memory without any other perf differences” (okay, I added the “magically” part 😆); and etc.&lt;/p&gt;
    &lt;p&gt;To understand the differences properly, we need to understand the difference in policies. First and foremost, Server GC does not adapt to the application size — it was never a goal. Server GC looks mostly at the survival rate of each generation and does GCs based on that (there are a number of other factors that affect when GCs are triggered but survival rate is one of the most significant). In the last DATAS post I talked about the number of heaps which can affect the heap size significantly, especially in workloads that allocate a lot of temporary data. Since Server GC creates the same number of heaps as the number of cores the process is allowed to use, it means you can see very different heap sizes when running the same app with a different number of cores (by running it on a machine with a different number of cores or let your process use different number of cores on the same machine).&lt;/p&gt;
    &lt;p&gt;DATAS, on the other hand, aims to adapt to the application size which means you should see similar heap sizes even when the number of cores varies a lot. So there’s no “DATAS will reduce memory by X%” compared to Server GC.&lt;/p&gt;
    &lt;p&gt;If we look at the “Max heap size” metric for asp.net benchmarks, it’s obvious that Server GC behaves very differently when running on a 28-core machine (28c) vs a 12-core machine (12c) –&lt;/p&gt;
    &lt;p&gt;Careful readers will notice that the order of which color is on top is not consistent. For example, for MultipleQueriesPlatform, the max heap size is actually much larger for 12c than 28c. Looking at the data in more detail reveals that the max heap size happens at the very beginning of the test for the 12c case –&lt;/p&gt;
    &lt;p&gt;(Heap size (before) is right before a GC before that GC could possibly shrink the heap size. So “Max Heap Size” would be the max of this metric)&lt;/p&gt;
    &lt;p&gt;This is because at the beginning, there were a lot more allocations happened before the first GC happened on 28c with 28 heaps. So after that GC, a smaller survival rate was observed which caused the gen0 budget to be much smaller than on 12c. 12c quickly dropped to the steady state which has a much lower heap size than 28c. For steady state, these benchmarks always exhibit a much higher heap size on 28c. This illustrates 2 points –if you just measure “max heap size”, it can easily be affected by the non-steady state behavior; secondly, the heap size can vary a lot due to the machine the test runs on. Note that these effects can be magnified because we are looking at small benchmarks, but the reasoning applies to real-world apps.&lt;/p&gt;
    &lt;p&gt;With DATAS we see this picture -&lt;/p&gt;
    &lt;p&gt;The max heap sizes are very similar on 28c and 12c which is exactly what DATAS is for — it adapts to the application size.&lt;/p&gt;
    &lt;p&gt;Do I need to care if I’m using Workstation GC?&lt;/p&gt;
    &lt;p&gt;The answer depends on why you are using Workstation GC. If you are using Workstation GC because your workload simply does not call for using Server GC at all, then there’s no need to change. This could be due to your app being single threaded or the allocation is simply not stressful and you are totally fine with having one thread doing the collection work, in which case Workstation GC not only suffices but is exactly the correct choice to make.&lt;/p&gt;
    &lt;p&gt;But if you are using it because Server GC’s memory usage was too large and you are just using Workstation to limit the memory usage, you could find DATAS very attractive because it can both limit the memory usage and make the GC pauses lower with more GC threads doing the collection work.&lt;/p&gt;
    &lt;head rend="h2"&gt;How DATAS does its job&lt;/head&gt;
    &lt;p&gt;If you understood how DATAS does it job, it would be natural to arrive at the recommendations below for deciding if DATAS is for you. You could also skip this section, but I always like to understand how something works if I care about it, so I can come to my own conclusions instead of just memorizing some rules. In the last blog post I mentioned some details of DATAS at the time (.NET 8), noting that it would likely change dramatically — and it did, both in design and implementation. The implementation we had in .NET 8 was mostly for functional — we spent very little time in tuning. The majority of the tuning work happened after .NET 8.&lt;/p&gt;
    &lt;p&gt;The goal of DATAS is to adapt to the application size, or the LDS (Live Data Size). So there needs to be some way to adapt to it. Because the .NET GC is generational, it means we don’t collect the whole heap often. And since most full GCs we do are background GCs which don’t compact, it’s reasonable to approximate the LDS with the space objects take up in the old generations, i.e., (total size — fragmentation). Another convenient number to use when you do your perf investigations is to look at the promoted size when a full GC is done.&lt;/p&gt;
    &lt;p&gt;In the last blog post I mentioned the conserve memory config is part of the DATAS implementation — that part did not change. But conserve memory only affects when full GCs are triggered. For apps that allocate very frequently, unless these are temporary UOH objects, most of the GCs are ephemeral GCs. And ephemeral generation sizes can be a significant portion of the whole heap especially for small heaps.&lt;/p&gt;
    &lt;p&gt;After experimenting with various approaches, I settled on the approach of “adapting to the app size while maintaining reasonable performance” which consisted of 2 key components -&lt;/p&gt;
    &lt;p&gt;1) introduced a concept of “Budget Computed via DATAS (BCD)” which is calculated based on the application size and gives us an upper bound of the gen0 budget for that size, which can approximate the generation size for gen0 (since there’s pinning it may not be exactly the generation size for gen0).&lt;/p&gt;
    &lt;p&gt;2) within this upper bound, we can further reduce memory if we can still maintain reasonable performance. And we define this “reasonable performance” with a target Throughput Cost Percentage (TCP). This takes into consideration both GC pauses and how much allocating threads have to wait. But you can approximate TCP with % pause time in GC in steady state. The idea is to keep TCP around this target if we can, which means if the workload gets lighter, we’d be adjusting the gen0 budget smaller. And that in turn means gen0 will be smaller before the next GC, which translates to smaller heap size. The default target TCP is 2%. This can be changed via the GCDTargetTCP config.&lt;/p&gt;
    &lt;p&gt;Let’s look at 2 example scenarios to see how this manifests. For simplicity, I’m ignoring background GCs, and I’ll use % pause time in GC to approximate TCP.&lt;/p&gt;
    &lt;p&gt;Scenario A — I have an e-commerce app which stores the whole catalog in memory, and this remains the same during the process lifetime. This is our LDS. Now the process starts to process requests and for each request there’s memory allocated and only used for the duration of that request.&lt;/p&gt;
    &lt;p&gt;During peak hours, it processes many concurrent requests. We hit our max budget which is our BCD. Let’s say this is 1gb, it means we are doing a GC each time 1GB is allocated. If we use the % pause time in GC to approximate TCP, let’s say during each second it allocates 1GB and observes one GC that has a 20ms pause. So the % time in GC is 2%. And that’s the same as our target TCP.&lt;/p&gt;
    &lt;p&gt;When it’s outside the peak hours and handling way fewer concurrent requests, let’s say we allocate ~200MB per second. If we keep our 1GB budget, it means we are doing a GC every 5s. And our % time in GC would be (20ms / 5s = 0.4%), much lower than 2%. So to reach the target TCP we’d want to reduce the budget and trigger a GC much sooner. If we reduce the budget to 200MB, and we’ll still use 20ms as our GC pause just to make it simple (it’ll likely be shorter as it’s roughly proportional to the survival and there’s likely less survival out of 200MB vs 1GB), now we are achieving 2% TCP again.&lt;/p&gt;
    &lt;p&gt;So for this scenario, the heap size is reduced by ~800MB when it’s outside peak hours. Depending on your total heap size, this can be a very significant reduction.&lt;/p&gt;
    &lt;p&gt;Scenario B is built on top of A but we’ll throw in a cache that’s part of the LDS but gets smaller during lighter workload as we don’t need to cache as much. Because the LDS is smaller it means your BCD will be smaller as it’s a function of LDS. So during the lighter workload, the gen0 budget will be further reduced which again reflects the adapting to size nature. The conserve memory mechanism is still in effect too and would adjust the old generation budget and size accordingly.&lt;/p&gt;
    &lt;p&gt;Notice that so far I have not talked about the number of heaps at all! This is completely taken care of by DATAS itself so you don’t need to worry about it. Previously, some of our customers were using the GCHeapCount config to specify the number of heaps for Server GC. But DATAS makes it more robust as it can take advantage of more heaps if needed (which usually means shorter individual pause times) and reduces the heap size when the LDS goes down, without your having to specify a heap count yourself.&lt;/p&gt;
    &lt;p&gt;DATAS has specific events that indicate the actual TCP and LDS but that requires you to programmatically get them via the TraceEvent library. The approximations I mentioned above are sufficient for almost all perf investigations.&lt;/p&gt;
    &lt;head rend="h2"&gt;When DATAS might not be applicable to your scenario&lt;/head&gt;
    &lt;p&gt;If you read the previous sections, what’s listed below hopefully makes sense.&lt;/p&gt;
    &lt;p&gt;1) If you have no use for free memory, you don’t need DATAS&lt;/p&gt;
    &lt;p&gt;This one should be obvious — why change it at all if you don’t have any use for the memory that gets freed up by DATAS anyway? You can turn DATAS off by the GCDynamicAdaptationMode config.&lt;/p&gt;
    &lt;p&gt;I’ve come across a few first party teams who simply didn’t need DATAS — they have dedicated machines to run their processes and have no use for free memory as they don’t plan to run anything else on the machine. So they have no use for DATAS. One team did say “now we probably want to think about taking advantage of free memory” (they were not thinking about it because Server GC isn’t aggressive at reducing memory usage). So for them, they will disable DATAS for now but will enable it when they can take advantage of memory during non peak hours.&lt;/p&gt;
    &lt;p&gt;2) If startup perf is critical, DATAS is not for you&lt;/p&gt;
    &lt;p&gt;DATAS always starts with 1 heap. We cannot predict how stressful your workload will be and since we are optimizing for size here, it starts with the smallest heap count which is 1. So if your startup perf is critical, you will see a regression because it takes time to go from 1 heap to multiple.&lt;/p&gt;
    &lt;p&gt;3) If you do not tolerate any throughput regression, DATAS may not be for you&lt;/p&gt;
    &lt;p&gt;If this includes the startup throughput, as 2) also states, DATAS is not for you. However, some scenarios aren’t concerned with startup perf so DATAS may or may not be desirable. Let’s say your % pause time in GC is 1% with Server GC, you can just set the GCDTargetTCP config to 1. If you were restricting the heap count you could very possibly see a perf improvement because the pause time can be shorter with DATAS. If the adaptation to the size aspect is beneficial to you, using DATAS can be a much better choice. But as stated in 1) if you don’t have any use for the freed up memory anyway, it wouldn’t justify spending time on using DATAS.&lt;/p&gt;
    &lt;p&gt;4) If you are doing mostly gen2 GCs, DATAS may not be for you&lt;/p&gt;
    &lt;p&gt;One case I haven’t spent much time tuning is when your scenario mostly does gen2 GCs (this is almost always due to excessive allocation of temporary large objects). If this is the case for you, and if you’ve tried DATAS and weren’t happy with the results, I would suggest to disable DATAS. You could investigate to see if you can make it work by following the tuning section if it’s justified to spend the time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tuning DATAS if necessary&lt;/head&gt;
    &lt;p&gt;I’ve tried DATAS on some first party workloads and in general it worked out great. I’ll show a couple of examples where the default parameters of DATAS weren’t great but tuning one or 2 configs made it work.&lt;/p&gt;
    &lt;p&gt;Customer case 1&lt;/p&gt;
    &lt;p&gt;This is a server app running on dedicated machines. But they are in the process of containerizing it so there’s definitely merit to use DATAS. With DATAS they observed a 6.8% regression in throughput with a 10% reduction in working set. For now they’ve disabled DATAS — I will explain how I debugged it and determined what DATAS config to use to make it work if/when they want to enable DATAS.&lt;/p&gt;
    &lt;p&gt;Because DATAS limits the largest gen0 budget based on the LDS, we want to see if we are hitting that limit. It’d be easiest if you captured a GC trace with DATAS and one without DATAS. If you are seeing more GCs triggered, that means most likely you are hitting that limit.&lt;/p&gt;
    &lt;p&gt;You can approximate the TCP with what’s shown in the “% Pause Time” column, and gen0 budget with the “Gen0 Alloc MB” column. And you’d want to find the phase when you have the highest % pause time and see if you are triggering more GCs.&lt;/p&gt;
    &lt;p&gt;So for this particular customer, here are some excerpts of the GC (I’ve trimmed down the columns of the GCStats view) –&lt;/p&gt;
    &lt;p&gt;Without DATAS&lt;/p&gt;
    &lt;p&gt;With DATAS&lt;/p&gt;
    &lt;p&gt;Comparing their gen0 budget and % pause time in GC -&lt;/p&gt;
    &lt;p&gt;So gen0 budget without DATAS is 2.6x with DATAS. Another useful thing we notice is the % Pause Time is basically exactly the target TCP — 2%. That tells us that this is working exactly as by design from DATAS’s POV. But without DATAS we got 2.6x budget so naturally we triggered GC less frequently and % pause time is 1.2 instead of 2.1.&lt;/p&gt;
    &lt;p&gt;But if we want to enable DATAS and not regress throughput for this phase, we’d like to have DATAS use a larger gen0 budget. To do that we should understand how DATAS determines the BCD. Since we are adapting to the size, we want to multiply the size with something. But this should not be a constant value because when the size is very small, this multiplier should be quite large — if the LDS is only 2MB (which is totally possible for a tiny app), we wouldn’t want to trigger a GC for every 0.2MB of allocation — the overhead would be too high. Let’s say we want to allow 20MB of allocation before triggering a GC, that makes the multiplier 10. But if the LDS is 20GB, we wouldn’t want to allocate 200GB before doing a GC, which means we want a much smaller multiplier. This means a power function but we also want to clamp it between a min and max value -&lt;/p&gt;
    &lt;code&gt;m = constant / sqrt (LDS);&lt;lb/&gt;// default for max_m is 10&lt;lb/&gt;m = min (max_m, m);&lt;lb/&gt;// default for min_m is 0.1&lt;lb/&gt;m = max (min_m, m);&lt;/code&gt;
    &lt;p&gt;The actual formula for the power function is&lt;/p&gt;
    &lt;code&gt;m = (20 - conserve_memory) / sqrt (LDS / 1000 / 1000);&lt;/code&gt;
    &lt;p&gt;which can be simplified to&lt;/p&gt;
    &lt;code&gt;m = (20 - conserve_memory) * 1000 / sqrt (LDS);&lt;lb/&gt;m = (20 - 5) * 1000 / sqrt (LDS);&lt;lb/&gt;m = 15000 / sqrt (LDS);&lt;/code&gt;
    &lt;p&gt;So the constant is 15000, or we could just say it’s 15 if we use MB for size. here’re some example with different LDS -&lt;/p&gt;
    &lt;p&gt;This constant, max_m and min_m can all be adjusted by configs. Please see the config page for detailed explanation.&lt;/p&gt;
    &lt;p&gt;Now it’s quite obvious why DATAS came up with the gen0 budget and how we can adjust it. If we want to bring this up to the same budget without DATAS, we’d want to use the GCDGen0GrowthPercent config to increase the constant to 2.6x, and increase min_m with the GCDGen0GrowthMinFactor config so it’s not clamped to 0.1 — you don’t need to be very accurate since you just need to make it not be the limiting factor. So in this case if we use 15GB to approximate the LDS (the “Promoted (mb)” column for both gen2 GCs says ~15GB), and without DATAS the gen0 budget is 4.22GB. So min_m should be around (4.22/15 = 0.28). We can just set min_m to 300 which translates to 0.3 of LDS.&lt;/p&gt;
    &lt;p&gt;Customer case 2&lt;/p&gt;
    &lt;p&gt;This is an asp.net app on a staging server from the customer that represents one of their key scenarios. I used a load test tool to generate variable workloads.&lt;/p&gt;
    &lt;p&gt;The team was already using some GC configs -&lt;/p&gt;
    &lt;p&gt;· GCHeapCount is set to 2 to use 2 heaps&lt;/p&gt;
    &lt;p&gt;· Affinity is turned off with the GCNoAffinitize config.&lt;/p&gt;
    &lt;p&gt;If the GCHeapCount config is specified, DATAS would be disabled because it’s telling the GC to not change the heap count. And since changing the heap count is one of the key mechanisms to adjust perf for DATAS, it’s an indication to disable DATAS.&lt;/p&gt;
    &lt;p&gt;Because this is a process that co-exist with many others on the same machine, before DATAS was available they chose to give it 2 heaps to limit the memory usage while still getting reasonable throughput. But this is not flexible — when the load becomes higher the throughput can suffer with 2 heaps and also the GC pauses can be noticeably higher since there’s only 2 GC threads collecting. They can adjust the number of GC heaps but that means more work and since Server GC isn’t very aggressive at reducing memory usage they can end up with a much bigger heap than desired when the load is lighter.&lt;/p&gt;
    &lt;p&gt;I’ll demonstrate how using DATAS makes this robust. When I made the load pretty high I could see that % pause time in GC is quite high — not surprising with just 2 heaps. So I enabled DATAS by simply getting rid of the GCHeapCount config (I kept the GCNoAffinitize config as I still wanted the GC threads to not be affinitized). I could see the % pause time in GC was also high because even with BCD we still ended up triggering GCs quite often. So I decided to make BCD 2x the default value with the GCDGen0GrowthPercent config (I didn’t need to use the GCDGen0GrowthMinFactor config since 2x is still well within our max_m/min_m clamping values). And now the process behaves in a much more desirable way with the following characteristics –&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the % pause time is dramatically lower. With the default DATAS the % pause time is basically comparable and the heap size is noticeably lower. Depending on your optimization goal this could be exactly what you want. DATAS is able to achieve this with smaller budgets and more GC threads doing the collection work. But I know for this customer, they don’t want % pause time in GC to be this high as it affects their throughput. I could also make DATAS use a smaller target TCP but in this case the default TCP seems quite sufficient.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;individual GC pauses are a lot lower since we have a lot more GC threads collecting.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;when the load becomes lighter (# of concurrent client threads went from 200 to 100), the heap also becomes smaller. And we are still maintaining a much lower % pause time in GC and individual GC pauses.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I hope this helps with your DATAS tuning, if you need to do any.&lt;/p&gt;
    &lt;head rend="h2"&gt;DATAS Events&lt;/head&gt;
    &lt;p&gt;I expect most users never needing to look at these events, so I’ll keep it brief. The approximations that I mentioned above should suffice. For the small number of folks who want to do a detailed analysis for whatever reason, DATAS fires an event that accurately represents the metrics we discussed. Note that we only use these events programmatically, so they are not surfaced in PerfView’s Events view (all you’ll see is the GC/DynamicTraceEvent which shows you the name but not individual fields of that event). See this blog article for an example how to programmatically retrieve GC info as a list of TraceGC objects from a trace.&lt;/p&gt;
    &lt;p&gt;LDS and TCP are indicated in the SizeAdaptationTuning event, assuming you have a gc object of the type TraceGC —&lt;/p&gt;
    &lt;code&gt;// LDS&lt;lb/&gt;gc.DynamicEvents().SizeAdaptationTuning?.TotalSOHStableSize&lt;lb/&gt;// TCP&lt;lb/&gt;gc.DynamicEvents().SizeAdaptationTuning?.TcpToConsider&lt;/code&gt;
    &lt;p&gt;This event is not fired every GC since we only check to see if we need to change the tuning for DATAS every few GCs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://maoni0.medium.com/preparing-for-the-net-10-gc-88718b261ef2"/><published>2025-09-24T10:37:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45358940</id><title>Huntington's disease treated for first time</title><updated>2025-09-24T14:39:04.232469+00:00</updated><content>&lt;doc fingerprint="2e099265b8279da2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Huntington's disease successfully treated for first time&lt;/head&gt;
    &lt;p&gt;One of the cruellest and most devastating diseases – Huntington's – has been successfully treated for the first time, say doctors.&lt;/p&gt;
    &lt;p&gt;The disease runs through families, relentlessly kills brain cells and resembles a combination of dementia, Parkinson's and motor neurone disease.&lt;/p&gt;
    &lt;p&gt;An emotional research team became tearful as they described how data shows the disease was slowed by 75% in patients.&lt;/p&gt;
    &lt;p&gt;It means the decline you would normally expect in one year would take four years after treatment, giving patients decades of "good quality life", Prof Sarah Tabrizi told BBC News.&lt;/p&gt;
    &lt;p&gt;The new treatment is a type of gene therapy given during 12 to 18 hours of delicate brain surgery.&lt;/p&gt;
    &lt;p&gt;The first symptoms of Huntington's disease tend to appear in your 30s or 40s and is normally fatal within two decades – opening the possibility that earlier treatment could prevent symptoms from ever emerging.&lt;/p&gt;
    &lt;p&gt;Prof Tabrizi, director of the University College London Huntington's Disease Centre, described the results as "spectacular".&lt;/p&gt;
    &lt;p&gt;"We never in our wildest dreams would have expected a 75% slowing of clinical progression," she said.&lt;/p&gt;
    &lt;p&gt;None of the patients who have been treated are being identified, but one was medically retired and has returned to work. Others in the trial are still walking despite being expected to need a wheelchair.&lt;/p&gt;
    &lt;p&gt;Treatment is likely to be very expensive. However, this is a moment of real hope in a disease that hits people in their prime and devastates families.&lt;/p&gt;
    &lt;p&gt;Huntington's runs through Jack May-Davis' family. He has the faulty gene that causes the disease, as did his dad, Fred, and his grandmother, Joyce.&lt;/p&gt;
    &lt;p&gt;Jack said it was "really awful and horrible" watching his dad's inexorable decline.&lt;/p&gt;
    &lt;p&gt;The first symptoms appeared in Fred's late 30s, including changes in behaviour and the way he moved. He eventually needed 24/7 palliative care before he died at the age of 54, in 2016.&lt;/p&gt;
    &lt;p&gt;Jack is 30, a barrister's clerk, newly engaged to Chloe and has taken part in research at UCL to turn his diagnosis into a positive.&lt;/p&gt;
    &lt;p&gt;But he'd always known he was destined to share his father's fate, until today.&lt;/p&gt;
    &lt;p&gt;Now he says the "absolutely incredible" breakthrough has left him "overwhelmed" and able to look to a future that "seems a little bit brighter, it does allow me to think my life could be that much longer".&lt;/p&gt;
    &lt;p&gt;Huntington's disease is caused by an error in part of our DNA called the huntingtin gene.&lt;/p&gt;
    &lt;p&gt;If one of your parents has Huntington's disease, there's a 50% chance that you will inherit the altered gene and will eventually develop Huntington's too.&lt;/p&gt;
    &lt;p&gt;This mutation turns a normal protein needed in the brain – called the huntingtin protein – into a killer of neurons.&lt;/p&gt;
    &lt;p&gt;The goal of the treatment is to reduce levels of this toxic protein permanently, in a single dose.&lt;/p&gt;
    &lt;p&gt;The therapy uses cutting edge genetic medicine combining gene therapy and gene silencing technologies.&lt;/p&gt;
    &lt;p&gt;It starts with a safe virus that has been altered to contain a specially designed sequence of DNA.&lt;/p&gt;
    &lt;p&gt;This is infused deep into the brain using real-time MRI scanning to guide a microcatheter to two brain regions - the caudate nucleus and the putamen. This takes 12 to 18 hours of neurosurgery.&lt;/p&gt;
    &lt;p&gt;The virus then acts like a microscopic postman – delivering the new piece of DNA inside brain cells, where it becomes active.&lt;/p&gt;
    &lt;p&gt;This turns the neurons into a factory for making the therapy to avert their own death.&lt;/p&gt;
    &lt;p&gt;The cells produce a small fragment of genetic material (called microRNA) that is designed to intercept and disable the instructions (called messenger RNA) being sent from the cells' DNA for building mutant huntingtin.&lt;/p&gt;
    &lt;p&gt;This results in lower levels of mutant huntingtin in the brain.&lt;/p&gt;
    &lt;p&gt;Results from the trial - which involved 29 patients - have been released in a statement by the company uniQure, but have not yet been published in full for review by other specialists.&lt;/p&gt;
    &lt;p&gt;The data showed that three years after surgery there was an average 75% slowing of the disease based on a measure which combines cognition, motor function and the ability to manage in daily life.&lt;/p&gt;
    &lt;p&gt;The data also shows the treatment is saving brain cells. Levels of neurofilaments in spinal fluid – a clear sign of brain cells dying – should have increased by a third if the disease continued to progress, but was actually lower than at the start of the trial.&lt;/p&gt;
    &lt;p&gt;"This is the result we've been waiting for," said Prof Ed Wild, consultant neurologist at the National Hospital for Neurology and Neurosurgery at UCLH.&lt;/p&gt;
    &lt;p&gt;"There was every chance that we would never see a result like this, so to be living in a world where we know this is not only possible, but the actual magnitude of the effect is breathtaking, it's very difficult to fully encapsulate the emotion."&lt;/p&gt;
    &lt;p&gt;He said he was "a bit teary" thinking about the impact it could have on families.&lt;/p&gt;
    &lt;p&gt;The treatment was considered safe, although some patients did develop inflammation from the virus that caused headaches and confusion that either resolved or needed steroid treatment.&lt;/p&gt;
    &lt;p&gt;Prof Wild anticipates the therapy "should last for life" because brain cells are not replaced by the body in the same manner as blood, bone and skin are constantly renewed.&lt;/p&gt;
    &lt;p&gt;Approximately 75,000 people have Huntington's disease in the UK, US and Europe with hundreds of thousands carrying the mutation meaning they will develop the disease.&lt;/p&gt;
    &lt;p&gt;UniQure says it will apply for a licence in the US in the first quarter of 2026 with the aim of launching the drug later that year. Conversations with authorities in the UK and Europe will start next year, but the initial focus is on the US.&lt;/p&gt;
    &lt;p&gt;Dr Walid Abi-Saab, the chief medical officer at uniQure, said he was "incredibly excited" about what the results mean for families, and added that the treatment had "the potential to fundamentally transform" Huntington's disease.&lt;/p&gt;
    &lt;p&gt;However, the drug will not be available for everyone due to the highly complex surgery and the anticipated cost.&lt;/p&gt;
    &lt;p&gt;"It will be expensive for sure," says Prof Wild.&lt;/p&gt;
    &lt;p&gt;There isn't an official price for the drug. Gene therapies are often pricey, but their long-term impact means that can still be affordable. In the UK, the NHS does pay for a £2.6m-per-patient gene therapy for haemophilia B.&lt;/p&gt;
    &lt;p&gt;Prof Tabrizi says this gene therapy "is the beginning" and will open the gates for therapies that can reach more people.&lt;/p&gt;
    &lt;p&gt;She paid tribute to the "truly brave" volunteers who took part in the trial, saying she was "overjoyed for the patients and families".&lt;/p&gt;
    &lt;p&gt;She is already working with a group of young people who know they have the gene, but don't yet have symptoms – known as stage zero Huntington's – and is aiming to do the first prevention trial to see if the disease can be significantly delayed or even stopped completely.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/cevz13xkxpro"/><published>2025-09-24T11:37:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45358980</id><title>Yt-dlp: Upcoming new requirements for YouTube downloads</title><updated>2025-09-24T14:39:03.039576+00:00</updated><content>&lt;doc fingerprint="806fb82cc43ebc72"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 10.2k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;Beginning very soon, you'll need to have the JavaScript runtime Deno installed to keep YouTube downloads working as normal.&lt;/head&gt;
    &lt;head rend="h2"&gt;Why?&lt;/head&gt;
    &lt;p&gt;Up until now, yt-dlp has been able to use its built-in JavaScript "interpreter" to solve the JavaScript challenges that are required for YouTube downloads. But due to recent changes on YouTube's end, the built-in JS interpreter will soon be insufficient for this purpose. The changes are so drastic that yt-dlp will need to leverage a proper JavaScript runtime in order to solve the JS challenges.&lt;/p&gt;
    &lt;head rend="h2"&gt;What do I need to do?&lt;/head&gt;
    &lt;head rend="h3"&gt;Everyone will need to install Deno.&lt;/head&gt;
    &lt;p&gt;yt-dlp will also need a few JavaScript components, and this may require additional action from you depending on how you installed yt-dlp:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Official PyInstaller-bundled executable users (e.g.&lt;/p&gt;&lt;code&gt;yt-dlp.exe&lt;/code&gt;,&lt;code&gt;yt-dlp_macos&lt;/code&gt;,&lt;code&gt;yt-dlp_linux&lt;/code&gt;, etc):&lt;list rend="ul"&gt;&lt;item&gt;No additional action required (besides having Deno). All the necessary JavaScript components will be bundled with these executables.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;PyPI package users (e.g. installed with&lt;/p&gt;&lt;code&gt;pip&lt;/code&gt;,&lt;code&gt;pipx&lt;/code&gt;, etc):&lt;list rend="ul"&gt;&lt;item&gt;Install and upgrade yt-dlp with the &lt;code&gt;default&lt;/code&gt;optional dependency group included, e.g.:&lt;code&gt;pip install -U "yt-dlp[default]"&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Install and upgrade yt-dlp with the &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Official zipimport binary users (the&lt;/p&gt;&lt;code&gt;yt-dlp&lt;/code&gt;Unix executable):&lt;list rend="ul"&gt;&lt;item&gt;Run yt-dlp with an additional flag to allow Deno to download &lt;code&gt;npm&lt;/code&gt;dependencies --or-- install yt-dlp's JS solver package in your Python environment. (The flag name and the package name are both still TBD.)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Run yt-dlp with an additional flag to allow Deno to download &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Third-party package users (e.g. installed with&lt;/p&gt;&lt;code&gt;pacman&lt;/code&gt;,&lt;code&gt;brew&lt;/code&gt;, etc):&lt;list rend="ul"&gt;&lt;item&gt;The action required will depend on how your third-party package repository decides to handle this change. But the options available for "official zipimport binary users" should work for you as well.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/yt-dlp/yt-dlp/issues/14404"/><published>2025-09-24T11:41:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359074</id><title>EU age verification app not planning desktop support</title><updated>2025-09-24T14:39:02.108809+00:00</updated><content>&lt;doc fingerprint="e70383268912763c"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 2&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Hi I found multiple usability issues with this solution.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The focus is so strong on the app, that it assumes everyone owns a smartphone. The other day I saw a granny on the bus with a phone that was 2cms thick and predates the famous Nokia 3310. How is she and other users without a smartphone supposed to verify their age online?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How will this impact the browsing experience on the web? Every website has GDPR checkboxes these days which somewhat disrupts browsing experience if browsing in for example incognito mode. Imagine if you want to browse the web privately. Websites don't know who you are so you will have to verify your age every single time. This makes the web unusable for anyone who wants to browse the web privately. Especially on a pc. A solution would be to have some sort of browser extension that handles it automatically. Since you at least claim to value privacy that could work. But it wouldn't really look trustworthy. Note this doesn't only apply to incognito but browsing the web in general. Like trying to compare various news sites. Doing this for every website to visit is a major hindrance usability wise.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;What will the cost be of implementing this? My trust in the EU to develop affordable and good technologies has diminished since we created a Peppol access point for our company. The solution was made using technologies only java has proper libraries for. Locking the developer to that language and eco system. Of course not a big issue for a big company. But a small start up won't be able to survive if they have to implement this.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/eu-digital-identity-wallet/av-doc-technical-specification/issues/22"/><published>2025-09-24T11:52:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359201</id><title>WiGLE: Wireless Network Mapping</title><updated>2025-09-24T14:39:01.769015+00:00</updated><content>&lt;doc fingerprint="8e2ed84303c8a523"&gt;
  &lt;main&gt;
    &lt;p&gt;Toggle navigation View Basic Search Advanced Search Map Uploads Info Android App FAQ App FAQ Forums Mastodon Bsky Twitter Stats Tools Downloads API Account CSV Upload Login User Name Password Forgot your password? keep me logged-in New? Register All the networks. Found by Everyone. &amp;lt;&amp;lt; Latitude Longitude SSID BSSID Date Range: 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 Possible FreeNet Possible Commercial Net No Labels Only Discovered By Me Only Discovered By Others Coloring: density QoS channel View: Greyscale Nightvision Standard Notes: Zoom in to see individual SSIDs. cell tower: blue QoS: Quality of Signal is a metric based on the number of observations and observers Statistics Over Time WiFi Networks Over Time [Full-screen Graph] WiFi Encryption Over Time [Full-screen Graph] [2 Years only Graph] Mouse-over graphs to interact with data. Select a range to zoom in, double click to zoom back out. Modify the number in the corner to smooth over multiple days. Full-screen graphs available! × × × × Join WiGLE × A Message from WiGLE&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wigle.net/index"/><published>2025-09-24T12:10:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359356</id><title>Rights groups urge UK PM Starmer to abandon plans for mandatory digital ID</title><updated>2025-09-24T14:39:01.228683+00:00</updated><content>&lt;doc fingerprint="8da196ac48835d99"&gt;
  &lt;main&gt;
    &lt;p&gt;Big Brother Watch and other human rights, civil liberties, digital rights, and racial justice organisations have written to the Prime Minister urging him to abandon plans for a mandatory digital ID.&lt;/p&gt;
    &lt;p&gt;The letter comes just days before an expected statement from Keir Starmer announcing the rollout of a mandatory digital ID scheme aimed at deterring illegal immigration.&lt;/p&gt;
    &lt;p&gt;The leaders of Big Brother Watch, Article 19, Connected by Data, Liberty, Open Rights Group, The Runnymede Trust, and Unlock Democracy argue that digital ID would change our relationship with the state, cause irreversible damage to our civil liberties, and fail to deter illegal immigration:&lt;/p&gt;
    &lt;p&gt;“Mandatory digital ID would fundamentally change the relationship between the population and the state by requiring frequent identity checks as we navigate our daily lives. Although the current digital ID proposals are being considered in the context of immigration, there is no guarantee that a future government would not make digital ID a requirement to access a range of public and private services.”&lt;/p&gt;
    &lt;p&gt;The joint letter can be found using this link.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spokespeople are available for interview. Please direct enquiries or requests for interviews to info@bigbrotherwatch.org.uk or 07730439257&lt;/item&gt;
      &lt;item&gt; Read Big Brother Watch’s recent report “Checkpoint Britain: the dangers of digital ID and why privacy must be protected”&lt;/item&gt;
      &lt;item&gt; Big Brother Watch will be hosting events on the dangers of digital ID during the Labour and Conservative Party conferences in Liverpool and Manchester. Contact info@bigbrotherwatch.org.uk for details”&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bigbrotherwatch.org.uk/press-releases/rights-groups-urge-starmer-to-abandon-plans-for-mandatory-digital-id/"/><published>2025-09-24T12:28:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359378</id><title>US Airlines Push to Strip Away Travelers' Rights by Rolling Back Key Protections</title><updated>2025-09-24T14:39:00.856622+00:00</updated><content>&lt;doc fingerprint="567c9a5a01baa55b"&gt;
  &lt;main&gt;
    &lt;p&gt;Home»AIRLINE NEWS» American Joins Delta, Southwest, United and Other US Airlines Push to Strip Away Travelers’ Rights and Add More Fees by Rolling Back Key Protections in New Deregulation Move&lt;/p&gt;
    &lt;p&gt;American Joins Delta, Southwest, United and Other US Airlines Push to Strip Away Travelers’ Rights and Add More Fees by Rolling Back Key Protections in New Deregulation Move&lt;/p&gt;
    &lt;p&gt;American Airlines joins with Delta, Southwest, United, and other US airlines are pushing to remove key protections for passengers and add more fees by rolling back rules, claiming it will lower costs and boost competition, but it may leave travelers with fewer rights and more hidden charges. Under the guise of lowering costs and boosting competition, these changes are likely to result in the erosion of consumer rights – the right to cancel a ticket with an automatic refund, transparency of pricing, and the right to sit with your family on the same reservation. Airlines claim that removing these ‘protections’ will decrease airfare and increase competition on the routes. The prospects for travelers, on the other hand, will likely be more fees, less certainty of receiving the service paid for, and diminished responsibility from the airlines for service failures. If these projections become reality, deregulation will aggravate the air travel experience for the consumer making it more expensive and more opaque.&lt;/p&gt;
    &lt;p&gt;The Airline Industry’s Deregulatory Push&lt;/p&gt;
    &lt;p&gt;The U.S. airline industry is pushing for a significant rollback of consumer protections, which many see as a major step backward for air travel. Airline lobbyists, representing carriers like American, Delta, Southwest, United, and the Airlines for America (A4A) association, have laid out a detailed agenda that would fundamentally alter the landscape of air travel, making it more difficult for passengers to know what they’re actually paying for and less likely to receive compensation when things go wrong.&lt;/p&gt;
    &lt;p&gt;This agenda centers on weakening or eliminating four major consumer protections:&lt;/p&gt;
    &lt;p&gt;Automatic Refunds for Cancellations: Airlines want to remove the requirement to provide automatic refunds when flights are cancelled or significantly altered. Passengers may instead receive only vouchers or no compensation at all, leaving them without recourse in the event of a major flight disruption.&lt;/p&gt;
    &lt;p&gt;Transparency of Fees: The airlines also aim to strip away rules that require them to disclose all fees (like baggage, seat assignments, and service charges) upfront. Instead of the clear, itemized pricing system that passengers currently rely on, airlines could hide fees until later in the booking process, making the true cost of a ticket much higher than expected.&lt;/p&gt;
    &lt;p&gt;Family Seating Guarantees: Under current regulations, airlines must ensure that families with young children are seated together without additional charges. This would no longer be guaranteed under the new proposal, meaning families could face extra costs just to sit next to one another.&lt;/p&gt;
    &lt;p&gt;Accessibility Protections for Disabled Passengers: The deregulation proposal also targets protections for disabled passengers, weakening their access to support and assistance during air travel.&lt;/p&gt;
    &lt;p&gt;The Airline Industry’s Argument: Deregulation as a Path to Lower Prices&lt;/p&gt;
    &lt;p&gt;The airline industry’s argument for deregulation is grounded in a belief that removing these protections will lead to lower prices, more competition, and better services for consumers. Airline lobbyists argue that deregulation, which began with the Airline Deregulation Act of 1978, has led to increased competition, lower airfares, and more choices for passengers.&lt;/p&gt;
    &lt;p&gt;Advertisement&lt;/p&gt;
    &lt;p&gt;However, while some might agree that competition can drive prices down, there’s a serious concern that deregulation could lead to more surprise charges and less accountability for airlines. Instead of benefiting consumers, deregulation may open the door for airlines to charge excessive fees for basic services, which are often hidden until later in the booking process. This could leave passengers paying far more than they anticipated and receiving less value for their money.&lt;/p&gt;
    &lt;p&gt;The Airlines’ Detailed Deregulatory Agenda&lt;/p&gt;
    &lt;p&gt;The Airlines for America (A4A), the industry group representing major U.S. airlines, has strongly supported deregulation, arguing that it has benefited both airlines and passengers since the 1970s. In a recent document, A4A outlined their full deregulatory wish list, which includes the following key points:&lt;/p&gt;
    &lt;p&gt;Advertisement&lt;/p&gt;
    &lt;p&gt;Support for Deregulation: A4A strongly advocates for the continuation of deregulation, claiming that it has led to lower prices, increased competition, and better services for consumers. The group argues that removing regulations would allow airlines to better compete in the market and reinvest in improving services for passengers.&lt;/p&gt;
    &lt;p&gt;Criticism of the Biden Administration’s Regulatory Actions:&lt;/p&gt;
    &lt;p&gt;Ancillary Fee Transparency: A4A opposes the U.S. Department of Transportation’s (DOT) rules requiring airlines to disclose ancillary fees upfront, arguing that these rules exceed the DOT’s authority and don’t provide any clear benefits to consumers.&lt;/p&gt;
    &lt;p&gt;Refund Rules: A4A calls for the repeal or revision of refund rules that, according to them, go beyond what’s required by law, imposing unnecessary costs on airlines without providing any clear benefit to the public.&lt;/p&gt;
    &lt;p&gt;Flight Delay and Cancellations: The group also criticizes DOT’s policies on flight delays and cancellations, claiming the rules unfairly penalize airlines, particularly when disruptions are caused by factors beyond their control.&lt;/p&gt;
    &lt;p&gt;Deregulatory Priorities: A4A outlines several changes they would like to see the DOT pursue:&lt;/p&gt;
    &lt;p&gt;Rescinding Unlawful Regulations: A4A seeks the repeal of certain regulations, such as family seating and mobility aid assistance rules, which they argue exceed DOT’s authority.&lt;/p&gt;
    &lt;p&gt;Limiting Refund Rules: The group wants to limit DOT’s authority on flight refund rules, particularly for minor operational changes, such as changes to flight numbers or itineraries that don’t cause harm to passengers.&lt;/p&gt;
    &lt;p&gt;Economic Impact of Deregulation: A4A points to the success of deregulation, citing a rise in low-cost carriers, which have made air travel more affordable. They also highlight the significant decrease in airfare prices, which has directly benefited consumers.&lt;/p&gt;
    &lt;p&gt;Investment in Airline Operations: A4A argues that deregulation has allowed airlines to reinvest in their services, improving customer satisfaction and innovation.&lt;/p&gt;
    &lt;p&gt;Support for Technological Innovation: The airline industry is also backing the use of technology, including artificial intelligence (AI) and biometrics, to improve operational efficiency and the customer experience.&lt;/p&gt;
    &lt;p&gt;Why Deregulation is a Concern for Passengers&lt;/p&gt;
    &lt;p&gt;While the airline industry argues that deregulation will lead to lower prices and more competition, critics are skeptical. Here are the key reasons why deregulation could harm consumers:&lt;/p&gt;
    &lt;p&gt;More Hidden Fees: If airlines are no longer required to disclose fees upfront, passengers may face a barrage of surprise charges, from baggage fees to seat selection costs. The cost of air travel could increase significantly, even if base fares appear lower.&lt;/p&gt;
    &lt;p&gt;No Guarantees for Families: The proposal to eliminate the guarantee that families will be seated together without extra charges could lead to more stress for families travelling with young children. Parents may find themselves paying additional fees just to sit next to their kids.&lt;/p&gt;
    &lt;p&gt;Less Accountability for Cancellations: Airlines would have more power to decide whether or not to refund passengers for flight cancellations. This could lead to more vouchers, rather than cash refunds, leaving passengers at the mercy of airlines’ own policies.&lt;/p&gt;
    &lt;p&gt;Weaker Protections for Disabled Passengers: The weakening of accessibility regulations could make it more difficult for passengers with disabilities to access the services and assistance they need during their travel.&lt;/p&gt;
    &lt;p&gt;Less Competition, Not More: While deregulation advocates claim it will lead to more competition, the reality is that fewer protections for consumers could allow major airlines to exploit passengers without fear of consequences. Smaller carriers may also struggle to compete on an uneven playing field.&lt;/p&gt;
    &lt;p&gt;The Risk of Over-Regulation vs. Consumer Protection&lt;/p&gt;
    &lt;p&gt;The battle between over-regulation and consumer protection is a complex issue. While it’s true that some regulations may stifle innovation, the protections that are in place help ensure fair treatment and transparency for consumers. The question is not whether airlines should be regulated, but how much regulation is necessary to strike a balance between profitability and protecting passengers.&lt;/p&gt;
    &lt;p&gt;In Europe, stricter regulations have led to fewer delays and cancellations, and the market remains competitive with budget airlines thriving under the current system. The fear is that deregulation in the U.S. could result in a situation where airlines dominate the market, and passengers are left with fewer rights and more fees.&lt;/p&gt;
    &lt;p&gt;What Passengers Can Do&lt;/p&gt;
    &lt;p&gt;As a passenger, it’s important to stay informed about these changes and advocate for your rights. Here are some steps you can take:&lt;/p&gt;
    &lt;p&gt;Stay Informed: Keep up with the latest news and updates on airline regulations.&lt;/p&gt;
    &lt;p&gt;Contact Your Representatives: Let your senators and congress members know how you feel about the deregulation of the airline industry.&lt;/p&gt;
    &lt;p&gt;Know Your Rights: Understand the protections you currently have and how they might change.&lt;/p&gt;
    &lt;p&gt;American Airlines, Delta, Southwest, United, and other U.S. airlines are pushing to remove key protections for passengers and add more fees by rolling back regulations, arguing that it will lower costs and increase competition, but it could also lead to fewer rights and more hidden charges for travelers.&lt;/p&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;The deregulation push by U.S. airlines is a major threat to passenger rights. While airlines argue that deregulation will lead to cheaper fares and more competition, the reality is likely to be more fees, less transparency, and fewer protections for passengers. If successful, this move could turn back the clock to a time when flying was riddled with hidden charges and unfair treatment.&lt;/p&gt;
    &lt;p&gt;The future of air travel depends on consumers, advocacy groups, and lawmakers standing up for passenger rights. The airline industry may be pushing for deregulation, but it’s up to the public to ensure that the changes made are in the best interest of all passengers, not just the airlines. The battle is not just about cheaper tickets, but about ensuring that air travel remains fair, transparent, and accountable for everyone.&lt;/p&gt;
    &lt;p&gt;We use cookies on our website to give you the most relevant experience by remembering your preferences and repeat visits. By clicking “Accept”, you consent to the use of ALL the cookies.&lt;/p&gt;
    &lt;p&gt;This website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience.&lt;/p&gt;
    &lt;p&gt;Necessary cookies are absolutely essential for the website to function properly. These cookies ensure basic functionalities and security features of the website, anonymously.&lt;/p&gt;
    &lt;p&gt;Cookie&lt;/p&gt;
    &lt;p&gt;Duration&lt;/p&gt;
    &lt;p&gt;Description&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-analytics&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Analytics".&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-functional&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;The cookie is set by GDPR cookie consent to record the user consent for the cookies in the category "Functional".&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-necessary&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookies is used to store the user consent for the cookies in the category "Necessary".&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-others&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Other.&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-performance&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Performance".&lt;/p&gt;
    &lt;p&gt;viewed_cookie_policy&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;The cookie is set by the GDPR Cookie Consent plugin and is used to store whether or not user has consented to the use of cookies. It does not store any personal data.&lt;/p&gt;
    &lt;p&gt;Functional cookies help to perform certain functionalities like sharing the content of the website on social media platforms, collect feedbacks, and other third-party features.&lt;/p&gt;
    &lt;p&gt;Performance cookies are used to understand and analyze the key performance indexes of the website which helps in delivering a better user experience for the visitors.&lt;/p&gt;
    &lt;p&gt;Analytical cookies are used to understand how visitors interact with the website. These cookies help provide information on metrics the number of visitors, bounce rate, traffic source, etc.&lt;/p&gt;
    &lt;p&gt;Advertisement cookies are used to provide visitors with relevant ads and marketing campaigns. These cookies track visitors across websites and collect information to provide customized ads.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.travelandtourworld.com/news/article/american-joins-delta-southwest-united-and-other-us-airlines-push-to-strip-away-travelers-rights-and-add-more-fees-by-rolling-back-key-protections-in-new-deregulation-move/"/><published>2025-09-24T12:30:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359388</id><title>My Ed(1) Toolbox</title><updated>2025-09-24T14:39:00.371263+00:00</updated><content>&lt;doc fingerprint="f768818853b18a8d"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;My ed(1) Toolbox&lt;/head&gt;By Artyom Bologov&lt;p&gt;Apparently, I’m a huge ed(1) fan. I keep posting about it and use it as e.g. my Git editor, sudo editing tool, and my static site generator. But am I using it raw and standard as it is? Sometimes yes, but mostly no. This post is a listing of all the ed implementations and scripts I use.&lt;/p&gt;&lt;head rend="h2"&gt;GNU ed + red—Eternal Classics #&lt;/head&gt;&lt;p&gt;ed(1) is the standard text editor. And it’s available on most UNIX/POSIX-derived systems. (Some Linux distributions don’t provide it in default installation anymore, but that’s on them!) So relying on ed(1) and its powers is a good bet.&lt;/p&gt;&lt;p&gt;That’s why I always have GNU ed installed on my systems. It’s battle-tested, intuitive, and easily scriptable.&lt;/p&gt;&lt;p&gt;Bundled with GNU ed (and any POSIX-compliand ed(1) really), red(1) is the “restricted” version of ed(1). It’s locked to the directory it’s called in. And has no ability to pass through to the shell. I find it relatively useless though: I use ed(1) on secure systems and never allow anyone to access my precious ed(1) session. But still, red(1) is nice to have!&lt;/p&gt;&lt;head rend="h2"&gt;oed—OpenBSD ed #&lt;/head&gt;&lt;p&gt;Now, GNU ed is not conforming to POSIX in some behaviors:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;It has more CLI flags&lt;/item&gt;&lt;item&gt; It has &lt;code&gt;wq&lt;/code&gt;&lt;/item&gt;&lt;item&gt;It has POSIX extended regular expressions (EREs,) while most other implementations don’t. Thus making EREs a non-portable extension&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I risk introducing non-portable behavior if I only focus on GNU ed. And I want to have my scripts (including my website build scripts) portable across implementations.&lt;/p&gt;&lt;p&gt; So I installed OpenBSD ed from the repository kindly provided by one of the maintainers. And now I can safely replace &lt;code&gt;ed&lt;/code&gt; with &lt;code&gt;oed&lt;/code&gt; for most of my scripts.
As the least effort shot at portability.
It’s too convenient to not use it now.

&lt;/p&gt;&lt;head rend="h2"&gt;wed—ed wImproved #&lt;/head&gt;&lt;p&gt; I asked it on GNU ed mailing list whether they might support scripting abilities. Like sed(1) &lt;code&gt;-e&lt;/code&gt; and &lt;code&gt;-f&lt;/code&gt; flags or as a special executable for it.
One of the maintainers (predictably) replied that they (mostly) abide by POSIX and won’t add it.

&lt;/p&gt;&lt;p&gt;But! there was a person that emailed me personally and recommended slewsys ed as a version of ed(1) supporting scripts (among many other things.) So I installed it and called it wed(1) just to distinguish this re-implementation from The ed(1).&lt;/p&gt;&lt;p&gt;I don’t really use wed(1)—I’m fine with standard ed(1) (and my scripting wrapper for it). Bust still, it’s a modern and user-friendly extension. Want to get started with ed(1) but don’t want to deviate from the tradition?—this is the one to start with, probably.&lt;/p&gt;&lt;p&gt;Don’t get wed to an anime hologram. Get wed(1).&lt;/p&gt;&lt;head rend="h2"&gt;aed—Blaphemy Against Minimalism #&lt;/head&gt;&lt;p&gt;I understand the complaints about ed(1) being somewhat hostile to new users. It’s usually mitigated by the time spend with this magnificent software. But still, ed(1) is not perfect and might need some modernization.&lt;/p&gt;&lt;p&gt;So I made aed(1) as a better and more interactive ed(1). It’s mostly abusing Readline and shell scripts to deliver a friendlier experience. With syntax highlighting and perfectly inline-editable inputs.&lt;/p&gt;&lt;p&gt;So once you’re comfortable with basic ed(1). (Or it’s slightly friendlier wed(1) version.) You might want to speed up your workflows with aed(1)!&lt;/p&gt;&lt;p&gt;I might’ve gone too far though.&lt;/p&gt;&lt;head rend="h2"&gt;xed—You Don’t Need sed #&lt;/head&gt;&lt;p&gt;I have a user-friendly ed(1) on me now for interactive use. One use-case for ed(1) is not covered yet though—scripting! Having to do this type of newline-delimited scripts is too verbose compared to sed(1) ones.&lt;/p&gt;&lt;p&gt;What if I told you this is doable with a one-liner using my xed(1) script? Here:&lt;/p&gt;&lt;p&gt;It’s not the prettiest one, but it’s fulfilling many sed(1) use-cases. Speaking of the devil...&lt;/p&gt;&lt;head rend="h2"&gt;sed and ex... No. #&lt;/head&gt;&lt;p&gt;You don’t need ex(1) either, because it’s too vi(1)-oriented. They promised ed(1) eXtended, but we got ed(1) Fucked Up. Commands are incompatible with ed(1). Configuration is useless in ex(1) mode. Overall, ex(1) is just a poorly integrated back-end for vi(1).&lt;/p&gt;&lt;head rend="h2"&gt;My own ed(1) implementations #&lt;/head&gt;&lt;p&gt;If one likes some piece of software as an idea, they will inevitably try to reproduce it. So I did. I implemented ed(1) in Brainfuck under the aegis of Brainfuck Enterprise Solutions. I also did one in BASIC, pushing the limits of the no-memory BASIC as far as possible. And, finally, I did ed(1) in Modal, a term-rewriting-only system. All of these are useable... to a certain extent. But they don’t compare to the magnificence and purity of the Standard Text Editor.&lt;/p&gt;&lt;head rend="h2"&gt;Use ed(1) #&lt;/head&gt;&lt;p&gt;Whatever implementation you pick (pick aed(1)!), use it and love it. Because ed(1) deserves your love 😌&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aartaka.me/my-ed.html"/><published>2025-09-24T12:31:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359524</id><title>Learning Persian with Anki, ChatGPT and YouTube</title><updated>2025-09-24T14:39:00.306488+00:00</updated><content>&lt;doc fingerprint="9290b055cdb636d2"&gt;
  &lt;main&gt;
    &lt;p&gt;I’ve been learning Persian (Farsi) for a while now, and I’m using a bunch of tools for it. The central one is certainly Anki, a spaced repetition app to train memory. I’m creating my own never-ending deck of cards, with different types of content, for different purposes. The most frequent type of cards is grammar focused phrases (very rarely single words) coming sometimes from my own daily life, but also very often directly from videos of the Persian Learning YouTube channel, created by Majid, a very talented and nice Persian teacher, in my opinion.&lt;/p&gt;
    &lt;p&gt;Let’s take an example, suppose there is this slide in one of Majid’s videos:&lt;/p&gt;
    &lt;p&gt;From this, I will extract three screenshots (with the MacOS screenshot tool). First, to create a card of type “basic” (one side). I use this type of card to exercise my reading, which is very difficult and remains stubbornly slow, even though I know the 32 letters of the Persian alphabet quite well by now. But the different ways of writing them (which varies by their position in the word) and the fact that the vowels are not present makes it an enduringly challenging task.&lt;/p&gt;
    &lt;p&gt;The next type of card I create with the two remaining screenshots is “basic and reversed”, which actually creates two cards (one for each direction), one with some romanized phrase, and the other with the English or French translation:&lt;/p&gt;
    &lt;p&gt;When I review these cards in my daily Anki routine, this is where ChatGPT enters into play. First I have set a “Persian” project with these instructions:&lt;/p&gt;
    &lt;p&gt;With this project, every time I have a doubt or don’t remember something in Anki, I just take a screenshot and paste it in the project:&lt;/p&gt;
    &lt;p&gt;With this, I have an instant refresher on any notion, in any context. Sometimes I need to do this over and over, before it gels into a deeper, more instant and visceral “knowledge”.&lt;/p&gt;
    &lt;p&gt;The next set of techniques is also based on YouTube. I use a Chrome extension called Dual Subtitles (which only works of course with videos having actual dual sources of subtitles):&lt;/p&gt;
    &lt;p&gt;The dual subtitles serve a couple of purposes: first as a source of new Anki cards (I create the cards directly, again with screenshots in the clipboard).&lt;/p&gt;
    &lt;p&gt;I also use the Tweaks for YouTube extension, which allows me to get extra keyboard shortcuts, to go back and forward only 1 second, instead of the built-in 5 seconds.&lt;/p&gt;
    &lt;p&gt;With these YouTube extensions, I have developed this particular “technique” to improve my vocal understanding:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I listen at 75% speed&lt;/item&gt;
      &lt;item&gt;I use the “dual subtitles” browser extension to have both the Farsi and English subtitles at the same time (I set the Farsi one slightly bigger)&lt;/item&gt;
      &lt;item&gt;Every time a new sentence appears, I read it very quickly first in English (I pause if I need to), and then I listen carefully to the voice, to let the meaning and sound of Farsi infuse my mind (this part is very subtle but the most important: you must “feel” that you understand, and this feeling must cover even the words that you don’t know; because the meaning of the sentence is currently present and active in your mind, because you just read the English part, I believe that its mapping with the Farsi words that you then hear is particularly efficient, at least that’s my theory)&lt;/item&gt;
      &lt;item&gt;I also read the Farsi script, to improve my understanding, and disambiguate certain words for which it’s hard for me to hear what is exactly said&lt;/item&gt;
      &lt;item&gt;I repeat out loud what has been said also, which is quite important&lt;/item&gt;
      &lt;item&gt;Most importantly: I repeat this process (for a single video) over and over, in order to reach a stage where I genuinely understand what is said, in real-time, which is a very powerful and exhilarating feeling.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cjauvin.github.io/posts/learning-persian/"/><published>2025-09-24T12:45:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359604</id><title>How to Lead in a Room Full of Experts</title><updated>2025-09-24T14:39:00.000564+00:00</updated><content>&lt;doc fingerprint="f3751d93156404b7"&gt;
  &lt;main&gt;
    &lt;p&gt;Here is a realization I made recently. I'm sitting in a room full of smart people. On one side are developers who understand the ins and outs of our microservice architecture. On the other are the front-end developers who can debug React in their sleep. In front of me is the product team that has memorized every possible user path that exists on our website. And then, there is me. The lead developer. I don't have the deepest expertise on any single technology.&lt;/p&gt;
    &lt;p&gt;So what exactly is my role when I'm surrounded by experts? Well, that's easy. I have all the answers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical Leadership&lt;/head&gt;
    &lt;p&gt;OK. Technically, I don't have all the answers. But I know exactly where to find them and connect the pieces together.&lt;/p&gt;
    &lt;p&gt;When the backend team explains why a new authentication service would take three weeks to build, I'm not thinking about the OAuth flows or JWT token validation. Instead, I think about how I can communicate it to the product team who expects it done "sometime this week." When the product team requests a "simple" feature, I'm thinking about the 3 teams that need to be involved to update the necessary microservices.&lt;/p&gt;
    &lt;p&gt;Leadership in technical environments isn't about being the smartest person in the room. It's about being the most effective translator.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leading is a Social Skill&lt;/head&gt;
    &lt;p&gt;I often get "eye rolls" when I say this to developers: You are not going to convince anyone with facts. In a room full of experts, your technical credibility gets you a seat at the table, but your social skills determine whether anything productive happens once you're there.&lt;/p&gt;
    &lt;p&gt;Where ideally you will provide documentation that everyone can read and understand, in reality, you need to talk to get people to understand. People can get animated when it comes to the tools they use. When the database team and the API team are talking past each other about response times, your role isn't to lay down the facts. Instead it's to read the room and find a way to address technical constraints and unclear requirements. It means knowing when to let a heated technical debate continue because it's productive, and when to intervene because it's become personal.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leading is Remembering the Goal&lt;/head&gt;
    &lt;p&gt;When you are an expert in your field, you love to dive deep. It's what makes you experts. But someone needs to keep one eye on the forest while everyone else is examining the trees.&lt;/p&gt;
    &lt;p&gt;I've sat through countless meetings where engineers debated the merits of different caching strategies while the real issue was that we hadn't clearly defined what "fast enough" meant for the user experience. The technical discussion was fascinating, but it wasn't moving us toward shipping.&lt;/p&gt;
    &lt;p&gt;As a leader, your job isn't to have sophisticated technical opinions. It's to ask how this "discussion" can move us closer to solving our actual problem.&lt;/p&gt;
    &lt;p&gt;When you understand a problem, and you have a room full of experts, the solution often emerges from the discussion. But someone needs to clearly articulate what problem we're actually trying to solve.&lt;/p&gt;
    &lt;p&gt;When a product team says customers are reporting the app is too slow, that's not a clear problem. It's a symptom. It might be that users are not noticing when the shopping cart is loaded, or that maybe we have an event that is not being triggered at the right time. Or maybe the app feels sluggish during peak hours. Each of those problems has different solutions, different priorities, and different trade-offs. Each expert might be looking at the problem with their own lense, and may miss the real underlying problem.&lt;/p&gt;
    &lt;p&gt;Your role as a leader is to make sure the problem is translated in a way the team can clearly understand the problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leading is Saying "I Don't Know"&lt;/head&gt;
    &lt;p&gt;By definition, leading is knowing the way forward. But in reality, in a room full of experts, pretending to know everything makes you look like an idiot.&lt;/p&gt;
    &lt;p&gt;Instead, "I don't know, but let's figure it out" becomes a superpower. It gives your experts permission to share uncertainty. It models intellectual humility. And it keeps the focus on moving forward rather than defending ego. It's also an opportunity to let your experts shine.&lt;/p&gt;
    &lt;p&gt;Nothing is more annoying than a lead who needs to be the smartest person in every conversation. Your database expert spent years learning how to optimize queries - let them be the hero when performance issues arise. Your security specialist knows threat models better than you, give them the floor when discussing architecture decisions.&lt;/p&gt;
    &lt;p&gt;Make room for some productive discussion. When two experts disagree about implementation approaches, your job isn't to pick the "right" answer. It's to help frame the decision in terms of trade-offs, timeline, and user impact.&lt;/p&gt;
    &lt;p&gt;Your value isn't in having all the expertise. It's in recognizing which expertise is needed when, and creating space for the right people to contribute their best work.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Translation Challenge&lt;/head&gt;
    &lt;p&gt;There was this fun blog post I read recently about how non-developers read tutorials written by developers. What sounds natural to you, can be complete gibberish to someone else. As a lead, you constantly need to think about your audience. You need to learn multiple languages to communicate the same thing:&lt;/p&gt;
    &lt;p&gt;Developer language: "The authentication service has a dependency on the user service, and if we don't implement proper circuit breakers, we'll have cascading failures during high load."&lt;/p&gt;
    &lt;p&gt;Product language: "If our login system goes down, it could take the entire app with it. We need to build in some safeguards, which will add about a week to the timeline but prevent potential outages."&lt;/p&gt;
    &lt;p&gt;Executive language: "We're prioritizing system reliability over feature velocity for this sprint. This reduces risk of user-facing downtime that could impact revenue."&lt;/p&gt;
    &lt;p&gt;All three statements describe the same technical decision, but each is crafted for its audience. Your experts shouldn't have to learn product speak, and your product team shouldn't need to understand circuit breaker patterns. But someone needs to bridge that gap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond "Because, that's why!"&lt;/head&gt;
    &lt;p&gt;"I'm the lead, and we are going to do it this way." That's probably the worst way to make a decision. That might work in the short term, but it erodes trust and kills the collaborative culture that makes expert teams thrive.&lt;/p&gt;
    &lt;p&gt;Instead, treat your teams like adults and communicate the reason behind your decision:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"We're choosing the more conservative approach because the cost of being wrong is high, and we can iterate later."&lt;/item&gt;
      &lt;item&gt;"I know this feels like extra work, but it aligns with our architectural goals and will save us time on the next three features."&lt;/item&gt;
      &lt;item&gt;"This isn't the most elegant solution, but it's the one we can ship confidently within our timeline."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The more comfortable you become with not being the expert, the more effective you become as a leader.&lt;/p&gt;
    &lt;p&gt;When you stop trying to out-expert the experts, you can focus on what expert teams actually need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clear problem definitions&lt;/item&gt;
      &lt;item&gt;Context for decision-making&lt;/item&gt;
      &lt;item&gt;Translation between different perspectives&lt;/item&gt;
      &lt;item&gt;Protection from unnecessary complexity&lt;/item&gt;
      &lt;item&gt;Space to do their best work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Your role isn't to have all the answers. It's to make sure the right questions get asked, the right people get heard, and the right decisions get made for the right reasons.&lt;/p&gt;
    &lt;p&gt;Technical leadership in expert environments is less about command and control, and more about connection and context. You're not the conductor trying to play every instrument. You're the one helping the orchestra understand what song they're playing together.&lt;/p&gt;
    &lt;p&gt;That's a much more interesting challenge than trying to be the smartest person in the room.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://idiallo.com/blog/how-to-lead-in-a-room-full-of-experts"/><published>2025-09-24T12:52:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45360475</id><title>Just Let Me Select Text</title><updated>2025-09-24T14:38:59.502552+00:00</updated><content>&lt;doc fingerprint="cb024a5731ccf78e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Just Let Me Select Text&lt;/head&gt;By Artyom Bologov&lt;head rend="h2"&gt;Untranslatable Bios #&lt;/head&gt;&lt;p&gt;I’m lonely. Like everyone-ish else. Naturally, I’m on Bumble. (Because Tinder is a rape-friendly lure trap.) When work calls get boring I inevitably start swiping (mostly left 😢)&lt;/p&gt;&lt;p&gt;There are lots of tourists in Armenia in the summer. From all over the world really. Speaking a stupefying range of languages. With bios and prompt answers in these numerous languages. Not necessarily discernible to me due to my language learning stagnation.&lt;/p&gt;&lt;p&gt;So there’s this profile of a pretty German girl. With bio and prompts in (an undeniably beautiful) German. Speaking English, she made the decision to use her mother tongue for the bio. A totally valid choice.&lt;/p&gt;&lt;p&gt;So I want to know the story she tells with her profile:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Select her bio,&lt;/item&gt;&lt;item&gt;copy it,&lt;/item&gt;&lt;item&gt;paste into a translator,&lt;/item&gt;&lt;item&gt;look up the exact meaning of some mistranslated German word,&lt;/item&gt;&lt;item&gt;and realize the unexpected poetic meaning she put into these 300 chars.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Except… I can’t do that. The text is not selectable/copyable in Bumble app. I have to do a bunch of relatively unsurmountable steps to do what should’ve taken half a minute. Like screenshot the profile and scrape the text with iOS Photos text recognition. Or use some OCR (web)app elsewhere. It’s… discouraging. Thus I give up and swipe left. A shame—she was beautiful at the very least!&lt;/p&gt;&lt;head rend="h2"&gt;Media #&lt;/head&gt;&lt;p&gt;By making the text in your UI non-selectable, you turn it into… an image essentially? Images, audio, video, and interactive JS-heavy pages are multidimentional media. Not really manipulable and referenceable in any reasonable way. (Not even with Media Fragments—they were turned down by everyone.) You lose a whole dimension (🥁) of functionality and benefit by going with such media or their semblance text.&lt;/p&gt;&lt;p&gt; Podcasts are not easy to roll back to useful part. Video transcripts don’t make sense without the visuals. Web graphics are opaque &lt;code&gt;&amp;lt;canvas&amp;gt;&lt;/code&gt;-es you can’t gut.

&lt;/p&gt;&lt;p&gt;Text is copyable. Text is translatable. Text is accessible (as in a11y.) Text is lightweight. Text is fundamental to how we people process information.&lt;/p&gt;&lt;p&gt;That’s why we still use text in our UIs. We want to convey the meaning. We strive to provide unambiguous instructions. We need to be understood. So why make the text harder to process and understand?&lt;/p&gt;&lt;head rend="h2"&gt;Stop It #&lt;/head&gt;&lt;p&gt;Whenever you disable text selection/copying on your UI, you commit a crime against the user. Crime against comprehension. Crime against accessibility. Crime against the meaning. Stop incapacitating your users, allow them to finally use the text.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aartaka.me/select-text.html"/><published>2025-09-24T13:56:37+00:00</published></entry></feed>