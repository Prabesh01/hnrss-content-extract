<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-18T10:45:06.361070+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46302337</id><title>A Safer Container Ecosystem with Docker: Free Docker Hardened Images</title><updated>2025-12-18T10:45:22.717174+00:00</updated><content>&lt;doc fingerprint="2dc4a1591d2c01c9"&gt;
  &lt;main&gt;
    &lt;p&gt;Containers are the universal path to production for most developers, and Docker has always been the steward of the ecosystem. Docker Hub has over 20 billion monthly pulls, with nearly 90% of organizations now relying on containers in their software delivery workflows. That gives us a responsibility: to help secure the software supply chain for the world.&lt;/p&gt;
    &lt;p&gt;Why? Supply-chain attacks are exploding. In 2025, they caused more than $60 billion in damage, tripling from 2021. No one is safe. Every language, every ecosystem, every build and distribution step is a target.&lt;/p&gt;
    &lt;p&gt;For this reason, we launched Docker Hardened Images (DHI), a secure, minimal, production-ready set of images, in May 2025, and since then have hardened over 1,000 images and helm charts in our catalog. Today, we are establishing a new industry standard by making DHI freely available and open source to everyone who builds software. All 26 Million+ developers in the container ecosystem. DHI is fully open and free to use, share, and build on with no licensing surprises, backed by an Apache 2.0 license. DHI now gives the world a secure, minimal, production-ready foundation from the very first pull.&lt;/p&gt;
    &lt;p&gt;If it sounds too good to be true, here’s the bottom line up front: every developer and every application can (and should!) use DHI without restrictions. When you need continuous security patching, applied in under 7 days, images for regulated industries (e.g., FIPS, FedRAMP), you want to build customized images on our secure build infrastructure, or you need security patches beyond end-of-life, DHI has commercial offerings. Simple.&lt;/p&gt;
    &lt;p&gt;Since the introduction of DHI, enterprises like Adobe and Qualcomm have bet on Docker for securing their entire enterprise to achieve the most stringent levels of compliance, while startups like Attentive and Octopus Deploy have accelerated their ability to get compliance and sell to larger businesses.&lt;/p&gt;
    &lt;p&gt;Now everyone and every application can build securely from the first &lt;code&gt;docker build&lt;/code&gt;. Unlike other opaque or proprietary hardened images, DHI is compatible with Alpine and Debian, trusted and familiar open source foundations teams already know and can adopt with minimal change. And while some vendors suppress CVEs in their feed to maintain a green scanner, Docker is always transparent, even when we’re still working on patches, because we fundamentally believe you should always know what your security posture is. The result: dramatically reduced CVEs (guaranteed near zero in DHI Enterprise), images up to 95 percent smaller, and secure defaults without ever compromising transparency or trust.&lt;/p&gt;
    &lt;p&gt;There’s more. We’ve already built Hardened Helm Charts to leverage DHI images in Kubernetes environments; those are open source too. And today, we’re expanding that foundation with Hardened MCP Servers. We’re bringing DHI’s security principles to the MCP interface layer, the backbone of every agentic app. And starting now, you can run hardened versions of the MCP servers developers rely on most: Mongo, Grafana, GitHub, and more. And this is just the beginning. In the coming months, we will extend this hardened foundation across the entire software stack with hardened libraries, hardened system packages, and other secure components everyone depends on. The goal is simple: be able to secure your application from &lt;code&gt;main()&lt;/code&gt; down. &lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt;The philosophy of Docker Hardened Images&lt;/head&gt;
    &lt;p&gt;Base images define your application’s security from the very first layer, so it’s critical to know exactly what goes into them. Here’s how we approach it.&lt;/p&gt;
    &lt;p&gt;First: total transparency in every part of our minimal, opinionated, secure images.&lt;/p&gt;
    &lt;p&gt;DHI uses a distroless runtime to shrink the attack surface while keeping the tools developers rely on. But security is more than minimalism; it requires full transparency. Too many vendors blur the truth with proprietary CVE scoring, downgraded vulnerabilities, or vague promises about reaching SLSA Build Level 3.&lt;/p&gt;
    &lt;p&gt;DHI takes a different path. Every image includes a complete and verifiable SBOM. Every build provides SLSA Build Level 3 provenance. Every vulnerability is assessed using transparent public CVE data; we won’t hide vulnerabilities when we haven’t fixed them. Every image comes with proof of authenticity. The result: a secure foundation you can trust, built with clarity, verified with evidence, and delivered without compromise.&lt;/p&gt;
    &lt;p&gt;Second: Migrating to secure images takes real work, and no one should pretend otherwise. But as you’d expect from Docker, we’ve focused on making the DX incredibly easy to use. As we mentioned before, DHI is built on the open source foundations the world already trusts, Debian and Alpine, so teams can adopt it with minimal friction. We’re reducing that friction even more: Docker’s AI assistant can scan your existing containers and recommend or even apply equivalent hardened images; the feature is experimental as this is day one, but we’ll quickly GA it as we learn from real world migrations.&lt;/p&gt;
    &lt;p&gt;Lastly: we think about the most aggressive SLAs and longest support times and make certain that every piece of DHI can support that when you need it.&lt;/p&gt;
    &lt;p&gt;DHI Enterprise, the commercial offering of DHI, includes a 7-day commitment for critical CVE remediation, with a roadmap toward one day or less. For regulated industries and mission-critical systems, this level of trust is mandatory. Achieving it is hard. It demands deep test automation and the ability to maintain patches that diverge from upstream until they are accepted. That is why most organizations cannot do this on their own. In addition, DHI Enterprise allows organizations to easily customize DHI images, leveraging Docker’s build infrastructure which takes care of the full image lifecycle management for you, ensuring that build provenance and compliance is maintained. For example, typically organizations need to add certificates and keys, system packages, scripts, and so on. DHI’s build service makes this trivial.&lt;/p&gt;
    &lt;p&gt;Because our patching SLAs and our build service carry real operational cost, DHI has historically been one commercial offering. But our vision has always been broader. This level of security should be available to everyone, and the timing matters. Now that the evidence, infrastructure, and industry partnerships are in place, we are delivering on that vision. That is why today we are making Docker Hardened Images free and open source.&lt;/p&gt;
    &lt;p&gt;This move carries the same spirit that defined Docker Official Images over a decade ago. We made them free, kept them free, and backed them with clear docs, best practices, and consistent maintenance. That foundation became the starting point for millions of developers and partners.&lt;/p&gt;
    &lt;p&gt;Now we’re doing it again. DHI being free is powered by a rapidly growing ecosystem of partners, from Google, MongoDB, and the CNCF delivering hardened images to security platforms like Snyk and JFrog Xray integrating DHI directly into their scanners. Together, we are building a unified, end-to-end supply chain that raises the security bar for the entire industry.&lt;/p&gt;
    &lt;head rend="h4"&gt;“Docker’s move to make its hardened images freely available under Apache 2.0 underscores its strong commitment to the open source ecosystem. Many CNCF projects can already be found in the DHI catalog, and giving the broader community access to secure, well-maintained building blocks helps us strengthen the software supply chain together. It’s exciting to see Docker continue to invest in open collaboration and secure container infrastructure.”&lt;/head&gt;
    &lt;p&gt;Jonathan Bryce&lt;/p&gt;
    &lt;p&gt;Executive Director at the Cloud Native Computing Foundation&lt;/p&gt;
    &lt;head rend="h4"&gt;“Software supply chain attacks are a severe industry problem. Making Docker Hardened Images free and pervasive should underpin faster, more secure software delivery across the industry by making the right thing the easy thing for developers.”&lt;/head&gt;
    &lt;p&gt;James Governor&lt;/p&gt;
    &lt;p&gt;Analyst and Co-founder, RedMonk&lt;/p&gt;
    &lt;head rend="h4"&gt;“Security shouldn’t be a premium feature. By making hardened images free, Docker is letting every developer, not just big enterprises, start with a safer foundation. We love seeing tools that reduce noise and toil, and we’re ready to run these secure workloads on Google Cloud from day one”&lt;/head&gt;
    &lt;p&gt;Ryan J. Salva&lt;/p&gt;
    &lt;p&gt;Senior Director of Product at Google, Developer Experiences&lt;/p&gt;
    &lt;head rend="h4"&gt;“At MongoDB, we believe open source plays a central role in how modern software is built, enabling flexibility, choice, and developer productivity. That’s why we’re excited about free Docker Hardened Images for MongoDB. These images provide trusted, ready-to-deploy building blocks on proven Linux foundations such as Alpine and Debian, and with an Apache 2.0 license, they remain fully open source and free for anyone to use. With Docker Hub’s global reach and MongoDB’s commitment to reliability and safety, we are making it easier to build with confidence on a secure and open foundation for the future”&lt;/head&gt;
    &lt;p&gt;Jim Scharf&lt;/p&gt;
    &lt;p&gt;Chief Technology Officer, MongoDB&lt;/p&gt;
    &lt;head rend="h4"&gt;“We’re excited to partner with Docker to deliver secure, enterprise-grade AI workloads from development to production. With over 50 million users and the majority of Fortune 500 trusting Anaconda to help them operate at enterprise scale securely, this partnership with Docker brings that same foundation to Docker Hardened Images. This enables teams to spend less time managing risk and more time innovating, while reducing the time from idea to production.”&lt;/head&gt;
    &lt;p&gt;David DeSanto&lt;/p&gt;
    &lt;p&gt;Chief Executive Officer, Anaconda&lt;/p&gt;
    &lt;head rend="h4"&gt;“Socket stops malicious packages at install time, and Docker Hardened Images (DHI) give those packages a trustworthy place to run. With free DHI, teams get both layers of protection without lifting a finger. Pull a hardened image, run npm install, and the Socket firewall embedded in the DHI is already working for you. That is what true secure-by-default should look like, and we’re excited to partner with Docker and make it happen at their scale.”&lt;/head&gt;
    &lt;p&gt;Feross Aboukhadijeh&lt;/p&gt;
    &lt;p&gt;Founder and CEO, Socket&lt;/p&gt;
    &lt;head rend="h4"&gt;“Teams building with Temporal orchestrate mission-critical workflows, and Docker is how they deploy those services in production. Making Docker Hardened Images freely available gives our users a very strong foundation for those workflows from day one, and Extended Lifecycle Support helps them keep long running systems secure without constant replatforming.”&lt;/head&gt;
    &lt;p&gt;Maxim Fateev&lt;/p&gt;
    &lt;p&gt;Chief Technology Officer, Temporal&lt;/p&gt;
    &lt;head rend="h4"&gt;“At CircleCI, we know teams need to validate code as fast as they can generate it—and that starts with a trusted foundation. Docker Hardened Images eliminate a critical validation bottleneck by providing pre-secured, continuously verified components right from the start, helping teams ship fast, with confidence.”&lt;/head&gt;
    &lt;p&gt;Rob Zuber&lt;/p&gt;
    &lt;p&gt;Chief Technology Officer, CircleCI&lt;/p&gt;
    &lt;head rend="h4"&gt;“We evaluated multiple options for hardened base images and chose Docker Hardened Images (DHI) for its alignment with our supply chain security posture, developer tooling compatibility, Docker’s maturity in this space, and integration with our existing infrastructure. Our focus was on balancing trust, maintainability, and ecosystem compatibility.”&lt;/head&gt;
    &lt;p&gt;Vikram Sethi&lt;/p&gt;
    &lt;p&gt;Principal Scientist, Adobe&lt;/p&gt;
    &lt;head rend="h4"&gt;“Developers deserve secure foundations that do not slow them down. By making Docker Hardened Images freely available, Docker is making it easier than ever to secure the software supply chain at the source. This helps eliminate risk before anything touches production, a mission shared by LocalStack. At LocalStack, we are especially excited that developers will be able to use these hardened, minimal images for our emulators, helping teams finally break free from constant CVE firefighting.”&lt;/head&gt;
    &lt;p&gt;Waldemar Hummer&lt;/p&gt;
    &lt;p&gt;Co-Founder and CTO at LocalStack&lt;/p&gt;
    &lt;head rend="h2"&gt;A Secure Path for Every Team and Business&lt;/head&gt;
    &lt;p&gt;Everyone now has a secure foundation to start from with DHI. But businesses of all shapes and sizes often need more. Compliance requirements and risk tolerance may demand CVE patches ahead of upstream the moment the source becomes available. Companies operating in enterprise or government sectors must meet strict standards such as FIPS or STIG. And because production can never stop, many organizations need security patching to continue even after upstream support ends.&lt;/p&gt;
    &lt;p&gt;That is why we now offer three DHI options, each built for a different security reality.&lt;/p&gt;
    &lt;p&gt;Docker Hardened Images: Free for Everyone. DHI is the foundation modern software deserves: minimal hardened images, easy migration, full transparency, and an open ecosystem built on Alpine and Debian.&lt;/p&gt;
    &lt;p&gt;Docker Hardened Images (DHI) Enterprise: DHI Enterprise delivers the guarantees that organizations, governments, and institutions with strict security or regulatory demands rely on. FIPS-enabled and STIG-ready images. Compliance with CIS benchmarks. SLA-backed remediations they can trust for critical CVEs in under 7 days. And those SLAs keep getting shorter as we push toward one-day (or less) critical fixes.&lt;/p&gt;
    &lt;p&gt;For teams that need more control, DHI Enterprise delivers. Change your images. Configure runtimes. Install tools like curl. Add certificates. DHI Enterprise gives you unlimited customization, full catalog access, and the ability to shape your images on your terms while staying secure.&lt;/p&gt;
    &lt;p&gt;DHI Extended Lifecycle Support (ELS): ELS is a paid add-on to DHI Enterprise, built to solve one of software’s hardest problems. When upstream support ends, patches stop but vulnerabilities don’t. Scanners light up, auditors demand answers, and compliance frameworks expect verified fixes. ELS ends that cycle with up to five additional years of security coverage, continuous CVE patches, updated SBOMs and provenance, and ongoing signing and auditability for compliance.&lt;/p&gt;
    &lt;p&gt;You can learn more about these options here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Here’s how to get started&lt;/head&gt;
    &lt;p&gt;Securing the container ecosystem is something we do together. Today, we’re giving the world a stronger foundation to build on. Now we want every developer, every open source project, every software vendor, and every platform to make Docker Hardened Images the default.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Join our launch webinar to get hands-on and learn what’s new.&lt;/item&gt;
      &lt;item&gt;Start using Docker Hardened Images today for free.&lt;/item&gt;
      &lt;item&gt;Explore the docs and bring DHI into your workflows&lt;/item&gt;
      &lt;item&gt;Join our partner program and help raise the security bar for everyone.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lastly, we are just getting started, and if you’re reading this and want to help build the future of container security, we’d love to meet you. Join us.&lt;/p&gt;
    &lt;head rend="h2"&gt;Authors’ Notes&lt;/head&gt;
    &lt;head rend="h3"&gt;Christian Dupuis&lt;/head&gt;
    &lt;p&gt;Today’s announcement marks a watershed moment for our industry. Docker is fundamentally changing how applications are built-secure by default for every developer, every organization, and every open-source project.&lt;/p&gt;
    &lt;p&gt;This moment fills me with pride as it represents the culmination of years of work: from the early days at Atomist building an event-driven SBOM and vulnerability management system, the foundation that still underpins Docker Scout today, to unveiling DHI earlier this year, and now making it freely available to all. I am deeply grateful to my incredible colleagues and friends at Docker who made this vision a reality, and to our partners and customers who believed in us from day one and shaped this journey with their guidance and feedback.&lt;/p&gt;
    &lt;p&gt;Yet while this is an important milestone, it remains just that, a milestone. We are far from done, with many more innovations on the horizon. In fact, we are already working on what comes next.&lt;/p&gt;
    &lt;p&gt;Security is a team sport, and today Docker opened the field to everyone. Let’s play.&lt;/p&gt;
    &lt;head rend="h3"&gt;Michael Donovan&lt;/head&gt;
    &lt;p&gt;I joined Docker to positively impact as many developers as possible. This launch gives every developer the right to secure their applications without adding toil to their workload. It represents a monumental shift in the container ecosystem and the digital experiences we use every day.&lt;/p&gt;
    &lt;p&gt;I’m extremely proud of the product we’ve built and the customers we serve every day. I’ve had the time of my life building this with our stellar team and I’m more excited than ever for what’s to come next.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.docker.com/blog/docker-hardened-images-for-every-developer/"/><published>2025-12-17T17:13:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46303277</id><title>How SQLite is tested</title><updated>2025-12-18T10:45:22.240174+00:00</updated><content>&lt;doc fingerprint="a6ca0bbe6c201f3b"&gt;
  &lt;main&gt;
    &lt;p&gt;The reliability and robustness of SQLite is achieved in part by thorough and careful testing.&lt;/p&gt;
    &lt;p&gt;As of version 3.42.0 (2023-05-16), the SQLite library consists of approximately 155.8 KSLOC of C code. (KSLOC means thousands of "Source Lines Of Code" or, in other words, lines of code excluding blank lines and comments.) By comparison, the project has 590 times as much test code and test scripts - 92053.1 KSLOC.&lt;/p&gt;
    &lt;p&gt;There are four independent test harnesses used for testing the core SQLite library. Each test harness is designed, maintained, and managed separately from the others.&lt;/p&gt;
    &lt;p&gt;The TCL Tests are the original tests for SQLite. They are contained in the same source tree as the SQLite core and like the SQLite core are in the public domain. The TCL tests are the primary tests used during development. The TCL tests are written using the TCL scripting language. The TCL test harness itself consists of 27.2 KSLOC of C code used to create the TCL interface. The test scripts are contained in 1390 files totaling 23.2MB in size. There are 51445 distinct test cases, but many of the test cases are parameterized and run multiple times (with different parameters) so that on a full test run millions of separate tests are performed.&lt;/p&gt;
    &lt;p&gt;The TH3 test harness is a set of proprietary tests, written in C that provide 100% branch test coverage (and 100% MC/DC test coverage) to the core SQLite library. The TH3 tests are designed to run on embedded and specialized platforms that would not easily support TCL or other workstation services. TH3 tests use only the published SQLite interfaces. TH3 consists of about 76.9 MB or 1055.4 KSLOC of C code implementing 50362 distinct test cases. TH3 tests are heavily parameterized, though, so a full-coverage test runs about 2.4 million different test instances. The cases that provide 100% branch test coverage constitute a subset of the total TH3 test suite. A soak test prior to release does about 248.5 million tests. Additional information on TH3 is available separately.&lt;/p&gt;
    &lt;p&gt;The SQL Logic Test or SLT test harness is used to run huge numbers of SQL statements against both SQLite and several other SQL database engines and verify that they all get the same answers. SLT currently compares SQLite against PostgreSQL, MySQL, Microsoft SQL Server, and Oracle 10g. SLT runs 7.2 million queries comprising 1.12GB of test data.&lt;/p&gt;
    &lt;p&gt;The dbsqlfuzz engine is a proprietary fuzz tester. Other fuzzers for SQLite mutate either the SQL inputs or the database file. Dbsqlfuzz mutates both the SQL and the database file at the same time, and is thus able to reach new error states. Dbsqlfuzz is built using the libFuzzer framework of LLVM with a custom mutator. There are 336 seed files. The dbsqlfuzz fuzzer runs about one billion test mutations per day. Dbsqlfuzz helps ensure that SQLite is robust against attack via malicious SQL or database inputs.&lt;/p&gt;
    &lt;p&gt;In addition to the four main test harnesses, there are many other small programs that implement specialized tests. Here are a few examples:&lt;/p&gt;
    &lt;p&gt;All of the tests above must run successfully, on multiple platforms and under multiple compile-time configurations, before each release of SQLite.&lt;/p&gt;
    &lt;p&gt;Prior to each check-in to the SQLite source tree, developers typically run a subset (called "veryquick") of the Tcl tests consisting of about 304.7 thousand test cases. The veryquick tests include most tests other than the anomaly, fuzz, and soak tests. The idea behind the veryquick tests are that they are sufficient to catch most errors, but also run in only a few minutes instead of a few hours.&lt;/p&gt;
    &lt;p&gt;Anomaly tests are tests designed to verify the correct behavior of SQLite when something goes wrong. It is (relatively) easy to build an SQL database engine that behaves correctly on well-formed inputs on a fully functional computer. It is more difficult to build a system that responds sanely to invalid inputs and continues to function following system malfunctions. The anomaly tests are designed to verify the latter behavior.&lt;/p&gt;
    &lt;p&gt;SQLite, like all SQL database engines, makes extensive use of malloc() (See the separate report on dynamic memory allocation in SQLite for additional detail.) On servers and workstations, malloc() never fails in practice and so correct handling of out-of-memory (OOM) errors is not particularly important. But on embedded devices, OOM errors are frighteningly common and since SQLite is frequently used on embedded devices, it is important that SQLite be able to gracefully handle OOM errors.&lt;/p&gt;
    &lt;p&gt;OOM testing is accomplished by simulating OOM errors. SQLite allows an application to substitute an alternative malloc() implementation using the sqlite3_config(SQLITE_CONFIG_MALLOC,...) interface. The TCL and TH3 test harnesses are both capable of inserting a modified version of malloc() that can be rigged to fail after a certain number of allocations. These instrumented mallocs can be set to fail only once and then start working again, or to continue failing after the first failure. OOM tests are done in a loop. On the first iteration of the loop, the instrumented malloc is rigged to fail on the first allocation. Then some SQLite operation is carried out and checks are done to make sure SQLite handled the OOM error correctly. Then the time-to-failure counter on the instrumented malloc is increased by one and the test is repeated. The loop continues until the entire operation runs to completion without ever encountering a simulated OOM failure. Tests like this are run twice, once with the instrumented malloc set to fail only once, and again with the instrumented malloc set to fail continuously after the first failure.&lt;/p&gt;
    &lt;p&gt;I/O error testing seeks to verify that SQLite responds sanely to failed I/O operations. I/O errors might result from a full disk drive, malfunctioning disk hardware, network outages when using a network file system, system configuration or permission changes that occur in the middle of an SQL operation, or other hardware or operating system malfunctions. Whatever the cause, it is important that SQLite be able to respond correctly to these errors and I/O error testing seeks to verify that it does.&lt;/p&gt;
    &lt;p&gt;I/O error testing is similar in concept to OOM testing; I/O errors are simulated and checks are made to verify that SQLite responds correctly to the simulated errors. I/O errors are simulated in both the TCL and TH3 test harnesses by inserting a new Virtual File System object that is specially rigged to simulate an I/O error after a set number of I/O operations. As with OOM error testing, the I/O error simulators can be set to fail just once, or to fail continuously after the first failure. Tests are run in a loop, slowly increasing the point of failure until the test case runs to completion without error. The loop is run twice, once with the I/O error simulator set to simulate only a single failure and a second time with it set to fail all I/O operations after the first failure.&lt;/p&gt;
    &lt;p&gt;In I/O error tests, after the I/O error simulation failure mechanism is disabled, the database is examined using PRAGMA integrity_check to make sure that the I/O error has not introduced database corruption.&lt;/p&gt;
    &lt;p&gt;Crash testing seeks to demonstrate that an SQLite database will not go corrupt if the application or operating system crashes or if there is a power failure in the middle of a database update. A separate white-paper titled Atomic Commit in SQLite describes the defensive measures SQLite takes to prevent database corruption following a crash. Crash tests strive to verify that those defensive measures are working correctly.&lt;/p&gt;
    &lt;p&gt;It is impractical to do crash testing using real power failures, of course, and so crash testing is done in simulation. An alternative Virtual File System is inserted that allows the test harness to simulate the state of the database file following a crash.&lt;/p&gt;
    &lt;p&gt;In the TCL test harness, the crash simulation is done in a separate process. The main testing process spawns a child process which runs some SQLite operation and randomly crashes somewhere in the middle of a write operation. A special VFS randomly reorders and corrupts the unsynchronized write operations to simulate the effect of buffered filesystems. After the child dies, the original test process opens and reads the test database and verifies that the changes attempted by the child either completed successfully or else were completely rolled back. The integrity_check PRAGMA is used to make sure no database corruption occurs.&lt;/p&gt;
    &lt;p&gt;The TH3 test harness needs to run on embedded systems that do not necessarily have the ability to spawn child processes, so it uses an in-memory VFS to simulate crashes. The in-memory VFS can be rigged to make a snapshot of the entire filesystem after a set number of I/O operations. Crash tests run in a loop. On each iteration of the loop, the point at which a snapshot is made is advanced until the SQLite operations being tested run to completion without ever hitting a snapshot. Within the loop, after the SQLite operation under test has completed, the filesystem is reverted to the snapshot and random file damage is introduced that is characteristic of the kinds of damage one expects to see following a power loss. Then the database is opened and checks are made to ensure that it is well-formed and that the transaction either ran to completion or was completely rolled back. The interior of the loop is repeated multiple times for each snapshot with different random damage each time.&lt;/p&gt;
    &lt;p&gt;The test suites for SQLite also explore the result of stacking multiple failures. For example, tests are run to ensure correct behavior when an I/O error or OOM fault occurs while trying to recover from a prior crash.&lt;/p&gt;
    &lt;p&gt;Fuzz testing seeks to establish that SQLite responds correctly to invalid, out-of-range, or malformed inputs.&lt;/p&gt;
    &lt;p&gt;SQL fuzz testing consists of creating syntactically correct yet wildly nonsensical SQL statements and feeding them to SQLite to see what it will do with them. Usually some kind of error is returned (such as "no such table"). Sometimes, purely by chance, the SQL statement also happens to be semantically correct. In that case, the resulting prepared statement is run to make sure it gives a reasonable result.&lt;/p&gt;
    &lt;p&gt;The concept of fuzz testing has been around for decades, but fuzz testing was not an effective way to find bugs until 2014 when Michal Zalewski invented the first practical profile-guided fuzzer, American Fuzzy Lop or "AFL". Unlike prior fuzzers that blindly generate random inputs, AFL instruments the program being tested (by modifying the assembly-language output from the C compiler) and uses that instrumentation to detect when an input causes the program to do something different - to follow a new control path or loop a different number of times. Inputs that provoke new behavior are retained and further mutated. In this way, AFL is able to "discover" new behaviors of the program under test, including behaviors that were never envisioned by the designers.&lt;/p&gt;
    &lt;p&gt;AFL proved adept at finding arcane bugs in SQLite. Most of the findings have been assert() statements where the conditional was false under obscure circumstances. But AFL has also found a fair number of crash bugs in SQLite, and even a few cases where SQLite computed incorrect results.&lt;/p&gt;
    &lt;p&gt;Because of its past success, AFL became a standard part of the testing strategy for SQLite beginning with version 3.8.10 (2015-05-07) until it was superseded by better fuzzers in version 3.29.0 (2019-07-10).&lt;/p&gt;
    &lt;p&gt;Beginning in 2016, a team of engineers at Google started the OSS Fuzz project. OSS Fuzz uses a AFL-style guided fuzzer running on Google's infrastructure. The Fuzzer automatically downloads the latest check-ins for participating projects, fuzzes them, and sends email to the developers reporting any problems. When a fix is checked in, the fuzzer automatically detects this and emails a confirmation to the developers.&lt;/p&gt;
    &lt;p&gt;SQLite is one of many open-source projects that OSS Fuzz tests. The test/ossfuzz.c source file in the SQLite repository is SQLite's interface to OSS fuzz.&lt;/p&gt;
    &lt;p&gt;OSS Fuzz no longer finds historical bugs in SQLite. But it is still running and does occasionally find issues in new development check-ins. Examples: [1] [2] [3].&lt;/p&gt;
    &lt;p&gt;Beginning in late 2018, SQLite has been fuzzed using a proprietary fuzzer called "dbsqlfuzz". Dbsqlfuzz is built using the libFuzzer framework of LLVM.&lt;/p&gt;
    &lt;p&gt;The dbsqlfuzz fuzzer mutates both the SQL input and the database file at the same time. Dbsqlfuzz uses a custom Structure-Aware Mutator on a specialized input file that defines both an input database and SQL text to be run against that database. Because it mutates both the input database and the input SQL at the same time, dbsqlfuzz has been able to find some obscure faults in SQLite that were missed by prior fuzzers that mutated only SQL inputs or only the database file. The SQLite developers keep dbsqlfuzz running against trunk in about 16 cores at all times. Each instance of dbsqlfuzz program is able to evalutes about 400 test cases per second, meaning that about 500 million cases are checked every day.&lt;/p&gt;
    &lt;p&gt;The dbsqlfuzz fuzzer has been very successful at hardening the SQLite code base against malicious attack. Since dbsqlfuzz has been added to the SQLite internal test suite, bug reports from external fuzzers such as OSSFuzz have all but stopped.&lt;/p&gt;
    &lt;p&gt;Note that dbsqlfuzz is not the Protobuf-based structure-aware fuzzer for SQLite that is used by Chromium and described in the Structure-Aware Mutator article. There is no connection between these two fuzzers, other than the fact that they are both based on libFuzzer The Protobuf fuzzer for SQLite is written and maintained by the Chromium team at Google, whereas dbsqlfuzz is written and maintained by the original SQLite developers. Having multiple independently-developed fuzzers for SQLite is good, as it means that obscure issues are more likely to be uncovered.&lt;/p&gt;
    &lt;p&gt;Near the end of January 2024, a second libFuzzer-based tool called "jfuzz" came into use. Jfuzz generates corrupt JSONB blobs and feeds them into the JSON SQL functions to verify that the JSON functions are able to safely and efficiently deal with corrupt binary inputs.&lt;/p&gt;
    &lt;p&gt;SQLite seems to be a popular target for third-parties to fuzz. The developers hear about many attempts to fuzz SQLite and they do occasionally get bug reports found by independent fuzzers. All such reports are promptly fixed, so the product is improved and that the entire SQLite user community benefits. This mechanism of having many independent testers is similar to Linus's law: "given enough eyeballs, all bugs are shallow".&lt;/p&gt;
    &lt;p&gt;One fuzzing researcher of particular note is Manuel Rigger. Most fuzzers only look for assertion faults, crashes, undefined behavior (UB), or other easily detected anomalies. Dr. Rigger's fuzzers, on the other hand, are able to find cases where SQLite computes an incorrect answer. Rigger has found many such cases. Most of these finds are obscure corner cases involving type conversions and affinity transformations, and a good number of the finds are against unreleased features. Nevertheless, his finds are still important as they are real bugs, and the SQLite developers are grateful to be able to identify and fix the underlying problems.&lt;/p&gt;
    &lt;p&gt;Historical test cases from AFL, OSS Fuzz, and dbsqlfuzz are collected in a set of database files in the main SQLite source tree and then rerun by the "fuzzcheck" utility program whenever one runs "make test". Fuzzcheck only runs a few thousand "interesting" cases out of the billions of cases that the various fuzzers have examined over the years. "Interesting" cases are cases that exhibit previously unseen behavior. Actual bugs found by fuzzers are always included among the interesting test cases, but most of the cases run by fuzzcheck were never actual bugs.&lt;/p&gt;
    &lt;p&gt;Fuzz testing and 100% MC/DC testing are in tension with one another. That is to say, code tested to 100% MC/DC will tend to be more vulnerable to problems found by fuzzing and code that performs well during fuzz testing will tend to have (much) less than 100% MC/DC. This is because MC/DC testing discourages defensive code with unreachable branches, but without defensive code, a fuzzer is more likely to find a path that causes problems. MC/DC testing seems to work well for building code that is robust during normal use, whereas fuzz testing is good for building code that is robust against malicious attack.&lt;/p&gt;
    &lt;p&gt;Of course, users would prefer code that is both robust in normal use and resistant to malicious attack. The SQLite developers are dedicated to providing that. The purpose of this section is merely to point out that doing both at the same time is difficult.&lt;/p&gt;
    &lt;p&gt;For much of its history SQLite has been focused on 100% MC/DC testing. Resistance to fuzzing attacks only became a concern with the introduction of AFL in 2014. For a while there, fuzzers were finding many problems in SQLite. In more recent years, the testing strategy of SQLite has evolved to place more emphasis on fuzz testing. We still maintain 100% MC/DC of the core SQLite code, but most testing CPU cycles are now devoted to fuzzing.&lt;/p&gt;
    &lt;p&gt;While fuzz testing and 100% MC/DC testing are in tension, they are not completely at cross-purposes. The fact that the SQlite test suite does test to 100% MC/DC means that when fuzzers do find problems, those problems can be fixed quickly and with little risk of introducing new errors.&lt;/p&gt;
    &lt;p&gt;There are numerous test cases that verify that SQLite is able to deal with malformed database files. These tests first build a well-formed database file, then add corruption by changing one or more bytes in the file by some means other than SQLite. Then SQLite is used to read the database. In some cases, the bytes changes are in the middle of data. This causes the content of the database to change while keeping the database well-formed. In other cases, unused bytes of the file are modified, which has no effect on the integrity of the database. The interesting cases are when bytes of the file that define database structure get changed. The malformed database tests verify that SQLite finds the file format errors and reports them using the SQLITE_CORRUPT return code without overflowing buffers, dereferencing NULL pointers, or performing other unwholesome actions.&lt;/p&gt;
    &lt;p&gt;The dbsqlfuzz fuzzer also does an excellent job of verifying that SQLite responds sanely to malformed database files.&lt;/p&gt;
    &lt;p&gt;SQLite defines certain limits on its operation, such as the maximum number of columns in a table, the maximum length of an SQL statement, or the maximum value of an integer. The TCL and TH3 test suites both contains numerous tests that push SQLite right to the edge of its defined limits and verify that it performs correctly for all allowed values. Additional tests go beyond the defined limits and verify that SQLite correctly returns errors. The source code contains testcase macros to verify that both sides of each boundary have been tested.&lt;/p&gt;
    &lt;p&gt;Whenever a bug is reported against SQLite, that bug is not considered fixed until new test cases that would exhibit the bug have been added to either the TCL or TH3 test suites. Over the years, this has resulted in thousands and thousands of new tests. These regression tests ensure that bugs that have been fixed in the past are not reintroduced into future versions of SQLite.&lt;/p&gt;
    &lt;p&gt;Resource leak occurs when system resources are allocated and never freed. The most troublesome resource leaks in many applications are memory leaks - when memory is allocated using malloc() but never released using free(). But other kinds of resources can also be leaked: file descriptors, threads, mutexes, etc.&lt;/p&gt;
    &lt;p&gt;Both the TCL and TH3 test harnesses automatically track system resources and report resource leaks on every test run. No special configuration or setup is required. The test harnesses are especially vigilant with regard to memory leaks. If a change causes a memory leak, the test harnesses will recognize this quickly. SQLite is designed to never leak memory, even after an exception such as an OOM error or disk I/O error. The test harnesses are zealous to enforce this.&lt;/p&gt;
    &lt;p&gt;The SQLite core, including the unix VFS, has 100% branch test coverage under TH3 in its default configuration as measured by gcov. Extensions such as FTS3 and RTree are excluded from this analysis.&lt;/p&gt;
    &lt;p&gt;There are many ways to measure test coverage. The most popular metric is "statement coverage". When you hear someone say that their program as "XX% test coverage" without further explanation, they usually mean statement coverage. Statement coverage measures what percentage of lines of code are executed at least once by the test suite.&lt;/p&gt;
    &lt;p&gt;Branch coverage is more rigorous than statement coverage. Branch coverage measures the number of machine-code branch instructions that are evaluated at least once on both directions.&lt;/p&gt;
    &lt;p&gt;To illustrate the difference between statement coverage and branch coverage, consider the following hypothetical line of C code:&lt;/p&gt;
    &lt;quote&gt;if( a&amp;gt;b &amp;amp;&amp;amp; c!=25 ){ d++; }&lt;/quote&gt;
    &lt;p&gt;Such a line of C code might generate a dozen separate machine code instructions. If any one of those instructions is ever evaluated, then we say that the statement has been tested. So, for example, it might be the case that the conditional expression is always false and the "d" variable is never incremented. Even so, statement coverage counts this line of code as having been tested.&lt;/p&gt;
    &lt;p&gt;Branch coverage is more strict. With branch coverage, each test and each subblock within the statement is considered separately. In order to achieve 100% branch coverage in the example above, there must be at least three test cases:&lt;/p&gt;
    &lt;p&gt;Any one of the above test cases would provide 100% statement coverage but all three are required for 100% branch coverage. Generally speaking, 100% branch coverage implies 100% statement coverage, but the converse is not true. To reemphasize, the TH3 test harness for SQLite provides the stronger form of test coverage - 100% branch test coverage.&lt;/p&gt;
    &lt;p&gt;A well-written C program will typically contain some defensive conditionals which in practice are always true or always false. This leads to a programming dilemma: Does one remove defensive code in order to obtain 100% branch coverage?&lt;/p&gt;
    &lt;p&gt;In SQLite, the answer to the previous question is "no". For testing purposes, the SQLite source code defines macros called ALWAYS() and NEVER(). The ALWAYS() macro surrounds conditions which are expected to always evaluate as true and NEVER() surrounds conditions that are always evaluated to false. These macros serve as comments to indicate that the conditions are defensive code. In release builds, these macros are pass-throughs:&lt;/p&gt;
    &lt;quote&gt;#define ALWAYS(X) (X) #define NEVER(X) (X)&lt;/quote&gt;
    &lt;p&gt;During most testing, however, these macros will throw an assertion fault if their argument does not have the expected truth value. This alerts the developers quickly to incorrect design assumptions.&lt;/p&gt;
    &lt;quote&gt;#define ALWAYS(X) ((X)?1:assert(0),0) #define NEVER(X) ((X)?assert(0),1:0)&lt;/quote&gt;
    &lt;p&gt;When measuring test coverage, these macros are defined to be constant truth values so that they do not generate assembly language branch instructions, and hence do not come into play when calculating the branch coverage:&lt;/p&gt;
    &lt;quote&gt;#define ALWAYS(X) (1) #define NEVER(X) (0)&lt;/quote&gt;
    &lt;p&gt;The test suite is designed to be run three times, once for each of the ALWAYS() and NEVER() definitions shown above. All three test runs should yield exactly the same result. There is a run-time test using the sqlite3_test_control(SQLITE_TESTCTRL_ALWAYS, ...) interface that can be used to verify that the macros are correctly set to the first form (the pass-through form) for deployment.&lt;/p&gt;
    &lt;p&gt;Another macro used in conjunction with test coverage measurement is the testcase() macro. The argument is a condition for which we want test cases that evaluate to both true and false. In non-coverage builds (that is to say, in release builds) the testcase() macro is a no-op:&lt;/p&gt;
    &lt;quote&gt;#define testcase(X)&lt;/quote&gt;
    &lt;p&gt;But in a coverage measuring build, the testcase() macro generates code that evaluates the conditional expression in its argument. Then during analysis, a check is made to ensure tests exist that evaluate the conditional to both true and false. Testcase() macros are used, for example, to help verify that boundary values are tested. For example:&lt;/p&gt;
    &lt;quote&gt;testcase( a==b ); testcase( a==b+1 ); if( a&amp;gt;b &amp;amp;&amp;amp; c!=25 ){ d++; }&lt;/quote&gt;
    &lt;p&gt;Testcase macros are also used when two or more cases of a switch statement go to the same block of code, to make sure that the code was reached for all cases:&lt;/p&gt;
    &lt;quote&gt;switch( op ){ case OP_Add: case OP_Subtract: { testcase( op==OP_Add ); testcase( op==OP_Subtract ); /* ... */ break; } /* ... */ }&lt;/quote&gt;
    &lt;p&gt;For bitmask tests, testcase() macros are used to verify that every bit of the bitmask affects the outcome. For example, in the following block of code, the condition is true if the mask contains either of two bits indicating either a MAIN_DB or a TEMP_DB is being opened. The testcase() macros that precede the if statement verify that both cases are tested:&lt;/p&gt;
    &lt;quote&gt;testcase( mask &amp;amp; SQLITE_OPEN_MAIN_DB ); testcase( mask &amp;amp; SQLITE_OPEN_TEMP_DB ); if( (mask &amp;amp; (SQLITE_OPEN_MAIN_DB|SQLITE_OPEN_TEMP_DB))!=0 ){ ... }&lt;/quote&gt;
    &lt;p&gt;The SQLite source code contains 1184 uses of the testcase() macro.&lt;/p&gt;
    &lt;p&gt;Two methods of measuring test coverage were described above: "statement" and "branch" coverage. There are many other test coverage metrics besides these two. Another popular metric is "Modified Condition/Decision Coverage" or MC/DC. Wikipedia defines MC/DC as follows:&lt;/p&gt;
    &lt;p&gt;In the C programming language where &amp;amp;&amp;amp; and || are "short-circuit" operators, MC/DC and branch coverage are very nearly the same thing. The primary difference is in boolean vector tests. One can test for any of several bits in bit-vector and still obtain 100% branch test coverage even though the second element of MC/DC - the requirement that each condition in a decision take on every possible outcome - might not be satisfied.&lt;/p&gt;
    &lt;p&gt;SQLite uses testcase() macros as described in the previous subsection to make sure that every condition in a bit-vector decision takes on every possible outcome. In this way, SQLite also achieves 100% MC/DC in addition to 100% branch coverage.&lt;/p&gt;
    &lt;p&gt;Branch coverage in SQLite is currently measured using gcov with the "-b" option. First the test program is compiled using options "-g -fprofile-arcs -ftest-coverage" and then the test program is run. Then "gcov -b" is run to generate a coverage report. The coverage report is verbose and inconvenient to read, so the gcov-generated report is processed using some simple scripts to put it into a more human-friendly format. This entire process is automated using scripts, of course.&lt;/p&gt;
    &lt;p&gt;Note that running SQLite with gcov is not a test of SQLite — it is a test of the test suite. The gcov run does not test SQLite because the -fprofile-args and -ftest-coverage options cause the compiler to generate different code. The gcov run merely verifies that the test suite provides 100% branch test coverage. The gcov run is a test of the test - a meta-test.&lt;/p&gt;
    &lt;p&gt;After gcov has been run to verify 100% branch test coverage, then the test program is recompiled using delivery compiler options (without the special -fprofile-arcs and -ftest-coverage options) and the test program is rerun. This second run is the actual test of SQLite.&lt;/p&gt;
    &lt;p&gt;It is important to verify that the gcov test run and the second real test run both give the same output. Any differences in output indicate either the use of undefined or indeterminate behavior in the SQLite code (and hence a bug), or a bug in the compiler. Note that SQLite has, over the previous decade, encountered bugs in each of GCC, Clang, and MSVC. Compiler bugs, while rare, do happen, which is why it is so important to test the code in an as-delivered configuration.&lt;/p&gt;
    &lt;p&gt;Using gcov (or similar) to show that every branch instruction is taken at least once in both directions is a good measure of test suite quality. But even better is showing that every branch instruction makes a difference in the output. In other words, we want to show not only that every branch instruction both jumps and falls through but also that every branch is doing useful work and that the test suite is able to detect and verify that work. When a branch is found that does not make a difference in the output, that suggests that code associated with the branch can be removed (reducing the size of the library and perhaps making it run faster) or that the test suite is inadequately testing the feature that the branch implements.&lt;/p&gt;
    &lt;p&gt;SQLite strives to verify that every branch instruction makes a difference using mutation testing. A script first compiles the SQLite source code into assembly language (using, for example, the -S option to gcc). Then the script steps through the generated assembly language and, one by one, changes each branch instruction into either an unconditional jump or a no-op, compiles the result, and verifies that the test suite catches the mutation.&lt;/p&gt;
    &lt;p&gt;Unfortunately, SQLite contains many branch instructions that help the code run faster without changing the output. Such branches generate false-positives during mutation testing. As an example, consider the following hash function used to accelerate table-name lookup:&lt;/p&gt;
    &lt;quote&gt;55 static unsigned int strHash(const char *z){ 56 unsigned int h = 0; 57 unsigned char c; 58 while( (c = (unsigned char)*z++)!=0 ){ /*OPTIMIZATION-IF-TRUE*/ 59 h = (h&amp;lt;&amp;lt;3) ^ h ^ sqlite3UpperToLower[c]; 60 } 61 return h; 62 }&lt;/quote&gt;
    &lt;p&gt;If the branch instruction that implements the "c!=0" test on line 58 is changed into a no-op, then the while-loop will loop forever and the test suite will fail with a time-out. But if that branch is changed into an unconditional jump, then the hash function will always return 0. The problem is that 0 is a valid hash. A hash function that always returns 0 still works in the sense that SQLite still always gets the correct answer. The table-name hash table degenerates into a linked-list and so the table-name lookups that occur while parsing SQL statements might be a little slower, but the end result will be the same.&lt;/p&gt;
    &lt;p&gt; To work around this problem, comments of the form "&lt;code&gt;/*OPTIMIZATION-IF-TRUE*/&lt;/code&gt;" and
"&lt;code&gt;/*OPTIMIZATION-IF-FALSE*/&lt;/code&gt;" are inserted into the SQLite
source code to tell the mutation testing script to ignore some branch
instructions.



&lt;/p&gt;
    &lt;p&gt;The developers of SQLite have found that full coverage testing is an extremely effective method for locating and preventing bugs. Because every single branch instruction in SQLite core code is covered by test cases, the developers can be confident that changes made in one part of the code do not have unintended consequences in other parts of the code. The many new features and performance improvements that have been added to SQLite in recent years would not have been possible without the availability of full-coverage testing.&lt;/p&gt;
    &lt;p&gt;Maintaining 100% MC/DC is laborious and time-consuming. The level of effort needed to maintain full-coverage testing is probably not cost effective for a typical application. However, we think that full-coverage testing is justified for a very widely deployed infrastructure library like SQLite, and especially for a database library which by its very nature "remembers" past mistakes.&lt;/p&gt;
    &lt;p&gt;Dynamic analysis refers to internal and external checks on the SQLite code which are performed while the code is live and running. Dynamic analysis has proven to be a great help in maintaining the quality of SQLite.&lt;/p&gt;
    &lt;p&gt;The SQLite core contains 6754 assert() statements that verify function preconditions and postconditions and loop invariants. Assert() is a macro which is a standard part of ANSI-C. The argument is a boolean value that is assumed to always be true. If the assertion is false, the program prints an error message and halts.&lt;/p&gt;
    &lt;p&gt;Assert() macros are disabled by compiling with the NDEBUG macro defined. In most systems, asserts are enabled by default. But in SQLite, the asserts are so numerous and are in such performance critical places, that the database engine runs about three times slower when asserts are enabled. Hence, the default (production) build of SQLite disables asserts. Assert statements are only enabled when SQLite is compiled with the SQLITE_DEBUG preprocessor macro defined.&lt;/p&gt;
    &lt;p&gt;See the Use Of assert in SQLite document for additional information about how SQLite uses assert().&lt;/p&gt;
    &lt;p&gt;Valgrind is perhaps the most amazing and useful developer tool in the world. Valgrind is a simulator - it simulates an x86 running a Linux binary. (Ports of Valgrind for platforms other than Linux are in development, but as of this writing, Valgrind only works reliably on Linux, which in the opinion of the SQLite developers means that Linux should be the preferred platform for all software development.) As Valgrind runs a Linux binary, it looks for all kinds of interesting errors such as array overruns, reading from uninitialized memory, stack overflows, memory leaks, and so forth. Valgrind finds problems that can easily slip through all of the other tests run against SQLite. And, when Valgrind does find an error, it can dump the developer directly into a symbolic debugger at the exact point where the error occurs, to facilitate a quick fix.&lt;/p&gt;
    &lt;p&gt;Because it is a simulator, running a binary in Valgrind is slower than running it on native hardware. (To a first approximation, an application running in Valgrind on a workstation will perform about the same as it would running natively on a smartphone.) So it is impractical to run the full SQLite test suite through Valgrind. However, the veryquick tests and the coverage of the TH3 tests are run through Valgrind prior to every release.&lt;/p&gt;
    &lt;p&gt;SQLite contains a pluggable memory allocation subsystem. The default implementation uses system malloc() and free(). However, if SQLite is compiled with SQLITE_MEMDEBUG, an alternative memory allocation wrapper (memsys2) is inserted that looks for memory allocation errors at run-time. The memsys2 wrapper checks for memory leaks, of course, but also looks for buffer overruns, uses of uninitialized memory, and attempts to use memory after it has been freed. These same checks are also done by valgrind (and, indeed, Valgrind does them better) but memsys2 has the advantage of being much faster than Valgrind, which means the checks can be done more often and for longer tests.&lt;/p&gt;
    &lt;p&gt;SQLite contains a pluggable mutex subsystem. Depending on compile-time options, the default mutex system contains interfaces sqlite3_mutex_held() and sqlite3_mutex_notheld() that detect whether or not a particular mutex is held by the calling thread. These two interfaces are used extensively within assert() statements in SQLite to verify mutexes are held and released at all the right moments, in order to double-check that SQLite does work correctly in multi-threaded applications.&lt;/p&gt;
    &lt;p&gt;One of the things that SQLite does to ensure that transactions are atomic across system crashes and power failures is to write all changes into the rollback journal file prior to changing the database. The TCL test harness contains an alternative OS backend implementation that helps to verify this is occurring correctly. The "journal-test VFS" monitors all disk I/O traffic between the database file and rollback journal, checking to make sure that nothing is written into the database file which has not first been written and synced to the rollback journal. If any discrepancies are found, an assertion fault is raised.&lt;/p&gt;
    &lt;p&gt;The journal tests are an additional double-check over and above the crash tests to make sure that SQLite transactions will be atomic across system crashes and power failures.&lt;/p&gt;
    &lt;p&gt;In the C programming language, it is very easy to write code that has "undefined" or "implementation defined" behavior. That means that the code might work during development, but then give a different answer on a different system, or when recompiled using different compiler options. Examples of undefined and implementation-defined behavior in ANSI C include:&lt;/p&gt;
    &lt;p&gt;Since undefined and implementation-defined behavior is non-portable and can easily lead to incorrect answers, SQLite works very hard to avoid it. For example, when adding two integer column values together as part of an SQL statement, SQLite does not simply add them together using the C-language "+" operator. Instead, it first checks to make sure the addition will not overflow, and if it will, it does the addition using floating point instead.&lt;/p&gt;
    &lt;p&gt;To help ensure that SQLite does not make use of undefined or implementation defined behavior, the test suites are rerun using instrumented builds that try to detect undefined behavior. For example, test suites are run using the "-ftrapv" option of GCC. And they are run again using the "-fsanitize=undefined" option on Clang. And again using the "/RTC1" option in MSVC. Then the test suites are rerun using options like "-funsigned-char" and "-fsigned-char" to make sure that implementation differences do not matter either. Tests are then repeated on 32-bit and 64-bit systems and on big-endian and little-endian systems, using a variety of CPU architectures. Furthermore, the test suites are augmented with many test cases that are deliberately designed to provoke undefined behavior. For example: "SELECT -1*(-9223372036854775808);".&lt;/p&gt;
    &lt;p&gt;The sqlite3_test_control(SQLITE_TESTCTRL_OPTIMIZATIONS, ...) interface allows selected SQL statement optimizations to be disabled at run-time. SQLite should always generate exactly the same answer with optimizations enabled and with optimizations disabled; the answer simply arrives quicker with the optimizations turned on. So in a production environment, one always leaves the optimizations turned on (the default setting).&lt;/p&gt;
    &lt;p&gt;One verification technique used on SQLite is to run an entire test suite twice, once with optimizations left on and a second time with optimizations turned off, and verify that the same output is obtained both times. This shows that the optimizations do not introduce errors.&lt;/p&gt;
    &lt;p&gt;Not all test cases can be handled this way. Some test cases check to verify that the optimizations really are reducing the amount of computation by counting the number of disk accesses, sort operations, full-scan steps, or other processing steps that occur during queries. Those test cases will appear to fail when optimizations are disabled. But the majority of test cases simply check that the correct answer was obtained, and all of those cases can be run successfully with and without the optimizations, in order to show that the optimizations do not cause malfunctions.&lt;/p&gt;
    &lt;p&gt;The SQLite developers use an on-line checklist to coordinate testing activity and to verify that all tests pass prior to each SQLite release. Past checklists are retained for historical reference. (The checklists are read-only for anonymous internet viewers, but developers can log in and update checklist items in their web browsers.) The use of checklists for SQLite testing and other development activities is inspired by The Checklist Manifesto .&lt;/p&gt;
    &lt;p&gt;The latest checklists contain approximately 200 items that are individually verified for each release. Some checklist items only take a few seconds to verify and mark off. Others involve test suites that run for many hours.&lt;/p&gt;
    &lt;p&gt;The release checklist is not automated: developers run each item on the checklist manually. We find that it is important to keep a human in the loop. Sometimes problems are found while running a checklist item even though the test itself passed. It is important to have a human reviewing the test output at the highest level, and constantly asking "Is this really right?"&lt;/p&gt;
    &lt;p&gt;The release checklist is continuously evolving. As new problems or potential problems are discovered, new checklist items are added to make sure those problems do not appear in subsequent releases. The release checklist has proven to be an invaluable tool in helping to ensure that nothing is overlooked during the release process.&lt;/p&gt;
    &lt;p&gt;Static analysis means analyzing source code at compile-time to check for correctness. Static analysis includes compiler warning messages and more in-depth analysis engines such as the Clang Static Analyzer. SQLite compiles without warnings on GCC and Clang using the -Wall and -Wextra flags on Linux and Mac and on MSVC on Windows. No valid warnings are generated by the Clang Static Analyzer tool "scan-build" either (though recent versions of clang seem to generate many false-positives.) Nevertheless, some warnings might be generated by other static analyzers. Users are encouraged not to stress over these warnings and to instead take solace in the intense testing of SQLite described above.&lt;/p&gt;
    &lt;p&gt;Static analysis has not been helpful in finding bugs in SQLite. Static analysis has found a few bugs in SQLite, but those are the exceptions. More bugs have been introduced into SQLite while trying to get it to compile without warnings than have been found by static analysis.&lt;/p&gt;
    &lt;p&gt;SQLite is open source. This gives many people the idea that it is not well tested as commercial software and is perhaps unreliable. But that impression is false. SQLite has exhibited very high reliability in the field and a very low defect rate, especially considering how rapidly it is evolving. The quality of SQLite is achieved in part by careful code design and implementation. But extensive testing also plays a vital role in maintaining and improving the quality of SQLite. This document has summarized the testing procedures that every release of SQLite undergoes with the hope of inspiring confidence that SQLite is suitable for use in mission-critical applications.&lt;/p&gt;
    &lt;p&gt;This page was last updated on 2025-05-31 13:08:22Z&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sqlite.org/testing.html"/><published>2025-12-17T18:15:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46305428</id><title>OBS Studio Gets a New Renderer</title><updated>2025-12-18T10:45:21.911255+00:00</updated><content>&lt;doc fingerprint="9759de1b6431109d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;OBS Studio Gets A New Renderer: How OBS Adopted Metal&lt;/head&gt;
    &lt;p&gt;Starting with OBS Studio 32.0.0 a new renderer backend based on Apple's Metal graphics API is available for users to test as an experimental alternative to the existing OpenGL backend on macOS. This marks an important step in OBS Studio's development to adapt one of the modern APIs that deliberately broke with the past to unlock better performance and efficiency gains for end users, but also require fundamental changes to how an application interacts with a GPU.&lt;/p&gt;
    &lt;p&gt;These fundamental changes were necessary to achieve the goals of lower overhead, faster performance, and to better represent the way that modern GPUs actually work, particularly when the GPU is used for more than just 3D rendering. Yet other changes were also necessary due to the way Metal was specifically designed to fit into Apple's operating systems.&lt;/p&gt;
    &lt;p&gt;Due to the sheer amount of information around this topic, it made sense to split it into separate posts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The first post (this one) will go into specific challenges and insights from writing the Metal backend for OBS Studio.&lt;/item&gt;
      &lt;item&gt;The second post will look at the history of 3D graphics APIs, their core differences, and how the design of the new generation creates challenges for existing renderers like the one in OBS Studio.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Metal backend is explicitly marked as "Experimental" in the application because it has some known quirks (for which no good solutions have been found yet, more about that below) but also because it has not yet been tested by a larger user audience. The OpenGL renderer is still the default choice offered to users and will be available for the foreseeable future, but we still would like to invite users to try out the Metal backend for themselves, and report any critical bugs they might encounter.&lt;/p&gt;
    &lt;p&gt;Better yet, if you happen to have prior experience working with Metal on Apple Silicon platforms (including iPhones), we'd be happy to hear feedback about specific aspects of the current implementation or even review pull-requests with improvements to it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1.1: The Why Of Metal&lt;/head&gt;
    &lt;p&gt;In June 2014 Apple announced (and - more importantly - also released to developers) the first version of Metal for iPhones with Apple's A7 SOC, extending its support to current Macs of the time in 2015. Thus Metal was not only available on what became later known as "Apple Silicon" GPUs, but also Intel, AMD, and NVIDIA GPUs, being the first "next generation" graphics API to support all mainstream GPUs of the time.&lt;/p&gt;
    &lt;p&gt;Metal combined many benefits to Apple specifically: The API was based on concepts and ideas that not only already found their way into AMD's "Mantle" API (which was announced in September 2013) but had also been discussed for and adopted to some degree in the existing graphics APIs (OpenGL and Direct3D) at the time. But as a new API written from scratch it had the chance to omit all the legacy aspects of existing APIs and fully lean into these new concepts. It was able to provide the performance gains unlocked by this different approach and as it was originally designed for iOS and macOS, an entirely new API implemented in Objective-C and Swift (the latter of which was also introduced in 2014).&lt;/p&gt;
    &lt;p&gt;While OpenGL did (and still does) provide a well-established C-based API, it incorporates a binding model that is considered inelegant by many. Direct3D to this day uses a C++-based object-oriented API design (primarily via the COM binary interface), making it easier for developers to keep track of state and objects in their application code. Notably existing APIs in COM-based systems are not allowed to change, instead new variants have to be introduced, providing a decent amount of backwards-compatibility for applications originally written for older versions of Direct3D.&lt;/p&gt;
    &lt;p&gt;Metal takes Direct3D's object-oriented approach one step further by combining it with the more "verbal" API design common in Objective-C and Swift in an attempt to provide a more intuitive and easier API for app developers to use (and not just game developers) and to further motivate those to integrate more 3D and general GPU functionality into their apps. This lower barrier of entry was very much the point of earlier Metal versions, which combined the comparatively easier API with extensive graphics debugging capabilities built right into Xcode, providing developers with in-depth insights into every detail of their GPU processing (including built-in debugging of shader code) in the same IDE used for the rest of their application development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1.2: Differences In API Design&lt;/head&gt;
    &lt;p&gt;All modern APIs share the same concepts and approaches in their designs, which attempt to solve a major design issue of OpenGL and Direct3D:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The old APIs took care of a lot of resource management and synchronization for the developer, "hiding" much of the actual complexity involved in preparing the GPU for workloads.&lt;/item&gt;
      &lt;item&gt;The old APIs presented the GPU as a big state machine (particularly OpenGL) where the developer can change bits and pieces between issuing draw calls.&lt;/item&gt;
      &lt;item&gt;Due to the possibility to change bits and pieces of a pipeline at will, the old APIs had to check if the current "state" of the API is valid before issuing any work to the GPU, adding considerable overhead to the API "driver".&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The new APIs removed most of that and now require developers to manage their resources and synchronization themselves, only providing methods to communicate their intent to the API and thus the GPU.&lt;/item&gt;
      &lt;item&gt;The new APIs present the GPU as a highly parallelized processing unit that can be issued a list of commands that will be added to a queue and are then picked up by the GPU, "drawing" just being one of many operations.&lt;/item&gt;
      &lt;item&gt;To avoid having to re-validate the pipeline state before each draw call, pipelines have become immutable objects, whose validity is checked once during their creation, removing the overhead from draw commands using the pipeline, leaving some overhead whenever the pipelines themselves are switched.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course there are many other differences (large and small) but those are the ones that had the most impact on the attempt to write a "next generation" graphics API backend for OBS Studio. The current renderer is obviously built around the way the old APIs work (particularly Direct3D) and thus makes certain assumptions about the state the APIs will be in at any given point in time. The issue is that those assumptions are not correct as soon as any of those new APIs are used and a lot of the work that the APIs used to do for OBS Studio now has to be taken care of by the application itself.&lt;/p&gt;
    &lt;p&gt;So a decision had to be made: Either the core renderer can be updated or even rewritten to take care of the responsibilities expected by the modern APIs, adapting more modern "indirect drawing" techniques in Direct3D and OpenGL to make them more compatible with that approach (making those renderers potentially more performant as well), or put this additional work entirely into the backend for one of the new APIs, leaving the core renderer as-is. At least for the Metal backend, the second path was chosen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1.3: The Expectations Of OBS Studio's Renderer&lt;/head&gt;
    &lt;p&gt;Before the new release, OBS Studio shipped with two graphics APIs: Direct3D on Windows and OpenGL on Linux and macOS. This is achieved by having a core rendering subsystem that is (for the most part) API-independent and requires an API-specific backend to be loaded at runtime. This backend then implements a generic set of render commands issued by the core renderer and translates those into the actual commands and operations required by its API. That said, the core renderer has some quirks that become apparent once one tries to add support for an API that works slightly different than it might expect:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;As OBS Studio's shader files are effectively written in an older dialect of Microsoft's High Level Shader Language (HLSL), every shader needs to be analyzed and translated into a valid variant for the modern API at runtime.&lt;/item&gt;
      &lt;item&gt;Shaders are expected to use the same data structure as input and output type, as well as support global variables.&lt;/item&gt;
      &lt;item&gt;The application expects all operations made by the graphics API to be strictly sequential, and to always execute operations involving the same resources strictly in order of submission to the API.&lt;/item&gt;
      &lt;item&gt;OBS Studio's texture operations (creating textures from data loaded by the application, updating textures, reading from textures, and more) are directly modelled after Direct3D's API design. Any other API needs to work around this expectation and do housekeeping "behind the scenes" to meet it.&lt;/item&gt;
      &lt;item&gt;OBS Studio also expects to be able to render previews (such as the program view, the main preview, the multi-view, and others) directly at its own pace (and framerate), expecting every platform to effectively behave like Windows and provide "swapchains" with the "discard model" used by DXGI, and thus is not decoupled from the render loop for the video output.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most of these issues either fall into the realm of shaders or the API design itself, all of which had to be overcome by the Metal backend.&lt;/p&gt;
    &lt;head rend="h3"&gt;Part 1.3.1: Transpiling Shaders&lt;/head&gt;
    &lt;p&gt;OBS Studio makes extensive use of shader programs that run on the GPU to do efficient image processing. Both &lt;code&gt;libobs&lt;/code&gt; (OBS Studio's main library that includes the core renderer) as well as plugins like the first-party &lt;code&gt;obs-filters&lt;/code&gt; plugin provide their own "effect" files, which are implemented using the HLSL dialect mentioned above.&lt;/p&gt;
    &lt;p&gt;These effects files contain "techniques", each potentially made of up of a number of render passes (although all current OBS Studio effect files use a single pass) that provide a vertex and fragment shader pair. The vertex shader is the little program a GPU runs for every vertex (a "point" of a triangle) to calculate its actual position in a scene relative to a camera looking at it. The fragment shader (also called "pixel shader" in Direct3D) calculates the actual color value for each fragment (a visible pixel in the output image).&lt;/p&gt;
    &lt;p&gt;To make these files work with OpenGL and Direct3D, they need to be converted into bespoke shader source code for each "technique" first, which OBS Studio achieves through multiple steps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each effect file is parsed in its entirety and converted into a data structure representing each "part" of the effect file:&lt;list rend="ul"&gt;&lt;item&gt;The "uniforms", that is data (or a data structure) that is updated by application code at every rendered frame.&lt;/item&gt;&lt;item&gt;The "vertex" or "fragment" data (usually a data struct) that is kept in GPU memory.&lt;/item&gt;&lt;item&gt;Texture descriptions (textures can be 1-, 2-, or 3-dimensional) and sampler descriptions (samplers describe how a color value is read from a texture for use in a shader).&lt;/item&gt;&lt;item&gt;The shader functions, including their return type and argument types.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Additionally each technique (and pass(es) within) are parsed into a nested data structure:&lt;list rend="ul"&gt;&lt;item&gt;OBS Studio will then iterate over every technique and its passes to pick up the names of the vertex and fragment shader functions mentioned in each.&lt;/item&gt;&lt;item&gt;The uniforms, shader data, as well as the texture and sampler descriptions, are shared among each technique within the same file. The created data structures are used to re-create the (partial) HLSL source code that was parsed originally.&lt;/item&gt;&lt;item&gt;The shader functions and their content are then copied and a "main" function is generated (as the entry-point for the shader) calling the actual shader function. The generated final HLSL string is kept as the shader representation of each "technique".&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Each technique is sent in its HLSL form to each graphics API and is then transpiled into its API-specific form:&lt;list rend="ul"&gt;&lt;item&gt;For Direct3D this means replacing text tokens with their more current variants.&lt;/item&gt;&lt;item&gt;For GLSL this means parsing the HLSL string back into structured data again, before iterating over this data and composing a GLSL shader string from it. As shader function code is not analyzed, it needs to be parsed word-for-word and translated into GLSL-specific variants if necessary.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Adapting this process for Metal led to a few challenges, born out of the stricter shader language used by the API:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MSL is stricter around types and semantics:&lt;list rend="ul"&gt;&lt;item&gt;Direct3D uses "semantics" to mark parts of shader data structs and give them meaning, like &lt;code&gt;TEXCOORD0&lt;/code&gt;,&lt;code&gt;COLOR0&lt;/code&gt;, or&lt;code&gt;SV_VertexID&lt;/code&gt;, while OpenGL uses global variables that shader code instead reads from or writes into.&lt;/item&gt;&lt;item&gt;Metal uses a similar semantics-based model as Direct3D via attributes, but their use is more strict. Some attributes are allowed for input data, but not for output data.&lt;/item&gt;&lt;item&gt;Thus the same struct definition cannot be used as input and output type definition, instead the single struct type used by HLSL needs to be split into two separate structs for MSL.&lt;/item&gt;&lt;item&gt;Every function's content then needs to be scanned for any use of the struct type and needs to be replaced with the appropriate input or output variant.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Direct3D uses "semantics" to mark parts of shader data structs and give them meaning, like &lt;/item&gt;
      &lt;item&gt;MSL has no support for global variables:&lt;list rend="ul"&gt;&lt;item&gt;This means that "uniforms" as used by Direct3D (and set up by OBS Studio's renderer) cannot be used by Metal - uniform data needs to be provided as a buffer of data in GPU memory.&lt;/item&gt;&lt;item&gt;This buffer of data can be referenced as an input parameter to a shader function, thus any use of the global variable (used by HLSL and GLSL) needs to be replaced with a use of the function argument.&lt;/item&gt;&lt;item&gt;However any other function called by the "main" shader function also accessing a global variable needs this local variable passed explicitly to it, thus - once again - every function's code needs to be parsed and analyzed.&lt;/item&gt;&lt;item&gt;Any time a function uses a global, it needs to have a new function argument added to its signature to accept the "global" data as an explicit function parameter.&lt;/item&gt;&lt;item&gt;Any time a function calls a function that uses a global, it also needs to have its signature changed to accept the data explicitly and also change the call signature to pass the data along.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And these are just two examples of major differences in the shader language that require the transpiler to almost rewrite every effect file used by &lt;code&gt;libobs&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here's a trivial example, OBS Studio's most basic vertex shader that simply multiplies each vertex position with a matrix to calculate the actual coordinates in "clip space" (coordinates that describe the position as a percentage of the width and height of the camera's view):&lt;/p&gt;
    &lt;code&gt;uniform float4x4 ViewProj;

struct VertInOut {
    float4 pos : POSITION;
    float2 uv : TEXCOORD0;
};

VertInOut VSDefault(VertInOut vert_in)
{
    VertInOut vert_out;
    vert_out.pos = mul(float4(vert_in.pos.xyz, 1.0), ViewProj);
    vert_out.uv  = vert_in.uv;
    return vert_out;
}

VertInOut main(VertInOut vert_in)
{
    return VSDefault(vert_in);
}&lt;/code&gt;
    &lt;p&gt;In this shader the view projection matrix is provided as a global variable called &lt;code&gt;ViewProj&lt;/code&gt; and it's used by the &lt;code&gt;VSDefault&lt;/code&gt; shader function. The Metal Shader variant needs to be a bit more explicit about the flow of data:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;metal_stdlib&amp;gt;
using namespace metal;

typedef struct {
    float4x4 ViewProj;
} UniformData;

typedef struct {
    float4 pos [[attribute(0)]];
    float2 uv [[attribute(1)]];
} VertInOut_In;

typedef struct {
    float4 pos [[position]];
    float2 uv;
} VertInOut_Out;

VertInOut_Out VSDefault(VertInOut_In vert_in, constant UniformData &amp;amp;uniforms) {
    VertInOut_Out vert_out;
    vert_out.pos = (float4(vert_in.pos.xyz, 1.0)) * (uniforms.ViewProj);
    vert_out.uv  = vert_in.uv;
    return vert_out;
}

[[vertex]] VertInOut_Out _main(VertInOut_In vert_in [[stage_in]], constant UniformData &amp;amp;uniforms [[buffer(30)]]) {
    return VSDefault(vert_in, uniforms);
}&lt;/code&gt;
    &lt;p&gt;As explained above, the single &lt;code&gt;VertInOut&lt;/code&gt; struct had to be split into two separate variants for input and output, as the &lt;code&gt;attribute(n)&lt;/code&gt; mapping is only valid for input data. It uses a pattern more common in modern APIs where memory is typically organized in larger heaps into which all other data (buffers, textures, etc.) is placed and referenced. In this case the &lt;code&gt;stage_in&lt;/code&gt; decoration allows the developer to access vertex or fragment data for which a vertex descriptor had been set up and is used for convenience. Otherwise the variable would just represent a buffer of GPU memory.&lt;/p&gt;
    &lt;p&gt;To tell Metal which part of the output structure contains the calculated vertex positions, the corresponding field has to be decorated with &lt;code&gt;[[position]]&lt;/code&gt; or return a &lt;code&gt;float4&lt;/code&gt; explicitly. Every vertex shader has to do one or the other, as it would otherwise fail shader compilation.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;uniform&lt;/code&gt; global used by HLSL is transformed into a buffer variable: The uniform data is uploaded into the buffer in slot 30 and referenced by the &lt;code&gt;[[buffer(30)]]&lt;/code&gt; decorator and uses the ampersand (&lt;code&gt;&amp;amp;&lt;/code&gt;) to make it a C++ reference using the &lt;code&gt;constant&lt;/code&gt; address space attribute, which marks this reference to be read-only. The &lt;code&gt;uniforms&lt;/code&gt; reference is also passed into the &lt;code&gt;VSDefault&lt;/code&gt; function, as the transpiler detected that the function accesses &lt;code&gt;ViewProj&lt;/code&gt; within its function body, and thus adds it as an argument to the function signature and converts the reference of &lt;code&gt;ViewProj&lt;/code&gt; into the correct form &lt;code&gt;uniforms.ViewProj&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similar work has to be done for all those cases where GLSL or HLSL will opportunistically accept data with the "wrong" types and alias or convert them into the correct one. Metal does not allow this, the developer has to put any such conversion into code explicitly and also requires any function call to match the function signature. Here is one such example:&lt;/p&gt;
    &lt;code&gt;float PS_Y(FragPos frag_in)
{
    float3 rgb = image.Load(int3(frag_in.pos.xy, 0)).rgb;
    float y = dot(color_vec0.xyz, rgb) + color_vec0.w;
    return y;
}&lt;/code&gt;
    &lt;p&gt;In this example the HLSL shader uses the &lt;code&gt;Load&lt;/code&gt; function to load color values from a texture and passes in a vector of 3 signed integer values. A valid Metal Shader variant would look like this:&lt;/p&gt;
    &lt;code&gt;float PS_Y(FragPos frag_in, constant UniformData &amp;amp;uniforms, texture2d&amp;lt;float&amp;gt; image) {
    float3 rgb = image.read(uint2(int2(frag_in.pos.xy)), uint( int( 0))).rgb;
    float y = dot(uniforms.color_vec0.xyz, rgb) + uniforms.color_vec0.w;
    return y;
}&lt;/code&gt;
    &lt;p&gt;The signature of the corresponding &lt;code&gt;read&lt;/code&gt; function in MSL requires a vector of 2 unsigned integer values and a single unsigned integer. Thus the transpiler needs to detect any (known) invocation of a function that uses type aliasing or other kinds of type punning and explicitly convert the provided function arguments into the types required by the MSL shader function, in this case converting a single &lt;code&gt;int3&lt;/code&gt; into a pair of &lt;code&gt;uint2&lt;/code&gt; and &lt;code&gt;uint&lt;/code&gt; and ensuring that the data passed into the &lt;code&gt;uint&lt;/code&gt; constructor is actually of a specific type that can be converted.&lt;/p&gt;
    &lt;p&gt;These and many other changes are necessary because MSL is effectively C++14 code and thus requires the same adherence to type safety and function signatures as any other application code written in C++. This allows sharing header files of data type and structure definitions between application and shader code, but also requires shader code to be more correct than it had to be for HLSL or GLSL in the past. And in the case of OBS Studio, the transpiler has to partially "rewrite" the HLSL shader code into more compliant MSL code at runtime.&lt;/p&gt;
    &lt;head rend="h3"&gt;Part 1.3.2: Pretending To Be Direct3D&lt;/head&gt;
    &lt;p&gt;The next hurdle was to simulate the behavior of Direct3D inside the Metal API implementation. As it was deemed infeasible to rewrite the core renderer itself, it must be "kept in the dark" about what actually happens behind the scenes.&lt;/p&gt;
    &lt;p&gt;One of the main jobs OBS Studio has to do for every video frame is convert images (or "frames") provided by the CPU into textures on the GPU, and - depending on the video encoder used - convert the final output texture back into a frame in CPU memory that can be sent to the encoder. In Direct3D this involves the "mapping" and "unmapping" of a texture:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;When a texture is "mapped" it is made available for the application code in CPU memory.&lt;/item&gt;
      &lt;item&gt;When the texture is mapped for writing, Direct3D will provide a pointer to some CPU memory that it has either allocated directly or had been "lying around" from an earlier map operation. OBS Studio can then copy its frame data into the location.&lt;/item&gt;
      &lt;item&gt;When the texture is mapped for reading, Direct3D will provide a pointer to CPU memory that contains a copy of the texture data from GPU memory. OBS Studio can copy this data into its own "cache" of frames.&lt;/item&gt;
      &lt;item&gt;When OBS Studio is done with either operation, it has to "unmap" the texture. The provided pointer is then invalidated and any data pointed to it can and will be changed randomly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By itself a naïve implementation of this operation can have severe consequences: When a texture is mapped for writing, any pending GPU operation (e.g., a shader's sampler reading color data from it) might be blocked from being executed as the texture is still being written to. Likewise when a texture is mapped for reading, any pending CPU operation (e.g., copying the frame data into a cache for the video encoder) might have to wait for any GPU operation that currently uses that texture.&lt;/p&gt;
    &lt;p&gt;As part of its resource tracking behind the scenes, Direct3D 11 keeps track of texture access operations and will try to keep any such interruptions to a minimum, which leads specifically to the kind of overhead the modern APIs try to avoid:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;When a texture is mapped for writing, Direct3D will keep track of the mapping request and provide a pointer to get the data copied into, even while the texture is in use, and schedule a synchronization of the texture data.&lt;/item&gt;
      &lt;item&gt;Once the texture is unmapped, Direct3D will upload the data to GPU memory and once this has taken place, schedule an update of the texture with the new image data.&lt;/item&gt;
      &lt;item&gt;If any consecutive draw call that uses the same texture was issued by the application code after the unmapping, Direct3D will implicitly ensure that all pending GPU commands are scheduled and thus the texture update will take place before any consecutive GPU command might access the same texture.&lt;/item&gt;
      &lt;item&gt;When a texture is mapped for reading, Direct3D will also ensure that all pending GPU commands writing to that texture are executed first to ensure that the copy it will provide will represent the result of any draw commands issued by the application before that.&lt;/item&gt;
      &lt;item&gt;These are just examples of what might happen, as the specifics are highly dependent on the current state of the pipelines, the internal caches of the API's "driver" and other heuristics.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;OBS Studio's entire render pipeline is designed around the characteristics of the &lt;code&gt;map&lt;/code&gt; and &lt;code&gt;unmap&lt;/code&gt; commands in Direct3D 11 and expect any other graphics API to behave in a similar way. Metal (as other modern APIs) does not do all of this work (Metal 3 will indeed still do hazard tracking of resources, but developers can opt-out of this behavior, and Metal 4 removed it entirely) and thus the API-specific backend has to simulate the behavior of Direct3D 11 in its own implementation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;When a texture is mapped for writing, a GPU buffer is opportunistically set up to hold the image data.&lt;/item&gt;
      &lt;item&gt;As the Metal backend is only supported on Apple Silicon devices, GPU and CPU share the same memory. This means that a pointer to the buffer's memory can be directly shared with the renderer,&lt;/item&gt;
      &lt;item&gt;When the texture is "unmapped", a simple block transfer (or blit) operation is scheduled on the GPU to copy the contents of the GPU buffer into the GPU texture. The unmap operation will "wait" until the blit operation has been scheduled on the GPU to prohibit the renderer from issuing any new render commands which would potentially run in parallel.&lt;/item&gt;
      &lt;item&gt;When a texture is mapped for reading, the same pointer to the GPU buffer is shared with application code. As the buffers are never used by any render command directly, no hazard tracking is necessary. An "unmapping" thus does nothing.&lt;/item&gt;
      &lt;item&gt;"Staging" a texture for reading thus only requires scheduling a blit from the source texture into its associated staging buffer (as buffers and textures are effectively the same thing and differ only by their API). To ensure that no further render commands are issued by the application, the copy operation is made to wait until it is completed by the GPU, but also has to ensure that if the source texture has any pending render commands, that those are scheduled to be run on the GPU explicitly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The same applies to other operations closely following Direct3D's design: To ensure that the Metal backend reacts in a way that meets the way OBS Studio's renderer expects, it had the "hidden" functionality of Direct3D implemented explicitly, particularly the tracking of texture use by prior render commands before any staging takes place.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1.4: But Wait, There's More (Issues)&lt;/head&gt;
    &lt;p&gt;One major reason why the backend is considered "experimental" is due to the way its preview rendering had to be implemented for now. To understand the core reason for those issues, it is important to first understand how OBS Studio expects preview rendering to work, which closely shaped by the way DXGI (DirectX Graphics Infrastructure) allows applications to present 3D content on Windows. DXGI swapchains hold a number of buffers with image data created by the application, one of which the application can ask to be "presented" by the operating system.&lt;/p&gt;
    &lt;p&gt;To avoid an application potentially generating too many frames than can be presented (and thus potentially blocking the application), DXGI swapchains can be set to have a "discard" mode and a synchronization interval of &lt;code&gt;0&lt;/code&gt;, which effectively allows an application to force the presentation of a back buffer immediately (without waiting for the operating system's own screen refresh interval) and because the contents of the buffer are discarded, it becomes available to be overwritten by the application with the the next frame's data.&lt;/p&gt;
    &lt;p&gt;Unfortunately this is the opposite of how Metal (or more precisely Metal Layers) allows one to render 3D contents into application windows: With the introduction of ProMotion on iPhones and Macbooks, macOS controls the effective frame rates used by devices to provide fluid motion during interaction, but potentially throttle the desktop rendering to single digit framerates when no interaction or updates happen. This allows iOS and macOS to limit the operating system framerate (and the framerate of all applications within it) when the device is set to "low power" mode and is thus not designed to be "overruled" by an application (as it would allow such an application to violate the "low power" decision made by the user).&lt;/p&gt;
    &lt;p&gt;Thus applications cannot just render frames to be presented by the OS at their own pace, instead they have to ask for a surface to render to and the number of surfaces an application can be provided with is limited. This means that if OBS Studio is running at twice the frame rate of the operating system, it would exhaust its allowance of surfaces to render into. And because OBS Studio renders previews as part of its single render loop, any delay introduced by a preview render operation also stalls or delays the generation of new frames for video encoders.&lt;/p&gt;
    &lt;p&gt;The solution (at least for this first experimental release) was for OBS Studio to always render into a texture that "pretends" to be the window surface to allow the renderer to finish all its work in its required frame time. Then a different thread (running globally for the entire app with a fixed screen refresh interval) will pick up this texture, request a window surface to render into, and then schedule a block transfer (blit) to copy the contents into the surface. This also requires explicit synchronization of the textures' use on the GPU to ensure that the "pretender" texture is fully rendered to by OBS before the copy to the surface is executed.&lt;/p&gt;
    &lt;p&gt;In a nutshell: Whenever the operating system compositor requires a refresh, a separate thread will wait for OBS Studio to render a new frame of the associated preview texture and only once that has happened, the texture data is copied into the surface provided by the compositor. But as the preview rendering is now decoupled from the main render loop, framerate inconsistencies are inevitable. The kernel-level timer provided by the operating system will not align with OBS Studio's render loop (which requires the render thread to be woken up when it's calculated "time for the next frame" has been reached) and thus either a new frame might have been rendered in time or an old frame could not be copied in time.&lt;/p&gt;
    &lt;p&gt;Starting with macOS 14, the approach suggested by Apple would require OBS Studio to decouple these intervals even further, as every window (and thus potentially every preview) will have its own independent timer at which the application will receive a call from the operating system with a surface ready to be rendered into, which would bring a whole new set of potential challenges as it's even further removed from how OBS Studio expects to be able to render previews in the application.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1.5: The Hidden Costs Of Modern Graphics APIs&lt;/head&gt;
    &lt;p&gt;The complexity of the endeavor was high and took several months of research, trial and error, bugfixing, multiple redesigns of specific aspects, and many long weekends. During the course of development it also became clear why some applications that simply switched from OpenGL to Vulkan or Direct3D 11 to Direct3D 12 might have potentially faced worse performance than before, seemingly contradicting those API's promise of improved performance.&lt;/p&gt;
    &lt;p&gt;Part of the reason is that a lot of the work that had been taken care of the by API's drivers are now application responsibilities and thus need to be taken care of by developers themselves. This however requires a more intimate understanding of "how the GPU works" and familiarization with those parts of Direct3D or OpenGL that were purposefully hidden from developers up to this point. And it also requires a more in-depth understanding of how the render commands interact with each other, what their dependencies are, and similarly how to encode and communicate these dependencies to the GPU to ensure that render commands are executed in the right order.&lt;/p&gt;
    &lt;p&gt;In the case of the Metal backend, this means that a certain amount of overhead that was removed from the API itself had to be reintroduced into the backend again, as even though it would be in a far better position for adoption, the core renderer was not available for a rewrite. Nevertheless even with this overhead, the Metal backend provides multiple benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In Release configuration and even in its non-optimized current form, it performs as well or even better as the OpenGL renderer.&lt;/item&gt;
      &lt;item&gt;In Debug configuration it provides an amazing set of capabilities to analyze render passes in all their detail, including shader debugging, texture lookups, and so much more.&lt;/item&gt;
      &lt;item&gt;As it's written in Swift, it uses a safer programming language than the OpenGL renderer, one that is at the same time easier and less time consuming to work with.&lt;/item&gt;
      &lt;item&gt;Because preview rendering is effectively handled separately from the main render loop, the Metal renderer enables EDR previews on macOS for high-bitrate video setups.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As far as maintenance of the macOS version of OBS Studio is concerned, the Metal backend brings considerable benefits as Xcode can now provide insights that haven't been available to developers since the deprecation of OpenGL on all Apple platforms in 2018. But it is due to these complexities that the project would like to extend an invitation to all developers (and users that might be so inclined) to provide feedback and suggestions for changes to improve the design and implementation of the Metal backend to first move it out of the "experimental" stage and later make it the default graphics backend on macOS.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://obsproject.com/blog/obs-studio-gets-a-new-renderer"/><published>2025-12-17T20:59:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46305585</id><title>I got hacked: My Hetzner server started mining Monero</title><updated>2025-12-18T10:45:21.713394+00:00</updated><content>&lt;doc fingerprint="ac0ad19969a51b25"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I got hacked, my server started mining Monero this morning.&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Edit: This got way more attention than I was ever expecting. I originally asked claude to draft it from a transcript of my panicked messages and the feedback was clear, nobody likes AI slop. As a result I’ve redrafted it this morning to fix some inaccuracies and make it sound human. - Jake&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Or: How I learned that “I don’t use Next.js” doesn’t mean your dependencies don’t use Next.js&lt;/p&gt;
    &lt;head rend="h2"&gt;8:25 AM: The Email&lt;/head&gt;
    &lt;p&gt;I woke up to this beauty from Hetzner:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Dear Mr Jake Saunders,&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;We have indications that there was an attack from your server. Please take all necessary measures to avoid this in the future and to solve the issue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;We also request that you send a short response to us. This response should contain information about how this could have happened and what you intend to do about it. In the event that the following steps are not completed successfully, your server can be blocked at any time after the 2025-12-17 12:46:15 +0100.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Attached was evidence of network scanning from my server to some IP range in Thailand. Great. Nothing says “good morning” like an abuse report and the threat of getting your infrastructure shut down in 4 hours.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Background: I run a Hetzner server with Coolify. It runs all my stuff, like my little corner of the internet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;My IoT Side Project&lt;/item&gt;
      &lt;item&gt;This Blog&lt;/item&gt;
      &lt;item&gt;Analytics&lt;/item&gt;
      &lt;item&gt;My dads site (he’s an electrician)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;8:30 AM: Oh Fuck&lt;/head&gt;
    &lt;p&gt;First thing I did was SSH in and check the load average:&lt;/p&gt;
    &lt;p&gt;I run a bunch of Go backend services and some SvelteKit frontend stuff on there. My grand total of daily users peaks at 20, so something was very wrong.&lt;/p&gt;
    &lt;p&gt;I ran &lt;code&gt;ps aux&lt;/code&gt; to see what was eating my CPU:&lt;/p&gt;
    &lt;p&gt;819% CPU usage. On a process called &lt;code&gt;javae&lt;/code&gt; running from &lt;code&gt;/tmp/.XIN-unix/&lt;/code&gt;. And multiple &lt;code&gt;xmrig&lt;/code&gt; processes - that’s
literally cryptocurrency mining software (Monero, specifically).&lt;/p&gt;
    &lt;p&gt;Looks like I’d been mining cryptocurrency for someone since December 7th. For ten days. Brilliant.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Investigation&lt;/head&gt;
    &lt;p&gt;My first thought was “I’m completely fucked.” My host had been running a crypto miner for a week, the whole think was borked. Time to just nuke it from orbit and rebuild.&lt;/p&gt;
    &lt;p&gt;Fortunately, I had the foresight to do a little detective work beforehand to at least learn how I’d been compromised so I could learn for the future. I set out to do this with the help of Claude (this is not my speciality).&lt;/p&gt;
    &lt;p&gt;First, I noticed something interesting. All these processes were running as user &lt;code&gt;1001&lt;/code&gt;. Not root. Not a system user.
UID 1001.&lt;/p&gt;
    &lt;p&gt;Let me check what’s actually running:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Deleted ports from this list as i feel like the might expose some inner workings.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Crucially, I was running Umami - a privacy-focused analytics tool I’d re-deployed 9 days ago to track traffic on my blog. I redeployed it because it had started acting up and I wasn’t sure why. The timing was suspicious to me.&lt;/p&gt;
    &lt;p&gt;Let me check which container has user 1001:&lt;/p&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;p&gt;There it is. Container &lt;code&gt;a42f72cb1bc5&lt;/code&gt; - that’s my Umami analytics container. And it’s got a whole &lt;code&gt;xmrig-6.24.0&lt;/code&gt;
directory sitting in what should be Next.js server internals.&lt;/p&gt;
    &lt;p&gt;The mining command in the process list confirmed it:&lt;/p&gt;
    &lt;p&gt;Someone had exploited my analytics container and was mining Monero using my CPU. Nice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wait, I Don’t Use Next.js&lt;/head&gt;
    &lt;p&gt;I’d actually seen a post on HN referencing this Reddit post about a critical Next.js (CVE-2025-66478). My immediate reaction was “lol who cares, I don’t run Next.js.”&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Oh my sweet summer child.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Except… Umami is built with Next.js. I did not know this, nor did I bother looking. Oops.&lt;/p&gt;
    &lt;p&gt;The vulnerability (CVE-2025-66478) was in Next.js’s React Server Components deserialization. The “Flight” protocol that RSC uses to serialize/deserialize data between client and server had an unsafe deserialization flaw. An attacker could send a specially crafted HTTP request with a malicious payload to any App Router endpoint, and when deserialized, it would execute arbitrary code on the server.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Attacker sends crafted HTTP request to Umami’s Next.js endpoint&lt;/item&gt;
      &lt;item&gt;RSC deserializes the malicious payload&lt;/item&gt;
      &lt;item&gt;RCE achieved via unsafe deserialization&lt;/item&gt;
      &lt;item&gt;Download and install cryptominers&lt;/item&gt;
      &lt;item&gt;Profit (for them)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So much for “I don’t use Next.js.”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Panic: Has It Escaped the Container?&lt;/head&gt;
    &lt;p&gt;This is where I started to properly panic. Looking at that process list:&lt;/p&gt;
    &lt;p&gt;That path - &lt;code&gt;/tmp/.XIN-unix/javae&lt;/code&gt; - looks like it’s on the host filesystem, not inside a container. That means it
can get access to my database, all my environment variables, the works. Claude was telling me I’d need to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Assume everything is compromised&lt;/item&gt;
      &lt;item&gt;Check for rootkits, backdoors, persistence mechanisms&lt;/item&gt;
      &lt;item&gt;Probably rebuild from scratch&lt;/item&gt;
      &lt;item&gt;Spend my entire day unfucking this&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I checked for persistence mechanisms:&lt;/p&gt;
    &lt;p&gt;No malicious cron jobs. No fake systemd services pretending to be &lt;code&gt;nginxs&lt;/code&gt; or &lt;code&gt;apaches&lt;/code&gt;. That’s… good?&lt;/p&gt;
    &lt;p&gt;But I still needed to know: Did the malware actually escape the container or not?&lt;/p&gt;
    &lt;head rend="h2"&gt;The Moment of Truth&lt;/head&gt;
    &lt;p&gt;The test was, if &lt;code&gt;/tmp/.XIN-unix/javae&lt;/code&gt; exists on the host, I’m fucked. If it doesn’t exist, then apparently what I’m
seeing is
just Docker’s default behavior of showing container processes in the host’s &lt;code&gt;ps&lt;/code&gt; output, but they’re actually isolated.&lt;/p&gt;
    &lt;p&gt;IT NEVER ESCAPED.&lt;/p&gt;
    &lt;p&gt;Or at least it doesn’t look like it. We can downgrade this incident from DEFCON1 to ‘point a gun at it and do some more checks, but no guillotine yet’.&lt;/p&gt;
    &lt;p&gt;The malware was entirely contained within the Umami container. Apparently, when you run &lt;code&gt;ps aux&lt;/code&gt; on a Docker host, you
see processes
from all containers because they share the same kernel. But those processes are in their own mount namespace - they
can’t see or touch the host filesystem.&lt;/p&gt;
    &lt;p&gt;I verified what user that container was actually running as:&lt;/p&gt;
    &lt;p&gt;So here’s what I now know, and why I’m not totally fucked:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Container ran as user &lt;code&gt;nextjs&lt;/code&gt;(UID 1001), not root.&lt;/item&gt;
      &lt;item&gt;Container was not privileged.&lt;/item&gt;
      &lt;item&gt;Container had zero volume mounts.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Which means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run processes inside the container&lt;/item&gt;
      &lt;item&gt;Mine cryptocurrency&lt;/item&gt;
      &lt;item&gt;Scan networks (hence the Hetzner abuse report)&lt;/item&gt;
      &lt;item&gt;Consume 100% CPU&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The malware could NOT:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Access the host filesystem&lt;/item&gt;
      &lt;item&gt;Install cron jobs&lt;/item&gt;
      &lt;item&gt;Create systemd services&lt;/item&gt;
      &lt;item&gt;Persist across container restarts&lt;/item&gt;
      &lt;item&gt;Escape to other containers&lt;/item&gt;
      &lt;item&gt;Install rootkits&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Container isolation actually worked. Nice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dockerfiles vs. Auto-Generated Images&lt;/head&gt;
    &lt;p&gt;There were a couple of things which saved me in this case IMO compared to the Reddit post I linked:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I write all my own dockerfiles for my applications. This isn;t a silver bullet on it’s own but compared to autogenerated ones you have a better idea of what’s in there.&lt;/item&gt;
      &lt;item&gt;Coolify and Dockers approach to containerization in general. I’ve since learned that we can’t rely on container separation for security but honestly it seems better than running everything on the host.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Reddit post I’d seen earlier? That guy got completely screwed because his container was running as root. The malware could:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install cron jobs for persistence&lt;/item&gt;
      &lt;item&gt;Create systemd services&lt;/item&gt;
      &lt;item&gt;Write anywhere on the filesystem&lt;/item&gt;
      &lt;item&gt;Survive reboots&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So in this case, container isolation had worked!&lt;/p&gt;
    &lt;p&gt;What I did not do, was keep track of the tolling I was using and what tooling that was using. In fact, I installed Umami from Coolify’s services screen. I didn’t even configure it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Obviously none of this is Umami’s fault by the way. They released a fix for their free software like a week ago. I just didn’t think to do anything about it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;The Fix&lt;/head&gt;
    &lt;p&gt;CPU back to normal. It’s been two days since and my CPU is just chilling at like 5%.&lt;/p&gt;
    &lt;p&gt;I also enabled UFW (which I should have done ages ago):&lt;/p&gt;
    &lt;p&gt;This blocks all inbound connections except SSH, HTTP, and HTTPS. No more exposed PostgreSQL ports, no more RabbitMQ ports open to the internet. In my mind this shouldn’t be too big a deal because 5432 wasn’t open to he host from the docker container. But worth doing.&lt;/p&gt;
    &lt;p&gt;I sent Hetzner a brief explanation:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Investigation complete. The scanning originated from a compromised Umami analytics container (CVE-2025-66478).&lt;/p&gt;
      &lt;p&gt;The container ran as non-root user with no privileged access or host mounts, so the compromise was fully contained. Container has been removed and firewall hardened.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;They closed the ticket within an hour.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons Learned&lt;/head&gt;
    &lt;head rend="h3"&gt;1. “I don’t use X” doesn’t mean your dependencies don’t use X&lt;/head&gt;
    &lt;p&gt;I don’t write Next.js applications. But I run third-party tools that are built with Next.js. When CVE-2025-66478 was disclosed, I thought “not my problem.” Wrong.&lt;/p&gt;
    &lt;p&gt;Know what your dependencies are actually built with. That “simple analytics tool” is a full web application with a complex stack.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Container isolation works (when configured properly)&lt;/head&gt;
    &lt;p&gt;This could have been so much worse. If that container had been running as root, or had volume mounts to sensitive directories, or had access to the Docker socket, I’d be writing a very different blog post about rebuilding my entire infrastructure.&lt;/p&gt;
    &lt;p&gt;Instead, I deleted one container and moved on with my day.&lt;/p&gt;
    &lt;p&gt;Write your own Dockerfiles. Understand what user your processes run as. Avoid &lt;code&gt;USER root&lt;/code&gt; unless you have a very
good reason. Don’t mount volumes you don’t need. Don’t give containers &lt;code&gt;--privileged&lt;/code&gt; access.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. The sophistication gap&lt;/head&gt;
    &lt;p&gt;This malware wasn’t like those people who auto-poll for &lt;code&gt;/wpadmin&lt;/code&gt; every time I make a DNS change. This was spicy.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disguised itself in legitimate-looking paths (&lt;code&gt;/app/node_modules/next/dist/server/lib/&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Used process names that blend in (&lt;code&gt;javae&lt;/code&gt;,&lt;code&gt;runnv&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Attempted to establish persistence&lt;/item&gt;
      &lt;item&gt;According to other reports, even had “killer scripts” to murder competing miners&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But it was still limited by container isolation. Good security practices beat sophisticated malware.&lt;/p&gt;
    &lt;head rend="h3"&gt;4. Defense in depth matters&lt;/head&gt;
    &lt;p&gt;Even though the container isolation held, I still should have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Had a firewall enabled from day one (not “I’ll do it later”)&lt;/item&gt;
      &lt;item&gt;Been running fail2ban to stop those SSH brute force attempts&lt;/item&gt;
      &lt;item&gt;Had proper monitoring/alerting (I only noticed because of the Hetzner email)&lt;/item&gt;
      &lt;item&gt;Updated Umami when the CVE was disclosed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I got lucky. Container isolation saved me from my own laziness.&lt;/p&gt;
    &lt;head rend="h2"&gt;What I’m Doing Differently&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;No more Umami. - Meh, I’ve gone back on this. This wasn’t Umami’s fault, and their open source software is super cool. I’ve rebooted a fresh version of Umami.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;Audit all third-party containers. Going through everything I run and checking: &lt;list rend="ul"&gt;&lt;item&gt;What user does it run as?&lt;/item&gt;&lt;item&gt;What volumes does it have?&lt;/item&gt;&lt;item&gt;When was it last updated?&lt;/item&gt;&lt;item&gt;Do I actually need it?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;SSH hardening. Moving to key-based authentication only, disabling password auth, and setting up fail2ban.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Proper monitoring. Setting up alerts for CPU usage, load average, and suspicious network activity. I shouldn’t find out about compromises from my hosting provider. I actually have grafana and Node exporter set up, but it’s not good unless I go look at it!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;Regular security updates. No more “I’ll update it later.” If there’s a CVE, I patch or I remove the service.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Silver Lining&lt;/head&gt;
    &lt;p&gt;This was actually a pretty good learning experience. I got to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Practice incident response on a real compromise (never done this before!)&lt;/item&gt;
      &lt;item&gt;Prove that container isolation actually works&lt;/item&gt;
      &lt;item&gt;Learn about Docker namespaces, user mapping, and privilege boundaries&lt;/item&gt;
      &lt;item&gt;Harden my infrastructure without the pressure of active data loss&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And I only lost about 2 hours of my morning before work. Could’ve been way worse.&lt;/p&gt;
    &lt;p&gt;Though I do wonder how much Monero I mined for that dickhead. Based on the CPU usage and duration… probably enough for them to have a nice lunch. You’re welcome, mysterious attacker. Hope you enjoyed it.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Umami analytics (built with Next.js) had an RCE vulnerability.&lt;/item&gt;
      &lt;item&gt;Got exploited, installed cryptominers&lt;/item&gt;
      &lt;item&gt;Mined Monero for 10 days at 1000%+ CPU&lt;/item&gt;
      &lt;item&gt;Container isolation saved me because it ran as non-root with no mounts&lt;/item&gt;
      &lt;item&gt;Fix: &lt;code&gt;docker rm umami&lt;/code&gt;and enable firewall&lt;/item&gt;
      &lt;item&gt;Lesson: Know what your dependencies are built with, and configure containers properly&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.jakesaunders.dev/my-server-started-mining-monero-this-morning/"/><published>2025-12-17T21:13:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46305945</id><title>Cloudflare Radar 2025 Year in Review</title><updated>2025-12-18T10:45:21.629275+00:00</updated><content/><link href="https://radar.cloudflare.com/year-in-review/2025"/><published>2025-12-17T21:44:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46306456</id><title>Developers can now submit apps to ChatGPT</title><updated>2025-12-18T10:45:21.459725+00:00</updated><content>&lt;doc fingerprint="9a87ba79bb8743ae"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Developers can now submit apps to ChatGPT&lt;/head&gt;
    &lt;p&gt;We’re opening app submissions for review and publication in ChatGPT, and users can discover apps in the app directory.&lt;/p&gt;
    &lt;p&gt;Earlier this year at DevDay, we introduced apps in ChatGPT. Starting today, developers can submit apps for review and publication in ChatGPT by following our app submission guidelines(opens in a new window). Apps extend ChatGPT conversations by bringing in new context and letting users take actions like order groceries, turn an outline into a slide deck, or search for an apartment. We’ve published resources to help developers build high-quality apps that users will love—based on what we’ve learned since DevDay—like best practices on what makes a great ChatGPT app(opens in a new window), open-source example apps(opens in a new window), an open-sourced UI library(opens in a new window) for chat-native interfaces, and a step-by-step quickstart guide(opens in a new window).&lt;/p&gt;
    &lt;p&gt;We’re also introducing an app directory right inside ChatGPT, where users can browse featured apps or search for any published app. The app directory is discoverable from the tools menu or directly from chatgpt.com/apps. Developers can also use deep links on other platforms to send users right to their app page in the directory.&lt;/p&gt;
    &lt;p&gt;Once users connect to apps, apps can get triggered during conversations when @ mentioned by name, or when selected from the tools menu. We’re also experimenting with ways to surface relevant, helpful apps directly within conversations—using signals like conversational context, app usage patterns, and user preferences—and giving users clear ways to provide feedback.&lt;/p&gt;
    &lt;p&gt;Building a great ChatGPT app starts with designing for real user intent. Developers can use the Apps SDK—now in beta—to build chat-native experiences that bring context and action directly into ChatGPT. The strongest apps are tightly scoped, intuitive in chat, and deliver clear value by either completing real-world workflows that start in conversation or enabling new, fully AI-native experiences inside ChatGPT. We recommend reviewing the app submission guidelines(opens in a new window) early to help you build a high-quality app. Additional documentation and examples are available in the developer resource hub(opens in a new window).&lt;/p&gt;
    &lt;p&gt;Once ready, developers can submit apps for review and track approval status in the OpenAI Developer Platform(opens in a new window). Submissions include MCP connectivity details, testing guidelines, directory metadata, and country availability settings. The first set of approved apps will begin rolling out gradually in the new year. Apps that meet our quality and safety standards are eligible to be published in the app directory, and apps that resonate with users may be featured more prominently in the directory or recommended by ChatGPT in the future.&lt;/p&gt;
    &lt;p&gt;In this early phase, developers can link out from their ChatGPT apps to their own websites or native apps to complete transactions for physical goods. We’re exploring additional monetization options over time, including digital goods, and will share more as we learn from how developers and users build and engage.&lt;/p&gt;
    &lt;p&gt;All developers are required to follow the app submission guidelines(opens in a new window) around safety, privacy, and transparency. Apps must comply with OpenAI’s usage policies, be appropriate for all audiences, and adhere to third-party terms of service when accessing their content. Developers must include clear privacy policies with every app submission and we require developers to only request the information needed to make their apps work.&lt;/p&gt;
    &lt;p&gt;When a user connects to a new app, we will disclose what types of data may be shared with the third party and provide the app’s privacy policy for review. And users are always in control: disconnect an app at any time, and it immediately loses access.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. Over time, we want apps in ChatGPT to feel like a natural extension of the conversation, helping people move from ideas to action, while building a thriving ecosystem for developers. As we learn from developers and users, we’ll continue refining the experience for everyone. We also plan to grow the ecosystem of apps in ChatGPT, make apps easier to discover, and expand the ways developers can reach users and monetize their work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/developers-can-now-submit-apps-to-chatgpt/"/><published>2025-12-17T22:27:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46306894</id><title>Gut bacteria from amphibians and reptiles achieve tumor elimination in mice</title><updated>2025-12-18T10:45:20.225667+00:00</updated><content>&lt;doc fingerprint="b468c19a0aaf4982"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Gut Bacteria from Amphibians and Reptiles Achieve Complete Tumor Elimination&lt;/head&gt;
    &lt;p&gt;ãKey Research Achievementsã&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Demonstration that natural bacteria isolated from amphibian and reptile intestines achieve complete tumor elimination with single administration&lt;/item&gt;
      &lt;item&gt;Combines direct bacterial killing of cancer cells with immune system activation for comprehensive tumor destruction&lt;/item&gt;
      &lt;item&gt;Outperforms existing chemotherapy and immunotherapy with no adverse effects on normal tissues&lt;/item&gt;
      &lt;item&gt;Expected applications across diverse solid tumor types, opening new avenues for cancer treatment&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ãResearch Overviewã&lt;/p&gt;
    &lt;p&gt;A research team of Prof. Eijiro Miyako at the Japan Advanced Institute of Science and Technology (JAIST) has discovered that the bacterium Ewingella americana, isolated from the intestines of Japanese tree frogs (Dryophytes japonicus), possesses remarkably potent anticancer activity. This groundbreaking research has been published in the international journal Gut Microbes.&lt;lb/&gt;While the relationship between gut microbiota and cancer has attracted considerable attention in recent years, most approaches have focused on indirect methods such as microbiome modulation or fecal microbiota transplantation. In contrast, this study takes a completely different approach: isolating, culturing, and directly administering individual bacterial strains intravenously to attack tumors--- representing an innovative therapeutic strategy.&lt;lb/&gt;The research team isolated a total of 45 bacterial strains from the intestines of Japanese tree frogs, Japanese fire belly newts (Cynops pyrrhogaster), and Japanese grass lizards (Takydromus tachydromoides). Through systematic screening, nine strains demonstrated antitumor effects, with E. americana exhibiting the most exceptional therapeutic efficacy.&lt;/p&gt;
    &lt;p&gt;ãResearch Detailsã&lt;/p&gt;
    &lt;p&gt;Remarkable Therapeutic Efficacy&lt;lb/&gt;In a mouse colorectal cancer model, a single intravenous administration of E. americana achieved complete tumor elimination with a 100% complete response (CR) rate. This dramatically surpasses the therapeutic efficacy of current standard treatments, including immune checkpoint inhibitors (anti-PD-L1 antibody) and liposomal doxorubicin (chemotherapy agent) (Figure 1).&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Figure 1. Anticancer efficacy: Ewingella americana versus conventional therapies. Tumor response: single i.v. dose of E. americana (200 ÂµL, 5 Ã 10â¹ CFU/mL); four doses of doxorubicin or anti-PD-L1 (200 ÂµL, 2.5 mg/kg per dose); PBS as control. Data: mean Â± SEM (n = 5). ****, p &amp;lt; 0.0001 (Student's two-sided t-test).&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Dual-Action Anticancer Mechanism&lt;/p&gt;
    &lt;p&gt;E. americana attacks cancer through two complementary mechanisms (Figure 2):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Direct Cytotoxic Effect: As a facultative anaerobic bacterium, E. americana selectively accumulates in the hypoxic tumor microenvironment and directly destroys cancer cells. Bacterial counts within tumors increase approximately 3,000-fold within 24 hours post-administration, efficiently attacking tumor tissue.&lt;/item&gt;
      &lt;item&gt;Immune Activation Effect: The bacterial presence powerfully stimulates the immune system, recruiting T cells, B cells, and neutrophils to the tumor site. Pro-inflammatory cytokines (TNF-Î±, IFN-Î³) produced by these immune cells further amplify immune responses and induce cancer cell apoptosis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Figure 2. Mechanisms underlying Ewingella americana antitumor effects.&lt;/p&gt;
    &lt;p&gt;Tumor-Specific Accumulation Mechanism&lt;/p&gt;
    &lt;p&gt;E. americana selectively accumulates in tumor tissues with zero colonization in normal organs. This remarkable tumor specificity arises from multiple synergistic mechanisms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hypoxic Environment: The characteristic hypoxia of tumor tissues promotes anaerobic bacterial proliferation&lt;/item&gt;
      &lt;item&gt;Immunosuppressive Environment: CD47 protein expressed by cancer cells creates local immunosuppression, forming a permissive niche for bacterial survival&lt;/item&gt;
      &lt;item&gt;Abnormal Vascular Structure: Tumor vessels are leaky, facilitating bacterial extravasation&lt;/item&gt;
      &lt;item&gt;Metabolic Abnormalities: Tumor-specific metabolites support selective bacterial growth&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Excellent Safety Profile&lt;/p&gt;
    &lt;p&gt;Comprehensive safety evaluation revealed that E. americana demonstrates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rapid blood clearance (half-life ~1.2 hours, completely undetectable at 24 hours)&lt;/item&gt;
      &lt;item&gt;Zero bacterial colonization in normal organs including liver, spleen, lung, kidney, and heart&lt;/item&gt;
      &lt;item&gt;Only transient mild inflammatory responses, normalizing within 72 hours&lt;/item&gt;
      &lt;item&gt;No chronic toxicity during 60-day extended observation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ãFuture Directionsã&lt;/p&gt;
    &lt;p&gt;This research has established proof-of-concept for a novel cancer therapy using natural bacteria. Future research and development will focus on:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Expansion to Other Cancer Types: Efficacy validation in breast cancer, pancreatic cancer, melanoma, and other malignancies&lt;/item&gt;
      &lt;item&gt;Optimization of Administration Methods: Development of safer and more effective delivery approaches including dose fractionation and intratumoral injection&lt;/item&gt;
      &lt;item&gt;Combination Therapy Development: Investigation of synergistic effects with existing immunotherapy and chemotherapy&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This research demonstrates that unexplored biodiversity represents a treasure trove for novel medical technology development and holds promise for providing new therapeutic options for patients with refractory cancers.&lt;/p&gt;
    &lt;p&gt;ãGlossaryã&lt;/p&gt;
    &lt;p&gt;Facultative Anaerobic Bacteria: Bacteria capable of growing in both oxygen-rich and oxygen-depleted environments, enabling selective proliferation in hypoxic tumor regions.&lt;lb/&gt;Complete Response (CR): Complete tumor elimination confirmed by diagnostic examination following treatment.&lt;lb/&gt;Immune Checkpoint Inhibitor: Drugs that release cancer cell-mediated immune suppression, enabling T cells to attack cancer cells.&lt;lb/&gt;CD47: A cell surface protein that emits "don't eat me" signals; cancer cells overexpress this to evade immune attack.&lt;/p&gt;
    &lt;p&gt;ãPublication Informationã&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Title:&lt;/cell&gt;
        &lt;cell&gt;Discovery and characterization of antitumor gut microbiota from amphibians and reptiles: Ewingella americana as a novel therapeutic agent with dual cytotoxic and immunomodulatory properties&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Authors:&lt;/cell&gt;
        &lt;cell&gt;Seigo Iwata, Nagi Yamashita, Kensuke Asukabe, Matomo Sakari, Eijiro Miyako*&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Journal:&lt;/cell&gt;
        &lt;cell&gt;Gut Microbes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DOI:&lt;/cell&gt;
        &lt;cell&gt;10.1080/19490976.2025.2599562&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;ãResearch Fundingã&lt;/p&gt;
    &lt;p&gt;This research was supported by:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Japan Society for the Promotion of Science (JSPS) KAKENHI Grant-in-Aid for Scientific Research (A) (Grant No. 23H00551)&lt;/item&gt;
      &lt;item&gt;JSPS KAKENHI Grant-in-Aid for Challenging Research (Pioneering) (Grant No. 22K18440)&lt;/item&gt;
      &lt;item&gt;JSPS Program for Forming Japan's Peak Research Universitiesï¼J-PEAKSï¼ (Grant No. JPJS00420230006)&lt;/item&gt;
      &lt;item&gt;Japan Science and Technology Agency (JST) Program for Co-creating Startup Ecosystem (Grant No. JPMJSF2318)&lt;/item&gt;
      &lt;item&gt;JST SPRING (Grant No. JPMJSP2102)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;December 15, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jaist.ac.jp/english/whatsnew/press/2025/12/17-1.html"/><published>2025-12-17T23:11:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46307306</id><title>Ask HN: Does anyone understand how Hacker News works?</title><updated>2025-12-18T10:45:19.425629+00:00</updated><content>&lt;doc fingerprint="a9808191fb5e50f"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;When working on my projects and talking to investors, I often hear: “Just post it on Hacker News or Reddit and show that people love it.”&lt;/p&gt;
      &lt;p&gt;What I find strange is that Hacker News feels oddly opaque. I’ve never met anyone who can clearly explain how it works in practice. Not just the rules, but the dynamics: what’s repeatable, what’s luck, and what actually matters.&lt;/p&gt;
      &lt;p&gt;By using the Kevin Bacon-number idea: I can usually get within three degrees of separation of well-known technologists like Linus Torvalds, but I can’t seem to get within three steps of someone who confidently understands how HN works.&lt;/p&gt;
      &lt;p&gt;So I’m asking sincerely: Does anyone here feel they understand Hacker News? If so, what are the real levers, and what do people consistently misunderstand?&lt;/p&gt;
      &lt;p&gt;PS: This question comes from a mix of genuine curiosity and personal frustration. I’m honestly trying to understand how HN works in practice.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46307306"/><published>2025-12-17T23:58:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46307500</id><title>TikTok unlawfully tracks shopping habits and use of dating apps?</title><updated>2025-12-18T10:45:17.499207+00:00</updated><content>&lt;doc fingerprint="8d979cb02375e28d"&gt;
  &lt;main&gt;
    &lt;p&gt;TikTok not only tracks its users while they are using the TikTok app itself, but it is increasingly integrated with many other websites and apps. For example, TikTok was able to track a person’s Grindr usage on his smartphone. However, that’s not all: In addition to tracking users across the digital space, TikTok also refuses to provide an interested users with a copy of all of their personal data. Therefore, noyb has filed two complaints against TikTok and its data-sharing partners AppsFlyer and Grindr.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Complaint against TikTok for incomplete answer of access request&lt;/item&gt;
      &lt;item&gt;Complaint against TikTok, AppsFlyer and Grindr for data sharing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unlawful tracking across apps. It’s no secret that TikTok is rather data hungry. After all, the popular video platform’s algorithm seems to know exactly what content users want to see. However, it’s not well known that TikTok also tracks you while using other apps. A user found out about this unlawful tracking practice through an access request – which showed that e.g. his usage of Grindr was sent to TikTok, likely via the Israeli tracking company AppsFlyer - which allows TikTok to draw conclusions about his sexual orientation and sex life. This is specially protected data under Article 9 GDPR, which can only be processed in exceptional cases. TikTok initially even withheld this information from the user, which violates Article 15 GDPR. Only after repeated inquiries, TikTok revealed that it knows which apps he used, what he did within these apps (for example adding a product to the shopping cart) - and that this data also included information about his usage of the gay dating app Grindr.&lt;/p&gt;
    &lt;p&gt;Kleanthi Sardeli, data protection lawyer at noyb: “Like many of its US counterparts, TikTok increasingly collects data from other apps and sources. This allows the Chinese app to gain a full picture of people’s online activity. The fact that data from another app revealed this user’s sexual orientation and sex life is just one of the more extreme examples.”&lt;/p&gt;
    &lt;p&gt;Accomplices in unlawful data processing. TikTok was only able to receive this information with the help of the Israeli data company AppsFlyer and Grindr itself. AppsFlyer most likely functions as a kind of intermediary, which receives the sensitive data about the complainant from Grindr and then passed it on to TikTok. The problem: Neither AppsFlyer nor Grindr have a valid legal basis under Article 6(1) GDPR to share the complainant’s personal data with third parties such as TikTok. And they most certainly don’t have any valid reason to share his sensitive data under Article 9(1) GDPR. At no point in time did the complainant consent to the sharing of his data.&lt;/p&gt;
    &lt;p&gt;Insufficient reply to access request. Users should generally be informed about the recipients of personal data and even get a copy of said data. However, TikTok seems to structurally violate the users’ right to get such a copy. TikTok refers its users to a “download tool”, but later admitted that this tool only holds what it deems the most “relevant” data – and by far not all personal data. Even after repeated inquiries to add the missing information, TikTok didn’t provide information about which data is being processed and for what purpose. By doing so, TikTok clearly violates Articles 12 and 15 GDPR, which require companies to provide the information in full and in an easily understandable format.&lt;/p&gt;
    &lt;p&gt;Lisa Steinfeld, data protection lawyer at noyb: “TikTok directs its users to an inherently incomplete ‘download tool’. It’s fair to assume that thousands of users were sent to this scam tool, which structurally doesn’t comply with the legal requirements to provide a full copy of one’s own personal data.”&lt;/p&gt;
    &lt;p&gt;Complaints filed in Austria. noyb has therefore filed two complaints with the Austrian data protection authority (DSB). The first complaint is against TikTok and revolves around the incomplete reply to the complainant’s access request. The second complaint is against TikTok, AppsFlyer and Grindr and deals with the undefined processing of off-TikTok data, the lack of a valid legal basis for the data sharing and processing and the violation of Article 9(1) GDPR. We request TikTok to provide the complainant with the missing information and all three companies to stop the unlawful processing of his personal data. Last but not least, we suggest that the authority impose an “effective, proportionate and dissuasive” fine under Article 83 GDPR to prevent similar violations in the future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://noyb.eu/en/tiktok-unlawfully-tracks-your-shopping-habits-and-your-use-dating-apps"/><published>2025-12-18T00:25:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46307973</id><title>Ask HN: Those making $500/month on side projects in 2025 – Show and tell</title><updated>2025-12-18T10:45:16.659952+00:00</updated><content>&lt;doc fingerprint="5fce0ba49f761a69"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;It's the time of the year again, so I'd be interested hear what new (and old) ideas have come up. Previously asked on:&lt;/p&gt;
      &lt;p&gt;2024 → https://news.ycombinator.com/item?id=42373343&lt;/p&gt;
      &lt;p&gt;2023 → https://news.ycombinator.com/item?id=38467691&lt;/p&gt;
      &lt;p&gt;2022 → https://news.ycombinator.com/item?id=34190421&lt;/p&gt;
      &lt;p&gt;2021 → https://news.ycombinator.com/item?id=29667095&lt;/p&gt;
      &lt;p&gt;2020 → https://news.ycombinator.com/item?id=24947167&lt;/p&gt;
      &lt;p&gt;2019 → https://news.ycombinator.com/item?id=20899863&lt;/p&gt;
      &lt;p&gt;2018 → https://news.ycombinator.com/item?id=17790306&lt;/p&gt;
      &lt;p&gt;2017 → https://news.ycombinator.com/item?id=15148804&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46307973"/><published>2025-12-18T01:36:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46308893</id><title>Judge hints Vizio TV buyers may have rights to source code licensed under GPL</title><updated>2025-12-18T10:45:16.464544+00:00</updated><content>&lt;doc fingerprint="ad2631190aa62937"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Judge hints Vizio TV buyers may have rights to source code licensed under GPL&lt;/head&gt;
    &lt;head rend="h2"&gt;Tentative ruling signals a potential win for SFC’s copyleft enforcement push&lt;/head&gt;
    &lt;p&gt;Electronics biz Vizio may be required by a California court to provide source code for its SmartCast TV software, which is allegedly based on open source code licensed under the GPLv2 and LGPLv2.1.&lt;/p&gt;
    &lt;p&gt;The legal complaint from the Software Freedom Conservancy (SFC) seeks access to the SmartCast source code so that Vizio customers can make changes and improvements to the platform, something that ought to be possible for code distributed under the GPL. On Thursday, California Superior Court Judge Sandy Leal issued a tentative ruling in advance of a hearing, indicating support for part of SFC's legal challenge.&lt;/p&gt;
    &lt;p&gt;The tentative ruling is not a final decision, but it signals the judge's inclination to grant the SFC's motion for summary adjudication, at least in part.&lt;/p&gt;
    &lt;p&gt;"The tentative ruling [PDF] grants SFC's motion on the issue that a direct contract was made between SFC and Vizio when SFC's systems administrator, Paul Visscher, requested the source code to a TV that SFC has purchased," the SFC said in a blog post. "This contract obligated Vizio to provide SFC the complete and corresponding source code."&lt;/p&gt;
    &lt;p&gt;The SFC initially asked Vizio to publish its SmartCast source code back in August 2018, based on its claim that the software relies on various applications and libraries that are licensed under the GPLv2 and LGPLv2.1, including the Linux kernel, alsa-utils, GNU bash, GNU awk, bluez, BusyBox, and other components.&lt;/p&gt;
    &lt;p&gt;Vizio responded in 2019 but the code it provided was incomplete, according to the SFC, and didn't fulfill the company's obligations.&lt;/p&gt;
    &lt;p&gt;After two more years of negotiation, the SFC sued Vizio in October 2021, a relatively uncommon event in the FOSS world due to the cost and difficulty of enforcing open source licenses. SFC director of compliance Denver Gingerich told The Register that software developer Sebastian Steck's open source licensing victory in Germany against AVM in January appears to be the first time the LGPL has been successfully litigated.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Introducing NTFSplus – because just one NTFS driver for Linux is never enough&lt;/item&gt;
      &lt;item&gt;Strong Java LTS arrives with the release of 25&lt;/item&gt;
      &lt;item&gt;Ubuntu 25.10 plans to swap GNU coreutils for Rust&lt;/item&gt;
      &lt;item&gt;We call this kernel saunters: How Apple rearranged its XNU core with exclaves&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Karen Sandler, executive director of the SFC, told The Register in an email that the hearing went well, though Vizio's legal counsel "stridently disagreed" with the legal analysis in the tentative ruling.&lt;/p&gt;
    &lt;p&gt;"Judge Leal said she would take the matter 'under submission' which means she will think about it further," Sandler said. "After the Court went off the record, Leal's clerk specifically verified the Court reporter could provide an expedited transcript, so Leal will likely review the hearing transcript soon."&lt;/p&gt;
    &lt;p&gt;Sandler expects Leal will examine the filings again before issuing her opinion, which is likely to be issued in the next few weeks.&lt;/p&gt;
    &lt;p&gt;Bradley Kühn, policy fellow at the SFC, posted about the hearing (in a personal capacity) in a Mastodon thread. His account expresses some skepticism about the validity of Vizio's legal arguments.&lt;/p&gt;
    &lt;p&gt;"Vizio is effectively saying if you're in a #GPL lawsuit, you lose your right to even ask for source at all!!!" he observed.&lt;/p&gt;
    &lt;p&gt;Vizio did not immediately respond to a request for comment. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/12/05/vizio_gpl_source_code_ruling/"/><published>2025-12-18T04:27:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46309061</id><title>'Ghost jobs' are on the rise – and so are calls to ban them</title><updated>2025-12-18T10:45:16.223589+00:00</updated><content>&lt;doc fingerprint="6e0191d496ad4df7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Should more be done to tackle 'ghost jobs', vacancies that don't exist?&lt;/head&gt;
    &lt;p&gt;The phrase "ghost jobs" might sound like something from Halloween, but it refers to the practice of employers advertising vacancies that don't exist.&lt;/p&gt;
    &lt;p&gt;In some cases the positions may have already been filled, but in others the job might not have ever been available.&lt;/p&gt;
    &lt;p&gt;It's a real and continuing problem on both sides of the Atlantic.&lt;/p&gt;
    &lt;p&gt;Up to 22% of jobs advertised online last year were positions listed with no intent to hire, according to a study across the US, UK and Germany by recruitment software provider Greenhouse.&lt;/p&gt;
    &lt;p&gt;A separate UK study put the figure even higher, at 34%.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the most recent official data from the US Bureau of Labor Statistics shows that while there were 7.2 million job vacancies back in August, only 5.1 million people were hired.&lt;/p&gt;
    &lt;p&gt;Why are firms posting ghost jobs, and what is being done to tackle the problem?&lt;/p&gt;
    &lt;p&gt;In the US, a jobhunting tech worker called Eric Thompson is making politicians in Washington DC increasingly aware of the issue.&lt;/p&gt;
    &lt;p&gt;In October of last year Mr Thompson, who has more than 20 years of experience in the tech sector, was made redundant from a start-up. He spent the following two months unsuccessfully applying for hundreds of jobs.&lt;/p&gt;
    &lt;p&gt;"I looked at everything under the sun, applying for positions at my current level, and ones that were more senior and junior," he says.&lt;/p&gt;
    &lt;p&gt;It dawned on Mr Thompson that some of the advertised jobs simply didn't exist. The experience led him to set up a working group calling for legislation to ban the practice of fake job adverts in the US.&lt;/p&gt;
    &lt;p&gt;Continuing to meet with members of the US Congress, he has led the formulation of proposed legislation called The Truth in Job Advertising &amp;amp; Accountability Act.&lt;/p&gt;
    &lt;p&gt;This calls for expiration dates for listings when hiring is paused or completed, auditable hiring records, and penalties for employers who post misleading or non-existent roles. Mr Thompson hopes that some members of Congress will sponsor the legislation.&lt;/p&gt;
    &lt;p&gt;He has also started a petition, which has so far generated over 50,000 signatures. Alongside the signatures, he says he receives messages from people describing how ghost jobs have chipped away at their confidence and impacted their mental health. Something he describes as "shameful".&lt;/p&gt;
    &lt;p&gt;The New Jersey and California state legislatures are also looking at banning ghost jobs.&lt;/p&gt;
    &lt;p&gt;The Canadian province of Ontario, is however, leading the way. As from 1 January companies will have to disclose whether an advertised vacancy is actively being filled.&lt;/p&gt;
    &lt;p&gt;Ontario is also moving to tackle the separate recruitment issue of "ghosting", whereby companies don't reply to applicants. Firms in the province with more than 25 employees will now have to reply to someone they have interviewed with 45 days. However, they still won't need to contact anyone they didn't chose to interview.&lt;/p&gt;
    &lt;p&gt;Deborah Hudson, an employment lawyer based in Toronto, says she's already been approached by companies "trying to get it right". But she has concerns about how the rules will be enforced.&lt;/p&gt;
    &lt;p&gt;"My cynical side, after almost 20 years in this field, wonders how they're actually going to monitor and regulate this. I don't think the government has the resources to investigate, so employers may still get away with noncompliance. But if people run into problems, they can make a complaint and it will be looked into."&lt;/p&gt;
    &lt;p&gt;Elsewhere in Canada, and in the US and UK there is no legal requirement to reply to candidates. Nor are there any current moves in the UK to tackle either ghost jobs or recruitment ghosting.&lt;/p&gt;
    &lt;p&gt;Ailish Davies, a jobseeker from Leicester in the UK, says that being ghosted by the small firms and big corporations alike is "soul destroying".&lt;/p&gt;
    &lt;p&gt;She adds: "The amount of time I've spent putting effort into tailoring an application, to hear nothing back, it knocks you down."&lt;/p&gt;
    &lt;p&gt;Ms Davies, who has been working in marketing for more than 10 years, describes one occasion where a hiring manager asked for her availability for an interview, and after she replied, she never heard back.&lt;/p&gt;
    &lt;p&gt;"Employers should treat job seekers with more compassion because the current job market is not a nice place to be."&lt;/p&gt;
    &lt;p&gt;Jasmine Escalera is a career coach and recruitment expert based in Miami.&lt;/p&gt;
    &lt;p&gt;She first became aware of ghost jobs through the women she coached. "They kept seeing the same job posted again and again, and asking me if they should reapply.&lt;/p&gt;
    &lt;p&gt;"They were applying into a black hole. The morale of any job seeker gets crushed."&lt;/p&gt;
    &lt;p&gt;So why are companies posting ghost jobs? Dr Escalera's research suggests a variety of reasons.&lt;/p&gt;
    &lt;p&gt;"We surveyed hiring managers, and found some companies post positions to create a talent pool," she says. "It isn't that they don't want to hire, it's more they're not hiring immediately.&lt;/p&gt;
    &lt;p&gt;"Others, we found, were inflating numbers and trying to show their company is growing, even if it's not."&lt;/p&gt;
    &lt;p&gt;Dr Escalera adds that she has also heard examples of companies posting jobs to obtain and sell data.&lt;/p&gt;
    &lt;p&gt;Whatever the reason for the fake adverts, Dr Escalera cautions that it is giving governments a false picture of job markets, which has negative, real-world consequences.&lt;/p&gt;
    &lt;p&gt;"We use data to develop policy and understand what market trends look like, and so if that data is somehow skewed, then we're not able to create the policies or provide the support that job seekers and employees need right now," she says.&lt;/p&gt;
    &lt;p&gt;For jobhunters hoping to avoid ghost jobs, Dr Escalera advises that they try to network with hiring managers.&lt;/p&gt;
    &lt;p&gt;"You will know a position is real if you're having conversations with real humans who work at that organisations," she says.&lt;/p&gt;
    &lt;p&gt;But, she adds, you should also look for red flags. "If you see that a job is being posted multiple times during a certain time frame, or that the job posting has been open for a while, then it is possible the posting is staying open because the job is not intended to be filled."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/clyzvpp8g3vo"/><published>2025-12-18T05:06:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46309124</id><title>More than half of researchers now use AI for peer review, often against guidance</title><updated>2025-12-18T10:45:15.321193+00:00</updated><content>&lt;doc fingerprint="af87911e80271988"&gt;
  &lt;main&gt;
    &lt;p&gt;Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.&lt;/p&gt;
    &lt;p&gt;More than 50% of researchers have used artificial intelligence while peer reviewing manuscripts, according to a survey of some 1,600 academics across 111 countries by the publishing company Frontiers.&lt;/p&gt;
    &lt;p&gt;“It’s good to confront the reality that people are using AI in peer-review tasks,” says Elena Vicario, Frontiers’ director of research integrity. But the poll suggests that researchers are using AI in peer review “in contrast with a lot of external recommendations of not uploading manuscripts to third-party tools”, she adds.&lt;/p&gt;
    &lt;p&gt;Some publishers, including Frontiers, allow limited use of AI in peer review, but require reviewers to disclose it. Like most other publishers, Frontiers forbids reviewers from uploading unpublished manuscripts to chatbot websites because of concerns about confidentiality, sensitive data and compromising authors’ intellectual property.&lt;/p&gt;
    &lt;p&gt;The survey report calls on publishers to respond to the growing use of AI across scientific publishing and implement policies that are better suited to the ‘new reality’. Frontiers itself has launched an in-house AI platform for peer reviewers across all of its journals. “AI should be used in peer review responsibly, with very clear guides, with human accountability and with the right training,” says Vicario.&lt;/p&gt;
    &lt;p&gt;“We agree that publishers can and should proactively and robustly communicate best practices, particularly disclosure requirements that reinforce transparency to support responsible AI use,” says a spokesperson for the publisher Wiley, which is based in Hoboken, New Jersey. In a similar surveypublished earlier this year, Wiley found that “researchers have relatively low interest and confidence in AI use cases for peer review,” they add. “We are not seeing anything in our portfolio that contradicts this.”&lt;/p&gt;
    &lt;p&gt;Checking, searching and summarizing&lt;/p&gt;
    &lt;p&gt;The Frontiers’ survey found that, among the respondents who use AI in peer review, 59% use it to help write their peer-review reports. Twenty-nine per cent said they use it to summarize the manuscript, identify gaps or check references. And 28% use AI to flag potential signs of misconduct, such as plagiarism and image duplication (see ‘AI assistance’).&lt;/p&gt;
    &lt;p&gt;Mohammad Hosseini, who studies research ethics and integrity at Northwestern University Feinberg School of Medicine in Chicago, Illinois, says the survey is “a good attempt to gauge the acceptability of the use of AI in peer review and the prevalence of its use in different contexts”.&lt;/p&gt;
    &lt;p&gt;Some researchers are running their own tests to determine how well AI models support peer review. Last month, engineering scientist Mim Rahimi at the University of Houston in Texas designed an experiment to test whether the large language model (LLM) GPT-5 could review a Nature Communications paper1 he co-authored.&lt;/p&gt;
    &lt;p&gt;He used four different set-ups, from entering basic prompts asking the LLM to review the paper without additional context to providing it with research articles from the literature to help it to evaluate his paper’s novelty and rigour. Rahimi then compared the AI-generated output with the actual peer-review reports that he had received from the journal, and discussed his findings in a YouTube video.&lt;/p&gt;
    &lt;p&gt;His experiment showed that GPT-5 could mimic the structure of a peer-review report and use polished language, but that it failed to produce constructive feedback and made factual errors. Even advanced prompts did not improve the AI’s performance — in fact, the most complex set-up generated the weakest peer review. Another study found that AI-generated reviews of 20 manuscripts tended to match human ones but fell short on providing detailed critique.&lt;/p&gt;
    &lt;p&gt;Enjoying our latest content? Log in or create an account to continue&lt;/p&gt;
    &lt;p&gt;Access the most recent journalism from Nature's award-winning team&lt;/p&gt;
    &lt;p&gt;Explore the latest features &amp;amp; opinion covering groundbreaking research&lt;/p&gt;
    &lt;p&gt;Correction 16 December 2025: An earlier version of this article incorrectly stated that Frontiers’ AI tool for peer reviewers offers only a limited set of prompts.&lt;/p&gt;
    &lt;p&gt;References&lt;/p&gt;
    &lt;p&gt;Hassan, A., Afshari, M. &amp;amp; Rahimi, M. Nature Commun.16, 6333 (2025).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nature.com/articles/d41586-025-04066-5"/><published>2025-12-18T05:20:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46309407</id><title>Egyptian Hieroglyphs: Lesson 1</title><updated>2025-12-18T10:45:08.707369+00:00</updated><content>&lt;doc fingerprint="ef8d98be2ed9e3d5"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Egyptian Hieroglyphs: Lesson 1&lt;/head&gt;
    &lt;head rend="h2"&gt;General Overview&lt;/head&gt;
    &lt;head rend="h3"&gt;Reading Hieroglyphs&lt;/head&gt;
    &lt;p&gt;The ancient Egyptians created a highly flexible hieroglyphic system of writing. Hieroglyphs could be arranged in both columns and rows and could be read from the left or from the right, depending on how they were written. This allowed the ancient Egyptians to effortlessly integrate their writing with art, blurring the boundary between art and script. In the example below, the god Amun, imn, is written each of the possible combinations.&lt;/p&gt;
    &lt;p&gt;Left to Right&lt;/p&gt;
    &lt;p&gt;Right to Left&lt;/p&gt;
    &lt;p&gt;Left to Right&lt;/p&gt;
    &lt;p&gt;Right to Left&lt;/p&gt;
    &lt;p&gt;Although the task of reading hieroglyphs from the right direction may seem daunting at first, there is a simple trick that will allow you to easily identify the correct direction from which to begin:&lt;/p&gt;
    &lt;p&gt;Look for a hieroglyph with a face and read toward it. &lt;/p&gt;
    &lt;p&gt;When the figure is facing to the left, begin reading from the left. If they are facing right, begin from the right. When there are hieroglyphs are stacked on top of each other, the top sign should always be read before lower sign. Another feature of the Egyptian writing system that you might have noticed is “group writing.” Rather than placing hieroglyphs side-by-side, they were arranged in a way to reduce empty space: taller signs stand alone, while smaller signs are stacked on top of each other.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transliteration and the Alphabet&lt;/head&gt;
    &lt;p&gt;Transliteration is the method of converting one script into another, also referred to as transcription. The hieroglyphs in the alphabet are called “uniliterals,” because they represent a single consonant. The ancient Egyptian language also contains biliterals and triliterals, which represent two and three consonants, respectively. The alphabet can be found in the chart below.&lt;/p&gt;
    &lt;p&gt;It will be helpful to memorize the alphabet not only because they occur often in texts, but also because Egyptologists arrange dictionaries in this order. So if you are unsure of the meaning of a word, but know how to transliterate it, knowing the alphabet will help you find the word faster than flipping frantically between the four different types of “H’s” to find the right one.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Alphabet&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Hieroglyph&lt;/cell&gt;
        &lt;cell&gt;Transliteration&lt;/cell&gt;
        &lt;cell&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓄿&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Vulture&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓇋&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;i, j&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Reed leaf&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓇋𓇋&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;y&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Double Reed Leaf&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓂝&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;ʿ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Arm&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓅱 𓏲&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;w&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Quail chick, rope curl&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓃀&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;b&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Foot&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓊪&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;p&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Stool&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓆑&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;f&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Horned viper&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓅓 𓐝&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;m&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Owl&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓈖 𓋔&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;n&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Water, red crown&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓂋&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;r&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Mouth&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓉔&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;h&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;House plan&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓎛&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;ḥ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Rope&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓐍&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;ḫ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Unknown&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓄡&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;ẖ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Animal belly and tail&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓊃&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;z&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Door bolt&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓋴&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Folded cloth&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓈙&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;š&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Pool&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓈎&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;q&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Hill&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓎡&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;k&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Basket with handle&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓎼&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;g&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Stand for vessel&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓏏&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;t&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Bread loaf&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓍿&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;ṯ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Tethering rope&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓂧&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;d&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Hand&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;𓆓&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;ḏ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Cobra&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You might have noticed that there aren’t any vowels in the alphabet. They exist in the language, but the hieroglyphic script omitted them, which makes things difficult when it comes to pronunciation.&lt;/p&gt;
    &lt;p&gt;There’s a lot of current research into pronunciation (e.g. Allen’s Ancient Egyptian Phonology), but I was never particularly interested in it. If you are, then I suggest grabbing the book I listed, but it’s far beyond the scope of these lessons. For our purposes, I’ll simply describe the general conventions used in an introductory glyphs course.&lt;/p&gt;
    &lt;p&gt;Add an “e” in between the consonants.&lt;/p&gt;
    &lt;p&gt;Yup. That’s the basic approach when vocalizing transliterations. There’s some leeway when it comes to names and places (e.g. Amon vs Amun vs Amen), but in general, adding “e” in between consonants is just fine.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pronunciation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Glyph&lt;/cell&gt;
        &lt;cell&gt;Translit.&lt;/cell&gt;
        &lt;cell&gt;Pronunciation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓄿&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;“ah” as in “yacht”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓇋&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;i, j&lt;/cell&gt;
        &lt;cell&gt;“ee” as in “feet”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓇋𓇋&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;y&lt;/cell&gt;
        &lt;cell&gt;“ee” as in “feet”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓂝&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;ʿ&lt;/cell&gt;
        &lt;cell&gt;“ah” as in “yacht”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓅱 𓏲&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;w&lt;/cell&gt;
        &lt;cell&gt;“oo” as in “blue”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓃀&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;b&lt;/cell&gt;
        &lt;cell&gt;“b” as in “bed”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓊪&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;“p” as in “pet”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓆑&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;f&lt;/cell&gt;
        &lt;cell&gt;“f” as in “fish”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓅓 𓐝&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;m&lt;/cell&gt;
        &lt;cell&gt;“m” as in “map”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓈖 𓋔&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;n&lt;/cell&gt;
        &lt;cell&gt;“n” as in “neat”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;𓂋&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;r&lt;/cell&gt;
        &lt;cell&gt;“r” as in “ready”&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Glyph&lt;/cell&gt;
        &lt;cell&gt;Translit.&lt;/cell&gt;
        &lt;cell&gt;Pronunciation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓉔&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;h&lt;/cell&gt;
        &lt;cell&gt;“h” as in “hat”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓎛&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;ḥ&lt;/cell&gt;
        &lt;cell&gt;“h” as in “hat”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓐍&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;ḫ&lt;/cell&gt;
        &lt;cell&gt;“kh” as in Bach&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓄡&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;ẖ&lt;/cell&gt;
        &lt;cell&gt;“kyah” similar to the preceding sound&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓊃&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;z&lt;/cell&gt;
        &lt;cell&gt;“s” as in “sand”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓋴&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;s&lt;/cell&gt;
        &lt;cell&gt;“s” as in “sand”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓈙&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;š&lt;/cell&gt;
        &lt;cell&gt;“sh” as in “fish”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓈎&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;q&lt;/cell&gt;
        &lt;cell&gt;“k” as in “kite”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓎡&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;k&lt;/cell&gt;
        &lt;cell&gt;“k” as in “kite”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓎼&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;g&lt;/cell&gt;
        &lt;cell&gt;“g” as in “girl”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓏏&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;t&lt;/cell&gt;
        &lt;cell&gt;“t” as in “tape”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓍿&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;ṯ&lt;/cell&gt;
        &lt;cell&gt;“tch” as in “chart”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓂧&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;d&lt;/cell&gt;
        &lt;cell&gt;“d” as in “dog”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;𓆓&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;ḏ&lt;/cell&gt;
        &lt;cell&gt;“dj” as in “sledge”&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Pronunciation Examples&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Word&lt;/cell&gt;
        &lt;cell&gt;Transliteration&lt;/cell&gt;
        &lt;cell&gt;Pronunciation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓄔𓅓&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;sḏm&lt;/cell&gt;
        &lt;cell&gt;sedjem&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓄔𓅓𓆑&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;sḏm.f&lt;/cell&gt;
        &lt;cell&gt;sedjemef&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓁹𓋴&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;ir.s&lt;/cell&gt;
        &lt;cell&gt;ires (ear-ess)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓊹𓀭&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;nṯr&lt;/cell&gt;
        &lt;cell&gt;netcher&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;𓊹𓊹𓊹&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;nṯrw&lt;/cell&gt;
        &lt;cell&gt;netcheroo&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Phonograms, Ideograms, and Determinatives&lt;/head&gt;
    &lt;p&gt;When hieroglyphs are used to represent these real world things, they are called ideograms. For example, the hieroglyph&lt;/p&gt;
    &lt;p&gt;You might be wondering how you’d know whether a hieroglyph was being used as a ideogram or not.&lt;/p&gt;
    &lt;p&gt;Phonograms are hieroglyphs that represent a specific sound (phonetic value). Using phonograms, scribes could spell out words. For example, we could combine the hieroglyphs for mouth,&lt;/p&gt;
    &lt;p&gt;A determinative is a hieroglyph that does not have a phonetic value so it is not transliterated. They are placed at the end of words and provide a general meaning of the word. For example, we may not know what the word&lt;/p&gt;
    &lt;p&gt;Ideogram&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;p&gt;Phonogram&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;p&gt;Determinative&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;p&gt;General meaning: movement.&lt;/p&gt;
    &lt;p&gt;Actual definiton of ptpt: to trample.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memorization&lt;/head&gt;
    &lt;p&gt;This section will contain either a vocabulary list, sign list, or both. Although it would be great if you memorized the lists, it’s not required at this point. The more you interact with hieroglyphic texts, in these exercises or elsewhere, the more you’ll begin to notice some of the more common signs and words. After a while, you’ll begin to naturally remember them.&lt;/p&gt;
    &lt;p&gt;Or you’ll get tired of looking them up and memorize them on the spot. Flashcards are great for that!&lt;/p&gt;
    &lt;head rend="h4"&gt;Biliterals&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓁹&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;𓏇&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;𓏠&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;𓎟&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;𓁷&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;𓏞&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ir&lt;/cell&gt;
        &lt;cell&gt;mi&lt;/cell&gt;
        &lt;cell&gt;mn&lt;/cell&gt;
        &lt;cell&gt;nb&lt;/cell&gt;
        &lt;cell&gt;ḥr&lt;/cell&gt;
        &lt;cell&gt;sš&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;triliterals&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;𓋹&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;𓄤&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;𓊹&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ʿnḫ&lt;/cell&gt;
        &lt;cell&gt;nfr&lt;/cell&gt;
        &lt;cell&gt;nṯr&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Exercises&lt;/head&gt;
    &lt;p&gt;Exercise 1.&lt;/p&gt;
    &lt;p&gt;Identify the direction the hieroglyphs should be read (right to left or left to right), and then in your head, identify the order in which each individual glyph should be read. If you need help, refer to the “Reading Hieroglyphs” section.&lt;/p&gt;
    &lt;p&gt;Transliteration and Translation&lt;/p&gt;
    &lt;p&gt;gm.n.f sw m pr&lt;/p&gt;
    &lt;p&gt;He found him in the house.&lt;/p&gt;
    &lt;p&gt;Left to Right&lt;/p&gt;
    &lt;p&gt;2.&lt;/p&gt;
    &lt;p&gt;Transliteration and Translation&lt;/p&gt;
    &lt;p&gt;nswt bity nb t3wy nb hʿw nb-m3ʿt-rʿ&lt;/p&gt;
    &lt;p&gt;King of Upper and Lower Egypt, Lord of the Two Lands, Lord of Diadems, Nebmaatre (Amenhotep III)&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Right to left.&lt;/p&gt;
    &lt;p&gt;The last sequence of hieroglyphs might be confusing. Why 13, 11, and 12 and not 11-13? This is a common feature called “honorific transposition” and often found in names. When a certain god or goddess is being honored, their name is pushed to the front. In this case, Re is being honored, so his name is at the head. We’ll learn more about this feature in the next lesson.&lt;/p&gt;
    &lt;p&gt;Transliteration and Translation&lt;/p&gt;
    &lt;p&gt;sḏm.n.f mdt imn&lt;/p&gt;
    &lt;p&gt;He heard the speech of Amun.&lt;/p&gt;
    &lt;p&gt;4.&lt;/p&gt;
    &lt;p&gt;Transliteration and Translation&lt;/p&gt;
    &lt;p&gt;ḥmt nswt tiy&lt;/p&gt;
    &lt;p&gt;Royal wife, Tiye&lt;/p&gt;
    &lt;p&gt;Left to right.&lt;/p&gt;
    &lt;p&gt;This one is kinda mean, unless you remembered that the sedge (see problem #1 above) is transliterated as nswt. Although you’ve been introduced to honorific transposition, I haven’t mentioned it applies to nswt, among others.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.egyptianhieroglyphs.net/egyptian-hieroglyphs/lesson-1/"/><published>2025-12-18T06:08:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46309571</id><title>What is an elliptic curve? (2019)</title><updated>2025-12-18T10:45:08.530707+00:00</updated><content>&lt;doc fingerprint="944dda954a133365"&gt;
  &lt;main&gt;
    &lt;p&gt;Elliptic curves are pure and applied, concrete and abstract, simple and complex.&lt;/p&gt;
    &lt;p&gt;Elliptic curves have been studied for many years by pure mathematicians with no intention to apply the results to anything outside math itself. And yet elliptic curves have become a critical part of applied cryptography.&lt;/p&gt;
    &lt;p&gt;Elliptic curves are very concrete. There are some subtleties in the definition—more on that in a moment—but they’re essentially the set of points satisfying a simple equation. And yet a lot of extremely abstract mathematics has been developed out of necessity to study these simple objects. And while the objects are in some sense simple, the questions that people naturally ask about them are far from simple.&lt;/p&gt;
    &lt;head rend="h2"&gt;Preliminary definition&lt;/head&gt;
    &lt;p&gt;A preliminary definition of an elliptic curve is the set of points satisfying&lt;/p&gt;
    &lt;p&gt;y² = x³ + ax + b.&lt;/p&gt;
    &lt;p&gt;This is a theorem, not a definition, and it requires some qualifications. The values x, y, a, and b come from some field, and that field is an important part of the definition of an elliptic curve. If that field is the real numbers, then all elliptic curves do have the form above, known as the Weierstrass form. For fields of characteristic 2 or 3, the Weierstrass form isn’t general enough. Also, we require that&lt;/p&gt;
    &lt;p&gt;4a³ + 27b² ≠ 0.&lt;/p&gt;
    &lt;p&gt;The other day I wrote about Curve1174, a particular elliptic curve used in cryptography. The points on this curve satisfy&lt;/p&gt;
    &lt;p&gt;x² + y² = 1 – 1174 x² y²&lt;/p&gt;
    &lt;p&gt;This equation does not specify an elliptic curve if we’re working over real numbers. But Curve1174 is defined over the integers modulo p = 2251 – 9. There it is an elliptic curve. It is equivalent to a curve in Weierstrass, though that’s not true when working over the reals. So whether an equation defines an elliptic curve depends on the field the constituents come from.&lt;/p&gt;
    &lt;head rend="h2"&gt;Not an ellipse, not a curve&lt;/head&gt;
    &lt;p&gt;An elliptic curve is not an ellipse, and it may not be a curve in the usual sense.&lt;/p&gt;
    &lt;p&gt;There is a connection between elliptic curves and ellipses, but it’s indirect. Elliptic curves are related to the integrals you would write down to find the length of a portion of an ellipse.&lt;/p&gt;
    &lt;p&gt;Working over the real numbers, an elliptic curve is a curve in the geometric sense. Working over a finite field, an elliptic curve is a finite set of points, not a continuum. Working over the complex numbers, an elliptic curve is a two-dimensional surface. The name “curve” is extended by analogy to elliptic curves over general fields.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final definition&lt;/head&gt;
    &lt;p&gt;In this section we’ll give the full definition of an algebraic curve, though we’ll be deliberately vague about some of the details.&lt;/p&gt;
    &lt;p&gt;The definition of an elliptic curve is not in terms of equations of a particular form. It says an elliptic curve is a&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;smooth,&lt;/item&gt;
      &lt;item&gt;projective,&lt;/item&gt;
      &lt;item&gt;algebraic curve,&lt;/item&gt;
      &lt;item&gt;of genus one,&lt;/item&gt;
      &lt;item&gt;having a specified point O.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Working over real numbers, smoothness can be specified in terms of derivatives. But what does smoothness mean working over a finite field? You take the derivative equations from the real case and extend them by analogy to other fields. You can “differentiate” polynomials in settings where you can’t take limits by defining derivatives algebraically. (The condition 4a³ + 27b² ≠ 0 above is to guarantee smoothness.)&lt;/p&gt;
    &lt;p&gt;Informally, projective means we add “points at infinity” as necessary to make things more consistent. Formally, we’re not actually working with pairs of coordinates (x, y) but equivalence classes of triples of coordinates (x, y, z). You can usually think in terms of pairs of values, but the extra value is there when you need it to deal with points at infinity. More on that here.&lt;/p&gt;
    &lt;p&gt;An algebraic curve is the set of points satisfying a polynomial equation.&lt;/p&gt;
    &lt;p&gt;The genus of an algebraic curve is roughly the number of holes it has. Over the complex numbers, the genus of an algebraic curve really is the number of holes. As with so many ideas in algebra, a theorem from a familiar context is taken as a definition in a more general context.&lt;/p&gt;
    &lt;p&gt;The specified point O, often the point at infinity, is the location of the identity element for the group addition. In the post on Curve1174, we go into the addition in detail, and the zero point is (0, 1).&lt;/p&gt;
    &lt;p&gt;In elliptic curve cryptography, it’s necessary to specify another point, a base point, which is the generator for a subgroup. This post gives an example, specifying the base point on secp256k1, a curve used in the implementation of Bitcoin.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.johndcook.com/blog/2019/02/21/what-is-an-elliptic-curve/"/><published>2025-12-18T06:40:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46310104</id><title>RCE via ND6 Router Advertisements in FreeBSD</title><updated>2025-12-18T10:45:08.158559+00:00</updated><content>&lt;doc fingerprint="b5bc3d1b6c816b83"&gt;
  &lt;main&gt;-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA512
=============================================================================
FreeBSD-SA-25:12.rtsold Security Advisory
The FreeBSD Project
Topic: Remote code execution via ND6 Router Advertisements
Category: core
Module: rtsold
Announced: 2025-12-16
Credits: Kevin Day
Affects: All supported versions of FreeBSD.
Corrected: 2025-12-16 23:39:32 UTC (stable/15, 15.0-STABLE)
2025-12-16 23:43:01 UTC (releng/15.0, 15.0-RELEASE-p1)
2025-12-16 23:45:05 UTC (stable/14, 14.3-STABLE)
2025-12-16 23:43:25 UTC (releng/14.3, 14.3-RELEASE-p7)
2025-12-16 23:44:10 UTC (stable/13, 13.4-STABLE)
2025-12-16 23:43:33 UTC (releng/13.5, 13.5-RELEASE-p8)
CVE Name: CVE-2025-14558
For general information regarding FreeBSD Security Advisories,
including descriptions of the fields above, security branches, and the
following sections, please visit .
I. Background
rtsold(8) and rtsol(8) are programs which process router advertisement
packets as part of the IPv6 stateless address autoconfiguration (SLAAC)
mechanism.
II. Problem Description
The rtsol(8) and rtsold(8) programs do not validate the domain search list
options provided in router advertisement messages; the option body is passed
to resolvconf(8) unmodified.
resolvconf(8) is a shell script which does not validate its input. A lack of
quoting meant that shell commands pass as input to resolvconf(8) may be
executed.
III. Impact
Systems running rtsol(8) or rtsold(8) are vulnerable to remote code execution
from systems on the same network segment. In particular, router advertisement
messages are not routable and should be dropped by routers, so the attack does
not cross network boundaries.
IV. Workaround
No workaround is available. Users not using IPv6, and IPv6 users that do not
configure the system to accept router advertisement messages, are not affected.
A network interface listed by ifconfig(8) accepts router advertisement messages
if the string "ACCEPT_RTADV" is present in the nd6 option list.
V. Solution
Upgrade your vulnerable system to a supported FreeBSD stable or
release / security branch (releng) dated after the correction date.
Perform one of the following:
1) To update your vulnerable system via a binary patch:
Systems running a RELEASE version of FreeBSD on the amd64 or arm64 platforms,
or the i386 platform on FreeBSD 13, can be updated via the freebsd-update(8)
utility:
# freebsd-update fetch
# freebsd-update install
2) To update your vulnerable system via a source code patch:
The following patches have been verified to apply to the applicable
FreeBSD release branches.
a) Download the relevant patch from the location below, and verify the
detached PGP signature using your PGP utility.
# fetch https://security.FreeBSD.org/patches/SA-25:12/rtsold.patch
# fetch https://security.FreeBSD.org/patches/SA-25:12/rtsold.patch.asc
# gpg --verify rtsold.patch.asc
b) Apply the patch. Execute the following commands as root:
# cd /usr/src
# patch &amp;lt; /path/to/patch
c) Recompile the operating system using buildworld and installworld as
described in .
Restart the applicable daemons, or reboot the system.
VI. Correction details
This issue is corrected as of the corresponding Git commit hash in the
following stable and release branches:
Branch/path Hash Revision
- -------------------------------------------------------------------------
stable/15/ 6759fbb1a553 stable/15-n281548
releng/15.0/ 408f5c61821f releng/15.0-n280998
stable/14/ 26702912e857 stable/14-n273051
releng/14.3/ 3c54b204bf86 releng/14.3-n271454
stable/13/ 4fef5819cca9 stable/13-n259643
releng/13.5/ 35cee6a90119 releng/13.5-n259186
- -------------------------------------------------------------------------
Run the following command to see which files were modified by a
particular commit:
# git show --stat
Or visit the following URL, replacing NNNNNN with the hash:
To determine the commit count in a working tree (for comparison against
nNNNNNN in the table above), run:
# git rev-list --count --first-parent HEAD
VII. References
The latest revision of this advisory is available at
-----BEGIN PGP SIGNATURE-----
iQIzBAEBCgAdFiEEthUnfoEIffdcgYM7bljekB8AGu8FAmlB+cMACgkQbljekB8A
Gu9YXA//UpSYz4dseSTcDElpN6jp/2W0+OKDYVqRkH0PaLwZX8iGugm8QwqCxLoL
m1xK2BJir15wuUYmD++EYbjHajXrKIPaD+sW9KjqxgxDVsQWwfl9ZND743JM5TFE
Y3fx8halkChIwtNGCNDHTu5N2DmEPoTO03jOqKqjH6PZwJ6ycYTw4zJvPdP5eDiT
+zWpTNNm0VCkBQQB7ukJGku3zWAh4swZWylP2GvyzifcYKR3Z4OGhDdwQCBa99cn
jC67D7vURTqlk4pcTFJ6JrIVRIQJdNWQGRou3hAedE59bpAZZc8B/fd//Ganmrit
CBG1kMLYVxtV3/12+maEt/DLEMM7isGJPQiSWYe+qseBcdakmuJ8hdR8HKTqrK40
57ZO59CnzEFr49DrrTD4B97cJwtrXLWtUp4LiXxuYy0CkCl8CiXvcgovCBusQpx+
r68dgbfcH0UY/ryQp0ZWTI1y3NKmOSuPVpkW4Ss0BeGESlA4DJHuEwIs1D4TnOJL
90C5D7v7jeOtdXhZ6BHVLtXB+nn8zMpAO209H/pRQWJdAEpABheKCgisP9C80g6h
kM300GZjH4joYDyFbMYrW6uWfylwDFC1g8MdFi8yjZzEEOfrKNcY63b+Kx+c3xNL
hIa8yUcjLYHvMRnjTQU1bgUVU+SmW6n05HcqtWV7VKh39ATJcX4=
=TK7t
-----END PGP SIGNATURE-----&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.freebsd.org/security/advisories/FreeBSD-SA-25:12.rtsold.asc"/><published>2025-12-18T08:12:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46310146</id><title>GitHub Actions for Self-Hosted Runners Price Increase Postponed</title><updated>2025-12-18T10:45:07.967165+00:00</updated><content>&lt;doc fingerprint="8dc5214d4f9242e1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GitHub Actions for Self-Hosted Runners Price Increase Postponed&lt;/head&gt;
    &lt;p&gt;GitHub update on the postponement of the price increase for GitHub Actions for self-hosted runners to $0.002 per minute. Below is a screenshot of the update shared on X (@github).&lt;/p&gt;
    &lt;p&gt;GitHub had earlier announced in a post we published that it would increase the cost of self-hosted runners from $0.00 (free) to $0.002 per minute, starting March 1, 2026. This increase has been postponed, which means it will not apply until a new decision is made.&lt;/p&gt;
    &lt;p&gt;GitHub stated that it has canceled the price increase after reviewing developer feedback. It added that it will take time to listen to customers and partners. For now, GitHub Actions for self-hosted runners remain free.&lt;/p&gt;
    &lt;p&gt;The good news is that the 39% price reduction for hosted runners will continue as previously announced.&lt;/p&gt;
    &lt;p&gt;We will continue to monitor GitHub Actions price changes and update them on GitHub Actions Pricing history page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pricetimeline.com/news/189"/><published>2025-12-18T08:18:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46310650</id><title>Online Textbook for Braid groups and knots and tangles</title><updated>2025-12-18T10:45:07.683526+00:00</updated><content>&lt;doc fingerprint="bd5c832e5e813eb9"&gt;
  &lt;main&gt;
    &lt;p&gt;Skip to main content \( \newcommand{\lt}{&amp;lt;} \newcommand{\gt}{&amp;gt;} \newcommand{\amp}{&amp;amp;} \) Open Algebra and Knots: REDOAK Edition Matthew Salomone Contents Prev Up Next Contents Prev Up Next Front Matter 1 Braids and Permutations Anagram Algebra From Braid-A-Grams to Braids Invariants for Braids Braids and Knots 2 Rational Tangles The Rational Tangle Dance Tangles and Continued Fractions Rational Knots and Conway Notation Some Objectives and Videos 3 Knots and Links A Three-Color Problem Knot Diagrams and Invariants Coloration and the Alexander Polynomial Some Objectives and Videos Resources A GNU Free Documentation License Authored in PreTeXt Front Matter 1 Braids and Permutations 2 Rational Tangles 3 Knots and Links Resources&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://matthematics.com/redoak/redoak.html"/><published>2025-12-18T09:40:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46310823</id><title>How getting richer made teenagers less free</title><updated>2025-12-18T10:45:07.421061+00:00</updated><content>&lt;doc fingerprint="3e53814d9fc6875b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How getting richer made teenagers less free&lt;/head&gt;
    &lt;head rend="h3"&gt;We value children more than ever. But we're suffocating them.&lt;/head&gt;
    &lt;p&gt;In 1913, journalist Helen Todd talked to hundreds of 14- to 16-year-olds working in American factories. Most of their fathers were dead or had crippling health issues thanks to decades of work in unsafe factories, and their mothers were supporting an average of five children on low wages. By doing piecemeal work for excruciatingly low pay in dangerous factories, the teenagers were keeping their families afloat.&lt;/p&gt;
    &lt;p&gt;Todd asked these teenage laborers whether they would choose work in the factory or school if their families were rich enough that they didn’t need to work.&lt;/p&gt;
    &lt;p&gt;Overwhelmingly, they chose the factory:&lt;/p&gt;
    &lt;p&gt;“The children don’t holler at ye and call ye a Christ-killer in a factory.”&lt;/p&gt;
    &lt;p&gt;“They don’t call ye a Dago.”&lt;/p&gt;
    &lt;p&gt;“They’re good to you at home when you earn money.”&lt;/p&gt;
    &lt;p&gt;“Youse can eat sittin’ down, when youse work.”&lt;/p&gt;
    &lt;p&gt;“You can go to the nickel show.”&lt;/p&gt;
    &lt;p&gt;“You don’t have to work so hard at night when you get home.”&lt;/p&gt;
    &lt;p&gt;“Yer folks don’t hit ye so much.”&lt;/p&gt;
    &lt;p&gt;“You can buy shoes for the baby.”&lt;/p&gt;
    &lt;p&gt;“You can give your mother yer pay envelop.”&lt;/p&gt;
    &lt;p&gt;“What ye learn in school ain’t no’ good. Ye git paid just as much in the factory if ye never was there. Our boss he never went to school.”&lt;/p&gt;
    &lt;p&gt;“That boy can’t speak English, and he gets six dollars. I only get four dollars, and I’ve been through the sixth grade.”&lt;/p&gt;
    &lt;p&gt;“When my brother is fourteen, I’m going to get him a job here. Then, my mother says, we’ll take the baby out of the ‘Sylum for the Half Orphans.”&lt;/p&gt;
    &lt;p&gt;No one in America today lives under the cloud of desperation that these children did. In the last century, economic growth has transformed our society from every conceivable angle. But one we don’t dwell on much is how it has transformed childhood.&lt;/p&gt;
    &lt;p&gt;In 1910, shortly before these children were interviewed, 16% of American children died before age 5 and 19% before age 18. Just 13.5% of adults had a high school diploma, and the median adult had about an eighth-grade education. The 1910 Census found 41% of 14- to 15-year-old boys gainfully occupied (that is, in the workforce), and 79% of boys and young men aged 16 to 20.&lt;/p&gt;
    &lt;p&gt;Naturally, these changes — in the odds that children survive to adulthood, in the age at which they first work, in how many of them complete high school — have profoundly shaped our conception of childhood.&lt;/p&gt;
    &lt;head rend="h3"&gt;Kids’ lives are better — but involve much less autonomy&lt;/head&gt;
    &lt;p&gt;As the nation grew wealthier and more children began to survive to adulthood, we became vastly more protective of them — and permitted them far fewer risks. It’s hard to invest (either emotionally or literally) in children when poverty, disease, and starvation haunt your days. And now that we are less desperately poor, we can afford to ask less of our children — no family need choose between sending their 14-year-old to the factories or surrendering their baby to an orphanage.&lt;/p&gt;
    &lt;p&gt;Today, legal protections for minors are more expansive than they ever have been. Cultural expectations have shifted enormously. Americans hit their children less than we used to. We spend more time playing with them. We spend, of course, far more money on them. We supervise them more.&lt;/p&gt;
    &lt;p&gt;“The very successes achieved in improving children’s lives led to an escalation in what came to be seen as the minimal standard for children’s well-being,” wrote Peter Stearns in his history of American child-rearing. “Levels of anxiety experienced by parents did not correlate with what might have been registered as historic progress in children’s quality of life.”&lt;/p&gt;
    &lt;p&gt;Obviously, “kids rarely die these days” is a massive change for the good, and I’m also not exactly here to defend children dropping out of middle school to tape labels on cigarettes for six cents per thousand (as one child featured in Todd’s article does).&lt;/p&gt;
    &lt;p&gt;But the same forces that worked to eliminate child labor and exploitation and gave parents more room and incentive to invest in their progeny have also worked to strip children of independence.&lt;/p&gt;
    &lt;p&gt;This month, The Argument polled voters about modern parenting. I found it striking how far our society has pushed back the age at which children are trusted with even the barest autonomy — or, from another angle, how many years we expect parents to dedicate all their time to closely supervising them. (The full crosstabs are available to paying subscribers at the bottom of this post.)&lt;/p&gt;
    &lt;p&gt;We asked “At what age do you think it is appropriate for a child to stay home alone for an hour or two?” To my astonishment, 36% of respondents said that it was not appropriate until “between the ages of 14 and 17.”&lt;/p&gt;
    &lt;p&gt;Are a third of you really refusing to leave your 13-year-olds home alone for a couple hours while you go to the grocery store? Or are those respondents the ones who don’t have children?&lt;/p&gt;
    &lt;p&gt;I asked my colleague Lakshya Jain to break the data down for me, and parents aren’t much different than nonparents here — 37% of parents and 35% of nonparents said it wasn’t appropriate until the child was aged 14 to 17.&lt;/p&gt;
    &lt;p&gt;Or take the responses to another question we asked: “When parents allow a 10-year-old child to play alone in a nearby park for three hours, should they be investigated by Child Protective Services for potential neglect?” Again, 36% of respondents said that they should — and since it only takes one person to make a CPS call, many of your neighbors thinking it’s wildly inappropriate for a child to play alone at the park could amount to an effective ban on doing so.&lt;/p&gt;
    &lt;p&gt;If you don’t have kids, it can be pretty hard to have a good mental picture of what capabilities a 10-year-old has and doesn’t have, so I expect some readers may be adrift in trying to estimate whether this survey result is reasonable or nuts. And 10-year-olds also vary enormously in their maturity and common sense. But I interact with lots of kids, so let me tell you: This is absolutely nuts.&lt;/p&gt;
    &lt;p&gt;Ten-year-olds are way past the age where you have to worry about them running into the road; I would trust the majority of 10-year-olds to play unsupervised for a few hours, and parents deciding whether to allow this have far more knowledge than anyone else of their specific child. Cellphones mean that it’s easy for a kid to contact their parents immediately if something comes up. When I was 10, I babysat for the neighbors, and I was a perfectly adequate babysitter; I think in most U.S. states today, that might be regarded as child neglect.&lt;/p&gt;
    &lt;p&gt;The role of CPS in accelerating this transition to a highly supervised, highly limited childhood is probably underrated. Around 35% of American families have been investigated by CPS. In most of these cases, no maltreatment will be found — only about 1 in 8 families will ever have a finding of maltreatment. But obviously it is terrifying, as a parent, to be investigated, even if you are found to be doing fine; it will naturally heighten the anxiety experienced by parents and lead them to further restrict the activities of their children.&lt;/p&gt;
    &lt;p&gt;If you get CPS called on you for letting your 10-year-old play at the park across the street, you aren’t likely to do it again even if CPS drops the investigation.&lt;/p&gt;
    &lt;p&gt;But despite my trepidations, the very population most likely to have CPS called on them are the ones most likely to support state intervention. Fully 50% of Black voters in our poll agreed that allowing a 10-year-old to play unsupervised at a park for a few hours was grounds for a CPS call. Just 33% of white voters and 37% of Hispanic voters said the same.&lt;/p&gt;
    &lt;p&gt;This might reflect the relatively higher rates of risk faced by Black children who are more likely to be victims of crimes, but the absolute risk is small enough for all children that playing freely in the park clearly passes the cost-benefit sniff test — especially when you consider the alternative.&lt;/p&gt;
    &lt;p&gt;In about a century, we’ve gone from a world where many 14-year-olds are the breadwinners for their family — bad! — to a world where many of them aren’t even trusted to be in the house without a babysitter — also bad!&lt;/p&gt;
    &lt;p&gt;There has to be a middle ground, where we ensure that 14-year-olds don’t permanently foreclose opportunities for their future selves, ensure they all get a good education, and also don’t make them miserable by extending their adolescence a full decade during which they’re cut off from all the parts of the world that offer autonomy and meaning.&lt;/p&gt;
    &lt;p&gt;Many analyses of the consequences of our ethos of extended childhood focus on the indirect effects on the parents. They focus on how the increasing demands of parenthood mean that parents are much less happy, without any corresponding benefit to their children. Or they focus on declining birth rates. These are serious problems, but it’s worth not skipping over the direct impacts on teenagers themselves.&lt;/p&gt;
    &lt;p&gt;Teenagers are allowed to do less and less in the physical world, even as (thanks to technological advancement) they have more and more access to the digital world. I’m not going to recapitulate Jonathan Haidt here; I actually feel very confused about precisely the role that Instagram and TikTok play in the stress and unhappiness of the modern American adolescent. But I feel on much more solid ground saying that the effects of the digital world on our kids is worse when it is the only world they have access to.&lt;/p&gt;
    &lt;p&gt;When teenagers aren’t trusted to walk over to a friend’s house or play in the park, when they almost never have a part-time job where they can earn a paycheck and meet expectations that aren’t purely artificial, then I think it’s much harder for them to have a realistic, non-algorithm-driven worldview and concrete life goals they can work toward.&lt;/p&gt;
    &lt;p&gt;We don’t have good data on teen suicides in 1910 — and it has historically been common for suicides to be recorded as accidents, making it extremely hard to confidently compare across time periods. All the same, I think teen suicides are up. In 1950, there were 2.7 suicides per 100,000 15- to 19-year-olds. Today, there are 7.5 (though that’s down from a 1990s peak of 13.2).&lt;/p&gt;
    &lt;p&gt;No one could possibly examine what it was like to be a teenager a century ago and soberly call for a return to it. Things are better now.&lt;/p&gt;
    &lt;p&gt;Still, it would be a tragedy if the explosion of prosperity that freed our children from labor traps them under increasing supervision and diminished opportunities for meaningful choice and meaningful participation in society. Today’s child endangerment doesn’t come from dangerous machines, high mortality rates, or a lack of K-12 opportunities — it often comes from a lack of agency.&lt;/p&gt;
    &lt;p&gt;In the last 100 years, we gave our children better and safer childhoods. Now it’s time to give them the teen years they deserve.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theargumentmag.com/p/how-getting-richer-made-teenagers"/><published>2025-12-18T10:03:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46310856</id><title>It's all about momentum</title><updated>2025-12-18T10:45:06.973255+00:00</updated><content>&lt;doc fingerprint="644910141921c8b4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;It's all about momentum, innit?&lt;/head&gt;
    &lt;p&gt;In physics and in your life, the only metric you should care about is momentum.&lt;/p&gt;
    &lt;p&gt;I enjoy rally games. What made them truly click for me is understanding that rally driving is all about weight transfer. A car is a spring, and any of your inputs, throttle, brake or steering, unsettles the 1000 kg of steel you're controlling in a particular direction. If you brake too hard, most of the weight moves forward, your steering wheels will have better grip but could easily lock up; the back of the car lifts and sends you spinning. If you steer too hard in one direction, the lateral forces could make you tip over. This phenomenon is especially noticeable when driving older rally cars, which do not have modern tyres and quick-responding engines. You need to remain mindful that you are controlling a hunk of metal moving at large speed, and you need to apply the least amount of force to nudge it in the direction you want. The last thing you want to do is to be abrupt with your inputs and velocity changes.&lt;/p&gt;
    &lt;p&gt;People naturally resist change, and even more so as they get older. We pursue the ideal of being flexible and agile, but that's all aspirational nonsense that is often demanded of us, despite the reality that deep down we are more akin to freight trains, let alone rally cars. Slow to start, slow to change direction, and only once we get going the magic happens.&lt;/p&gt;
    &lt;p&gt;Take the mandated two weeks of yearly holidays, for example. Have they ever been restorative? After a whole year of being immersed in your work, worrying about your daily chores, it's going to take a while to switch off, and a while longer to actually start to get used to the fact that you are in an unfamiliar setting, sleeping in an unknown bed. Why even subject yourself to all this stress for little benefit, one wonders. Your habits and routine condition your momentum, and anything unfamiliar, even if it is sitting on a lovely beach, will feel uncomfortable, and uncomfortable again readjusting to working 40 hours a week.&lt;/p&gt;
    &lt;p&gt;In the past 3 years I have picked up the habit of dedicating the morning hours of 9 to noon for creative, mental work. Every morning starts a blank slate, the stresses of yesterday hopefully digested and integrated into my psyche by sleep. What I quickly learned is that whatever I do in the first hour after waking up will set the tone for the entire day. If I read social media, my head will fill with nonsense I truly don't give a shit about, and will develop into a thirst for quick dopamine which escalates as the day rolls by. Any action, really, will set me in a particular direction and then it's too late to do anything about it. The only thing that has been working for me is to be completely intolerant of any distractions in the morning. Until noon, my phone is silenced. My email client is closed. Social media is blocked on all my devices. My chores and admin work are scheduled for the afternoon. This routine doesn't always work out, urgent matters might waste my precious morning, and it's healthy to accept that I can try and salvage the rest of the day, not to end up scrolling the internet all day.&lt;/p&gt;
    &lt;p&gt;Cal Newport talks about this exact idea in his book about deep work, but deep work is the just the result of being mindful of your momentum, of being the conductor of a freight train. Truly, the difference between your TV-bingeing self and your dream of being a writer has never been about willpower, or practice, or to never have acquired a taste for the liquor. It is all about being extremely jealous of your attention, setting aside time to pursue your craft and changing your whole life around this dream of yours. The train needs enough space to maneuver and to get going. On the other hand, creativity has never been about sheer effort; you don't need much sweating to go far. Consistency is key. Thirty minutes a day are much better than one day a week.&lt;/p&gt;
    &lt;p&gt;I will talk about habits, and capturing high-entropy spurts of creative energy in another post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://combo.cc/posts/its-all-about-momentum-innit/"/><published>2025-12-18T10:09:10+00:00</published></entry></feed>