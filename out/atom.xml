<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-14T08:38:31.559433+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45231852</id><title>My first impressions of Gleam</title><updated>2025-09-14T08:40:52.600523+00:00</updated><content>&lt;doc fingerprint="7c882e31ab6d3836"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;My First Impressions of Gleam&lt;/head&gt;
    &lt;p&gt;I’m looking for a new programming language to learn this year, and Gleam looks like the most fun. It’s an Elixir-like language that supports static typing.&lt;/p&gt;
    &lt;p&gt;I read the language tour, and it made sense to me, but I need to build something before I can judge a programming language well.&lt;/p&gt;
    &lt;p&gt;I’m sharing some notes on my first few hours using Gleam in case they’re helpful to others learning Gleam or to the team developing the language.&lt;/p&gt;
    &lt;head rend="h2"&gt;My project: Parsing old AIM logs 🔗︎&lt;/head&gt;
    &lt;p&gt;I used AOL Instant Messenger from about 1999 to 2007. For most of that time, I used AIM clients that logged my conversations, but they varied in formats. Most of the log formats are XML or HTML, which make re-reading those logs a pain.&lt;/p&gt;
    &lt;p&gt;The simplest AIM logs are the plaintext logs, which look like this:&lt;/p&gt;
    &lt;code&gt;Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005
[18:44] Jane: hi
[18:55] Me: hey whats up
Session Close (Jane): Mon Sep 12 18:56:02 2005
&lt;/code&gt;
    &lt;p&gt;Every decade or so, I try writing a universal AIM log parser to get all of my old logs into a consistent, readable format. Unfortunately, I always get bored and give up partway through. My last attempt was seven years ago, when I tried doing it in Python 2.7.&lt;/p&gt;
    &lt;p&gt;Parsing logs is a great match for Gleam because some parts of the project are easy (e.g., parsing the plaintext logs), so I can do the easy parts while I get the hang of Gleam as a language and gradually build up to the harder log formats and adding a web frontend.&lt;/p&gt;
    &lt;p&gt;I’ve also heard that functional languages lend themselves especially well to parsing tasks, and I’ve never understood why, so it’s a good opportunity to learn.&lt;/p&gt;
    &lt;head rend="h2"&gt;My background in programming languages 🔗︎&lt;/head&gt;
    &lt;p&gt;I’ve been a programmer for 20 years, but I’m no language design connoisseur. I’m sharing things about Gleam I find unintuitive or difficult to work with, but they’re not language critiques, just candid reactions.&lt;/p&gt;
    &lt;p&gt;I’ve never worked in a langauge that’s designed for functional programming. The closest would be JavaScript. The languages I know best are Go and Python.&lt;/p&gt;
    &lt;head rend="h2"&gt;How do I parse command-line args? 🔗︎&lt;/head&gt;
    &lt;p&gt;The first thing I wanted to do was figure out how to parse a command-line argument so I could call my app like this:&lt;/p&gt;
    &lt;code&gt;./log-parser ~/logs/aim/plaintext
&lt;/code&gt;
    &lt;p&gt;But there’s no Gleam standard library module for reading command-line arguments. I found glint, and it felt super complicated for just reading one command-line argument. Then, I realized there’s a simpler third-party library called argv.&lt;/p&gt;
    &lt;p&gt;I can parse the command-line argument like this:&lt;/p&gt;
    &lt;code&gt;pub fn main() {
  case argv.load().arguments {
    [path] -&amp;gt; io.println("command-line arg is " &amp;lt;&amp;gt; path)
    _ -&amp;gt; io.println("Usage: gleam run &amp;lt;directory_path&amp;gt;")
  }
}
&lt;/code&gt;
    &lt;code&gt;$ gleam run ~/whatever
   Compiled in 0.01s
    Running log_parser.main
command-line arg is /home/mike/whatever
&lt;/code&gt;
    &lt;p&gt;Cool, easy enough!&lt;/p&gt;
    &lt;head rend="h2"&gt;What does &lt;code&gt;gleam build&lt;/code&gt; do? 🔗︎&lt;/head&gt;
    &lt;p&gt;I got my program to run with &lt;code&gt;gleam run&lt;/code&gt;, but I was curious if I could compile an executable like &lt;code&gt;go build&lt;/code&gt; or &lt;code&gt;zig build&lt;/code&gt; does.&lt;/p&gt;
    &lt;code&gt;$ gleam build
   Compiled in 0.01s
&lt;/code&gt;
    &lt;p&gt;Hmm, compiled what? I couldn’t see a binary anywhere.&lt;/p&gt;
    &lt;p&gt;The documentation for &lt;code&gt;gleam build&lt;/code&gt; just says “Build the project” but doesn’t explain what it builds or where it stores the build artifact.&lt;/p&gt;
    &lt;p&gt;There’s a &lt;code&gt;build&lt;/code&gt; directory, but it doesn’t produce an obvious executable.&lt;/p&gt;
    &lt;code&gt;$ rm -rf build &amp;amp;&amp;amp; gleam build
Downloading packages
 Downloaded 5 packages in 0.00s
  Compiling argv
  Compiling gleam_stdlib
  Compiling filepath
  Compiling gleeunit
  Compiling simplifile
  Compiling log_parser
   Compiled in 0.52s

$ ls -1 build/
dev
gleam-dev-erlang.lock
gleam-dev-javascript.lock
gleam-lsp-erlang.lock
gleam-lsp-javascript.lock
gleam-prod-erlang.lock
gleam-prod-javascript.lock
packages
&lt;/code&gt;
    &lt;p&gt;From poking around, I think the executables are under &lt;code&gt;build/dev/erlang/log_parser/ebin/&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;$ ls -1 build/dev/erlang/log_parser/ebin/
log_parser.app
log_parser.beam
log_parser@@main.beam
log_parser_test.beam
plaintext_logs.beam
plaintext_logs_test.beam
&lt;/code&gt;
    &lt;p&gt;Those appear to be BEAM bytecode, so I can’t execute them directly. I assume I could get run the BEAM VM manually and execute those files somehow, but that doesn’t sound appealing.&lt;/p&gt;
    &lt;p&gt;So, I’ll stick to &lt;code&gt;gleam run&lt;/code&gt; to run my app, but I wish &lt;code&gt;gleam build&lt;/code&gt; had a better explanation of what it produced and what the developer can do with it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Let me implement the simplest possible parser 🔗︎&lt;/head&gt;
    &lt;p&gt;To start, I decided to write a function that does basic parsing of plaintext logs.&lt;/p&gt;
    &lt;p&gt;So, I wrote a test with what I wanted.&lt;/p&gt;
    &lt;code&gt;pub fn parse_simple_plaintext_log_test() {
  "
Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005
[18:44] Jane: hi
[18:55] Me: hey whats up
Session Close (Jane): Mon Sep 12 18:56:02 2005
"
  |&amp;gt; string.trim
  |&amp;gt; plaintext_logs.parse
  |&amp;gt; should.equal(["hi", "hey whats up"])
}
&lt;/code&gt;
    &lt;p&gt;Eventually, I want to parse all the metadata in the conversation, including names, timestamps, and session information. But as a first step, all my function has to do is read an AIM chat log as a string and emit a list of the chat messages as separate strings.&lt;/p&gt;
    &lt;p&gt;That meant my actual function would look like this:&lt;/p&gt;
    &lt;code&gt;pub fn parse(contents: String) -&amp;gt; List(String) {
  // Note: todo is a Gleam language keyword to indicate unfinished code.
  todo
}
&lt;/code&gt;
    &lt;p&gt;Just to get it compiling, I add in a dummy implementation:&lt;/p&gt;
    &lt;code&gt;pub fn parse(contents: String) -&amp;gt; List(String) {
  ["fake", "data"]
}
&lt;/code&gt;
    &lt;p&gt;And I can test it like this:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
warning: Unused variable
  ┌─ /home/mike/code/gleam-log-parser2/src/plaintext_logs.gleam:1:14
  │
1 │ pub fn parse(contents: String) -&amp;gt; List(String) {
  │              ^^^^^^^^^^^^^^^^ This variable is never used

Hint: You can ignore it with an underscore: `_contents`.

   Compiled in 0.22s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["fake", "data"]
     output:

Finished in 0.008 seconds
1 tests, 1 failures
&lt;/code&gt;
    &lt;p&gt;Cool, that’s what I expected. The test is failing because it’s returning hardcoded dummy results that don’t match my test.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adjusting my brain to a functional language 🔗︎&lt;/head&gt;
    &lt;p&gt;Okay, now it’s time to implement the parsing for real. I need to implement this function:&lt;/p&gt;
    &lt;code&gt;pub fn parse(contents: String) -&amp;gt; List(String) {
  todo
}
&lt;/code&gt;
    &lt;p&gt;At this point, I kind of froze up. It struck me that Gleam excludes so many of the tools I’m used to in other languages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There are no &lt;code&gt;if&lt;/code&gt;statements&lt;/item&gt;
      &lt;item&gt;There are no loops&lt;/item&gt;
      &lt;item&gt;There’s no &lt;code&gt;return&lt;/code&gt;keyword&lt;/item&gt;
      &lt;item&gt;There are no list index accessors&lt;list rend="ul"&gt;&lt;item&gt;e.g., you can’t access the n-th element of a &lt;code&gt;List&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;e.g., you can’t access the n-th element of a &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What do I even do? Split the string into tokens and then do something with that?&lt;/p&gt;
    &lt;p&gt;Eventually, I realized for a simple implementation, I wanted to just split the string into lines, so I want to do this:&lt;/p&gt;
    &lt;code&gt;pub fn parse(contents: String) -&amp;gt; List(String) {
  string.split(contents, on: "\n")
}
&lt;/code&gt;
    &lt;p&gt;If I test again, I get this:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
   Compiled in 0.21s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005", "[18:44] Jane: hi", "[18:55] Me: hey whats up", "Session Close (Jane): Mon Sep 12 18:56:02 2005"]
     output:

Finished in 0.009 seconds
1 tests, 1 failures
&lt;/code&gt;
    &lt;p&gt;Okay, now I’m a little closer.&lt;/p&gt;
    &lt;head rend="h2"&gt;How do I iterate over a list in a language with no loops? 🔗︎&lt;/head&gt;
    &lt;p&gt;I turned my logs into a list of lines, but that’s where I got stuck again.&lt;/p&gt;
    &lt;p&gt;I’m so used to &lt;code&gt;for&lt;/code&gt; loops that my brain kept thinking, “How do I do a &lt;code&gt;for&lt;/code&gt; loop to iterate over the elements?”&lt;/p&gt;
    &lt;p&gt;I realized I needed to call &lt;code&gt;list.map&lt;/code&gt;. I need to define a function that acts on each element of the list.&lt;/p&gt;
    &lt;code&gt;import gleam/list
import gleam/string

fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; line
  }
}

pub fn parse(contents: String) -&amp;gt; List(String) {
  string.split(contents, on: "\n")
  |&amp;gt; list.map(parse_line)
}
&lt;/code&gt;
    &lt;p&gt;This is my first time using pattern matching in any language, and it’s neat, though it’s still so unfamiliar that I find it hard to recognize when to use it.&lt;/p&gt;
    &lt;p&gt;Zooming in a bit on the pattern matching, it’s here:&lt;/p&gt;
    &lt;code&gt;  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; line
  }
&lt;/code&gt;
    &lt;p&gt;It evaluates the &lt;code&gt;line&lt;/code&gt; variable and matches it to one of the subsequent patterns within the braces. If the line starts with &lt;code&gt;"Session Start"&lt;/code&gt; (the &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; means the preceding string is a prefix), then Gleam executes the code after the &lt;code&gt;-&amp;gt;&lt;/code&gt;, which in this case is just the empty string. Same for &lt;code&gt;"Session Close"&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If the line doesn’t match the &lt;code&gt;"Session Start"&lt;/code&gt; or &lt;code&gt;"Session Close"&lt;/code&gt; patterns, Gleam executes the last line in the &lt;code&gt;case&lt;/code&gt; which just matches any string. In that case, it evaluates to the same string. Meaning &lt;code&gt;"hi"&lt;/code&gt; would evaluate to just &lt;code&gt;"hi"&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is where it struck me how strange it feels to not have a &lt;code&gt;return&lt;/code&gt; keyword. In every other language I know, you have to explicitly return a value from a function with a &lt;code&gt;return&lt;/code&gt; keyword, but in Gleam, the return value is just the value from the last line that Gleam executes in the function.&lt;/p&gt;
    &lt;p&gt;If I run my test, I get this:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
   Compiled in 0.22s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["", "[18:44] Jane: hi", "[18:55] Me: hey whats up", ""]
     output:

Finished in 0.009 seconds
1 tests, 1 failures
&lt;/code&gt;
    &lt;p&gt;Again, this is what I expected, and I’m a bit closer to my goal.&lt;/p&gt;
    &lt;p&gt;I’ve converted the &lt;code&gt;"Session Start"&lt;/code&gt; and &lt;code&gt;"Session End"&lt;/code&gt; lines to empty strings, and the middle two elements of the list are the lines that have AIM messages in them.&lt;/p&gt;
    &lt;p&gt;The remaining work is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strip out the time and sender parts of the log lines.&lt;/item&gt;
      &lt;item&gt;Filter out empty strings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Scraping an AIM message from a line 🔗︎&lt;/head&gt;
    &lt;p&gt;At this point, I have a string like this:&lt;/p&gt;
    &lt;code&gt;[18:55] Me: hey whats up
&lt;/code&gt;
    &lt;p&gt;And I need to extract just the portion after the sender’s name to this:&lt;/p&gt;
    &lt;code&gt;hey whats up
&lt;/code&gt;
    &lt;p&gt;My instinct is to use a string split function and split on the &lt;code&gt;:&lt;/code&gt; character. I see that there’s &lt;code&gt;string.split&lt;/code&gt; which returns &lt;code&gt;List(String)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;There’s also a &lt;code&gt;string.split_once&lt;/code&gt; function, which should work because I can split once on &lt;code&gt;: &lt;/code&gt;(note the trailing space after the colon).&lt;/p&gt;
    &lt;p&gt;The problem is that &lt;code&gt;split_once&lt;/code&gt; returns &lt;code&gt;Result(#(String, String), Nil)&lt;/code&gt;, a type that feels scarier to me. It’s a two-tuple wrapped in a &lt;code&gt;Result&lt;/code&gt;, which means that the function can return an error on failure. It’s confusing that &lt;code&gt;split_once&lt;/code&gt; can fail whereas &lt;code&gt;split&lt;/code&gt; cannot, so for simplicity, I’ll go with &lt;code&gt;split&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
      echo string.split(line, on: ": ")
      todo
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;If I run my test, I get this:&lt;/p&gt;
    &lt;code&gt;$ gleam test
warning: Todo found
   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:10:7
   │
10 │       todo
   │       ^^^^ This code is incomplete

This code will crash if it is run. Be sure to finish it before
running your program.

Hint: I think its type is `String`.


   Compiled in 0.01s
    Running log_parser_test.main
src/plaintext_logs.gleam:9
["[18:44] Jane", "hi"]
&lt;/code&gt;
    &lt;p&gt;Good. That’s doing what I want. I’m successfully isolating the &lt;code&gt;"hi"&lt;/code&gt; part, so now I just have to return it.&lt;/p&gt;
    &lt;head rend="h2"&gt;How do I access the last element of a list? 🔗︎&lt;/head&gt;
    &lt;p&gt;At this point, I feel close to victory. I’ve converted the line to a list of strings, and I know the string I want is the last element of the list, but how do I grab it?&lt;/p&gt;
    &lt;p&gt;In most other languages, I’d just say &lt;code&gt;line_parts[1]&lt;/code&gt;, but Gleam’s lists have no accessors by index.&lt;/p&gt;
    &lt;p&gt;Looking at the &lt;code&gt;gleam/list&lt;/code&gt; module, I see a &lt;code&gt;list.last&lt;/code&gt; function, so I try that:&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
       string.split(line, on: ": ")
       |&amp;gt; list.last
       |&amp;gt; echo
       |&amp;gt; todo
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;If I run that, I get:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
warning: Todo found
   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:12:11
   │
12 │        |&amp;gt; todo
   │           ^^^^ This code is incomplete

This code will crash if it is run. Be sure to finish it before
running your program.

Hint: I think its type is `fn(Result(String, Nil)) -&amp;gt; String`.


   Compiled in 0.24s
    Running log_parser_test.main
src/plaintext_logs.gleam:11
Ok("hi")
&lt;/code&gt;
    &lt;p&gt;A bit closer! I’ve extracted the last element of the list to find &lt;code&gt;"hi"&lt;/code&gt;, but now it’s wrapped in a &lt;code&gt;Result&lt;/code&gt; type.&lt;/p&gt;
    &lt;p&gt;I can unwrap it with &lt;code&gt;result.unwrap&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
       string.split(line, on: ": ")
       |&amp;gt; list.last
       |&amp;gt; result.unwrap("")
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;Re-running &lt;code&gt;gleam test&lt;/code&gt; yields:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
   Compiled in 0.22s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["", "hi", "hey whats up", ""]
     output:

Finished in 0.008 seconds
1 tests, 1 failures
&lt;/code&gt;
    &lt;p&gt;Great! That did what I wanted. I reduced the messages lines to just the contents of the messages.&lt;/p&gt;
    &lt;head rend="h2"&gt;Filtering out empty strings 🔗︎&lt;/head&gt;
    &lt;p&gt;The only thing that’s left is to filter the empty strings out of the list, which is straightforward enough with &lt;code&gt;list.filter&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;pub fn parse(contents: String) -&amp;gt; List(String) {
  string.split(contents, on: "\n")
  |&amp;gt; list.map(parse_line)
  |&amp;gt; list.filter(fn(s) { !string.is_empty(s) })
}
&lt;/code&gt;
    &lt;p&gt;And I re-run the tests:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
   Compiled in 0.22s
    Running log_parser_test.main
.
Finished in 0.007 seconds
1 tests, 0 failures
&lt;/code&gt;
    &lt;p&gt;Voilà! The tests now pass!&lt;/p&gt;
    &lt;head rend="h2"&gt;Tidying up string splitting 🔗︎&lt;/head&gt;
    &lt;p&gt;My tests are now passing, so theoretically, I’ve achieved my initial goal.&lt;/p&gt;
    &lt;p&gt;I could declare victory and call it a day. Or, I could refactor!&lt;/p&gt;
    &lt;p&gt;I’ll refactor.&lt;/p&gt;
    &lt;p&gt;I feel somewhat ashamed of my string splitting logic, as it didn’t feel like idiomatic Gleam. Can I do it without getting into result unwrapping?&lt;/p&gt;
    &lt;p&gt;Re-reading it, I realize I can solve it with this newfangled pattern matching thing. I know that the string will split into a list with two elements, so I can create a pattern for a two-element list:&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
       case string.split(line, on: ": ") {
          [_, message] -&amp;gt; message
          _ -&amp;gt; ""
       }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;That feels a little more elegant than calling &lt;code&gt;result.last&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Can I tidy this up further? I avoided &lt;code&gt;string.split_once&lt;/code&gt; because the type was too confusing, but it’s probably the better option if I expect only one split, so what does that look like?&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
       echo string.split_once(line, on: ": ")
       todo
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;To inspect the data, I run my test again:&lt;/p&gt;
    &lt;code&gt;$ gleam test
[...]
src/plaintext_logs.gleam:9
Ok(#("[18:44] Jane", "hi"))
&lt;/code&gt;
    &lt;p&gt;Okay, that doesn’t look as scary as I thought. Even though my first instinct is to unwrap the error and access the last element in the tuple (which actually is easy for tuples, just not lists), I know at this point that there’s probably a pattern-matchy way. And there is:&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
       case string.split_once(line, on: ": ") {
        Ok(#(_, message)) -&amp;gt; message
        _ -&amp;gt; ""
       }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;Ok(#(_, message))&lt;/code&gt; pattern will match a successful result from &lt;code&gt;split_once&lt;/code&gt;, which is a two-tuple of &lt;code&gt;String&lt;/code&gt; wrapped in an &lt;code&gt;Ok&lt;/code&gt; result. The other &lt;code&gt;case&lt;/code&gt; option is the catchall that returns an empty string.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting rid of the empty string hack 🔗︎&lt;/head&gt;
    &lt;p&gt;One of the compelling features of Gleam for me is its static typing, so it feels hacky that I’m abusing the empty string to represent a lack of message on a particular line. Can I use the type system instead of using empty strings as sentinel values?&lt;/p&gt;
    &lt;p&gt;The pattern in Gleam for indicating that something might fail but the failure isn’t necessarily an error is &lt;code&gt;Result(&amp;lt;type&amp;gt;, Nil)&lt;/code&gt;, so let me try to rewrite it that way:&lt;/p&gt;
    &lt;code&gt;import gleam/list
import gleam/result
import gleam/string

fn parse_line(line: String) -&amp;gt; Result(String, Nil) {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; Error(Nil)
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; Error(Nil)
    line -&amp;gt; {
       case string.split_once(line, on: ": ") {
        Ok(#(_, message)) -&amp;gt; Ok(message)
        _ -&amp;gt; Error(Nil)
       }
    }
  }
}

pub fn parse(contents: String) -&amp;gt; List(String) {
  string.split(contents, on: "\n")
  |&amp;gt; list.map(parse_line)
  |&amp;gt; result.values
}
&lt;/code&gt;
    &lt;p&gt;Great! I like being more explicit that the lines without messages return &lt;code&gt;Error(Nil)&lt;/code&gt; rather than an empty string. Also, &lt;code&gt;result.values&lt;/code&gt; is more succinct for filtering empty lines than the previous &lt;code&gt;list.filter(fn(s) { !string.is_empty(s) })&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Overall reflections 🔗︎&lt;/head&gt;
    &lt;p&gt;After spending a few hours with Gleam, I’m enjoying it. It pushes me out of my comfort zone the right amount where I feel like I’m learning new ways of thinking about programming but not so much that I’m too overwhelmed to learn anything.&lt;/p&gt;
    &lt;p&gt;The biggest downside I’m finding with Gleam is that it’s a young language with a relatively small team. It just turned six years old, but it looks like the founder was working on it solo until a year ago. There are now a handful of core maintainers, but I don’t know if any of them work on Gleam full-time, so the ecosystem is a bit limited. I’m looking ahead to parsing other log formats that are in HTML and XML, and there are Gleam HTML and XML parsers, but they don’t seem widely used, so I’m not sure how well they’ll work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Love: Pipelines 🔗︎&lt;/head&gt;
    &lt;p&gt;I love love love Gleam’s pipeline syntax. You can see me using it in the test with the &lt;code&gt;|&amp;gt;&lt;/code&gt; characters:&lt;/p&gt;
    &lt;code&gt; "..."
  |&amp;gt; string.trim
  |&amp;gt; plaintext_logs.parse
  |&amp;gt; should.equal(["hi", "hey whats up"])
&lt;/code&gt;
    &lt;p&gt;The non-pipeline equivalent of the test would look like this:&lt;/p&gt;
    &lt;code&gt;pub fn parse_simple_plaintext_log_test() {
  let input = "..."
  let trimmed = string.trim(input)
  let parsed = plaintext_logs.parse(trimmed)

  should.equal(parsed, ["hi", "hey whats up"])
}
&lt;/code&gt;
    &lt;p&gt;It looks like wet garbage by comparison.&lt;/p&gt;
    &lt;p&gt;Now that I’ve seen pipelines, they feel so obvious and conspicuously missing in every other programming language I use.&lt;/p&gt;
    &lt;p&gt;I’ve enjoyed pipelining in bash, but it never occurred to me how strange it is that other programming languages never adopted it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Like: Example-centric documentation 🔗︎&lt;/head&gt;
    &lt;p&gt;The Gleam documentation is a bit terse, but I like that it’s so example-heavy.&lt;/p&gt;
    &lt;p&gt;I learn best by reading examples, so I appreciate that so much of the Gleam standard library is documented with examples showing simple usage of each API function.&lt;/p&gt;
    &lt;head rend="h3"&gt;Like: Built-in unused symbol warnings 🔗︎&lt;/head&gt;
    &lt;p&gt;I like that the Gleam compiler natively warns about unused functions, variables, and imports. And I like that these are warnings rather than errors.&lt;/p&gt;
    &lt;p&gt;In Go, I get frustrated during debugging when I temporarily comment something out and then the compiler stubbornly refuses to do anything until I fix the stupid import, which I then have to un-fix when I finish whatever I was debugging.&lt;/p&gt;
    &lt;head rend="h3"&gt;Like: &lt;code&gt;todo&lt;/code&gt; keyword 🔗︎&lt;/head&gt;
    &lt;p&gt;One of my favorite dumb programming jokes happened at my first programming job about 15 years ago. On a group email thread with several C++ developers, my friend shared a hot tip about C++ development.&lt;/p&gt;
    &lt;p&gt;He said that if we were ever got fed up with arcane C++ compilation errors, we could just add a special line to our source code, and then even invalid C++ code would compile successfully:&lt;/p&gt;
    &lt;code&gt;#pragma always_compile
&lt;/code&gt;
    &lt;p&gt;Spoiler alert: it’s not a real C++ preprocessor directive.&lt;/p&gt;
    &lt;p&gt;But I’ve found myself occasionally wishing languages had something like this when I’m in the middle of development and don’t care about whatever bugs the compiler is trying to protect me from.&lt;/p&gt;
    &lt;p&gt;Gleam’s &lt;code&gt;todo&lt;/code&gt; is almost like a &lt;code&gt;#pragma always_compile&lt;/code&gt;. Even if your code is invalid, the Gleam compiler just says, “Okay, fine. I’ll run it anyway.”&lt;/p&gt;
    &lt;p&gt;You can see this when I was in the middle of implementing &lt;code&gt;parse_line&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;fn parse_line(line: String) -&amp;gt; String {
  case line {
    "Session Start" &amp;lt;&amp;gt; _ -&amp;gt; ""
    "Session Close" &amp;lt;&amp;gt; _ -&amp;gt; ""
    line -&amp;gt; {
      echo string.split(line, on: ": ")
      todo
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;If I take out the &lt;code&gt;todo&lt;/code&gt;, Gleam refuses to run the code at all:&lt;/p&gt;
    &lt;code&gt;$ gleam test
  Compiling log_parser
error: Type mismatch
   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:8:5
   │
 8 │ ╭     line -&amp;gt; {
 9 │ │       echo string.split(line, on: ": ")
10 │ │     }
   │ ╰─────^

This case clause was found to return a different type than the previous
one, but all case clauses must return the same type.

Expected type:

    String

Found type:

    List(String)
&lt;/code&gt;
    &lt;p&gt;Right, I’m returning an incorrect type, so why would the compiler cooperate with me?&lt;/p&gt;
    &lt;p&gt;But adding &lt;code&gt;todo&lt;/code&gt; lets me run the function anyway, which helps me understand what the code is doing even though I haven’t finished implementing it:&lt;/p&gt;
    &lt;code&gt;$ gleam test
warning: Todo found
   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:10:7
   │
10 │       todo
   │       ^^^^ This code is incomplete

This code will crash if it is run. Be sure to finish it before
running your program.

Hint: I think its type is `String`.


  Compiling log_parser
   Compiled in 0.21s
    Running log_parser_test.main
src/plaintext_logs.gleam:9
["[18:44] Jane", "hi"]
F
[...]
Finished in 0.007 seconds
1 tests, 1 failures
&lt;/code&gt;
    &lt;head rend="h3"&gt;Like: Pattern matching 🔗︎&lt;/head&gt;
    &lt;p&gt;I find pattern matching elegant and concise, though it’s the part of Gleam I find hardest to adjust to. It feels so different from procedural style of programming I’m accustomed to in other languages I know.&lt;/p&gt;
    &lt;p&gt;The downside is that I have a hard time recognizing when pattern matching is the right tool, and I also find pattern matching harder to read. But I think that’s just inexperience, and I think with more practice, I’ll be able to think in pattern matching.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dislike: Error handling 🔗︎&lt;/head&gt;
    &lt;p&gt;I find Gleam’s error handling pretty awkward, especially because errors ruin the beauty of nice, tidy pipelines.&lt;/p&gt;
    &lt;p&gt;For example, if I had a string processing pipeline like this:&lt;/p&gt;
    &lt;code&gt;string.split(line, on: "-")
|&amp;gt; list.last
|&amp;gt; result.unwrap("") // Ugly!
|&amp;gt; string.uppercase
&lt;/code&gt;
    &lt;p&gt;That &lt;code&gt;result.unwrap&lt;/code&gt; line feels so ugly and out of place to me. I wish the syntax was like this:&lt;/p&gt;
    &lt;code&gt;string.split(line, on: ": ")
|&amp;gt; try list.last
|&amp;gt; string.uppercase
|&amp;gt; Ok
&lt;/code&gt;
    &lt;p&gt;Where &lt;code&gt;try&lt;/code&gt; causes the function to return an error, kind of like in Zig.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dislike: Small core language 🔗︎&lt;/head&gt;
    &lt;p&gt;I don’t know if this is a long-term design choice or if it’s just small for now because it’s an indie-developed language, but the first thing about Gleam that stood out to me is how few built-in features there are.&lt;/p&gt;
    &lt;p&gt;For example, there’s no built-in feature for iterating over the elements of a &lt;code&gt;List&lt;/code&gt; type, and the type itself doesn’t expose a function to iterate it, so you have to use the &lt;code&gt;gleam/list&lt;/code&gt; module in the standard library.&lt;/p&gt;
    &lt;p&gt;Similarly, if a function can fail, it returns a &lt;code&gt;Result&lt;/code&gt; type, and there are no built-in functions for handling a &lt;code&gt;Result&lt;/code&gt;, so you have to use the &lt;code&gt;gleam/result&lt;/code&gt; module to check if the function succeeded.&lt;/p&gt;
    &lt;p&gt;To me, that functionality feels so core to the language that it would be part of the language itself, not the standard library.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dislike: Limited standard library 🔗︎&lt;/head&gt;
    &lt;p&gt;In addition to the language feeling small, the standard library feels pretty limited as well.&lt;/p&gt;
    &lt;p&gt;There are currently only 19 modules in the Gleam standard library. Conspicuously absent are modules for working with the filesystem (the de facto standard seems to be the third-party simplifile module).&lt;/p&gt;
    &lt;p&gt;For comparison, the standard libraries for Python and Go each have about 250 modules. Although, in fairness, those languages have about 1000x the resources as Gleam.&lt;/p&gt;
    &lt;head rend="h2"&gt;Source code 🔗︎&lt;/head&gt;
    &lt;p&gt;The source code for this project is available on Codeberg:&lt;/p&gt;
    &lt;p&gt;Commit 291e6d is the version that matches this blog post.&lt;/p&gt;
    &lt;p&gt;Thanks to Isaac Harris-Holt for helpful feedback on this post.&lt;/p&gt;
    &lt;head rend="h2"&gt;Read My Book&lt;/head&gt;
    &lt;p&gt;I'm writing a book of simple techniques to help developers improve their writing.&lt;/p&gt;
    &lt;p&gt;My book will teach you how to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create clear and pleasant software tutorials&lt;/item&gt;
      &lt;item&gt;Attract readers and customers through blogging&lt;/item&gt;
      &lt;item&gt;Write effective emails&lt;/item&gt;
      &lt;item&gt;Minimize pain in writing design documents&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Be the first to know when I post cool stuff&lt;/head&gt;
    &lt;p&gt;Subscribe to get my latest posts by email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mtlynch.io/notes/gleam-first-impressions/"/></entry><entry><id>https://news.ycombinator.com/item?id=45232299</id><title>Show HN: CLAVIER-36 – A programming environment for generative music</title><updated>2025-09-14T08:40:52.354703+00:00</updated><content>&lt;doc fingerprint="536fd5a56c585c00"&gt;
  &lt;main&gt;
    &lt;p&gt;×&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://clavier36.com/p/LtZDdcRP3haTWHErgvdM"/></entry><entry><id>https://news.ycombinator.com/item?id=45232562</id><title>Four-year wedding crasher mystery solved</title><updated>2025-09-14T08:40:52.069512+00:00</updated><content>&lt;doc fingerprint="67528137d6155029"&gt;
  &lt;main&gt;
    &lt;p&gt;A baffled bride has solved the mystery of the awkward-looking stranger who crashed her wedding four years ago.&lt;/p&gt;
    &lt;p&gt;Michelle Wylie and her husband, John, registered the presence of their unidentifiable guest only as they looked through photographs of their wedding in the days after the happy occasion.&lt;/p&gt;
    &lt;p&gt;Who was the tall man in a dark suit, distinguished by the look of quiet mortification on his face? But their family and friends could offer no explanation, nor could hotel staff at the Carlton hotel in Prestwick, where the event took place in November 2021. An appeal on Facebook likewise yielded no clues.&lt;/p&gt;
    &lt;p&gt;Eventually, with the mystery still niggling, Wylie asked the popular Scottish content creator Dazza to cast the online net wider – and a sheepish Andrew Hillhouse finally stepped forward.&lt;/p&gt;
    &lt;p&gt;In his explanatory post on Facebook, Hillhouse admitted that he had been “cutting it fine, as I’m known to do” when he pulled up at the wedding venue with five minutes to spare. Spotting a piper and other guests, he followed them into the hotel – “I remember thinking to myself: ‘Cool, this is obviously the right place’” – unaware that he had the address completely wrong and was supposed to be at a ceremony 2 miles away in Ayr.&lt;/p&gt;
    &lt;p&gt;He was initially unperturbed to find himself surrounded by strangers as the ceremony began – at the marriage he was due to attend, the only person he knew was the bride, Michaela, while his partner, Andrew, was part of the wedding party. It was when an entirely different bride came walking down the aisle that he realised: “OMG that’s not Michaela … I was at the wrong wedding!”&lt;/p&gt;
    &lt;p&gt;Hillhouse said: “You can’t exactly stand up and walk out of a wedding mid-ceremony, so I just had to commit to this act and spent the next 20 minutes awkwardly sitting there trying to be as inconspicuous as my 6ft 2 ass could be.”&lt;/p&gt;
    &lt;p&gt;At the end of the ceremony, Hillhouse, who is from Troon, was hoping to make a discreet exit, only to be waylaid by the wedding photographer, who insisted he join other guests for a group shot. He can be spotted looming uncomfortably at the very back of the crowd.&lt;/p&gt;
    &lt;p&gt;His post continued: “Rushed outside, made some phone calls and made my way to the correct wedding, where I was almost as popular as the actual bride and groom, and spent most of the night retelling that story to people.”&lt;/p&gt;
    &lt;p&gt;For Michelle Wylie, this amiable resolution brings to a close years of speculation.&lt;/p&gt;
    &lt;p&gt;She told BBC Scotland: “It would come into my head and I’d be like: ‘Someone must know who this guy is.’ I said a few times to my husband: ‘Are you sure you don’t know this guy, is he maybe from your work?’ We wondered if he was a mad stalker.”&lt;/p&gt;
    &lt;p&gt;She is now Facebook friends with Hillhouse and the pair have met in person to cement their coincidental bond.&lt;/p&gt;
    &lt;p&gt;“I could not stop laughing,” said Wylie. “We can’t believe we’ve found out who he is after almost four years.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/uk-news/2025/sep/12/wedding-crasher-mystery-solved-four-years-bride-scotland"/></entry><entry><id>https://news.ycombinator.com/item?id=45232565</id><title>486Tang – 486 on a credit-card-sized FPGA board</title><updated>2025-09-14T08:40:51.988599+00:00</updated><content>&lt;doc fingerprint="3a1d592425a09ee2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;486Tang - 486 on a credit-card-sized FPGA board&lt;/head&gt;
    &lt;p&gt;Yesterday I released 486Tang v0.1 on GitHub. It’s a port of the ao486 MiSTer PC core to the Sipeed Tang Console 138K FPGA. I’ve been trying to get an x86 core running on the Tang for a while. As far as I know, this is the first time ao486 has been ported to a non-Altera FPGA. Here’s a short write‑up of the project.&lt;/p&gt;
    &lt;p&gt;Thanks to everyone coming from Hacker News! If 486Tang caught your eye, I share progress updates and related projects over on X.&lt;/p&gt;
    &lt;head rend="h2"&gt;486Tang Architecture&lt;/head&gt;
    &lt;p&gt;Every FPGA board is a little different. Porting a core means moving pieces around and rewiring things to fit. Here are the major components in 486Tang:&lt;/p&gt;
    &lt;p&gt;Compared to ao486 on MiSTer, there are a few major differences:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Switching to SDRAM for main memory. The MiSTer core uses DDR3 as main memory. Obviously, at the time of the 80486, DDR didn’t exist, so SDRAM is a natural fit. I also wanted to dedicate DDR3 to the framebuffer; time‑multiplexing it would have been complicated. So SDRAM became the main memory and DDR3 the framebuffer. The SDRAM on Tang is 16‑bit wide while ao486 expects 32‑bit accesses, which would normally mean one 32‑bit word every two cycles. I mitigated this by running the SDRAM logic at 2× the system clock so a 32‑bit word can be read or written every CPU cycle (“double‑pumping” the memory).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;SD‑backed IDE. On MiSTer, the core forwards IDE requests to the ARM HPS over a fast HPS‑FPGA link; the HPS then accesses a VHD image. Tang doesn’t have a comparable high‑speed MCU‑to‑FPGA interface—only a feeble UART—so I moved disk storage into the SD card and let the FPGA access it directly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Boot‑loading module. A PC needs several things to boot: BIOS, VGA BIOS, CMOS settings, and IDE IDENTIFY data (512 bytes). Since I didn’t rely on an MCU for disk data, I stored all of these in the first 128 KB of the SD card. A small boot loader module reads them into main memory and IDE, and then releases the CPU when everything is ready.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;System bring-up with the help of a whole-system simulator&lt;/head&gt;
    &lt;p&gt;After restructuring the system, the main challenge was bringing it up to a DOS prompt. A 486 PC is complex—CPU and peripherals—more so than the game consoles I’ve worked on. The ao486 CPU alone is &amp;gt;25K lines of Verilog, versus a few K for older cores like M68K. Debugging on hardware was painful: GAO builds took 10+ minutes and there were many more signals to probe. Without a good plan, it would be unmanageable and bugs could take days to isolate—not viable for a hobby project.&lt;/p&gt;
    &lt;p&gt;My solution was Verilator for subsystem and whole‑system simulation. The codebase is relatively mature, so I skipped per‑module unit tests and focused on simulating subsystems like VGA and a full boot to DOS. Verilator is fast enough to reach a DOS prompt in a few minutes—an order of magnitude better if you factor in the complete waveforms you get in simulation. The trick, then, is surfacing useful progress and error signals. A few simple instrumentation hooks were enough for me:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Bochs BIOS can print debug strings to port 0x8888 in debug builds. I intercept and print these (the yellow messages in the simulator). The same path exists on hardware—the CPU forwards them over UART—so BIOS issues show up immediately without waiting for a GAO build.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Subsystem‑scoped tracing. For Sound Blaster, IDE, etc., I added&lt;/p&gt;&lt;code&gt;--sound&lt;/code&gt;,&lt;code&gt;--ide&lt;/code&gt;flags to trace I/O operations and key state changes. This is much faster than editing Verilog or using GAO.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bochs BIOS assembly listings are invaluable. I initially used a manual disassembly—old console habits—without symbols, which was painful. Rebuilding Bochs and using the official listings solved that.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A lot of the bugs were in the new glue I added, as expected. ao486 itself is mature. Still, a few issues only showed up on this toolchain/hardware, mostly due to toolchain behavior differences. In one case a variable meant to be static behaved like an automatic variable and didn’t retain state across invocations, so a CE pulse never occurred. Buried deep, it took a while to find.&lt;/p&gt;
    &lt;p&gt;Here’s a simulation session. On the left the simulated 486 screen. On the right is the simulator terminal output. You can see the green VGA output and yellow debug output, along with other events like INT 15h and video VSYNCs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance optimizations&lt;/head&gt;
    &lt;p&gt;With simulation help, the core ran on Tang Console—just not fast. The Gowin GW5A isn’t a particularly fast FPGA. Initial benchmarks put it around a 25 MHz 80386.&lt;/p&gt;
    &lt;p&gt;The main obstacle to clock speed is long combinational paths. When you find a critical path, you either shorten it or pipeline it by inserting registers—both risks bugs. A solid test suite is essential; I used test386.asm to validate changes.&lt;/p&gt;
    &lt;p&gt;Here are a few concrete wins:&lt;/p&gt;
    &lt;p&gt;Reset tree and fan-out reduction. Gowin’s tools didn’t replicate resets aggressively enough (even with “Place → Replicate Resources”). One reset net had &amp;gt;5,000 fan-out, which ballooned delays. Manually replicating the reset and a few other high‑fan-out nets helped a lot.&lt;/p&gt;
    &lt;p&gt;Instruction fetch optimization. A long combinational chain sat in the decode/fetch interface. In &lt;code&gt;decoder_regs.v&lt;/code&gt;, the number of bytes the fetcher may accept was computed using the last decoded instruction’s length:&lt;/p&gt;
    &lt;code&gt;reg [3:0] decoder_count;
assign acceptable_1     = 4'd12 - decoder_count + consume_count;
always @(posedge clk) begin
  ...
  decoder_count &amp;lt;= after_consume_count + accepted;
end
&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;12&lt;/code&gt; is the buffer size, &lt;code&gt;decoder_count&lt;/code&gt; is the current occupancy, and &lt;code&gt;consume_count&lt;/code&gt; is the length of the outgoing instruction. Reasonable—but computing &lt;code&gt;consume_count&lt;/code&gt; (opcode, ModR/M, etc.) was on the Fmax‑limiting path. By the way, this is one of several well-known problems of the x86 - variable length instructions complicating decoding, another is complex address modes and “effective address” calculation.&lt;/p&gt;
    &lt;p&gt;The fix was to drop the dependency on &lt;code&gt;consume_count&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;assign acceptable_1    = 4'd12 - decoder_count;
&lt;/code&gt;
    &lt;p&gt;This may cause the fetcher to “under‑fetch” for one cycle because the outgoing instruction’s space isn’t reclaimed immediately. But &lt;code&gt;decoder_count&lt;/code&gt; updates next cycle, reclaiming the space. With a 12‑byte buffer, the CPI impact was negligible and Fmax improved measurably on this board.&lt;/p&gt;
    &lt;p&gt;TLB optimization. The Translation Lookaside Buffer (TLB) is a small cache that translates virtual to physical addresses. ao486 uses a 32‑entry fully‑associative TLB with a purely combinational read path—zero extra cycles, but a long path on every memory access (code and data).&lt;/p&gt;
    &lt;p&gt;DOS workloads barely stress the TLB; even many 386 extenders use a flat model. As a first step I converted the TLB to 4‑way set‑associative. That’s simpler and already slightly faster than fully‑associative for these workloads. There’s room to optimize further since the long combinational path rarely helps.&lt;/p&gt;
    &lt;p&gt;A rough v0.1 end‑to‑end result: about +35% per Landmark 6 benchmarks, reaching roughly 486SX‑20 territory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reflections&lt;/head&gt;
    &lt;p&gt;Here are a few reflections after the port:&lt;/p&gt;
    &lt;p&gt;Clock speed scaling. I appreciate the lure of the megahertz race now. Scaling the whole system clock was the most effective lever—more so than extra caches or deeper pipelines at this stage. Up to ~200–300 MHz, CPU, memory, and I/O can often scale together. After that, memory latency dominates, caches grow deeper, and once clock speeds stop increasing, multiprocessing takes over—the story of the 2000s.&lt;/p&gt;
    &lt;p&gt;x86 vs. ARM. Working with ao486 deepened my respect for x86’s complexity. John Crawford’s 1990 paper “The i486 CPU: Executing Instructions in One Clock Cycle” is a great read; it argues convincingly against scrapping x86 for a new RISC ISA given the software base (10K+ apps then). Compatibility was the right bet, but the baggage is real. By contrast, last year’s ARM7‑based GBATang felt refreshingly simple: fixed‑length 32‑bit instructions, saner addressing, and competitive performance. You can’t have your cake and eat it.&lt;/p&gt;
    &lt;p&gt;So there you have it—that’s 486Tang in v0.1. Thanks for reading, and see you next time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/"/></entry><entry><id>https://news.ycombinator.com/item?id=45233237</id><title>Recreating the US/* time zone situation</title><updated>2025-09-14T08:38:38.020891+00:00</updated><content>&lt;doc fingerprint="fda9a4ae35d8380"&gt;
  &lt;main&gt;
    &lt;p&gt;Authors: Mingshi Wu&lt;/p&gt;
    &lt;p&gt;The Great Firewall of China (GFW) experienced the largest leak of internal documents in its history on Thursday September 11, 2025. Over 500 GB of source code, work logs, and internal communication records were leaked, revealing details of the GFW’s research, development, and operations.&lt;/p&gt;
    &lt;p&gt;The leak originated from a core technical force behind the GFW: Geedge Networks (whose chief scientist is Fang Binxing) and the MESA Lab at the Institute of Information Engineering, Chinese Academy of Sciences. The documents show that the company not only provides services to governments in places like Xinjiang, Jiangsu, and Fujian, but also exports censorship and surveillance technology to countries such as Myanmar, Pakistan, Ethiopia, Kazakhstan, and other unidentified country under the “Belt and Road” framework.&lt;/p&gt;
    &lt;p&gt;The significance and far-reaching implications of this leak are substantial. Due to the massive volume of data, GFW Report will continue to analyze and provide updates on the current page and on the Net4People.&lt;/p&gt;
    &lt;p&gt;Enlace Hacktivista has provided the access to the leak:&lt;/p&gt;
    &lt;p&gt;The leaked files total about 600 GB. Among them, the file &lt;code&gt;mirror/repo.tar&lt;/code&gt; alone, as an archive of the RPM packaging server, takes up 500 GB.&lt;/p&gt;
    &lt;p&gt;For detailed instructions on how to use the specific files, David Fifield has already provided a more thorough explanation on Net4People.&lt;/p&gt;
    &lt;code&gt;     7206346  mirror/filelist.txt
497103482880  mirror/repo.tar
 14811058515  geedge_docs.tar.zst
  2724387262  geedge_jira.tar.zst
 35024722703  mesalab_docs.tar.zst
 63792097732  mesalab_git.tar.zst
       71382  A HAMSON-EN.docx
       16982  A Hamson.docx
      161765  BRI.docx
       14052  CPEC.docx
     2068705  CTF-AWD.docx
       19288  Schedule.docx
       26536  TSG Solution Review Description-20230208.docx
      704281  TSG-问题.docx
       35040  chat.docx
       27242  ty-Schedule.docx
      111244  待学习整理-23年MOTC-SWG合同草本V.1-2020230320.docx
       52049  打印.docx
      418620  替票证明.docx
      260551  领导修改版-待看Reponse to Customer's Suggestions-2022110-V001--1647350669.docx
&lt;/code&gt;
    &lt;p&gt;Due to the highly sensitive nature of these leaked materials, we strongly advise anyone who chooses to download and analyze them to take proper operational security precautions. It may be possible that these files may contain potentially risky content and accessing them in an insecure environment could expose you to surveillance or malware.&lt;/p&gt;
    &lt;p&gt;Please consider analyzing these files only in an isolated (virtual) machine without internet access.&lt;/p&gt;
    &lt;p&gt;Great Firewall of China (GFW) is an umbrella term for a series of Internet censorship systems. Behind it, teams for research and development, operations, hardware, and management each play their roles and coordinate with one another. In addition to fixed government agencies (such as the CNCERT), different entities provide technical support depending on individual contracts and tenders. This leak originates from an important branch of the GFW’s R&amp;amp;D capacity: Geedge Networks and MESA Lab. The MESA lab is affiliated with the Institute of Information Engineering, Chinese Academy of Sciences (IIE, CAS).&lt;/p&gt;
    &lt;p&gt;The origins trace back to Fang Binxing, the “Father of the Great Firewall”, coming to Beijing. At the end of 2008, he established the National Engineering Laboratory for Information Content Security (NELIST), initially based at the Institute of Computing Technology, Chinese Academy of Sciences. Beginning in 2012, the supporting institution changed to the Institute of Information Engineering, Chinese Academy of Sciences. In January 2012, some NELIST personnel formed a team at IIE, and in June 2012 the team was officially named the Processing Architecture Team, English name MESA (Massive Effective Stream Analysis). Below is an excerpt from MESA’s self-introduction:&lt;/p&gt;
    &lt;code&gt;MESA Timeline

   January 2012: Liu Qingyun, Sun Yong, Zheng Chao, Yang Rong, Qin Peng, Liu Yang, and Li Jia formed a team at IIE;
   June 2012: The team was officially named the Processing Architecture Team, English name MESA (Massive Effective Stream Analysis);
   2012: Liu Qingyun was selected for IIE’s inaugural “Rising Star” talent program;
   2012: Yang Wei and Zhou Zhou joined the team;
   2012: The team successfully completed the cybersecurity assurance task for the 18th National Congress;
   January 2013: MESA’s first PhD trainee, Liu Tingwen, graduated successfully;
   2013: Li Shu, Liu Junpeng, and Liu Xueli joined the team;
   December 2013: The MESA team received IIE’s 2013 Major Scientific and Technological Progress Award;
   2014: Zhou Zhou was selected for IIE’s “Rising Star” talent program;
   2014: The MESA component SAPP platform began large-scale engineering deployment;
   2014: Zhang Peng, Yu Lingjing, and Jia Mengdie joined the team;
   2015: Zheng Chao was selected for IIE’s “Rising Star” talent program, and Zhang Peng was selected for IIE’s “Outstanding Talent Introduction” program;
   August 2015: MESA moved from the Agriculture Bureau to the Huayan Beili office area;
   July 2015: PhD student Sha Hongzhou trained by MESA graduated successfully, and Liu Xiaomei received Outstanding Graduate honors;
   2016: Dou Fenghu, Zhu Yujia, Wang Fengmei, Li Zhao, Lu Qiuwen, Du Meijie, Shen Yan, and Fang Xupeng joined MESA in succession, and the team expanded rapidly;
   2016: The team undertook multiple major engineering projects, with annual contracted revenue exceeding 35 million;
   December 2016: The MESA team participated in winning the National Science and Technology Progress Award (Second Prize);
   2018: Sun Yong and Zhou Zhou received the 2017 National State Secrecy Science and Technology Award (Second Prize);
&lt;/code&gt;
    &lt;p&gt;By 2018, Fang Binxing had also established himself in Hainan, and Geedge (Hainan) Information Technology Co., Ltd. (Geedge Networks Ltd.) was founded in the same year. Fang served as chief scientist, and the “core R&amp;amp;D personnel came from universities and research institutes such as the Chinese Academy of Sciences, Harbin Institute of Technology, and Beijing University of Posts and Telecommunications.” Much of this talent came from MESA—for example, Zheng Chao served as CTO. Attentive readers will notice that many mentors and students from the MESA timeline appear in the leaked Geedge company git commits.&lt;/p&gt;
    &lt;p&gt;The non–source-code portion of the leaked files has already been analyzed in detail by multiple professional teams. Below are David Fifield’s notes on related media reports and technical write-ups. Please note that the source-code portion of the leak has not yet been analyzed:&lt;/p&gt;
    &lt;p&gt;The source-code portion of the leaked files has not yet been carefully analyzed. This leak is significant and far-reaching. Given the large volume of material, GFW Report will continue to update our analysis and findings on the current page as well as on Net4People.&lt;/p&gt;
    &lt;p&gt;This report was first published on GFW Report. We also actively updated our analysis and findings on Net4People.&lt;/p&gt;
    &lt;p&gt;We encourage you to share questions, comments, analysis, or additional evidence on this topic, either publicly or privately. Our private contact information can be found in the footer of the GFW Report website.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rachelbythebay.com/w/2025/09/12/tz/"/></entry><entry><id>https://news.ycombinator.com/item?id=45233415</id><title>Geedge and MESA leak: Analyzing the great firewall’s largest document leak</title><updated>2025-09-14T08:38:37.477609+00:00</updated><content>&lt;doc fingerprint="fda9a4ae35d8380"&gt;
  &lt;main&gt;
    &lt;p&gt;Authors: Mingshi Wu&lt;/p&gt;
    &lt;p&gt;The Great Firewall of China (GFW) experienced the largest leak of internal documents in its history on Thursday September 11, 2025. Over 500 GB of source code, work logs, and internal communication records were leaked, revealing details of the GFW’s research, development, and operations.&lt;/p&gt;
    &lt;p&gt;The leak originated from a core technical force behind the GFW: Geedge Networks (whose chief scientist is Fang Binxing) and the MESA Lab at the Institute of Information Engineering, Chinese Academy of Sciences. The documents show that the company not only provides services to governments in places like Xinjiang, Jiangsu, and Fujian, but also exports censorship and surveillance technology to countries such as Myanmar, Pakistan, Ethiopia, Kazakhstan, and other unidentified country under the “Belt and Road” framework.&lt;/p&gt;
    &lt;p&gt;The significance and far-reaching implications of this leak are substantial. Due to the massive volume of data, GFW Report will continue to analyze and provide updates on the current page and on the Net4People.&lt;/p&gt;
    &lt;p&gt;Enlace Hacktivista has provided the access to the leak:&lt;/p&gt;
    &lt;p&gt;The leaked files total about 600 GB. Among them, the file &lt;code&gt;mirror/repo.tar&lt;/code&gt; alone, as an archive of the RPM packaging server, takes up 500 GB.&lt;/p&gt;
    &lt;p&gt;For detailed instructions on how to use the specific files, David Fifield has already provided a more thorough explanation on Net4People.&lt;/p&gt;
    &lt;code&gt;     7206346  mirror/filelist.txt
497103482880  mirror/repo.tar
 14811058515  geedge_docs.tar.zst
  2724387262  geedge_jira.tar.zst
 35024722703  mesalab_docs.tar.zst
 63792097732  mesalab_git.tar.zst
       71382  A HAMSON-EN.docx
       16982  A Hamson.docx
      161765  BRI.docx
       14052  CPEC.docx
     2068705  CTF-AWD.docx
       19288  Schedule.docx
       26536  TSG Solution Review Description-20230208.docx
      704281  TSG-问题.docx
       35040  chat.docx
       27242  ty-Schedule.docx
      111244  待学习整理-23年MOTC-SWG合同草本V.1-2020230320.docx
       52049  打印.docx
      418620  替票证明.docx
      260551  领导修改版-待看Reponse to Customer's Suggestions-2022110-V001--1647350669.docx
&lt;/code&gt;
    &lt;p&gt;Due to the highly sensitive nature of these leaked materials, we strongly advise anyone who chooses to download and analyze them to take proper operational security precautions. It may be possible that these files may contain potentially risky content and accessing them in an insecure environment could expose you to surveillance or malware.&lt;/p&gt;
    &lt;p&gt;Please consider analyzing these files only in an isolated (virtual) machine without internet access.&lt;/p&gt;
    &lt;p&gt;Great Firewall of China (GFW) is an umbrella term for a series of Internet censorship systems. Behind it, teams for research and development, operations, hardware, and management each play their roles and coordinate with one another. In addition to fixed government agencies (such as the CNCERT), different entities provide technical support depending on individual contracts and tenders. This leak originates from an important branch of the GFW’s R&amp;amp;D capacity: Geedge Networks and MESA Lab. The MESA lab is affiliated with the Institute of Information Engineering, Chinese Academy of Sciences (IIE, CAS).&lt;/p&gt;
    &lt;p&gt;The origins trace back to Fang Binxing, the “Father of the Great Firewall”, coming to Beijing. At the end of 2008, he established the National Engineering Laboratory for Information Content Security (NELIST), initially based at the Institute of Computing Technology, Chinese Academy of Sciences. Beginning in 2012, the supporting institution changed to the Institute of Information Engineering, Chinese Academy of Sciences. In January 2012, some NELIST personnel formed a team at IIE, and in June 2012 the team was officially named the Processing Architecture Team, English name MESA (Massive Effective Stream Analysis). Below is an excerpt from MESA’s self-introduction:&lt;/p&gt;
    &lt;code&gt;MESA Timeline

   January 2012: Liu Qingyun, Sun Yong, Zheng Chao, Yang Rong, Qin Peng, Liu Yang, and Li Jia formed a team at IIE;
   June 2012: The team was officially named the Processing Architecture Team, English name MESA (Massive Effective Stream Analysis);
   2012: Liu Qingyun was selected for IIE’s inaugural “Rising Star” talent program;
   2012: Yang Wei and Zhou Zhou joined the team;
   2012: The team successfully completed the cybersecurity assurance task for the 18th National Congress;
   January 2013: MESA’s first PhD trainee, Liu Tingwen, graduated successfully;
   2013: Li Shu, Liu Junpeng, and Liu Xueli joined the team;
   December 2013: The MESA team received IIE’s 2013 Major Scientific and Technological Progress Award;
   2014: Zhou Zhou was selected for IIE’s “Rising Star” talent program;
   2014: The MESA component SAPP platform began large-scale engineering deployment;
   2014: Zhang Peng, Yu Lingjing, and Jia Mengdie joined the team;
   2015: Zheng Chao was selected for IIE’s “Rising Star” talent program, and Zhang Peng was selected for IIE’s “Outstanding Talent Introduction” program;
   August 2015: MESA moved from the Agriculture Bureau to the Huayan Beili office area;
   July 2015: PhD student Sha Hongzhou trained by MESA graduated successfully, and Liu Xiaomei received Outstanding Graduate honors;
   2016: Dou Fenghu, Zhu Yujia, Wang Fengmei, Li Zhao, Lu Qiuwen, Du Meijie, Shen Yan, and Fang Xupeng joined MESA in succession, and the team expanded rapidly;
   2016: The team undertook multiple major engineering projects, with annual contracted revenue exceeding 35 million;
   December 2016: The MESA team participated in winning the National Science and Technology Progress Award (Second Prize);
   2018: Sun Yong and Zhou Zhou received the 2017 National State Secrecy Science and Technology Award (Second Prize);
&lt;/code&gt;
    &lt;p&gt;By 2018, Fang Binxing had also established himself in Hainan, and Geedge (Hainan) Information Technology Co., Ltd. (Geedge Networks Ltd.) was founded in the same year. Fang served as chief scientist, and the “core R&amp;amp;D personnel came from universities and research institutes such as the Chinese Academy of Sciences, Harbin Institute of Technology, and Beijing University of Posts and Telecommunications.” Much of this talent came from MESA—for example, Zheng Chao served as CTO. Attentive readers will notice that many mentors and students from the MESA timeline appear in the leaked Geedge company git commits.&lt;/p&gt;
    &lt;p&gt;The non–source-code portion of the leaked files has already been analyzed in detail by multiple professional teams. Below are David Fifield’s notes on related media reports and technical write-ups. Please note that the source-code portion of the leak has not yet been analyzed:&lt;/p&gt;
    &lt;p&gt;The source-code portion of the leaked files has not yet been carefully analyzed. This leak is significant and far-reaching. Given the large volume of material, GFW Report will continue to update our analysis and findings on the current page as well as on Net4People.&lt;/p&gt;
    &lt;p&gt;This report was first published on GFW Report. We also actively updated our analysis and findings on Net4People.&lt;/p&gt;
    &lt;p&gt;We encourage you to share questions, comments, analysis, or additional evidence on this topic, either publicly or privately. Our private contact information can be found in the footer of the GFW Report website.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gfw.report/blog/geedge_and_mesa_leak/en/"/></entry><entry><id>https://news.ycombinator.com/item?id=45233713</id><title>RIP pthread_cancel</title><updated>2025-09-14T08:38:36.987880+00:00</updated><content>&lt;doc fingerprint="262621567e35bc58"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;RIP pthread_cancel&lt;/head&gt;
    &lt;p&gt;I posted about adding pthread_cancel use in curl about three weeks ago, we released this in curl 8.16.0 and it blew up right in our faces. Now, with #18540 we are ripping it out again. What happened?&lt;/p&gt;
    &lt;head rend="h2"&gt;short recap&lt;/head&gt;
    &lt;p&gt;pthreads define “Cancelation points”, a list of POSIX functions where a pthread may be cancelled. In addition, there is also a list of functions that may be cancelation points, among those &lt;code&gt;getaddrinfo()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;getaddrinfo()&lt;/code&gt; is exactly what we are interested in for &lt;code&gt;libcurl&lt;/code&gt;. It blocks
until it has resolved a name. That may hang for a long time and &lt;code&gt;libcurl&lt;/code&gt;
is unable to do anything else. Meh. So, we start a pthread and let that
call &lt;code&gt;getaddrinfo()&lt;/code&gt;. &lt;code&gt;libcurl&lt;/code&gt; can do other things while that thread runs.&lt;/p&gt;
    &lt;p&gt;But eventually, we have to get rid of the pthread again. Which means we either have to &lt;code&gt;pthread_join()&lt;/code&gt; it - which means a blocking wait. Or we
call &lt;code&gt;pthread_detach()&lt;/code&gt; - which returns immediately but the thread keeps
on running. Both are bad when you want to do many, many transfers. Either we block and
stall or we let pthreads pile up in an uncontrolled way.&lt;/p&gt;
    &lt;p&gt;So, we added &lt;code&gt;pthread_cancel()&lt;/code&gt; to interrupt a running &lt;code&gt;getaddrinfo()&lt;/code&gt;
and get rid of the pthread we no longer needed. So the theory. And, after
some hair pulling, we got this working.&lt;/p&gt;
    &lt;head rend="h2"&gt;cancel yes, leakage also yes!&lt;/head&gt;
    &lt;p&gt;After releasing curl 8.16.0 we got an issue reported in #18532 that cancelled pthreads leaked memory.&lt;/p&gt;
    &lt;p&gt;Digging into the glibc source shows that there is this thing called &lt;code&gt;/etc/gai.conf&lt;/code&gt;
which defines how &lt;code&gt;getaddrinfo()&lt;/code&gt; should sort returned answers.&lt;/p&gt;
    &lt;p&gt;The implementation in glibc first resolves the name to addresses. For these, it needs to allocate memory. Then it needs to sort them if there is more than one address. And in order to do that it needs to read &lt;code&gt;/etc/gai.conf&lt;/code&gt;. And in order to do that
it calls &lt;code&gt;fopen()&lt;/code&gt; on the file. And that may be a pthread “Cancelation Point”
(and if not, it surely calls &lt;code&gt;open()&lt;/code&gt; which is a required cancelation point).&lt;/p&gt;
    &lt;p&gt;So, the pthread may get cancelled when reading &lt;code&gt;/etc/gai.conf&lt;/code&gt; and leak all
the allocated responses. And if it gets cancelled there, it will try to
read &lt;code&gt;/etc/gai.conf&lt;/code&gt; again the next time it has more than one address
resolved.&lt;/p&gt;
    &lt;p&gt;At this point, I decided that we need to give up on the whole &lt;code&gt;pthread_cancel()&lt;/code&gt;
strategy. The reading of &lt;code&gt;/etc/gai.conf&lt;/code&gt; is one point where a cancelled
&lt;code&gt;getaddrinfo()&lt;/code&gt; may leak. There might be others. Clearly, glibc is not really
designed to prevent leaks here (admittedly, this is not trivial).&lt;/p&gt;
    &lt;head rend="h2"&gt;RIP&lt;/head&gt;
    &lt;p&gt;Leaking memory potentially on something &lt;code&gt;libcurl&lt;/code&gt; does over and over again is
not acceptable. We’d rather pay the price of having to eventually wait on
a long running &lt;code&gt;getaddrinfo()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Applications using &lt;code&gt;libcurl&lt;/code&gt; can avoid this by using &lt;code&gt;c-ares&lt;/code&gt; which resolves
unblocking and without the use of threads. But that will not be able to do
everything that glibc does.&lt;/p&gt;
    &lt;p&gt;DNS continues to be tricky to use well.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eissing.org/icing/posts/rip_pthread_cancel/"/></entry><entry><id>https://news.ycombinator.com/item?id=45234323</id><title>The case against social media is stronger than you think</title><updated>2025-09-14T08:38:36.925218+00:00</updated><content/><link href="https://arachnemag.substack.com/p/the-case-against-social-media-is"/></entry><entry><id>https://news.ycombinator.com/item?id=45234460</id><title>Safe C++ proposal is not being continued</title><updated>2025-09-14T08:38:36.687650+00:00</updated><content>&lt;doc fingerprint="d662bd2f20e73a5c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Safe C++ proposal is not being continued&lt;/head&gt;
    &lt;p&gt;One year ago, the Safe C++ proposal was made. The goal was to add a safe subset/context into C++ that would give strong guarantees (memory safety, type safety, thread safety) similar to what Rust provides, without breaking existing C++ code. It was an extension or superset of C++. The opt-in mechanism was to explicitly mark parts of the code that belong to the safe context. The authors even state:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Code in the safe context exhibits the same strong safety guarantees as code written in Rust.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The rest remains “unsafe” in the usual C++ sense. This means that existing code continues to work, while new or refactored parts can gain safety. For those who write Rust, Safe C++ has many similarities with Rust, sometimes with adjustments to fit C++’s design. Also, because C++ already has a huge base of “unsafe code”, Safe C++ has to provide mechanisms for mixing safe and unsafe, and for incremental migration. In that sense, all of Safe C++’s safe features are opt-in. Existing code compiles and works as before. Introducing safe context doesn’t break code that doesn’t use it.&lt;/p&gt;
    &lt;p&gt;The proposal caught my interest. It seemed like a good compromise to make C++ safe, although there were open or unresolved issues, which is completely normal for a draft proposal. For example, how error reporting for the borrow checker and lifetime errors would work, or how generic code and templates would interact with lifetime logic and safe/unsafe qualifiers. These are just some of the points, the proposal is very long and elaborate. Moreover, I am not a programming language designer, so there might be better alternatives.&lt;/p&gt;
    &lt;p&gt;Anyway, today I discovered that the proposal will no longer be pursued. When I thought about the proposal again this morning, I realized I hadn’t read any updates on it for some time. So I searched and found some answers on Reddit.&lt;/p&gt;
    &lt;p&gt;The response from Sean Baxter, one of the original authors of the Safe C++ proposal:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The Safety and Security working group voted to prioririze Profiles over Safe C++. Ask the Profiles people for an update. Safe C++ is not being continued.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And again:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The Rust safety model is unpopular with the committee. Further work on my end won’t change that. Profiles won the argument. All effort should go into getting Profile’s language for eliminating use-after-free bugs, data races, deadlocks and resource leaks into the Standard, so that developers can benefit from it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So I went to read the documents related to Profiles[1][2][3][4]. I try to summarize what I understood: they are meant to define modes of C++ that impose constraints on how you use the language and library, in order to guarantee certain safety properties. They are primarily compile-time constraints, though in practice some checks may be implemented using library facilities that add limited runtime overhead. Instead of introducing entirely new language constructs, profiles mostly restrict existing features and usages. The idea is that you can enable a profile, and any code using it agrees to follow the restrictions. If you don’t enable it, things work as before. So it’s backwards-compatible.&lt;/p&gt;
    &lt;p&gt;Profiles seem less radical and more adoptable, a safer-by-default C++ without forcing the Rust model that aims to tackle the most common C++ pitfalls. I think Safe C++ was more ambitious: introducing new syntax, type qualifiers, safe vs unsafe contexts, etc. Some in the committee felt that was too heavy, and Profiles are seen as a more pragmatic path. The main objection is obvious: one could say that Profiles restrict less than what Safe C++ aimed to provide.&lt;/p&gt;
    &lt;p&gt;Reading comments here and there, there is visible resistance in the community toward adopting the Rust model, and from a certain point of view, I understand it. If you want to write like Rust, just write Rust. Historically, C++ is a language that has often taken features from other worlds and integrated them into itself. In this case, I think that safety subsets of C++ already exist informally somehow. Profiles are an attempt to standardize and unify something that already exists in practice. Technically, they don’t add new fundamental semantics. Instead, they provide constraints, obligations and guarantees.&lt;/p&gt;
    &lt;p&gt;In my opinion, considering the preferences of the committee and the entire C++ community, although I appreciated the Safe C++ proposal and was looking forward to seeing concrete results, considering the C++ context I believe that standardizing and integrating the Profiles as proposed is a much more realistic approach. Profiles might not be perfect, but they are better than nothing. They will likely be uneven in enforcement and weaker than Safe C++ in principle. They won’t give us silver-bullet guarantees, but they are a realistic path forward.&lt;/p&gt;
    &lt;p&gt;[1] Core safety profiles for C++26&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sibellavia.lol/posts/2025/09/safe-c-proposal-is-not-being-continued/"/></entry><entry><id>https://news.ycombinator.com/item?id=45235293</id><title>AMD’s RDNA4 GPU architecture</title><updated>2025-09-14T08:38:36.495951+00:00</updated><content>&lt;doc fingerprint="72d3f278b06081b4"&gt;
  &lt;main&gt;
    &lt;p&gt;RDNA4 is AMD’s latest graphics-focused architecture, and fills out their RX 9000 line of discrete GPUs. AMD noted that creating a good gaming GPU requires understanding both current workloads, as well as taking into account what workloads might look like five years in the future. Thus AMD has been trying to improve efficiency across rasterization, compute, and raytracing. Machine learning has gained importance including in games, so AMD’s new GPU architecture caters to ML workloads as well.&lt;/p&gt;
    &lt;p&gt;From AMD’s perspective, RDNA4 represents a large efficiency leap in raytracing and machine learning, while also improving on the rasterization front. Improved compression helps keep the graphics architecture fed. Outside of the GPU’s core graphics acceleration responsibility, RDNA4 brings improved media and display capabilities to round out the package.&lt;/p&gt;
    &lt;head rend="h1"&gt;Media Engine&lt;/head&gt;
    &lt;p&gt;The Media Engine provides hardware accelerated video encode and decode for a wide range of codecs. High end RDNA4 parts like the RX 9070XT have two media engines. RDNA4’s media engines feature faster decoding speed, helping save power during video playback by racing to idle. For video encoding, AMD targeted better quality in H.265, H.265, and AV1, especially in low latency encoding.&lt;/p&gt;
    &lt;p&gt;Low latency encoder modes are mostly beneficial for streaming, where delays caused by the media engine ultimately translate to a delayed stream. Reducing latency can make quality optimizations more challenging. Video codecs strive to encode differences between frames to economize storage. Buffering up more frames gives the encoder more opportunities to look for similar content across frames, and lets it allocate more bitrate budget for difficult sequences. But buffering up frames introduces latency. Another challenge is some popular streaming platforms mainly use H.264, an older codec that’s less efficient than AV1. Newer codecs are being tested, so the situation may start to change as the next few decades fly by. But for now, H.264 remains important due to its wide support.&lt;/p&gt;
    &lt;p&gt;Testing with an old gameplay clip from Elder Scrolls Online shows a clear advantage for RDNA4’s media engine when testing with the latency-constrained VBR mode and encoder tuned for low latency encoding (-usage lowlatency -rc vbr_latency). Netflix’s VMAF video quality metric gives higher scores for RDNA4 throughout the bitrate range. Closer inspection generally agrees with the VMAF metric.&lt;/p&gt;
    &lt;p&gt;RDNA4 does a better job preserving high contrast outlines. Differences are especially visible around text, which RDNA4 handles better than its predecessor while using a lower bitrate. Neither result looks great with such a close look, with blurred text on both examples and fine detail crushed in video encoding artifacts. But it’s worth remembering that the latency-constrained VBR mode uses a VBV buffer of up to three frames, while higher latency modes can use VBV buffer sizes covering multiple seconds of video. Encoding speed has improved slightly as well, jumping from ~190 to ~200 FPS from RDNA3.5 to RDNA4.&lt;/p&gt;
    &lt;head rend="h1"&gt;Display Engine&lt;/head&gt;
    &lt;p&gt;The display engine fetches on-screen frame data from memory, composites it into a final image, and drives it to the display outputs. It’s a basic task that most people take for granted, but the display engine is also a good place to perform various image enhancements. A traditional example is using a lookup table to apply color correction. Enhancements at the display engine are invisible to user software, and are typically carried out in hardware with minimal power cost. On RDNA4, AMD added a “Radeon Image Sharpening” filter, letting the display engine sharpen the final image. Using dedicated hardware at the display engine instead of the GPU’s programmable shaders means that the sharpening filter won’t impact performance and can be carried out with better power efficiency. And, AMD doesn’t need to rely on game developers to implement the effect. Sharpening can even apply to the desktop, though I’m not sure why anyone would want that.&lt;/p&gt;
    &lt;p&gt;Power consumption is another important optimization area for display engines. Traditionally that’s been more of a concern for mobile products, where maximizing battery life under low load is a top priority. But RDNA4 has taken aim at multi-monitor idle power with its newer display engine. AMD’s presentation stated that they took advantage of variable refresh rates on FreeSync displays. They didn’t go into more detail, but it’s easy to imagine what AMD might be doing. High resolution and high refresh rate displays translate to high pixel rates. That in turn drives higher memory bandwidth demands. Dynamically lowering refresh rates could let RDNA4’s memory subsystem enter a low power state while still meeting refresh deadlines.&lt;/p&gt;
    &lt;p&gt;I have a RX 9070 hooked up to a Viotek GN24CW 1080P display via HDMI, and a MSI MAG271QX 1440P capable of refresh rates up to 360 Hz. The latter is connected via DisplayPort. The RX 9070 manages to keep memory at idle clocks even at high refresh rate settings. Moving the mouse causes the card to ramp up memory clocks and consume more power, hinting that RDNA4 is lowering refresh rates when screen contents don’t change. Additionally, RDNA4 gets an intermediate GDDR6 power state that lets it handle the 1080P 60 Hz + 1440P 240 Hz combination without going to maximum memory clocks. On RDNA2, it’s more of an all or nothing situation. The older card is more prone to ramping up memory clocks to handle high pixel rates, and power consumption remains high even when screen contents don’t change.&lt;/p&gt;
    &lt;head rend="h1"&gt;Compute Changes&lt;/head&gt;
    &lt;p&gt;RDNA4’s Workgroup Processor retains the same high level layout as prior RDNA generations. However, it gets major improvements targeted towards raytracing, like improved raytracing units and wider BVH nodes, a dynamic register allocation mode, and a scheduler that no longer suffers false memory dependencies between waves. I covered those in previous articles. Besides those improvements, AMD’s presentation went over a couple other details worth discussing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scalar Floating Point Instructions&lt;/head&gt;
    &lt;p&gt;AMD has a long history of using a scalar unit to offload operations that are constant across a wave. Scalar offload saves power by avoiding redundant computation, and frees up the vector unit to increase performance in compute-bound sequences. RDNA4’s scalar unit gains a few floating point instructions, expanding scalar offload opportunities. This capability debuted on RDNA3.5, but RDNA4 brings it to discrete GPUs.&lt;/p&gt;
    &lt;p&gt;While not discussed in AMD’s presentation, scalar offload can bring additional performance benefits because scalar instructions sometimes have lower latency than their vector counterparts. Most basic vector instructions on RDNA4 have 5 cycle latency. FP32 adds and multiples on the scalar unit have 4 cycle latency. The biggest latency benefits still come from offloading integer operations though.&lt;/p&gt;
    &lt;head rend="h2"&gt;Split Barriers&lt;/head&gt;
    &lt;p&gt;GPUs use barriers to synchronize threads and enforce memory ordering. For example, a s_barrier instruction on older AMD GPUs would cause a thread to wait until all of its peers in the workgroup also reached the s_barrier instruction. Barriers degrade performance because any thread that happened to reach the barrier faster would have to stall until its peers catch up.&lt;/p&gt;
    &lt;p&gt;RDNA4 splits the barrier into separate “signal” and “wait” actions. Instead of s_barrier, RDNA4 has s_barrier_signal and s_barrier_wait. A thread can “signal” the barrier once it produces data that other threads might need. It can then do independent work, and only wait on the barrier once it needs to use data produced by other threads. The s_barrier_wait will then stall the thread until all other threads in the workgroup have signalled the barrier.&lt;/p&gt;
    &lt;head rend="h1"&gt;Memory Subsystem&lt;/head&gt;
    &lt;p&gt;The largest RDNA4 variants have a 8 MB L2 cache, representing a substantial L2 capacity increase compared to prior RDNA generations. RDNA3 and RDNA2 maxed out at 6 MB and 4 MB L2 capacities, respectively. AMD found that difficult workloads like raytracing benefit from the larger L2. Raytracing involves pointer chasing during BVH traversal, and it’s not surprising that it’s more sensitive to accesses getting serviced from the slower Infinity Cache as opposed to L2. In the initial scene in 3DMark’s DXR feature test, run in Explorer Mode, RDNA4 dramatically cuts down the amount of data that has to be fetched from beyond L2.&lt;/p&gt;
    &lt;p&gt;RDNA2 still does a good job of keeping data in L2 in absolute terms. But it’s worth noting that hitting Infinity Cache on both platforms adds more than 50 ns of extra latency over a L2 hit. That’s well north of 100 cycles because both RDNA2 and RDNA4 run above 2 GHz. While AMD’s graphics strategy has shifted towards making the faster caches bigger, it still contrasts with Nvidia’s strategy of putting way more eggs in the L2 basket. Blackwell’s L2 cache serves the functions of both AMD’s L2 and Infinity Cache, and has latency between those two cache levels. Nvidia also has a flexible L1/shared memory allocation scheme that can give them more low latency caching capacity in front of L2, depending on a workload’s requested local storage (shared memory) capacity.&lt;/p&gt;
    &lt;p&gt;A mid-level L1 cache was a familiar fixture on prior RDNA generations. It’s conspicuously missing from RDNA4, as well as AMD’s presentation. One possibility is that L1 cache hitrate wasn’t high enough to justify the complexity of an extra cache level. Perhaps AMD felt its area and transistor budget was better allocated towards increasing L2 capacity. To support this theory, L1 hitrate on RDNA1 was often below 50%. At the same time, the RDNA series always enjoyed a high bandwidth and low latency L2. Putting more pressure on L2 in exchange for reducing L2 misses may have been an enticing tradeoff. Another possibility is that AMD ran into validation issues with the L1 cache and decided to skip it for this generation. There’s no way to verify either possibility of course, but I think the former reasons make more sense.&lt;/p&gt;
    &lt;p&gt;Beyond tweaking the cache hierarchy, RDNA4 brings improvements to transparent compression. AMD emphasized that they’re using compression throughout the SoC, including at points like the display engine and media engine. Compressed data can be stored in caches, and decompressed before being written back to memory. Compression cuts down on data transfer, which reduces bandwidth requirements and improves power efficiency.&lt;/p&gt;
    &lt;p&gt;Transparent compression is not a new feature. It has a long history of being one tool in the GPU toolbox for reducing memory bandwidth usage, and it would be difficult to find any modern GPU without compression features of some sort. Even compression in other blocks like the display engine have precedent. Intel’s display engines for example use Framebuffer Compression (FBC), which can write a compressed copy of frame data and keep fetching the compressed copy to reduce data transfer power usage as long as the data doesn’t change. Prior RDNA generations had compression features too, and AMD’sdocumentation summarizes some compression targets. While AMD didn’t talk about compression efficiency, I tried to take similar frame captures using RGP on both RDNA1 and RDNA4 to see if there’s a large difference in memory access per frame. It didn’t quite work out the way I expected, but I’ll put them here anyway and discuss why evaluating compression efficacy is challenging.&lt;/p&gt;
    &lt;p&gt;The first challenge is that both architectures satisfy most memory requests from L0 or L1. AMD slides on RDNA1 suggest the L0 and L1 only hold decompressed data, at least for delta color compression. Compression does apply to L2. For RDNA4, AMD’s slides indicate it applies to the Infinity Cache too. However, focusing on data transfer to and from the L2 wouldn’t work due the large cache hierarchy differences between those RDNA generations.&lt;/p&gt;
    &lt;p&gt;Another issue is, it’s easy to imagine a compression scheme that doesn’t change the number of cache requests involved. For example, data might be compressed to only take up part of a cacheline. A request only causes a subset of the cacheline to be read out, which a decompressor module expands to the full 128B. Older RDNA1 slides are ambiguous about this, indicating that DCC operates on 256B granularity (two cachelines) without providing further details.&lt;/p&gt;
    &lt;p&gt;In any case, compression may be a contributing factor in RDNA4 being able to achieve better performance while using a smaller Infinity Cache than prior generations, despite only having a 256-bit GDDR6 DRAM setup.&lt;/p&gt;
    &lt;head rend="h1"&gt;SoC Features&lt;/head&gt;
    &lt;p&gt;AMD went over RAS, or reliability, availability, and serviceability features in RDNA4. Modern chips use parity and ECC to detect errors and correct them, and evidently RDNA4 does the same. Unrecoverable errors are handled with driver intervention, by “re-initializing the relevant portion of the SoC, thus preventing the platform from shutting down”. There’s two ways to interpret that statement. One is that the GPU can be re-initialized to recover from hardware errors, obviously affecting any software relying on GPU acceleration. Another is that some parts of the GPU can be re-initialized while the GPU continues handling work. I think the former is more likely, though I can imagine the latter being possible in limited forms too. For example, an unrecoverable error reading from GDDR6 can hypothetically be fixed if that data is backed by a duplicate in system memory. The driver could transfer known-good data from the host to replace the corrupted copy. But errors with modified data would be difficult to recover from, because there might not be an up-to-date copy elsewhere in the system.&lt;/p&gt;
    &lt;p&gt;On the security front, microprocessors get private buses to “critical blocks” and protected register access mechanisms. Security here targets HDCP and other DRM features, which I don’t find particularly amusing. But terminology shown on the slide is interesting, because MP0 and MP1 are also covered in AMD’s CPU-side documentation. On the CPU side, MP0 (microprocessor 0) handles some Secure Encrypted Virtualization (SEV) features. It’s sometimes called the Platform Security Processor (PSP) too. MP1 on CPUs is called the System Management Unit (SMU), which covers power control functions. Curiously AMD’s slide labels MP1 and the SMU separately on RDNA4. MP0/MP1 could have completely different functions on GPUs of course. But the common terminology raises the possibility that there’s a lot of shared work between CPU and GPU SoC design. RAS is also a very traditional CPU feature, though GPUs have picked up RAS features over time as GPU compute picked up steam.&lt;/p&gt;
    &lt;head rend="h2"&gt;Infinity Fabric&lt;/head&gt;
    &lt;p&gt;One of the most obvious examples of shared effort between the CPU and GPU sides is Infinity Fabric making its way to graphics designs. This started years ago with Vega, though back then using Infinity Fabric was more of an implementation detail. But years later, Infinity Fabric components provided an elegant way to implement a large last level cache, or multi-socket coherent systems with gigantic iGPUs (like MI300A).&lt;/p&gt;
    &lt;p&gt;The Infinity Fabric memory-side subsystem on RDNA4 consists of 16 CS (Coherent Station) blocks, each paired with a Unified Memory Controller (UMC). Coherent Stations receive requests coming off the graphics L2 and other clients. They ensure coherent memory access by either getting data from a UMC, or by sending a probe if another block has a more up-to-date copy of the requested cacheline. The CS is a logical place to implement a memory side cache, and each CS instance has 4 MB of cache in RDNA4.&lt;/p&gt;
    &lt;p&gt;To save power, Infinity Fabric supports DVFS (dynamic voltage and frequency scaling) to save power, and clocks between 1.5 and 2.5 GHz. Infinity Fabric bandwidth is 1024 bits per clock, which suggests the Infinity Cache can provide 2.5 TB/s of theoretical bandwidth. That roughly lines up with results from Nemes’s Vulkan-based GPU cache and memory bandwidth microbenchmark.&lt;/p&gt;
    &lt;p&gt;AMD also went over their ability to disable various SoC components to harvest dies and create different SKUs. Shader Engines, WGPs, and memory controller channels can be disabled. AMD and other manufacturers have used similar harvesting capabilities in the past. I’m not sure what’s new here. Likely, AMD wants to re-emphasize their harvesting options.&lt;/p&gt;
    &lt;p&gt;Finally, AMD mentioned that they chose a monolithic design for RDNA4 because it made sense for a graphics engine of its size. They looked at performance goals, package assembly and turnaround time, and cost. After evaluating those factors, they decided a monolithic design was the right option. It’s not a surprise. After all, AMD used monolithic designs for lower end RDNA3 products with smaller graphics engines, and only used chiplets for the largest SKUs. Rather, it’s a reminder that there’s no one size fits all solution. Whether a monolithic or chiplet-based design makes more sense depends heavily on design goals.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;RDNA4 brings a lot of exciting improvements to the table, while breaking away from any attempt to tackle the top end performance segment. Rather than going for maximum performance, RDNA4 looks optimized to improve efficiency over prior generations. The RX 9070 offers similar performance to the RX 7900XT in rasterization workloads despite having a lower power budget, less memory bandwidth, and a smaller last level cache. Techspot also shows the RX 9070 leading with raytracing workloads, which aligns with AMD's goal of enhancing raytracing performance.&lt;/p&gt;
    &lt;p&gt;AMD achieves this efficiency using compression, better raytracing structures, and a larger L2 cache. As a result, RDNA4 can pack its performance into a relatively small 356.5 mm² die and use a modest 256-bit GDDR6 memory setup. Display and media engine improvements are welcome too. Multi-monitor idle power feels like a neglected area for discrete GPUs, even though I know many people use multiple monitors for productivity. Lowering idle power in those setups is much appreciated. On the media engine side, AMD’s video encoding capabilities have often lagged behind the competition. RDNA4’s progress at least prevents AMD from falling as far behind as they have before.&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipsandcheese.com/p/amds-rdna4-gpu-architecture-at-hot"/></entry><entry><id>https://news.ycombinator.com/item?id=45235648</id><title>Myocardial infarction may be an infectious disease</title><updated>2025-09-14T08:38:35.318153+00:00</updated><content>&lt;doc fingerprint="6ecc39dea85fc3ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Myocardial infarction may be an infectious disease&lt;/head&gt;
    &lt;p&gt;According to the recently published research, an infection may trigger myocardial infarction. Using a range of advanced methodologies, the research found that, in coronary artery disease, atherosclerotic plaques containing cholesterol may harbour a gelatinous, asymptomatic biofilm formed by bacteria over years or even decades. Dormant bacteria within the biofilm remain shielded from both the patient’s immune system and antibiotics because they cannot penetrate the biofilm matrix.&lt;/p&gt;
    &lt;p&gt;A viral infection or another external trigger may activate the biofilm, leading to the proliferation of bacteria and an inflammatory response. The inflammation can cause a rupture in the fibrous cap of the plaque, resulting in thrombus formation and ultimately myocardial infarction.&lt;/p&gt;
    &lt;p&gt;Professor Pekka Karhunen, who led the study, notes that until now, it was assumed that events leading to coronary artery disease were only initiated by oxidised low-density lipoprotein (LDL), which the body recognises as a foreign structure.&lt;/p&gt;
    &lt;p&gt;“Bacterial involvement in coronary artery disease has long been suspected, but direct and convincing evidence has been lacking. Our study demonstrated the presence of genetic material – DNA – from several oral bacteria inside atherosclerotic plaques,” Karhunen explains.&lt;/p&gt;
    &lt;p&gt;The findings were validated by developing an antibody targeted at the discovered bacteria, which unexpectedly revealed biofilm structures in arterial tissue. Bacteria released from the biofilm were observed in cases of myocardial infarction. The body’s immune system had responded to these bacteria, triggering inflammation which ruptured the cholesterol-laden plaque.&lt;/p&gt;
    &lt;p&gt;The observations pave the way for the development of novel diagnostic and therapeutic strategies for myocardial infarction. Furthermore, they advance the possibility of preventing coronary artery disease and myocardial infarction by vaccination.&lt;/p&gt;
    &lt;p&gt;The study was conducted by Tampere and Oulu Universities, Finnish Institute for Health and Welfare and the University of Oxford. Tissue samples were obtained from individuals who had died from sudden cardiac death, as well as from patients with atherosclerosis who were undergoing surgery to cleanse carotid and peripheral arteries.&lt;/p&gt;
    &lt;p&gt;The research is part of an extensive EU-funded cardiovascular research project involving 11 countries. Significant funding was also provided by the Finnish Foundation for Cardiovascular Research and Jane and Aatos Erkko Foundation.&lt;/p&gt;
    &lt;p&gt;The research article Viridans Streptococcal Biofilm Evades Immune Detection and Contributes to Inflammation and Rupture of Atherosclerotic Plaques was published in the Journal of the American Heart Association on 6 August 2025. Read the article online&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt;Further information&lt;/head&gt;
    &lt;p&gt;Professor Pekka Karhunen&lt;lb/&gt;Faculty of Medicine and Health Technology&lt;lb/&gt;Tampere University&lt;lb/&gt;pekka.j.karhunen [at] tuni.fi (pekka[dot]j[dot]karhunen[at]tuni[dot]fi)&lt;lb/&gt;Tel. +358 400 511361&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tuni.fi/en/news/myocardial-infarction-may-be-infectious-disease"/></entry><entry><id>https://news.ycombinator.com/item?id=45235676</id><title>Will AI be the basis of many future industrial fortunes, or a net loser?</title><updated>2025-09-14T08:38:35.076942+00:00</updated><content>&lt;doc fingerprint="bb051051dfb505ec"&gt;
  &lt;main&gt;
    &lt;p&gt;Fortunes are made by entrepreneurs and investors when revolutionary technologies enable waves of innovative, investable companies. Think of the railroad, the Bessemer process, electric power, the internal combustion engine, or the microprocessor—each of which, like a stray spark in a fireworks factory, set off decades of follow-on innovations, permeated every part of society, and catapulted a new set of inventors and investors into power, influence, and wealth.&lt;/p&gt;
    &lt;p&gt;Yet some technological innovations, though societally transformative, generate little in the way of new wealth; instead, they reinforce the status quo. Fifteen years before the microprocessor, another revolutionary idea, shipping containerization, arrived at a less propitious time, when technological advancement was a Red Queen’s race, and inventors and investors were left no better off for non-stop running.&lt;/p&gt;
    &lt;p&gt;Anyone who invests in the new new thing must answer two questions: First, how much value will this innovation create? And second, who will capture it? Information and communication technology (ICT) was a revolution whose value was captured by startups and led to thousands of newly rich founders, employees, and investors. In contrast, shipping containerization was a revolution whose value was spread so thin that in the end, it made only a single founder temporarily rich and only a single investor a little bit richer.&lt;/p&gt;
    &lt;p&gt;Is generative AI more like the former or the latter? Will it be the basis of many future industrial fortunes, or a net loser for the investment community as a whole, with a few zero-sum winners here and there?&lt;/p&gt;
    &lt;p&gt;There are ways to make money investing in the fruits of AI, but they will depend on assuming the latter—that it is once again a less propitious time for inventors and investors, that AI model builders and application companies will eventually compete each other into an oligopoly, and that the gains from AI will accrue not to its builders but to customers. A lot of the money pouring into AI is therefore being invested in the wrong places, and aside from a couple of lucky early investors, those who make money will be the ones with the foresight to get out early.&lt;/p&gt;
    &lt;p&gt;The microprocessor was revolutionary, but the people who invented it at Intel in 1971 did not see it that way—they just wanted to avoid designing desktop calculator chipsets from scratch every time. But outsiders realized they could use the microprocessor to build their own personal computers, and enthusiasts did. Thousands of tinkerers found configurations and uses that Intel never dreamed of. This distributed and permissionless invention kicked off a “great surge of development,” as the economist Carlota Perez calls it, triggered by technology but driven by economic and societal forces.[1]&lt;/p&gt;
    &lt;p&gt;There was no real demand for personal computers in the early 1970s; they were expensive toys. But the experimenters laid the technical groundwork and built a community. Then, around 1975, a step-change in the cost of microprocessors made the personal computer market viable. The Intel 8080 had an initial list price of $360 ($2,300 in today’s dollars). MITS could barely turn a profit on its Altair at a bulk price of $75 each ($490 today). But when MOS Technologies started selling its 6502 for $25 ($150 today), Steve Wozniak could afford to build a prototype Apple. The 6502 and the similarly priced Zilog Z80 forced Intel’s prices down. The nascent PC community started spawning entrepreneurs and a score of companies appeared, each with a slightly different product.&lt;/p&gt;
    &lt;p&gt;You couldn’t have known in the mid-1970s that the PC (and PC-like products, such as ATMs, POS terminals, smartphones, etc.) would revolutionize everything. While Steve Jobs was telling investors that every household would someday have a personal computer (a wild underestimate, as it turned out), others questioned the need for personal computers at all. As late as 1979, Apple’s ads didn’t tell you what a personal computer could do—it asked what you did with it.[2] The established computer manufacturers (IBM, HP, DEC) had no interest in a product their customers weren’t asking for. Nobody “needed” a computer, and so PCs weren’t bought—they were sold. Flashy startups like Apple and Sinclair used hype to get noticed, while companies with footholds in consumer electronics like Atari, Commodore, and Tandy/RadioShack used strong retail connections to put their PCs in front of potential customers.&lt;/p&gt;
    &lt;p&gt;The market grew slowly at first, accelerating only as experiments led to practical applications like the spreadsheet, introduced in 1979. As use grew, observation of use caused a reduction in uncertainty, leading to more adoption in a self-reinforcing cycle. This kind of gathering momentum takes time in every technological wave: It took almost 30 years for electricity to reach half of American households, for example, and it took about the same amount of time for personal computers.[3] When a technological revolution changes everything, it takes a huge amount of innovation, investment, storytelling, time, and plain old work. It also sucks up all the money and talent available. Like Kuhn’s paradigms in science, any technology not part of the wave’s techno-economic paradigm will seem like a sideshow.[4]&lt;/p&gt;
    &lt;p&gt;Source: [3]&lt;/p&gt;
    &lt;p&gt;The nascent growth of PCs attracted investors—venture capitalists—who started making risky bets on new companies. This development incentivized more inventors, entrepreneurs, and researchers, which in turn drew in more speculative capital.&lt;/p&gt;
    &lt;p&gt;Companies like IBM, the computing behemoth before the PC, saw poor relative performance. They didn’t believe the PC could survive long enough to become capable in their market and didn’t care about new, small markets that wanted a cheaper solution.&lt;/p&gt;
    &lt;p&gt;Retroactively, we give the PC pioneers the powers of prophets rather than visionaries. But at the time, nobody outside of a small group of early adopters paid any attention. Establishment media like The New York Times didn’t take the PC seriously until after IBM’s was introduced in August 1981. In the entire year of 1976, when Apple Computer was founded, the NYT mentioned PCs only four times.[5] Apparently, only the crazy ones, the misfits, the rebels, and the troublemakers were paying attention.&lt;/p&gt;
    &lt;p&gt;Source: [5]&lt;/p&gt;
    &lt;p&gt;It’s the element of surprise that should strike us most forcefully when we compare the early days of the computer revolution to today. No one took note of personal computers in the 1970s. In 2025, AI is all we seem to talk about.&lt;/p&gt;
    &lt;p&gt;Big companies hate surprises. That’s why uncertainty makes a perfect moat for startups. Apple would never have survived IBM entering the market in 1979, and only lived to compete another day after raising $100 million in its 1980 IPO. It was the only remaining competitor after the IBM-induced winnowing.[6]&lt;/p&gt;
    &lt;p&gt;Source: [6]&lt;/p&gt;
    &lt;p&gt;As the tech took hold and started to show promise, innovations in software, memory, and peripherals like floppy disk drives and modems joined it. They reinforced one another, with each advance putting pressure on the technologies adjacent to it. When any part of the system held back the other parts, investors rushed to fund that sector. As increases in PC memory allowed more complicated software, for example, there became a need for more external storage, which caused VC Dave Marquardt to invest in disk drive manufacturer Seagate in 1980. Seagate gave Marquardt a 40x return when it went public in 1981. Other investors noticed, and some $270 million was plowed into the industry in the following three years.[7]&lt;/p&gt;
    &lt;p&gt;Money also poured into the underlying infrastructure—fiber optic networks, chip making, etc.—so that capacity was never a bottleneck. Companies which used the new technological system to outperform incumbents began to take market share, and even staid competitors realized they needed to adopt the new thing or die. The hype became a froth which became an investment bubble: the dot-com frenzy of the late 1990s. The ICT wave was therefore similar to previous ones—like the investment mania of the 1830s and the Roaring ‘20s, which followed the infrastructure buildout of canals and railways, respectively—in which the human response to each stage predictably generated the next.&lt;/p&gt;
    &lt;p&gt;When the dot-com bubble popped, society found it disapproved of the excesses in the sector and governments found they had the popular support to reassert authority over the tech companies and their investors. This put a brake on the madness. Instead of the reckless innovation of the bubble, companies started to expand into proven markets, and financiers moved from speculating to investing. Entrepreneurs began to focus on finding applications rather than on innovating the underlying technologies. Technological improvements continued, but change became more evolutionary than revolutionary.&lt;/p&gt;
    &lt;p&gt;As change slowed, companies gained the confidence to invest for the longer term. They began to combine various parts of the system in new ways to create value for a wider group of users. The massive overbuilding of fiber optic telecom networks and other infrastructure during the frenzy left plenty of cheap capacity, keeping the costs of expansion down. It was a great time to be a businessperson and investor.&lt;/p&gt;
    &lt;p&gt;In contrast, society did not need a bubble to pop to start excoriating AI. Given that the backlash to tech has been going on for a decade, this seems normal to us. But the AI backlash differs from the general high regard, earlier in the cycle, enjoyed by the likes of Bill Gates, Steve Jobs, Jeff Bezos, and others who built big tech businesses. The world hates change, and only gave tech a pass in the ‘80s and ‘90s because it all still seemed reversible: it could be made to go away if it turned out badly. This gave the early computer innovators some leeway to experiment. Now that everyone knows computers are here to stay, AI is not allowed the same wait-and-see attitude. It is seen as part of the ICT revolution.&lt;/p&gt;
    &lt;p&gt;Perez, the economist, breaks each technological wave into four predictable phases: irruption, frenzy, synergy, and maturity. Each has a characteristic investment profile.&lt;/p&gt;
    &lt;p&gt;The middle two, frenzy and synergy, are the easy ones for investors. Frenzy is when everyone piles in and investors are rewarded for taking big risks on unproven ideas, culminating in the bubble, when paper profits disappear. When rationality returns, the synergy phase begins, as companies make their products usable and productive for a wide array of users. Synergy pays those who are patient, picky, and can bring more than just money to the table.&lt;/p&gt;
    &lt;p&gt;Irruption and maturity are more difficult to invest in.&lt;/p&gt;
    &lt;p&gt;Investing in the 1970s was harder than it might look in hindsight. To invest from 1971 through 1975, you had to be either a true believer or a conglomerator with a knuckle-headed diversification strategy. Intel was a great investment, though it looked at first like a previous-wave electronics company. MOS Technologies was founded in 1969 to compete with Texas Instruments but sold a majority of itself to Allen-Bradley to stay afloat. Zilog was funded in 1975 by Exxon (Exxon!). Apple was a great investment, but it had none of the hallmarks of what VCs look for, as the PC was still a solution in search of a problem.&lt;/p&gt;
    &lt;p&gt;It was later irruption, in the early 1980s, when great opportunities proliferated: PC makers (Compaq, Dell), software and operating systems (Microsoft, Electronic Arts, Adobe), peripherals (Seagate), workstations (Sun), and computer stores (Businessland), among others. If you invested in the winners, you did well. But there was still more money than ideas, which meant that it was no golden age for investing. By 1983, there were more than 70 companies competing in the disk drive sector alone, and valuations collapsed. There were plenty of people whose fortunes were established in the 1970s and 1980s, and many VCs made their names in that era. But the biggest advantage to being an irruption-stage investor was building institutional knowledge to invest early and well in the frenzy and synergy phases.&lt;/p&gt;
    &lt;p&gt;Investing in the maturity phase is even more difficult. In irruption, it’s hard to see what will happen; in maturity, nothing much happens at all. The uncertainty about what will work and how customers and society will react is almost gone. Things are predictable, and everyone acts predictably.&lt;/p&gt;
    &lt;p&gt;The lack of dynamism allows the successful synergy companies to remain entrenched (see: the Nifty 50 and FAANG), but growth becomes harder. They start to enter each other’s markets, conglomerate, raise prices, and cut costs. The era of products priced to entice new customers ends, and quality suffers. The big companies continue to embrace the idea of revolutionary innovation, but feel the need to control how their advances are used. R&amp;amp;D spending is redirected from product and process innovation toward increasingly fruitless attempts to find ways to extend the current paradigm. Companies frame this as a drive to win, but it’s really a fear of losing.&lt;/p&gt;
    &lt;p&gt;Innovation can happen during maturity, sometimes spectacularly. But because these innovations only find support if they fit into the current wave’s paradigm, they are easily captured in the dominant companies’ gravity wells. This means making money as an entrepreneur or investor in them is almost impossible. Generative AI is clearly being captured by the dominant ICT companies, which raises the question of whether this time will be different for inventors and investors—a different question from whether AI itself is a revolutionary technology.&lt;/p&gt;
    &lt;p&gt;Shipping containerization was a late-wave innovation that changed the world, kicked off our modern era of globalization, resulted in profound changes to society and the economy, and contributed to rapid growth in well-being. But there were, perhaps, only one or two people who made real money investing in it.&lt;/p&gt;
    &lt;p&gt;The year 1956 was late in the previous wave. But that year, the company soon to be known as SeaLand revolutionized freight shipping with the launch of the first containership, the Ideal-X. SeaLand’s founder, Malcom McLean, had an epiphany that the job to be done by truckers, railroads, and shipping lines was to move goods from shipper to destination, not to drive trucks, fill boxcars, or lade boats. SeaLand allowed freight to transfer seamlessly from one mode to another, saving time, making shipping more predictable, and cutting costs—both the costs of loading, unloading, and reloading, and the cost of a ship sitting idly in port as it was loaded and unloaded.[8]&lt;/p&gt;
    &lt;p&gt;The benefits of containerization, if it could be made to happen, were obvious. Everybody could see the efficiencies, and customers don’t care how something gets to where they can buy it, as long as it does. But longshoremen would lose work, politicians would lose the votes of those who lost work, port authorities would lose the support of the politicians, federal regulators would be blamed for adverse consequences, railroads might lose freight to shipping lines, shipping lines might lose freight to new shipping lines, and it would all cost a mint. Most thought McLean would never be able to make it work.&lt;/p&gt;
    &lt;p&gt;McLean squeezed through the cracks of the opposition he faced. He bought and retrofitted war surplus ships, lowering costs. He went after the coastal shipping trade, a dying business in the age of the new interstates, to avoid competition. He set up shop in Newark, NJ, rather than the shipping hub of Hell’s Kitchen, to get buy-in from the port authority and avoid Manhattan congestion. And he made a deal with the New York longshoremen’s union, which was only possible because he was a small player whom they figured was not a threat.&lt;/p&gt;
    &lt;p&gt;Source: [10]&lt;/p&gt;
    &lt;p&gt;But competitors and regulators moved too quickly for McLean to seize the few barriers to entry that might have been available to him: domination of the ports, exclusive agreements with shippers or other forms of transportation, standardization on proprietary technology, etc.[9] When it started to look like it might work, around 1965, the obvious advantages of containerization meant that every large shipping line entered the business, and competition took off. Even though containerized freight was less than 1% of total trade by 1968, the number of containerships was already ramping fast.[10] Capacity outstripped demand for years.&lt;/p&gt;
    &lt;p&gt;The increase in competition led to a rate war, which led to squeezed profits, which in turn led to consolidation and cartels. Meanwhile, the cost of building ever-larger container ships and the port facilities to deal with them meant the business became hugely capital intensive. McLean saw the writing on the wall and sold SeaLand to R.J. Reynolds in January 1969. He was, perhaps, the only entrepreneur to get out unscathed.&lt;/p&gt;
    &lt;p&gt;It took a long time for the end-to-end vision to be realized. But around 1980, a dramatic drop began in the cost of sea freight.[11] This contributed to a boom in international trade[12] and allowed manufacturers to move away from higher-wage to lower-wage countries, making containerization irreversible.&lt;/p&gt;
    &lt;p&gt;Source: [11]&lt;/p&gt;
    &lt;p&gt;Some people did make money, of course; someone always does. McLean did, as did shipping magnate Daniel Ludwig, who had invested $8.5 million in SeaLand’s predecessor, McLean Industries, at $8.50 per share in 1965 and sold in 1969 for $50 per share.[13] Shipbuilders made money, too: between 1967 and 1972, some $10 billion ($80 billion in 2025 dollars) was spent building containerships. The contractors that built the new container ports also made money. And, later, shipping lines that consolidated and dominated the business, like Maersk and Evergreen, became very large. But, “for R.J. Reynolds, and for other companies that had chased fast growth by buying into container shipping in the late 1960s, their investments brought little but disappointment.”[14] Aside from McLean and Ludwig, it is hard to find anyone who became rich from containerization itself, because competition and capex costs made it hard to grow fast or achieve high margins.&lt;/p&gt;
    &lt;p&gt;Source: [12]&lt;/p&gt;
    &lt;p&gt;The business ended up being dominated primarily by the previous incumbents, and the margins went to the companies shipping goods, not the ones they shipped through. Companies like IKEA benefited from cheap shipping, going from a provincial Scandinavian company in 1972 to the world’s largest furniture retailer by 2008; container shipping was a perfect fit for IKEA’s flat-pack furniture. Others, like Walmart, used the predictability enabled by containerization to lower inventory and its associated costs.&lt;/p&gt;
    &lt;p&gt;With hindsight, it’s easy to see how you could have invested in containerization: not in the container shipping industry itself, but in the industries that benefited from containerization. But even here, the success of companies like Walmart, Costco, and Target was coupled with the failure of others. The fallout from containerization set Sears and Woolworth on downward spirals, put the final nail in the coffin of Montgomery Ward and A&amp;amp;P, and drove Macy’s into bankruptcy before it was rescued and downsized by Federated. Meanwhile, in North Carolina, “the furniture capital of the world,” furniture makers tried to compete with IKEA by importing cheap pieces from China. They ended up being replaced by their suppliers.[15]&lt;/p&gt;
    &lt;p&gt;If there had been more time to build moats, there might have been a few dominant containerization companies, and the people behind them would be at the top of the Forbes 400, while their investors would be legendary. But moats take time to build and, unlike the personal computer, the adoption of containerization wasn’t a surprise—every business with interests at stake had a strategic plan immediately.&lt;/p&gt;
    &lt;p&gt;The economist Joseph Schumpeter said “perfect competition is and always has been temporarily suspended whenever anything new is being introduced.”[16] But containerization shows this isn’t true at the end of tech waves. And because there is no economic profit during perfect competition, there is no money to be made by innovators during maturity. Like containerization, the introduction of AI did not lead to a period of protected profits for its innovators. It led to an immediate competitive free-for-all.&lt;/p&gt;
    &lt;p&gt;Let’s grant that generative AI is revolutionary (but also that, as is becoming increasingly clear, this particular tech is now already in an evolutionary stage). It will create a lot of value for the economy, and investors hope to capture some of it. When, who, and how depends on whether AI is the end of the ICT wave, or the beginning of a new one.&lt;/p&gt;
    &lt;p&gt;If AI had started a new wave, there would have been an extended period of uncertainty and experimentation. There would have been a population of early adopters experimenting with their own models. When thousands or millions of tinkerers use the tech to solve problems in entirely new ways, its uses proliferate. But because they are using models owned by the big AI companies, their ability to fully experiment is limited to what’s allowed by the incumbents, who have no desire to permit an extended challenge to the status quo.&lt;/p&gt;
    &lt;p&gt;This doesn’t mean AI can’t start the next technological revolution. It might, if experimentation becomes cheap, distributed and permissionless—like Wozniak cobbling together computers in his garage, Ford building his first internal combustion engine in his kitchen, or Trevithick building his high-pressure steam engine as soon as James Watt’s patents expired. When any would-be innovator can build and train an LLM on their laptop and put it to use in any way their imagination dictates, it might be the seed of the next big set of changes—something revolutionary rather than evolutionary. But until and unless that happens, there can be no irruption.&lt;/p&gt;
    &lt;p&gt;AI is instead the epitome of the ICT wave. The computing visionaries of the 1960s set out to build a machine that could think, which their successors eventually did, by extending gains in algorithms, chips, data, and data center infrastructure. Like containerization, AI is an extension of something that came before, and therefore no one is surprised by what it can and will do. In the 1970s, it took time for people to wrap their heads around the desirability of powerful and ubiquitous computing. But in 2025, machines that think better than previous machines are easy for people to understand.&lt;/p&gt;
    &lt;p&gt;Consider the extent to which the progress of AI rhymes with the business evolution of containerization:&lt;/p&gt;
    &lt;p&gt;In the “AI rhymes” column, the first four items are already underway. How you should invest depends on whether you believe Nos. 5–7 are next.&lt;/p&gt;
    &lt;p&gt;Economists are predicting that AI will increase global GDP somewhere between 1%[17] to more than 7%[18] over the next decade, which is $1–7 trillion of new value created. The big question is where that money will stick as it flows through the value chain.&lt;/p&gt;
    &lt;p&gt;Most AI market overviews have a score or more categories, breaking each of them into customer and industry served. But these will change dramatically over the next few years. You could, instead, just follow the money to simplify the taxonomy of companies:&lt;/p&gt;
    &lt;p&gt;What the history of containerization suggests is that, if you aren’t already an investor in a model company, you shouldn’t bother. Sam Altman and a few other early movers may make a fortune, as McLean and Ludwig did. But the huge costs of building and running a model, coupled with intense competition, means there will, in the end, be only a few companies, each funded and owned by the largest tech companies. If you’re already an investor, congratulations: There will be consolidation, so you might get an exit.&lt;/p&gt;
    &lt;p&gt;Domain-specific models—like Cursor or Harvey—will be part of the consolidation. These are probably the most valuable models. But fine-tuning is relatively cheap, and there are big economies of scope. On the other hand, just as Google had to buy Invite Media in 2010 to figure out how to sell to ad agencies, domain-specific model companies that have earned the trust of their customers will be prime acquisition targets. And although it seems possible that models which generate things other than language—like Midjourney or Runway—might use their somewhat different architecture to carve out a separate technological path, the LLM companies have easily entered this space as well. Whether this applies to companies like Osmo remains to be seen.&lt;/p&gt;
    &lt;p&gt;While it’s too late to invest in the model companies, the profusion of those using the models to solve specific problems is ongoing: Perplexity, InflectionAI, Writer, Abridge, and a hundred others. But if any of these become very valuable, the model companies will take their earnings, either through discriminatory pricing or vertical integration. Success, in other words, will mean defeat—always a bad thesis. At some point, model companies and app companies will converge: There will simply be AI companies, and only a few of them. There will be some winners, as always, but investments in the app layer as a whole will lose money.&lt;/p&gt;
    &lt;p&gt;The same caveat applies, however: If an app company can build a customer base or an amazing team, it might be acquired. But these companies aren’t really technology companies at all; they are building a market on spec and have to be priced as such. A further caveat is that there will be investors who make a killing arbitraging FOMO-panicked acquirors willing to massively overpay. But this is not really “investing.”&lt;/p&gt;
    &lt;p&gt;There might be an investment opportunity in companies that manage the interface between the AI giants and their customers, or protect company data from the model companies—like Hugging Face or Glean—because these businesses are by nature independent of the models. But no analogue in the post-containerization shipping market became very large. Even the successful intermediation companies in the AI space will likely end up mid-sized because the model companies will not allow them to gain strategic leverage—another consequence of the absence of surprise.&lt;/p&gt;
    &lt;p&gt;When an industry is going to be big but there is uncertainty about how it will play out, it often makes sense to swim upstream to the industry’s suppliers. In the case of AI, this means the chip providers, data companies, and cloud/data center companies: SambaNova, Scale AI, and Lambda, as well as those that have been around for a long time, like Nvidia and Bloomberg.&lt;/p&gt;
    &lt;p&gt;The case for data is mixed. General data—i.e., things most people know, including everything anyone knew more than, say, 10 years ago, and most of what was learned after that—is a commodity. There may be room for a few companies to do the grunt work of collating and tagging it, but since the collating and tagging might best be done by AI itself, there will not be a lot of pricing leverage. Domain-specific models will need specialist data, and other models will try to answer questions about the current moment. Specific, timely, and hard to reproduce data will be valuable. This is not a new market, of course—Bloomberg and others have done well by it. A more concentrated customer base will lower prices for this data, while wider use will raise revenues. On balance, this will probably be a plus for the industry, though not a huge one. There will be new companies built, but only a couple worth investing in.&lt;/p&gt;
    &lt;p&gt;The high capex of AI companies will primarily be spent with the infrastructure companies. These companies are already valued with this expectation, so there won’t be an upside surprise. But consider that shipbuilding benefited from containerization from 1965 until demand collapsed after about 1973.[19] If AI companies consolidate or otherwise act in concert, even a slight downturn that forces them to conserve cash could turn into a serious, sudden, and long-lasting decline in infrastructure spending. This would leave companies like Nvidia and its emerging competitors—who must all make long-term commitments to suppliers and for capacity expansion—unable to lower costs to match the new, smaller market size. Companies priced for an s-curve are overpriced if there’s a peak and decline.&lt;/p&gt;
    &lt;p&gt;Source: [19]&lt;/p&gt;
    &lt;p&gt;All of which means that investors shouldn’t swim upstream, but fish downstream: companies whose products rely on achieving high-quality results from somewhat ambiguous information will see increased productivity and higher profits. These sectors include professional services, healthcare, education, financial services, and creative services, which together account for between a third and a half of global GDP and have not seen much increased productivity from automation. AI can help lower costs, but as with containerization, how individual businesses incorporate lower costs into their strategies—and what they decide to do with the savings—will determine success. To put it bluntly, using cost savings to increase profits rather than grow revenue is a loser’s game.&lt;/p&gt;
    &lt;p&gt;The companies that will benefit most rapidly are those whose strategies are already conditional on lowering costs. IKEA’s longtime strategy was to sell quality furniture for low prices and make it up on volume. After containerization made it possible for them to go worldwide, IKEA became the world’s largest retailer and Ingvar Kamprad (the IK of IKEA) became a billionaire. Similarly, Walmart, whose strategy was high volume and low prices in underserved markets, benefited from both cost savings and just-in-time supply chains, allowing increased product variety and lower inventory costs.&lt;/p&gt;
    &lt;p&gt;Today’s knowledge-work companies that already prioritize the same values are the least risky way to bet on AI, but new companies will form or re-form with a high-volume, low-cost strategy, just as Costco did in the early 1980s. New companies will compete with the incumbents, but with a clean slate and hindsight. Regardless, there are few barriers to entry, so each of these firms will face stiff competition and operate in fragmented markets. Experienced management and flawless execution will be key.&lt;/p&gt;
    &lt;p&gt;Being an entrepreneur will be a fabulous proposition in these sectors. Being an investor will be harder. Companies will not need much private capital—IKEA never needed to raise risk capital, and Costco raised only one round in 1983 before going public in 1985—because implementing cost-savings technology is not capital intensive. As with containerization, there will be a long lag between technology trigger and the best investments. The opportunities will be later.&lt;/p&gt;
    &lt;p&gt;Stock pickers will also make money, but they need to be choosy. At the high end of projections, an additional 7% in GDP growth over ten years within one third of the economy gives a tailwind of only about 2% per year to these companies—even less if productivity growth from older ICT products abates. The primary value shift will be to companies that are embracing the strategic implications of AI from companies that are not, the way Walmart benefited from Sears, which took advantage of cheaper goods prices but did not reinvent itself.&lt;/p&gt;
    &lt;p&gt;Consumers, however, will be the biggest beneficiaries. Previous waves of mechanization benefited labor productivity in manufacturing, driving prices down and saving consumers money. But increased labor productivity in manufacturing also led to higher manufacturing wages. Wages in services businesses had to rise to compete, even though these businesses did not benefit from productivity gains. This caused the price of services to rise.[20] The share of household spending on food and clothing went from 55% in 1918 to 16% in 2023,[21] but the cost of knowledge-intensive services like healthcare and education have grown well above inflation.&lt;/p&gt;
    &lt;p&gt;Something similar will happen with AI: Knowledge-intensive services will get cheaper, allowing consumers to buy more of them, while services that require person-to-person interaction will get more expensive, taking up a greater percentage of household spending. This points to obvious opportunities in both. But the big news is that most of the new value created by AI will be captured by consumers, who should see a wider variety of knowledge-intensive goods at reasonable prices, and wider and more affordable access to services like medical care, education, and advice.&lt;/p&gt;
    &lt;p&gt;There is nothing better than the beginning of a new wave, when the opportunities to envision, invent, and build world-changing companies leads to money, fame, and glory. But there is nothing more dangerous for investors and entrepreneurs than wishful thinking. The lessons learned from investing in tech over the last 50 years are not the right ones to apply now. The way to invest in AI is to think through the implications of knowledge workers becoming more efficient, to imagine what markets this efficiency unlocks, and to invest in those. For decades, the way to make money was to bet on what the new thing was. Now, you have to bet on the opportunities it opens up.&lt;/p&gt;
    &lt;p&gt;Jerry Neumann is a retired venture investor, writing and teaching about innovation.&lt;/p&gt;
    &lt;p&gt;Chart: Jovanovic, B., &amp;amp; Rousseau, P., “General purpose technologies. Handbook of Economic Growth”, 1(05), p. 1194. [Online] Available: https://doi.org/10.1016/S1574-0684(05)01018-X&lt;/p&gt;
    &lt;p&gt;Certain sectors, like medical technology and pharma, are funded regardless of the dominant tech because they are too fundamental to ever be a sideshow.&lt;/p&gt;
    &lt;p&gt;Source: Author search of The New York Times archives for "microcomputer", "personal computer", and "home computer", no ads, no classified ads, no table of contents.&lt;/p&gt;
    &lt;p&gt;Chart: data from Dediu, H., “The Next 40”, Asymco, March 2016. [Online] Available: https://www.asymco.com/2016/03/28/the-next-40/&lt;/p&gt;
    &lt;p&gt;Sahlman, W.A. and H.H. Stevenson. “Capital market myopia.” Journal of Business Venturing, 1985, 7-30.&lt;/p&gt;
    &lt;p&gt;This section draws very heavily from Marc Levinson’s The Box (Princeton University Press, 2006), both essential and a great read.&lt;/p&gt;
    &lt;p&gt;The United States Maritime Administration began a process to standardize containers as early as 1958, just two years after the initial voyage of the Ideal-X.&lt;/p&gt;
    &lt;p&gt;McKinsey, “Brave New World: Container transport in 2043” [Online] Available: https://www.mckinsey.com/~/media/mckinsey/industries/travel-logistics-and-infrastructure/our-insights/brave-new-world-container-transport-in-2043/brave-new-world-container-transport-in-2043.pdf, 2018; chart: Levinson, p. 221.&lt;/p&gt;
    &lt;p&gt;Chart data: OECD Economic Outlook, Volume 2007/1 No. 81, June. [Online] Available: http://dx.doi.org/10.1787/032883306727&lt;/p&gt;
    &lt;p&gt;Chart data: Michel Fouquin &amp;amp; Jules Hugot , 2016. "Two Centuries of Bilateral Trade and Gravity Data: 1827-2014," CEPII Working Paper 2016-14 , May 2016, CEPII. https://www.cepii.fr/pdf_pub/wp/2016/wp2016-14.pdf. Processed by Our World in Data&lt;/p&gt;
    &lt;p&gt;McLean couldn’t resist re-entering the shipping business, buying another shipping line, USL, in 1978. He had driven USL into bankruptcy by 1986 and declared personal bankruptcy soon after.&lt;/p&gt;
    &lt;p&gt;Mullin, John, The Rise and Sudden Decline of North Carolina Furniture Making, Federal Reserve Bank of Richmond, Econ Focus, Fourth Quarter 2020. [Online] Available: https://www.richmondfed.org/publications/research/econ_focus/2020/q4/economic_history&lt;/p&gt;
    &lt;p&gt;Acemoglu, Daron, “The Simple Macroeconomics of AI”, 2024. [Online] Available: https://economics.mit.edu/sites/default/files/2024-04/The%20Simple%20Macroeconomics%20of%20AI.pdf&lt;/p&gt;
    &lt;p&gt;Goldman Sachs, “Generative AI could raise global GDP by 7%”, April 5, 2023. [Online] Available: https://www.goldmansachs.com/insights/articles/generative-ai-could-raise-global-gdp-by-7-percent&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://joincolossus.com/article/ai-will-not-make-you-rich/"/></entry><entry><id>https://news.ycombinator.com/item?id=45236079</id><title>Pass: Unix Password Manager</title><updated>2025-09-14T08:38:34.616973+00:00</updated><content>&lt;doc fingerprint="3210e9094b301af0"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;Introducing &lt;code&gt;pass&lt;/code&gt;&lt;/head&gt;&lt;p&gt;Password management should be simple and follow Unix philosophy. With &lt;code&gt;pass&lt;/code&gt;, each password lives inside of a &lt;code&gt;gpg&lt;/code&gt; encrypted file whose filename is the title of the website or resource that requires the password. These encrypted files may be organized into meaningful folder hierarchies, copied from computer to computer, and, in general, manipulated using standard command line file management utilities.&lt;/p&gt;&lt;p&gt;&lt;code&gt;pass&lt;/code&gt; makes managing these individual password files extremely easy. All passwords live in &lt;code&gt;~/.password-store&lt;/code&gt;, and &lt;code&gt;pass&lt;/code&gt; provides some nice commands for adding, editing, generating, and retrieving passwords. It is a very short and simple shell script. It's capable of temporarily putting passwords on your clipboard and tracking password changes using &lt;code&gt;git&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;You can edit the password store using ordinary unix shell commands alongside the &lt;code&gt;pass&lt;/code&gt; command. There are no funky file formats or new paradigms to learn. There is bash completion so that you can simply hit tab to fill in names and commands, as well as completion for zsh and fish available in the completion folder. The very active community has produced many impressive clients and GUIs for other platforms as well as extensions for &lt;code&gt;pass&lt;/code&gt; itself.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;pass&lt;/code&gt; command is extensively documented in its man page.&lt;/p&gt;&lt;head rend="h3"&gt;Using the password store&lt;/head&gt;&lt;p&gt;We can list all the existing passwords in the store:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass
Password Store
âââ Business
â   âââ some-silly-business-site.com
â   âââ another-business-site.net
âââ Email
â   âââ donenfeld.com
â   âââ zx2c4.com
âââ France
    âââ bank
    âââ freebox
    âââ mobilephone
&lt;/code&gt;&lt;p&gt;And we can show passwords too:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass Email/zx2c4.com
sup3rh4x3rizmynam3
&lt;/code&gt;&lt;p&gt;Or copy them to the clipboard:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass -c Email/zx2c4.com
Copied Email/jason@zx2c4.com to clipboard. Will clear in 45 seconds.
&lt;/code&gt;&lt;p&gt;There will be a nice password input dialog using the standard &lt;code&gt;gpg-agent&lt;/code&gt; (which can be configured to stay authenticated for several minutes), since all passwords are encrypted.&lt;/p&gt;&lt;p&gt;We can add existing passwords to the store with &lt;code&gt;insert&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass insert Business/cheese-whiz-factory
Enter password for Business/cheese-whiz-factory: omg so much cheese what am i gonna do
&lt;/code&gt;&lt;p&gt;This also handles multiline passwords or other data with &lt;code&gt;--multiline&lt;/code&gt; or &lt;code&gt;-m&lt;/code&gt;, and passwords can be edited in your default text editor using &lt;code&gt;pass edit pass-name&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;The utility can &lt;code&gt;generate&lt;/code&gt; new passwords using &lt;code&gt;/dev/urandom&lt;/code&gt; internally:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass generate Email/jasondonenfeld.com 15
The generated password to Email/jasondonenfeld.com is:
$(-QF&amp;amp;Q=IN2nFBx
&lt;/code&gt;&lt;p&gt;It's possible to generate passwords with no symbols using &lt;code&gt;--no-symbols&lt;/code&gt; or &lt;code&gt;-n&lt;/code&gt;, and we can copy it to the clipboard instead of displaying it at the console using &lt;code&gt;--clip&lt;/code&gt; or &lt;code&gt;-c&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;And of course, passwords can be removed:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass rm Business/cheese-whiz-factory
rm: remove regular file â/home/zx2c4/.password-store/Business/cheese-whiz-factory.gpgâ? y
removed â/home/zx2c4/.password-store/Business/cheese-whiz-factory.gpgâ
&lt;/code&gt;&lt;p&gt;If the password store is a git repository, since each manipulation creates a git commit, you can synchronize the password store using &lt;code&gt;pass git push&lt;/code&gt; and &lt;code&gt;pass git pull&lt;/code&gt;, which call &lt;code&gt;git-push&lt;/code&gt; or &lt;code&gt;git-pull&lt;/code&gt; on the store.&lt;/p&gt;&lt;p&gt;You can read more examples and more features in the man page.&lt;/p&gt;&lt;head rend="h3"&gt;Setting it up&lt;/head&gt;&lt;p&gt;To begin, there is a single command to initialize the password store:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass init "ZX2C4 Password Storage Key"
mkdir: created directory â/home/zx2c4/.password-storeâ
Password store initialized for ZX2C4 Password Storage Key.
&lt;/code&gt;&lt;p&gt;Here, &lt;code&gt;ZX2C4 Password Storage Key&lt;/code&gt; is the ID of my GPG key. You can use your standard GPG key or use an alternative one especially for the password store as shown above. Multiple GPG keys can be specified, for using pass in a team setting, and different folders can have different GPG keys, by using &lt;code&gt;-p&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;We can additionally initialize the password store as a git repository:&lt;/p&gt;&lt;code&gt;zx2c4@laptop ~ $ pass git init
Initialized empty Git repository in /home/zx2c4/.password-store/.git/
zx2c4@laptop ~ $ pass git remote add origin kexec.com:pass-store
&lt;/code&gt;&lt;p&gt;If a git repository is initialized, &lt;code&gt;pass&lt;/code&gt; creates a git commit each time the password store is manipulated.&lt;/p&gt;&lt;p&gt;There is a more detailed initialization example in the man page.&lt;/p&gt;&lt;head rend="h2"&gt;Download&lt;/head&gt;&lt;p&gt;The latest version is 1.7.4.&lt;/p&gt;&lt;head rend="h3"&gt;Ubuntu / Debian&lt;/head&gt;&lt;code&gt;$ sudo apt-get install pass&lt;/code&gt;&lt;head rend="h3"&gt;Fedora / RHEL&lt;/head&gt;&lt;code&gt;$ sudo yum install pass&lt;/code&gt;&lt;head rend="h3"&gt;openSUSE&lt;/head&gt;&lt;code&gt;$ sudo zypper in password-store&lt;/code&gt;&lt;head rend="h3"&gt;Gentoo&lt;/head&gt;&lt;code&gt;# emerge -av pass&lt;/code&gt;&lt;head rend="h3"&gt;Arch&lt;/head&gt;&lt;code&gt;$ pacman -S pass&lt;/code&gt;&lt;head rend="h3"&gt;Macintosh&lt;/head&gt;&lt;p&gt;The password store is available through the Homebrew package manager:&lt;/p&gt;&lt;code&gt;$ brew install pass&lt;/code&gt;&lt;head rend="h3"&gt;FreeBSD&lt;/head&gt;&lt;code&gt;# pkg install password-store&lt;/code&gt;&lt;head rend="h3"&gt;Tarball&lt;/head&gt;The tarball contains a generic makefile, for which a simple&lt;code&gt;sudo make install&lt;/code&gt; should do the trick.

&lt;head rend="h3"&gt;Git Repository&lt;/head&gt;&lt;p&gt;You may browse the git repository or clone the repo:&lt;/p&gt;&lt;code&gt;$ git clone https://git.zx2c4.com/password-store&lt;/code&gt;

&lt;p&gt;All releases are tagged, and the tags are signed with 0xA5DE03AE.&lt;/p&gt;&lt;head rend="h2"&gt;Data Organization&lt;/head&gt;&lt;head rend="h3"&gt;Usernames, Passwords, PINs, Websites, Metadata, et cetera&lt;/head&gt;&lt;p&gt;The password store does not impose any particular schema or type of organization of your data, as it is simply a flat text file, which can contain arbitrary data. Though the most common case is storing a single password per entry, some power users find they would like to store more than just their password inside the password store, and additionally store answers to secret questions, website URLs, and other sensitive information or metadata. Since the password store does not impose a scheme of it's own, you can choose your own organization. There are many possibilities.&lt;/p&gt;&lt;p&gt;One approach is to use the multi-line functionality of pass (&lt;code&gt;--multiline&lt;/code&gt; or &lt;code&gt;-m&lt;/code&gt; in &lt;code&gt;insert&lt;/code&gt;), and store the password itself on the first line of the file, and the additional information on subsequent lines. For example, &lt;code&gt;Amazon/bookreader&lt;/code&gt; might look like this:&lt;/p&gt;&lt;code&gt;Yw|ZSNH!}z"6{ym9pI
URL: *.amazon.com/*
Username: AmazonianChicken@example.com
Secret Question 1: What is your childhood best friend's most bizarre superhero fantasy? Oh god, Amazon, it's too awful to say...
Phone Support PIN #: 84719&lt;/code&gt;

&lt;p&gt;This is the preferred organzational scheme used by the author. The &lt;code&gt;--clip&lt;/code&gt; / &lt;code&gt;-c&lt;/code&gt; options will only copy the first line of such a file to the clipboard, thereby making it easy to fetch the password for login forms, while retaining additional information in the same file.&lt;/p&gt;&lt;p&gt;Another approach is to use folders, and store each piece of data inside a file in that folder. For example &lt;code&gt;Amazon/bookreader/password&lt;/code&gt; would hold bookreader's password inside the &lt;code&gt;Amazon/bookreader&lt;/code&gt; directory, and &lt;code&gt;Amazon/bookreader/secretquestion1&lt;/code&gt; would hold a secret question, and &lt;code&gt;Amazon/bookreader/sensitivecode&lt;/code&gt; would hold something else related to bookreader's account. And yet another approach might be to store the password in &lt;code&gt;Amazon/bookreader&lt;/code&gt; and the additional data in &lt;code&gt;Amazon/bookreader.meta&lt;/code&gt;. And even another approach might be use multiline, as outlined above, but put the URL template in the filename instead of inside the file.&lt;/p&gt;&lt;p&gt;The point is, the possibilities here are extremely numerous, and there are many other organizational schemes not mentioned above; you have the freedom of choosing the one that fits your workflow best.&lt;/p&gt;&lt;head rend="h3"&gt;Extensions for &lt;code&gt;pass&lt;/code&gt;&lt;/head&gt;&lt;p&gt;In order to faciliate the large variety of uses users come up with, &lt;code&gt;pass&lt;/code&gt; supports extensions. Extensions installed to &lt;code&gt;/usr/lib/password-store/extensions&lt;/code&gt; (or some distro-specific variety of such) are always enabled. Extensions installed to &lt;code&gt;~/.password-store/.extensions/COMMAND.bash&lt;/code&gt; are enabled if the &lt;code&gt;PASSWORD_STORE_ENABLE_EXTENSIONS&lt;/code&gt; environment variable is &lt;code&gt;true&lt;/code&gt; Read the man page for more details.&lt;/p&gt;&lt;p&gt;The community has produced many such extensions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;pass-tomb: manage your password store in a Tomb&lt;/item&gt;&lt;item&gt;pass-update: an easy flow for updating passwords&lt;/item&gt;&lt;item&gt;pass-import: a generic importer tool from other password managers&lt;/item&gt;&lt;item&gt;pass-extension-tail: a way of printing only the tail of a file&lt;/item&gt;&lt;item&gt;pass-extension-wclip: a plugin to use wclip on Windows&lt;/item&gt;&lt;item&gt;pass-otp: support for one-time-password (OTP) tokens&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Compatible Clients&lt;/head&gt;&lt;p&gt;The community has assembled an impressive list of clients and GUIs for various platforms:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;passmenu: an extremely useful and awesome dmenu script&lt;/item&gt;&lt;item&gt;qtpass: cross-platform GUI client&lt;/item&gt;&lt;item&gt;Android-Password-Store: Android app&lt;/item&gt;&lt;item&gt;passforios: iOS app&lt;/item&gt;&lt;item&gt;pass-ios: (older) iOS app&lt;/item&gt;&lt;item&gt;passff: Firefox plugin&lt;/item&gt;&lt;item&gt;browserpass: Chrome plugin&lt;/item&gt;&lt;item&gt;Pass4Win: Windows client&lt;/item&gt;&lt;item&gt;pext_module_pass: module for Pext&lt;/item&gt;&lt;item&gt;gopass: Go GUI app&lt;/item&gt;&lt;item&gt;upass: interactive console UI&lt;/item&gt;&lt;item&gt;alfred-pass: Alfred integration&lt;/item&gt;&lt;item&gt;pass-alfred: Alfred integration&lt;/item&gt;&lt;item&gt;simple-pass-alfred: Alfred integration&lt;/item&gt;&lt;item&gt;pass.applescript: OS X integration&lt;/item&gt;&lt;item&gt;pass-git-helper: git credential integration&lt;/item&gt;&lt;item&gt;password-store.el: an emacs package&lt;/item&gt;&lt;item&gt;XMonad.Prompt.Pass: prompt for Xmonad&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Migrating to &lt;code&gt;pass&lt;/code&gt;&lt;/head&gt;&lt;p&gt;To free password data from the clutches of other (bloated) password managers, various users have come up with different password store organizations that work best for them. Some users have contributed scripts to help import passwords from other programs:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;1password2pass.rb: imports 1Password txt or 1pif data&lt;/item&gt;&lt;item&gt;keepassx2pass.py: imports KeepassX XML data&lt;/item&gt;&lt;item&gt;keepass2csv2pass.py: imports Keepass2 CSV data&lt;/item&gt;&lt;item&gt;keepass2pass.py: imports Keepass2 XML data&lt;/item&gt;&lt;item&gt;fpm2pass.pl: imports Figaro's Password Manager XML data&lt;/item&gt;&lt;item&gt;lastpass2pass.rb: imports Lastpass CSV data&lt;/item&gt;&lt;item&gt;kedpm2pass.py: imports Ked Password Manager data&lt;/item&gt;&lt;item&gt;revelation2pass.py: imports Revelation Password Manager data&lt;/item&gt;&lt;item&gt;gorilla2pass.rb: imports Password Gorilla data&lt;/item&gt;&lt;item&gt;pwsafe2pass.sh: imports PWSafe data&lt;/item&gt;&lt;item&gt;kwallet2pass.py: imports KWallet data&lt;/item&gt;&lt;item&gt;roboform2pass.rb: imports Roboform data&lt;/item&gt;&lt;item&gt;password-exporter2pass.py: imports password-exporter data&lt;/item&gt;&lt;item&gt;pwsafe2pass.py: imports pwsafe data&lt;/item&gt;&lt;item&gt;firefox_decrypt: full blown Firefox password interface, which supports exporting to pass&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Credit &amp;amp; License&lt;/head&gt;&lt;p&gt;&lt;code&gt;pass&lt;/code&gt; was written by Jason A. Donenfeld of zx2c4.com and is licensed under the GPLv2+.&lt;/p&gt;&lt;head rend="h3"&gt;Contributing&lt;/head&gt;&lt;p&gt;This is a very active project with a healthy dose of contributors. The best way to contribute to the password store is to join the mailing list and send git formatted patches. You may also join the discussion in &lt;code&gt;#pass&lt;/code&gt; on Libera.Chat.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.passwordstore.org/"/></entry><entry><id>https://news.ycombinator.com/item?id=45236263</id><title>Two Slice, a font that's only 2px tall</title><updated>2025-09-14T08:38:34.556810+00:00</updated><content>&lt;doc fingerprint="75fdb0fbb35c444f"&gt;
  &lt;main&gt;
    &lt;p&gt;A font that's only 2px tall, and somewhat readable! Uppercase and lowercase have some different variants, in case you find one more readable than the other. Numbers (sort of) and some punctuation marks are included.&lt;/p&gt;
    &lt;p&gt;You can probably read this, even if you wish you couldn't.&lt;lb/&gt;It tends to be easier to read at smaller sizes.&lt;/p&gt;
    &lt;p&gt;Try it out below, or download it (under CC BY-SA license, so you can use it commercially but you have to give credit).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://joefatula.com/twoslice.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45236473</id><title>How the restoration of ancient Babylon is drawing tourists back to Iraq</title><updated>2025-09-14T08:38:34.079952+00:00</updated><content>&lt;doc fingerprint="b6c41b464ca1b51c"&gt;
  &lt;main&gt;
    &lt;p&gt;Mentioned in the sacred texts of all three Abrahamic faiths, the ancient Mesopotamian city of Babylon, in modern-day Iraq, is today undergoing a revival. Two World Monuments Fund (WMF) projects are nearing completion and much-needed cultural tourism is returning.&lt;/p&gt;
    &lt;p&gt;One project mitigates groundwater damage to the north retaining wall of the Ishtar Gate. The second is a restoration of the Temple of Ninmakh, dedicated to the Sumerian mother goddess. The team hopes there will be an official reopening for the temple this autumn, after which it will be available for gatherings such as weddings and concerts, as well as for the Babylon Festival, a celebration of international cultures that takes place every spring.&lt;/p&gt;
    &lt;p&gt;Largely funded by the US embassy in Baghdad, the restoration of the temple and the north retaining wall are part of the Future of Babylon Project, initiated 15 years ago, which aims to document, waterproof and stabilise structures throughout the 2,500-acre site. (The US embassy cancelled funding for a planned walkway spanning the site of the Ishtar Gate in July due to budget cuts.)&lt;/p&gt;
    &lt;head rend="h4"&gt;Visitor boom&lt;/head&gt;
    &lt;p&gt;The completion of these two projects coincides with a boom in tourism. Even in the midday heat, when tour guides refuse to emerge from their office, visitors from Romania, Russia and Iran enthusiastically explore attractions including the largely intact Lion of Babylon, the processional way and the museum next to a reconstructed Ishtar Gate.&lt;/p&gt;
    &lt;p&gt;The return of heritage tourism is one of Iraq’s few recent success stories. Even as sectarian tensions simmer and the electrical grid has yet to be restored 22 years after it was destroyed in the US invasion, Babylon is being reborn.&lt;/p&gt;
    &lt;p&gt;“We’ve had record numbers of visitors this year,” Raad Hamid Abdullah, Babylon’s antiquities and heritage inspector, tells The Art Newspaper. In 2024 Babylon hosted 43,530 Iraqi tourists and 5,370 foreign tourists, an increase from 36,957 Iraqi visitors and 4,109 foreigners in 2023, he says.&lt;/p&gt;
    &lt;p&gt;“Now even locals from the adjoining city of Babil are coming,” Abdullah says. “It has once more become a popular place for family gatherings and wedding parties,” he says, adding proudly, “Babylon is a symbol of Iraq.”&lt;/p&gt;
    &lt;head rend="h4"&gt;Babylon, the survivor&lt;/head&gt;
    &lt;p&gt;Around 80km south of Baghdad, comprising both the ruins of the ancient city as well as surrounding villages and agricultural areas, Babylon is a survivor. From its peak as the Neo-Babylonian capital under King Nebuchadnezzar II through to the Iraq War, when American and Polish troops ran roughshod over its ruins and a decade later, Islamic State (Isis) threatened its very existence, the ancient city has witnessed empires come and go.&lt;/p&gt;
    &lt;p&gt;Babylon has survived decades of looting and ongoing environmental challenges. Construction, too, has taken a toll over the years. In 1927 the British ran a railway line through the site, and in the 1980s Saddam Hussein built a highway through part of it, along with a palace for himself, complete with helipad. There are still three non-functioning oil pipelines, two built in the 1970s and 1980s and a more recent third one—work on it was blocked after Iraq’s General Authority for Antiquities and Heritage filed a lawsuit in 2012. Babylon was only recognised as a Unesco World Heritage Site in 2019.&lt;/p&gt;
    &lt;p&gt;Now the Egyptian architect Ahmed Abdelgawad, an expert in mud brick buildings, is working with the WMF to train locals in the traditional art that befits the Temple of Ninmakh, named after the mother goddess associated with creation, birth and healing who breathed life into humankind via small clay figures in their likeness.&lt;/p&gt;
    &lt;p&gt;Years of war-related damage and neglect combined with poorly executed mid-century “reconstruction” methods resulted in serious structural problems at the temple. Corrosion caused by the intrusion of increasingly salty groundwater is the product of prolonged droughts and soil erosion in climate-vulnerable Iraq.&lt;/p&gt;
    &lt;head rend="h4"&gt;Traditional mud-brick techniques&lt;/head&gt;
    &lt;p&gt;The archway at the entrance of Ninmakh’s inner sanctum—on the verge of collapse in 2022—was successfully restored at the end of May. “We had to totally dismantle the old arch,” Abdelgawad says. “It was full of cracks and worn by weather. So we took it apart and rebuilt it with mud bricks.”&lt;/p&gt;
    &lt;p&gt;The traditional art of making special low-salt mud brick begins with sourcing soil with low salt levels, which is then mixed with sand, grit and straw.&lt;/p&gt;
    &lt;p&gt;“This is the first arch in Iraq restored totally from mud bricks,” says Osama Hisham, the Future of Babylon project manager.&lt;/p&gt;
    &lt;p&gt;A similar but saltier mix of mud brick and bitumen was used to repair the wooden roof of the temple, which was being eroded by termites.&lt;/p&gt;
    &lt;p&gt;Hisham says the temple now comprises poplar timber from the forests of Mosul in northern Iraq, mud from Babylon and reeds from the marshes in the south. A place that has symbolised the heart of Iraq has now been restored with materials from across the nation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Groundwater zapping&lt;/head&gt;
    &lt;p&gt;Meanwhile, the north retaining walls at the Ishtar Gate, reconstructed in the past century with cement that damaged the remains of the historical monument, were demolished and replaced with new retaining walls providing better water management. These new walls—essentially boxes filled with stones, based on an ancient Egyptian construction technique, Hisham says—absorb sunlight from the southern side and effectively vaporise groundwater coming from the northern side.&lt;/p&gt;
    &lt;p&gt;The Babylonians, he says, dealt with groundwater intrusion by creating an elevation by“cutting the arch of the gate and burying it, then using it as a foundation for a new gate”. As a result of this technique, the Ishtar Gate built by Nebuchadnezzar II, where the WMF is currently finishing work on the north retaining wall, is seven metres below the ancient city, with only two metres remaining above.&lt;/p&gt;
    &lt;head rend="h4"&gt;Disintegration&lt;/head&gt;
    &lt;p&gt;A subsequent spectacular blue-glazed gate Nebuchadnezzar II built on top of that gate gradually disintegrated in the aftermath of the fall of the Babylonian Empire in the sixth century BC. A replica installed in the 1950s now greets visitors to Babylon.&lt;/p&gt;
    &lt;p&gt;Many Iraqis would like to see the reconstruction of the Ishtar Gate returned from the Pergamon Museum in Berlin. The gate is made of brick fragments from excavations carried out by the Deutsche Orient-Gesellschaft (German Oriental Society) from 1899 to 1917.&lt;/p&gt;
    &lt;p&gt;But Hisham says that even the Ishtar Gate in Berlin is only 20% original. The gate in Babylon, he points out, is 80% original.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theartnewspaper.com/2025/09/12/how-the-restoration-of-ancient-babylon-is-helping-to-draw-tourists-back-to-iraq"/></entry><entry><id>https://news.ycombinator.com/item?id=45236774</id><title>Visual programming is stuck on the form</title><updated>2025-09-14T08:38:33.354223+00:00</updated><content>&lt;doc fingerprint="3a839a010e6409d9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Visual programming is stuck on the form&lt;/head&gt;
    &lt;p&gt;Underlying great creations that you love—be it music, art, or technology—its form (what it looks like) is driven by an underpinning internal logic (how it works). I noticed this pattern while watching a talk on cellular automaton and realized it's "form follows function" paraphrased from a slightly different angle. Inventing a form is a hard task, so you must approach it obliquely—by first illuminating the underlying function.&lt;/p&gt;
    &lt;p&gt;This made me realize something crucial about visual programming: it’s stuck on form, rather than letting form follow function. Visual programming has long been trapped in the node-and-wires paradigm because its designers are overly fixated on form, neglecting the underlying function that should drive it. So as a whole, the field is stuck in a local minima. How can we break out of it and how can we find a function for the field that underpins the form?&lt;/p&gt;
    &lt;head rend="h2"&gt;A clue from CellPond&lt;/head&gt;
    &lt;p&gt;I was watching a talk and was struck not just by the presentation but also by a specific quote from Lu Wilson in a talk about CellPond–a visual programming language that expanded my expectations for cellular automata. And that's given that I'd already seen my share of the Game of Life by John Conway and read lots of A New Kind of Science by Stephen Wolfram.&lt;/p&gt;
    &lt;p&gt;But even though Lu Wilson spent the last 10 minutes showing you the fantastic visuals, none of that was the point. The actual tasty result is that there is a virtual machine with only four operations underlying the CellPond system. And these four operations correspond with memory operations we're familiar with in CPUs: read, write, allocate, and deallocate. To me, that connection was utterly surprising. The grid of patterns (form) was informed and driven by the underlying virtual machine (function).&lt;/p&gt;
    &lt;quote&gt;"I think if you were to learn from CellPond, you'd take away not just the UI—but you can take the UI too if you want. I was very surprised by this because, in all my reading of past solutions to these problems, they were all about the high-level user interface; they were about the UI. I thought I'd have to build layers upon layers of UI, but really, as soon as the low-level stuff was sorted out, the UI just figured itself out."&lt;lb/&gt;- Lu Wilson (🐤 · 🦋)&lt;/quote&gt;
    &lt;p&gt;I wondered: how did Lu Wilson come up with the underlying function? It seemed magical. This puzzling revelation made me realize it wasn’t just about the UI—there was a deeper principle at play.&lt;/p&gt;
    &lt;head rend="h2"&gt;Form follows function&lt;/head&gt;
    &lt;p&gt;In the subsequent months, I kept turning it over in my head. The key lay with the opening quote.&lt;/p&gt;
    &lt;quote&gt;When you figure out the low-level stuff, the UI all falls into place.&lt;/quote&gt;
    &lt;p&gt;It wasn't until a drive while I was listening to Paul Graham's A Taste for Makers that I made the connection. The CellPond talk was a demonstration of the oft-repeated adage of "form follows function." Here's the relevant excerpt:&lt;/p&gt;
    &lt;quote&gt;In art, the highest place has traditionally been given to paintings of people. There is something to this tradition, and not just because pictures of faces get to press buttons in our brains that other pictures don't. We are so good at looking at faces that we force anyone who draws them to work hard to satisfy us. If you draw a tree and you change the angle of a branch five degrees, no one will know. When you change the angle of someone's eye five degrees, people notice.&lt;lb/&gt;When Bauhaus designers adopted Sullivan's "form follows function," what they meant was, form should follow function. And if function is hard enough, form is forced to follow it, because there is no effort to spare for error. Wild animals are beautiful because they have hard lives."&lt;lb/&gt;- Paul Graham A Taste for Makers&lt;/quote&gt;
    &lt;p&gt;Honestly, I had never thought much about "form follows function." It seems obvious enough when you hear it for the first time. Sure, given an interface, why else would it express anything other than its purpose? It would seem counterproductive otherwise.&lt;/p&gt;
    &lt;p&gt;It wasn't until I was forced to invent a form did I really understood what it meant. The adage "form follows function" is for those tasked to invent the form, not for when you're given it. In my own words, it's this:&lt;/p&gt;
    &lt;p&gt;If a design is any good, how something looks, feels, and works is a naked expression of its function, its algebra, its rationality–its underlying nature. To design a form, you should not just come up with it out of thin air. You have to attack the problem obliquely and work out its function first. Once the function–the underlying nature, internal consistency, and algebra–is worked out, the form will fall out as a consequence of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Three faces of function&lt;/head&gt;
    &lt;p&gt;What I mean by "underlying nature" isn't that it exists independently of human creation; rather, every design is embedded in an environment that shapes its intrinsic properties. The function of anything useful is always in the context of its environment. When we understand the context of a well-designed thing, we understand why it looks the way it does. An animal form reflects its adaptation to the ecological niche in its environment.&lt;/p&gt;
    &lt;p&gt;By "rationality", I mean some kind of internal consistency. The function of something well-designed will have a certain repeated symmetry. Given a choice of design, it'll consistently use the same thing in as many scenarios as possible. Good game design enables a single item to serve multiple functions. The gravity gun in Half-Life 2 enables players to pick up and launch objects. It's used for turning environmental items into weapons, solving physics-based puzzles, and for navigating hard-to-reach areas. In Minecraft, the water bucket can extinguish fires, create waterfalls for safe descent, irrigate farmland, and serve as a barrier against certain enemies.&lt;/p&gt;
    &lt;p&gt;By "algebra", I mean a set of rules about how a design's components compose. Most games have a physics engine that computes how objects in a game interact with each other in space. It's a "movement calculator." Legend of Zelda: Breath of the Wild additionally has a chemistry engine that it uses to compute how different materials interact with each other. It's a "state calculator."&lt;/p&gt;
    &lt;p&gt;In summary, function represents the intangible structure governing the relationships, interactions, and contextual fit of a design’s underlying components. A form can't exist outside of its function, and its function is shaped by its environment. We can observe and interact with the form directly, but not its function. We can exist in the environment, but the function is invisible to us without a lot of work to infer it.&lt;/p&gt;
    &lt;p&gt;A form not informed by function feels disjointed, inconsistent, and frustrating. Without an underlying function to underpin the form, the shape of form is simply at the inconsistent whims of the designer. Functions keep designers honest about the purpose of form: in service of function. Of course you can explore and play with form independent of function, but that's the jurisdiction of art, not design.&lt;/p&gt;
    &lt;head rend="h2"&gt;To invent a form, start with the function&lt;/head&gt;
    &lt;p&gt;"Form follows function" is advice for people making something, especially those whose work has a very visible interface facing the end user. To invent a form, start with the function. But it's easy to make errors of two kinds, even if you already know this in your head.&lt;/p&gt;
    &lt;p&gt;The first kind of error is to pursue form without considering function. Instead, you must ignore the form, at least initially, and focus on figuring out the function first. This is largely due to the intangible nature of function. It's an easy mistake to focus on form, even far into your creative career.&lt;/p&gt;
    &lt;p&gt;This mistake is understandable. Whenever people interact with anything, their initial contact is the interface—the bridge between user and design. For anyone new to something, it's natural to start by engaging with that interface, because it's what they're most familiar with. So when they turn around to make something in that domain, they start with the interface, the form. You can see this readily: new creatives in a field start by copying the masters before finding their own voice.&lt;/p&gt;
    &lt;p&gt;It's also understandable because function is largely more abstract and more intangible than form. It's harder to get a grip on something amorphous, and you may have to start off with something concrete. It can be part of the process to draw up concrete examples first. In fact, when confronted with an unfamiliar domain, this can be quite productive in getting a handle on it. But it can be easy to forget and take a step back and ask: "what is the common underlying logic or abstraction to all these examples?" When you are able to take a step back, you're using the concrete examples as a stepping stone to figuring out the underlying function.&lt;/p&gt;
    &lt;p&gt;The error of the second kind is pursuing function without considering the user. As a warning for those that lean too far on the other side of the precipice, this doesn't mean you can ignore the end user when figuring out the function. If we could represent the utility of the underlying function as a vector, it would still need to point in the direction of the user. The underlying function must support and give context to the visible form built on top. Both are built so the direction and magnitude of their utility vector can support the user in the direction of their goals.&lt;/p&gt;
    &lt;p&gt;Too many back-end engineers misinterpret 'form follows function' as a license to design arbitrary database tables and APIs, assuming that the front end will compensate. That's how we get terrible interfaces where the end user needs to be aware of the data model to use it effectively, like Git.&lt;/p&gt;
    &lt;p&gt;When it comes to visual programming, I think it's stuck in the error of the first kind, with its focus on form.&lt;/p&gt;
    &lt;head rend="h2"&gt;Visual programming is not just node-and-wires&lt;/head&gt;
    &lt;p&gt;Node-and-wire diagrams have become a lazy default. Most visual language designers never ask whether those boxes and arrows genuinely help programmers. It’s a classic case of letting form precede function.&lt;/p&gt;
    &lt;p&gt;When one looks through the Visual Programming Codex, it's obvious an overwhelming majority are based on the node-and-wires model. Not just that, but there are mostly only two variations:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The nodes represent data, and the wires represent functions&lt;/item&gt;
      &lt;item&gt;The nodes represent functions, and the wires represent data shunted between functions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Did many of them settle on it because it's the best visual representation to help aid the process of programming? Or did they use it because they're mimicking an existing form?&lt;/p&gt;
    &lt;p&gt;I think node-and-wires is popular because visual programming designers make the fundamental assumption that the underlying nature and logic of programming is just traditional textual programming. If that's your assumption, then you'd naturally think all you have to do is find visual representations for existing textual language constructs. Hence node-and-wires is the form you get when you take pure functions as the underlying logic underpinning the form.&lt;/p&gt;
    &lt;p&gt;On first glance, node-and-wires seem like a good fit. The wires going into a node are like the input parameters of a pure function, and the wires going out are like the output value. But what about differentiating between the definition of a function versus calling it? Often in node-and-wires visual languages, there's no separation. The definition is the application. What about passing around functions or thunks? Much of the power in pure functional programming lies in the power of higher-order functions, and I haven't seen very good node-and-wires representation of that. After decades of trying, most pure functional programming is still largely expressed in text. To me, that's damning evidence against the practice of using nodes-and-wires to model functions. Text is still the better form for expressing the underlying logic of functional programming.&lt;/p&gt;
    &lt;p&gt;Imperative programming with node-and-wires fares no better. A loop in LabVIEW gives no more advantage or clarity over writing it in text. Seeing the totality of a sequence of steps in parallel in a circuit-like diagram doesn't solve the fundamental problem with imperative programs; it doesn't help the developer understand combinatorial state explosions or state changes over time.&lt;/p&gt;
    &lt;p&gt;I think where node-and-wires have provided the biggest advantage is in specific domains in which a) there's massive value to examine intermediate data and values between transformations and b) there's a well-known visual representation of that intermediate data and value. This has been demonstrated in visual languages like Unreal Engine's Blueprint for game programming shaders and Max/MSP for sound synthesis in music. But these have been limited to these narrow domains. Visual programming has not found a foothold in general purpose programming domains.&lt;/p&gt;
    &lt;head rend="h2"&gt;Modeling problems&lt;/head&gt;
    &lt;p&gt;What then, if not node-and-wires? The aim here is to uncover an alternative underlying logic—one that can more effectively drive the form in visual programming. How would you go about finding another underlying function in "form follows function" if not the current programming paradigms we know? I think this is the wrong question. Although correct in direction and spirit, I think a better question is: how should we model problems that can leverage the computational power of our visual cortex?&lt;/p&gt;
    &lt;p&gt;We write programs primarily to model and solve real-world problems. We go through the exercise of encoding the problem model in programming languages, because we can automate the generation of solutions. And the reason why we keep banging on the visual programming door is because we understand intuitively that our visual cortex is an under-leveraged power tool.&lt;/p&gt;
    &lt;p&gt;The human visual cortex is a powerful pattern recognition apparatus. It can quickly compare lengths, distinguish foreground from background, recognize spatial patterns, and other amazing feats of perception, all at a glance. We leverage it in data visualizations to make sense of large quantities of data, but we haven't been able to leverage it to make sense of computational systems.&lt;/p&gt;
    &lt;p&gt;If we had a visual programming language that could leverage the human visual cortex, then at any zoom-level of abstraction, at a glance we could understand the overall structure of the program as it relates to the domain at that level of abstraction. And if we were looking at a running program, then we could get an idea of the overall state and process. Yes, we have bespoke visualizations of running programs in the form of metrics and dashboards. But we don't have a universal visual language to represent the structure or state of a program that applies to different programs.&lt;/p&gt;
    &lt;p&gt;What about text? Aren't textual glyphs a kind of visual language? Not in the way I mean. For text to be considered a visual programming language, it'd have to leverage the human visual cortex at different zoom-levels of the program. Certainly, with syntax highlighting we leverage the visual cortex and use color to distinguish between different syntactical elements. This counts. But we only get this at the level of a function. It doesn't apply when we zoom out to the overall structure of the code base. And there's certainly no zoom-out level in which we get visual understanding at the level of the problem domain.&lt;/p&gt;
    &lt;p&gt;The closest thing I can think of that might fit the bill is APL and its ilk. By condensing operators into single characters, sequences form idioms. Just as we recognize whole words rather than individual letters, idioms allow us to comprehend entire operations without parsing each symbol. So as you zoom out of the code, you can see the meaning of the code by identifying common idioms. Strangely, it seems many APL environments don't feature syntax highlighting.&lt;/p&gt;
    &lt;p&gt;So if visual programming is to be useful, I think the angle of attack is to find a way to model problems, and this might not be the same way that we model problems in textual languages–even if the underpinning implementation is all lambdas and Turing machines. So how do we model problems?&lt;/p&gt;
    &lt;head rend="h2"&gt;Entities and relationships&lt;/head&gt;
    &lt;p&gt;I'll say up front, I don't know what modeling problems should look like. Nonetheless, it seems there are two main aspects for any system we're interested in:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;visually representing the entities in a problem domain&lt;/item&gt;
      &lt;item&gt;visually representing the entity relationships.[2]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Regardless of the paradigm, imperative, object-oriented, functional, or logical, there are both "entities" (structs, objects, compound values, terms) and "how they relate" (imperative processes, messages, functions, rules and predicates). If I had to take a stab at it, I'd start here.&lt;/p&gt;
    &lt;p&gt;Of the two, representing the different entities in a problem domain seems more amenable to visual programming because they're nouns. Most of the things we see around us are nouns. Hence, we can imagine that inert data representing entities would have a canonical visual representation. But even then, entities often have far more attributes than we might want to visualize at a time to understand its purpose and behavior. How do we choose what attribute is important to show? And what should be the visual form for the attribute in these entities?&lt;/p&gt;
    &lt;p&gt;The two questions are related, but to drive the point home, I'll focus on the second one. If we have some struct with two attributes in some generic language, how would we visually represent them?&lt;/p&gt;
    &lt;code&gt;struct Foo {
  bar: float,
  baz: float
}&lt;/code&gt;
    &lt;p&gt;We might think a universally useful representation of a collection of these instances is two histograms: one for &lt;code&gt;bar&lt;/code&gt; and one for &lt;code&gt;baz&lt;/code&gt;. For any given instance, its corresponding value could be highlighted on the histogram.&lt;/p&gt;
    &lt;p&gt;Is this useful? Answer depends on our task at hand. There's no one-size-fits-all visualization of entities. What if I told you &lt;code&gt;bar&lt;/code&gt; is an x-coordinate and &lt;code&gt;baz&lt;/code&gt; is the y-coordinate? Now, perhaps a visualization that's more fitting is a scatterplot where each instance is represented as an &lt;code&gt;x&lt;/code&gt;. We put the relationship between &lt;code&gt;bar&lt;/code&gt; and &lt;code&gt;baz&lt;/code&gt; in a spatial relationship to see if our visual cortex could recognize a pattern.&lt;/p&gt;
    &lt;p&gt;In the histogram visualization, I wouldn't be able to use my visual cortex to discern the relationships between &lt;code&gt;bar&lt;/code&gt; and &lt;code&gt;baz&lt;/code&gt; traces out a flower. However, in the spatial canvas visualization, I could easily see the flower trace because by pitting &lt;code&gt;bar&lt;/code&gt; and &lt;code&gt;baz&lt;/code&gt; in a spatial relationship, I'm creating a mapping that makes an affordance for my visual cortex.&lt;/p&gt;
    &lt;p&gt;This only worked because there was a spatial relationship between &lt;code&gt;bar&lt;/code&gt; and &lt;code&gt;baz&lt;/code&gt;, especially if I know they represent &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; coordinates. We couldn't just look at the data and easily discern what visualization to use. The label and the intention of the user also give meaning to what visualization is best suited for an entity. Hence, I think there's no one-size-fits-all visualization for entities. There's no single mapping of attributes to visualizations that makes sense, unless the user's intention and goals remain fixed. &lt;/p&gt;
    &lt;p&gt;Besides entities, every program encodes relationships between its entities. How do we visually represent their relationships in a way that's illuminating at a glance without devolving into an illegible spaghetti mess? Relationships can be harder to model, because they're typically invisible to us, as they're often inferred.&lt;/p&gt;
    &lt;p&gt;Like the example with representing entities visually, representing relationships visually is likely to depend on both the goals of the user as well as the meaning of the entities at hand. I suspect a good visual representation of the relationship between two tables in a query is going to be different than a good visual representation of the relationship between two pieces of middleware in a web stack. However, I do think we can do better than a line.&lt;/p&gt;
    &lt;p&gt;The go-to representation of a relationship is often the line or an arrow, where it connects two things on the canvas together. The trouble with lines is that they doesn't scale with the visual cortex. After a couple dozen lines, we lose track of any sense of the overall relationships between entities. But I don't think this can be the only way. The visual cortex also relates visual elements if they have the same color or if they're spatially clustered together. As the previous example on a plot of &lt;code&gt;bar&lt;/code&gt; and &lt;code&gt;baz&lt;/code&gt; showed, relationships could be spatial, by which we can plot them spatially to reveal relationships, without directly drawing lines and arrows everywhere.&lt;/p&gt;
    &lt;p&gt;As before, it's hard to draw any generally productive conclusions on how to best visually represent relationships between entities without knowing the goal of the user as well as the meaning behind the entity and relationships we're trying to represent. The only point I'm trying to drive home is that we have more tools at our disposal besides lines and arrows, because the visual cortex is perceptive and discerning about colors, groupings, and motion. We typically use these visual elements haphazardly, if at all, rather than as a deliberate attempt to leverage it for understanding. And that's just in graphic design and data visualization. It's completely overlooked in program structure, debugging, and domain problem modeling.&lt;/p&gt;
    &lt;p&gt;At this point, those that hear entities and relationships might be drawn to ask, isn't this just object-oriented programming? It is true that object-oriented thinking trains you to identify entities in the problem domain and model their relationships through method calls and messaging. However, object-oriented programs suffer from private state whose effects are observable from the outside littered everywhere, making it hard to reason about program behavior. What I'm saying is orthogonal to and doesn't invalidate what we've learned about structuring programs in the past 3 decades. To sum up, I'm saying the unit of representation for visually representing programs may not be the function and its input and output parameters, as node-and-wire visual programmers are likely to do. It might be something else, which can leverage the power of the visual cortex.&lt;/p&gt;
    &lt;head rend="h2"&gt;Computation is figuring out the next state&lt;/head&gt;
    &lt;p&gt;Modeling problems as entities and their relationships is only half the equation. By only modeling entities and their relationships, we've only described a static world. We can do that already without computers; it's commonly done on whiteboards in tech companies around the world. Every time we go up to the whiteboard with a coworker to talk through a problem, we're trying to leverage the power of our visual cortex to help us reason through it. But unlike our textual programs, whiteboards aren't computational.&lt;/p&gt;
    &lt;p&gt;If whiteboards were computational, they might show how the state of the problem changes over time, or how it changes in response to different external inputs or effects. Thus, the question is, how do we visually represent how the system state should evolve over time or in response to external inputs? [1]&lt;/p&gt;
    &lt;p&gt;Cellular automaton systems typically express computation through rulesets. Rulesets are typically expressed as a pure functional transformation between the current state and the next state. Taking rule 110 in 1D cellular automaton as an example, the state of the next cell depends on the three cells above it. Given the three cell pattern above, this is what the cell in the next line should be. You can see this like a β-reduction, substituting symbols with other symbols until we can substitute no further, with the resulting value as our answer.&lt;/p&gt;
    &lt;p&gt;As the CellPond talk at the top of the page points out, rulesets for more complicated behaviors, like trains on tracks have a combinatorial explosion of rules. One of CellPond's innovations was to have rulesets that represent (or generates?) groups of rulesets, so that visually expressing the rulesets remains tractable for humans.&lt;/p&gt;
    &lt;p&gt;But pure functions are just mappings. Any pure function can be replaced by an equivalent infinite table of key-value pairs. Rulesets are just explicit mappings of inputs to outputs. Hence, if rulesets are to be tractable, we must be able to express not just how a single current state maps to the next state, but how entire groups of states map to a next state.&lt;/p&gt;
    &lt;p&gt;We have familiar mechanisms in textual programming to express a selection of groups of input states in a succinct way. We have boolean logic in if expressions. We have maps and filters. We have select and where clauses in SQL queries. But we have no universal and composable ways of expressing this selection of previous states and mapping them to next states. Additionally, we don't have universally recognized ways of expressing this mapping from groups of inputs to outputs for state types other than a grid of cells.&lt;/p&gt;
    &lt;head rend="h2"&gt;A different way forward&lt;/head&gt;
    &lt;p&gt;Certainly, it could be possible that multi-dimensional aspects of a codebase would be quite hard to represent in its entirety visually. But I don't think it's a stretch to say that we lean pretty hard on the symbolic reasoning parts of our brain for programming and the visual reasoning parts of our brain are underleveraged.&lt;/p&gt;
    &lt;p&gt;Visual programming hasn't been very successful because it doesn't help developers with any of the actual problems they have when building complex systems. I think this is a result of ignoring the adage "form follows function" and trying to grow a form out of traditional programming paradigms that fail to provide good affordances–the utility vector is pointing the wrong way–for those actual problems in complex systems. To make headway, I think we should focus on discovering underlying logic and function of how to model problems visually on a canvas–not just the entities, but also their relationships. In addition to modeling problems, we also have to discover how to model transformations and transitions of state, so our models are also computational.&lt;/p&gt;
    &lt;p&gt;We have the hardware: our visual cortex is a powerhouse for pattern recognition and spatial reasoning. We just don’t have the right computational grammar to feed it. If we want a visual programming breakthrough, we have to leave the legacy of text-based paradigms behind and unearth a new kind of function—one that only makes sense visually. Once we do, the right ‘form’ will follow so obviously, we’ll wonder why we waited so long.&lt;/p&gt;
    &lt;p&gt;[1] One way is with visual rule sets. This almost feels like declarative or logic programming. But as the Cell Pond talk at the top of the essay pointed out, unless you have a representation of rule sets that can be expanded, you suffer combinatorial explosion.&lt;/p&gt;
    &lt;p&gt;[2] Depending on who you are, this can sound either like object-oriented programming or category theory.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://interjectedfuture.com/visual-programming-is-stuck-on-the-form/"/></entry><entry><id>https://news.ycombinator.com/item?id=45237184</id><title>High Altitude Living – 8,000 ft and above (2021)</title><updated>2025-09-14T08:38:33.030304+00:00</updated><content>&lt;doc fingerprint="262b085f75a39ad6"&gt;
  &lt;main&gt;
    &lt;p&gt;Living at high altitude reduces risk of dying from heart disease: Low oxygen may spur genes to create blood vessels. Summary: Researchers have found that people living at higher altitudes have a lower chance of dying from heart disease and live longer.&lt;/p&gt;
    &lt;p&gt;Jeanne and I live at 8,400 feet (2.560 meters). We were talking about visitors coming (for workshops, friends, etc.) and if you are coming from a low elevation what you need to be aware of. There is a thing called Acute Mountain Sickness (AMS). It's real and can be very disabling for some people.&lt;/p&gt;
    &lt;p&gt;The best defense is to start at 5,000 feet and stay there for 3 days, drinking water like crazy and resting. Then, come up to the higher elevation and give yourself a day or two to adjust. That usually works.&lt;/p&gt;
    &lt;p&gt;Beyond the AMS, there is the sun! The sun up here will fry you fast! The UV is very very high up here and we have 300 days of sun every year. It's difficult NOT to be out in it. Water, clothing that covers your arms, legs, etc., and a hat. This is the best way to protect yourself from burning up. You will dehydrate and get very sick.&lt;/p&gt;
    &lt;p&gt;There's a lot to think about when we are considering having people visit.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://studioq.com/blog/2021/5/30/high-altitude-living-8000-ft-and-above-2450-meters"/></entry><entry><id>https://news.ycombinator.com/item?id=45237442</id><title>A single, 'naked' black hole confounds theories of the young cosmos</title><updated>2025-09-14T08:38:32.710887+00:00</updated><content>&lt;doc fingerprint="b70810f358824b5b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Single, ‘Naked’ Black Hole Rewrites the History of the Universe&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;A black hole unlike any seen before has been spotted in the early universe. It’s huge and appears to be essentially on its own, with few stars circling it. The object, which may represent a whole new class of enormous “naked” black holes, upends the textbook understanding of the young universe.&lt;/p&gt;
    &lt;p&gt;“This is completely off the scale,” said Roberto Maiolino, an astrophysicist at the University of Cambridge who helped reveal the nature of the object in a preprint posted on August 29. “It’s terribly exciting. It’s highly informative.”&lt;/p&gt;
    &lt;p&gt;“It’s pushing the boundaries on what we think might be true, what we think might happen,” said Dale Kocevski, an astronomer at Colby College who was not involved in the new research.&lt;/p&gt;
    &lt;p&gt;Astronomers spied the bare black hole using the James Webb Space Telescope (JWST) — a mega-instrument built by NASA and its partners in part to reveal how galaxies formed during the universe’s first billion years. This new black hole, which is as heavy as 50 million suns and is dubbed QSO1, clashes with the old, provisional account of the galaxy formation process, which did not start with black holes. Black holes were thought to have come along only after a galaxy’s stars gravitationally collapsed into black holes that then merged and grew. But Maiolino and his colleagues described a solitary leviathan with no parent galaxy in sight.&lt;/p&gt;
    &lt;p&gt;The question now is how this black hole came to exist.&lt;/p&gt;
    &lt;p&gt;The most exciting — and controversial — possibility dates back to a 1971 proposal from the British physicist Stephen Hawking: that black holes arose in the primordial soup of the Big Bang itself. In that case, the object would have been sitting in the dark since the universe’s first moments, waiting for stars and galaxies to illuminate it.&lt;/p&gt;
    &lt;p&gt;QSO1 is one of hundreds of similar-looking objects nicknamed “little red dots” that JWST has spotted in its first few years of peering into the deepest recesses of time. Astrophysicists can’t say yet whether these dots are all black holes or not, and in general they’re still confused about the universe’s chaotic childhood. But the telescope’s snapshots suggest a rowdy young cosmos that fabricated big black holes and galaxies both together and independently, or maybe even a universe where black holes were among the first large structures in existence — dark tapioca bubbles in an otherwise smoothly blended cosmic tea.&lt;/p&gt;
    &lt;p&gt;QSO1 and the rest of the little red dots “tell us we don’t know anything,” said John Regan, a theorist at Maynooth University in Ireland. “It has been really exciting and very electrifying for the field.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Pale Red Dots&lt;/head&gt;
    &lt;p&gt;Lukas Furtak, an astronomer at Ben-Gurion University in Israel, knew QSO1 was extraordinary the moment he saw it — or the moment he saw its three reflections hiding among a smattering of splotchy white galaxies in an image taken by JWST in 2023. It’s “something that pops out immediately,” Furtak said over Zoom, clicking on three nearly imperceptible red specks. “There are three red point sources here, here, this one up here.”&lt;/p&gt;
    &lt;p&gt;In the image, a fortuitous placement of galaxies and dark matter has bent light rays traveling from background objects just as a glass lens might; this “gravitational lens” reveals objects deeper in the early universe than the telescope could otherwise see. The lens magnifies and stretches the stuff behind it, sometimes creating multiple images of it. Furtak was mapping out the banana-shaped smears of galaxies that the lens had projected into multiple places when he spotted the three red dots of QSO1.&lt;/p&gt;
    &lt;p&gt;The dots caught his eye because they show no signs of stretching. He knew that the only thing that looks like a small, round point even after getting stretched out is an even smaller, rounder point. This was no galaxy, he figured; it must be a black hole, a concentration of mass so dense that its gravity creates an inescapable zone of space around it.&lt;/p&gt;
    &lt;p&gt;Over the next six months, Furtak and collaborators directed JWST to stare at each of the three red dots for 40 hours each to take a census of the colors of light coming from the object, known as a spectrum. That study concluded that QSO1 is very likely a glowing black hole packing a mass of tens of millions of suns into a span of at most 100 light-years across, seen as it appeared when the universe was just 750 million years old. (Today the cosmos is approaching 14 billion years old.)&lt;/p&gt;
    &lt;p&gt;QSO1 was one of the first little red dots found. There are now over 300 of them, and a spirited debate over their nature has raged for two years. They have some classic features of glowing black holes, but not others. And estimates of their masses have (until now) been somewhat indirect. As a result, some astrophysicists have argued — as one group did in an analysis of more than 100 little red dots in August — that the objects are really just odd-looking galaxies with no black holes after all.&lt;/p&gt;
    &lt;p&gt;“The field has been obsessed with them,” Kocevski said. “Very rarely do you find things you can’t explain.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Zooming In&lt;/head&gt;
    &lt;p&gt;In December 2024, Maiolino, together with Hannah Übler, now at the Max Planck Institute for Extraterrestrial Physics, and other collaborators, trained JWST on QSO1 for another 10 hours. They zoomed in on the dot until it resolved into a pixelated splotch, and measured the specific colors coming from each pixel. From these spectra, they calculated the speed at which the stuff shining in each pixel was moving toward us or away from us. The scientists found that bright material — likely hot gas — swirled around in a furious vortex, one that backed up Furtak’s preliminary findings.&lt;/p&gt;
    &lt;p&gt;Their closer look, detailed in a pair of preprints posted in May and August, definitively revealed QSO1’s identity.&lt;/p&gt;
    &lt;p&gt;One clue was its mass. By reconstructing the vortex, the team directly measured the mass of the object it was orbiting: 50 million times more massive than our sun. This matched what Furtak and his collaborators had found. (This result alone marks a big step forward: It suggests that the simpler indirect mass measurement based on the whole object’s spectrum works for young black holes, which had been a point of contention.)&lt;/p&gt;
    &lt;p&gt;Moreover, the group found no evidence of a starry galaxy around QSO1. The gas orbits the central pixel just as the Earth orbits the sun — indicating that mass is packed into a point. The team estimates that the black hole makes up at least two-thirds of the mass of QSO1, with the remaining third being gas and perhaps a smattering of stars. Regan, who wasn’t involved in the research, thinks they’re being conservative and that QSO1 could be as much as 90% black hole. “We’ve never seen anything like that before,” he said.&lt;/p&gt;
    &lt;p&gt;Finally, the pixel-by-pixel spectra also revealed that the gas orbiting the black hole is essentially pure hydrogen, an element that dates back to the Big Bang. Stars shine by fusing hydrogen into heavier atoms, and when stars explode, they scatter those heavier elements everywhere. QSO1 seemingly reached its current form before many nearby stars had lived and died.&lt;/p&gt;
    &lt;p&gt;“The most plausible explanation seems to be [that] the black hole developed before the galaxy,” said Marta Volonteri, a theorist at the Paris Institute of Astrophysics who helped with the new analysis of QSO1.&lt;/p&gt;
    &lt;head rend="h2"&gt;Shrouded Origins&lt;/head&gt;
    &lt;p&gt;A top task for astrophysicists now will be to sort out how QSO1 and its ilk formed, and how they became the supermassive black holes that sit at the centers of starry galaxies today. Supermassive black holes, which can weigh as much as billions of solar masses, can be seen anchoring galaxies by the end of the universe’s first billion years.&lt;/p&gt;
    &lt;p&gt;Supermassive black holes have long troubled astrophysicists. They know that galaxies can make black holes when their big stars burn and die. Those stellar corpses merge and feed on gas and dust, growing larger. The conventional story is that this growth eventually results in one giant black hole sitting in the center of the galaxy. The problem is that all this feeding and merging takes time, and astrophysicists struggle to imagine it happening fast enough to result in the supermassive black holes seen by the universe’s billion-year mark. So theorists have spent decades coming up with a menu of alternative theories about their formation.&lt;/p&gt;
    &lt;p&gt;Now QSO1, which has no galaxy to speak of, shows that there must indeed be another way.&lt;/p&gt;
    &lt;p&gt;So how might the universe directly manufacture gigantic black holes? Maiolino’s group favors Hawking’s proposal. The Big Bang produced an infant universe that was denser in some spots than in others. A sufficient density could have collapsed straight into a black hole, which would then have grown by absorbing any matter around it. After hundreds of millions of years, some of these “primordial” black holes might have reached gigantic proportions — appearing much like QSO1.&lt;/p&gt;
    &lt;p&gt;“It’s the most plausible explanation that I see,” Volonteri said. But “I’m sure in the next six months there will be a thousand people coming out with other theories.”&lt;/p&gt;
    &lt;p&gt;They won’t have to wait six months. Even before QSO1’s discovery, Priyamvada Natarajan, a theoretical astrophysicist at Yale University, and collaborators had already published two non-primordial theories that could account for QSO1’s origin.&lt;/p&gt;
    &lt;p&gt;The first theory supposes that the Big Bang produced dense spots that didn’t collapse immediately. Instead, they evolved into clouds of gas over hundreds of thousands of years. Residual radiation from the Big Bang stopped these clouds from cooling and fracturing into stars, letting them become massive enough to collapse straight into black holes. In a paper posted in June, researchers led by Wenzer Qin at New York University called these slightly later-blooming giants “not-quite-primordial” black holes.&lt;/p&gt;
    &lt;p&gt;Or perhaps QSO1 did come from a galaxy after all — one that quickly formed, made a big black hole, and vanished. In 2014, Natarajan and Tal Alexander of the Weizmann Institute of Science in Israel described a scenario where one star in an especially starry region collapses into a large black hole that then zooms around like Pac-Man, hoovering up gas and ballooning to a huge size. The other early stars then wink out quickly, leaving the giant black hole to its own devices.&lt;/p&gt;
    &lt;p&gt;None of these origin stories fits QSO1 snugly, though each is possible. The only scenario that’s essentially ruled out is the textbook one of stars collapsing, merging and feeding on an orbiting disk of gas.&lt;/p&gt;
    &lt;p&gt;QSO1 isn’t the first unconventional black hole spotted by JWST, though it’s the barest one. Another striking find sits in a galaxy called UHZ1, which formed less than half a billion years after the Big Bang. By combining JWST observations with X-rays collected from the object by the Chandra X-ray Observatory in 2022, Natarajan and collaborators determined that UHZ1 is also more black hole than surrounding galaxy. This and a handful of other features led the group to conclude that UHZ1’s black hole was born when a cloud of gas largely skipped the star stage and collapsed directly — a theory that also might work for QSO1.&lt;/p&gt;
    &lt;p&gt;The challenge — and excitement — for astronomers is that they’re confronting a new era of cosmic history for the first time, and it’s proving tough to make sense of the scene. Regan compares the situation to developing a whole theory of humanity based on adults and teenagers — the adolescent and mature galaxies we could see prior to the launch of JWST. Now observing little red dots is the equivalent of discovering toddlers, messy new entities that are hard for researchers to interpret based on what they’ve seen before. “It’s a different vibe,” he said. “They’re running around like lunatics.”&lt;/p&gt;
    &lt;p&gt;Editor’s note: Priyamvada Natarajan is a member of Quanta Magazine’s scientific advisory board. She was interviewed for this story but did not otherwise contribute to its production.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/a-single-naked-black-hole-rewrites-the-history-of-the-universe-20250912/"/></entry><entry><id>https://news.ycombinator.com/item?id=45237717</id><title>Refurb Weekend: Silicon Graphics Indigo² Impact 10000</title><updated>2025-09-14T08:38:32.418726+00:00</updated><content>&lt;doc fingerprint="a62c555709a6b5f5"&gt;
  &lt;main&gt;&lt;p&gt;Counting this sucker, there are three SGI systems presently in the house. (I had a chance many years ago to land one of SGI's early 68K IRIS machines, I think an IRIS 3110, but I was still in a small apartment back then and hadn't the space. I've always regretted turning that one down.) The "big" one is a 900MHz R16000 SGI Fuel (codenamed "Asterix") with 4GB of RAM and a V12 DCD graphics card; the line was first introduced in 2002 and is best described as a single-node Origin 3000. This particular machine has a penchant for chewing up timekeeper batteries and power supplies, and true to form it's recently devoured another set (probably capacitors in the PSU's case), so a future Refurb Weekend will involve converting it to ATX. The Fuel certainly has its detractors, mostly Tezro owners, but I've already hit my quota for big, hot and loud RISC workstations with my trusty Power Mac Quad G5 (the Raptor Talos II is rather more tractable). The Fuel, by contrast, is reasonably quiet and for me powerful enough, I love the bright red tower case, and its more commodity PC-like design might seem chintzier but I find it also makes it easier to work on.&lt;/p&gt;&lt;p&gt;The other one you've met and is my personal favourite: the pizzabox SGI Indy (codenamed "Guinness"), introduced in 1993. This Indy's most recent appearance was as the Hyper-G server for the RDI PrecisionBook laptop, which was running the Hyper-G Harmony client. The Indy was the first SGI workstation I ever personally touched. I don't have the original IndyCam grab anymore, just this cropped picture, but here I am photographed using one of the Indys as an undergraduate at the Salk Institute circa 1995-ish:&lt;/p&gt;Ostensibly they were there for X-ray crystallography rendering but they also played a lot of sgidoom. It didn't help my envy that the lower-end Indys were almost kinda vaguely affordable for a college student, criminal sidehustles or unsavoury loan terms notwithstanding. Though I never did buy one in college, this Indy I picked up later came with the Indybag to tote it in.&lt;p&gt;The SGI Indy makes a good segue into the Indigo2 — as I'll render it for the remainder of this article — because they have many similarities in their architecture since the Indy is in large part descended from it. In fact, the lowest-binned Indigo 2 systems, with a hardware IP ("Inhouse Processor") identifier of IP22, look the same as the Indy (IP24) to system software which also reports it out as an IP22. The Indy's more limited processor and graphics options, undoubtedly for purposes of market distinction, led to the common joke that the Indy was "an Indigo without the 'go'." While this Indy has 24-bit XL 2-D graphics and a 150MHz R4400SC (i.e., with L2 cache), a processor second only to the higher-tiered R4400s and R5000, all but the most anaemic contemporary configurations of the Indigo2 would have outclassed it.&lt;/p&gt;&lt;p&gt;The first Indigo 2 systems, in a teal case which also wasn't indigo, emerged in January 1993 and made up the IP22 family based on R4000-series CPUs, maxxing out in 1995 with the 250MHz R4400SC sporting 2MB of L2 cache. The launch configuration with a 100MHz R4000 would set you back about $35,000, an eye-watering $78,000+ in 2025 dollars. Initially the IP22 family shipped with Extreme graphics (codenamed "Ultra"), a three-card 24-bit 2D/3D rendering system that was roughly a doubled-up Elan ("Express" graphics) from the original Indigo with twice the Geometry Engines and Raster Engines (eight GE7 and two RE3.1 units respectively). Because the teal Indigo2s have only four card slots, the Extreme graphics board set ended up occupying most of them. Each slot has an EISA connector but only three of them also have GIO64 connectors for SGI's high-speed bus interconnect, and two of those are actually shared.&lt;/p&gt;&lt;p&gt;In mid-1993 additional lower-tier options were offered: "Newport" graphics, sold as the XL for speedy 2-D graphics but no 3-D, and two options both called XZ for cheaper 3-D, one identical to the Indigo Elan (four GEs, one RE) and one with half the GEs (two). The advantage of these lower-tier options, besides cost, was getting some card slots back if you didn't need Extreme-level acceleration. The four-GE XZ and the XL were also sold for the Indy, which came out around the same time, along with an even cheaper 8-bit-depth XL card for entry-level Indys.&lt;/p&gt;The Indigo2 jumped a processor generation in October 1994 when SGI announced the POWER Indigo². This family (IP26) incorporated the R8000 from the high-end POWER Onyx, which was the first MIPS IV processor (the R4000 was the first MIPS III) and explicitly designed around floating point performance such that its codename was "TFP" for "Tremendous Floating-Point." Despite the moniker, however, the R8010 FPU was in fact a separate chip decoupled from the main CPU with a queue allowing for limited out-of-order execution of mixed FP and integer instructions. The R8000 itself was strictly in-order yet a superscalar design issuing up to four instructions per cycle on a five-stage integer pipeline (although also highly pipelined the R4000 and R4400 are merely scalar CPUs; by contrast the R4600 mostly did away with pipelining its FPU, which made a relatively weak core even worse). The main R8000 CPU has conventional on-die L1 caches plus external L2 cache. Interestingly, the L2 cache is all but obligatory because it ends up shared with the R8010 which uses it as L1. These are serviced by two separate identical "tag RAMs" containing the cache tags, and the external cache has its own five-stage pipeline.&lt;p&gt;The entire amalgamation made for a complex chipset to design, taking years of development time, and to the chagrin of MIPS management its release date slipped from 1993 when it might even have been a plausible Indigo2 launch configuration. By this point SGI had acquired MIPS Computer Systems as its subsidiary MIPS Technologies, Inc. (MTI) and made the R8000's completion an engineering priority. It was just as expensive to fabricate — Toshiba produced the R8000 and R8010 at 0.7μm, totaling over 3.4 million transistors between the two — and after the POWER Onyx launched in July 1994 the 75MHz POWER Indigo2 finally emerged in October for a cool $46,000 (over $105,000 in 2025 simoleons) with 2MB L2 cache, XZ graphics, 64MB of RAM, a 2GB SCSI disk, monitor, keyboard and mouse. Compare that to the 200MHz R4400SC configuration introduced at the same time with 24-bit XL graphics, 32MB of RAM, 1GB disk, monitor, keyboard and mouse for "just" $24,500 — you paid a lot for the special little front case badge these rather rare systems sported. And if you had to ask how much a POWER Onyx was, well, you couldn't afford it.&lt;/p&gt;&lt;p&gt;Meanwhile, the GE7 Geometry Engine's transform performance started to become a liability as CPUs increased in speed and could compute them faster, and the presently available Indigo 2 options lacked any support for hardware texture mapping. Indeed, texel-capable SGI graphics hardware had existed for at least several years, first on SGI's earlier Crimson systems using PowerVision VGX graphics, but it had yet to be deployed in any workstation. As a new faster graphics option for the Indigo2 SGI started with the GE10 from the Onyx's RealityEngine² and cranked up the new GE11 to 960,000 MFLOPs (compare to the eight GE7s in the Extreme that together produced about 256,000), and paired it with a new faster RE4 raster engine. For the upper-binned configurations, they also added a texture engine option with its own RAM ("TRAM"), and the highest of the three configurations had an extra GE11, RE4 and pixel processor for blending, depth and dithering. SGI called this new architecture IMPACT.&lt;/p&gt;SGI made the case purple (still not indigo) for the IMPACT family, putting a special silver "IMPACT" badge on the front as shown in this 1995 ad I joined by hand from two magazine page scans. It was also somewhat internally different: to accommodate IMPACT graphics' greater power and bus demands, SGI reversed the four card slots' loadout so that there were only three EISA card connectors but four GIO64 ones (though still only two logical slots), and added individual supplemental power connectors to each. These in turn were backed by a beefier 385W power supply. A variation called "IMPACT Ready" had all the system upgrades but no IMPACT card, which could be bought separately and added later (the original graphics options would still mostly work).&lt;p&gt;Although the first Indigo2 IMPACTs were announced in July 1995, a conspicuous absence among them was an R8000 configuration: the systems weren't offered with one. While SGI had drivers for it internally, the company had quietly concluded the R8K was an engineering dead-end. Its design made it difficult to evolve and it ultimately topped out at just 90MHz (and the Indigo2 only had the 75MHz part); on top of that, it ran hot and power-hungry, pulling 13W at 75MHz while the R4400 could pull as little as eight watts at 200MHz, and Toshiba couldn't seem to make it any cheaper. Similarly, although it generally achieved the floating point dominance it was designed to, its integer performance improvements proved more modest and it was difficult for programmers to squeeze out optimum throughput.&lt;/p&gt;&lt;p&gt;SGI instead put MTI on finishing the processor generation after it. As far back as 1991 MIPS Computer Systems had been developing specifications for a "next-generation" RISC part which it grandiosely called the R10000 and planned to release in 1994. Codenamed "T5," MIPS prevailed upon its usual partners to contribute to its development, eventually receiving a reported $150 million in investments. The R10000's development was somewhat slowed by the R8000, causing SGI/MTI to fund Quantum Effect Design's R5000 in the meantime for their low end hardware (our Cobalt RaQ 2 uses a later derivative, and an R5000 option was offered for the Indy, though never for the Indigo2). Although announced in July 1995 in 175MHz and 195MHz speeds, significant production problems delayed its release until March 1996 for SGI's high-end servers and the NEC-fabbed ones had to be recalled in July because of unexpected shutdowns.&lt;/p&gt;During the estimated $10 million recall for the faulty R10Ks, SGI finally introduced the chip (presumably fixed) to the Indigo2 IMPACT line as the IP28 at both the 175MHz and 195MHz speeds. (I'll talk more about the R10K when we get to the CPU module.) These last and mightiest IMPACT systems got their own silver badge that read "IMPACT 10000" — which brings us to our system today. This machine here is actually the last variant of the IMPACT 10Ks, with Solid IMPACT graphics. Recall I mentioned three bins for IMPACT. As introduced initially SGI produced a High IMPACT R10000 system for $43,000 ($88,500 in 2025 dollars) which was the middle tier with texture mapping, and a Maximum IMPACT system for $55,000 ($113,300) with texture mapping and the extra rendering hardware. Solid IMPACT systems lack texture mapping and, as the lowest tier, were introduced shortly afterwards as a cheaper option (around $34,000 or in 2025 $70,000). Finally, an even less expensive variation called Killer IMPACT was also offered, which was nothing more than Solid IMPACT paired with the lesser 175MHz R10K and thus arguably unworthy of the name. There weren't many of these as the later Octane and O2 systems were more popular by then, but this is actually one of them that has been secondarily upgraded, and I'll show you that in a minute.&lt;p&gt;I should also mention that this thing weighs a ton (or more accurately a bit over 40 pounds) and was a real adventure dragging it out to the staging area.&lt;/p&gt;&lt;p&gt;There were a couple other minor variations on the Indigo2 that we won't talk further about here, though I mention them for reference. Like the Challenge S, which was a modified headless Indy used as a cheaper rackmount, there was a headless teal Challenge M with no graphics option (though one can be installed, making it a regular Indigo2). This variation was rebadged by Control Data Corporation and Siemens Nixdorf but is the same machine otherwise. The case was also used for the Challenge M Vault, an overgrown SCSI enclosure that added another 5.25" bay and two 3.5" bays, and repurposed the expansion card cage for four more 3.5" drives. Obviously the Vault isn't actually a workstation and I've never seen one myself.&lt;/p&gt;Gratuitous shots of the case badges. SGI certainly made some physically striking systems. The "10000" on the case badge got a little banged-up prior to my acquisition but the whole setup still looks awfully cool. The rear didn't clean up particularly well, but it's still instructive. There is only one card in the slots, the Solid IMPACT card itself, because pretty much everything else essential is on-board. It has a 13W3 video port, though this type of port never got a standardized pinout and only video cables and converters wired for SGI graphics will work properly with it (naturally I have some in stock). We'll have more to say about video output later on. The DE-9 next to it is for an active-shutter 3-D stereo viewer, which excites me as a 3-D enthusiast, though sadly I don't have anything that works with it right now.&lt;p&gt;Below that on either side are the external 50-pin SCSI port and the audio connectors for microphone, line in, line out, headphones, and serial digital stereo. This is what used to be called IEC958 and is now called IEC 60958, better known nowadays in its consumer-grade form as S/PDIF. The connection here transmits and receives AES3 (AES/BDU) stereo PCM audio over an unbalanced line at sampling rates up to 48kHz. All the jacks are 1/8" (3.5mm) TRS.&lt;/p&gt;The other ports are two PS/2 ports for keyboard and mouse (a regular PC keyboard and mouse will do), two MiniDIN-8 serial ports (classic Mac serial port cables work), the AUI Ethernet connector and 25-pin IEEE-1284 parallel port, and at the base a 10BaseT Ethernet port. The AUI and 10bT connectors are the same NIC, and if both are connected then only the 10bT port is active. Additional NICs could be installed in the EISA slots with appropriate drivers. The manual cautions you not to "throw the mouse at co-workers."&lt;p&gt;This unit appears to have been upgraded aftermarket: the model number CMNB007AF175 corresponds to the 175MHz variant, and a 175MHz R10K plus Solid IMPACT graphics would be a Killer IMPACT system. However, it's actually got a 195MHz CPU module and I'll prove that when we bring the machine up. Machines that SGI upgraded or refurbished in-house have additional stickers on the Indigo² plate and the model number plate, though this one has neither, so it must have been added after the fact.&lt;/p&gt;&lt;p&gt;The copper coverplate is a bit of a mystery, but may have been intended for a T1 port or something similar like the Indy's and was simply never populated on this board.&lt;/p&gt;Finally, the power supply. These don't have a great reputation but are not trivially replaceable by an AT or ATX substitute, and it was one of the two components I was most expecting to have failed. It's possible to replace their capacitors if necessary but doing so involves several caps on the high-voltage side, which could be risky or dangerous depending on your skill set. (Mine sucks.) To get to the other component I was pretty sure had failed, we need to get to the logic board now. The front door opens down, or it would if one of the hinges weren't broken, to reveal a 5.25" bay occupied by a(n apparent) CD-ROM caddy-fed drive and then a black 3.5" bay. Next to the 5.25" bay is the master power button and a smaller reset button. At the top are two clips which we pull down. These clips enable removing the entire front bezel, revealing not only the 5.25" bay but actually two hard disks. They are all on their own benighted little custom sleds that we'll have to do battle with later. For now, we slide their restraining clips to the left and pull the sleds from the system. The hard disks and the optical drive do not appear to have been factory-issue either. (Note from the future: they weren't.) Notice the alignment prongs and the oversized rear connector which carries not only the SCSI bus, but also SCSI ID and power lines. SGI offered a DAT option which went in the top 3.5" bay, and there were also SCSI 3.5" floppy and QIC 5.25" options. We then turn it horizontal and remove the feetsies, or what SGI staidly calls "workstation stands." These were 3D printed by someone on Nekochan (RIP) and while the colour doesn't quite match, they're reasonably robust and look a lot better than the scuffed-up O.G. ones you might pay a mint for on eBay. I got them a while ago when I still had immediate plans for this thing and they've been with it ever since. The feetsies are important because the side slats' vents must remain unobstructed when vertical, especially if you have one of the bigger video options. With the bezel off and everything out, we can just undo its moorings and lift the top case up, which rotates back on little clips in the rear until it comes off. Keep that in the back of your head for when we reassemble it. The naked chassis. You can see the card cage and riser (and cooling fan), the drive bays connected by a big stiff ribbon, the power supply, and (peeping out in the top middle section) a small portion of the logic board. This was all very dusty, so the canned air came out at this point to clear away the bunnies and debris. To expose more of the logic board we'll need to remove the 5.25" tray first. This is secured by two captive screws in front. It then simply slides back out of its retaining clips and can be flipped over to the side (it isn't necessary to remove the flat connecting cable). With the tray out of the way we now see the main CPU module and RAM SIMM slots. Let's talk a little more about the R10K.&lt;p&gt;The R10000 was MTI's first out-of-order core. The chip has a seven-stage pipeline and fetches four instructions every cycle from the I-cache for decoding. These (except for NOPs and jumps) in turn feed three instruction queues for integer, FP and address operations, each of which can accept up to four instructions themselves, which dispatch reordered operations to two integer (add/shift/move or multiply/divide), two FP (add or multiply/move) and one load/store execution unit. Up to 32 instructions can be in-flight. Each execution unit maintains its own multi-stage pipeline, except for high-latency division and square root units that hang off the FP multiplier, with integer instructions having the lowest latency. The address queue is uniquely a circular FIFO so that instruction order is preserved for tracking dependencies and maintaining sequential-memory consistency. Instruction reordering is assisted by 64-register rename files for both GPRs and FPRs, alongside a separate condition file recording in parallel if the result was non-zero so that conditional move instructions need only test a single bit instead of the whole register.&lt;/p&gt;The R10K implements a 64-entry translation lookaside buffer and is also capable of speculative execution, predicting branches using a 512-entry history table and saving state in a four-entry branch stack. Unusually for such a design, it does not predict branch targets, relying on its out-of-order core to do useful work during the additional cycle required to compute them (a problem with branch-heavy code that could not be easily parallelized). It carried 32K of L1 instruction cache and two interleaved 16K L1 data caches, plus supporting L2 cache (called the "data streaming cache") anywhere from 512K to 16MB, and could be set up for "glueless" SMP out of the box with up to three other CPUs — or, with custom hardware, potentially hundreds. The CPU implements the Avalanche bus, a muxed 64-bit bus which directly interfaces with the L2, apparently unrelated to an earlier experimental bus for the PA-7100. In theory SGI Avalanche could run at CPU speed, but in practice its overhead limited it to 100MHz in uniprocessor configurations and 80MHz under SMP, or around 540 MB/s. While this was reportedly enough to keep a 4-CPU system fed, it paled in comparison to wider non-multiplexed buses like the PowerPC 620 which could maintain roughly twice the bandwidth.&lt;p&gt;Although the first iteration was intended to run up to 200MHz, poor yields caused problems above 180MHz and it was introduced at the slightly slower maximum speed of 195MHz as shown here. Despite being a third source for the R4400 IDT chose not to produce the R10000, so it was fabricated by NEC and Toshiba on a 350nm process with 6.8 million transistors and a die area of 298 square millimetres. NEC-fabbed units initially drew so much power that they caused unexpected system shutdowns and forced SGI into that very expensive recall I mentioned earlier. The R10K eventually reached 250MHz in 1997 with a process shrink to 250nm, though this wasn't ever offered for the Indigo2.&lt;/p&gt;&lt;p&gt;The R10000 turned out to be a far more significant microarchitectural landmark than SGI had intended. We'll come back to this when we finish the story at the end.&lt;/p&gt;The chip speed is confirmed by the part number, a PMT5 030-0966-004 indicating an R10000 (technically an R10000SC) with 1MB of L2 cache. This chip runs rather hot as the huge heatsink and active cooling fan would indicate. The part I was pretty sure was dead is (surprise surprise) a Dallas DS1286 timekeeper chip (top left/northwest corner). Dallas timekeepers have built-in batteries and last a fairly long time, but eventually crap out and are notoriously not intended to be user-refurbished. Despite the name they do other things as well such as provide a watchdog timer and a small amount of battery-backed "NVRAM."&lt;p&gt;The death of a timekeeper in an SGI has various effects depending on the machine. In most cases this just affects the clock, as is the case with my battery-munching Fuel, though at least in the Fuel's case the battery is external and can be replaced. However, in machines like the Indy, things like the on-board Ethernet MAC address are kept there as well and the network hardware won't function until the chip is refurbished and reprogrammed. It should be noted that the "NVRAM" in these things is nowhere near large enough to maintain the PROM environment variables; that is stored elsewhere.&lt;/p&gt;&lt;p&gt;Because this machine is related to the Indy I decided better safe than sorry (note from the future: it fortunately appears at least the IP28 Indigo2 doesn't keep the MAC address or any other vital system data in the DS1286 either). Unfortunately we can't just remove it because the riser card for the slots is in the way, so we'll need to get that free. At this point I removed the stiff blue power supply connector from the riser card, which also grants easier access to the 72-pin SIMM RAM slots.&lt;/p&gt;RAM is installed in three groups of four identical SIMMs each. The spec is parity FPM — EDO reportedly doesn't work — 60ns or faster. This machine came with only the default 64MB of RAM (as four 16MB SIMMs), which isn't great, so I found someone selling a four-pack of 32MB SIMMs of the same spec with the plan to order it if we get this machine to power up.&lt;p&gt;Officially the IP26 and IP28 boards only accept up to 640MB of RAM, limited to eight 64MB SIMMs because of heat concerns. It turns out this isn't actually a problem with most SIMMs and virtually any IP28 can accept 768MB (all 64MB SIMMs) just fine. The little IP22 is limited to 32MB SIMMs, however, and thus 384MB of RAM. The situation is more complicated with the IP26 as the R8000 CPU card encroaches on some of the RAM slots, necessitating "low profile" SIMMs, and there is also some question over SGI's insistence that installing any 64MB SIMM will require at least one bank to consist of all 32MB SIMMs.&lt;/p&gt;&lt;p&gt;It is possible, at some expense, to cram 1GB of RAM into an IP28 using 128MB "32x36" parity SIMMs. The memory controller is hard-limited to 1GB, however, so you could only populate two of the banks this way to yield 1024MB from eight 128MB SIMMs. Here's a more thorough explanation.&lt;/p&gt;To get the riser card up we'll need to take the Solid IMPACT card out. This is a single card, so it's a little bit less of a hassle than the stacked multicard graphics options. The card cage has a door which you can simply pull down on to open. The Solid IMPACT card connects only to the GIO64 bus and the supplemental power connector (the EISA connector for that slot is open). Those huge heat sinks again should say something about how much work this card ends up doing. The card is identified as an 030-0786-004. Although I don't know for sure which is which, my guess is that the two chips labeled SGI/ISD 099-9028-001 ("V101 REVA") are the twin PP1 pixel processors that handle blending, depth and dithering, since they're next to the video output, and the chip labeled ADV7162KS170 is the single HQ3 command processor. The larger square of the two heat sinks would then most likely be for the GE11 Geometry Engine (because it has the most gates of any of the chips) and the smaller rectangular one for the RE4 Raster Engine and the SDRAM framebuffer. The cards are securely held not only by the card edge and the usual screw in the slot, but also by this retention pin which slots down in front of the card edge by the door. The video card can now be pulled out (there are handy loops if you like) and set aside. Now to pull up the riser card. This is secured in several places, so we'll start with the screw in the back. Unfortunately this screw was pretty badly stripped. I'm not sure if someone had tried to do some other upgrade on this machine and mucked it up, but either way no screwdriver could turn it, so I grabbed a pair of pliers and cranked it off. I also don't know who J. R. Vala is, but if you wanted his attention, this is the riser card for you. Next we undo a little brass-coloured screw at the bottom which secures the riser card to the logic board. Now we can pull the riser card up out of its connectors (but gently so as not to bend anything). We don't need to yank it up all the way to get the timekeeper out, but if you did want to remove it, you'll want to remove the twisted black-and-yellow cable as well as the stiff flat blue cable. Now we can grab that chip, which is conveniently socketed. Expecting I would have to do this at some point, I had already pre-purchased a repair board for the DS1286 that you could simply wrap the needed pins around to provide a proper battery holder. However, it should have been a tip-off to me that the repair board has a crystal on it, because the DS1286 has a crystal too. (I am not complaining: I bought a replacement DS1386 module for my Indy from the same seller and that has worked very well.) There seem to be some DS1286s that lack the crystal and battery, which would actually make them more like a DS1284. Those chips (and the DS1284 generally) can be very easily converted with this board. However, as you can see here, the DS1286 in this machine already has those pins yanked back. Undoubtedly it was made out of a DS1284 with the battery and crystal epoxied onto the top in a similar fashion. Because it was shot anyway I got out the Dremel and decided to see how far I could shave it down. Unfortunately the entire top chamber is epoxy; you can't just cut into it or dig out chunks. I did grind down to the battery and what I thought was the crystal, but in the end all I ended up doing at that point was just making a mess, nor did I find any obvious metal portions I could solder the conversion board to. But it was only $10, so live and learn. I dithered over buying a DS1284 to retrofit but the problem with buying ICs on eBay is getting re-marked crap or chips that are absolutely fraudulent. However, while looking at chips from dodgy international sellers I ran across an individual producing a small board for the Tektronix TDS524A digital scope, which also uses the DS1286. This board has a little replaceable coin cell on it plus a DS1284Q and crystal. More to the point it also has pads on top, so an alternative battery pack can be fitted. Although it is twice as big as the original DS1284 (in area, anyway), it still fits. Before installing it I soldered a 2AA battery holder to the battery pads so that when the lithium battery it came with craps out eventually, I can just put in lithium AAs (don't put alkaline in this, they'll leak) and not have to go through this whole disassembly again. Sheathing it with electrical tape to ensure the upper pins and the battery pack won't short, it fits neatly in the socket. At this point we should now make sure we can power on the system, so I put back down the riser card (leaving out the stripped rear screw) and reinstalled the video card and retention pin. Although the machine will bring up a console on the serial ports, I also wanted to know if the video card is working, so I got out an SGI-wired 13W3 to VGA converter and connected one of my utility LCD panels. I then plugged it into the wall, crossed my fingers and pressed the power button. To my delight the machine did indeed power up and make its little happy jingle chord! However, nothing appeared on the screen and the Indigo2's LED stayed amber, suggesting the video card was unhappy about something.&lt;p&gt;The "something" in this case was probably the monitor. One thing that 13W3-based video cards do have in common is that they're all sync-on-green, which is to say that the horizontal and vertical sync signals are mixed in with the green channel. Many monitors nowadays, including this particular ViewSonic LCD panel, don't understand sync-on-green anymore and won't be able to sync. Conversely, as an unusual case, I have a NEC multisync CRT monitor that does sync to such a signal, but still displays the green anyway, giving it the wrong colour.&lt;/p&gt;There is a more definitive solution for this problem and we will address it later on, but fortunately it turned out my INOGENI USB VGA capture box does understand sync-on-green, and the PROM monitor display rate is 60Hz which the INOGENI will accept. For the time being, the M1 MacBook Air and VLC can thus serve as the monitor (and we can also take screen grabs).&lt;p&gt;With the Mac connected I reset the Indigo2, and this time the SGI's LED turned back to green and we got a picture. The picture said the machine was unbootable, though we already knew that because we pulled the drives out. I powered it off and plugged in a PS/2 keyboard and mouse. Normally you shouldn't operate the machine with the case off because it can't run its airflow normally, but there is sufficient ventilation here even though you can certainly feel the heat from the CPU module.&lt;/p&gt;I then secured the battery holder to the metal back of the riser card with some Velcro. It fit perfectly. With the battery holder in place and the timekeeper now working again, I installed the hard disks and the optical drive, and powered it on with the capture box connected. Now that I knew the machine was working, I went ahead and ordered the extra RAM, but we have other tasks to complete in the meantime. Let's switch to the screen grabs, which have been cropped and corrected for aspect ratio. One of the things that made SGI MIPS (and its early PC hardware, but more on that later) interesting was the graphical boot PROMs. These are based on the Advanced RISC Compouting (ARC) specification modified for SGI's usage ("ARCS"). ARC came out of the Advanced Computing Environment (ACE) consortium originally founded by Compaq, Microsoft, MIPS (pre-acquisition), DEC and SCO in 1991, later joined by SGI, Control Data Corporation, Prime Computer, Zenith and others. Notably absent were Sun Microsystems, Hewlett-Packard and IBM, who never participated — nor Intel.&lt;p&gt;At this time in the industry RISC was believed to be the future — but so was Windows NT, or what was then referred to as OS/2 3.0 or "Portable OS/2," due to its planned wide portability and the existing software it supported. ACE concentrated on 32-bit x86 using conventional BIOS (due to Compaq's influence) and near-future RISC workstations using ARC; they identified two platforms, namely SCO UNIX and the future Windows NT, that would run on both. MIPS was heavily touted as the architecture for these workstations, both from SGI and MIPS' presence, and the absence of anyone else more powerful to say otherwise.&lt;/p&gt;&lt;p&gt;Within a year, however, ACE rapidly degenerated into squabble and collapsed: market appetite for the alternative ARC workstation didn't develop as planned, and SGI buying MIPS was the last straw for some participants who saw the purchase as SGI trying to corner the architecture. DEC, which was already working on what would become Alpha anyway, de-emphasized its MIPS offerings as a result and eventually got out of the business altogether. Intel, for its part, accelerated development on Pentium in response and made non-x86 alternatives comparatively even less desirable. Although ARC foundered, and no computer was ever fully compliant with its specification, it maintained a long-standing legacy presence in Windows NT which still specified it for boot devices until Windows Vista. Likewise, the RISC systems that could boot Windows NT generally used an ARC console to do so, even ones that weren't MIPS-based like various Alpha-based workstations in the form of AlphaBIOS, and some RS/6000s.&lt;/p&gt;&lt;p&gt;No SGI MIPS hardware ever booted NT natively, though through the influence of ACE where MIPS was supposed to reign supreme these boot PROMs implement ARC too, at least in their own fashion. But what makes them most outstanding to modern users is that they have a full mouse-based GUI congruent with IRIX and the ability to do some basic tasks built-in.&lt;/p&gt;Messages appear as well-rendered dialogue boxes, sometimes extracted from console output and highlighted to the user. If you clicked the Stop For Maintenance button (or pressed Escape) you would enter the main menu right here before starting the OS. Other output appeared in a text window where additional detail was required. However, our disks still don't boot and the message doesn't tell us why, so we proceed to the PROM menu. The System Maintenance menu has six options accessible by button clicks or the numbers 1-6. Using the keyboard is particularly handy when your mouse doesn't work or if, as in our circumstance, you're struggling with screen lag due to the capture box. The most immediately useful is the Command Monitor, so we press 5. This pops up in a new window. (That also means there was no password on the PROM. If one was set, and we don't know what it is, removing a jumper on the board under the CPU card will allow you to reset it.) We first list our hinv, the hardware inventory, using the -v option for verbose output. This reports we have an IP28, 64MB of RAM, a 195MHz R10K and Solid IMPACT graphics, as expected. The Iris Audio Processor is the standard onboard audio codec used in many SGI-MIPS systems. The environment (printenv) didn't look too interesting other than its apparently residual IP address, but the MAC address did appear and seemed valid, so we shouldn't need to worry about that like we would with an Indy. Nevertheless, just in case anything was corrupted, I did a resetenv at this point to force defaults. With the default environment I tried to boot again, and this time got a marginally more useful message: Boot file not found on device: scsi(0)disk(1)rdisk(0)partition(8)/sash. This is an ARC path and should be fairly self-explanatory (basically SCSI bus 0, i.e., the internal bus, ID 1, LUN 0, partition 8). This partition is where the standalone shell should be found to bring up the IRIX kernel but it's not finding it. hinv -t will show you the device tree. Most of it makes sense, but when we get to the internal SCSI bus (starting with adapter SCSI WD33C93B key 0, a Western Digital Fast SCSI-2 controller) it gets very weird. Note as background that the controller in SGI hardware has ID 0 (not 7 like, say, Macintoshes), so connected devices start at ID 1. One of the hard disks is indeed at ID 1 — and then the other one is sprayed over every other ID. I also didn't see the optical drive anywhere in that list. Given that one of the hard disks got an ID despite the other one going nuts, this device should have gotten one also. Nevertheless, I decided to grab one of my caddies, stick an IRIX Tools CD in it and see if I could bring up a miniroot. This disc in particular is known good and readable by the Indy. The drive accepted the caddy and seemed to be fine with the disc ... ... but trying to bring up the miniroot from it (using the PROM menu install option) showed no bootable devices. I shut down the system and pulled out all the devices again, then had a look at the optical drive specifically. The sleds carry power, SCSI and SCSI ID lines. It looked like it was hooked up correctly but it was strange to see a La Cie sticker on it. That sounded like a Mac drive which had been repurposed. Extracting it from the sled, the drive revealed itself as a Sony CDU948S. I noticed that there was no parity jumper, and indeed the manual seems to indicate that parity can't even be enabled on this drive, which would make it unbootable. I imagine the prior owner simply went with what he had available. We'll replace that now.&lt;p&gt;On the shelf I had some old Toshiba XM-5401B SCSI CD-ROMs which I had purchased for another project long since forgotten. These are highly compatible and will generally boot just about anything, assuming they're working, which the first one wasn't (wouldn't eject its tray without a lot of force). I tried the second one.&lt;/p&gt;While you don't need to connect the SCSI ID cable and can assign IDs manually, it's easier to let the machine do it. On this drive the SCSI ID cable goes on in this orientation. I installed the new old CD-ROM drive alone and booted the system back into the command monitor. hinv -t shows the drive, on ID 1, and no more drives-all-over-the-place nonsense. This also means the SCSI controller wasn't likely at fault. And the installer option sees it too, and will offer to boot from it, but, uh ... ... um, the CD's already in there ... ... so both drives are bad.&lt;p&gt;The only other internal SCSI CD-ROM I had at hand was a Nakamichi CD changer, but that seemed like a waste, so I scrounged around in the server room for alternatives. In the stack of parts for the Fuel I found a spare factory-issue SCSI DVD-ROM (Toshiba SD-M1711). I knew this was bootable in the Fuel, so it probably would also be bootable here.&lt;/p&gt;I got it the ID cable upside down the first time, but the second time it went on cleanly and got assigned an ID. In the Command Monitor it showed as ID 6 and was recognized as a CD-ROM. To make sure of what I was dealing with, I ran hinv this time with -t -p to give me ARC paths, showing the full ARC path for the device would be scsi(0)cdrom(6) — there is shorthand for this, I promise. I then tried to bring up the standalone shell from the tools CD with boot -f scsi(0)cdrom(6)partition(8)/sash64 and this time it worked! The Sony drive went on the parts shelf for another system to use some other time. That took care of the optical drive, so I next looked at the IBM drive that had been spamming the SCSI bus (the one in the top bay). The problem was obvious: the ID cable wasn't connected, so my best guess is the hard disk thought it was ID 0 and promptly clashed with the controller. In fact, the cable wasn't anywhere near long enough to have even reached the drive's ID pins — which also meant it couldn't possibly have been bootable either. I ejected its tray and turned my attention to the second drive. With the drive installed by itself, it comes up once again as ID 1, so I put the DVD-ROM back in and decided to try system recovery to see what might be on it. The optical drive is recognised and we insert the CD. And, immediately, the PROM starts copying the installation tools to disk. This is notable: that means the hard disk has something the PROM recognizes as a swap partition, where the miniroot lives during system recovery operations. The crash recovery kernel successfully starts within the console and asks for the machine's hostname. As it happens I've already picked a name for the Indigo2, and it is ... purplehaze. Catchy, no? This is what I meant by the PROM monitor intercepting certain strings on the console. Because I reset the environment way back when, it now has a 192.168.1.2/24 address which the kernel treats as unconfigured. This is a string the kernel simply emits, but as the PROM monitor services the console, it sees it arrive and promotes it to a dialogue box instead. There isn't really anything to restore from, so I forced it into a shell at this point. The fuller hinv available from an IRIX shell prompt shows everything we expect it to, and the miniroot's /dev shows that the remaining hard disk has partitions at 0, 1, 6, 7 and 15. This might suggest the disk was formatted XFS at some point, since 0 would be the root, 1 would be swap, 6 would be /usr, 7 would be the whole thing minus the partition volume header in 8, and 15 would be the XFS log. I didn't see a partition 10, but that could be an artifact of how the disk was imported by crash recovery. However, there was no partition 8 for the standalone shell and other tools at all. The miniroot also didn't want to mount partition 0 as either XFS or EFS. There is a small chance this was due to an incompatible way the partition filesystem was made, but the Tools version here is 6.5.9 and should be recent enough to understand XFS version 2 directories. The most likely conclusion is that the disk was partitioned but no filesystems were actually created, which means this machine was never actually bootable when I received it.&lt;p&gt;That's no problem — we'll just start fresh. And, since we're going to have to install IRIX from scratch anyway, let's do it on solid state.&lt;/p&gt;There are only two sleds, and since the second drive we were using could actually be installed and enumerated (just not mounted), I decided to take the sled from the first drive that had no working ID cable. We'll then replace it with a ZuluSCSI. The particular ZuluSCSI I selected has a bottom plastic carrier that I attached the sled to. Unfortunately the clip on these things is spring-loaded and said spring is not secured particularly well, which ended up getting loose while I tried to adjust the sled's position in the drive bay. This required taking the carrier off again to rethread it. Leftover was a couple of prop bars which I didn't need to mount the ZuluSCSI, so those will be put in the junk drawer in case they're useful for something else. To cable it, however, we'll need an extension: the sled's connector is meant for a hard disk extending all the way back and can't be pulled further without damaging it. Happily, I found a little SCSI extender of the right size in the box of tricks and a Molex-to-Berg power connector. The last step was to create a big 18GB empty image on the 32GB SD card as our IRIX volume (something like dd if=/dev/zero of=HD1.img bs=1024 count=18874368 will do). We can't install the SCSI ID cable on the ZuluSCSI, but we know that the emulated disk can come up safely as device 1, so we'll tag the disk image as such. With the ZuluSCSI's sled installed in the top bay, we obligingly see the lights of both the DVD-ROM and the ZuluSCSI flash as we power on the system ... ... and both devices are visible in the device tree in hinv -t -p. If we were still working with the second partitioned disk we could simply start the installer at this point, but the disk image on the ZuluSCSI isn't partitioned yet. This requires manually bringing up the fx partitioner from the CD. To save my fingers I used the ARC shorthand for the device paths: instead of scsi(0)cdrom(6)partition(8) we can just say dksc(0,6,8), and likewise for scsi(0)cdrom(6)partition(7), making the boot command from the Command Monitor boot -f dksc(0,6,8)/sash64 dksc(0,6,7)/stand/fx.64 --x (the --x option starts expert mode). The partitioner duly starts from disc. We accept the defaults, which would be dksc(0,1,0) for the image on the ZuluSCSI. fx immediately determines the image is unpartitioned and sets up a default configuration. We'll then label the disk. We really just need it to create the sgiinfo portion (used for administrative purposes) but no harm in having it do the rest again. This is almost instantaneous. From the label menu don't forget to sync it to disk to ensure the new disk label is written, which I almost did (!), after which you can exit. Back in the PROM menu, we'll proceed with the IRIX installer.&lt;p&gt;IRIX, the primary Unix for SGI-MIPS, originated on their first MIPS systems, the SGI IRIS 4D (thus the name, derived from "IRIS UNIX," and not actually an acronym). Prior 68K systems ran "GL2," based on UniSoft's port of UNIX System V; IRIX is a true Unix as well, likewise descended originally from UNIX System V, though it wasn't actually badged as IRIX until 3.0 in 1988 which corresponded to SVR3 with components from 4.3BSD. This "first" release implemented a distinctive window manager called 4Sight designed to resemble their prior "multiple exposure" (mex) interface. Unfortunately for SGI 4Sight was implemented with Sun NeWS and NeWS lost to X11, so IRIX 4.0 switched to X11R4 and the similar Motif-based 4Dwm window manager, which ultimately became IRIX's most lasting visual signature. When people talk about using IRIX, most of the time they're really talking about 4Dwm, a notable contrast against other proprietary Unices which largely used the HP VUE-derived CDE.&lt;/p&gt;&lt;p&gt;IRIX 6.0 made the jump to 64-bit, and 6.5 was the last major version, based on SVR4 and released in 1998. SGI cut off support for earlier machines like the Indy and this Indigo2 at 6.5.22, which is the version we will install, and subsequently locked versions up to 6.5.30 behind a support contract requirement. 6.5.30 was the last official release of IRIX in August 2006, which still supports later machines like the Fuel, and thus the Fuel system here runs that instead. SGI has never ported IRIX to any other platform nor made IRIX open source, nor is the company's current incarnation likely to ever do so, though some portions of source code were officially made available such as the XFS file system (which yours truly uses for the boot volume in my Fedora Linux Raptor POWER9). Various clone window managers exist today that try to recapture the style and substance of 4Dwm on modern hardware, but they end up falling in the uncanny valley in various ways, and in the end there's still nothing like the original.&lt;/p&gt;&lt;p&gt;It is certainly possible to run other operating systems on SGI-MIPS, notably NetBSD up to IP32 and hacked versions of the former OpenBSD port, but I like IRIX too much to do that.&lt;/p&gt;We now have a valid swap partition on the ZuluSCSI image, so we can actually start the installation miniroot this time. Despite the new timekeeper our clock is a little off and we'll correct that from the NTP server when we get networking up, but we don't have a filesystem yet on the image, which the installer will now offer to create. (This step is what I suspect didn't happen with the second hard disk.) Again, this is very fast on the ZuluSCSI, and we go right into the installer from there. From-scratch IRIX installations of later versions involve a lot of disk swapping and overlays. This article is long enough already without me trying to do a comprehensive IRIX installation guide, and it would be fruitless anyway because there are so many permutations. However, this install guide is pretty good for most intents and purposes. There are also various means of doing the installation from a network server, but I don't have this set up, and I try not to do so many IRIX installs that it would become worth it to do so. Here we'll load up the install sets ... ... marvel at the cutting-edge included applications like Java 1.4, Acrobat Reader 4.05b and Netscape Navigator 4.8a ... ... and, after resolving the inevitable conflicts between packages, finally start the installation. I did this all manually with CDs. In the future I might try just loading them all as ISOs on the ZuluSCSI as well. A number of disc swaps later, we are "finished" ... ... except for having to re-quickstart all the ELF files that were just installed. Since this leaves us with a nice clean install of IRIX I could potentially use later, I powered the system off and backed up the disk image. The new sticks of RAM had arrived, so I pulled the 5.25" tray one last time to install them. This brings us to 192MB, but for some unexplained reason the system then put up the solid amber LED again when I tried to reboot. I got out a serial cable this time.&lt;quote&gt;System Maintenance Menu 1) Start System 2) Install System Software 3) Run Diagnostics 4) Recover System 5) Enter Command Monitor Option? 5 Command Monitor. Type "exit" to return to the menu. &amp;gt;&amp;gt; hinv System: IP28 Processor: 195 Mhz R10000, with FPU Primary I-cache size: 32 Kbytes Primary D-cache size: 32 Kbytes Secondary cache size: 1024 Kbytes Memory size: 192 Mbytes SCSI Disk: scsi(0)disk(1) SCSI CDROM: scsi(0)cdrom(6) Audio: Iris Audio Processor: version A2 revision 1.1.0&lt;/quote&gt;&lt;p&gt;For some horrible reason the Solid IMPACT card was suddenly not being seen. I decided to see what the diagnostics would say about that, since it was obviously just working and I hadn't messed with the graphics card or the riser. This can run from the new install of IRIX; we don't need the CD.&lt;/p&gt;&lt;quote&gt;&amp;gt;&amp;gt; exit System Maintenance Menu 1) Start System 2) Install System Software 3) Run Diagnostics 4) Recover System 5) Enter Command Monitor Option? 3 Starting diagnostic program... Press &amp;lt;Esc&amp;gt; to return to the menu. Checking for Distribution CD-ROM on scsi(0)cdrom(6). dks0d6s8: Device not ready: Medium not present dks0d6s8: drive is not ready Distribution CD-ROM not found. Booting installed IDE. SGI Version 6.5 IP28 IDE field Oct 6, 2003 System: IP28 Processor: 195 Mhz R10000, with FPU Primary I-cache size: 32 Kbytes Primary D-cache size: 32 Kbytes Secondary cache size: 1024 Kbytes Memory size: 192 Mbytes Graphics: Solid Impact SCSI Disk: scsi(0)disk(1) SCSI CDROM: scsi(0)cdrom(6) Testing Impact graphics.&lt;/quote&gt;&lt;p&gt;Notice that when the diagnostics program started, it did see the board, and could drive it for testing.&lt;/p&gt;The full diagnostics check took about a half hour to run, but in the end everything seemed fine ...&lt;quote&gt;TEST RESULTS: CPU tests completed. FPU tests completed. Audio tests completed. SCSI tests completed. Impact graphics board tests completed. Diagnostics completed - press &amp;lt;Enter&amp;gt; to continue&lt;/quote&gt;&lt;p&gt;... so I'm not sure what happened there. The system rebooted uneventfully and showed a proper green LED this time, though on the maiden boot of the operating system it switched to a non-60Hz mode the INOGENI didn't like and required me to hook up the Hall scan converter. These next few images are a bit fuzzy as a result. Sorry about that.&lt;/p&gt;Yes, yes, we'll eventually fix the clock. Yes, yes, we'll eventually plug in the network. Four default logins are created, root (duh), demos, guest and EZsetup. None of these have a password yet. Don't put this on an unprotected network exposed to the outside world, please.&lt;p&gt;I don't really need the easy setup, but for demonstration purposes here it is.&lt;/p&gt;EZsetup's post-install wizard features four steps to greatness: security, networking (because nothing says security like networking), creating a non-root account, and customizing your work environment, which mostly means dumbing down 4Dwm and setting up Netscape. Security, in this case, means creating passwords and setting permissions. This machine will spend the rest of its days with me only ever on the hardwired non-routable internal network, but this step is certinaly better than nothing for those of you who may not have the locked-down playground I have here for such machines. After you've accomplished these trivial token tasks, you can quit, though the account remains available. On our next boot the refresh rate issue sorted itself out and we come to the nice clean 4Dwm desktop of our new user account. I have more applications to install, but before we do that, we need to put its top case back on and find it a better place of lurkage than under that table. Shutting IRIX down. The top cover is reinstalled by fitting these rear plastic tabs into slots in the metal back panel and rotating it back down into position. And, of course, they're old plastic, so they immediately tried to separate and snap off from the upper lid when I inserted them. Light-set high quality cyanoacrylate time. Despite this, the top cover still wouldn't come all the way down onto its clips — it was getting stuck on the ZuluSCSI's plastic carrier, which needed to be pushed back a little. Of course, the minute I did that, the plastic clip on the drive sled snapped. I got it glued back together but it had snapped at the very thinnest section, naturally, and the spring chose this moment to come loose again at the same time. After some screaming and infuriated reassembly the sled's clip was good for exactly one insertion before it snapped again in the same place, but only one insertion was required; the new position of the carrier allowed the top case to finally come down fully and lock. I'll deal with that if I ever have to take it out again, which comfortably should be never.&lt;p&gt;I also put back the second drive after disconnecting its power. I don't really have anywhere else to keep the sled and at least that drive is formatted and wired up, so it might as well stay there.&lt;/p&gt;With the front bezel back on, we have a green LED and a clean boot into the system maintenance menu. It's time to assign it a home. I found a nice cleared-out area where it could sit vertically next to homer, the HP 9000/350 rack in the server room, and dragged it on back. As we won't be using the MacBook as its display anymore, we'll need to do something about the sync-on-green. Such boxes are not very common nowadays, but Software Integrators still sells brand-new active sync converters (model #7053) that filter out the sync signal from the green channel and turn it into composite sync which most monitors will recognize. I use one of them with my Indy and I keep a couple more in stock (not affiliated, just satisfied). They aren't cheap and they're active components that require their own power supply, but these boxes are probably the least expensive ones you'll find and they're still manufactured. With the network up, files are obtained, and serious sgidoom and Mozilla 1.7 business ensues. I was pleased to note that despite not having an external LED light to connect to, the internal activity LED of the ZuluSCSI can be easily seen through the bezel apertures with the front door down. Shutting down. Now that I know the new RAM works, I think I'll fill up the other bank with an additional 128MB, plus I'm toying with tracking down a 10/100 Ethernet card for the EISA slots. It's stereo-capable, so I should figure out how that works, and of course if a High IMPACT or Max IMPACT card turns up at a decent price, I'd be a&lt;p&gt;With our refurb complete, let's finish the story. The R10000 was only intended to be SGI's next processor architecture, but it ended up being their last. Originally, MIPS' market strategy was to create high-end chips and then shrink them, which it did successfully for the R3000 and R4000. However, that strategy fell apart with the R8000, and in the R10000 era MTI instead decided on tiered chips for multiple market points by continuing to iterate on earlier designs. This even progressed to the point that MTI planned on making a midrange "D2" follow-on to the R5000, which wasn't even their design, and a separate high-end "H1" chip codenamed the "Beast."&lt;/p&gt;&lt;p&gt;The revised strategy might have worked except that MTI didn't have enough design resources for multiple microarchitectures. So SGI went back to the old strategy: big powerful cores that could be scaled down.&lt;/p&gt;In May 1997 SGI announced their new MIPS roadmap (here reproduced from Microprocessor Report May 12, 1997), starting with an enhancement of the R10K called the R12000. It would be built on the current 250nm process size, enlarge the number of instructions in flight to 48 from 32 and add a new pipeline stage, implement a new branch target address cache with 32 entries a la contemporary PowerPC, and expand the branch history table to 2048 entries from 512 (compensating for the longer pipeline's larger mispredict penalty) while doubling the size of the way-prediction table to better support large L2 caches. SGI expected the R12000 to hit 300MHz or better and would only need about ten percent more die space.&lt;p&gt;The "Beast" H1 remained in SGI's plans, now scheduled for 1999 at 250nm and 2000 at 180nm, to be followed by an even higher-performance "H2" core codenamed "Alien" somewhere around 2001. Along the way SGI-MTI gamely predicted a six-fold increase in integer performance from the 350nm R10K to the terminal H2. Although Alien was too far in the future to be definite, Beast had some lofty and specific goals; it was to be a MIPS V CPU with MDMX SIMD extensions, implementing a 256-bit cache bus and 256-bit main memory bus at speeds of 200MHz or higher. Alien, on the other hand, was intended for large NUMA multiprocessing systems and correspondingly with even higher bandwidth, by no means coincidentally similar to Cray that SGI had just bought out.&lt;/p&gt;&lt;p&gt;Industry analysts didn't find R12K particularly compelling against expected future competitors and expected Beast would provide a stronger punch. However, after about a year's worth of work — which was only three months after the roadmap announcement — SGI abruptly canned Beast to the market's general surprise, claiming that R12K would be more scalable than anticipated and they would redirect engineers to Alien instead. More ominous to customers was SGI's quieter simultaneous announcement of "Intel-based" Windows NT systems to emerge in 1998, which SGI called the "Visual PC" initiative. Indeed, in 1998 SGI shut down Alien development as well, something that had been all but openly expected, and spun off MTI as a separate company in March to focus on MIPS in the embedded market. The "Visual PC" emerged in August 1999 as the SGI Visual Workstation running Windows NT 4. The first two machines in the series were not fully standard Pentium II and III PCs; they maintained custom graphical boot PROMs and used SGI's bespoke Cobalt graphics, requiring a custom HAL for NT 4 and Windows 2000. Architecturally these early VWs were similar to the SGI O2, while later units were essentially commodity PC hardware with Nvidia Quadro GPUs.&lt;/p&gt;&lt;p&gt;As Itanium started looming over the market, SGI too lost its corporate mind and jumped on the IA-64 bandwagon. Meanwhile, despite vague noises about porting IRIX to x86 (or possibly even Itanic) or providing an emulation layer, the existing MIPS install base proved simply too large to ignore, and SGI did not want to lose existing customers with large ecosystem investments to other vendors — especially since the roadmap for Itanium was far from certain. For this legacy market SGI retained some of the MIPS engineers to continue refining the R10K core and produced new hardware around it. The R12000 finally started shipping in February 1999 initially at speeds up to 300MHz, fabbed by NEC and Toshiba at 250nm and reducing the 7.15 million transistor die to 229mm², though delays and the Osborne effect limited its availability until May. From the R12K to the R14000 and R16000 (codenamed "N0"), each generation had a corresponding "A" subtype with boosted clock speeds, but other than die and process shrinks few other microarchitectural improvements were made. In general these late CPUs were clocked relatively low to reduce heat and power usage. The last, mightiest and rarest R16000A, fabbed by NEC at 110nm in 2004, topped out at 1.0GHz and was only produced for specific customers in limited quantities. None of these chips ever made the jump to MIPS V.&lt;/p&gt;SGI did consider two more chips in the series, though the company was increasingly in the weeds by this point and neither was produced. The best known of these was the R18000 (codenamed "N1"), which SGI presented an early version of in 2001. Specifically intended for SGI's ccNUMA servers, its core was recognizably based on the R10K but each FP unit now had independent multiply, add, divide and square root capability as well as multiply-add, and the core also got an additional load/store unit. A new "SysTF" bus used twin DDR links, one a large non-multiplexed 128-bit read path plus a smaller 64-bit multiplexed write and address path, as well as preserving Avalanche ("SysAD") compatibility. Up to 1MB of L2 cache was on chip, and up to 64MB of external L3 cache (with cache tags on-die) was supported. It was to be fabbed by NEC on a 130nm process in nine layers of copper, and planned for around 2004.&lt;p&gt;In 2002 SGI suggested N1 could be fabbed at 110nm and run up to 800MHz, but also mentioned an "N2" to appear in 2005 which was widely believed to be a future R20000. SGI promised speeds of 1GHz or more and up to 8 gigaflops of FP performance per core, though the company provided little other information. Both chips were quietly cancelled.&lt;/p&gt;&lt;p&gt;On the graphics side, the terminal VPro ("Odyssey" and "InfinitePerformance") architecture could at best only tread water: it had some very advanced lighting and colour features, but suffered from inordinately weak memory bandwidth and poor texture mapping performance, and was expensive to produce on top of that. SGI abandoned it for Nvidia GPUs which, despite some being badged as VPro cards, were not compatible with legacy MIPS. The MIPS server line offically came to an end with the Intel Itanium 2 (McKinley) Altix in January 2003, SGI's first Itanic systems, running Windows NT and Linux; the workstations were subsequently succeeded by the McKinley-based Prism in April 2005 using the same architecture and ATI FireGL GPUs. They were poorly priced and poorly performing, another nail in the coffin of Itanium, and consequently failed to restore SGI to its former glory. All MIPS product lines were finally terminated in December 2006.&lt;/p&gt;&lt;p&gt;I have some more machines to work on and disposition, by the way. You'll be seeing those soon.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://oldvcr.blogspot.com/2025/09/refurb-weekend-silicon-graphics-indigo.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45238055</id><title>Models of European Metro Stations</title><updated>2025-09-14T08:38:31.836422+00:00</updated><content>&lt;doc fingerprint="3c5f4669f52d16b9"&gt;
  &lt;main&gt;
    &lt;p&gt;Select a city and a station on the map or on the selectors:&lt;/p&gt;
    &lt;p&gt;Alicante's rapid transit system is called TRAM, which is a train-tram and is operated by FGV, owned by the Valencian regional government. This system has an underground line in the city center, with three stations on it. The layout consists with one or two mezzanines on level -1 and two side or one island platforms at level -2. Luceros and Mercado stations have direct connection with the underground parking lots, which are located above the TRAM tunnel.&lt;/p&gt;
    &lt;p&gt;Metro services in Amsterdam can be grouped in two groups. The North-South line (in Dutch, Noord/Zuidlijn) opened recently and M52 services run on it. The central part of the line is underground, running at a considerable depth with two parallel tunnels. The other group of lines consists of services M50, M51, M53 and M54. This line runs mostly overground or elevated, parallel to the Dutch Railways lines. In the city center this line goes underground but at a small depth.&lt;/p&gt;
    &lt;p&gt;Most of the underground stations consist of a mezzannine at level -1 and a island platform at level -2. The vast majority of overground and elevated stations also have island platforms, but the hall where the turnstiles and ticket machines are found is located on the ground level.&lt;/p&gt;
    &lt;p&gt;Taking into account that most of the metro network runs parallel to rail lines, transfers between these two modes of transport are quick and easy.&lt;/p&gt;
    &lt;p&gt;The Antwerp premetro (underground tramway) opened in 1975. One of the achievements consisted in linking both banks of the Scheldt river.&lt;/p&gt;
    &lt;p&gt;Despite the tram network being huge, the Premetro is short but complex. Lines can be grouped in two. The first group consists in an east-west line with two branches on the eastern side. However, there is a tunnel allowing services between both branches, thus forming a triangle. The second group of lines consists in a tunnel opened in 2015, connecting the city center with the eastern suburbs with a fast service, since most of stations are not opened yet.&lt;/p&gt;
    &lt;p&gt;The Central railway station is linked with Astrid and Diamant premetro stations, and together with an underground bike parking, they form a hub. The rail triangle is located in between those two stations. Since the premetro tunnels were built with metro standards, same-level track crossings are not allowed, therefore Astrid and Diamant stations had to be built with platforms at two different levels.&lt;/p&gt;
    &lt;p&gt;Barcelona is the queen of the long passageways. Until 1995, all transfer stations consisted of corridors with lenghts over 100 m, except for Sagrera and Catalunya.&lt;/p&gt;
    &lt;p&gt;Among the reasons for having such long corridors there is the lack of planning or the vision of the metro network as a bunch of individual lines. As an example: line 1 and line 4 were extended to Urquinaona in 1932, but both lines were not connected until 1972, as they were originally operated by different companies. In PlaÃ§a de Sants station, the L5 platforms were built as close as possible to the existing ones from L1, which opened 43 years before. However, there is a gap of 150 m, with the national rail tracks located in between.&lt;/p&gt;
    &lt;p&gt;Moreover, since the extension of the metro network was slower than the growth of the city, during the 60s and 70s, transfer stations were built 100-150 m apart, in order to increase the accessibility to the metro. Verdaguer is a good examples of this practice.&lt;/p&gt;
    &lt;p&gt;Right after 1980, transfer stations were designed in a more proper way, being the L2 stations the best example. A new type of transfer appeared 15 years ago with the extension of the metro towards hilly areas of the city: the vertical transfers. In those stations (eg: Vall d'Hebron, Fondo, Zona UniversitÃ ria, Collblanc), a big shaft was built in order to fit either high-capacity lifts or series of escalators to reach the platforms.&lt;/p&gt;
    &lt;p&gt;The classic layout of metro stations in Barcelona is simple: on level -1 there is one or two mezzanines and in level -2 there are two side platforms, but since 2000 the new stations tend to have a central plafrom instead. In transfer stations, there is tipically a corridor linking the mezzanines of both lines.&lt;/p&gt;
    &lt;p&gt;Mandatory to mention the so-called Barcelona solution or Spanish solution: stations with two tracks and three platforms, where passengers alight using one platform and board using the opposite one, improving the flow of passengers and reducing the dwell time. Barcelona was not the first city to implement it, but it was named after most of the stations built in the 30s, 40s and 50s were designed with such layout.&lt;/p&gt;
    &lt;p&gt;The U-Bahn and the S-Bahn are the two rapid transit systems of Berlin. The city-state owns the first and the German railways own the second. The S-Bahn normally runs overground or elevated and the U-Bahn does it underground, as the name suggests in German, but this is not a norm since some parts of the U-Bahn are elevated and vice versa.&lt;/p&gt;
    &lt;p&gt;The oldest lines were built either in elevated sections or underground, but very close to the surface, creating a direct connection between the street and the platforms in most of the cases. The lines built after World War I are a bit deeper, in order to fit mezzanines and corridors between the street and the platform levels.&lt;/p&gt;
    &lt;p&gt;Transfer stations have a very easy tolopolgy, since stations are located close to the ground level and island platforms are predominant.&lt;/p&gt;
    &lt;p&gt;Despite the metro opened in 1995, its layout follows former metric-gauge rail lines that were converted to metro, such as the Bilbao - Plentzia line. Metro Bilbao operates lines 1 and 2 and Euskotren operates line 3, together with other suburban and regional services.&lt;/p&gt;
    &lt;p&gt;Due to the orography of the city, stations may have different layouts. The stations located in the city center are deep, since the Nerbion river is located nearby and there are plenty of hills located in the meanders of the river. The stations of the line 1 branch are located on surface, since they were part of the former Plentzia railway.&lt;/p&gt;
    &lt;p&gt;Line 3 benefits from having already existing stations, such as Bilbao-Aduana (currently called Zazpikaleak/Casco Viejo) and Matiko, both part of the former Plentzia line, despite the connection between those two stations is made through a new double-track tunnel.&lt;/p&gt;
    &lt;p&gt;Some transfer stations have effective designs, such as Lutxana or the former Bolueta station.&lt;/p&gt;
    &lt;p&gt;All the rapid transit lines in Boston are centenary. Three of them originally opened as underground tramway tunnels in downtown Boston, but only two were converted to pure metro lines. The Green line is still an underground tram line.&lt;/p&gt;
    &lt;p&gt;Most of the downtown stations are quite unusual, as they were built using early construction methods and most were designed to serve as tram stations. For instance, some stations in the Orange line have offset platforms. At State station, the Orange line platforms are in different levels.&lt;/p&gt;
    &lt;p&gt;Underground stations tend to be located close to the surface. This can be appreciated at Government Center, where the Green line platform has a very particular shape because the streets located above ground were quite narrow when the line was constructed, and therefore impossible to dig wider tunnels.&lt;/p&gt;
    &lt;p&gt;Despite being a city where the rapid transit lines were built without a network plan, transfer stations have a good layout, providing short and quick connections.&lt;/p&gt;
    &lt;p&gt;The Brussels metro is a good example of a planned system. It was planned in the 60s, and the four existing lines were built following this criteria: if for practical reasons, the section to be opened was not long enough to be operated as a metro line, the section would open provisionally as a premetro (underground tram), with temporary low floor platforms and temporary ramps connecting the streets with the tunnel. Once the construction of the line is advanced, the premetro operation is cut and the line is converted into a full metro line, removing the ramps and elevating the platforms to metro standards. Currently lines 1, 2, 5 and 6 are fully metros and lines 3, 4 and 7 are premetros.&lt;/p&gt;
    &lt;p&gt;The whole network has been built following the same plan. Transfer stations have a good design as well, with the notable exception of DÃ¨ BrouckÃ¨re, where there is a long corridor. Cross-platform transfers can be found in Beekkant and Gare du Midi. Arts-Loi, Montgomery and Rogier have crossing interchanges.&lt;/p&gt;
    &lt;p&gt;Most stations have side platforms located at level -2, whilst level -1 is reserved for mezzanines. Stations on line 6 have island platforms because trains run on the left side instead. Stations on lines 3/4 were built using the Spanish solution, where passengers board using the island platform (except in Gare du Midi).&lt;/p&gt;
    &lt;p&gt;In order to reduce fare evasion, most stations have been equipped with turnstiles during the last decade, replacing the existing honor system which is still present in some stations due to the complexity of its layout.&lt;/p&gt;
    &lt;p&gt;Budapest was the first city in continental Europe to have a metro. Line 1 opened in 1896. Similar to other systems opened around 1900, tunnels have a narrow profile. Trains are only 2.60 m high and 30 m long.&lt;/p&gt;
    &lt;p&gt;Lines 2 and 3 have a similar style to other Eastern European metros, despite the decoration has nothing to do. Line 4 was built recently. In these three lines, tracks are laid in two parallel tunnels that cross Budapest at a considerable depth. Stations have island platforms in two parallel galleries, which are connected with the ticket hall through a long escalator. The ticket hall can either be located at ground level or just underground. In the latter case, there are direct staircases connecting the hall with the tramway platforms or bus stops.&lt;/p&gt;
    &lt;p&gt;The northernmost and southernmost parts of line 3, tracks run underground but at a very little depth. Here stations have side platforms, which are located at level -2 (and a mezzanine at level -1) or directly at level -1, together with an independent booking hall for each direction.&lt;/p&gt;
    &lt;p&gt;Transfers differ, depending on the station. In DeÃ¡k Ferenc TÃ©r and KÃ¡lvin TÃ©r, transfers consist in a quick connection through two escalators and a short corridor. In other stations, transfers may be longer since one line might be running close to the surface while the other may be running very underground (eg: Keleti pÃ¡lyaudvar, BatthyÃ¡ny tÃ©r)&lt;/p&gt;
    &lt;p&gt;The Bucharest metro consists of 5 lines, despite lines M1 and M3 share their tracks and line M5 has two branches. The original plan from the 70s consisted of three lines: a east-west line (M3 and southern section of M1), a north-south line (M2) and a circle line circa 40 km long.&lt;/p&gt;
    &lt;p&gt;Unlike other Eastern European metros, the Bucharest one was not built at a considerable depth but using cut-and-cover methods. The platforms are typically located at level -2 or -1 and tend to be central, despite some stations in line M1 have side platforms.&lt;/p&gt;
    &lt;p&gt;The Subte is the oldest metro network in Latin America. It consists of 6 lines. 4 of them run east-west (A, B, D, E) and the other two are north-south (C, F).&lt;/p&gt;
    &lt;p&gt;The entire network runs underground and except for line H, the tunnels were build close to the surface. Most of the stations have side platforms but those stations that are or had been terminus usually have island platforms.&lt;/p&gt;
    &lt;p&gt;The lower level of an average station comprises the platforms. If there is enough space between the platform level and the street level, a mezzanine can be found, containing the ticket booths and turnstiles. In the opposite case, each platform has independent accesses and turnstiles are located at the platform level and there are no overpasses or underpasses.&lt;/p&gt;
    &lt;p&gt;The Subte applies a bizarrenaming criteria. Station names are independent for each line. So, a transfer station will have multiple names, one for each line calling at it (eg: PueyrredÃ³n (B) and Corrientes (F) are part of the same station). But there is also the opposite situation, where there are stations with the same name in different lines: Callao and PueyrredÃ³n are four stations located in lines B and D, but they have nothing to do.&lt;/p&gt;
    &lt;p&gt;Copenhagen has a modern and automatic metro. Both the Metro and the S-tog have an honor system and most underground stations look like the same: mezzanine with vending machines at level -1, a landing at level -2 and island platforms at level -3. The connection between the platform and the mezzanine is provided by lifts and escalators. Upbound escalators are separated from the downbound ones.&lt;/p&gt;
    &lt;p&gt;Transfer corridors are connected to mezzanines in Frederiksberg and Kongens Nytorv, but not in NÃ¸rreport, where the corridor heading to the S-tog starts at the end of the Metro platform.&lt;/p&gt;
    &lt;p&gt;Despite Frankfurt has officially a U-Bahn (pure metro), technically is a Stadtbahn (light rail), despite the platform level is high even for stops located at the street. The S-Bahn is also part of the basic rail network of the city.&lt;/p&gt;
    &lt;p&gt;The main transportation hubs were desinged to provide quick transfers between the rail lines. Most transfer combinations involve passing through a staircase or an escalator, but in Hauptwache, a cross-platform is provided for transfers between S-Bahn and U-Bahn lines U6 and U7.&lt;/p&gt;
    &lt;p&gt;Glasgow has a very peculiar subway. It opened in 1896 as a circle line powered by cable, like the San Francisco cable cars. The subway was fully renovated between 1977 and 1980 in order to change its operation to electric power, build new workshops, relocate stations and refurbish the rest of stations.&lt;/p&gt;
    &lt;p&gt;Before the renewal, all stations had a very narrow island platform. A staircase located at the end of one platform lead to the ticket hall and the exit, typically located inside the station building. Most of the stations retain this layout today. However, escalators have been installed in some stations, to connect the ticket hall with the street. The stations with the highest patronage were completely rebuilt: Buchanan Street has a central and a side platform nowadays. In St Enoch, two side platforms replace the former island platform. Partick station was built during the renewal works in order to allow transferring from the Subway to ScotRail.&lt;/p&gt;
    &lt;p&gt;Hannover has a Stadtbahn, which is a mixture of a tramway that runs in a metro-like tunnel in the city centre.&lt;/p&gt;
    &lt;p&gt;There are three trunk tunnels in Hannover (A, B, C) that meet in KrÃ¶pcke station. Tunnels B and C run overlapped between KrÃ¶pcke and Aegidientorplatz.&lt;/p&gt;
    &lt;p&gt;The layout of an average station is simple and similar to the ones in other cities: a mezzanine at level -1 and side platforms at level -2.&lt;/p&gt;
    &lt;p&gt;KrÃ¶pcke station is a huge complex with lots of accesses, escalators, staircases and mezzanines, since is the only point where the three tunnels meet. Hauptbahnhof station (main station) has elevated rail platforms and an underground station for the Stadtbahn, composed of two island platforms and four tracks. Aegidientorplatz station has a peculiar layout, since the mezzanine is at level -1, the northbound platform is at level -2 (with 2 tracks) and the southbound platform is at level -3 (with 2 tracks as well).&lt;/p&gt;
    &lt;p&gt;The four lines of the Lyon metro are technically different. Lines A and B and D are rubber tired. Line A has manual conduction and line D was already opened with automatic trains. Line B switched from manual driving to automatic operation in 2022.&lt;/p&gt;
    &lt;p&gt;Line C partially follows the route of a former funicular that had a pent of 176â°. Therefore it operates as a rack railway between HÃ´tel de Ville and Croix Rousse, since normal metro lines have a maximum pent of 40â°.&lt;/p&gt;
    &lt;p&gt;Lines A and B run at a very little depth. Even line A crosses the Rhone using the box girder of Pont Morand. The stations of this line only have one underground level, with side platforms and turnstiles on it. Some stations have underpasses, connecting both platforms. Line D tunnels are not located deep either, but most stations have a mezzanine between the street and platform levels. The latter situation also occurs in the eastern section of line A.&lt;/p&gt;
    &lt;p&gt;Lyon had an honor system until two decades ago. Since some staion have secondary accesses connecting the streets with the platform, it is common to find turnstiles in the platforms.&lt;/p&gt;
    &lt;p&gt;Most of the transfer stations have an efficient layout, such as HÃ´tel de Ville, Saxe-Gambetta or Charpennes. However, the links between at the two main railway stations (Part-Dieu and Perrache) are long.&lt;/p&gt;
    &lt;p&gt;Currently the Lisbon metro consists of four lines. Until the mid-90s, the network had a Y-shaped single line. This line was split into three (Blue, Yellow and Green) and the Red line was built as a completely new line. Therefore, all the internal metro transfers are less than 25 years old.&lt;/p&gt;
    &lt;p&gt;The layout of the stations differs slightly: Baixa-Chiado and Campo Grande have parallel platforms for the two lines serving the stations. Saldanha, SÃ£o SebastiÃ£o or Oriente allow quick transfers that involve climbing stairs or elevators. However, in Alameda or Entre Campos the transfer consists in a long passageway.&lt;/p&gt;
    &lt;p&gt;The standard layout for the stations built before the Carnation Revolution consisted in two side platforms at level -2 and a mezzanine at level -1. These stations featured short platforms, that were extended years later to allow 6-car trainsets. Together with these platform extensions, secondary mezzanine and accesses were added. The stations built recently tend to have a single mezzanine and are located at deeper, since they were built in hilly areas.&lt;/p&gt;
    &lt;p&gt;The London Underground (or the Tube) is the oldest in the world and it consists of two different networks: the sub-surface lines (running close to the surface) and the deep tube lines (runing deep, with tunnels that ressemble a tube).&lt;/p&gt;
    &lt;p&gt;The sub-surface lines are the ones inherited from the Metropolitan Railway and from the District Railway, two lines built using cut-and-cover methods or running elevated or at ground level, which were steam operated until the early 20th century. Stations consists in a building located at ground level, containing the ticket offices and fare gates, and the platforms placed at level -1. The stations were located inside a block of houses whenever possible and most of them had a canopy (still in place in Paddington, Bayswater or Earl's Court). With the electrification of the tracks and real estate speculation, most of the stations have been covered with concrete labs (such as in Gloucester Road or Mansion House).&lt;/p&gt;
    &lt;p&gt;The deep tube lines began to be built in the last decades of the 19th century, when the boring methods were a bit developed. They were already planned to be operated with electric trains. The tunnels were bored at a depth of 20 m and until the construction of the Victoria Line, they followed the streets. Stations consists of a building hosting the ticket offices and the fare gates, which are connected with the platforms (most of them are island platforms) via lifts and spiral staircases located inside of shafts. In 1913 an escalator was installed at Earl's Court as part of a test to replace lifts. Since the result was favorable, the practice of building stations with lifts and spiral staircases in shafts was abandoned. From this moment on, the new stations were built with escalators, and the existing ones (especially those with a high ridership) were transformed.&lt;/p&gt;
    &lt;p&gt;Initially each line was operated by a different company, so transfers between lines weres not granted. This was corrected after the nationalisation of the Tube.&lt;/p&gt;
    &lt;p&gt;Some stations host huge flows of passengers. This obligated the local authorities to build wider or newer passageways and escalators in some transfer stations. That is the reason some transfer stations have one-way corridors as well, such as in Oxford Circus or Victoria.&lt;/p&gt;
    &lt;p&gt;technically, the Madrid metro lines can be divided into two: the narrow profile lines (1-5 and the Ramal) and the wide profile lines (6-12). The narrow profile lines were the first to be built and were inspired by the Paris metro. That is why old stations have side platforms located at level -2 and one mezzanines at level -1. Decades after the opening, lines 1 and 3 got their platforms extended from 60 m to 90 m long and secondary mezzanines and accesses were added.&lt;/p&gt;
    &lt;p&gt;The lines with a wider profile were built with the aim of fitting larger trains and larger stations. Moreover, these lines were built a highest depth in comparison to the narrow profile lines, at a depth of 15 to 25 m below surface level. The streets and the platforms are connected through 3 to 5 series of staircases and escalator, with the ticket hall in between. Therefore, access times are longer compared to narrow gauge lines.&lt;/p&gt;
    &lt;p&gt;Since the 90s, the design and layout of new stations have been standadrised, with stations being built using cut-and-cover methods with slurry walls.&lt;/p&gt;
    &lt;p&gt;Madrid has plenty of different layouts for transfer stations. In comparison with other cities, Madrid has a great amount of stations containing long corridors, but not at the level of Barcelona.&lt;/p&gt;
    &lt;p&gt;Possibly the most prominent stations are the macrohubs built in the first decade of the 2000s, with huge mezzanines, wide staircases and lots of lifts and escalators, allowing quick connections between metro, commuter train and interurban buses as well. The best example is Nuevos Ministerios, but ChamartÃn, Sol, PrÃncipe PÃo, Plaza de Castilla or Moncloa also need to be mentioned.&lt;/p&gt;
    &lt;p&gt;The Marseille metro is formed by two lines that cross themselves in Saint-Charles and Castellane, both located in the city centre. The links between lines in these two stations are quick, since in Saint-Charles line 2 is between the two tracks of line 1 and in Castellane both lines have their platform very close to the intersect point.&lt;/p&gt;
    &lt;p&gt;There are three kinds of stations, according to their layout: the suburban, which are elevated (eg: Bougainville, Ste-Margueritte, La Rose); the underground ones close to the surface, with the ticket hall at level -1 and side platforms at level -2 (eg: Baille, PÃ©rier); and the deep centric stations, which have island platforms and a single access consisting of long escalators connecting the street level with the mezzanine (eg: Cinq Avenues, Estrangin).&lt;/p&gt;
    &lt;p&gt;Noailles station is the most particular since the former tram tunnel was diverted when metro line 2 was built. Currently the former tunnel serves as a passageway, and the former tram terminus is a ticket hall.&lt;/p&gt;
    &lt;p&gt;Line 1 opened in 1964 and was the first metro line in the world to be built using slurry walls. Line 2 opened four years, following the same design standards for line 1. Stations have a very functional layout, since passenger flows were seriously taken into account. Almost all stations have one-way staircases connecting the side platforms (located at level -2) and mezzanines (at level -1): one for passengers entering and another for passengers alighting.&lt;/p&gt;
    &lt;p&gt;The same concept was latter aplied to lines 3 and 5, opened decades later, despite both having a radically different design in their stations, compared to lines 1 and 2. Most of their stations also have mezzanines at level -1 and side platforms at level -2.&lt;/p&gt;
    &lt;p&gt;Engineers opted to superimpose the tracks in the central section of line 3, where tracks follow the narrow streets of the old city. As a result, the stations of this sections are a maze of lifts, one-way staircases and escalators, combined with a peculiar decoration.&lt;/p&gt;
    &lt;p&gt;The transfers are typically short and quick, especially in Loreto, Centrale, Cadorna and Repubblica. The only two stations having long corridors are Lotto and Porta Venezia.&lt;/p&gt;
    &lt;p&gt;The Paris Metro has the most labyrinthine interchanges in Europe.&lt;/p&gt;
    &lt;p&gt;Almost all the metro network was opened before World War 2. The first transfer stations, opened in the early 1900s, had simple transfers, with bidirectional passageways. Since 1920, the company responsible for the metro began to install portillons automatiques (automatic doors) in some stations. The portillons automatiques are doors located at the entrance of a platform, that blocks the access to it once a train enters the station, in order to reduce the dwell time and prevent last-second passenger boarding the trains. Therefore, in order to limit the access to platforms but not the exits, they decided to build two-directional corridors and staircases. That is how the stations began being underground mazes.&lt;/p&gt;
    &lt;p&gt;In the 1970s the RER (suburban railway) was opened. This meant that some metro stations had to be partially rebuilt.&lt;/p&gt;
    &lt;p&gt;Taking the advantade that the spacing between stations is one of the lowest in the world, some RER stations were placed between two metro stations, so both could be connected with the RER. The most extreme case is that 6 metro and RER stations are connected underground (St-Augustin, St-Lazare, Haussmann-St-Lazare, Havre-Caumartin, Auber and OpÃ©ra).&lt;/p&gt;
    &lt;p&gt;The average metro station in Paris consists of a mezzanine at level -1 and two side platforms at level -2. The termini of the oldest lines used to have a loop (Place d'Italie M5, Ãtolie M6, Nation M6, Porte Dauphine M2). Some of these stations were actually doubled, since there was a station for the trains that terminate there and another one for the trains (and passengers) beginning their journey.&lt;/p&gt;
    &lt;p&gt;The New York subway combines an extensive network of elevated trains (in Brooklyn and in the Bronx) with an underground train network that was built in the first half of the 20th century, especially in Manhattan. The subway links Manhattan with the other boroughs with tunnels or via the well known bridges, such as the Williamsburg Bridge or the Manhattan Bridge&lt;/p&gt;
    &lt;p&gt;A characteristic feature of this subway is the existence of local and express services in separate but parallel tracks, so a tunnel can have 4 tracks (2 per direction and 2 for each type of service). The express trains run in the central tracks and the local trains run in the side ones.&lt;/p&gt;
    &lt;p&gt;The elevated sections have stations placed above the streets and they can be accessed via staircases located in the sidewalks. Ticket halls are located at level +1, inside the viaduct, and platforms are at level +2, partially covered by shelters. The stations with express services tend to have central two platforms.&lt;/p&gt;
    &lt;p&gt;The underground lines run very close to the surface and they have plenty of piles of steel between the tracks and in the platforms. The stations that are served only by local services have side platforms at level -1, together with the ticket halls. Platforms are not linked via an underpass or an overpass. Express stations have two island platforms at level -2, allowing cross platform transfers between the local and express trains. In this kind of stations, mezzanines with the fare gates are located at level -1.&lt;/p&gt;
    &lt;p&gt;The three lines of the Prague metro form a radial network. All the lines cross at three selected stations in the city center. Almost the totality of the network is underground. The first line that was built (C) is the one running closer to the surface, especially at the city center and in the southern section. Lines A and B run much deeper.&lt;/p&gt;
    &lt;p&gt;The deep stations have island platforms at the deepest point of the station. The ticket hall, which is located beneath the street level, is connected with the platforms through escalators. A few stations even have two mezzanines, one at each end of the station.&lt;/p&gt;
    &lt;p&gt;Line C has not very deep stations at the southern part. They are composed of a island platform at level -1 and two ticket hall buildings located at ground level, each one at a different end of the platform.&lt;/p&gt;
    &lt;p&gt;At MÅ¯stek i Florenc, both metro lines run at a considerable depth, so both stations are linked through a rather short corridor, located at a level between both metro lines. Hoewever, Muzeum station has line C located close to the surface, whilst line A station is deep. Both lines are connected through a series of escalators.&lt;/p&gt;
    &lt;p&gt;Most of the metro stations have shops and stores in the ticket halls.&lt;/p&gt;
    &lt;p&gt;The Rome metro has few lines compared to the extent of the city. Until a few years ago, there were only two lines that intersected at Termini station, which is the central railway station as well. Because of it, Termini station is the one with the highest ridership of the city and most of the times it gets crowded. This station has one-way corridors for entering, exiting and even for transferring. A decade ago, the city had to build an extra set of escalators and corridors to decongest the existing link connecting lines A and B.&lt;/p&gt;
    &lt;p&gt;The other interchange station is San Giovanni. Line C opened in 2018 but the final passageway between both lines is not completed yet. Currently, passengers need to exit through the faregates and enter again.&lt;/p&gt;
    &lt;p&gt;The stations of the southern section of line B were opened in the 1950s and are very simple. The construction of the northeastern section of line B and the entire line A is much more recent and their stations were designed with one-way staircases and passageways.&lt;/p&gt;
    &lt;p&gt;The Rotterdam metro has 5 different services that can be grouped into two lines. One of these services even arrives to the city of the Hague. The design of the stations is rather simple. For stations located underground, the layout consists of a mezzanine is located at level -1 and two side platforms at level -2. Elevated stations tend to have a island platform at an upper level and a mezzanine at the street level.&lt;/p&gt;
    &lt;p&gt;Blaak and Beurs have layouts were two lines cross, one over the other. The connection is provided via direct staircases linking both platforms.&lt;/p&gt;
    &lt;p&gt;The metropolis of SÃ£o Paulo has a rapid transit network operated by three different companies. This is a relatively new metro (it was opened in 1968, despite some lines were converted from existing railways), with a layout and design thought to hold huge flows of passengers. Trains can be up to 200 m long.&lt;/p&gt;
    &lt;p&gt;Stations are very wide. Some of them, such as SÃ© or Luz (line 1) have the Barcelona solution, in order to ease the passenger flows.&lt;/p&gt;
    &lt;p&gt;Saragossa does not have metro but the commuter line C-1 has an urban section with three underground stations. This section was covered when the high speed rail line arrived to the city.&lt;/p&gt;
    &lt;p&gt;All the stations have a building at street level that hosts vending machines and the fare gates. Platforms are located at level -1, just under street level.&lt;/p&gt;
    &lt;p&gt;Miraflores station has a temporary access to a car parking, since the urbanisation of the street over the rail line has not been finished and the station building cannot be used. This station also has an underpass to reach the secondary platform.&lt;/p&gt;
    &lt;p&gt;Goya station has connection with the tramway, at Fernando el CatÃ³lico station. The connection has to be performed by crossing two zebra crossings.&lt;/p&gt;
    &lt;p&gt;The Valencia metro is actually the merge of different narrow gauge suburban railways that were connected via a tunnel crossing the city center. So this system is somehow the hybrid of a premetro and a rail, since in the city of Valencia is like a metro, but in the suburbs this works like a rail with lots of level crossings and single track sections.&lt;/p&gt;
    &lt;p&gt;The vast majority of underground stations have one or two mezzanines at level -1 and two side platforms at level -2. There are some stations with a different layout compared to the others, especially in lines 3 and 5: BailÃ©n, Avinguda del Cid, Ãngel GuimerÃ and ColÃ³n have island platforms. Alameda has four tracks and three platforms (2 for the Rafelbunyol line and 2 for the MarÃtim line).&lt;/p&gt;
    &lt;p&gt;XÃ tiva station has a radically diferent layout, as the platforms are overlapped because there is the juntion of the line towards BailÃ©n and JesÃºs just 20 m after the platform end (on the eastern side).&lt;/p&gt;
    &lt;p&gt;Line 1 has two types of stations: the ones located in the suburbs, having side platforms at level -1 and ticket halls at street level, and the ones located in the city centre, having a island platform at level -2 and a mezzanine at level -1.&lt;/p&gt;
    &lt;p&gt;The stations on line 2 are similar to the ones with a island platform on line 1, with the difference that the platforms are located a bit deeper.&lt;/p&gt;
    &lt;p&gt;There are two main transfer stations. ÅwiÄtokrzyska is the crossing point of both metro lines and the transfer is quick. Half a kilometer to the south, there is the hub consisting of Centrum metro station, ÅrÃ³dmieÅcie suburban rail station and the Central rail station. The latter two form a huge underground complex, combined with a shopping mall. The connection of Centrum and ÅrÃ³dmieÅcie stations is done at street level.&lt;/p&gt;
    &lt;p&gt;Part of the current metro network (U-Bahn) is inherited from a primitive urban rail network known as Stadtbahn that was steam-powered until the 1920s. Once the network was electrified, the rolling stock used was similar to the ones used for the tramways. Most of its network was elevated or ran at a street level, but without level crossings.&lt;/p&gt;
    &lt;p&gt;During the 60s, the municipal government decided to build a couple of tram tunnels. At the same time, they also planned a metro network consisting of a basic network, with parts of it coming both from the Stadtbahn (U4) and from the newly tram tunnels (part of current U2), asides from the construction of line U1. In the 1980s the second phase took place: line U3 was built and line U6 was integrated from the Stadtbahn.&lt;/p&gt;
    &lt;p&gt;The layout of the stations depends on the time they were built. The non-underground stations that were part of the Stadtbahn have side platforms and the access is made via the station buildings, which are in the Art Nouveau style. The stations that were part of the tram tunnel are located under street level and are quite simple: offset side platforms at level -1. The new stations built from the 1970s on typically have a island platform at the lower level and two mezzanines at the upper level, which can be either on street level or below, and is connected with the platforms via escalators or elevators. The depth of the latter stations is variable, but the ones located in the city center may be very deep.&lt;/p&gt;
    &lt;p&gt;All the transfer stations have been built after 1970 and their layout reflects the idea of having efficient links. Except for Praterstern, all stations have quick transfers.&lt;/p&gt;
    &lt;p&gt;The current Oslo T-bane system originated from an old network of suburban trams that developed in the western suburbs of the city during the first half of the 20th century, and a modern metro system built from the 1960s onwards in the eastern part of the city. Both networks were progressively merged and unified between the 1970s and the 2010s.&lt;/p&gt;
    &lt;p&gt;The Oslo metro runs mostly elevated or on the surface, but there are also shallow underground sections. Surface stations usually have two side platforms. Access to the platforms is open, using an honor system, and access to the platforms is performed directly from street level through ramps. In some places, there is a Narvesen kiosk that serves as both a ticket reseller and a convenience store.&lt;/p&gt;
    &lt;p&gt;The underground stations are located either in the city center or in the northeastern suburbs. Most stations are situated at a shallow depth and have two side platforms at the deepest level. At the immediately higher level, there is a lobby that can be either at street level or underground. The connection between levels is made through ramps and staircases.&lt;/p&gt;
    &lt;p&gt;There are a few stations that are situated at considerable depth: RomsÃ¥s, EllingsrudÃ¥sen, Stortinget, Nydalen, and Vestli. Access to the first two is only possible through large-capacity elevators that connect the street to the concourse, in addition to a ramp of over 200 meters in length that also links the street to the concourse level.&lt;/p&gt;
    &lt;p&gt;The Gothenburg tram has an underground station. Hammarkullen is located at a considerable depth, in a hilly suburb in the northeastern part of the city. The tracks pass through a dual tube tunnel. Trams run on the left only in this specific part, as the station has island platforms and the rolling stock only has doors on the right side. From the platform level, there is a flight of escalators and an inclined elevator that leads to the lobby, located at street level.&lt;/p&gt;
    &lt;p&gt;Hamburg has two rapid transit systems: the U-Bahn and the S-Bahn. The U-Bahn, which is the proper metro system, is one of the oldest in Europe. The first circular line was constructed combining elevated, at-grade and shallow underground sections. Subsequent extensions have also been built alternating these three typologies. However, the central section of U2 and the branch of U4 were constructed at a greater depth.&lt;/p&gt;
    &lt;p&gt;In general, the underground stations are quite shallow and have a island platform (side platforms in the original U3 stations). Some stations have an intermediate level serving as a lobby. On the other hand, the elevated stations have a street-level building (where the lobby is located) and a island platform or two side platforms at level +1.&lt;/p&gt;
    &lt;p&gt;The S-Bahn uses sections of the German railway network. The construction of the City S-Bahn (an underground line through the city center) started in the 1960s. Their stations have large island platforms located at level -2, generally with lobbies at level -1, standing at the ends of the platforms.&lt;/p&gt;
    &lt;p&gt;As for interchanges, some have been designed to facilitate cross-transfer connections (Barmbek for U3-U3 connections, Wandsbek-Gartenstadt, Berliner Tor, Altona, Hauptbahnhof between S-Bahn trains, Norderstedt Mitte), with parallel platforms (Ohlsdorf, Barmbek for U3-S1 connections, Hauptbahnhof SÃ¼d, ElbbrÃ¼cken) or with direct transfers via a single flight of stairs (Schlump, Jungfernstieg).&lt;/p&gt;
    &lt;p&gt;There are other transfers that were poorly designed, such as the long passageway between Rathaus and Jungfernstieg, or the street-level transfer at Wandsbeker Chaussee, Dammtor/Stephansplatz, or Sternschanze.&lt;/p&gt;
    &lt;p&gt;Essen's Stadtbahn has two trunk lines. The one that runs through Rathaus station is a group of tram lines that run underground in the city center, with tram vehicles and low-level platforms. The lines that pass through Hirschlandplatz are light metros.&lt;/p&gt;
    &lt;p&gt;There is a variety of station typologies. The shallower stations usually have side platforms, while the trunk line stations of the light metro have island platforms, which are situated at a considerable depth.&lt;/p&gt;
    &lt;p&gt;Dortmund's Stadtbahn consists of three trunk lines located in the city center. The east-west lines are operated with low-floor trams, while the other two are operated with high-floor light metros. Most stations are located at a shallow depth. The station vestibules are either at street level or at an intermediate level between the street and the platform.&lt;/p&gt;
    &lt;p&gt;In Dortmund, there is also a monorail system (H-Bahn) that serves the Technical University of Dortmund (TU Dortmund). It consists of two lines. The track is single-track, but there are some stations with passing loops. All stations are elevated or at-grade.&lt;/p&gt;
    &lt;p&gt;Bochum's Stadtbahn consists of a north-south line, as well as several tram lines that run through underground sections in the city center. Most Stadtbahn stations have island platforms and vestibules located at an intermediate level between the street and the platforms. In some stations, the platforms are situated at a higher depth, such as Rathaus Nord and Hauptbahnhof.&lt;/p&gt;
    &lt;p&gt;The Bochum Stadtbahn consists of a north-south line, as well as several tram lines that run through underground sections in the city center. Most Stadtbahn stations have island platforms and entrances located at an intermediate level between the street and the platforms. In some stations, the platforms are situated at a higher depth, such as Rathaus Nord and Hauptbahnhof.&lt;/p&gt;
    &lt;p&gt;The Stadtbahn stations in MÃ¼lheim are different. The stations that are close to the Ruhr have platforms at a considerable depth to allow the tunnels to pass underneath the river. The remaining stations have a typical configuration of any Stadtbahn network in Germany, with a vestibule at level -1 and platforms (either side or central) at level -2.&lt;/p&gt;
    &lt;p&gt;The Hauptbahnhof station is an intermodal hub that consist of elevated rail platforms, four underground Stadtbahn tracks with two island platforms and two underground bus platforms parallel to the Stadtbahn ones.&lt;/p&gt;
    &lt;p&gt;All Stadtbahn and underground tram stations in Duisburg have island platforms. These platforms are usually located at level -2, while level -1 is reserved for the lobbies.&lt;/p&gt;
    &lt;p&gt;There are two stations with a particular layout: Hauptbahnhof and KÃ¶nig-Heinrich-Platz, which are the transferring points for the north-south and east-west lines. These stations have two parallel platforms, stacked on different levels (-2 and -3). Different combinations of escalators and staircases allow for transfers, access, and exit from the station.&lt;/p&gt;
    &lt;p&gt;DÃ¼sseldorf has two Stadtbahn main lines (north-south and northeast-southeast), as well as an underground tram main line. Both Stadtbahn lines share tunnels between the central station (Hauptbahnhof) and the city center (Heinrich-Heine-Allee), with tracks running parallel to each other. At Hauptbahnhof and Heinrich-Heine-Allee, the platforms serving the four tracks are located at the same level. However, at the intermediate stations, platforms are overimposed, being the northbound one on the upper level.&lt;/p&gt;
    &lt;p&gt;On the other hand, the underground tram stations have low side platforms at level -2 and a mezzanine at level -1.&lt;/p&gt;
    &lt;p&gt;Almost all metro stations in Turin follow a standard design consisting of two side platforms at level -3 and a lobby at level -1. The connection between the lobby and the platforms is made through escalators that go directly from the platforms to the lobby, elevators, or two flights of stairs.&lt;/p&gt;
    &lt;p&gt;Porta Nuova is the central rail station and has a different layout: the lobby is located at level -1 and platforms are at level -2, which can be reached by stairs or an elevator. There is a direct connection from the metro mezzanine to the railway station original hall, which is now part of a shopping center.&lt;/p&gt;
    &lt;p&gt;Porta Susa station has metro platforms at level -4, an intermediate level at -3, and an open hall located inside the large canopy that constits the railway station building. The railway station itself consists of three levels, with the eastern part of the station featuring a huge glass roof canopy measuring 380 meters long by 30 meters wide. Level -2 also contains the four rail platforms. On level -1 there are four passageways that connect the western accesses to the station and act as an intermediate level to connect crowds coming from the different platforms.&lt;/p&gt;
    &lt;p&gt;The Lausanne metro consists of a light metro line (m1) and an automated rubber-tired metro line (m2), which originated from a funicular that was later converted to rack railway operation.&lt;/p&gt;
    &lt;p&gt;The stations on the m1 are either locatred underground or at street level. Some stations have only one platform with direct access to the street, as the track is single-track. Stations with passing loops usually have two side platforms. The terminal station at Renens CFF is part of the Swiss Federal Railways station.&lt;/p&gt;
    &lt;p&gt;On the other hand, the stations on the m2 line are more varied. All stations have side platforms, except Sallaz. Some have platforms are located at street level, like Ouchy or Grancy. Others are very swallow and can be accessed via a staircase or ramp, such as DÃ©lices or CHUV.&lt;/p&gt;
    &lt;p&gt;Finally, some stations have a unique layout due to the topography of the old town, particularly the deep and narrow Flon Valley. Flon station is a main hub. It is located at Place de l'Europe. The m1 station is at the same level as the square, while the LEB (railway) and m2 stations are at level -2. From level -2, five elevators ascend to the height of the Grand Pont, located about twenty meters above.&lt;/p&gt;
    &lt;p&gt;Similarly, the Riponne station is situated in the Flon River Valley. The station is located at an intermediate level between the street following the valley and the Riponne Bridge. Four elevators (two per platform) connect the three levels.&lt;/p&gt;
    &lt;p&gt;The Porto metro is a newly constructed light metro. It has two trunk lines: one north-south and another east-west, with a section that follows the path of the former narrow-gauge lines that departed from Trindade, which now serves as a transfer station between the two trunk lines.&lt;/p&gt;
    &lt;p&gt;The surface stations have the typical design of a modern tram station, with shelters and pedestrian crossings at the ends of the platforms to switch platforms or exit the station.&lt;/p&gt;
    &lt;p&gt;The underground stations have side platforms at the deepest level, but there are differences in the layout of intermediate levels and the location of the booking hall.&lt;/p&gt;
    &lt;p&gt;The Munich U-Bahn is a metro system formed by 6 different lines, plus two that only operate during rush hours. The network has lots of spurs, but in the city center all those lines merge and form 3 trunk lines, in addition to an east-west trunk line of the S-Bahn, which, due to its frequency and performance, can be considered a metro, just like in Berlin and Hamburg.&lt;/p&gt;
    &lt;p&gt;Most stations have island platforms located at level -2 and a couple of lobbies located at level -1, just above the ends of the platforms. Any station has turnstiles: a honor system is used instead. In some specific stations the platforms may be located at a greater depth, which has required the installation of long escalators, like the ones in the central part of the U4 and U5.&lt;/p&gt;
    &lt;p&gt;One of the characteristics of the Munich metro is the presence of cross-platform transfers at many stations, with coordination on the arrivals and departures. Some examples include Scheidplatz, Innsbrucker Ring or Neuperlach SÃ¼d. Additionally, some other stations with terminating trains usually have three or four tracks, such as Hauptbahnhof (U1-U2), MÃ¼nchner Freiheit, Olympiazentrum, Kolumbusplatz, FrÃ¶ttmaning, or ImplerstraÃe. Some of these stations have elevators that provide direct access between the platforms and the street.&lt;/p&gt;
    &lt;p&gt;The most centric S-Bahn stations use the Barcelona solution. At Hauptbahnhof and Karlsplatz, passengers board using the island platform and alight via the side platforms; while at Marienplatz, due to narrow streets, the two tracks are overlapped: passengers aight from the train using the right hand side in the direction of travel, and board through the opposite doors. Along with the two levels dedicated to the S-Bahn (-2 and -3), there is also the U-Bahn station located at level -4, whose platforms are separated by nearly a hundred meters since the streets above are narrow and the City Hall building is inbetween.&lt;/p&gt;
    &lt;p&gt;In other transfer stations, the line change is direct through a flight of stairs (Odeonsplatz, Hauptbahnhof, Sendlinger Tor) or through the vestibules in the case of most U-Bahn to S-Bahn transfers.&lt;/p&gt;
    &lt;p&gt;The Nuremberg U-Bahn has three lines. Two of them, U2 and U3, share tracks in the central section and have automatic operation.&lt;/p&gt;
    &lt;p&gt;The station layout for most of the stations is quite simple. First of all, the ticketing policy is based in an honor system, so there aren't turnstiles in any station. The platforms are always located at the lowest level of the station, usually at level -1 or -2 (if the station has a mezzanine at level -1). In some cases, there is direct access from the street to the platform. All stations have elevators.&lt;/p&gt;
    &lt;p&gt;The two interchange stations between the U-Bahn lines are designed to provide quick transfers. At Hauptbahnhof, the U1 station is located one level below the U2 and U3 station, and only a short flight of stairs is needed to change. At PlÃ¤rrer station, a cross-platform transfer is ensured in both directions, as the platforms of both lines are stacked.&lt;/p&gt;
    &lt;p&gt;The Stuttgart Stadtbahn is quite complex, but its operation can be simplified into cross-valley lines and the valley lines. Outside the city center, tracks run on street level and most stations ressemble modern tramway stops. In the city center, stations are underground and have side platforms at level -2 and one or two lobbies at level -1.&lt;/p&gt;
    &lt;p&gt;The stations in the central section of the S-Bahn have a island platform at level -2 and lobbies placed at both ends of the platforms, at an intermediate level between the platform and the street.&lt;/p&gt;
    &lt;p&gt;The Lille metro consists of two VAL (light automated vehicle) lines. A VAL system is characterized by very narrow and short vehicles that operate at a high frequency in order to accommodate the demand. The narrow gauge is a direct consequence of minimizing tunnel construction costs.&lt;/p&gt;
    &lt;p&gt;Initially, the metro operated on an honor system, so the stations were designed to allow easy connections between streets and the plaforms. The subsequent installation of turnstiles means that some stations may only have them installed in certain accesses, while others may have a weird distribution turnstiles.&lt;/p&gt;
    &lt;p&gt;The main station of the network is Gare de Flandre, which is the central railway station and also a common station for both metro lines and tram lines, whose platforms are underground. This station provides a cross-platform transfer between the metro lines. Additionally, the transfer between the metro and the tram is also very fast. As a curiosity, the tram station has a platform exclusively for passenger alighting, since it is the last station.&lt;/p&gt;
    &lt;p&gt;The other interchange station between the two metro lines is Porte des Postes, where the platforms of the two lines intersect at different levels, but transfers are quick.&lt;/p&gt;
    &lt;p&gt;The Palma metro is a recent construction. The section closest to the city center was built by taking advantage of the underground section of the Palma-Inca line at the entrance to the city. Between the Intermodal Station at PlaÃ§a d'Espanya and Son Costa - Son Fortesa, the metro tracks run parallel to the aforementioned railway line.&lt;/p&gt;
    &lt;p&gt;The EstaciÃ³ Intermodal is the main transportation hub in Palma. There is an underground bus station located next to the underground railway and metro station. In fact, this hub is the starting point for all the interurban bus lines and rail lines originating from Palma.&lt;/p&gt;
    &lt;p&gt;All the underground section was built very shallow. In fact, the stations located on Gran Via Asima have the same design, with platforms and lobbies located at level -1 and an underground passage at level -2 equipped with escalators and elevators.&lt;/p&gt;
    &lt;p&gt;The Son Sardina station is located on the surface and provides an interchange with the Palma-SÃ³ller line. The UIB station, on the other hand, has a street-level passenger building, although it does not host any facilities.&lt;/p&gt;
    &lt;p&gt;The Brescia metro consists of a single line that opened in 2013. The rolling stock comprises Ansaldobredaâs automatic trains, also in operation on Milan metro lines 4 and 5. The line runs shallowly in the northern section, deeply in the city center, and on surface level and elevated through the eastern suburbs. An honor system is employed, eliminating the need for turnstiles.&lt;/p&gt;
    &lt;p&gt;Some of the deepest stations, such as Marconi, Ospedale, Stazione FS, Bresciadue, or Volta, follow a standard design with two side platforms at level -3. These platforms are connected to the mezzanine at level -2 via a staircase and two independent escalators. From the mezzanine, there is access to an intermediate level (-1) and a further flight of stairs leading to the street.&lt;/p&gt;
    &lt;p&gt;Shallow stations typically consist of two side platforms directly accessible from the street, while elevated stations feature an island platform.&lt;/p&gt;
    &lt;p&gt;Despite having 10 lines in operation, the Istanbul metro is quite modern. Except for lines 1 and 2, built in the 80s and 90s, the rest of the network was constructed in the 21st century. Extensions are underway in both the European and Asian parts of the city. The tram network complements the metro, which incorporates turnstiles at each stop. Additionally, the public transportation network includes four funiculars, three of which are entirely underground.&lt;/p&gt;
    &lt;p&gt;The metro lines exhibit significant differences in technical features, including train type, automation, track count, and electrification system. Construction methods for tunnels and stations vary from line to line, affecting station layouts. With the exception of line 1 which runs shallow or elevated, all lines run at considerable depths. Access and egress times are relatively high and some transfer layouts may result in long passageways or unnecessary multiple level changes. Moreover, transfers are not free, as passage through turnstiles is required.&lt;/p&gt;
    &lt;p&gt;All lines were constructed with dual-tube tunnels at depths ranging from 20 to 45 meters below ground level, except for line 1, which is shallower, and line 6, which was built with a single track and with passing loops at each station. Some stations are located as deep as 70 meters, only accessible via elevators in certain cases.&lt;/p&gt;
    &lt;p&gt;The Malaga metro comprises two lines operated with a tramway fleet. Line 2 is entirely underground. Line 1 is mostly underground, but the western section of line 1 was built as a light metro with level crossings. The underground sections are shallow, with most stations featuring a lobby at level -1 and an island platform at level -2.&lt;/p&gt;
    &lt;p&gt;The common section is peculiar. Each line has its own tracks. The tunnel has two levels, with two tracks on each one. Between El Perchel and Guadalmedina stations, two tracks interchange their levels, in a section located within two curves. La UniÃ³n station on line 1 has also overlapped platforms. Atarazanas station only has a single platform.&lt;/p&gt;
    &lt;p&gt;The Seville metro consists of a single line that began construction in the 70s but was not opened until 2009. The rolling stock comprises tramway vehicles.&lt;/p&gt;
    &lt;p&gt;The central section, built in the 70s, features a dual-tube tunnel constructed using TBMs. Stations generally have a lobby at level -1 or -2, followed by an intermediate level and an island platform at the lowest level. Platform screen doors protect the tracks.&lt;/p&gt;
    &lt;p&gt;The western section, crossing the Guadalquivir river, includes elevated, surface, and underground sections. Ciudad Expo, the only station with both an island platform and a lobby at street level, stands out. Other stations have two side platforms, with the lobby located at either the upper or lower level, depending on the station's location.&lt;/p&gt;
    &lt;p&gt;The line runs shallowly between NerviÃ³n and Cocheras, with all stations featuring side platforms and a single lobby.&lt;/p&gt;
    &lt;p&gt;The Palma metro comprises a single line connecting the city center and the university campus in the north. The majority of the line is underground, with a surface-level section around Son Sardina. The segment between EstaciÃ³ Intermodal and Son Costa â Son Fortesa features quadruple tracks for metro and mainline traffic.&lt;/p&gt;
    &lt;p&gt;EstaciÃ³ Intermodal serves as a transportation hub with 10 platforms, a bus station at level -2, and a mezzanine at level -1. Jacint Verdaguer station has two island platforms at level -2 and a mezzanine at level -1. Son Costa â Son Fortesa station features two unconnected island platforms at level -1. The UIB station has two side platforms at level -1 and a ground-level hall. Son Sardina station is at ground level, with two side platforms connected by an underpass and linking to the SÃ³ller Railway station. The remaining stations share a similar design, with side platforms at level -1, each directly accessible from the street, and an underpass connecting both platforms.&lt;/p&gt;
    &lt;p&gt;The Naples rail network comprises multiple lines operated by different agencies. The definition of what is part of the metro network varies.&lt;/p&gt;
    &lt;p&gt;Line 1, opened in 1993, connects the old city by the sea with the upper districts. The line forms a loop to climb the hill. Most stations are at great depths, except the section between Colli Aminei and Piscninola. Connections to platforms vary by station, but the booking halls are commonly located at level -1. Some feature a single flight of parallel escalators (Montedonzelli, Policlinico), high-capacity lifts (Duomo), or two or three flights of stairs. Stations between Colli Amiei and Museo have island platforms, while those in the old city have side platforms. Quattro Giornate station has overlapped platforms.&lt;/p&gt;
    &lt;p&gt;Line 2, owned by the Italian State, opened in 1925 with third rail electrification but was later converted to overhead lines. Montesanto and Cavour are the deepest stations, with a ground-level building comprising ticket office and turnstiles. Two flights of stairs and escalators connect to an intermediate level (-2), with two side platforms at level -3. Other stations, like Mergellina or Campi Flegei, are at ground level with grand station buildings. Piazza Garibaldi has two tracks and three platforms, with a recent layout change.&lt;/p&gt;
    &lt;p&gt;Line 6, closed since 2013, featured a narrow profile with trains only 2.20 meters wide.&lt;/p&gt;
    &lt;p&gt;The Arcobaleno line (Line 11) connects Piscinola with Aversa, built with cut-and-cover methods. The layouts resemble Milan metro lines 1 and 2 and Rome's southern section of line A. The Circumvesuviana and Circumflegrea lines are also part of Naples' rail network, with some underground stations.&lt;/p&gt;
    &lt;p&gt;Rennes has two metro lines.&lt;/p&gt;
    &lt;p&gt;Line A is a VAL, an automatic light metro using technology developed by Siemens. The trains have a width of 2.08 m. Most of the line runs underground, with two elevated stations (La Poterie and Pontchaillou). The layout of the stations varies considerably, but all stations have side platforms on the lowest level. The lobbies are either located at street level or at level -1. Anatole France and Jacques Cartier are the deepest stations on this line. Initially, the stations had an honor system, but turnstiles were installed in 2020. Due to this reason, some lobbies might have a peculiar placement of the turnstiles.&lt;/p&gt;
    &lt;p&gt;Line B is a Neoval, an automatic line that uses more modern technology developed by Siemens. Trains operate in 2-car compositions, but stations allow for 3-car trains. The entire line runs underground except for the easternmost section, which is elevated. Platforms are located on the lowest level, normally at level -2 or -3. Mezzanines are located at level -1. Sainte-Anne is an interchange station located at the city center. Transfers, access, and egress are performed through a complex system of one-way corridors, stairs, and escalators.&lt;/p&gt;
    &lt;p&gt;Topo is a narrow-gauge rail line operated by Euskotren Trena, an agency owned by the Basque Government. This line runs through many tunnels, earning it the nickname "Topo," which means âmoleâ in Spanish. The line underwent a major renovation in the last 10 years, and a variant is being built both in the city center of San Sebastian and in the neighboring town of Pasaia. As a consequence, the current main station of Amara will be closed and dismantled.&lt;/p&gt;
    &lt;p&gt;The layout of the stations differs. Lugaritz, Anoeta, and Herrera have a station building at ground level and two side platforms at level -1. Loiola and Pasaia stations are elevated. Intxaurrondo and Altza stations are located deep underground and have layouts and aesthetics similar to those of the Bilbao metro, once designed by Norman Foster.&lt;/p&gt;
    &lt;p&gt;The Sofia metro consists of 4 different services that operate on 2 physical lines. Despite having rolling stock manufactured by a Russian company, the aesthetics and layouts differ from the Soviet-influenced metros of eastern European countries. Generally speaking, the average station has lobbies located at levels -1 and the platforms at level -2. Most stations have side platforms, but the oldest ones, located in the central part of lines 1 and 4, have side platforms. Stations in the common section of lines 1 and 4 have low-cost platform screen doors.&lt;/p&gt;
    &lt;p&gt;All stations on line 3 have side platforms, normally located at level -2. Mezzanines are commonly located at level -1. The connection between platforms and the mezzanine is performed by multiple staircases or elevators. Most of the accesses to the stations are equipped with elevators.&lt;/p&gt;
    &lt;p&gt;The Toulouse metro is formed by two lines that form a cross-shaped network. Both lines use VAL technology and are automated. The layout of line A stations varies. In the city center, the line runs through a dual-tube tunnel at a great depth to cross the Garonne river. Long escalators reach the platforms at Capitole and Esquirol. Stations located outside the city center tend to have a lobby at level -1 and two side platforms at level -2 or -3. The western terminus at Basso Cambo is elevated.&lt;/p&gt;
    &lt;p&gt;Most of the line B stations have a similar layout consisting of a lobby at level -1, an intermediate level used for entering passengers at level -2, and two side platforms at level -3. Passengers egressing the station can use escalators that connect directly the platforms with the lobbies. Jean JaurÃ¨s is the main hub on the network. The station was designed to have one-way passages and escalators. The line B station uses the Barcelona solution, boarding through the island platform. Passengers need to pass through turnstiles to change from one line to the other. These turnstiles are used only for statistical purposes and to manage flows, as the tariff system guarantees free transfers.&lt;/p&gt;
    &lt;p&gt;In 1962, the citizens of Zurich rejected in a referendum the conversion of tram lines in the city center into a premetro. In 1973, the citizens rejected again the construction of a metro network. However, a short section of the tunnels was already being built to conduct rolling stock tests. In 1978, the citizens approved integrating the existing tunnel into the tram network, which was completed in 1986. These stations were built at a considerable depth, as they are located in a hill between the valleys of the Limmat and the Glatt rivers. All stations have island platforms. Since trams in Zurich only have doors on the right side, they run on the left side of the tunnel. All the stations have multiple accesses, some reachable only via elevators, others via a long flight of stairs and escalators, which in some cases have the lower side in a level located even below the platform level.â&lt;/p&gt;
    &lt;p&gt;Merseyrail is the suburban train network managed by the Liverpool City Region, and it is operated by a private company through a concession. The network consists of two directly operated lines.&lt;/p&gt;
    &lt;p&gt;The Wirral Line connects central Liverpool with the Wirral peninsula, located south of the River Mersey, via a tunnel opened in 1886. The two stations adjacent to the river (James Street and Hamilton Square) have platforms located at a great depth. The connection between the street-level concourse and the pre-platform level is made through three high-capacity elevators, in addition to emergency stairs, which at Hamilton Square can also be used regularly.&lt;/p&gt;
    &lt;p&gt;The Northern Line is a north-south line created in 1977, after connecting by a tunnel two separate lines that departed from Exchange and Liverpool Central terminals. In the 1970s, the Wirral Line terminal, which ended in a double track at Liverpool Central, was also modified to become a one-way loop served by three stations spread throughout central Liverpool. These stations are very deep. The connection between platforms, intermediate levels and street-level concourses is made through two or three sets of escalators. All underground stations are accessible to people with reduced mobility.&lt;/p&gt;
    &lt;p&gt;Genoa has a metro line that runs through the city, parallel to the coast. Due to the city's difficult topography, the construction methods for each section are different, resulting in a curvy route with very heterogeneous stations.&lt;/p&gt;
    &lt;p&gt;Both terminals, Brin and Brignole, are located on viaducts. Additionally, Brignole metro station is integrated parallel to the railway station.&lt;/p&gt;
    &lt;p&gt;Dinegro is the only station built using the cut-and-cover method and is the most shallow in the network. Both platforms are connected via an underpass. This station has mÃºltiple tracks, three of them with platforms, in addition to other tracks that are used as workshops and depots.&lt;/p&gt;
    &lt;p&gt;Darsena and San Giorgio stations have an island platform and are deeper than Dinegro but shallower than Sarzano-SantâAgostino and De Ferrari stations, which have two side platforms each located in two different tunnels.&lt;/p&gt;
    &lt;p&gt;The most complex station in the network is Principe: it consists of the metro station, an underground station national rail network, and the surface railway station. The surface station is nestled in a trench between mountains and has a yard with 5 platforms and 9 tracks. Corridors connect the underpass of the surface-level station with a concourse that either leads to the underground railway station, of which only one of the two tracks is in service, or to the metro station through a series of underground walkways. The metro station has a semi-underground concourse, which connects to the mezzanine, located at the upper level of the platform, via a staircase.&lt;/p&gt;
    &lt;p&gt;Charleroi's premetro consists of a circular line that encircles the city center and three branches towards the suburbs and nearby towns. The network is operated as four radial services, which start from the central loop.&lt;/p&gt;
    &lt;p&gt;The central ring is a mix of tram with priority (between Tirou and Gare Centrale) and a fully segregated line, mostly underground but with an elevated section between Gare Centrale and Palais. Line 4 operates completely segregated outside the city ring, with trams running on the left side. Lines 1 and 2 run also segregated from the rest of traffic between the city center and PÃ©tria, and then as a tram line between PÃ©tria and Monument, just like line 3 once past Piges.&lt;/p&gt;
    &lt;p&gt;The station layouts are rather simple. Regardless of whether they are on a viaduct or in a tunnel, almost all have a central platform, which is accessed directly from the street, sometimes via an intermediate level, without turnstiles.&lt;/p&gt;
    &lt;p&gt;Palais station has four tracks: two of them are in service, a third without regular service, and the fourth leads to a loop that reverses the direction of trains. Waterloo station has two platforms and three tracks, but only the central platform is in operation.&lt;/p&gt;
    &lt;p&gt;The Montreal metro consists of a network of 4 lines with rubber-tired trains. Nearly all of the network was built during the 1960s, 70s, and 80s, and is entirely underground, with tunnels often (but not always) following the alignment of the streets.&lt;/p&gt;
    &lt;p&gt;Each station has unique architecture and style. Station depth also varies. However, most stations feature two side platforms, each 500 feet long (152.4 m), and all entrances are equipped with enclosures to protect against winter temperatures. In some cases, these enclosures also host station lobbies.&lt;/p&gt;
    &lt;p&gt;Transfer stations were designed to offer practical and fast connections. Snowdon and Lionel-Groulx stations feature overlapped platforms, allowing for cross-platform transfers, while at Berri-UQAM, the platforms for the green and orange lines intersect at adjacent levels. At Jean-Talon, transferring between the orange and blue lines is also quick, with the platforms arranged in a stacked layout.&lt;/p&gt;
    &lt;p&gt;One unique feature of Montreal is that, in the city center, metro stations connect directly to the underground city known as RÃSO, which links city blocks underground.&lt;/p&gt;
    &lt;p&gt;Ottawaâs rail transit system (O-Train) includes an east-west light rail line called the Confederation Line and a north-south rail line called the Trillium Line.&lt;/p&gt;
    &lt;p&gt;The entire network is above ground, except for the central part of the Confederation Line, which runs at a depth of 17 to 25 meters below street level. The three underground stations have side platforms located on the lowest level. Each station has two independent entrances connecting the mezzanines located at either end of the platforms, which are accessible via multiple stair flights.&lt;/p&gt;
    &lt;p&gt;Outside of the city center, stations may be located at street level, elevated, or in a trench, with most featuring side platforms. Some stations are transfer points between light rail and bus services, such as Tunneyâs Pasture, Hurdman, and Blair. Ottawa also has several BRT lines, some of which have fully segregated infrastructure, including stops shaped like rail stations, with escalators, elevators, and lobbies with fare validators.&lt;/p&gt;
    &lt;p&gt;The Toronto subway has three lines. The network is relatively shallow, with some sections running above ground.&lt;/p&gt;
    &lt;p&gt;In the oldest sections, the tunnel alignment is parallel to the street one but is shifted several meters toward the blocks of houses. Due to the cut-and-cover construction method used in these segments, it was necessary to expropriate and demolish existing buildings. Some of these clearings were used to build transfer hubs containing bus and streetcar bays, together with the passenger building, which also serves as the subway entrance. All bus and streetcar platforms are located within the paid area of the station, providing free transfers between the subway and surface transit.&lt;/p&gt;
    &lt;p&gt;Regarding subway line transfers, Sheppard-Yonge, Bloor-Yonge, and St. George stations provide direct transfers via a single flight of stairs, while at Spadina, the transfer involves a walkway over 200 meters long.&lt;/p&gt;
    &lt;p&gt;Calgary has a light rail system called the C-Train. In the downtown area, it operates like a streetcar, with a low stop spacing, but in the suburbs, the spacing between stations is greater, and the trains have dedicated platforms, although some crossings still use level crossings.&lt;/p&gt;
    &lt;p&gt;As for stations, there is only one underground station (Westbrook). Some other stations have platform access at ground level, while others use overpasses that lead to platforms through lobbies with waiting areas.&lt;/p&gt;
    &lt;p&gt;Vancouverâs SkyTrain consists of three automated metro lines. The network has a radial layout, and only two of the three lines reach downtown. As the name suggests, most of the metro is elevated, with only the most central segments being underground.&lt;/p&gt;
    &lt;p&gt;The Expo Line runs through downtown in a two-level tunnel, with stations located at significant depths. The Canada Line is much shallower, with each station featuring a different layout: Waterfront, Vancouver City Centre, Yaletown-Roundhouse, and Olympic Village have a lobby on level -1 and an island platform on level -2; King Edward has overlapped platforms; while Oakridge and Langara have platforms on level -1 connected by an underpass.&lt;/p&gt;
    &lt;p&gt;The main interchange of the network, Waterfront, serves as the terminal for the Expo and Canada Lines, as well as the commuter train and the ferry to North Vancouver. The transfer here is not ideal, as the Canada Line is in an area with separate fare gates, and the distance between this line and the ferry terminal is 350 meters.&lt;/p&gt;
    &lt;p&gt;Seattle has a light rail line. Initially, a tunnel was built for buses and trolleybuses in the downtown area, running from International District / Chinatown to Westlake, which opened in 1990. Stations in this section had side platforms on level -2 and two lobbies on level -1, with platforms spaced widely enough to allow one bus to overtake another.&lt;/p&gt;
    &lt;p&gt;The tunnel closed in 2005 for conversion to light rail, which opened in 2009. Until 2019, it operated as a mixed-use tunnel for both buses and light rail. In 2016, the network expanded northward, adding a segment with four underground stations and one elevated station. Platforms at the underground stations are more than 20 meters deep, and each one has a unique layout.&lt;/p&gt;
    &lt;p&gt;On the southern section, there is Beacon Hill Station, accessible only via four high-capacity elevators that descend to the platforms, located about 50 meters below ground.&lt;/p&gt;
    &lt;p&gt;Stockholm has three subway lines that cross the city center and split into two or three spurs at each end. The city itself is in an archipelago, so the metro lines were planned and built through a complex geography and geology, featuring surface stretches, elevated viaducts, shallow underground sections, and deep tunnels.&lt;/p&gt;
    &lt;p&gt;The green line, the first to be built, is considered to be the shallowest. In the city center, it runs in shallow tunnels built using the cut-and-cover method, and in the suburbs, the line runs either elevated or at surface level. The red line combines deep underground segments in the north with a mix of surface and deep sections in the south. The blue line runs at great depth along nearly its entire length.&lt;/p&gt;
    &lt;p&gt;All stations are accessible to persons with reduced mobility thanks to the installation of lifts. Deep stations typically feature two island platforms at the lowest level (-2), connecting to lobbies located at or near the surface (level -1) via a set of 3 or 4 parallel escalators, in addition to an inclined lift which also runs parallel. Stockholm has one of the highest numbers of inclined lifts in its subway system. Some deep stations have cavern-like platform designs.&lt;/p&gt;
    &lt;p&gt;Leipzig has a tunnel that crosses the city centre, with four underground stations, all featuring island platforms at the deepest level. Although the tunnels are twin-tube and were built using tunnel boring machines, the stations were constructed using the cut-and-cover method, except for Hauptbahnhof, which was built using the mining method.&lt;/p&gt;
    &lt;p&gt;Karlsruhe has a tram-train system. In 2021, a T-shaped underground line was opened, which is served by six stations, one of which is a double station. All stations have side platforms located on level -2, except for Kongresszentrum, which has its platforms on level -1. The -1 level of the other stations consists of various concourses.&lt;/p&gt;
    &lt;p&gt;The platforms have sections with different heights to ensure compatibility with all rolling stock in terms of accessibility.&lt;/p&gt;
    &lt;p&gt;The Bielefeld Stadtbahn consists of a north-south trunk line that branches into four lines, both in the north and in the south. The central section and a bunch of the northern spurs are underground. Most stations have island platforms on the deepest level, with accesses at both ends of each platform.&lt;/p&gt;
    &lt;p&gt;Hauptbahnhof is the junction station on the northern side. The station has a concourse at level -1, two platforms and three tracks at level -2; plus one track and one platform at level -3, which crosses the station diagonally.&lt;/p&gt;
    &lt;p&gt;Gelsenkirchen has a Stadtbahn line operated with low-floor trains. Except for Heinrich-KÃ¶nig-StraÃe, which is located at a junction, all stations have a platform at the lowest level. In some stations, access to the platforms is directly from the street; in others, access points converge in the concourse.&lt;/p&gt;
    &lt;p&gt;The Wuppertal Schwebebahn is a suspended monorail opened in 1901, with a single line that follows the course of the Wupper River, except at the western end, where it follows a street.&lt;/p&gt;
    &lt;p&gt;The stations are elevated and located above the river or the street. All stations have side platforms, so trains have doors on only one side. The platforms are around 30 meters long.&lt;/p&gt;
    &lt;p&gt;Cologne has a Stadtbahn made up of different services that cross the city. Most of the underground sections are located in the center and in the north. Some of these lines are operated with low-floor trams, while others use high-floor trains. This results in some stations having platforms with two different heights.&lt;/p&gt;
    &lt;p&gt;The stations typically have a simple structure with platforms at level -2 and one or two concourses at level -1, although some stations have platforms at level -1. The new north-south line, partially constructed and served by lines 5 and 17, runs deeper than the other lines and has central platforms at almost all stations. However, currently only one of the two tracks is operational. Finally, line 13, which runs along the GÃ¼rtel (ring road), includes an elevated section.&lt;/p&gt;
    &lt;p&gt;As for transfer stations, some are designed for quick and short transfers, such as Friesenplatz or Ebertplatz. The central station is served by two different, adjacent Stadtbahn stations.&lt;/p&gt;
    &lt;p&gt;Bonn has a Stadtbahn consisting of a line that runs parallel to both the Cologne â Mainz railway line and the Rhine River. It has two underground sections: one in the center and south of Bonn, and another in the Bad Godesberg district, plus a couple of isolated underground stations on a branch line.&lt;/p&gt;
    &lt;p&gt;The stations are simple. All have side platforms at level -2 and one or two concourses at level -1.&lt;/p&gt;
    &lt;p&gt;Entering, exiting, or transferring at a metro station is part of the daily routine for millions of people worldwide. The layout of these stations directly impacts people's mobility. If time could be converted into money, we would realize that a minute spent transferring is more expensive than a minute spent inside a moving train or bus. This temporal cost of transferring includes waiting time as well as the time spent moving from one line to another. Additionally, the physical effort that it may require should also be considered.&lt;/p&gt;
    &lt;p&gt;Some cities have taken intermodality as a serious issue, while others have not given importance. There are several key aspects to ensure quality to the transfers: distance, the lack of architectural barriers, timetable coordination and a good wayfinding system, among others.&lt;/p&gt;
    &lt;p&gt;For the last 10 years I have been able to draw around 2,547 stations from different European cities, motivated by the curiosity of understanding how engineers were able to fit underground stations comprising 4 or 5 lines under Place de la RÃ©publique in Paris or the Puerta del Sol in Madrid.&lt;/p&gt;
    &lt;p&gt;A pen, a notebook, a bit of spatial vision and the willingness to navigate all the staircases, corridors, platforms and mezzanines are enough to draw a station. Some may content errors, despite I try to complement themwith information found in the internet: historic, construction and survey maps, pics and videos, as well with data about train lengths.&lt;/p&gt;
    &lt;p&gt;Due to the boredom provoked by the COVID-19 lockdown in 2020, I decided to digitalize all the sketches I had drawn in since the early 2010s, plus all the station plans I collected from construction projects. I have also drawn stations from cities I have never been. Sources may vary, but some of them come from construction projects, other sketches found in Wikimedia Commons (Boston).&lt;/p&gt;
    &lt;p&gt;Short and fast transfers are a common practice for networks that had a solid planning behind it.&lt;/p&gt;
    &lt;p&gt;As the name suggests, the transform is performed by crossing a island platform, from one track to the opposite one. This layout allows passengers to change doing a physical effort close to zero. In some cities, timetables are planned to coordinate the arrivals and departures at stations having a cross-platform interchange, removing the waiting times and reducing the transfer time to a few seconds.&lt;/p&gt;
    &lt;p&gt;Some examples are: Gare du Midi (Brussels), Mehringdamm (Berlin), PrÃncipe PÃo (Madrid, lines 6-10) and LÃ¤ngenfeldgasse (Vienna).&lt;/p&gt;
    &lt;p&gt;This station consists of two lines crossing perpendicularly at different levels. One station is over the other, allowing passengers to change rapidly by climbing (or descending) a short staircase.&lt;/p&gt;
    &lt;p&gt;Several transfer stations in Berlin have this layout: Berliner Str., Bismarckstr., Gleisdreieck, Hermannplatz, Hermannstr., Leopoldplatz, Osloer Str., Ostktreuz, SÃ¼dkreuz, SchÃ¶neberg and Wesktreuz.&lt;/p&gt;
    &lt;p&gt;Other examples outside Berlin are: Gorg (Barcelona), Downtown Crossing (Boston), Arts-Loi (Brussels), Hauptwache (Frankfurt), Colombia (Madrid), Miromesnil (Paris), Beurs (Rotterdam) and Stephansplatz (Vienna).&lt;/p&gt;
    &lt;p&gt;The platforms of two lines are built parallel, but cross-transfer is not warranted. Transfer is made by passing through an overpass or an underpass.&lt;/p&gt;
    &lt;p&gt;Some examples include: ParalÂ·lel (Barcelona), Brandenburger Tor (Berlin), Baixa-Chiado and Campo Grande (Lisboa), Jussieu and Villiers (ParÃs).&lt;/p&gt;
    &lt;p&gt;This layout is found in systems where the network was not planned or was poorly planned. In Barcelona this kind of transfers were promoted in order to increase accessibility to public transportation in some areas.&lt;/p&gt;
    &lt;p&gt;Due to the inconvenience of walking through the never-ending corridors, some transfer stations are part of the general culture of the mentioned cities, such as Passeig de GrÃ cia (Barcelona, 270 m appr.), Diego de LeÃ³n (Madrid, 250 m appr.), Stadtmitte (Berlin, 150 m appr.) o ChÃ¢telet (Paris, 170 m appr.).&lt;/p&gt;
    &lt;p&gt;Some cities have very deep lines while in others tunnels are close to the surface. This depends of the soil, the construction method and the presence of obstacles, such as rivers, rocks, caves or underground water. Some cities may have both kinds of lines. In Western Europe, people tend to think about London when thinking about deep stations, but this is a common practice in many ex-communist European cities (Moscow, Prague, Budapest, etc), as well as in cities located in archipelagos such as Stockholm or Helsinki. These stations have a huge gallery containing escalators, connecting the ticket hall with the platform level.&lt;/p&gt;
    &lt;p&gt;In Madrid, almost all the stations constructed during the 70s and 80s were built at a considerable depth and consist of some series of escalators. In Barcelona, some stations in lines 9 and 10 were built so deep that platforms can only be reached by elevator.&lt;/p&gt;
    &lt;p&gt;Examples of stations including platforms at very different levels include: Collblanc and Zona UniversitÃ ria (Barcelona), Plaza de EspaÃ±a (Madrid), DeÃ¡k Ferenc TÃ©r (Budapest), Madeleine (Paris), Schottenring (Viena).&lt;/p&gt;
    &lt;p&gt;This is one of the three most important rail stations in Milan, together with Centrale and Cadorna.&lt;/p&gt;
    &lt;p&gt;The rail station comprises two parts: a surface terminus opened in 1961 ânot depictedâ with 20 platforms, and two underground platforms that are part of the Passante Ferroviario line, opened in 1997. Next to the underground section of the railway station, there are the two metro stations.&lt;/p&gt;
    &lt;p&gt;This gigantic hub is located in the geographic centre of Paris. It actually consists of two metro stations (les Halles and ChÃ¢telet) plus the RER station (ChÃ¢telet â Les Halles), located between the mentioned metro stations.&lt;/p&gt;
    &lt;p&gt;ChÃ¢telet metro station is located in the surroundings of place du ChÃ¢telet. The station is served by 5 lines and their platforms are placed apart. Therefore, in the mid-2010 the station was divided in sectors for practical reasons. The Seine sector is located next to the river and it is the stopping point for lines 7 and 11.&lt;/p&gt;
    &lt;p&gt;A long corridor with moving walkways connects the Seine sector with the Rivoli sector, located next to the street with the same name. This sector hosts the platforms of lines 1, 4 and 14, which are interconnected through a maze of one-direction corridors, escalators and staircases. Beneath the line 1 platforms, there is a corridor with moving walkways connecting with the RER station. There is a supplementary corridor connecting the RER station with lines 4 and 14.&lt;/p&gt;
    &lt;p&gt;The Forum sector is comprised by the RER station, Les Halles metro station and a large shopping mall called Forum des Halles. This site was constructed after demolishing the former central market in the 1970s and it was fully renovated in the 2010s. The main access from the street to the train station is found inside the shopping centre.&lt;/p&gt;
    &lt;p&gt;The underground link between Atocha and ChamartÃn was built under the Paseo de la Castellana. A station was constructed near some government offices and it was opened in 1967.&lt;/p&gt;
    &lt;p&gt;The metro line 6 station was opened in 1979, located about 100 m to the west of the rail station. Line 10 station was opened three years later, as part of the old line 8 (Fuencarral â Av. AmÃ©rica), and it was located in parallel to the rail station, but on the eastern side.&lt;/p&gt;
    &lt;p&gt;This station became important with the extension of line 8 âthe line serving the airport. The whole station is refurbished and a new mezzanine is created, even with check-in counters as an extension of the airport services. These counters were closed few years later.&lt;/p&gt;
    &lt;p&gt;In 2008 a second tunnel connecting Atocha and ChamartÃn was opened. However, this tunnel was located at a lower level. This station is ranked the 3rd in number of passengers.&lt;/p&gt;
    &lt;p&gt;Nation is located in the east of the city. It is served by 4 metro lines and a RER line.&lt;/p&gt;
    &lt;p&gt;All the complex is located the square. The mezzanines serving the metro station are located in level -1. The platforms of metro lines 1, 2 and 6 are located at level -2. Level -3 contains the transfer corridors connecting the metro lines, as well the booking hall for the RER. Line 9 platforms are located at level -4, while level -5 hosts the exit hall from passengers coming from the RER. The RER platforms are located at the deepest level of the hub.&lt;/p&gt;
    &lt;p&gt;One common feature about the Parisian metro stations is that some corridors are actually one-way. In Nation, this is taken to the limit. Almost all the corridors and staricases are one way, even to the point that the entrances to the RER station are different to the exits.&lt;/p&gt;
    &lt;p&gt;The complex comprises an elevated railway station with 6 platforms, 2 of which are specific for the Stadtbahn line (est-west) of the S-Bahn. On the eastern side, there is the U6 U-Bahn station, located underground; whereas in the west side, there are the platforms of the NordsÃ¼d S-Bahn line. Despite having been built next to the river, both underground stations are not located very deep. Above the U6 platforms, at the street level, there are some bus stops, as well as a tram stop.&lt;/p&gt;
    &lt;p&gt;This station has historic significance during the Cold War, since it served as a multi-modal station for both western and eastern transportation networks, despite being physically located in East Berlin;and as a border pass as well.&lt;/p&gt;
    &lt;p&gt;The East Berlin station only comprised the two elevated platforms dedicated nowadays to the S-Bahn, plus a bunch of bus and tram stops. The rest of the elevated platforms were part of the West Berlin network, together with the U-Bahn and NordsÃ¼d S-Bahn stations. Internal transfer between the western lines was allowed. Exiting to the street was only allowed by passing through the border.&lt;/p&gt;
    &lt;p&gt;This was the only station of the western network to be opened in East Berlin. The rest of stations in U6, U8 and the NordsÃ¼dbahn were closed between 1961 and 1989 and received the popular name of ghost stations.&lt;/p&gt;
    &lt;p&gt;Saint-Lazare railway terminus is the oldest station in the city. It is part of a much larger complex of connected stations, consisting of Saint-Augustin, Saint-Lazare, Haussmann-Saint-Lazare, Havre-Caumartin, Auber and OpÃ©ra.&lt;/p&gt;
    &lt;p&gt;The railway station has 27 platforms and it is served by the suburban Transilien lines J and L, as well TER trains to other regions.&lt;/p&gt;
    &lt;p&gt;The metro station is served by 4 lines. Beneath Cour du Havre there is a circular concourse that acted as the booking hall for the former Nord-Sud Metro (lines 12 and 13). Lines 3 and 14 are located beneath Cour de Rome. Together with the extension of line 14 from Madeleine to Saint-Lazare, a large concourse was built, removing most of the existing one-way corridors under Cour de Rome. One-way corridors are still present on the paths heading towards lines 12 and 13.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://stations.albertguillaumes.cat/"/></entry></feed>