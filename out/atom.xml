<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-21T09:38:53.411696+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45963350</id><title>Okta's NextJS-0auth troubles</title><updated>2025-11-21T09:39:01.311522+00:00</updated><content>&lt;doc fingerprint="ae60955bb9606999"&gt;
  &lt;main&gt;
    &lt;p&gt;In October, I reported two security issues to Okta’s auth0/nextjs-auth0 project, here and here. The latter bug, an oauth parameter injection, allows for a range of types of abuse, like scoping tokens for unintended services, setting &lt;code&gt;redirect_uri&lt;/code&gt; and &lt;code&gt;scope&lt;/code&gt; to arbitrary values to leak tokens, and so on.&lt;/p&gt;
    &lt;p&gt;The patch was simple enough, so I opened a PR:&lt;/p&gt;
    &lt;code&gt;diff --git a/src/server/helpers/with-page-auth-required.ts b/src/server/helpers/with-page-auth-required.ts
index 41af2dfe..f07046b8 100644
--- a/src/server/helpers/with-page-auth-required.ts
+++ b/src/server/helpers/with-page-auth-required.ts
@@ -196,7 +196,7 @@ export const appRouteHandlerFactory =
           : opts.returnTo;
       const { redirect } = await import("next/navigation.js");
       redirect(
-        `${config.loginUrl}${opts.returnTo ? `?returnTo=${returnTo}` : ""}`
+        `${config.loginUrl}${opts.returnTo ? `?returnTo=${encodeURIComponent(returnTo)}` : ""}`
       );
     }
     return handler(params);
&lt;/code&gt;
    &lt;p&gt;All’s well that ends well, right? Obviously, no.&lt;/p&gt;
    &lt;p&gt;The PR, 3 weeks later, was closed by the maintainer, an auth0 (an Okta company) employee, with the following comment:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This change is superseded by #2413. This was done to ensure that commits are signed. Orignal contribution history has been preserved. Hence closing this PR now.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Hmm, let’s take a look at that PR:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;auth0/nextjs-auth0 #2413&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Hmm. That patch looks familiar. And who is Simen Olsen?&lt;/p&gt;
    &lt;p&gt;Pushing back on the attribution error, I replied:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;history has been preserved&lt;/p&gt;
      &lt;p&gt;no it hasn’t. I don’t know who “Simen A. W. Olsen my@simen.io” is but it isn’t me and my commit here doesn’t reference that name or email address at all. Was it ai generated or something?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Of course, the answer was: yes. It was AI slop. Just like my previous post about gixy-ng (a fun read for anybody dealing with nginx), the developer had used CoPilot to somebow generate their patches:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Hi @MegaManSec I sincerely apologize for this attribution error.&lt;/p&gt;
      &lt;p&gt;Can confirm that an AI workflow was used to created the rebased commit, which got confused with OP details. I’ve added a correction to #2413, and will ensure the changelog is updated.&lt;/p&gt;
      &lt;p&gt;Thank you for calling this out, we’ll make sure this doesn’t happen again.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Not only did the maintainer state the above, they also used AI to generate the response! In a now-deleted comment, they clearly used some AI to respond to my complaint:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;auth0/nextjs-auth0 #2413’s now-deleted comment&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;With the classic ChatGPT “you are absolutely correct”, it’s pretty frustrating that this developer used AI to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Take my report/PR and commit it themselves.&lt;/item&gt;
      &lt;item&gt;Used AI to commit it, removing my attribution.&lt;/item&gt;
      &lt;item&gt;Used AI to “apologise” for using AI, then stated that “it won’t happen again” – (yeah right; please provide a detailed explanation how you’re going to ensure that, when clearly a 1-line code change is too much for your AI to handle without breaking).&lt;/item&gt;
      &lt;item&gt;Refused to fix the commit to remove the invalid / AI-generated-slop details, and add back mine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Indeed, asking:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I would appreciate force-pushing a fix for the commit to properly include my information in the commit.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was told that they cannot change it. That seems like a copyright infringement to me: taking somebody else’s code, then changing the author’s name?&lt;/p&gt;
    &lt;p&gt;What I really find the most interesting is really how this AI slop even came to be. I cannot find any reference to the email address “my@simen.io” anywhere online. On GitHub, the only reference to this email address is from the nextjs-auth0 PR. Simen Olsen has never contributed to any of the nextjs-auth0 repositories as far as I can tell (searching &lt;code&gt;org:auth0 author:simenandre&lt;/code&gt; on GitHub), and that doesn’t even seem to be their real email address. so was this some type of ai hallucination? And why? The code change was tiny. I just totally don’t get it: I have literally never had any AI tooling fail like this and come up with some other person’s (fake) contact details. It’s simply absurd; are auth0’s engineers using some extremely (extremely) low quality local model or something? If ChatGPT failed like this for me even once every thousand times, I would simply never use it again.&lt;/p&gt;
    &lt;p&gt;In the end, at the time of writing this, the auth0/nextjs-auth0 maintainer, Tushar Pandey, who made all of these mistakes, has not fixed attribution mistake in the commit history. In addition to this, that first bug, which allows for arbitrary account hijacking in this software, has been fixed after 3 weeks, with new versions of the nextjs-auth0 software released, but Okta’s security people stating that “unless you create a video abusing this vulnerability, we aren’t going to accept this as a security issue” – LMAO; “yeah, it’s a vulnerability, we fixed in the code, it can be used to takeover accounts, but you need to create a video”. Hilarious. That’s just another case to add to my list of hilarious problems related to reporting security issue, that my next post will document.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://joshua.hu/ai-slop-okta-nextjs-0auth-security-vulnerability"/><published>2025-11-18T10:17:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45976693</id><title>Free interactive tool that shows you how PCIe lanes work on motherboards</title><updated>2025-11-21T09:39:01.090390+00:00</updated><content/><link href="https://mobomaps.com"/><published>2025-11-19T07:13:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45984461</id><title>Show HN: F32 – An Extremely Small ESP32 Board</title><updated>2025-11-21T09:39:00.556477+00:00</updated><content>&lt;doc fingerprint="1d0752cb4aed53fa"&gt;
  &lt;main&gt;
    &lt;p&gt;The f32 is an ultra-compact ESP32 development board designed to mount directly behind a USB-C receptacle. The PCB measures just 9.85 mm x 8.45 mm. It's powered by the ESP32-C3FH4 microcontroller and was created primarily for research and as a bit of a stress test for the ESP32, since it intentionally ignores many standard design guidelines. There's only one exposed GPIO and it is connected to an onboard LED, so most of the development on here would be more catered for wifi/web.&lt;/p&gt;
    &lt;p&gt;To test the f32 an example application was created that users can interact with. The application turns the f32 into a captive portal so when it's powered on it will show up as an open access point that the user can select from available WiFi networks. The user is then automatically sent to the f32's control page where they can interact with some of its basic functionality such as turning on an LED or scanning for surrounding WiFi networks. There's also an "About" page that provides a small overview of the device. Below are some screenshots and a gif of interacting with the device.&lt;/p&gt;
    &lt;p&gt;Initially the f32 didn't seem to want to work. I couldn't get it to connect to any networks or broadcast it's own network. Im 100% sure this is due to the poor antenna circuitry or lack of, but I did manage to get it functional after adding an additional tiny antenna onto the chip antenna as seen in the picture below. This was just a piece of bent wire soldered to the end lead and floating above the first lead.&lt;/p&gt;
    &lt;p&gt;Since I don't have fancy signal testing equipment I relied on some manual testing such as seeing if I can still connect to the device and control the LED. In a clear line of sight test with the f32 placed about 3ft off the ground I was able to connect and perform scans/control the LED at roughly 120ft! This can be seen in my highly necessary depiction below.&lt;/p&gt;
    &lt;p&gt;The PCB was designed using DipTrace and manufactured by PCBWay with a board thickness of 0.6mm, min hole size of 0.2mm, and min track/spacing of 4/4mil. At the time of making this it only cost $10.75 for 5 boards shipped! That still blows my mind. PCBWay does also offer assembly services, but I chose to assemble this at home and suffer a bit. This took a bit of trial and error with such small parts, but I decided the best way for me was to ditch the stencil and make flux my best friend. &lt;/p&gt;
    &lt;p&gt;Tools &amp;amp; parts used:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SRA Solder 135 Rosin Paste Soldering Flux&lt;/item&gt;
      &lt;item&gt;Thin Kester Solder 63/37&lt;/item&gt;
      &lt;item&gt;Pinecil Soldering Iron&lt;/item&gt;
      &lt;item&gt;Fine Soldering Iron Tip&lt;/item&gt;
      &lt;item&gt;Rework Station&lt;/item&gt;
      &lt;item&gt;Soldering Hotplate&lt;/item&gt;
      &lt;item&gt;Digital Microscope or Jewelers Loupe&lt;/item&gt;
      &lt;item&gt;Fine Tip Tweezer (DON'T cheap out on tweezers! A good tweezer is a game changer)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Steps to building one:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Send the gerber file &lt;code&gt;f32_gerber.zip&lt;/code&gt;found in the&lt;code&gt;hardware&lt;/code&gt;folder to PCBWay with the specs mentioned above.&lt;/item&gt;
      &lt;item&gt;Order the components noted in &lt;code&gt;f32_bom.pdf&lt;/code&gt;. These parts can be found on both DigiKey and Mouser except the antenna. I don't remember where I had originally ordered them, but I believe they are CrossAir CA-C03.&lt;list rend="ul"&gt;&lt;item&gt;** Tip: Always order more than you need, especially with components as small as these.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clean the pcb really well with 99% Alcohol.&lt;/item&gt;
      &lt;item&gt;Starting with the top side (Antenna side) apply a thin layer of soldering flux across the entire board using a tooth pick.&lt;/item&gt;
      &lt;item&gt;Using a soldering iron with a fine tip apply some solder to the tip and then go across all the exposed pads.&lt;/item&gt;
      &lt;item&gt;Clean the board again with 99% alcohol and verify all the pads on this side have some solder on them.&lt;/item&gt;
      &lt;item&gt;Apply another thin layer of flux to the same side.&lt;/item&gt;
      &lt;item&gt;Using tweezers and a microscope/loupe start placing the top components following the reference guide &lt;code&gt;f32_reference.pdf&lt;/code&gt;.&lt;list rend="ul"&gt;&lt;item&gt;** Tip: I found placing the larger components last helps.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Gently move the board onto the soldering hotplate or use the rework station to heat the solder back up and watch the components wiggle into place.&lt;/item&gt;
      &lt;item&gt;Repeat with Bottom side. &lt;list rend="ul"&gt;&lt;item&gt;Bottom side must be done using a rework hot air gun, not possible with hotplate.&lt;/item&gt;&lt;item&gt;Place the USB-C receptacle last.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Clean entire board using alcohol and a fine toothbrush.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After assembly you can use ESP-IDF VSCode extension or Arduino and upload whatever you'd like to the board or you can upload my example application using the steps below.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make sure you are in the base directory of this repo and have access to &lt;code&gt;esptool.py&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Make sure your &lt;code&gt;esptool&lt;/code&gt;version is at&lt;code&gt;v4&lt;/code&gt;+&lt;/item&gt;
      &lt;item&gt;Run the following command replacing &lt;code&gt;&amp;lt;PORT&amp;gt;&lt;/code&gt;with whichever port the device is connected to i.e. on Windows typically something like&lt;code&gt;COM5&lt;/code&gt;or on Linux&lt;code&gt;/dev/ttyACM0&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;esptool.py -p &amp;lt;PORT&amp;gt; -b 460800 --before default_reset --after hard_reset --chip esp32c3 write_flash --flash_mode dio --flash_freq 80m --flash_size 2MB 0x0 firmware/bootloader.bin 0x10000 firmware/f32_internal.bin 0x8000 firmware/partition-table.bin 
&lt;/code&gt;
    &lt;p&gt;Well that's up to you to decide. I started this project for some personal research and also a fun learning experience. I had always wanted a project that used 01005 components ever since I had accidentally ordered some years ago. Whatever you choose to use it for, please note that this design intentionally neglects several fundamental components such as proper decoupling capacitors, an antenna matching circuit, USB termination resistors, and likely more. It does function, but it’s intentionally bare.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Expose more GPIOs on the sides of the PCB to make it a mountable PCB.&lt;/item&gt;
      &lt;item&gt;Improve antenna circuitry.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lastly, fun coincidence, the ESP32 chip, the antenna, and the LDO all are "C3" models!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/PegorK/f32"/><published>2025-11-19T20:09:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45990934</id><title>Interactive World History Atlas Since 3000 BC</title><updated>2025-11-21T09:38:59.985183+00:00</updated><content>&lt;doc fingerprint="8607bdb6bda831db"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Interactive World History Atlas since 3000 BC&lt;/head&gt;
    &lt;head rend="h2"&gt;World History Maps &amp;amp; Timelines. Kingdoms, Battles, Expeditions.&lt;lb/&gt; Comparative History, Political, Military, Art, Science, Literature, Religion, Philosophy. Maps based on vector database.&lt;/head&gt;
    &lt;p/&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://geacron.com/home-en/"/><published>2025-11-20T09:52:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45991738</id><title>Adversarial poetry as a universal single-turn jailbreak mechanism in LLMs</title><updated>2025-11-21T09:38:59.817613+00:00</updated><content>&lt;doc fingerprint="dc9d341acdd39acb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computation and Language&lt;/head&gt;&lt;p&gt; [Submitted on 19 Nov 2025 (v1), last revised 20 Nov 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for Large Language Models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of 3 open-weight LLM judges, whose binary safety assessments were validated on a stratified human-labeled subset. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Matteo Prandi [view email]&lt;p&gt;[v1] Wed, 19 Nov 2025 10:14:08 UTC (31 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 20 Nov 2025 03:34:44 UTC (30 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2511.15304"/><published>2025-11-20T12:01:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45993296</id><title>Nano Banana Pro</title><updated>2025-11-21T09:38:59.608482+00:00</updated><content>&lt;doc fingerprint="7c1622329c3e4b84"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Nano Banana Pro&lt;/head&gt;
    &lt;p&gt;Just a few months ago we released Nano Banana, our Gemini 2.5 Flash Image model. From restoring old photos to generating mini figurines, Nano Banana was a big step in image editing that empowered casual creators to express their creativity.&lt;/p&gt;
    &lt;p&gt;Today, we’re introducing Nano Banana Pro (Gemini 3 Pro Image), our new state-of-the art image generation and editing model. Built on Gemini 3 Pro, Nano Banana Pro uses Gemini’s state-of-the-art reasoning and real-world knowledge to visualize information better than ever before.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Nano Banana Pro helps you bring any idea or design to life&lt;/head&gt;
    &lt;p&gt;Nano Banana Pro can help you visualize any idea and design anything - from prototypes, to representing data as infographics, to turning handwritten notes into diagrams.&lt;/p&gt;
    &lt;p&gt;With Nano Banana Pro, now you can:&lt;/p&gt;
    &lt;p&gt;Generate more accurate, context-rich visuals based on enhanced reasoning, world knowledge and real-time information&lt;/p&gt;
    &lt;p&gt;With Gemini 3’s advanced reasoning, Nano Banana Pro doesn’t just create beautiful images, it also helps you create more helpful content. You can get accurate educational explainers to learn more about a new subject, like context-rich infographics and diagrams based on the content you provide or facts from the real world. Nano Banana Pro can also connect to Google Search's vast knowledge base to help you create a quick snapshot for a recipe or visualize real-time information like weather or sports.&lt;/p&gt;
    &lt;p&gt;An infographic of the common house plant, String of Turtles, with information on origins, care essentials and growth patterns.&lt;/p&gt;
    &lt;p&gt;Prompt: Create an infographic about this plant focusing on interesting information.&lt;/p&gt;
    &lt;p&gt;Step-by-step infographic for making Elaichi Chai (cardamom tea), demonstrating the ability to visualize recipes and real-world information.&lt;/p&gt;
    &lt;p&gt;Prompt: Create an infographic that shows how to make elaichi chai&lt;/p&gt;
    &lt;p&gt;We used Nano Banana Pro to pull in real-time weather via Search grounding to build a pop-art infographic.&lt;/p&gt;
    &lt;p&gt;Generate better visuals with more accurate, legible text directly in the image in multiple languages&lt;/p&gt;
    &lt;p&gt;Nano Banana Pro is the best model for creating images with correctly rendered and legible text directly in the image, whether you’re looking for a short tagline, or a long paragraph. Gemini 3 is great at understanding depth and nuance, which unlocks a world of possibilities with image editing and generation - especially with text. Now you can create more detailed text in mockups or posters with a wider variety of textures, fonts and calligraphy. With Gemini’s enhanced multilingual reasoning, you can generate text in multiple languages, or localize and translate your content so you can scale internationally and/or share content more easily with friends and family.&lt;/p&gt;
    &lt;p&gt;A black and white storyboard sketch showing an establishing shot, medium shot, close-up, and POV shot for a film scene.&lt;/p&gt;
    &lt;p&gt;Prompt: Create a storyboard for this scene&lt;/p&gt;
    &lt;p&gt;The word 'BERLIN' integrated into the architecture of a city block, spanning across multiple buildings.&lt;/p&gt;
    &lt;p&gt;Prompt: View of a cozy street in Berlin on a bright sunny day, stark shadows. the old houses are oddly shaped like letters that spell out "BERLIN" Colored in Blue, Red, White and black. The houses still look like houses and the resemblance to letters is subtle.&lt;/p&gt;
    &lt;p&gt;Calligraphy inspired by meaning, showcasing the ability to generate expressive text with a wider variety of textures and fonts.&lt;/p&gt;
    &lt;p&gt;Prompt: make 8 minimalistic logos, each is an expressive word, and make letters convey a message or sound visually to express the meaning of this word in a dramatic way. composition: flat vector rendering of all logos in black on a single white background&lt;/p&gt;
    &lt;p&gt;A beverage campaign concept showcasing accurate translation and rendering of English text into Korean.&lt;/p&gt;
    &lt;p&gt;Prompt: translate all the English text on the three yellow and blue cans into Korean, while keeping everything else the same&lt;/p&gt;
    &lt;p&gt;A graphic design featuring the word 'TYPOGRAPHY' with a retro, screen-printed texture.&lt;/p&gt;
    &lt;p&gt;Prompt: A vibrant, eye-catching "TYPOGRAPHY" design on a textured off-white background. The letters are bold, blocky, extra condensed and create a 3D effect with overlapping layers of bright blue and hot pink, each with a halftone dot pattern, evoking a retro print aesthetic. 16:9 aspect ratio&lt;/p&gt;
    &lt;p&gt;Blending text and texture in a creative way by integrating the phrase into a woodchopping scene.&lt;/p&gt;
    &lt;p&gt;Prompt: Create an image showing the phrase "How much wood would a woodchuck chuck if a woodchuck could chuck wood" made out of wood chucked by a woodchuck.&lt;/p&gt;
    &lt;p&gt;Create high-fidelity visuals with upgraded creative capabilities&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consistency by design: With Nano Banana Pro, you can blend more elements than ever before, using up to 14 images and maintaining the consistency and resemblance of up to 5 people. Whether turning sketches into products or blueprints into photorealistic 3D structures, you can now bridge the gap between concept and creation. Apply your desired visual look and feel to your mockups with ease, ensuring your branding remains seamless and consistent across every touchpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Maintaining the consistency of up to 14 inputs, including multiple characters, across a complex composition.&lt;/p&gt;
    &lt;p&gt;Prompt: A medium shot of the 14 fluffy characters sitting squeezed together side-by-side on a worn beige fabric sofa and on the floor. They are all facing forwards, watching a vintage, wooden-boxed television set placed on a low wooden table in front of the sofa. The room is dimly lit, with warm light from a window on the left and the glow from the TV illuminating the creatures' faces and fluffy textures. The background is a cozy, slightly cluttered living room with a braided rug, a bookshelf with old books, and rustic kitchen elements in the background. The overall atmosphere is warm, cozy, and amused.&lt;/p&gt;
    &lt;p&gt;Craft lifestyle scenes by combining multiple elements.&lt;/p&gt;
    &lt;p&gt;Prompt: Combine these images into one appropriately arranged cinematic image in 16:9 format and change the dress on the mannequin to the dress in the image&lt;/p&gt;
    &lt;p&gt;Create surreal landscapes by combining multiple input elements.&lt;/p&gt;
    &lt;p&gt;Prompt: Combine these images into one appropriately arranged cinematic image in 16:9 format&lt;/p&gt;
    &lt;p&gt;A high-fashion editorial shot set in a desert landscape that maintains the consistency and resemblance of the people from the 6 input photos.&lt;/p&gt;
    &lt;p&gt;Prompt: Put these five people and this dog into a single image, they should fit into a stunning award-winning shot in the style if [sic] a fashion editorial. The identity of all five people and their attire and the dog must stay consistent throughout but they can and should be seen from different angles and distances in [sic] as is most natural and suitable to the scene. Make the colour and lighting look natural on them all, they look like they naturally fit into this fashion show.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Studio-quality creative controls: With Nano Banana Pro's new capabilities we are putting advanced creative controls directly into your hands. Select, refine and transform any part of an image with improved localized editing. Adjust camera angles, change the focus and apply sophisticated color grading, or even transform scene lighting (e.g. changing day to night or creating a bokeh effect). Your creations are ready for any platform, from social media to print, thanks to a range of available aspect ratios and available 2K and 4K resolution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Change the look and feel of an image for a range of platforms by adapting the aspect ratio.&lt;/p&gt;
    &lt;p&gt;Prompt: change aspect ratio to 1:1 by reducing background. The character, remains exactly locked in its current position&lt;/p&gt;
    &lt;p&gt;Lighting and focus controls applied to transform a scene from day to night.&lt;/p&gt;
    &lt;p&gt;Prompt: Turn this scene into nighttime&lt;/p&gt;
    &lt;p&gt;Obscure or enlighten a section of your image with lighting controls to achieve specific dramatic effects.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Prompt: Generate an image with an intense chiaroscuro effect. The man should retain his original features and expression. Introduce harsh, directional light, appearing to come from above and slightly to the left, casting deep, defined shadows across the face. Only slivers of light illuminating his eyes and cheekbones, the rest of the face is in deep shadow.&lt;/p&gt;
    &lt;p&gt;Bring out the details of your composition by adjusting the depth of field or focal point (e.g., focusing on the flowers).&lt;/p&gt;
    &lt;p&gt;Prompt: Focus on the flowers&lt;/p&gt;
    &lt;head rend="h2"&gt;How you can try Nano Banana Pro today&lt;/head&gt;
    &lt;p&gt;Across our products and services, you now have a choice: the original Nano Banana for fast, fun editing, or Nano Banana Pro for complex compositions requiring the highest quality and visually sophisticated results.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consumers and students: Rolling out globally in the Gemini app when you select ‘Create images’ with the ‘Thinking’ model. Our free-tier users will receive limited free quotas, after which they will revert to the original Nano Banana model. Google AI Plus, Pro and Ultra subscribers receive higher quotas. For AI Mode in Search, Nano Banana Pro is available in the U.S. for Google AI Pro and Ultra subscribers. For NotebookLM, Nano Banana Pro is also available for subscribers globally.&lt;/item&gt;
      &lt;item&gt;Professionals: We're upgrading image generation in Google Ads to Nano Banana Pro to put cutting-edge creative and editing power directly into the hands of advertisers globally. It’s also rolling out starting today to Workspace customers in Google Slides and Vids.&lt;/item&gt;
      &lt;item&gt;Developers and enterprise: Starting to roll out in the Gemini API and Google AI Studio, and in Google Antigravity to create rich UX layouts &amp;amp; mockups; enterprises can start building in Vertex AI for scaled creation today and it’s coming soon to Gemini Enterprise.&lt;/item&gt;
      &lt;item&gt;Creatives: Starting to roll out to Google AI Ultra subscribers in Flow, our AI filmmaking tool, to give creatives, filmmakers and marketers even more precision and control over their frames and scenes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to identify AI-generated images in the Gemini app&lt;/head&gt;
    &lt;p&gt;We believe it’s critical to know when an image is AI-generated. This is why all media generated by Google’s tools are embedded with our imperceptible SynthID digital watermark.&lt;/p&gt;
    &lt;p&gt;Today, we are putting a powerful verification tool directly in consumers’ hands: you can now upload an image into the Gemini app and simply ask if it was generated by Google AI, thanks to SynthID technology. We are starting with images, but will expand to audio and video soon.&lt;/p&gt;
    &lt;p&gt;In addition to SynthID, we will maintain a visible watermark (the Gemini sparkle) on images generated by free and Google AI Pro tier users, to make images even more easy to detect as Google AI-generated.&lt;/p&gt;
    &lt;p&gt;Recognizing the need for a clean visual canvas for professional work, we will remove the visible watermark from images generated by Google AI Ultra subscribers and within the Google AI Studio developer tool.&lt;/p&gt;
    &lt;p&gt;You can find out more about how we’re increasing transparency in AI content with SynthID in our blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/ai/nano-banana-pro/"/><published>2025-11-20T15:04:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45994854</id><title>Android and iPhone users can now share files, starting with the Pixel 10</title><updated>2025-11-21T09:38:59.295109+00:00</updated><content>&lt;doc fingerprint="afecea04cdf1cf3c"&gt;
  &lt;main&gt;
    &lt;p&gt;When it comes to sharing moments between family and friends, what device you have shouldn’t matter — sharing should just work. But we’ve heard from many people that they want a simpler way to share files between devices.&lt;/p&gt;
    &lt;p&gt;Today, we’re introducing a way for Quick Share to work with AirDrop. This makes file transfer easier between iPhones and Android devices, and starts rolling out today to the Pixel 10 family.&lt;/p&gt;
    &lt;p&gt;We built this with security at its core, protecting your data with strong safeguards that were tested by independent security experts. It’s just one more way we’re bringing better compatibility that people are asking for between operating systems, following our work on RCS and unknown tracker alerts.&lt;/p&gt;
    &lt;p&gt;We’re looking forward to improving the experience and expanding it to more Android devices. See it in action on the Pixel 10 Pro in this video, and try it out for yourself!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/products/android/quick-share-airdrop/"/><published>2025-11-20T17:04:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45995394</id><title>Launch HN: Poly (YC S22) – Cursor for Files</title><updated>2025-11-21T09:38:58.808588+00:00</updated><content>&lt;doc fingerprint="1d40ab5b628d8f0a"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hello world, this is Abhay from Poly (&lt;/p&gt;https://poly.app&lt;p&gt;). We’re building an app to replace Finder/File Explorer with something more intelligent and searchable. Think of it like Dropbox + NotebookLM + Perplexity for terabytes of your files. Here’s a quick demo: &lt;/p&gt;https://www.youtube.com/watch?v=RsqCySU4Ln0&lt;p&gt;.&lt;/p&gt;&lt;p&gt;Poly can search your content in natural language, across a broad range of file types and down to the page, paragraph, pixel, or point in time. We also provide an integrated agent that can take actions on your files such as creating, editing, summarizing, and researching. Any action that you can take, the agent can also take, from renaming, moving, tagging, annotating, and organizing files for you. The agent can also read URLs, youtube links, and can search the web and even download files for you.&lt;/p&gt;&lt;p&gt;Here are some public drives that you can poke around in (note: it doesn’t work in Safari yet—sorry! we’re working on it.)&lt;/p&gt;&lt;p&gt;Every issue of the Whole Earth Catalogue: https://poly.app/shared/whole-earth-catalogues&lt;/p&gt;&lt;p&gt;Archive of old Playstation Manuals: https://poly.app/shared/playstation-manuals-archive&lt;/p&gt;&lt;p&gt;Mini archive of Orson Welles interviews and commercial spots: https://poly.app/shared/orson-welles-archive&lt;/p&gt;&lt;p&gt;Archive of Salvador Dali’s paintings for Alice in Wonderland: https://poly.app/shared/salvador-dali-alice-in-wonderland&lt;/p&gt;&lt;p&gt;To try it out, navigate to one of these public folders and use the agent or search to find things. The demo video above can give you an idea of how the UI roughly works. Select files by clicking on them. Quick view by pressing space. Open the details for any file by pressing cmd + i. You can search from the top middle bar (or press cmd + K), and all searches will use semantic similarity and search within the files. Or use the agent from the bottom right tools menu (or press cmd + ?) and you can ask about the files, have the agent search for you, summarize things, etc.&lt;/p&gt;&lt;p&gt;We decided to build this after launching an early image-gen company back in March 2022, and realizing how painful it was for users to store, manage, and search their libraries, especially in a world of generative media. Despite our service having over 150,000 users at that point, we realized that our true calling was fixing the file browser to make it intelligent, so we shut our service down in 2023 and pivoted to this.&lt;/p&gt;&lt;p&gt;We think Poly will be a great fit for anyone that wants to do useful things with their files, such as summarizing research papers, finding the right media or asset, creating a shareable portfolio, searching for a particular form or document, and producing reports and overviews. Of course, it’s a great way to organize your genAI assets as well. Or just use it to organize notes, links, inspo, etc.&lt;/p&gt;&lt;p&gt;Under the hood, Poly is built on our advanced search model, Polyembed-v1 that natively supports multimodal search across text, documents, spreadsheets, presentations, images, audio, video, PDFs, and more. We allow you to search by phrase, file similarity, color, face, and several other kinds of features. The agent is particularly skilled at using the search, so you can type in something like “find me the last lease agreement I signed” and it can go look for it by searching, reading the first few files, searching again if nothing matches, etc. But the quality of our embed model means it almost always finds the file in the first search.&lt;/p&gt;&lt;p&gt;It works identically across web and desktop, except on desktop it syncs your cloud files to a folder (just like google drive). On the web we use clever caching to enable offline support and file conflict recovery. We’ve taken great pains to make our system faster than your existing file browser, even if you’re using it from a web browser.&lt;/p&gt;&lt;p&gt;File storage plans are currently at: 100GB free tier, paid tier is 2TB at $10/m, and 1c per GB per month on top of the 2TB. We also have rate limits for agent use that vary at different tiers.&lt;/p&gt;&lt;p&gt;We’re excited to expand with many features over the following months, including “virtual files” (store your google docs in Poly), sync from other hosting providers, mobile apps, an MCP ecosystem for the agent, access to web search and deep research modes, offline search, local file support (on desktop), third-party sources (WebDAV, NAS), and a whole lot more.&lt;/p&gt;&lt;p&gt;Our waitlist is now open and we’ll be letting folks in starting today! Sign up at https://poly.app.&lt;/p&gt;&lt;p&gt;We’d also love to hear your thoughts (and concerns) about what we’re building, as we’re early in this journey so your feedback can very much shape the future of our company!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45995394"/><published>2025-11-20T17:47:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45995740</id><title>Microsoft makes Zork open-source</title><updated>2025-11-21T09:38:58.677269+00:00</updated><content>&lt;doc fingerprint="3d455b5d81aae591"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Preserving code that shaped generations: Zork I, II, and III go Open Source&lt;/head&gt;
    &lt;p&gt;WRITTEN BY&lt;/p&gt;
    &lt;p&gt;/en-us/opensource/blog/author/stacey-haffner&lt;/p&gt;
    &lt;p&gt;/en-us/opensource/blog/author/scott-hanselman&lt;/p&gt;
    &lt;head rend="h3"&gt;A game that changed how we think about play&lt;/head&gt;
    &lt;p&gt;When Zork arrived, it didn’t just ask players to win; it asked them to imagine. There were no graphics, no joystick, and no soundtrack, only words on a screen and the player’s curiosity. Yet those words built worlds more vivid than most games of their time. What made that possible wasn’t just clever writing, it was clever engineering.&lt;/p&gt;
    &lt;p&gt;Beneath that world of words was something quietly revolutionary: the Z-Machine, a custom-built engine. Z-Machine is a specification of a virtual machine, and now there are many Z-Machine interpreters that we used today that are software implementations of that VM. The original mainframe version of Zork was too large for early home computers to handle, so the team at Infocom made a practical choice. They split it into three games titled Zork I, Zork II, and Zork III, all powered by the same underlying system. This also meant that instead of rebuilding the game for each platform, they could use the Z-Machine to interpret the same story files on any computer. That design made Zork one of the first games to be truly cross-platform, appearing on Apple IIs, IBM PCs, and more.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preserving a piece of history&lt;/head&gt;
    &lt;p&gt;Game preservation takes many forms, and it’s important to consider research as well as play. The Zork source code deserves to be preserved and studied. Rather than creating new repositories, we’re contributing directly to history. In collaboration with Jason Scott, the well-known digital archivist of Internet Archive fame, we have officially submitted upstream pull requests to the historical source repositories of Zork I, Zork II, and Zork III. Those pull requests add a clear MIT LICENSE and formally document the open-source grant.&lt;/p&gt;
    &lt;p&gt;Each repository includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Source code for Zork I, Zork II, and Zork III.&lt;/item&gt;
      &lt;item&gt;Accompanying documentation where available, such as build notes, comments, and historically relevant files.&lt;/item&gt;
      &lt;item&gt;Clear licensing and attribution, via MIT LICENSE.txt and repository-level metadata.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This release focuses purely on the code itself. It does not include commercial packaging or marketing materials, and it does not grant rights to any trademarks or brands, which remain with their respective owners. All assets outside the scope of these titles’ source code are intentionally excluded to preserve historical accuracy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Running Zork I-III today&lt;/head&gt;
    &lt;p&gt;More than forty years later, Zork is still alive and easier than ever to play. The games remain commercially available via The Zork Anthology on Good Old Games. For those who enjoy a more hands on approach, the games can be compiled and run locally using ZILF, the modern Z-Machine interpreter created by Tara McGrew. ZILF compiles ZIL files into Z3s that can be run with Tara’s own ZLR which is a sentence I never thought I’d write, much less say out loud! There are a huge number of wonderful Z-machine runners across all platforms for you to explore.&lt;/p&gt;
    &lt;p&gt;Here's how to get started running Zork locally with ZILF. From the command line, compile and assembly the zork1.zil into a runnable z3 file.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;"%ZILF_PATH%\zilf.exe" zork1.zil &lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;"%ZILF_PATH%\zapf.exe" zork1.zap zork1-ignite.z3&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Then run your Z3 file in a Zmachine runner. I’m using Windows Frotz from David Kinder based on Stefan Jokisch’s Frotz core:&lt;/p&gt;
    &lt;head rend="h3"&gt;Continuing the journey&lt;/head&gt;
    &lt;p&gt;We will use the existing historical repositories as the canonical home for Zork’s source. Once the initial pull requests land under the MIT License, contributions are welcome. We chose MIT for its simplicity and openness because it makes the code easy to study, teach, and build upon. File issues, share insights, or submit small, well-documented improvements that help others learn from the original design. The goal is not to modernize Zork but to preserve it as a space for exploration and education.&lt;/p&gt;
    &lt;p&gt;Zork has always been more than a game. It is a reminder that imagination and engineering can outlast generations of hardware and players. Bringing this code into the open is both a celebration and a thank you to the original Infocom creators for inventing a universe we are still exploring, to Jason Scott and the Internet Archive for decades of stewardship and partnership, and to colleagues across Microsoft OSPO, Xbox, and Activision who helped make open source possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://opensource.microsoft.com/blog/2025/11/20/preserving-code-that-shaped-generations-zork-i-ii-and-iii-go-open-source"/><published>2025-11-20T18:13:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45995816</id><title>The Lions Operating System</title><updated>2025-11-21T09:38:57.871507+00:00</updated><content>&lt;doc fingerprint="bccc914e63abc302"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Lions Operating System #&lt;/head&gt;
    &lt;quote&gt;LionsOS is currently undergoing active research and development, it does not have a concrete verification story yet. It is not expected for LionsOS to be stable at this time, but it is available for others to experiment with.&lt;/quote&gt;
    &lt;p&gt;LionsOS is an operating system based on the seL4 microkernel with the goal of making the achievements of seL4 accessible. That is, to provide performance, security, and reliability.&lt;/p&gt;
    &lt;p&gt;LionsOS is being developed by the Trustworthy Systems research group at UNSW Sydney in Australia.&lt;/p&gt;
    &lt;p&gt;It is not a conventional operating system, but contains composable components for creating custom operating systems that are specific to a particular task. Components are joined together using the Microkit tool.&lt;/p&gt;
    &lt;p&gt;The principles on which a LionsOS system is built are laid out fully in the sDDF design document; but in brief they are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Components are connected by lock-free queues using an efficient model-checked signalling mechanism.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As far as is practical, operating systems components do a single thing. Drivers for instance exist solely to convert between a hardware interface and a set of queues to talk to the rest of the system.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Components called virtualisers handle multiplexing and control, and conversion between virtual and IO addresses for drivers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Information is shared only where necessary, via the queues, or via published information pages.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The system is static: it does not adapt to changing hardware, and does not load components at runtime. There is a mechanism for swapping components of the same type at runtime, to implement policy changes, or to reboot a virtual machine with a new Linux kernel.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To be successful, many more components are needed. Pull requests to the various repositories are welcome. See the page on contributing for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lionsos.org"/><published>2025-11-20T18:19:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45995834</id><title>NTSB Preliminary Report – UPS Boeing MD-11F Crash [pdf]</title><updated>2025-11-21T09:38:57.734281+00:00</updated><content/><link href="https://www.ntsb.gov/Documents/Prelimiary%20Report%20DCA26MA024.pdf"/><published>2025-11-20T18:20:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45996585</id><title>Data-at-Rest Encryption in DuckDB</title><updated>2025-11-21T09:38:57.437070+00:00</updated><content>&lt;doc fingerprint="bf9f389225e9b2d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Data-at-Rest Encryption in DuckDB&lt;/head&gt;
    &lt;p&gt;TL;DR: DuckDB v1.4 ships database encryption capabilities. In this blog post, we dive into the implementation details of the encryption, show how to use it and demonstrate its performance implications.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you would like to use encryption in DuckDB, we recommend using the latest stable version, v1.4.2. For more details, see the latest release blog post.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Many years ago, we read the excellent “Code Book” by Simon Singh. Did you know that Mary, Queen of Scots, used an encryption method harking back to Julius Caesar to encrypt her more saucy letters? But alas: the cipher was broken and the contents of the letters got her executed.&lt;/p&gt;
    &lt;p&gt;These days, strong encryption software and hardware is a commodity. Modern CPUs come with specialized cryptography instructions, and operating systems small and big contain mostly-robust cryptography software like OpenSSL.&lt;/p&gt;
    &lt;p&gt;Databases store arbitrary information, it is clear that many if not most datasets of any value should perhaps not be plainly available to everyone. Even if stored on tightly controlled hardware like a cloud virtual machine, there have been many cases of files being lost through various privilege escalations. Unsurprisingly, compliance frameworks like the common SOC 2 “highly recommend” encrypting data when stored on storage mediums like hard drives.&lt;/p&gt;
    &lt;p&gt;However, database systems and encryption have a somewhat problematic track record. Even PostgreSQL, the self-proclaimed “The World's Most Advanced Open Source Relational Database” has very limited options for data encryption. SQLite, the world’s “Most Widely Deployed and Used Database Engine” does not support data encryption out-of-the-box, its encryption extension is a $2000 add-on.&lt;/p&gt;
    &lt;p&gt;DuckDB has supported Parquet Modular Encryption for a while. This feature allows reading and writing Parquet files with encrypted columns. However, while Parquet files are great and reports of their impending death are greatly exaggerated, they cannot – for example – be updated in place, a pretty basic feature of a database management system.&lt;/p&gt;
    &lt;p&gt;Starting with DuckDB 1.4.0, DuckDB supports transparent data encryption of data-at-rest using industry-standard AES encryption.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;DuckDB's encryption does not yet meet the official NIST requirements.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Some Basics of Encryption&lt;/head&gt;
    &lt;p&gt;There are many different ways to encrypt data, some more secure than others. In database systems and elsewhere, the standard is the Advanced Encryption Standard (AES), which is a block cipher algorithm standardized by US NIST. AES is a symmetric encryption algorithm, meaning that the same key is used for both encryption and decryption of data.&lt;/p&gt;
    &lt;p&gt;For this reason, most systems choose to only support randomized encryption, meaning that identical plaintexts will always yield different ciphertexts (if used correctly!). The most commonly used industry standard and recommended encryption algorithm is AES – Galois Counter Mode (AES-GCM). This is because on top of its ability to randomize encryption, it also authenticates data by calculating a tag to ensure data has not been tampered with.&lt;/p&gt;
    &lt;p&gt;DuckDB v1.4 supports encryption at rest using AES-GCM-256 and AES-CTR-256 (counter mode) ciphers. AES-CTR is a simpler and faster version of AES-GCM, but less secure, since it does not provide authentication by calculating a tag. The 256 refers to the size of the key in bits, meaning that DuckDB now only supports GCM with 32-byte keys.&lt;/p&gt;
    &lt;p&gt;GCM and CTR both require as input a (1) plaintext, (2) an initialization vector (IV) and (3) an encryption key. Plaintext is the text that a user wants to encrypt. An IV is a unique bytestream of usually 16 bytes, that ensures that identical plaintexts get encrypted into different ciphertexts. A number used once (nonce) is a bytestream of usually 12 bytes, that together with a 4-byte counter construct the IV. Note that the IV needs to be unique for every encrypted block, but it does not necessarily have to be random. Reuse of the same IV is problematic, since an attacker could XOR the two ciphertexts and extract both messages. The tag in AES-GCM is calculated after all blocks are encrypted, pretty much like a checksum, but it adds an integrity check that securely authenticates the entire ciphertext.&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation in DuckDB&lt;/head&gt;
    &lt;p&gt;Before diving deeper into how we actually implemented encryption in DuckDB, we’ll explain some things about the DuckDB file format.&lt;/p&gt;
    &lt;p&gt;DuckDB has one main database header which stores data that enables it to correctly load and verify a DuckDB database. At the start of each DuckDB main database header, the magic bytes (“DUCKDB”) are stored and read upon initialization to verify whether the file is a valid DuckDB database file. The magic bytes are followed by four 8-byte of flags that can be set for different purposes.&lt;/p&gt;
    &lt;p&gt;When a database is encrypted in DuckDB, the main database header remains plaintext at all times, since the main header contains no sensitive data about the contents of the database file. Upon initializing an encrypted database, DuckDB sets the first bit in the first flag to indicate that the database is encrypted. After setting this bit, additional metadata is stored that is necessary for encryption. This metadata entails the (1) database identifier, (2) 8 bytes of additional metadata for e.g. the encryption cipher used, and (3) the encrypted canary.&lt;/p&gt;
    &lt;p&gt;The database identifier is used as a “salt”, and consists of 16 randomly generated bytes created upon initialization of each database. The salt is often used to ensure uniqueness, i.e., it makes sure that identical input keys or passwords are transformed into different derived keys. The 8-bytes of metadata comprise the key derivation function (first byte), usage of additional authenticated data (second byte), the encryption cipher (third byte), and the key length (fifth byte). After the metadata, the main header uses the encrypted canary to check if the input key is correct.&lt;/p&gt;
    &lt;head rend="h3"&gt;Encryption Key Management&lt;/head&gt;
    &lt;p&gt;To encrypt data in DuckDB, you can use practically any plaintext or base64 encoded string, but we recommend using a secure 32-byte base64 key. The user itself is responsible for the key management and thus for using a secure key. Instead of directly using the plain key provided by the user, DuckDB always derives a more secure key by means of a key derivation function (kdf). The kdf is a function that reduces or extends the input key to a 32-byte secure key. If the correctness of the input key is checked by deriving the secure key and decrypting the canary, the derived key is managed in a secure encryption key cache. This cache manages encryption keys for the current DuckDB context and ensures that the derived encryption keys are never swapped to disk by locking its memory. To strengthen security even more, the original input keys are immediately wiped from memory when the input keys are transformed into secure derived keys.&lt;/p&gt;
    &lt;head rend="h3"&gt;DuckDB Block Structure&lt;/head&gt;
    &lt;p&gt;After the main database header, DuckDB stores two 4KB database headers that contain more information about e.g. the block (header) size and the storage version used. After keeping the main database header plaintext, all remaining headers and blocks are encrypted when encryption is used.&lt;/p&gt;
    &lt;p&gt;Blocks in DuckDB are by default 256KB, but their size is configurable. At the start of each plaintext block there is an 8-byte block header, which stores an 8-byte checksum. The checksum is a simple calculation that is often used in database systems to check for any corrupted data.&lt;/p&gt;
    &lt;p&gt;For encrypted blocks however, its block header consists of 40 bytes instead of 8 bytes for the checksum. The block header for encrypted blocks contains a 16-byte nonce/IV and, optionally, a 16-byte tag, depending on which encryption cipher is used. The nonce and tag are stored in plaintext, but the checksum is encrypted for better security. Note that the block header always needs to be 8-bytes aligned to calculate the checksum.&lt;/p&gt;
    &lt;head rend="h3"&gt;Write-Ahead-Log Encryption&lt;/head&gt;
    &lt;p&gt;The write ahead log (WAL) in database systems is a crash recovery mechanism to ensure durability. It is an append-only file that is used in scenarios where the database crashed or is abruptly closed, and when not all changes are written yet to the main database file. The WAL makes sure these changes can be replayed up to the last checkpoint; which is a consistent snapshot of the database at a certain point in time. This means, when a checkpoint is enforced, which happens in DuckDB by either (1) closing the database or (2) reaching a certain threshold for storage, the WAL gets written into the main database file.&lt;/p&gt;
    &lt;p&gt;In DuckDB, you can force the creation of a WAL by setting&lt;/p&gt;
    &lt;code&gt;PRAGMA disable_checkpoint_on_shutdown;
PRAGMA wal_autocheckpoint = '1TB';
&lt;/code&gt;
    &lt;p&gt;This way you’ll disable a checkpointing on closing the database, meaning that the WAL does not get merged into the main database file. In addition, by setting wal_autocheckpoint to a high threshold, this will avoid intermediate checkpoints to happen and the WAL will persist. For example, we can create a persistent WAL file by first setting the above PRAGMAs, then attach an encrypted database, and then create a table where we insert 3 values.&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.db' AS enc (
    ENCRYPTION_KEY 'asdf',
    ENCRYPTION_CIPHER 'GCM'
);
CREATE TABLE enc.test (a INTEGER, b INTEGER);
INSERT INTO enc.test VALUES (11, 22), (13, 22), (12, 21)
&lt;/code&gt;
    &lt;p&gt;If we now close the DuckDB process, we can see that there is a &lt;code&gt;.wal&lt;/code&gt; file shown: &lt;code&gt;encrypted.db.wal&lt;/code&gt;. But how is the WAL created internally?&lt;/p&gt;
    &lt;p&gt;Before writing new entries (inserts, updates, deletes) to the database, these entries are essentially logged and appended to the WAL. Only after logged entries are flushed to disk, a transaction is considered as committed. A plaintext WAL entry has the following structure:&lt;/p&gt;
    &lt;p&gt;Since the WAL is append-only, we encrypt a WAL entry per value. For AES-GCM this means that we append a nonce and a tag to each entry. The structure in which we do this is depicted in below. When we serialize an encrypted entry to the encrypted WAL, we first store the length in plaintext, because we need to know how many bytes we should decrypt. The length is followed by a nonce, which on its turn is followed by the encrypted checksum and the encrypted entry itself. After the entry, a 16-byte tag is stored for verification.&lt;/p&gt;
    &lt;p&gt;Encrypting the WAL is triggered by default when an encryption key is given for any (un)encrypted database.&lt;/p&gt;
    &lt;head rend="h3"&gt;Temporary File Encryption&lt;/head&gt;
    &lt;p&gt;Temporary files are used to store intermediate data that is often necessary for large, out-of-core operations such as sorting, large joins and window functions. This data could contain sensitive information and can, in case of a crash, remain on disk. To protect this leftover data, DuckDB automatically encrypts temporary files too.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Structure of Temporary Files&lt;/head&gt;
    &lt;p&gt;There are three different types of temporary files in DuckDB: (1) temporary files that have the same layout as a regular 256KB block, (2) compressed temporary files and (3) temporary files that exceed the standard 256KB block size. The former two are suffixed with .tmp, while the latter is distinguished by a suffix with .block. To keep track of the size of .block temporary files, they are always prefixed with its length. As opposed to regular database blocks, temporary files do not contain a checksum to check for data corruption, since the calculation of a checksum is somewhat expensive.&lt;/p&gt;
    &lt;head rend="h4"&gt;Encrypting Temporary Files&lt;/head&gt;
    &lt;p&gt;Temporary files are encrypted (1) automatically when you attach an encrypted database or (2) when you use the setting &lt;code&gt;SET temp_file_encryption = true&lt;/code&gt;. In the latter case, the main database file is plaintext, but the temporary files will be encrypted. For the encryption of temporary files DuckDB internally generates temporary keys. This means that when the database crashes, the temporary keys are also lost. Temporary files cannot be decrypted in this case and are then essentially garbage.&lt;/p&gt;
    &lt;p&gt;To force DuckDB to produce temporary files, you can use a simple trick by just setting the memory limit low. This will create temporary files once the memory limit is exceeded. For example, we can create a new encrypted database, load this database with TPC-H data (SF 1), and then set the memory limit to 1 GB. If we then perform a large join, we force DuckDB to spill intermediate data to disk. For example:&lt;/p&gt;
    &lt;code&gt;SET memory_limit = '1GB';
ATTACH 'tpch_encrypted.db' AS enc (
    ENCRYPTION_KEY 'asdf',
    ENCRYPTION_CIPHER 'cipher'
);
USE enc;
CALL dbgen(sf = 1);

ALTER TABLE lineitem
    RENAME TO lineitem1;
CREATE TABLE lineitem2 AS
    FROM lineitem1;
CREATE OR REPLACE TABLE ans AS
    SELECT l1.* , l2.*
    FROM lineitem1 l1
    JOIN lineitem2 l2 USING (l_orderkey , l_linenumber);
&lt;/code&gt;
    &lt;p&gt;This sequence of commands will result in encrypted temporary files being written to disk. Once the query completes or when the DuckDB shell is exited, the temporary files are automatically cleaned up. In case of a crash however, it may happen that temporary files will be left on disk and need to be cleaned up manually.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Use Encryption in DuckDB&lt;/head&gt;
    &lt;p&gt;In DuckDB, you can (1) encrypt an existing database, (2) initialize a new, empty encrypted database or (3) reencrypt a database. For example, let's create a new database, load this database with TPC-H data of scale factor 1 and then encrypt this database.&lt;/p&gt;
    &lt;code&gt;INSTALL tpch;
LOAD tpch;
ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
ATTACH 'unencrypted.duckdb' AS unencrypted;
USE unencrypted;
CALL dbgen(sf = 1);
COPY FROM DATABASE unencrypted TO encrypted;
&lt;/code&gt;
    &lt;p&gt;There is not a trivial way to prove that a database is encrypted, but correctly encrypted data should look like random noise and has a high entropy. So, to check whether a database is actually encrypted, we can use tools to calculate the entropy or visualize the binary, such as ent and binocle.&lt;/p&gt;
    &lt;p&gt;When we use ent after executing the above chunk of SQL, i.e., &lt;code&gt;ent encrypted.duckdb&lt;/code&gt;, this will result in an entropy of 7.99999 bits per byte. If we do the same for the plaintext (unencrypted) database, this results in 7.65876 bits per byte. Note that the plaintext database also has a high entropy, but this is due to compression.&lt;/p&gt;
    &lt;p&gt;Let’s now visualize both the plaintext and encrypted data with binocle. For the visualization we created both a plaintext DuckDB database with scale factor of 0.001 of TPC-H data and an encrypted one:&lt;/p&gt;
    &lt;head&gt;Click here to see the entropy of a plaintext database&lt;/head&gt;
    &lt;head&gt;Click here to see the entropy of an encrypted database&lt;/head&gt;
    &lt;p&gt;In these figures, we can clearly observe that the encrypted database file seems completely random, while the plaintext database file shows some clear structure in its binary data.&lt;/p&gt;
    &lt;p&gt;To decrypt an encrypted database, we can use the following SQL:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
ATTACH 'new_unencrypted.duckdb' AS unencrypted;
COPY FROM DATABASE encrypted TO unencrypted;
&lt;/code&gt;
    &lt;p&gt;And to reencrypt an existing database, we can just simply copy the old encrypted database to a new one, like:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
ATTACH 'new_encrypted.duckdb' AS new_encrypted (ENCRYPTION_KEY 'xxxx');
COPY FROM DATABASE encrypted TO new_encrypted;
&lt;/code&gt;
    &lt;p&gt;The default encryption algorithm is AES GCM. This is recommended since it also authenticates data by calculating a tag. Depending on the use case, you can also use AES CTR. This is faster than AES GCM since it skips calculating a tag after encrypting all data. You can specify the CTR cipher as follows:&lt;/p&gt;
    &lt;code&gt;ATTACH 'encrypted.duckdb' AS encrypted (
    ENCRYPTION_KEY 'asdf',
    ENCRYPTION_CIPHER 'CTR'
);
&lt;/code&gt;
    &lt;p&gt;To keep track of which databases are encrypted, you can query this by running:&lt;/p&gt;
    &lt;code&gt;FROM duckdb_databases();
&lt;/code&gt;
    &lt;p&gt;This will show which databases are encrypted, and which cipher is used:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;database_name&lt;/cell&gt;
        &lt;cell role="head"&gt;database_oid&lt;/cell&gt;
        &lt;cell role="head"&gt;path&lt;/cell&gt;
        &lt;cell role="head"&gt;…&lt;/cell&gt;
        &lt;cell role="head"&gt;encrypted&lt;/cell&gt;
        &lt;cell role="head"&gt;cipher&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;encrypted&lt;/cell&gt;
        &lt;cell&gt;2103&lt;/cell&gt;
        &lt;cell&gt;encrypted.duckdb&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;true&lt;/cell&gt;
        &lt;cell&gt;GCM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;unencrypted&lt;/cell&gt;
        &lt;cell&gt;2050&lt;/cell&gt;
        &lt;cell&gt;unencrypted.duckdb&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;memory&lt;/cell&gt;
        &lt;cell&gt;592&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;system&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;temp&lt;/cell&gt;
        &lt;cell&gt;1995&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
        &lt;cell&gt;…&lt;/cell&gt;
        &lt;cell&gt;false&lt;/cell&gt;
        &lt;cell&gt;NULL&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;5 rows — 10 columns (5 shown)&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation and Performance&lt;/head&gt;
    &lt;p&gt;Here at DuckDB, we strive to achieve a good out-of-the-box experience with zero external dependencies and a small footprint. Encryption and decryption, however, are usually performed by pretty heavy external libraries such as OpenSSL. We would much prefer not to rely on external libraries or statically linking huge codebases just so that people can use encryption in DuckDB without additional steps. This is why we actually implemented encryption twice in DuckDB, once with the (excellent) Mbed TLS library and once with the ubiquitous OpenSSL library.&lt;/p&gt;
    &lt;p&gt;DuckDB already shipped parts of Mbed TLS because we use it to verify RSA extension signatures. However, for maximum compatibility we actually disabled the hardware acceleration of MbedTLS, which has a performance impact. Furthermore, Mbed TLS is not particularly hardened against things like nasty timing attacks. OpenSSL on the other hand contains heavily vetted and hardware-accelerated code to perform AES operations, which is why we can also use it for encryption.&lt;/p&gt;
    &lt;p&gt;In DuckDB Land, OpenSSL is part of the &lt;code&gt;httpfs&lt;/code&gt; extension. Once you load that extension, encryption will automatically switch to using OpenSSL. After we shipped encryption in DuckDB 1.4.0, security experts actually found issues with the random number generator we used in Mbed TLS mode. Even though it would be difficult to actually exploit this, we disabled writing to databases in MbedTLS mode from DuckDB 1.4.1. Instead, DuckDB now (version 1.4.2+) tries to auto-install and auto-load the &lt;code&gt;httpfs&lt;/code&gt; extension whenever a write is attempted. We might be able to revisit this in the future, but for now this seems the safest path forward that still allows high compatibility for reading. In OpenSSL mode, we always used a cryptographically-safe random number generation so that mode is unaffected.&lt;/p&gt;
    &lt;p&gt;Encrypting and decrypting database files is an additional step in writing tables to disk, so we would naturally assume that there is some performance impact. Let’s investigate the performance impact of DuckDB’s new encryption feature with a very basic experiment.&lt;/p&gt;
    &lt;p&gt;We first create two DuckDB database files, one encrypted and one unencrypted. We use the TPC-H benchmark generator again to create the table data, particularly the (somewhat tired) &lt;code&gt;lineitem&lt;/code&gt; table.&lt;/p&gt;
    &lt;code&gt;INSTALL httpfs;
INSTALL tpch;
LOAD tpch;

ATTACH 'unencrypted.duckdb' AS unencrypted;
CALL dbgen(sf = 10, catalog = 'unencrypted');

ATTACH 'encrypted.duckdb' AS encrypted (ENCRYPTION_KEY 'asdf');
CREATE TABLE encrypted.lineitem AS FROM unencrypted.lineitem;
&lt;/code&gt;
    &lt;p&gt;Now we use DuckDB’s neat &lt;code&gt;SUMMARIZE&lt;/code&gt; command three times: once on the unencrypted database, and once on the encrypted database using MbedTLS and once on the encrypted database using OpenSSL. We set a very low memory limit to force more reading and writing from disk.&lt;/p&gt;
    &lt;code&gt;SET memory_limit = '200MB';
.timer on

SUMMARIZE unencrypted.lineitem;
SUMMARIZE encrypted.lineitem;

LOAD httpfs; -- use OpenSSL
SUMMARIZE encrypted.lineitem;
&lt;/code&gt;
    &lt;p&gt;Here are the results on a fairly recent MacBook: &lt;code&gt;SUMMARIZE&lt;/code&gt; on the unencrypted table took ca. 5.4 seconds. Using Mbed TLS, this went up to around 6.2 s. However, when enabling OpenSSL the end-to-end time went straight back to 5.4 s. How is this possible? Is decryption not expensive? Well, there is a lot more happening in query processing than reading blocks from storage. So the impact of decryption is not all that huge, even when using a slow implementation. Secondly, when using hardware acceleration in OpenSSL, the overall overhead of encryption and decryption becomes almost negligible.&lt;/p&gt;
    &lt;p&gt;But just running summarization is overly simplistic. Real™ database workloads include modifications to data, insertion of new rows, updates of rows, deletion of rows etc. Also, multiple clients will be updating and querying at the same time. So we re-surrected the full TPC-H “Power” test from our previous blog post “Changing Data with Confidence and ACID”. We slightly tweaked the benchmark script to enable the new database encryption. For this experiment, we used the OpenSSL encryption implementation due to the issues outlined above. We observe Power@Size” and “Throughput@Size”. The former is raw sequential query performance, while the latter measures multiple parallel query streams in the presence of updates.&lt;/p&gt;
    &lt;p&gt;When running on the same MacBook with DuckDB 1.4.1 and a “scale factor” of 100, we get a Power@Size metric of 624,296 and a Throughput@Size metric of 450,409 without encryption.&lt;/p&gt;
    &lt;p&gt;When we enable encryption, the results are almost unchanged, confirming the observation of the small microbenchmark above. However, the relationship between available memory and the benchmark size means that we’re not stressing temporary file encryption. So we re-ran everything with an 8GB memory limit. We confirmed constant reading and writing to and from disk in this configuration by observing operating system statistics. For the unencrypted case, the Power@Size metric predictably went down to 591,841 and Throughput@Size went down to 153,690. And finally, we could observe a slight performance decrease with Power@Size of 571,985 and Throughput@Size of 145,353. However, that difference is not very great either and likely not relevant in real operational scenarios.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;With the new encrypted database feature, we can now safely pass around DuckDB database files with all information inside them completely opaque to prying eyes. This allows for some interesting new deployment models for DuckDB, for example, we could now put an encrypted DuckDB database file on a Content Delivery Network (CDN). A fleet of DuckDB instances could attach to this file read-only using the decryption key. This elegantly allows efficient distribution of private background data in a similar way like encrypted Parquet files, but of course with many more features like multi-table storage. When using DuckDB with encrypted storage, we can also simplify threat modeling when – for example – using DuckDB on cloud providers. While in the past access to DuckDB storage would have been enough to leak data, we can now relax paranoia regarding storage a little, especially since temporary files and WAL are also encrypted. And the best part of all of this, there is almost no performance overhead to using encryption in DuckDB, especially with the OpenSSL implementation.&lt;/p&gt;
    &lt;p&gt;We are very much looking forward to what you are going to do with this feature, and please let us know if you run into any issues.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://duckdb.org/2025/11/19/encryption-in-duckdb"/><published>2025-11-20T19:26:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45996860</id><title>CBP is monitoring US drivers and detaining those with suspicious travel patterns</title><updated>2025-11-21T09:38:57.022025+00:00</updated><content>&lt;doc fingerprint="ec590b983760b349"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Border Patrol is monitoring US drivers and detaining those with ‘suspicious’ travel patterns&lt;/head&gt;
    &lt;p&gt;The U.S. Border Patrol is monitoring millions of American drivers nationwide in a secretive program to identify and detain people whose travel patterns it deems suspicious. (AP video: Marshall Ritzel)&lt;/p&gt;
    &lt;p&gt;The U.S. Border Patrol is monitoring millions of American drivers nationwide in a secretive program to identify and detain people whose travel patterns it deems suspicious, The Associated Press has found.&lt;/p&gt;
    &lt;p&gt;The predictive intelligence program has resulted in people being stopped, searched and in some cases arrested. A network of cameras scans and records vehicle license plate information, and an algorithm flags vehicles deemed suspicious based on where they came from, where they were going and which route they took. Federal agents in turn may then flag local law enforcement.&lt;/p&gt;
    &lt;p&gt;Suddenly, drivers find themselves pulled over — often for reasons cited such as speeding, failure to signal, the wrong window tint or even a dangling air freshener blocking the view. They are then aggressively questioned and searched, with no inkling that the roads they drove put them on law enforcement’s radar.&lt;/p&gt;
    &lt;p&gt;Once limited to policing the nation’s boundaries, the Border Patrol has built a surveillance system stretching into the country’s interior that can monitor ordinary Americans’ daily actions and connections for anomalies instead of simply targeting wanted suspects. Started about a decade ago to fight illegal border-related activities and the trafficking of both drugs and people, it has expanded over the past five years.&lt;/p&gt;
    &lt;p&gt;The Border Patrol has recently grown even more powerful through collaborations with other agencies, drawing information from license plate readers nationwide run by the Drug Enforcement Administration, private companies and, increasingly, local law enforcement programs funded through federal grants. Texas law enforcement agencies have asked Border Patrol to use facial recognition to identify drivers, documents show.&lt;/p&gt;
    &lt;p&gt;This active role beyond the borders is part of the quiet transformation of its parent agency, U.S. Customs and Border Protection, into something more akin to a domestic intelligence operation. Under the Trump administration’s heightened immigration enforcement efforts, CBP is now poised to get more than $2.7 billion to build out border surveillance systems such as the license plate reader program by layering in artificial intelligence and other emerging technologies.&lt;/p&gt;
    &lt;p&gt;The result is a mass surveillance network with a particularly American focus: cars.&lt;/p&gt;
    &lt;p&gt;This investigation, the first to reveal details of how the program works on America’s roads, is based on interviews with eight former government officials with direct knowledge of the program who spoke on the condition of anonymity because they weren’t authorized to speak to the media, as well as dozens of federal, state and local officials, attorneys and privacy experts. The AP also reviewed thousands of pages of court and government documents, state grant and law enforcement data, and arrest reports.&lt;/p&gt;
    &lt;p&gt;The Border Patrol has for years hidden details of its license plate reader program, trying to keep any mention of the program out of court documents and police reports, former officials say, even going so far as to propose dropping charges rather than risk revealing any details about the placement and use of their covert license plate readers. Readers are often disguised along highways in traffic safety equipment like drums and barrels.&lt;/p&gt;
    &lt;p&gt;The Border Patrol has defined its own criteria for which drivers’ behavior should be deemed suspicious or tied to drug or human trafficking, stopping people for anything from driving on backcountry roads, being in a rental car or making short trips to the border region. The agency’s network of cameras now extends along the southern border in Texas, Arizona and California, and also monitors drivers traveling near the U.S.-Canada border.&lt;/p&gt;
    &lt;p&gt;And it reaches far into the interior, impacting residents of big metropolitan areas and people driving to and from large cities such as Chicago and Detroit, as well as from Los Angeles, San Antonio, and Houston to and from the Mexican border region. In one example, AP found the agency has placed at least four cameras in the greater Phoenix area over the years, one of which was more than 120 miles (193 kilometers) from the Mexican frontier, beyond the agency’s usual jurisdiction of 100 miles (161 kilometers) from a land or sea border. The AP also identified several camera locations in metropolitan Detroit, as well as one placed near the Michigan-Indiana border to capture traffic headed towards Chicago or Gary, Indiana, or other nearby destinations.&lt;/p&gt;
    &lt;p&gt;Border Patrol’s parent agency, U.S. Customs and Border Protection, said they use license plate readers to help identify threats and disrupt criminal networks and are “governed by a stringent, multi-layered policy framework, as well as federal law and constitutional protections, to ensure the technology is applied responsibly and for clearly defined security purposes.”&lt;/p&gt;
    &lt;p&gt;“For national security reasons, we do not detail the specific operational applications,” the agency said. While the U.S. Border Patrol primarily operates within 100 miles of the border, it is legally allowed “to operate anywhere in the United States,” the agency added.&lt;/p&gt;
    &lt;p&gt;While collecting license plates from cars on public roads has generally been upheld by courts, some legal scholars see the growth of large digital surveillance networks such as Border Patrol’s as raising constitutional questions. Courts have started to recognize that “large-scale surveillance technology that’s capturing everyone and everywhere at every time” might be unconstitutional under the Fourth Amendment, which protects people from unreasonable searches, said Andrew Ferguson, a law professor at George Washington University.&lt;/p&gt;
    &lt;p&gt;Today, predictive surveillance is embedded into America’s roadways. Mass surveillance techniques are also used in a range of other countries, from authoritarian governments such as China to, increasingly, democracies in the U.K. and Europe in the name of national security and public safety.&lt;/p&gt;
    &lt;p&gt;“They are collecting mass amounts of information about who people are, where they go, what they do, and who they know … engaging in dragnet surveillance of Americans on the streets, on the highways, in their cities, in their communities,” Nicole Ozer, the executive director of the Center for Constitutional Democracy at UC Law San Francisco, said in response to the AP’s findings. “These surveillance systems do not make communities safer.”&lt;/p&gt;
    &lt;head rend="h2"&gt;‘We did everything right and had nothing to hide’&lt;/head&gt;
    &lt;p&gt;In February, Lorenzo Gutierrez Lugo, a driver for a small trucking company that specializes in transporting furniture, clothing and other belongings to families in Mexico, was driving south to the border city of Brownsville, Texas, carrying packages from immigrant communities in South Carolina’s low country.&lt;/p&gt;
    &lt;p&gt;Gutierrez Lugo was pulled over by a local police officer in Kingsville, a small Texas city near Corpus Christi that lies about 100 miles from the Mexican border. The officer, Richard Beltran, cited the truck’s speed of 50 mph (80 kph) in a 45 mph (72 kph) zone as the reason for the stop.&lt;/p&gt;
    &lt;p&gt;But speeding was a pretext: Border Patrol had requested the stop and said the black Dodge pickup with a white trailer could contain contraband, according to police and court records. U.S. Route 77 passes through Kingsville, a route that state and federal authorities scrutinize for trafficking of drugs, money and people.&lt;/p&gt;
    &lt;p&gt;Gutierrez Lugo, who through a lawyer declined to comment, was interrogated about the route he drove, based on license plate reader data, per the police report and court records. He consented to a search of his car by Beltran and Border Patrol agents, who eventually arrived to assist.&lt;/p&gt;
    &lt;p&gt;They unearthed no contraband. But Beltran arrested Gutierrez Lugo on suspicion of money laundering and engaging in organized criminal activity because he was carrying thousands of dollars in cash — money his supervisor said came directly from customers in local Latino communities, who are accustomed to paying in cash. No criminal charges were ultimately brought against Gutierrez Lugo and an effort by prosecutors to seize the cash, vehicle and trailer as contraband was eventually dropped.&lt;/p&gt;
    &lt;p&gt;Luis Barrios owns the trucking company, Paquetería El Guero, that employed the driver. He told AP he hires people with work authorization in the United States and was taken aback by the treatment of his employee and his trailer.&lt;/p&gt;
    &lt;p&gt;“We did everything right and had nothing to hide, and that was ultimately what they found,” said Barrios, who estimates he spent $20,000 in legal fees to clear his driver’s name and get the trailer out of impound.&lt;/p&gt;
    &lt;p&gt;Border Patrol agents and local police have many names for these kinds of stops: “whisper,” “intel” or “wall” stops. Those stops are meant to conceal — or wall off — that the true reason for the stop is a tip from federal agents sitting miles away, watching data feeds showing who’s traveling on America’s roads and predicting who is “suspicious,” according to documents and people interviewed by the AP.&lt;/p&gt;
    &lt;p&gt;In 2022, a man from Houston had his car searched from top to bottom by Texas sheriff’s deputies outside San Antonio after they got a similar tipoff from Border Patrol agents about the driver, Alek Schott.&lt;/p&gt;
    &lt;p&gt;Federal agents observed that Schott had made an overnight trip from Houston to Carrizo Springs, Texas, and back, court records show. They knew he stayed overnight in a hotel about 80 miles (129 kilometers) from the U.S.-Mexico border. They knew that in the morning Schott met a female colleague there before they drove together to a business meeting.&lt;/p&gt;
    &lt;p&gt;At Border Patrol’s request, Schott was pulled over by Bexar County sheriff’s deputies. The deputies held Schott by the side of the road for more than an hour, searched his car and found nothing.&lt;/p&gt;
    &lt;p&gt;“The beautiful thing about the Texas Traffic Code is there’s thousands of things you can stop a vehicle for,” said Joel Babb, the sheriff’s deputy who stopped Schott’s car, in a deposition in a lawsuit Schott filed alleging violations of his constitutional rights.&lt;/p&gt;
    &lt;p&gt;According to testimony and documents released as part of Schott’s lawsuit, Babb was on a group chat with federal agents called Northwest Highway. Babb deleted the WhatsApp chat off his phone but Schott’s lawyers were able to recover some of the text messages.&lt;/p&gt;
    &lt;p&gt;Through a public records act request, the AP also obtained more than 70 pages of the Northwest Highway group chats from June and July of this year from a Texas county that had at least one sheriff’s deputy active in the chat. The AP was able to associate numerous phone numbers in both sets of documents with Border Patrol agents and Texas law enforcement officials.&lt;/p&gt;
    &lt;p&gt;The chat logs show Border Patrol agents and Texas sheriffs deputies trading tips about vehicles’ travel patterns — based on suspicions about little more than someone taking a quick trip to the border region and back. The chats show how thoroughly Texas highways are surveilled by this federal-local partnership and how much detailed information is informally shared.&lt;/p&gt;
    &lt;p&gt;In one exchange a law enforcement official included a photo of someone’s driver’s license and told the group the person, who they identified using an abbreviation for someone in the country illegally, was headed westbound. “Need BP?,” responded a group member whose number was labeled “bp Intel.” “Yes sir,” the official answered, and a Border Patrol agent was en route.&lt;/p&gt;
    &lt;p&gt;Border Patrol agents and local law enforcement shared information about U.S. citizens’ social media profiles and home addresses with each other after stopping them on the road. Chats show Border Patrol was also able to determine whether vehicles were rentals and whether drivers worked for rideshare services.&lt;/p&gt;
    &lt;p&gt;In Schott’s case, Babb testified that federal agents “actually watch travel patterns on the highway” through license plate scans and other surveillance technologies. He added: “I just know that they have a lot of toys over there on the federal side.”&lt;/p&gt;
    &lt;p&gt;After finding nothing in Schott’s car, Babb said “nine times out of 10, this is what happens,” a phrase Schott’s lawyers claimed in court filings shows the sheriff’s department finds nothing suspicious in most of its searches. Babb did not respond to multiple requests for comment from AP.&lt;/p&gt;
    &lt;p&gt;The Bexar County sheriff’s office declined to comment due to pending litigation and referred all questions about the Schott case to the county’s district attorney. The district attorney did not respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;The case is pending in federal court in Texas. Schott said in an interview with the AP: “I didn’t know it was illegal to drive in Texas.”&lt;/p&gt;
    &lt;head rend="h2"&gt;‘Patterns of life’ and license plates&lt;/head&gt;
    &lt;p&gt;Today, the deserts, forests and mountains of the nation’s land borders are dotted with checkpoints and increasingly, surveillance towers, Predator drones, thermal cameras and license plate readers, both covert and overt.&lt;/p&gt;
    &lt;p&gt;Border Patrol’s parent agency got authorization to run a domestic license plate reader program in 2017, according to a Department of Homeland Security policy document. At the time, the agency said that it might use hidden license plate readers ”for a set period of time while CBP is conducting an investigation of an area of interest or smuggling route. Once the investigation is complete, or the illicit activity has stopped in that area, the covert cameras are removed,” the document states.&lt;/p&gt;
    &lt;p&gt;But that’s not how the program has operated in practice, according to interviews, police reports and court documents. License plate readers have become a major — and in some places permanent — fixture of the border region.&lt;/p&gt;
    &lt;p&gt;In a budget request to Congress in fiscal year 2024, CBP said that its Conveyance Monitoring and Predictive Recognition System, or CMPRS, “collects license plate images and matches the processed images against established hot lists to assist … in identifying travel patterns indicative of illegal border related activities.” Several new developer jobs have been posted seeking applicants to help modernize its license plate surveillance system in recent months. Numerous Border Patrol sectors now have special intelligence units that can analyze license plate reader data, and tie commercial license plate readers to its national network, according to documents and interviews.&lt;/p&gt;
    &lt;p&gt;Border Patrol worked with other law enforcement agencies in Southern California about a decade ago to develop pattern recognition, said a former CBP official who spoke on the condition of anonymity for fear of reprisal. Over time, the agency learned to develop what it calls “patterns of life” of vehicle movements by sifting through the license plate data and determining “abnormal” routes, evaluating if drivers were purposely avoiding official checkpoints. Some cameras can take photos of a vehicle’s plates as well as its driver’s face, the official said.&lt;/p&gt;
    &lt;p&gt;Another former Border Patrol official compared it to a more technologically sophisticated version of what agents used to do in the field — develop hunches based on experience about which vehicles or routes smugglers might use, find a legal basis for the stop like speeding and pull drivers over for questioning.&lt;/p&gt;
    &lt;p&gt;The cameras take pictures of vehicle license plates. Then, the photos are “read” by the system, which automatically detects and distills the images into numbers and letters, tied to a geographic location, former CBP officials said. The AP could not determine how precisely the system’s algorithm defines a quick turnaround or an odd route. Over time, the agency has amassed databases replete with images of license plates, and the system’s algorithm can flag an unusual “pattern of life” for human inspection.&lt;/p&gt;
    &lt;p&gt;The Border Patrol also has access to a nationwide network of plate readers run by the Drug Enforcement Administration, documents show, and was authorized in 2020 to access license plate reader systems sold by private companies. In documents obtained by the AP, a Border Patrol official boasted about being able to see that a vehicle that had traveled to “Dallas, Little Rock, Arkansas and Atlanta” before ending up south of San Antonio.&lt;/p&gt;
    &lt;p&gt;Documents show that Border Patrol or CBP has in the past had access to data from at least three private sector vendors: Rekor, Vigilant Solutions and Flock Safety.&lt;/p&gt;
    &lt;p&gt;Through Flock alone, Border Patrol for a time had access to at least 1,600 license plate readers across 22 states, and some counties have reported looking up license plates on behalf of CBP even in states like California and Illinois that ban sharing data with federal immigration authorities, according to an AP analysis of police disclosures. A Flock spokesperson told AP the company “for now” had paused its pilot programs with CBP and a separate DHS agency, Homeland Security Investigations, and declined to discuss the type or volume of data shared with either federal agency, other than to say agencies could search for vehicles wanted in conjunction with a crime. No agencies currently list Border Patrol as receiving Flock data. Vigilant and Rekor did not respond to requests for comment.&lt;/p&gt;
    &lt;p&gt;Also from AP’s investigation into the use of surveillance technology:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;U.S. tech firms to a large degree designed and built China’s surveillance state, playing a far greater role in enabling rights abuses than known before.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Across five Republican and Democratic administrations, the U.S. government has repeatedly allowed and even actively helped American firms to sell technology to Chinese police.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Where Border Patrol places its cameras is a closely guarded secret. However, through public records requests, the AP obtained dozens of permits the agency filed with Arizona and Michigan for permission to place cameras on state-owned land. The permits show the agency frequently disguises its cameras by concealing them in traffic equipment like the yellow and orange barrels that dot American roadways, or by labeling them as jobsite equipment. An AP photographer in October visited the locations identified in more than two dozen permit applications in Arizona, finding that most of the Border Patrol’s hidden equipment remains in place today. Spokespeople for the Arizona and Michigan departments of transportation said they approve permits based on whether they follow state and federal rules and are not privy to details on how license plate readers are used.&lt;/p&gt;
    &lt;p&gt;Texas, California, and other border states did not provide documents in response to the AP’s public records requests.&lt;/p&gt;
    &lt;p&gt;CBP’s attorneys and personnel instructed local cities and counties in both Arizona and Texas to withhold records from the AP that might have revealed details about the program’s operations, even though they were requested under state open records laws, according to emails and legal briefs filed with state governments. For example, CBP claimed records requested by the AP in Texas “would permit private citizens to anticipate weaknesses in a police department, avoid detection, jeopardize officer safety, and generally undermine police efforts.” Michigan redacted the exact locations of Border Patrol equipment, but the AP was able to determine general locations from the name of the county.&lt;/p&gt;
    &lt;p&gt;One page of the group chats obtained by the AP shows that a participant enabled WhatsApp’s disappearing messages feature to ensure communications were deleted automatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transformation of CBP into intelligence agency&lt;/head&gt;
    &lt;p&gt;The Border Patrol’s license plate reader program is just one part of a steady transformation of its parent agency, CBP, in the years since 9/11 into an intelligence operation whose reach extends far beyond borders, according to interviews with former officials.&lt;/p&gt;
    &lt;p&gt;CBP has quietly amassed access to far more information from ports of entry, airports and intelligence centers than other local, state and federal law enforcement agencies. And like a domestic spy agency, CBP has mostly hidden its role in the dissemination of intelligence on purely domestic travel through its use of whisper stops.&lt;/p&gt;
    &lt;p&gt;Border Patrol has also extended the reach of its license plate surveillance program by paying for local law enforcement to run plate readers on their behalf.&lt;/p&gt;
    &lt;p&gt;A federal grant program called Operation Stonegarden, which has existed in some form for nearly two decades, has handed out hundreds of millions of dollars to buy automated license plate readers, camera-equipped drones and other surveillance gear for local police and sheriffs agencies. Stonegarden grant funds also pay for local law enforcement overtime, which deputizes local officers to work on Border Patrol enforcement priorities. Under President Donald Trump, the Republican-led Congress this year allocated $450 million for Stonegarden to be handed out over the next four fiscal years. In the previous four fiscal years, the program gave out $342 million.&lt;/p&gt;
    &lt;p&gt;In Cochise County, Arizona, Sheriff Mark Dannels said Stonegarden grants, which have been used to buy plate readers and pay for overtime, have let his deputies merge their mission with Border Patrol’s to prioritize border security.&lt;/p&gt;
    &lt;p&gt;“If we’re sharing our authorities, we can put some consequences behind, or deterrence behind, ‘Don’t come here,’” he said.&lt;/p&gt;
    &lt;p&gt;In 2021, the Ward County, Texas, sheriff sought grant funding from DHS to buy a “covert, mobile, License Plate Reader” to pipe data to Border Patrol’s Big Bend Sector Intelligence Unit. The sheriff’s department did not respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;Other documents AP obtained show that Border Patrol connects locally owned and operated license plate readers bought through Stonegarden grants to its computer systems, vastly increasing the federal agency’s surveillance network.&lt;/p&gt;
    &lt;p&gt;How many people have been caught up in the Border Patrol’s dragnet is unknown. One former Border Patrol agent who worked on the license plate reader pattern detection program in California said the program had an 85% success rate of discovering contraband once he learned to identify patterns that looked suspicious. But another former official in a different Border Patrol sector said he was unaware of successful interdictions based solely on license plate patterns.&lt;/p&gt;
    &lt;p&gt;In Trump’s second term, Border Patrol has extended its reach and power as border crossings have slowed to historic lows and freed up agents for operations in the heartland. Border Patrol Sector Chief Gregory Bovino, for example, was tapped to direct hundreds of agents from multiple DHS agencies in the administration’s immigration sweeps across Los Angeles, more than 150 miles (241 kilometers) from his office in El Centro, California. Bovino later was elevated to lead the aggressive immigration crackdown in Chicago. Numerous Border Patrol officials have also been tapped to replace ICE leadership.&lt;/p&gt;
    &lt;p&gt;The result has been more encounters between the agency and the general public than ever before.&lt;/p&gt;
    &lt;p&gt;“We took Alek’s case because it was a clear-cut example of an unconstitutional traffic stop,” said Christie Hebert, who works at the nonprofit public interest law firm Institute for Justice and represents Schott. ”What we found was something much larger — a system of mass surveillance that threatens people’s freedom of movement.”&lt;/p&gt;
    &lt;p&gt;AP found numerous other examples similar to what Schott and the delivery driver experienced in reviewing court records in border communities and along known smuggling routes in Texas and California. Several police reports and court records the AP examined cite “suspicious” travel patterns or vague tipoffs from the Border Patrol or other unnamed law enforcement agencies. In another federal court document filed in California, a Border Patrol agent acknowledged “conducting targeted analysis on vehicles exhibiting suspicious travel patterns” as the reason he singled out a Nissan Altima traveling near San Diego.&lt;/p&gt;
    &lt;p&gt;In cases reviewed by the AP, local law enforcement sometimes tried to conceal the role the Border Patrol plays in passing along intelligence. Babb, the deputy who stopped Schott, testified he typically uses the phrase “subsequent to prior knowledge” when describing whisper stops in his police reports to acknowledge that the tip came from another law enforcement agency without revealing too much in written documents he writes memorializing motorist encounters.&lt;/p&gt;
    &lt;p&gt;Once they pull over a vehicle deemed suspicious, officers often aggressively question drivers about their travels, their belongings, their jobs, how they know the passengers in the car, and much more, police records and bodyworn camera footage obtained by the AP show. One Texas officer demanded details from a man about where he met his current sexual partner. Often drivers, such as the one working for the South Carolina moving company, were arrested on suspicion of money laundering merely for carrying a few thousand dollars worth of cash, with no apparent connection to illegal activity. Prosecutors filed lawsuits to try to seize money or vehicles on the suspicion they were linked to trafficking.&lt;/p&gt;
    &lt;p&gt;Schott warns that for every success story touted by Border Patrol, there are far more innocent people who don’t realize they’ve become ensnared in a technology-driven enforcement operation.&lt;/p&gt;
    &lt;p&gt;“I assume for every one person like me, who’s actually standing up, there’s a thousand people who just don’t have the means or the time or, you know, they just leave frustrated and angry. They don’t have the ability to move forward and hold anyone accountable,” Schott said. “I think there’s thousands of people getting treated this way.”&lt;/p&gt;
    &lt;p&gt;—-&lt;/p&gt;
    &lt;p&gt;Tau reported from Washington, Laredo, San Antonio, Kingsville and Victoria, Texas. Burke reported from San Francisco. AP writers Aaron Kessler in Washington, Jim Vertuno in San Antonio, AP video producer Serginho Roosblad in Bisbee, Arizona, and AP photographers Ross D. Franklin in Phoenix and David Goldman in Houston contributed reporting. Ismael M. Belkoura in Washington also contributed.&lt;/p&gt;
    &lt;p&gt;—-&lt;/p&gt;
    &lt;p&gt;Contact AP’s global investigative team at [email protected] or https://www.ap.org/tips/.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://apnews.com/article/immigration-border-patrol-surveillance-drivers-ice-trump-9f5d05469ce8c629d6fecf32d32098cd"/><published>2025-11-20T19:52:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45997212</id><title>New OS aims to provide (some) compatibility with macOS</title><updated>2025-11-21T09:38:56.418252+00:00</updated><content>&lt;doc fingerprint="c7963a51c657afa6"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Don't speak English? Read this in: Italiano, Türkçe, Deutsch, Indonesia, 简体中文, 繁體中文, Português do Brasil, 한국어, فارسی, Magyar&lt;/head&gt;
    &lt;p&gt;ravynOS is a new open source OS project that aims to provide a similar experience and some compatibility with macOS on x86-64 (and eventually ARM) systems. It builds on the solid foundations of FreeBSD, existing open source packages in the same space, and new code to fill the gaps.&lt;/p&gt;
    &lt;p&gt;The main design goals are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Source compatibility with macOS applications (i.e. you could compile a Mac application on ravynOS and run it)&lt;/item&gt;
      &lt;item&gt;Similar GUI metaphors and familiar UX (file manager, application launcher, top menu bar that reflects the open application, etc)&lt;/item&gt;
      &lt;item&gt;Compatible with macOS folder layouts (/Library, /System, /Users, /Volumes, etc) and perhaps filesystems (HFS+, APFS) as well as fully supporting ZFS&lt;/item&gt;
      &lt;item&gt;Self-contained applications in App Bundles, AppDirs, and AppImage files - an installer-less experience for /Applications&lt;/item&gt;
      &lt;item&gt;Mostly maintain compatibility with the FreeBSD base system and X11 - a standard Unix environment under the hood&lt;/item&gt;
      &lt;item&gt;Compatible with Linux binaries via FreeBSD's Linux support&lt;/item&gt;
      &lt;item&gt;Eventual compatibility with x86-64/arm64 macOS binaries (Mach-O) and libraries&lt;/item&gt;
      &lt;item&gt;Pleasant to use, secure, stable, and performant&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please visit ravynos.com for more info: Release Notes | Screenshots | FAQ&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can you help build the dream? See the current projects/needs in CONTRIBUTING.md!&lt;/item&gt;
      &lt;item&gt;Our Discord server.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;#ravynOS-general:matrix.org&lt;/code&gt;- join via Element.io&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is the top level of the FreeBSD source directory.&lt;/p&gt;
    &lt;p&gt;FreeBSD is an operating system used to power modern servers, desktops, and embedded platforms. A large community has continually developed it for more than thirty years. Its advanced networking, security, and storage features have made FreeBSD the platform of choice for many of the busiest web sites and most pervasive embedded networking and storage devices.&lt;/p&gt;
    &lt;p&gt;For copyright information, please see the file COPYRIGHT in this directory. Additional copyright information also exists for some sources in this tree - please see the specific source directories for more information.&lt;/p&gt;
    &lt;p&gt;The Makefile in this directory supports a number of targets for building components (or all) of the FreeBSD source tree. See build(7), config(8), FreeBSD handbook on building userland, and Handbook for kernels for more information, including setting make(1) variables.&lt;/p&gt;
    &lt;p&gt;For information on the CPU architectures and platforms supported by FreeBSD, see the FreeBSD website's Platforms page.&lt;/p&gt;
    &lt;p&gt;For official FreeBSD bootable images, see the release page.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Directory&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bin&lt;/cell&gt;
        &lt;cell&gt;System/user commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;cddl&lt;/cell&gt;
        &lt;cell&gt;Various commands and libraries under the Common Development and Distribution License.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;contrib&lt;/cell&gt;
        &lt;cell&gt;Packages contributed by 3rd parties.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;crypto&lt;/cell&gt;
        &lt;cell&gt;Cryptography stuff (see crypto/README).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;etc&lt;/cell&gt;
        &lt;cell&gt;Template files for /etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gnu&lt;/cell&gt;
        &lt;cell&gt;Commands and libraries under the GNU General Public License (GPL) or Lesser General Public License (LGPL). Please see gnu/COPYING and gnu/COPYING.LIB for more information.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;include&lt;/cell&gt;
        &lt;cell&gt;System include files.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;kerberos5&lt;/cell&gt;
        &lt;cell&gt;Kerberos5 (Heimdal) package.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;lib&lt;/cell&gt;
        &lt;cell&gt;System libraries.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;libexec&lt;/cell&gt;
        &lt;cell&gt;System daemons.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;release&lt;/cell&gt;
        &lt;cell&gt;Release building Makefile &amp;amp; associated tools.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;rescue&lt;/cell&gt;
        &lt;cell&gt;Build system for statically linked /rescue utilities.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sbin&lt;/cell&gt;
        &lt;cell&gt;System commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;secure&lt;/cell&gt;
        &lt;cell&gt;Cryptographic libraries and commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;share&lt;/cell&gt;
        &lt;cell&gt;Shared resources.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stand&lt;/cell&gt;
        &lt;cell&gt;Boot loader sources.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sys&lt;/cell&gt;
        &lt;cell&gt;Kernel sources (see sys/README.md).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;targets&lt;/cell&gt;
        &lt;cell&gt;Support for experimental &lt;code&gt;DIRDEPS_BUILD&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tests&lt;/cell&gt;
        &lt;cell&gt;Regression tests which can be run by Kyua. See tests/README for additional information.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;tools&lt;/cell&gt;
        &lt;cell&gt;Utilities for regression testing and miscellaneous tasks.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;usr.bin&lt;/cell&gt;
        &lt;cell&gt;User commands.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;usr.sbin&lt;/cell&gt;
        &lt;cell&gt;System administration commands.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For information on synchronizing your source tree with one or more of the FreeBSD Project's development branches, please see FreeBSD Handbook.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ravynsoft/ravynos"/><published>2025-11-20T20:24:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45997914</id><title>New Glenn Update</title><updated>2025-11-21T09:38:56.214385+00:00</updated><content/><link href="https://www.blueorigin.com/news/new-glenn-upgraded-engines-subcooled-components-drive-enhanced-performance"/><published>2025-11-20T21:21:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45998047</id><title>GitHut – Programming Languages and GitHub (2014)</title><updated>2025-11-21T09:38:56.063617+00:00</updated><content>&lt;doc fingerprint="cd0138c983fa12d1"&gt;
  &lt;main&gt;
    &lt;p&gt; GitHut is an attempt to visualize and explore the complexity of the universe of programming languages used across the repositories hosted on GitHub.&lt;lb/&gt; Programming languages are not simply the tool developers use to create programs or express algorithms but also instruments to code and decode creativity. By observing the history of languages we can enjoy the quest of human kind for a better way to solve problems, to facilitate collaboration between people and to reuse the effort of others.&lt;lb/&gt; GitHub is the largest code host in the world, with 3.4 million users. It's the place where the open-source development community offers access to most of its projects. By analyzing how languages are used in GitHub it is possible to understand the popularity of programming languages among developers and also to discover the unique characteristics of each language. &lt;/p&gt;
    &lt;p&gt; GitHub provides publicly available API to interact with its huge dataset of events and interaction with the hosted repositories.&lt;lb/&gt; GitHub Archive takes this data a step further by aggregating and storing it for public consumption. GitHub Archive dataset is also available via Google BigQuery. &lt;lb/&gt; The quantitative data used in GitHut is collected from GitHub Archive. The data is updated on a quarterly basis.&lt;lb/&gt; An additional note about the data is about the large amount of records in which the programming language is not specified. This particular characteristic is extremely evident for the Create Events (of repository), therefore it is not possible to visualize the trending language in terms of newly created repositories. For this reason the Activity value (in terms of number of changes pushed) has been considered the best metric for the popularity of programming languages. &lt;lb/&gt; The release year of the programming language is based on the table Timeline of programming languages from Wikipedia. &lt;lb/&gt; For more information on the methodology of the data collection check-out the publicly available GitHub repository of GitHut. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://githut.info/"/><published>2025-11-20T21:33:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45999038</id><title>Over-regulation is doubling the cost</title><updated>2025-11-21T09:38:55.687784+00:00</updated><content>&lt;doc fingerprint="698010174ea1b620"&gt;
  &lt;main&gt;
    &lt;p&gt;After building a software company to a multi-billion dollar exit, I made the jump to hardware. Now I’m working on carbon removal + steel at Charm Industrial, and electric long-haul trucking with Revoy. It’s epically fun to be building in the real world, but little did I expect that more than half the cost of building a hardware company would come from regulatory bottlenecks. Despite a huge push for climate fixes and the bipartisan geopolitical desire to bring industry back to the USA, I’ve been shocked to find that the single biggest barrier—by far—is over-regulation from the massive depth of bureaucracy.&lt;/p&gt;
    &lt;p&gt;Hardtech companies of all flavors are being forced to burn through limited capital while they wait for regulatory clarity and/or permits. This creates a constant cycle of cost increases that ultimately flows to consumers, it lowers investment in the US manufacturing and industrial base, it delays innovative new hardware getting into the hands of consumers and businesses, and at the end of the day, it leaves us all worse off, stuck with a quality of life pegged to technology developed decades ago.&lt;/p&gt;
    &lt;p&gt;Regulatory delays and bottlenecks have added millions of pounds of pollutants like PM2.5, NOₓ and CO₂ to our air from the continuation of business as usual, instead of the deployment of clean technologies from my two hardtech efforts alone. While CO₂ is a long-term climate issue, PM2.5 and NOₓ are immediate major drivers of asthma and excess morbidity. Both operations have high bipartisan appeal—and we’ve never been denied a permit—because we’re fundamentally cleaning up things that matter to everyone: dirty air, wildfires, orphaned oil wells. Revoy is also helping deflate the cost of long-haul freight. But none of that has made getting freedom to operate easy. For creative new technologies the default answer is “no” because there isn’t a clear path to permitting at all, and figuring out that path itself takes years — time that startups can’t afford to wait.&lt;/p&gt;
    &lt;p&gt;Regulation obviously has a critical role in protecting people and the environment, but the sheer volume, over-specificity and sometimes ambiguity of those same regulations is now actively working against those goals! We’re unintentionally blocking the very things that would improve our environment. We’ve become a society that blocks all things, and we need to be a society that builds great things every day. The rest of this article gets very specific about the astronomical costs regulations are imposing on us as a society, and the massive positive impact that could be unleashed by cutting back regulation that is working against new, cost-saving, creative technology that could also be making people and the environment healthy again.&lt;/p&gt;
    &lt;p&gt;To make it concrete: both Charm and Revoy are capital-efficient hardtech companies, but Charm will spend low hundreds of millions to get to breakeven, and Revoy will spend tens of millions. In both cases, more than half of the total cost of building each company has gone to counterproductive regulatory burden. I’m hellbent on pushing through these barriers, but the unspoken reality is that our regulatory morass is the deathbed of thousands of hardtech companies that could be drastically improving our lives. We must unleash them.&lt;/p&gt;
    &lt;head rend="h2"&gt;$300M in Societal Cost &amp;amp; $125M in Burden for Charm&lt;/head&gt;
    &lt;p&gt;Charm produces and delivers verified carbon removal to companies like Google, Microsoft and JPMorgan. Charm’s breakthrough was realizing that you could take CO₂ captured in farm &amp;amp; forestry plant residues, convert it into a carbon-rich, BBQ sauce-like liquid (it’s literally the smoke flavor in BBQ sauce), and inject it into old oil wells to permanently remove carbon from the atmosphere. This has all kinds of co-benefits like reducing the massive overburden of wildfire fuels, cleaning up &amp;amp; plugging nasty orphaned oil wells, and improving PM2.5 and NOₓ air quality by avoiding that biomass being burned instead.&lt;/p&gt;
    &lt;p&gt;And yet… there was a hangup: what kind of injection well is this? Should it be permitted as a Class I disposal, Class II oilfield disposal, or Class V experimental? This question on permitting path took four years to answer. Four years to decide which path to use, not even the actual permit! It took this long because regulators are structurally faced with no upside, only downside legal risk in taking a formal position on something new. Even when we’d done an enormous amount of lab and field work with bio-oil to understand its safety and behavior at surface and subsurface conditions. A regulator faces little cost to moving incredibly cautiously, but a major cost if they approve something that triggers activist pushback.&lt;/p&gt;
    &lt;p&gt;In the end, we’re grateful that—eventually—a state regulator took the reins and reviewed, managed, and issued the first-ever Class V bio-oil sequestration permit, through what was still an incredibly complex and detailed 14-month review process.&lt;/p&gt;
    &lt;p&gt;Now imagine that, instead of the 5.5 years from first contact to issued permit, it had only taken the 6 months it actually required to get everyone across the regulatory establishment to agree on a Class V pathway, we would have had 5 additional years operating the well. That’s the equivalent, from our real supply chain, of sinking at least 30,000 tonnes of carbon per year at $600/tonne. Looking only at this one aspect, this delay came with a $90M price tag for Charm. We’ve also spent untold millions on regulatory affairs at all levels of government, not to mention the missed acceleration in sales, and other direct hard costs spent in R&amp;amp;D and processing bio-oil for inefficient and expensive injection into salt caverns instead.&lt;/p&gt;
    &lt;p&gt;But the public health burden created by this regulatory slowness is where it gets really crazy. This one regulatory delay meant we all got subjected to decreased air quality from an additional 30,000 tonnes per year of pile burning. The resulting particulate emissions alone are estimated to have caused a mindblowing $40m/year in healthcare costs. This is $200M in additional healthcare burden over those five years, mostly borne by Medicare and Medicaid. There are additional costs to NOₓ emissions and more that take it to $300M.&lt;/p&gt;
    &lt;p&gt;In total, the total cost to society of this single regulatory delay will be about $400M: $120-150M of unnecessary cost to Charm, and the bulk of it—$300M or so—borne by the public in healthcare costs. I’m not sharing these numbers to complain or make excuses; Charm is still on the path to having a huge impact and we’re among the lucky few that can survive these delays. What pains me most is the 5 years of lost carbon removal and pollutant reduction, and the compounding effect that has on all our health and healthcare costs. Over-regulation is now working against the very things it’s intended to protect.&lt;/p&gt;
    &lt;p&gt;Regulators do their absolute best with the system they have, but the combined effects of: (1) extremely detailed and complex regulation, (2) chaotic budgets and understaffing that disrupt an efficient process, and (3) endless lawsuits against regulators since 1970s-era Naderism have created an atmosphere of fear. If we want to solve the climate crisis, build abundance, lower costs, and generate wealth for all, this has to change. We need to delete and simplify reams of regulations. We need to pay regulators well, and we need to trust our regulators to operate quickly and decisively by putting reasonable limits on endless activist legal challenges.&lt;/p&gt;
    &lt;head rend="h2"&gt;&amp;gt;$25M in Unnecessary Burden for Revoy&lt;/head&gt;
    &lt;p&gt;Revoy’s breakthrough was realizing that you could lower long-haul freight costs and electrify long-haul semi trucks by leaving the diesel tractor in place and dropping an electric powertrain onto the back of the semi. Today, we boost semis from 7 mpg to 120 mpg, driving a 94% reduction in fuel consumption. This slashes emissions that negatively impact both air quality and climate.&lt;/p&gt;
    &lt;p&gt;And yet again… a hangup: what exactly is this electric doohickey? Is it a truck? A trailer? Something else? It was clear from the regulations that it was a “converter dolly”. But getting complete alignment on that simple fact across an alphabet soup of government agencies spanning both federal and state—NHTSA, FMCSA, FHWA, state transit authorities, air quality management districts, state DMVs, highway patrols and more—took years.&lt;/p&gt;
    &lt;p&gt;A “powered converter dolly” isn’t even a new thing! Here’s one from the sixties that ran on diesel to help trucks get over mountain passes:&lt;/p&gt;
    &lt;p&gt;There were some bright spots. The Federal Motor Carrier Safety Administration (FMCSA) and the National Highway Transportation Safety Administration (NHTSA) quickly converged on informal definitional clarity, and then eventually a Highway Patrol Captain who was eager to get innovative electric vehicles on the road pushed it through with a state DMV to register the first four Revoys. But bringing along the rest of the agencies, and the rest of the states, was not fast. It delayed deployments, soaked up hundreds of thousands of dollars of legal and lobbyist time (not to mention all the corresponding time on the government side that all of us taxpayers have to bear), and maybe most importantly… even with a formal memo from the Federal DOT, it is still not 100% resolved in some states.&lt;/p&gt;
    &lt;p&gt;As one example, one state agency has asked Revoy to do certified engine testing to prove that the Revoy doesn’t increase emissions of semi trucks. And that Revoy must do this certification across every single truck engine family. It costs $100,000 per certification and there are more than 270 engine families for the 9 engines that our initial partners use. That’s $27,000,000 for this one regulatory item. And keep in mind that this is to certify that a device—whose sole reason for existence is to cut pollution by &amp;gt;90%, and which has demonstrably done so across nearly 100,000 miles of testing and operations—is not increasing the emissions of the truck. It’s a complete waste of money for everyone.&lt;/p&gt;
    &lt;p&gt;And that $27M dollar cost doesn’t include the cost to society. This over-regulation will delay deployment of EV trucks by years, increasing NOₓ and PM 2.5 air pollution exposure for many of society’s least well-off who live near freeways. The delayed deployment will also increase CO₂ emissions that threaten the climate and environment. Revoy’s Founder (Ian Rust) and I actually disagree on what exactly it is about the regulatory environment that needs to change, but we agree it’s completely broken and hurting both people and the planet.&lt;/p&gt;
    &lt;p&gt;In every interaction I have with regulators, I’m reminded that they’re good people doing god’s work operating in a fundamentally broken system. A regulatory system that structurally insists on legalistic, ultra-extreme caution is bound to generate a massive negative return for society.&lt;/p&gt;
    &lt;p&gt;If we had a regulatory system that could move fast to experiment with creative new technologies, we’d live in a world where our environment gets cleaned up faster, where awesome new hardware was constantly improving our lives by making things better and cheaper, and where large-scale hardtech innovation happened here at home in the USA, not in China.&lt;/p&gt;
    &lt;p&gt;As we collectively work to build more manufacturing capacity at home and build the next wave of technologies to power the economy, we need to grapple with the real bottlenecks holding us back. I hope other hardtech founders will publicly share more of their stories as well (the stories I’ve heard in private would shock you). Props to Blake Scholl for doing so.&lt;/p&gt;
    &lt;p&gt;We need a come-to-jesus about regulatory limits, timelines, and scope. Yes, we need basic and strong protections for clear harms, but we need to unleash every hardworking American, not just a few companies with massive funding, to invent and build hardware again. We need to combine many approaches to get there: expedited reviews for new technology, freedom to operate by default, permits by right-not-process, deleting as many regulatory steps as possible, and more. CA YIMBY’s successful push to pass a deluge of housing acceleration laws in the past two years could serve as a model. America building things again is the foundation of a prosperous, powerful, and clean America.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rein.pk/over-regulation-is-doubling-the-cost"/><published>2025-11-20T22:58:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46000303</id><title>Measuring Latency (2015)</title><updated>2025-11-21T09:38:55.021006+00:00</updated><content>&lt;doc fingerprint="6188c7d7315406b5"&gt;
  &lt;main&gt;&lt;p&gt;Okay, maybe not everything you know about latency is wrong. But now that I have your attention, we can talk about why the tools and methodologies you use to measure and reason about latency are likely horribly flawed. In fact, they’re not just flawed, they’re probably lying to your face.&lt;/p&gt;&lt;p&gt;When I went to Strange Loop in September, I attended a workshop called “Understanding Latency and Application Responsiveness” by Gil Tene. Gil is the CTO of Azul Systems, which is most renowned for its C4 pauseless garbage collector and associated Zing Java runtime. While the workshop was four and a half hours long, Gil also gave a 40-minute talk called “How NOT to Measure Latency” which was basically an abbreviated, less interactive version of the workshop. If you ever get the opportunity to see Gil speak or attend his workshop, I recommend you do. At the very least, do yourself a favor and watch one of his recorded talks or find his slide decks online.&lt;/p&gt;&lt;p&gt;The remainder of this post is primarily a summarization of that talk. You may not get anything out of it that you wouldn’t get out of the talk, but I think it can be helpful to absorb some of these ideas in written form. Plus, for my own benefit, writing about them helps solidify it in my head.&lt;/p&gt;&lt;head rend="h3"&gt;What is Latency?&lt;/head&gt;&lt;p&gt;Latency is defined as the time it took one operation to happen. This means every operation has its own latency—with one million operations there are one million latencies. As a result, latency cannot be measured as work units / time. What we’re interested in is how latency behaves. To do this meaningfully, we must describe the complete distribution of latencies. Latency almost never follows a normal, Gaussian, or Poisson distribution, so looking at averages, medians, and even standard deviations is useless.&lt;/p&gt;&lt;p&gt;Latency tends to be heavily multi-modal, and part of this is attributed to “hiccups” in response time. Hiccups resemble periodic freezes and can be due to any number of reasons—GC pauses, hypervisor pauses, context switches, interrupts, database reindexing, cache buffer flushes to disk, etc. These hiccups never resemble normal distributions and the shift between modes is often rapid and eclectic.&lt;/p&gt;&lt;p&gt;How do we meaningfully describe the distribution of latencies? We have to look at percentiles, but it’s even more nuanced than this. A trap that many people fall into is fixating on “the common case.” The problem with this is that there is a lot more to latency behavior than the common case. Not only that, but the “common” case is likely not as common as you think.&lt;/p&gt;&lt;p&gt;This is partly a tooling problem. Many of the tools we use do not do a good job of capturing and representing this data. For example, the majority of latency graphs produced by Grafana, such as the one below, are basically worthless. We like to look at pretty charts, and by plotting what’s convenient we get a nice colorful graph which is quite readable. Only looking at the 95th percentile is what you do when you want to hide all the bad stuff. As Gil describes, it’s a “marketing system.” Whether it’s the CTO, potential customers, or engineers—someone’s getting duped. Furthermore, averaging percentiles is mathematically absurd. To conserve space, we often keep the summaries and throw away the data, but the “average of the 95th percentile” is a meaningless statement. You cannot average percentiles, yet note the labels in most of your Grafana charts. Unfortunately, it only gets worse from here.&lt;/p&gt;&lt;p&gt;Gil says, “The number one indicator you should never get rid of is the maximum value. That is not noise, that is the signal. The rest of it is noise.” To this point, someone in the workshop naturally responded with “But what if the max is just something like a VM restarting? That doesn’t describe the behavior of the system. It’s just an unfortunate, unlikely occurrence.” By ignoring the maximum, you’re effectively saying “this doesn’t happen.” If you can identify the cause as noise, you’re okay, but if you’re not capturing that data, you have no idea of what’s actually happening.&lt;/p&gt;&lt;head rend="h3"&gt;How Many Nines?&lt;/head&gt;&lt;p&gt;But how many “nines” do I really need to look at? The 99th percentile, by definition, is the latency below which 99% of the observations may be found. Is the 99th percentile rare? If we have a single search engine node, a single key-value store node, a single database node, or a single CDN node, what is the chance we actually hit the 99th percentile?&lt;/p&gt;&lt;p&gt;Gil describes some real-world data he collected which shows how many of the web pages we go to actually experience the 99th percentile, displayed in table below. The second column counts the number of HTTP requests generated by a single access of the web page. The third column shows the likelihood of one access experiencing the 99th percentile. With the exception of google.com, every page has a probability of 50% or higher of seeing the 99th percentile.&lt;/p&gt;&lt;p&gt;The point Gil makes is that the 99th percentile is what most of your web pages will see. It’s not “rare.”&lt;/p&gt;&lt;p&gt;What metric is more representative of user experience? We know it’s not the average or the median. 95th percentile? 99.9th percentile? Gil walks through a simple, hypothetical example: a typical user session involves five page loads, averaging 40 resources per page. How many users will not experience something worse than the 95th percentile? 0.003%. By looking at the 95th percentile, you’re looking at a number which is relevant to 0.003% of your users. This means 99.997% of your users are going to see worse than this number, so why are you even looking at it?&lt;/p&gt;&lt;p&gt;On the flip side, 18% of your users are going to experience a response time worse than the 99.9th percentile, meaning 82% of users will experience the 99.9th percentile or better. Going further, more than 95% of users will experience the 99.97th percentile and more than 99% of users will experience the 99.995th percentile.&lt;/p&gt;&lt;p&gt;The median is the number that 99.9999999999% of response times will be worse than. This is why median latency is irrelevant. People often describe “typical” response time using a median, but the median just describes what everything will be worse than. It’s also the most commonly used metric.&lt;/p&gt;&lt;p&gt;If it’s so critical that we look at a lot of nines (and it is), why do most monitoring systems stop at the 95th or 99th percentile? The answer is simply because “it’s hard!” The data collected by most monitoring systems is usually summarized in small, five or ten second windows. This, combined with the fact that we can’t average percentiles or derive five nines from a bunch of small samples of percentiles means there’s no way to know what the 99.999th percentile for the minute or hour was. We end up throwing away a lot of good data and losing fidelity.&lt;/p&gt;&lt;head rend="h3"&gt;A Coordinated Conspiracy&lt;/head&gt;&lt;p&gt;Benchmarking is hard. Almost all latency benchmarks are broken because almost all benchmarking tools are broken. The number one cause of problems in benchmarks is something called “coordinated omission,” which Gil refers to as “a conspiracy we’re all a part of” because it’s everywhere. Almost all load generators have this problem.&lt;/p&gt;&lt;p&gt;We can look at a common load-testing example to see how this problem manifests. With this type of test, a client generally issues requests at a certain rate, measures the response time for each request, and puts them in buckets from which we can study percentiles later.&lt;/p&gt;&lt;p&gt;The problem is what if the thing being measured took longer than the time it would have taken before sending the next thing? What if you’re sending something every second, but this particular thing took 1.5 seconds? You wait before you send the next one, but by doing this, you avoided measuring something when the system was problematic. You’ve coordinated with it by backing off and not measuring when things were bad. To remain accurate, this method of measuring only works if all responses fit within an expected interval.&lt;/p&gt;&lt;p&gt;Coordinated omission also occurs in monitoring code. The way we typically measure something is by recording the time before, running the thing, then recording the time after and looking at the delta. We put the deltas in stats buckets and calculate percentiles from that. The code below is taken from a Cassandra benchmark.&lt;/p&gt;&lt;p&gt;However, if the system experiences one of the “hiccups” described earlier, you will only have one bad operation and 10,000 other operations waiting in line. When those 10,000 other things go through, they will look really good when in reality the experience was really bad. Long operations only get measured once, and delays outside the timing window don’t get measured at all.&lt;/p&gt;&lt;p&gt;In both of these examples, we’re omitting data that looks bad on a very selective basis, but just how much of an impact can this have on benchmark results? It turns out the impact is huge.&lt;/p&gt;&lt;p&gt;Imagine a “perfect” system which processes 100 requests/second at exactly 1 ms per request. Now consider what happens when we freeze the system (for example, using CTRL+Z) after 100 seconds of perfect operation for 100 seconds and repeat. We can intuitively characterize this system:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The average over the first 100 seconds is 1 ms.&lt;/item&gt;&lt;item&gt;The average over the next 100 seconds is 50 seconds.&lt;/item&gt;&lt;item&gt;The average over the 200 seconds is 25 seconds.&lt;/item&gt;&lt;item&gt;The 50th percentile is 1 ms.&lt;/item&gt;&lt;item&gt;The 75th percentile is 50 seconds.&lt;/item&gt;&lt;item&gt;The 99.99th percentile is 100 seconds.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Now we try measuring the system using a load generator. Before freezing, we run 100 seconds at 100 requests/second for a total of 10,000 requests at 1 ms each. After the stall, we get one result of 100 seconds. This is the entirety of our data, and when we do the math, we get these results:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The average over the 200 seconds is 10.9 ms (should be 25 seconds).&lt;/item&gt;&lt;item&gt;The 50th percentile is 1 ms.&lt;/item&gt;&lt;item&gt;The 75th percentile is 1 ms (should be 50 seconds).&lt;/item&gt;&lt;item&gt;The 99.99th percentile is 1 ms (should be 100 seconds).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Basically, your load generator and monitoring code tell you the system is ready for production, when in fact it’s lying to you! A simple “CTRL+Z” test can catch coordinated omission, but people rarely do it. It’s critical to calibrate your system this way. If you find it giving you these kind of results, throw away all the numbers—they’re worthless.&lt;/p&gt;&lt;p&gt;You have to measure at random or “fair” rates. If you measure 10,000 things in the first 100 seconds, you have to measure 10,000 things in the second 100 seconds during the stall. If you do this, you’ll get the correct numbers, but they won’t be as pretty. Coordinated omission is the simple act of erasing, ignoring, or missing all the “bad” stuff, but the data is good.&lt;/p&gt;&lt;p&gt;Surely this data can still be useful though, even if it doesn’t accurately represent the system? For example, we can still use it to identify performance regressions or validate improvements, right? Sadly, this couldn’t be further from the truth. To see why, imagine we improve our system. Instead of pausing for 100 seconds after 100 seconds of perfect operation, it handles all requests at 5 ms each after 100 seconds. Doing the math, we get the following:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The 50th percentile is 1 ms&lt;/item&gt;&lt;item&gt;The 75th percentile is 2.5 ms (stall showed 1 ms)&lt;/item&gt;&lt;item&gt;The 99.99th percentile is 5 ms (stall showed 1 ms)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This data tells us we hurt the four nines and made the system 5x worse! This would tell us to revert the change and go back to the way it was before, which is clearly the wrong decision. With bad data, better can look worse. This shows that you cannot have any intuition based on any of these numbers. The data is garbage.&lt;/p&gt;&lt;p&gt;With many load generators, the situation is actually much worse than this. These systems work by generating a constant load. If our test is generating 100 requests/second, we run 10,000 requests in the first 100 seconds. When we stall, we process just one request. After the stall, the load generator sees that it’s 9,999 requests behind and issues those requests to catch back up. Not only did it get rid of the bad requests, it replaced them with good requests. Now the data is twice as wrong as just dropping the bad requests.&lt;/p&gt;&lt;p&gt;What coordinated omission is really showing you is service time, not response time. If we imagine a cashier ringing up customers, the service time is the time it takes the cashier to do the work. The response time is the time a customer waits before they reach the register. If the rate of arrival is higher than the service rate, the response time will continue to grow. Because hiccups and other phenomena happen, response times often bounce around. However, coordinated omission lies to you about response time by actually telling you the service time and hiding the fact that things stalled or waited in line.&lt;/p&gt;&lt;head rend="h3"&gt;Measuring Latency&lt;/head&gt;&lt;p&gt;Latency doesn’t live in a vacuum. Measuring response time is important, but you need to look at it in the context of load. But how do we properly measure this? When you’re nearly idle, things are nearly perfect, so obviously that’s not very useful. When you’re pedal to the metal, things fall apart. This is somewhat useful because it tells us how “fast” we can go before we start getting angry phone calls.&lt;/p&gt;&lt;p&gt;However, studying the behavior of latency at saturation is like looking at the shape of your car’s bumper after wrapping it around a pole. The only thing that matters when you hit the pole is that you hit the pole. There’s no point in trying to engineer a better bumper, but we can engineer for the speed at which we lose control. Everything is going to suck at saturation, so it’s not super useful to look at beyond determining your operating range.&lt;/p&gt;&lt;p&gt;What’s more important is testing the speeds in between idle and hitting the pole. Define your SLAs and plot those requirements, then run different scenarios using different loads and different configurations. This tells us if we’re meeting our SLAs but also how many machines we need to provision to do so. If you don’t do this, you don’t know how many machines you need.&lt;/p&gt;&lt;p&gt;How do we capture this data? In an ideal world, we could store information for every request, but this usually isn’t practical. HdrHistogram is a tool which allows you to capture latency and retain high resolution. It also includes facilities for correcting coordinated omission and plotting latency distributions. The original version of HdrHistogram was written in Java, but there are versions for many other languages.&lt;/p&gt;&lt;head rend="h3"&gt;To Summarize&lt;/head&gt;&lt;p&gt;To understand latency, you have to consider the entire distribution. Do this by plotting the latency distribution curve. Simply looking at the 95th or even 99th percentile is not sufficient. Tail latency matters. Worse yet, the median is not representative of the “common” case, the average even less so. There is no single metric which defines the behavior of latency. Be conscious of your monitoring and benchmarking tools and the data they report. You can’t average percentiles.&lt;/p&gt;&lt;p&gt;Remember that latency is not service time. If you plot your data with coordinated omission, there’s often a quick, high rise in the curve. Run a “CTRL+Z” test to see if you have this problem. A non-omitted test has a much smoother curve. Very few tools actually correct for coordinated omission.&lt;/p&gt;&lt;p&gt;Latency needs to be measured in the context of load, but constantly running your car into a pole in every test is not useful. This isn’t how you’re running in production, and if it is, you probably need to provision more machines. Use it to establish your limits and test the sustainable throughputs in between to determine if you’re meeting your SLAs. There are a lot of flawed tools out there, but HdrHistogram is one of the few that isn’t. It’s useful for benchmarking and, since histograms are additive and HdrHistogram uses log buckets, it can also be useful for capturing high-volume data in production.&lt;/p&gt;Follow @tyler_treat&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bravenewgeek.com/everything-you-know-about-latency-is-wrong/"/><published>2025-11-21T01:50:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46001889</id><title>Olmo 3: Charting a path through the model flow to lead open-source AI</title><updated>2025-11-21T09:38:53.959034+00:00</updated><content>&lt;doc fingerprint="ad5e3b78241b8f9a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Olmo 3: Charting a path through the model flow to lead open-source AI&lt;/head&gt;
    &lt;p&gt;November 20, 2025&lt;/p&gt;
    &lt;p&gt;Ai2&lt;/p&gt;
    &lt;p&gt;Language models are often treated as snapshots—brief captures of a long and carefully curated development process. But sharing only the end result obscures the rich context needed to modify, adapt, and extend a model's capabilities. Many meaningful adjustments require integrating domain-specific knowledge deep within the development pipeline, not merely at the final stage. To truly advance open AI development and research, the entire model flow – not just its endpoint – should be accessible and customizable. The model flow is the full lifecycle of an LM: every stage, checkpoint, dataset, and dependency required to create and modify it. By exposing this complete process, the goal is to engender greater trust and enable more effective adaptation, collaboration, and innovation.&lt;/p&gt;
    &lt;p&gt;With today's release of Olmo 3, we're empowering the open source community with not only state-of-the-art open models, but the entire model flow and full traceability back to training data.&lt;/p&gt;
    &lt;p&gt;At its center is Olmo 3-Think (32B), the best fully open 32B-scale thinking model that for the first time lets you inspect intermediate reasoning traces and trace those behaviors back to the data and training decisions that produced them. Olmo 3 is a family of compact, dense models at 7 billion and 32 billion parameters that can run on everything from laptops to research clusters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo 3-Base (7B, 32B) is our most powerful base model yet. When evaluated on our expanded, diverse evaluation suite, Olmo 3-Base delivers the strongest performance among fully open base models – where training data, code, and weights are all publicly available, like Stanford's Marin and Swiss AI's Apertus – and achieves competitive performance with some of the best open-weights base models of comparable size and architecture, including Qwen 2.5 and Gemma 3. Achieving strong results in programming, reading comprehension, and math problem solving, Olmo 3-Base maintains performance at extended context lengths (~up to 65K tokens)—providing a versatile foundation for continued pretraining, targeted fine-tuning, and reinforcement learning and making it easy to build in specialized capabilities like reasoning, tool use (function calling), and instruction following through post-training.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Think (7B, 32B) is our flagship post-trained reasoning set built on Olmo 3-Base. At a time when few organizations are releasing truly open models at this scale, Olmo 3-Think (32B) serves as a workhorse for RL research, long-horizon reasoning, and other advanced experiments that require substantial compute. On our suite of reasoning benchmarks (discussed below), it's the strongest fully open thinking model we're aware of, narrowing the gap to the best open-weight models of similar scale – such as Qwen 3 32B – while training on roughly 6x fewer tokens. Olmo 3-Think (7B) brings the same design and training approach to an even more efficient form factor, surfacing intermediate thinking steps for complex prompts while making open, inspectable reasoning accessible on more modest hardware.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Instruct (7B) is a chat and quick-response focused post-train of Olmo 3-Base that handles multi-turn, instruction-following, tool use, and more. In our evaluations, it matches or outperforms open-weight models including Qwen 2.5, Gemma 3, and Llama 3.1, and narrows the gap with Qwen 3 model families at a similar scale—delivering a strong, fully open alternative for high-quality conversational and tool-using agents.&lt;/item&gt;
      &lt;item&gt;Olmo 3-RL Zero (7B), is a fully open reinforcement learning pathway built on Olmo 3-Base, designed to bootstrap complex reasoning behaviors and enable clear benchmarking of RL algorithms. We release four series of checkpoints from domain-focused training on math, code, instruction following, and general chat, enabling careful study of reinforcement learning with verifiable rewards (RLVR).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead of a single set of frozen weights, Olmo 3 offers multiple, fully documented paths through development: the Instruct path for everyday chat and tool use, the RL Zero path for RL experimentation from base models, and the Think/reasoning path for models that leverage inference-time scaling to unlock complex reasoning and agentic behaviors. Each path is a concrete example of how to shape behavior from the same base model, and you’re free to fork or remix them—start with Olmo 3-Base, explore your own supervised fine-tuning (SFT) or direct preference optimization (DPO) recipe for instruct-style use cases, or plug in a new RL objective to probe different tradeoffs. The flow itself becomes a rich, reusable object—not just a record of how we built Olmo 3, but a scaffold for how you can build your own systems.&lt;/p&gt;
    &lt;p&gt;Explore the Model Flow&lt;/p&gt;
    &lt;p&gt;Click on any stage to learn more about it and download artifacts.&lt;/p&gt;
    &lt;p&gt;The Olmo 3 checkpoints we're releasing represent our initial paths targeting our goals around reasoning, tool use, and general capabilities – we have exciting plans for other ways to leverage Olmo 3-Base 32B. But because we're releasing the entire flow, you can intervene at any point: swap in domain-specific data during mid-training, adjust post-training for your use case, or build on an earlier checkpoint that better suits your needs.&lt;/p&gt;
    &lt;p&gt;As with Olmo and Olmo 2, we’re releasing all components of the Olmo 3 flow – data, code, model weights, and checkpoints – under permissive open source licenses.&lt;/p&gt;
    &lt;p&gt;Try Olmo 3 | Download the models &amp;amp; data | Read the report&lt;/p&gt;
    &lt;head rend="h3"&gt;Strong performance across the board&lt;/head&gt;
    &lt;p&gt;We run the Olmo 3 checkpoints through a broad, updated benchmark suite, grouping dozens of industry-standard tasks (plus a few new ones we introduce) into several capability clusters. Together, the clustered suite and these held-out tasks give us a capability profile of Olmo 3—a clear picture of how well it solves math problems, codes, uses tools, answers general-knowledge questions, and more.&lt;/p&gt;
    &lt;p&gt;At a high level, the Olmo 3 family delivers the strongest fully open base and thinking models we’re aware of. Olmo 3-Base 32B outperforms other fully open base models, and Olmo 3-Think 32B emerges as the strongest fully open thinking model.&lt;/p&gt;
    &lt;p&gt;Our results were made possible by rigorous data curation at every stage of training, a carefully designed training recipe for each model, and a set of new algorithmic and infrastructure advances across data processing, training, and reinforcement learning. We also introduce an enhanced reinforcement learning framework that guides the development of our models and is particularly essential for our thinking models. To design the training recipe and coordinate targeted improvements across a wide range of capabilities at each stage of the model training pipeline, our development framework balances distributed innovation with centralized evaluation.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Base, with a training pipeline that first focuses on broad coverage over diverse text, code, and math, then concentrates on harder distributions to sharpen programming, quantitative reasoning, and reading comprehension, is clearly the strongest set of fully open base models in our evaluations. It’s also arguably the best 32B model in the entire ecosystem of models with open weights, performing impressively in programming, reading comprehension, math problem solving, and long-context benchmarks like RULER, which tests information retrieval from lengthy texts. Olmo 3-Base (7B) and Olmo 3-Base (32) maintain quality at extended context lengths and integrate cleanly with RL workflows, providing a robust foundation for continued pretraining and post-training.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Think, which turns the Base into a reasoning model by training on multi-step problems spanning math, code, and general problem solving, then running the thinking SFT → thinking DPO → RLVR model flow to elicit high-quality reasoning traces, competes with or exceeds several open-weight reasoning models of similar sizes. On math benchmarks, Olmo 3-Think (7B) matches Qwen 3 8B on MATH and comes within a few points on AIME 2024 and 2025, and also leads all comparison models on HumanEvalPlus for coding—performing strongly on MBPP and LiveCodeBench to demonstrate particular strength in code-intensive reasoning. On broader reasoning tasks like BigBench Hard and AGI Eval English, Olmo 3-Think (7B) remains competitive with Qwen 3 8B reasoning and Qwen 3 VL 8B Thinker while staying fully open and slightly smaller.&lt;/p&gt;
    &lt;p&gt;For the 32B model, Olmo 3-Think scales these trends up and becomes one of the strongest fully open reasoning models in its class. Olmo 3-Think (32B) either wins or sits within roughly two points of the best open-weight model on MATH, OMEGA, BigBenchHard, HumanEvalPlus, PopQA, and IFEval. It ties Qwen 3 VL 32B Thinking for the top score on the OMEGA suite while staying clearly ahead of Gemma 3 27B Instruct and competitive with DeepSeek R1 Distill 32B on math and reasoning. On broader knowledge and QA, Olmo 3-Think (32B) is effectively neck-and-neck with the Qwen 3 models on PopQA. And in instruction following, Olmo 3-Think (32B) tops this subset on IFEval and remains solid on IFBench and AlpacaEval 2 LC—offering a strong default for reasoning workloads at the 32B scale.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Instruct, which produces shorter sequences than the corresponding Olmo 3-Think models to improve inference efficiency and is designed to focus on general chat, tool use, and synthetic data generation, outperforms comparably-sized open-weight models. Olmo 3-Instruct ties or surpasses Qwen 2.5, Gemma 3, and Llama 3.1 in our evaluations, and competes with the Qwen 3 family at similar scale, delivering strong function calling performance and instruction-following capabilities in a fully open 7B model.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Olmo 3 architecture and training stages&lt;/head&gt;
    &lt;p&gt;Olmo 3 uses a decoder-only transformer architecture and multi-stage training pipeline. Pretraining runs in three stages—an initial large-scale training run that builds broad capabilities; a mid-training phase that focuses on harder material like math, code, and reading comprehension; and a final long-context extension stage that trains the model on very long documents. Together with architectural enhancements, this yields a more capable, efficient base for the Olmo 3 family.&lt;/p&gt;
    &lt;p&gt;Post-training then specializes the pretrained model for different use cases. Building on Olmo 2, each pathway follows a three-stage recipe – SFT, preference tuning with DPO, and RLVR – but in Olmo 3, we expose this as a fully documented model flow with complete customization over each training stage and dataset mix.&lt;/p&gt;
    &lt;p&gt;Instead of releasing only the final weights, we provide checkpoints from each major training milestone: the base pretrained model, the mid-trained model after targeted skill enhancement, the long-context-extended version, plus post-training checkpoints for the Olmo 3-Think, Olmo 3-Instruct, and Olmo 3-RL Zero flows. You can study how capabilities emerge over time, run ablations on specific stages, and fork the model at whatever point best fits your data, compute, and goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expanded training data&lt;/head&gt;
    &lt;p&gt;Compared to Olmo 2, we scaled data collection and significantly strengthened our dataset curation methods. Continuing our commitment to full transparency, we’re releasing several new, higher-quality datasets that cover every stage of base model training and post-training—from initial learning to specialized skills like complex reasoning and long-context understanding. This means anyone can see exactly what data shaped the model’s capabilities, reproduce our results, and reuse these datasets to train their own AI systems.&lt;/p&gt;
    &lt;p&gt;Olmo 3 is pretrained on Dolma 3, a new ~9.3-trillion-token corpus drawn from web pages, science PDFs processed with olmOCR, codebases, math problems and solutions, and encyclopedic text. From this pool, we construct Dolma 3 Mix, a 5.9-trillion-token (~6T) pretraining mix with a higher proportion of coding and mathematical data than earlier Dolma releases, plus much stronger decontamination via extensive deduplication, quality filtering, and careful control over data mixing. We follow established web standards in collecting training data and don’t collect from sites that explicitly disallow it, including paywalled content.&lt;/p&gt;
    &lt;p&gt;On top of this, we introduce two Dolma 3-based mixes for later stages of base model training. Dolma 3 Dolmino is our mid-training mix: 100B training tokens sampled from a ~2.2T-token pool of high-quality math, science, code, instruction-following, and reading-comprehension data, including reasoning traces that also enable RL directly on the base model. Dolma 3 Longmino is our long-context mix: ~50B training tokens drawn from a 639B-token pool of long documents combined with mid-training data to teach Olmo 3 to track information over very long inputs (like reports, logs, and multi-chapter documents).&lt;/p&gt;
    &lt;p&gt;We also introduce Dolci, a new post-training data suite tailored specifically for reasoning, tool use, and instruction following. Dolci provides separate mixes for each stage of post-training: SFT, DPO, and RLVR. For SFT, Dolci aggregates state-of-the-art datasets that advance step-by-step reasoning, tool use, and high-quality conversational behavior; for DPO, it supplies high-quality contrastive preference data; and for RL, it includes hard, diverse prompts across math, coding, instruction following, and general chat.&lt;/p&gt;
    &lt;p&gt;Together, Dolma 3 and Dolci give Olmo 3 a fully open data curriculum from first token to final post-trained checkpoint.&lt;/p&gt;
    &lt;head rend="h3"&gt;Efficient training stack&lt;/head&gt;
    &lt;p&gt;We pretrained Olmo 3 on a cluster of up to 1,024 H100 GPUs; we achieved training throughput of 7.7K tokens per device per second for Olmo 3-Base (7B). We mid-trained on 128 H100 GPUs, and post-trained on a set of 256 H100s.&lt;/p&gt;
    &lt;p&gt;For Olmo 3, building on the work we did for Olmo 2, we were able to significantly improve the efficiency of our post-training code. By moving SFT from Open Instruct (our post-training codebase, prioritizing flexibility) to Olmo Core (our pretraining codebase, designed to maximize efficiency), we increased throughput (tokens/second) by 8x. Similarly, by incorporating in-flight weight updates, continuous batching, and a lot of threading improvements, we made our RL training 4x more efficient—resulting in training runs that are significantly cheaper and faster.&lt;/p&gt;
    &lt;p&gt;A note on our 32B models: We believe 32B sits in a sweet spot for research and tinkering. 32B models are big enough to support strong, competitive performance, but still small enough that a wide audience can fine-tune and deploy them on accessible hardware.&lt;/p&gt;
    &lt;p&gt;For more details, including ablations, please read our technical report.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transparency at the core&lt;/head&gt;
    &lt;p&gt;A core goal of Olmo 3 is not just to open the model flow, but to make it actionable for people who want to understand and improve model behavior. Olmo 3 integrates with OlmoTrace, our tool for tracing model outputs back to training data in real time.&lt;/p&gt;
    &lt;p&gt;For example, in the Ai2 Playground, you can ask Olmo 3-Think (32B) to answer a general-knowledge question, then use OlmoTrace to inspect where and how the model may have learned to generate parts of its response. This closes the gap between training data and model behavior: you can see not only what the model is doing, but why—and adjust data or training decisions accordingly.&lt;/p&gt;
    &lt;p&gt;To further promote transparency and explainability, we’re making every training and fine-tuning dataset available for download, all under a permissive license that allows for custom deployment and reuse. The datasets come in a range of mixes to accommodate different storage and hardware constraints, from several billion tokens all the way up to 6 trillion.&lt;/p&gt;
    &lt;p&gt;Our new tooling for data processing allows you to de-contaminate, tokenize, and de-duplicate data in the same way we did for Olmo 3’s corpora. All the tooling is open source, enabling you to replicate our training curves or run controlled ablations across data mixes and objectives.&lt;/p&gt;
    &lt;p&gt;Our Olmo utilities and software cover the whole development cycle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo-core is a state-of-the-art framework for distributed model training.&lt;/item&gt;
      &lt;item&gt;Open Instruct is our post-training pipeline.&lt;/item&gt;
      &lt;item&gt;datamap-rs is a pure-Rust toolkit for large-scale cleaning.&lt;/item&gt;
      &lt;item&gt;duplodocus for ultra-efficient fuzzy de-duplication.&lt;/item&gt;
      &lt;item&gt;OLMES is a toolkit for reproducible evals. It includes our brand-new eval collection OlmoBaseEval, which we used for Olmo 3 base model development.&lt;/item&gt;
      &lt;item&gt;decon removes test sets from training data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Importantly, our tooling allows you to instrument complex tasks and analyze intermediate traces to understand where the models succeed—or struggle. Because the Olmo 3 data recipes, training pipeline, and checkpoints are open, independent teams can connect model behavior back to measurable properties.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ready to deploy and use&lt;/head&gt;
    &lt;p&gt;Together, the Olmo 3 family makes it easier to build trustworthy features quickly, whether for research, education, or applications. By making every development step available and inspectable, we're enabling entirely new categories of research. You can run experiments on any training phase, understand exactly how different techniques contribute to model capabilities, and build on our work at whatever stage makes sense for your project.&lt;/p&gt;
    &lt;p&gt;For scientists, the fully open flow exposes the model’s inner workings, so you can instrument experiments across coding, reasoning, RL, and tool use.&lt;/p&gt;
    &lt;p&gt;If you care about AI you can study, audit, and improve, Olmo 3 is for you. Try the demos in the Ai2 Playground, explore the documentation, and build on the released weights and checkpoints. Then tell us what you discover—we invite the community to validate, critique, and extend our findings.&lt;/p&gt;
    &lt;p&gt;True openness in AI isn't just about access—it's about trust, accountability, and shared progress. We believe the models shaping our future should be fully inspectable, not black boxes. Olmo 3 represents a different path: one where anyone can understand, verify, and build upon the AI systems that increasingly influence our world. This is what open-first means—not just releasing weights, but sharing the complete knowledge needed to advance AI responsibly: the flow.&lt;/p&gt;
    &lt;p&gt;Deep dive with Olmo lead researchers Hanna Hajishirzi and Noah Smith on how – and why – we built Olmo 3, and what comes next:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://allenai.org/blog/olmo3"/><published>2025-11-21T06:50:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46002161</id><title>It's Hard to Build an Oscillator</title><updated>2025-11-21T09:38:53.828806+00:00</updated><content/><link href="https://lcamtuf.substack.com/p/its-hard-to-build-an-oscillator"/><published>2025-11-21T07:45:53+00:00</published></entry></feed>