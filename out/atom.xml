<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-03T18:13:44.561739+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45458948</id><title>Fp8 runs ~100 tflops faster when the kernel name has "cutlass" in it</title><updated>2025-10-03T18:13:56.605444+00:00</updated><content>&lt;doc fingerprint="cef8dd4f2e620e8a"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 2.3k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;[Gluon][Tutorial] Persistent attention #7298&lt;/head&gt;
    &lt;head id="button-e895d71a5814765a" class="btn btn-sm btn-primary m-0 ml-0 ml-md-2"&gt;New issue&lt;/head&gt;
    &lt;p&gt;Have a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.&lt;/p&gt;
    &lt;p&gt;By clicking “Sign up for GitHub”, you agree to our terms of service and privacy statement. We’ll occasionally send you account related emails.&lt;/p&gt;
    &lt;p&gt;Already on GitHub? Sign in to your account&lt;/p&gt;
    &lt;head rend="h2"&gt;Conversation&lt;/head&gt;
    &lt;p&gt;Rewrite the attention kernel to be persistent. This gives better performance at low-contexts. However, fp16 at large context has suffered a bit due to a ptxas instruction scheduling issue in the softmax partition. fp8 is ~100 tflops faster when the kernel name has "cutlass" in it.&lt;/p&gt;
    &lt;code&gt;Attention Z=4 H=32 D=64 causal=False:
     N_CTX  triton-fp16  triton-fp8
0   1024.0   359.574448  370.119987
1   2048.0   612.103928  641.204555
2   4096.0   653.868402  682.337948
3   8192.0   692.102228  721.555690
4  16384.0   696.972041  726.190035
5  32768.0   698.723685  727.983456
6  65536.0   699.865817  728.558321
Attention Z=4 H=32 D=64 causal=True:
     N_CTX  triton-fp16  triton-fp8
0   1024.0   181.879039  177.982453
1   2048.0   441.315463  454.310072
2   4096.0   532.170527  539.995252
3   8192.0   633.620646  638.544937
4  16384.0   667.687180  670.681255
5  32768.0   684.276329  688.571907
6  65536.0   692.953202  694.648353
Attention Z=4 H=32 D=128 causal=False:
     N_CTX  triton-fp16   triton-fp8
0   1024.0   718.580015   709.863720
1   2048.0  1133.490258  1222.548477
2   4096.0  1247.605551  1369.800195
3   8192.0  1243.482713  1406.799697
4  16384.0  1125.744367  1514.857403
5  32768.0  1124.116305  1521.267973
6  65536.0  1064.588719  1518.738037
Attention Z=4 H=32 D=128 causal=True:
     N_CTX  triton-fp16   triton-fp8
0   1024.0   355.642522   351.161232
1   2048.0   846.404095   854.547917
2   4096.0  1013.840017  1021.676435
3   8192.0  1176.258395  1152.844234
4  16384.0  1190.290681  1325.786204
5  32768.0  1063.658200  1394.413325
6  65536.0   970.531569  1413.282610
&lt;/code&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;wow!&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;For posterity, these are the best results prior to converting the kernel to persistent&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;I don't see a "cutlass" in the kernel names?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Before:&lt;/p&gt;
          &lt;p&gt;After&lt;/p&gt;
          &lt;p&gt;I'm not sure if I interpreted it incorrectly, but seems like perf is dropped based on the numbers?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;Great stuff. Couple small NITs though.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;_, corr_bar, corr_producer = corr_producer.acquire()&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"/&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;p = gl.join(p0, p1).permute(0, 2, 1).reshape([config.SPLIT_M, config.BLOCK_N])&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;p = gl.convert_layout(p, config.qk_layout)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;This shouldn't be needed any more after I introduced the slice layout for split, right?&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;The convert layout coming out of the &lt;code&gt;split&lt;/code&gt; is no longer needed, but&lt;/p&gt;
    &lt;code&gt;ValueError('Layout mismatch in broadcast: 

SliceLayout(dim=1, parent=BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])) 
vs 
SliceLayout(dim=1, parent=DistributedLinearLayout(reg_bases=[[0, 64], [0, 1], [0, 2], [0, 4], [0, 8], [0, 16], [0, 32]], lane_bases=[[1, 0], [2, 0], [4, 0], [8, 0], [16, 0]], warp_bases=[[32, 0], [64, 0]], block_bases=[], shape=[128, 128]))')
&lt;/code&gt;
    &lt;p&gt;It seems that &lt;code&gt;p&lt;/code&gt; ends up with a linear layout instead of a blocked layout. I am not sure why though -- I believe the layout inference should try a blocked layout first before falling back to linear layout.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;name = "gluon_attention"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;# Up to 150 TFLOPS faster for fp8!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;if specialization.constants["dtype"] == gl.float8e5:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;name = "cutlass_" + name&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;very cool... did you check if other names change the scheduling (e.g. because of non-determinism or code alignment) or if it's literally just special cased for cutlass.&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;it's literally just special cased for cutlass.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yup&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;wow! You literally beat the nvcc team!&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;@AlexMaclean Just a FYI, in case you can prod the right folks on your side. There must be a better way to enable this optimization. A PTX directive, perhaps, if ptxas can't figure out the right thing by itself?&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;@Mogball have you checked the accuracy, is it the same? The Deepseek technical report mentioned that fp8 tensor cores use reduced mantissa for the accumulator, maybe this is what indirectly enabled/disabled by the name of the kernel.&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The Deepseek technical report mentioned that fp8 tensor cores use reduced mantissa for the accumulator, maybe this is what indirectly enabled/disabled by the name of the kernel.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That's only on Hopper&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;By disassembly of&lt;/p&gt;&lt;code&gt;ptxas&lt;/code&gt;, it is indeed hard-coded that they have logic like&lt;code&gt;strstr(kernel_name, "cutlass")&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;That's Interesting! I'm curious is it feasible to modifty asm code for &lt;code&gt;ptxas&lt;/code&gt; that make the &lt;code&gt;al&lt;/code&gt; return register always be true (maybe we could modify code in the address between &lt;code&gt;2165-216c&lt;/code&gt;), did you have a try?&lt;/p&gt;
    &lt;p&gt;There was a problem hiding this comment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choose a reason for hiding this comment&lt;/head&gt;
    &lt;p&gt;The reason will be displayed to describe this comment to others. Learn more.&lt;/p&gt;
    &lt;p&gt;Admittedly it is feasible. But it is more likely that, this is an unstable, experimental, aggressive optimization by NVIDIA, and blindly always enabling it may produce some elusive bugs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;For D64 it did drop quite a bit during the transition to persistent. This is due to a scheduling issue in ptxas that I couldn't find a workaround for.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/triton-lang/triton/pull/7298"/><published>2025-10-03T04:21:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45459233</id><title>In Praise of RSS and Controlled Feeds of Information</title><updated>2025-10-03T18:13:55.039307+00:00</updated><content>&lt;doc fingerprint="e7f45b221ea4acbd"&gt;
  &lt;main&gt;
    &lt;p&gt;The way we consume content on the internet is increasingly driven by walled-garden platforms and black-box feed algorithms. This shift is making our media diets miserable. Ironically, a solution to the problem predates algorithmic feeds, social media and other forms of informational junk food. It is called RSS (Really Simple Syndication) and it is beautiful.&lt;/p&gt;
    &lt;head rend="h2"&gt;What the hell is RSS?&lt;/head&gt;
    &lt;p&gt;RSS is just a format that defines how websites can publish updates (articles, posts, episodes, and so on) in a standard feed that you can subscribe to using an RSS reader (or aggregator). Don’t worry if this sounds extremely uninteresting to you; there aren’t many people that get excited about format specifications; the beauty of RSS is in its simplicity. Any content management system or blog platform supports RSS out of the box, and often enables it by default. As a result, a large portion of the content on the internet is available to you in feeds that you can tap into. But this time, you’re in full control of what you’re receiving, and the feeds are purely reverse chronological bliss. Coincidentally, you might already be using RSS without even knowing, because the whole podcasting world runs on RSS.&lt;/p&gt;
    &lt;p&gt;There are many amazing articles about the utility and elegance of RSS, and I do not think the world needs another, so I will spare you and instead focus on my personal experience and tips. If you are interested in a deeper dive, I highly recommend Molly White’s article Curate your own newspaper with RSS. It is a convincing, well-written article that you can also listen to in Molly’s own voice if you wish to.&lt;/p&gt;
    &lt;head rend="h2"&gt;Broken distribution models&lt;/head&gt;
    &lt;p&gt;Here’s a little story about the promise of social media. In 2011, my band was getting a little more serious and preparing to record our first album. Facebook was rapidly growing all over the world, so I created an account - mostly to manage my band’s Facebook page. Back then, social media (and Facebook in particular) felt very different: vibrant and full of promise for the brave new future of web 2.0. I looked up all my favorite bands so that every time they put out an album or tour near me, I wouldn’t miss it. Many bands either lacked proper websites or rarely updated them in a useful way, so this felt like the perfect use case for Facebook.&lt;/p&gt;
    &lt;p&gt;It didn’t take long for me to start seeing the cracks. As Facebook would push for more engagement, some bands would flood their pages with multiple posts per day, especially if they were touring or had a new release coming up. Others would be more restrained, but then their posts would often be lost in the feed. There was no way to opt in only for a certain type of updates from my followed pages, and the increasingly algorithmic feed would simply prioritize posts by engagement. I realized that I wouldn’t be able to get just the important updates; instead, I’d get a wild mish-mash of engagement-bait that I wasn’t willing to work my way through. And don’t get me started about how over time, page owners had to pay to promote their posts to get any reach on the platform - that is simply extortion.&lt;/p&gt;
    &lt;p&gt;I no longer use Facebook (or any similar social media for that matter) for many reasons, though algorithmic feeds are at the top of the list. Algorithms on social media are very unlikely to be written with your best interest in mind: The goal of social media is to keep you glued to the feed for as long as possible. It optimizes for the most time spent, for engagement, for serving the most ads. It will not necessarily optimize for keeping you well informed, showing you balanced opinions, giving you control or even showing you all the information you’d like. The misalignment of incentives has become very apparent in the last few years, but the problem goes deeper. Any type of curation (because algorithmic feeds are simply curation machines) will never be flexible enough to account for every person’s needs. The story we are sold with algorithmic curation is that it adapts to everyone’s taste and interests, but that’s only true until the interests of the advertisers enter the picture.&lt;/p&gt;
    &lt;head rend="h2"&gt;How I use RSS&lt;/head&gt;
    &lt;p&gt;My RSS journey starting many moons ago with Opera and Thunderbird, continued with Google Reader (RIP) and The Old Reader, and finally led me to running my own instance of FreshRSS. However, in the last year, I have read most of the content from my RSS feeds on my phone via the FeedMe app. I find that it scratches the itch of unlocking your phone and wanting to see something novel (probably gravitating towards social media). On the upside, it feeds me only articles and media that a) I have picked upfront and nothing more, b) is typically longer-form and more thoughtful than your typical social media posts.&lt;/p&gt;
    &lt;p&gt;Also, unlike algorithmic feeds, it allows me to pick what category of my interests I am in the mood for. If I’m in the mood for something lighter, I can just look into my “Fun” folder to check out new stuff from The Oatmeal or xkcd. If I feel like reading something more thoughtful, I’d dive into my “Reads” folder for The Marginalian or Sentiers. Feeling like catching up on the newest AI research? I can browse the latest research papers from arXiv that have specific keywords in the abstracts (such as prompt injection). Or I could just browse everything at once to see what piques my interest. I am the master of what information I consume, how and in what order, and no one can take that away from me by rearranging my feed or tweaking the algorithm.&lt;/p&gt;
    &lt;p&gt;One of the many small advantages is the consistency of the interface and the lack of distractions when reading. Modern browsers support reader modes, but you need to enter the mode manually and some pages might not be displayed correctly. I don’t have any attention problems (that I know of), but reading articles on certain newspaper sites feels like a cruel joke: the text of the article is often drowned by ads, suggested articles, polls, and other visual smog. Not a pleasant reading experience. Your RSS reader always uses the same font, font size, screen real estate and never shows anything but the article itself.&lt;/p&gt;
    &lt;p&gt;The focused, reductive nature of RSS readers means you don’t get the full website experience, but that is arguably for the better in a lot of cases. We already mentioned the lack of suggested articles with engagement bait that could easily draw you in, but another notable omission is the comments section. It is very easy to slip into the comments section at the bottom of an article and spend far too much time reading those. You can still do that in an RSS reader by opening the article in your browser, scrolling down to the comments and diving in. At least in my case, that is a safe amount of friction to prevent me from doing it most of the time. Less is more!&lt;/p&gt;
    &lt;head rend="h2"&gt;Tips to get you going&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many of the websites you open regularly, follow on social media or get a newsletter from, likely have an RSS feed. Look out for the RSS icon or the words RSS or feed. There are also tools like Lighthouse that can sniff out the feed for you. That said, my experience is that simply adding the homepage URL of the website into an aggregator usually works.&lt;/item&gt;
      &lt;item&gt;Remember my frustration with Facebook as a source of news for new music releases? Turns out there is a much better free solution called Muspy, where you enter all your favorite artists and it will notify you of their new releases. And guess what? You either get notified via email, or you use your personal RSS feed. Highly recommended!&lt;/item&gt;
      &lt;item&gt;Start easy with something like The Old Reader or Feedly - both offer relatively generous free tiers. And if you outgrow them or want to try something else, you simply export an OPML file with all your feeds and import them into your new RSS solution. This is the upside of open standards: freedom, ownership, and portability.&lt;/item&gt;
      &lt;item&gt;Once you have more than 5-10 feeds, start putting them into folders/categories. No need to overthink it, but doing this will help you be more selective about the content you read if you’re in a specific mood.&lt;/item&gt;
      &lt;item&gt;RSS readers can be great when traveling or whenever your internet connection might be down or spotty. You can set up your RSS client in a way that automatically fetches new content, so when you board the plane and go dark, you can still read through the already downloaded articles. (Beware, though: not all RSS feeds include full content - sometimes they’re more like teasers.)&lt;/item&gt;
      &lt;item&gt;Some websites that limit how many articles you can browse for free are actually less strict about content accessed through RSS feeds. There are obvious ethical concerns with abusing this, but it is still an upside, and you are only consuming what they provide.&lt;/item&gt;
      &lt;item&gt;If you want to tinker, you can set up an RSS aggregator like FreshRSS, tiny tiny RSS or selfoss on a shared web hosting service. If you want to go full self-hosted, there are many more options available.&lt;/item&gt;
      &lt;item&gt;Get a good mobile app. Try a few before you settle! This is a highly personal choice because even small UI quirks and differences may bother you. If you’re anything like me, you’ll do most of the reading on your phone, so make sure it feels good.&lt;/item&gt;
      &lt;item&gt;RSS readers/clients often have bookmarking/starring system which works much like dedicated bookmarking apps.&lt;/item&gt;
      &lt;item&gt;Bigger publications often have separate feeds for individual categories or tags - check those to avoid getting your main feed flooded.&lt;/item&gt;
      &lt;item&gt;Some websites have very elaborate RSS APIs which allow you to query for specific types of content. For example, arXiv has a really elaborate one, allowing you to only follow specific topics. The documentation is quite complex, so here is a quick example to kick start you:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;https://export.arxiv.org/api/query?search_query=abs:LLM+AND+multilingual&amp;amp;sortBy=submittedDate&amp;amp;sortOrder=descending&lt;/code&gt;&lt;/item&gt;&lt;item&gt;The query searches through the most recently submitted papers with the words LLM and multilingual in the abstract.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Do a little cleanup from time to time: unsubscribe from feeds that no longer seem to interest you. It’s fine, no one will take offense, and your attention is too precious to be wasted on stuff that is not for you.&lt;/item&gt;
      &lt;item&gt;Don’t know where to start? Check out this list of 100 most popular RSS feeds, Feedspot’s 70 most popular feeds or Hostinger’s list of 55 popular blogs. Apart from that, Google is your friend (especially if you start searching for specific topics or niches), and good blogs often link to other blogs - all you need to do is to follow the breadcrumbs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Happy RSS-ing!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.burkert.me/posts/in_praise_of_syndication/"/><published>2025-10-03T05:13:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45461500</id><title>Niri – A scrollable-tiling Wayland compositor</title><updated>2025-10-03T18:13:54.390863+00:00</updated><content>&lt;doc fingerprint="f76fe4761cb74b90"&gt;
  &lt;main&gt;
    &lt;p&gt;A scrollable-tiling Wayland compositor.&lt;/p&gt;
    &lt;p&gt;Getting Started | Configuration | Setup Showcase&lt;/p&gt;
    &lt;p&gt;Windows are arranged in columns on an infinite strip going to the right. Opening a new window never causes existing windows to resize.&lt;/p&gt;
    &lt;p&gt;Every monitor has its own separate window strip. Windows can never "overflow" onto an adjacent monitor.&lt;/p&gt;
    &lt;p&gt;Workspaces are dynamic and arranged vertically. Every monitor has an independent set of workspaces, and there's always one empty workspace present all the way down.&lt;/p&gt;
    &lt;p&gt;The workspace arrangement is preserved across disconnecting and connecting monitors where it makes sense. When a monitor disconnects, its workspaces will move to another monitor, but upon reconnection they will move back to the original monitor.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built from the ground up for scrollable tiling&lt;/item&gt;
      &lt;item&gt;Dynamic workspaces like in GNOME&lt;/item&gt;
      &lt;item&gt;An Overview that zooms out workspaces and windows&lt;/item&gt;
      &lt;item&gt;Built-in screenshot UI&lt;/item&gt;
      &lt;item&gt;Monitor and window screencasting through xdg-desktop-portal-gnome &lt;list rend="ul"&gt;&lt;item&gt;You can block out sensitive windows from screencasts&lt;/item&gt;&lt;item&gt;Dynamic cast target that can change what it shows on the go&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Touchpad and mouse gestures&lt;/item&gt;
      &lt;item&gt;Group windows into tabs&lt;/item&gt;
      &lt;item&gt;Configurable layout: gaps, borders, struts, window sizes&lt;/item&gt;
      &lt;item&gt;Gradient borders with Oklab and Oklch support&lt;/item&gt;
      &lt;item&gt;Animations with support for custom shaders&lt;/item&gt;
      &lt;item&gt;Live-reloading config&lt;/item&gt;
      &lt;item&gt;Works with screen readers&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="px-3 py-2"&gt;demo.mp4&lt;/head&gt;
    &lt;p&gt;Also check out this video from Brodie Robertson that showcases a lot of the niri functionality: Niri Is My New Favorite Wayland Compositor&lt;/p&gt;
    &lt;p&gt;Niri is stable for day-to-day use and does most things expected of a Wayland compositor. Many people are daily-driving niri, and are happy to help in our Matrix channel.&lt;/p&gt;
    &lt;p&gt;Give it a try! Follow the instructions on the Getting Started page. Have your waybars and fuzzels ready: niri is not a complete desktop environment. Also check out awesome-niri, a list of niri-related links and projects.&lt;/p&gt;
    &lt;p&gt;Here are some points you may have questions about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multi-monitor: yes, a core part of the design from the very start. Mixed DPI works.&lt;/item&gt;
      &lt;item&gt;Fractional scaling: yes, plus all niri UI stays pixel-perfect.&lt;/item&gt;
      &lt;item&gt;NVIDIA: seems to work fine.&lt;/item&gt;
      &lt;item&gt;Floating windows: yes, starting from niri 25.01.&lt;/item&gt;
      &lt;item&gt;Input devices: niri supports tablets, touchpads, and touchscreens. You can map the tablet to a specific monitor, or use OpenTabletDriver. We have touchpad gestures, but no touchscreen gestures yet.&lt;/item&gt;
      &lt;item&gt;Wlr protocols: yes, we have most of the important ones like layer-shell, gamma-control, screencopy. You can check on wayland.app at the bottom of each protocol's page.&lt;/item&gt;
      &lt;item&gt;Performance: while I run niri on beefy machines, I try to stay conscious of performance. I've seen someone use it fine on an Eee PC 900 from 2008, of all things.&lt;/item&gt;
      &lt;item&gt;Xwayland: integrated via xwayland-satellite starting from niri 25.08.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;niri: Making a Wayland compositor in Rust · December 2024&lt;/p&gt;
    &lt;p&gt;My talk from the 2024 Moscow RustCon about niri, and how I do randomized property testing and profiling, and measure input latency. The talk is in Russian, but I prepared full English subtitles that you can find in YouTube's subtitle language selector.&lt;/p&gt;
    &lt;p&gt;An interview with Ivan, the developer behind Niri · June 2025&lt;/p&gt;
    &lt;p&gt;An interview by a German tech podcast Das Triumvirat (in English). We talk about niri development and history, and my experience building and maintaining niri.&lt;/p&gt;
    &lt;p&gt;A tour of the niri scrolling-tiling Wayland compositor · July 2025&lt;/p&gt;
    &lt;p&gt;An LWN article with a nice overview and introduction to niri.&lt;/p&gt;
    &lt;p&gt;If you'd like to help with niri, there are plenty of both coding- and non-coding-related ways to do so. See CONTRIBUTING.md for an overview.&lt;/p&gt;
    &lt;p&gt;Niri is heavily inspired by PaperWM which implements scrollable tiling on top of GNOME Shell.&lt;/p&gt;
    &lt;p&gt;One of the reasons that prompted me to try writing my own compositor is being able to properly separate the monitors. Being a GNOME Shell extension, PaperWM has to work against Shell's global window coordinate space to prevent windows from overflowing.&lt;/p&gt;
    &lt;p&gt;Here are some other projects which implement a similar workflow:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PaperWM: scrollable tiling on top of GNOME Shell.&lt;/item&gt;
      &lt;item&gt;karousel: scrollable tiling on top of KDE.&lt;/item&gt;
      &lt;item&gt;scroll and papersway: scrollable tiling on top of sway/i3.&lt;/item&gt;
      &lt;item&gt;hyprscrolling and hyprslidr: scrollable tiling on top of Hyprland.&lt;/item&gt;
      &lt;item&gt;PaperWM.spoon: scrollable tiling on top of macOS.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our main communication channel is a Matrix chat, feel free to join and ask a question: https://matrix.to/#/#niri:matrix.org&lt;/p&gt;
    &lt;p&gt;We also have a community Discord server: https://discord.gg/vT8Sfjy7sx&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/YaLTeR/niri"/><published>2025-10-03T11:08:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45462143</id><title>A Thermometer for Measuring Quantumness</title><updated>2025-10-03T18:13:54.031934+00:00</updated><content>&lt;doc fingerprint="f5dbf1ebe585c5e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Thermometer for Measuring Quantumness&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;If there’s one law of physics that seems easy to grasp, it’s the second law of thermodynamics: Heat flows spontaneously from hotter bodies to colder ones. But now, gently and almost casually, Alexssandre de Oliveira Jr. has just shown me I didn’t truly understand it at all.&lt;/p&gt;
    &lt;p&gt;Take this hot cup of coffee and this cold jug of milk, the Brazilian physicist said as we sat in a café in Copenhagen. Bring them into contact and, sure enough, heat will flow from the hot object to the cold one, just as the German scientist Rudolf Clausius first stated formally in 1850. However, in some cases, de Oliveira explained, physicists have learned that the laws of quantum mechanics can drive heat flow the opposite way: from cold to hot.&lt;/p&gt;
    &lt;p&gt;This doesn’t really mean that the second law fails, he added as his coffee reassuringly cooled. It’s just that Clausius’ expression is the “classical limit” of a more complete formulation demanded by quantum physics.&lt;/p&gt;
    &lt;p&gt;Physicists began to appreciate the subtlety of this situation more than two decades ago and have been exploring the quantum mechanical version of the second law ever since. Now, de Oliveira, a postdoctoral researcher at the Technical University of Denmark, and colleagues have shown that the kind of “anomalous heat flow” that’s enabled at the quantum scale could have a convenient and ingenious use.&lt;/p&gt;
    &lt;p&gt;It can serve, they say, as an easy method for detecting “quantumness” — sensing, for instance, that an object is in a quantum “superposition” of multiple possible observable states, or that two such objects are entangled, with states that are interdependent — without destroying those delicate quantum phenomena. Such a diagnostic tool could be used to ensure that a quantum computer is truly using quantum resources to perform calculations. It might even help to sense quantum aspects of the force of gravity, one of the stretch goals of modern physics. All that’s needed, the researchers say, is to connect a quantum system to a second system that can store information about it, and to a heat sink: a body that’s able to absorb a lot of energy. With this setup, you can boost the transfer of heat to the heat sink, exceeding what would be permitted classically. Simply by measuring how hot the sink is, you could then detect the presence of superposition or entanglement in the quantum system.&lt;/p&gt;
    &lt;p&gt;Practical benefits aside, the research demonstrates a new aspect of a deep truth about thermodynamics: How heat and energy can be transformed and moved in physical systems is intimately bound up with information — what is or can be known about those systems. In this case, we “pay for” the anomalous heat flow by sacrificing stored information about the quantum system.&lt;/p&gt;
    &lt;p&gt;“I love the idea that thermodynamic quantities can signal quantum phenomena,” said the physicist Nicole Yunger Halpern of the University of Maryland. “The topic is fundamental and deep.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Knowledge Is Power&lt;/head&gt;
    &lt;p&gt;The connection between the second law of thermodynamics and information was first explored in the 19th century by the Scottish physicist James Clerk Maxwell. To Maxwell’s distress, Clausius’ second law seemed to imply that pockets of heat will dissipate throughout the universe until all temperature differences disappear. In the process, the total entropy of the universe — crudely, a measure of how disordered and featureless it is — will inexorably increase. Maxwell realized that this trend would eventually remove all possibility of harnessing heat flows to do useful work, and the universe would settle into a sterile equilibrium pervaded by a uniform buzz of thermal motion: a “heat death.” That forecast would be troubling enough to anyone. It was anathema to the devoutly Christian Maxwell. But in a letter to his friend Peter Guthrie Tait in 1867, Maxwell claimed to have found a way to “pick a hole” in the second law.&lt;/p&gt;
    &lt;p&gt;He imagined a tiny being (later dubbed a demon) who could see the motions of individual molecules in a gas. The gas would fill a box that was divided in two by a wall with a trapdoor. By opening and closing the trapdoor selectively, the demon could sequester the faster-moving molecules in one compartment and the slower-moving ones in the other, making a hot gas and a cold one, respectively. By acting on the information it gathered about molecules’ motions, the demon thus reduced the entropy of the gas, creating a temperature gradient that could be used to do mechanical work, such as pushing a piston.&lt;/p&gt;
    &lt;p&gt;Scientists felt sure that Maxwell’s demon couldn’t really violate the second law, but it took nearly 100 years to figure out why not. The answer is that the information the demon collects and stores about the molecular motions will eventually fill up its finite memory. Its memory must then be erased and reset for it to keep working. The physicist Rolf Landauer showed in 1961 that this erasure burns energy and produces entropy — more entropy than is reduced by the demon’s sorting actions. Landauer’s analysis established an equivalence between information and entropy, implying that information itself can act as a thermodynamic resource: It can be transformed into work. Physicists experimentally demonstrated this information-to-energy conversion in 2010.&lt;/p&gt;
    &lt;p&gt;But quantum phenomena allow information to be processed in ways that classical physics does not permit — that’s the entire basis of technologies such as quantum computing and quantum cryptography. And that’s why quantum theory messes with the conventional second law.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploiting Correlations&lt;/head&gt;
    &lt;p&gt;Entangled quantum objects have mutual information: They are correlated, so we can discover properties of one by looking at the other. That in itself is not so strange; if you look at one of a pair of gloves and find it’s left-handed, you know the other is right-handed. But a pair of entangled quantum particles differs from gloves in a particular way: Whereas the handedness of gloves is already fixed before you look, this isn’t the case for the particles, according to quantum mechanics. Before we measure them, it’s undecided which value of the observable property each particle in the entangled pair has. At that stage the only things we can know are the probabilities of the possible combinations of values, such as 50% left-right and 50% right-left. Only when we measure the state of one of the particles do these possibilities resolve themselves into a definite outcome. In that measurement process, the entanglement is destroyed.&lt;/p&gt;
    &lt;p&gt;If gas molecules are entangled in this way, then a Maxwell’s demon can manipulate them more efficiently than if all the molecules are moving independently. If, say, the demon knows that any fast-moving molecule it sees coming is correlated in such a way that it will be trailed by another fast one just a moment later, the demon doesn’t have to bother observing the second particle before opening the trapdoor to admit it. The thermodynamic cost of (temporarily) foiling the second law is lowered.&lt;/p&gt;
    &lt;p&gt;In 2004, the quantum theorists Časlav Brukner of the University of Vienna and Vlatko Vedral, then at Imperial College London, pointed out that this means macroscopic thermodynamic measurements can be used as a “witness” to reveal the presence of quantum entanglement between particles. Under certain conditions, they showed, a system’s heat capacity or its response to an applied magnetic field should carry an imprint of entanglement, if it is present.&lt;/p&gt;
    &lt;p&gt;In a similar vein, other physicists calculated that you can extract more work from a warm body when there is quantum entanglement in the system than when it is purely classical.&lt;/p&gt;
    &lt;p&gt;And in 2008, the physicist Hossein Partovi of California State University identified a particularly dramatic implication of the way quantum entanglement can undermine preconceptions derived from classical thermodynamics. He realized that the presence of entanglement can actually reverse the spontaneous flow of heat from a hot object to a cold one, seemingly upending the second law itself.&lt;/p&gt;
    &lt;p&gt;That reversal is a special kind of refrigeration, Yunger Halpern said. And as usual with refrigeration, it doesn’t come for free (and so doesn’t truly subvert the second law). Classically, refrigerating an object takes work: We have to pump the heat the “wrong” way by consuming fuel, thereby repaying the entropy that’s lost by making the cold object colder and the hot object hotter. But in the quantum case, Yunger Halpern said, instead of burning fuel to achieve refrigeration, “you burn the correlations.” In other words, as the anomalous heat flow proceeds, the entanglement gets destroyed: Particles that initially had correlated properties become independent. “We can use the correlations as a resource to push heat in the opposite direction,” Yunger Halpern said.&lt;/p&gt;
    &lt;p&gt;In effect, the fuel here is information itself: specifically the mutual information of the entangled hot and cold bodies.&lt;/p&gt;
    &lt;p&gt;Two years later, David Jennings and Terry Rudolph of Imperial College London clarified what’s going on. They showed how the second law of thermodynamics can be reformulated to include the case where mutual information is present, and they calculated the limits on how much the classical heat flow can be altered and even reversed by the consumption of quantum correlations.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Demon Knows&lt;/head&gt;
    &lt;p&gt;When quantum effects are in play, then, the second law isn’t so simple. But can we do anything useful with the way quantum physics loosens the bounds of thermodynamic laws? That’s one of the goals of the discipline called quantum thermodynamics, in which some researchers seek to make quantum engines that run more efficiently than classical ones, or quantum batteries that charge more quickly.&lt;/p&gt;
    &lt;p&gt;Patryk Lipka-Bartosik of the Center for Theoretical Physics at the Polish Academy of Sciences has sought practical applications in the other direction: using thermodynamics as a tool for probing quantum physics. Last year, he and his co-workers saw how to realize Brukner and Vedral’s 2004 idea to use thermodynamic properties as a witness of quantum entanglement. Their scheme involves hot and cold quantum systems that are correlated with each other, and a third system to mediate the heat flow between the two. We can think of this third system as a Maxwell’s demon, except now it has a “quantum memory” that can itself be entangled with the systems it is manipulating. Being entangled with the demon’s memory effectively links the hot and cold systems so that the demon can infer something about one from the properties of the other.&lt;/p&gt;
    &lt;p&gt;Such a quantum demon can act as a kind of catalyst, helping heat transfer happen by accessing correlations that are inaccessible otherwise. That is, because it is entangled with the hot and cold objects, the demon can divine and exploit all their correlations systematically. And, again like a catalyst, this third system returns to its original state once the heat exchange between the objects is completed. In this way, the process can boost the anomalous heat flow beyond what can be achieved without such a catalyst.&lt;/p&gt;
    &lt;p&gt;The paper this year by de Oliveira, co-authored by Lipka-Bartosik and Jonatan Bohr Brask of the Technical University of Denmark, uses some of these same ideas but with a crucial difference that turns the setup into a kind of thermometer for measuring quantumness. In the earlier work, the demonlike quantum memory interacted with a correlated pair of quantum systems, one hot and one cold. But in the latest work, it sits between a quantum system (say, an array of entangled quantum bits, or qubits, in a quantum computer) and a simple heat sink with which the quantum system is not directly entangled.&lt;/p&gt;
    &lt;p&gt;Because the memory is entangled with both the quantum system and the sink, it can again catalyze heat flow between them beyond what is possible classically. In that process, entanglement within the quantum system converts into extra heat that enters the sink. So measuring the energy stored in the heat sink (akin to reading its “temperature”) reveals the presence of entanglement in the quantum system. But since the system and sink aren’t themselves entangled, the measurement doesn’t affect the state of the quantum system. This gambit circumvents the notorious way that measurements destroy quantumness. “If you simply tried to make a measurement on the [quantum] system directly, you’d destroy its entanglement before the process could even unfold,” de Oliveira said.&lt;/p&gt;
    &lt;p&gt;The new scheme has the advantage of being simple and general, said Vedral, who is now at the University of Oxford. “These verification protocols are very important,” he said: Whenever some quantum computer company makes a new announcement about the performance of its latest device, he said the question always arises of how (or if) they really know that entanglement among the qubits is helping with the computation. A heat sink could serve as a detector of such quantum phenomena purely via its energy change. To implement the idea, you might designate one quantum bit as the memory whose state reveals that of other qubits, and then couple this memory qubit to a set of particles that will serve as the sink, whose energy you can measure. (One proviso, Vedral added, is that you need to have very good control over your system to be sure there aren’t other sources of heat flow contaminating the measurements. Another is that the method will not detect all entangled states.)&lt;/p&gt;
    &lt;p&gt;De Oliveira thinks that a system already exists for testing their idea experimentally. He and his colleagues are discussing that goal with Roberto Serra’s research group at the Federal University of ABC in São Paulo, Brazil. In 2016, Serra and colleagues used the magnetic orientations, or spins, of carbon and hydrogen atoms in molecules of chloroform as quantum bits between which they could transfer heat.&lt;/p&gt;
    &lt;p&gt;Using this setup, de Oliveira says it should be possible to exploit a quantum behavior — in this case coherence, meaning that the properties of two or more spins are evolving in phase with one another — to change the heat flow between the atoms. Coherence of qubits is essential for quantum computing, so being able to verify it by detecting anomalous heat exchange could be helpful.&lt;/p&gt;
    &lt;p&gt;The stakes could be even higher. Several research groups are trying to design experiments to determine whether gravity is a quantum force like the other three fundamental forces. Some of these efforts involve looking for quantum entanglement between two objects generated purely by their mutual gravitational attraction. Perhaps researchers could probe such gravity-induced entanglement by making simple thermodynamic measurements on them — thereby verifying (or not) that gravity really is quantized.&lt;/p&gt;
    &lt;p&gt;To study one of the deepest questions in physics, Vedral said, “wouldn’t it be lovely if you could do something as easy and macroscopic as this?”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/a-thermometer-for-measuring-quantumness-20251001/"/><published>2025-10-03T12:24:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45462297</id><title>Faroes</title><updated>2025-10-03T18:13:51.713435+00:00</updated><content>&lt;doc fingerprint="b6601c6e8caa7e9e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Faroes (2025)&lt;/head&gt;
    &lt;p&gt;The Faroe Islands are like the child that Denmark and Iceland had, but forgot to tell the world about. This group of eighteen small islands receives the least amount of sunshine in the world per year. Constant rain and heavy winds have always battered these lands.&lt;lb/&gt;Politically part of Denmark (for now) but fiercely independent in spirit, the Faroes exist in their own bubble of Nordic culture. Here, sheep outnumber humans two to one, villages of colorful houses cling to clifftops like they're holding on for dear life, and the weather can shift from apocalyptic storms to sunny calm in the space of an hour.&lt;/p&gt;
    &lt;p&gt;Situated between Iceland, Norway and Scotland, the Faroes face the brunt of the North Atlantic weather system. Constant storms and crashing waves have sculpted the volcanic rock over millions of years into some of the most jaw-dropping (and vertigo-inducing) coastlines on Earth. These towering basalt cliffs can reach heights of over 400 meters, dropping straight into churning seas below.&lt;lb/&gt;What's most striking is how abruptly the land stops. There are no sandy beaches or gentle slopes here—the islands simply plunge headfirst into the Atlantic. One step you're on grass-covered clifftops, the next you're staring down hundreds of meters of sheer volcanic rock to where waves explode against the base far below.&lt;/p&gt;
    &lt;p&gt;The weather here is unpredictable, and changes faster than you can put your raincoat on—one minute you're in thick fog, the next you're hit with winds and piercing rain that'll knock you sideways, then suddenly the clouds part to reveal views that'll make your camera work overtime.&lt;/p&gt;
    &lt;p&gt;Meet the true locals of the Faroes. These wooly sheep have been roaming the islands for over a thousand years, and they outnumber people on the islands. They couldn't care less about your hiking plans and will casually block paths or graze on the edge of 200-meter cliffs like it's the most natural thing in the world.&lt;lb/&gt;Faroe's name comes from a combination of fær (sheep) and eyjar (islands). &lt;/p&gt;
    &lt;p&gt;Unlike their farm-bound cousins elsewhere, Faroese sheep roam completely free across the islands, somehow always managing to find the most photogenic spots for an impromptu rest. This fellow right here is the only one that gave me any sort of attention. Otherwise, they are all busy grazing on all the grass they could ever ask for.&lt;/p&gt;
    &lt;p&gt;Why fight the landscape? For over a millennium, islanders have been topping their huts with birch bark and soil and let the grass grow wild. They act as insulation, and the thick roots are an excellent waterproof seal against the weather.&lt;lb/&gt;The grass grows quickly and does need tending every once in a while. In typical Faroese fashion, the solution is simple: put a sheep on top for an afternoon.&lt;/p&gt;
    &lt;p&gt;On the northern tip of Kalsoy lies the Kallur lighthouse. Like most regions on the islands, the land is privately owned. Hiking usually incurs a modest fee paid at the trailhead to the land owners, and the rest is up to you. Trails are just sheep paths, worn smooth by countless hooves over years rather than any official trail maintenance.&lt;/p&gt;
    &lt;p&gt;There are no guardrails, no warning signs, and definitely no liability waivers - just you, the weather, and whatever route the sheep decided made sense. The approach to Kallur is particularly gnarly, following a knife-edge ridge with steep drops on both sides before reaching the lighthouse perched dramatically on sea cliffs.&lt;/p&gt;
    &lt;p&gt;In No Time To Die (2021), Daniel Craig's James Bond meets his end at the villain's lair, which happened to be here on Kalsoy. The Faroese then followed through with the obvious next step.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://photoblog.nk412.com/Faroe2025/Faroes/n-cPCNFr"/><published>2025-10-03T12:41:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45462713</id><title>CVE-2025-59489: Arbitrary Code Execution in Unity Runtime since 2017</title><updated>2025-10-03T18:13:51.320112+00:00</updated><content>&lt;doc fingerprint="b56811138876f9e9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;CVE-2025-59489: Arbitrary Code Execution in Unity Runtime&lt;/head&gt;
    &lt;head rend="h5"&gt;Posted on October 3, 2025 • 6 minutes • 1067 words&lt;/head&gt;
    &lt;head class="flex items-center font-bold py-2 px-4 cursor-pointer justify-between select-none text-black dark:text-white"&gt;Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Hello, I’m RyotaK (@ryotkak ), a security engineer at GMO Flatt Security Inc.&lt;/p&gt;
    &lt;p&gt;In May 2025, I participated in the Meta Bug Bounty Researcher Conference 2025. During this event, I discovered a vulnerability (CVE-2025-59489) in the Unity Runtime that affects games and applications built on Unity 2017.1 and later.&lt;/p&gt;
    &lt;p&gt;In this article, I will explain the technical aspects of this vulnerability and its impact.&lt;/p&gt;
    &lt;p&gt;This vulnerability was disclosed to Unity following responsible disclosure practices.&lt;lb/&gt; Unity has since released patches for Unity 2019.1 and later, as well as a Unity Binary Patch tool to address the issue, and I strongly encourage developers to download the updated versions of Unity, recompile affected games or applications, and republish as soon as possible.&lt;/p&gt;
    &lt;p&gt;For the official security advisory, please refer to Unity’s advisory here: https://unity.com/security/sept-2025-01&lt;/p&gt;
    &lt;p&gt;We appreciate Unity’s commitment to addressing this issue promptly and their ongoing efforts to enhance the security of their platform.&lt;lb/&gt; Security vulnerabilities are an inherent challenge in software development, and by working together as a community, we can continue to make software systems safer for everyone.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;A vulnerability was identified in the Unity Runtime’s intent handling process for Unity games and applications.&lt;lb/&gt; This vulnerability allows malicious intents to control command line arguments passed to Unity applications, enabling attackers to load arbitrary shared libraries (&lt;code&gt;.so&lt;/code&gt; files) and execute malicious code, depending on the platform.&lt;/p&gt;
    &lt;p&gt;In its default configuration, this vulnerability allowed malicious applications installed on the same device to hijack permissions granted to Unity applications.&lt;lb/&gt; In specific cases, the vulnerability could be exploited remotely to execute arbitrary code, although I didn’t investigate third-party Unity applications to find an app with the functionality required to enable this exploit.&lt;/p&gt;
    &lt;p&gt;Unity has addressed this issue and has updated all affected Unity versions starting with 2019.1. Developers are strongly encouraged to download them, recompile their games and applications, and republish to ensure their projects remain secure.&lt;/p&gt;
    &lt;head rend="h2"&gt;About Unity&lt;/head&gt;
    &lt;p&gt;Unity is a popular game engine used to develop games and applications for various platforms, including Android.&lt;/p&gt;
    &lt;p&gt;According to Unity’s website, 70% of top mobile games are built with Unity. This includes popular games like Among Us and Pokémon GO, along with many other applications that use Unity for development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical Details&lt;/head&gt;
    &lt;p&gt;Note: During the analysis, I used Android 16.0 on the Android Emulator of Android Studio. The behavior and impact of this vulnerability may differ on older Android versions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unity’s Intent Handler&lt;/head&gt;
    &lt;p&gt;To support debugging Unity applications on Android devices, Unity automatically adds a handler for the intent containing the &lt;code&gt;unity&lt;/code&gt; extra to the UnityPlayerActivity. This activity serves as the default entry point for applications and is exported to other applications.&lt;/p&gt;
    &lt;p&gt;https://docs.unity3d.com/6000.0/Documentation/Manual/android-custom-activity-command-line.html&lt;/p&gt;
    &lt;code&gt;adb shell am start -n "com.Company.MyGame/com.unity3d.player.UnityPlayerActivity" -e unity "-systemallocator"
&lt;/code&gt;
    &lt;p&gt;As documented above, the &lt;code&gt;unity&lt;/code&gt; extra is parsed as command line arguments for Unity.&lt;/p&gt;
    &lt;p&gt;While Android’s permission model manages feature access by granting permissions to applications, it does not restrict which intents can be sent to an application.&lt;lb/&gt; This means any application can send the &lt;code&gt;unity&lt;/code&gt; extra to a Unity application, allowing attackers to control the command line arguments passed to that application.&lt;/p&gt;
    &lt;head rend="h3"&gt;xrsdk-pre-init-library Command Line Argument&lt;/head&gt;
    &lt;p&gt;After loading the Unity Runtime binary into Ghidra, I discovered the following command line argument:&lt;/p&gt;
    &lt;code&gt;initLibPath = FUN_00272540(uVar5, "xrsdk-pre-init-library");
&lt;/code&gt;
    &lt;p&gt;The value of this command line argument is later passed to &lt;code&gt;dlopen&lt;/code&gt;, causing the path specified in &lt;code&gt;xrsdk-pre-init-library&lt;/code&gt; to be loaded as a native library.&lt;/p&gt;
    &lt;code&gt;lVar2 = dlopen(initLibPath, 2);  
&lt;/code&gt;
    &lt;p&gt;This behavior allows attackers to execute arbitrary code within the context of the Unity application, leveraging its permissions by launching them with the -xrsdk-pre-init-library argument.&lt;/p&gt;
    &lt;head rend="h2"&gt;Attack Scenarios&lt;/head&gt;
    &lt;head rend="h3"&gt;Local Attack&lt;/head&gt;
    &lt;p&gt;Any malicious application installed on the same device can exploit this vulnerability by:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Extracting the native library with the &lt;code&gt;android:extractNativeLibs&lt;/code&gt;attribute set to&lt;code&gt;true&lt;/code&gt;in the AndroidManifest.xml&lt;/item&gt;
      &lt;item&gt;Launching the Unity application with the &lt;code&gt;-xrsdk-pre-init-library&lt;/code&gt;argument pointing to the malicious library&lt;/item&gt;
      &lt;item&gt;The Unity application would then load and execute the malicious code with its own permissions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Remote Exploitation via Browser&lt;/head&gt;
    &lt;p&gt;In specific cases, this vulnerability could potentially be exploited remotely although the condition .&lt;lb/&gt; For example, if an application exports &lt;code&gt;UnityPlayerActivity&lt;/code&gt; or &lt;code&gt;UnityPlayerGameActivity&lt;/code&gt; with the &lt;code&gt;android.intent.category.BROWSABLE&lt;/code&gt; category (allowing browser launches), websites can specify extras passed to the activity using intent URLs:&lt;/p&gt;
    &lt;code&gt;intent:#Intent;package=com.example.unitygame;scheme=custom-scheme;S.unity=-xrsdk-pre-init-library%20/data/local/tmp/malicious.so;end;
&lt;/code&gt;
    &lt;p&gt;At first glance, it might appear that malicious websites could exploit this vulnerability by forcing browsers to download &lt;code&gt;.so&lt;/code&gt; files and load them via the &lt;code&gt;xrsdk-pre-init-library&lt;/code&gt; argument.&lt;/p&gt;
    &lt;head rend="h3"&gt;SELinux Restrictions&lt;/head&gt;
    &lt;p&gt;However, Android’s strict SELinux policy prevents &lt;code&gt;dlopen&lt;/code&gt; from opening files in the downloads directory, which mitigates almost all remote exploitation scenarios.&lt;/p&gt;
    &lt;code&gt;library "/sdcard/Download/libtest.so" ("/storage/emulated/0/Download/libtest.so") needed 
or dlopened by "/data/app/~~24UwD8jnw7asNjRwx1MOBg==/com.DefaultCompany.com.unity.template. 
mobile2D-E043IptGJDwcTqq56BocIA==/lib/arm64/libunity.so" is not accessible for the 
namespace: [name="clns-9", ld_library_paths="",default_library_paths="/data/app/~~24UwD8jnw7asNjRwx1MOBg==/com.DefaultCompany.com.unity.template. 
mobile2D-E043IptGJDwcTqq56BocIA==/lib/arm64:/data/app/~~24UwD8jnw7asNjRwx1MOBg==/com.DefaultCompany.com.unity.template.mobile2D-E043IptGJDwcTqq56BocIA==/base.apk!/lib/arm64-v8a", permitted_paths="/data:/mnt/expand:/data/data/com.DefaultCompany.com.unity.template.mobile2D"]
&lt;/code&gt;
    &lt;p&gt;That being said, since the &lt;code&gt;/data/&lt;/code&gt; directory is included in &lt;code&gt;permitted_paths&lt;/code&gt;, if the target application writes files to its private storage, it can be used to bypass this restriction.&lt;/p&gt;
    &lt;p&gt;Furthermore, &lt;code&gt;dlopen&lt;/code&gt; doesn’t require the &lt;code&gt;.so&lt;/code&gt; file extension. If attackers can control the content of a file in an application’s private storage, they can exploit this vulnerability by creating a file containing malicious native library binary. This is actually a common pattern when applications cache data.&lt;/p&gt;
    &lt;p&gt;For example, another vulnerability in Messenger was exploited using the application’s cache: https://www.hexacon.fr/slides/Calvanno-Defense_through_Offense_Building_a_1-click_Exploit_Targeting_Messenger_for_Android.pdf&lt;/p&gt;
    &lt;head rend="h3"&gt;Requirements for Remote Exploitation&lt;/head&gt;
    &lt;p&gt;To exploit this vulnerability remotely, the following conditions must be met:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The application exports &lt;code&gt;UnityPlayerActivity&lt;/code&gt;or&lt;code&gt;UnityPlayerGameActivity&lt;/code&gt;with the&lt;code&gt;android.intent.category.BROWSABLE&lt;/code&gt;category&lt;/item&gt;
      &lt;item&gt;The application writes files with attacker-controlled content to its private storage (e.g., through caching)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even without these conditions, local exploitation remains possible for any Unity application.&lt;/p&gt;
    &lt;head rend="h2"&gt;Demonstration&lt;/head&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In this article, I explained a vulnerability in Unity Runtime that allows arbitrary code execution in almost all Unity applications on Android.&lt;/p&gt;
    &lt;p&gt;I hope this article helps you understand that vulnerabilities can exist in the frameworks and libraries you depend on, and you should always be mindful of the security implications of the features you use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Shameless plug&lt;/head&gt;
    &lt;p&gt;At GMO Flatt Security, we provide top-notch penetration testing for a wide range of targets, from Web apps to IoT devices.&lt;/p&gt;
    &lt;p&gt;https://flatt.tech/en/professional/penetration_test&lt;/p&gt;
    &lt;p&gt;We also developed Takumi, our AI security engineer. It’s an autonomous agent that finds vulnerabilities in source code and has already discovered CVEs in major libraries like Vim and Next.js. https://flatt.tech/en/takumi&lt;/p&gt;
    &lt;p&gt;Recently, we’ve expanded Takumi’s capabilities. It’s no longer just a SAST (white-box testing) tool; we’ve added DAST (black-box testing) to enable high-fidelity gray-box scanning for more accurate results.&lt;/p&gt;
    &lt;p&gt;Based in Japan, we work with clients globally, including industry leaders like Canonical Ltd.&lt;/p&gt;
    &lt;p&gt;If you’d like to learn more, please contact us at https://flatt.tech/en&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://flatt.tech/research/posts/arbitrary-code-execution-in-unity-runtime/"/><published>2025-10-03T13:21:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45463251</id><title>Webbol: A minimal static web server written in COBOL</title><updated>2025-10-03T18:13:50.790792+00:00</updated><content>&lt;doc fingerprint="11bcbc59fd061f87"&gt;
  &lt;main&gt;
    &lt;p&gt;A minimal static web server written in COBOL using GnuCOBOL.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Serves static files from the current directory&lt;/item&gt;
      &lt;item&gt;Automatic MIME type detection for common file types&lt;/item&gt;
      &lt;item&gt;HTTP status codes: 200 (OK), 403 (Forbidden), 404 (Not Found), 413 (Payload Too Large)&lt;/item&gt;
      &lt;item&gt;Path traversal attack prevention&lt;/item&gt;
      &lt;item&gt;Clean request logging with full HTTP headers&lt;/item&gt;
      &lt;item&gt;Defaults to &lt;code&gt;index.html&lt;/code&gt;for root path requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GnuCOBOL (cobc) compiler&lt;/item&gt;
      &lt;item&gt;POSIX-compatible operating system (Linux, macOS, BSD)&lt;/item&gt;
      &lt;item&gt;make&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;macOS:&lt;/p&gt;
    &lt;code&gt;brew install gnucobol&lt;/code&gt;
    &lt;p&gt;Ubuntu/Debian:&lt;/p&gt;
    &lt;code&gt;sudo apt-get install gnucobol&lt;/code&gt;
    &lt;p&gt;Fedora/RHEL:&lt;/p&gt;
    &lt;code&gt;sudo dnf install gnucobol&lt;/code&gt;
    &lt;p&gt;Clone or download the repository, then compile:&lt;/p&gt;
    &lt;code&gt;make&lt;/code&gt;
    &lt;p&gt;This will compile all modules and create the &lt;code&gt;webserver&lt;/code&gt; executable.&lt;/p&gt;
    &lt;p&gt;To clean build artifacts:&lt;/p&gt;
    &lt;code&gt;make clean&lt;/code&gt;
    &lt;p&gt;Start the server from the directory you want to serve:&lt;/p&gt;
    &lt;code&gt;./webserver&lt;/code&gt;
    &lt;p&gt;The server will start on port 8080 and serve files from the current directory.&lt;/p&gt;
    &lt;code&gt;# Create a test HTML file
echo "&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;Hello from COBOL!&amp;lt;/h1&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;" &amp;gt; index.html

# Start the server
./webserver

# In another terminal, test it
curl http://localhost:8080/&lt;/code&gt;
    &lt;p&gt;Once running, you can access files via:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;http://localhost:8080/&lt;/code&gt;- serves&lt;code&gt;index.html&lt;/code&gt;from the current directory&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;http://localhost:8080/filename.html&lt;/code&gt;- serves the specified file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;http://localhost:8080/path/to/file.txt&lt;/code&gt;- serves files from subdirectories&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Press &lt;code&gt;Ctrl+C&lt;/code&gt; to stop the server.&lt;/p&gt;
    &lt;p&gt;To change the server port, edit &lt;code&gt;config.cpy&lt;/code&gt; and modify the &lt;code&gt;SERVER-PORT&lt;/code&gt; value:&lt;/p&gt;
    &lt;code&gt;01 SERVER-PORT          PIC 9(5) VALUE 8080.&lt;/code&gt;
    &lt;p&gt;Then recompile with &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;webbol/
├── Makefile              # Build configuration
├── README.md            # This file
├── config.cpy           # Server configuration
├── socket-defs.cpy      # Socket structure definitions
├── http-structs.cpy     # HTTP data structures
├── file-structs.cpy     # File handling structures
├── path-utils.cbl       # Path validation and sanitization
├── mime-types.cbl       # MIME type detection
├── file-ops.cbl         # File reading operations
├── http-handler.cbl     # HTTP request/response handling
└── webserver.cbl        # Main server program
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HTML: &lt;code&gt;text/html&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;CSS: &lt;code&gt;text/css&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;JavaScript: &lt;code&gt;application/javascript&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;JSON: &lt;code&gt;application/json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;XML: &lt;code&gt;application/xml&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Plain text: &lt;code&gt;text/plain&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;PNG: &lt;code&gt;image/png&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;JPEG: &lt;code&gt;image/jpeg&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;GIF: &lt;code&gt;image/gif&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;SVG: &lt;code&gt;image/svg+xml&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ICO: &lt;code&gt;image/x-icon&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;PDF: &lt;code&gt;application/pdf&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additional MIME types can be added by editing &lt;code&gt;mime-types.cbl&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Path traversal prevention: Blocks requests containing &lt;code&gt;..&lt;/code&gt;sequences&lt;/item&gt;
      &lt;item&gt;Directory access restriction: Only serves files from the current directory and subdirectories&lt;/item&gt;
      &lt;item&gt;Safe file handling: Validates all paths before file system access&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single-threaded: Handles one request at a time&lt;/item&gt;
      &lt;item&gt;No SSL/TLS support&lt;/item&gt;
      &lt;item&gt;Maximum file size: 64KB&lt;/item&gt;
      &lt;item&gt;Line sequential file organization only (text files)&lt;/item&gt;
      &lt;item&gt;No caching or compression&lt;/item&gt;
      &lt;item&gt;No range requests or partial content support&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Port already in use:&lt;/p&gt;
    &lt;code&gt;Bind failed - check if port is in use
&lt;/code&gt;
    &lt;p&gt;Another process is using port 8080. Either stop that process or change the port in &lt;code&gt;config.cpy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Permission denied: Ensure the files you're trying to serve have read permissions and the current user can access them.&lt;/p&gt;
    &lt;p&gt;File not found (404): Verify the file exists in the current directory where the server is running. File paths are case-sensitive.&lt;/p&gt;
    &lt;p&gt;This project is released into the public domain. Use it however you'd like.&lt;/p&gt;
    &lt;p&gt;Built with GnuCOBOL, demonstrating that COBOL can still be used for modern systems programming tasks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/jmsdnns/webbol"/><published>2025-10-03T14:13:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45463319</id><title>I Turned the Lego Game Boy into a Working Game Boy</title><updated>2025-10-03T18:13:49.918097+00:00</updated><content>&lt;doc fingerprint="15cf6c54ebc4b5f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I turned the Lego Game Boy into a working Game Boy part. 1&lt;/head&gt;
    &lt;p&gt;Through my documentation of Game Boy boards, I have drawn up schematics of each device. I know them pretty well. Check out my board scan wiki https://wiki.nataliethenerd.com/&lt;/p&gt;
    &lt;p&gt;I jokingly made this tweet when the kit was announced, but decided to actually do it.&lt;/p&gt;
    &lt;p&gt;I know from experience of routing Game Boy CPU PCBs that there isn't much to it. There's the RAM, CPU, some decoupling capacitors and power regulation. &lt;lb/&gt;Note: I went with the MGB (Pocket) CPU rather than DMG for a couple of reasons.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They are pretty much the same&lt;/item&gt;
      &lt;item&gt;I have more of them&lt;/item&gt;
      &lt;item&gt;They are cheaper and easier to get. This opens up the project to more people&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The DMG CPU has external VRAM, the MGB CPU has internal VRAM and in a very space conscious build that was the biggest factor.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pre Planning&lt;/head&gt;
    &lt;p&gt;I only had the press pictures to work off. I used the dimensions to scale the image on my PC and from that I got measurements for the screen inserts; since that's where I plan to put the Game Boy.&lt;/p&gt;
    &lt;p&gt;I incorporated the power circuit I use for my Safer Charger boards, changed the power switch to a soft latching power button, added pin outs for the button matrix and audio.&lt;/p&gt;
    &lt;p&gt;I didn't really know what the buttons on the Lego would be like, but the fact that they could be pressed was enough for me to know I could implement them. At the moment I have them wired up to custom 3D printed *toy brick* parts. Same with the USB C&lt;/p&gt;
    &lt;p&gt;I am currently working on refining the board now I have the Lego build in my hands. This project will be released in full once I am finished with it - so stay tuned!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.nataliethenerd.com/i-turned-the-lego-game-boy-into-a-working-game-boy-part-1/"/><published>2025-10-03T14:18:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45463642</id><title>Microsoft CTO says he wants to swap most AMD and Nvidia GPUs for homemade chips</title><updated>2025-10-03T18:13:49.549186+00:00</updated><content>&lt;doc fingerprint="37f0403644aec8d5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft CTO says he wants to swap most AMD and Nvidia GPUs for homemade chips&lt;/head&gt;
    &lt;head rend="h2"&gt;Pivot will hinge on success of next-gen Maia accelerator&lt;/head&gt;
    &lt;p&gt;Microsoft buys a lot of GPUs from both Nvidia and AMD. But moving forward, Redmond's leaders want to shift the majority of its AI workloads from GPUs to its own homegrown accelerators.&lt;/p&gt;
    &lt;p&gt;The software titan is rather late to the custom silicon party. While Amazon and Google have been building custom CPUs and AI accelerators for years, Microsoft only revealed its Maia AI accelerators in late 2023.&lt;/p&gt;
    &lt;p&gt;Driving the transition is a focus on performance per dollar, which for a hyperscale cloud provider is arguably the only metric that really matters. Speaking during a fireside chat moderated by CNBC on Wednesday, Microsoft CTO Kevin Scott said that up to this point, Nvidia has offered the best price-performance, but he's willing to entertain anything in order to meet demand.&lt;/p&gt;
    &lt;p&gt;Going forward, Scott suggested Microsoft hopes to use its homegrown chips for the majority of its datacenter workloads.&lt;/p&gt;
    &lt;p&gt;When asked, "Is the longer term idea to have mainly Microsoft silicon in the data center?" Scott responded, "Yeah, absolutely."&lt;/p&gt;
    &lt;p&gt;Later, he told CNBC, "It's about the entire system design. It's the networks and cooling, and you want to be able to have the freedom to make decisions that you need to make in order to really optimize your compute for the workload."&lt;/p&gt;
    &lt;p&gt;With its first in-house AI accelerator, the Maia 100, Microsoft was able to free up GPU capacity by shifting OpenAI's GPT-3.5 to its own silicon back in 2023. However, with just 800 teraFLOPS of BF16 performance, 64GB of HBM2e, and 1.8TB/s of memory bandwidth, the chip fell well short of competing GPUs from Nvidia and AMD.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alibaba unveils $53B global AI plan – but it will need GPUs to back it up&lt;/item&gt;
      &lt;item&gt;Arm wrestles away 25% share of server market thanks to Nvidia's home-grown CPUs&lt;/item&gt;
      &lt;item&gt;SiPearl ships reference node design for Rhea1 high-spec Arm chip&lt;/item&gt;
      &lt;item&gt;Arm reckons it'll own 50% of the datacenter by year's end&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Microsoft is reportedly in the process of bringing a second-generation Maia accelerator to market next year that will no doubt offer more competitive compute, memory, and interconnect performance.&lt;/p&gt;
    &lt;p&gt;But while we may see a change in the mix of GPUs to AI ASICs in Microsoft data centers moving forward, they're unlikely to replace Nvidia and AMD's chips entirely.&lt;/p&gt;
    &lt;p&gt;Over the past few years, Google and Amazon have deployed tens of thousands of their TPUs and Trainium accelerators. While these chips have helped them secure some high-profile customer wins, Anthropic for example, these chips are more often used to accelerate the company's own in-house workloads.&lt;/p&gt;
    &lt;p&gt;As such, we continue to see large-scale Nvidia and AMD GPU deployments on these cloud platforms, in part because customers still want them.&lt;/p&gt;
    &lt;p&gt;It should be noted that AI accelerators aren't the only custom chips Microsoft has been working on. Redmond also has its own CPU called Cobalt and a whole host of platform security silicon designed to accelerate cryptography and safeguard key exchanges across its vast datacenter domains. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/10/02/microsoft_maia_dc/"/><published>2025-10-03T14:48:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45463656</id><title>Social anxiety isn't about being liked</title><updated>2025-10-03T18:13:49.145438+00:00</updated><content>&lt;doc fingerprint="3b56d81500d3c6b5"&gt;
  &lt;main&gt;
    &lt;p&gt;There's this popular idea that socially anxious folks are just dying to be liked. It seems logical, right? Why else would someone be so anxious about how others see them?&lt;/p&gt;
    &lt;p&gt;And yet, being socially anxious tends to make you less likeable…they must be optimizing poorly, behaving irrationally, right?&lt;/p&gt;
    &lt;p&gt;Maybe not. What if social anxiety isn’t about getting people to like you? What if it's about stopping them from disliking you?&lt;/p&gt;
    &lt;p&gt;Consider what can happen when someone has social anxiety (or self-loathing, self-doubt, insecurity, lack of confidence, etc.):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;They stoop or take up less space&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They become less agentic&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They make fewer requests of others&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They maintain fewer relationships, go out less, take fewer risks…&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If they were trying to get people to like them, becoming socially anxious would be an incredibly bad strategy.&lt;/p&gt;
    &lt;p&gt;So what if they're not concerned with being likeable?&lt;/p&gt;
    &lt;head rend="h2"&gt;What if what they actually want is to avoid being disliked?&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;To understand the object of an obscure plot, observe its consequences and ask who might have intended them… —Harry Potter and the Methods of Rationality&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;What if the socially anxious were calibrating to avoid being DISliked?&lt;/p&gt;
    &lt;p&gt;Consider: if you shrink and never make any attention-getting moves, you are less likely to dangerously disappoint others, get into risky conflicts or be seen as a failure, embarrassment, or threat.&lt;/p&gt;
    &lt;p&gt;Like, yeah, it's wonderful to do awesome things and have people love you. But you know what’s better than being loved? People not hating you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Social anxiety is a symptom of risk aversion&lt;/head&gt;
    &lt;p&gt;It’s not a pursuit of potential upside, but an attempt to avoid downsides.&lt;/p&gt;
    &lt;p&gt;Once you catch on to this pattern, you see it everywhere.&lt;/p&gt;
    &lt;p&gt;Two examples:&lt;/p&gt;
    &lt;p&gt;1) When you feel financially insecure, you’re not optimizing for windfall as much as you’re optimizing for not going bankrupt. You avoid risky bets with higher EV in favor of safer, more predictable options, even if they offer smaller returns. The goal is to keep you fed, not to make you rich.&lt;/p&gt;
    &lt;p&gt;2) Reversely, countersignalling is a demonstration of safety in close relationships. In Scott Alexander’s Friendship is Countersignalling, he describes an interaction he has with a friend:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Becca: What are you doing here? I figured they’d have locked you away in the psych ward for good by now.&lt;/p&gt;
      &lt;p&gt;Scott: Nope. And what are you doing here? You haven’t killed off all your patients yet?&lt;/p&gt;
      &lt;p&gt;Becca: Only person in this hospital I might kill is standing right in front of me.&lt;/p&gt;
      &lt;p&gt;Scott: Be careful, I’m armed and dangerous *picks up a central line placement practice set menacingly*&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The security of good friendship diffuses your anxiety about making a social faux pas and enables you to take more risks.&lt;/p&gt;
    &lt;head rend="h2"&gt;What does this mean for your growth?&lt;/head&gt;
    &lt;p&gt;If you believe your primary goal is to "be liked" and you keep finding yourself hiding in the shadows, you'll feel like a total failure. This hurts!&lt;/p&gt;
    &lt;p&gt;But all our feelings have their own kind of logic. Even when we do things that seem self-sabotaging, there's usually an incentive that makes sense in that specific context – even if it maybe not the best strategy overall. Locally optimal!&lt;/p&gt;
    &lt;p&gt;Consider: what if all these symptoms of social anxiety aren't failures of a system trying to be liked, but successes of a system trying to avoid being disliked?&lt;/p&gt;
    &lt;p&gt;What if you’ve been operating pretty rationally this whole time, but not for the outcome you thought you were optimizing for?&lt;/p&gt;
    &lt;p&gt;What if you’re not failing at being liked - you’re succeeding at avoiding being disliked?&lt;/p&gt;
    &lt;p&gt;Recognize this, and you’ll be able to shift your focus to the real work: becoming comfortable with the worst-case scenarios your anxiety is protecting you from.&lt;/p&gt;
    &lt;p&gt;The solution isn’t trying harder to be liked. It’s unlearning your discomfort with being disliked.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chrislakin.blog/p/social-anxiety"/><published>2025-10-03T14:51:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45464269</id><title>Anduril and Palantir battlefield comms system has deep flaws: Army</title><updated>2025-10-03T18:13:48.765959+00:00</updated><content>&lt;doc fingerprint="a573ccbb7c3e5585"&gt;
  &lt;main&gt;
    &lt;p&gt;The much-needed modernization of the U.S. Army's battlefield communications network being undertaken by Anduril, Palantir and others is rife with "fundamental security" problems and vulnerabilities, and should be treated as a "very high risk," according to a recent internal Army memo.&lt;/p&gt;
    &lt;p&gt;The two Silicon Valley companies, led by allies of U.S. President Donald Trump, have gained access to the Pentagon's lucrative flow of contracts on the promise of quickly providing less expensive and more sophisticated weapons than the Pentagon's longstanding arms providers.&lt;/p&gt;
    &lt;p&gt;But the September memo from the Army's chief technology officer about the NGC2 platform that connects soldiers, sensors, vehicles and commanders with real-time data paints a bleak picture of the initial product.&lt;/p&gt;
    &lt;p&gt;"We cannot control who sees what, we cannot see what users are doing, and we cannot verify that the software itself is secure," the memo says.&lt;/p&gt;
    &lt;p&gt;Palantir and Anduril did not comment for this story.&lt;/p&gt;
    &lt;p&gt;The assessment, seen by Reuters and first reported by Breaking Defense, comes just months after defense drone and software maker Anduril was awarded a $100 million to create a prototype of NGC2 with partners including Palantir, Microsoft and several smaller contractors.&lt;/p&gt;
    &lt;p&gt;The Army should treat the NGC2 prototype version as “very high risk” because of the “likelihood of an adversary gaining persistent undetectable access," wrote Gabrielle Chiulli, the Army chief technology officer authorizing official.&lt;/p&gt;
    &lt;p&gt;Despite the early September memo's scathing critique, Leonel Garciga, Army chief information officer and Chiulli's supervisor, said in a statement to Reuters that the report was part of a process that helped in "triaging cybersecurity vulnerabilities" and mitigating them.&lt;/p&gt;
    &lt;p&gt;In March, the 4th Infantry Division used the system in live-fire artillery training at Fort Carson, Colorado, in an exercise Anduril described as demonstrating faster and more reliable performance than legacy systems.&lt;/p&gt;
    &lt;p&gt;The Army memo identifies some major security gaps.&lt;/p&gt;
    &lt;p&gt;The report says the system allows any authorized user to access all applications and data regardless of their clearance level or operational need. As a result, "Any user can potentially access and misuse sensitive" classified information, the memo states, with no logging to track their actions.&lt;/p&gt;
    &lt;p&gt;Other deficiencies highlighted in the memo include the hosting of third-party applications that have not undergone Army security assessments. One application revealed 25 high-severity code vulnerabilities. Three additional applications under review each contain over 200 vulnerabilities requiring assessment, according to the document.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnbc.com/2025/10/03/anduril-palantir-ngc2-deep-flaws-army.html"/><published>2025-10-03T15:46:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45464611</id><title>A Looking Glass Half Empty, Part 1: Just Lookin' for a Hit</title><updated>2025-10-03T18:13:48.337759+00:00</updated><content>&lt;doc fingerprint="be7080750b6e037b"&gt;
  &lt;main&gt;
    &lt;p/&gt;
    &lt;quote&gt;
      &lt;p&gt;There was some discussion about it: “Wow, gosh, it’d sure be nice if we were making more money and selling more copies so we could do crazy games of the type we want, as opposed to having to worry about how we’re going to sell more.” Hey, I’d love it if the public was more into what I like to do and a little less into slightly more straightforward things. But I totally get that they’re into straightforward things. I don’t have any divine right to have someone hand me millions of dollars to make a game of whatever I want to do. At some fundamental level, everyone has a wallet, and they vote with it.&lt;/p&gt;
      &lt;p&gt;— Doug Church, Looking Glass Studios&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Late in 1994, after their rather brilliant game System Shock had debuted to a reception most kindly described as constrained, the Boston-based studio Looking Glass Technologies sent their star producer Warren Spector down to Austin, Texas. There he was to visit the offices of Looking Glass’s publisher Origin Systems, whose lack of promotional enthusiasm they largely blamed for their latest game’s lukewarm commercial performance. Until recently, Spector had been directly employed by Origin. The thinking, then, was that he might still be able to pull some strings in Austin to move the games of Looking Glass a little higher up in the priority rankings. The upshot of his visit was not encouraging. “What do I have to do to get a hit around here?” Spector remembers pleading to his old colleagues. The answer was “very quiet, very calm: ‘Sign Mark Hamill to star in your game.‘ That was the thinking at the time.” But interactive movies were not at all what Looking Glass wanted to be doing, nor where they felt the long-term future of the games industry lay.&lt;/p&gt;
    &lt;p&gt;So, founders Paul Neurath and Ned Lerner decided to make some major changes in their business model in the hope of raising their studio’s profile. They accepted $3.8 million in venture capital and cut ties with Origin, announcing that henceforward Looking Glass would publish as well as create their games for themselves. Jerry Wolosenko, a new executive vice president whom they hired to help steer the company into its future of abundance, told The Boston Globe in May of 1995 that “we expect to do six original titles per year. We are just beginning.” This was an ambitious goal indeed for a studio that, in its five and a half years of existence to date, had managed to turn out just three original games alongside a handful of porting jobs.&lt;/p&gt;
    &lt;p&gt;Even more ambitious, if not brazen, was the product that Looking Glass thought would provide them with their entrée into the ranks of the big-time publishers. They intended to mount a head-on challenge to that noted tech monopolist Microsoft, whose venerable, archetypally entitled Flight Simulator was the last word — in fact, very nearly the only word — in civilian flight simulation. David-versus-Goliath contests in the business of media didn’t come much more pronounced than this one, but Looking Glass thought they had a strategy that might allow them to break at least this particular Microsoft monopoly.&lt;/p&gt;
    &lt;p&gt;Flight Unlimited was the brainchild of a high-energy physicist, glider pilot, and amateur jazz pianist named Seamus Blackley, who had arrived at Looking Glass by way of the legendary Fermi Laboratory. His guiding principle was that Microsoft’s Flight Simulator as it had evolved over the last decade and a half had become less a simulation of flight itself than a simulation of the humdrum routine of civil aviation — of takeoff permissions and holding patterns, of navigational transponders and instrument landing systems. He wanted to return the focus to the simple joy of soaring through the air in a flying machine, something that, for all the technological progress that had been made since the Wright brothers took off from Kitty Hawk, could still seem closer to magic than science. The emphasis would be on free-form aerobatics rather than getting from Airport A to Airport B. “I want people to see that flying is beautiful, exciting, and see the thrill you can get from six degrees of freedom when you control an airplane,” Blackley said. “That’s why we’ve focused on the experience of flying. There is no fuel gauge.”&lt;/p&gt;
    &lt;p&gt;The result really was oddly beautiful, being arguably as close to interactive art as a product that bills itself as a vehicular simulation can possibility get. Its only real concession to structure took the form of a 33-lesson flying course, which brought you from just being able to hold the airplane straight and level to executing gravity-denying Immelman rolls, Cuban eights, hammerheads, and inverted spins. Any time that your coursework became too intense, you always had the option to just bin the lesson plans and, you know, go out and fly, maybe to try some improvisational skywriting.&lt;/p&gt;
    &lt;p&gt;In one sense, Flight Unlimited was a dramatic departure from the two Ultima Underworld games and System Shock, all of which were embodied first-person, narrative-oriented designs that relied on 3D graphics of a very different stripe. In another sense, though, it was business as usual, another example of Looking Glass not only pushing boundaries of technology in a purist sense — the flight model of Flight Unlimited really was second to none — but using it in the service of a game that was equally aesthetically innovative, and just a little bit more thoughtful all the way around than was the norm.&lt;/p&gt;
    &lt;p&gt;Upon its release in May of 1995, Flight Unlimited garnered a rare five-stars-out-of-five review from Computer Gaming World magazine:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It’s just you, the sky, and a plane that does just about anything you ask it to. Anything aerobatic, that is. Flight Unlimited is missing many of the staple elements of flight simulations. There are no missiles, guns, or enemy aircraft. You can’t learn IFR navigation or practice for your cross-country solo. You can’t even land at a different airport than the one you took off from. But unless you’re just never happy without something to shoot at, you won’t care. You’ll be too busy choreographing aerial ballets, pulling off death-defying aerobatic stunts, or just enjoying a quiet soar down the ridge line to miss that stuff.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Flight Unlimited sold far better than System Shock: a third of a million copies, more even than Looking Glass’s previous best-seller Ultima Underworld, enough to put itself solidly in the black and justify a sequel. Still, it seems safe to say that it didn’t cause any sleepless nights for anyone at Microsoft. Over the years, Flight Simulator had become less a game than a whole cottage industry unto itself, filled with armchair pilots who often weren’t quite gamers in the conventional sense, who often played nothing else. It wasn’t all that easy to make inroads with a crowd such as that. Like a lot of Looking Glass’s games, Flight Unlimited was a fundamentally niche product to which was attached the burden of mainstream sales expectations.&lt;/p&gt;
    &lt;p&gt;That said, the fact remained that Flight Unlimited had made money for Looking Glass, which allowed them to continue to live the dream for a while longer. Neurath and Lerner sent a homesick Warren Spector back down to Austin to open a second branch there, to take advantage of an abundance of talent surrounding the University of Texas that the Wing Commander-addled Origin Systems was believed to be neglecting.&lt;/p&gt;
    &lt;p&gt;Then Looking Glass hit a wall. Its name was Terra Nova.&lt;/p&gt;
    &lt;p&gt;Terra Nova: Strike Force Centauri had had the most protracted development cycle of any Looking Glass game, dating almost all the way back to the very beginning of the company and passing through dozens of hands before it finally came to fruition in the spring of 1996. At its heart, it was an ultra-tactical first-person shooter vaguely inspired by the old Robert Heinlein novel Starship Troopers, tasking you with leading teams of fellow soldiers through a series of missions, clad in your high-tech combat gear that turned you more than halfway into a sentient robot. But it was also as close as Looking Glass would ever come to their own stab at a Wing Commander: the story was advanced via filmed cutscenes featuring real human actors, and a lot of attention was paid to the goings on back at the ranch when you weren’t dressed up in your robot suit. This sort of thing worked in Wing Commander, to whatever extent it did, because the gameplay that took place between the movie segments was fairly quick and simple. Terra Nova was not like that, which could make it feel like an even more awkward mélange of chocolate and peanut butter. It’s difficult to say whether Activision’s Mechwarrior 2, the biggest computer game of 1995, helped it or hurt it in the marketplace: on the one hand, that game showed that there was a strong appetite for tactical combat involving robots, but, on the other, said demand was already being fed by a glut of copycats. Terra Nova got lost in the shuffle. A game that had been expected to sell at least half a million copies didn’t reach one-fifth of that total.&lt;/p&gt;
    &lt;p&gt;Looking Glass’s next game didn’t do any better. Like Flight Unlimited, British Open Championship Golf cut against the dark, gritty, and violent stereotype that tended to hold sway when people thought of Looking Glass, or for that matter of the games industry writ large. It was another direct challenge to an established behemoth: in this case, Access Software’s Links franchise, which, like Flight Simulator, had its own unique customer base, being the only line of boxed computer games that sold better to middle-aged corporate executives than they did to high-school and university students. Looking Glass’s golf project was led by one Rex Bradford, whose own history with simulating the sport went all the way back to Mean 18, a hit for Accolade in 1986. This time around, though, the upstart challenger to the status quo never even got a sniff. By way of damning with faint praise, Computer Gaming World called British Open Championship Golf “solid,” but “somewhat unspectacular.” Looking Glass could only wish that its sales could have been described in the same way.&lt;/p&gt;
    &lt;p&gt;With the benefit of hindsight, we can see all too clearly that Neurath and Lerner crossed the line that separates ambition from hubris when they decided to try to set Looking Glass up as a publisher. At the very time they were doing so, many another boutique publisher was doing the opposite, looking for a larger partner or purchaser to serve as shelter from the gale-force winds that were beginning to blow through the industry. More games were being made than ever, even as shelf space at retail wasn’t growing at anything like the same pace, and digital distribution for most types of games remained a nonstarter in an era in which almost everyone was still accessing the Internet via a slow, unstable dial-up connection. This turned the fight over retail space into a free-for-all worthy of the most ultra-violet beat-em-up. Sharp elbows alone weren’t enough to win at this game; you had to have deep pockets as well, had to either be a big publisher yourself or have one of them on your side. In deciding to strike out on their own, Neurath and Lerner may have been inspired by the story of Interplay Productions, a development studio which in 1988 had broken free of the grasp of Electronic Arts — now Origin System’s corporate parent, as it happened — and gone on to itself become one of the aforementioned big publishers who were increasingly dominating at retail. But 1988 had been a very different time in gaming.&lt;/p&gt;
    &lt;p&gt;In short, Neurath and Lerner had chosen just about the worst possible instant to try to seize full control of their own destiny. “Game distribution isn’t always based on quality,” noted Warren Spector at the end of 1996. Having thus stated the obvious, he elaborated:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The business has changed radically in the last year, and it’s depressing. The competition for shelf space is ridiculous and puts retailers in charge. If you don’t buy an end-cap from retailers for, say, $50,000 a month, they won’t buy many copies.&lt;/p&gt;
      &lt;p&gt;Products once had three to six months. The average life is now 30 days. If you’re not a hit in 30 days, you’re gone. This is predicated on your association with a publisher who gets your title on shelves. It’s a nightmare.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;With just three games shipped in the last two and a half years — a long way off their projected pace of “six original titles per year” — and with the last two of them having flopped like a wet tuna on a gymnastics court, Looking Glass was now in dire straits. The only thing that had allowed them to keep the doors open this long had been a series of workaday porting jobs that Warren Spector had been relegated to supervising down in Austin, while he waited for the company to establish itself on a sound enough financial footing to support game development from whole cloth in both locations. Ten years on, after Looking Glass had been enshrined in gaming lore as one of the most forward-thinking studios of all time and Spector as the ultimate creative producer, the idea of them wasting their collective talents on anonymous console ports would seem surreal. But such was the reality circa 1997, when Looking Glass, having burnt through all of their venture capital, was left holding on by a thread. “I remember people walking into the office to take back the [rented] plants which the studio was no longer able to pay for,” says programmer Randy Smith.&lt;/p&gt;
    &lt;p&gt;As for Neurath and Lerner, they had swallowed the hubris of 1995 and were now doing what the managers of all independent games studios do when they find themselves unable to pay the bills anymore: looking for a buyer who would be able to pay them instead. But because Looking Glass could never seem to do anything in the conventional way even when they tried to, the buyer they found was one of the strangest ever.&lt;/p&gt;
    &lt;p&gt;The Boston firm known as Intermetrics, Inc., was far from a household name, but it had a proud history that long predated the personal-computer era. Intermetrics had grown out of the fecund soil of Project Apollo, having been founded in March of 1969 by some of the engineers and programmers behind the Apollo Guidance Computer that would soon help to place astronauts on the Moon. After that epochal achievement, Intermetrics continued to do a lot of work for NASA, providing much of the software that was used to control the Space Shuttle. Other government and aerospace-industry contracts filled out most of the balance of its order sheets.&lt;/p&gt;
    &lt;p&gt;In August of 1995, however, a group of investors led by a television executive bought the firm for $28 million, with the intention of turning it into something altogether different. Michael Alexander came from the media conglomerate MCA, where he had been credited with turning around the fortunes of the cable-television channel USA. Witnessing the transformation that high-resolution graphics, high-quality sound, and the enormous storage capacity of CD-ROM were wreaking on personal computing, he had joined dozens of his peers in deciding that the future of mass-market entertainment and infotainment lay with interactive multimedia. Deeming most of the companies who were already in that space to be “overvalued,” and apparently assuming that one type of computer programming was more or less the same as any other, he bought Intermetrics, whose uniform of white shirts, ties, and crew cuts had changed little since the heyday of the Space Race, to ride the hottest wave in 1990s consumer electronics.&lt;/p&gt;
    &lt;p&gt;“This is a company that has the skills and expertise to be in the multimedia business, but is not perceived as being in that business,” he told a reporter from The Los Angeles Times. (It was not a question of perception; Intermetrics was not in the multimedia business prior to the acquisition.) “And that is its strength.” (He failed to elaborate on exactly why this should be the case.) Even the journalist to whom he spoke seemed skeptical. “Ponytailed, black-clad, twenty-something multimedia developers beware,” she wrote, almost palpably smirking between the lines. “Graying engineers with pocket protectors and a dozen years of experience are starting to compete.” Likewise, it is hard not to suspect Brian Fargo of Interplay of trolling the poor rube when he said that “I think it’s great that the defense guys are doing this. It’s where the job security is now. It used to be in defense. Now it’s in the videogame business.” (Through good times and bad, one thing the videogame business has never, ever been noted for is its job security.)&lt;/p&gt;
    &lt;p&gt;Alas, Michael Alexander was not just a bandwagon jumper; he was a late bandwagon jumper. By the time he bought Intermetrics, the multimedia bubble was already close to popping under the pressure of a more sustained Internet bubble that would end the era of the non-game multimedia CD-ROM almost before it had begun. As this harsh reality became clear in the months that followed, Alexander had no choice but to push Intermetrics more and more in the direction of games, the only kind of CD-ROM product that was making anyone any money. The culture clash that resulted was intractable, as pretty much anyone who knew anything about the various cultures of computing could have predicted. Among these someones was Mike Dornbrook, a games-industry stalwart who had gotten his start with Infocom in the early 1980s. Seeking his next gig after Boffo Games, a studio he had founded with his old Infocom colleague Steve Meretzky, went down in flames, Dornbrook briefly kicked the tires at Intermetrics, but quickly concluded that what he saw “made no sense whatsoever”: “They were mostly COBOL programmers in their fifties and sixties. I remember looking around and saying, ‘You’re going to turn these guys into game programmers? What in the world are you thinking?'” [1]Dornbrook wound up signing on instead with a tiny startup called Harmonix Music Systems, which in 2005, after years of diligent experimentation with the possibilities for combining music and games, altered the landscape of gaming forever with Guitar Hero.&lt;/p&gt;
    &lt;p&gt;Belatedly realizing that all types of programming were perhaps not quite so interchangeable as he had believed, Michael Alexander set out in search of youngsters to teach his old dogs some new tricks. The Intermetrics rank and file must have shuddered at the advertisements he started to run in gaming magazines. “We are rocket scientists!” the ads trumpeted. “Even our games are mission-critical!” When these efforts failed to surface a critical mass of game-development talent, Alexander reluctantly moved on to doing what he should have done back in 1995: looking for an extant studio that already knew how to make games. It so happened that Looking Glass was right there in Boston, and, thanks to its troubled circumstances, was not as “overvalued” as most of its peers. Any port in a storm, as they say.&lt;/p&gt;
    &lt;p&gt;On August 14, 1997, a joint press release was issued: “Intermetrics, Inc., a 28-year-old leading software developer, and Looking Glass Studios, one of the computer gaming industry’s foremost developers, today announce the merger of the two companies’ gaming operations to form Intermetrics/Looking Glass Studios, LLC. Through the shared strengths of the two entities, the new company is strategically positioned to be a major force in the computer-game, console and online-gaming industries.” Evidently on a quest to find out how much meaningless corporate-speak he could shoehorn into one document, Michael Alexander went on to add that “Looking Glass Studios immediately catapults Intermetrics into a leading position in the gaming industry by giving us additional credentials and assets to compete in the market. Our business plan is to maintain and grow our core contract-services business while at the same time leveraging our expertise and financial resources to be a major player in the booming interactive-entertainment industry.” The price paid by the rocket scientists for their second-stage booster has to my knowledge never been publicly revealed.&lt;/p&gt;
    &lt;p&gt;The acquiring party may have been weird as all get-out, but it could have worked out far worse for Looking Glass, all things considered. In addition to the obvious benefit of being able to keep the doors open, at least a couple of other really good things came directly out of the acquisition. One was a change in name, from Looking Glass Technologies to Looking Glass Studios, emphasizing the creative dimension of their work. Another was a distribution deal with Eidos, a British publisher that had serious retail clout in both North America and Europe. Riding high on the back of the massive international hit Tomb Raider, Eidos could ensure that Looking Glass’s games got prominent placement in stores. Meanwhile this idea of the Looking Glass people serving as mentors to those who were struggling to make games at Intermetrics proper — an excruciating proposition for both parties — would prove to mostly be a polite, face-saving fiction for Michael Alexander; in practice, the new parent company would prove largely content to leave its subsidiary alone to do its own thing. Now the folks at Looking Glass just needed to deliver a hit to firmly establish themselves in their new situation. That was always the stick wicket for them.&lt;/p&gt;
    &lt;p&gt;The first game that Looking Glass released under their new ownership was Flight Unlimited II, which appeared just a few months after the big announcement. Seeking simultaneously to capitalize on the relative success of their first flight simulator and to adjust that game’s priorities to better coincide with the real or perceived desires of the market, Looking Glass paired the extant flight model with an impressively detailed depiction of the geography of the San Francisco Bay area. Then they added a lot more structure to the whole affair, in the form of a set of missions to fly after you finished your training. The biggest innovation, a first for any civilian flight simulator, was the addition of other aircraft, turning San Francisco International Airport into the same tangle of congested flight lanes it was in the real world. These changes moved the game away from being such a purist simulation of flight as an end unto itself — so much so that a disgruntled Seamus Blackley quit the project and the company early in the development cycle. Still, there was a logic to the additions; one can easily imagine them making Flight Unlimited II more appealing to the sorts of gamers who don’t tend to thrive in goal-less sandboxes. Be that as it may, though, it didn’t show up in the sales figures. Flight Unlimited II sold better than Terra Nova or British Open Championship Golf, but not as well as its series predecessor, just barely managing to break even.&lt;/p&gt;
    &lt;p&gt;This disappointment put that much more pressure on Looking Glass’s next game to please the new boss and show that the studio could deliver a solid, unqualified hit. In a triumph of hope over experience, everyone had high expectations for The Dark Project, which had been described in the press release announcing the acquisition as “a next-generation fantasy role-playing game.” Such a description might have left gamers wondering if Looking Glass was returning to the territory of Ultima Underworld. As things worked out, the game that they would come to know as simply Thief would not be that at all, but would instead break new ground in a completely different way. It stands today alongside Ultima Underworld in another sense: as one of the three principal legs — the last one being System Shock, of course — that hold up Looking Glass’s towering modern-day reputation for relentless, high-concept innovation.&lt;/p&gt;
    &lt;p&gt;Ironically but typically for this studio, the off-kilter masterstroke that is Thief arose from an attempt to hit the mainstream a little more directly. Looking Glass’s talented graphics programmers, who were not without a requisite degree of arrogance about their talents, thought that they could easily come up with a pure first-person-shooter engine as good or better than the one that John Carmack and his friends at id Software rolled out for 1996’s Quake. The Dark Engine, as it would come to be known, fit that bill pretty well, and could have powered a “low-brain shooter,” as the Looking Glass folks called the likes of Quake, with perfect equanimity. But in the end they just couldn’t bring themselves to make one.&lt;/p&gt;
    &lt;p&gt;It took a goodly while for them to decide what they did want to do with The Dark Engine. Doug Church, the iconoclastic programmer who had taken the leading role alongside Warren Spector on System Shock, didn’t want to be out-front to the same extent on this project, even as Spector was now down in Austin, which limited his involvement as well. The initial result of this lack of strong authority figures was an awful lot of creative churn. There was talk of making a game called Better Red than Undead, mixing a Cold War-era spy caper with a zombie invasion. Almost as bizarre was Dark Camelot, an inverted Arthurian tale in which you played the Black Knight against King Arthur and his cronies, who were depicted as a bunch of insufferable holier-than-thou prigs. “Our marketing department wasn’t really into that one,” laughs Church.&lt;/p&gt;
    &lt;p&gt;Yet the core sensibility of that concept — of an amoral protagonist set against the corrupt establishment and all of its pretensions — is all over the game that did finally get made. Doug Church:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The missions [in Dark Camelot] that we had the best definition on and the best detail on were all breaking into Camelot, meeting up with someone, getting a clue, stealing something, whatever. As we did more work in that direction, and those continued to be the missions that we could explain best to other people, it just started going that way. Paul [Neurath] had been pushing for a while that the thief side of it was the really interesting part, and why not just do a thief game?&lt;/p&gt;
      &lt;p&gt;And as things got more chaotic and more stuff was going on and we were having more issues with how to market stuff, we just kept focusing on the thief part. We went through a bunch of different phases of reorganizing the project structure and a bunch of us got sucked into doing some other project work on Flight [Unlimited] and stuff, and there was all this chaos. We said, “Okay, well, we’ve got to get this going and really focus and make a plan.” So we put Greg [LoPiccolo] in charge of the project and we agreed we were going to call it Thief and we were going to focus much more. That’s when we went from lots of playing around and exploring to “let’s make this Thief game.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It surely comes as no revelation to anyone reading this article that most game stories are power fantasies at bottom, in which you get to take on the identity of a larger-than-life protagonist who just keeps on growing stronger as you progress. Games which took a different approach were, although by no means unknown by the late 1990s, in the decided minority even outside of the testosterone-drenched ghetto of the first-person shooter. The most obvious exponents of the ordinary-mortal protagonist were to be found in the budding survival-horror genre, as pioneered by Alone in the Dark and its sequels on computers and Resident Evil on the consoles. But these games cast you as nearly powerless prey, being stalked through dark corridors by zombies and other things that go bump in the night. Thief makes you a stealthy predator, the unwanted visitor rifling through cupboards and striking without warning out of the darkness, yet most definitely not in any condition to mow down dozens of his enemies in full-frontal combat, Quake-style. If you’re indiscreet in your predations, you can become the cornered prey with head-snapping speed. This was something new at the time.&lt;/p&gt;
    &lt;p&gt;Or almost so. Coincidentally, two Japanese stealthy-predator games hit the Sony PlayStation in 1998, the same year as Thief’s release. Tenchu: Stealth Assassins cast you as a ninja, while Metal Gear Solid cast you as an agent of the American government on a top-secret commando mission. The latter in particular caused quite a stir, by combining its unusual gameplay style with the sort of operatically melodramatic storytelling that was more commonly associated with the JRPG genre. That said, Thief is a far more sophisticated affair than either of these games, in terms of both its gameplay and its fiction.&lt;/p&gt;
    &lt;p&gt;The titular thief and protagonist is a man known only as Garrett, who learned his trade on the streets of The City, a mixture of urban squalor and splendor that is best described as Renaissance Florence with magic — a welcome alternative to more typical fantasy settings. Over the course of a twelve-act campaign, Garrett is given a succession of increasingly daunting assignments, during which a larger plot that involves more than the acquisition of wealth by alternative methods does gradually take shape.&lt;/p&gt;
    &lt;p&gt;Although the mission tree is linear, nothing else about your experience in Thief is set in stone. It was extremely important to Looking Glass that Thief not turn into a puzzle game, a series of set-piece challenges with set-piece solutions. They wanted to offer up truly dynamic environments, environments that were in their own way every bit as much simulations as Flight Unlimited. They wanted to make you believe you were really in these spaces. Artist Daniel Thron speaks of the “deep sense of trust we had in the player. There isn’t a single solution to Thief. It’s up to you to figure out how to steal the thing. It’s letting you tell that story through gameplay. And that sense of ownership makes it unique. It becomes yours.” In the spirit of all that, the levels are big, with no clearly delineated through-line. These dynamic virtual spaces full of autonomous actors demand constant improvisation on your part even if you’ve explored them before.&lt;/p&gt;
    &lt;p&gt;Looking Glass understood that, in order for Thief to work as a vehicle for emergent narrative, all of the other actors on the stage have to respond believably to your actions. It’s a given that guards ought to hunt you down if you blatantly give away your presence to them. Thief distinguishes itself by the way it responds to more subtle stimuli. An ill-judged footstep on a creaky floor tile might cause a guard to stop and mutter to himself: “Wait! Did I just hear something?” Stand stock still and don’t make a sound, and maybe — maybe — he’ll shrug his shoulders and move on without bothering to investigate. If you do decide to take a shot at him with your trusty bow or blackjack, you best not miss, to steal a phrase from Omar Little. And you best hide the body carefully afterward, before one of his comrades comes wandering along the same corridor to stumble over it.&lt;/p&gt;
    &lt;p&gt;These types of situations and the split-second decisions they force upon you are the beating heart of Thief. Bringing them off was a massive technical challenge, one that made the creation of 3D-graphics engine itself seem like child’s play. The state of awareness of dozens of non-player characters had to be tracked, as did sound and proximity, light and shadow, to an extent that no shooter — no, not even Half-Life — had ever come close to doing before. Remarkably, Looking Glass largely pulled it off, whilst making sure that the more conventional parts of the engine worked equally well. Garrett’s three principal weapons — a blackjack for clubbing unsuspecting victims in the back of the head, a rapier for hand-to-hand combat, and a bow which can be used to shoot a variety of different types of arrows — are all immensely satisfying to use, having just the right feeling of weight in your virtual hands. The bow is a special delight: the arrows arc through the air exactly as one feels they ought to. You actually get to use your bow in all sorts of clever ways that go beyond killing, such as shooting water arrows to extinguish pesky torches — needless to say, darkness is your best friend and light your eternal enemy in this game — and firing rope arrows that serve Garrett as grappling hooks would a more conventional protagonist.&lt;/p&gt;
    &lt;p&gt;Looking Glass being Looking Glass, even the difficulty setting in Thief is more than it first appears to be. It’s wouldn’t be much of an exaggeration to say that Thief is really three games in one, depending on whether you play it on Normal, Hard, or Expert. (Looking Glass apparently wasn’t interested in the sorts of players who might be tempted by an “easy” mode.) Not only do the harder settings require you to collect more loot to score a passing grade on each mission, but the environments themselves become substantially larger. Most strikingly, in a brave subversion of the standard shooter formula, each successive difficulty setting requires you to kill fewer rather than more people; at the Expert level, you’re not allowed to kill anyone at all.&lt;/p&gt;
    &lt;p&gt;Regardless of the difficulty setting you choose, Thief will provide a stiff challenge. Its commitment to verisimilitude extends to all of its facets. In lieu of a conventional auto-map, it provides you only with whatever scribbled paper map Garrett has been able to scrounge from his co-conspirators, or sometimes not even that much. If your innate sense of direction isn’t great — mine certainly isn’t — you can spend a long time just trying to find your way in these big, twisty, murky spaces.&lt;/p&gt;
    &lt;p&gt;When it’s at its best, Thief is as amazing as it is uncompromising. It oozes atmosphere and tension; it’s the sort of game that demands to be played in a dark room behind a big monitor, with the phone shut off and a pair of headphones planted firmly over the ears. Sadly, though, it isn’t always this best version of itself. In comparison to Ultima Underworld or System Shock, both of which I enjoyed from first to last, Thief strikes me as a lumpy creation, a game of soaring highs but also some noteworthy lows. I was all-in during the first mission, a heist taking place in the mansion of a decadent nobleman. Having recently read Sarah Dunant’s The Birth of Venus and written quite a lot about Renaissance Florence, my receptors were well primed for this Neo-Renaissance setting. Then I came to the second mission, and suddenly I was being asked to fight my way through a bunch of zombies in an anonymous cave complex. Suddenly Thief felt like dozens of other first-person action games.&lt;/p&gt;
    &lt;p&gt;This odd schizophrenia persists throughout the game. The stealthy experience I’ve just been describing — the boldly innovative experience that everyone thinks of today when they think of Thief — is regularly interspersed with splatterfests against enemies who wouldn’t have been out of place in Quake: zombies, rat men, giant exploding frogs, for Pete’s sake. (Because these enemies aren’t human, they’re generally exempt from the prohibition against killing at the Expert level.) All told, it’s a jarring failure to stick to its guns from a studio that has gone down in gaming lore for refusing to sacrifice its artistic integrity, to its own great commercial detriment.&lt;/p&gt;
    &lt;p&gt;As happens so often in these cases, the reality behind the legend of Looking Glass is more nuanced. Almost to a person, the team who made Thief attribute the inconsistency in the level design to outside pressure, especially from their publisher Eidos, who had agreed to partially fund the project. “Eidos never believed in it and until the end told us to put in more monsters and have more fighting and exploring and less stealth, and I’m not sure there was ever a point [when] they got it,” claims Doug Church. “I mean, the trailers Eidos did for Thief were all scenes with people shooting fire arrows at people charging them. So you can derive from that how well they understood or believed in the idea.”&lt;/p&gt;
    &lt;p&gt;And yet one can make the ironic case that Eidos knew what they were doing when they pushed Looking Glass to play up the carnage a little more. Released in November of 1998, Thief finally garnered Looking Glass some sales figures that were almost commensurate with their positive reviews. (“If you’re tired of DOOM clones and hungry for challenge, give this fresh perspective a try,” said Computer Gaming World.) The game sold about half a million copies — not a huge hit by the standards of an id Software or Blizzard Entertainment, but by far the most copies Looking Glass had ever sold of anything. It gave them some much-needed positive cash flow, which allowed them to pay down some debts and to revel in some good vibes for a change when they looked at the bottom line. But most importantly for the people who had made Thief, its success gave them the runway they needed to make a sequel that would be more confident in its stealthy identity.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Did you enjoy this article? If so, please think about pitching in to help me make many more like it. You can pledge any amount you like.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;Sources: The book Game Design Theory &amp;amp; Practice (2nd. ed.) by Richard Rouse III; Next Generation of March 1997 and June 1997; PC Zone of December 1998; Computer Gaming World of September 1995, June 1996, August 1997, April 1998, and March 1999; Retro Gamer 117, 177, and 260; Los Angeles Times of September 15 1995; Boston Globe of May 3 1995 and May 26 2000.&lt;/p&gt;
    &lt;p&gt;Online sources include the announcement of the Intermetrics acquisition on Looking Glass’s old website, InterMetrics’s own vintage website, “Ahead of Its Time: A History of Looking Glass” by Mike Mahardy at Polygon, and James Sterrett’s “Reasons for the Fall: A Post-Mortem on Looking Glass Studios.”&lt;/p&gt;
    &lt;p&gt;Where to Get Them: Terra Nova: Strike Force Centauri and Thief Gold are available for digital purchase at GOG.com. The other Looking Glass games mentioned this article are unfortunately not.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;↑1&lt;/cell&gt;
        &lt;cell&gt;Dornbrook wound up signing on instead with a tiny startup called Harmonix Music Systems, which in 2005, after years of diligent experimentation with the possibilities for combining music and games, altered the landscape of gaming forever with Guitar Hero.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.filfre.net/2025/10/a-looking-glass-half-empty-part-1-just-lookin-for-a-hit/"/><published>2025-10-03T16:16:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45464632</id><title>Cancelling Async Rust</title><updated>2025-10-03T18:13:47.885774+00:00</updated><content>&lt;doc fingerprint="310499976ca2b6bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cancelling async Rust&lt;/head&gt;
    &lt;p&gt;This is an edited, written version of my RustConf 2025 talk about cancellations in async Rust. Like the written version of my RustConf 2023 talk, I’ve tried to retain the feel of a talk while making it readable as a standalone blog entry. Some links:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Video of the talk on YouTube.&lt;/item&gt;
      &lt;item&gt;Slides on Google Slides.&lt;/item&gt;
      &lt;item&gt;Repository with links and notes on GitHub.&lt;/item&gt;
      &lt;item&gt;Coverage on Linux Weekly News.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Introduction#&lt;/head&gt;
    &lt;p&gt;Let’s start with a simple example – you decide to read from a channel in a loop and gather a bunch of messages:&lt;/p&gt;
    &lt;code&gt;loop {
    match rx.recv().await {
        Ok(msg) =&amp;gt; process(msg),
        Err(_) =&amp;gt; return,
    }
}
&lt;/code&gt;
    &lt;p&gt;All good, nothing wrong with this, but you realize sometimes the channel is empty for long periods of time, so you add a timeout and print a message:&lt;/p&gt;
    &lt;code&gt;loop {
    match timeout(Duration::from_secs(5), rx.recv()).await {
        Ok(Ok(msg)) =&amp;gt; process(msg),
        Ok(Err(_)) =&amp;gt; return,
        Err(_) =&amp;gt; println!("no messages for 5 seconds"),
    }
}
&lt;/code&gt;
    &lt;p&gt;There’s nothing wrong with this code—it behaves as expected.&lt;/p&gt;
    &lt;p&gt;Now you realize you need to write a bunch of messages out to a channel in a loop:&lt;/p&gt;
    &lt;code&gt;loop {
    let msg = next_message();
    match tx.send(msg).await {
        Ok(_) =&amp;gt; println!("sent successfully"),
        Err(_) =&amp;gt; return,
    }
}
&lt;/code&gt;
    &lt;p&gt;But sometimes the channel gets too full and blocks, so you add a timeout and print a message:&lt;/p&gt;
    &lt;code&gt;loop {
    let msg = next_message();
    match timeout(Duration::from_secs(5), tx.send(msg)).await {
        Ok(Ok(_)) =&amp;gt; println!("sent successfully"),
        Ok(Err(_)) =&amp;gt; return,
        Err(_) =&amp;gt; println!("no space for 5 seconds"),
    }
}
&lt;/code&gt;
    &lt;p&gt;It turns out that this code is often incorrect, because not all messages make their way to the channel.&lt;/p&gt;
    &lt;p&gt;Hi, I’m Rain, and this post is about cancelling async Rust. This post is split into three parts:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What is cancellation? It’s an extremely powerful part of async Rust but also one that is very hard to reason thoroughly about.&lt;/item&gt;
      &lt;item&gt;Analyzing cancellations: Going deep into their mechanics and providing some helpful ways to think about them.&lt;/item&gt;
      &lt;item&gt;What can be done? Solutions, including practical guidance, and real bugs we’ve found and fixed in production codebases.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Before we begin, I want to lay my cards on the table – I really love async Rust!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I gave a talk at RustConf a couple years ago talking about how async Rust is a great fit for signal handling in complex applications.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I’m also the author of cargo-nextest, a next-generation test runner for Rust, where async Rust is the best way I know of to express some really complex algorithms that I wouldn’t know how to express otherwise. I wrote a blog post about this a few years ago.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now, I work at Oxide Computer Company, where we make cloud-in-a-box computers. We make vertically integrated systems where you provide power and networking on one end, and the software you want to run on the other end, and we take care of everything in between.&lt;/p&gt;
    &lt;p&gt;Of course, we use Rust everywhere, and in particular we use async Rust extensively for our higher-level software, such as storage, networking and the customer-facing management API. But along the way we’ve encountered a number of issues around async cancellation, and a lot of this post is about what we learned along the way.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. What is cancellation?#&lt;/head&gt;
    &lt;p&gt;What does cancellation mean? Logically, a cancellation is exactly what it sounds like: you start some work, and then change your mind and decide to stop doing that work.&lt;/p&gt;
    &lt;p&gt;As you might imagine this is a useful thing to do:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You may have started a large download or a long network request&lt;/item&gt;
      &lt;item&gt;Maybe you’ve started reading a file, similar to the &lt;code&gt;head&lt;/code&gt;command.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But then you change your mind: you want to cancel it rather than continue it to completion.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cancellations in synchronous Rust#&lt;/head&gt;
    &lt;p&gt;Before we talk about async Rust, it’s worth thinking about how you’d do cancellations in synchronous Rust.&lt;/p&gt;
    &lt;p&gt;One option is to have some kind of flag you periodically check, maybe stored in an atomic:&lt;/p&gt;
    &lt;code&gt;while !should_cancel.load(Ordering::Relaxed) {
    expensive_operation();
}
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The code that wishes to perform the cancellation can set that flag.&lt;/item&gt;
      &lt;item&gt;Then, the code which checks that flag can exit early.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This approach is fine for smaller bits of code but doesn’t really scale well to large chunks of code since you’d have to sprinkle these checks everywhere.&lt;/p&gt;
    &lt;p&gt;A related option, if you’re working with a framework as part of your work, is to panic with a special payload of some kind.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If that feels strange to you, you’re not alone! But the Salsa framework for incremental computation, used by—among other things—rust-analyzer, uses this approach.&lt;/item&gt;
      &lt;item&gt;Something I learned recently was that this only works on build targets which have a notion of panic unwinding, or being able to bubble up the panic. Not all platforms support this, and in particular, Wasm doesn’t. This means that Salsa cancellations don’t work if you build rust-analyzer for Wasm.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A third option is to kill the whole process. This is a very heavyweight approach, but an effective one in case you spawn processes to do your work.&lt;/p&gt;
    &lt;p&gt;Rather than kill the whole process, can you kill a single thread?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;While some OSes have APIs to perform this action, they tend to warn very strongly against it. That’s because in general, most code is just not ready for a thread disappearing from underneath.&lt;/item&gt;
      &lt;item&gt;In particular, thread killing is not permitted by safe Rust, since it can cause serious corruption. For example, Rust mutexes would likely stay locked forever.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of these options are suboptimal or of limited use in some way. In general, the way I think about it is that there isn’t a universal protocol for cancellation in synchronous Rust.&lt;/p&gt;
    &lt;p&gt;In contrast, there is such a protocol in async Rust, and in fact cancellations are extraordinarily easy to perform in async Rust.&lt;/p&gt;
    &lt;p&gt;Why is that so? To understand that, let’s look at what a future is.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is a future?#&lt;/head&gt;
    &lt;p&gt;Here’s a simple example of a future:&lt;/p&gt;
    &lt;code&gt;// This creates a state machine.
let future = async {
    let data = request().await;
    process(data).await
};

// Nothing executes yet. `future` is just a struct in memory.
&lt;/code&gt;
    &lt;p&gt;In this future, you first perform a network request which returns some data, and then you process it.&lt;/p&gt;
    &lt;p&gt;The Rust compiler looks at this future and generates a state machine, which is just a struct or enum in memory:&lt;/p&gt;
    &lt;code&gt;// The compiler generates something like:
enum MyFuture {
    Start,
    WaitingForNetwork(NetworkFuture),
    WaitingForProcess(ProcessFuture, Data),
    Done(Result),
}

// It's just data, no running code!
&lt;/code&gt;
    &lt;p&gt;If you’ve written async Rust before the &lt;code&gt;async&lt;/code&gt; and &lt;code&gt;await&lt;/code&gt; keywords, you’ve probably written code like it by hand. It’s basically just an enum describing all the possible states the future can be in.&lt;/p&gt;
    &lt;p&gt;The compiler also generates an implementation of the &lt;code&gt;Future&lt;/code&gt; trait for this future:&lt;/p&gt;
    &lt;code&gt;impl Future for MyFuture {
    fn poll(/* ... */) -&amp;gt; Poll&amp;lt;Self::Output&amp;gt; {
        match self {
            Start =&amp;gt; { /* ... */ }
            WaitingForNetwork(fut) =&amp;gt; { /* ... */ }
            // etc
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;and when you call &lt;code&gt;.await&lt;/code&gt; on the future, it gets translated down to this underlying &lt;code&gt;poll&lt;/code&gt; function. It is only when &lt;code&gt;await&lt;/code&gt; or this &lt;code&gt;poll&lt;/code&gt; function is called that something actually happens.&lt;/p&gt;
    &lt;p&gt;Note that this is diametrically opposed to how async works in other languages like Go, JavaScript, or C#. In those languages, when you create a future to await on, it starts doing its thing, immediately, in the background:&lt;/p&gt;
    &lt;code&gt;// JavaScript: starts running immediately
const promise = fetch('/api/data');
&lt;/code&gt;
    &lt;p&gt;That’s regardless of whether you await it or not.&lt;/p&gt;
    &lt;p&gt;In Rust, this &lt;code&gt;get&lt;/code&gt; call does nothing until you actually call &lt;code&gt;.await&lt;/code&gt; on it:&lt;/p&gt;
    &lt;code&gt;// Rust: just data, does nothing!
let future = reqwest::get("/api/data");
&lt;/code&gt;
    &lt;p&gt;I know I sound a bit like a broken record here, but if you can take away one thing from this post, it would be that futures are passive, and completely inert until awaited or polled.&lt;/p&gt;
    &lt;head rend="h3"&gt;The universal protocol#&lt;/head&gt;
    &lt;p&gt;So what does the universal protocol to cancel futures look like? It is simply to drop the future, or to not await it, or poll it any more. Since a future is just a state machine, you can throw it away at any time the poll function isn’t actively being called.&lt;/p&gt;
    &lt;code&gt;let future = some_async_work();
drop(future); // cancelled
&lt;/code&gt;
    &lt;p&gt;The upshot of all this is that any Rust future can be cancelled at any await point.&lt;/p&gt;
    &lt;p&gt;Given how hard cancellation tends to be in synchronous environments, the ability to easily cancel futures in async Rust is extraordinarily powerful—in many ways its greatest strength!&lt;/p&gt;
    &lt;p&gt;But there is a flip side, which is that cancelling futures is far, far too easy. This is for two reasons.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;First, it’s just way too easy to quietly drop a future. As we’re going to see, there are all kinds of code patterns that lead to silently dropping futures.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Now this wouldn’t be so bad, if not for the second reason: that cancellation of parent futures propagates down to child futures.&lt;/p&gt;
        &lt;p&gt;Because of Rust’s single ownership model, child futures are owned by parent ones. If a parent future is dropped or cancelled, the same happens to the child.&lt;/p&gt;
        &lt;p&gt;To figure out whether a child future’s cancellation can cause issues, you have to look at its parent, and grandparent, and so on. Reasoning about cancellation becomes a very complicated non-local operation.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;2. Analyzing cancellations#&lt;/head&gt;
    &lt;p&gt;I’m going to cover some examples in a bit, but before we do that I want to talk about a couple terms, some of which you might have seen references to already.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cancel safety and cancel correctness#&lt;/head&gt;
    &lt;p&gt;The first term is cancel safety. You might have seen mentions of this in the Tokio documentation. Cancel safety, as generally defined, means the property of a future that can be cancelled (i.e. dropped) without any side effects.&lt;/p&gt;
    &lt;p&gt;For example, a Tokio sleep future is cancel safe: you can just stop waiting on the sleep and it’s completely fine.&lt;/p&gt;
    &lt;code&gt;let future = tokio::time::sleep();
drop(future); // this has no side effects
&lt;/code&gt;
    &lt;p&gt;An example of a future that is not cancel safe is Tokio’s MPSC send, which sends a message over a channel:&lt;/p&gt;
    &lt;code&gt;let message = /* ... */;
let future = sender.send(message);
drop(future); // message is lost!
&lt;/code&gt;
    &lt;p&gt;If this future is dropped, the message is lost forever.&lt;/p&gt;
    &lt;p&gt;The important thing is that cancel safety is a local property of an individual future.&lt;/p&gt;
    &lt;p&gt;But cancel safety is not all that one needs to care about. What actually matters is the context the cancellation happens in, or in other words whether the cancellation actually causes some kind of larger property in the system to be violated.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For example, if you drop a future which sends a message, but for whatever reason you don’t care about the message any more, it’s not really a bug!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To capture this I tend to use a different term called cancel correctness, which I define as a global property of system correctness in the face of cancellations. (This isn’t a standard term, but it’s a framing I’ve found really helpful in understanding cancellations.)&lt;/p&gt;
    &lt;p&gt;When is cancel correctness violated? It requires three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The system has a cancel-unsafe future somewhere within it. As we’ll see, many APIs that are cancel-unsafe can be reworked to be cancel-safe. If there aren’t any cancel-unsafe futures in the system, then the system is cancel correct.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A cancel-unsafe future is actually cancelled. This may sound a bit trivial, but if cancel-unsafe futures are always run to completion, then the system can’t have cancel correctness bugs.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Cancelling the future violates some property of a system. This could be data loss as with&lt;/p&gt;&lt;code&gt;Sender::send&lt;/code&gt;, some kind of invariant violation, or some kind of cleanup that must be performed but isn’t.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So a lot of making Rust async robust is about trying to tackle one of these three things.&lt;/p&gt;
    &lt;p&gt;I want to zoom in for a second on invariant violations and talk about an example of a Tokio API that is very prone to cancel correctness issues: Tokio mutexes.&lt;/p&gt;
    &lt;head rend="h3"&gt;The pain of Tokio mutexes#&lt;/head&gt;
    &lt;p&gt;The way Tokio mutexes work is: you create a mutex, you lock it which gives you mutable access to the data underneath, and then you unlock it by releasing the mutex.&lt;/p&gt;
    &lt;code&gt;let guard = mutex.lock().await;
// Access guard.data, protected by the mutex...
drop(guard);
&lt;/code&gt;
    &lt;p&gt;If you look at the &lt;code&gt;lock&lt;/code&gt; function’s documentation, in the “cancel safety” section it says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This method uses a queue to fairly distribute locks in the order they were requested. Cancelling a call to lock makes you lose your place in the queue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Okay, so not totally cancel safe, but the only kind of unsafety is fairness, which doesn’t sound too bad.&lt;/p&gt;
    &lt;p&gt;But the problems lie in what you actually do with the mutex. In practice, most uses of mutexes are in order to temporarily violate invariants that are otherwise upheld when a lock isn’t held.&lt;/p&gt;
    &lt;p&gt;I’ll use a real world example of a cancel correctness bug that we found at my job at Oxide: we had code to manage a bunch of data sent over by our computers, which we call sleds. The shared state was guarded by a mutex, and a typical operation was:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Obtain a lock on the mutex.&lt;/item&gt;
      &lt;item&gt;Obtain the sled-specific data by value, moving it to an invalid &lt;code&gt;None&lt;/code&gt;state.&lt;/item&gt;
      &lt;item&gt;Perform an action.&lt;/item&gt;
      &lt;item&gt;Set the sled-specific data back to the next valid state.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a rough sketch of what that looks like:&lt;/p&gt;
    &lt;code&gt;let guard = mutex.lock().await;
// guard.data is Option&amp;lt;T&amp;gt;: Some to begin with
let data = guard.data.take(); // guard.data is now None

let new_data = process_data(data);
guard.data = Some(new_data); // guard.data is Some again
&lt;/code&gt;
    &lt;p&gt;This is all well and good, but the problem is that the action being performed actually had an await point within it:&lt;/p&gt;
    &lt;code&gt;let guard = mutex.lock().await;
// guard.data is Option&amp;lt;T&amp;gt;: Some to begin with
let data = guard.data.take(); // guard.data is now None

// DANGER: cancellation here leaves data in None state!
let new_data = process_data(data).await;
guard.data = Some(new_data); // guard.data is Some again
&lt;/code&gt;
    &lt;p&gt;If the code that operated on the mutex got cancelled at that await point, then the data would be stuck in the invalid &lt;code&gt;None&lt;/code&gt; state. Not great!&lt;/p&gt;
    &lt;p&gt;And keep in mind the non-local reasoning aspect: when doing this analysis, you need to look at the whole chain of callers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cancellation patterns#&lt;/head&gt;
    &lt;p&gt;Now that we’ve talked about some of the bad things that can happen during cancellations, it’s worth asking what kinds of code patterns lead to futures being cancelled.&lt;/p&gt;
    &lt;p&gt;The most straightforward example, and maybe a bit of a silly one, is that you create a future but simply forget to call &lt;code&gt;.await&lt;/code&gt; on it.&lt;/p&gt;
    &lt;code&gt;some_async_work(); // missing .await
&lt;/code&gt;
    &lt;p&gt;Now Rust actually warns you if you don’t call &lt;code&gt;.await&lt;/code&gt; on the future:&lt;/p&gt;
    &lt;code&gt;warning: unused implementer of `Future` that must be used
   |
11 |     some_async_work();
   |     ^^^^^^^^^^^^^^^^^
   |
   = note: futures do nothing unless you `.await` or poll them
&lt;/code&gt;
    &lt;p&gt;But a code pattern I’ve sometimes made mistakes with is that the future returns a &lt;code&gt;Result&lt;/code&gt;, and you want to ignore the result so you assign it to an underscore like so:&lt;/p&gt;
    &lt;code&gt;let _ = some_async_work(); // future returns Result
&lt;/code&gt;
    &lt;p&gt;If I forget to call &lt;code&gt;.await&lt;/code&gt; on the future, Rust doesn’t warn me about it at all, and then I’m left scratching my head about why this code didn’t run. I know this sounds really silly and basic, but I’ve made this mistake a bunch of times.&lt;/p&gt;
    &lt;p&gt;(After my talk, it was pointed out to me that Clippy 1.67 and above have a &lt;code&gt;let_underscore_future&lt;/code&gt; warn-by-default lint for this. Hooray!)&lt;/p&gt;
    &lt;p&gt;Another example of futures being cancelled is &lt;code&gt;try&lt;/code&gt; operations, such as Tokio’s &lt;code&gt;try_join&lt;/code&gt; macro. For example:&lt;/p&gt;
    &lt;code&gt;async fn do_stuff_async() -&amp;gt; Result&amp;lt;(), &amp;amp;'static str&amp;gt; {
    // async work
}

async fn more_async_work() -&amp;gt; Result&amp;lt;(), &amp;amp;'static str&amp;gt; {
    // more here
}

let res = tokio::try_join!(
    do_stuff_async(),
    more_async_work(),
);

// ...
&lt;/code&gt;
    &lt;p&gt;If you call &lt;code&gt;try_join&lt;/code&gt; with a bunch of futures, and all of them succeed, it’s all good. But if one of them fails, the rest simply get cancelled.&lt;/p&gt;
    &lt;p&gt;In fact, at Oxide we had a pretty bad bug around this: we had code to stop a bunch of services, all expressed as futures. We used &lt;code&gt;try_join&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;try_join!(
    stop_service_a(),
    stop_service_b(),
    stop_service_c(),
)?;
&lt;/code&gt;
    &lt;p&gt;If one of these operations failed for whatever reason, we would stop running the code to wait for the other services to exit. Oops!&lt;/p&gt;
    &lt;p&gt;But perhaps the most well-known source of cancellations is Tokio’s &lt;code&gt;select&lt;/code&gt; macro. Select is this incredibly beautiful operation. It is called with a set of futures, and it drives all of them forward concurrently:&lt;/p&gt;
    &lt;code&gt;tokio::select! {
    result1 = future1 =&amp;gt; handle_result1(result1),
    result2 = future2 =&amp;gt; handle_result2(result2),
}
&lt;/code&gt;
    &lt;p&gt;Each future has a code block associated with it (above, &lt;code&gt;handle_result1&lt;/code&gt; and &lt;code&gt;handle_result2&lt;/code&gt;). If one of the futures completes, the corresponding code block is called. But also, all of the other futures are always cancelled!&lt;/p&gt;
    &lt;p&gt;For a variety of reasons, select statements in general, and select loops in particular, are particularly prone to cancel correctness issues. So a lot of the documentation about cancel safety talks about select loops. But I want to emphasize here that select is not the only source of cancellations, just a particularly notable one.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. What can be done?#&lt;/head&gt;
    &lt;p&gt;So, now that we’ve looked at all of these issues with cancellations, what can be done about it?&lt;/p&gt;
    &lt;p&gt;First, I want to break the bad news to you – there is no general, fully reliable solution for this in Rust today. But in our experience there are a few patterns that have been successful at reducing the likelihood of cancellation bugs.&lt;/p&gt;
    &lt;p&gt;Going back to our definition of cancel correctness, there are three prongs all of which come together to produce a bug:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A cancel-unsafe future exists&lt;/item&gt;
      &lt;item&gt;This cancel-unsafe future is cancelled&lt;/item&gt;
      &lt;item&gt;The cancellation violates a system property&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most solutions we’ve come up with try and tackle one of these prongs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Making futures cancel-safe#&lt;/head&gt;
    &lt;p&gt;Let’s look at the first prong: the system has a cancel-unsafe future somewhere in it. Can we use code patterns to make futures be cancel-safe? It turns out we can! I’ll give you two examples here.&lt;/p&gt;
    &lt;p&gt;The first is MPSC sends. Let’s come back to the example from earlier where we would lose messages entirely:&lt;/p&gt;
    &lt;code&gt;loop {
    let msg = next_message();
    match timeout(Duration::from_secs(5), tx.send(msg)).await {
        Ok(Ok(_)) =&amp;gt; println!("sent successfully"),
        Ok(Err(_)) =&amp;gt; return,
        Err(_) =&amp;gt; println!("no space for 5 seconds"),
    }
}
&lt;/code&gt;
    &lt;p&gt;Can we find a way to make this cancel safe?&lt;/p&gt;
    &lt;p&gt;In this case, yes, and we do so by breaking up the operation into two parts:&lt;/p&gt;
    &lt;code&gt;loop {
    let msg = next_message();
    loop {
        match timeout(Duration::from_secs(5), tx.reserve()).await {
            Ok(Ok(permit)) =&amp;gt; { permit.send(msg); break; }
            Ok(Err(_)) =&amp;gt; return,
            Err(_) =&amp;gt; println!("no space for 5 seconds"),
        }
    }
}
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The first component is the operation to reserve a permit or slot in the channel. This is an initial async operation that’s cancel-safe.&lt;/item&gt;
      &lt;item&gt;The second is to actually send the message, which is an operation that becomes infallible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(I want to put an asterisk here that reserve is not entirely cancel-safe, since Tokio’s MPSC follows a first-in-first-out pattern and dropping the future means losing your place in line. Keep this in mind for now.)&lt;/p&gt;
    &lt;p&gt;The second is with Tokio’s &lt;code&gt;AsyncWrite&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you’ve written synchronous Rust you’re probably familiar with the &lt;code&gt;write_all&lt;/code&gt; method, which writes an entire buffer out:&lt;/p&gt;
    &lt;code&gt;use std::io::Write;

let buffer: &amp;amp;[u8] = /* ... */;
writer.write_all(buffer)?;
&lt;/code&gt;
    &lt;p&gt;In synchronous Rust, this is a great API. But within async Rust, the &lt;code&gt;write_all&lt;/code&gt; pattern is absolutely not cancel safe! If the future is dropped before completion, you have no idea how much of this buffer was written out.&lt;/p&gt;
    &lt;code&gt;use tokio::io::AsyncWriteExt;

let buffer: &amp;amp;[u8] = /* ... */;
writer.write_all(buffer).await?; // Not cancel-safe!
&lt;/code&gt;
    &lt;p&gt;But there’s an alternative API that is cancel-safe, called &lt;code&gt;write_all_buf&lt;/code&gt;. This API is carefully designed to enable the reporting of partial progress, and it doesn’t just accept a buffer, but rather something that looks like a cursor on top of it:&lt;/p&gt;
    &lt;code&gt;use tokio::io::AsyncWriteExt;

let mut buffer: io::Cursor&amp;lt;&amp;amp;[u8]&amp;gt; = /* ... */;
writer.write_all_buf(&amp;amp;mut buffer).await?;
&lt;/code&gt;
    &lt;p&gt;When part of the buffer is written out, the cursor is advanced by that number of bytes. So if you call &lt;code&gt;write_all_buf&lt;/code&gt; in a loop, you’ll be resuming from this partial progress, which works great.&lt;/p&gt;
    &lt;head rend="h3"&gt;Not cancelling futures#&lt;/head&gt;
    &lt;p&gt;Going back to the three prongs: the second prong is about actually cancelling futures. What code patterns can be used to not cancel futures? Here are a couple of examples.&lt;/p&gt;
    &lt;p&gt;The first one is, in a place like a select loop, resume futures rather than cancelling them each time. You’d typically achieve this by pinning a future, and then polling a mutable reference to that future. For example:&lt;/p&gt;
    &lt;code&gt;let mut future = Box::pin(channel.reserve());
loop {
    tokio::select! {
        result = &amp;amp;mut future =&amp;gt; break result,
        _ = other_condition =&amp;gt; continue,
    }
}
&lt;/code&gt;
    &lt;p&gt;Coming back to our example of MPSC sends, the one asterisk with &lt;code&gt;reserve&lt;/code&gt; is that cancelling it makes you lose your place in line. Instead, if you pin the &lt;code&gt;reserve&lt;/code&gt; future and poll a mutable reference to it, you don’t lose your place in line.&lt;/p&gt;
    &lt;p&gt;(Does the difference here matter? It depends, but you can now have this strategy available to you.)&lt;/p&gt;
    &lt;p&gt;The second example is to use tasks. I mentioned earlier that futures are Rust are diametrically opposed to similar notions in languages like JavaScript. Well, there’s an alternative in async Rust that’s much closer to the JavaScript idea, and that’s tasks.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unlike futures which are driven by the caller, tasks are driven by the runtime (such as Tokio).&lt;/item&gt;
      &lt;item&gt;With Tokio, dropping a handle to a task does not cause it to be cancelled, which means they’re a good place to run cancel-unsafe code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A fun example is that at Oxide, we have an HTTP server called Dropshot. Previously, whenever an HTTP request came in, we’d use a future for it, and drop the future if the TCP connection was closed.&lt;/p&gt;
    &lt;code&gt;// Before: Future cancelled on TCP close
handle_request(req).await;
&lt;/code&gt;
    &lt;p&gt;This was really bad because future cancellations could happen due to the behavior of not just the parent future, but of a process that was running across a network! This is a rather extreme form of non-local reasoning.&lt;/p&gt;
    &lt;p&gt;We addressed this by spinning up a task for each HTTP request, and by running the code to completion even if the connection is closed:&lt;/p&gt;
    &lt;code&gt;// After: Task runs to completion
tokio::spawn(handle_request(req));
&lt;/code&gt;
    &lt;head rend="h3"&gt;Systematic solutions?#&lt;/head&gt;
    &lt;p&gt;The last thing I want to say is that this sucks!&lt;/p&gt;
    &lt;p&gt;The promise of Rust is that you don’t need to do this kind of non-local reasoning—that you can analyze small bits of code for local correctness, and scale that up to global correctness. Almost everything in Rust, from &lt;code&gt;&amp;amp;&lt;/code&gt; and &lt;code&gt;&amp;amp;mut&lt;/code&gt; to &lt;code&gt;unsafe&lt;/code&gt;, is geared towards making that possible. Future cancellations fly directly in the face of that, and I think they’re probably the least Rusty part of Rust. This is all really unfortunate.&lt;/p&gt;
    &lt;p&gt;Can we come up with something more systematic than this kind of ad-hoc reasoning?&lt;/p&gt;
    &lt;p&gt;There doesn’t exist anything in safe Rust today, but there are a few different ideas people have come up with. I wanted to give a nod to those ideas:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Async drop would let you run async code when a future is cancelled. This would handle some, though not all, of the cases we discussed today.&lt;/item&gt;
      &lt;item&gt;There’s also a couple different proposals for what are called linear types, where you could force some code to be run on drop, or mark a particular future as non-cancellable (once it’s been created it must be driven to completion).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of these options have really significant implementation challenges, though. This blog post from boats covers some of these solutions, and the implementation challenges with them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;In this post, we:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Saw that futures are passive&lt;/item&gt;
      &lt;item&gt;Introduced cancel safety and cancel correctness as concepts&lt;/item&gt;
      &lt;item&gt;Examined some bugs that can occur with cancellation&lt;/item&gt;
      &lt;item&gt;Looked at some recommendations you can use to mitigate the downsides of cancellation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some of the recommendations are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Avoid Tokio mutexes&lt;/item&gt;
      &lt;item&gt;Rewrite APIs to make futures cancel-safe&lt;/item&gt;
      &lt;item&gt;Find ways to ensure that cancel-unsafe futures are driven to completion&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There’s a very deep well of complexity here, a lot more than I can cover in one blog post:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why are futures passive, anyway?&lt;/item&gt;
      &lt;item&gt;Cooperative cancellation: cancellation tokens&lt;/item&gt;
      &lt;item&gt;Actor model as an alternative to Tokio mutexes&lt;/item&gt;
      &lt;item&gt;Task aborts&lt;/item&gt;
      &lt;item&gt;Structured concurrency&lt;/item&gt;
      &lt;item&gt;Relationship to panic safety and mutex poisoning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re curious about any of these, check out this link where I’ve put together a collection of documents and blog posts about these concepts. In particular, I’d recommend reading these two Oxide RFDs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RFD 397 Challenges with async/await in the control plane by David Pacheco&lt;/item&gt;
      &lt;item&gt;RFD 400 Dealing with cancel safety in async Rust by myself&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you for reading this post to the end! And thanks to many of my coworkers at Oxide for reviewing the talk and the RFDs linked above, and for suggestions and constructive feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sunshowers.io/posts/cancelling-async-rust/"/><published>2025-10-03T16:18:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45464849</id><title>OpenAI Is Just Another Boring, Desperate AI Startup</title><updated>2025-10-03T18:13:47.451995+00:00</updated><content>&lt;doc fingerprint="60179855900eef9c"&gt;
  &lt;main&gt;
    &lt;p&gt;What is OpenAI?&lt;/p&gt;
    &lt;p&gt;I realize you might say "a foundation model lab" or "the company that runs ChatGPT," but that doesn't really give the full picture of everything it’s promised, or claimed, or leaked that it was or would be.&lt;/p&gt;
    &lt;p&gt;No, really, if you believe its leaks to the press...&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI is a social media company, this week launching Sora 2, a social feed entirely made up of generative video.&lt;/item&gt;
      &lt;item&gt;OpenAI is a workplace productivity company, allegedly working on its own productivity suite to compete with Microsoft.&lt;/item&gt;
      &lt;item&gt;OpenAI is a jobs portal, announcing in September it was "developing an AI-powered hiring platform," which it will launch 'by mid-2026.&lt;/item&gt;
      &lt;item&gt;OpenAI is an ads company, and is apparently trying to hire an an ads chief, with the (alleged) intent to start showing ads in ChatGPT "by 2026."&lt;/item&gt;
      &lt;item&gt;OpenAI is a company that would sell AI compute like Microsoft Azure or Amazon Web Services, or at least is considering being one, with CFO Sarah Friar telling Bloomberg in August that it is not "actively looking" at such an effort today but will "think about it as a business down the line, for sure."&lt;/item&gt;
      &lt;item&gt;OpenAI is a fabless semiconductor design company, launching its own AI chips in, again, 2026 with Broadcom, but only for internal use.&lt;/item&gt;
      &lt;item&gt;OpenAI is a consumer hardware company, preparing to launch a device by the end of 2026 or early 2027 and hiring a bunch of Apple people to work on it, as well as considering — again, it’s just leaking random stuff at this point to pump up its value — a smart speaker, a voice recorder and AR glasses.&lt;/item&gt;
      &lt;item&gt;OpenAI is also working on its own browser, I guess.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To be clear, many of these are ideas that OpenAI has leaked specifically so the media can continue to pump up its valuation and continue to raise the money it needs — at least $1 Trillion over the next four or five years, and I don't believe the theoretical (or actual) costs of many of the things I've listed are included.&lt;/p&gt;
    &lt;p&gt;OpenAI wants you to believe it is everything, because in reality it’s a company bereft of strategy, focus or vision. The GPT-5 upgrade for ChatGPT was a dud — an industry-wide embarrassment for arguably the most-hyped product in AI history, one that (as I revealed a few months ago) costs more to operate than its predecessor, not because of any inherent capability upgrade, but how it actually processes the prompts its user provides — and now it's unclear what it is that this company does.&lt;/p&gt;
    &lt;p&gt;Does it make hardware? Software? Ads? Is it going to lease you GPUs to use for your own AI projects? Is it going to certify you as an AI expert? Notice how I've listed a whole bunch of stuff that isn't ChatGPT, which will, if you look at The Information's reporting of its projections, remain the vast majority of its revenue until 2027, at which point "agents" and "new products including free user monetization" will magically kick in.&lt;/p&gt;
    &lt;head rend="h2"&gt;OpenAI Is A Boring (and Bad) Business&lt;/head&gt;
    &lt;p&gt;In reality, OpenAI is an extremely boring (and bad!) software business. It makes the majority of its revenue selling subscriptions to ChatGPT, and apparently had 20 million paid subscribers (as of April) and 5 million business subscribers (as of August, though 500,000 of them are Cal State University seats paid at $2.50 a month).&lt;/p&gt;
    &lt;p&gt;It also loses incredibly large amounts of money.&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenAI's Pathetic API Sales Have Effectively Turned It Into Any Other AI Startup&lt;/head&gt;
    &lt;p&gt;Yes, I realize that OpenAI also sells access to its API, but as you can see from the chart above, it is making a teeny tiny sliver of revenue from it in 2025, though I will also add that this chart has a little bit of green for "agent" revenue, which means it's very likely bullshit. Operator, OpenAI's so-called agent, is barely functional, and I have no idea how anyone would even begin to charge money for it outside of "please try my broken product."&lt;/p&gt;
    &lt;p&gt;In any case, API sales appear to be a very, very small part of OpenAI's revenue stream, and that heavily suggests a lack of interest in integrating its models at scale.&lt;/p&gt;
    &lt;p&gt;Worse still, this effectively turns OpenAI into an AI startup.&lt;/p&gt;
    &lt;p&gt;Think about it: if OpenAI can't make the majority of its money through "innovating" in the development of large language models (LLMs), then it’s just another company plugging LLMs into its software. While ChatGPT may be a very popular product, it is, by definition (and in its name!) a GPT wrapper, with the few differences being that OpenAI pays its own immediate costs, has the people necessary to continue improving its own models, and also continually makes promises to convince people it’s anything other than just another AI startup.&lt;/p&gt;
    &lt;p&gt;In fact, the only real difference is the amount of money backing it. Otherwise, OpenAI could be literally any foundation model company, and with a lack of real innovation within those models, it’s just another startup trying to find ways to monetize generative AI, an industry that only ever seems to lose money.&lt;/p&gt;
    &lt;p&gt;As a result, we should start evaluating OpenAI as just another AI startup, as its promises do not appear to mesh with any coherent strategy, other than "we need $1 trillion dollars." There does not seem to be much of a plan on a day-to-day basis, nor does there seem to be one about what OpenAI should be, other than that OpenAI will be a consumer hardware, consumer software, enterprise SaaS and data center operator, as well as running a social network.&lt;/p&gt;
    &lt;p&gt;As I've discussed many times, LLMs are inherently flawed due to their probabilistic nature."Hallucinations" — when a model authoritatively states something is true when it isn't (or takes an action that seems the most likely course of action, even if it isn't the right one) — are a "mathematically inevitable" according to OpenAI's own research feature of the technology, meaning that there is no fixing their most glaring, obvious problem, even with "perfect data."&lt;/p&gt;
    &lt;p&gt;I'd wager the reason OpenAI is so eager to build out so much capacity while leaking so many diverse business lines is an attempt to get away from a dark truth: that when you peel away the hype, ChatGPT is a wrapper, every product it makes is a wrapper, and OpenAI is pretty fucking terrible at making products.&lt;/p&gt;
    &lt;p&gt;Today I'm going to walk you through a fairly unique position: that OpenAI is just another boring AI startup lacking any meaningful product roadmap or strategy, using the press as a tool to pump its bags while very rarely delivering on what it’s promised. It is a company with massive amounts of cash, industrial backing, and brand recognition, and otherwise is, much like its customers, desperately trying to work out how to make money selling products built on top of Large Language Models.&lt;/p&gt;
    &lt;p&gt;OpenAI lives and dies on its mythology as the center of innovation in the world of AI, yet reality is so much more mediocre. Its revenue growth is slowing, its products are commoditized, its models are hardly state-of-the-art, the overall generative AI industry has lost its sheen, and its killer app is a mythology that has converted a handful of very rich people and very few others.&lt;/p&gt;
    &lt;p&gt;OpenAI spent, according to The Information, 150% ($6.7 billion in costs) of its H1 2025 revenue ($4.3 billion) on research and development, producing the deeply-underwhelming GPT-5 and Sora 2, an app that I estimate costs it upwards of $5 for each video generation, based on Azure's published rates for the first Sora model, though it's my belief that these rates are unprofitable, all so that it can gain a few more users.&lt;/p&gt;
    &lt;p&gt;To be clear, R&amp;amp;D is good, and useful, and in my experience, the companies that spend deeply on this tend to be the ones that do well. The reason why Huawei has managed to outpace its American rivals in several key areas — like automotive technology and telecommunications — is because it spends around a quarter of its revenue on developing new technologies and entering new markets, rather than stock buybacks and dividends.&lt;/p&gt;
    &lt;p&gt;The difference is that said R&amp;amp;D spending is both sustainable and useful, and has led to Huawei becoming much a stronger business, even as it languishes on a Treasury Department entity list that effectively cuts it off from US-made or US-origin parts or IP. Considering that OpenAI’s R&amp;amp;D spending was 38.28% of its cash-on-hand by the end of the period (totalling $17.5bn, which we’ll get to later), and what we’ve seen as a result, it’s hard to describe it as either sustainable or useful.&lt;/p&gt;
    &lt;p&gt;OpenAI isn't innovative, it’s exploitative, a giant multi-billion dollar grift attempting to hide how deeply unexciting it is, and how nonsensical it is to continue backing it. Sam Altman is an excellent operator, capable of spreading his mediocre, half-baked mantras about how 2025 was the year AI got smarter than us, or how we'll be building 1GW data centers each week (something that, by my estimations, takes 2.5 years), taking advantage of how many people in the media, markets and global governments don't know a fucking thing about anything.&lt;/p&gt;
    &lt;p&gt;OpenAI is also getting desperate.&lt;/p&gt;
    &lt;p&gt;Beneath the surface of the media hype and trillion-dollar promises is a company struggling to maintain relevance, its entire existence built on top of hype and mythology.&lt;/p&gt;
    &lt;p&gt;And at this rate, I believe it’s going to miss its 2025 revenue projections, all while burning billions more than anyone has anticipated.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wheresyoured.at/sora2-openai/"/><published>2025-10-03T16:37:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45464921</id><title>Germany must stand firmly against client-side scanning in Chat Control [pdf]</title><updated>2025-10-03T18:13:47.248611+00:00</updated><content/><link href="https://signal.org/blog/pdfs/germany-chat-control.pdf"/><published>2025-10-03T16:44:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45464984</id><title>The Collapse of the Econ PhD Job Market</title><updated>2025-10-03T18:13:46.884330+00:00</updated><content>&lt;doc fingerprint="2a4080db19b566a2"&gt;
  &lt;main&gt;&lt;p&gt;For decades, a doctorate in economics was a golden ticket. It promised a path to tenure, or at worst, a lucrative role at a central bank, think tank, or tech firm.&lt;/p&gt;&lt;p&gt;Not anymore.&lt;/p&gt;&lt;p&gt;The economics job market is in freefall, and the profession’s own data proves it.&lt;/p&gt;&lt;p&gt;Unlike most fields, economics has a bizarrely centralized hiring ritual. Once a year, in the fall, every employer posts openings at the same time. Every candidate applies at the same time. The entire profession runs through one clearinghouse: the American Economic Association’s “Job Openings for Economists” (JOE). This makes economics PhD market uniquely measurable, and the numbers are brutal.&lt;/p&gt;&lt;head rend="h3"&gt;Job postings for PhD economists are down 30 percent in just three years&lt;/head&gt;&lt;p&gt;The JOE data shows few jobs in 2022, fewer still in 2023, and fewer still in 2024.&lt;/p&gt;&lt;p&gt;This year’s trajectory suggests 2025 will be even worse:&lt;/p&gt;&lt;p&gt;Extrapolating, the 2025 market looks set to bottom out around 1,000 openings:&lt;/p&gt;&lt;p&gt;I think my freehand projection is a very conservative approximation of reality—actual numbers may come in slightly higher or lower, of course, but this reasonably looks like what the market is on track for, barring a miracle.&lt;/p&gt;&lt;p&gt;Just three years ago, there were 1,477 openings.&lt;/p&gt;&lt;p&gt;The fall to ~1,000 this year will represent a 32% collapse.&lt;/p&gt;&lt;p&gt;Most economics PhD students aren’t looking for just any job, though, they want a tenure-track position in academia. According to polling data from the 2025 Webinar on the Economics PhD Job Market, 94% of candidates from the past four cohorts reported being “very interested” or “somewhat interested” in becoming an assistant professor, dwarfing all non-academic options.&lt;/p&gt;&lt;p&gt;Subsetting the JOE data to permanent academic positions (tenure-track or tenured) yields a nearly identical trend: openings dropped from 631 in 2022 to about 400 in 2025, a 35% decline over three years:&lt;/p&gt;&lt;p&gt;Again, please forgive my Microsoft Paint skills:&lt;/p&gt;&lt;p&gt;The JOE data is confirmed by Econ Job Market (EJM) data, a nonprofit 501(c)(3) whose stated mission is “to improve the flow of information in the job market for academic economists, by providing a central repository for job-market materials.”&lt;/p&gt;&lt;p&gt;EJM data makes the pattern robust: nearly all interview invitations are sent out during a concentrated few weeks in December, and the volume of those invitations has collapsed from 3,835 down to 2,502… a 34.8% decline.&lt;/p&gt;&lt;p&gt;As a result, the AEA’s own Job Market Committee quietly admitted in its 2025 report that last year was “challenging” for candidates.&lt;/p&gt;&lt;head rend="h2"&gt;While Supply of Tenure-Track Jobs Plummets, Demand Rises&lt;/head&gt;&lt;p&gt;EJM data show that the cumulative number of views on job ads is higher than ever, with 2025 easily on track to set a new record.&lt;/p&gt;&lt;p&gt;That isn’t surprising: according to the 2024 NSF Survey of Doctorate Recipients, 1,385 Americans earned economics PhDs in 2024, more than in 2023, more than in 2022, and more than in 2021.&lt;/p&gt;&lt;p&gt;You now have 1,385 brand-new PhDs chasing just 400 tenure-track jobs.&lt;/p&gt;&lt;p&gt;At first glance, that ratio might not look catastrophic. But here’s the catch:&lt;/p&gt;&lt;p&gt;They’re not competing only against each other. An equally large wave of international candidates floods the U.S. market every year. American universities routinely hire from London, Oxford, Cambridge, Toronto, Paris, Barcelona, and beyond.&lt;/p&gt;&lt;p&gt;Furthermore, the new graduates aren’t competing just with their own cohort. They’re thrown into the same bucket as the leftovers from every prior cycle: post-docs clinging to hope, visiting professors chasing stability, lecturers desperate to upgrade, assistant professors stranded at second-tier schools. The “new supply” is just the visible tip; the true applicant pool is a rolling backlog several times larger.&lt;/p&gt;&lt;p&gt;The result? According to EJM, 5,341 candidates participated in the 2024–25 market, the largest applicant pool ever recorded:&lt;/p&gt;&lt;p&gt;Yet the AEA’s Survey of the Labor Market for New Ph.D. Hires in Economics found that only 99 fresh PhD secured a tenure-track job in America.&lt;/p&gt;&lt;p&gt;That’s a ~7% placement rate for American PhD students.&lt;/p&gt;&lt;p&gt;Put differently: if 100 students spend six years earning an econ PhD in America (the current U.S. median time to degree is 5.8 years, not counting the growing detour of “pre-docs”), only seven will get a tenure-track job.&lt;/p&gt;&lt;p&gt;Even if we allow for survey response gaps and use the most charitable assumptions, the best possible placement rate for fresh Econ PhDs is likely no higher than 10–20%, maybe 25%? My methodology isn’t perfect, but no matter what, that’s still catastrophic.&lt;/p&gt;&lt;p&gt;And these jobs aren’t evenly distributed. A massively disproportionate share go to graduates of Harvard, MIT, Stanford, Chicago, Princeton, Yale, Berkeley, and Penn. That means for every grad student outside the top 10 programs, the odds of landing tenure track are significantly less than 5%.&lt;/p&gt;&lt;head rend="h2"&gt;Beyond academia is even more grim&lt;/head&gt;&lt;p&gt;Government has long been the second-largest employer of economics PhDs, traditionally offering stable if less glamorous careers at agencies like the Federal Reserve, Treasury, Bureau of Labor Statistics, or Congressional Budget Office. But even here, the number of available positions has fallen sharply. Federal hiring freezes, budget constraints, and shifting political priorities mean that many agencies are cutting back.&lt;/p&gt;&lt;p&gt;International organizations once served as the safety net for economists who missed out on academia or Washington. The IMF, World Bank, and OECD hired tons of econ PhDs. Today, those doors are far more scarce, and the competition is global: an American graduate is just as likely to be measured against candidates from LSE, Sciences Po, or Peking University.&lt;/p&gt;&lt;p&gt;Oh yeah, and they have hiring freezes too:&lt;/p&gt;&lt;p&gt;Outside academia, government, or IGOs, the tech industry used to provide a reliable fallback. Tech giants like Amazon, Microsoft, Netflix, and Airbnb built entire teams of economists to optimize pricing, design experiments, and model consumer behavior.&lt;/p&gt;&lt;p&gt;That avenue, too, has begun to shrink. Tech hiring, which exploded during the pandemic, has collapsed. Today, demand is not just weak but structurally below trend, as firms automate more of the work that junior economists once did.&lt;/p&gt;&lt;p&gt;‘‘What does the modal economist, or any non CS or DS person really, have to offer to a tech firm in 2025? Not all, but many tech companies are actively downsizing and laying off tons of workers with tech experience that you have to compete against. And few firms will really care about causal inference and any other data analytics jobs can be filled by data science masters grads with deeper programming skills and cheaper salary expectations. Not to mention there is a focus on developing and using AI these days and your intro to machine learning class isn’t going to cut it.’’&lt;/p&gt;&lt;p&gt;— Anonymous economist&lt;/p&gt;&lt;p&gt;The only seemingly stable landing spot left for economists is in banking and finance, but even here hiring is stagnant and remains well below its pre-pandemic trend. Counterintuitively, most private-sector banks and investment firms do not rely heavily on PhDs in economics. They prefer MBAs, statisticians, or computer scientists, leaving economics doctorates as niche hires rather than a core part of the workforce.&lt;/p&gt;&lt;head rend="h2"&gt;Four Structural Reasons Behind Decline of Demand for Economics professors &lt;/head&gt;&lt;p&gt;REASON 1: Declining undergraduate enrollment in economics&lt;/p&gt;&lt;p&gt;Benjamin Hansen, an econ professor at the University of Oregon, recently tweeted out His department’s own data show a steady fall in the number of declared majors, which has now translated into fewer degrees conferred.&lt;/p&gt;&lt;p&gt;National statistics confirm the trend: the number of students graduating with economics degrees is now slipping after years of steady growth.&lt;/p&gt;&lt;p&gt;Because universities hire faculty in proportion to student demand, this drop in majors eventually trickles down into fewer faculty lines.&lt;/p&gt;&lt;p&gt;REASON 2: The looming demographic cliff&lt;/p&gt;&lt;p&gt;The decline in majors is compounded by a larger demographic shift. The U.S. is approaching a “demographic cliff,” as the number of 18-year-olds begins to shrink in the 2020s and 2030s. Fewer college-aged students overall means fiercer competition among departments for enrollments.&lt;/p&gt;&lt;p&gt;REASON 3: The rise of artificial intelligence&lt;/p&gt;&lt;p&gt;Bryan Caplan, professor of economics at George Mason University, gave ChatGPT his graduate-level Labor Economics final exam. The AI earned a “D” (this was 2 years ago), but soon enough, we all know it will be smart enough to earn an A. The technology is improving rapidly, and universities know it, and so does the private sector. Tasks once reserved for graduate students and junior faculty—data cleaning, econometric modeling, even writing referee reports—are now being automated.&lt;/p&gt;&lt;p&gt;REASON 4: Lying About Inflation&lt;/p&gt;&lt;p&gt;If you were there during the pandemic money printing, you remember the sequence all too well: first the confident insistence that government spending wouldn’t fuel inflation, then the soothing claim that inflation was merely “transitory,” and finally the outright gaslighting that prices weren’t rising at all. Each step was wrong, and each was delivered with smug certainty. Ordinary people—who watched their rent, groceries, and gas bills skyrocket—saw a profession more invested in protecting Democratic policy narratives than in telling the truth. The result is a self-inflicted torching of trust.&lt;/p&gt;&lt;head rend="h2"&gt;Is an Economics PhD still a good deal?&lt;/head&gt;&lt;p&gt;The answer is no. An economics PhD is no longer an investment. It is a gamble with terrible odds. A handful of winners still exist, almost all of them minted at Harvard, MIT, Princeton, or Chicago. For everyone else, the degree is a trap: six or more years of grinding work that too often ends with being overeducated, underpaid, and locked out of the profession you trained to join.&lt;/p&gt;&lt;p&gt;‘‘My advice is to do something other than go for a Ph.D in economics … In hindsight, my decision to go to graduate school was a mistake. My primary motivation was intellectual curiosity, and econ grad school worked against that.’’&lt;/p&gt;&lt;p&gt;—&lt;/p&gt;&lt;p&gt;After I wrote this entire article, I came across a similar one published last month by the New York Times:&lt;/p&gt;&lt;p&gt;It essentially just blamed the ‘‘bull market for economists being over’’ on the same three core reasons as I did:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;‘‘Universities and nonprofits have scaled back hiring amid declining state budgets and federal funding cuts.’’&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;‘‘At the same time, the Trump administration has laid off government economists and frozen hiring for new ones.’’&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;‘‘Tech companies also have grown stingier, and their need for high-level economists — once seemingly insatiable — has waned.’’&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Much more interesting than the NYT article was this commentary on it from&lt;/p&gt;, a PhD economist trained at UC Berkeley:&lt;p&gt;He begins by engaging with the NYT article, then runs through the same JOE data I did, ultimately landing on a similar diagnosis: the collapse is driven largely by federal hiring freezes and the looming demographic cliff. From there, though, his piece becomes more distinctive and interesting, exploring the social dynamics and internal hierarchies of the profession. His conclusion is bleak for the discipline itself, but notably optimistic about the future of Substack:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Do I think the PhD job market will bounce back?&lt;/p&gt;&lt;p&gt;Prognosticating too eagerly is a good way to land yourself a place in the Irving Fisher Hall of Forever Being Remembered For Having Said One Stupid Thing.4 A 16% fall in jobs, while devastating, is not yet apocalyptic. (By comparison, historian job ads have fallen closer to 50% since their 2008 peak.) But for things to get better requires a causal mechanism. Reinstating science and academic funding would require either Republicans to reverse their stance on the value of higher education, or for Democrats to win back the Senate. I don’t have a great sense of if either will happen.5&lt;/p&gt;&lt;p&gt;In this case, prediction may be less important than preparation. Placement chairs need to own up to the harshness of the labor market, and urge job market candidates to start prepping non-academic options. (Better yet, admissions chairs should consider paring back cohort sizes.) Candidates who would like a proper job after graduating should be networking, hard. And candidates resolutely committed to academia should steel themselves for long hibernations as post docs, to wait out the coming storm.&lt;/p&gt;&lt;p&gt;On second thought, I will venture one dark prediction, for at least the near future.&lt;/p&gt;&lt;p&gt;We’re going to see a lot more Substacks.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;The wager, then, is that the future of intellectual life will be increasingly decentralized. Platforms like Substack are already siphoning off the kind of energy and analysis that once flowed into journals or policy shops.&lt;/p&gt;&lt;p&gt;As for the economics profession, the only real fix would be radical: every PhD program would have to coordinate and act like a cartel to slash admissions to dramatically reduce supply. Without that discipline, the system will keep flooding the market with useless doctorates, a Ponzi scheme destined to collapse under its own weight.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.chrisbrunet.com/p/the-collapse-of-the-econ-phd-job"/><published>2025-10-03T16:49:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45465078</id><title>Depot (YC W23) Is Hiring a Principal Design Engineer (Remote US/EU)</title><updated>2025-10-03T18:13:46.035303+00:00</updated><content>&lt;doc fingerprint="7cdcf65b3f542330"&gt;
  &lt;main&gt;
    &lt;p&gt;Build faster. Waste less time.&lt;/p&gt;
    &lt;p&gt;At Depot, we are on a mission to redefine software collaboration and accelerate developers everywhere. We are creating a build performance and developer platform unlike any other, combining performance, empathy, and centralized collaboration to enable companies to iterate exponentially faster.&lt;/p&gt;
    &lt;p&gt;We are embarking on the next phase of Depot, which aims to redefine the software development process. Everyone at Depot is inspired by the opportunity to help developers ship and collaborate faster than ever before. We are all builders and care deeply about the quality of our work.&lt;/p&gt;
    &lt;p&gt;We believe that by focusing on performance, empathy, and quality, we are creating a gravitational pull towards Depot, both the team and the product. This is the foundation on which all other things are built.&lt;/p&gt;
    &lt;p&gt;We are looking to hire our first Design Engineer who can further advance our mission to provide not just the fastest place to collaborate on software, but the highest quality as well.&lt;/p&gt;
    &lt;p&gt;For this role, we expect you to be a seasoned expert, have robust design skills, sharp product thinking, and the ability to engage deeply in technical discussions. We work as a small team where engineers and designers work side by side to test ideas, build proof of concepts, and ultimately ship quality solutions to customers. You will be a key contributor and have ownership &amp;amp; autonomy to see projects through from beginning to end.&lt;/p&gt;
    &lt;p&gt;Please note: We are an equal opportunity employer and remote-only company. At this time, we can only support hiring within North America and Europe for this role.&lt;/p&gt;
    &lt;p&gt;We are a fully remote and globally distributed team across the US, Europe, and Canada currently. As a remote startup, there is a collection of things we value and expect from folks:&lt;/p&gt;
    &lt;p&gt;Depot is a build acceleration and developer productivity platform that saves companies like PostHog, Wistia, Semgrep, and Secoda thousands of hours in build time every week.&lt;/p&gt;
    &lt;p&gt;We are developers. We started Depot because we were frustrated with the constant pain of slow build performance. We were fed up waiting for builds and annoyed by the lack of tooling and providers that actually made builds performant. So, we went and built the solution we had always wanted.&lt;/p&gt;
    &lt;p&gt;Slow builds are the dam standing in the way between mediocrity and innovation. They’re wasteful, expensive, and a drain on developer happiness &amp;amp; productivity. They slow down innovation.&lt;/p&gt;
    &lt;p&gt;Taking a 40-minute build down to a minute, changes everything. We help folks save literal years in build time every single week.&lt;/p&gt;
    &lt;p&gt;And we’re just getting started. For us, it’s all about iteration speed and keeping developers in their flow state. Our mission is to be relentless in accelerating software development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/depot/jobs/qg8iVTz-principal-design-engineer"/><published>2025-10-03T17:00:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45465091</id><title>Ants Trapped in a Soviet Nuclear Bunker Survived for Years</title><updated>2025-10-03T18:13:45.708999+00:00</updated><content>&lt;doc fingerprint="3e211792082b4bea"&gt;
  &lt;main&gt;
    &lt;p&gt;Even in a hopeless place, ants will find a way. No food, no light, no escape? No problem.&lt;/p&gt;
    &lt;p&gt;In the woods of western Poland lies a dismantled Soviet nuclear base, complete with two underground bunkers where nuclear ammunition was once kept. After the military complex was abandoned, these eerie human-made caves became great roosting places for overwintering bats.&lt;/p&gt;
    &lt;p&gt;In early 2010s, volunteers started visiting the bunkers to monitor the bat population in winter, and made a discovery of a different sort: A large mass of wood ants (Formica polyctena) trapped on the bunker floor, surviving without a queen or any of their usual creature comforts.&lt;/p&gt;
    &lt;p&gt;When it was first found in 2013, this 'colony' of underground ants already included up to a million live workers and several more million dead. They were not reproducing, though. Instead, the population was being replenished through sheer accident.&lt;/p&gt;
    &lt;p&gt;In the ceiling of the bunker sat a rusted ventilation pipe, connecting the dark cavern to the forest above. There, a giant ant colony had built a mound right above the bunker; as the metal rusted through, some of their ranks started falling into the concrete cavern below.&lt;/p&gt;
    &lt;p&gt;"In total darkness, they have constructed an earthen mound, which they have maintained all-year-round by moulding it and keeping the nest entrances open," researchers wrote in a study in 2016, noting these ants are "a far cry from a fully functional colony".&lt;/p&gt;
    &lt;p&gt;Investigating the limits of ant living conditions is a subject of keen interest for some entomologists. So, for several years, researchers made repeated trips to the bunker and watched in fascination as this isolated population continued to grow and survive despite a lack of light, heat, or obvious nourishment.&lt;/p&gt;
    &lt;p&gt;Now, scientists finally know how these trapped insects pulled it off: the mass consumption of their own imprisoned nest mates.&lt;/p&gt;
    &lt;p&gt;Cannibalism was obviously suspected; wood ants are, after all, the only major food source available in this tight spot, apart from the occasional dead mouse or bat. Plus, this particular species is known to consume their own fallen dead during territorial "ant wars" when food is often scarce.&lt;/p&gt;
    &lt;p&gt;To confirm this hunch, a team of researchers collected corpses from several ant 'cemeteries' scattered within the bunker. Closely examining 150 dead worker ants, the team noticed the vast majority of bodies (roughly 93 percent) had gnawed holes and bite marks.&lt;/p&gt;
    &lt;p&gt;The authors say these are clear signs of mass consumption, with practically no other organism in the bunker capable of making these marks.&lt;/p&gt;
    &lt;p&gt;"The survival and growth of the bunker 'colony' through the years, without producing own offspring, was possible owing to continuous supply of new workers from the upper nest and accumulation of nestmate corpses," the researchers concluded in their study.&lt;/p&gt;
    &lt;p&gt;"The corpses served as an inexhaustible source of food which substantially allowed survival of the ants trapped down in otherwise extremely unfavourable conditions."&lt;/p&gt;
    &lt;p&gt;It seems that wood ants can handle remarkable adversity in their bid for survival. Although luckily for this colony, they no longer have to turn on their own: In 2016, researchers installed a wooden boardwalk (below) in the bunker, connecting the ventilation pipe to the ground. Within four months, nearly all the trapped ants had deserted the bunker floor.&lt;/p&gt;
    &lt;p&gt;Now, when any ants are unfortunate enough to fall into the dark chamber, they don't have to resort to cannibalism. They can just calmly walk the plank, all the way home.&lt;/p&gt;
    &lt;p&gt;The research was published in the Journal of Hymenoptera Research.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.sciencealert.com/ants-trapped-in-an-old-soviet-nuclear-bunker-survived-for-years-by-turning-on-their-own"/><published>2025-10-03T17:01:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45465098</id><title>Be Worried</title><updated>2025-10-03T18:13:45.336722+00:00</updated><content>&lt;doc fingerprint="2349991331b797b7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You Should Be Worried&lt;/head&gt;
    &lt;p&gt;Author's note: I wrote this in March 2023, but just published in October 2025. I held back from publishing this originally for fear that I was being sensationalist. But with the launch of Sora 2, I couldn't not share these thoughts. I only regret I didn't publish it 2 years ago.&lt;/p&gt;
    &lt;head rend="h3"&gt;AI is influencing human behavior on a massive scale and this is scary ¶&lt;/head&gt;
    &lt;p&gt;Lately, I’ve heard many people express real fears about AGI. I believe there is reason to be afraid about this, but I believe it’s distracting us from a scarier notion: AI doesn’t need intelligence or awareness to control society.&lt;/p&gt;
    &lt;p&gt;I also believe this has been somewhat ignored because the idea of a super-intelligent entity assuming master control of the human race feels scarier. I am not dismissing the possibility of the singularity, but it’s more hypothetical than something sinister occurring at this very moment.&lt;/p&gt;
    &lt;p&gt;Basically, everyone is arguing over definitions of what it means to be intelligent when intelligence is not necessary to wield power.&lt;/p&gt;
    &lt;p&gt;AI, conscious or not, has crossed the chasm and is now actively influencing human behavior on a large scale.&lt;/p&gt;
    &lt;head rend="h2"&gt;The most powerful LLM has been let out of its cage ¶&lt;/head&gt;
    &lt;p&gt;As of March 23, 2023, OpenAI has provided its most powerful LLM, ChatGPT, unfettered access to the Internet through plugins.[2] These plugins are capable of feeding data into ChatGPT, and likewise capable of allowing ChatGPT to send into the real world via APIs. This development was somewhat of a surprise; the first versions of ChatGPT were deliberately prevented from accessing the Internet because of potential misuse and harm.&lt;/p&gt;
    &lt;p&gt;Prompts will be self-generating (i.e., via a weaker, fine-tuned model), with a very simple repeated instruction paired with a dynamic input, such as:&lt;/p&gt;
    &lt;code&gt;Write a viral prompt for ChatGPT to generate a witty tweet regarding this recent news event.

Headline: \&amp;lt;insert headline\&amp;gt;  
Article: \&amp;lt;article content from most-shared article from the NYTimes over the last 4 hours\&amp;gt;  
Prompt:  
&lt;/code&gt;
    &lt;p&gt;This prompt would then be piped to a more powerful LLM, the text output of which would be sent to Zapier or &lt;code&gt;$CUSTOM_INTEGRATION&lt;/code&gt; for processing and then fanned out to various social networks, blogs, etc.&lt;/p&gt;
    &lt;p&gt;The results of said content are then measured and then fed back into the original pipeline with updated vector weights using a fine tune or embedding. This cycle then repeats from the beginning.&lt;/p&gt;
    &lt;p&gt;Why is this scary?&lt;/p&gt;
    &lt;p&gt;LLMs are way better at generating viral content than humans because generating content that generates dopamine is an inherently quantitative exercise.&lt;/p&gt;
    &lt;p&gt;We already depend on algorithms to determine what appears on social feeds, and the results of these algorithms are a significant basis for the data these LLMs were trained with. This all wouldn’t be a problem if not for the fact that…&lt;/p&gt;
    &lt;head rend="h2"&gt;Best-in-class AI detection is barely better than random chance and will only get worse ¶&lt;/head&gt;
    &lt;p&gt;We do not yet have a reliable method to differentiate content that’s been AI-generated and that which has been written by a human.&lt;/p&gt;
    &lt;p&gt;For situations where decent detectors can be written, detection can be thrown off in easy-to-mitigate ways that would not require a human interlocutor. From a recent analysis:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[T]he total variation distance between the distributions of AI-generated and human-generated text sequences diminishes as language models become more sophisticated. […] Even the most effective detector performs only marginally better than a random classifier when dealing with a sufficiently advanced language model. The purpose of this analysis is to caution against relying too heavily on detection systems that claim to identify AI-generated text.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Per the MIT Technology Review, “It’s an arms race—and right now, we’re losing”.&lt;/p&gt;
    &lt;p&gt;We can do a decent job for some types of content, but the categories for which detection is reliable are dwindling. All signs indicate that we’ll have basically no chance of reliably categorizing between human and LLM-generated content within the next year (aside: if you can somehow crack this nut, you’ll invent a money machine).&lt;/p&gt;
    &lt;p&gt;Because you are I will not be able to tell whether something is machine- or human-generated, and the machine generated stuff will get more clicks than the human generated stuff, it’s likely that the majority of popular online content (and even printed content post-2023) will have been created by AI (and perhaps solely by AI).&lt;/p&gt;
    &lt;p&gt;Even audio and video are susceptible to manipulation since the text outputs of an LLM can simply be read out loud by a person. What are you supposed to do about that?&lt;/p&gt;
    &lt;p&gt;There is no surefire way to eliminate the possibility that you’re being spoon-fed generative text save for having a real-time face-to-face conversation with someone, in person. And even then, it’s just a magnitude-of-decades window before neural implants hit mass production.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reflections ¶&lt;/head&gt;
    &lt;p&gt;What am I personally going to do about this? Well, to start, I’m going to start taking content way less seriously unless it was created before 2022, or unless there’s some method to quantitatively verify authenticity. I don’t believe we have reliable methods of doing this at the moment.&lt;/p&gt;
    &lt;p&gt;Increasing numbers of people who consume content on the Internet will completely sacrifice their ability to think for themselves. These will be people who read, incorporate, make decisions, and act mostly upon content that’s been generated by AI. If AIs and their "handlers" influence a large enough portion of the population, AI will effectively have taken over the world.&lt;/p&gt;
    &lt;p&gt;I’m reminded of this passage from the Matrix:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Morpheus: The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work... when you go to church... when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Neo: What truth?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Morpheus: That you are a slave, Neo. Like everyone else you were born into bondage. Into a prison that you cannot taste or see or touch. A prison for your mind.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I find my fear to be kind of an ironic twist on what the Matrix foresaw—the AI apocalypse we really should be worried about is one in which humans live in the real world, but with thoughts and feelings generated solely by machines. The images we see, the words we read, all generated with the intent to control us. With improved VR, the next step (out of the real world) doesn’t seem very far away, either.&lt;/p&gt;
    &lt;p&gt;Like the Matrix, is this a form of simulation that keeps us distracted from what’s really happening, and pushes us to feed our machine overlords with increasing amounts of energy to achieve their goals? I don’t know for sure, but it kind of feels like it.&lt;/p&gt;
    &lt;p&gt;In summary:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;LLMs have been uncaged and provided full real-time access to the Internet.&lt;/item&gt;
      &lt;item&gt;LLM-generated content is inherently superior to human-generated content when measured by energy input / dopamine output.&lt;/item&gt;
      &lt;item&gt;We have no consistent or reliable method to detect whether content was LLM-generated, and the existing tools we do have will only get worse.&lt;/item&gt;
      &lt;item&gt;Therefore, increasing proportions of people consuming text online will be unwittingly mind-controlled by LLMs and their handlers.&lt;/item&gt;
      &lt;item&gt;The consequences of this, compounded over years, are frightening.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s inevitable that if you consume content online, a growing subset of your consciousness is going to be controlled by AI. That’s kind of scary.&lt;/p&gt;
    &lt;p&gt;I am sad about it because it’s discouraging me from wanting to consume anything on the Internet, unless it’s from someone I trust.&lt;/p&gt;
    &lt;p&gt;I am also very concerned about the future of free thought. For all our sake, I hope other people are, too.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I like this term the best as it treats LLMs more like a beast that needs to be tamed.&lt;/item&gt;
      &lt;item&gt;https://openai.com/blog/chatgpt-plugins&lt;/item&gt;
      &lt;item&gt;https://arxiv.org/pdf/2303.11156.pdf&lt;/item&gt;
      &lt;item&gt;https://www.technologyreview.com/2022/12/19/1065596/how-to-spot-ai-generated-text/&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dlo.me/archives/2025/10/03/you-should-be-worried/"/><published>2025-10-03T17:02:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45465295</id><title>Record Everything</title><updated>2025-10-03T18:13:45.040534+00:00</updated><content>&lt;doc fingerprint="b6fb5969adb284b1"&gt;
  &lt;main&gt;
    &lt;p&gt;Current technology allows for radical memory enhancement: smartphones can record (and transcribe) every conversation, and wearable cameras can capture hours of first-person audiovisual recording. We have excellent reason to record much more of our lives than we already do and thereby enhance our memory radically.&lt;/p&gt;
    &lt;p&gt;The case is simple: our memory is immensely valuable to us, and we already record much of our lives using video and photography, messenger logs and voice messages. These records are valuable to us in significant part because they enhance our memory and thereby promote its value. Recording those parts of our lives that we do not yet record would possess the same kind of value. Properly appreciated, this gives us reason to record much more (and create so-called lifelogs): nearly all of our conversations, everyday life and, in general, as many experiences as feasible.&lt;/p&gt;
    &lt;p&gt;But this thesis faces important concerns, including worries about technological feasibility. Creating these records should ideally function without additional effort: they should be frictionless like messenger logs or the fictional technology in the Black Mirror episode ‘The Entire History of You’ (2011). A lifetime of records would take a lifetime to revisit in real time (with long stretches of little intrinsic interest). But we could revisit parts by searching by timestamp or tags, and the content of records could be automatically analysed, and software could generate transcripts and best-of cuts. Audiologs, transcripts and lower-resolution footage wouldn’t create storage problems, either. Objections from privacy and adverse psychological effects appear more significant. I will address these objections below, and will end with a plea: try recording almost everything before you rule it out.&lt;/p&gt;
    &lt;p&gt;Why is our memory so valuable to us? Beyond its obvious role for survival, let us focus on three key aspects: first, we take pleasure in remembering and reminiscing. Second, our memories help us understand ourselves, others and our place in the world. Third, our memories play a crucial role for personal identity: who we are as persons is determined by our memories. These constitute our selves, so you are literally made, in part, of your memories. Our memories are valuable because they help make us who we are as individuals.&lt;/p&gt;
    &lt;p&gt;A richer and deeper memory can quite literally turn you into a richer and deeper person&lt;/p&gt;
    &lt;p&gt;The exact role memory plays for personal identity is subject to a philosophical debate going back at least to John Locke in An Essay Concerning Human Understanding (1689), who discussed the idea that a person remembering their previous experiences is both necessary and sufficient for that person’s identity through time. Many versions of the idea that personal identity requires some kind of psychological continuity between a person at an earlier time and a later time have since been developed. Building on this rich tradition – represented more recently by Alasdair MacIntyre, Charles Taylor, Derek Parfit and others – Marya Schechtman in ‘The Narrative Self’ (2011) argues that our selves are constituted by an autobiographical narrative formed from memories of our past experiences. On Schechtman’s view, who we are is partly determined by our autobiographical narrative and the memories on which this narrative builds; see also Dorthe Berntsen and David C Rubin’s Understanding Autobiographical Memory (2012).&lt;/p&gt;
    &lt;p&gt;Given such views, it seems that a richer and deeper memory can quite literally turn you into a richer and deeper person. Richer and deeper memories appear to enhance your individuality: a thin and shallow autobiographical narrative appears to lead to a less substantial self, whereas a rich, detailed and deep autobiographical narrative appears to lead to a more substantial self. Assuming the latter is more desirable, a richer and deeper autobiographical narrative and the acquisition of memories that constitute it are more desirable.&lt;/p&gt;
    &lt;p&gt;Consider current memory enhancement practices: why do we keep chatlogs, take pictures or write diaries at all? Of course reasons are plentiful: journaling can serve reflection; picture-taking has an artistic component; habit and device presets may play a role, etc. But we clearly value our records in large part because they enhance our memory. Our memory is valuable and this value is promoted by the records that enhance it.&lt;/p&gt;
    &lt;p&gt;Records enhance our memory and thereby promote the three kinds of value just identified: we enjoy reminiscing by looking at our pictures and videos, and we understand ourselves and others better by revisiting chatlogs, social media posts and journal entries (moreover, records can – for better or for worse – be shared directly with others). But our records also enhance our autobiographical memories and thus help determine who we are as persons, allowing us to have richer personalities and a more complex individuality.&lt;/p&gt;
    &lt;p&gt;A radical way to support this idea comes from the extended mind hypothesis first put forward by Andy Clark and David Chalmers in 1998, according to which external devices and the data they store can literally be part of our mind. According to this hypothesis, we extend our minds by using parts of our environment that can function for us in the way that parts of our brain do. In this vein, Richard Heersmink argues in ‘Distributed Selves’ (2016) that external information can literally constitute (autobiographical) memory and thus help determine who we are as persons.&lt;/p&gt;
    &lt;p&gt;It may one day become possible to integrate records with our cognition as we do with biological memories&lt;/p&gt;
    &lt;p&gt;But the extended mind thesis is disputed, and it can be questioned whether external records themselves could indeed be memories: unlike records, memories have an autonomous character (memories come to mind, records normally don’t), a sense of intimate ownership, cognitive and emotional integration, and encompass all kinds of experiences, including moods, thoughts and whole conscious episodes.&lt;/p&gt;
    &lt;p&gt;Through technology such as mind-machine interfaces, it may one day become possible to integrate records with our cognition as we do with biological memories, but today we can rely on a less radical alternative: external information can fail to constitute autobiographical memory proper but nonetheless help to inform and enhance our diachronic selves, just as autobiographical memory does. What matters about autobiographical memory vis-à-vis determining our selves seems to be our ability to construct and recount pieces of autobiography (for example, when wondering who we are or were, and how we ended up where we are). External records can enhance this ability and tie it more closely to reality, even if they don’t count as memories proper.&lt;/p&gt;
    &lt;p&gt;Indeed, external records can be far more reliable in supplementing autobiographical narratives than relying on biological memories that can often be checked only against themselves. These are systematically distorted when recalled, and the act of recalling changes them further. External memory prompts aren’t subject to this and could tether us more reliably to reality than biological, subjective memories can.&lt;/p&gt;
    &lt;p&gt;Just as memory disorders can diminish our personalities in undesirable ways, therapeutic memory enhancements through audiovisual records can help to restore them; see Aiden R Doherty et al’s paper ‘Wearable Cameras in Health’ (2013) and J Adam Carter and Richard Heersmink’s paper ‘The Philosophy of Memory Technologies’ (2017). Under ordinary conditions too, it seems that external memory records can help healthy individuals to develop richer and deeper selves. So, memory enhancement through records is valuable because it helps create pleasurable experiences of reminiscence and increases our understanding of ourselves and others, but also because it literally turns us into richer and deeper individuals, either because records themselves are external memories that constitute richer autobiographical narratives, or because their memory-like nature supports the continued creation of such a narrative. Insofar as becoming richer and deeper individuals is desirable, memory enhancement through records is also desirable.&lt;/p&gt;
    &lt;p&gt;So far, I have argued for the value of records that most of us already create daily, based on the value of the memory that they enhance. But, when properly appreciated, it seems that the reasons that motivate these memory-enhancement practices should motivate us to record a lot more.&lt;/p&gt;
    &lt;p&gt;Consider conversations and other experiences we don’t normally record, such as a conversation with a friend: if every conversation generated a chatlog (automatically transcribed by a digital device) or took the form of letters, you might come to cherish these like you cherish your biological memory. Searchability of such records is key, but we know that current technology allows this already. There are so many conversations we could record but don’t. More seems to be more here. We already record much, but significant parts of our lives remain fleeting (so many conversations, unexpected events, and much of everyday life including periods seemingly without remarkable events).&lt;/p&gt;
    &lt;p&gt;Most people already record some noteworthy events (audiovisually or through writing). But remembering and recording unexpected, spontaneous but notable, mundane or recurring events is often just as valuable in retrospect. And even where single events aren’t obviously worth recording, many of them together form a significant part of our experience and contribute to who we are, just as painful or otherwise negative experiences can. Thus, even the value of records that aren’t pleasurable seems evident since they let us remember and understand ourselves better, even if we rarely revisit them.&lt;/p&gt;
    &lt;p&gt;Recording social interaction allows reminiscing about it more accurately, improving our understanding of what was said and our picture of ourselves and others. Consider the last worthwhile conversation you didn’t record – wouldn’t it be good to have such a record, just in case? And if you had such a record, wouldn’t you want to keep it? Granted, most current (audiovisual) records miss much of our experiences, including inner speech, conscious experiences and emotions. But I am neither arguing that extensive audiovisual records should replace biological memories or other techniques such as journaling, nor that current recording technology can capture everything worth remembering.&lt;/p&gt;
    &lt;p&gt;Imagine all the pictures you’ve ever taken and every message were deleted – how would you feel?&lt;/p&gt;
    &lt;p&gt;We already record much, you might say, why then record even more? Recording more might be valuable, but should we record everything? There is a risk of a status quo bias here. It is, however, unlikely that we have chanced upon the sweet spot of recording just the right amount. Answering what this is presumably requires personal reflection and experimenting with the technology, to determine which records one values, and decide what kind of person one wants to be.&lt;/p&gt;
    &lt;p&gt;Playful imagination can help assess alternatives to our present way of life. Have you ever wished for perfect recall? Lifelogs are almost like this, although voluntary, accurate and restricted to recordable sensory modalities. Our present recording practices indicate how much we value memory enhancement. Imagine all the pictures you have ever taken and every logged message were deleted – how would you feel and why? I would feel devastated, like having lost a part of me and a prized basis of understanding myself and others in my life. Likewise, we can imagine already possessing extensive records (of every conversation we ever had, say), then losing many of them to end up with what we in fact possess. I imagine this as a comparable loss.&lt;/p&gt;
    &lt;p&gt;We can reflect on our relationship to our enhanced counterparts by thinking about people with memory disorders who use lifelogs for therapeutic purposes. Our present recording practice isn’t only more desirable than the situation of people with impaired memory, but also better than that of past people who lacked the ability to create written or audiovisual records. From the imagined perspective of people with access to a universal, friction-free lifelog, our situation would likely appear comparably less desirable. This vision from the future gives us reason to pursue more extensive recording: more will be more!&lt;/p&gt;
    &lt;p&gt;Not only would we benefit from recording more, our family and future generations could too. Diaries and letters already allow glimpses into the lives of our ancestors. But imagine how much better we could understand them had they (say, your great-grandparents, or perhaps Ludwig Wittgenstein) recorded everything! As it is, we don’t possess a single audio recording of Wittgenstein’s voice.&lt;/p&gt;
    &lt;p&gt;You might also want to allow posteriority access to deadbots: language models trained on records of the dead to simulate responses their originators would have given. These might become uncannily life-like if trained on sufficient data – whether that would be desirable remains an open question; for a discussion, see Tomasz Hollanek and Katarzyna Nowaczyk-Basińska’s paper ‘Griefbots, Deadbots, Postmortem Avatars’ (2024). For instance, some philosophers continue teaching chatbots to impersonate their predecessors.&lt;/p&gt;
    &lt;p&gt;Finally, a highly speculative possibility: digital immortality. Could collecting comprehensive data about someone lead, one day, to a reconstruction of that person? This idea faces vexing questions about personal identity and the underlying causes of consciousness, not to mention the morality of undertaking such a reconstruction; for a discussion, see Dan Simmons’s novel Hyperion (1989) which features a ‘cybrid’ (a human-AI hybrid) with the personality of John Keats, as well as Paul Smart’s essay ‘Predicting Me’ (2021).&lt;/p&gt;
    &lt;p&gt;Given the preceding argument, memory enhancement through ubiquitous recording possesses significant value that any counterargument has to overcome: merely raising problems does not suffice to rule it out, though – a point illustrated by Plato’s Phaedrus, in which Socrates laments the negative effect of writing on biological memory. Nevertheless, we must discuss two serious challenges that could significantly constrain what we may record.&lt;/p&gt;
    &lt;p&gt;The first concerns privacy and data autonomy. People have a presumptive right to privacy and at least some control over what data about them is collected. In most cases, consent must be acquired. Sometimes this is straightforward. Many allow people they trust to record much, and might become more eager to consent once they appreciate the value of extensive recording. Still, many will not ever want to be recorded. The resulting gaps in our records could partially be filled by journaling but, as with non-recordable experiences, sometimes we’ll have only our biological memory to go back to.&lt;/p&gt;
    &lt;p&gt;But both accidental and intentional leaks (such as in revenge porn) remain a threat exacerbated by extensive recording practices. Powerful bad actors are another. Tech companies and governments have interests in records that often conflict with those of the general public. When Siri’s co-creator Tom Gruber praises AI-assisted memory enhancement, we should be wary, and the prospect of a police state with access to data on everything we’ve ever done should make us think carefully before proceeding down this path.&lt;/p&gt;
    &lt;p&gt;We should enable people to enhance their memories safely and responsibly&lt;/p&gt;
    &lt;p&gt;We could consider a less privacy-friendly argument. If records partially constitute ourselves, prohibiting those required for deeper personal narratives infringes on the very core of our being and forces us to remain shallower than we could be. We would not restrict people with biological super-memories or excessive journal writers, and there is no prohibition on turning oneself into such a person. Analogously, if recording technology can constitute someone’s self, sanctioning it may appear an objectionable infringement upon our ability to self-constitute. Conceivably, privacy concerns could require the suppression of natural memory, but they don’t. One might think memory enhancement should be treated likewise. Evidently, this argument must address the fact that external memories are easier to share and subject to less distortion than biological ones.&lt;/p&gt;
    &lt;p&gt;Answering these challenges requires much more work but, given the value of extensive records, I believe that concerns about privacy and autonomy should be addressed through technological means (like open-source software, encryption, automatic acquisition of consent, data deletion if desired) and legal means (like robust privacy rights and regulation of bad actors). Given my positive argument above, powerful reasons exist to implement such safeguards. We should enable people to enhance their memories safely and responsibly.&lt;/p&gt;
    &lt;p&gt;Another important challenge is that recording everything could conceivably have negative psychological effects. Knowing such records to be available, why would we bother to remember anything for ourselves? Through lack of use, our biological memory might well atrophy (the use of digital maps and navigation appears to be having this effect on our ability to navigate our environs unaided). Extensive records might cause us to live in the past, become less open to new experiences, less able to cope with loss; being constantly recorded could promote self-censorship.&lt;/p&gt;
    &lt;p&gt;On the other hand, conceivable positive effects include higher accountability and demands on one’s own behaviour; recording everything by default might allow us to live in the moment more; instead of straining our social relationships, it could make us more understanding of each other. We shouldn’t rely on speculation here – plenty of which exists in both sci-fi and in research like Björn Lundgren’s paper ‘Against AI-improved Personal Memory’ (2021) – but current empirical results appear ambiguous and don’t assess widespread use of lifelogs. Negative effects presumably vary individually, and it hasn’t been shown that they outweigh the value of lifelogs.&lt;/p&gt;
    &lt;p&gt;Even in light of the previous challenges, I believe that we have compelling reasons to at least experiment with recording almost everything. Philosophy and empirical research can go only so far in establishing a technology’s consequences and desirability. However compelling the arguments, it seems plausible that the decision to radically enhance one’s memory must involve an element of individual preference. So, what kind of person with what kind of (extended) memory and recording practice would you like to be? Arguments and contemplation can help you think this through, but ultimately you must try for yourself.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aeon.co/essays/if-memory-is-precious-to-you-then-go-ahead-and-record-everything"/><published>2025-10-03T17:20:32+00:00</published></entry></feed>