<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-18T22:10:09.516288+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45963780</id><title>Cloudflare Global Network experiencing issues</title><updated>2025-11-18T22:10:18.376278+00:00</updated><content>&lt;doc fingerprint="40d4ee565e054cd0"&gt;
  &lt;main&gt;
    &lt;p&gt;Cloudflare services are currently operating normally. We are no longer observing elevated errors or latency across the network.&lt;/p&gt;
    &lt;p&gt;Our engineering teams continue to closely monitor the platform and perform a deeper investigation into the earlier disruption, but no configuration changes are being made at this time.&lt;/p&gt;
    &lt;p&gt;At this point, it is considered safe to re-enable any Cloudflare services that were temporarily disabled during the incident. We will provide a final update once our investigation is complete.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 17:44 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We continue to monitor the system through recovery and we are seeing errors and latency return to normal levels. A full post-incident investigation and details about the incident will be made available asap.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 17:14 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We continue to see errors drop as we work through services globally and clearing remaining errors and latency.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 16:46 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We continue to see errors and latency improve but still have reports of intermittent errors. The team continues to monitor the situation as it improves, and looking for ways to accelerate full recovery.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 16:27 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Bot scores will be impacted intermittently while we undergo global recovery. We will update once we believe bot scores are fully recovered.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 16:04 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;The team is continuing to focus on restoring service post-fix. We are mitigating several issues that remain post-deployment.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 15:40 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to monitor for any further issues.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 15:23 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Some customers may be still experiencing issues logging into or using the Cloudflare dashboard. We are working on a fix to resolve this, and continuing to monitor for any further issues.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 14:57 UTC&lt;/p&gt;
    &lt;p&gt;Monitoring&lt;/p&gt;
    &lt;p&gt;A fix has been implemented and we believe the incident is now resolved. We are continuing to monitor for errors to ensure all services are back to normal.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 14:42 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We've deployed a change which has restored dashboard services. We are still working to remediate broad application services impact&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 14:34 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to work on a fix for this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 14:22 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing working on restoring service for application services customers.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:58 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing working on restoring service for application services customers.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:35 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We have made changes that have allowed Cloudflare Access and WARP to recover. Error levels for Access and WARP users have returned to pre-incident rates. We have re-enabled WARP access in London.&lt;/p&gt;
    &lt;p&gt;We are continuing to work towards restoring other services.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:13 UTC&lt;/p&gt;
    &lt;p&gt;Identified&lt;/p&gt;
    &lt;p&gt;The issue has been identified and a fix is being implemented.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:09 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;During our attempts to remediate, we have disabled WARP access in London. Users in London trying to access the Internet via WARP will see a failure to connect.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:04 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to investigate this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 12:53 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to investigate this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 12:37 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are seeing services recover, but customers may continue to observe higher-than-normal error rates as we continue remediation efforts.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 12:21 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to investigate this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 12:03 UTC&lt;/p&gt;
    &lt;p&gt;Investigating&lt;/p&gt;
    &lt;p&gt;Cloudflare is experiencing an internal service degradation. Some services may be intermittently impacted. We are focused on restoring service. We will update as we are able to remediate. More updates to follow shortly.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 11:48 UTC&lt;/p&gt;
    &lt;p&gt;This incident affected: Cloudflare Sites and Services (Access, Bot Management, CDN/Cache, Dashboard, Firewall, Network, WARP, Workers).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cloudflarestatus.com/incidents/8gmgl950y3h7"/><published>2025-11-18T11:35:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45964816</id><title>Mathematics and Computation (2019) [pdf]</title><updated>2025-11-18T22:10:16.857108+00:00</updated><content/><link href="https://www.math.ias.edu/files/Book-online-Aug0619.pdf"/><published>2025-11-18T12:35:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45966018</id><title>Experiment: Making TypeScript immutable-by-default</title><updated>2025-11-18T22:10:16.473796+00:00</updated><content>&lt;doc fingerprint="eadcf3715b288ba1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Experiment: making TypeScript immutable-by-default&lt;/head&gt;
    &lt;p&gt;I like programming languages where variables are immutable by default. For example, in Rust, &lt;code&gt;let&lt;/code&gt; declares an immutable variable and &lt;code&gt;let mut&lt;/code&gt; declares a mutable one. I’ve long wanted this in other languages, like TypeScript, which is mutable by default—the opposite of what I want!&lt;/p&gt;
    &lt;p&gt;I wondered: is it possible to make TypeScript values immutable by default?&lt;/p&gt;
    &lt;p&gt;My goal was to do this purely with TypeScript, without changing TypeScript itself. That meant no lint rules or other tools. I chose this because I wanted this solution to be as “pure” as possible…and it also sounded more fun.&lt;/p&gt;
    &lt;p&gt;I spent an evening trying to do this. I failed but made progress! I made arrays and &lt;code&gt;Record&lt;/code&gt;s immutable by default, but I couldn’t get it working for regular objects. If you figure out how to do this completely, please contact me—I must know!&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 1: obliterate the built-in libraries&lt;/head&gt;
    &lt;p&gt;TypeScript has built-in type definitions for JavaScript APIs like &lt;code&gt;Array&lt;/code&gt; and &lt;code&gt;Date&lt;/code&gt; and &lt;code&gt;String&lt;/code&gt;. If you’ve ever changed the &lt;code&gt;target&lt;/code&gt; or &lt;code&gt;lib&lt;/code&gt; options in your TSConfig, you’ve tweaked which of these definitions are included. For example, you might add the “ES2024” library if you’re targeting a newer runtime.&lt;/p&gt;
    &lt;p&gt;My goal was to swap the built-in libraries with an immutable-by-default replacement.&lt;/p&gt;
    &lt;p&gt;The first step was to stop using any of the built-in libraries. I set the &lt;code&gt;noLib&lt;/code&gt; flag in my TSConfig, like this:&lt;/p&gt;
    &lt;code&gt;{
  "compilerOptions": {
    "noLib": true
  }
}
&lt;/code&gt;
    &lt;p&gt;Then I wrote a very simple script and put it in &lt;code&gt;test.ts&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;console.log("Hello world!");
&lt;/code&gt;
    &lt;p&gt;When I ran &lt;code&gt;tsc&lt;/code&gt;, it gave a bunch of errors:&lt;/p&gt;
    &lt;code&gt;Cannot find global type 'Array'.
Cannot find global type 'Boolean'.
Cannot find global type 'Function'.
Cannot find global type 'IArguments'.
Cannot find global type 'Number'.
Cannot find global type 'Object'.
Cannot find global type 'RegExp'.
Cannot find global type 'String'.
&lt;/code&gt;
    &lt;p&gt;Progress! I had successfully obliterated any default TypeScript libraries, which I could tell because it couldn’t find core types like &lt;code&gt;String&lt;/code&gt; or &lt;code&gt;Boolean&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Time to write the replacement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 2: a skeleton standard library&lt;/head&gt;
    &lt;p&gt;This project was a prototype. Therefore, I started with a minimal solution that would type-check. I didn’t need it to be good!&lt;/p&gt;
    &lt;p&gt;I created &lt;code&gt;lib.d.ts&lt;/code&gt; and put the following inside:&lt;/p&gt;
    &lt;code&gt;// In lib.d.ts:
declare var console: any;

interface Boolean {}
interface Function {}
interface IArguments {}
interface Number {}
interface RegExp {}
interface String {}
interface Object {}

// TODO: We'll update this soon.
interface Array&amp;lt;T&amp;gt; {}
&lt;/code&gt;
    &lt;p&gt;Now, when I ran &lt;code&gt;tsc&lt;/code&gt;, I got no errors! I’d defined all the built-in types that TypeScript needs, and a dummy &lt;code&gt;console&lt;/code&gt; object.&lt;/p&gt;
    &lt;p&gt;As you can see, this solution is impractical for production. For one, none of these interfaces have any properties! &lt;code&gt;"foo".toUpperCase()&lt;/code&gt; isn’t defined, for example. That’s okay because this is only a prototype. A production-ready version would need to define all of those things—tedious, but should be straightforward.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 3: making arrays immutable&lt;/head&gt;
    &lt;p&gt;I decided to tackle this with a test-driven development style. I’d write some code that I want to type-check, watch it fail to type-check, then fix it.&lt;/p&gt;
    &lt;p&gt;I updated &lt;code&gt;test.ts&lt;/code&gt; to contain the following:&lt;/p&gt;
    &lt;code&gt;// In test.ts:
const arr = [1, 2, 3];

// Non-mutation should be allowed.
console.log(arr[1]);
console.log(arr.map((n) =&amp;gt; n + 1));

// @ts-expect-error Mutation should not be allowed.
arr[0] = 9;
// @ts-expect-error Mutation should not be allowed.
arr.push(4);
&lt;/code&gt;
    &lt;p&gt;This tests three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Creating arrays with array literals is possible.&lt;/item&gt;
      &lt;item&gt;Non-mutating operations, like &lt;code&gt;arr[1]&lt;/code&gt;and&lt;code&gt;arr.map()&lt;/code&gt;, are allowed.&lt;/item&gt;
      &lt;item&gt;Operations that mutate the array, like &lt;code&gt;arr[1] = 9&lt;/code&gt;, are disallowed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When I ran &lt;code&gt;tsc&lt;/code&gt;, I saw two errors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;arr[0] = 9&lt;/code&gt;is allowed. There’s an unused&lt;code&gt;@ts-expect-error&lt;/code&gt;there.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;arr.map&lt;/code&gt;doesn’t exist.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So I updated the &lt;code&gt;Array&lt;/code&gt; type in &lt;code&gt;lib.d.ts&lt;/code&gt; with the following:&lt;/p&gt;
    &lt;code&gt;// In lib.d.ts:
interface Array&amp;lt;T&amp;gt; {
  readonly [n: number]: T;

  map&amp;lt;U&amp;gt;(
    callbackfn: (value: T, index: number, array: readonly T[]) =&amp;gt; U,
    thisArg?: any
  ): U[];
}
&lt;/code&gt;
    &lt;p&gt;The property accessor—the &lt;code&gt;readonly [n: number]: T&lt;/code&gt; line—tells TypeScript that you can access array properties by numeric index, but they’re read-only. That should make &lt;code&gt;arr[1]&lt;/code&gt; possible but &lt;code&gt;arr[1] = 9&lt;/code&gt; impossible.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;map&lt;/code&gt; method definition is copied from the TypeScript source code with no changes (other than some auto-formatting). That should make it possible to call &lt;code&gt;arr.map()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Notice that I did not define &lt;code&gt;push&lt;/code&gt;. We shouldn’t be calling that on an immutable array!&lt;/p&gt;
    &lt;p&gt;I ran &lt;code&gt;tsc&lt;/code&gt; again and…success! No errors! We now have immutable arrays!&lt;/p&gt;
    &lt;p&gt;At this stage, I’ve shown that it’s possible to configure TypeScript to make all arrays immutable with no extra annotations. No need for &lt;code&gt;readonly string[]&lt;/code&gt; or &lt;code&gt;ReadonlyArray&amp;lt;number&amp;gt;&lt;/code&gt;! In other words, we have some immutability by default.&lt;/p&gt;
    &lt;p&gt;This code, like everything in this post, is simplistic. There are lots of other array methods, like &lt;code&gt;filter()&lt;/code&gt; and &lt;code&gt;join()&lt;/code&gt; and &lt;code&gt;forEach()&lt;/code&gt;! If this were made production-ready, I’d make sure to define all the read-only array methods.&lt;/p&gt;
    &lt;p&gt;But for now, I was ready to move on to mutable arrays.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 4: mutable arrays&lt;/head&gt;
    &lt;p&gt;I prefer immutability, but I want to be able to define a mutable array sometimes. So I made another test case:&lt;/p&gt;
    &lt;code&gt;// In test.ts:
const arr = [1, 2, 3] as MutableArray&amp;lt;number&amp;gt;;
arr[0] = 9;
arr.push(4);
&lt;/code&gt;
    &lt;p&gt;Notice that this requires a little extra work to make the array mutable. In other words, it’s not the default.&lt;/p&gt;
    &lt;p&gt;TypeScript complained that it can’t find &lt;code&gt;MutableArray&lt;/code&gt;, so I defined it:&lt;/p&gt;
    &lt;code&gt;// In lib.d.ts:
interface MutableArray&amp;lt;T&amp;gt; extends Array&amp;lt;T&amp;gt; {
  [n: number]: T;
  push(...items: T[]): number;
}
&lt;/code&gt;
    &lt;p&gt;And again, type-checks passed!&lt;/p&gt;
    &lt;p&gt;Now, I had mutable and immutable arrays, with immutability as the default. Again, this is simplistic, but good enough for this proof-of-concept!&lt;/p&gt;
    &lt;p&gt;This was exciting to me. It was possible to configure TypeScript to be immutable by default, for arrays at least. I didn’t have to fork the language or use any other tools.&lt;/p&gt;
    &lt;p&gt;Could I make more things immutable?&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 5: the same for &lt;code&gt;Record&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;I wanted to see if I could go beyond arrays. My next target was the &lt;code&gt;Record&lt;/code&gt; type, which is a TypeScript utility type. So I defined another pair of test cases similar to the ones I made for arrays:&lt;/p&gt;
    &lt;code&gt;// In test.ts:

// Immutable records
const obj1: Record&amp;lt;string, string&amp;gt; = { foo: "bar" };
console.log(obj1.foo);
// @ts-expect-error Mutation should not be allowed.
obj1.foo = "baz";

// Mutable records
const obj2: MutableRecord&amp;lt;string, string&amp;gt; = { foo: "bar" };
obj2.foo = "baz";
&lt;/code&gt;
    &lt;p&gt;TypeScript complained that it couldn’t find &lt;code&gt;Record&lt;/code&gt; or &lt;code&gt;MutableRecord&lt;/code&gt;. It also complained about an unused &lt;code&gt;@ts-expect-error&lt;/code&gt;, which meant that mutation was allowed.&lt;/p&gt;
    &lt;p&gt;I rolled up my sleeves and fixed those errors like this:&lt;/p&gt;
    &lt;code&gt;// In lib.d.ts:
declare type PropertyKey = string | number | symbol;
type Record&amp;lt;KeyT extends PropertyKey, ValueT&amp;gt; = {
  readonly [key in KeyT]: ValueT;
};
type MutableRecord&amp;lt;KeyT extends PropertyKey, ValueT&amp;gt; = {
  [key in KeyT]: ValueT;
};
&lt;/code&gt;
    &lt;p&gt;Now, we have &lt;code&gt;Record&lt;/code&gt;, which is an immutable key-value pair, and the mutable version too. Just like arrays!&lt;/p&gt;
    &lt;p&gt;You can imagine extending this idea to other built-in types, like &lt;code&gt;Set&lt;/code&gt; and &lt;code&gt;Map&lt;/code&gt;. I think it’d be pretty easy to do this the same way I did arrays and records. I’ll leave that as an exercise to the reader.&lt;/p&gt;
    &lt;head rend="h2"&gt;Failed step 6: plain objects&lt;/head&gt;
    &lt;p&gt;My final test was to make regular objects (not records or arrays) immutable. Unfortunately for me, I could not figure this out.&lt;/p&gt;
    &lt;p&gt;Here’s the test case I wrote:&lt;/p&gt;
    &lt;code&gt;// In test.ts:
const obj = { foo: "bar" };
console.log(obj.foo);
// @ts-expect-error Mutation should not be allowed.
obj.foo = "baz";
&lt;/code&gt;
    &lt;p&gt;This stumped me. No matter what I did, I could not write a type that would disallow this mutation. I tried modifying the &lt;code&gt;Object&lt;/code&gt; type every way I could think of, but came up short!&lt;/p&gt;
    &lt;p&gt;There are ways to annotate &lt;code&gt;obj&lt;/code&gt; to make it immutable, but that’s not in the spirit of my goal. I want it to be immutable by default!&lt;/p&gt;
    &lt;p&gt;Alas, this is where I gave up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Can you figure this out?&lt;/head&gt;
    &lt;p&gt;I wanted to make TypeScript immutable by default. I was able to do this with arrays, &lt;code&gt;Record&lt;/code&gt;s, and other types like &lt;code&gt;Map&lt;/code&gt; and &lt;code&gt;Set&lt;/code&gt;. Unfortunately, I couldn’t make it work for plain object definitions like &lt;code&gt;obj = { foo: "bar" }&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;There’s probably a way to enforce this with lint rules, either by disallowing mutation operations or by requiring &lt;code&gt;Readonly&lt;/code&gt; annotations everywhere. I’d like to see what that looks like.&lt;/p&gt;
    &lt;p&gt;If you figure out how to make TypeScript immutable by default with no other tools, I would love to know, and I’ll update my post. I hope my failed attempt will lead someone else to something successful.&lt;/p&gt;
    &lt;p&gt;Again, please contact me if you figure this out, or have any other thoughts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://evanhahn.com/typescript-immutability-experiment/"/><published>2025-11-18T13:56:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45966251</id><title>Nearly all UK drivers say headlights are too bright</title><updated>2025-11-18T22:10:15.883502+00:00</updated><content>&lt;doc fingerprint="24df8d635d03762d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Nearly all drivers say headlights are too bright&lt;/head&gt;
    &lt;p&gt;Nearly all UK drivers said they thought headlights were too bright and that they have been dazzled by oncoming vehicles, according to a major study.&lt;/p&gt;
    &lt;p&gt;The government said last week that it will take a closer look at the design of cars and headlamps after concerns about lights dazzling drivers.&lt;/p&gt;
    &lt;p&gt;A study commissioned by the Department for Transport (DfT) found 97% of people surveyed found they were regularly or sometimes distracted by oncoming vehicles and 96% thought most or some headlights were too bright.&lt;/p&gt;
    &lt;p&gt;Dr Shaun Helman, who led the research for Berkshire-based Transport Research Laboratory (TRL), said it provides "compelling evidence" that lights' glare is a "genuine issue for UK drivers".&lt;/p&gt;
    &lt;p&gt;New measures will be included in the government's upcoming Road Safety Strategy, reflecting what is becoming an increasingly fraught issue for road users.&lt;/p&gt;
    &lt;p&gt;TRL's data suggests that LED and whiter headlamps may be linked to glare and that drivers might find their whiteness harder to cope with.&lt;/p&gt;
    &lt;p&gt;Of those surveyed, 33% said they had either stopped driving or are driving less at night because of lights, while another 22% said they would like to drive less at night but have no choice.&lt;/p&gt;
    &lt;p&gt;A total of 1,850 drivers, matched to the age and gender split of the country's licence holding population, were surveyed for their views.&lt;/p&gt;
    &lt;p&gt;TRL said LED lights used in vehicles are brighter, more concentrated and emit more blue light, which human eyes struggle with more at night.&lt;/p&gt;
    &lt;p&gt;The RAC's senior policy officer Rod Dennis said: "Having campaigned hard for this study, we welcome its findings which independently confirm what drivers have been telling us – that rather than being an imagined phenomenon, some bright headlights do cause a glare problem.&lt;/p&gt;
    &lt;p&gt;"While drivers clearly benefit from high-performing headlights, it's important this doesn't lead to others suffering the effects of dazzle, so a balance needs to be struck," he added.&lt;/p&gt;
    &lt;p&gt;Mr Dennis said that it is "vital" TRL's report is "reviewed carefully to put us on a path towards changes that ultimately benefit all road users."&lt;/p&gt;
    &lt;p&gt;Denise Voon, a clinical advisor at The College of Optometrists, said the DfT should "take immediate, actionable steps to support drivers and commission more detailed research, specifically into how headlight regulations need to change".&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/c1j8ewy1p86o"/><published>2025-11-18T14:11:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45966435</id><title>Short Little Difficult Books</title><updated>2025-11-18T22:10:15.652976+00:00</updated><content/><link href="https://countercraft.substack.com/p/short-little-difficult-books"/><published>2025-11-18T14:23:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45967211</id><title>Gemini 3</title><updated>2025-11-18T22:10:15.333577+00:00</updated><content>&lt;doc fingerprint="f9d7a1cf9b3f9a95"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A new era of intelligence with Gemini 3&lt;/head&gt;
    &lt;head rend="h3"&gt;A note from Google and Alphabet CEO Sundar Pichai:&lt;/head&gt;
    &lt;p&gt;Nearly two years ago we kicked off the Gemini era, one of our biggest scientific and product endeavors ever undertaken as a company. Since then, it’s been incredible to see how much people love it. AI Overviews now have 2 billion users every month. The Gemini app surpasses 650 million users per month, more than 70% of our Cloud customers use our AI, 13 million developers have built with our generative models, and that is just a snippet of the impact we’re seeing.&lt;/p&gt;
    &lt;p&gt;And we’re able to get advanced capabilities to the world faster than ever, thanks to our differentiated full stack approach to AI innovation — from our leading infrastructure to our world-class research and models and tooling, to products that reach billions of people around the world.&lt;/p&gt;
    &lt;p&gt;Every generation of Gemini has built on the last, enabling you to do more. Gemini 1’s breakthroughs in native multimodality and long context window expanded the kinds of information that could be processed — and how much of it. Gemini 2 laid the foundation for agentic capabilities and pushed the frontiers on reasoning and thinking, helping with more complex tasks and ideas, leading to Gemini 2.5 Pro topping LMArena for over six months.&lt;/p&gt;
    &lt;p&gt;And now we’re introducing Gemini 3, our most intelligent model, that combines all of Gemini’s capabilities together so you can bring any idea to life.&lt;/p&gt;
    &lt;p&gt;It’s state-of-the-art in reasoning, built to grasp depth and nuance — whether it’s perceiving the subtle clues in a creative idea, or peeling apart the overlapping layers of a difficult problem. Gemini 3 is also much better at figuring out the context and intent behind your request, so you get what you need with less prompting. It’s amazing to think that in just two years, AI has evolved from simply reading text and images to reading the room.&lt;/p&gt;
    &lt;p&gt;And starting today, we’re shipping Gemini at the scale of Google. That includes Gemini 3 in AI Mode in Search with more complex reasoning and new dynamic experiences. This is the first time we are shipping Gemini in Search on day one. Gemini 3 is also coming today to the Gemini app, to developers in AI Studio and Vertex AI, and in our new agentic development platform, Google Antigravity — more below.&lt;/p&gt;
    &lt;p&gt;Like the generations before it, Gemini 3 is once again advancing the state of the art. In this new chapter, we’ll continue to push the frontiers of intelligence, agents, and personalization to make AI truly helpful for everyone.&lt;/p&gt;
    &lt;p&gt;We hope you like Gemini 3, we'll keep improving it, and look forward to seeing what you build with it. Much more to come!&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing Gemini 3: our most intelligent model that helps you bring any idea to life&lt;/head&gt;
    &lt;p&gt;Demis Hassabis, CEO of Google DeepMind and Koray Kavukcuoglu, CTO of Google DeepMind and Chief AI Architect, Google, on behalf of the Gemini team&lt;/p&gt;
    &lt;p&gt;Today we’re taking another big step on the path toward AGI and releasing Gemini 3.&lt;/p&gt;
    &lt;p&gt;It’s the best model in the world for multimodal understanding and our most powerful agentic and vibe coding model yet, delivering richer visualizations and deeper interactivity — all built on a foundation of state-of-the-art reasoning.&lt;/p&gt;
    &lt;p&gt;We’re beginning the Gemini 3 era by releasing Gemini 3 Pro in preview and making it available today across a suite of Google products so you can use it in your daily life to learn, build and plan anything. We’re also introducing Gemini 3 Deep Think — our enhanced reasoning mode that pushes Gemini 3 performance even further — and giving access to safety testers before making it available to Google AI Ultra subscribers.&lt;/p&gt;
    &lt;head rend="h2"&gt;State-of-the-art reasoning with unprecedented depth and nuance&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro can bring any idea to life with its state-of-the-art reasoning and multimodal capabilities. It significantly outperforms 2.5 Pro on every major AI benchmark.&lt;/p&gt;
    &lt;p&gt;It tops the LMArena Leaderboard with a breakthrough score of 1501 Elo. It demonstrates PhD-level reasoning with top scores on Humanity’s Last Exam (37.5% without the usage of any tools) and GPQA Diamond (91.9%). It also sets a new standard for frontier models in mathematics, achieving a new state-of-the-art of 23.4% on MathArena Apex.&lt;/p&gt;
    &lt;p&gt;Beyond text, Gemini 3 Pro redefines multimodal reasoning with 81% on MMMU-Pro and 87.6% on Video-MMMU. It also scores a state-of-the-art 72.1% on SimpleQA Verified, showing great progress on factual accuracy. This means Gemini 3 Pro is highly capable at solving complex problems across a vast array of topics like science and mathematics with a high degree of reliability.&lt;/p&gt;
    &lt;p&gt;Gemini 3 is state-of-the-art across a range of key AI benchmarks. See details on our evaluation methodology.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Pro also brings a new level of depth and nuance to every interaction. Its responses are smart, concise and direct, trading cliché and flattery for genuine insight — telling you what you need to hear, not just what you want to hear. It acts as a true thought partner that gives you new ways to understand information and express yourself, from translating dense scientific concepts by generating code for high-fidelity visualizations to creative brainstorming.&lt;/p&gt;
    &lt;p&gt;Gemini 3 can code a visualization of plasma flow in a tokamak and write a poem capturing the physics of fusion.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gemini 3 Deep Think&lt;/head&gt;
    &lt;p&gt;Gemini 3 Deep Think mode pushes the boundaries of intelligence even further, delivering a step-change in Gemini 3’s reasoning and multimodal understanding capabilities to help you solve even more complex problems.&lt;/p&gt;
    &lt;p&gt;In testing, Gemini 3 Deep Think outperforms Gemini 3 Pro’s already impressive performance on Humanity’s Last Exam (41.0% without the use of tools) and GPQA Diamond (93.8%). It also achieves an unprecedented 45.1% on ARC-AGI-2 (with code execution, ARC Prize Verified), demonstrating its ability to solve novel challenges.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Deep Think mode excels on some of the most challenging AI benchmarks. See details on our evaluation methodology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gemini 3 helps you learn, build and plan anything&lt;/head&gt;
    &lt;head rend="h3"&gt;Learn anything&lt;/head&gt;
    &lt;p&gt;Gemini was built from the start to seamlessly synthesize information about any topic across multiple modalities, including text, images, video, audio and code. Gemini 3 pushes the frontier of multimodal reasoning to help you learn in ways that make sense for you by combining its state-of-the-art reasoning, vision and spatial understanding, leading multilingual performance, and 1 million-token context window.&lt;/p&gt;
    &lt;p&gt;For example, if you want to learn how to cook in your family tradition, Gemini 3 can decipher and translate handwritten recipes in different languages into a shareable family cookbook. Or if you want to learn about a new topic, you can give it academic papers, long video lectures or tutorials and it can generate code for interactive flashcards, visualizations or other formats that will help you master the material. It can even analyze videos of your pickleball match, identify areas where you can improve and generate a training plan for overall form improvements.&lt;/p&gt;
    &lt;p&gt;Gemini 3 can help you learn and preserve family cooking traditions. Try it in Gemini Canvas.&lt;/p&gt;
    &lt;p&gt;Gemini 3 can help you analyze complex information like research papers and can generate code for an interactive guide.&lt;/p&gt;
    &lt;p&gt;Get expert-level sports analysis on your pickleball match to help improve your game.&lt;/p&gt;
    &lt;p&gt;To help you make better sense of information on the web, AI Mode in Search now uses Gemini 3 to enable new generative UI experiences like immersive visual layouts and interactive tools and simulations, all generated completely on the fly based on your query.&lt;/p&gt;
    &lt;p&gt;Learn a complex topic like how RNA polymerase works with generative UI in AI Mode in Search.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build anything&lt;/head&gt;
    &lt;p&gt;Building on the success of 2.5 Pro, Gemini 3 delivers on the promise of bringing any idea to life for developers. It’s exceptional at zero-shot generation and handles complex prompts and instructions to render richer, more interactive web UI.&lt;/p&gt;
    &lt;p&gt;Gemini 3 is the best vibe coding and agentic coding model we’ve ever built – making our products more autonomous and boosting developer productivity. It tops the WebDev Arena leaderboard by scoring an impressive 1487 Elo. It also scores 54.2% on Terminal-Bench 2.0, which tests a model’s tool use ability to operate a computer via terminal and it greatly outperforms 2.5 Pro on SWE-bench Verified (76.2%), a benchmark that measures coding agents.&lt;/p&gt;
    &lt;p&gt;You can now build with Gemini 3 in Google AI Studio, Vertex AI, Gemini CLI and our new agentic development platform, Google Antigravity. It’s also available in third-party platforms like Cursor, GitHub, JetBrains, Manus, Replit and more.&lt;/p&gt;
    &lt;p&gt;Code a retro 3D spaceship game with richer visualizations and improved interactivity. Try it in AI Studio.&lt;/p&gt;
    &lt;p&gt;Bring your imagination to life by building, deconstructing and remixing detailed 3D voxel art using code. Try it in AI Studio.&lt;/p&gt;
    &lt;p&gt;Build a playable sci-fi world with shaders using Gemini 3. Try it in AI Studio.&lt;/p&gt;
    &lt;p&gt;You can vibe code richer, more interactive web UI and apps with Gemini 3.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introducing a new agent-first development experience&lt;/head&gt;
    &lt;p&gt;As model intelligence accelerates with Gemini 3, we have the opportunity to reimagine the entire developer experience. Today we’re releasing Google Antigravity, our new agentic development platform that enables developers to operate at a higher, task-oriented level.&lt;/p&gt;
    &lt;p&gt;Using Gemini 3’s advanced reasoning, tool use and agentic coding capabilities, Google Antigravity transforms AI assistance from a tool in a developer’s toolkit into an active partner. While the core of Google Antigravity is a familiar AI IDE experience, its agents have been elevated to a dedicated surface and given direct access to the editor, terminal and browser. Now, agents can autonomously plan and execute complex, end-to-end software tasks simultaneously on your behalf while validating their own code.&lt;/p&gt;
    &lt;p&gt;In addition to Gemini 3 Pro, Google Antigravity also comes tightly coupled with our latest Gemini 2.5 Computer Use model for browser control and our top-rated image editing model Nano Banana (Gemini 2.5 Image).&lt;/p&gt;
    &lt;p&gt;Google Antigravity uses Gemini 3 to drive an end-to-end agentic workflow for a flight tracker app. The agent independently plans, codes the application and validates its execution through browser-based computer use.&lt;/p&gt;
    &lt;head rend="h3"&gt;Plan anything&lt;/head&gt;
    &lt;p&gt;Since introducing the agentic era with Gemini 2, we’ve made a lot of progress, not only advancing Gemini’s coding agent abilities, but also improving its ability to reliably plan ahead over longer horizons. Gemini 3 demonstrates this by topping the leaderboard on Vending-Bench 2, which tests longer horizon planning by managing a simulated vending machine business. Gemini 3 Pro maintains consistent tool usage and decision-making for a full simulated year of operation, driving higher returns without drifting off task.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Pro demonstrates better long-horizon planning to generate significantly higher returns compared to other frontier models.&lt;/p&gt;
    &lt;p&gt;This means Gemini 3 can better help you get things done in everyday life. By combining deeper reasoning with improved, more consistent tool use, Gemini 3 can take action on your behalf by navigating more complex, multi-step workflows from start to finish — like booking local services or organizing your inbox — all while under your control and guidance.&lt;/p&gt;
    &lt;p&gt;Google AI Ultra subscribers can try these agentic capabilities in the Gemini app with Gemini Agent today. We’ve learned a lot improving Gemini’s agentic capabilities, and we’re excited to see how you use it as we expand to more Google products soon.&lt;/p&gt;
    &lt;p&gt;Gemini Agent can help you organize your Gmail inbox. Try it now in the Gemini app for Google AI Ultra subscribers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building Gemini 3 responsibly&lt;/head&gt;
    &lt;p&gt;Gemini 3 is our most secure model yet, and has undergone the most comprehensive set of safety evaluations of any Google AI model to date. The model shows reduced sycophancy, increased resistance to prompt injections and improved protection against misuse via cyberattacks.&lt;/p&gt;
    &lt;p&gt;In addition to our in-house testing for the critical domains in our Frontier Safety Framework, we've also partnered on evaluations with world-leading subject matter experts, provided early access to bodies like the UK AISI, and obtained independent assessments from industry experts like Apollo, Vaultis, Dreadnode and more. For more information, see the Gemini 3 model card.&lt;/p&gt;
    &lt;head rend="h2"&gt;The next era of Gemini&lt;/head&gt;
    &lt;p&gt;This is just the start of the Gemini 3 era. As of today, Gemini 3 starts rolling out:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For everyone in the Gemini app and for Google AI Pro and Ultra subscribers in AI Mode in Search&lt;/item&gt;
      &lt;item&gt;For developers in the Gemini API in AI Studio, our new agentic development platform, Google Antigravity; and Gemini CLI&lt;/item&gt;
      &lt;item&gt;For enterprises in Vertex AI and Gemini Enterprise&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For Gemini 3 Deep Think mode, we’re taking extra time for safety evaluations and input from safety testers before making it available to Google AI Ultra subscribers in the coming weeks.&lt;/p&gt;
    &lt;p&gt;We plan to release additional models to the Gemini 3 series soon so you can do more with AI. We look forward to getting your feedback and seeing what you learn, build and plan with Gemini.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/products/gemini/gemini-3/"/><published>2025-11-18T15:09:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45967814</id><title>Google Antigravity</title><updated>2025-11-18T22:10:15.117176+00:00</updated><link href="https://antigravity.google/"/><published>2025-11-18T15:47:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45968121</id><title>The code and open-source tools I used to produce a science fiction anthology</title><updated>2025-11-18T22:10:14.820498+00:00</updated><content>&lt;doc fingerprint="2fc09a362ef7ad11"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;&lt;p&gt; Last month I published &lt;/p&gt;Think Weirder: The Year's Best Science Fiction Ideas&lt;p&gt;, a 16-story anthology featuring Greg Egan, Isabel J. Kim, Ray Nayler, Caroline M. Yoachim, and twelve other wonderful authors. The book ended up being the #1 New Release in the Short Stories Anthologies category for a short time on Amazon, outselling many other newly released short story anthologies published by the big NYC publishers with large marketing departments. &lt;/p&gt;&lt;/div&gt;
      &lt;p&gt; I'm not a professional publisher. I have a full-time job and two small kids, so all of this work happened after my kids went to sleep. I had to use my time judiciously, which meant creating an efficient process. Fortunately I'm a programmer, and it turns out that programming skills translate surprisingly well to book publishing. This post is about how I built a complete publishing pipeline using Python, YAML files, and LaTeX â and why you might want to do something similar if you're considering publishing a book. I know that by writing this I'll have my choices questioned by professional designers, but hopefully the software concepts will be helpful. &lt;/p&gt;
      &lt;p&gt; My initial thought: can I really do ALL of this? &lt;/p&gt;
      &lt;p&gt; When I started this project, I had some worries. Professional publishers have entire departments of specialists. How could I possibly handle all of that myself? &lt;/p&gt;
      &lt;p&gt; The answer turned out to be: build tools that automate the repetitive parts, and use simple file formats that make everything transparent and debuggable. &lt;/p&gt;
      &lt;p&gt; Step 1: Tracking stories with plain text files &lt;/p&gt;
      &lt;p&gt; The first challenge was tracking hundreds of candidate stories from different magazines. I read 391 stories published in 2024 before selecting the final 16. That's a lot of stories to keep organized. &lt;/p&gt;
      &lt;p&gt; I could have used a spreadsheet, but I went with plain YAML files instead. Here's why this worked well for me: &lt;/p&gt;
      &lt;div&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Git-friendly: Every decision I made was tracked in version control&lt;/item&gt;
          &lt;item&gt;Human-readable: I could open any file in a text editor and understand what I was looking at&lt;/item&gt;
          &lt;item&gt;Easy to build scripts around: I wrote several Python functions to do different kinds of metadata introspection that I'll go through&lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
      &lt;p&gt; The structure looks like this: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;data/
  story-progress.yaml       # Central tracking file
  markets.yaml              # Magazine metadata
  themes.yaml               # Theme occurrence tracking
  subgenres.yaml            # Subgenre tallies
stories/
  clarkesworld-magazine/
    nelson_11_24.yaml       # Individual story files
    pak_06_24.yaml
  reactor-magazine/
    larson_breathing.yaml
  ...&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Each story file is pure YAML containing the full story text plus metadata: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;title: "Twenty-Four Hours"
author: H.H. Pak
market: clarkesworld-magazine
url: https://clarkesworldmagazine.com/pak_06_24/
word_count: 4540
year: 2024
slug: pak_06_24
summary: ...&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Not all stories have public URLs available, but that's OK because all of the fields are optional. The central &lt;code&gt;story-progress.yaml&lt;/code&gt; tracks editorial state:
&lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;clarkesworld-magazine-nelson_11_24:
  title: "LuvHomeâ¢"
  author: Resa Nelson
  market: clarkesworld-magazine
  status: accepted  # or: not_started/relevant/rejected
  date_added: '2024-09-08T08:22:47.033192'&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Step 2: A simple command-line tool &lt;/p&gt;
      &lt;p&gt; I built a small Python CLI tool (&lt;code&gt;se.py&lt;/code&gt;) to help me navigate all this data. Since I do all this work at night after my kids go to sleep, I wanted something fast that mirrored a lot of the other work I do on the command line. The tool is simple:
&lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;python se.py âhelp
usage: se.py [-h] {markets,stories,relevant,decide,accepted,compile} ...

Story Evaluator CLI

positional arguments:
  {markets,stories,relevant,decide,accepted,compile}
                        Available commands
    markets             List markets
    stories             Manage stories
    relevant            List URLs for stories marked as relevant
    decide              Make accept/reject decisions on relevant stories
    accepted            Manage accepted stories
    compile             Show anthology compilation statistics

optional arguments:
  -h, âhelp            show this help message and exit&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; The &lt;code&gt;compile&lt;/code&gt; command ended up being really useful â it gave me instant feedback on anthology size and composition:
&lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;ANTHOLOGY COMPILATION STATISTICS
============================================================
Total Stories: 16
Total Word Count: 115,093 words
Average Word Count: 7,193 words
Unique Authors: 16
Markets Represented: 4

STORIES BY MARKET:
  analog-magazine: 2 stories (12.5%)
  asimovs-magazine: 2 stories (12.5%)
  clarkesworld-magazine: 10 stories (62.5%)
  reactor-magazine: 2 stories (12.5%)&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; This was really helpful during the selection process. I could quickly check how far along I was toward my ~120k word goal, and make sure I hadn't accidentally included multiple stories by the same author. &lt;/p&gt;
      &lt;p&gt; Step 3: Typesetting the print book &lt;/p&gt;
      &lt;p&gt; This part surprised me the most. I initially thought I'd have to learn Adobe InDesign or pay someone to do the typesetting. But I decided to use LaTeX instead, since I had some previous experience with it (another publishing friend sent me some of his example files, and I had some academic experience). The process worked out better than expected. &lt;/p&gt;
      &lt;p&gt; I used XeLaTeX with the &lt;code&gt;memoir&lt;/code&gt; document class. Here's what I liked about this approach:
&lt;/p&gt;
      &lt;div&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Reproducible: I can rebuild the entire book from source in a few seconds, and I can use the same templates next year&lt;/item&gt;
          &lt;item&gt;Professional typography: LaTeX handles ligatures, kerning, and line breaking better than I could manually&lt;/item&gt;
          &lt;item&gt;Custom fonts: I used Crimson Pro for body text and Rajdhani for titles&lt;/item&gt;
          &lt;item&gt;Again, version control that I'm used to: The entire book is just text files in Git&lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
      &lt;p&gt; The main parts of the master file for the book are really simple: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;\documentclass[final,11pt,twoside]{memoir}
\usepackage{compelling}

\begin{document}
\begin{frontmatter}
  \include{title}
  \tableofcontents
\end{frontmatter}

\begin{mainmatter}
  \include{introduction}
  \include{death-and-the-gorgon}
  \include{the-best-version-of-yourself}
  % ... 14 more stories
  \include{acknowledgements}
\end{mainmatter}
\end{document}&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;div&gt;&lt;p&gt; All the formatting rules live in &lt;/p&gt;&lt;code&gt;compelling.sty&lt;/code&gt;&lt;p&gt;, a custom style package. &lt;/p&gt;Here's a link to the full, messy file&lt;p&gt;. Some highlights: &lt;/p&gt;&lt;/div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;% 6x9 inch trade paperback size
\setstocksize{9in}{6in}
\settrimmedsize{9in}{6in}{*}

% Margins
\setlrmarginsandblock{1.00in}{0.75in}{*}
\setulmarginsandblock{0.75in}{0.75in}{*}

% Typography nerding
\usepackage[final,protrusion=true,factor=1125,
            stretch=70,shrink=70]{microtype}

% Custom fonts loaded from local files
\setromanfont[
  Ligatures=TeX,
  Path=./Crimson_Pro/static/,
  UprightFont=CrimsonPro-Regular,
  BoldFont=CrimsonPro-Bold,
  ItalicFont=CrimsonPro-Italic,
  BoldItalicFont=CrimsonPro-BoldItalic
]{Crimson Pro}


\setsansfont[
  Path=./Rajdhani/,
  UprightFont=Rajdhani-Bold,
  BoldFont=Rajdhani-Bold,
  ItalicFont=Rajdhani-Bold,
  BoldItalicFont=Rajdhani-Bold
]{Rajdhani}

% Chinese font family for CJK characters
\newfontfamily\chinesefont{PingFang SC}&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; The &lt;code&gt;microtype&lt;/code&gt; package does a lot of subtle work with character spacing and line breaking that makes the text look professionally typeset.
&lt;/p&gt;
      &lt;p&gt; I wanted story titles in bold sans-serif with author names underneath in a lighter gray. Here's how I set that up: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;\renewcommand{\chapter}[2]{
    \pagestyle{DefaultStyle}
    \stdchapter*{
        \sffamily
        \LARGE 
        \textbf{\MakeUppercase{#1}}
        \\ 
        \large 
        \color{dark-gray} 
        {\MakeUppercase{#2}}
    }
    \addcontentsline{toc}{chapter}{
        \protect\parbox[t]{\dimexpr\textwidth-3em}{
            \sffamily#1
            \\ 
            \protect\small
            \protect\color{gray}
            \protect\textit{#2}
        }
    }
    \def\leftmark{#1}
    \def\rightmark{#2}
}&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; This redefines the &lt;code&gt;chapter&lt;/code&gt; command to take two arguments, the title and byline, and sets up both the chapter formatting, TOC formatting, and makes sure that the title and byline are printed in the headers on alternating pages.
&lt;/p&gt;
      &lt;p&gt; Now every story file just says: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;\chapter{Death and the Gorgon}{by Greg Egan}
[story content]&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Most authors send me stories as HTML, PDF, or word, so I needed a way to convert them to LaTeX. I wrote a simple Python script to do this, which saved me a huge amount of manual formatting work. &lt;/p&gt;
      &lt;p&gt; Step 4: Creating the ebook &lt;/p&gt;
      &lt;p&gt; Print was one thing, but I also needed an ebook. This turned out to be easier than I expected because I could reuse all the LaTeX source I'd already created. &lt;/p&gt;
      &lt;p&gt; I used Pandoc to convert from LaTeX to EPUB: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;# Convert LaTeX to EPUB
pandoc 2025.tex -o Think_Weirder_2025.epub \
  âtoc \
  âepub-cover-image=cover_optimized.jpg \
  âcss=epub-style.css \
  âmetadata title="Think Weirder" \
  âmetadata author="Edited by Joe Stech"&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Pandoc's default table of contents only showed story titles. But I wanted author names too, like you see in print anthologies. EPUBs are just zipped collections of XHTML files, so I wrote a small post-processing script: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;def modify_toc(nav_content, authors):
    """Add author bylines to TOC entries."""
    pattern = r'&amp;lt;a href="([^"]+)"&amp;gt;([^&amp;lt;]+)&amp;lt;/a&amp;gt;'

    def add_author(match):
        href, title = match.group(1), match.group(2)
        chapter_id = extract_id_from_href(href)

        if chapter_id in authors:
            author = authors[chapter_id]
            return f'&amp;lt;a href="{href}"&amp;gt;{title}&amp;lt;br /&amp;gt;\n' \
                   f'&amp;lt;em&amp;gt;{author}&amp;lt;/em&amp;gt;&amp;lt;/a&amp;gt;'
        return match.group(0)

    return re.sub(pattern, add_author, nav_content)&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; The script unzips the EPUB, finds the navigation file, adds author bylines, and rezips everything. Now the ebook table of contents matches the print version. &lt;/p&gt;
      &lt;p&gt; What I learned &lt;/p&gt;
      &lt;p&gt; The whole process took longer than I expected â many months of night work. The simple software I wrote really made it a feasible one-person project though, and motivates me to go through the whole process again next year. &lt;/p&gt;
      &lt;p&gt; Staying organized is crucial. When hundreds of stories are involved, it's easy to forget details, so using &lt;code&gt;se.py&lt;/code&gt; to save metadata in the moment that could be sliced and diced later was so important.
&lt;/p&gt;
      &lt;p&gt; Reproducible builds were a lifesaver. I made changes to the book layout right up until the week before publication. Because I could rebuild the entire book in seconds, and everything was backed up in git, I could experiment freely without worrying about breaking things. &lt;/p&gt;
      &lt;p&gt; Simple file formats made me comfortable. When something went wrong, I could always open a YAML file or look at the LaTeX source and understand what was happening. I never hit a point where the tools were a black box. &lt;/p&gt;
      &lt;p&gt; I didn't need to understand everything up front. I learned LaTeX details as I went (arguably I still don't really understand LaTeX). Same with Pandoc. I got something basic working first, then incrementally improved it. &lt;/p&gt;
      &lt;p&gt; Can you do this too? &lt;/p&gt;
      &lt;p&gt; If you're thinking about publishing a book â whether it's an anthology, a novel, or a collection of technical writing â I think this approach is worth considering. There's something motivating about having a detailed understanding of every step in the production process. If you have questions feel free to reach out, I love talking about this hobby! You can email me at joe@thinkweirder.com. &lt;/p&gt;
      &lt;div&gt;&lt;p&gt; And if you enjoy concept-driven science fiction that is heavy on novel ideas, check out &lt;/p&gt;Think Weirder! &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://compellingsciencefiction.com/posts/the-code-and-open-source-tools-i-used-to-produce-a-science-fiction-anthology.html"/><published>2025-11-18T16:10:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45968362</id><title>Solving a million-step LLM task with zero errors</title><updated>2025-11-18T22:10:14.533985+00:00</updated><content>&lt;doc fingerprint="189cb409d4d7b6cf"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 12 Nov 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Solving a Million-Step LLM Task with Zero Errors&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AI&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2511.09030"/><published>2025-11-18T16:26:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45968611</id><title>Strix Halo's Memory Subsystem: Tackling iGPU Challenges</title><updated>2025-11-18T22:10:14.264623+00:00</updated><content>&lt;doc fingerprint="2efaf135353868c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Strix Halo’s Memory Subsystem: Tackling iGPU Challenges&lt;/head&gt;
    &lt;p&gt;Editor’s Note (11/2/2025): Due to an error in moving the article over from Google Docs to Substack, the “Balancing CPU and GPU Bandwidth Demands” section was missing some Cyberpunk 2077 data. Apologizes for the mistake!&lt;/p&gt;
    &lt;p&gt;AMD’s Strix Halo aspires to deliver high CPU and GPU performance within a mobile device. Doing so presents the memory subsystem with a complicated set of demands. CPU applications are often latency sensitive with low bandwidth demands. GPU workloads are often latency tolerant and bandwidth hungry. Then, multitasking requires high memory capacity. Mobile devices need low power draw. Finally, the whole package has to fit within a price tag acceptable to consumers. Investigating how AMD dealt with those challenges should make for a good time.&lt;/p&gt;
    &lt;head rend="h1"&gt;Credit&lt;/head&gt;
    &lt;p&gt;ASUS has kindly sampled the ROG Flow Z13, which implements Strix Halo in a tablet form factor with 32 GB of LPDDR5X. They’ve made deep dives like this possible, and we greatly appreciate their support.&lt;/p&gt;
    &lt;p&gt;RX 7600 results were provided by Azralee from the Chips and Cheese Discord.&lt;/p&gt;
    &lt;head rend="h1"&gt;GPU&lt;/head&gt;
    &lt;p&gt;Strix Halo’s GPU uses a similar cache setup to AMD’s older and smaller mobile chips. As on Strix Point and Hawk Point (Zen 4 mobile), Strix Halo’s GPU is split into two Shader Arrays. Each Shader Array has 256 KB of L1 mid-level cache, and a 2 MB L2 services the entire GPU. Latencies to those GPU-private caches are in line with other RDNA3 and RDNA3.5 implementations. AMD likely kept L2 capacity at 2 MB because a 32 MB memory side cache (Infinity Cache, or MALL) takes over as the GPU’s last level cache.The L2 only has to catch enough traffic to prevent the Infinity Cache from getting overwhelmed. The resulting cache setup is similar to the one in the RX 7600, a lower midrange RDNA3 discrete card.&lt;/p&gt;
    &lt;p&gt;The Infinity Cache on Strix Halo has slightly higher latency compared to implementations in AMD’s discrete cards. DRAM latency from the GPU is higher as well. Compared to AMD’s other mobile CPUs with iGPUs though, the 32 MB Infinity Cache offers a large cache capacity increase.&lt;/p&gt;
    &lt;p&gt;Nemes’s Vulkan bandwidth test achieves just under 1 TB/s from Infinity Cache. The figures align well with performance counter data. Taken together with the chip’s 2 GHz FCLK, bandwidth test results suggest the GPU has a 512B/cycle path to the interconnect. If so, each of the GPU’s eight Infinity Fabric endpoints has a 64B/cycle link.&lt;/p&gt;
    &lt;p&gt;As a memory side cache, Infinity Cache can theoretically handle any access to physical addresses backed by DRAM. In an earlier interview with Cheese (George), AMD indicated that Infinity Cache was focused on the GPU, and that its behavior could change with firmware releases. Some of that change has happened already. When I first started testing Strix Halo just after Hot Chips 2025, results from my OpenCL microbenchmarks reflected Infinity Cache’s presence. I used that OpenCL code to figure out Data Fabric performance events. But PMU data collected from games suggested Infinity Cache wasn’t used once a game went into the background. Hardware doesn’t know whether a process is running in the foreground or background. That’s something the operating system knows, and that info would have to be communicated to hardware via drivers. Therefore, Infinity Cache policy can change on the fly from software control.&lt;/p&gt;
    &lt;p&gt;At that time, Nemes’s Vulkan-based code didn’t reflect Infinity Cache’s presence. PMU data showed a match between CS and UMC traffic, indicating the microbenchmark wasn’t taking advantage of Infinity Cache rather than the cache struggling with the access pattern. I was in the middle of investigating what Infinity Cache did or didn’t apply to when Windows updated. Then, foreground/background status no longer had any effect. Nemes’s Vulkan code was also able to observe the Infinity Cache.&lt;/p&gt;
    &lt;p&gt;Early observations on Infinity Cache behavior aren’t relevant today, but they do show Infinity Cache’s behavior is influenced by factors beyond a memory request’s origination point. Not all GPU requests install into the cache, and AMD can change cache policy on the fly. AMD could tune behavior with future updates too.&lt;/p&gt;
    &lt;p&gt;One early observation from OpenCL remained consistent though. Infinity Cache isn’t used for a buffer created with the CL_MEM_ALLOC_HOST_PTR flag and managed with zero-copy map/unmap APIs. CL_MEM_ALLOC_HOST_PTR requests an allocation from host-visible memory. On systems with discrete GPUs, AMD tends to handle that by allocating memory from DRAM attached to the CPU.&lt;/p&gt;
    &lt;p&gt;Intuitively, that flag shouldn’t make a difference on integrated GPUs. I’m not sure why it affects Infinity Cache behavior. Perhaps Strix Halo splits address ranges for the CPU and GPU under the hood, and the CPU’s address ranges aren’t cacheable from the Infinity Cache’s perspective.&lt;/p&gt;
    &lt;p&gt;AMD’s discrete Radeon RX 9070 shows similar behavior, with Infinity Cache not being used for host-side memory. Latency to host memory goes up to nearly a microsecond on RX 9070, while it remains unchanged on Strix Halo. Integrated GPUs have an advantage with zero-copy compute code, and it shows.&lt;/p&gt;
    &lt;p&gt;To further check zero-copy behavior, I have a test that allocates a 256 MB buffer using OpenCL’s Shared Virtual Memory APIs and only modifies a single 32-bit value. Strix Halo supports fine-grained buffer sharing like other recent AMD GPUs, meaning applications can use results generated from the GPU without calling map/unmap functions.&lt;/p&gt;
    &lt;p&gt;Strix Halo shows low latencies in line with zero-copy behavior. It’s worth noting that not all integrated GPUs can avoid a copy under the hood.&lt;/p&gt;
    &lt;p&gt;Copy APIs like clEnqueueReadBuffer and clEnqueueWriteBuffer are still relevant, because they’re the traditional way to work with discrete GPUs. Those APIs often use the copy queue and DMA engines, which handle data movement without involving general purpose compute units. Strix Halo can achieve high copy bandwidth in the CPU to GPU direction, but not the other way around.&lt;/p&gt;
    &lt;p&gt;Performance counter data suggests copies to the GPU don’t go through the Infinity Cache. During a copy, the shared memory controllers should observe both a read from CPU-side memory and a write to GPU-side memory. But there’s nowhere near 100% overhead compared to software measurements.&lt;/p&gt;
    &lt;p&gt;Bandwidth is lower in the other direction, but curiously CS-level bandwidth is similar. The memory controllers see less bandwidth, indicating some requests were handled on-chip, likely by Infinity Cache. Curiously, there’s way more than 100% overhead when comparing PMU data to software-visible copy bandwidth.&lt;/p&gt;
    &lt;head rend="h1"&gt;CPU&lt;/head&gt;
    &lt;p&gt;Strix Halo’s CPU side superficially resembles AMD’s flagship desktop parts, with 16 Zen 5 cores split across two Core Complex Dies (CCDs). However, these CCDs use TSMC’s InFO_oS for connectivity to the IO die rather than on-PCB traces. The CCD has 32B/cycle of bandwidth to the system in both the read and write directions.&lt;/p&gt;
    &lt;p&gt;Therefore, Strix Halo’s CCDs have more bandwidth at the die boundary than their desktop counterparts, but only in the write direction. It’s an advantage that’s likely to have minimal impact because reads often outnumber writes by a large margin.&lt;/p&gt;
    &lt;p&gt;Other CPU chiplet designs have more bandwidth at die boundaries, including the Compute Tile on Intel’s Meteor Lake and AMD’s own “GMI-Wide” configuration. GMI-Wide uses two links between the CCD and IO die to maximize cross-die bandwidth in lower core count server chips. Even though GMI-Wide doesn’t use advanced packaging, it has significantly more cross-die bandwidth than Strix Halo.&lt;/p&gt;
    &lt;p&gt;In a loaded latency test with reads, a Strix Halo CCD can reach high bandwidth levels at lower latency than standard GMI-Narrow CCDs. Part of that is likely down to its high bandwidth LPDDR5X setup, which a single CCD can’t come close to saturating. But that advantage doesn’t come through until bandwidth loads pass 45-55 GB/s. Before that, LPDDR5X’s high baseline latency puts Strix Halo at a disadvantage. At very high bandwidth load, Intel Meteor Lake’s higher cross-die bandwidth keeps it ahead. AMD’s GMI-Wide setup shows what a bandwidth-focused cross-die link can do, providing excellent bandwidth at low latency.&lt;/p&gt;
    &lt;p&gt;Bringing both CCDs into play gives Strix Halo a lead over Meteor Lake. I’m starting the test by placing bandwidth load on CCD1 while running the latency test on CCD0. That gives lower latency at bandwidth loads below 60 GB/s because contention at the CCD interface is taken out of the picture. Latency does increase as I spread bandwidth load across both dies, and rises beyond 200 ns as the test approaches die-to-die bandwidth limits. However, a read-only pattern is still limited by cross-die bandwidth and falls far short of the 256 GB/s that the LPDDR5X setup is theoretically capable of.&lt;/p&gt;
    &lt;p&gt;Advanced packaging may provide latency benefits too. Regular AMD CCDs use SerDes (serializer-deserializer) blocks, which convert signals for transport over lower quality PCB traces. Zen 2’s Infinity Fabric On-Package (IFOP) SerDes for example uses 32 transmit and 40 receive lanes running at a very high clock. Forwarded clock signals per lane data bundle help tackle clock skew that comes up with high speed parallel transmission over wires of unequal lengths. CRC helps ensure data integrity.&lt;/p&gt;
    &lt;p&gt;All of that adds power and latency overhead. Strix Halo’s InFO_oS packaging doesn’t require SerDes. But any latency advantage is difficult to observe in practice. DRAM requests are the most common type of off-CCD traffic. High LPDDR5X latency masks any latency advantage when looking at DRAM requests, as shown above. Cache coherency traffic is another form of off-CCD traffic, and doesn’t involve DRAM. However, testing that with a “core to core latency” test that bounces cachelines between core pairs also doesn’t provide favorable results for Strix Halo.&lt;/p&gt;
    &lt;p&gt;AMD handles cross-CCX cache coherency at Coherent Stations (CS-es) that sit right in front of the memory controllers. Memory traffic is interleaved across memory channels and thus CS instances based on their physical address. I try hitting different physical addresses by testing with various cacheline offsets into a 4 KB page, which gives me different combinations of L3 slices and memory controller + CS pairs. Values within a single run reflect variation based on the tested core pair, while different runs display variation from different memory subsystem blocks owning the tested address.&lt;/p&gt;
    &lt;p&gt;Cross-CCX latencies on Strix Halo land in the 100-120 ns range depending on the location of the tested core pair, responsible L3 slice, and responsible CS. It’s significantly higher on typical desktop systems or prior mobile chips from AMD. For example, the Ryzen 9 9900X tends to have cross-CCX latencies in the 80-90 ns range, which is in line with prior Zen generations. It’s about 20 ns faster than Strix Halo.&lt;/p&gt;
    &lt;p&gt;Therefore, I don’t have a satisfactory answer about Strix Halo’s cross-die latency. Latency may indeed be lower at die boundaries. But everything past that boundary has higher latency compared to other client systems, making any advantage invisible to software.&lt;/p&gt;
    &lt;head rend="h1"&gt;Balancing CPU and GPU Bandwidth Demands&lt;/head&gt;
    &lt;p&gt;Sharing a memory controller across the CPU and GPU comes with advantages, like making zero-copy behavior more natural to pull off. But it comes with challenges too. CPU and GPU memory requests can contend with each other for DRAM access. Contention surfaces as higher latency. From Zen 4 onward, AMD’s L3 performance monitoring unit (PMU) can measure average latency in nanoseconds for requests external to the core cluster. PMU data isn’t directly comparable to software measurements, because it only accounts for latency after the point of a L3 miss. But it is consistent in slightly under-estimating software observed latency when running a simple latency microbenchmark. When gaming, I typically see low CPU bandwidth demands and correspondingly mild latency increases over the baseline.&lt;/p&gt;
    &lt;p&gt;The same doesn’t hold true when gaming on Strix Halo’s integrated GPU. Latency rises far above the baseline of around 140 ns. I logged average latency over 1 second intervals, and many of those intervals saw latency figures around 200 ns across several games&lt;/p&gt;
    &lt;p&gt;I wrote a microbenchmark to investigate how CPU memory latency is impacted by GPU-sde bandwidth load. As with the CPU loaded latency test, I run a latency test thread on a CPU core. But instead of using a read-only pattern, I do a standard C=A+B computation across large arrays on the GPU. To control GPU bandwidth load, I can have each OpenCL kernel invocation do more math with A and B before writing the result to C. Results show increased latency at higher GPU bandwidth demands. Other recent iGPUs show similar behavior.&lt;/p&gt;
    &lt;p&gt;In-game CPU bandwidth demands are low, but not as low as a simple latency test. I tried running a couple of read bandwidth threads on top of the test above. Strix Halo seems to let its GPU squeeze out the CPU when under extreme bandwidth demands. Latency suffers, passing 300 ns at one point.&lt;/p&gt;
    &lt;p&gt;Plotting L3 and memory controller PMU data with 1 second intervals helps capture the relationship between latency and bandwidth usage in more complex workloads. The points don’t track well with microbenchmark data collected with a single CPU-side latency test thread. Perhaps there’s enough CPU-side bandwidth demand to cause contention at both the die-to-die interface and the memory controllers. Or maybe, CPU and GPU bandwidth spikes tend to line up within those 1 second intervals. Whatever the case, PMU data highlights how Strix Halo’s CPU cores need high cache hitrates more than their desktop counterparts.&lt;/p&gt;
    &lt;p&gt;Cyberpunk 2077’s built-in benchmark is largely CPU bound when run at 1080P with medium settings and no upscaling. I used Intel’s Arc B580 on desktop systems, since it has vaguely similar compute power to Strix Halo’s iGPU. Results show a large gap between Strix Halo and AMD’s desktop platform, even though both use the same Zen 5 cores.&lt;/p&gt;
    &lt;p&gt;Memory latency under load is largely not a problem with CPU-only workloads, even when considering heavily multithreaded ones. Total bandwidth demands are much lower and actually well within the capabilities of a 128-bit DDR5 setup. That explains why AMD was able to take on quad channel HEDT parts using a desktop dual channel platform back in the Zen 2 days. Good caching likely played a role, and Strix Halo continues to have 4 MB of last level cache per core. PMU data from Cinebench, code compilation, and AV1 video encoding loosely align with microbenchmark results. Latency barely strays above the baseline. Y-Cruncher is an exception. It’s very bandwidth hungry and not cache friendly. Its bandwidth demands are several times higher, and often go beyond a dual channel DDR5-5600 setup’s capabilities. Strix Halo is a good choice for that type of workload. But in the client space, bandwidth hungry CPU applications tend to be exceptions.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words: A GPU with an Integrated CPU?&lt;/head&gt;
    &lt;p&gt;Observations above suggest Strix Halo’s Infinity Fabric and DRAM setup focuses on feeding the GPU and as a result the CPU gets the short end of the stick. High Infinity Fabric endpoint count and a wide LPDDR5X bus provide high bandwidth at high latency. CPU workloads tend to be latency sensitive and contention can make that even worse.&lt;/p&gt;
    &lt;p&gt;Other aspects of the memory subsystem de-prioritize the CPU as well. CPU accesses don’t fill into that cache, but still do a lookup likely to maintain cache coherency with the GPU. That cache lookup at the very least costs power and might add latency, even though it’ll almost never result in a hit. Lack of GMI-Wide style bandwidth is another example.&lt;/p&gt;
    &lt;p&gt;AMD’s decisions are understandable. Most client workloads have light bandwidth requirements.Strix Halo’s memory system design lets it perform well in portable gaming devices like the ROG Flow Z13. But it does make tradeoffs. And extrapolating from those tradeoffs suggests iGPU designs will face steeper challenges at higher performance tiers.&lt;/p&gt;
    &lt;p&gt;For its part, Strix Halo strikes a good balance. It enjoys iGPU advantages without being large enough for the disadvantages to hurt. I hope AMD continues to target Strix Halo’s market segment with updated designs, and look forward to seeing where they go next.&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipsandcheese.com/p/strix-halos-memory-subsystem-tackling"/><published>2025-11-18T16:41:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45969250</id><title>Pebble, Rebble, and a path forward</title><updated>2025-11-18T22:10:13.891587+00:00</updated><content>&lt;doc fingerprint="fc4b101b5b1408f0"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I believe the Pebble community, Core Devices, Rebble and I all want the same thing. We love our Pebbles and want them to keep working long into the future. We love the community that has sprung up around Pebble, and how it’s persevered - next year will be the 14th anniversary of the original Kickstarter campaign!&lt;/p&gt;
      &lt;p&gt;But I have to respond to claims made by Rebble posted on their blog yesterday. I will link to their post so you can read their side of the story, and I’ve asked them to link back to this blog post from theirs.&lt;/p&gt;
      &lt;p&gt;Look - I’m the first person to call myself out when I fail. I wrote a detailed blog post about Success and Failure at Pebble and often write in detail about learning from my mistakes. But in this specific case, you’ll find that I’ve done my utmost to respect the Pebble legacy and community. Rebble is misleading the community with false accusations.&lt;/p&gt;
      &lt;p&gt;For those just passing through, here’s the TLDR: &lt;/p&gt;
      &lt;p&gt;Core Devices is a small company I started in 2025 to relaunch Pebble and build new Pebble smartwatches. Rebble is a non-profit organization that has supported the Pebble community since 2017. Rebble has done a ton of great work over the years and deserves recognition and support for that.&lt;/p&gt;
      &lt;p&gt;Core Devices and Rebble negotiated an agreement where Core would pay $0.20/user/month to support Rebble services. But the agreement broke down after over the following disagreement. &lt;/p&gt;
      &lt;p&gt;Rebble believes that they ‘100%’ own the data of the Pebble Appstore. They’re attempting to create a walled garden around 13,000 apps and faces that individual Pebble developers created and uploaded to the Pebble Appstore between 2012 and 2016. Rebble later scraped this data in 2017. &lt;/p&gt;
      &lt;p&gt;I disagree. I’m working hard to keep the Pebble ecosystem open source. I believe the contents of the Pebble Appstore should be freely available and not controlled by one organization. &lt;/p&gt;
      &lt;p&gt;Rebble posted a blog post yesterday with a bunch of false accusations, and in this post I speak to each of them.&lt;/p&gt;
      &lt;p&gt;Sections&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Dec 2016 - Pebble shut down. Some IP was sold to Fitbit. I blogged about why I think we failed. Fitbit continued to run the Pebble Appstore and web services for 1.5 years. I really appreciated that.&lt;list rend="ul"&gt;&lt;item&gt;Rebble organization grew out of the official Pebble Developers Discord.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;July 2018, Fitbit shut down the Pebble appstore.&lt;list rend="ul"&gt;&lt;item&gt;Before it shut down, Rebble (and others) scraped all 13,000 apps and metadata from the Pebble Appstore. Rebble began hosting a copy of the appstore. They created a new Dev Portal where developers could upload new apps, roughly 500 have been uploaded since July 2018.&lt;/item&gt;&lt;item&gt;Rebble also reverse engineered many Pebble web services (weather, timeline and voice transcription) and provided them as a paid service for the Pebble community.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Jan 2025 - Google open sourced PebbleOS, breathing new life into the community.&lt;/item&gt;
        &lt;item&gt;March 2025 - I announced a new company (Core Devices) and 2 new watches - store.rePebble.com&lt;/item&gt;
        &lt;item&gt;November 2025 - we finished shipping out 5,000 Pebble 2 Duos. We’re working hard on Pebble Time 2. We’re aiming to start shipping in January.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Accusation 1: ‘Rebble paid for the work that [Eric] took as a base for his commercial watches’&lt;/p&gt;
      &lt;p&gt;Facts:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;I think they’re accusing me of ‘stealing’ open source contributions to PebbleOS that Rebble paid for. This is entirely false.&lt;/item&gt;
        &lt;item&gt;We did not take any PebbleOS work Rebble paid for ‘as a base for [our] commercial watches’. &lt;del rend="overstrike"&gt;To my best of my knowledge&lt;/del&gt;&lt;del rend="overstrike"&gt;,&lt;/del&gt;&lt;del rend="overstrike"&gt;Rebble never paid the&lt;/del&gt;&lt;del rend="overstrike"&gt;developer who ported NimBLE into PebbleOS.&lt;/del&gt;&lt;del rend="overstrike"&gt;My best guess is that they are referring to Rebble having paid CodeCoup, the company behind&lt;/del&gt;&lt;del rend="overstrike"&gt;NimBLE&lt;/del&gt;&lt;del rend="overstrike"&gt;, to fix some bugs that affected older non-Core Devices watches. Any Rebble-sponsored CodeCoup commits are not present in our repo. In fact, the opposite is true - we paid Codecoup $10,000 to fix multiple BLE stack issues, some of them on the host side that benefit all devices, including old Pebbles.&lt;/del&gt; Update: I’m told Rebble did pay him, months later. My point is valid - when we shifted development to our repo, Rebble had not paid anything. More broadly, I reject the premise that using open source software under the terms of the license, regardless of who funds development, is ‘stealing’.&lt;/item&gt;
        &lt;item&gt;We started using our own repo for PebbleOS development because PRs on the Rebble repo reviews were taking too long. We only had one firmware engineer at the time (now we have a whopping 2!) and he felt like he was being slowed down too much. All of our contributions to PebbleOS have been 100% open source.&lt;/item&gt;
        &lt;item&gt;Overall, the feedback that PebbleOS could benefit from open governance is well taken. Long term, PebbleOS would be a good fit for open source organization with experience in open governance, like Apache or Linux Foundation. I wrote about this last week.&lt;/item&gt;
        &lt;item&gt;With our small team and fairly quick development schedule, it's true that we haven't PRed our changes into Rebble’s repo. It’s tough to prioritize this while we are busy fixing bugs and getting ready for Pebble Time 2.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Accusation 2: ‘Core took Rebble’s work’ on &lt;code&gt;libpebblecommon&lt;/code&gt; to create &lt;code&gt;libpebble3&lt;/code&gt;&lt;/p&gt;
      &lt;p&gt;Facts:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;The majority (&amp;gt;90%) of our new open source&lt;code&gt;libpebble3&lt;/code&gt; library was written by Core Devices employees.  The remainder comes from &lt;code&gt;libpebblecommon&lt;/code&gt;, another open source library written by two people.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;In April 2025, Core purchased the copyright to the &lt;code&gt;libpebblecommon&lt;/code&gt; code from the two maintainers and incorporated it into &lt;code&gt;libpebble3&lt;/code&gt;**, which is also open source**.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;All our contributions to &lt;code&gt;libpebble3&lt;/code&gt; are GPL-3.0 licensed. Here’s the motivation behind that our licensing strategy for this repo. We use the same CLA agreement as Matrix, QT and MySQL. Our CLA explicitly includes a clause that requires to Core Devices to distribute all contributions under an OSI-compatible FOSS license (e.g. GPLv3).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Note that neither Rebble &lt;code&gt;libpebblecommon&lt;/code&gt; maintainer signed the Rebble blog post.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Side note regarding Cobble, I don’t think Rebble even knows this but in 2024, I personally spent over $30,000 to support its development, way before PebbleOS was open source. It was my own way to support the community.&lt;/p&gt;
      &lt;p&gt;Accusation 3: ‘Core promised that they would let Rebble maintain and own the developer site’&lt;/p&gt;
      &lt;p&gt;Facts:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Nothing of the sort was agreed upon. See the full written agreement that Core Devices has with Rebble towards the bottom. Rebble agreed that Core would host the developer site.&lt;/item&gt;
        &lt;item&gt;I have been maintaining and updating the developer site personally - all open source. Having two sources of truth would be confusing for the community.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Accusation 4: ‘[Eric] scraped our app store, in violation of the agreement that we reached with him previously’&lt;/p&gt;
      &lt;p&gt;Note: ‘scraping’ usually means to automated extraction of data from a website.&lt;/p&gt;
      &lt;p&gt;Facts: &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Here’s what happened. I wanted to highlight some of my favourite watchfaces on the Pebble Appstore. Last Monday Nov 10, after I put my kids to sleep and between long calls with factories in Asia, I started building a webapp to help me quickly go through Pebble Appstore and decide which were my top picks.&lt;/item&gt;
        &lt;item&gt;Let me be crystal clear - my little webapp did not download apps or ‘scrape’ anything from Rebble. The webapp displayed the name of each watchface and screenshots and let me click on my favs. I used it to manually look through 6000 watchfaces with my own eyes. I still have 7,000 to go. Post your server logs, they will match up identically to the app I (well…Claude) wrote (source code here)&lt;/item&gt;
        &lt;item&gt;I integrated these picks into the Pebble Appstore on Saturday and posted about it on Sunday.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;All of four of these accusations could have been clarified simply by asking me. Instead, Rebble decided to post them on their blog and threaten a lawsuit. &lt;/p&gt;
      &lt;p&gt;How did we get here?&lt;/p&gt;
      &lt;p&gt;Why are there dueling blog posts in the Pebbleverse? &lt;/p&gt;
      &lt;p&gt;I think most of the people are behind Rebble are great and the community overall is awesome. I know they truly mean well, but there are many aspects of the org that are severely troubling. I am very close with one of the Rebble board members, who I consider a personal friend. Over the years, I learned a lot about the organization and helped coach him through some major disputes between board members. &lt;/p&gt;
      &lt;p&gt;I exchanged literally thousands of messages with my friend on this topic over the span of 3 years. I refrained from getting too involved, despite being asked several times to join Rebble as a board member or lead the organization. I demurred - I saw how painful it was for him and I had no interest in being part of that. &lt;/p&gt;
      &lt;p&gt;Core Devices + Rebble: 2025&lt;/p&gt;
      &lt;p&gt;PebbleOS is now open source! Yay. This is thanks to the work of many Googlers, ex-Pebblers and others - I called out (hopefully) all of them in my blog post in March. I really wanted Rebble to be a part of the Pebble revival going forward. I hired 3 people from Rebble to join Core Devices. I regularly brought up Rebble’s efforts over the years.&lt;/p&gt;
      &lt;p&gt;I engaged with Rebble folks in discussions in the spring on how we could formally work together, and then made some concrete proposals in the summer. One difficulty was that Core Devices is a business with customers and schedules. This didn’t always sync up with the timeframes of a non-profit. Things became very drawn out. It was very hard to pin people down, even on simple stuff like what the goals of Rebble as an organization were. &lt;/p&gt;
      &lt;p&gt;Regardless, I continued pushing to make Rebble a key part of the Pebble relaunch.&lt;/p&gt;
      &lt;p&gt;By August, we finally got close to an agreement.&lt;/p&gt;
      &lt;p&gt;On September 30 2025, we agreed to the following document and published respective blog posts (ours, theres). Core Devices would pay Rebble $0.20/user/month. I considered it a donation to a group that has done so much to support the community. But I purposely pushed for openness - no single group (Core Devices or Rebble) should be in control. &lt;/p&gt;
      &lt;p&gt;Notice the final bullet in the App store section: &lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;All binary/metadata (including historical apps) will be published as archive file (no scraping Rebble services) &lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;Looking back, we should have had more clear wording in this agreement. But this was after months of chat discussions and hours of Zoom calls. I honestly thought that we had reached an agreement to make the archive open, like in this message I received from a Rebble board member.&lt;/p&gt;
      &lt;p&gt;By the end of October, Rebble has changed their mind about providing an archive file.&lt;/p&gt;
      &lt;p&gt;Not withstanding their false accusations of theft, the crux of our disagreement is the archive of 13,000 Pebble apps and watchfaces that were uploaded to the Pebble Appstore in July 2018 before it was shut down. &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;I believe that these apps and watchfaces should be archived publicly and freely accessible by anyone. They should not held behind a walled garden by one organization. I repeatedly advocated for hosting this data on a neutral 3rd party like Archive.org.&lt;/item&gt;
        &lt;item&gt;Rebble believes ‘the data behind the Pebble App Store is 100% Rebble’ (this is a direct quote from their blog post). They repeatedly refer to all watchfaces and watchapps as ‘our data’.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;This is just plainly false. The apps and watchfaces were originally uploaded by individual developers to an appstore run by a company that no longer exists. These folks created beautiful work and shared them freely with the Pebble community. I’ve spoken with numerous Pebble app developers about this. After the fall of Pebble Tech Corp, none of them envisioned one single organization claiming ownership of their work and restricting access, or charging money for access.&lt;/p&gt;
      &lt;p&gt;Let’s do the right thing - honour the original developers and create a free publicly available archive of their beautiful watchfaces and watchapps. &lt;/p&gt;
      &lt;p&gt;It's easy to assume the worst in situations like this. But our plan for the appstore is pretty straightforward. We’re working on rewriting the appstore frontend to be native in the mobile app rather than a web view. Rebble’s appstore backend API will be the data source. Rebble’s dev portal is where developers upload apps. No subscription or Rebble account will not be required to download apps. We intend to curate how the appstore is displayed Pebble app.&lt;/p&gt;
      &lt;p&gt;We’re excited to see other Pebble-supporting mobile apps pop up - like MicroPebble and GadgetBridge, offering different features and experiences. We’d love to support these efforts with open source code or financially.&lt;/p&gt;
      &lt;p&gt;Reading things like ‘We’re happy to let them build whatever they want as long as it doesn’t hurt Rebble’ in their blog post worries me. Take our voice-to-text and weather features. Rebble currently offers these as part of their paid subscription. Our new Pebble mobile app includes a on-device speech-to-text feature. We’re planning to include weather for free in our app and make the data available to all watchfaces so you don’t need to configure each one separately. These features are better for users but would they ‘hurt’ Rebble? Will I need to ask permission from Rebble before building these features? It’s clear that the goals of a non-profit and device manufacturer will not always be in alignment.&lt;/p&gt;
      &lt;p&gt;Now consider the appstore. It’s a fundamental part of the Pebble experience. Even before yesterday’s accusations, I felt wary about relying too heavily on a 3rd party like Rebble to provide such a critical service. When people buy a watch from Core Devices, they expect to be able to download apps and watchfaces. If Rebble leadership changes their mind, how can I be certain I can deliver a good experience for our customers? This is one of the primary reasons I think it’s important for an archive of the Pebble Appstore to be freely available.&lt;/p&gt;
      &lt;p&gt;Rebble - prove that you believe in an open, unrestricted Pebble community. Tear down the walled garden you are trying to create. Publish your copy of the Pebble Appstore archive. Stop saying that you ‘100%’ own other developers data. Let’s move on from this ridiculous sideshow and focus on making Pebble awesome!&lt;/p&gt;
      &lt;p&gt;I’ve worked hard to structure everything that we’re doing to be sustainable for the long term, and to do right by the Pebble community. I think Rebble should do the same. &lt;/p&gt;
      &lt;p&gt;I earned almost nothing from Pebble Tech Corp. I paid myself a $65,000 salary each year. I did not get any payout through the asset sale. I fought to make sure that all Pebble employees were taken care of as best as possible, and that the Pebble community would live on. I believe that at every turn, I’ve done right by the community.&lt;/p&gt;
      &lt;p&gt;I didn’t relaunch Pebble to make a lot of money. My goal this time round is to make it sustainable. I want to continue making more watches and cool gadgets. There are no investors. I am taking huge risks doing this. I relaunched it because I love Pebble and want it to live on long into the future. Generally, I am excited and positive for the future, despite everything.&lt;/p&gt;
      &lt;p&gt;For everyone else, again, I apologize for the extreme amounts of inside baseball and the better things you could be doing with your time. I’ll leave the comments open here. Please refrain from any personal attacks or vicious comments (at myself or other people) - follow the HN guidelines.&lt;/p&gt;
      &lt;p&gt;Eric Migicovsky&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ericmigi.com/blog/pebble-rebble-and-a-path-forward/"/><published>2025-11-18T17:24:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45969708</id><title>Show HN: Guts – convert Golang types to TypeScript</title><updated>2025-11-18T22:10:13.178539+00:00</updated><content>&lt;doc fingerprint="17201a0ffe1c1fc0"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;guts&lt;/code&gt; is a tool to convert golang types to typescript for enabling a consistent type definition across the frontend and backend. It is intended to be called and customized as a library, rather than as a command line executable.&lt;/p&gt;
    &lt;p&gt;See the simple example for a basic usage of the library.&lt;/p&gt;
    &lt;code&gt;type SimpleType[T comparable] struct {
	FieldString     string
	FieldInt        int
	FieldComparable T
	FieldTime       time.Time
}&lt;/code&gt;
    &lt;p&gt;Gets converted into&lt;/p&gt;
    &lt;code&gt;type Comparable = string | number | boolean;

// From main/main.go
interface SimpleType&amp;lt;T extends Comparable&amp;gt; {
    FieldString: string;
    FieldInt: number;
    FieldComparable: T;
    FieldTime: string;
}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;guts&lt;/code&gt; is a library, not a command line utility. This is to allow configuration with code, and also helps with package resolution.&lt;/p&gt;
    &lt;p&gt;See the simple example for a basic usage of the library. A larger example can be found in the Coder repository.&lt;/p&gt;
    &lt;code&gt;// Step 1: Create a new Golang parser
golang, _ := guts.NewGolangParser()

// Optional: Preserve comments from the golang source code
// This feature is still experimental and may not work in all cases
golang.PreserveComments()

// Step 2: Configure the parser
_ = golang.IncludeGenerate("github.com/coder/guts/example/simple")
// Step 3: Convert the Golang to the typescript AST
ts, _ := golang.ToTypescript()
// Step 4: Mutate the typescript AST
ts.ApplyMutations(
    config.ExportTypes, // add 'export' to all top level declarations
)
// Step 5: Serialize the typescript AST to a string
output, _ := ts.Serialize()
fmt.Println(output)&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;guts&lt;/code&gt; first parses a set of golang packages. The Go AST is traversed to find all the types defined in the packages.&lt;/p&gt;
    &lt;p&gt;These types are placed into a simple AST that directly maps to the typescript AST.&lt;/p&gt;
    &lt;p&gt;Using goja, these types are then serialized to typescript using the typescript compiler API.&lt;/p&gt;
    &lt;p&gt;The generator aims to do the bare minimum type conversion. An example of a common opinion, is to use types to represent enums. Without the mutation, the following is generated:&lt;/p&gt;
    &lt;code&gt;export enum EnumString {
    EnumBar = "bar",
    EnumBaz = "baz",
    EnumFoo = "foo",
    EnumQux = "qux"
}&lt;/code&gt;
    &lt;p&gt;Add the mutation:&lt;/p&gt;
    &lt;code&gt;ts.ApplyMutations(
	config.EnumAsTypes,
)
output, _ := ts.Serialize()&lt;/code&gt;
    &lt;p&gt;And the output is:&lt;/p&gt;
    &lt;code&gt;export type EnumString = "bar" | "baz" | "foo" | "qux";&lt;/code&gt;
    &lt;p&gt;The guts package was created to offer a more flexible, programmatic alternative to existing Go-to-TypeScript code generation tools out there.&lt;/p&gt;
    &lt;p&gt;The other solutions out there function as command-line utilities with yaml configurability. &lt;code&gt;guts&lt;/code&gt; is a library, giving it a much more flexible and dynamic configuration that static generators can’t easily support.&lt;/p&gt;
    &lt;p&gt;Unlike many of its counterparts, guts leverages the official TypeScript compiler under the hood, ensuring that the generated TypeScript definitions are semantically correct, syntactically valid, and aligned with the latest language features.&lt;/p&gt;
    &lt;p&gt;An incredible website to visualize the AST of typescript: https://ts-ast-viewer.com/&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/coder/guts"/><published>2025-11-18T17:55:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45969909</id><title>I am stepping down as the CEO of Mastodon</title><updated>2025-11-18T22:10:12.924995+00:00</updated><content>&lt;doc fingerprint="6c92901d0c810af1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;My next chapter with Mastodon&lt;/head&gt;
    &lt;p&gt;Eugen Rochko&lt;/p&gt;
    &lt;p&gt;Strategy &amp;amp; Product Advisor, Founder&lt;/p&gt;
    &lt;p&gt;After nearly 10 years, I am stepping down as the CEO of Mastodon and transferring my ownership of the trademark and other assets to the Mastodon non-profit. Over the course of my time at Mastodon, I have centered myself less and less in our outward communications, and to some degree, this is the culmination of that trend. Mastodon is bigger than me, and though the technology we develop on is itself decentralized—with heaps of alternative fediverse projects demonstrating that participation in this ecosystem is possible without our involvement—it benefits our community to ensure that the project itself which so many people have come to love and depend on remains true to its values. There are too many examples of founder egos sabotaging thriving communities, and while I’d like to think myself an exception, I understand why people would prefer better guardrails.&lt;/p&gt;
    &lt;p&gt;But it would be uncouth for me to pretend that there isn’t some self-interest involved. Being in charge of a social media project is, turns out, quite the stressful endeavour, and I don’t have the right personality for it. I think I need not elaborate that the passion so many feel for social media does not always manifest in healthy ways. You are to be compared with tech billionaires, with their immense wealth and layered support systems, but with none of the money or resources. It manifests in what people expect of you, and how people talk about you. I remember somebody jokingly suggesting that I challenge Elon Musk to a fight (this was during his and Mark Zuckerberg’s martial arts feud), and quietly thinking to myself, I am literally not paid enough for that. I remember also, some Spanish newspaper article that for some reason, concluded that I don’t dress as fashionably as Jeff Bezos, based on the extremely sparse number of pictures of myself I have shared on the web. Over an entire decade, these tiny things chip away at you slowly. Some things chip faster. I steer clear of showing vulnerability online, but there was a particularly bad interaction with a user last summer that made me realise that I need to take a step back and find a healthier relationship with the project, ultimately serving as the impetus to begin this restructuring process.&lt;/p&gt;
    &lt;p&gt;As for what the legacy of my run will be, I find hard to answer. For one, I think it is not up for me to judge. On the other hand, it is as much about what didn’t happen as it is about what did. I’ve always thought that one of the most important responsibilities I had was to say “no”. It is not a popular thing to do, nor is it a fun thing to do, but being pulled into too many different directions at once can spell disaster for any project. I’d like to think I avoided some trouble by being careful. But I’m also aware that my aversion to public appearances cost Mastodon some opportunities in publicity. Ultimately, while I cannot take sole credit for it, I am nevertheless most proud of how far we’ve made it over these last 10 years. From the most barebones project written out of my childhood bedroom, to one of the last remaining and thriving pieces of the original, community-centred internet.&lt;/p&gt;
    &lt;p&gt;I have so much passion for Mastodon and the fediverse. The fediverse is an island within an increasingly dystopian capitalist hellscape. And from my perspective, Mastodon is our best shot at bringing this vision of a better future to the masses. This is why I’m sticking around, albeit in a more advisory, and less public, role.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.joinmastodon.org/2025/11/my-next-chapter-with-mastodon/"/><published>2025-11-18T18:13:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45970338</id><title>Show HN: RowboatX – open-source Claude Code for everyday automations</title><updated>2025-11-18T22:10:12.326351+00:00</updated><content>&lt;doc fingerprint="fd746fd3152ca8f9"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✨ Create background agents with full shell access &lt;list rend="ul"&gt;&lt;item&gt;E.g. "Generate a NotebookLM-style podcast from my saved articles every morning"&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;🔧 Connect any MCP server to add capabilities &lt;list rend="ul"&gt;&lt;item&gt;Add MCP servers and RowboatX handles the integration&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;🎯 Let RowboatX control and monitor your background agents &lt;list rend="ul"&gt;&lt;item&gt;Easily inspect state on the filesystem&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Inspired by Claude Code, RowboatX brings the same shell-native power to background automations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Set your LLM API key. Supports OpenAI, Anthropic, Gemini, OpenRouter, LiteLLM, Ollama, and more.&lt;/p&gt;
        &lt;code&gt;export OPENAI_API_KEY=your-openai-api-key&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install RowboatX&lt;/p&gt;
        &lt;quote&gt;npx @rowboatlabs/rowboatx&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;$ rowboatx&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add MCP: 'Add this MCP server config: &amp;lt;config&amp;gt; '&lt;/item&gt;
      &lt;item&gt;Explore tools: 'What tools are there in &amp;lt;server-name&amp;gt; '&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;$ rowboatx&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;'Create agent to do X.'&lt;/item&gt;
      &lt;item&gt;'... Attach the correct tools from &amp;lt;mcp-server-name&amp;gt; to the agent'&lt;/item&gt;
      &lt;item&gt;'... Allow the agent to run shell commands including ffmpeg'&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;$ rowboatx&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;'Make agent &amp;lt;background-agent-name&amp;gt; run every day at 10 AM'&lt;/item&gt;
      &lt;item&gt;'What agents do I have scheduled to run and at what times'&lt;/item&gt;
      &lt;item&gt;'When was &amp;lt;background-agent-name&amp;gt; last run'&lt;/item&gt;
      &lt;item&gt;'Are any agents waiting for my input or confirmation'&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;rowboatx --agent=&amp;lt;agent-name&amp;gt; --input="xyz" --no-interactive=true&lt;/code&gt;
    &lt;code&gt;rowboatx --agent=&amp;lt;agent-name&amp;gt; --run_id=&amp;lt;run_id&amp;gt; # resume from a previous run&lt;/code&gt;
    &lt;p&gt;You can configure your models in &lt;code&gt;~/.rowboat/config/models.json&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;{
  "providers": {
    "openai": {
      "flavor": "openai"
    },
    "openai-compatible-host": {
      "flavor": "openai",
      "baseURL": "http://localhost:2000/...",
      "apiKey": "...",
      "headers": {
        "foo": "bar"
      }
    },
    "anthropic": {
      "flavor": "anthropic"
    },
    "google": {
      "flavor": "google"
    },
    "ollama": {
      "flavor": "ollama"
    }
  },
  "defaults": {
    "provider": "openai",
    "model": "gpt-5"
  }
}&lt;/code&gt;
    &lt;p&gt;To use Rowboat Classic UI (not RowboatX), refer to Classic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/rowboatlabs/rowboat"/><published>2025-11-18T18:50:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45970391</id><title>OrthoRoute – GPU-accelerated autorouting for KiCad</title><updated>2025-11-18T22:10:12.177069+00:00</updated><content>&lt;doc fingerprint="f2d15d492c15bffa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;OrthoRoute — GPU-Accelerated Autorouting for KiCad&lt;/head&gt;
    &lt;p&gt;OrthoRoute is a GPU-accelerated PCB autorouter that uses a Manhattan lattice and the PathFinder algorithm to route high-density boards. Built as a KiCad plugin using the IPC API, it handles complex designs with thousands of nets that make traditional push-and-shove routers give up.&lt;/p&gt;
    &lt;p&gt;Never trust the autorouter, but at least this one is fast.&lt;/p&gt;
    &lt;head rend="h4"&gt;This document is a complement to the README in the Github repository. The README provides information about performance, capabilities, and tests. This document reflects more on the why and how OrthoRoute was developed.&lt;/head&gt;
    &lt;head rend="h1"&gt;Why I Built This&lt;/head&gt;
    &lt;p&gt;This is a project born out of necessity. Another thing I was working on needed an enormous backplane. A PCB with sixteen connectors, with 1,100 pins on each connector. That’s 17,600 individual pads, and 8,192 airwires that need to be routed. Here, just take a look:&lt;/p&gt;
    &lt;p&gt;Look at that shit. Hand routing this would take months. For a laugh, I tried FreeRouting, the KiCad autorouter plugin, and it routed 4% of the traces in seven hours. If that trend held, which it wouldn’t, that would be a month of autorouting. And it probably wouldn’t work in the end. I had a few options, all of which would take far too long&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I could route the board by hand. This would be painful and take months, but I would get a good-looking board at the end.&lt;/item&gt;
      &lt;item&gt;I could YOLO everything and just let the FreeRouting autorouter handle it. It would take weeks, because the first traces are easy, the last traces take the longest. This would result in an ugly board.&lt;/item&gt;
      &lt;item&gt;I could spend a month or two building my own autorouter plugin for KiCad. I have a fairly powerful GPU and I thought routing a PCB is a very parallel problem. I could also implement my own routing algorithms to make the finished product look good.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When confronted with a task that will take months, always choose the more interesting path.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New KiCad API, and a ‘Traditional’ Autorouter&lt;/head&gt;
    &lt;p&gt;KiCad, Pre-version 9.0, had a SWIG-based plugin system. There are serious deficits with this system compared to the new IPC plugin system released with KiCad 9. The SWIG-based system was locked to the Python environment bundled with KiCad. Process isolation, threading, and performance constraints were a problem. Doing GPU programming with CuPy or PyTorch, while not impossible, is difficult.&lt;/p&gt;
    &lt;p&gt;The new IPC plugin system for KiCad is a godsend. The basic structure of the OrthoRoute plugin looks something like this:&lt;/p&gt;
    &lt;p&gt;The OrthoRoute plugin communicates with KiCad via the IPC API over a UNIX-ey socket. This API is basically a bunch of C++ classes that gives me access to board data – nets, pads, copper pour geometry, airwires, and everything else. This allows me to build a second model of a PCB inside a Python script and model it however I want. With a second model of a board inside my plugin, all I have to do is draw the rest of the owl.&lt;/p&gt;
    &lt;head rend="h2"&gt;Development of the Manhattan Routing Engine&lt;/head&gt;
    &lt;p&gt;After wrapping my head around the the ability to read and write board information to and from KiCad, I had to figure out a way to route this stupidly complex backplane. A non-orthogonal autorouter is a good starting point, but I simply used that as an exercise to wrap my head around the KiCad IPC API. The real build is a ‘Manhattan Orthogonal Routing Engine’, the tool needed to route my mess of a backplane.&lt;/p&gt;
    &lt;head rend="h3"&gt;Project PathFinder&lt;/head&gt;
    &lt;p&gt;The algorithm used for this autorouter is PathFinder: a negotiation-based performance-driven router for FPGAs. My implementation of PathFinder treats the PCB as a graph: nodes are intersections on an x–y grid where vias can go, and edges are the segments between intersections where copper traces can run. Each edge and node is treated as a shared resource.&lt;/p&gt;
    &lt;p&gt;PathFinder is iterative. In the first iteration, all nets (airwires) are routed greedily, without accounting for overuse of nodes or edges. Subsequent iterations account for congestion, increasing the “cost” of overused edges and ripping up the worst offenders to re-route them. Over time, the algorithm converges to a PCB layout where no edge or node is over-subscribed by multiple nets.&lt;/p&gt;
    &lt;p&gt;With this architecture – the PathFinder algorithm on a very large graph, within the same order of magnitude of the largest FPGAs – it makes sense to run the algorithm with GPU acceleration. There are a few factors that went into this decision:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Everyone who’s routing giant backplanes probably has a gaming PC. Or you can rent a GPU from whatever company is advertising on MUNI bus stops this month.&lt;/item&gt;
      &lt;item&gt;The PathFinder algorithm requires hundreds of billions of calculations for every iteration, making single-core CPU computation glacially slow.&lt;/item&gt;
      &lt;item&gt;With CUDA, I can implement a SSSP (parallel Dijkstra) to find a path through a weighted graph very fast.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Adapting FPGA Algorithms to PCBs&lt;/head&gt;
    &lt;p&gt;The original PathFinder paper was, “A Negotiation-Based Performance-Driven Router for FPGAs” and from 1995, this meant early FPGAs like the Xilinx 3000 series and others manufactured by Tryptych. These devices were simple, and to get a good idea of how they worked, check out Ken Shirriff’s blog. Here’s what the inside of a Xilinx XC2064 looks like:&lt;/p&gt;
    &lt;p&gt;That looks complicated, but it’s really exceptionally simple. All the LUTs, or logic elements, are connected to each other with wires. Where the wires cross over, there are fuzes. Burn the fuzes and you’ve connected the wires together. It’s a simple graph and all the complexity of the actual paths inside the chip are abstracted away. For a circuit board, I don’t have this luxury. I have to figure out how to get the signal from the pads on the top layer of the PCB and ‘drill down’ with vias into the grid. I need to come up with some way to account for both the edges of the graph and nodes of the graph, something that’s untread territory with the PathFinder algorithm.&lt;/p&gt;
    &lt;p&gt;The first step of that is the pad escape planner that pre-computes the escape routing of all the pads. Because the entire Manhattan Routing Engine is designed for a backplane, we can make some assumptions: All of the components are going to be SMD, because THT parts would kill the efficiency of a routing lattice. The components are going to be arranged on a grid, and just to be nice I’d like some ‘randomization’ in where it puts the vias punching down into the grid. Here’s what the escape planning looks like:&lt;/p&gt;
    &lt;head rend="h3"&gt;How PathFinder Almost Killed Me, and How I made PathFinder not suck&lt;/head&gt;
    &lt;p&gt;I found every bug imaginable while developing OrthoRoute. For one, congestion of nets would grow each iterations. The router would start fine with 9,495 edges with congestion in iteration 1. Then iteration 2: 18,636 edges. Iteration 3: 36,998 edges. The overuse was growing by 3× per iteration instead of converging. Something was fundamentally broken. The culprit? History costs were decaying instead of accumulating. The algorithm needs to remember which edges were problematic in past iterations, but my implementation had &lt;code&gt;history_decay=0.995&lt;/code&gt;, so it was forgetting 0.5% of the problem every iteration. By iteration 10, it had forgotten everything. No memory = no learning = explosion.&lt;/p&gt;
    &lt;p&gt;With the history fixed, I ran another test. I got oscillation. The algorithm would improve for 12 iterations (9,495 → 5,527, a 42% improvement!), then spike back to 11,817, then drop to 7,252, then spike to 14,000. The pattern repeated forever. The problem was “adaptive hotset sizing”—when progress slowed, the algorithm would enlarge the set of nets being rerouted from 150 to 225, causing massive disruption. Fixing the hotset at 100 nets eliminated the oscillation.&lt;/p&gt;
    &lt;p&gt;Even with fixed hotsets, late-stage oscillation returned after iteration 15. Why? The present cost factor escalates exponentially: &lt;code&gt;pres_fac = 1.15^iteration&lt;/code&gt;. By iteration 19, present cost was 12.4× stronger than iteration 1, completely overwhelming history (which grows linearly). The solution: cap &lt;code&gt;pres_fac_max=8.0&lt;/code&gt; to keep history competitive throughout convergence.&lt;/p&gt;
    &lt;p&gt;PathFinder is designed for FPGAs, and each and every Xilinx XC3000 chip is the same as every other XC3000 chip. Configuring the parameters for an old Xilinx chip means every routing problem will probably converge on that particular chip. PCBs are different; every single PCB is different from every other PCB. There is no single set of history, pressure, and decay parameters that will work on every single PCB.&lt;/p&gt;
    &lt;p&gt;What I had to do was figure out these paramaters on the fly. So that’s what I did. Right now I’m using Board-adaptive parameters for the Manhattan router. Before beginning the PathFinder algorithm it analyzes the board in KiCad for the number of signal layers, how many nets will be routed, and how dense the set of nets are. It’s clunky, but it kinda works.&lt;/p&gt;
    &lt;p&gt;Where PathFinder was tuned once for each family of FPGAs, I’m auto-tuning it for the entire class of circuit boards. A huge backplane gets careful routing and an Arduino clone gets fast, aggressive routing. The hope is that both will converge – produce a valid routing solution – and maybe that works. Maybe it doesn’t. There’s still more work to do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Routing The Monster Board&lt;/head&gt;
    &lt;p&gt;After significant testing with “small” boards (actually 500+ net subsets of my large backplane, with 18 layers), I started work on the entire purpose of this project, the 8000+ net, 17000 pad monster board. There was one significant problem: it wouldn’t fit on my GPU. Admittedly, I only have a 16GB Nvidia 5080, but even this was far too small for the big backplane.&lt;/p&gt;
    &lt;p&gt;This led me to develop a ‘cloud routing solution’. It boils down to extracting a “OrthoRoute PCB file” from the OrthoRoute plugin. From there, I rent a Linux box with a GPU and run the autorouting algorithm with a headless mode. This produces an “OrthoRoute Solution file”. I import this back into KiCad by running the OrthoRoute plugin on my local machine, and importing the solution file, then pushing that to KiCad.&lt;/p&gt;
    &lt;p&gt;Here’s the result:&lt;/p&gt;
    &lt;p&gt;That’s it, that’s the finished board. A few specs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;44,233 blind and buried vias. 68,975 track segments.&lt;/item&gt;
      &lt;item&gt;Routed on an 80GB A100 GPU, rented on vast.io. The total VRAM required to route this board was 33.5 GB, so close to being under 32GB and allowing me to rent a cheaper GPU&lt;/item&gt;
      &lt;item&gt;Total time to route this board to completion was 41 hours. This is far better than the months it would have taken FreeRouting to route this board, but it’s still not fast.&lt;/item&gt;
      &lt;item&gt;The routing result is good but not great. A big problem is the DRC-awareness of the escape pad planning. There are traces that don’t quite overlap, but because of the geometry generated by the escape route planner they don’t pass a strict DRC. This could be fixed in future versions. There are also some overlapping traces in what PathFinder generated. Not many, but a few.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While the output from my autorouter isn’t perfect, no one would expect an autorouter to produce a perfect result, ready for production. It’s an autorouter, something you shouldn’t trust. Turning the result for OrthoRoute into a DRC-compliant board took a few days, but it was far easier than the intractable problem of eight thousand airwires I had at the beginning.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Future of OrthoRoute&lt;/head&gt;
    &lt;p&gt;I built this for one reason: to route my pathologically large backplane. Mission accomplished. And along the way, I accidentally built something more useful than I expected.&lt;/p&gt;
    &lt;p&gt;OrthoRoute proves that GPU-accelerated routing isn’t just theoretical, and that algorithms designed for routing FPGAs can be adapted to the more general class of circuit boards. It’s fast, too. The Manhattan lattice approach handles high-density designs that make traditional autorouters choke. And the PathFinder implementation converges in minutes on boards that would take hours or days with CPU-based approaches.&lt;/p&gt;
    &lt;p&gt;More importantly, the architecture is modular. The hard parts—KiCad IPC integration, GPU acceleration framework, DRC-aware routing space generation are done. Adding new routing strategies on top of this foundation is straightforward. Someone could implement different algorithms, optimize for specific board types, or extend it to handle flex PCBs.&lt;/p&gt;
    &lt;p&gt;The code is up on GitHub. I’m genuinely curious what other people will do with it. Want to add different routing strategies? Optimize for RF boards? Extend it to flex PCBs? PRs welcome, contributors welcome.&lt;/p&gt;
    &lt;p&gt;And yes, you should still manually route critical signals. But for dense digital boards with hundreds of mundane power and data nets? Let the GPU handle it while you grab coffee. That’s what autorouters are for.&lt;/p&gt;
    &lt;p&gt;Never trust the autorouter. But at least this one is fast.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bbenchoff.github.io/pages/OrthoRoute.html"/><published>2025-11-18T18:54:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45970519</id><title>Trying out Gemini 3 Pro with audio transcription and a new pelican benchmark</title><updated>2025-11-18T22:10:11.924152+00:00</updated><content>&lt;doc fingerprint="7dd78fcf8f3f9a91"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Trying out Gemini 3 Pro with audio transcription and a new pelican benchmark&lt;/head&gt;
    &lt;p&gt;18th November 2025&lt;/p&gt;
    &lt;p&gt;Google released Gemini 3 Pro today. Here’s the announcement from Sundar Pichai, Demis Hassabis, and Koray Kavukcuoglu, their developer blog announcement from Logan Kilpatrick, the Gemini 3 Pro Model Card, and their collection of 11 more articles. It’s a big release!&lt;/p&gt;
    &lt;p&gt;I had a few days of preview access to this model via AI Studio. The best way to describe it is that it’s Gemini 2.5 upgraded to match the leading rival models.&lt;/p&gt;
    &lt;p&gt;Gemini 3 has the same underlying characteristics as Gemini 2.5. The knowledge cutoff is the same (January 2025). It accepts 1 million input tokens, can output up to 64,000 tokens, and has multimodal inputs across text, images, audio, and video.&lt;/p&gt;
    &lt;head rend="h4"&gt;Benchmarks&lt;/head&gt;
    &lt;p&gt;Google’s own reported numbers (in the model card) show it scoring slightly higher against Claude 4.5 Sonnet and GPT-5.1 against most of the standard benchmarks. As always I’m waiting for independent confirmation, but I have no reason to believe those numbers are inaccurate.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pricing&lt;/head&gt;
    &lt;p&gt;It terms of pricing it’s a little more expensive than Gemini 2.5 but still cheaper than Claude Sonnet 4.5. Here’s how it fits in with those other leading models:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Input (per 1M tokens)&lt;/cell&gt;
        &lt;cell role="head"&gt;Output (per 1M tokens)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GPT-5.1&lt;/cell&gt;
        &lt;cell&gt;$1.25&lt;/cell&gt;
        &lt;cell&gt;$10.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gemini 2.5 Pro&lt;/cell&gt;
        &lt;cell&gt; ≤ 200k tokens: $1.25&lt;p&gt;&amp;gt; 200k tokens: $2.50&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; ≤ 200k tokens: $10.00&lt;p&gt;&amp;gt; 200k tokens: $15.00&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gemini 3 Pro&lt;/cell&gt;
        &lt;cell&gt; ≤ 200k tokens: $2.00&lt;p&gt;&amp;gt; 200k tokens: $4.00&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; ≤ 200k tokens: $12.00&lt;p&gt;&amp;gt; 200k tokens: $18.00&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude Sonnet 4.5&lt;/cell&gt;
        &lt;cell&gt; ≤ 200k tokens: $3.00&lt;p&gt;&amp;gt; 200k tokens: $6.00&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; ≤ 200k tokens: $15.00&lt;p&gt;&amp;gt; 200k tokens: $22.50&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Claude Opus 4.1&lt;/cell&gt;
        &lt;cell&gt;$15.00&lt;/cell&gt;
        &lt;cell&gt;$75.00&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Trying it out against a complex image&lt;/head&gt;
    &lt;p&gt;That screenshot of the benchmarks from above looked like a good test for Gemini 3’s multimodal support. I fed it that image URL and asked it to generate alt text for the image:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;llm -m gemini-3-pro-preview -a https://static.simonwillison.net/static/2025/gemini-3-benchmarks.jpg 'Alt text for this image, include all figures and make them comprehensible to a screen reader user'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here’s what I got back:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A comprehensive table comparing four AI models across various performance benchmarks. The columns listed are: Benchmark Name, Description, Gemini 3 Pro (highlighted in blue), Gemini 2.5 Pro, Claude Sonnet 4.5, and GPT-5.1.&lt;/p&gt;
      &lt;p&gt;Humanity’s Last Exam (Academic reasoning)&lt;/p&gt;
      &lt;item&gt;No tools: Gemini 3 Pro 37.5%, Gemini 2.5 Pro 21.6%, Claude Sonnet 4.5 13.7%, GPT-5.1 26.5%.&lt;/item&gt;
      &lt;item&gt;With search and code execution: Gemini 3 Pro 45.8% (others have no data).&lt;/item&gt;
      &lt;p&gt;ARC-AGI-2 (Visual reasoning puzzles; ARC Prize Verified)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 31.1%, Gemini 2.5 Pro 4.9%, Claude Sonnet 4.5 13.6%, GPT-5.1 17.6%.&lt;/item&gt;
      &lt;p&gt;GPQA Diamond (Scientific knowledge; No tools)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 91.9%, Gemini 2.5 Pro 86.4%, Claude Sonnet 4.5 83.4%, GPT-5.1 88.1%.&lt;/item&gt;
      &lt;p&gt;AIME 2025 (Mathematics)&lt;/p&gt;
      &lt;item&gt;No tools: Gemini 3 Pro 95.0%, Gemini 2.5 Pro 88.0%, Claude Sonnet 4.5 87.0%, GPT-5.1 94.0%.&lt;/item&gt;
      &lt;item&gt;With code execution: Gemini 3 Pro 100%, Claude Sonnet 4.5 100%.&lt;/item&gt;
      &lt;p&gt;MathArena Apex (Challenging Math Contest problems)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 23.4%, Gemini 2.5 Pro 0.5%, Claude Sonnet 4.5 1.6%, GPT-5.1 1.0%.&lt;/item&gt;
      &lt;p&gt;MMMU-Pro (Multimodal understanding and reasoning)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 81.0%, Gemini 2.5 Pro 68.0%, Claude Sonnet 4.5 68.0%, GPT-5.1 76.0%.&lt;/item&gt;
      &lt;p&gt;ScreenSpot-Pro (Screen understanding)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 72.7%, Gemini 2.5 Pro 11.4%, Claude Sonnet 4.5 36.2%, GPT-5.1 3.5%.&lt;/item&gt;
      &lt;p&gt;CharXiv Reasoning (Information synthesis from complex charts)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 81.4%, Gemini 2.5 Pro 69.6%, Claude Sonnet 4.5 68.5%, GPT-5.1 69.5%.&lt;/item&gt;
      &lt;p&gt;OmniDocBench 1.5 (OCR; Overall Edit Distance, lower is better)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 0.115, Gemini 2.5 Pro 0.145, Claude Sonnet 4.5 0.145, GPT-5.1 0.147.&lt;/item&gt;
      &lt;p&gt;Video-MMMU (Knowledge acquisition from videos)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 87.6%, Gemini 2.5 Pro 83.6%, Claude Sonnet 4.5 77.8%, GPT-5.1 80.4%.&lt;/item&gt;
      &lt;p&gt;LiveCodeBench Pro (Competitive coding problems; Elo Rating, higher is better)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 2,439; Gemini 2.5 Pro 1,775; Claude Sonnet 4.5 1,418; GPT-5.1 2,243.&lt;/item&gt;
      &lt;p&gt;Terminal-Bench 2.0 (Agentic terminal coding; Terminus-2 agent)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 54.2%, Gemini 2.5 Pro 32.6%, Claude Sonnet 4.5 42.8%, GPT-5.1 47.6%.&lt;/item&gt;
      &lt;p&gt;SWE-Bench Verified (Agentic coding; Single attempt)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 76.2%, Gemini 2.5 Pro 59.6%, Claude Sonnet 4.5 77.2%, GPT-5.1 76.3%.&lt;/item&gt;
      &lt;p&gt;t2-bench (Agentic tool use)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 85.4%, Gemini 2.5 Pro 54.9%, Claude Sonnet 4.5 84.7%, GPT-5.1 80.2%.&lt;/item&gt;
      &lt;p&gt;Vending-Bench 2 (Long-horizon agentic tasks; Net worth (mean), higher is better)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro $5,478.16; Gemini 2.5 Pro $573.64; Claude Sonnet 4.5 $3,838.74; GPT-5.1 $1,473.43.&lt;/item&gt;
      &lt;p&gt;FACTS Benchmark Suite (Held out internal grounding, parametric, MM, and search retrieval benchmarks)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 70.5%, Gemini 2.5 Pro 63.4%, Claude Sonnet 4.5 50.4%, GPT-5.1 50.8%.&lt;/item&gt;
      &lt;p&gt;SimpleQA Verified (Parametric knowledge)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 72.1%, Gemini 2.5 Pro 54.5%, Claude Sonnet 4.5 29.3%, GPT-5.1 34.9%.&lt;/item&gt;
      &lt;p&gt;MMMLU (Multilingual Q&amp;amp;A)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 91.8%, Gemini 2.5 Pro 89.5%, Claude Sonnet 4.5 89.1%, GPT-5.1 91.0%.&lt;/item&gt;
      &lt;p&gt;Global PIQA (Commonsense reasoning across 100 Languages and Cultures)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 93.4%, Gemini 2.5 Pro 91.5%, Claude Sonnet 4.5 90.1%, GPT-5.1 90.9%.&lt;/item&gt;
      &lt;p&gt;MRCR v2 (8-needle) (Long context performance)&lt;/p&gt;
      &lt;item&gt;128k (average): Gemini 3 Pro 77.0%, Gemini 2.5 Pro 58.0%, Claude Sonnet 4.5 47.1%, GPT-5.1 61.6%.&lt;/item&gt;
      &lt;item&gt;1M (pointwise): Gemini 3 Pro 26.3%, Gemini 2.5 Pro 16.4%, Claude Sonnet 4.5 (not supported), GPT-5.1 (not supported).&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;I have not checked every line of this but a loose spot-check looks accurate to me.&lt;/p&gt;
    &lt;p&gt;That prompt took 1,105 input and 3,901 output tokens, at a cost of 5.6824 cents.&lt;/p&gt;
    &lt;p&gt;I ran this follow-up prompt:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;llm -c 'Convert to JSON'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;You can see the full output here, which starts like this:&lt;/p&gt;
    &lt;code&gt;{
  "metadata": {
    "columns": [
      "Benchmark",
      "Description",
      "Gemini 3 Pro",
      "Gemini 2.5 Pro",
      "Claude Sonnet 4.5",
      "GPT-5.1"
    ]
  },
  "benchmarks": [
    {
      "name": "Humanity's Last Exam",
      "description": "Academic reasoning",
      "sub_results": [
        {
          "condition": "No tools",
          "gemini_3_pro": "37.5%",
          "gemini_2_5_pro": "21.6%",
          "claude_sonnet_4_5": "13.7%",
          "gpt_5_1": "26.5%"
        },
        {
          "condition": "With search and code execution",
          "gemini_3_pro": "45.8%",
          "gemini_2_5_pro": null,
          "claude_sonnet_4_5": null,
          "gpt_5_1": null
        }
      ]
    },&lt;/code&gt;
    &lt;head rend="h4"&gt;Analyzing a city council meeting&lt;/head&gt;
    &lt;p&gt;To try it out against an audio file I extracted the 3h33m of audio from the video Half Moon Bay City Council Meeting—November 4, 2025. I used &lt;code&gt;yt-dlp&lt;/code&gt; to get that audio:&lt;/p&gt;
    &lt;code&gt;yt-dlp -x --audio-format m4a 'https://www.youtube.com/watch?v=qgJ7x7R6gy0'&lt;/code&gt;
    &lt;p&gt;That gave me a 74M m4a file, which I ran through Gemini 3 Pro like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;llm -m gemini-3-pro-preview -a /tmp/HMBCC\ 11⧸4⧸25\ -\ Half\ Moon\ Bay\ City\ Council\ Meeting\ -\ November\ 4,\ 2025\ \[qgJ7x7R6gy0\].m4a 'Output a Markdown transcript of this meeting. Include speaker names and timestamps. Start with an outline of the key meeting sections, each with a title and summary and timestamp and list of participating names. Note in bold if anyone raised their voices, interrupted each other or had disagreements. Then follow with the full transcript.'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;That failed with an “Internal error encountered” message, so I shrunk the file down to a more manageable 38MB using &lt;code&gt;ffmpeg&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;ffmpeg -i "/private/tmp/HMB.m4a" -ac 1 -ar 22050 -c:a aac -b:a 24k "/private/tmp/HMB_compressed.m4a"&lt;/code&gt;
    &lt;p&gt;Then ran it again like this (for some reason I had to use &lt;code&gt;--attachment-type&lt;/code&gt; this time):&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;llm -m gemini-3-pro-preview --attachment-type /tmp/HMB_compressed.m4a 'audio/aac' 'Output a Markdown transcript of this meeting. Include speaker names and timestamps. Start with an outline of the key meeting sections, each with a title and summary and timestamp and list of participating names. Note in bold if anyone raised their voices, interrupted each other or had disagreements. Then follow with the full transcript.'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;This time it worked! The full output is here, but it starts like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Here is the transcript of the Half Moon Bay City Council meeting.&lt;/p&gt;
      &lt;head&gt;Meeting Outline&lt;/head&gt;
      &lt;p&gt;1. Call to Order, Updates, and Public Forum&lt;/p&gt;
      &lt;item&gt;Summary: Mayor Brownstone calls the meeting to order. City Manager Chidester reports no reportable actions from the closed session. Announcements are made regarding food insecurity volunteers and the Diwali celebration. During the public forum, Councilmember Penrose (speaking as a citizen) warns against autocracy. Citizens speak regarding lease agreements, downtown maintenance, local music events, and homelessness outreach statistics.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:00:00—00:13:25&lt;/item&gt;
      &lt;item&gt;Participants: Mayor Brownstone, Matthew Chidester, Irma Acosta, Deborah Penrose, Jennifer Moore, Sandy Vella, Joaquin Jimenez, Anita Rees.&lt;/item&gt;
      &lt;p&gt;2. Consent Calendar&lt;/p&gt;
      &lt;item&gt;Summary: The Council approves minutes from previous meetings and a resolution authorizing a licensing agreement for Seahorse Ranch. Councilmember Johnson corrects a pull request regarding abstentions on minutes.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:13:25—00:15:15&lt;/item&gt;
      &lt;item&gt;Participants: Mayor Brownstone, Councilmember Johnson, Councilmember Penrose, Vice Mayor Ruddick, Councilmember Nagengast.&lt;/item&gt;
      &lt;p&gt;3. Ordinance Introduction: Commercial Vitality (Item 9A)&lt;/p&gt;
      &lt;item&gt;Summary: Staff presents a new ordinance to address neglected and empty commercial storefronts, establishing maintenance and display standards. Councilmembers discuss enforcement mechanisms, window cleanliness standards, and the need for objective guidance documents to avoid subjective enforcement.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:15:15—00:30:45&lt;/item&gt;
      &lt;item&gt;Participants: Karen Decker, Councilmember Johnson, Councilmember Nagengast, Vice Mayor Ruddick, Councilmember Penrose.&lt;/item&gt;
      &lt;p&gt;4. Ordinance Introduction: Building Standards &amp;amp; Electrification (Item 9B)&lt;/p&gt;
      &lt;item&gt;Summary: Staff introduces updates to the 2025 Building Code. A major change involves repealing the city’s all-electric building requirement due to the 9th Circuit Court ruling (California Restaurant Association v. City of Berkeley). Public speaker Mike Ferreira expresses strong frustration and disagreement with “unelected state agencies” forcing the City to change its ordinances.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:30:45—00:45:00&lt;/item&gt;
      &lt;item&gt;Participants: Ben Corrales, Keith Weiner, Joaquin Jimenez, Jeremy Levine, Mike Ferreira, Councilmember Penrose, Vice Mayor Ruddick.&lt;/item&gt;
      &lt;p&gt;5. Housing Element Update &amp;amp; Adoption (Item 9C)&lt;/p&gt;
      &lt;item&gt;Summary: Staff presents the 5th draft of the Housing Element, noting State HCD requirements to modify ADU allocations and place a measure on the ballot regarding the “Measure D” growth cap. There is significant disagreement from Councilmembers Ruddick and Penrose regarding the State’s requirement to hold a ballot measure. Public speakers debate the enforceability of Measure D. Mike Ferreira interrupts the vibe to voice strong distaste for HCD’s interference in local law. The Council votes to adopt the element but strikes the language committing to a ballot measure.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:45:00—01:05:00&lt;/item&gt;
      &lt;item&gt;Participants: Leslie (Staff), Joaquin Jimenez, Jeremy Levine, Mike Ferreira, Councilmember Penrose, Vice Mayor Ruddick, Councilmember Johnson.&lt;/item&gt;
      &lt;head&gt;Transcript&lt;/head&gt;
      &lt;p&gt;Mayor Brownstone [00:00:00] Good evening everybody and welcome to the November 4th Half Moon Bay City Council meeting. As a reminder, we have Spanish interpretation services available in person and on Zoom.&lt;/p&gt;
      &lt;p&gt;Victor Hernandez (Interpreter) [00:00:35] Thank you, Mr. Mayor, City Council, all city staff, members of the public. [Spanish instructions provided regarding accessing the interpretation channel on Zoom and in the room.] Thank you very much.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Those first two lines of the transcript already illustrate something interesting here: Gemini 3 Pro chose NOT to include the exact text of the Spanish instructions, instead summarizing them as “[Spanish instructions provided regarding accessing the interpretation channel on Zoom and in the room.]”.&lt;/p&gt;
    &lt;p&gt;I haven’t spot-checked the entire 3hr33m meeting, but I’ve confirmed that the timestamps do not line up. The transcript closes like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Mayor Brownstone [01:04:00] Meeting adjourned. Have a good evening.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That actually happens at 3h31m5s and the mayor says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Okay. Well, thanks everybody, members of the public for participating. Thank you for staff. Thank you to fellow council members. This meeting is now adjourned. Have a good evening.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I’m disappointed about the timestamps, since mismatches there make it much harder to jump to the right point and confirm that the summarized transcript is an accurate representation of what was said.&lt;/p&gt;
    &lt;p&gt;This took 320,087 input tokens and 7,870 output tokens, for a total cost of $1.42.&lt;/p&gt;
    &lt;head rend="h4"&gt;And a new pelican benchmark&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro has a new concept of a “thinking level” which can be set to low or high (and defaults to high). I tried my classic Generate an SVG of a pelican riding a bicycle prompt at both levels.&lt;/p&gt;
    &lt;p&gt;Here’s low—Gemini decided to add a jaunty little hat (with a comment in the SVG that says &lt;code&gt;&amp;lt;!-- Hat (Optional Fun Detail) --&amp;gt;&lt;/code&gt;):&lt;/p&gt;
    &lt;p&gt;And here’s high. This is genuinely an excellent pelican, and the bicycle frame is at least the correct shape:&lt;/p&gt;
    &lt;p&gt;Honestly though, my pelican benchmark is beginning to feel a little bit too basic. I decided to upgrade it. Here’s v2 of the benchmark, which I plan to use going forward:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;Generate an SVG of a California brown pelican riding a bicycle. The bicycle must have spokes and a correctly shaped bicycle frame. The pelican must have its characteristic large pouch, and there should be a clear indication of feathers. The pelican must be clearly pedaling the bicycle. The image should show the full breeding plumage of the California brown pelican.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;For reference, here’s a photo I took of a California brown pelican recently (sadly without a bicycle):&lt;/p&gt;
    &lt;p&gt;Here’s Gemini 3 Pro’s attempt at high thinking level for that new prompt:&lt;/p&gt;
    &lt;p&gt;And for good measure, here’s that same prompt against GPT-5.1—which produced this dumpy little fellow:&lt;/p&gt;
    &lt;p&gt;And Claude Sonnet 4.5, which didn’t do quite as well:&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What happens if AI labs train for pelicans riding bicycles? - 13th November 2025&lt;/item&gt;
      &lt;item&gt;Reverse engineering Codex CLI to get GPT-5-Codex-Mini to draw me a pelican - 9th November 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://simonwillison.net/2025/Nov/18/gemini-3/"/><published>2025-11-18T19:05:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45971601</id><title>Oracle is underwater on its 'astonishing' $300B OpenAI deal</title><updated>2025-11-18T22:10:11.439784+00:00</updated><content>&lt;doc fingerprint="6611325292f6e3b9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Oracle is already underwater on its ‘astonishing’ $300bn OpenAI deal&lt;/head&gt;
    &lt;p&gt;Simply sign up to the Technology sector myFT Digest -- delivered directly to your inbox.&lt;/p&gt;
    &lt;p&gt;It’s too soon to be talking about the Curse of OpenAI, but we’re going to anyway.&lt;/p&gt;
    &lt;p&gt;Since September 10, when Oracle announced a $300bn deal with the chatbot maker, its stock has shed $315bn* in market value:&lt;/p&gt;
    &lt;p&gt;OK, yes, it’s a gross simplification to just look at market cap. But equivalents to Oracle shares are little changed over the same period (Nasdaq Composite, Microsoft, Dow Jones US Software Index), so the $60bn loss figure is not entirely wrong. Oracle’s “astonishing quarter” really has cost it nearly as much as one General Motors, or two Kraft Heinz.&lt;/p&gt;
    &lt;p&gt;Investor unease stems from Big Red betting a debt-financed data farm on OpenAI, as MainFT reported last week. We’ve nothing much to add to that report other than the below charts showing how much Oracle has, in effect, become OpenAI’s US public market proxy:&lt;/p&gt;
    &lt;p&gt;The theory goes that OpenAI is in a rush to &lt;del&gt;define&lt;/del&gt; discover AGI, and Oracle is uniquely able to scale the compute capacity it needs. Oracle promises the lowest upfront costs and fastest path to income generation among the hyperscalers because it’s a data centre tenant rather than the landlord. &lt;/p&gt;
    &lt;p&gt;Alternatively, Oracle doesn’t have as much operating profit to burn as its competitors, so is throwing everything it can at supporting its one big customer in exchange for an IOU:&lt;/p&gt;
    &lt;p&gt;At an analyst day last month in Las Vegas, Oracle said it was aiming for cloud computing revenue of $166bn by 2030:&lt;/p&gt;
    &lt;p&gt;To get there, Oracle’s capex budget for the current financial year ending May is $35bn. The consensus has annual capex levelling out at around $80bn a year in 2029, after which revenues continue to ramp:&lt;/p&gt;
    &lt;p&gt;And from 2027, the majority of revenue would be coming from OpenAI:&lt;/p&gt;
    &lt;p&gt;But Oracle’s net debt is already at 2.5 times ebitda, having more than doubled since 2021, and it’s expected to nearly double again by 2030. Cash flow is forecast to remain negative for five straight years:&lt;/p&gt;
    &lt;p&gt;So while the OpenAI agreement has been more than written off the equity, the risk of unfunded expansion remains and the cost of hedging Oracle debt is at a three-year high.&lt;/p&gt;
    &lt;p&gt;We need to add the usual warnings: Credit-default-swap liquidity isn’t great; the increased demand for Oracle CDS comes after $18bn of bond sales in September; a CDS premium in the low 100 basis points isn’t that exciting; and some firms taking the other side of the trade are no mugs. Still, pointy.&lt;/p&gt;
    &lt;p&gt;Beyond the charts, a broader question relates to whether an OpenAI deal is still worth announcing.&lt;/p&gt;
    &lt;p&gt;A few months ago, any kind of agreement with OpenAI could make a share price go up. OpenAI did very nicely out of its power to reflect glory, most notably in October when it took AMD warrants as part of a chip deal that bumped share price by 24 per cent.&lt;/p&gt;
    &lt;p&gt;But Oracle is not the only laggard. Broadcom and Amazon are both down following OpenAI deal news, while Nvidia’s barely changed since its investment agreement in September. Without a share price lift, what’s the point? A combined trillion dollars of AI capex might look like commitment, but investment fashions are fickle.&lt;/p&gt;
    &lt;p&gt;* Calculation and graph updated at 11am GMT for shares outstanding, and text tweaked at 2pm GMT to reflect a less clickbait headline&lt;/p&gt;
    &lt;p&gt;Further reading:&lt;lb/&gt;— Oracle’s astonishing jam-tomorrow OpenAI trade (FTAV)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ft.com/content/064bbca0-1cb2-45ab-85f4-25fdfc318d89"/><published>2025-11-18T20:29:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45971726</id><title>GitHub: Git operation failures</title><updated>2025-11-18T22:10:11.042033+00:00</updated><content>&lt;doc fingerprint="7096aa564e934ff0"&gt;
  &lt;main&gt;
    &lt;p&gt;This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:59 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Git Operations is operating normally.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:56 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are seeing full recovery after rolling out the fix and all services are operational.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:55 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Codespaces is operating normally.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:55 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We have shipped a fix and are seeing recovery in some areas and will continue to provide updates.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:36 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We have identified the likely cause of the incident and are working on a fix. We will provide another update as we get closer to deploying the fix.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:27 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Codespaces is experiencing degraded availability. We are continuing to investigate.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:25 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are currently investigating failures on all Git operations, including both SSH and HTTP.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:11 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are seeing failures for some git http operations and are investigating&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 20:52 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Git Operations is experiencing degraded availability. We are continuing to investigate.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 20:39 UTC&lt;/p&gt;
    &lt;p&gt;Investigating&lt;/p&gt;
    &lt;p&gt;We are currently investigating this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 20:39 UTC&lt;/p&gt;
    &lt;p&gt;This incident affected: Git Operations and Codespaces.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.githubstatus.com/incidents/5q7nmlxz30sk"/><published>2025-11-18T20:40:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45972390</id><title>Bild AI (YC W25) Is Hiring: Make Housing Affordable</title><updated>2025-11-18T22:10:10.228990+00:00</updated><content>&lt;doc fingerprint="19b82e91a7888e36"&gt;
  &lt;main&gt;
    &lt;p&gt;AI that understands construction blueprints&lt;/p&gt;
    &lt;p&gt;Puneet and I (Roop) founded Bild AI to tackle the mess that is blueprint reading, cost estimation, and permit applications in construction. It's a tough technical problem that requires the newest CV and AI approaches, and we’re impact-driven to make it more efficient to build more houses, hospitals, and schools. Featured on Business Insider.&lt;/p&gt;
    &lt;p&gt;Bild AI is an early-stage startup with a ton of really difficult technical challenges to solve. We're building blueprint understanding with a model-garden approach, so there is a lots of ground to break. We raised from the top VCs in the world before demo day and have a customer-obsessed approach to product development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/bild-ai/jobs/m2ilR5L-founding-engineer-applied-ai"/><published>2025-11-18T21:29:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45972519</id><title>Blender 5.0 Released</title><updated>2025-11-18T22:10:09.995017+00:00</updated><content/><link href="https://www.blender.org/download/releases/5-0/"/><published>2025-11-18T21:39:18+00:00</published></entry></feed>