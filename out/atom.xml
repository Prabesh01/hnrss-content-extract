<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-21T16:38:47.265348+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45319399</id><title>The bloat of edge-case first libraries</title><updated>2025-09-21T16:39:16.164909+00:00</updated><content>&lt;doc fingerprint="6e55d2411b082c98"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The bloat of edge-case first libraries&lt;/head&gt;
    &lt;p&gt;This is just some of what I’ve been pondering recently - particularly in terms of how we ended up with such overly-granular dependency trees.&lt;/p&gt;
    &lt;p&gt;I think we’ve ended up with many libraries in our ecosystem which are edge-case-first, the opposite to what I’d expect. I’ll give a few examples and some thoughts around this, mostly in the hope we can start to trim some of it away.&lt;/p&gt;
    &lt;head rend="h1"&gt;The problem&lt;/head&gt;
    &lt;p&gt;I believe a lot of the questionably small libraries hiding in our deep dependency trees are a result of over-engineering for inputs and edge cases we’ve probably never seen.&lt;/p&gt;
    &lt;p&gt;For example, say we’re building a &lt;code&gt;clamp&lt;/code&gt; function:&lt;/p&gt;
    &lt;code&gt;export function clamp(value: number, min: number, max: number): number {
  return Math.min(Math.max(value, min), max);
}
&lt;/code&gt;
    &lt;p&gt;Pretty simple!&lt;/p&gt;
    &lt;p&gt;What if someone passes nonsensical ranges? Let’s handle that.&lt;/p&gt;
    &lt;code&gt;export function clamp(value: number, min: number, max: number): number {
  if (min &amp;gt; max) {
    throw new Error('min must be less than or equal to max');
  }
  return Math.min(Math.max(value, min), max);
}
&lt;/code&gt;
    &lt;p&gt;This is probably as far as I’d go. But let’s over-engineer - what if someone passes a number-like string?&lt;/p&gt;
    &lt;code&gt;export function clamp(value: number | string, min: number | string, max: number | string): number {
  if (typeof value === 'string' &amp;amp;&amp;amp; Number.isNaN(Number(value))) {
    throw new Error('value must be a number or a number-like string');
  }
  if (typeof min === 'string' &amp;amp;&amp;amp; Number.isNaN(Number(min))) {
    throw new Error('min must be a number or a number-like string');
  }
  if (typeof max === 'string' &amp;amp;&amp;amp; Number.isNaN(Number(max))) {
    throw new Error('max must be a number or a number-like string');
  }
  if (Number(min) &amp;gt; Number(max)) {
    throw new Error('min must be less than or equal to max');
  }
  return Math.min(Math.max(value, min), max);
}
&lt;/code&gt;
    &lt;p&gt;At this point, it seems clear to me we’ve just poorly designed our function. It solely exists to clamp numbers, so why would we accept strings?&lt;/p&gt;
    &lt;p&gt;But hey, let’s go further! What if other libraries also want to accept such loose inputs? Let’s extract this into a separate library:&lt;/p&gt;
    &lt;code&gt;import isNumber from 'is-number';

export function clamp(value: number | string, min: number | string, max: number | string): number {
  if (!isNumber(value)) {
    throw new Error('value must be a number or a number-like string');
  }
  if (!isNumber(min)) {
    throw new Error('min must be a number or a number-like string');
  }
  if (!isNumber(max)) {
    throw new Error('max must be a number or a number-like string');
  }
  if (Number(min) &amp;gt; Number(max)) {
    throw new Error('min must be less than or equal to max');
  }
  return Math.min(Math.max(value, min), max);
}
&lt;/code&gt;
    &lt;p&gt;Whoops! We’ve just created the infamous &lt;code&gt;is-number&lt;/code&gt; library!&lt;/p&gt;
    &lt;head rend="h1"&gt;How it should be&lt;/head&gt;
    &lt;p&gt;This, in my opinion, is poor technical design we’ve all ended up dealing with over the years. Carrying the baggage of these overly-granular libraries that exist to handle edge cases we’ve probably never encountered.&lt;/p&gt;
    &lt;p&gt;I think it should have been:&lt;/p&gt;
    &lt;code&gt;export function clamp(value: number, min: number, max: number): number {
  return Math.min(Math.max(value, min), max);
}
&lt;/code&gt;
    &lt;p&gt;Maybe with some &lt;code&gt;min &amp;lt;= max&lt;/code&gt; validation, but even that is debatable. At this point, you may as well inline the &lt;code&gt;Math.min(Math.max(...))&lt;/code&gt; expression instead of using a dependency.&lt;/p&gt;
    &lt;p&gt;We should be able to define our functions to accept the inputs they are designed for, and not try to handle every possible edge case.&lt;/p&gt;
    &lt;p&gt;There are two things at play here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data types&lt;/item&gt;
      &lt;item&gt;Values&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A well designed library would assume the right data types have been passed in, but may validate that the values make sense (e.g. &lt;code&gt;min&lt;/code&gt; is less than or equal to &lt;code&gt;max&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;These over-engineered libraries have decided to implement both at runtime - essentially run-time type checking and value validation. One could argue that this is just a result of building in the pre-TypeScript era, but that still doesn’t justify the overly specific value validation (e.g. the real &lt;code&gt;is-number&lt;/code&gt; also checks that it is finite).&lt;/p&gt;
    &lt;head rend="h1"&gt;What we shouldn’t do&lt;/head&gt;
    &lt;p&gt;We shouldn’t build edge-case-first libraries, i.e. those which solve for edge cases we have yet to encounter or are unlikely to ever encounter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: &lt;code&gt;is-arrayish&lt;/code&gt; (76M downloads/week)&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;is-arrayish&lt;/code&gt; library determines if a value is an &lt;code&gt;Array&lt;/code&gt; or behaves like one.&lt;/p&gt;
    &lt;p&gt;There will be some edge cases where this matters a lot, where we want to accept something we can index into but don’t care if it is a real &lt;code&gt;Array&lt;/code&gt; or not.&lt;/p&gt;
    &lt;p&gt;However, the common use case clearly will not be that and we could’ve just used &lt;code&gt;Array.isArray()&lt;/code&gt; all along.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: &lt;code&gt;is-number&lt;/code&gt; (90M downloads/week)&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;is-number&lt;/code&gt; library determines if a value is a positive, finite number or number-like string (maybe we should name it &lt;code&gt;is-positive-finite-number&lt;/code&gt; to be more accurate).&lt;/p&gt;
    &lt;p&gt;Again, there will be edge cases where we want to deal with number-like strings or we want to validate that a number is within a range (e.g. finite).&lt;/p&gt;
    &lt;p&gt;The common use case will not be this. The common use case will be that we want to check &lt;code&gt;typeof n === 'number'&lt;/code&gt; and be done with it.&lt;/p&gt;
    &lt;p&gt;For those edge cases where we want to additionally validate what kind of number it is, we could use a library (but one which exists for the validation, not for the type check).&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: &lt;code&gt;pascalcase&lt;/code&gt; (9.7M downloads/week)&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;pascalcase&lt;/code&gt; library transforms text to PascalCase.&lt;/p&gt;
    &lt;p&gt;It has 1 dependency (&lt;code&gt;camelcase&lt;/code&gt;) and accepts a variety of input types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;strings&lt;/item&gt;
      &lt;item&gt;null&lt;/item&gt;
      &lt;item&gt;undefined&lt;/item&gt;
      &lt;item&gt;arrays of strings&lt;/item&gt;
      &lt;item&gt;functions&lt;/item&gt;
      &lt;item&gt;arbitrary objects with &lt;code&gt;toString&lt;/code&gt;methods&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In reality, almost every user will be passing a &lt;code&gt;string&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: &lt;code&gt;is-regexp&lt;/code&gt; (10M downloads/week)&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;is-regexp&lt;/code&gt; library checks if a value is a &lt;code&gt;RegExp&lt;/code&gt; object, and supports cross-realm values.&lt;/p&gt;
    &lt;p&gt;In reality, almost every user will be passing a &lt;code&gt;RegExp&lt;/code&gt; object, and not one from another realm.&lt;/p&gt;
    &lt;p&gt;For context, cross-realm values can happen when you retrieve a value from an &lt;code&gt;iframe&lt;/code&gt; or VM for example:&lt;/p&gt;
    &lt;code&gt;const iframe = document.createElement('iframe');
iframe.contentWindow.RegExp === RegExp; // false

const iframeRegex = iframe.contentWindow.someRegexp;

iframeRegex instanceof RegExp; // false
isRegex(iframeRegex); // true
&lt;/code&gt;
    &lt;p&gt;This is indeed useful, and I do support this myself in chai (which I maintain). However, this is an edge case most libraries don’t need to care about.&lt;/p&gt;
    &lt;head rend="h1"&gt;What we should do&lt;/head&gt;
    &lt;p&gt;We should build libraries which solve the common use case and make assumptions about the input types they will be given.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: scule (1.8M downloads/week)&lt;/head&gt;
    &lt;p&gt;scule is a library for transforming casing of text (e.g. camel case, etc).&lt;/p&gt;
    &lt;p&gt;It only accepts inputs it is designed for (strings and arrays of strings) and has zero dependencies.&lt;/p&gt;
    &lt;p&gt;In most of the functions it exports, it assumes valid input data types.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example: dlv (14.9M downloads/week)&lt;/head&gt;
    &lt;p&gt;dlv is a library for deep property access.&lt;/p&gt;
    &lt;p&gt;It only accepts strings and arrays of strings as the path to access, and assumes this (i.e. does no validation).&lt;/p&gt;
    &lt;head rend="h1"&gt;Validation is important&lt;/head&gt;
    &lt;p&gt;Validation is important, and I want to be clear that I’m not saying we should stop validating our data.&lt;/p&gt;
    &lt;p&gt;However, we should usually be validating the data in the project that owns it (e.g. at the app level), and not in every library that later consumes it as input.&lt;/p&gt;
    &lt;p&gt;Deep dependencies applying validation like this actually shift the burden from where it belongs (at data boundaries) to deep in the dependency tree.&lt;/p&gt;
    &lt;p&gt;Often at this point, it is invisible to the consumer of the library.&lt;/p&gt;
    &lt;p&gt;How many people are passing values into &lt;code&gt;is-number&lt;/code&gt; (via other libraries), not realising it will prevent them from using negative numbers and &lt;code&gt;Infinity&lt;/code&gt;?&lt;/p&gt;
    &lt;head rend="h1"&gt;A note on overly-granular libraries&lt;/head&gt;
    &lt;p&gt;This post isn’t about overly-granular libraries in general, but I’d like to briefly mention them for visibility.&lt;/p&gt;
    &lt;p&gt;An overly-granular library is one where someone took a useful library and split it up into an almost atomic-level of granularity.&lt;/p&gt;
    &lt;p&gt;Some examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;shebang-regex&lt;/code&gt;- 2LOC, does the same as&lt;code&gt;startsWith('#!')&lt;/code&gt;, 86M downloads/week&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;is-whitespace&lt;/code&gt;- 7LOC, checks if a string is only whitespace, 1M downloads/week&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;is-npm&lt;/code&gt;- 8LOC, checks&lt;code&gt;npm_config_user_agent&lt;/code&gt;or&lt;code&gt;npm_package_json&lt;/code&gt;are set, 7M downloads/week&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a personal preference some maintainers clearly prefer. The thought seems to be that by having atomic libraries, you can easily build your next library mostly from the existing building blocks you have.&lt;/p&gt;
    &lt;p&gt;I don’t really agree with this and think downloading a package for &lt;code&gt;#!&lt;/code&gt; 86 million times a week is a bit much.&lt;/p&gt;
    &lt;head rend="h1"&gt;What can be done about this?&lt;/head&gt;
    &lt;p&gt;The e18e community is already tackling a lot of this by contributing performance improvements across the ecosystem, including removing and replacing dependencies with more modern, performant ones.&lt;/p&gt;
    &lt;p&gt;Through these efforts, there’s already a useful list of replacements and an ESLint plugin.&lt;/p&gt;
    &lt;head rend="h2"&gt;As a maintainer&lt;/head&gt;
    &lt;p&gt;If you’re maintaining a library, it would be worth reviewing your dependencies to see if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Any are replaceable by native functionality these days (e.g. &lt;code&gt;Array.isArray&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Any are replaceable by smaller, less granular and/or more performant alternatives (e.g. &lt;code&gt;scule&lt;/code&gt;instead of&lt;code&gt;pascalcase&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Any are redundant if you make more assumptions about input types&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tools like npmgraph can help you visualise your dependency tree to make this task easier.&lt;/p&gt;
    &lt;p&gt;Also, being stricter around input types will allow you to reduce a lot of code and dependencies.&lt;/p&gt;
    &lt;p&gt;If you can assume the data being passed in is the correct type, you can leave validation up to the consumer.&lt;/p&gt;
    &lt;head rend="h2"&gt;As a user&lt;/head&gt;
    &lt;p&gt;Keep a close eye on your dependencies (both deep and direct), and what alternatives are available to your direct dependencies.&lt;/p&gt;
    &lt;p&gt;Often, it is easy to stick with a dependency from long ago and forget to re-visit it one day in case there is a better way. Many of these packages are possible natively, or have more modern alternatives.&lt;/p&gt;
    &lt;p&gt;Useful tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;npmgraph for visualising your dependency tree&lt;/item&gt;
      &lt;item&gt;node-modules.dev for visualising your dependencies and lots of useful meta data&lt;/item&gt;
      &lt;item&gt;Dependabot for keeping your dependencies up to date&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On the topic of data, it is also worth ensuring validation happens at data boundaries rather than being delegated to various dependencies. Try to validate the type and value up front, before passing into dependencies.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Most of these libraries exist to handle edge cases that do certainly exist. However, we are all paying the cost of that rather than only those who need to support those edge cases.&lt;/p&gt;
    &lt;p&gt;This is the wrong way around. Libraries should implement the main use case, and alternatives (or plugins) can exist to provide the edge cases the minority needs.&lt;/p&gt;
    &lt;p&gt;We should all be more aware of what is in our dependency tree, and should push for more concise, lighter libraries.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://43081j.com/2025/09/bloat-of-edge-case-libraries"/><published>2025-09-21T02:09:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45319690</id><title>iFixit iPhone Air teardown</title><updated>2025-09-21T16:39:15.912306+00:00</updated><content>&lt;doc fingerprint="b569c1074bc717dd"&gt;
  &lt;main&gt;
    &lt;p&gt;To be honest, we were holding our breath for the iPhone Air. Thinner usually means flimsier, harder to fix, and more glued-down parts. But the iPhone Air proves otherwise. Apple has somehow built its thinnest iPhone ever without tanking repairability.&lt;/p&gt;
    &lt;p&gt;Just a few months ago, Samsung’s Galaxy S25 Edge pulled off a similar trick in an ultra-thin package. How’d they do it? And how’d Apple follow suit?&lt;/p&gt;
    &lt;p&gt;The secret: Thinner can actually be more repairable, with clever design.&lt;/p&gt;
    &lt;head rend="h2"&gt;Clever Use of Space&lt;/head&gt;
    &lt;p&gt;Apple made one huge design shift in the Air, which they teased in their keynote and we confirmed with our Lumafield Neptune CT scanner: The middle of this phone is basically just a battery with a frame around it. Apple popped the logic board up above the battery, a large part of how their design got thinner without compromising repair.&lt;/p&gt;
    &lt;p&gt;When we score repairability, 80% of our score is determined by the ease of replacing the parts that are most important and most likely to break. To figure this out, we build a model of the repair process. What’s the path you have to take to get to the battery, or to the screen? We call this the “disassembly tree.” The ideal (if unlikely) disassembly tree is flat. No parts in the way of other parts.&lt;/p&gt;
    &lt;p&gt;A thin device often means, advantageously, a flat disassembly tree. Stacked parts are thicker than parts side-by-side. Our friends over at Framework have been saying this for a long time: It’s totally possible to make a thin and light device that’s also built for repair. The Framework Laptop has done this from the start, with nearly all major components accessible when you remove the cover.&lt;/p&gt;
    &lt;p&gt;And that’s exactly what we’re seeing in the Air. The logic board shift freed up room for the battery and helped the phone stay thin without cramming parts on top of each other. It also conveniently puts less stress on the board if the phone flexes in your pocket. It’s a smart workaround for the “bendgate” problems that haunted earlier slim iPhone designs. Not that the Air’s really going to be bending much, as Zack’s test at Jerry Rig Everything suggests.&lt;/p&gt;
    &lt;p&gt;(By the way, did you see we’re teaming up with Zack to bring you a toolkit that’s made for on-the-go repairs and durability testing?)&lt;/p&gt;
    &lt;p&gt;The Air trims a few extras compared to the Plus line it succeeds, losing the lower speaker and a rear camera. Like the 16e, it’s got just a single rear camera.&lt;/p&gt;
    &lt;p&gt;Inside, though, it packs the upgraded C1X modem, a new N1 WiFi chip, and the A19 Pro system-on-chip, all tucked into the logic board sandwich. It’s a lean, efficient setup that makes the most of limited space. This reduced complexity also contributes to quicker disassembly—fewer features, fewer parts, and fewer points of failure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Battery Life? Eh. Battery Swaps? No Big Deal&lt;/head&gt;
    &lt;p&gt;There’s been a lot of buzz about battery life on this phone. Apple said “all-day battery life,” and tech reviewers of the world, noting the lack of watt-hour specificity and immediate announcement of an add-on battery pack, said, “really now?” At 12.26 Wh, this battery is certainly smaller than recent iPhones (closest comparison being the 13 Pro’s 11.97 Wh), and that raises questions about longevity. More charging cycles usually means faster wear. Still, Apple’s efficiency tricks give it solid runtime, at least for now.&lt;/p&gt;
    &lt;p&gt;But no battery lasts forever, so how difficult will swaps be? We’re relieved to see that the Air has all the greatest hits of the last few iPhone battery designs.&lt;/p&gt;
    &lt;p&gt;The Air’s battery is easy to find and accessible through the back glass thanks to Apple’s dual entry design. Even better, it’s a metal-encased battery. This thin layer of armor makes it more bend resistant and safer to replace. Even better than that, it’s mounted with electrically debonding adhesive strips. Hook them up to a power source and the battery lifts right out, no dangerous prying required. We used our FixHub Portable Power Station for an easy 12 V, and each strip freed after about 70 seconds.&lt;/p&gt;
    &lt;p&gt;Even though it’s comparably a small battery, its heft accounts for 28% of the phone’s total weight, more than any other component.&lt;/p&gt;
    &lt;p&gt;And in a fun twist, we’ve confirmed that it’s the exact same cell found in Apple’s MagSafe battery pack. You can swap between them and the phone still boots up just fine. Like a rear-mounted spare tire on an SUV, an iPhone Air with a MagSafe battery pack is ready for an on-the-go swap, if you will. Granted it’ll take a bit more than a tire iron to make it happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;Modular Port, but How About Parts to Back It Up?&lt;/head&gt;
    &lt;p&gt;How about other likely-to-fail parts? USB-C ports are among the most common failure points in modern phones. Ports tend to collect moisture, which can cause corrosion, and no one is immune to pocket lint. Not to mention the standard port problems caused by mechanical wear and tear.&lt;/p&gt;
    &lt;p&gt;Now, to be clear, if your phone stops charging consistently, you shouldn’t jump straight to replacing the port. Every time you stick a charge cable into the port, you’re jamming pocket lint against the back. Give your charge port a cleanout before you replace it.&lt;/p&gt;
    &lt;head rend="h3"&gt;How to Clean the Ports on your Electronic Device&lt;/head&gt;
    &lt;p&gt;Use this guide to clean the ports on your…&lt;/p&gt;
    &lt;p&gt;But when you do need to replace an Air charging port, you’ll be glad to know it’s decently modular, following the trend of the last few iPhone models. It’s a tedious process, with delicate flex cables, adhesive, and hard-to-reach screws, but it’s still feasible.&lt;/p&gt;
    &lt;p&gt;Interestingly, the modularity of the USB-C port doesn’t seem to be a serviceability choice. Apple won’t do USB-C repairs in-house and they don’t sell replacement ports for iPhones. Of course that won’t stop us from selling the parts as soon as we can get them—and regardless of intent, this modularity is nice to have.&lt;/p&gt;
    &lt;p&gt;Third-party parts manufacturers may take a bit to catch up, since this is a brand new architecture for the housing of the USB port. Apple reportedly used 3D printing to shrink the housing to fit the slim frame of the 6.5mm iPhone Air.&lt;/p&gt;
    &lt;p&gt;Apple says this process reduced material usage by 33% compared to conventional forging processes. Granted, the USB-C port is already tiny. But this isn’t the only place they’re using it: The Apple Watch Ultra 3 uses the same titanium-printing process in its case.&lt;/p&gt;
    &lt;p&gt;We took a close look at the titanium material in the USB-C port, with our Evident DSX2000 microscope.&lt;/p&gt;
    &lt;p&gt;What we saw was fascinating: these regular bubble-like structures.&lt;/p&gt;
    &lt;p&gt;We tapped in some friends in the additive manufacturing industry, who said it wasn’t quite like any metal 3D printing they’d seen before. Their best guess is that Apple’s using a binder or aerosol jet process in addition to some after-printing machining. This aligns with a binder jetting patent Apple inherited back in 2015 when they acquired Metaio. Whatever the exact process, the result is some truly impressive titanium manipulation.&lt;/p&gt;
    &lt;p&gt;(If you’re a metal 3D printing expert and want to give us your thoughts in the comments, we’d love to hear from you!)&lt;/p&gt;
    &lt;head rend="h2"&gt;How Strong Is Thin?&lt;/head&gt;
    &lt;p&gt;Titanium may have retired from the rest of the iPhone line (possibly for geopolitical more than technical reasons) but it’s back as the backbone of this slim smartphone. This tough metal is a good choice, but it’s only as strong as its weak points. Our empty-frame bend test snapped the Air at its plastic antenna passthroughs—a necessity if you want your phone to phone properly. CT scans make it clear: Apple reinforced the center section, but the top and bottom remain vulnerable.&lt;/p&gt;
    &lt;p&gt;Of course, the center is where the phone is most likely to bend, and so far testing hasn’t given any indication of undue flexibility. Will that design affect the durability of the phone? We doubt we’ll see instances of Airs snapping at the ends, but only time will tell.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Verdict: A 7 out of 10 Repairability Score&lt;/head&gt;
    &lt;p&gt;At 6.5 mm, the Air is a hair thinner than Samsung’s Galaxy S25 Edge, yet it manages to keep modular parts and early battery access. Apple’s dual entry design makes battery swaps simple and keeps the fancy OLED out of harm’s way. Electrically debonding adhesive makes battery replacements a lot more consistent than traditional or stretch-release adhesive, and most other major components are simple to access and remove. Apple also kept their best-in-class clipped- and screwed-in screen and back glass architecture, enabling quick reassembly without requiring special adhesive.&lt;/p&gt;
    &lt;p&gt;Combined with Apple’s continued commitment to day-one repair manuals, the iPhone Air earns a provisional 7 out of 10 repairability score. (We’re waiting on Apple to make good on their parts availability commitment as well as final results on our parts pairing tests. Their recent track record’s pretty good, though.)&lt;/p&gt;
    &lt;p&gt;Apple has proved that thin doesn’t have to mean unfixable. The iPhone Air is slimmer than any iPhone before it, but its layout and design tradeoffs make repairs more approachable, not less. It still has limits, but the design shows that good engineering can make even the slimmest devices last longer in the real world. Successful field test for your new foldable, Apple. We’re onto you!&lt;/p&gt;
    &lt;p&gt;More Apple 2025 lineup teardowns coming soon. Bonus round: Can TechWoven handle… hot sauce?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ifixit.com/News/113171/iphone-air-teardown"/><published>2025-09-21T03:09:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45319876</id><title>Spectral Labs releases SGS-1: the first generative model for structured CAD</title><updated>2025-09-21T16:39:15.823012+00:00</updated><content>&lt;doc fingerprint="a55b8b523846d2a2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing SGS-1&lt;/head&gt;
    &lt;p&gt;Spectral Labs releases SGS-1: the first generative model for structured CAD.&lt;/p&gt;
    &lt;p&gt;Today we are announcing SGS-1, a foundation model that can generate fully manufacturable and parametric 3D geometry. You can try a research preview of SGS-1 here.&lt;/p&gt;
    &lt;p&gt;Given an image or a 3D mesh, SGS-1 can generate CAD B-Rep parts in STEP format. Unlike all other existing generative models, SGS-1 outputs are accurate and can be edited easily in traditional CAD software.&lt;/p&gt;
    &lt;p&gt;Overview of SGS-1 - users can provide an image or “dumb” 3D file, and get back a parametric B-Rep file that can be easily edited to match specific dimensions&lt;/p&gt;
    &lt;p&gt;SGS-1 shows strong general results, producing much more complex and diverse CAD shapes than existing methods.&lt;/p&gt;
    &lt;p&gt;Illustrative results from SGS-1&lt;/p&gt;
    &lt;p&gt;SGS-1 can be used for real-world engineering tasks. In the below example, SGS-1 is used to design a bracket for a roller assembly from partial context and a text description (additional details below in Generating Parametric Geometry in Assembly Context section).&lt;/p&gt;
    &lt;head rend="h2"&gt;Results and comparing SGS-1 to prior models&lt;/head&gt;
    &lt;p&gt;We compare SGS-1 to SOTA multimodal reasoning LLMs and open-source image-to-CAD models: GPT-5 thinking, a large reasoning model by OpenAI that can produce CadQuery code to represent parametric geometry, and HoLa, a 205M parameter latent diffusion model with 181M parameter VAE that generate B-Rep geometry conditioned on a single input image. We develop a benchmark set of 75 images depicting medium to high complexity parametric geometry, sourced from CAD image renders of various styles, engineering sketches, and images generated by generative AI models. Model performance is evaluated by successful/failed creation of a single valid watertight solid that is an accurate representation of the input image using distance metrics (Success Ratio).&lt;/p&gt;
    &lt;p&gt;Quantitative evaluations&lt;/p&gt;
    &lt;p&gt;We run each model 10 times and show scores for all 10 runs, as well as for the best output of the 10. Although GPT-5 and HoLa BRep can attain non-zero performance on the easiest images, SGS-1 is the best performing model with at least one success for all but the most complex objects.&lt;/p&gt;
    &lt;p&gt;Outputs from the SOTA large reasoning model (GPT-5) demonstrate a clear lack of spatial understanding, producing outputs that are unusable or too simple to actually be useful. We use both SGS-1 and GPT-5 to generate the parametric geometry for the rail mount from the input image, in order to produce the desired target complete assembly.&lt;/p&gt;
    &lt;p&gt;SGS-1 accurately represents the geometry and can be plugged into an assembly context, while the output from the large reasoning model is missing core spatial features.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generating Parametric Geometry in Assembly Context&lt;/head&gt;
    &lt;p&gt;With SGS-1, you can create new parametric geometry within your current assembly context. In this example, SGS-1 takes in a partial CAD assembly and a text description/image of a bracket, and produces a 3D design for a bracket that is feasible for the context.&lt;/p&gt;
    &lt;p&gt;First, render the partial assembly and come up with a text description of the parts you want to add. Next, run it through SGS-1, which will output a parametric B-Rep in the form of a downloadable STEP fileFinally, import the STEP file into your partial assembly and adjust dimensions until the part fits correctly into the assembly&lt;/p&gt;
    &lt;p&gt;SGS-1 is capable of generating diverse designs for tasks like this - several bracket designs created by SGS-1 are shown below:&lt;/p&gt;
    &lt;head rend="h2"&gt;Converting Sketches and Engineering Drawings to B-Rep&lt;/head&gt;
    &lt;p&gt;SGS-1 can be used to convert simple freehand sketches and engineering drawings into geometry that you can work in in your CAD editor. In this example, we run sketches and drawings through SGS-1 to create parametric geometry.&lt;/p&gt;
    &lt;p&gt;Use SGS-1 to transform sketches and drawings into 3D CAD files&lt;/p&gt;
    &lt;p&gt;This works well on simple hand sketches, enabling powerful design workflows.&lt;/p&gt;
    &lt;p&gt;This also works on structured engineering drawings.&lt;/p&gt;
    &lt;head rend="h2"&gt;Automating Reverse Engineering and STL to STEP File Conversion&lt;/head&gt;
    &lt;p&gt;SGS-1 can be used to convert scans and standalone STL or other mesh files to parametric STEP files without any human input, automating reverse engineering of many shapes.&lt;/p&gt;
    &lt;p&gt;Use SGS-1 to convert dumb 3D representations to parametric geometry&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;SGS-1 is designed to generate parametric 3D geometry for engineering use cases, and struggles when tasked with generating creative assets and organic shapes with complex curvature. In addition, SGS-1 has a limited 3D resolution and struggles with generating very thin structures. Finally, SGS-1 cannot create full assemblies in one shot. We plan to address these limitations with our next model generation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;SGS-1 represents a significant step forward for foundation models that can generate 3D geometry for engineering tasks. We plan to continue pushing forward the frontier, by training models that can engineer physical systems of increasing complexity. The next generation of models will be natively multimodal, support larger and more complex spatial context, and will be capable of performing more advanced physical reasoning through longer range planning. As we continue to scale up these models, we are excited about scaling up reinforcement learning using physical simulation feedback, which will unlock new physical reasoning capabilities for our models.&lt;/p&gt;
    &lt;p&gt;If you are interested in deploying SGS-1 or collaborating on research, please contact us through this form.&lt;/p&gt;
    &lt;p&gt;We are also hiring! Our team is composed of top AI researchers and engineers with previous experience at institutions such as Autodesk Research, Samsung Research, CMU, and Meta. If you're interested in our work and mission, please get in touch.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.spectrallabs.ai/research/SGS-1"/><published>2025-09-21T03:46:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45320431</id><title>Vibe coding cleanup as a service</title><updated>2025-09-21T16:39:15.682155+00:00</updated><content>&lt;doc fingerprint="315083d599a714cd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Vibe Coding Cleanup as a Service&lt;/head&gt;
    &lt;p&gt;A new service category is quietly emerging in tech: Vibe Coding cleanup. What started as LinkedIn jokes about “fixing AI messes” has become a real business opportunity. The harsh reality nobody wants to admit: most AI-generated code is production-unready, and companies are desperately hiring specialists to fix it before their technical debt spirals out of control.&lt;/p&gt;
    &lt;head rend="h2"&gt;The vibe coding explosion&lt;/head&gt;
    &lt;p&gt;When Andrej Karpathy coined “vibe coding” in early 2025, he perfectly captured how developers now work: chatting with AI to generate entire functions instead of writing them. The approach promises 10x productivity gains through natural language programming. GitHub reports that 92% of developers now use AI coding tools, with Copilot alone generating billions of lines of code monthly.&lt;/p&gt;
    &lt;p&gt;But there’s a problem nobody talks about at conferences. GitClear’s analysis of 150 million lines of code reveals AI assistance correlates with 41% more code churn - code that gets reverted or rewritten within two weeks. Stanford researchers found that developers using AI assistants produce significantly less secure code while believing it’s more secure. The tools amplify bad practices: no input validation, outdated dependencies, and architectural decisions that make senior engineers weep.&lt;/p&gt;
    &lt;head rend="h2"&gt;The cleanup economy is real&lt;/head&gt;
    &lt;p&gt;404 Media’s investigation reveals developers are building entire careers around fixing AI-generated code. Hamid Siddiqi manages 15-20 cleanup projects simultaneously, charging premium rates to untangle what he calls “AI spaghetti” - inconsistent interfaces, redundant functions, and business logic that makes no sense. Software consultancy Ulam Labs now advertises “Vibe Coding cleanup” as a core service.&lt;/p&gt;
    &lt;p&gt;The demand is so high that VibeCodeFixers.com launched as a dedicated marketplace. Within weeks, 300 specialists signed up and dozens of projects were matched. Founder Swatantra Sohni describes a typical client: “They burned through $5,000 in OpenAI credits, have a half-working prototype they’re emotionally attached to, and need it production-ready yesterday.” TechCrunch reports that 25% of Y Combinator’s current startup cohort has codebases that are 95% AI-generated, highlighting the massive scale of this trend across Silicon Valley.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why AI code fails at scale&lt;/head&gt;
    &lt;p&gt;The fundamental issue isn’t that AI writes bad code - it’s that it writes locally optimized code without understanding system context. Stack Overflow’s analysis shows AI excels at small, isolated tasks but fails at architectural decisions. Every prompt creates technical debt: inconsistent patterns, duplicated logic, and security holes that automated scanners miss.&lt;/p&gt;
    &lt;p&gt;Georgetown University research shows that at least 48% of AI-generated code contains security vulnerabilities. The tools leak secrets into code, suggest deprecated libraries, and create race conditions that only appear under load. Worse, developers often don’t understand the generated code well enough to spot these issues. Thoughtworks warns this creates “competency debt” - teams lose the ability to maintain their own systems as they become dependent on AI-generated code they don’t fully understand.&lt;/p&gt;
    &lt;head rend="h2"&gt;The market opportunity&lt;/head&gt;
    &lt;p&gt;The Vibe Coding cleanup market is growing rapidly, though exact numbers are hard to pin down. What we know: Gartner predicts 75% of enterprise software engineers will use AI code assistants by 2028. If even a fraction of those projects need cleanup - and current data suggests most will - we’re looking at a massive emerging market.&lt;/p&gt;
    &lt;p&gt;The economics are compelling. Startups save weeks getting to MVP with Vibe Coding, then spend comparable time and budget on cleanup. But that’s still faster than traditional development. The specialists who can efficiently refactor AI messes command $200-400/hour rates. Some are building productized services: fixed-price cleanup packages, AI code audits, and “vibe-to-production” pipelines.&lt;/p&gt;
    &lt;p&gt;Thoughtworks reports that refactoring activity has declined while code churn increases with AI assistance, with most AI-assisted projects requiring significant cleanup before production. Multiple consultancies are now hiring specifically for “AI code remediation” roles. The market is real, growing, and largely untapped.&lt;/p&gt;
    &lt;head rend="h2"&gt;What this means for engineering&lt;/head&gt;
    &lt;p&gt;We’re witnessing a fundamental shift in how software gets built. AI handles the initial implementation, humans handle architecture, testing, and cleanup. It’s not the future we expected, but it’s the one we’re getting.&lt;/p&gt;
    &lt;p&gt;Gergely Orosz argues AI tools are like “very eager junior developers” - they write code quickly but need constant supervision. The difference is that AI juniors never become seniors. They’ll always need cleanup specialists.&lt;/p&gt;
    &lt;p&gt;This creates interesting career paths. Junior developers who master Vibe Coding cleanup can command senior salaries within two years. Senior engineers who understand both AI capabilities and limitations become invaluable. Companies that build robust cleanup processes gain competitive advantage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our stance&lt;/head&gt;
    &lt;p&gt;At Donado Labs, we’ve cleaned up enough vibe-coded disasters to recognize the pattern. AI acceleration works, but only with professional cleanup built into the process. We use AI for prototyping and routine tasks, but architecture and critical logic remain human-written. Our “Vibe to Production” service takes AI prototypes and makes them enterprise-ready: proper testing, security hardening, and documentation that won’t make your successor cry.&lt;/p&gt;
    &lt;p&gt;The companies succeeding with AI coding aren’t the ones using it most - they’re the ones using it smartly. They prototype with AI, then invest in cleanup before technical debt compounds. They treat Vibe Coding like any other tool: powerful but dangerous without expertise.&lt;/p&gt;
    &lt;p&gt;Next time someone claims AI will replace programmers, ask them who’s going to clean up the code. That’s where the real opportunity lies.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://donado.co/en/articles/2025-09-16-vibe-coding-cleanup-as-a-service/"/><published>2025-09-21T06:01:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45322050</id><title>Meta exposé author faces bankruptcy after ban on criticising company</title><updated>2025-09-21T16:39:15.509048+00:00</updated><content>&lt;doc fingerprint="2ea8491b1aa230fe"&gt;
  &lt;main&gt;
    &lt;p&gt;A former Meta executive who wrote an explosive exposé making allegations about the social media company’s dealings with China and its treatment of teenagers is said to be “on the verge of bankruptcy” after publishing the book.&lt;/p&gt;
    &lt;p&gt;An MP has claimed in parliament that Mark Zuckerberg’s company was trying to “silence and punish” Sarah Wynn-Williams, the former director of global public policy at Meta’s precursor, Facebook, after her decision to speak out about her time at the company.&lt;/p&gt;
    &lt;p&gt;Louise Haigh, the former Labour transport secretary, said Wynn-Williams was facing a fine of $50,000 (£37,000) every time she breached an order secured by Meta preventing her from talking disparagingly about the company.&lt;/p&gt;
    &lt;p&gt;Wynn-Williams made a series of claims about the social media company’s behaviour and culture in her book Careless People, published this year. It also contained allegations of sexual harassment denied by the company. It states she was fired for “poor performance and toxic behaviour”.&lt;/p&gt;
    &lt;p&gt;However, the former diplomat was barred from publicising the memoir after Meta, which owns Facebook and Instagram, secured a ruling preventing her from doing so. She subsequently appeared before a US Senate judiciary subcommittee, in which she said Meta worked “hand in glove” with Beijing over censorship tools – something the company has denied.&lt;/p&gt;
    &lt;p&gt;Pan Macmillan, which published the memoir, said it had sold more than 150,000 copies across all formats. The book was also named in The Sunday Times‘ bestselling hardbacks of 2025 so far. The paperback edition is due to be published early next year.&lt;/p&gt;
    &lt;p&gt;New York magazine has previously reported that Wynn-Williams was paid an advance for the book of more than $500,000 (£370,000).&lt;/p&gt;
    &lt;p&gt;Haigh highlighted Wynn-Williams’s case in the House of Commons during a debate about employment rights on Monday. She said Wynn-Williams’s decision to speak out had plunged her into financial peril.&lt;/p&gt;
    &lt;p&gt;“Despite previous public statements that Meta no longer uses NDAs [non-disclosure agreements] in cases of sexual harassment – which Sarah has repeatedly alleged – she is being pushed to financial ruin through the arbitration system in the UK, as Meta seeks to silence and punish her for speaking out,” she said.&lt;/p&gt;
    &lt;p&gt;“Meta has served a gagging order on Sarah and is attempting to fine her $50,000 for every breach of that order. She is on the verge of bankruptcy. I am sure that the whole house and the government will stand with Sarah as we pass this legislation to ensure that whistleblowers and those with the moral courage to speak out are always protected.”&lt;/p&gt;
    &lt;p&gt;It is understood that the $50,000 figure represents the damages Wynn-Williams has to pay for material breaches of the separation agreement she signed when she left Meta in 2017. Meta has emphasised that Wynn-Williams entered into the non-disparagement agreement voluntarily as part of her departure.&lt;/p&gt;
    &lt;p&gt;Meta said that to date, Wynn-Williams had not been forced to make any payments under the agreement.&lt;/p&gt;
    &lt;p&gt;The company did not wish to comment on Haigh’s intervention. It has previously said that Wynn-Williams’s Senate testimony was “divorced from reality and riddled with false claims” about China and the company’s treatment of teenagers.&lt;/p&gt;
    &lt;p&gt;Meta has described the book as a “mix of out-of-date and previously reported claims about the company and false accusations about our executives”. It has said she was fired for “poor performance and toxic behaviour” and that an investigation concluded she made misleading and unfounded allegations of harassment.&lt;/p&gt;
    &lt;p&gt;It said the ruling preventing her from publicising the memoir confirmed the “false and defamatory book should never have been published”.&lt;/p&gt;
    &lt;p&gt;The ruling stated Wynn-Williams should stop promoting the book and, to the extent she could, stop further publication. It did not order any action by Pan Macmillan.&lt;/p&gt;
    &lt;p&gt;Wynn-Williams has not spoken in public since appearing at the Senate hearing in April. In a written statement this month, she said she was grateful that the US Senate was continuing to investigate Meta’s behaviour.&lt;/p&gt;
    &lt;p&gt;“I wish I could say more,” she said. “I urge other tech employees and those who are thinking of whistleblowing to share what they know before more children are harmed.”&lt;/p&gt;
    &lt;p&gt;Her lawyer confirmed Wynn-Williams “remains silenced about the very matters Congress is investigating, despite clear and unanimous voices from Congress calling on Meta to end their arbitration proceedings which threaten to bankrupt her”.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/technology/2025/sep/21/meta-expose-author-sarah-wynn-williams-faces-bankruptcy-after-ban-on-criticising-company"/><published>2025-09-21T12:15:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45322328</id><title>Sequoia: Rust OpenPGP Implementation</title><updated>2025-09-21T16:39:14.992193+00:00</updated><content>&lt;doc fingerprint="79278d7e6d4d1a6a"&gt;
  &lt;main&gt;
    &lt;p&gt;Skip to content GitLab Next Menu Why GitLab Pricing Contact Sales Explore Why GitLab Pricing Contact Sales Explore Sign in Get free trial sequoia Loading&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gitlab.com/sequoia-pgp/sequoia"/><published>2025-09-21T13:00:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45322623</id><title>Disk Utility still can't check and repair APFS volumes and containers (2021)</title><updated>2025-09-21T16:39:14.878194+00:00</updated><content>&lt;doc fingerprint="a711dd1523b6157c"&gt;
  &lt;main&gt;
    &lt;p&gt;Checking and repairing disks is one of the more important tasks performed by Disk Utility, but ever since the introduction of APFS, it has been more fraught than it should have been. One of its most persistent and pervasive problems has been complete failure because Disk Utility has been unable to unmount volumes or containers. To my shock, in Monterey 12.0.1 this problem appears worse than ever, and I now have one disk which Disk Utility is completely unable to check or repair. This article suggests ways around this, while we wait for Apple to fix this bug.&lt;/p&gt;
    &lt;p&gt;For much of this period, the First Aid tool in Disk Utility has relied on the command tool &lt;code&gt;fsck_apfs&lt;/code&gt; to do the work, calling it using two options, &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt;. The &lt;code&gt;y&lt;/code&gt; option simply agrees to make all the repairs suggested by the tool, but the &lt;code&gt;x&lt;/code&gt; option is private. I suspect that privacy isn’t sinister, merely allowing communication between the tool and app using XPC.&lt;/p&gt;
    &lt;p&gt;Best practice for performing disk checks and repairs like this isn’t with a live file system, and those options won’t work when the item being checked is still mounted. So to prepare for the call to &lt;code&gt;fsck_apfs&lt;/code&gt;, Disk Utility has to unmount the volume or container, and that’s the step which appears to go awry.&lt;/p&gt;
    &lt;p&gt;In Catalina and Big Sur, the error reported was confusing, and the recommendation to “back up the data on this volume” inappropriate. It would have been far better if Disk Utility had told us honestly that “this is a known bug, and some day we might get round to fixing it.”&lt;/p&gt;
    &lt;p&gt;That some day clearly hasn’t come in Monterey 12.0.1. When I was researching yesterday’s article about how to check Time Machine backup volumes, it hit me again and again, so I went back and had a closer look at what now goes wrong, and what to do about it.&lt;/p&gt;
    &lt;p&gt;This may happen persistently when you try to check and repair an APFS volume.&lt;/p&gt;
    &lt;p&gt;It can also happen every time with an APFS container.&lt;/p&gt;
    &lt;p&gt;Oddly, though, it doesn’t seem to affect HFS+ volumes.&lt;/p&gt;
    &lt;p&gt;One way around this is to cave in, start up in Recovery, and use Disk Utility there, where there’s no excuse for problems unmounting anything. If you’re intending to check and repair the boot volume group (System and/or Data) then this is the preferred way. macOS does now provide a good means of ‘freezing’ file system access if you do try that when running in normal user mode, but Recovery is always better.&lt;/p&gt;
    &lt;p&gt;The best news of all is that you can still use the command tool &lt;code&gt;fsck_apfs&lt;/code&gt; directly, and work around this bug in Disk Utility. The bizarre twist is that you can use Disk Utility’s Unmount tool to unmount volumes and containers which the app itself appears unable to unmount successfully. Here’s a summary of the process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;In Disk Utility (or Terminal) obtain the device name of the APFS container or volume you want to check. In this case, I’ll use &lt;code&gt;disk7s2&lt;/code&gt;, which is the sort of volume name you’re looking for, or&lt;code&gt;disk7&lt;/code&gt;for a container.&lt;/item&gt;
      &lt;item&gt;In Disk Utility (or Terminal) unmount the container or volume, by selecting it and clicking on the Unmount tool. When you’re checking a container, it’s best to unmount each of its volumes before unmounting the container itself.&lt;/item&gt;
      &lt;item&gt;Open Terminal and type the chosen command with the correct device name. Then enter your admin user’s password at the prompt.&lt;/item&gt;
      &lt;item&gt;Watch as the volume or container is checked.&lt;/item&gt;
      &lt;item&gt;Once that has completed, consider whether the container or volume needs any repair using &lt;code&gt;fsck_apfs&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;In Disk Utility (or Terminal) mount the container or volume again, by selecting it and clicking on the Mount tool.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The command to use depends on whether you just want to check the item, or repair it as well. For the former, use the &lt;code&gt;-n&lt;/code&gt; option, and for the latter the &lt;code&gt;-y&lt;/code&gt; option will perform all repairs automatically. I advise you to include all snapshots, which are important, but if you want to exclude them all, use the &lt;code&gt;-S&lt;/code&gt; option.&lt;/p&gt;
    &lt;p&gt;If the volume is encrypted, you could keep it mounted and use the &lt;code&gt;-l&lt;/code&gt; option to check the live file system, or you can use a command like&lt;code&gt;diskutil apfs unlockVolume /dev/disk7s2 -nomount&lt;/code&gt;&lt;lb/&gt; to unlock the volume without mounting it (thanks to kapitainsky for suggesting that).&lt;/p&gt;
    &lt;p&gt;For example, choose between the commands:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;sudo fsck_apfs -n /dev/disk7s2&lt;/code&gt;just to check the volume disk7s2 but not repair it, and include snapshots.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sudo fsck_apfs -y /dev/disk7s2&lt;/code&gt;to check and repair all errors automatically, and include snapshots.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sudo fsck_apfs -n -S /dev/disk7s2&lt;/code&gt;to check but not repair, but excluding all snapshots.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sudo fsck_apfs -n -S /dev/disk7&lt;/code&gt;to check but not repair the container disk7, excluding all snapshots.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can find details of all available options in &lt;code&gt;man fsck_apfs&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Apple recommends that you first check and repair volumes within a container, then the container itself, and finally the disk (which you can do completely within Disk Utility). That is oddly the exact opposite order previously recommended by many, and duplicates checks on volumes which are normally repeated when you check their container.&lt;/p&gt;
    &lt;p&gt;The disk which I have such problems with is a little unusual in that it’s partitioned into two: a small HFS+ volume, and a much larger APFS container. The irony is that Disk Utility’s advice to back up the affect volume is being offered for my Time Machine backup, which is not only a backup volume itself, but can’t be backed up because its backup snapshots can’t be copied to another disk.&lt;/p&gt;
    &lt;p&gt;It will be so good when Apple finally sorts these problems we’ve suffered for the last four years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eclecticlight.co/2021/11/19/disk-utility-still-cant-check-and-repair-apfs-volumes-and-containers/"/><published>2025-09-21T13:37:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45322819</id><title>I forced myself to spend a week in Instagram instead of Xcode</title><updated>2025-09-21T16:39:14.730112+00:00</updated><content>&lt;doc fingerprint="3bcc78561ea6cefc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I forced myself to spend a week in Instagram instead of Xcode&lt;/head&gt;
    &lt;head rend="h3"&gt;This is what happens when you ban yourself from coding&lt;/head&gt;
    &lt;head rend="h4"&gt;Let’s set the stage:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The Lagree Buddy app has enough quality features that this is an actual, useful thing that I feel comfortable promoting &amp;amp; charging money for.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There are more features I want to build, but they’re bigger features and are still weeks from releasing (because I need to QA them in the real world).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Since the hard paywall and overhauled onboarding (discussed here), there has been a noticeable increase in purchases (woohoo! see chart below)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Given the information above, I figured I should try an experiment.&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;What if I spent every business hour on marketing/distribution instead of coding and building more features?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That felt so uncomfortable to even say out loud because it meant spending a week on social media and talking to people about the app. But it also meant not hiding inside the code.&lt;/p&gt;
    &lt;p&gt;So here’s a detailed (but slightly stream of consciousness) recap of how that week went.&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 1 || Monday, August 25th&lt;/head&gt;
    &lt;p&gt;If any influencers out there want to give me some tips on how to IG correctly, please do. But my current thought process is to craft a story arc for one day. And then to break that arc into 4-6 posts, to be posted every 3 hours or so.&lt;/p&gt;
    &lt;p&gt;I do this because I don’t want it to feel salesy or spammy. So I at least inject some sort of narrative. So today’s story arc was:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Working on new feature at home, but oh no! it’s broken!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Thought I fixed it and took it to class, but oh no! still broken!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I found out why it was broken, explained it, and took it into another class …&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FIXED.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FIXED 2.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FIXED 3.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I also spent the day reaching out to new studios &amp;amp; trainers I saw wearing Apple Watches. Sent out 10 today and got 2 responses.&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 2 || Tuesday, August 26th&lt;/head&gt;
    &lt;p&gt;I bought a Microformer last week, so let’s use the thing for some content!&lt;/p&gt;
    &lt;p&gt;I signed up for Lagreeing at Home, found an instructor with an Apple Watch, snapped a lot of photos, and got busy creating and scheduling another story arc.&lt;/p&gt;
    &lt;p&gt;I almost immediately got a response from Lagreeing at Home about collabing, and I’d have to say… that’s a pretty big win. Lagreeing at Home was born out of the pandemic and has been an incredible presence in the Lagree community, so hearing some affirmation from them is a fantastic feeling.&lt;/p&gt;
    &lt;p&gt;I wanted to do more cold DM’s to studios and trainers today, too, but sometimes the Apple Gods say “try again tomorrow” (wheel of death! see below).&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 3 || Wednesday, August 27th&lt;/head&gt;
    &lt;p&gt;It’s only been two days, but I’ve been struggling with the actual creation of content itself. Struggling might be the wrong word… underestimate? I underestimated the amount of work it takes!&lt;/p&gt;
    &lt;p&gt;I planned everything out on Monday. But then you have to obviously create the stuff, and all of that takes so long! So today, today is going to be an FAQ series. But the question is how to create it creatively so it’s not just social media slop.&lt;/p&gt;
    &lt;p&gt;And after playing with a basic Q&amp;amp;A and not liking how it looked (aka ugly and sales-y) ... I went hunting on Reddit and found this incredible tool (Postfully) to create fake text messages and came up with this for my Q&amp;amp;A instead:&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 4 || Thursday, August 28th&lt;/head&gt;
    &lt;p&gt;I signed up for THE Sebastian Lagree’s class on Sunday. Three reasons why:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I want to show him the app in person&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I’m running out of content and I need some photos of a new studio 😅&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I’m getting sick of social media and wanted to do something in real life loll&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I needed a break from creating content, so I cheated and repurposed my IG stories into TikTok videos. But I didn’t want to give up on my mission, so I fell back on cold messaging studios and trainers.&lt;/p&gt;
    &lt;p&gt;Getting any response is a nice feeling, but this message back from new studio Hold Fitness was incredible! She saw my original post on Reddit from 6 months ago!&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 5 || Friday, August 29th&lt;/head&gt;
    &lt;p&gt;I am absolutely ITCHING to get back to the code!!&lt;/p&gt;
    &lt;p&gt;But I am trying to stick to my mission for the week and am racking my brain on what content to come up without sounding repetitive or spammy. Thank goodness I went to class yesterday and took a bunch of b-roll footage because an idea quickly formed that felt different enough from everything before it this week.&lt;/p&gt;
    &lt;p&gt;This might have been the hardest day to stick to this challenge. Because everything inside of me was screaming:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“CAN WE FOR THE LOVE OF GOD GET OFF OF SOCIAL MEDIA AND DO SOMETHING PRODUCTIVE LIKE BUILD MORE FEATURES.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But the thing that I had to constantly remind myself of is that this is productive.&lt;/p&gt;
    &lt;p&gt;Build it and they will come is a fallacy.&lt;/p&gt;
    &lt;p&gt;You have to tell people about the damn thing. It just doesn’t feel productive because you’re over here churning and burning things that feel like they have a shelf-life of 24 hours (if even that). And sending out cold DM’s and hoping for 1 or 2 responses.&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 6 || Saturday, August 30th&lt;/head&gt;
    &lt;p&gt;Day of rest. &lt;lb/&gt;Sorry, not sorry.&lt;lb/&gt;But I did not code, so the challenge is still intact.&lt;/p&gt;
    &lt;head rend="h4"&gt;Day 7 || Sunday August 31th&lt;/head&gt;
    &lt;p&gt;Sebastian Lagree day!!&lt;/p&gt;
    &lt;p&gt;I drove to Brentwood to take a class from SEBASTIAN LAGREE himself. I was pretty nervous because I wasn’t sure if he was going to think my app was stupid or if it was weird, but he honestly couldn’t have been nicer.&lt;/p&gt;
    &lt;p&gt;He’s also an insanely good instructor, which seems obvious, but when you only see someone on IG as an “influencer”, you’re not entirely sure what to expect.&lt;/p&gt;
    &lt;p&gt;But his class was f-ing legit. But be warned, it may or may not be 60 minutes long 😂. Most classes I’ve taken are 45 minutes, but if you like to get there early (like I do), he will start the class early and end the class late and have you BURNING. My max HR hit 172, and I was sore for a week.&lt;/p&gt;
    &lt;p&gt;It was excellent.&lt;/p&gt;
    &lt;p&gt;I also had an entire plan of going home and editing/posting the content from this day, but like I said, I was sore for a week. So, the immediate aftermath of this class was me just lying down on the couch. No content was edited, posted, or thought about for the rest of the day loll.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Footnote: Because I’m always looking at people’s wrists for Apple Watches in class now, I couldn’t help but notice the lady next to me. Most people wear a fitness tracker in class or nothing at all. But this lady next to me was rocking this:&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;End of Week Results &amp;amp; Takeaways&lt;/head&gt;
    &lt;p&gt;Cold DMs actually work (sometimes). I sent maybe 30-40 messages total and got 4-5 meaningful responses. That's not amazing, but it's infinitely better than the zero responses you get when you never reach out at all.&lt;/p&gt;
    &lt;p&gt;Marketing opens doors that code never could. Sebastian seeing my app in person matters more than any feature I could have shipped that week. You can't build your way into someone's awareness - you have to actually show up.&lt;/p&gt;
    &lt;p&gt;You can't analytics your way to relationships. The numbers didn't move (see chart below), but the connections did. Having Lagreeing at Home and Sebastian Lagree be aware of the app and the person behind the app should pay dividends in the future.&lt;/p&gt;
    &lt;p&gt;Content creation is way harder than I thought. As an engineer, I assumed making an Instagram story would take 10 minutes. Turns out creating something that doesn't look like garbage takes a couple of hours. And doing it daily? Forget about it. I planned a whole week of content on Monday and ended up creating everything day-of because I completely underestimated the work involved.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.pixelpusher.club/p/i-forced-myself-to-spend-a-week-in"/><published>2025-09-21T14:00:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323008</id><title>UUIDv7 in Postgres 18. With time extraction</title><updated>2025-09-21T16:39:14.509303+00:00</updated><content>&lt;doc fingerprint="3d1d03952c8f26ac"&gt;
  &lt;main&gt;
    &lt;p&gt;PostgreSQL 18 is on the horizon, with beta testing now underway. Among the many improvements in this release is support for UUIDv7. A timestamp-based UUID variant that plays nicely with btree indexes. In this post, we'll discuss UUIDs in general, why UUIDv7 is so useful and how you'll want to use it in Postgres.&lt;/p&gt;
    &lt;head rend="h2"&gt;PostgreSQL 18&lt;/head&gt;
    &lt;p&gt;PostgreSQL 18 beta 1 was released few days ago. The release is packed with new features, improvements and bug fixes. As usual, the community is encouraged to try it out and report issues, with the goal of shipping a high quality release in September.&lt;/p&gt;
    &lt;p&gt;The highlights of the release include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Async I/O (with io_uring) — 2-3x speedups on seq scans, vacuums&lt;/item&gt;
      &lt;item&gt;Skip scan on multi-column btree indexes + smarter OR/IN optimizations&lt;/item&gt;
      &lt;item&gt;Keep planner stats during major upgrades&lt;/item&gt;
      &lt;item&gt;UUIDv7 functions&lt;/item&gt;
      &lt;item&gt;Virtual generated columns&lt;/item&gt;
      &lt;item&gt;OAuth login + md5 deprecation warning&lt;/item&gt;
      &lt;item&gt;EXPLAIN ANALYZE now shows I/O, CPU, WAL&lt;/item&gt;
      &lt;item&gt;Temporal constraints, LIKE on nondeterministic collation, casefolding&lt;/item&gt;
      &lt;item&gt;New wire protocol version: 3.2 (first since 2003!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While &lt;code&gt;uuidv7()&lt;/code&gt; is not the most exciting feature (that would be async I/O), it's probably the most awaited one. It was close to being added in 17, and many users have been a bit disappointed that it didn't make the cut. I'm so excited about it, that I decided to take the beta for a spin and write a blog post about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a UUID and why are they useful?&lt;/head&gt;
    &lt;p&gt;UUIDs are 128-bit values used as identifiers for various items - anything from transactions to companies. They are designed to be unique across space and time and can be generated efficiently at high rates without depending on centralized services.&lt;/p&gt;
    &lt;p&gt;Traditionally, relational databases used auto-incrementing types (like &lt;code&gt;SERIAL&lt;/code&gt; or &lt;code&gt;identity&lt;/code&gt;) to generate unique identifiers. This can be done efficiently on a single machine (although there are drawbacks even in this case), but once you need to scale out, you need a way to generate identifiers that are unique across all nodes. Instagram team wrote a short blog about their migration to UUIDs as they sharded their Postgres database.&lt;/p&gt;
    &lt;p&gt;UUIDs are useful as primary keys in databases in several common scenarios:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generating unique IDs in a distributed database:&lt;lb/&gt;While many distributed databases can support auto-increment (identity) columns, they have limitations and performance issues.&lt;/item&gt;
      &lt;item&gt;Unguessable public identifiers:&lt;lb/&gt;Properly generated, UUIDs can't be guessed, predicted or used to infer information about the system. If you use auto-increment as a customer identifier, for instance, attackers can scan all existing identifiers and attempt to use them, they can guess the next identifier and estimate how many customers you have.&lt;/item&gt;
      &lt;item&gt;Allowing clients to generate identifiers:&lt;lb/&gt;Using UUIDs allows clients to generate identifiers that they can use without coordinating with the server. This is useful in mobile apps and serverless environments where you want to minimize communication to the server.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a result of these benefits, UUIDs are used as primary keys in many databases. However, there are also 3 concerns with the use of UUIDs in databases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sorting: UUIDs are not meaningfully sortable by value.&lt;/item&gt;
      &lt;item&gt;Index locality: New UUIDs are not close to each other in the index, this means that inserts will be performed at random locations. This can cause index bloat and other performance issues, as you can see in the charts in this blog post.&lt;/item&gt;
      &lt;item&gt;Size: UUIDs are 128-bit values. Most developers default to using &lt;code&gt;INT&lt;/code&gt;(32-bit) or&lt;code&gt;BIGINT&lt;/code&gt;(64-bit) for their primary keys. For tables with large number of very small records, this can be meaningful overhead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As I'll explain in the next section, UUIDv7 addresses 2 out of these 3 concerns.&lt;/p&gt;
    &lt;p&gt;The size of the UUID may be a problem when disk space or network bandwidth is limited, but it is worth noting that modern CPUs can compare 128-bit values in a single instruction (&lt;code&gt;CMEQ&lt;/code&gt;, part of SIMD instructions), so database operations on UUIDs are highly optimized. The key here is to make sure you use binary representation of UUIDs (proper UUID type) in both the database and the application, and not the string representation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why UUIDv7?&lt;/head&gt;
    &lt;p&gt;UUIDs were first standardized in RFC 4122 in 2005. This RFC defines 5 variants of UUIDs, of which variant 1 and 4 are the most common. The specification was later revised to add variants 6-8 in RFC 9562 which was published in May 2024 (although the first public working draft was published in 2020). Happy Birthday RFC 9562 and UUIDv7!&lt;/p&gt;
    &lt;p&gt;To motivate the specification update, RFC 9562 discusses the common use case of using UUIDs as primary keys in databases:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;One area in which UUIDs have gained popularity is database keys ... but UUID versions 1-5, which were originally defined by [RFC4122], lack certain other desirable characteristics, such as:&lt;/p&gt;
      &lt;p&gt;UUID versions that are not time ordered, such as UUIDv4 (described in Section 5.4), have poor database-index locality. This means that new values created in succession are not close to each other in the index; thus, they require inserts to be performed at random locations. The resulting negative performance effects on the common structures used for this (B-tree and its variants) can be dramatic.&lt;/p&gt;
      &lt;p&gt;many widely distributed database applications and large application vendors have sought to solve the problem of creating a better time-based, sortable unique identifier for use as a database key. This has led to numerous implementations over the past 10+ years solving the same problem in slightly different ways.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The RFC proceeds to specify 16 (!) different implementations of non-standard UUIDs, each with their own trade-offs. This includes the popular &lt;code&gt;ULID&lt;/code&gt;, Twitter's &lt;code&gt;Snowflake&lt;/code&gt;, Instagram's &lt;code&gt;ShardId&lt;/code&gt; and many more.
All of these implementations were evaluated when designing the new specification.&lt;/p&gt;
    &lt;p&gt;While the new RFC specifies 3 new variants of UUIDs, the only interesting one is UUIDv7. UUIDv6 is introduced for backwards compatibility only - the RFC says "Systems that do not involve legacy UUIDv1 SHOULD use UUIDv7 instead". UUIDv8 provides a format for experimental and vendor-specific extensions.&lt;/p&gt;
    &lt;p&gt;UUIDv7 solves both the sorting and index locality concerns. It uses Unix Epoch timestamp as the most significant 48 bits, keeping the other 74 bits for random values (additional bits are used for version and variant). This makes UUIDs sortable by time sequence and unique. The standard also provides the option to include millisecond timestamp in the UUID and/or carefully seeded counter, to support ordering within a single second (if needed). As a result, UUIDv7 is a great fit for use as a primary key in databases - it is guaranteed to be unique, sortable and have good index locality.&lt;/p&gt;
    &lt;head rend="h2"&gt;UUIDv7 in PostgreSQL 18&lt;/head&gt;
    &lt;p&gt;Until PostgreSQL 18, UUIDv7 was not natively supported. The built-in &lt;code&gt;gen_random_uuid()&lt;/code&gt; function generated UUIDv4, and while the popular &lt;code&gt;uuid-ossp&lt;/code&gt; extension added
support for additional UUID variants, it was limited to the variants specified in RFC 4122.&lt;/p&gt;
    &lt;p&gt;PostgreSQL 18 adds a new function: &lt;code&gt;uuidv7()&lt;/code&gt;, which generates UUIDv7 values. The Postgres implementation includes a 12-bit sub-millisecond timestamp fraction immediately after the timestamp (as allowed but not required by the standard). This guarantees monotonicity for all UUIDv7 values generated by the same Postgres session (same backend process).&lt;/p&gt;
    &lt;p&gt;For consistency, PostgreSQL 18 added &lt;code&gt;uuidv4()&lt;/code&gt; as an alias for &lt;code&gt;gen_random_uuid()&lt;/code&gt;, to match the naming.&lt;/p&gt;
    &lt;p&gt;Calling &lt;code&gt;uuidv7()&lt;/code&gt; will generate a new UUIDv7 value where the timestamp is the current time. If you need to generate a UUIDv7 value for a different time, you can pass an optional &lt;code&gt;interval&lt;/code&gt; to the function.&lt;/p&gt;
    &lt;p&gt;Postgres' existing functions for extracting timestamp and version from a UUID are also updated to support UUIDv7. Here is an example of how to use the new functions:&lt;/p&gt;
    &lt;code&gt;postgres=# select uuidv7();
                uuidv7
--------------------------------------
 0196ea4a-6f32-7fd0-a9d9-9c815a0750cd
(1 row)

postgres=# select uuidv7(INTERVAL '1 day');
                uuidv7
--------------------------------------
 0196ef74-8d09-77b0-a84b-5301262f05ad
(1 row)

postgres=# SELECT uuid_extract_version(uuidv4());
 uuid_extract_version
----------------------
                    4
(1 row)

postgres=# SELECT uuid_extract_version(uuidv7());
 uuid_extract_version
----------------------
                    7
(1 row)

postgres=# SELECT uuid_extract_timestamp(uuidv7());
   uuid_extract_timestamp
----------------------------
 2025-05-19 20:50:40.381+00
(1 row)

postgres=# SELECT uuid_extract_timestamp(uuidv7(INTERVAL '1 hour'));
   uuid_extract_timestamp
----------------------------
 2025-05-19 21:50:59.388+00
(1 row)

postgres=# SELECT uuid_extract_timestamp(uuidv7(INTERVAL '-1 day'));
   uuid_extract_timestamp
----------------------------
 2025-05-18 20:51:15.774+00
(1 row)
&lt;/code&gt;
    &lt;p&gt;Using &lt;code&gt;uuidv7()&lt;/code&gt; as the primary key in a table is straightforward, and together with the ability to extract the timestamp, it makes it easy to use the UUID as a sortable key and even inspect the creation time of the record:&lt;/p&gt;
    &lt;code&gt;CREATE TABLE test (
    id uuid DEFAULT uuidv7() PRIMARY KEY,
    name text
);

INSERT INTO test (name) VALUES ('foo');
INSERT INTO test (name) VALUES ('bar');
-- this will be sorted to the beginning of the list since we are making it 1h older than the other two
INSERT INTO test (id, name) VALUES (uuidv7(INTERVAL '-1 hour'), 'oldest');

SELECT uuid_extract_timestamp(id), name FROM test ORDER BY id;

   uuid_extract_timestamp   |  name
----------------------------+--------
 2025-05-19 19:55:43.87+00  | oldest
 2025-05-19 20:55:01.304+00 | foo
 2025-05-19 20:55:01.305+00 | bar
(3 rows)
&lt;/code&gt;
    &lt;p&gt;All these functions are documented in the PostgreSQL documentation and if you are interested in the implementation details, you can review the patch.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try it out!&lt;/head&gt;
    &lt;p&gt;Once PostgreSQL 18 is released, you will be able to use &lt;code&gt;uuidv7()&lt;/code&gt; and all the other new functionality by installing it as you normally do.
While the official release is planned for September, &lt;code&gt;Beta 1&lt;/code&gt; version is already available and the community encourages users to try it out and report issues.&lt;/p&gt;
    &lt;p&gt;The installations instructions for the beta versions and nightly snapshots are available here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;
    &lt;p&gt;PostgreSQL 18 delivers practical improvements that experienced developers will really appreciate. Native support for UUIDv7 is a quiet but impactful addition that addresses long-standing pain points in database design.&lt;/p&gt;
    &lt;p&gt;UUIDs have always been a tradeoff: secure, guaranteed to be unique, efficient to generate in distributed systems. but with performance drawbacks for use with B-tree indexes. UUIDv7 brings the best of both worlds — globally unique, yet ordered in a way that plays nicely with B-tree indexes and write-heavy workloads. Postgres 18 makes them that much more convenient to use.&lt;/p&gt;
    &lt;p&gt;If you've ever hesitated to use UUIDs for primary keys, this is your chance to revisit that decision. Try the beta, test it in your schema, and see how it behaves. Whether you're building multi-tenant apps or just want more stable ID generation, UUIDv7 is worth a look.&lt;/p&gt;
    &lt;p&gt;The best way to shape the future of Postgres is to get involved early — so go ahead, spin up a test instance and let the community know what you find.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.thenile.dev/blog/uuidv7"/><published>2025-09-21T14:24:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323027</id><title>The Beginner's Textbook for Homomorphic Encryption</title><updated>2025-09-21T16:39:14.280393+00:00</updated><content>&lt;doc fingerprint="3d9db40ad4d580ca"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Cryptography and Security&lt;/head&gt;&lt;p&gt; [Submitted on 7 Mar 2025 (v1), last revised 8 Sep 2025 (this version, v15)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:The Beginner's Textbook for Fully Homomorphic Encryption&lt;/head&gt;View PDF&lt;quote&gt;Abstract:Fully Homomorphic Encryption (FHE) is a cryptographic scheme that enables computations to be performed directly on encrypted data, as if the data were in plaintext. After all computations are performed on the encrypted data, it can be decrypted to reveal the result. The decrypted value matches the result that would have been obtained if the same computations were applied to the plaintext data.&lt;lb/&gt;FHE supports basic operations such as addition and multiplication on encrypted numbers. Using these fundamental operations, more complex computations can be constructed, including subtraction, division, logic gates (e.g., AND, OR, XOR, NAND, MUX), and even advanced mathematical functions such as ReLU, sigmoid, and trigonometric functions (e.g., sin, cos). These functions can be implemented either as exact formulas or as approximations, depending on the trade-off between computational efficiency and accuracy.&lt;lb/&gt;FHE enables privacy-preserving machine learning by allowing a server to process the client's data in its encrypted form through an ML model. With FHE, the server learns neither the plaintext version of the input features nor the inference results. Only the client, using their secret key, can decrypt and access the results at the end of the service protocol. FHE can also be applied to confidential blockchain services, ensuring that sensitive data in smart contracts remains encrypted and confidential while maintaining the transparency and integrity of the execution process. Other applications of FHE include secure outsourcing of data analytics, encrypted database queries, privacy-preserving searches, efficient multi-party computation for digital signatures, and more.&lt;lb/&gt;As this book is an open project (this https URL), we welcome FHE experts to join us as collaborators to help expand the draft.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Ronny Ko [view email]&lt;p&gt;[v1] Fri, 7 Mar 2025 04:29:11 UTC (33 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 13 Mar 2025 15:18:50 UTC (5,237 KB)&lt;/p&gt;&lt;p&gt;[v3] Fri, 14 Mar 2025 03:22:13 UTC (5,239 KB)&lt;/p&gt;&lt;p&gt;[v4] Sun, 13 Apr 2025 13:14:01 UTC (5,258 KB)&lt;/p&gt;&lt;p&gt;[v5] Sat, 26 Apr 2025 18:20:16 UTC (5,275 KB)&lt;/p&gt;&lt;p&gt;[v6] Sun, 4 May 2025 15:31:10 UTC (5,302 KB)&lt;/p&gt;&lt;p&gt;[v7] Mon, 12 May 2025 17:20:32 UTC (5,099 KB)&lt;/p&gt;&lt;p&gt;[v8] Tue, 20 May 2025 16:04:22 UTC (5,062 KB)&lt;/p&gt;&lt;p&gt;[v9] Mon, 26 May 2025 03:42:34 UTC (5,062 KB)&lt;/p&gt;&lt;p&gt;[v10] Sun, 1 Jun 2025 08:45:01 UTC (4,570 KB)&lt;/p&gt;&lt;p&gt;[v11] Sun, 8 Jun 2025 04:45:52 UTC (4,571 KB)&lt;/p&gt;&lt;p&gt;[v12] Mon, 30 Jun 2025 13:04:04 UTC (4,570 KB)&lt;/p&gt;&lt;p&gt;[v13] Mon, 7 Jul 2025 09:54:47 UTC (4,570 KB)&lt;/p&gt;&lt;p&gt;[v14] Wed, 13 Aug 2025 04:21:08 UTC (4,570 KB)&lt;/p&gt;&lt;p&gt;[v15] Mon, 8 Sep 2025 05:39:49 UTC (4,570 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2503.05136"/><published>2025-09-21T14:26:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323187</id><title>New thermoelectric cooling breakthrough nearly doubles efficiency</title><updated>2025-09-21T16:39:14.101779+00:00</updated><content>&lt;doc fingerprint="31bed7467a83e1fc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;New cooling breakthrough nearly doubles efficiency&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Date:&lt;/item&gt;
      &lt;item rend="dd-1"&gt;September 20, 2025&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Source:&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Johns Hopkins University Applied Physics Laboratory&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Summary:&lt;/item&gt;
      &lt;item rend="dd-3"&gt;CHESS thin-film materials nearly double refrigeration efficiency compared to traditional methods. Scalable and versatile, they promise applications from household cooling to space exploration.&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Share:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Researchers at the Johns Hopkins Applied Physics Laboratory (APL) in Laurel, Maryland, have developed a new, easily manufacturable solid-state thermoelectric refrigeration technology with nano-engineered materials that is twice as efficient as devices made with commercially available bulk thermoelectric materials. As global demand grows for more energy-efficient, reliable and compact cooling solutions, this advancement offers a scalable alternative to traditional compressor-based refrigeration.&lt;/p&gt;
    &lt;p&gt;In a paper published in Nature Communications, a team of researchers from APL and refrigeration engineers from Samsung Research demonstrated improved heat-pumping efficiency and capacity in refrigeration systems attributable to high-performance nano-engineered thermoelectric materials invented at APL known as controlled hierarchically engineered superlattice structures (CHESS).&lt;/p&gt;
    &lt;p&gt;The CHESS technology is the result of 10 years of APL research in advanced nano-engineered thermoelectric materials and applications development. Initially developed for national security applications, the material has also been used for noninvasive cooling therapies for prosthetics and won an R&amp;amp;D 100 award in 2023.&lt;/p&gt;
    &lt;p&gt;"This real-world demonstration of refrigeration using new thermoelectric materials showcases the capabilities of nano-engineered CHESS thin films," said Rama Venkatasubramanian, principal investigator of the joint project and chief technologist for thermoelectrics at APL. "It marks a significant leap in cooling technology and sets the stage for translating advances in thermoelectric materials into practical, large-scale, energy-efficient refrigeration applications."&lt;/p&gt;
    &lt;p&gt;A New Benchmark for Solid-State Cooling&lt;/p&gt;
    &lt;p&gt;The push for more efficient and compact cooling technologies is fueled by a variety of factors, including population growth, urbanization and an increasing reliance on advanced electronics and data infrastructure. Conventional cooling systems, while effective, are often bulky, energy intensive and reliant on chemical refrigerants that can be harmful to the environment.&lt;/p&gt;
    &lt;p&gt;Thermoelectric refrigeration is widely regarded as a potential solution. This method cools by using electrons to move heat through specialized semiconductor materials, eliminating the need for moving parts or harmful chemicals, making these next-generation refrigerators quiet, compact, reliable and sustainable. Bulk thermoelectric materials are used in small devices like mini-fridges, but their limited efficiency, low heat-pumping capacity and incompatibility with scalable semiconductor chip fabrication have historically prevented their wider use in high-performance systems.&lt;/p&gt;
    &lt;p&gt;In the study, researchers compared refrigeration modules using traditional bulk thermoelectric materials with those using CHESS thin-film materials in standardized refrigeration tests, measuring and comparing the electrical power needed to achieve various cooling levels in the same commercial refrigerator test systems. Samsung Research's Life Solution Team, led by executive vice president Joonhyun Lee, collaborated with APL to validate the results through detailed thermal modeling, quantifying heat loads and thermal resistance parameters to ensure accurate performance evaluation under real-world conditions.&lt;/p&gt;
    &lt;p&gt;The results were striking: Using CHESS materials, the APL team achieved nearly 100% improvement in efficiency over traditional thermoelectric materials at room temperature (around 80 degrees Fahrenheit, or 25 C). They then translated these material-level gains into a near 75% improvement in efficiency at the device level in thermoelectric modules built with CHESS materials and a 70% improvement in efficiency in a fully integrated refrigeration system, each representing a significant improvement over state-of-the-art bulk thermoelectric devices. These tests were completed under conditions that involved significant amounts of heat pumping to replicate practical operation.&lt;/p&gt;
    &lt;p&gt;Built to Scale&lt;/p&gt;
    &lt;p&gt;Beyond improving efficiency, the CHESS thin-film technology uses remarkably less material -- just 0.003 cubic centimeters, or about the size of a grain of sand, per refrigeration unit. This reduction in material means APL's thermoelectric materials could be mass-produced using semiconductor chip production tools, driving cost efficiency and enabling widespread market adoption.&lt;/p&gt;
    &lt;p&gt;"This thin-film technology has the potential to grow from powering small-scale refrigeration systems to supporting large building HVAC applications, similar to the way that lithium-ion batteries have been scaled to power devices as small as mobile phones and as large as electric vehicles," Venkatasubramanian said.&lt;/p&gt;
    &lt;p&gt;Additionally, the CHESS materials were created using a well-established process commonly used to manufacture high-efficiency solar cells that power satellites and commercial LED lights.&lt;/p&gt;
    &lt;p&gt;"We used metal-organic chemical vapor deposition (MOCVD) to produce the CHESS materials, a method well known for its scalability, cost-effectiveness and ability to support large-volume manufacturing," said Jon Pierce, a senior research engineer who leads the MOCVD growth capability at APL. "MOCVD is already widely used commercially, making it ideal for scaling up CHESS thin-film thermoelectric materials production."&lt;/p&gt;
    &lt;p&gt;These materials and devices continue to show promise for a broad range of energy harvesting and electronics applications in addition to the recent advances in refrigeration. APL plans to continue to partner with organizations to refine the CHESS thermoelectric materials with a focus on boosting efficiency to approach that of conventional mechanical systems. Future efforts include demonstrating larger-scale refrigeration systems, including freezers, and integrating artificial intelligence-driven methods to optimize energy efficiency in compartmentalized or distributed cooling in refrigeration and HVAC equipment.&lt;/p&gt;
    &lt;p&gt;"Beyond refrigeration, CHESS materials are also able to convert temperature differences, like body heat, into usable power," said Jeff Maranchi, Exploration Program Area manager in APL's Research and Exploratory Development Mission Area. "In addition to advancing next-generation tactile systems, prosthetics and human-machine interfaces, this opens the door to scalable energy-harvesting technologies for applications ranging from computers to spacecraft -- capabilities that weren't feasible with older bulkier thermoelectric devices."&lt;/p&gt;
    &lt;p&gt;"The success of this collaborative effort demonstrates that high-efficiency solid-state refrigeration is not only scientifically viable but manufacturable at scale," said Susan Ehrlich, an APL technology commercialization manager. "We're looking forward to continued research and technology transfer opportunities with companies as we work toward translating these innovations into practical, real-world applications."&lt;/p&gt;
    &lt;p&gt;Story Source:&lt;/p&gt;
    &lt;p&gt;Materials provided by Johns Hopkins University Applied Physics Laboratory. Note: Content may be edited for style and length.&lt;/p&gt;
    &lt;p&gt;Journal Reference:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Jake Ballard, Matthew Hubbard, Sung-Jin Jung, Vanessa Rojas, Richard Ung, Junwoo Suh, MinSoo Kim, Joonhyun Lee, Jonathan M. Pierce, Rama Venkatasubramanian. Nano-engineered thin-film thermoelectric materials enable practical solid-state refrigeration. Nature Communications, 2025; 16 (1) DOI: 10.1038/s41467-025-59698-y&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cite This Page:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.sciencedaily.com/releases/2025/09/250919085242.htm"/><published>2025-09-21T14:43:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323207</id><title>DXGI debugging: Microsoft put me on a list</title><updated>2025-09-21T16:39:13.450211+00:00</updated><content>&lt;doc fingerprint="a61f8113c3239dc4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;DXGI debugging: Microsoft put me on a list&lt;/head&gt;
    &lt;p&gt;Why does Space Station 14 crash with ANGLE on ARM64? 6 hours later…&lt;/p&gt;
    &lt;p&gt;So. I’ve been continuing work on getting ARM64 builds out for Space Station 14. The thing I was working on yesterday were launcher builds, specifically a single download that supports both ARM64 and x64. I’d already gotten the game client itself running natively on ARM64, and it worked perfectly fine in my dev environment. I wrote all the new launcher code, am pretty sure I got it right. Zip it up, test it on ARM64, aaand…&lt;/p&gt;
    &lt;p&gt;The game client crashes on Windows ARM64. Both in my VM and on Julian’s real Snapdragon X laptop.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging: logs&lt;/head&gt;
    &lt;p&gt;The client logs are empty. They suspiciously cut out right after SDL is initialized.&lt;/p&gt;
    &lt;p&gt;Of course it isn’t that easy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging: pulling WinDbg out of the shed&lt;/head&gt;
    &lt;p&gt;Given that there’s no logs, this has to be a native crash. That means it’s WinDbg time.&lt;/p&gt;
    &lt;p&gt;So at first I decided to start &lt;code&gt;Space Station 14 Launcher.exe&lt;/code&gt; directly through WinDbg. This is annoying because I have to go into child processes (with &lt;code&gt;.childdbg 1&lt;/code&gt;) twice, and for some reason there’s a lot of waiting, but it does work…&lt;/p&gt;
    &lt;p&gt;The game crashes in &lt;code&gt;USER32!GetDC&lt;/code&gt; on an illegal instruction, somewhere after SDL does something. I barely glanced at the disassembly but it made no sense to me, so I just assumed there’s some UB happening and didn’t think much of it. After all, why would the implementation of &lt;code&gt;GetDC()&lt;/code&gt; have broken assembly?&lt;/p&gt;
    &lt;code&gt;(3148.35e4): Illegal instruction - code c000001d (first chance)
(3148.35e4): Unknown exception - code c000041d (!!! second chance !!!)
*** WARNING: Unable to verify checksum for C:\Users\Luna\Downloads\SS14.Launcher_Windows\bin_arm64\loader\SDL3.DLL
USER32!GetDC+0x8:
00007ff9`f7be9548 ee8e1db0 ???
&lt;/code&gt;
    &lt;p&gt;WinDbg was also unable to pull stack frames from C# code. It did, thankfully, clearly communicate why this was. Yep, it’s our friend &lt;code&gt;mscordaccore&lt;/code&gt; again!&lt;/p&gt;
    &lt;code&gt;CLRDLL: Consider using ".cordll -lp &amp;lt;path&amp;gt;" command to specify .NET runtime directory.
CLRDLL: Consider using ".cordll -lp &amp;lt;path&amp;gt;" command to specify .NET runtime directory.
&lt;/code&gt;
    &lt;p&gt;However, my attempts to actually follow said instructions were completely fruitless, giving this error:&lt;/p&gt;
    &lt;code&gt;0:027:ARM64EC&amp;gt; .cordll -lp C:\Users\Luna\Downloads\SS14.Launcher_Windows\dotnet_arm64\shared\Microsoft.NETCore.App\9.0.9
CLRDLL: Consider using ".cordll -lp &amp;lt;path&amp;gt;" command to specify .NET runtime directory.
CLR DLL status: ERROR: Unable to load DLL C:\Users\Luna\Downloads\SS14.Launcher_Windows\dotnet_arm64\shared\Microsoft.NETCore.App\9.0.9\mscordaccore_AMD64_arm64_9.0.925.41916.dll, Win32 error 0n87
&lt;/code&gt;
    &lt;p&gt;Why is it trying to run an &lt;code&gt;AMD64&lt;/code&gt; binary? Wait is WinDbg not natively compiled for ARM64? Sigh. Let’s just do it without C# debugging, I can probably manage based off the SDL stack trace. So I pull &lt;code&gt;SDL3.pdb&lt;/code&gt; from our server, drop it next to &lt;code&gt;SDL3.dll&lt;/code&gt;, and then use the UI to reload the symbols. And that gets us a bit further, we now have proper function names for SDL3!&lt;/p&gt;
    &lt;p&gt;So I double click one of the entries in the UI’s stack trace view. And the entire debugger breaks. Stack trace view goes empty. Every action I try to make causes more of these errors to be printed:&lt;/p&gt;
    &lt;code&gt;Machine is not a possible execution machine
Unable to get current machine context, HRESULT 0x8000FFFF
Machine is not a possible execution machine
Unable to get current machine context, HRESULT 0x8000FFFF
Machine is not a possible execution machine
Unable to get current machine context, HRESULT 0x8000FFFF
Machine is not a possible execution machine
Machine is not a possible execution machine
&lt;/code&gt;
    &lt;p&gt;Now even WinDbg is broken??&lt;/p&gt;
    &lt;p&gt;Googling these errors gave nothing useful. One of them gave not a single result. After just pondering the error for a moment, I thought “wait, why is the command prompt still saying &lt;code&gt;ARM64EC&amp;gt;&lt;/code&gt;? ARM64EC is for emulation, but the active debugger processes (&lt;code&gt;SS14.Launcher.exe&lt;/code&gt; and &lt;code&gt;SS14.Loader.exe&lt;/code&gt;) are both native ARM64.&lt;/p&gt;
    &lt;p&gt;Turns out that it’s because I started &lt;code&gt;Space Station 14 Launcher.exe&lt;/code&gt; directly. You see, that executable is x64 native, and its only job is to set up the .NET environment and launch the actual ARM64 executable. Something about starting the debugging session with that program causes WinDbg to get extremely confused when later looking at the child processes it spawns.&lt;/p&gt;
    &lt;p&gt;From this point on I just started launching &lt;code&gt;SS14.Launcher.exe&lt;/code&gt; directly1. This means I wasn’t setting up the same &lt;code&gt;DOTNET_ROOT&lt;/code&gt; (because WinDbg can’t set environment variables when launching things… yes really), but this didn’t really matter. This fixed both the “Machine is not a possible execution machine” errors and the issues with showing C# stack traces. I guess WinDbg is compiled for ARM64 after all, and it just decided to run an x64 debug host when you start debugging an x64 application. Fair enough I guess?&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging: what’s SDL doing?&lt;/head&gt;
    &lt;p&gt;After figuring out all of the above, we could really get started. I also opted to swap out &lt;code&gt;SDL3.dll&lt;/code&gt; with a locally-built copy, so that the debugger could locate source files2. What SDL is doing is pretty straight forward: the first time the window is shown, it clears the background with GDI commands:&lt;/p&gt;
    &lt;p&gt;I mean… this is like, fine, right? I mean I don’t know much about this code, but why would this crash on ARM but not x64??? The window is valid. &lt;code&gt;GetDC()&lt;/code&gt; is an extremely fundamental Win32 function call. If there was something broken with it, my OS would not be usable. What the fuck is going on?&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;if (ShouldClearWindowOnEraseBackground(data))&lt;/code&gt; allows it to be disabled via a hint, which can be specified by environment variable. This fixes the crash… until you open a second OS window, then SDL3 calls &lt;code&gt;GetDC()&lt;/code&gt; once again and that crashes. Not a solution.&lt;/p&gt;
    &lt;p&gt;So I checked the actual &lt;code&gt;USER32!GetDC&lt;/code&gt; again, and this time I actually paid attention to the disassembly code instead of glossing over it. What the fuck? &lt;code&gt;pacibsp&lt;/code&gt; is missing at the start. It’s loading an address for a jump that only jumps to the next instruction, which is invalid. In some runs, said instruction was instead a broken &lt;code&gt;x26&lt;/code&gt;-relative &lt;code&gt;str&lt;/code&gt; instruction that AV’d because the register was all zeroes.&lt;/p&gt;
    &lt;p&gt;At this point let’s introduce the villain. You might have noticed it in the call stack: &lt;code&gt;DXGI!My_GetDC&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For those who aren’t well-versed in DirectX stuff: DXGI is a fundamental part of DirectX ever since DirectX 10 (Vista). For those who have never modded a game before: a detour is a hack that injects instructions into other functions at runtime, to do evil shit. Why the hell is Microsoft using this in DXGI?&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging: DXGI despair&lt;/head&gt;
    &lt;p&gt;Through the debugging adventure, I ended up putting a breakpoint on every call to &lt;code&gt;USER32!GetDC&lt;/code&gt;. The first few calls are fine, but then the last one, the one that crashes, is not.&lt;/p&gt;
    &lt;p&gt;At this point I got really desperate. “Asking in low-level programming Discords”-level desperate. I ended up asking for help in the DirectX Discord (yes, there’s an official DirectX Discord, and there’s many MS employees in there).&lt;/p&gt;
    &lt;p&gt;I would like to thank Jesse Natalia from the DirectX Discord for responding swiftly to my messages in there.&lt;/p&gt;
    &lt;p&gt;After some back and forth there, I wanted to catch DXGI in the act. Maybe that would tell me something, I don’t know. So with a simple &lt;code&gt;ba w4 USER32!GetDC&lt;/code&gt;, I put a hardware breakpoint for whenever something would write to &lt;code&gt;USER32!GetDC&lt;/code&gt;. I did have to awkwardly “run the program for just a little bit” because &lt;code&gt;USER32.dll&lt;/code&gt; isn’t loaded immediately at program startup.3&lt;/p&gt;
    &lt;p&gt;While writing this blog post I realized there is an intelligent way to do this. It’s called &lt;code&gt;sxe ld USER32.dll&lt;/code&gt;. I’ve literally written about it in this blog before. Oops.&lt;/p&gt;
    &lt;p&gt;Now this is very interesting. The bottom of the stack trace is quite expected: SDL creates a window, uses ANGLE’s EGL implementation for this, that does a bunch of stuff, and eventually creates a DXGI swapchain. But then what is &lt;code&gt;UpgradeSwapEffect&lt;/code&gt;? And why is it installing a detour?&lt;/p&gt;
    &lt;p&gt;Ah, I already know what this is.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimizing windowed games: flip model&lt;/head&gt;
    &lt;p&gt;Right. So. DirectX.&lt;/p&gt;
    &lt;p&gt;When you create a DirectX swapchain, you specify an “effect”, which falls into two categories: “bitblt” and “flip”. To make a long story short: bitblt is the “original” one, while flip is the much more modern one added in Windows 84. It’s more efficient and performant, and all software should be using it. Furthermore, on modern versions of Windows and with a GPU supporting “Multiplane Overlays”, flip model actually enables windowed games to be displayed with zero additional latency over “exclusive” fullscreen mode.&lt;/p&gt;
    &lt;p&gt;Of course, many games never get updated, or they’re stuck on an ancient version. And many of these games don’t care. So in Windows 11, Microsoft added “Optimizations for windowed games”, which forcibly enables flip model on games that are still using bitblt. Why does DXGI need to install detours for this? Probably some compatibility shit with the bitblt model. I don’t have any deep knowledge of how Win32 GDI stuff works, but it’s not hard for me to imagine there’s some interplay here they need to take care of. I can also confirm that disabling the feature in Windows’ settings menu fixes the crash!&lt;/p&gt;
    &lt;p&gt;If you’re wondering why SS14 isn’t using flip model: it’s because we can’t. We’re not creating the swapchain directly, ANGLE is. And ANGLE is continuing to use &lt;code&gt;SWAP_EFFECT_SEQUENTIAL&lt;/code&gt;. I actually once experimented with SS14 managing the swapchain, but this ran into some ANGLE limitations and I never got around to ironing out all the edge cases and crashes. I’d rather just spend the brain power on ditching OpenGL, rather than trying to continue working with this broken API.5.&lt;/p&gt;
    &lt;p&gt;So here we are. The entire debugging story so far, you’re like, “surely Microsoft didn’t break DXGI on ARM64, huh???” But now it’s becoming plausible. There’s barely any native ARM64 Windows games, and surely none that are using bitblt swapchains. And guess what, you don’t even need &lt;code&gt;GetDC()&lt;/code&gt; for modern DirectX games. SDL does it because it’s heavily designed for OpenGL. Most games run in x64 emulation, and that presumably works fine. Everything adds up to it being possible this just genuinely fell under the radar at Microsoft.&lt;/p&gt;
    &lt;p&gt;This should be pretty easy to verify in a minimal example. I cloned an old DirectX SDK sample, updated it to be compiled for ARM64, added some &lt;code&gt;GetDC()&lt;/code&gt; calls, aaand… nope, no crash. Then I spent quite a while trying various stuff: comparing the swapchain creation code with that of ANGLE, changing various parameters, verifying whether the detour was being installed (it wasn’t). But eventually, I did find it.&lt;/p&gt;
    &lt;p&gt;It’s the filename.&lt;/p&gt;
    &lt;p&gt;Of course it’s the goddamn filename.&lt;/p&gt;
    &lt;head rend="h2"&gt;I’m on a list&lt;/head&gt;
    &lt;p&gt;It only happens when the program is called &lt;code&gt;SS14.Loader.exe&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The final piece of the puzzle. It didn’t happen in a dev environment because then the exe isn’t named &lt;code&gt;SS14.Loader.exe&lt;/code&gt;. Microsoft only enables “Optimizations for windowed games” on a specific list of games. And guess what, none of those select games are on ARM64, at least until I was unfortunate enough to port mine. How did we get on the list? Who knows.&lt;/p&gt;
    &lt;p&gt;Microsoft put me on a list, that ships with every Windows install. And this list actually broke my game. Achievement unlocked!&lt;/p&gt;
    &lt;head rend="h2"&gt;Addendum: why ANGLE, and about OpenGL on Windows ARM64&lt;/head&gt;
    &lt;p&gt;Traditionally, OpenGL on Windows has been implemented by the 3 IHVs (Nvidia, AMD, Intel). If they didn’t explicitly go out of their way to add it to their drivers, you’d have no OpenGL beyond 1.0. Those new Snapdragon X devices, however, use Microsoft’s new-ish “OpenGL on D3D12” driver. It’s actually part of Mesa!&lt;/p&gt;
    &lt;p&gt;The problem with Space Station 14 is that said driver is broken for us, causing severe graphical artifacts and flickering. I had been aware of this for years, because the same driver is used for the GPU acceleration of WSL2, but I never bothered to report it 😬. So for the purpose of porting SS14 to ARM64 Windows, I decided to just immediately force on ANGLE on Qualcomm devices, and call it a day.&lt;/p&gt;
    &lt;p&gt;What I didn’t know until yesterday is that the OpenGL on D3D12 driver does not ship with Qualcomm’s drivers! It’s on the Microsoft store! I can even install it in my VM and get it to emulate OpenGL on top of DirectX’s software renderer (WARP), just like I had been doing with ANGLE. I’ve finally bothered to report the graphical issues, so hopefully it gets fixed eventually. If it does get fixed, Microsoft Store distribution means it shouldn’t take too long to trickle down to users, and then we can stop enforcing ANGLE on Qualcomm devices.&lt;/p&gt;
    &lt;p&gt;For Space Station 14, I will say that this means I’ll be postponing official Windows ARM64 support for now. At least until either bug (OpenGL on D3D12 or ARM64 DXGI detours) are fixed. Or when I finally rewrite the renderer to drop OpenGL, that’s also an option.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Debugging&lt;/p&gt;&lt;code&gt;SS14.Loader.exe&lt;/code&gt;directly would be a pain in the ass because it needs like a dozen arguments and environment variables configured by the launcher. I’d rather not. ↩︎&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I assume there’s a way to configure WinDbg to load these if the paths don’t line up properly… but I wouldn’t know how. Lol. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This is because, being a .NET app, most native libraries are dynamically loaded at runtime. Only libraries that are direct dependencies of the&lt;/p&gt;&lt;code&gt;.exe&lt;/code&gt;are available in the “initial debugger break” period before the program really starts. ↩︎&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you were one of those people that held onto Windows 7 for as long as possible, this is the kind of shit you were missing out on. Seriously, 8.1 was fine. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fuck EGL especially. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://slugcat.systems/post/25-09-21-dxgi-debugging-microsoft-put-me-on-a-list/"/><published>2025-09-21T14:45:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323297</id><title>How to Stop Functional Programming</title><updated>2025-09-21T16:39:12.748988+00:00</updated><content>&lt;doc fingerprint="7ad3b18718b551b1"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;How to stop functional programming&lt;/head&gt;
    &lt;p&gt;The following has never happened to me but I often hear stories.&lt;/p&gt;
    &lt;p&gt;You go into work and discover that a coworker isn't happy with some code you wrote because they don't understand it. They go to your manager and tell them that you're being a problem by writing code they don't understand. Your manager, being very skilled in conflict resolution, makes a technical decision to avoid whatever tool you used which caused the problem. In your case it was functional programming.&lt;/p&gt;
    &lt;p&gt;That's it. You've been told. No more functional programming.&lt;/p&gt;
    &lt;p&gt;The manager has figured out what's good for the business and you figure that listening is what's good for your job.&lt;/p&gt;
    &lt;p&gt;You get back to your desk and take a ticket from JIRA. You've got to add a page listing a person's coworkers to your internal employee directory. First you write a function you need.&lt;/p&gt;
    &lt;code&gt;def userCoworkers(u: User): List[Employee] =
  u.departments.flatMap(_.employees)&lt;/code&gt;
    &lt;p&gt;But it's pure. You're doing functional programming! Stop&lt;/p&gt;
    &lt;code&gt;def userCoworkers(u: User): List[Employee] = {
  val coworkers = ListBuffer[Employee]()
  for { d &amp;lt;- departments }
    coworkers ++ d.employees
  coworkers.toList
}&lt;/code&gt;
    &lt;p&gt;Well, there's a side-effect involved, but the whole method is pure. You're still doing functional programming!&lt;/p&gt;
    &lt;code&gt;def userCoworkers(u: User): List[Employee] = {
  logger.info("Collecting coworkers")
  val coworkers = ListBuffer[Employee]()
  for { d &amp;lt;- departments }
    coworkers ++ d.employees
  coworkers.toList
}&lt;/code&gt;
    &lt;p&gt;Now the method has 1 external side-effect. Is that enough? With "no functional programming" you've been given a lower-bound of 1 side-effect per method but we don't really know what the ideal number is. Hopefully you can slip it through code review.&lt;/p&gt;
    &lt;p&gt;After this exercise you've learned how easy it is to not do functional programming.&lt;/p&gt;
    &lt;p&gt;You show it to your product manager. They didn't realise how many coworkers the average person had. The page is huge. They ask you to just change it to a number.&lt;/p&gt;
    &lt;p&gt;You're going to have to add numbers together. Without being pure. You're going to have to think about this one.&lt;/p&gt;
    &lt;p&gt;Maybe you should ask your manager how to do it. Good luck.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://brianmckenna.org/blog/howtostopfp"/><published>2025-09-21T14:55:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323619</id><title>A C library offering generic, contiguous dynamic arrays with O(1) amortized push</title><updated>2025-09-21T16:39:12.394036+00:00</updated><content>&lt;doc fingerprint="6cf0826d02a71269"&gt;
  &lt;main&gt;
    &lt;p&gt;Vec is a generic, fast, leak‑safe dynamic array for C. It stores elements contiguously, grows geometrically (×2) for amortized O(1) push, and offers a method‑style API that feels natural if you like object syntax in C. The library is defensive by default: overflow guards before allocations, bounds‑checked accessors, and well‑defined behavior for empty/shrink/destroy.&lt;/p&gt;
    &lt;p&gt;Why you might want it&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Contiguous memory: better cache locality and prefetching than node‑based structures, fewer pointer indirections, great for iteration and SIMD‑friendly data.&lt;/item&gt;
      &lt;item&gt;Geometric growth: doubles capacity at the edge, so most pushes are O(1). Resizes are infrequent and predictable.&lt;/item&gt;
      &lt;item&gt;Truly generic in C: type erasure via &lt;code&gt;elem_size&lt;/code&gt;— no macros, no code generation, no void* arithmetic in user code.&lt;/item&gt;
      &lt;item&gt;Ergonomic API: free‑function API in all builds; optional method‑style function pointers if you like object syntax in C; &lt;code&gt;begin&lt;/code&gt;/&lt;code&gt;end&lt;/code&gt;for clean for‑loops.&lt;/item&gt;
      &lt;item&gt;Safety first: guards against &lt;code&gt;size_t&lt;/code&gt;overflow on capacity math, safe&lt;code&gt;shrink(0)&lt;/code&gt;,&lt;code&gt;pop&lt;/code&gt;is a no‑op on empty,&lt;code&gt;vec_at&lt;/code&gt;returns NULL out‑of‑bounds,&lt;code&gt;destroy&lt;/code&gt;zeros state. The test suite runs clean under valgrind.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Design in a nutshell&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Contiguous storage: elements live in a single growable buffer. This maximizes spatial locality and enables tight pointer iteration: &lt;code&gt;for (T* it = begin; it != end; ++it) { ... }&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Growth strategy: capacity grows by ×2 when needed. The classic amortized analysis shows O(1) average push because the cost of copy during reallocation is spread across many future inserts.&lt;/item&gt;
      &lt;item&gt;Size vs capacity: &lt;code&gt;len&lt;/code&gt;is the number of elements;&lt;code&gt;capacity&lt;/code&gt;is how many fit without reallocating. Use&lt;code&gt;vec_reserve&lt;/code&gt;to pre‑grow, and&lt;code&gt;vec_shrink&lt;/code&gt;to release slack back to the allocator.&lt;/item&gt;
      &lt;item&gt;Robust realloc: &lt;code&gt;vec_shrink&lt;/code&gt;handles&lt;code&gt;len == 0&lt;/code&gt;by freeing and nulling the buffer (no dangling pointer from&lt;code&gt;realloc(ptr, 0)&lt;/code&gt;). All allocation paths guard against&lt;code&gt;SIZE_MAX / elem_size&lt;/code&gt;overflow before multiplying.&lt;/item&gt;
      &lt;item&gt;Predictable pointers: pointers from &lt;code&gt;vec_at&lt;/code&gt;,&lt;code&gt;vec_begin&lt;/code&gt;,&lt;code&gt;vec_end&lt;/code&gt;, and&lt;code&gt;vec_back&lt;/code&gt;are stable until a resizing operation (&lt;code&gt;push&lt;/code&gt;that grows,&lt;code&gt;reserve&lt;/code&gt;,&lt;code&gt;shrink&lt;/code&gt;). After a grow/shrink, reacquire pointers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Memory model (what you can rely on)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The backing storage is a single &lt;code&gt;unsigned char*&lt;/code&gt;block. Elements are laid out back‑to‑back with stride&lt;code&gt;elem_size&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Iteration uses standard pointer arithmetic over your element type: &lt;code&gt;T* it = (T*)vec_begin(&amp;amp;v); for (; it != (T*)vec_end(&amp;amp;v); ++it) ...&lt;/code&gt;(method‑style variants exist too).&lt;/item&gt;
      &lt;item&gt;Because the data is contiguous, bulk ops like &lt;code&gt;memcpy&lt;/code&gt;/&lt;code&gt;memmove&lt;/code&gt;for remove/shrink are straightforward and efficient.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Build, Install, Test&lt;/p&gt;
    &lt;code&gt;# Build static library in ./build
make

# Install header and library (default PREFIX=/usr/local)
sudo make install

# Run tests (links against installed lib, uses &amp;lt;vec.h&amp;gt;)
make test
&lt;/code&gt;
    &lt;p&gt;Linking in your project&lt;/p&gt;
    &lt;code&gt;cc -std=c11 -O2 your_app.c -lvec -o your_app
&lt;/code&gt;
    &lt;p&gt;Compile‑time option&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LITE mode (no method pointers in &lt;code&gt;Vec&lt;/code&gt;):&lt;list rend="ul"&gt;&lt;item&gt;Build the library with &lt;code&gt;-DLITE&lt;/code&gt;to minimize the&lt;code&gt;Vec&lt;/code&gt;footprint and expose only the free‑function API.&lt;/item&gt;&lt;item&gt;Example: &lt;code&gt;CFLAGS="-std=c11 -O2 -DLITE" make clean &amp;amp;&amp;amp; make&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Build the library with &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Usage examples&lt;/p&gt;
    &lt;p&gt;Free‑function style (works in all builds):&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;vec.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
  Vec v = vec_new(sizeof(int));
  for (int i = 1; i &amp;lt;= 5; ++i) vec_push(&amp;amp;v, &amp;amp;i);

  for (int *it = (int*)vec_begin(&amp;amp;v); it != (int*)vec_end(&amp;amp;v); ++it) {
    printf("%d ", *it);
  }
  printf("| at[0]=%d | back=%d\n",
         *(int*)vec_at(&amp;amp;v, 0),
         *(int*)vec_back(&amp;amp;v));

  vec_destroy(&amp;amp;v);
  return 0;
}&lt;/code&gt;
    &lt;p&gt;Method‑style (requires non‑LITE build):&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;vec.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
  Vec v = vec_new(sizeof(char));
  const char s[] = "abcdef";
  for (size_t i = 0; i &amp;lt; sizeof(s)-1; ++i) v.push(&amp;amp;v, &amp;amp;s[i]);

  for (char *it = v.begin(&amp;amp;v); it != v.end(&amp;amp;v); ++it) printf("%c ", *it);
  printf("| at[1]=%c | back=%c\n",
         *(char*)v.at(&amp;amp;v, 1),
         *(char*)v.back(&amp;amp;v));

  v.destroy(&amp;amp;v);
  return 0;
}&lt;/code&gt;
    &lt;p&gt;Safety &amp;amp; semantics&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allocation safety: capacity growth checks avoid &lt;code&gt;size_t&lt;/code&gt;overflow; on failure, operations return&lt;code&gt;NULL&lt;/code&gt;/&lt;code&gt;0&lt;/code&gt;and set&lt;code&gt;errno = ENOMEM&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;vec_at(v,i)&lt;/code&gt;: returns&lt;code&gt;NULL&lt;/code&gt;when&lt;code&gt;i &amp;gt;= len&lt;/code&gt;or data is&lt;code&gt;NULL&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;vec_pop(v)&lt;/code&gt;: no‑op when empty.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;vec_shrink(v)&lt;/code&gt;: if&lt;code&gt;len == 0&lt;/code&gt;, frees storage and sets&lt;code&gt;data=NULL&lt;/code&gt;,&lt;code&gt;capacity=0&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;vec_destroy(v)&lt;/code&gt;: frees and resets all fields to a safe, zero state.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Complexity&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;push&lt;/code&gt;: amortized O(1) with ×2 growth.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;reserve&lt;/code&gt;: O(n) only when it must reallocate; O(1) otherwise.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;remove(i)&lt;/code&gt;: O(n − i) due to tail shift.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;at&lt;/code&gt;,&lt;code&gt;back&lt;/code&gt;,&lt;code&gt;begin&lt;/code&gt;,&lt;code&gt;end&lt;/code&gt;,&lt;code&gt;clear&lt;/code&gt;,&lt;code&gt;pop&lt;/code&gt;: O(1).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;API reference&lt;/p&gt;
    &lt;p&gt;Types&lt;/p&gt;
    &lt;code&gt;typedef struct Vec {
  unsigned char *data;
  size_t len;
  size_t capacity;
  size_t elem_size;

#ifndef LITE
  void* (*push)   (struct Vec*, const void*);
  void  (*pop)    (struct Vec*);
  void  (*shrink) (struct Vec*);
  void* (*at)     (struct Vec*, size_t);
  void  (*destroy)(struct Vec*);
  int   (*reserve)(struct Vec*, size_t);
  void  (*clear)  (struct Vec*);
  void* (*back)   (struct Vec*);
  int   (*remove) (struct Vec*, size_t);
  void* (*begin)  (struct Vec*);
  void* (*end)    (struct Vec*);
#endif
} Vec;&lt;/code&gt;
    &lt;p&gt;Constructors &amp;amp; memory&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;Vec vec_new(size_t elem_size);&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;New empty vector for elements of &lt;code&gt;elem_size&lt;/code&gt;bytes.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;New empty vector for elements of &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;void vec_destroy(Vec* v);&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Free internal storage and reset fields to a zero/NULL state.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Capacity &amp;amp; size&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;int vec_reserve(Vec* v, size_t new_capacity);&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Ensure capacity ≥ &lt;code&gt;new_capacity&lt;/code&gt;. Returns 1 on success, 0 on failure (errno=ENOMEM).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Ensure capacity ≥ &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;void vec_shrink(Vec* v);&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Shrink allocation to exactly &lt;code&gt;len&lt;/code&gt;. If&lt;code&gt;len==0&lt;/code&gt;, free storage and set&lt;code&gt;capacity=0&lt;/code&gt;.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Shrink allocation to exactly &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;void vec_clear(Vec* v);&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Set &lt;code&gt;len=0&lt;/code&gt;without freeing capacity.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Set &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Element access &amp;amp; modification&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;void* vec_push(Vec* v, const void* elem);&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Append a copy of &lt;code&gt;*elem&lt;/code&gt;(of size&lt;code&gt;elem_size&lt;/code&gt;). Returns pointer to inserted slot, or&lt;code&gt;NULL&lt;/code&gt;(errno=ENOMEM).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Append a copy of &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;void vec_pop(Vec* v);&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Remove last element if any; no‑op when empty.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;int vec_remove(Vec* v, size_t i);&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Remove element at &lt;code&gt;i&lt;/code&gt;(shifts tail). Returns 1 on success, 0 if&lt;code&gt;i &amp;gt;= len&lt;/code&gt;.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Remove element at &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;void* vec_at(Vec* v, size_t i);&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Pointer to element at &lt;code&gt;i&lt;/code&gt;, or&lt;code&gt;NULL&lt;/code&gt;when out‑of‑bounds.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Pointer to element at &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;void* vec_back(Vec* v);&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Pointer to last element, or &lt;code&gt;NULL&lt;/code&gt;when empty.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Pointer to last element, or &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Iteration&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;void* vec_begin(Vec* v);&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Pointer to the first element, or &lt;code&gt;NULL&lt;/code&gt;when empty.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Pointer to the first element, or &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;void* vec_end(Vec* v);&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;One‑past‑the‑end pointer (safe to compare, not to dereference). When &lt;code&gt;len==0&lt;/code&gt;, may be&lt;code&gt;NULL&lt;/code&gt;.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;One‑past‑the‑end pointer (safe to compare, not to dereference). When &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Method‑style (non‑LITE)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The same operations are exposed as function pointers on &lt;code&gt;Vec&lt;/code&gt;when built without&lt;code&gt;-DLITE&lt;/code&gt;.&lt;list rend="ul"&gt;&lt;item&gt;Example: &lt;code&gt;v.push(&amp;amp;v, &amp;amp;elem);&lt;/code&gt;and&lt;code&gt;for (T* it = v.begin(&amp;amp;v); it != v.end(&amp;amp;v); ++it) ...&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Example: &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Portability&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uses &lt;code&gt;SIZE_MAX&lt;/code&gt;from&lt;code&gt;&amp;lt;stdint.h&amp;gt;&lt;/code&gt;(C99+). Tested with standard libcs on Linux/macOS.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/L-A-Marchetti/Vec"/><published>2025-09-21T15:29:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323623</id><title>How far can you go by train in 5 hours? (interactive map)</title><updated>2025-09-21T16:39:12.301333+00:00</updated><content/><link href="https://old.chronotrains.com"/><published>2025-09-21T15:29:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323793</id><title>Oxford loses top 3 university ranking for the first time</title><updated>2025-09-21T16:38:48.832379+00:00</updated><content>&lt;doc fingerprint="c0fc00182795c8d8"&gt;
  &lt;main&gt;
    &lt;p&gt;The University of Oxford has fallen out of the top three universities in the UK for the first time, according to The Times and The Sunday Times Good University Guide for 2026.&lt;/p&gt;
    &lt;p&gt;Both Oxford and Cambridge universities have been supplanted by Durham University, which now holds the third-place spot among the top universities in the UK.&lt;/p&gt;
    &lt;p&gt;Oxford and Cambridge are tied for fourth in the 2026 rankings, after falling due to their relatively poor performance in the latest National Student Survey.&lt;/p&gt;
    &lt;p&gt;Durham University was named The Times’s University of the Year, although the number-one ranked university in the UK remained the London School of Economics and Political Science (LSE) for the second year in a row.&lt;/p&gt;
    &lt;p&gt;Second place was held by the University of St Andrews, again for the second consecutive year.&lt;/p&gt;
    &lt;p&gt;While the University of St Andrews ranked very highly in student experience and teaching quality, it lost out to the LSE in graduate prospects and research quality.&lt;/p&gt;
    &lt;p&gt;Durham University improved by 30 places year-on-year in its students’ evaluation of teaching quality, which was the main driver in securing its third place in the overall university league table.&lt;/p&gt;
    &lt;p&gt;“Durham is an outstanding place to study. We ensure that every student can grow and thrive here,” said Durham University Vice-Chancellor Professor Karen O’Brien.&lt;/p&gt;
    &lt;p&gt;“Our loyal, engaged alumni are testament to the impressive career prospects that await our graduates.”&lt;/p&gt;
    &lt;p&gt;The table below shows the top 20 universities in the United Kingdom, according to The Times University Rankings 2026.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Rank&lt;/cell&gt;
        &lt;cell role="head"&gt;University&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;London School of Economics and Political Science&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;University of St Andrews&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;Durham University&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4/5&lt;/cell&gt;
        &lt;cell&gt;University of Cambridge&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4/5&lt;/cell&gt;
        &lt;cell&gt;University of Oxford&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;Imperial College London&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;University of Bath&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;University of Warwick&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;University College London&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;University of Bristol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;University of Strathclyde&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;Loughborough University&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;University of Sheffield&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;University of Exeter&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;Lancaster University&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;University of Birmingham&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;17&lt;/cell&gt;
        &lt;cell&gt;University of Southampton&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;University of Liverpool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;King’s College London&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;University of York&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hotminute.co.uk/2025/09/19/oxford-loses-top-3-university-ranking-for-the-first-time/"/><published>2025-09-21T15:51:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323856</id><title>LaLiga's Anti-Piracy Crackdown Triggers Widespread Internet Disruptions in Spain</title><updated>2025-09-21T16:38:48.613130+00:00</updated><content/><link href="https://reclaimthenet.org/laligas-anti-piracy-crackdown-triggers-widespread-internet-disruptions"/><published>2025-09-21T15:57:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45323901</id><title>Review: Project Xanadu – The Internet That Might Have Been</title><updated>2025-09-21T16:38:48.289989+00:00</updated><content>&lt;doc fingerprint="3f9c91321ca38a88"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Your Review: Project Xanadu - The Internet That Might Have Been&lt;/head&gt;
    &lt;head rend="h3"&gt;Finalist #12 in the Review Contest&lt;/head&gt;
    &lt;p&gt;[This is one of the finalists in the 2025 review contest, written by an ACX reader who will remain anonymous until after voting is done. I’ll be posting about one of these a week for several months. When you’ve read them all, I’ll ask you to vote for a favorite, so remember which ones you liked]&lt;/p&gt;
    &lt;head rend="h1"&gt;1. The Internet That Would Be&lt;/head&gt;
    &lt;p&gt;In July 1945, Vannevar Bush was riding high.&lt;/p&gt;
    &lt;p&gt;As Director of the Office of Scientific Research and Development, he’d won World War II. His proximity fuse intercepted hundreds of V-1s and destroyed thousands of tanks, carving a path for Allied forces through the French countryside. Back in 1942, he’d advocated to President Roosevelt the merits of Oppenheimer’s atomic bomb. Roosevelt and his congressional allies snuck hundreds of millions in covert funding to the OSRD’s planned projects in Oak Ridge and Los Alamos. Writing directly and secretively to Bush, a one-line memo in June expressed Roosevelt’s total confidence in his Director: “Do you have the money?”&lt;/p&gt;
    &lt;p&gt;Indeed he did. The warheads it bought would fall on Hiroshima and Nagasaki in mere weeks. The Germans had already given up; Victory in the Pacific was nigh. So Bush was thinking ahead.&lt;/p&gt;
    &lt;p&gt;In The Atlantic, Bush returned to a pre-war obsession with communication and knowledge-exchange. His essay, “As We May Think,” imagined a new metascientifical endeavor (emphasis mine):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Science has provided the swiftest communication between individuals; it has provided a record of ideas and has enabled man to manipulate and to make extracts from that record so that knowledge evolves and endures throughout the life of a race rather than that of an individual.&lt;/p&gt;
      &lt;p&gt;There is a growing mountain of research. But there is increased evidence that we are being bogged down today as specialization extends. The investigator is staggered by the findings and conclusions of thousands of other workers—conclusions which he cannot find time to grasp, much less to remember, as they appear. Yet specialization becomes increasingly necessary for progress, and the effort to bridge between disciplines is correspondingly superficial.&lt;/p&gt;
      &lt;p&gt;…&lt;/p&gt;
      &lt;p&gt;The difficulty seems to be, not so much that we publish unduly in view of the extent and variety of present day interests, but rather that publication has been extended far beyond our present ability to make real use of the record. The summation of human experience is being expanded at a prodigious rate, and the means we use for threading through the consequent maze to the momentarily important item is the same as was used in the days of square-rigged ships.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Bush thought we were ripe for a paradigm shift. Some new method of spreading research, connecting it across fields and domains, and making new discoveries in the in-betweens. The most exciting Next Big Thing of the era was microfilm, and so when Bush let his imagination run a little wild,1 he envisioned a machine enabling us to do grand new things with long books shrunk into tidy rolls:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, “memex” will do. A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.&lt;/p&gt;
      &lt;p&gt;It consists of a desk, and while it can presumably be operated from a distance, it is primarily the piece of furniture at which he works. On the top are slanting translucent screens, on which material can be projected for convenient reading. There is a keyboard, and sets of buttons and levers. Otherwise it looks like an ordinary desk.&lt;/p&gt;
      &lt;p&gt;In one end is the stored material. The matter of bulk is well taken care of by improved microfilm. Only a small part of the interior of the memex is devoted to storage, the rest to mechanism. Yet if the user inserted 5000 pages of material a day it would take him hundreds of years to fill the repository, so he can be profligate and enter material freely.&lt;/p&gt;
      &lt;p&gt;Most of the memex contents are purchased on microfilm ready for insertion. Books of all sorts, pictures, current periodicals, newspapers, are thus obtained and dropped into place. Business correspondence takes the same path. And there is provision for direct entry. On the top of the memex is a transparent platen. On this are placed longhand notes, photographs, memoranda, all sorts of things. When one is in place, the depression of a lever causes it to be photographed onto the next blank space in a section of the memex film, dry photography being employed.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Not only could you read and even add to the memex—you could recombine and link works between each other with ease. “This is the essential feature of the memex. The process of tying two items together is the important thing,” Bush wrote. As a memex user explored his vast library of human thought, he could leave a “trail” of connected articles and photos and his own commentaries. He could connect these trails to one another, split them into fractally expanding branches, save them, and access them over and over again. He could even share his trails with friends, allowing them to insert copies into their own memexes, where they could be expanded and branched and shared again.&lt;/p&gt;
    &lt;p&gt;I’ll remind you—the year was 1945.&lt;/p&gt;
    &lt;head rend="h1"&gt;2. First Experiments in Hyper-cyber-space&lt;/head&gt;
    &lt;p&gt;Bush never did much to make his memex a reality. He was too busy building the National Science Foundation and trying to prevent a nuclear arms race. He had no time to fiddle around with desk-sized personal libraries, fighting Truman’s hawkish hyperfocus on hydrogen warheads.&lt;/p&gt;
    &lt;p&gt;But Doug Engelbart didn’t have much else to do.&lt;/p&gt;
    &lt;p&gt;He was a Navy man, a radar technician, just 20 years old when he shipped out of San Francisco. As he tells it, the entire crew were very nervous, seeing as they were being sent off to invade Japan. But just as the ship sailed past the Bay Bridge, “the captain came out on the bridge and looked down on us. ‘Japan just surrendered!’ he shouts. And suddenly all propriety leaves us, and we all say, ‘well then, for Christ’s sake, turn around!’”&lt;/p&gt;
    &lt;p&gt;Of course, they didn’t, and so Engelbart spent two years faffing around in the Philippines. He lived on a remote island with nothing to do but read and read and read. He spent his first five days camping out by a little stilt hut with a sign reading “Red Cross Library”—and in the Red Cross Library, there was a copy of the September 1945 issue of LIFE magazine in which Vannevar Bush’s description of the memex had been reprinted.&lt;/p&gt;
    &lt;p&gt;Engelbart claimed that he found the idea “intriguing,” but had lots of radar-technician-ing to do or something, and so it didn’t really resurface for him until 15 years later, when he was writing his Augmenting Human Intellect: A Conceptual Framework. Engelbart quoted heavily from Bush’s article, and commented:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The associative trails whose establishment and use within the files he describes at some length provide a beautiful example of a new capability in symbol structuring that derives from new artifact-process capability, and that provides new ways to develop and portray concept structures. Any file is a symbol structure whose purpose is to represent a variety of concepts and concept structures in a way that makes them maximally available and useful to the needs of the human's mental-structure development—within the limits imposed by the capability of the artifacts and human for jointly executing processes of symbol-structure manipulation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;After his Framework was published in 1962, under the Stanford Research Institute, Engelbart founded the Augmentation Research Center to make, in essence, some version of the Memex a reality. The ARC received funding from NASA and ARPA, and after six years, Engelbart released his oN-Line System (NLS). It was a revelation.&lt;/p&gt;
    &lt;p&gt;Engelbart had invented a vast array of tools—including, according to his own Institute:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;the mouse&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2-dimensional display editing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;in-file object addressing, linking&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;hypermedia&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;outline processing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;flexible view control&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;multiple windows&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;cross-file editing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;integrated hypermedia email&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;hypermedia publishing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;document version control&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;shared-screen teleconferencing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;computer-aided meetings&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;formatting directives&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;context-sensitive help&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;distributed client-server architecture&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;uniform command syntax&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;universal "user interface" front-end module&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;multi-tool integration&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;grammar-driven command language interpreter&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;protocols for virtual terminals&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;remote procedure call protocols&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;compilable "Command Meta Language"&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Live on stage, in the year 1968, Engelbart started up the NLS, opened a document, and typed some words into it. The words, he said, constituted a statement. And statements made up a file. Engelbart copied, manipulated, saved, and loaded his words and statements and files, zipping around with his newly-invented mouse. He demonstrated his ability to embed documents in one another—images with links to statements, words nested and categorized by one another, files filled with metadata.&lt;/p&gt;
    &lt;p&gt;And then he paused, and the screen went blank. He explained that he and his colleagues at the ARC had been using this system to do their daily work for the last six months. He mentioned that they had, now, six consoles up and running. He showed the crowd a real document, then navigated to a statement within it. “This presentation is devoted to the AHIRC.”&lt;/p&gt;
    &lt;p&gt;“What is the AHIRC?” he asked.&lt;/p&gt;
    &lt;p&gt;Engelbart “froze” the initial statement, clicked on the acronym, and below the words “Augmented-Human-Intellect Research Center” appeared. He kept clicking and freezing, and a trail of nested and related information appeared—a list of funders, a graph of staffing over time, a mission statement. This was hypermedia. These were hyperlinks, he explained. NLS was a hypertext system.&lt;/p&gt;
    &lt;p&gt;The presentation went on for 90 minutes longer, and became known as The Mother of All Demos.2 At around the 75-minute mark, Engelbart shows that two different NLS users could edit a single document simultaneously. While this was extremely impressive functionality, it was achieved with time-sharing—computation was done on a single machine, switching rapidly between tasks—and became infeasible the very next year, when ARPANET was released and the number of machines you could connect to one system grew rapidly.&lt;/p&gt;
    &lt;p&gt;Engelbart’s hypertext system was impressive in its own right, even without collaborativity. And still, little came of it—Andy van Dam, an attendee and revolutionary computer scientist himself, would reflect decades later: “Everybody was blown away … and nothing else happened. There was almost no further impact.” Engelbart’s ideas were just a little too out there.&lt;/p&gt;
    &lt;p&gt;ARC quickly faded into obscurity. In 1972, Engelbart joined an organization called Erhard Seminars Training. EST, or “est” as it was marketed, offered a 60-hour self-improvement course for tech entrepreneurs modeled loosely on Zen Buddhism. Critics suggested that the est course was a mind-control method aimed at raising an authoritarian army. It was quite credibly branded a cult. The founder of est, Werner Erhard, was accused of tax fraud (he fought the claims and won $200,000 from the IRS) and incest (by his daughter, who later recanted).&lt;/p&gt;
    &lt;p&gt;Engelbart served, for many years, on est’s board of directors.&lt;/p&gt;
    &lt;p&gt;His researchers all left for greener, less cult-y pastures, and ARC died with hardly a whimper. No one really wanted to associate with Engelbart. His crackpot theories about an internet modeled after the memex fell into disrepute, and, if he was remembered at all, it was for the invention of the mouse. No one cared anymore about the memex, or hypertext.&lt;/p&gt;
    &lt;head rend="h1"&gt;3. Hyper-dreams of Hyper-everything&lt;/head&gt;
    &lt;p&gt;Well, one man cared.&lt;/p&gt;
    &lt;p&gt;Ted Nelson was born in 1937 to two twenty-year-olds, Ralph Nelson and Celeste Holm. His parents divorced in 1939, leaving him to be raised by his grandparents. Both Nelson (the elder) and Holm would go on to extremely-successful film careers: the former became an Emmy-winning director; the latter an Oscar-winning actress. And, at first, Ted seemed to be following in their footsteps.&lt;/p&gt;
    &lt;p&gt;As a philosophy major at Swarthmore College, he produced a film called The Epiphany of Slocum Furlow, which he described as “a short comedy about loneliness at college and the meaning of life.”3 Nelson also claims to have “[d]irected [and written] book and lyrics for what was apparently the first rock musical” in his junior year at Swarthmore.&lt;/p&gt;
    &lt;p&gt;Thankfully, his interest in a career as an entertainer soon waned, and Nelson went off to study sociology in grad school—first at the University of Chicago, then at Harvard. Nelson took a computer class at Harvard, in 1960, and “[his] world exploded.”4 He realized the incredible power of computing, quickly intuited that these new machines could be generally applied to everything, and founded Project Xanadu.5&lt;/p&gt;
    &lt;p&gt;Initially, Xanadu’s scope was pretty limited. Word processors weren’t around yet, but Nelson wanted to build something strikingly similar: he wanted to write a program that could store and display documents, with version histories and edits all stored and displayed at the same time too. Later, Nelson would call this version-history feature “intercomparison.” (Strange coinages will be a… theme; I’m just trying to get you ready.)&lt;/p&gt;
    &lt;p&gt;Nelson began working on an implementation, but his feature wishlist grew quickly, and he didn’t really know what he was doing, so in 1965, he sought help. He prepared a talk for the Association for Computing Machinery, and dropped, quite frankly, a bomb on the audience:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The kinds of file structures required if we are to use the computer for personal files and as an adjunct to creativity are wholly different in character from those customary in business and scientific data processing. They need to provide the capacity for intricate and idiosyncratic arrangements, total modifiability, undecided alternatives, and thorough internal documentation.&lt;/p&gt;
      &lt;p&gt;The original idea was to make a file for writers and scientists, much like the personal side of Bush's Memex, that would do the things such people need with the richness they would want. But there are so many possible specific functions that the mind reels. These uses and considerations become so complex that the only answer is a simple and generalized building-block structure, user-oriented and wholly general-purpose.&lt;/p&gt;
      &lt;p&gt;The resulting file structure is explained and examples of its use are given.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ted Nelson was building the memex.&lt;/p&gt;
    &lt;p&gt;Of course, he wasn’t a very technical guy, and so his talk mostly focused on the philosophy of Xanadu, not its implementation. He commented (emphasis mine):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are three false or inadequate theories of how writing is properly done. The first is that writing is a matter of inspiration. While inspiration is useful, it is rarely enough in itself. “Writing is 10% inspiration, 90% perspiration,” is a common saying. But this leads us to the second false theory, that “writing consists of applying the seat of the pants to the seat of the chair.” Insofar as sitting facilitates work, this view seems reasonable, but it also suggests that what is done while sitting is a matter of comparative indifference; probably not.&lt;/p&gt;
      &lt;p&gt;The third false theory is that all you really need is a good outline, created on prior consideration, and that if the outline is correctly followed the required text will be produced. For most good writers this theory is quite wrong. Rarely does the original outline predict well what headings and sequence will create the effects desired: the balance of emphasis, sequence of interrelating points, texture of insight, rhythm, etc. We may better call the outlining process inductive: certain interrelations appear to the author in the material itself, some at the outset and some as he works. He can only decide which to emphasize, which to use as unifying ideas and principles, and which to slight or delete, by trying. Outlines in general are spurious, made up after the fact by examining the segmentation of a finished work. If a finished work clearly follows an outline, that outline probably has been hammered out of many inspirations, comparisons and tests.&lt;/p&gt;
      &lt;p&gt;Between the inspirations, then, and during the sitting, the task of writing is one of rearrangement and reprocessing, and the real outline develops slowly. The original crude or fragmentary texts created at the outset generally undergo many revision processes before they are finished. Intellectually they are pondered, juxtaposed, compared, adapted, transposed, and judged; mechanically they are copied, overwritten with revision markings, rearranged and copied again. This cycle may be repeated many times. The whole grows by trial and error in the processes of arrangement, comparison and retrenchment.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Nelson recognized that the creation of knowledge is cyclical, recursive, self-referential. And he figured that our computer systems should accept and reflect that process:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If a writer is really to be helped by an automated system, it ought to do more than retype and transpose: it should stand by him during the early periods of muddled confusion, when his ideas are scraps, fragments, phrases, and contradictory overall designs. And it must help him through to the final draft with every feasible mechanical aid—making the fragments easy to find, and making easier the tentative sequencing and juxtaposing and comparing.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;How do you design such a system? To navigate intuitively within complex file systems, between document versions, and across source materials—to access all the scraps and fragments writers need to write—you would need to establish what Vannevar Bush called “tracks.” You would need to connect and save different ideas, linking them together. That was it—you needed links.&lt;/p&gt;
    &lt;p&gt;Nelson went further, though—it wouldn’t do to simply have links to all the other files, a writer needed to see the other files before him, needed them to be brought up and displayed alongside his current work on demand. The links needed to contain their targets within themselves—so Nelson called them hyperlinks. And he called text embedded with hyperlinks hypertext, and movies embedded in his structure became hyperfilms, and so on. Nelson wanted us using computers to write and create self-referential, intricately-interconnected (“intertwingled,” as he’d later put it), eminently-accessible hypermedia.&lt;/p&gt;
    &lt;p&gt;And recall, in 1965, state-of-the-art computing looked like this.&lt;/p&gt;
    &lt;p&gt;Ted Nelson was thinking far, far ahead.&lt;/p&gt;
    &lt;p&gt;Maybe too far ahead. Conference attendees were initially excited about his idea, but when he revealed himself to know very little about the technical task of building Xanadu—or even whether it was possible at all—interest evaporated.&lt;/p&gt;
    &lt;head rend="h1"&gt;4. Failing to Develop Xanadu&lt;/head&gt;
    &lt;p&gt;But Nelson was all in. He would later write, “This is not a technical issue, but rather moral, aesthetic and conceptual.” Nelson loved knowledge and connection and abstraction—mere technical details wouldn’t stop him from building the best possible computer system for producing and consuming information.&lt;/p&gt;
    &lt;p&gt;He met Doug Engelbart in the mid 60s, forming a friendship with the only other man taking hypertext seriously at the time, and hopped around unhappily between various academic and scientific appointments. At one point, he and Andy van Dam worked together and produced the Hypertext Editing System—released in 1967, just before Engelbart’s NLS. It was the first computer application to ever have an “undo” button—Nelson claims to this day that he invented it (and the “back” button).&lt;/p&gt;
    &lt;p&gt;Shortly thereafter, Nelson’s wife left him. In his 2010 autobiography, he writes, “She, reasonably, wanted a Nice Life; women want that sort of thing.” They had a son, whom Nelson continued to visit regularly. “Debbie has been a friend and great support all these years,” Nelson adds. “[S]he believed in me.”&lt;/p&gt;
    &lt;p&gt;Nelson gave a talk at Union Theological Seminary in 1968 that included this slide, which Nelson considers “the first depiction of what the personal computer turned out to be.”&lt;/p&gt;
    &lt;p&gt;Around the same time, Nelson claims to have called Vannevar Bush and told him about Project Xanadu. Bush “wanted very much to discuss it with” Nelson, but Nelson “hated him instantly [because] he sounded like a sports coach” and never contacted him again. This, of course, proved to be extremely self-destructive (though I can’t honestly say I would’ve done otherwise).&lt;/p&gt;
    &lt;p&gt;Because Xanadu was as good as dead. No one would give him the money he needed to work on it, especially not after Doug Engelbart poisoned the idea of hypertext.&lt;/p&gt;
    &lt;p&gt;Nelson went where there was funding, working briefly on an early word processor called Juggler of Text (JOT). …And then he lost investment, stopped working on the project, and moved to Chicago, where he’d been offered a job teaching at the University of Illinois, to start work on a book. He would call it Computer Lib.&lt;/p&gt;
    &lt;p&gt;In fact, he started work on another book at the same time, called Dream Machines. By the time he completed each of them, in 1974, ARPANET had been released, and his vision for Project Xanadu had evolved. He published the two works together—Computer Lib was his lamentation over the industry’s disdain for hypertext, and Dream Machines was Xanadu’s manifesto.&lt;/p&gt;
    &lt;p&gt;Nelson designed and printed the book himself. Its pages mostly look like this:&lt;/p&gt;
    &lt;p&gt;Self-referential, multimedia, creative, and fun—they were a blueprint for the internet he was building. In the Dream Machines half, Nelson writes, “The real dream is for ‘everything’ to be in the hypertext. Everything you read, you read from the screen (and can always get back to right away; everything you write, you write at the screen (and can cross-link to whatever you read).”&lt;/p&gt;
    &lt;p&gt;In one section Nelson asks himself, “Can It Be Done?” His answer: “I dunno.”&lt;/p&gt;
    &lt;p&gt;Remember, Xanadu wouldn’t only involve links between works—it required hyperlinks, which as Nelson understood them, would need to contain the targets in themselves. (Eventually, Nelson would give these embeddings a new name—“transclusions”—and hyperlink came to simply mean “link between hypertext files.”) Every link would run both ways, each hypertext file would know exactly which other files were linked to it and how.&lt;/p&gt;
    &lt;p&gt;This introduced a few problems, in the new interconnected ARPANET age:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;How do you keep track? Where’s the metadata stored? Can you afford enough space for it all?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Who’s keeping track? Nelson was already, allegedly, approached by the CIA over this all—how do you make sure hypertext is a free, democratizing technology that doesn’t spread government propaganda?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;What do you do about intellectual property? You don’t want everyone to be able to link everyone else’s work if each link contains the work itself—how do you ensure that people still get paid for their ideas?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nelson answered (in 1974):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The docuverse keeps track! Xanadu wouldn’t simply be a platform for linkage—it would be the repository for all existing connections between human thought. It would be a universal library.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Storage of the docuverse will be distributed, people can use pseudonyms, and eventually we’ll figure out some good system for authenticating the texts everyone’s linking to.6&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Simply put a royalty on the links. If you want to reference a copyrighted New York Times article, then you’ve got to pay the author a little bit. And if someone else links to what you’ve written, then you get a small payout. Presumably, you could build in caveats for short excerpts and fair use kinds of things—“a universal flexible rule [still] has to be worked out.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;He helpfully diagrammed the whole idea, in case it was at all confusing:&lt;/p&gt;
    &lt;p&gt;A pay-per-click system like Nelson described would first be implemented in 1996.&lt;/p&gt;
    &lt;p&gt;Computer Lib/Dream Machines became a cult favorite, and Nelson began to gather a small following. In 1979, he moved back to Swarthmore with a group of disciples, and they got to work. The crack team included:7&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Roger Gregory, a University of Michigan graduate and Ann Arbor local who’d been corresponding over telephone with Nelson since reading Computer Lib in 1974. Gregory was a whiz with hardware, but suffered from regular bouts of depression, sometimes so strong they would render him “incapable of working.” Gregory paid for the house in Pennsylvania.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mark Miller, a mathematical wunderkind who’d read Computer Lib and grokked it so hard that Nelson invited him to give a lecture to his UIC class when Miller was just 19, and a sophomore at Yale. The students all thought Nelson was crazy, and so they thought Miller was crazy too. Nelson thought him a genius.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stuart Greene was a UIC student who thought Nelson and Miller might not be so crazy. He was invited to Pennsylvania too. Nelson, in his autobiography, describes Greene as “the mystic who’d taught holography at 14.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Roland King, a linguist who, like Nelson, was super into an evangelical Christianity–associated theory of linguistics called “tagmemics.” I can’t make heads or tails of it, but Nelson describes it as a “romantic [extension] of the linguistic ideal.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Eric Hill, a 15-year-old hacker and indicted felon, who “had been dismissed by the judge with admiration.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In Swarthmore, Nelson hoped his decades-old dream of Xanadu would finally materialize.&lt;/p&gt;
    &lt;head rend="h1"&gt;5. Developing Xanadu&lt;/head&gt;
    &lt;p&gt;Ted Nelson had built Project Xanadu into, for lack of better terminology, a cult.8 He writes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We all were deeply concerned about the Bad Guys, who we saw as a combination of IBM and the government. (The others were all Libertarians, I still called myself a Cynical Socialist.) The Bad Guys would spy on people, withhold and block information, and give us inferior hypertext. We had to Do It Right, to help prevent this.&lt;/p&gt;
      &lt;p&gt;This meant using the standard business defenses—especially non-disclosure agreements (I made all of them sign) and secret proprietary algorithms.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Xanadians had a messiah—Ted Nelson—a gospel—Computer Lib—a persecution complex, a fearful dystopia—“inferior hypertext”—a hopeful utopia—Xanadu—and utter secrecy. Just six dudes in a rented house near Philly, building the internet, hiding from the Feds, signing NDAs, and saving the world.&lt;/p&gt;
    &lt;p&gt;Nelson spent a summer explaining the project to his team in its entirety. By the end, Gregory, Miller, and Greene were the only ones left. They told Nelson, “We’ll do it,” and moved to another suburb, where they finally began to work on an implementation of Xanadu. The three quickly figured out a new system that would allow users to reference and link to specific parts of a file—they called these links tumblers, and made them work with transfinite numbers. Suddenly, transclusions were really possible.&lt;/p&gt;
    &lt;p&gt;But after only a few early successes, the team’s progress stalled completely. Greene and Miller were young and left for jobs elsewhere, and so Gregory was left working on Xanadu alone.&lt;/p&gt;
    &lt;p&gt;Nelson, meanwhile, ran a magazine called Creative Computing for a while, then tried again to build his JOT word processor—this time for the Apple II—then spent a year in San Antonio pitching a watered-down version of Xanadu (rebranded as “Vortext”) to a tech company called Datapoint. Datapoint wasn’t buying, but kept Nelson on in some sort of fake, primitive email job anyway.&lt;/p&gt;
    &lt;p&gt;Gregory kept working on Xanadu in Philadelphia, slowly running out of money. Ted Nelson held an “Ecstasy party” in San Antonio: “A number of us floated down the river on inner tubes. It was quite lovely.”&lt;/p&gt;
    &lt;p&gt;In 1987, like he did every year, Roger Gregory went to The Hackers Conference in Saratoga to show off the latest unimpressive version of Xanadu. There, he met a man named John Walker—founder of the wildly successful Autodesk—and pitched the project to him. Incredibly, Walker was interested, and after tense negotiations with Nelson, agreed to fund Xanadu in earnest.&lt;/p&gt;
    &lt;p&gt;Beginning in 1988, Autodesk poured millions of dollars into the project, and a programming team led by Gregory finally started to make real progress. Walker said of Xanadu: “In 1980, it was the shared goal of a small group of brilliant technologists. By 1989, it will be a product. And by 1995, it will begin to change the world.”&lt;/p&gt;
    &lt;p&gt;Sweeping rhetoric—clear deadlines.&lt;/p&gt;
    &lt;p&gt;The team came nowhere close to meeting them. Infighting broke out between two factions—while Gregory simply wanted to patch together his old C code, insisting his product “was within six months of shipping,” the whiz-kid Mark Miller came back from his new job at Xerox PARC, alongside a half-dozen of his closest friends, and insisted on a perfectionistic rewrite in a more flexible language, Smalltalk.&lt;/p&gt;
    &lt;p&gt;The PARC faction began to drive Gregory up the wall. According to Nelson, it got to the point that he “was throwing things and acting crazy.” So Nelson called John Walker, the two “summoned Roger to meet [them] at John’s house at Muir Beach, and Walker told Roger he was no longer in charge.”&lt;/p&gt;
    &lt;p&gt;Miller took over and began the rewrite in Smalltalk. Walker’s deadline came and went, and the team delivered nothing. Xanadu’s offices descended into chaos—Miller anointed two PARC programmers to be “co-architects,” and the three of them increasingly left the rest of the team out of the loop. For four years, Miller dawdled about, adding features, giving them clever names (files were “berts,” after Bertrand Russell, and so, for symmetry’s sake, royalty-generating transclusions became “ernies”), and never building them.9&lt;/p&gt;
    &lt;p&gt;Meanwhile, Ted Nelson was living on a houseboat, attending sex retreats and Keristan orgies, and giving talks in Singapore. He recorded a new soundtrack for his student film, the one from 1959.&lt;/p&gt;
    &lt;p&gt;In 1992, Autodesk’s stock cratered, and they divested entirely from Xanadu. Miller lamented that his program was just six months from completion.&lt;/p&gt;
    &lt;p&gt;Ted Nelson started a film studio to make a movie with Doug Engelbart, then left for Japan to get a PhD.&lt;/p&gt;
    &lt;p&gt;Xanadu’s code was open-sourced in the late 90s.&lt;/p&gt;
    &lt;head rend="h1"&gt;6. The World Wide Web&lt;/head&gt;
    &lt;p&gt;In March 1989, a British computer scientist named Tim Berners-Lee, working at CERN, wrote a proposal for a system unifying hypertext and the internet. It was ignored.&lt;/p&gt;
    &lt;p&gt;In 1990, Berners-Lee resubmitted his proposal, it was accepted, and he began to work on the World Wide Web.&lt;/p&gt;
    &lt;p&gt;The WWW had a number of advantages over Xanadu:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;It was much simpler—Ted Nelson wrote of it disparagingly: “Where were annotation and marginal notes? Where was version management? Where was rights management? Where were multi-ended links? Where were third-party links? Where were transclusions? This ‘World Wide Web’ was just a lame text format and a lot of connected directories.” As it turns out, it’s much easier to build a lame text format and a lot of connected directories!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It had institutional buy-in from the start. CERN was huge, it saw promise in the WWW, and it gave Berners-Lee plenty of funding, latitude, and staffing.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tim Berners-Lee wasn’t a self-important lunatic. He didn’t join cults, nor did he start them. He didn’t attend sex workshops, nor did he intern at them. He was British and proper and serious, and so people took him and his work Britishly, properly, and seriously.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And so, despite Xanadu’s 30-year head start, the Web won the race.&lt;/p&gt;
    &lt;p&gt;By the occasion of Autodesk’s divestiture from Xanadu, everyone knew Berners-Lee’s creation was the Next Big Thing. It was released publicly in 1993—four years past John Walker’s deadline for Xanadu—and Netscape went public in 1995—Walker’s revolution came right on schedule.&lt;/p&gt;
    &lt;p&gt;But what kind of revolution was it, exactly?&lt;/p&gt;
    &lt;head rend="h1"&gt;7. This Is Hell.&lt;/head&gt;
    &lt;p&gt;Ted Nelson pulls no punches.&lt;/p&gt;
    &lt;p&gt;Think about the Web we have today. The 2.0 and 3.0 (however you choose to identify it) revolutions included.&lt;/p&gt;
    &lt;p&gt;What parts of Nelson’s wishlist have we checked off? What are we missing?&lt;/p&gt;
    &lt;p&gt;Ultimately the Web really is “just a lame text format and a lot of connected directories.” We’re reading and writing, publishing new kinds of media, calling up documents like crazy, democratizing publication to a fault, and… ah. Well, that’s all.&lt;/p&gt;
    &lt;p&gt;Vannevar Bush wrote, in 1945 (emphasis mine):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our ineptitude in getting at the record is largely caused by the artificiality of systems of indexing. When data of any sort are placed in storage, they are filed alphabetically or numerically, and information is found (when it is) by tracing it down from subclass to subclass. It can be in only one place, unless duplicates are used; one has to have rules as to which path will locate it, and the rules are cumbersome. Having found one item, moreover, one has to emerge from the system and re-enter on a new path.&lt;/p&gt;
      &lt;p&gt;The human mind does not work that way. It operates by association. With one item in its grasp, it snaps instantly to the next that is suggested by the association of thoughts, in accordance with some intricate web of trails carried by the cells of the brain. It has other characteristics, of course; trails that are not frequently followed are prone to fade, items are not fully permanent, memory is transitory. Yet the speed of action, the intricacy of trails, the detail of mental pictures, is awe-inspiring beyond all else in nature.&lt;/p&gt;
      &lt;p&gt;Man cannot hope fully to duplicate this mental process artificially, but he certainly ought to be able to learn from it. In minor ways he may even improve, for his records have relative permanency. The first idea, however, to be drawn from the analogy concerns selection. Selection by association, rather than indexing, may yet be mechanized.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Unlike Doug Engelhart, and unlike Ted Nelson, Tim Berners-Lee never read about Bush’s memex. He built a system that connected people like never before—but made little effort to facilitate the connection of ideas. There are no trails on the World Wide Web—instead, there are misattributed quotes, dead one-way links, constant plagiarism scandals, and widespread misinformation and mutual distrust. It’s often said that we’re living in a ‘post-truth society’. The words we write and videos we share have become entirely unmoored from the ideas underlying them. Strangely, the Web has facilitated more disconnection than was ever possible before.&lt;/p&gt;
    &lt;p&gt;Ted Nelson, in his own oblique and dodgy way, predicted the failure mode we’re now seeing: “This is not a technical issue, but rather moral, aesthetic and conceptual.” We built our global information-sharing system quickly, efficiently, and technically, when we should’ve treated it as a philosophical and aesthetic puzzle as much as a computational one, and built carefully and precisely.&lt;/p&gt;
    &lt;p&gt;Tim Berners-Lee took inspiration from the artificial citation and index and reference paradigm of old—he simply scaled up the paper-based system that Vannevar Bush knew was getting out of hand in the 1940s. He gave us a Web shaped like a machine—not a memex shaped like a mind—and then let everyone in the world talk to everyone else on his alien, unwelcoming platform. He built a cold and inhuman Web—so why would we be shocked that the online world became a cold and inhuman one?&lt;/p&gt;
    &lt;head rend="h1"&gt;8. Whither Xanadu?&lt;/head&gt;
    &lt;p&gt;It’s extremely hard to like Ted Nelson once you’ve read his autobiography. For instance, in the space of just two pages, he writes about how incredibly virtuous he is for not selling out to Bill Gates, that “friends often tell [him], ‘Oh, you should get a MacArthur Genius Grant!’,” and that Robin Williams once “squatted down beside” him and said: “I think it’s wonderful what you’ve done for the world.”&lt;/p&gt;
    &lt;p&gt;I don’t think I want to be Ted Nelson’s friend. He very clearly believes that he’s the Internet Messiah.&lt;/p&gt;
    &lt;p&gt;The only thing that gives me pause is that he might be right.&lt;/p&gt;
    &lt;p&gt;In 2014, a primitive Xanadu demo was released on the Web. (If you have a Windows machine, another nicer-looking demo exists for you to download.) I mean it when I say “primitive.” This isn’t close to the full product Nelson has been promising since 1965.&lt;/p&gt;
    &lt;p&gt;But as you play with the demo, scrolling and clicking around, you might just catch a glimpse. It’s all right there. All of the underlying ideas—the scraps and fragments of our nonlinear, recursive thought—traced back to their source. If you squint, almost to the point of closing your eyes, but not quite—you can just make it out. A hypertext system with connection, accountability, verifiability. A mind-shaped system—a real memex.&lt;/p&gt;
    &lt;p&gt;Maybe it looks a little unnatural, what you see when you squint at Xanadu—what a pain it would be to write in a Xanadu editor, you think. How ugly is that design!&lt;/p&gt;
    &lt;p&gt;But give the sight a little charity—imagine billions of dollars, maybe trillions, poured into Xanadu. Making it more beautiful, more intuitive. Imagine you’d never seen the Web before—no habits built, no understanding of what a webpage could or should be. What’s so wrong with Xanadu?&lt;/p&gt;
    &lt;p&gt;Why shouldn’t the internet look (and work) a little more like this?&lt;/p&gt;
    &lt;p&gt;For that matter, why doesn’t it?&lt;/p&gt;
    &lt;p&gt;Xanadu had a huge head start. Ted Nelson coined the term “hypertext.” He was doing all of this way before anyone else. He had a mind for design, he was smart, he was charismatic. Why didn’t he become the Steve Jobs of the Web?&lt;/p&gt;
    &lt;p&gt;I think we can, in large part, trace it back to Doug Engelbart, who, by blind, dumb luck, found himself on a remote Philippine island for two years with nothing to do but hang out in a big hut full of magazines. And there he happened to read Vannevar Bush’s essay, and then, fifteen years later, the thought happened to pop back into his head, and he happened to be a little better positioned, a little better at technology than Ted Nelson, and so he happened to make comprehensive hypertext a highly-visible reality before anyone else.&lt;/p&gt;
    &lt;p&gt;And then Engelbart joined and helped lead a mind-control cult, and so everyone became very wary of hypertext projects—especially hypertext projects led by cult-y weirdos—and then when Ted Nelson spent decades trying to get anyone interested in Xanadu, anyone at all, they just wouldn’t fund him.&lt;/p&gt;
    &lt;p&gt;Of course, Nelson deserves plenty of blame too. In many ways, he really was a nutjob, and he certainly wasn’t capable of building Xanadu on his own—still, the concept itself was solid! If Nelson hadn’t turned down Vannevar Bush and Bill Gates and Robin Williams and the half-dozen other famous people he claims were kissing his ass at one point or another, maybe someone sometime could’ve figured out how to build it for him. But he couldn’t do it. Nelson was too busy play-acting as a great, tortured, persecuted genius. By the time he’d become pacified enough to let Autodesk help him build Xanadu, he was too pacified to exercise any sort of authority or discipline over his project anymore. He just went to his sex parties and watched it all burn.&lt;/p&gt;
    &lt;head rend="h1"&gt;9. Lo and Behold&lt;/head&gt;
    &lt;p&gt;In 2016, Werner Herzog made a documentary called Lo and Behold, Reveries of the Connected World. In an interview after the film was released, Herzog explained his motivation:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I think we have to abandon this kind of false security that everything is settled now, that we have so much assistance by digital media and robots and artificial intelligence. At the same time, we overlook how vulnerable all this is, and how we are losing the essentials that make us human.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In Lo and Behold, between conversations with TCP/IP inventor Bob Kahn and a baby-faced non-insane Elon Musk, around the 11-minute mark, Herzog visits Ted Nelson on his houseboat.&lt;/p&gt;
    &lt;p&gt;His narration explains that Nelson has often been called insane. On screen, the near-octogenarian explains, as lucidly and self-importantly as ever: “There are two contradictory slogans: one is that continuing to do the same thing and expecting a different result is the definition of insanity. On the other hand, you say, ‘if at first you don’t succeed, try, try again.’ I prefer the latter. Because I don’t want to be remembered as the guy who didn’t.” Herzog replies: “To us, you appear to be the only one around who is clinically sane.”&lt;/p&gt;
    &lt;p&gt;The two shake hands, and Nelson produces a small camera from his pocket, taking a photo of Herzog and his crew. No doubt, he will file the picture somewhere in his vast, interlinked personal archives, where it will sit and wait, until the day that Xanadu is finally launched, to be uploaded to a true digital memex.&lt;/p&gt;
    &lt;p&gt;By all accounts, that day is only six months away.&lt;/p&gt;
    &lt;p&gt;Before getting onto the information-sharing mechanisms of the future, Vannevar Bush did a little imagining about information-recording too: he suggested that Bell Labs’ Vocoder (an early mechanical phoneme-to-text system) could be combined with a stenotype (a human operated, much more extensive, speaking speed–capable phoneme-to-text system) to produce a working speech-to-text machine. Then researchers would have no need to learn typing or to hire a secretary—they could simply speak their findings aloud, and have them automatically entered into the record! It’s interesting to me how this both absolutely came to be—lots of people use very impressively functional speech-to-text systems nowadays—and also largely didn’t—I typed the words you’re reading now with my own non-automated hands. This theme will recur—Bush having very good and important ideas that everyone claims inspiration from, but actually end up mostly perverting or ignoring.&lt;/p&gt;
    &lt;p&gt;Bush also wrote, presciently-though-not-quite-as-presciently-as-Turing-ly, that “[w]e may some day click off arguments on a machine with the same assurance that we now enter sales on a cash register.” He thought this would be a fairly deterministic process—eventually we’d find some way to encode our semantics perfectly into computer-readable symbols, and then we could use those new computer-readable symbols to construct logical arguments. This isn’t really what today’s arguing-machines do at all, but if you squint enough, it’s not a terribly inaccurate picture.&lt;/p&gt;
    &lt;p&gt;It’s on Youtube; I think you should watch it. When I was younger, my dad had me watch Steve Jobs’ iPhone presentation; held it up as a prime example of tech and sales, innovation and elegance all rolled up. I liked it at the time. Now, having watched Engelbart’s presentation, I recognize it for what it is: patronizing, mass-market garbage. It’s just nowhere near as cool.&lt;/p&gt;
    &lt;p&gt;This one’s on Youtube too. I don’t really recommend it. It’s pretty much what you’d expect upon hearing the description “late 1950s experimental student film about being a college student.” In some regard, it’s impressive for what it is, but it’s also very much what it is.&lt;/p&gt;
    &lt;p&gt;Here, I’m quoting Nelson’s autobiography, published in 2010. It’s called POSSIPLEX: Movies, Intellect, Creative Control, My Computer Life and the Fight for Civilization, and it’s even weirder than the title suggests.&lt;/p&gt;
    &lt;p&gt;Taking a page out of Jon Bois’ playbook, I’m gonna recommend you stop here for a moment, put on your headphones, turn the volume down to a not-so-misophonic level, and listen to twenty seconds or so of “Doomed Moon” from the 32-second mark, while staring unblinkingly at the words Project Xanadu. Your reading experience will be much enhanced.&lt;/p&gt;
    &lt;p&gt;In the 1987 edition of Computer Lib/Dream Machines, Nelson writes, “these are now called ‘authentication systems;’ very sophisticated ones exist, and the government is trying to suppress them.” He’s referring to public key cryptography, which wasn’t invented until 1976, and how an NSA official named Joseph A. Meyer had contacted three researchers—named Rivest, Shamir, and Adleman—just before they released a paper in 1977 that introduced a revolutionary new cryptosystem based on the public-key breakthrough.&lt;/p&gt;
    &lt;p&gt;My description of these men comes both from Nelson’s autobiography and from a classic article in the June 1995 edition of WIRED magazine called “The Curse of Xanadu.” The author, Gary Wolf, takes a somewhat less charitable view of Ted Nelson than I do: he describes Xanadu as “the longest-running vaporware project in the history of computing” and Nelson as “the king of unsuccessful software development.” In my view, the last 30 years of internet history have been extremely kind to Nelson’s legacy, and are reason to disregard much of Wolf’s snottiness in the article. (I do still recommend reading it, though, for a more detailed play-by-play of Xanadu’s history.)&lt;/p&gt;
    &lt;p&gt;What is it with hypertext pioneers and cults? I wonder if this simply has to do with the fact that these guys were so ahead of their time—the big guys like Tim Berners-Lee didn’t even start thinking about hypertext until 1980. Nelson had, at this point, been at it for 20 years—the kind of person who does that is also the kind of person who writes in his autobiography, “I knew ten times more fifty years ago, when I started in computers, than most people think I know now,” and also absolutely the kind of person who starts a cult.&lt;/p&gt;
    &lt;p&gt;Well, the team did manage one accomplishment during these years: in 1990, Robin Hanson showed up and ran the first ever corporate prediction market at Xanadu. Its employees assigned a 7% probability to verification of the cold fusion experiment in the next year, and a 70% probability to releasing Xanadu before Deng Xiaoping died. Cold fusion was debunked, and Deng died long before any version of Xanadu would be released. Bonus trivia: this story from Robin Hanson is how I first learned of Xanadu’s existence!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.astralcodexten.com/p/your-review-project-xanadu-the-internet"/><published>2025-09-21T16:03:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45324021</id><title>Linux Ready to Upstream Support for Google's PSP Encryption for TCP Connections</title><updated>2025-09-21T16:38:48.032912+00:00</updated><content>&lt;doc fingerprint="17339149d0b79ca1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Linux Ready To Upstream Support For Google's PSP Encryption For TCP Connections&lt;/head&gt;
    &lt;p&gt; Not to be confused with AMD's Platform Security Processor (PSP), but Google's PSP Security Protocol (PSP) for encryption in-transit for TCP network connections is now ready for the mainline kernel. This initial PSP encryption support for network connections is set to arrive with the upcoming Linux 6.18 kernel. &lt;lb/&gt;After going through thirteen rounds of review, this Google PSP Security Protocol support for encrypting data in transit with TCP connections is expected to be merged for the Linux 6.18 kernel. This kernel code has been successfully tested with PSP-capable CX7 NICs.&lt;lb/&gt;Google's PSP shares some concepts with IPsec ESP and is an encryption encapsulation layer on top of IP for large-scale data center needs. Google already has been using the PSP Security Protocol within their data centers while back in 2022 they open-sourced the architecture specification.&lt;lb/&gt;Over IPsec, Google engineered PSP for greater simplicity, better functionality, and improved scalability. More background information on PSP can be found from this documentation patch. There is also the official Google PSP architecture specification (PDF).&lt;lb/&gt;Daniel Zahka wrote on the merge request now queued into net-next:&lt;lb/&gt;With the initial patches, only the NVIDIA-Mellanox MLX5 network driver is adapted to make use of PSP.&lt;lb/&gt;With this merge hitting the net-next networking subsystem tree this week, this initial PSP encryption support is expected to be submitted as part of the upcoming Linux 6.18 merge window.&lt;/p&gt;
    &lt;p&gt;After going through thirteen rounds of review, this Google PSP Security Protocol support for encrypting data in transit with TCP connections is expected to be merged for the Linux 6.18 kernel. This kernel code has been successfully tested with PSP-capable CX7 NICs.&lt;/p&gt;
    &lt;p&gt;Google's PSP shares some concepts with IPsec ESP and is an encryption encapsulation layer on top of IP for large-scale data center needs. Google already has been using the PSP Security Protocol within their data centers while back in 2022 they open-sourced the architecture specification.&lt;/p&gt;
    &lt;p&gt;Over IPsec, Google engineered PSP for greater simplicity, better functionality, and improved scalability. More background information on PSP can be found from this documentation patch. There is also the official Google PSP architecture specification (PDF).&lt;/p&gt;
    &lt;p&gt;Daniel Zahka wrote on the merge request now queued into net-next:&lt;/p&gt;
    &lt;quote&gt;"This is v13 of the PSP RFC posted by Jakub Kicinski one year ago. General developments since v1 include a fork of packetdrill with support for PSP added, as well as some test cases, and an implementation of PSP key exchange and connection upgrade integrated into the fbthrift RPC library.&lt;lb/&gt;...&lt;lb/&gt;The protocol can work in multiple modes including tunneling. But I'm mostly interested in using it as TLS replacement because of its superior offload characteristics. So this patch does three things:&lt;lb/&gt;- it adds "core" PSP code&lt;lb/&gt;PSP is offload-centric, and requires some additional care and feeding, so first chunk of the code exposes device info. This part can be reused by PSP implementations in xfrm, tunneling etc.&lt;lb/&gt;- TCP integration TLS style&lt;lb/&gt;Reuse some of the existing concepts from TLS offload, such as attaching crypto state to a socket, marking skbs as "decrypted", egress validation. PSP does not prescribe key exchange protocols. To use PSP as a more efficient TLS offload we intend to perform a TLS handshake ("inline" in the same TCP connection) and negotiate switching to PSP based on capabilities of both endpoints. This is also why I'm not including a software implementation. Nobody would use it in production, software TLS is faster, it has larger crypto records.&lt;lb/&gt;- mlx5 implementation"&lt;/quote&gt;
    &lt;p&gt;With the initial patches, only the NVIDIA-Mellanox MLX5 network driver is adapted to make use of PSP.&lt;/p&gt;
    &lt;p&gt;With this merge hitting the net-next networking subsystem tree this week, this initial PSP encryption support is expected to be submitted as part of the upcoming Linux 6.18 merge window.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.phoronix.com/news/PSP-Encryption-Linux-6.18"/><published>2025-09-21T16:17:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45324102</id><title>California bans masks meant to hide law enforcement officers' identities</title><updated>2025-09-21T16:38:47.728276+00:00</updated><content>&lt;doc fingerprint="66f8cb9d7a13c23b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;California bans masks meant to hide law enforcement officers' identities&lt;/head&gt;
    &lt;p&gt;SACRAMENTO, Calif — California Governor Gavin Newsom has signed a law banning law enforcement from wearing masks on duty except for things like riot gear, medical masks and undercover work.&lt;/p&gt;
    &lt;p&gt;Saying it's the first bill of its type in the country, Newsom — a Democrat and frequent critic of President Trump — said it was a sign of growing authoritarianism to have detentions in by masked men "hidden from accountability, any transparency, any oversight. That's Trump's America."&lt;/p&gt;
    &lt;p&gt;He said federal agents had operated for years in the state unmasked and called masking "a new construct conceived to terrorize our diverse communities."&lt;/p&gt;
    &lt;p&gt;The law, called by sponsors the "No Secret Police Act," comes in response to increased immigration enforcement in California, particularly in Los Angeles. Newsom went to L.A. for the bill signing.&lt;/p&gt;
    &lt;p&gt;President Trump's mass deportation program has led to scenes of masked federal agents detaining people off the streets, often without badges or indicating what agency they're from. The city has been the scene of protests against Immigration and Customs Enforcement (ICE) raids.&lt;/p&gt;
    &lt;head rend="h3"&gt;The impact of the law could be decided in court&lt;/head&gt;
    &lt;p&gt;The law takes effect next year. It's unclear exactly what powers state lawmakers have to regulate the conduct of federal law enforcement. University of California, Davis law professor Raquel Aldana says California is trying to use this law to identify what federal agents can and cannot do within a state.&lt;/p&gt;
    &lt;p&gt;She said there has to be a line somewhere for states outlawing — as hypothetical extremes — things like murder or torture by federal agents. "At some point, the answer has to be 'no' and I think this is what the state of California is trying to do. Establish limits as to how much the federal government can do within the jurisdiction of the state. It's an issue of state sovereignty."&lt;/p&gt;
    &lt;p&gt;Lawmakers in a few other states, including New York and Massachusetts, are considering similar moves.&lt;/p&gt;
    &lt;p&gt;ICE declined to comment when contacted by NPR. In general, ICE officials have said their agents wear masks to prevent from being identified in videos or photos posted online, sometimes resulting in threats to them or their families.&lt;/p&gt;
    &lt;p&gt;Newsom also signed several other bills including one that would require law enforcement to clearly display their name or badge number while on duty; another requiring schools to notify parents and staff when immigration enforcement is on site; and two others prohibiting schools and health care facilities from allowing ICE agents on their grounds without valid warrants.&lt;/p&gt;
    &lt;p&gt;Aldana noted that the legal impact of the mask law on federal law enforcement could be weakened because it does not apply to California state-level law enforcement.&lt;/p&gt;
    &lt;p&gt;But it does apply to police and sheriffs offices and other local agencies, who routinely operate unmasked. By July 2026, their agencies have to issue mask policies following the law.&lt;/p&gt;
    &lt;head rend="h3"&gt;California law enforcement groups have opposed it&lt;/head&gt;
    &lt;p&gt;Several California organizations representing local law enforcement agencies, including the California State Sheriff's Association, opposed the bill, calling it reckless and dangerous to officers and their families.&lt;/p&gt;
    &lt;p&gt;Riverside County Sheriff Chad Bianco, a Republican running in the 2026 race for governor, said the ban showed Newsom and Democratic lawmakers care more about the safety of criminals than officers.&lt;/p&gt;
    &lt;p&gt;"They didn't ban criminals from wearing masks, they didn't tell criminals that they had to identify themselves," Bianco said while campaigning in Northern California on Friday. "Every single person that voted for that needs to be eliminated in the next election. Anyone that votes for those people are absolute idiots."&lt;/p&gt;
    &lt;p&gt;The Peace Officers Research Association of California — representing over 80,000 public safety members — and the California Police Chiefs Association were also against the bill.&lt;/p&gt;
    &lt;p&gt;Giselle Garcia with NorCal Resist, a Sacramento-based mutual aid organization that provides legal assistance to immigrants at risk of deportation, said the ban would be helpful even if it's not enforceable.&lt;/p&gt;
    &lt;p&gt;"There's always a tension between state rights versus the federal government's rights and scope of power," Garcia said. "However, what we do know is that when things are codified under law, there is more of an opportunity to not just cite to law, but hopefully there's some kind of basis for litigation to really challenge when there's a federal government abuse happening."&lt;/p&gt;
    &lt;p&gt;Garcia said the organization's court observers have been verbally harassed and physically assaulted by ICE agents wearing masks who refuse to identify themselves when asked.&lt;/p&gt;
    &lt;p&gt;Gerardo Zavala covers state government and politics for Capradio.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.npr.org/2025/09/20/nx-s1-5548532/newsom-trump-masked-ice-agents"/><published>2025-09-21T16:23:58+00:00</published></entry></feed>