<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-02T00:44:16.410662+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45437893</id><title>Unix philosophy and filesystem access makes Claude Code amazing</title><updated>2025-10-02T00:44:24.745887+00:00</updated><content>&lt;doc fingerprint="370009d837378e9b"&gt;
  &lt;main&gt;
    &lt;p&gt;Noah Brier, September 30, 2025&lt;/p&gt;
    &lt;p&gt;If you've talked to me lately about AI, you've almost certainly been subject to a long soliloquy about the wonders of Claude Code. What started as a tool I ran in parallel with other tools to aid coding has turned into my full-fledged agentic operating system, supporting all kinds of workflows.&lt;/p&gt;
    &lt;p&gt;Most notably, Obsidian, the tool I use for note-taking. The difference between Obsidian and Notion or Evernote is that all the files are just plain old Markdown files stored on your computer. You can sync, style, and save them, but ultimately, it's still a text file on your hard drive. A few months ago, I realized that this fact made my Obsidian notes and research a particularly interesting target for AI coding tools. What first started with trying to open my vault in Cursor quickly moved to a sort of note-taking operating system that I grew so reliant on, I ended up standing up a server in my house so I could connect via SSH from my phone into my Claude Code + Obsidian setup and take notes, read notes, and think through things on the go.&lt;/p&gt;
    &lt;p&gt;A few weeks ago, I went on Dan Shipper's AI &amp;amp; I Podcast to wax poetic about my love for this setup. I did a pretty deep dive into the system I use, how it works, why it works, etc. I won't retread all those details—you can read the transcript or listen to the podcast—but I want to talk about a few other things related to Claude Code that I've come to realize since the conversation.&lt;/p&gt;
    &lt;p&gt;I've really struggled to answer this question. I'm also not sure it's better than Cursor for all things, but I do think there are a set of fairly exceptional pieces that work together in concert to make me turn to Claude Code whenever I need to build anything these days. Increasingly, that's not even about applying it to existing codebases as much as it's building entirely new things on top of its functionality (more on that in a bit).&lt;/p&gt;
    &lt;p&gt;So what's the secret? Part of it lies in how Claude Code approaches tools. As a terminal-based application, it trades accessibility for something powerful: native Unix command integration. While I typically avoid long blockquotes, the Unix Philosophy deserves an exception—Doug McIlroy's original formulation captures it perfectly:&lt;/p&gt;
    &lt;p&gt;The Unix philosophy is documented by Doug McIlroy in the Bell System Technical Journal from 1978:&lt;/p&gt;
    &lt;p&gt;It was later summarized by Peter H. Salus in A Quarter-Century of Unix (1994):&lt;/p&gt;
    &lt;p&gt;These fifty-year-old principles are exactly how LLMs want to use tools. If you look at how these models actually use the tools they're given, they are constantly "piping" output to input (albeit using their own fuzziness in between). (As an aside, the Unix | command allows you to string the output from one command into the input of another.) When models fail to weld their tools effectively, it is almost always because the tools are overly complex.&lt;/p&gt;
    &lt;p&gt;So part one of why Claude Code can be so mind-blowing is that the commands that power Unix happen to be perfectly suited for use by LLMs. This is both because they're simple and also incredibly well-documented, meaning the models had ample source material to teach them the literal ins and outs.&lt;/p&gt;
    &lt;p&gt;But that still wasn't the whole thing. The other piece was obviously Claude Code's ability to write code initially and, more recently, prose (for me, at least). But while other applications like ChatGPT and Claude can write output, there was something different going on here. Last week, while reading The Pragmatic Engineer's deep dive into how Claude Code is built. The answer was staring me in the face: filesystem access.&lt;/p&gt;
    &lt;p&gt;The filesystem changes everything. ChatGPT and Claude in the browser have two fatal flaws: no memory between conversations and a cramped context window. A filesystem solves both. Claude Code writes notes to itself, accumulates knowledge, and keeps running tallies. It has state and memory. It can think beyond a single conversation.&lt;/p&gt;
    &lt;p&gt;Back in 2022, when I first played with the GPT-3 API, I said that even if models never got better than they were in that moment, we would still have a decade to discover the use cases. They did get better—reasoning models made tool calling reliable—but the filesystem discovery proves my point.&lt;/p&gt;
    &lt;p&gt;I bring this up because in the Pragmatic Engineer interview, Boris Cherney, who built the initial version of Claude Code, uses it to describe the aha:&lt;/p&gt;
    &lt;p&gt;In AI, we talk about “product overhang”, and this is what we discovered with the prototype. Product overhang means that a model is able to do a specific thing, but the product that the AI runs in isn’t built in a way that captures this capability. What I discovered about Claude exploring the filesystem was pure product overhang. The model could already do this, but there wasn’t a product built around this capability!&lt;/p&gt;
    &lt;p&gt;Again, I'd argue it's filesystem + Unix commands, but the point is that the capability was there in the model just waiting to be woken up, and once it was, we were off to the races. Claude Code works as a blueprint for building reliable agentic systems because it captures model capabilities instead of limiting them through over-engineered interfaces.&lt;/p&gt;
    &lt;p&gt;I talked about my Claude Code + Obsidian setup, and I've actually taken it a step further by open-sourcing "Claudesidian," which pulls in a bunch of the tools and commands I use in my own Claude Code + Obsidian setup. It also goes beyond that and was a fun experimental ground for me. Most notably, I built an initial upgrade tool so that if changes are made centrally, you can pull them into your own Claudesidian, and the AI will help you check to see if you've made changes to the files being updated and, if so, attempt to smartly merge your changes with the new updates. Both projects follow the same Unix philosophy principles—simple, composable tools that do one thing well and work together. This is the kind of stuff that Claude Code makes possible, and why it's so exciting for me as a new way of building applications.&lt;/p&gt;
    &lt;p&gt;Speaking of which, one I'm not quite ready to release, but hopefully will be soon, is something I've been calling "Inbox Magic," though I'll surely come up with a better name. It's a Claude Code repo with access to a set of Gmail tools and a whole bunch of prompts and commands to effectively start operating like your own email EA. Right now, the functionality is fairly simple: it can obviously run searches or send emails on your behalf, but it can also do things like triage and actually run a whole training run on how you sound over email so it can more effectively draft emails for you. While Claude Code and ChatGPT both have access to my emails, they mostly grab one or two at a time. This system, because it can write things out to files and do lots of other fancy tricks, can perform a task like “find every single travel-related email in my inbox and use that to build a profile of my travel habits that I can use as a prompt to help ChatGPT/Claude do travel research that's actually aligned with my preferences.” Anyway, more on this soon, and if it's something you want to try out, ping me with your GitHub username, and as soon as I feel like I have something ready to test, I'll happily share it.&lt;/p&gt;
    &lt;p&gt;While I generally shy away from conclusions, I think there are a few here worth reiterating.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.alephic.com/writing/the-magic-of-claude-code"/><published>2025-10-01T14:05:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45438346</id><title>Show HN: Autism Simulator</title><updated>2025-10-02T00:44:24.574802+00:00</updated><link href="https://autism-simulator.vercel.app/"/><published>2025-10-01T14:48:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45438496</id><title>Building the heap: racking 30 petabytes of hard drives for pretraining</title><updated>2025-10-02T00:44:24.346799+00:00</updated><content>&lt;doc fingerprint="2fd91d7735ac7fcd"&gt;
  &lt;main&gt;
    &lt;p&gt;We built a storage cluster in downtown SF to store 90 million hours worth of video data. Why? We’re pretraining models to solve computer use. Compared to text LLMs like LLaMa-405B, which require ~60 TB of text data to train, videos are sufficiently large that we need 500 times more storage. Instead of paying the $12 million / yr it would cost to store all of this on AWS, we rented space from a colocation center in San Francisco to bring that cost down ~40x to $354k per year, including depreciation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why&lt;/head&gt;
    &lt;p&gt;Our use case for data is unique. Most cloud providers care highly about redundancy, availability, and data integrity, which tends to be unnecessary for ML training data. Since pretraining data is a commodity—we can lose any individual 5% with minimal impact—we can handle relatively large amounts of data corruption compared to enterprises who need guarantees that their user data isn’t going anywhere. In other words, we don’t need AWS’s 13 nines of reliability; 2 is more than enough.&lt;/p&gt;
    &lt;p&gt;Additionally, storage tends to be priced substantially above cost. Most companies use relatively small amounts of storage (even ones like Discord still use under a petabyte for messages), and the companies that use petabytes are so large that storage remains a tiny fraction of their total compute spend.&lt;/p&gt;
    &lt;p&gt;Data is one of our biggest contraints, and would be prohibitively expensive otherwise. As long as the cost predictions work out in favor of a local datacenter, and it would not consume too much of the core team’s time, it would make sense to stack hard drives ourselves. [1] 1. We talked to some engineers at the Internet Archive, which had basically the same problem as us; even after massive friends &amp;amp; family discounts on AWS, it was still 10 times more cost-effective to buy racks and store the data themselves!&lt;/p&gt;
    &lt;head rend="h2"&gt;The Cost Breakdown: Cloud Alternatives vs In-House&lt;/head&gt;
    &lt;p&gt;Internet and electricity total $17.5k as our only recurring expenses (the price of colocation space, cooling, etc were bundled into electricity costs). One-time costs were dominated by hard drive capex. [2] 2. When deciding the datacenter location we had multiple options across the Bay Area, including options in Fremont through Hurricane Electric for around $10k in setup fees and $12.8k per month, saving us $38.5k initially and $4.7k per month, but ended up opting for a datacenter that was only a couple blocks from our office in SF. Though this came at a premium, it was extremely helpful to get the initial nodes setup and for ongoing maintenance. Our team is just 5 people, so any friction in going to the datacenter would come at a noticeable cost to team productivity.&lt;/p&gt;
    &lt;p&gt;Table 1: Cost comparison of cloud alternatives vs in-house. AWS is $1,130,000/month including estimated egress, Cloudflare is $270,000/month (with bulk-discounted pricing), and our datacenter is $29,500/month (including recurring costs and depreciation).&lt;/p&gt;
    &lt;head rend="h3"&gt;Monthly Recurring Costs&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Item&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Internet&lt;/cell&gt;
        &lt;cell&gt;$7,500/month&lt;/cell&gt;
        &lt;cell&gt;100Gbps DIA from Zayo, 1yr term.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Electricity&lt;/cell&gt;
        &lt;cell&gt;$10,000/month&lt;/cell&gt;
        &lt;cell&gt;1 kW/PB, $330/kW. Includes cabinet space &amp;amp; cooling. 1yr term.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total Monthly&lt;/cell&gt;
        &lt;cell&gt;$17,500/month&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;One-Time Costs&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Item&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;Hard drives (HDDs)&lt;/cell&gt;
        &lt;cell&gt;$300,000&lt;/cell&gt;
        &lt;cell&gt;2,400 drives. Mostly 12TB used enterprise drives (3/4 SATA, 1/4 SAS). The JBOD DS4246s work for either.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage Infrastructure&lt;/cell&gt;
        &lt;cell&gt;NetApp DS4246 chassis&lt;/cell&gt;
        &lt;cell&gt;$35,000&lt;/cell&gt;
        &lt;cell&gt;100 dual SATA/SAS chassis, 4U each&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compute&lt;/cell&gt;
        &lt;cell&gt;CPU head nodes&lt;/cell&gt;
        &lt;cell&gt;$6,000&lt;/cell&gt;
        &lt;cell&gt;10 Intel RR2000s from eBay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Datacenter Setup&lt;/cell&gt;
        &lt;cell&gt;Install fee&lt;/cell&gt;
        &lt;cell&gt;$38,500&lt;/cell&gt;
        &lt;cell&gt;One-off datacenter install fee&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Labor&lt;/cell&gt;
        &lt;cell&gt;Contractors&lt;/cell&gt;
        &lt;cell&gt;$27,000&lt;/cell&gt;
        &lt;cell&gt;Contractors to help physically screw in / install racks and wire cables&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Networking &amp;amp; Misc&lt;/cell&gt;
        &lt;cell&gt;Install expenses&lt;/cell&gt;
        &lt;cell&gt;$20,000&lt;/cell&gt;
        &lt;cell&gt;Power cables, 100GbE QSFP CX4 NICs, Arista router, copper jumpers, one-time internet install fee&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total One-Time&lt;/cell&gt;
        &lt;cell&gt;$426,500&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Our price assuming three-year depreciation (including for the one-off install fees) is $17.5k/month in fixed monthly costs (internet, power, etc.) and $12k/month in depreciation, for $29.5k/month overall.&lt;/p&gt;
    &lt;p&gt;We compare our costs to two main providers: AWS’s public pricing numbers as a baseline, and Cloudflare’s discounted pricing for 30PB of storage. It’s important to note that AWS egress would be substantially lower if we utilized AWS GPUs. This is not reflected on our graph because AWS GPUs are priced at substantially above market prices and large clusters are difficult to attain, untenable at our compute scales.&lt;/p&gt;
    &lt;p&gt;Here are the pricing breakdowns:&lt;/p&gt;
    &lt;head rend="h3"&gt;AWS Pricing Breakdown&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Cost Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Monthly Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;$0.021/GB/month&lt;/cell&gt;
        &lt;cell&gt;$630,000&lt;/cell&gt;
        &lt;cell&gt;For data over 500TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Egress&lt;/cell&gt;
        &lt;cell&gt;$0.05/GB&lt;/cell&gt;
        &lt;cell&gt;$500,000&lt;/cell&gt;
        &lt;cell&gt;Entire dataset egressed quarterly (10 PB/month)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total AWS Monthly&lt;/cell&gt;
        &lt;cell&gt;$1,130,000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Cloudflare R2 Pricing&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Pricing Tier&lt;/cell&gt;
        &lt;cell role="head"&gt;Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Monthly Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Published Rate&lt;/cell&gt;
        &lt;cell&gt;$0.015/GB/month&lt;/cell&gt;
        &lt;cell&gt;$450,000&lt;/cell&gt;
        &lt;cell&gt;No egress fees&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Estimated Private Pricing [3] 3. Cloudflare has a more reasonable estimate for the 30 PB, placing it at an overall monthly cost of $270k without egress fees. We also have bulk-discounted pricing estimates after getting pricing quotes—this was our main point of comparison for the datacenter.&lt;/cell&gt;
        &lt;cell&gt;$0.009/GB/month&lt;/cell&gt;
        &lt;cell&gt;$270,000&lt;/cell&gt;
        &lt;cell&gt;Estimated rate for &amp;gt;20 PB scale&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That brings monthly costs to $38/TB/month for AWS, $10/TB/month for Cloudflare, and $1/TB/month for our datacenter—about 38x lower and 10x lower respectively. (At the very cheapest end of the spectrum, Backblaze has a $6/TB product that is unsuitable for model training due to egress speed limitations; their $15/TB Overdrive AI-specific storage product is closer to Cloudflare’s in price &amp;amp; performance)&lt;/p&gt;
    &lt;p&gt;While we use Cloudflare as a comparison point, we’ve sometimes done too much load for their R2 servers. In particular, in the past we’ve done enough load during large model training runs that they rate-limited us, later confirming we were saturating their metadata layer and the rate limit wasn’t synthetic. Because our metadata on the heap is so simple, and we have a 100Gbps DIA connection, we haven’t ran into any issues there. [4] 4. We love Cloudflare and use many of their products often; we include this anecdote as a fact about our scale being difficult to handle, not as a dig!&lt;/p&gt;
    &lt;p&gt;This setup was and is necessary for our video data pipelines, and we’re extremely happy that we made this investment. By gathering large scale data at low costs, we can be competitive with frontier labs with billions of dollars in capital.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setup/The Process&lt;/head&gt;
    &lt;p&gt;We cared a lot about getting this built fast, because this kind of project can easily stretch on for months if not careful. Hence Storage Stacking Saturday, or S3. We threw a hard drive stacking party in downtown SF and got our friends to come, offering food and custom-engraved hard drives to all who helped. The hard drive stacking started at 6am and continued for 36 hours (with a break to sleep), and by the end of that time we had 30 PB of functioning hardware racked and wired up. We brought in contractors for additional help and professional installation later on in the event.&lt;/p&gt;
    &lt;p&gt;People at the hard drive stacking party! Cool shots of the servers&lt;/p&gt;
    &lt;p&gt;Our software is 200 lines of Rust code for writing (to determine the drive to write data onto) and a nginx webserver for reading data, with a simple SQLite db for tracking metadata like which heap node each file is on and what data split it belongs to. We kept this obsessively simple instead of using MinIO or Ceph because we didn’t need any of the features they provided; it’s much, much simpler to debug a 200-line program than to debug Ceph, and we weren’t worried about redundancy or sharding. All our drives were formatted with XFS.&lt;/p&gt;
    &lt;p&gt;The storage software landscape offers many options, but every option available comes with drawbacks. People experienced with Ceph strongly warned us to avoid it unless we were willing to hire dedicated Ceph specialists—our research confirmed this advice. Ceph appears far more complex than justified for most use cases, only worthwhile for companies that absolutely need maximum performance and customizability and are prepared to invest heavily in tuning. Minio presents an interesting option if S3 compatibility is essential, but otherwise remains a bit too fancy for us and similar use-cases. Weka and Vast are absurdly expensive at 2k / TB / year or so and are primarily designed for NVMEs, not spinning disks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Post-Mortem&lt;/head&gt;
    &lt;p&gt;Building the datacenter was a large endeavor and we definitely learned lessons, both good and bad.&lt;/p&gt;
    &lt;head rend="h3"&gt;Things That We Got Correct&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We think the redundancy &amp;amp; capability tradeoffs we made are very reasonable at our disk speeds. We’re able to approximately saturate our 100G network for both read &amp;amp; write.&lt;/item&gt;
      &lt;item&gt;Doing this locally a couple blocks away was well worth it because of the amount of debugging and manual work needed.&lt;/item&gt;
      &lt;item&gt;Ebay is good to find vendors but bad to actually buy things with. After finding vendors, they can often individually supply all the parts we need and provide warranties, which are extremely valuable.&lt;/item&gt;
      &lt;item&gt;100G dedicated internet is pretty important, and much much easier to debug issues with than using cloud products.&lt;/item&gt;
      &lt;item&gt;Having high-quality cable management during the racking process saved us a ton of time debugging in the long run; making it easy to switch up the networking saved us a lot of headache.&lt;/item&gt;
      &lt;item&gt;We had a very strong simplicity prior, and this saved an immense amount of effort. We are quite happy that we didn’t use ceph or minio. Unlike e.g. nginx, they do not work out of the box. We were willing to write a simple Rust script and roughly saturated our network read &amp;amp; write at 100 Gbps without any fancy code.&lt;/item&gt;
      &lt;item&gt;We were basically right about the price and advantages this offered, and did not substantially overestimate the amount of time / effort it would take. While the improvements list is longer than this, most of those are minor; fundamentally we built a cluster rivaling massive clouds for 40x cheaper.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Difficult Bits&lt;/head&gt;
    &lt;p&gt;A map of reality only gets you so far—while setting up the datacenter we ran into a couple problems and unexpected challenges. We’ll include a list:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We used frontloaders instead of toploaders for our server rack. This meant we had to screw every single individual drive in—tedious for 2.4k HDDs&lt;/item&gt;
      &lt;item&gt;Our storage was not dense—we could have saved 5x the work on physical placement and screwing by having a denser array of hard drives&lt;/item&gt;
      &lt;item&gt;Shortcuts like daisy-chaining are usually a bad idea. We could have gotten substantially higher read/write speeds without daisy chaining networked nodes, giving each chassis its own HBA (Host Bus Adapter, not a significant cost).&lt;/item&gt;
      &lt;item&gt;Compatibility is key—specifically in networking functionally everything is locked to a specific brand. We had many pain points here. Fiber transceivers will ~never work unless used with the right brand, but copper cables are much more forgiving. FS.com is pretty good and well priced (though their speed estimates were pretty inconsistent); Amazon will also often have the parts you need rapidly.&lt;/item&gt;
      &lt;item&gt;Networking came at substantial cost and required experimentation. In general, with our relatively non-sensitive training data, we optimized for convenience and ease of use over all else: we did not use DHCP as our used enterprise switches didn’t support it out of the box, and we didn’t use NAT as we wanted public IPs for the nodes for convenient and performant access from our servers. (We firewalled off unused ports and had basic security with nginx secure_link; we would not be able to do this if handling customer data, but it was fine for our use case.) While this is an area where we would have saved time with a cloud solution, we had our networking up within days and kinks ironed out within ~3 weeks.&lt;/item&gt;
      &lt;item&gt;We were often bottlenecked by easy access to servers via monitor/keyboard; idle crash carts during setup are helpful.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Ideas Worth Trying&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Working KVMs are extremely useful, and you shouldn’t go without them or good IPMI. Physically going to a datacenter is really inconvenient, even if it’s a block away. IPMI is good, but only if you have pretty consistent machines.&lt;/item&gt;
      &lt;item&gt;Think through your management Ethernet network as much as your real network - it’s really nice to be able to SSH into servers while configuring the network, and IPMI is great!&lt;/item&gt;
      &lt;item&gt;Overprovision your network—e.g. if doable it’s worth having 400 Gigabit internally (you can use 100G cards etc for this!)&lt;/item&gt;
      &lt;item&gt;We could have substantially increased density at additional upfront cost by buying 90-drive SuperMicro SuperServers and putting 20TB drives into them. This would allow us to use 2 racks instead of 10, give us about the equivalent of 20 AMD 9654s in total CPU capacity, and use less total power.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How You Can Build This Yourself&lt;/head&gt;
    &lt;p&gt;Here’s what you need to replicate our setup.&lt;/p&gt;
    &lt;head rend="h3"&gt;Storage&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;10 CPU head nodes.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We used Intel Rr2000 with Dual Intel Gold 6148 and 128GB of DDR4 ECC RAM per server (which are incredibly cheap and roughly worked for our use cases) but you have a lot of flexibility in what you use.&lt;/item&gt;
          &lt;item&gt;If you use the above configuration you likely won’t be able to do anything at all CPU-intensive on the servers (like on-device data processing or ZFS data compression / deduplication / etc, which is valuable if you’re storing non-video data).&lt;/item&gt;
          &lt;item&gt;Our CPU nodes cost $600 each—it seems quite reasonable to us to spend up to $3k each if you want ZFS / compression or the abiliy to do data processing on-CPU.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;100 DS4246 chassis—each can hold 24 hard drives.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2,400 3.5 inch HDDs—need to be all SATA or all SAS in each chassis.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We would recommend SAS hard drives if possible [5] 5. if you use SAS drives you’ll need to deal with or disable mulipathing, which is reasonably simple as they roughly double speed over similar SATA drives.&lt;/item&gt;
          &lt;item&gt;We used a mix of 12TB and 14TB drives—basically any size should work, roughly the larger the better holding price constant (density makes stacking easier + in general increases resale value).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Physical parts to mount the chassis—you’ll need rails or l-brackets. We used l-brackets which worked well, as we haven’t needed to take the chassis out to slot hard drives. If you buy toploaders, you’ll need rails.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiple “crash carts” with monitors and keyboards that allow you to physically connect to your CPU head nodes and configure them—this is invaluable when you’re debugging network issues.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Network&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;A 100 GbE switch&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;A used Arista is fine, should be QSFP28, should cost about $1-2k.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;HBAs (Host Bus Adapters), which connect your head nodes to your DS4246 chassis.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;The best configuration we tried was with Broadcom 9305-16E HBAs, with 3x HBAs per server (make sure your server has physical space for them!) with SFF-8644 to QSFP mini SAS cables.&lt;/item&gt;
          &lt;item&gt;There are 4 slots per HBA, so you can cable each DS4246 chassis directly to the HBA. [6] 6. The option we ended up going with for convenience was putting LSI SAS9207-8e HBAs, which have 2 ports each, into the CPU head nodes- then daisy-chaining the DS4246s together with QSFP+ to QSFP+ DACs.. We deployed this on Storage Stacking Saturday, then while debugging speeds tried the above method on one of the servers and got to ~4 Gbps per chassis-but didn’t find it worth it to swap everything out in pure labor because of the way we had set up some of our head nodes such that they were difficult to take out. Insofar as it is reasonably cheap to just do the above thing to start and we’ve tested it to work, you should probably do as we say, not as we did in this case!&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network cards (NICs).&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We used Mellanox ConnectX-4 100GbE. Make sure they come in Ethernet mode and not Infiniband mode for ease of config.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DAC (Direct Attach Copper) or AOC (Active Optical) cables, to connect the NICs in your head nodes to your switch and therefore the internet. You almost certainly want DACs if your racks are close together, as they are far more compatible with arbitrary networking equipment than AOCs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We would recommend that you find a supplier to sell you the CPU head nodes with the HBAs and NICs installed—there are a number of used datacenter / enterprise parts suppliers who are willing to do this. This is a substantial positive because it means that you don’t have to spend hours installing the HBAs/NICs yourself and can have a substantially higher degree of confidence in your operations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Serial cables—you’ll need these to connect to your switch!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Optional but recommended: an Ethernet management network of some kind. If you can’t easily get ethernet, we’d recommend getting a wifi adapter like this and then a ethernet switch like this —it’s substantially easier to set up than the 100GbE, is a great backup for when that’s not working, and will allow you to do ~everything over SSH from the comfort of the office instead of in the datacenter.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Datacenter Requirements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3.5 kW of usable power per cabinet, with 10 4U chassis + 1 2U (cabinets are 42U tall)&lt;/item&gt;
      &lt;item&gt;1 spare cabinet for the 1U or 2U 100GbE switch (you can obviously also just swap out one of the 4U chassis in another cabinet for the switch).&lt;/item&gt;
      &lt;item&gt;1 42U cabinet per 3 PB of storage&lt;/item&gt;
      &lt;item&gt;A dedicated 100G connection (will come in as a fiber pair probably via QSFP28 LR4, but confirm with your datacenter provider before buying parts here!)&lt;/item&gt;
      &lt;item&gt;Ideally physically near your office—there is a lot of value in being able to walk over and debug issues instead of e.g. dealing with remote hands services to get internet to the nodes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some setup tips:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make sure to first properly configure your switch. Depending on your switch model this should be relatively straightforward—you’ll need to physically connect to the switch and then configure the specific port that your 100GbE is connected to (you’ll get a fiber cross-connect from your datacenter that you should plug into a QSFP28 transceiver. Make sure that you get a transceiver that is compatible in form with the ISP, probably LR4, and specifically branded with your switch brand, otherwise it is very unlikely to work). Depending on your ISP you might have to talk to them to make sure that you can get “light” through the fiber cables from both ends, which might involve rolling the fiber and otherwise making sure it’s working properly. &lt;list rend="ul"&gt;&lt;item&gt;If your switch isn’t working / you haven’t configured one before, I’d suggest trying to directly plug the fiber cable from the ISP into one of your 10 heap servers, making sure to buy a transceiver that is compatible with your NIC brand (e.g. Mellanox). Once you get it working from there, move over to your switch and get it working.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Once you can connect to the internet from your switch (simply ping 1.1.1.1 to check) you are ready to set up the netplans for the individual nodes. this is most easily done during the Ubuntu setup process, which will walk you through setting up internet for your CPU head nodes, but is also doable outside of that&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you have internet access to your nodes and have properly connected 1 cable to each DS4246, you should format &amp;amp; mount the drives on each node, test that all of them are properly working, and then you are ready to deploy any software you want.&lt;/p&gt;
    &lt;p&gt;If you end up building a similar storage cluster based on this writeup we’d love to hear from you—we’re very curious what can be improved, both in our guidance and in the object-level process. You can reach us at [email protected]&lt;/p&gt;
    &lt;p&gt;If you came away from this post excited about our work, we’d love to chat. We’re a research lab currently focused on pretraining models to use computers, with the long-term goal of building general models that can learn in-context and do arbitrary tasks while aligned with human values; we’re hiring top researchers and engineers to help us train these. If you’re interested in chatting, shoot us an email at [email protected].&lt;/p&gt;
    &lt;head rend="h3"&gt;Collaborators&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Neel Redkar&lt;/item&gt;
      &lt;item&gt;Devansh Pandey&lt;/item&gt;
      &lt;item&gt;Nicholas Charette&lt;/item&gt;
      &lt;item&gt;Galen Mead&lt;/item&gt;
      &lt;item&gt;Yudhister Kumar&lt;/item&gt;
      &lt;item&gt;Robert Avery&lt;/item&gt;
      &lt;item&gt;Raj Thimmiah&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://si.inc/posts/the-heap/"/><published>2025-10-01T15:00:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45438501</id><title>Ask HN: Who wants to be hired? (October 2025)</title><updated>2025-10-02T00:44:23.968402+00:00</updated><content>&lt;doc fingerprint="301182753412e0e7"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Share your information if you are looking for work. Please use this format:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  Location:
  Remote:
  Willing to relocate:
  Technologies:
  Résumé/CV:
  Email:
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Please only post if you are personally looking for work. Agencies, recruiters, job boards, and so on, are off topic here.&lt;/p&gt;
      &lt;p&gt;Readers: please only email these addresses to discuss work opportunities.&lt;/p&gt;
      &lt;p&gt;There's a site for searching these posts at https://www.wantstobehired.com.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45438501"/><published>2025-10-01T15:01:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45438503</id><title>Ask HN: Who is hiring? (October 2025)</title><updated>2025-10-02T00:44:23.448476+00:00</updated><content>&lt;doc fingerprint="3651195a341ae364"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Please state the location and include REMOTE for remote work, REMOTE (US) or similar if the country is restricted, and ONSITE when remote work is &lt;/p&gt;not&lt;p&gt; an option.&lt;/p&gt;&lt;p&gt;Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does.&lt;/p&gt;&lt;p&gt;Please only post if you are actively filling a position and are committed to responding to applicants.&lt;/p&gt;&lt;p&gt;Commenters: please don't reply to job posts to complain about something. It's off topic here.&lt;/p&gt;&lt;p&gt;Readers: please only email if you are personally interested in the job.&lt;/p&gt;&lt;p&gt;Searchers: try https://dheerajck.github.io/hnwhoishiring/, https://amber-williams.github.io/hackernews-whos-hiring/, http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension: https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....&lt;/p&gt;&lt;p&gt;Don't miss these other fine threads:&lt;/p&gt;&lt;p&gt;Who wants to be hired? https://news.ycombinator.com/item?id=45438501&lt;/p&gt;&lt;p&gt;Freelancer? Seeking freelancer? https://news.ycombinator.com/item?id=45438502&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45438503"/><published>2025-10-01T15:01:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45439721</id><title>Fossabot: AI code review for Dependabot/Renovate on breaking changes and impacts</title><updated>2025-10-02T00:44:23.329405+00:00</updated><content>&lt;doc fingerprint="af4799a7096b4451"&gt;
  &lt;main&gt;
    &lt;p&gt;Today we're announcing fossabot, a new AI Agent for making strategic dependency updates, backed by a comprehensive accuracy, consistency, and correctness framework.&lt;/p&gt;
    &lt;p&gt;fossabot is able to deliver completed work just like an engineer, including researching new versions, finding app impact and adapating code if needed. This product fulfills our philosophy for automating dependency updates and EdgeBit acquisition.&lt;/p&gt;
    &lt;p&gt;fossabot is currently available as a public preview, with a focus on the JavaScript and TypeScript ecosystems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Your dependencies are simultaneously moving too fast and too slow&lt;/head&gt;
    &lt;p&gt;For a decade, FOSSA has protected businesses from open source risk in two large categories: compliance and security. We’ve identified a new, third category of risk that is emerging: dependency churn and update stagnation.&lt;/p&gt;
    &lt;p&gt;AI coding agents churning out new repos and dependencies trees faster than we can follow.&lt;/p&gt;
    &lt;p&gt;At the same time, crown jewel apps can’t keep up with the fast pace of upstream development and fall more behind.&lt;/p&gt;
    &lt;p&gt;Neither are good, but fossabot is here to help...as if your best engineer managed updates 24/7.&lt;/p&gt;
    &lt;head rend="h3"&gt;Every dependency update program is broken&lt;/head&gt;
    &lt;p&gt;The root of the problem is that every enterprise dependency update program is broken. Why? Our tools can’t make strategic updates like our engineers are capable of.&lt;/p&gt;
    &lt;p&gt;Instead, enterprises focus is making the smallest update possible to fix an alert, only to do it again next month. No time is devoted to figuring out how to upgrade to the latest version of a package and the benefits it may bring to the app.&lt;/p&gt;
    &lt;p&gt;fossabot, our dependency updating AI agent, is capable of large complexity upgrades – the ones that require a senior engineer because they’re always an unexpected multi-hour research and coding task.&lt;/p&gt;
    &lt;p&gt;Bump lodash from &lt;code&gt;4.17.20&lt;/code&gt; to &lt;code&gt;4.17.21&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h3"&gt;Summary by fossabot&lt;/head&gt;
    &lt;p&gt;I recommend merging this lodash update from 4.17.20 to 4.17.21. This is a patch release that fixes several security vulnerabilities and includes performance improvements. Your application's usage patterns are compatible with this update.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;•Analyzed 47 files using lodash utilities across components/, utils/, and services/&lt;/item&gt;
      &lt;item&gt;•Verified no deprecated methods or breaking changes affect your codebase&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Change Details&lt;/head&gt;
    &lt;p&gt;1. Fixed prototype pollution vulnerability in merge function&lt;/p&gt;
    &lt;p&gt;2. Improved input validation for template method&lt;/p&gt;
    &lt;p&gt;3. Enhanced sanitization in defaultsDeep&lt;/p&gt;
    &lt;p&gt;fossabot started out as an internal tool and became invaluable to our engineers and trusted testers, so we’re releasing it as a public preview for all to use.&lt;/p&gt;
    &lt;p&gt;fossabot is available as a GitHub app and all users get $15 in free usage credit each month.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does fossabot work so well?&lt;/head&gt;
    &lt;p&gt;fossabot proposes strategic updates because it can balance risk vs. reward, understand breaking changes in the context of your app, and even adapt code to handle newer paradigms.&lt;/p&gt;
    &lt;p&gt;Existing updaters like Dependabot or Renovate can’t do this reasoning, so they end up being configured to be “dumb,” like patch releases only.&lt;/p&gt;
    &lt;p&gt;Plus, mechanically making the update is not the hard and slow part. It’s the research and understanding of risk to your app that takes forever and ultimately relegates most updates into the backlog forever.&lt;/p&gt;
    &lt;head rend="h3"&gt;Codebase Reasoning&lt;/head&gt;
    &lt;p&gt;fossabot analysis determines the impact of an update to your specific codebase and usage of dependencies instead of making guesses about compatibility, which allows for smart reasoning. Examples of this reasoning include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use a rewritten React library and update your component to use the more modern syntax&lt;/item&gt;
      &lt;item&gt;Upgrade a major version of a library safely because you use APIs in forward-compatible ways&lt;/item&gt;
      &lt;item&gt;Adapt your code to an undeclared behavior change in a patch update&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a partial excerpt of this reasoning in action:&lt;/p&gt;
    &lt;p&gt;fossabot uses a perfect balance of hard facts from static analysis paired with a scalable and detail-oriented AI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scale Through AI&lt;/head&gt;
    &lt;p&gt;fossabot outperforms human engineers because it can scale beyond what a reasonable person would do. It researches harder, deeper, and longer, with perfect memory about your first-party code, the dependency code and the library’s release notes, migration guides, and docs.&lt;/p&gt;
    &lt;p&gt;While a human would become fatigued after an hour (or even minutes), fossabot will keep going until every modified function is triaged and every impact is understood throughout your entire codebase.&lt;/p&gt;
    &lt;p&gt;No engineer can hold a full picture of dependency usage, especially when multiple teams are involved. fossabot is able to take in more analysis and relationships that can be mapped out in your brain.&lt;/p&gt;
    &lt;head rend="h3"&gt;Delivers Completed Tasks&lt;/head&gt;
    &lt;p&gt;Customers tell us that understanding the level of effort for a change can be just as hard as the update itself. When fossabot is in charge of your updates, you can skip all of this toil and receive completed tasks, delivered right to a pull request.&lt;/p&gt;
    &lt;p&gt;fossabot understands its limitations and can request assistance to “last-mile” an update across the finish line. Backed by our evaluation framework and ability to classify different types of updates, we’re confident in fossabot’s ability to handle large complexity updates in the JavaScript/TypeScript ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;From Internal Tool to Public Preview&lt;/head&gt;
    &lt;p&gt;Earlier this year, FOSSA engineers hypothesized that with the right context, we could eliminate the toil from dependency updates. We started providing a custom AI framework with details from FOSSA’s dependency metadata scanning, upgrade path guidance, and open source health signals. This grew into a robust breaking change detection engine that continues to surprise us with its detail and accuracy.&lt;/p&gt;
    &lt;p&gt;With breaking changes found, the next challenge was impact detection for each customer’s codebase. Static analysis is the ideal tool for this, which led to a partnership and eventual acquisition of EdgeBit, which pioneered a new type analysis that is designed for dependency update use-cases.&lt;/p&gt;
    &lt;p&gt;Static analysis prevents the AI agent from making silly mistakes, and in our experience, perfectly balances the desired fuzziness you gain from using AI agents and sub agents. fossabot resembles a “focused agent” that resembles a pipeline for determinism but includes agentic steps as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Accuracy, Consistency, Correctness&lt;/head&gt;
    &lt;p&gt;While iterating on fossabot, we quickly realized that the evaluation framework and ground truth dataset was just as important as the tool itself, and in many ways, just as challenging as writing the code.&lt;/p&gt;
    &lt;p&gt;fossabot continually scores itself on Accuracy, Consistency, Correctness (ACC) against a set of validated dependency updates with varying degrees of breaking changes, changed lines of code and usage of those libraries in real-world apps.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Accuracy, Consistency, Correctness by Group &amp;amp; Complexity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Group&lt;/cell&gt;
        &lt;cell&gt;Complexity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;routine_minor_updates&lt;/cell&gt;
        &lt;cell&gt;low&lt;/cell&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;high&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;multi_dependency_updates&lt;/cell&gt;
        &lt;cell&gt;low&lt;/cell&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;high&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;major_version_upgrades&lt;/cell&gt;
        &lt;cell&gt;low&lt;/cell&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;high&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;dev_dependencies&lt;/cell&gt;
        &lt;cell&gt;low&lt;/cell&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;high&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This process quickly highlighted a key learning: the importance of weighted scoring in our evaluation. A false positive (where the tool incorrectly deems a breaking change safe) carries a much higher cost in terms of potential disruption and lost trust than a false negative (where a safe update is flagged for extra scrutiny). This phase also helped us debunk early, overly simplistic assumptions, such as the fallacy that all major version upgrades are inherently breaking.&lt;/p&gt;
    &lt;p&gt;Our public preview is targeted at the JavaScript/TypeScript ecosystem because our ACC dataset is robustly populated — other ecosystems will follow shortly as we build out more ground truth.&lt;/p&gt;
    &lt;p&gt;We believe that several design decisions set at the genesis of fossabot make it a trusted foundation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Striving for determinism at key steps&lt;/item&gt;
      &lt;item&gt;Smartly using static analysis&lt;/item&gt;
      &lt;item&gt;Use AI to be doggedly persistent and detail oriented&lt;/item&gt;
      &lt;item&gt;Measuring ourselves against the ACC ground truth&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We hope fossabot earns your trust, and we’d love your feedback on analysis that looks great or needs refinement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try Out fossabot&lt;/head&gt;
    &lt;p&gt;fossabot’s public preview is available as a GitHub app. Every user gets $15 of analysis credit, replenished every month. Let loose the updates!&lt;/p&gt;
    &lt;p&gt;Today, fossabot will auto-analyze Pull Requests opened from Dependabot, Renovate or Snyk. Soon, fossabot will open its own PRs with pre-planning and pre-analysis taken into account.&lt;/p&gt;
    &lt;p&gt;Reach out to get a demo of fossabot and let's figure out how to get your teams caught up on updates.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fossa.com/blog/fossabot-dependency-upgrade-ai-agent/"/><published>2025-10-01T16:30:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45439997</id><title>The RAG Obituary: Killed by agents, buried by context windows</title><updated>2025-10-02T00:44:23.011897+00:00</updated><content>&lt;doc fingerprint="3740cad20bcd8702"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The RAG Obituary: Killed by Agents, Buried by Context Windows&lt;/head&gt;
    &lt;head rend="h3"&gt;Why Retrieval-Augmented Generation Won’t Survive the Context Revolution and the End of Chunking, Embeddings, and Rerankers as We Know Them.&lt;/head&gt;
    &lt;p&gt;I’ve been working in AI and search for a decade. First building Doctrine, the largest European legal search engine and now building Fintool, an AI-powered financial research platform that helps institutional investors analyze companies, screen stocks, and make investment decisions.&lt;/p&gt;
    &lt;p&gt;After three years of building, optimizing, and scaling LLMs with retrieval-augmented generation (RAG) systems, I believe we’re witnessing the twilight of RAG-based architectures. As context windows explode and agent-based architectures mature, my controversial opinion is that the current RAG infrastructure we spent so much time building and optimizing is on the decline.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Rise of Retrieval-Augmented Generation&lt;/head&gt;
    &lt;p&gt;In late 2022, ChatGPT took the world by storm. People started endless conversations, delegating crucial work only to realize that the underlying model, GPT-3.5 could only handle 4,096 tokens... roughly six pages of text!&lt;/p&gt;
    &lt;p&gt;The AI world faced a fundamental problem: how do you make an intelligent system work with knowledge bases that are orders of magnitude larger than what it can read at once?&lt;/p&gt;
    &lt;p&gt;The answer became Retrieval-Augmented Generation (RAG), an architectural pattern that would dominate AI for the next three years.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Mathematical Reality of Early LLMs&lt;/head&gt;
    &lt;p&gt;GPT-3.5 could handle 4,096 token and the next model GPT-4 doubled it to 8,192 tokens, about twelve pages. This wasn’t just inconvenient; it was architecturally devastating.&lt;/p&gt;
    &lt;p&gt;Consider the numbers: A single SEC 10-K filing contains approximately 51,000 tokens (130+ pages).&lt;/p&gt;
    &lt;p&gt;With 8,192 tokens, you could see less than 16% of a 10-K filing. It’s like reading a financial report through a keyhole!&lt;/p&gt;
    &lt;head rend="h3"&gt;The RAG Architecture: A Technical Deep Dive&lt;/head&gt;
    &lt;p&gt;RAG emerged as an elegant solution borrowed directly from search engines. Just as Google displays 10 blue links with relevant snippets for your query, RAG retrieves the most pertinent document fragments and feeds them to the LLM for synthesis.&lt;/p&gt;
    &lt;p&gt;The core idea is beautifully simple: if you can’t fit everything in context, find the most relevant pieces and use those. It turns LLMs into sophisticated search result summarizers.&lt;/p&gt;
    &lt;p&gt;Basically, LLMs can’t read the whole book but they can know who dies at the end; convenient!&lt;/p&gt;
    &lt;head rend="h4"&gt;The Chunking Challenge&lt;/head&gt;
    &lt;p&gt;Long documents need to be chunked into pieces and it’s when problems start. Those digestible pieces are typically 400-1,000 tokens each which is basically 300-750 words.&lt;/p&gt;
    &lt;p&gt;The problem? It isn’t as simple as cutting every 500 words.&lt;/p&gt;
    &lt;p&gt;Consider chunking a typical SEC 10-K annual report. The document has a complex hierarchical structure:&lt;/p&gt;
    &lt;p&gt;- Item 1: Business Overview (10-15 pages)&lt;/p&gt;
    &lt;p&gt;- Item 1A: Risk Factors (20-30 pages)&lt;/p&gt;
    &lt;p&gt;- Item 7: Management’s Discussion and Analysis (30-40 pages)&lt;/p&gt;
    &lt;p&gt;- Item 8: Financial Statements (40-50 pages)&lt;/p&gt;
    &lt;p&gt;After naive chunking at 500 tokens, critical information gets scattered:&lt;/p&gt;
    &lt;p&gt;- Revenue recognition policies split across 3 chunks&lt;/p&gt;
    &lt;p&gt;- A risk factor explanation broken mid-sentence&lt;/p&gt;
    &lt;p&gt;- Financial table headers separated from their data&lt;/p&gt;
    &lt;p&gt;- MD&amp;amp;A narrative divorced from the numbers it’s discussing&lt;/p&gt;
    &lt;p&gt;If you search for “revenue growth drivers,” you might get a chunk mentioning growth but miss the actual numerical data in a different chunk, or the strategic context from MD&amp;amp;A in yet another chunk!&lt;/p&gt;
    &lt;p&gt;At Fintool, we’ve developed sophisticated chunking strategies that go beyond naive text splitting:&lt;/p&gt;
    &lt;p&gt;- Hierarchical Structure Preservation: We maintain the nested structure from Item 1 (Business) down to sub-sections like geographic segments, creating a tree-like document representation&lt;/p&gt;
    &lt;p&gt;- Table Integrity: Financial tables are never split—income statements, balance sheets, and cash flow statements remain atomic units with headers and data together&lt;/p&gt;
    &lt;p&gt;- Cross-Reference Preservation: We maintain links between narrative sections and their corresponding financial data, preserving the “See Note X” relationships&lt;/p&gt;
    &lt;p&gt;- Temporal Coherence: Year-over-year comparisons and multi-period analyses stay together as single chunks&lt;/p&gt;
    &lt;p&gt;- Footnote Association: Footnotes remain connected to their referenced items through metadata linking&lt;/p&gt;
    &lt;p&gt;Each chunk at Fintool is enriched with extensive metadata:&lt;/p&gt;
    &lt;p&gt;- Filing type (10-K, 10-Q, 8-K)&lt;/p&gt;
    &lt;p&gt;- Fiscal period and reporting date&lt;/p&gt;
    &lt;p&gt;- Section hierarchy (Item 7 &amp;gt; Liquidity &amp;gt; Cash Position)&lt;/p&gt;
    &lt;p&gt;- Table identifiers and types&lt;/p&gt;
    &lt;p&gt;- Cross-reference mappings&lt;/p&gt;
    &lt;p&gt;- Company identifiers (CIK, ticker)&lt;/p&gt;
    &lt;p&gt;- Industry classification codes&lt;/p&gt;
    &lt;p&gt;This allows for more accurate retrieval but even our intelligent chunking can’t solve the fundamental problem: we’re still working with fragments instead of complete documents!&lt;/p&gt;
    &lt;p&gt;Once you have the chunks, you need a way to search them. One way is to embed your chunks.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Embedding and Retrieval Pipeline&lt;/head&gt;
    &lt;p&gt;Each chunk is converted into a high‑dimensional vector (typically 1,536 dimensions in most embedding models). These vectors live in a space where, theoretically, similar concepts are close together.&lt;/p&gt;
    &lt;p&gt;When a user asks a question, that question also becomes a vector. The system finds the chunks whose vectors are closest to the query vector using cosine similarity.&lt;/p&gt;
    &lt;p&gt;It’s elegant in theory and in practice, it’s a nightmare of edge cases.&lt;/p&gt;
    &lt;p&gt;Embedding models are trained on general text and struggle with specific terminologies. They find similarities but they can’t distinguish between “revenue recognition” (accounting policy) and “revenue growth” (business performance).&lt;/p&gt;
    &lt;p&gt;Consider that example: Query: “What is the company’s litigation exposure?&lt;/p&gt;
    &lt;p&gt;RAG searches for “litigation” and returns 50 chunks:&lt;/p&gt;
    &lt;p&gt;- Chunks 1-10: Various mentions of “litigation” in boilerplate risk factors&lt;/p&gt;
    &lt;p&gt;- Chunks 11-20: Historical cases from 2019 (already settled)&lt;/p&gt;
    &lt;p&gt;- Chunks 21-30: Forward-looking safe harbor statements&lt;/p&gt;
    &lt;p&gt;- Chunks 31-40: Duplicate descriptions from different sections&lt;/p&gt;
    &lt;p&gt;- Chunks 41-50: Generic “we may face litigation” warnings&lt;/p&gt;
    &lt;p&gt;What RAG Reports: $500M in litigation (from Legal Proceedings section)&lt;/p&gt;
    &lt;p&gt;What’s Actually There:&lt;/p&gt;
    &lt;p&gt;- $500M in Legal Proceedings (Item 3)&lt;/p&gt;
    &lt;p&gt;- $700M in Contingencies note (”not material individually”)&lt;/p&gt;
    &lt;p&gt;- $1B new class action in Subsequent Events&lt;/p&gt;
    &lt;p&gt;- $800M indemnification obligations (different section)&lt;/p&gt;
    &lt;p&gt;- $2B probable losses in footnotes (keyword “probable” not “litigation”)&lt;/p&gt;
    &lt;p&gt;The actual Exposure is $5.1B. 10x what RAG found. Oupsy! By late 2023, most builders realized pure vector search wasn’t enough.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hybrid Search: The Complexity That Actually Works&lt;/head&gt;
    &lt;p&gt;Enter hybrid search: combine semantic search (embeddings) with the traditional keyword search (BM25). This is where things get interesting.&lt;/p&gt;
    &lt;p&gt;BM25 (Best Matching 25) is a probabilistic retrieval model that excels at exact term matching. Unlike embeddings, BM25:&lt;/p&gt;
    &lt;p&gt;- Rewards Exact Matches: When you search for “EBITDA,” you get documents with “EBITDA,” not “operating income” or “earnings”&lt;/p&gt;
    &lt;p&gt;- Handles Rare Terms Better: Financial jargon like “CECL” (Current Expected Credit Losses) or “ASC 606” gets proper weight&lt;/p&gt;
    &lt;p&gt;- Document Length Normalization: Doesn’t penalize longer documents&lt;/p&gt;
    &lt;p&gt;- Term Frequency Saturation: Multiple mentions of “revenue” don’t overshadow other important terms&lt;/p&gt;
    &lt;p&gt;At Fintool, we’ve built a sophisticated hybrid search system:&lt;/p&gt;
    &lt;p&gt;1. Parallel Processing: We run semantic and keyword searches simultaneously&lt;/p&gt;
    &lt;p&gt;2. Dynamic Weighting: Our system adjusts weights based on query characteristics:&lt;/p&gt;
    &lt;p&gt;- Specific financial metrics? BM25 gets 70% weight&lt;/p&gt;
    &lt;p&gt;- Conceptual questions? Embeddings get 60% weight&lt;/p&gt;
    &lt;p&gt;- Mixed queries? 50/50 split with result analysis&lt;/p&gt;
    &lt;p&gt;3. Score Normalization: Different scoring scales are normalized using:&lt;/p&gt;
    &lt;p&gt;- Min-max scaling for BM25 scores&lt;/p&gt;
    &lt;p&gt;- Cosine similarity already normalized for embeddings&lt;/p&gt;
    &lt;p&gt;- Z-score normalization for outlier handling&lt;/p&gt;
    &lt;p&gt;So at the end the embeddings search and the keywords search retrieve chunks and the search engine combines them using Reciprocal Rank Fusion. RRF merges rankings so items that consistently appear near the top across systems float higher, even if no system put them at #1!&lt;/p&gt;
    &lt;p&gt;So now you think it’s done right? But hell no!&lt;/p&gt;
    &lt;head rend="h4"&gt;The Reranking Bottleneck: RAG’s Dirty Secret&lt;/head&gt;
    &lt;p&gt;Here’s what nobody talks about: even after all that retrieval work, you’re not done. You need to rerank the chunks one more time to get a good retrieval and it’s not easy. Rerankers are ML models that take the search results and reorder them by relevance to your specific query limiting the number of chunks sent to the LLM.&lt;/p&gt;
    &lt;p&gt;Not only LLMs are context poor, they also struggle when dealing with too much information. It’s vital to reduce the number of chunks sent to the LLM for the final answer.&lt;/p&gt;
    &lt;p&gt;The Reranking Pipeline:&lt;/p&gt;
    &lt;p&gt;1. Initial search retrieval with embeddings + keywords gets you 100-200 chunks&lt;/p&gt;
    &lt;p&gt;2. Reranker ranks the top 10&lt;/p&gt;
    &lt;p&gt;3. Top 10 are fed to the LLM to answer the question&lt;/p&gt;
    &lt;p&gt;Here is the challenge with reranking:&lt;/p&gt;
    &lt;p&gt;- Latency Explosion: Rerank adds between 300-2000ms per query. Ouch.&lt;/p&gt;
    &lt;p&gt;- Cost Multiplication: it adds significant extra cost to every query. For instance, Cohere Rerank 3.5 costs $2.00 per 1,000 search units, making reranking expensive.&lt;/p&gt;
    &lt;p&gt;- Context Limits: Rerankers typically handle few chunks (Cohere Rerank supports only 4096 tokens), so if you need to re-rank more than that, you have to split it into different parallel API calls and merge them!&lt;/p&gt;
    &lt;p&gt;- Another Model to Manage: One more API, one more failure point&lt;/p&gt;
    &lt;p&gt;Re-rank is one more step in a complex pipeline.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Infrastructure Burden of Traditional RAG&lt;/head&gt;
    &lt;p&gt;What I find difficult with RAG is what I call the “cascading failure problem”.&lt;/p&gt;
    &lt;p&gt;1. Chunking can fail (split tables) or be too slow (especially when you have to ingest and chunk gigabytes of data in real-time)&lt;/p&gt;
    &lt;p&gt;2. Embedding can fail (wrong similarity)&lt;/p&gt;
    &lt;p&gt;3. BM25 can fail (term mismatch)&lt;/p&gt;
    &lt;p&gt;4. Hybrid fusion can fail (bad weights)&lt;/p&gt;
    &lt;p&gt;5. Reranking can fail (wrong priorities)&lt;/p&gt;
    &lt;p&gt;Each stage compounds the errors of the previous stage. Beyond the complexity of hybrid search itself, there’s an infrastructure burden that’s rarely discussed.&lt;/p&gt;
    &lt;p&gt;Running production Elasticsearch is not easy. You’re looking at maintaining TB+ of indexed data for comprehensive document coverage, which requires 128-256GB RAM minimum just to get decent performance. The real nightmare comes with re-indexing. Every schema change forces a full re-indexing that takes 48-72 hours for large datasets. On top of that, you’re constantly dealing with cluster management, sharding strategies, index optimization, cache tuning, backup and disaster recovery, and version upgrades that regularly include breaking changes.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Fundamental Limitations of RAG for Complex Documents&lt;/head&gt;
    &lt;p&gt;Here are some structural limitations:&lt;/p&gt;
    &lt;p&gt;1. Context Fragmentation&lt;/p&gt;
    &lt;p&gt;- Long documents are interconnected webs, not independent paragraphs&lt;/p&gt;
    &lt;p&gt;- A single question might require information from 20+ documents&lt;/p&gt;
    &lt;p&gt;- Chunking destroys these relationships permanently&lt;/p&gt;
    &lt;p&gt;2. Semantic Search Fails on Numbers&lt;/p&gt;
    &lt;p&gt;- “$45.2M” and “$45,200,000” have different embeddings&lt;/p&gt;
    &lt;p&gt;- “Revenue increased 10%” and “Revenue grew by a tenth” rank differently&lt;/p&gt;
    &lt;p&gt;- Tables full of numbers have poor semantic representations&lt;/p&gt;
    &lt;p&gt;3. No Causal Understanding&lt;/p&gt;
    &lt;p&gt;- RAG can’t follow “See Note 12” → Note 12 → Schedule K&lt;/p&gt;
    &lt;p&gt;- Can’t understand that discontinued operations affect continuing operations&lt;/p&gt;
    &lt;p&gt;- Can’t trace how one financial item impacts another&lt;/p&gt;
    &lt;p&gt;4. The Vocabulary Mismatch Problem&lt;/p&gt;
    &lt;p&gt;- Companies use different terms for the same concept&lt;/p&gt;
    &lt;p&gt;- “Adjusted EBITDA” vs “Operating Income Before Special Items”&lt;/p&gt;
    &lt;p&gt;- RAG retrieves based on terms, not concepts&lt;/p&gt;
    &lt;p&gt;5. Temporal Blindness&lt;/p&gt;
    &lt;p&gt;- Can’t distinguish Q3 2024 from Q3 2023 reliably&lt;/p&gt;
    &lt;p&gt;- Mixes current period with prior period comparisons&lt;/p&gt;
    &lt;p&gt;- No understanding of fiscal year boundaries&lt;/p&gt;
    &lt;p&gt;These aren’t minor issues. They’re fundamental limitations of the retrieval paradigm.&lt;/p&gt;
    &lt;p&gt;Three months ago I stumbled on an innovation on retrievial that blew my mind&lt;/p&gt;
    &lt;head rend="h2"&gt;The Emergence of Agentic Search - A New Paradigm&lt;/head&gt;
    &lt;p&gt;In May 2025, Anthropic released Claude Code, an AI coding agent that works in the terminal. At first, I was surprised by the form factor. A terminal? Are we back in 1980? no UI?&lt;/p&gt;
    &lt;p&gt;Back then, I was using Cursor, a product that excelled at traditional RAG. I gave it access to my codebase to embed my files and Cursor ran a search n my codebase before answering my query. Life was good. But when testing Claude Code, one thing stood out:&lt;/p&gt;
    &lt;p&gt;It was better and faster and not because their RAG was better but because there was no RAG.&lt;/p&gt;
    &lt;head rend="h4"&gt;How Claude Code Search Works&lt;/head&gt;
    &lt;p&gt;Instead of a complex pipeline of chunking, embedding, and searching, Claude Code uses direct filesystem tools:&lt;/p&gt;
    &lt;p&gt;1. Grep (Ripgrep)&lt;/p&gt;
    &lt;p&gt;- Lightning-fast regex search through file contents&lt;/p&gt;
    &lt;p&gt;- No indexing required. It searches live files instantly&lt;/p&gt;
    &lt;p&gt;- Full regex support for precise pattern matching&lt;/p&gt;
    &lt;p&gt;- Can filter by file type or use glob patterns&lt;/p&gt;
    &lt;p&gt;- Returns exact matches with context lines&lt;/p&gt;
    &lt;p&gt;2. Glob&lt;/p&gt;
    &lt;p&gt;- Direct file discovery by name patterns&lt;/p&gt;
    &lt;p&gt;- Finds files like `**/*.py` or `src/**/*.ts` instantly&lt;/p&gt;
    &lt;p&gt;- Returns files sorted by modification time (recency bias)&lt;/p&gt;
    &lt;p&gt;- Zero overhead—just filesystem traversal&lt;/p&gt;
    &lt;p&gt;3. Task Agents&lt;/p&gt;
    &lt;p&gt;- Autonomous multi-step exploration&lt;/p&gt;
    &lt;p&gt;- Handle complex queries requiring investigation&lt;/p&gt;
    &lt;p&gt;- Combine multiple search strategies adaptively&lt;/p&gt;
    &lt;p&gt;- Build understanding incrementally&lt;/p&gt;
    &lt;p&gt;- Self-correct based on findings&lt;/p&gt;
    &lt;p&gt;By the way, Grep was invented in 1973. It’s so... primitive. And that’s the genius of it.&lt;/p&gt;
    &lt;p&gt;Claude Code doesn’t retrieve. It investigates:&lt;/p&gt;
    &lt;p&gt;- Runs multiple searches in parallel (Grep + Glob simultaneously)&lt;/p&gt;
    &lt;p&gt;- Starts broad, then narrows based on discoveries&lt;/p&gt;
    &lt;p&gt;- Follows references and dependencies naturally&lt;/p&gt;
    &lt;p&gt;- No embeddings, no similarity scores, no reranking&lt;/p&gt;
    &lt;p&gt;It’s simple, it’s fast and it’s based on a new assumption that LLMs will go from context poor to context rich.&lt;/p&gt;
    &lt;p&gt;Claude Code proved that with sufficient context and intelligent navigation, you don’t need RAG at all. The agent can:&lt;/p&gt;
    &lt;p&gt;- Load entire files or modules directly&lt;/p&gt;
    &lt;p&gt;- Follow cross-references in real-time&lt;/p&gt;
    &lt;p&gt;- Understand structure and relationships&lt;/p&gt;
    &lt;p&gt;- Maintain complete context throughout investigation&lt;/p&gt;
    &lt;p&gt;This isn’t just better than RAG—it’s a fundamentally different paradigm. And what works for code can work for any long documents that are not coding files.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Context Revolution: From Scarcity to Abundance&lt;/head&gt;
    &lt;p&gt;The context window explosion made Claude Code possible:&lt;/p&gt;
    &lt;p&gt;2022-2025 Context-Poor Era:&lt;/p&gt;
    &lt;p&gt;- GPT-4: 8K tokens (~12 pages)&lt;/p&gt;
    &lt;p&gt;- GPT-4-32k: 32K tokens (~50 pages)&lt;/p&gt;
    &lt;p&gt;2025 and beyond Context Revolution:&lt;/p&gt;
    &lt;p&gt;- Claude Sonnet 4: 200k tokens (~700 pages)&lt;/p&gt;
    &lt;p&gt;- Gemini 2.5: 1M tokens (~3,000 pages)&lt;/p&gt;
    &lt;p&gt;- Grok 4-fast: 2M tokens (~6,000 pages)&lt;/p&gt;
    &lt;p&gt;At 2M tokens, you can fit an entire year of SEC filings for most companies.&lt;/p&gt;
    &lt;p&gt;The trajectory is even more dramatic: we’re likely heading toward 10M+ context windows by 2027, with Sam Altman hinting at billions of context tokens on the horizon. This represents a fundamental shift in how AI systems process information. Equally important, attention mechanisms are rapidly improving—LLMs are becoming far better at maintaining coherence and focus across massive context windows without getting “lost” in the noise.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Claude Code Insight: Why Context Changes Everything&lt;/head&gt;
    &lt;p&gt;Claude Code demonstrated that with enough context, search becomes navigation:&lt;/p&gt;
    &lt;p&gt;- No need to retrieve fragments when you can load complete files&lt;/p&gt;
    &lt;p&gt;- No need for similarity when you can use exact matches&lt;/p&gt;
    &lt;p&gt;- No need for reranking when you follow logical paths&lt;/p&gt;
    &lt;p&gt;- No need for embeddings when you have direct access&lt;/p&gt;
    &lt;p&gt;It’s mind-blowing. LLMs are getting really good at agentic behaviors meaning they can organize their work into tasks to accomplish an objective.&lt;/p&gt;
    &lt;p&gt;Here’s what tools like ripgrep bring to the search table:&lt;/p&gt;
    &lt;p&gt;- No Setup: No index. No overhead. Just point and search.&lt;/p&gt;
    &lt;p&gt;- Instant Availability: New documents are searchable the moment they hit the filesystem (no indexing latency!)&lt;/p&gt;
    &lt;p&gt;- Zero Maintenance: No clusters to manage, no indices to optimize, no RAM to provision&lt;/p&gt;
    &lt;p&gt;- Blazing Fast: For a 100K line codebase, Elasticsearch needs minutes to index. Ripgrep searches it in milliseconds with zero prep.&lt;/p&gt;
    &lt;p&gt;- Cost: $0 infrastructure cost vs a lot of $$$ for Elasticsearch&lt;/p&gt;
    &lt;p&gt;So back to our previous example on SEC filings. An agent can SEC filing structure intrinsically:&lt;/p&gt;
    &lt;p&gt;- Hierarchical Awareness: Knows that Item 1A (Risk Factors) relates to Item 7 (MD&amp;amp;A)&lt;/p&gt;
    &lt;p&gt;- Cross-Reference Following: Automatically traces “See Note 12” references&lt;/p&gt;
    &lt;p&gt;- Multi-Document Coordination: Connects 10-K, 10-Q, 8-K, and proxy statements&lt;/p&gt;
    &lt;p&gt;- Temporal Analysis: Compares year-over-year changes systematically&lt;/p&gt;
    &lt;p&gt;For searches across thousands of companies or decades of filings, it might still use hybrid search, but now as a tool for agents:&lt;/p&gt;
    &lt;p&gt;- Initial broad search using hybrid retrieval&lt;/p&gt;
    &lt;p&gt;- Agent loads full documents for top results&lt;/p&gt;
    &lt;p&gt;- Deep analysis within full context&lt;/p&gt;
    &lt;p&gt;- Iterative refinement based on findings&lt;/p&gt;
    &lt;p&gt;My guess is traditional RAG is now a search tool among others and that agents will always prefer grep and reading the whole file because they are context rich and can handle long-running tasks.&lt;/p&gt;
    &lt;p&gt;Consider our $6.5B lease obligation question as an example:&lt;/p&gt;
    &lt;p&gt;Step 1: Find “lease” in main financial statements&lt;/p&gt;
    &lt;p&gt;→ Discovers “See Note 12”&lt;/p&gt;
    &lt;p&gt;Step 2: Navigate to Note 12&lt;/p&gt;
    &lt;p&gt;→ Finds “excluding discontinued operations (Note 23)”&lt;/p&gt;
    &lt;p&gt;Step 3: Check Note 23&lt;/p&gt;
    &lt;p&gt;→ Discovers $2B additional obligations&lt;/p&gt;
    &lt;p&gt;Step 4: Cross-reference with MD&amp;amp;A&lt;/p&gt;
    &lt;p&gt;→ Identifies management’s explanation and adjustments&lt;/p&gt;
    &lt;p&gt;Step 5: Search for “subsequent events”&lt;/p&gt;
    &lt;p&gt;→ Finds post-balance sheet $500M lease termination&lt;/p&gt;
    &lt;p&gt;Final answer: $5B continuing + $2B discontinued - $500M terminated = $6.5B&lt;/p&gt;
    &lt;p&gt;The agent follows references like a human analyst would. No chunks. No embeddings. No reranking. Just intelligent navigation.&lt;/p&gt;
    &lt;p&gt;Basically, RAG is like a research assistant with perfect memory but no understanding:&lt;/p&gt;
    &lt;p&gt;- “Here are 50 passages that mention debt”&lt;/p&gt;
    &lt;p&gt;- Can’t tell you if debt is increasing or why&lt;/p&gt;
    &lt;p&gt;- Can’t connect debt to strategic changes&lt;/p&gt;
    &lt;p&gt;- Can’t identify hidden obligations&lt;/p&gt;
    &lt;p&gt;- Just retrieves text, doesn’t comprehend relationships&lt;/p&gt;
    &lt;p&gt;Agentic search is like a forensic accountant:&lt;/p&gt;
    &lt;p&gt;- Follows the money systematically&lt;/p&gt;
    &lt;p&gt;- Understands accounting relationships (assets = liabilities + equity)&lt;/p&gt;
    &lt;p&gt;- Identifies what’s missing or hidden&lt;/p&gt;
    &lt;p&gt;- Connects dots across time periods and documents&lt;/p&gt;
    &lt;p&gt;- Challenges management assertions with data&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Agentic Search Represents the Future&lt;/head&gt;
    &lt;p&gt;1. Increasing Document Complexity&lt;/p&gt;
    &lt;p&gt;- Documents are becoming longer and more interconnected&lt;/p&gt;
    &lt;p&gt;- Cross-references and external links are proliferating&lt;/p&gt;
    &lt;p&gt;- Multiple related documents need to be understood together&lt;/p&gt;
    &lt;p&gt;- Systems must follow complex trails of information&lt;/p&gt;
    &lt;p&gt;2. Structured Data Integration&lt;/p&gt;
    &lt;p&gt;- More documents combine structured and unstructured data&lt;/p&gt;
    &lt;p&gt;- Tables, narratives, and metadata must be understood together&lt;/p&gt;
    &lt;p&gt;- Relationships matter more than isolated facts&lt;/p&gt;
    &lt;p&gt;- Context determines meaning&lt;/p&gt;
    &lt;p&gt;3. Real-Time Requirements&lt;/p&gt;
    &lt;p&gt;- Information needs instant processing&lt;/p&gt;
    &lt;p&gt;- No time for re-indexing or embedding updates&lt;/p&gt;
    &lt;p&gt;- Dynamic document structures require adaptive approaches&lt;/p&gt;
    &lt;p&gt;- Live data demands live search&lt;/p&gt;
    &lt;p&gt;4. Cross-Document Understanding&lt;/p&gt;
    &lt;p&gt;Modern analysis requires connecting multiple sources:&lt;/p&gt;
    &lt;p&gt;- Primary documents&lt;/p&gt;
    &lt;p&gt;- Supporting materials&lt;/p&gt;
    &lt;p&gt;- Historical versions&lt;/p&gt;
    &lt;p&gt;- Related filings&lt;/p&gt;
    &lt;p&gt;RAG treats each document independently. Agentic search builds cumulative understanding.&lt;/p&gt;
    &lt;p&gt;5. Precision Over Similarity&lt;/p&gt;
    &lt;p&gt;- Exact information matters more than similar content&lt;/p&gt;
    &lt;p&gt;- Following references beats finding related text&lt;/p&gt;
    &lt;p&gt;- Structure and hierarchy provide crucial context&lt;/p&gt;
    &lt;p&gt;- Navigation beats retrieval&lt;/p&gt;
    &lt;p&gt;The evidence is becoming clear. While RAG served us well in the context-poor era, agentic search represents a fundamental evolution. The potential benefits of agentic search are compelling:&lt;/p&gt;
    &lt;p&gt;- Elimination of hallucinations from missing context&lt;/p&gt;
    &lt;p&gt;- Complete answers instead of fragments&lt;/p&gt;
    &lt;p&gt;- Faster insights through parallel exploration&lt;/p&gt;
    &lt;p&gt;- Higher accuracy through systematic navigation&lt;/p&gt;
    &lt;p&gt;- Massive infrastructure cost reduction&lt;/p&gt;
    &lt;p&gt;- Zero index maintenance overhead&lt;/p&gt;
    &lt;p&gt;The key insight? Complex document analysis—whether code, financial filings, or legal contracts—isn’t about finding similar text. It’s about understanding relationships, following references, and maintaining precision. The combination of large context windows and intelligent navigation delivers what retrieval alone never could.&lt;/p&gt;
    &lt;p&gt;RAG was a clever workaround for a context-poor era. It helped us bridge the gap between tiny windows and massive documents, but it was always a band-aid. The future won’t be about splitting documents into fragments and juggling embeddings. It will be about agents that can navigate, reason, and hold entire corpora in working memory.&lt;/p&gt;
    &lt;p&gt;We are entering the post-retrieval age. The winners will not be the ones who maintain the biggest vector databases, but the ones who design the smartest agents to traverse abundant context and connect meaning across documents. In hindsight, RAG will look like training wheels. Useful, necessary, but temporary.&lt;/p&gt;
    &lt;p&gt;The next decade of AI search will belong to systems that read and reason end-to-end. Retrieval isn’t dead—it’s just been demoted.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nicolasbustamante.com/p/the-rag-obituary-killed-by-agents"/><published>2025-10-01T16:51:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45440431</id><title>OpenTSLM: Language models that understand time series</title><updated>2025-10-02T00:44:22.821268+00:00</updated><content>&lt;doc fingerprint="748d54ec193a388b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenTSLM&lt;/head&gt;
    &lt;p&gt;The Future of AI Delivered on Time&lt;/p&gt;
    &lt;p&gt;AI understands text, images, audio, and video.&lt;lb/&gt;But the real world runs on time.&lt;/p&gt;
    &lt;p&gt;Every heartbeat, price tick, sensor pulse, machine log, and user click is a temporal signal.&lt;lb/&gt;Current models can't reason about them.&lt;/p&gt;
    &lt;p&gt;We're changing that.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New Class of Foundation Models&lt;/head&gt;
    &lt;p&gt;Time-Series Language Models (TSLMs) are multimodal foundation models with time series as a native modality, next to text, enabling direct reasoning, explanation, and forecasting over temporal data in natural language.&lt;/p&gt;
    &lt;p&gt;Our research shows order-of-magnitude gains in temporal reasoning while running on smaller, faster backbones. TSLMs are not an add-on. They're a new modality for AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Core, Frontier Edge&lt;/head&gt;
    &lt;p&gt;OpenTSLM: Lightweight base models trained on public data, released openly. They set the standard for temporal reasoning and power a global developer and research ecosystem.&lt;/p&gt;
    &lt;p&gt;Frontier TSLMs: Advanced proprietary models trained on specialized data, delivering enterprise-grade performance and powering APIs, fine-tuning, and vertical solutions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our Vision&lt;/head&gt;
    &lt;p&gt;We're building the temporal interface for AI - the layer that connects continuous real-world signals to intelligent decisions and autonomous agents.&lt;/p&gt;
    &lt;p&gt;A universal TSLM will power proactive healthcare, adaptive robotics, resilient infrastructure, and new forms of human-AI collaboration.&lt;/p&gt;
    &lt;head rend="h2"&gt;About Us&lt;/head&gt;
    &lt;p&gt;OpenTSLM is a team of scientists, engineers, and builders from ETH, Stanford, Harvard, Cambridge, TUM, CDTM, Google, Meta, AWS, and beyond. We are the original authors of the OpenTSLM paper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.opentslm.com/"/><published>2025-10-01T17:25:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45440571</id><title>What good workplace politics looks like in practice</title><updated>2025-10-02T00:44:22.624068+00:00</updated><content>&lt;doc fingerprint="25ca331c7e610687"&gt;
  &lt;main&gt;
    &lt;p&gt;Say the word “politics” to most engineers and watch their face scrunch up like they just bit into a lemon. We’ve all been conditioned to believe that workplace politics is this dirty game played by manipulative ladder-climbers while the “real” engineers focus on the code.&lt;/p&gt;
    &lt;p&gt;I used to think the same way. For years as an engineer, I wore my hatred of politics like a badge of honor. I was above all that nonsense. I just wanted to ship. Politics was for those other people, the ones who didn’t have what it takes technically.&lt;/p&gt;
    &lt;p&gt;Now I think the opposite: politics isn’t the problem; bad politics is. And pretending politics doesn’t exist? That’s how bad politics wins.&lt;/p&gt;
    &lt;p&gt;Politics is just how humans coordinate in groups. It’s the invisible network of relationships, influence, and informal power that exists in every organization. You can refuse to participate, but that doesn’t make it go away. It just means decisions get made without you.&lt;/p&gt;
    &lt;p&gt;Think about the last time a terrible technical decision got pushed through at your company. Maybe it was adopting some overcomplicated architecture, or choosing a vendor that everyone knew was wrong, or killing a project that was actually working. I bet if you dig into what happened, you’ll find it wasn’t because the decision-makers were stupid. It’s because the people with the right information weren’t in the room. They “didn’t do politics.”&lt;/p&gt;
    &lt;p&gt;Meanwhile, someone who understood how influence works was in that room, making their case, building coalitions, showing they’d done their homework. And their idea won. Not because it was better, but because they showed up to play while everyone else was “too pure” for politics.&lt;/p&gt;
    &lt;p&gt;Ideas don’t speak. People do. And the people who understand how to navigate organizational dynamics, build relationships, and yes, play politics? Their ideas get heard.&lt;/p&gt;
    &lt;p&gt;When you build strong relationships across teams, understand what motivates different stakeholders, and know how to build consensus, you’re doing politics. When you take time to explain your technical decisions to non-technical stakeholders in language they understand, that’s politics. When you grab coffee with someone from another team to understand their challenges, that’s politics too.&lt;/p&gt;
    &lt;p&gt;Good politics is just being strategic about relationships and influence in the service of good outcomes.&lt;/p&gt;
    &lt;p&gt;The best technical leaders are incredibly political. They just don’t call it that. They call it “stakeholder management” or “building alignment” or “organizational awareness.” But it’s politics, and they’re good at it.&lt;/p&gt;
    &lt;p&gt;The engineers who refuse to engage with politics often complain that their companies make bad technical decisions. But they’re not willing to do what it takes to influence those decisions. They want a world where technical merit alone determines outcomes. That world doesn’t exist and never has.&lt;/p&gt;
    &lt;p&gt;This isn’t about becoming a scheming backstabber. As I wrote in Your Strengths Are Your Weaknesses, the same trait can be positive or negative depending on how you use it. Politics is the same way. You can use political skills to manipulate and self-promote, or you can use them to get good ideas implemented and protect your team from bad decisions.&lt;/p&gt;
    &lt;p&gt;Here’s what good politics looks like in practice:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Building relationships before you need them. That random coffee with someone from the data team? Six months later, they’re your biggest advocate for getting engineering resources for your data pipeline project.&lt;/item&gt;
      &lt;item&gt;Understanding the real incentives. Your VP doesn’t care about your beautiful microservices architecture. They care about shipping features faster. Frame your technical proposals in terms of what they actually care about.&lt;/item&gt;
      &lt;item&gt;Managing up effectively. Your manager is juggling competing priorities you don’t see. Keep them informed about what matters, flag problems early with potential solutions, and help them make good decisions. When they trust you to handle things, they’ll fight for you when it matters&lt;/item&gt;
      &lt;item&gt;Creating win-win situations. Instead of fighting for resources, find ways to help other teams while getting what you need. It doesn’t have to be a zero-sum game.&lt;/item&gt;
      &lt;item&gt;Being visible. If you do great work but nobody knows about it, did it really happen? Share your wins, present at all-hands, write those design docs that everyone will reference later.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The alternative to good politics isn’t no politics. It’s bad politics winning by default. It’s the loud person who’s wrong getting their way because the quiet person who’s right won’t speak up. It’s good projects dying because nobody advocated for them. It’s talented people leaving because they couldn’t navigate the organizational dynamics.&lt;/p&gt;
    &lt;p&gt;Stop pretending you’re above politics. You’re not. Nobody is. The only question is whether you’ll get good at it or keep losing to people who already are.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://terriblesoftware.org/2025/10/01/stop-avoiding-politics/"/><published>2025-10-01T17:36:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45441069</id><title>Jane Goodall has died</title><updated>2025-10-02T00:44:22.476220+00:00</updated><content>&lt;doc fingerprint="764eb86741a32728"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Jane Goodall, who transformed understanding of humankind by studying chimpanzees, dies at 91&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Share via&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Jane Goodall, the trailblazing naturalist whose intimate observations of chimpanzees in the African wild produced powerful insights that transformed basic conceptions of humankind, has died. She was 91.&lt;/p&gt;
    &lt;p&gt;A tireless advocate of preserving chimpanzees’ natural habitat, Goodall died on Wednesday morning in California of natural causes, the Jane Goodall Institute announced on its Instagram page.&lt;/p&gt;
    &lt;p&gt;“Dr. Goodall’s discoveries as an ethologist revolutionized science,” the Jane Goodall Institute said in a statement.&lt;/p&gt;
    &lt;p&gt;A protege of anthropologist Louis S.B. Leakey, Goodall made history in 1960 when she discovered that chimpanzees, humankind’s closest living ancestors, made and used tools, characteristics that scientists had long thought were exclusive to humans.&lt;/p&gt;
    &lt;p&gt;She also found that chimps hunted prey, ate meat, and were capable of a range of emotions and behaviors similar to those of humans, including filial love, grief and violence bordering on warfare.&lt;/p&gt;
    &lt;p&gt;In the course of establishing one of the world’s longest-running studies of wild animal behavior at what is now Tanzania’s Gombe Stream National Park, she gave her chimp subjects names instead of numbers, a practice that raised eyebrows in the male-dominated field of primate studies in the 1960s. But within a decade, the trim British scientist with the tidy ponytail was a National Geographic heroine, whose books and films educated a worldwide audience with stories of the apes she called David Graybeard, Mr. McGregor, Gilka and Flo.&lt;/p&gt;
    &lt;p&gt;“When we read about a woman who gives funny names to chimpanzees and then follows them into the bush, meticulously recording their every grunt and groom, we are reluctant to admit such activity into the big leagues,” the late biologist Stephen Jay Gould wrote of the scientific world’s initial reaction to Goodall.&lt;/p&gt;
    &lt;p&gt;But Goodall overcame her critics and produced work that Gould later characterized as “one of the Western world’s great scientific achievements.”&lt;/p&gt;
    &lt;p&gt;Tenacious and keenly observant, Goodall paved the way for other women in primatology, including the late gorilla researcher Dian Fossey and orangutan expert Birutė Galdikas. She was honored in 1995 with the National Geographic Society’s Hubbard Medal, which then had been bestowed only 31 times in the previous 90 years to such eminent figures as North Pole explorer Robert E. Peary and aviator Charles Lindbergh.&lt;/p&gt;
    &lt;p&gt;In her 80s she continued to travel 300 days a year to speak to schoolchildren and others about the need to fight deforestation, preserve chimpanzees’ natural habitat and promote sustainable development in Africa. She was in California as part of her speaking tour in the U.S. at the time of her death.&lt;/p&gt;
    &lt;p&gt;Jane Goodall brings “The Book of Hope” to the Los Angeles Times Book Club Feb. 25.&lt;/p&gt;
    &lt;p&gt;Goodall was born April 3, 1934, in London and grew up in the English coastal town of Bournemouth. The daughter of a businessman and a writer who separated when she was a child and later divorced, she was raised in a matriarchal household that included her maternal grandmother, her mother, Vanne, some aunts and her sister, Judy.&lt;/p&gt;
    &lt;p&gt;She demonstrated an affinity for nature from a young age, filling her bedroom with worms and sea snails that she rushed back to their natural homes after her mother told her they would otherwise die.&lt;/p&gt;
    &lt;p&gt;When she was about 5, she disappeared for hours to a dark henhouse to see how chickens laid eggs, so absorbed that she was oblivious to her family’s frantic search for her. She did not abandon her study until she observed the wondrous event.&lt;/p&gt;
    &lt;p&gt;“Suddenly with a plop, the egg landed on the straw. With clucks of pleasure the hen shook her feathers, nudged the egg with her beak, and left,” Goodall wrote almost 60 years later. “It is quite extraordinary how clearly I remember that whole sequence of events.”&lt;/p&gt;
    &lt;p&gt;Wildlife biologist Miguel Ordeñana was passionate about wild animals as a kid. In Jane Goodall, he found hope that would shape his life.&lt;/p&gt;
    &lt;p&gt;When finally she ran out of the henhouse with the exciting news, her mother did not scold her but patiently listened to her daughter’s account of her first scientific observation.&lt;/p&gt;
    &lt;p&gt;Later, she gave Goodall books about animals and adventure — especially the Doctor Dolittle tales and Tarzan. Her daughter became so enchanted with Tarzan’s world that she insisted on doing her homework in a tree.&lt;/p&gt;
    &lt;p&gt;“I was madly in love with the Lord of the Jungle, terribly jealous of his Jane,” Goodall wrote in her 1999 memoir, “Reason for Hope: A Spiritual Journey.” “It was daydreaming about life in the forest with Tarzan that led to my determination to go to Africa, to live with animals and write books about them.”&lt;/p&gt;
    &lt;p&gt;Her opportunity came after she finished high school. A week before Christmas in 1956 she was invited to visit an old school chum’s family farm in Kenya. Goodall saved her earnings from a waitress job until she had enough for a round-trip ticket.&lt;/p&gt;
    &lt;p&gt;She arrived in Kenya in 1957, thrilled to be living in the Africa she had “always felt stirring in my blood.” At a dinner party in Nairobi shortly after her arrival, someone told her that if she was interested in animals, she should meet Leakey, already famous for his discoveries in East Africa of man’s fossil ancestors.&lt;/p&gt;
    &lt;p&gt;She went to see him at what’s now the National Museum of Kenya, where he was curator. He hired her as a secretary and soon had her helping him and his wife, Mary, dig for fossils at Olduvai Gorge, a famous site in the Serengeti Plains in what is now northern Tanzania.&lt;/p&gt;
    &lt;p&gt;Leakey spoke to her of his desire to learn more about all the great apes. He said he had heard of a community of chimpanzees on the rugged eastern shore of Lake Tanganyika where an intrepid researcher might make valuable discoveries.&lt;/p&gt;
    &lt;p&gt;When Goodall told him this was exactly the kind of work she dreamed of doing, Leakey agreed to send her there.&lt;/p&gt;
    &lt;p&gt;It took Leakey two years to find funding, which gave Goodall time to study primate behavior and anatomy in London. She finally landed in Gombe in the summer of 1960.&lt;/p&gt;
    &lt;p&gt;On a rocky outcropping she called the Peak, Goodall made her first important observation. Scientists had thought chimps were docile vegetarians, but on this day about three months after her arrival, Goodall spied a group of the apes feasting on something pink. It turned out to be a baby bush pig.&lt;/p&gt;
    &lt;p&gt;Two weeks later, she made an even more exciting discovery — the one that would establish her reputation. She had begun to recognize individual chimps, and on a rainy October day in 1960, she spotted the one with white hair on his chin. He was sitting beside a mound of red earth, carefully pushing a blade of grass into a hole, then withdrawing it and poking it into his mouth.&lt;/p&gt;
    &lt;p&gt;When he finally ambled off, Goodall hurried over for a closer look. She picked up the abandoned grass stalk, stuck it into the same hole and pulled it out to find it covered with termites. The chimp she later named David Graybeard had been using the stalk to fish for the bugs.&lt;/p&gt;
    &lt;p&gt;“It was hard for me to believe what I had seen,” Goodall later wrote. “It had long been thought that we were the only creatures on earth that used and made tools. ‘Man the Toolmaker’ is how we were defined ...” What Goodall saw challenged man’s uniqueness.&lt;/p&gt;
    &lt;p&gt;When she sent her report to Leakey, he responded: “We must now redefine man, redefine tool, or accept chimpanzees as human!”&lt;/p&gt;
    &lt;p&gt;Goodall’s startling finding, published in Nature in 1964, enabled Leakey to line up funding to extend her stay at Gombe. It also eased Goodall’s admission to Cambridge University to study ethology. In 1965, she became the eighth person in Cambridge history to earn a doctorate without first having a bachelor’s degree.&lt;/p&gt;
    &lt;p&gt;In the meantime, she had met and in 1964 married Hugo Van Lawick, a gifted filmmaker who had traveled to Gombe to make a documentary about her chimp project. They had a child, Hugo Eric Louis — later nicknamed Grub — in 1967.&lt;/p&gt;
    &lt;p&gt;Goodall later said that raising Grub, who lived at Gombe until he was 9, gave her insights into the behavior of chimp mothers. Conversely, she had “no doubt that my observation of the chimpanzees helped me to be a better mother.”&lt;/p&gt;
    &lt;p&gt;“So,” Brett Morgen began, “you’ve been telling your story for so many years.&lt;/p&gt;
    &lt;p&gt;She and Van Lawick were married for 10 years, divorcing in 1974. The following year she married Derek Bryceson, director of Tanzania National Parks. He died of colon cancer four years later.&lt;/p&gt;
    &lt;p&gt;Within a year of arriving at Gombe, Goodall had chimps literally eating out of her hands. Toward the end of her second year there, David Graybeard, who had shown the least fear of her, was the first to allow her physical contact. She touched him lightly and he permitted her to groom him for a full minute before gently pushing her hand away. For an adult male chimpanzee who had grown up in the wild to tolerate physical contact with a human was, she wrote in her 1971 book “In the Shadow of Man,” “a Christmas gift to treasure.”&lt;/p&gt;
    &lt;p&gt;Her studies yielded a trove of other observations on behaviors, including etiquette (such as soliciting a pat on the rump to indicate submission) and the sex lives of chimps. She collected some of the most fascinating information on the latter by watching Flo, an older female with a bulbous nose and an amazing retinue of suitors who was bearing children well into her 40s.&lt;/p&gt;
    &lt;p&gt;Her reports initially caused much skepticism in the scientific community. “I was not taken very seriously by many of the scientists. I was known as a [National] Geographic cover girl,” she recalled in a CBS interview in 2012.&lt;/p&gt;
    &lt;p&gt;Her unorthodox personalizing of the chimps was particularly controversial. The editor of one of her first published papers insisted on crossing out all references to the creatures as “he” or “she” in favor of “it.” Goodall eventually prevailed.&lt;/p&gt;
    &lt;p&gt;Her most disturbing studies came in the mid-1970s, when she and her team of field workers began to record a series of savage attacks.&lt;/p&gt;
    &lt;p&gt;The incidents grew into what Goodall called the four-year war, a period of brutality carried out by a band of male chimpanzees from a region known as the Kasakela Valley. The marauders beat and slashed to death all the males in a neighboring colony and subjugated the breeding females, essentially annihilating an entire community.&lt;/p&gt;
    &lt;p&gt;It was the first time a scientist had witnessed organized aggression by one group of non-human primates against another. Goodall said this “nightmare time” forever changed her view of ape nature.&lt;/p&gt;
    &lt;p&gt;“During the first 10 years of the study I had believed ... that the Gombe chimpanzees were, for the most part, rather nicer than human beings,” she wrote in “Reason for Hope: A Spiritual Journey,” a 1999 book co-authored with Phillip Berman. “Then suddenly we found that the chimpanzees could be brutal — that they, like us, had a dark side to their nature.”&lt;/p&gt;
    &lt;p&gt;Critics tried to dismiss the evidence as merely anecdotal. Others thought she was wrong to publicize the violence, fearing that irresponsible scientists would use the information to “prove” that the tendency to war is innate in humans, a legacy from their ape ancestors. Goodall persisted in talking about the attacks, maintaining that her purpose was not to support or debunk theories about human aggression but to “understand a little better” the nature of chimpanzee aggression.&lt;/p&gt;
    &lt;p&gt;“My question was: How far along our human path, which has led to hatred and evil and full-scale war, have chimpanzees traveled?”&lt;/p&gt;
    &lt;p&gt;Her observations of chimp violence marked a turning point for primate researchers, who had considered it taboo to talk about chimpanzee behavior in human terms. But by the 1980s, much chimp behavior was being interpreted in ways that would have been labeled anthropomorphism — ascribing human traits to non-human entities — decades earlier. Goodall, in removing the barriers, raised primatology to new heights, opening the way for research on subjects ranging from political coalitions among baboons to the use of deception by an array of primates.&lt;/p&gt;
    &lt;p&gt;Chimp change&lt;/p&gt;
    &lt;p&gt;Her concern about protecting chimpanzees in the wild and in captivity led her in 1977 to found the Jane Goodall Institute to advocate for great apes and support research and public education. She also established Roots and Shoots, a program aimed at youths in 130 countries, and TACARE, which involves African villagers in sustainable development.&lt;/p&gt;
    &lt;p&gt;She became an international ambassador for chimps and conservation in 1986 when she saw a film about the mistreatment of laboratory chimps. The secretly taped footage “was like looking into the Holocaust,” she told interviewer Cathleen Rountree in 1998. From that moment, she became a globe-trotting crusader for animal rights.&lt;/p&gt;
    &lt;p&gt;In the 2017 documentary “Jane,” the producer pored through 140 hours of footage of Goodall that had been hidden away in the National Geographic archives. The film won a Los Angeles Film Critics Assn. Award, one of many honors it received.&lt;/p&gt;
    &lt;p&gt;In a ranging 2009 interview with Times columnist Patt Morrison, Goodall mused on topics from traditional zoos — she said most captive environments should be abolished — to climate change, a battle she feared humankind was quickly losing, if not lost already. She also spoke about the power of what one human can accomplish.&lt;/p&gt;
    &lt;p&gt;“I always say, ‘If you would spend just a little bit of time learning about the consequences of the choices you make each day’ — what you buy, what you eat, what you wear, how you interact with people and animals — and start consciously making choices, that would be beneficial rather than harmful.”&lt;/p&gt;
    &lt;p&gt;As the years passed, Goodall continued to track Gombe’s chimps, accumulating enough information to draw the arcs of their lives — from birth through sometimes troubled adolescence, maturity, illness and finally death.&lt;/p&gt;
    &lt;p&gt;She wrote movingly about how she followed Mr. McGregor, an older, somewhat curmudgeonly chimp, through his agonizing death from polio, and how the orphan Gilka survived to lonely adulthood only to have her babies snatched from her by a pair of cannibalistic female chimps.&lt;/p&gt;
    &lt;p&gt;Her reaction in 1972 to the death of Flo, a prolific female known as Gombe’s most devoted mother, suggested the depth of feeling that Goodall had for the animals. Knowing that Flo’s faithful son Flint was nearby and grieving, Goodall watched over the body all night to keep marauding bush pigs from violating her remains.&lt;/p&gt;
    &lt;p&gt;“People say to me, thank you for giving them characters and personalities,” Goodall once told CBS’s “60 Minutes.” “I said I didn’t give them anything. I merely translated them for people.”&lt;/p&gt;
    &lt;p&gt;Woo is a former Times staff writer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.latimes.com/obituaries/story/2025-10-01/jane-goodall-chimpanzees-dead"/><published>2025-10-01T18:10:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45441219</id><title>Announcing Tinker</title><updated>2025-10-02T00:44:22.146827+00:00</updated><content>&lt;doc fingerprint="1c5d0f35d932c938"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing Tinker&lt;/head&gt;
    &lt;p&gt;Today, we are launching Tinker, a flexible API for fine-tuning language models. It empowers researchers and hackers to experiment with models by giving them control over the algorithms and data while we handle the complexity of distributed training. Tinker advances our mission of enabling more people to do research on cutting-edge models and customize them to their needs.&lt;/p&gt;
    &lt;p&gt;Tinker lets you fine-tune a range of large and small open-weight models, including large mixture-of-experts models such as Qwen-235B-A22B. Switching from a small model to a large one is as simple as changing a single string in your Python code.&lt;/p&gt;
    &lt;p&gt;Tinker is a managed service that runs on our internal clusters and training infrastructure. We handle scheduling, resource allocation, and failure recovery. This allows you to get small or large runs started immediately, without worrying about managing infrastructure. We use LoRA so that we can share the same pool of compute between multiple training runs, lowering costs.&lt;/p&gt;
    &lt;p&gt;Tinker’s API gives you low-level primitives like &lt;code&gt;forward_backward&lt;/code&gt; and &lt;code&gt;sample&lt;/code&gt;, which can be used to express most common post-training methods. Even so, achieving good results requires getting many details right. That’s why we’re releasing an open-source library, the Tinker Cookbook, with modern implementations of post-training methods that run on top of the Tinker API.&lt;/p&gt;
    &lt;p&gt;Groups at Princeton, Stanford, Berkeley, and Redwood Research have already been using Tinker:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Princeton Goedel Team trained mathematical theorem provers&lt;/item&gt;
      &lt;item&gt;The Rotskoff Chemistry group at Stanford fine-tuned a model to complete chemistry reasoning tasks&lt;/item&gt;
      &lt;item&gt;Berkeley’s SkyRL group ran experiments on a custom async off-policy RL training loop with multi-agents and multi-turn tool-use.&lt;/item&gt;
      &lt;item&gt;Redwood Research used Tinker to RL Qwen3-32B on difficult AI control tasks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tinker is now in private beta for researchers and developers. You can sign up for the Tinker waitlist here. We will be onboarding users to the platform starting today.&lt;/p&gt;
    &lt;p&gt;If you’re an organization interested in using Tinker, please contact us here.&lt;/p&gt;
    &lt;p&gt;Tinker will be free to start. We will introduce usage-based pricing in the coming weeks.&lt;/p&gt;
    &lt;p&gt;We’re excited to see what you discover and make with Tinker!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thinkingmachines.ai/blog/announcing-tinker/"/><published>2025-10-01T18:20:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45441222</id><title>Increasing your practice surface area</title><updated>2025-10-02T00:44:21.478782+00:00</updated><content>&lt;doc fingerprint="bf92913dbb237681"&gt;
  &lt;main&gt;
    &lt;p&gt;The difference between being good and being great isn’t talent or formal training, but the invisible practice that happens when you're just living life.&lt;/p&gt;
    &lt;p&gt;Budapest. Sometime around 1978. It's past 1am and all the lights in a high-rise apartment are out, except for one. A Hungarian girl — not yet 10 years old — sits on the cold bathroom floor balancing a chessboard on her knees.&lt;/p&gt;
    &lt;p&gt;Her father opens the door and finds her there, crying, "Sofia! Leave the pieces alone!"&lt;/p&gt;
    &lt;p&gt;The girl looks up at him. "Daddy," she says almost desperately, "they won't leave me alone!"&lt;/p&gt;
    &lt;p&gt;If you aren't familiar with this story, the girl is Sofia Polgar. In the years following the above scene in the bathroom, she'd go on to achieve one of the highest-performing ratings in chess history, playing for Hungary in four Chess Olympiads and winning two team gold medals, one team silver, three individual golds, and one individual bronze.&lt;/p&gt;
    &lt;p&gt;A lot has been written about the training regimen that Sofia went through with her two sisters: 5–6 hours of daily chess practice alongside studies in multiple languages and high-level mathematics in an apartment packed with thousands of chess books and detailed filing systems of their opponents' histories.&lt;/p&gt;
    &lt;p&gt;But not much has been written — how could it be? — about all the hidden reps Sofia got in outside of her official sessions. Like most elite performers, she had dissolved the boundaries of what counts as training and become high in something I call "practice surface area." It means what it sounds like: the total volume of time and space in your life where practice can happen.&lt;/p&gt;
    &lt;p&gt;Let's say you and a friend decide to learn something new together. Guitar, chess, coding, whatever. You both sign up for the same class, practice for the same scheduled hour each day, watch the same YouTube tutorials.&lt;/p&gt;
    &lt;p&gt;Six weeks later, they’re proficient and you’re still stuttering through the basics.&lt;/p&gt;
    &lt;p&gt;We all know the standard explanation: talent. They’ve got it, you don’t. Some people are just wired for certain things. Better to cut your losses and find something that comes naturally to you.&lt;/p&gt;
    &lt;p&gt;Right?&lt;/p&gt;
    &lt;p&gt;Maybe! Usually what people mean when they call someone "talented" or a "natural" is that the person is genetically gifted. And genetics is real. But it's also not a very satisfying explanation because it's so nonspecific.&lt;/p&gt;
    &lt;p&gt;So if I may, I think what's actually taking place in most cases is a difference in practice surface area. You and your friend both officially practiced for the same "3 hours per week," but in reality your friend put in closer to 30. And they weren't even aware they were doing it.&lt;/p&gt;
    &lt;p&gt;They started hearing music differently. Every song on their commute became a lesson in chord progressions. Their fingers unconsciously worked through scales during meetings. They fell asleep running through the next day's session. They dreamed in tablature.&lt;/p&gt;
    &lt;p&gt;You began practicing guitar. They began living guitar.&lt;/p&gt;
    &lt;p&gt;I like studying world-class performers, and I can’t think of a single high-level pro who isn’t also high in practice surface area.&lt;/p&gt;
    &lt;p&gt;Take George Orwell. In his essay Why I Write, he reveals something that should have disqualified him from ever becoming a writer: he had a terrible time actually sitting down to write. The physical act of writing was torture for him. By his own admission, he would avoid it whenever possible.&lt;/p&gt;
    &lt;p&gt;So how did this writing-avoidant person become one of the most famous prose stylists of the 20th century?&lt;/p&gt;
    &lt;p&gt;Here’s the secret he buried in that same essay:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For fifteen years or more, I was carrying out a literary exercise of a quite different kind: this was the making up of a continuous “story” about myself, a sort of diary existing only in the mind… For minutes at a time this kind of thing would be running through my head: ‘He pushed the door open and entered the room. A yellow beam of sunlight, filtering through the muslin curtains, slanted on to the table, where a matchbox, half-open, lay beside the inkpot. With his right hand in his pocket he moved across to the window. Down in the street a tortoiseshell cat was chasing a dead leaf,’ etc. etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;From childhood until age twenty-five, Orwell was practicing descriptive prose every waking moment. He wasn’t "writing," he was just existing lol. But his brain was secretly logging thousands of hours of narrative practice.&lt;/p&gt;
    &lt;p&gt;This pattern shows up everywhere once you know to look for it.&lt;/p&gt;
    &lt;p&gt;Richard Feynman didn’t become a legendary teacher by practicing lectures. He became one by explaining physics to imaginary students while walking around campus. He’d work through problems out loud in empty rooms, turning every moment of solitude into a teaching rehearsal.&lt;/p&gt;
    &lt;p&gt;Bobby Fischer carried a pocket chess set everywhere and would analyze positions using ceiling tiles as boards while lying in bed. Insomnia became chess study. Waiting rooms became tournaments. His opponents thought they were facing someone with supernatural talent. They were actually facing someone who’d turned every idle moment into chess.&lt;/p&gt;
    &lt;p&gt;In fact I've found so many examples of high practice surface area that I created a companion piece to this essay filled with nothing but examples.&lt;/p&gt;
    &lt;p&gt;Here it is: The hidden training habits of 21 world-class performers.&lt;/p&gt;
    &lt;p&gt;It should go without saying that the best way to increase your practice surface area in a given field is to be obsessed with that field. Obsession makes quick work of formal and bounded training sessions, and it doesn't need "tips" on how to do so.&lt;/p&gt;
    &lt;p&gt;So the question then becomes, "How do I increase my pracrtice surface area if I'm not already obsessed?"&lt;/p&gt;
    &lt;p&gt;I've got a few ideas:&lt;/p&gt;
    &lt;p&gt;Identify the smallest possible practice unit that requires no equipment, setup, or specific location.&lt;/p&gt;
    &lt;p&gt;Like Bobby Fischer analyzing chess positions on ceiling tiles while lying in bed, you need a version of practice so minimal it can happen anywhere, requiring zero setup or equipment.&lt;/p&gt;
    &lt;p&gt;Waiting periods and dead time are great opportunities for visualization sessions where you mentally simulate perfect performance.&lt;/p&gt;
    &lt;p&gt;Michael Phelps would run “mental movies” of perfect races in waiting rooms and before sleep.&lt;/p&gt;
    &lt;p&gt;Layer your craft directly onto daily activities.&lt;/p&gt;
    &lt;p&gt;Maya Angelou composed entire poems while mopping floors. She claims to have used the rhythm of physical work as a metronome for her words.&lt;/p&gt;
    &lt;p&gt;Develop automatic mental habits that keep your craft running in the background of consciousness throughout the day.&lt;/p&gt;
    &lt;p&gt;Eminem can’t turn off the part of his brain that rhymes everything. Every conversation, interview, even argument becomes inadvertent freestyle practice as he generates rhyme patterns for everything he hears.&lt;/p&gt;
    &lt;p&gt;Convert physical limitations and situational constraints into practice parameters that force innovation.&lt;/p&gt;
    &lt;p&gt;The UFC fighter Anderson Silva would practice his striking combinations disguised as dancing at Brazilian clubs. He'd throw actual combat sequences to the rhythm while everyone thought he was just getting down.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.indiehackers.com/post/lifestyle/increasing-your-practice-surface-area-agxYGi9bL0gd1WYYQZAu"/><published>2025-10-01T18:20:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45442743</id><title>Evaluating the impact of AI on the labor market: Current state of affairs</title><updated>2025-10-02T00:44:21.294183+00:00</updated><content>&lt;doc fingerprint="7e4591f149bd8ce9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Evaluating the Impact of AI on the Labor Market: Current State of Affairs&lt;/head&gt;
    &lt;head rend="h2"&gt;Key Takeaways&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;While the occupational mix is changing more quickly than it has in the past, it is not a large difference and predates the widespread introduction of AI in the workforce.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Currently, measures of exposure, automation, and augmentation show no sign of being related to changes in employment or unemployment.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Better data is needed to fully understand the impact of AI on the labor market.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We plan on updating this analysis regularly moving forward to see how the impact of AI on the labor market changes over time.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How has AI impacted the labor market? Since generative AI was first introduced nearly three years ago, surveys show widespread public anxiety about AI’s potential for job losses. While it is impossible to accurately predict the future, we can examine how U.S. employment has changed since ChatGPT’s release in November 2022.&lt;/p&gt;
    &lt;p&gt;Our analysis complements other recent studies that provide nascent evidence of possible AI impacts on specific occupations and sub-populations, such as early career workers. We took a broader lens, widening the aperture to the whole labor market, and asked two main questions.&lt;/p&gt;
    &lt;p&gt;First, is the pace of labor market change in this 33-month period of employment disruption different from past periods of early technological change? Second, is there evidence of economy-wide employment effects? To answer these questions, we compare how quickly the occupational mix has changed across a range of measures since ChatGPT’s launch, and compare this to past disruptions from computers and the internet.&lt;/p&gt;
    &lt;p&gt;Overall, our metrics indicate that the broader labor market has not experienced a discernible disruption since ChatGPT’s release 33 months ago, undercutting fears that AI automation is currently eroding the demand for cognitive labor across the economy.1&lt;/p&gt;
    &lt;p&gt;While this finding may contradict the most alarming headlines, it is not surprising given past precedents. Historically, widespread technological disruption in workplaces tends to occur over decades, rather than months or years. Computers didn’t become commonplace in offices until nearly a decade after their release to the public, and it took even longer for them to transform office workflows. Even if new AI technologies will go on to impact the labor market as much, or more, dramatically, it is reasonable to expect that widespread effects will take longer than 33 months to materialize.&lt;/p&gt;
    &lt;p&gt;Of course, our analysis is not predictive of the future. We plan to continue monitoring these trends monthly to assess how AI’s job impacts might change. It is important to remember that the effects of new technologies are evolving and a simple snapshot in time is not enough to explicitly determine what the future holds.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is this Time Different? Changes in the Occupational Mix&lt;/head&gt;
    &lt;p&gt;The current rhetoric about AI mirrors the anxiety over earlier generations of technological progress. So far, is this time different?&lt;/p&gt;
    &lt;p&gt;First, we look at how quickly the overall occupational mix changed in the first 33 months since ChatGPT’s relative to previous periods of technological change. Figure 1 compares the occupational mix by month to the mix in a baseline period at the start of a major technological development (e.g. January 1996 is the baseline month for the growing adoption of the internet). The job mix for AI appears to be changing faster than it has in the past, although not markedly so. (See the appendix for a discussion of pre-trends.)&lt;/p&gt;
    &lt;p&gt;The occupational mix refers to the distribution of workers amongst all the jobs in the economy. In this context, a percentage point difference means that, relative to the start point, that percent of workers are in new occupations. This can occur by workers changing jobs, losing jobs, or unemployed people getting a new job. As such, this metric attempts to capture how different the sum of occupations that make up the labor force is relative to another point in time. By measuring this over the time generative AI has been publicly available, we can test the claim that AI is substantially changing the workforce by any of the methods mentioned above (pushing workers from one job to another, automating workers out of a job, or creating new jobs). Note that a change over this time period simply reflects change — it does not take a stance on the cause of that change. Note that if recent grads are not getting hired, that would show up in this measure in as much as recent grads (as discussed below) are often in different occupations than older workers.&lt;/p&gt;
    &lt;p&gt;With the development of the internet and the growing prevalence of computers, the turn of the 21st century gave rise to concerns over an imminent computerization of many jobs. Despite this automation anxiety, the occupational mix by 2002 was at most around 7 percentage points different than it was in 1996; i.e., only 7 percent of workers in 2002 would need to switch occupations to match the composition of the 1996 labor market.&lt;/p&gt;
    &lt;p&gt;Changes in the occupational mix since the advent of generative AI in 2022 seem to mirror the trends seen during the three comparison periods. The recent changes appear to be on a path only about 1 percentage point higher than it was at the turn of the 21st century with the adoption of the internet. Although recent trends seemingly outpace historical shifts in the occupational mix, the potential effects of AI on the labor market so far are not out of the ordinary. In fact, taking a closer look at recent years, the data suggests that this recent trend is not necessarily attributable to AI (Figure 2). Shifts in the occupational mix were well on their way during 2021, before the release of generative AI, and more recent changes do not seem any more pronounced, even as the use of AI continues to grow in popularity.&lt;/p&gt;
    &lt;p&gt;Repeating this analysis by industry similarly suggests a limited effect of AI. Figure 3 reports the change in the occupational mix from November 2022 within different industries. The Information, Financial Activities, and Professional and Business Services sectors have all seen larger shifts in the job mix compared to the shifts in the aggregate labor market, with the largest changes in the Information sector. (As a reminder the information sector includes things like newspapers, movies, and data processing.) These industries are among those with the highest exposure to generative AI. Although at first glance these changes may seem attributable to generative AI, the data again suggests that the trends within these industries started before the release of ChatGPT (Figures 4-6). In fact, over a broader time horizon, the large shifts in the Information Industry seem to be a feature of the industry itself rather than a consequence of any one technological development (Figure 7).&lt;/p&gt;
    &lt;p&gt;Looking over an even longer horizon, labor market volatility appears rather low. As a chart Jed Kolko shows, the change in the occupational mix seen in Figure 1 is sluggish compared to the change seen in the 40s and 50s (which reflected mass labor market changes due to world events). Kolko cautioned that “we simply don’t know for sure whether automation, algorithms, and AI will ultimately create more jobs than they destroy.”&lt;/p&gt;
    &lt;p&gt;The dissimilarity data we have examined indicates that there is no substantial acceleration in the rate of change in the composition of the labor market since the introduction of ChatGPT. Lacking that, there is nothing meaningful we can either attribute or misattribute to AI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Recent College Graduates&lt;/head&gt;
    &lt;p&gt;Figure 9 compares the occupational mix for recent college graduates (ages 20-24) to that of their older counterparts (ages 25-34).2 If generative AI were in fact substantially changing the labor market for recent college graduates, we would expect to see a growing dissimilarity in the occupational mix between these two groups. The dissimilarity has increased slightly faster in recent months than it did in a previous time period, which could be consistent with a recent paper from Brynjolfsson et al. showing a possible impact of AI on employment of early career workers. It could also simply reflect a slowing labor market. However, our results should be interpreted with caution particularly given small sample sizes.&lt;/p&gt;
    &lt;p&gt;Taking a closer look at the trend since January 2021, the dissimilarity between older and more recent college graduates rarely deviates outside of the 30-33% range (Figure 10). This implies that these trends of growing dissimilarity may pre-date ChatGPT’s release and may not be attributable to AI. However, there is perhaps some slight upward momentum more recently, though this is consistent with both Brynjolfsson et al.’s work and the CPS’s noisiness (and a slowing labor market hitting younger workers). Further, the same caution in interpretation given the small sample sizes holds in this figure as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Insights from AI "Exposure" and Usage&lt;/head&gt;
    &lt;p&gt;To better understand whether AI is impacting the labor market, we would want to analyze whether the share of workers in occupations that are most impacted by AI usage is changing over time. If AI were automating jobs at scale, we would expect to see a smaller share of workers in some of the jobs that are most negatively impacted.&lt;/p&gt;
    &lt;p&gt;Unfortunately, comprehensive usage data is not publicly available. The best available data we have is from OpenAI and Anthropic, respectively, that detail the occupations that are most “exposed” to genAI tools (a theoretical, forward-looking metric across all jobs) and that have the highest actual usage of one specific AI tool, Claude (a more narrow, present-focused metric). While imperfect, these data are our best approximation of AI job “risk”. (See discussion of limitations in the next section and in the appendix.)&lt;/p&gt;
    &lt;p&gt;Importantly, OpenAI and Anthropic are measuring different things and we look at them separately.&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenAI's "Exposure" Data&lt;/head&gt;
    &lt;p&gt;We use data from OpenAI that shows a measure of “exposure” to ChatGPT technology. This refers, generally, to whether utilizing ChatGPT4 technology can help reduce the time it takes to complete the occupation’s tasks by at least 50%. (The data appendix describes this metric in more detail.)&lt;/p&gt;
    &lt;p&gt;We utilize the “Beta” exposure metric, which also accounts for if a model with additional software built on top of it can help reduce task completion time, though this capability is weighted half that of “direct” exposure. An exposure score is created on a scale from 0 to 1 based on a percent of an occupation’s tasks that are “exposed” to genAI. The data appendix describes this metric in more detail.&lt;/p&gt;
    &lt;p&gt;For our purposes, we categorize occupations into three groups using their exposure score quintile; an occupation has the lowest degree of exposure if it falls in the first two quintiles, a medium degree if in the 3rd and 4th quintiles, and the highest degree if in the top quintile. In other words, these metrics look at relative not absolute exposure. We provide a further discussion of this in the appendix.&lt;/p&gt;
    &lt;p&gt;We ask: has the share of workers in occupational exposure quintiles changed since ChatGPT’s launch? Our analysis shows that it has not (Figure 11). The share of workers in the lowest, middle, and highest occupational exposure groups stay stable at around 29%, 46% and 18%, respectively.&lt;/p&gt;
    &lt;p&gt;Even when specifically examining the unemployed population, there is no clear growth in exposure to generative AI. Figure 12 depicts the average percentage of tasks exposed amongst unemployed workers by duration of unemployment. AI-driven displacement might suggest a growth in the proportion of exposed tasks amongst recently unemployed workers. Irrespective of the duration of unemployment, however, unemployed workers were in occupations where about 25 to 35 percent of tasks, on average, could be performed by generative AI. Although there is some variation between months, the data demonstrate no clear upward trend and no clear difference by the duration of unemployment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Anthropic's Usage Measure&lt;/head&gt;
    &lt;p&gt;Given Anthropic’s usage data’s novelty and uniqueness, there is no established standard for how to aggregate it. The usage Anthropic observes does not contain every single task in the O*NET task database, so the question of how to handle the missing tasks remains ambiguous. We proceed in two ways:&lt;/p&gt;
    &lt;p&gt;First, we ignore tasks that are not included in the Anthropic data and aggregate following the method detailed in the appendix. This method makes no assumption about how workers are using or could utilize Claude to perform the task, or how workers in occupations with tasks like those observed would use Claude. However, this method significantly limits the number of tasks and therefore occupations we can include. Further, an occupation may appear to have very high usage while in fact only a single of that occupation’s tasks were observed in the data.&lt;/p&gt;
    &lt;p&gt;Alternatively, we include all of the missing tasks and assume their usage is zero. There is some truth to this, as those tasks were not observed in Claude during the period in which the data was collected. However, it makes strong assumptions about those tasks’ potential usage. Two comparable tasks, where one appears in the data and one does not, would have totally different usage values. Given how the observed Claude data is a representation of the users who happened to utilize the model over a sample period, the following period could have had that similar task included.&lt;/p&gt;
    &lt;p&gt;In pursuit of a balanced and well-informed exploration of this data, we include results from both methods below.&lt;/p&gt;
    &lt;p&gt;Anthropic’s data on AI usage shows similar trends of stability over time, rather than disruption. The proportion of employment in occupations with high levels of task AI usage, whether automation or augmentation (as defined as more than half of AI usage), is stable at around 70% or 11%, respectively (Figure 15). When assuming that unobserved tasks indicate zero usage, however, these proportions drop to 3% and 0%, respectively. Repeating a similar analysis as above, Figures 16 and 18 report the occupation-level share of tasks that are automation or augmentation, respectively, amongst unemployed workers by duration of unemployment. Note: for this analysis, we use Anthropic’s most recent data on AI usage, which was released in mid-September. We discuss Anthropic’s data vintages more fully in the Appendix.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data Limitations — and Why Better Data is Needed&lt;/head&gt;
    &lt;p&gt;As previously noted, the metrics from OpenAI and Anthropic are imperfect proxies for AI risk and usage, while still being the best available.&lt;/p&gt;
    &lt;p&gt;A key limitation of OpenAI’s “exposure” data is that it is not based on actual usage, and should therefore be interpreted as a theoretical estimate of the jobs and sectors that could, in theory, be impacted. In reality, actual AI usage and workplace diffusion has varied dramatically between sectors and occupations with similar levels of “exposure.” For instance, generative AI tools were adopted extremely quickly and at mass scale among coders and software developers, who are in the top quintile of exposure. Meanwhile, adoption has lagged considerably in clerical sectors, despite a similar level of exposure. Thus analyzing occupations by exposure alone likely under-estimates potential labor market disruption, as the top quintiles of exposure will include occupations that are theoretically exposed but not actively using AI at a meaningful scale, and thus unlikely to see AI impacts.&lt;/p&gt;
    &lt;p&gt;A comparison of the OpenAI “exposure” data with the Anthropic usage data makes this limitation clear. The two measures appear to have only a limited correlation with one another (Figure 20).&lt;/p&gt;
    &lt;p&gt;Figure 20 consists of occupations that have data for both the OpenAI and Anthropic measures, which amounts to about 80% of CPS occupations in our sample. Figure 21 splits the data from figure 20 into quadrants along the axes of low-high exposure and low-high usage and then groups the data into its SOC job categories. Particularly striking is the greater range of different job categories in the quadrants with low usage (the left side of figure 20) and conversely the concentration in just a handful of categories in the high usage quadrants. Across both high and low exposure, high usage occupations are dominated by scientific and quantitative professions and general business occupations. The occupations clustered in the low usage/exposure (bottom left of figure 20) tend to be production occupations with little computerization.&lt;/p&gt;
    &lt;p&gt;Just as the OpenAI metric has limitations, so too does Anthropic’s usage data. Figure 22 shows the occupational shares of all “conversations” with Claude (the AI chatbot), and illustrates the occupation groups that are over- and under- represented in this usage, compared to their exposure ranking and employment share.&lt;/p&gt;
    &lt;p&gt;It is clear from the data that Claude’s usage is heavily dominated by one occupational group — computer and mathematical, which includes coders — and that arts and media (including writers) is also considerably overrepresented. While certainly coding is among the most prominent use cases of AI, it is likely that Claude’s userbase skews more heavily to these tasks due to Claude’s stand-out reputation among LLMs as being particularly good at writing and coding. New data recently published by OpenAI shows a broad pattern of usage among ChatGPT customers across a range of industries, including not only information services (including software development) but also professional services and even manufacturing. It is entirely possible, and even likely, that usage data from other AI models like Google’s Gemini or Microsoft’s Copilot would show different and more varied patterns of usage. Thus data from Claude usage alone is not representative of how workers across the economy are using AI chatbots and tools.&lt;/p&gt;
    &lt;p&gt;To accurately measure AI’s impact on the labor force, the most important data needed is comprehensive usage data from all the leading AI companies at the individual and enterprise level, including APIs. Anthropic has led the way in transparently sharing Claude usage data, including a new release of enterprise data. To further our understanding of AI’s impact, it is important that all leading AI labs do the same in a similarly transparent and privacy-protected way.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;While anxiety over the effects of AI on today’s labor market is widespread, our data suggests it remains largely speculative. The picture of AI’s impact on the labor market that emerges from our data is one that largely reflects stability, not major disruption at an economy-wide level. While generative AI looks likely to join the ranks of transformative, general purpose technologies, it is too soon to tell how disruptive the technology will be to jobs. The lack of widespread impacts at this early stage is not unlike the pace of change with previous periods of technological disruption. Preregistering areas where we would expect to see the impact and continuing to monitor monthly impacts will help us distinguish rumor from fact.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;p&gt;Code used to create the data can be found here.&lt;/p&gt;
    &lt;p&gt;We adapt Duncan and Duncan’s methodology to construct a dissimilarity index of the change in the occupational mix over time using monthly CPS data. Going month by month, we measure each occupation’s constituent percentage of the workforce and compare it to the starting month. To address noise in the data, we take a 12 month moving average for each month. We then sum up the absolute differences in percentage of workforce across all occupations to get our dissimilarity index for that given month. This captures both the advent of new occupations and the expansion or contraction of existing ones.&lt;/p&gt;
    &lt;p&gt;To examine generative AI’s impact, we begin in November 2022 and continue into the latest monthly CPS release in July, as this lines up with AI’s public introduction and the beginning of its adoption. We compare AI’s dissimilarity index to three other time periods:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;1984-1989: Capturing the popularization of PCs and the start of the computer revolution.&lt;/item&gt;
      &lt;item&gt;1996-2002: Capturing mass adoption of the internet in public life and the workplace.&lt;/item&gt;
      &lt;item&gt;2016-2019: A control period following the 2008 Recession recovery during which there was little change to the occupational mix.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Figure A1 adds pre-trends in the occupational mix for the prior 12 months to the data from Figure 1. In all but one period, the pre-trends show a similar degree of dissimilarity as compared to the trend observed over the first year. This lack of a substantial difference suggests the advent of new technologies has minimal immediate effects; i.e., shifts in the labor market take time to develop. Note that the large pre-trends in the “Computers” period are likely due to issues with the quality of data prior to 1981.&lt;/p&gt;
    &lt;p&gt;Using OpenAI’s occupational exposure data, we classify occupations as mildly, moderately, or highly exposed to AI provided that generative AI can reduce the time to complete at least one task (or a greater portion of tasks given the exposure level) by 50%. The raw occupational exposure data allocates tasks into one of three categories:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;No exposure: Generative AI cannot reduce the time to complete the task or lowers the quality of the output.&lt;/item&gt;
      &lt;item&gt;Direct exposure: Generative AI can reduce the time to complete a task by at least 50%.&lt;/item&gt;
      &lt;item&gt;LLM+ Exposed: Generative AI alone cannot reduce task completion time, but a piece of software built on top of a model could.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We utilize the GPT4 categorized “Beta” exposure, which gives a task a score of 1 if it is directly exposed, or a score of .5 if it is LLM+ Exposed. An occupation’s exposure is the weighted average of the exposure of each individual task.&lt;/p&gt;
    &lt;p&gt;Following the methods in OpenAI’s paper presenting these results, when aggregating exposure to the occupational level, a job’s “core” tasks have a weight of 1 while their “supplemental” tasks have a weight of .5. After exposure and usage are aggregated to the SOC code level, they are weighted by that occupation’s OES labor count. This weight is used when aggregating from SOC code to CPS code.&lt;/p&gt;
    &lt;p&gt;Given the exposure data was created a few years and thus does not include the more advanced capabilities of today’s large language models, a relative comparison of exposure appears more appropriate. Rather than measuring the now aged direct exposure, we broke GPT4 Beta exposure (after our aggregation and weighting) into quintiles and bucketed them as described in the text above. The average exposure, rounded to the second digit, for each quintile is as follows:&lt;/p&gt;
    &lt;p&gt;Figure A2 replicates the prior analysis in Figure 12 with occupation-level exposure groups instead defined using the absolute measure of exposure. The lowest and medium exposure groups (scores less than 0.4 and between 0.4 and 0.8, respectively) stably comprise around 45% of workers each, while the highest exposure group represents only around 2% of workers (scores greater than 0.8). Like in Figure 12, there is no observable trend over time.&lt;/p&gt;
    &lt;p&gt;Anthropic’s usage metrics are aggregated similarly to the OpenAI exposure data. We follow the method their researchers use here. We aggregate the individual kinds of usage into their respective categories for each task:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Augmentation = Validation + Task Iteration + Learning&lt;/item&gt;
      &lt;item&gt;Automation = Directive + Feedback Loop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We then weight the tasks using the same core/supplemental weighting method described above. Following that, the aggregation to CPS code level is identical to the OpenAI aggregation. &lt;lb/&gt;Throughout this piece we use Anthropic’s AI usage data from their most recent release in August. This data, however, only provides a static snapshot of AI usage. As more data is made available in future releases, the Budget Lab will continue to update this analysis to provide a more comprehensive picture of how changes in AI usage may be affecting the labor market. Figure A3 shows the trend in the automation/augmentation of tasks using data from a prior March release until August 2025 (note that to ensure consistency between the two releases, the August usage data excludes enterprise usage).&lt;/p&gt;
    &lt;head rend="h3"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Much of the press focuses on early career and white collar workers being in lower demand.&lt;/item&gt;
      &lt;item&gt;In any given year, recent and older college graduates account for around 1,100 and 5,600 observations (3% and 12% of the overall sample), respectively.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://budgetlab.yale.edu/research/evaluating-impact-ai-labor-market-current-state-affairs"/><published>2025-10-01T20:07:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45443200</id><title>Implementing /Usr Merge in Alpine</title><updated>2025-10-02T00:44:20.632525+00:00</updated><content>&lt;doc fingerprint="bf4d78560f34a7c0"&gt;
  &lt;main&gt;
    &lt;p&gt;The Alpine Linux Technical Steering Committee (TSC) has decided to change the base filesystem hierarchy. In the future, &lt;code&gt;/lib&lt;/code&gt;, &lt;code&gt;/bin&lt;/code&gt;, and &lt;code&gt;/sbin&lt;/code&gt; will be
symbolic links to their &lt;code&gt;/usr&lt;/code&gt; counterparts, and every package shall be
installed under the &lt;code&gt;/usr&lt;/code&gt; paths. For now, &lt;code&gt;/usr/bin&lt;/code&gt; and &lt;code&gt;/usr/sbin&lt;/code&gt; will
continue to be independent paths, but that might change if the Filesystem
Hierarchy Standard (FHS) gets updated.&lt;/p&gt;
    &lt;head rend="h3"&gt;Timeline&lt;/head&gt;
    &lt;p&gt;The transition to the /usr merge will be organised around 3 milestones:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Finish preparations: the preparations that have been in the works for months will be finished, and the Merge Request that finalizes the initial work will be merged. Any new edge installations will be /usr-merged from this point onwards.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Release of Alpine Linux 3.23: any new alpine installations of this and future releases will be /usr-merged. Users will be able to upgrade from older releases to 3.23 without being forced to /usr merge their systems, or the /usr-merge causing breakage. From this point onwards, users are encouraged to migrate existing installs.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;End-of-support for 3.22: after 3.22 is EOL non-/usr-merged systems will be considered officially unsupported. The following stable release (possibly 3.26 or 3.27) will make the /usr-merge compulsory and non-merged installations upgrading to it will break.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;How to migrate?&lt;/head&gt;
    &lt;p&gt;If you are using diskless installations and relying on &lt;code&gt;apkovl&lt;/code&gt; files to persist
configuration across reboots, you do not need to do anything. The versions of
&lt;code&gt;alpine-conf&lt;/code&gt; and &lt;code&gt;alpine-mkinitfs&lt;/code&gt; available in the 3.23 release correctly
unpack the &lt;code&gt;apkovl&lt;/code&gt; files on a system with the links in place.&lt;/p&gt;
    &lt;p&gt;If you have a long-living installation of Alpine Linux edge, or upgraded to 3.23 from an older release, you are encouraged test and migrate your system to be /usr-merged. The /usr-merged system will be better supported in the future, and will be safer to run. To migrate:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make sure you have a complete up-to-date system: &lt;code&gt;doas apk upgrade -aU&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Install the transitioning script: &lt;code&gt;doas apk add merge-usr&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Do a “dryrun” of the merging. If any errors are printed out, please open an issue in https://gitlab.alpinelinux.org/alpine/aports/–/issues/new : &lt;code&gt;doas merge-usr --dryrun&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;WARNING: Only run this step if the previous step succeeded. Do the merge: &lt;code&gt;doas merge-usr&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Cleanup the transitioning script, not needed anymore: &lt;code&gt;doas apk del merge-usr&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Does this affect me?&lt;/head&gt;
    &lt;p&gt;As a general user, this change should not affect you in any way. The symbolic links warrant backwards compatibility, and &lt;code&gt;PATH&lt;/code&gt; will not change, so
everything will continue working as it used to. There might be, however, some
small group of users affected by this change:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;People making heavy use of&lt;/p&gt;&lt;code&gt;apk audit --full&lt;/code&gt;functionality and the&lt;code&gt;protected_paths&lt;/code&gt;feature. Once the symlinks are in place, things that during the transitional period install under&lt;code&gt;/lib&lt;/code&gt;,&lt;code&gt;/bin&lt;/code&gt;, or&lt;code&gt;/sbin&lt;/code&gt;will produce false-positives due to&lt;code&gt;apk&lt;/code&gt;finding them in&lt;code&gt;/usr&lt;/code&gt;. For this reason, any usage of&lt;code&gt;/lib&lt;/code&gt;,&lt;code&gt;/bin&lt;/code&gt;, or&lt;code&gt;/sbin&lt;/code&gt;withing&lt;code&gt;protected_paths&lt;/code&gt;should be moved to the&lt;code&gt;/usr&lt;/code&gt;counterparts.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Users which have manually installed anything under&lt;/p&gt;&lt;code&gt;/lib&lt;/code&gt;,&lt;code&gt;/bin&lt;/code&gt;, or&lt;code&gt;/sbin&lt;/code&gt;might have trouble with the new update. Those paths were never meant to have binaries managed by the system administrator (FHS defines&lt;code&gt;/usr/local&lt;/code&gt;for that purpose), so it could be considered an unsupported configuration. The migration should move those files automatically to their&lt;code&gt;/usr&lt;/code&gt;counterparts, but it might not be throughly tested.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Users with&lt;/p&gt;&lt;code&gt;/&lt;/code&gt;and&lt;code&gt;/usr&lt;/code&gt;in different partitions. Once the /usr merge is implemented,&lt;code&gt;/&lt;/code&gt;may not contain the executables necessary to mount&lt;code&gt;/usr&lt;/code&gt;. Since version 3.21, Alpine’s mkinitfs mounts&lt;code&gt;/usr&lt;/code&gt;directly from the initramfs. Make sure you modify your configuration so that the modules required to mount&lt;code&gt;/usr&lt;/code&gt;are present in the initramfs.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Users with their own package repositories, that might still use&lt;/p&gt;&lt;code&gt;/lib&lt;/code&gt;,&lt;code&gt;/bin&lt;/code&gt;or&lt;code&gt;/sbin&lt;/code&gt;, and relative symlinks. Systems with such packages installed might break after the /usr merge is implemented. Please make sure that any 3rd party packages you have installed (if you have any) don’t install files to&lt;code&gt;/lib&lt;/code&gt;,&lt;code&gt;/bin&lt;/code&gt;or&lt;code&gt;/sbin&lt;/code&gt;.&lt;code&gt;abuild&lt;/code&gt;will already warn if doing otherwise, and will error-out in the future.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Why has this been done?&lt;/head&gt;
    &lt;p&gt;This has been done for 2 different sets of reasons:&lt;/p&gt;
    &lt;head rend="h4"&gt;Reasons&lt;/head&gt;
    &lt;p&gt;There are some potential benefits for Alpine from having installations /usr-merged:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reduce packaging maintenance: busybox symlinks and the applications they override do not always agree on the installation location. Moreover, those installation locations might change over time. Through this effort, the postmarketOS team has indeed fixed dozens of occurrences where installations would have two binaries for the same package: one by busybox in &lt;code&gt;/bin&lt;/code&gt;or&lt;code&gt;/sbin&lt;/code&gt;, and one by another package under&lt;code&gt;/usr&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The /usr-merged setup is the standard in most of the Linux ecosystem. Many maintainers might not test or support non-/usr-merge filesystems, resulting in additional maintenance for the Alpine Linux team.&lt;/item&gt;
      &lt;item&gt;Containerize more distribution-provided data under &lt;code&gt;/usr&lt;/code&gt;: making it easier to split the user-owned data from&lt;code&gt;/&lt;/code&gt;to the distribution-provided data in&lt;code&gt;/usr&lt;/code&gt;. Although not perfect, this makes it easier to do things like rebuilding&lt;code&gt;/usr&lt;/code&gt;, backup the system configuration, and potentially mount&lt;code&gt;/usr&lt;/code&gt;read-only.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;FAQ&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Q: The &lt;code&gt;/bin&lt;/code&gt;and&lt;code&gt;/lib&lt;/code&gt;directories were a helpful recovery environment.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;A: The historical reasons for the split&lt;/p&gt;&lt;code&gt;/usr&lt;/code&gt;were to have a recovery environment if the second disk (containing the&lt;code&gt;/usr&lt;/code&gt;directory) failed to boot. However, Linux distributions, including Alpine Linux, are using an initramfs which already boots up a minimal environment and with which the rest of the system will be booted.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Q: Does the /usr merge not make the file hierachy and packaging more complicated?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;A: No, exactly the opposite. As a result of the /usr merge there is only a single location for the distribution to install executables and libraries.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alpinelinux.org/posts/2025-10-01-usr-merge.html"/><published>2025-10-01T20:40:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45443298</id><title>The Company Man</title><updated>2025-10-02T00:44:20.559386+00:00</updated><content/><link href="https://www.lesswrong.com/posts/JH6tJhYpnoCfFqAct/the-company-man"/><published>2025-10-01T20:47:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45443304</id><title>Microsoft declares bring your Copilot to work day, usurping IT authority</title><updated>2025-10-02T00:44:20.263868+00:00</updated><content>&lt;doc fingerprint="3605bb1ae2dfe8d7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft declares bring your Copilot to work day, usurping IT authority&lt;/head&gt;
    &lt;head rend="h2"&gt;Use your home subscription with your work Microsoft 365 account&lt;/head&gt;
    &lt;p&gt;Your job may not support BYOD, but how about BYOC? Microsoft has declared that people can bring their personal Microsoft 365 subscriptions to work to access various Copilot features at companies that fail to provide an AI fix.&lt;/p&gt;
    &lt;p&gt;Redmond has done so unilaterally, effectively endorsing "shadow IT" – the practice of bringing unapproved software and devices into the workplace.&lt;/p&gt;
    &lt;p&gt;Earlier this year, Microsoft said it had adopted a new approach to shadow IT. "While earlier eras of our IT history focused on trying to prevent shadow IT, we are now concentrating on managing it," the biz said in a blog post. By "managing," Microsoft also means "enabling."&lt;/p&gt;
    &lt;p&gt;Samer Baroudi, senior product marketing manager at Microsoft, insists this is for your own good.&lt;/p&gt;
    &lt;p&gt;"This offers a safer alternative to other bring-your-own-AI scenarios, and empowers users with Copilot in their daily jobs while keeping IT firmly in control and all enterprise data protections intact," Baroudi explained in a blog post.&lt;/p&gt;
    &lt;p&gt;Makers of competing AI products might disagree.&lt;/p&gt;
    &lt;p&gt;Microsoft says that employees can sign into Microsoft 365 apps using both personal and work accounts and now can use Copilot features from their personal plan (Personal, Family, or Premium) for business documents – even if their work account lacks a Copilot license.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raspberry Pi prices hiked as AI gobbles all the memory&lt;/item&gt;
      &lt;item&gt;AI has had zero effect on jobs so far, says Yale study&lt;/item&gt;
      &lt;item&gt;Air Force admits SharePoint privacy issue as reports trickle out of possible breach&lt;/item&gt;
      &lt;item&gt;Hundreds of orgs urge Microsoft: don't kill off free Windows 10 updates&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IT admins miffed at having their authority usurped by a diktat from Redmond can console themselves with the knowledge that Copilot's level of access "is strictly governed by the user’s work account permissions, ensuring enterprise data remains protected." The user's Entra (work) identity governs file permissions and access controls.&lt;/p&gt;
    &lt;p&gt;Also, "IT retains full control and oversight" – apart from the bit about allowing this to happen in the first place.&lt;/p&gt;
    &lt;p&gt;Admins have the ability to disallow personal Copilot usage on work documents using cloud policy controls. And they can audit personal Copilot interactions and can apply enterprise identity, permission, and compliance policies.&lt;/p&gt;
    &lt;p&gt;Government tenants (GCC/DoD) for some reason don't support this capability, the one that Baroudi insists "does not create new data exposure risks."&lt;/p&gt;
    &lt;p&gt;Meanwhile, employees who decide to fire up their personal Copilot accounts within the workplace should be mindful that their prompts and responses will be captured by their employer.&lt;/p&gt;
    &lt;p&gt;As to why Microsoft would bother, Baroudi provides a hint in the FAQs detailing the bring-your-own-Copilot-to-work initiative that accompanies his post.&lt;/p&gt;
    &lt;quote&gt;Can use of Copilot from personal Microsoft 365 subscriptions help drive AI adoption?&lt;lb/&gt;Yes. It allows users to experience AI productivity benefits while IT retains control.&lt;/quote&gt;
    &lt;p&gt;Of course, when Microsoft next cites enterprise adoption statistics for its AI products, it will be worth asking whether the company is counting personal usage of Copilot. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2025/10/01/microsoft_consumer_copilot_corporate/"/><published>2025-10-01T20:48:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45443368</id><title>DARPA project for automated translation from C to Rust (2024)</title><updated>2025-10-02T00:44:19.020098+00:00</updated><content>&lt;doc fingerprint="eb7bf9db982fa7dc"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;DARPA initiates a new program to automate the translation of the worldâs highly vulnerable legacy C code to the inherently safer Rust programming language&lt;/head&gt;
    &lt;head rend="h4"&gt;Jul 31, 2024&lt;/head&gt;
    &lt;p&gt;Memory safety vulnerabilities are the most prevalent type of disclosed software vulnerability1 and affect a computer's memory in two primary ways. First, programming languages like C allow programmers to manipulate memory directly, making it easy to accidentally introduce errors in their program that would enable a seemingly routine operation to corrupt the state of memory. Second, memory safety issues can arise when a programming language exhibits an âundefined behavior.â Undefined behaviors happen when the programming language standard provides no specification or guidance on how the program should behave under conditions not explicitly defined in the standard.&lt;/p&gt;
    &lt;p&gt;After more than two decades of grappling with memory safety issues in C and C++, the software engineering community has reached a consensus. Relying on bug-finding tools is not enough. Even the Office of the National Cyber Director has called for more proactive approaches to eliminate memory safety vulnerabilities to reduce potential attacks2.&lt;/p&gt;
    &lt;p&gt;While it's been no secret that memory safe programming languages can eliminate memory safety vulnerabilities, the challenge has been rewriting legacy code at scale that matches the vastness of the problem. The C language was created in the 1970s and has become ubiquitous. It has been used to develop applications that run everything from modern smartphones to space vehicles and beyond. And the Department of Defense has long-lived systems that disproportionately depend on programming languages like C.&lt;/p&gt;
    &lt;p&gt;However, in recent years, a cultural shift toward the programming language Rust and recent breakthroughs in machine learning techniques, like large language models (LLMs), have created an environment that may lend itself to a new class of solutions.&lt;/p&gt;
    &lt;p&gt;DARPAâs Translating All C to Rust (TRACTOR) program wants to seize this opportunity by substantially automating the translation of the worldâs legacy C code to Rust.&lt;/p&gt;
    &lt;p&gt;âYou can go to any of the LLM websites, start chatting with one of the AI chatbots, and all you need to say is âhere's some C code, please translate it to safe idiomatic Rust code,â cut, paste, and something comes out, and it's often very good, but not always,â said Dr. Dan Wallach, DARPA program manager for TRACTOR. âThe research challenge is to dramatically improve the automated translation from C to Rust, particularly for program constructs with the most relevance."&lt;/p&gt;
    &lt;p&gt;TRACTOR will strive to create the same quality and style that a skilled Rust developer would produce, thereby eliminating the entire class of memory safety security vulnerabilities in C programs.&lt;/p&gt;
    &lt;p&gt;Wallach anticipates proposals that include novel combinations of software analysis, such as static and dynamic analysis, and large language models. The program will host public competitions throughout the effort to test the capabilities of the LLM-powered solutions.&lt;/p&gt;
    &lt;p&gt;"Rust forces the programmer to get things right,â said Wallach. âIt can feel constraining to deal with all the rules it forces, but when you acclimate to them, the rules give you freedom. They're like guardrails; once you realize they're there to protect you, you'll become free to focus on more important things."&lt;/p&gt;
    &lt;p&gt;DARPA will sponsor a Proposers Day on Aug. 26, 2024, which attendees can attend in person or virtually. Participants must register by Aug. 19, 2024. Details and registration info are available at SAM.Gov.&lt;/p&gt;
    &lt;p&gt;[1]https://www.cisa.gov/sites/default/files/2023-12/The-Case-for-Memory-Safe-Roadmaps-508c.pdf&lt;/p&gt;
    &lt;p&gt;[2]https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/memory-safety-fact-sheet/&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.darpa.mil/news/2024/memory-safety-vulnerabilities"/><published>2025-10-01T20:53:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45443462</id><title>Edge264 – Minimalist, high-performance software decoder for H.264/AVC video</title><updated>2025-10-02T00:44:18.281237+00:00</updated><content>&lt;doc fingerprint="2658de598d253911"&gt;
  &lt;main&gt;
    &lt;p&gt;Minimalist software decoder with state-of-the-art performance for the H.264/AVC video format.&lt;/p&gt;
    &lt;p&gt;Please note this is a work in progress and will be ready for use after making GStreamer/VLC plugins.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Supports Progressive High and MVC 3D profiles, up to level 6.2&lt;/item&gt;
      &lt;item&gt;Any resolution up to 8K UHD&lt;/item&gt;
      &lt;item&gt;8-bit 4:2:0 planar YUV output&lt;/item&gt;
      &lt;item&gt;Slices and Arbitrary Slice Order&lt;/item&gt;
      &lt;item&gt;Slice and frame multi-threading&lt;/item&gt;
      &lt;item&gt;Per-slice reference picture list&lt;/item&gt;
      &lt;item&gt;Memory Management Control Operations&lt;/item&gt;
      &lt;item&gt;Long-term reference frames&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows: x86, x64&lt;/item&gt;
      &lt;item&gt;Linux: x86, x64, ARM64&lt;/item&gt;
      &lt;item&gt;Mac OS: x64&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;edge264 is entirely developed in C using 128-bit vector extensions and vector intrinsics, and can be compiled with GNU GCC or LLVM Clang. SDL2 runtime library may be used (optional) to enable display with &lt;code&gt;edge264_test&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here are the &lt;code&gt;make&lt;/code&gt; options for tuning the compiled library file:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CC&lt;/code&gt;- C compiler used to convert source files to object files (default&lt;code&gt;cc&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CFLAGS&lt;/code&gt;- additional compilation flags passed to&lt;code&gt;CC&lt;/code&gt;and&lt;code&gt;TARGETCC&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TARGETCC&lt;/code&gt;- C compiler used to link object files into library file (default&lt;code&gt;CC&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;LDFLAGS&lt;/code&gt;- additional compilation flags passed to&lt;code&gt;TARGETCC&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TARGETOS&lt;/code&gt;- resulting file naming convention among&lt;code&gt;Windows&lt;/code&gt;|&lt;code&gt;Linux&lt;/code&gt;|&lt;code&gt;Darwin&lt;/code&gt;(default host)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;VARIANTS&lt;/code&gt;- comma-separated list of additional variants included in the library and selected at runtime (default&lt;code&gt;logs&lt;/code&gt;)&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;x86-64-v2&lt;/code&gt;- variant compiled for x86-64 microarchitecture level 2 (SSSE3, SSE4.1 and POPCOUNT)&lt;/item&gt;&lt;item&gt;&lt;code&gt;x86-64-v3&lt;/code&gt;- variant compiled for x86-64 microarchitecture level 3 (AVX2, BMI, LZCNT, MOVBE)&lt;/item&gt;&lt;item&gt;&lt;code&gt;logs&lt;/code&gt;- variant compiled with logging support in YAML format (headers and slices)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;BUILD_TEST&lt;/code&gt;- toggles compilation of&lt;code&gt;edge264_test&lt;/code&gt;(default&lt;code&gt;yes&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;FORCEINTRIN&lt;/code&gt;- enforce the use of intrinsics among&lt;code&gt;x86&lt;/code&gt;|&lt;code&gt;ARM64&lt;/code&gt;(for WebAssembly)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;$ make CFLAGS="-march=x86-64" VARIANTS=x86-64-v2,x86-64-v3 BUILD_TEST=no # example x86 build&lt;/code&gt;
    &lt;p&gt;The automated test program &lt;code&gt;edge264_test&lt;/code&gt; can browse files in a given directory, decoding each &lt;code&gt;&amp;lt;video&amp;gt;.264&lt;/code&gt; file and comparing its output with each sibling file &lt;code&gt;&amp;lt;video&amp;gt;.yuv&lt;/code&gt; if found. On the set of AVCv1, FRExt and MVC conformance bitstreams, 109/224 files are decoded without errors, the rest using yet unsupported features.&lt;/p&gt;
    &lt;code&gt;$ make
$ ./edge264_test --help # prints all options available
$ ffmpeg -i vid.mp4 -vcodec copy -bsf h264_mp4toannexb -an vid.264 # optional, converts from MP4 format
$ ./edge264_test -d vid.264 # replace -d with -b to benchmark instead of display&lt;/code&gt;
    &lt;p&gt;Here is a complete example that opens an input file in Annex B format from command line, and dumps its decoded frames in planar YUV order to standard output. See edge264_test.c for a more complete example which can also display frames.&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;sys/mman.h&amp;gt;
#include &amp;lt;sys/stat.h&amp;gt;
#include &amp;lt;sys/types.h&amp;gt;

#include "edge264.h"

int main(int argc, char *argv[]) {
	int fd = open(argv[1], O_RDONLY);
	struct stat st;
	fstat(fd, &amp;amp;st);
	uint8_t *buf = mmap(NULL, st.st_size, PROT_READ, MAP_SHARED, fd, 0);
	const uint8_t *nal = buf + 3 + (buf[2] == 0); // skip the [0]001 delimiter
	const uint8_t *end = buf + st.st_size;
	// auto threads, no logs, auto allocs
	Edge264Decoder *dec = edge264_alloc(-1, NULL, NULL, 0, NULL, NULL, NULL);
	Edge264Frame frm;
	int res;
	do {
		res = edge264_decode_NAL(dec, nal, end, 0, NULL, NULL, &amp;amp;nal);
		while (!edge264_get_frame(dec, &amp;amp;frm, 0)) {
			for (int y = 0; y &amp;lt; frm.height_Y; y++)
				write(1, frm.samples[0] + y * frm.stride_Y, frm.width_Y);
			for (int y = 0; y &amp;lt; frm.height_C; y++)
				write(1, frm.samples[1] + y * frm.stride_C, frm.width_C);
			for (int y = 0; y &amp;lt; frm.height_C; y++)
				write(1, frm.samples[2] + y * frm.stride_C, frm.width_C);
		}
	} while (res == 0 || res == ENOBUFS);
	edge264_free(&amp;amp;dec);
	munmap(buf, st.st_size);
	close(fd);
	return 0;
}&lt;/code&gt;
    &lt;p&gt;
      &lt;code&gt;const uint8_t * edge264_find_start_code(buf, end, four_byte)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Return a pointer to the next three or four byte (0)001 start code prefix, or &lt;code&gt;end&lt;/code&gt; if not found.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t * buf&lt;/code&gt;- first byte of buffer to search into&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t * end&lt;/code&gt;- first invalid byte past the buffer that stops the search&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;int four_byte&lt;/code&gt;- if 0 seek a 001 prefix, otherwise seek a 0001&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;Edge264Decoder * edge264_alloc(n_threads, log_cb, log_arg, log_mbs, alloc_cb, free_cb, alloc_arg)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Allocate and initialize a decoding context.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;int n_threads&lt;/code&gt;- number of background worker threads, with 0 to disable multithreading and -1 to detect the number of logical cores at runtime&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void (* log_cb)(const char * str, void * log_arg)&lt;/code&gt;- if not NULL, a&lt;code&gt;fputs&lt;/code&gt;-compatible function pointer that&lt;code&gt;edge264_decode_NAL&lt;/code&gt;will call to log every header, SEI or macroblock (requires the&lt;code&gt;logs&lt;/code&gt;variant otherwise fails at runtime, called from the same thread except macroblocks in multithreaded decoding)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void * log_arg&lt;/code&gt;- custom value passed to&lt;code&gt;log_cb&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;int log_mbs&lt;/code&gt;- set to 1 to enable logging of macroblocks&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void (* alloc_cb)(void ** samples, unsigned samples_size, void ** mbs, unsigned mbs_size, int errno_on_fail, void * alloc_arg)&lt;/code&gt;- if not NULL, a function pointer that&lt;code&gt;edge264_decode_NAL&lt;/code&gt;will call (on the same thread) instead of malloc to request allocation of samples and macroblock buffers for a frame (&lt;code&gt;errno_on_fail&lt;/code&gt;is ENOMEM for mandatory allocations, or ENOBUFS for allocations that may be skipped to save memory but reduce playback smoothness)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void (* free_cb)(void * samples, void * mbs, void * alloc_arg)&lt;/code&gt;- if not NULL, a function pointer that&lt;code&gt;edge264_decode_NAL&lt;/code&gt;and&lt;code&gt;edge264_free&lt;/code&gt;will call (on the same thread) to free buffers allocated through&lt;code&gt;alloc_cb&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void * alloc_arg&lt;/code&gt;- custom value passed to&lt;code&gt;alloc_cb&lt;/code&gt;and&lt;code&gt;free_cb&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;int edge264_decode_NAL(dec, buf, end, non_blocking, free_cb, free_arg, next_NAL)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Decode a single NAL unit containing any parameter set or slice.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder * dec&lt;/code&gt;- initialized decoding context&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t * buf&lt;/code&gt;- first byte of NAL unit (containing&lt;code&gt;nal_unit_type&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t * end&lt;/code&gt;- first byte past the buffer (max buffer size is 231-1 on 32-bit and 263-1 on 64-bit)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;int non_blocking&lt;/code&gt;- set to 1 if the current thread has other processing thus cannot block here&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void (* free_cb)(void * free_arg, int ret)&lt;/code&gt;- callback that may be called from another thread when multithreaded, to signal the end of parsing and release the NAL buffer&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void * free_arg&lt;/code&gt;- custom value that will be passed to&lt;code&gt;free_cb&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;const uint8_t ** next_NAL&lt;/code&gt;- if not NULL and the return code is&lt;code&gt;0&lt;/code&gt;|&lt;code&gt;ENOTSUP&lt;/code&gt;|&lt;code&gt;EBADMSG&lt;/code&gt;, will receive a pointer to the next NAL unit after the next start code in an Annex B stream&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Return codes are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0&lt;/code&gt;on success&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENOTSUP&lt;/code&gt;on unsupported stream (decoding may proceed but could return zero frames)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EBADMSG&lt;/code&gt;on invalid stream (decoding may proceed but could show visual artefacts, if you can check with another decoder that the stream is actually flawless, please consider filling a bug report 🙏)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EINVAL&lt;/code&gt;if the function was called with&lt;code&gt;dec == NULL&lt;/code&gt;or&lt;code&gt;dec-&amp;gt;buf == NULL&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENODATA&lt;/code&gt;if the function was called while&lt;code&gt;dec-&amp;gt;buf &amp;gt;= dec-&amp;gt;end&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENOMEM&lt;/code&gt;if&lt;code&gt;malloc&lt;/code&gt;failed to allocate memory&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENOBUFS&lt;/code&gt;if more frames should be consumed with&lt;code&gt;edge264_get_frame&lt;/code&gt;to release a picture slot&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EWOULDBLOCK&lt;/code&gt;if the non-blocking function would have to wait before a picture slot is available&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;int edge264_get_frame(dec, out, borrow)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Fetch the next frame ready for output.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder * dec&lt;/code&gt;- initialized decoding context&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Edge264Frame *out&lt;/code&gt;- a structure that will be filled with data for the frame returned&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;int borrow&lt;/code&gt;- if 0 the frame may be accessed until the next call to&lt;code&gt;edge264_decode_NAL&lt;/code&gt;, otherwise the frame should be explicitly returned with&lt;code&gt;edge264_return_frame&lt;/code&gt;. Note that access is not exclusive, it may be used concurrently as reference for other frames.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Return codes are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0&lt;/code&gt;on success (one frame is returned)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EINVAL&lt;/code&gt;if the function was called with&lt;code&gt;dec == NULL&lt;/code&gt;or&lt;code&gt;out == NULL&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ENOMSG&lt;/code&gt;if there is no frame to output at the moment&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While reference frames may be decoded ahead of their actual display (ex. B-Pyramid technique), all frames are buffered for reordering before being released for display:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Decoding a non-reference frame releases it and all frames set to be displayed before it.&lt;/item&gt;
      &lt;item&gt;Decoding a key frame releases all stored frames (but not the key frame itself which might be reordered later).&lt;/item&gt;
      &lt;item&gt;Exceeding the maximum number of frames held for reordering releases the next frame in display order.&lt;/item&gt;
      &lt;item&gt;Lacking an available frame buffer releases the next non-reference frame in display order (to salvage its buffer) and all reference frames displayed before it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;typedef struct Edge264Frame {
	const uint8_t *samples[3]; // Y/Cb/Cr planes
	const uint8_t *samples_mvc[3]; // second view
	const uint8_t *mb_errors; // probabilities (0..100) for each macroblock to be erroneous, NULL if there are no errors, values are spaced by stride_mb in memory
	int8_t pixel_depth_Y; // 0 for 8-bit, 1 for 16-bit
	int8_t pixel_depth_C;
	int16_t width_Y;
	int16_t width_C;
	int16_t height_Y;
	int16_t height_C;
	int16_t stride_Y;
	int16_t stride_C;
	int16_t stride_mb;
	uint32_t FrameId;
	uint32_t FrameId_mvc; // second view
	int16_t frame_crop_offsets[4]; // {top,right,bottom,left}, useful to derive the original frame with 16x16 macroblocks
	void *return_arg;
} Edge264Frame;&lt;/code&gt;
    &lt;p&gt;
      &lt;code&gt;void edge264_return_frame(dec, return_arg)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Give back ownership of the frame if it was borrowed from a previous call to &lt;code&gt;edge264_get_frame&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder * dec&lt;/code&gt;- initialized decoding context&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;void * return_arg&lt;/code&gt;- the value stored inside the frame to return&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;void edge264_flush(dec)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;For use when seeking, stop all background processing, flush all delayed frames while keeping them allocated, and clear the internal decoder state.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder * dec&lt;/code&gt;- initialized decoding context&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;void edge264_free(pdec)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Deallocate the entire decoding context, and unset the pointer.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Edge264Decoder ** pdec&lt;/code&gt;- pointer to a decoding context, initialized or not&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stress testing (in progress)&lt;/item&gt;
      &lt;item&gt;Multithreading (in progress)&lt;/item&gt;
      &lt;item&gt;Error recovery (in progress)&lt;/item&gt;
      &lt;item&gt;Integration in VLC/ffmpeg/GStreamer&lt;/item&gt;
      &lt;item&gt;ARM32&lt;/item&gt;
      &lt;item&gt;PAFF and MBAFF&lt;/item&gt;
      &lt;item&gt;4:0:0, 4:2:2 and 4:4:4&lt;/item&gt;
      &lt;item&gt;9-14 bit depths with possibility of different luma/chroma depths&lt;/item&gt;
      &lt;item&gt;Transform-bypass for macroblocks with QP==0&lt;/item&gt;
      &lt;item&gt;SEI messages&lt;/item&gt;
      &lt;item&gt;AVX-2 optimizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I use edge264 to experiment on new programming techniques to improve performance and code size over existing decoders, and presented a few of these techniques at FOSDEM'24 and FOSDEM'25.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Single header file - It contains all struct definitions, common constants and enums, SIMD aliases, inline functions and macros, and exported functions for each source file. To understand the code base you should look at this file first.&lt;/item&gt;
      &lt;item&gt;Code blocks instead of functions - The main decoding loop is a forward pipeline designed as a DAG loosely resembling hardware decoders, with nodes being non-inlined functions and edges being tail calls. It helps mutualize code branches wherever possible, thus reduces code size to help fit in L1 cache.&lt;/item&gt;
      &lt;item&gt;Tree branching - Directional intra modes are implemented with a jump table to the leaves of a tree then unconditional jumps down to the trunk. It allows sharing the bottom code among directional modes, to reduce code size.&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Global context register - The pointer to the main structure holding context data is assigned to a register when supported by the compiler (GCC).&lt;/del&gt;This technique was dropped as Clang eventually reached on-par performance, so there is little incentive to maintain this hack.&lt;/item&gt;
      &lt;item&gt;Default neighboring values (search &lt;code&gt;unavail_mb&lt;/code&gt;) - Tests for availability of neighbors are replaced with fake neighboring macroblocks around each frame. It reduces the number of conditional tests inside the main decoding loop, thus reduces code size and branch predictor pressure.&lt;/item&gt;
      &lt;item&gt;Relative neighboring offsets (look for &lt;code&gt;A4x4_int8&lt;/code&gt;and related variables) - Access to left/top macroblock values is done with direct offsets in memory instead of copying their values to a buffer beforehand. It helps to reduce the reads and writes in the main decoding loop.&lt;/item&gt;
      &lt;item&gt;Parsing uneven block shapes (look at function &lt;code&gt;parse_P_sub_mb&lt;/code&gt;) - Each Inter macroblock paving specified with mb_type and sub_mb_type is first converted to a bitmask, then iterated on set bits to fetch the correct number of reference indices and motion vectors. This helps to reduce code size and number of conditional blocks.&lt;/item&gt;
      &lt;item&gt;Using vector extensions - GCC's vector extensions are used along vector intrinsics to write more compact code. All intrinsics from Intel are aliased with shorter names, which also provides an enumeration of all SIMD instructions used in the decoder.&lt;/item&gt;
      &lt;item&gt;Register-saturating SIMD - Some critical SIMD algorithms use more simultaneous vectors than available registers, effectively saturating the register bank and generating stack spills on purpose. In some cases this is more efficient than splitting the algorithm into smaller bits, and has the additional benefit of scaling well with later CPUs.&lt;/item&gt;
      &lt;item&gt;Piston cached bitstream reader - The bitstream bits are read in a size_t[2] intermediate cache with a trailing set bit to keep track of the number of cached bits, giving access to 32/64 bits per read from the cache, and allowing wide refills from memory.&lt;/item&gt;
      &lt;item&gt;On-the-fly SIMD unescaping - The input bitstream is unescaped on the fly using vector code, avoiding a full preprocessing pass to remove escape sequences, and thus reducing memory reads/writes.&lt;/item&gt;
      &lt;item&gt;Multiarch SIMD programming - Using vector extensions along with aliased intrinsics allows supporting both Intel SSE and ARM NEON with around 80% common code and few #if #else blocks, while keeping state-of-the-art performance for both architectures.&lt;/item&gt;
      &lt;item&gt;The Structure of Arrays pattern - The frame buffer is stored with arrays for each distinct field rather than an array of structures, to express operations on frames with bitwise and vector operators (see AoS and SoA). The task buffer for multithreading also relies on it partially.&lt;/item&gt;
      &lt;item&gt;Deferred error checking - Error detection is performed once in each type of NAL unit (search for &lt;code&gt;return&lt;/code&gt;statements), by clamping all input values to their expected ranges, then expecting&lt;code&gt;rbsp_trailing_bit&lt;/code&gt;afterwards (with very high probability of catching an error if the stream is corrupted). This design choice is discussed in A case about parsing errors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Other yet-to-be-presented bits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimalistic API with FFI-friendly design (7 functions and 1 structure).&lt;/item&gt;
      &lt;item&gt;The bitstream caches for CAVLC and CABAC (search for &lt;code&gt;rbsp_reg&lt;/code&gt;) are stored in two size_t variables each, which may be mapped to Global Register Variables in the future.&lt;/item&gt;
      &lt;item&gt;The decoding of input symbols is interspersed with their parsing (instead of parsing to a &lt;code&gt;struct&lt;/code&gt;then decoding the data). It deduplicates branches and loops that are present in both parsing and decoding, and even eliminates the need to store some symbols (e.g. mb_type, sub_mb_type, mb_qp_delta).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With the help of a custom bitstream writer using the same YAML format edge264 outputs, a set of extensive tests are being created in tools/raw_tests to stress the darkest corners of this decoder. The following table lists them all, along with the files implementing them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;General tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All supported types of NAL units&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;supp-nals&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All unsupported types of NAL units&lt;/cell&gt;
        &lt;cell&gt;All unsupp&lt;/cell&gt;
        &lt;cell&gt;unsupp-nals&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Maximal header log-wise&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;max-logs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All conditions (incl. ignored) for detecting the start of a new frame&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;finish-frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;nal_ref_idc=0 on a IDR&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;non-ref-idr&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Missing rbsp_trailing_bit for all supported NAL types&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;no-trailing-bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NAL of less than 11 bytes starting/ending at page boundary&lt;/cell&gt;
        &lt;cell&gt;All OK&lt;/cell&gt;
        &lt;cell&gt;tiny-nal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SEI/slice referencing an uninitialized SPS/PPS&lt;/cell&gt;
        &lt;cell&gt;1 OK, 4 errors&lt;/cell&gt;
        &lt;cell&gt;missing-ps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two non-ref frames with decreasing POC&lt;/cell&gt;
        &lt;cell&gt;All OK, any order&lt;/cell&gt;
        &lt;cell&gt;non-ref-dec-poc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Horizontal/vertical cropping leaving zero space&lt;/cell&gt;
        &lt;cell&gt;All OK, 1x1 frames&lt;/cell&gt;
        &lt;cell&gt;zero-cropping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P/B slice with nal_unit_type=5 or max_num_ref_frames=0&lt;/cell&gt;
        &lt;cell&gt;4 OK, 2 errors&lt;/cell&gt;
        &lt;cell&gt;no-refs-P-B-slice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;IDR slice with frame_num&amp;gt;0&lt;/cell&gt;
        &lt;cell&gt;All OK, clamped to 0&lt;/cell&gt;
        &lt;cell&gt;pos-frame-num-idr&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A ref that must bump out higher POCs to enter DPB (C.4.5.2)&lt;/cell&gt;
        &lt;cell&gt;All OK, check output order&lt;/cell&gt;
        &lt;cell&gt;poc-out-of-order&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two ref frames with the same frame_num but differing POC, then a third frame referencing both&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gap in frame_num while gaps_in_frame_num_value_allowed_flag=0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Stream starting with non-IDR I frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Stream starting with P/B frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ref slice with delta_pic_order_cnt_bottom=-2**31, then a second frame referencing it&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two frames A/B with intersecting top/bottom POC intervals in all possible intersections&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A 32-bit POC overflow between 2 frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A B-frame referencing frames with more than 2**16 POC diff&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;num_ref_idx_active&amp;gt;15 in SPS then no override in slice for L0 and L1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice with more ref_pic_list_modifications than num_ref_idx_active/16 for L0 and L1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice with ref_pic_list_modifications duplicating a ref then referencing the second one&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice with insufficient ref frames with and without override of num_ref_idx_active for L0 and L1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A modification of RefPicList[0/1] to a non-existing short/long term frame, then referencing it in mb&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;33 IDR with long_term_reference_flag=0/1 while max_num_ref_frames=0 (8.2.5.1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A new reference while max_num_ref_frames are already all long-term&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All combinations of mmco on all non-existing/short/long refs, with at least twice each mmco&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two fields of the same frame being assigned different long-term frame indices then referenced&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;While all max_num_ref_frames are long-term, a ref_pic_list_modification that references all of them&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;An IDR picture with POC&amp;gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A picture with mmco=5 decoded after a picture with greater POC (8.2.1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A P/B frame with zero references before or received with a gap in frame_num equal to max_ref_frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A P/B frame referencing a non-existing/erroneous ref&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A B frame with colPic set to a non-existing frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A current frame mmco'ed to long-term while all max_num_ref_frames are already long-term&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A mmco marking a non-existing picture to long-term&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All combinations of IntraNxNPredMode with A/B/C/D unavailability with asserts for out-of-bounds reads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A direct Inter reference from colPic that is not present in RefPicList0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A residual block with all coeffs at maximum 32-bit values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two slices of the same frame separated by a currPic reset (ex. AUD)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two frames with the same POC yet differing TopFieldOrderCnt/BottomFieldOrderCnt&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Differing mmcos on two slices of the same frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Sending 2 IDR, then reaching the lowest possible POC, then getting all frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two slices with mmco=5 yet frame_num&amp;gt;0 (to make it look like a new frame)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;POCs spaced by more than half max bits, such that relying on a stale prevPicOrderCnt yields wrong POC&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Filling the DPB with 16 refs then setting max_num_ref_frames=1 and adding a new ref frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Adding a frame cropping after decoding a frame&lt;/cell&gt;
        &lt;cell&gt;Crop should not apply retroactively&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Making a Direct ref_pic be used after it has been unreferenced&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;poc_type=2 and non-ref frame followed by non-ref pic, and the opposite (7.4.2.1.1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;direct_8x8_inference_flag=1 with frame_mbs_only_flag=0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;checking that a gap in frame_num with poc_type==0 does not insert refs in B slices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A SPS changing frame format while currPic&amp;gt;=0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A frame allocator putting all allocs at start/end of a page boundary&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Parameter sets tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Invalid profile_idc=0/255&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Highest level_idc=255&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All unsupported values of chroma_format_idc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All unsupported values of bit_depth_luma/chroma&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;qpprime_y_zero_transform_bypass_flag=1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All scaling lists default/fallback rules and repeated values for all indices, with residual macroblock&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;log2_max_frame_num=4 and a frame referencing another with the same frame_num%4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;CAVLC tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid total_zeros=0-8-prefix+3-bit-suffix for TotalCoeffs in [0;15] for 4x4 and 2x2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Invalid total_zeros=31/63/127-prefix for TotalCoeffs in [0;15] for 4x4 and 2x2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid coeff_token=0-14-prefix+4-bit-suffix for nC=0/2/4, and valid 6-bit-values for nC=8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Invalid coeff_token=31/63/127-prefix for nC=0/2/4, and invalid 6-bit-values for nC=8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid levelCode=25-prefix+suffixLength-bit-suffix for all values of suffixLength&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid run_before for all values of zerosLeft&amp;lt;=7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Invalid run_before=31/63/127 for zerosLeft=7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Macroblock of maximal size for all values of mb_type&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;mb_qp_delta=-26/25 that overflows on both sides&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All valid inferences of nC for all values of nA/nB=unavail/other-slice/0-16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All coded_block_pattern=[0;47] for I and P/B slices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All combinations of intra_chroma_pred_mode and Intra4x4/8x8/16x16PredMode with A/B-unavailability&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All values of mb_type+sub_mb_types for I/P/B with ref_idx/mvds different than values from B_Direct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;mvd=[-32768/0/32767,-32768/0/32767] in a single 16x16 macroblock&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TotalCoeff=16 for a Intra16x16 AC block&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A residual block with run_length=14 making zerosLeft negative&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;CABAC tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mixing CAVLC and CABAC in a same frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Single slice with at least 8 cabac_zero_word&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;MVC tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All wrong combinations of non_idr_flag with nal_unit_type=1/5 and nal_ref_idc=0/1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;nal_unit_type=14 then filler unit then nal_unit_type=1/5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;An nal_unit_type=5 view paired with a non_idr_flag=0 P view, or a non_idr_flag=1 view&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Missing a base or non-base view&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Receiving a SSPS yet only base views then&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;16 ref base views while non base are non-refs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A SSPS with different pic_width_in_mbs/pic_height_in_mbs/chroma_format_idc than its SPS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A SSPS with num_views=1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A non-base view with weighted_bipred_idc=2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A non-base view with its base in RefPicList1[0] and direct_spatial_mv_pred_flag=0 (H.7.4.3)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice with num_ref_idx_l0_active&amp;gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;svc_extension_flag=1 on a MVC stream&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SSPS with additional_extension2_flag=1 and more trailing data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gap in frame_num of 16 frames on both views&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Specifying extra_frames=1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Receiving a non-base view before its base&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;A stream sending non-base views after a few frames have been output&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Error recovery tests&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected&lt;/cell&gt;
        &lt;cell role="head"&gt;Test files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tests to implement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A complete frame received twice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A slice of a frame received twice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Frame with correct and erroneous slice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All combinations erroneous/correct and all interval intersections on 2 slices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;All failures of malloc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;All (dis-)allowed bit positions at the end without rbsp_trailing_bit&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/tvlabs/edge264"/><published>2025-10-01T21:00:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45444004</id><title>Egg-Shaped Curves (2007)</title><updated>2025-10-02T00:44:17.154560+00:00</updated><content>&lt;doc fingerprint="454448e76ea37375"&gt;
  &lt;main&gt;
    &lt;p&gt;ï»¿&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;HOME&lt;/cell&gt;
        &lt;cell&gt;JAPANESE&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt; When we search for the internet about an oval curve, several sites of Egg Curves and Ovals, Oval and Cassini oval are found. But there seem to be little equations of a curve near a real egg shape. So, an equation of egg shaped curve which resembles closely to the shape of a chicken egg is pursued here apart from a mathematical definition of "oval curve". &lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1. Basic Derivation of Equation of Egg Shaped Curve&lt;/p&gt;
          &lt;p&gt; An equation of egg shaped curve (egg curve), which resembles to the shape of the actual egg more than Cassini oval etc. (continuing from the left), is obtained below. &lt;/p&gt;
          &lt;p&gt;In 2021, we received an inquiry about how to obtain Eq.(7). Certainly, the derivation of Eq. (7) does not seem to be so easy. Therefore, for reference, "the derivation of Eq. (7)" is shown.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt; For simple expression of the above equation, we displace the trajectory by the value of in the direction, and we introduce the next two constant values;&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"/&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;of (pink colored curve)&lt;/p&gt;
          &lt;p&gt;and the shape of an actual egg&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt; If Eq.(9) is rewritten, the following equation is led into the expression indicating the obvious relation to this enveloping circle. &lt;/p&gt;
          &lt;p&gt; The major axis is obtained as = as strictly understood from Eq.(9). However, the minor axis cannot be obtained analytically. On the other hand, one round length of an egg shaped curve is given by &lt;/p&gt;
          &lt;p&gt; In the next, the plane area of an egg shaped curve as shown in Fig.2 is given by the follows with the use of Eq.(9b). &lt;/p&gt;
          &lt;p&gt;2. The Volume and the surface area of Egg Shaped Figure in the Three Dimensional Space&lt;/p&gt;
          &lt;p&gt;and in the case of b=0, the volume of sphere having the radius a/2 is led to as&lt;/p&gt;
          &lt;p&gt;. (15c)&lt;/p&gt;
          &lt;p&gt;As a reference, the calculation process of Eq.(15a) is written as&lt;/p&gt;
          &lt;p&gt;In the next, the surface area of an egg shaped solid figure is calculated by the following equation.&lt;/p&gt;
          &lt;p&gt;, (16)&lt;/p&gt;
          &lt;p&gt;where is given by Eq.(9b), and is given by Eq.(14b).&lt;/p&gt;
          &lt;p&gt;More detail analyses are described in "How can I find the surface area of a normal chicken egg?".&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Surface area of the egg shaped solid figure =&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"/&gt;
      &lt;row span="9"/&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;
          &lt;p&gt;egg shaped curve =&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;egg shaped curve =&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt; Especially, the relationship between the ratio of the minor to the major axes and the values of volume and surface area of egg shaped solid figure is shown in Fig.3.&lt;/p&gt;
          &lt;p&gt;3. Expression of the Equations of the Egg Shaped Curves with the Use of the Intermediate Variable&lt;/p&gt;
          &lt;p&gt;return&lt;/p&gt;
          &lt;p&gt;4. The Case Using Ellipsoid instead of Circle as a Basic Figure&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt; If we solve Eq.(20) with the usual method of the solution of the 2nd order equation, we obtain that&lt;/p&gt;
          &lt;p&gt;return&lt;/p&gt;
          &lt;p&gt;5. The Case that the Constants "a" and/or "b" are/is out of the Defined Region&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;return&lt;/p&gt;
          &lt;p&gt;6. General Extension from Egg Shaped Curves to Pear Shaped Curves&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt; where condition of constant c indicated in Eq.(10) is released for the reason of the extension, the conditions of d &amp;gt; 0 and f &amp;gt; 0 are needed for giving a closed curve, and new conditions which are explained in Eqs.(29) and (31) described below are added. Furthermore, the constant b is not used in this equation because the relation of has already been used in the process of derivation of Eq.(10) described in the first section.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt; In the region of 0 &amp;lt; x&amp;lt; a in the above equation&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt; [Annotation] As a solution outside of 0 &amp;lt; x &amp;lt; a, the following equation also may exist besides Eq.(27).&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt; As seen in Fig.8, a closed curve goes out of the area of 0 &amp;lt; x &amp;lt; a when c &amp;gt; 2 whose condition does not satisfy Eq.(31). Moreover, as seen in Fig.9, a closed curve goes out of the area of 0 &amp;lt; x &amp;lt; a when e &amp;gt; 0 whose condition does not satisfy Eq.(29). &lt;/p&gt;
          &lt;p&gt;Furthermore, some interesting figures can be drawn as shown in Figs.10 and 11.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; return &lt;p&gt;7. The Higher Order Equation I&lt;/p&gt;&lt;p&gt;If we replace to x in Eq.(24), we rewrite the following equation in the eight order. Such obtained equation can also be solved analytically. Moreover, the conditions of constants does not vary. However, it must be paid attention that the region of a closed curve given by the following equation is .&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;The solution of the above equation is given as&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt; Some examples of the curves which are given by Eq.(32) or its solution Eq.(33) are shown in Figs.12 and 13 in the region of . &lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; As seen in Fig.13, a closed curve does not exist within when e &amp;gt; 0. This is as like as described in the previous section. &lt;p&gt;Furthermore, some interesting figures can be drawn as shown in Figs.14 and 15 corresponding to Figs.10 and 11 respectively.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; When the values of some constants are varied, 'spade shaped curves' are obtained as shown in Figs.16 and 17. &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; return &lt;p&gt;8. The Higher Order Equation II&lt;/p&gt;&lt;p&gt;If we replace to y in Eq.(32), we rewrite the following equation in the eight order. The conditions of constants and the region of closed curve to be obtained are the same as in the previous section.&lt;/p&gt;&lt;p&gt;(34)&lt;/p&gt;&lt;p&gt;The solution of the above equation is given as&lt;/p&gt;&lt;p&gt;(35)&lt;/p&gt;&lt;p&gt;Some examples of the curves which are given by Eq.(34) or its solution Eq.(35) are shown in Figs.18 and 19 in the region of .&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;As seen in Fig.19, a closed curve does not exist within when e &amp;gt; 0. This is also as like as described in the previous section.&lt;/p&gt;
          &lt;p&gt;Furthermore, some interesting figures can be drawn as shown in Figs.20 and 21 corresponding to Figs.10 and 11, or to Figs.14 and 15 respectively.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt; return &lt;p&gt;9. Sites and papers in which this site is cited&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;HOME&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nyjp07.com/index_egg_E.html"/><published>2025-10-01T21:54:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45444694</id><title>Cormac McCarthy's personal library</title><updated>2025-10-02T00:44:16.916518+00:00</updated><content>&lt;doc fingerprint="77a350d52da307b7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Two Years After Cormac McCarthy’s Death, Rare Access to His Personal Library Reveals the Man Behind the Myth&lt;/head&gt;
    &lt;head rend="h2"&gt;The famously reclusive novelist amassed a collection of thousands of books ranging in topics from philosophical treatises to advanced mathematics to the naked mole-rat&lt;/head&gt;
    &lt;p&gt;Cormac McCarthy, one of the greatest novelists America has ever produced and one of the most private, had been dead for 13 months when I arrived at his final residence outside Santa Fe, New Mexico. It was a stately old adobe house, two stories high with beam-ends jutting out of the exterior walls, set back from a country road in a valley below the mountains. First built in 1892, the house was expanded and modernized in the 1970s and extensively modified by McCarthy himself, who, it turns out, was a self-taught architect as well as a master of literary fiction.&lt;/p&gt;
    &lt;p&gt;I was invited to the house by two McCarthy scholars who were embroiled in a herculean endeavor. Working unpaid, with help from other volunteer scholars and occasional graduate students, they had taken it upon themselves to physically examine and digitally catalog every single book in McCarthy’s enormous and chaotically disorganized personal library. They were guessing it contained upwards of 20,000 volumes. By comparison, Ernest Hemingway, considered a voracious book collector, left behind a personal library of 9,000.&lt;/p&gt;
    &lt;p&gt;What makes McCarthy’s library so intriguing is not just its size, nor the fact that very few people know about it. His books, many of which are annotated with margin comments, promise to reveal far more about this elusive literary giant than the few cagey interviews he gave when he was alive. For as long as people have been reading McCarthy, they have speculated about which books and authors informed and inspired his work, a subject he was loath to discuss. They have wondered about his interests and true personality because all he presented to the public was a reclusive, austere, inscrutable facade.&lt;/p&gt;
    &lt;p&gt;When Bryan Giemza, a scholar of literature and humanities at Texas Tech University, offered me exclusive journalistic access to McCarthy’s library and the cataloging project, what he was really offering was an unprecedented insight into McCarthy’s life and work. As a further enticement, he said that Cormac’s younger brother Dennis McCarthy would be there. “Dennis probably knew him as well as anyone,” Giemza said.&lt;/p&gt;
    &lt;head rend="h4"&gt;Did You Know? Who was Cormac McCarthy?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cormac McCarthy is an award-winning novelist whose works often explored the American West with darkness and complexity. Among McCarthy's many awards are a Guggenheim Fellowship in 1969, a MacArthur Fellowship in 1981, a Pulitzer Prize for Fiction in 2007 for The Road and a National Book award in 1992 for All the Pretty Horses.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I parked behind the house between a silver 1966 Buick Riviera rusting on deflated tires and a weathered red Lincoln Mark VIII. These were among the last survivors of McCarthy’s little-known vehicle collection. Dennis had sold 13 other cars, including two Allard racing cars from the early 1950s, a 1992 Lotus and a Ford GT40 racing car. McCarthy, who labored in obscurity and chronic poverty until he was 60, became a multi-millionaire later in life and freely indulged his desires and obsessions, with classic sports cars high on the list. Most of the money came from Hollywood, which turned three of his novels—All the Pretty Horses, No Country for Old Men and The Road—into star-studded movies.&lt;/p&gt;
    &lt;p&gt;I knocked on the imposing front door, an Indo-Portuguese antique made of teak and fortified with iron strappings, metal studs, flattened nails and small chains. There was no response, so I tried the handle. The door swung open and revealed a dimly lit hallway reduced to a narrow passage by head-high stacks of cardboard boxes on both sides. All those boxes were packed with books.&lt;/p&gt;
    &lt;p&gt;The first room off the hallway—the room where McCarthy died at age 89—was now so crammed with book boxes that it was impenetrable. Scholars called it “the Beast Room.” The next room was nearly as full. One open box showed volumes about the architect Frank Lloyd Wright, country houses in Ireland, schizophrenia, African history and British antique rifle barrels.&lt;/p&gt;
    &lt;p&gt;In the dining room, underneath a beautiful hanging light fixture of wood and colored glass that McCarthy designed and built himself, scholars were sitting at the table, scanning books’ ISBN bar codes through their phones into the library cataloging software on their laptops.&lt;/p&gt;
    &lt;p&gt;I found Giemza in the living room, wrestling with an internet connectivity problem. “Cormac didn’t have Wi-Fi in the house, so we had to bring our own,” he said. Nor did McCarthy use a computer—ever. He typed out his pages on a cheap, durable Olivetti typewriter and, I learned later, did most of his work propped up on pillows in bed.&lt;/p&gt;
    &lt;p&gt;The living room, like the house in general, had a sturdy, old-fashioned and decidedly masculine feel, but its clean lines were obscured by a chaotic overlay of clutter—mainly books, but also piles of nameless junk and hundreds of bowls, glasses and kitchenware items still in their packaging. Some of the book boxes and loose books had been moved into the room for the cataloging project, but not the rest of it. One of the first discoveries made by the visiting scholars was that McCarthy was something of a hoarder. His particular fixation on kitchenware, much of it bargain kitchenware, remains mysterious and a mark of his eccentricity.&lt;/p&gt;
    &lt;p&gt;The second major discovery, discernible in his work but confirmed beyond doubt in his library, was that McCarthy was a genius-level intellectual polymath with an insatiable curiosity. His interests ranged from quantum physics, which he taught himself by reading 190 books on the notoriously challenging subject, to whale biology, violins, obscure corners of French history in the early Middle Ages, the highest levels of advanced mathematics and almost any other subject you can name.&lt;/p&gt;
    &lt;p&gt;Giemza marveled at the heavy-duty philosophy books they were finding. “Seventy-five titles by or about Wittgenstein so far,” he said, referring to the Austrian philosopher of mathematics, logic, language and the mind. “And most of them are annotated, meaning Cormac read them closely. A lot of Hegel. That was his light evening reading, apparently.”&lt;/p&gt;
    &lt;p&gt;In the living room was a pool table piled with books and a leather couch facing two tall windows and three sets of nine-foot-tall wooden bookshelves designed by McCarthy that held approximately 1,000 books. Moving closer, I saw they were nearly all nonfiction hardbacks with no obvious system of organization.&lt;/p&gt;
    &lt;p&gt;One shelf held volumes about Mesoamerican&lt;lb/&gt; history and archaeology, along with Charles Darwin’s collected notebooks, Victor Klemperer’s three-volume diary of the Nazi years, books about organic chemistry and sports cars, and an obscure volume titled The Biology of the Naked Mole-Rat (Monographs in Behavior and Ecology). Another shelf held books about Grand Prix and Formula 1 racing, a great passion of McCarthy’s, and the collected writings of Charles S. Peirce, the American scientist, philosopher and logician, in six fat volumes of dense, difficult prose. &lt;/p&gt;
    &lt;p&gt;Trying to take it all in, I felt both fascinated and overwhelmed. It seemed almost inconceivable that an author who produced 12 novels, two plays and five screenplays had also found the time, energy and brainpower to master architecture, woodworking, stonemasonry and a wide range of intellectual disciplines. Some of his math books were nearly all equations.&lt;/p&gt;
    &lt;p&gt;Then we found an intricate drawing he’d made for an engine modification to one of his cars, and another showing how to rifle a gun barrel with hand tools. We found dozens of well-thumbed engine repair manuals and auto mechanic’s tools in the outbuildings, and learned that he could disassemble, reassemble and redesign an engine to increase its horsepower. Then I learned he had an eidetic memory and could remember nearly everything he had read or heard, including the lyrics to thousands of songs. McCarthy was starting to seem like a man whose talents and intelligence were without limits, yet he lived in a hoarder’s shambles and couldn’t stop buying nonstick skillets and fruit bowls.&lt;/p&gt;
    &lt;p&gt;By studying his library more closely, I hoped to gain a better understanding of McCarthy, but it was possible the mystery of his character would only deepen.&lt;/p&gt;
    &lt;p&gt;Giemza introduced me to his colleague Stacey Peebles, a professor of film and English at Centre College in Danville, Kentucky, and the current president of the Cormac McCarthy Society. It was Peebles who first met with Dennis McCarthy, the author’s brother and literary executor, and suggested that the society take on the monumental task of cataloging the books. The society’s mission is to further the study and appreciation of McCarthy’s work, and Peebles thought there was enough material in the library to keep scholars busy for decades—scrutinizing the annotations, tracing connections between the research books and passages in the novels, interpreting literary and philosophical influences. “If we were a well-funded institution, we’d take all these boxes into an empty building where we had plenty of space to work in, a dedicated team of people and all the time we needed,” she said. But Peebles and her small team all have full-time jobs, so the project has required a trade-off between detail and efficiency. “We can’t be as meticulous as we’d like and scan all the annotations, because we’ve got limited time and a massive amount of books to get through.”&lt;/p&gt;
    &lt;p&gt;McCarthy often had a pencil when he was reading and would make tiny vertical marks next to sentences that interested him and add comments in the margins in small print handwriting. Sometimes he jotted down thoughts on slips of paper that he left between the pages. Inside The Life of Saint Teresa of Ávila by Herself, first published in 1565, we found him musing philosophically: “There is an intelligence to the universe (of which we are fractal) and that intelligence has a character and that character is benign. Intends well toward all things. How could it not?”&lt;/p&gt;
    &lt;p&gt;McCarthy is known for the bleak, violent nihilism in many of his novels, so it was a surprise to see him describing the universe as intelligent and well-&lt;lb/&gt; intentioned. He was a lapsed Catholic who went back and forth on the question of God’s existence, sometimes changing his mind from one day to the next. &lt;/p&gt;
    &lt;p&gt;Peebles was collecting her favorite annotated books on the pool table. One was Realism in Mathematics by Penelope Maddy. In the margins, Mc-&lt;lb/&gt; Carthy summarizes the author’s points and comments on them, frequently disagreeing. “Gibberish,” he noted at one point. It was an exciting find for the scholars because McCarthy mined this book deeply for his final novel, Stella Maris. Its protagonist, Alicia Western, is a young mathematical genius with schizophrenia.&lt;/p&gt;
    &lt;p&gt;Another of Peebles’ favorite finds is an annotated copy of Shakespeare’s Hamlet. McCarthy, who once said, “the ugly fact is books are made out of books,” borrowed and altered elements from Hamlet for his 1979 novel Suttree. It was his most ornate and poetic book and the closest he ever came to writing autobiographically. The main character is a troubled dropout who has rejected a life of privilege and responsibility in Knoxville, Tennessee, where McCarthy grew up as the black sheep among six children in a well-to-do Catholic family with strong Irish roots.&lt;/p&gt;
    &lt;p&gt;He was born in 1933 and christened Charles Joseph McCarthy Jr. after his father. In his youth, he was known as Charlie, or sometimes Doc, until he changed his name to Cormac as a young man, partly inspired by the medieval Irish king Cormac mac Airt. The name change was probably also a declaration of independence from his father. Charles McCarthy Sr. was an attorney who became the chief counsel for the Tennessee Valley Authority, and Cormac always characterized him as a domineering, violent man who beat him viciously for trivial offenses. (His brother Dennis disputes this description and says that Cormac was “grossly exaggerating.“)&lt;/p&gt;
    &lt;p&gt;McCarthy told his own son John, and some of his friends, that the beatings started when he was 3 years old. Readers and critics have often wondered where the darkness and violence in McCarthy’s work comes from, and, if Cormac’s characterization is true, his childhood might account for some of it. He loved his mother, Gladys, but she was psychologically fragile and frequently absent from the family in mental health institutions.&lt;/p&gt;
    &lt;p&gt;He hated his Catholic school and loved roaming outdoors. Talking about his school days in a (rare) 1992 interview, McCarthy said, “There was no hobby I didn’t have, name anything, no matter how esoteric, I had found it and dabbled in it.” He made money trapping muskrats around Knoxville and selling the pelts, and somehow also established himself as an authority on antique American rifles.&lt;/p&gt;
    &lt;p&gt;In 1953, McCarthy dropped out of the University of Tennessee, where he was studying engineering and physics, and joined the Air Force. He was stationed in Anchorage, where he became the radio disc jockey for the base, and started reading in earnest in his spare time. After four years, he returned to the University of Tennessee but dropped out again and started writing novels.&lt;/p&gt;
    &lt;p&gt;The first three, The Orchard Keeper, Outer Dark and Child of God, were gothic tales set in the rural Appalachia he knew from his youth. In lyrical prose with marvelously rendered vernacular speech, they tackled dark subjects—murder, infanticide, incest, necrophilia—while displaying a reverence for nature and folk traditions. The fourth novel was Suttree, McCarthy’s richly comedic evocation of 1950s Knoxville. These novels earned critical praise and prestigious grants and awards, but each sold more poorly than the last.&lt;/p&gt;
    &lt;p&gt;One of the few details we have about McCarthy’s personal life comes from his second wife, Anne&lt;lb/&gt; DeLisle, an English singer and dancer, whom he met on a ship to Ireland in 1965. Their home was a partially converted dairy barn outside Knoxville; they bathed in a lake. “Someone would call up and offer him $2,000 to come speak at a university about his books,” she once said. “And he would tell them that everything he had to say was there on the page. So we would eat beans for another week.”&lt;/p&gt;
    &lt;p&gt;After leaving her without an explanation in 1974, McCarthy drifted around cheap motels with his typewriter, a pile of books and a light bulb for good reading light. In 1976 McCarthy took up residence in El Paso and turned his attention on the American Southwest and northern Mexico, setting himself the task of learning the culture, history, natural history, geology, folkways and distinctive Spanish idioms of the borderlands.&lt;/p&gt;
    &lt;p&gt;According to a letter he wrote, McCarthy read over 300 books to research Blood Meridian (1985), an ultraviolent philosophical Western based on the true story of a state-funded scalp-hunting gang in the 1840s and 1850s. Now widely regarded as his greatest masterpiece, it sold a pitiful 1,883 copies when it was first published. McCarthy’s fortunes changed with the publication of All the Pretty Horses in 1992. This elegiac Western, set in 1949 and 1950 in Texas and Mexico, became a best seller, won a National Book Award, and was adapted into a movie starring Matt Damon and Penélope Cruz. McCarthy followed with two more novels about drifting cowboys, then shifted course with No Country for Old Men (2005), a crime thriller that the Coen brothers turned into a quadruple-Oscar-winning movie starring Josh Brolin, Javier Bardem and Tommy Lee Jones. Next came The Road, a post-apocalyptic father-son journey that won the Pulitzer Prize for fiction in 2007 and was made into a film with Viggo Mortensen playing the father.&lt;/p&gt;
    &lt;p&gt;McCarthy had moved to Santa Fe with his third wife, Jennifer Winkley, and their young son John in 2001. He found the town off-puttingly liberal, moneyed and artsy, and moved there for one reason only: His great friend Murray Gell-Mann, the Nobel Prize-winning physicist, invited him to join the Santa Fe Institute, serving as a sort of in-house literary intellectual. This elite scientific think tank, co-founded by Gell-Mann, brings together some of the world’s most brilliant minds to research complex interconnected systems. McCarthy had long preferred the company of scientists to that of literary people, and he delighted in the high-flying conversations at the institute. He went there nearly every day to work on his writing and kept up with all the institute’s scientific research.&lt;/p&gt;
    &lt;p&gt;McCarthy was famous for refusing to discuss his work, so there was widespread amazement in literary quarters when he agreed to do a televised interview in 2007 with Oprah Winfrey, who had picked The Road as her book club selection. Viewers saw a courteous, gray-haired Southerner with a high-domed forehead and a flashing smile. When Oprah asked if he was “passionate” about writing, he replied, “Passionate sounds like a pretty fancy word.”&lt;/p&gt;
    &lt;p&gt;Oprah, knowing it was true, asked if The Road was a love story to his young son John. “In a way, I suppose, that’s kind of embarrassing,” he said. She made slightly better headway on the subject of punctuation. McCarthy didn’t use quotation marks, hated semicolons and kept commas to the barest minimum. “There’s no reason to block the page up with weird little marks,” he said. “If you write properly, you shouldn’t have to punctuate.”&lt;/p&gt;
    &lt;p&gt;McCarthy could pull it off because he was a virtuoso, renowned for his powers of description and ear for dialogue. The Nobel Prize-winning novelist Saul Bellow extolled McCarthy’s “absolutely overpowering use of language, his life-giving and death-dealing sentences.” McCarthy’s detractors, meanwhile, found his writing overly mannered, his characters overly masculine, and accused him of relishing the violence he wrote about so vividly.&lt;/p&gt;
    &lt;p&gt;When McCarthy died in June 2023, after battling leukemia, prostate cancer, dehydration and what he once called “the sheer velocity of time,” the accolades were immediate, and fulsome. Stephen King called him the “last great white male American novelist.” Sebastian Junger compared him to Mount Everest. The Guardian headlined its remembrance with a prophecy: “His work will sing down the centuries.”&lt;/p&gt;
    &lt;p&gt;Dennis McCarthy, the youngest of the six children, made his way through the book-choked hallway into the book-strewn living room. A retired lawyer, editor and conservation biologist, Dennis published his first novel in 2021, a spiritual Western about Billy the Kid. Now 81, he was fit and trim, with blue eyes, a radiant smile and a strong resemblance to Cormac. “He was my best friend for 70 years and a fabulous older brother who always looked out for me,” he said. “We were very, very close.”&lt;/p&gt;
    &lt;p&gt;I asked him which authors his brother most admired. “Moby-Dick was Cormac’s favorite book without question, and Faulkner was more of an influence than he liked to admit,” he said. “He loved Hemingway’s short stories, James Joyce, Dostoyevsky and Shakespeare of course.” Readers and scholars had already identified these literary forebears, but it was satisfying to hear them confirmed.&lt;/p&gt;
    &lt;p&gt;Of the many thousands of books in the house, the basement and the outbuildings, how many had McCarthy actually read? “If you exclude the encyclopedias and reference books, I would guess about 85 percent,” Dennis said. “Cormac kept on ordering books after he was too sick and frail to read, because it was a compulsion, but until that point he would read for hours and hours nearly every day. He never left the house without a book. He never left the house without a gun. Both were equally unthinkable.”&lt;/p&gt;
    &lt;p&gt;Why was he always armed? “He was a conservative country boy from the South who understood that the world is a dangerous place.” When he was 24, McCarthy accidentally shot himself in the leg while practicing alone on a gun range in Tennessee. Dennis didn’t know any more details, because his brother refused to discuss the incident, but it was likely a quick-draw gone wrong.&lt;/p&gt;
    &lt;p&gt;When I asked Dennis about his brother’s reputation as a recluse, he said it was totally inaccurate. “He was very sociable and could get along with anybody. Well, almost anybody. He didn’t suffer fools gladly, or people who rushed up to him gushing about his books. But he had a lot of friends, and he loved dining and conversation, and five-hour lunches that sometimes turned into ten-hour lunches.”&lt;/p&gt;
    &lt;p&gt;Those friends included physicists and quark-discoverers Gell-Mann and George Zweig, the whale biologist Roger Payne, the movie star Josh Brolin, plus a bar owner in Tucson who calls himself God and a silver-tongued con man from Knoxville named John Sheddan, who appears exactly as himself under his own name in McCarthy’s penultimate novel, The Passenger.&lt;/p&gt;
    &lt;p&gt;Brolin got to know McCarthy during the filming of No Country for Old Men and was at the author’s bedside the night before he died. “It was me, his ex-wife, his son John, and that was it,” Brolin tells me on the phone. “He was telling these wild stories, about drinking wine with André the Giant in Paris, and all this stuff was coming out totally lucid, sharp, funny, inspired. Then he would go into this lost dementia and he’d be grabbing at stuff that wasn’t there. Then he’d go to sleep, and then he’d wake up and tell another story.” Soon after Brolin left, McCarthy drew his final breath.&lt;/p&gt;
    &lt;p&gt;McCarthy’s son John, the model for the boy character in The Road, was now 26 and sleeping in his father’s old bedroom upstairs. He’s a licensed pilot, a composer and a musician. The first time I met John, he was coming sleepily down the wooden stairs in search of coffee. I had just learned that Dennis had emptied two storage units full of books in El Paso and two more in Santa Fe and moved the boxes into the house for cataloging. “So I’m getting a totally unrealistic picture of what the house was like when you were growing up here,” I said to John.&lt;/p&gt;
    &lt;p&gt;“Not really,” he said. “This is pretty much how it was. Boxes everywhere. Piles of books everywhere. The hallway stacked up with boxes with a little path through the middle. Whole rooms so full of books you couldn’t go in there. It didn’t bother me at all.”&lt;/p&gt;
    &lt;p&gt;It was John who told me that McCarthy worked in bed—a California king with high-thread-count sheets, the Olivetti on a wooden platform with a leather pillow underneath it, and piles of typed pages, magazines, books and catalogs. Writing, McCarthy once said, was not a conscious process for him. He put a blank piece of paper in the Olivetti, the words arrived, and he typed them down. But that was just the first stage of an extensive rewriting and structuring process, and some of his books took 20 years or more to get right.&lt;/p&gt;
    &lt;p&gt;“Dad didn’t like being interrupted when he was working, or when he was reading,” John said. “‘No, no, no. I’m reading. Go away!’ he would say. But he was a great father, always there for me, and I learned so much from him. We would have these long conversations about science and history and music, and whatever else, and he was the funniest person I’ve ever met, just a natural comedian.”&lt;/p&gt;
    &lt;p&gt;I asked John what else his father collected apart from books, cars and kitchenware. “I would say clothes were the other big one. He had hundreds of tweed jackets, hundreds of shirts, hundreds of suits that he’d never worn.” John once spent three days dragging stuff out of a room he wanted to use as a bedroom. “When I was done, I said, ‘You ever think you might be a little bit of a hoarder?’ And he looks at me and he goes, ‘Yeah, probably.’ He attributed it to all those years when he had no money.”&lt;/p&gt;
    &lt;p&gt;Dennis isn’t buying that explanation. “Cormac always lived in chaos, which I found fascinating because he had such a fabulous artistic sense. He could design things beautifully and he dressed impeccably, but his living quarters were always a disaster. He was an incredibly complicated individual.”&lt;/p&gt;
    &lt;p&gt;The cataloging scholars could only spare four or five days at a time. Then they would go back to their jobs for a few months and try to carve out another long weekend in New Mexico. The stalwarts were Peebles and Rick and Jonathan Elmore, whip-smart twin brothers who looked nothing alike, taught at different colleges and wrote academic papers together about McCarthy’s work.&lt;/p&gt;
    &lt;p&gt;The cataloging was dusty, repetitive, eye-straining work, but it was conducted with good humor and camaraderie, and you never knew what might come out of the next box. One afternoon, after looking through a batch about Cistercian abbeys, violin makers, metaphysics, meta-ontology, the incest taboo and the material foundations of ancient Mesopotamian civilization, I said, “Was there anything he wasn’t interested in? Sewing perhaps?”&lt;/p&gt;
    &lt;p&gt;“Nope,” said Jonathan Elmore, an English professor at Louisiana Tech University. “We’ve cataloged books on needlework and quilting.” Rick noted McCarthy’s keen interest in clothes and fashion, which could, I granted, be described as sewing-related. McCarthy was a longtime subscriber to the fashion and style magazine W, and he had annotated many of his books about menswear. In his copy of The Suit: A Machiavellian Approach to Men’s Style, McCarthy penciled his opinion of slip-on dress shoes: “disgusting.” Further down the same page, next to a sentence praising shiny-buckled monk-strap shoes, he wrote, “yet more horror.”&lt;/p&gt;
    &lt;p&gt;The scholars treated annotations like pieces of treasure and would read them aloud to each other. Inside Reclaiming History: The Assassination of President John F. Kennedy, they found notes on a slip of paper, including a line about the assassin’s bullet: “it was going like a bat out of hell when it left the president’s head and in that crowd it is a pure freak of chance that it didn’t take out a citizen-spectator.”&lt;/p&gt;
    &lt;p&gt;The historical figures who interested McCarthy the most, judging by the number of books he owned about them, were Albert Einstein (114 books), Winston Churchill (88) and James Joyce (78). Architecture is the dominant subject in the collection, with 855 books. The human being whom McCarthy most admired, Dennis confirms, was Ludwig Wittgenstein. The team cataloged a staggering 142 books by or about the philosopher, with a high proportion annotated.&lt;/p&gt;
    &lt;p&gt;McCarthy’s fascination with Wittgenstein came as a surprise to the scholars, but it makes sense. As Rick Elmore, a philosophy professor at Appalachian State University with floral tattoos climbing up his neck, puts it, “Wittgenstein was always asking how the systems we use to represent the world relate to the world we want to represent. It’s one of the central questions in McCarthy’s work.”&lt;/p&gt;
    &lt;p&gt;With the exception of Moby-Dick in multiple, gorgeous leather-bound editions, the scholars found hardly any novels until they started cutting open boxes that Dennis retrieved from a storage locker in El Paso. Out came the entire canon of Western literature, from ancient Greece and Rome to the best novelists, poets and essayists of the 1970s, nearly all in cheap, worn, paperback editions. “These are the books that he read in his 20s and 30s and maybe into his 40s, and he was broke that whole time,” said Dennis. “Once he got money, Cormac bought all his books in hardback if possible, and for the last 40 years of his life he read almost no fiction at all.”&lt;/p&gt;
    &lt;p&gt;Why? The answer stems from McCarthy’s deeply disparaging view of modern society, which he considered lost, divorced from nature, history and tradition and heading toward social collapse and apocalypse. “Cormac considered contemporary fiction a waste of time,” said Dennis, “because contemporary writers no longer have a legitimate culture to feed their souls.”&lt;/p&gt;
    &lt;p&gt;One afternoon, Dennis was marveling at McCarthy’s storytelling abilities and comedic talents, and I asked him if there was anything, apart from housekeeping, that his brother had been bad at. He thought about it for a moment and said, “Marriage.”&lt;/p&gt;
    &lt;p&gt;McCarthy was married and divorced three times. “His wives needed more than he gave them,” Dennis said. “The work always came first for Cormac. He loved those women, but he loved himself more. He was a narcissist. And if he hadn’t been a narcissist, he never would have achieved the same heights of artistic greatness.”&lt;/p&gt;
    &lt;p&gt;The most enduring love of McCarthy’s life was a woman named Augusta Britt. As she revealed last year in interviews with Vanity Fair magazine, they began a sexual relationship when she was 17 and he was 43, and he took her to Mexico to evade the FBI, who were after him for statutory rape and Mann Act violations. Britt has said she didn’t feel sexually exploited and credits McCarthy for saving her life by rescuing her from an abusive situation in Tucson, but some readers and commentators have found McCarthy’s behavior with her beyond the pale. (Britt declined to comment for this piece.)&lt;/p&gt;
    &lt;p&gt;McCarthy and Britt were together as a couple for about four years. Even after they split up, “He never stopped loving her,” Dennis said. “He continued to see her on a regular basis, and they maintained a close relationship for the rest of his life.”&lt;/p&gt;
    &lt;p&gt;Piece by piece, the inscrutable mystique that McCarthy built around himself is falling away. Two biographies are on the way to publication, one by a friend of McCarthy’s named Laurence Gonzales, the other by literary biographer Tracy Daugherty, and Britt might collaborate on a book with Vincenzo Barney, who wrote her story in Vanity Fair. We also have McCarthy’s library, which perhaps more than any other source can illuminate the mind of the man who, as Peebles says, “built his life on books.”&lt;/p&gt;
    &lt;p&gt;On the first day of the final cataloging session, Peebles let out a hooting sound upon finding a dead bat at the bottom of a box. The downstairs of the house had been steadily accumulating dust for more than two years, since McCarthy’s death, and it was still crammed with books. The McCarthy scholars—Cormackians, as they call themselves—repacked the cataloged books, wrote the date and “Cataloged CMS” for Cormac McCarthy Society, and stacked them up wherever space could be found. The annotated books went into separate boxes marked “annotated” or were piled up on the pool table. The dead bat was left in the bottom of a book box.&lt;/p&gt;
    &lt;p&gt;When the project began, Peebles had hoped that all the books could be kept together in a single collection in some sort of Cormac McCarthy memorial building, but that wasn’t panning out. Dennis had arranged for the annotated books to join his brother’s papers, which include the notes and drafts for his entire body of work, at the Wittliff Collections archive at Texas State University. The Santa Fe Institute wanted a selection of the most intellectually rigorous academic books for a small library it was planning to build in honor of McCarthy. The rest of the books were going to the University of Tennessee in Knoxville, where he enrolled twice and failed to graduate.&lt;/p&gt;
    &lt;p&gt;In the digital realm, however, McCarthy’s library will live on as a complete entity, and the public will be able to inspect its cataloged titles free of charge. “Our goal, right from the outset, was to create an open-access database listing all the books in his collection,” Peebles said. “Anyone who wants to know what books McCarthy was reading, and whether he annotated them, will be able to log on and access that information.” The University of South Carolina Press has agreed to partner with Peebles to create a website for this purpose, and to publish a monograph by Peebles about the cataloging project. There’s talk of scanning all the annotations at some point and making them available on the website, but that is still theoretical.&lt;/p&gt;
    &lt;p&gt;Almost exactly a year after the project began, Peebles opened the very last box. Perhaps the best adjective for its contents is Cormackian. Peebles pulled them out and announced books about Mexican architecture and the French Renaissance court, Kierkegaard’s metaphors and the Texas Rangers, the neurobiology of mental illness, architecture and society in Normandy from 1120 to 1270, and the Gun Digest book of assault weapons.&lt;/p&gt;
    &lt;p&gt;She was unable to calculate the total number of books because the cataloging software didn’t account for multi-volume works. McCarthy’s 36-volume history of Utah, for example, registered as a single entry. Nor did the software tally multiple editions of the same book, so McCarthy’s 13 copies of Moby-Dick registered as one entry. The total number of entries was 18,520. Taking into account duplicate copies and multi-volume works, Peebles felt confident that McCarthy’s library contained just over 20,000 books, with 2,170 annotated.&lt;/p&gt;
    &lt;p&gt;Driving away from the house, with the taste of old book dust in my mouth, I marveled at the extraordinary force of McCarthy’s curiosity. I thought about the books on acousto-optics and lay intellectuals in the ninth-century Carolingian Empire. The $2,200 he spent on eight volumes of Horace Walpole’s collected letters. The $10,000 in several uncashed royalty checks that he used as a bookmark in the memoir of William Faulkner’s niece. To peer into someone’s library is to peer into their brain, and here, it seemed, was a mind that wanted to know everything.&lt;/p&gt;
    &lt;p&gt;Editors’ note: After the print version of this story was published, this version of the piece was updated with further comment from Dennis McCarthy. Also, on September 8, 2025, this article was updated with further details about the University of South Carolina Press’ involvement in this archival project.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.smithsonianmag.com/arts-culture/two-years-cormac-mccarthys-death-rare-access-to-personal-library-reveals-man-behind-myth-180987150/"/><published>2025-10-01T23:06:55+00:00</published></entry></feed>