<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-25T18:15:50.037321+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46751825</id><title>Hands-On with Two Apple Network Server Prototype ROMs</title><updated>2026-01-25T18:15:58.603230+00:00</updated><content>&lt;doc fingerprint="a60898770926b1d7"&gt;
  &lt;main&gt;&lt;p&gt;Here's why I need to do inventory more often.&lt;/p&gt;This is an Apple prototype ROM I am ashamed to admit I found in my own box of junk from various Apple Network Server parts someone at Apple Austin sent me in 2003. The 1996 Apple Network Server is one of Apple's more noteworthy white elephants and, to date, the last non-Macintosh computer (iOS devices notwithstanding) to come from Cupertino. Best known for being about the size of a generous dorm fridge and officially only running AIX 4.1, IBM's proprietary Unix for Power ISA, its complicated history is a microcosm of some of Apple's strangest days during the mid-1990s. At $10,000+ a pop (in 2026 dollars over $20,700), not counting the AIX license, they sold poorly and were among the first products on the chopping block when Steve Jobs returned in 1997.&lt;p&gt;stockholm, my own Apple Network Server 500, was a castoff I got in 1998 — practically new — when the University bookstore's vendor wouldn't support the hardware and it got surplused. It was the first Unix server I ever owned personally, over the years I ended up installing nearly every available upgrade, and it ran Floodgap.com just about nonstop until I replaced it with a POWER6 in 2012 (for which it still functions as an emergency reserve). Plus, as the University was still running RS/6000 systems back then, I had ready access to tons of AIX software which the ANS ran flawlessly. It remains one of the jewels of my collection.&lt;/p&gt;&lt;p&gt;So when the mythical ANS MacOS ROM finally surfaced, I was very interested. There had always been interest in getting the ANS to run MacOS back in the day (I remember wasting an afternoon trying with a Mac OS 8 CD) and it was a poorly-kept secret that at various points in its development it could, given its hardware basis as a heavily modified Power Macintosh 9500. Apple itself perceived this interest, even demonstrating it with Mac OS prior to its release, and leading then-CTO Ellen Hancock to later announce that the ANS would get ROM upgrades to allow it to run both regular Mac OS and, in a shock to the industry, Windows NT. This would have made the ANS the first and only Apple machine ever sold to support it.&lt;/p&gt;&lt;p&gt;Well, guess what. This is that pre-production ROM Apple originally used to demonstrate Mac OS, and another individual has stepped up with the NT ROMs which are also now in my possession. However, at that time it wasn't clear what the prototype ROM stick was — just a whole bunch of flash chips on a Power Mac ROM DIMM which my Apple contacts tell me was used to develop many other machines at the time — and there was no way I was sticking it into my beloved production 500. But we have a solution for that. Network Servers came in three sizes: the rackmount ANS 300 ("Deep Dish") which was never released except for a small number of prototypes, the baseline ANS 500 ("Shiner LE"), and the highest tier ANS 700 ("Shiner HE") which added more drive bays and redundant, hot-swappable power supplies.&lt;/p&gt;&lt;p&gt;Which brings us to this machine.&lt;/p&gt;Meet holmstock, my Network Server 700, and the second ANS in my collection (the third is my non-functional Shiner ESB prototype). This was a ship of Theseus that my friend CB and I assembled out of two partially working but rather thrashed 700s we got for "come and get them" in August 2003. It served as stockholm's body double for a number of years until stockholm was retired and holmstock went into cold storage as a holding bay for spare parts. This makes it the perfect system to try a dodgy ROM in.&lt;p&gt;I'll give you a spoiler now: it turns out the NT ROM isn't enough to install Windows NT by itself, even though it has some interesting attributes. Sadly this was not unexpected. But the pre-production ROM does work to boot Mac OS, albeit with apparent bugs and an injection of extra hardware. Let's get the 700 running again (call it a Refurb Weekend) and show the process.&lt;/p&gt;The 700 weighs around 85 pounds unloaded and is exactly like trying to cram a refrigerator into the backseat of your car (in this case my Honda Civic Si). While it does have wheels on the bottom, even the good ones don't have a great turning radius (and these aren't good), and getting it in and out of the car unavoidably means having to pick it up. Lift with your knees, not with your back.&lt;head rend="h3"&gt;Preparing the 700 for testing&lt;/head&gt;&lt;p&gt;This section is basically a cloaked Refurb Weekend, but even if you're familiar with ANS guts, I'm going to point out a few specific things relevant to ROM support as we go along. We want this machine as ship-shape as we can get it so that accurate observations can be made for posterity!&lt;/p&gt;&lt;p&gt;I would also like to thank my wife who chose to politely ignore the new noisy beast hulking in the living room for a few days.&lt;/p&gt;Continuing in the fridge motif, the 500 and 700 have a front keylock controlling a sliding door, along with a unique 4-line LCD which displays boot information and can be used as an output device in AIX and other operating systems. Unlike my very minimally yellowed 500 which has spent most of its life in quiet smoke-free server rooms, this one seemed to have gotten a bit more sun. Fortunately most of the chassis is painted metal which is also where most of the weight comes from. The keylock position on power-up is noted by the firmware; the leftmost is the service setting, the middle is a normal boot, and the rightmost (locked) position puts the machine into a power failsafe mode. The sliding door covers seven front drive bays, normally one with a CD-ROM, one with some sort of tape drive (typically a DAT/DDS drive, but a few have 8mm tape instead, both the same drives as sold for the Workgroup Server 95 and 9150), and the rest various hard drives which can be either independent or connected into an optional RAID. The 700 can take two more drives in a rear bracket. Although I have the RAID card, I never ended up installing it since a single drive was more than sufficient for what I was using it for. As most of the drive trays and both drive brackets had been removed from the two donor 700s used to assemble holmstock, I ended up just keeping a CD-ROM and two trays, and used the other open space for storage.&lt;p&gt;At the top are the NMI, reset and power buttons, plus a standard Mac floppy drive.&lt;/p&gt;&lt;p&gt;It is worth noting here that the internal bays are all serviced by two Symbios Logic 53C825A controllers, providing two Fast Wide SCSI busses running at 20MB/s. Unlike the typical Power Mac MESH (10MB/s) controller, the ANS internal SCSI controllers are unique to the ANS and appear in no other Apple product. Remember this for later. A second external SCSI bus is available on the rear, using the same (slower 5MB/s) CURIO SCSI/Ethernet/serial combo chip as other contemporary Power Macs and implementing an NCR 53C94.&lt;/p&gt;The rear (with the monitor power cable photobombing the shot) is much less yellowed. Ports are here for audio in and out (standard AWACS), ADB, two beige Mac MiniDIN-8 serial ports, VGA (oddly but happily a conventional HDI-15, not Apple's traditional DA-15), AAUI 10Mbit Ethernet (any AAUI Mac dongle will work), and the external SCSI bus DB-25. Six PCI slots are available. A second keylock secures the logic board which is on a slide-out drawer accessed with the two handles. Both rear panels have their own fans which are hot-swappable as well. Apple included a monitor dongle in the box.&lt;p&gt;It is also worth noting here that the onboard video is a Cirrus Logic 54M30, also unique to the ANS, and likewise also used in no other Apple product. We'll be coming back to this point too.&lt;/p&gt;Parenthetically, here are the keylocks (new replacements in my part box). They are wafer-lock keys of the same type used in the Quadra 950, Apple Workgroup Server 95 and Workgroup Server 9150. As sold Network Servers came with three keys, one front, one back and one spare, but they are all interchangeable. These keys have a small three-digit code engraved into the metal identifying the lock they are designed to fit. I also got out a lot of parts from storage just in case they were needed, some of which were in the 700 and some of which were separate. Besides my two boxes of tricks, I also pulled out a spare logic board, five boxes of RAM upgrade kits (these are only 16MB each, though, so this isn't as much memory as you'd think), a 200MHz CPU upgrade kit, several more loose CPUs I also have, and a RAID card just for fun.&lt;p&gt;I dimly recalled the machine may not have been working right when I committed it to storage, but we'll proceed as if it had been, starting with a visual inspection of the electronics.&lt;/p&gt;The keylock on the logic board drawer (shown here with the rear panel off so you can see how it operates) has just two positions. In the horizontal locked position, the board is connected to power and a metal tab prevents the drawer from coming out. In the vertical unlocked position, the board is disconnected and the tab is moved away from the chassis so the drawer can be pulled free. We turn the rear key, grab the handles and pull the board drawer out. This is the logic board (the spare in the bag). It has a broadly similar layout to other six-slot Power Macs and has many of the same chips, including a Grand Central (labeled I/O CNTRL, near the Cirrus Logic video ASIC), CURIO (labeled SCSI/ENET) and two Bandits (labeled as PCI BRIDGEs). However, it only has eight RAM DIMM slots instead of the 9500's twelve, and most of the system connections are consolidated into a single card edge at the top and a large power connector at the bottom. There are separate slots for the ROM DIMM, the CPU daughtercard and the L2 cache. Headers handle both internal SCSI busses, the mainboard fan and the rear keylock. A small red CUDA reset button is at the top left. Installed, the board sits in front of the mainboard fan which is primarily used to cool the CPU daughtercard. This daughtercard rides in plastic rails that serve as alignment guides and structural support. Tabs and a couple mounting screws hold the logic board in place in the drawer. The tabs, card rails and much of the drawer itself are unfortunately made from Amelioplastic, but this drawer is thick and not normally exposed to the exterior, and it mercifully remains in good physical condition. Note that when the drawer is open, the board is completely ungrounded, so only handle it with antistatic precautions.&lt;p&gt;I never store machines with their PRAM batteries installed (especially since my Shiner ESB prototype had been ruined by the previous owner doing so, during which time it leaked and corroded the logic board), but in this particular case since we will be messing with the system it is easier to reset the logic board if we never install the battery at all. With the machine unplugged, the battery out and the rear key unlocked (horizontal), the board will be completely depowered and will reset in about three minutes or so.&lt;/p&gt;The CPU card is much larger than the ones used in most other PCI Power Macs and was intended to accommodate a dual-processor SMP option which was never sold, though again some prototypes have escaped (I would love to get one). Unfortunately this means that Power Mac CPU cards can't upgrade an ANS and the highest-speed option is the 200MHz 604e card shown here, but any ANS CPU card will work in any ANS, so stockholm also has a 200MHz card. Bus speed and CPU speed are related: the 132MHz (base 500) and 176MHz 604 cards run the bus at 44MHz, but the 150MHz 604 (base 700) and 200MHz 604e cards run the bus at 50MHz.&lt;p&gt;At the top is the 700's standard 1MB L2 cache (the 500 came with 512K). These are allegedly regular Power Mac caches, and a Network Server 1MB cache should work in other Power Macs, but the 500 kernel-panicked with a Sonnet L2 cache upgrade and I eventually had to chase down a 1MB card pulled from another 700.&lt;/p&gt;Behind that is the ROM stick and the centrepiece of this article. They are not always labeled — one of my spares isn't — but when they are, the standard production ROM is part 341-0833. It is a regular 4MB ROM like other Old World Macs. We're going to test this machine with that before we go installing the others. To get a test report will require a minimum amount of RAM. The ANS uses the same 168-pin DIMMs as other Power Macs and can accept up to 512MB (anything greater is not supported by the memory controller), but uniquely needs 60ns parity RAM for highest performance. If any DIMM is not parity, then the system ROM disables parity for all DIMMs and sets the timing to 70ns, even if the RAM is faster. This is a non-trivial hit, especially at the fastest 50MHz bus speed, so you really want parity if you can get it. Here I'm using parity FPM, which was sold standard in the units (all units came with at least 32MB in two 16MB DIMMs) and in upgrade kits (16MB in two 8MB DIMMs), all manufactured by IBM as OEM under contract and sold at typically exorbitant Apple prices. Later on 64MB and 128MB parity DIMMs became available and stockholm has a full 512MB from eight 64MB parity sticks. RAM need not be installed in pairs, though this is preferred as the ANS supports interleaving. While EDO RAM should "just work" (treated as FPM), I've never tried parity EDO in an ANS. We'll put in two IBM 16MB parity FPM DIMMs to equal the base 32MB. With the drawer closed and the rear key locked, we plug in the server (no drives attached yet), turn the front key to service, and then press the front power button to get ... a mostly blank front LCD instead of startup messages.&lt;p&gt;Having worked with these beasts for decades, this appearance — a backlit LCD with a mostly blank or dark block display — almost certainly indicates a problem with the processor card, because enough of the logic board is working to power on the front panel but the CPU isn't running. Typically this is because the processor wormed itself out of the board and needs to be reseated, but you can also get something like this if the card went bad, and less commonly if the ROM stick isn't installed correctly.&lt;/p&gt;However (moving the monitor cord out of the way), we have a problem: we can't get the drawer to open wide enough to pull out and reseat the CPU card. We'll have to take the drawer off. As usual, removing the drawer is relatively easy (it's getting it back on that's the trick). Two plastic latches on the underside of the drawer, fortunately still also in good nick, slip into two gaps in the metal slide rails. Supporting the drawer with your other hand so it doesn't fall off, push each latch in and push back the rail to disengage it. The drawer then lifts off and can be put aside, preferably onto an antistatic mat. Here's the inside, where the logic board connects. The powerplane connector is at the bottom. The board at the top is the right half of the mezzanine (codenamed "HENDY"), with the slot for the logic board's card edge and a connector for the front panel. The mezz is a "horseshoe" that straddles both sides, better shown here with the top off. The other side has connectors for the NMI and reset buttons, floppy drive and SCSI busses. Those bus connectors come from the SCSI backplane on the other side, here with that panel off (which can now be removed because the drawer is out). Both the front (and in the 700, the rear) drive connectors hook up here. I'd forgotten I'd disconnected bus 1 when I stored it, so I later reconnected the cable to J11 before closing this back up. If you don't do this, besides drives not working, you may get spurious errors warning that the drive fan failed or is not present (see later on). The problem with the sliding rails turned out to be two-fold, first some stuck broken pieces of plastic which I removed, and second whatever lubricant Apple had used which over the decades had desiccated into gluey, copper-coloured gunk. I cleaned off most of the ick and then used WD-40 white lithium (not regular WD-40) on the rails and worked it back and forth into the bearings. If it's good enough for your garage door opener, it's good enough for your unusual Apple server. After about ten minutes of spraying and sliding, both rails now move smoothly and reach their maximum extents. I was very careful to wipe off any excess so there wouldn't be a mess later. Now to remount the drawer. This is not well-explained in the official Apple service manual, so I'll be more explicit here. On each slide are two small metal hooks. If you don't see the hooks, pull the slides forward until you do. On each slide, one of the hooks goes into a metal notch on the two metal rails mounted on the back of the drawer. On the top slide, the bottom hook engages; on the bottom slide, the top one does. Once you've done that, then while using one hand to support the drawer, pull each slide forward until it engages with each of the black latches (it will click into position). Now we can pull the drawer all the way out, pull out the 200MHz card and try to reseat it using the card guides. You shouldn't need to force it in, though it does need a bit of grunt to ensure both rows of contacts get into the slot.&lt;p&gt;Closing the drawer likewise doesn't require force per se, but the rear keylock will not turn unless you have the board fully engaged with the mezz and powerplane. There are thumbscrews but they don't really make much difference for this. Sometimes you have to slam it in a couple times, making sure the thumbscrews are completely loosened and out so that they don't get in the way. When the logic board is properly engaged and the drawer is fully closed, it should be easy to turn the rear key.&lt;/p&gt;Unfortunately reseating the processor card didn't fix it, so the next step is to try a different one. I'm saving the other 200MHz card as a spare for stockholm, but we have several 150MHz cards, so I selected one of those. And it starts up! We have messages on the LCD showing the 150MHz 604 (with 50MHz bus), 32MB of parity RAM and 1MB of L2 cache were all properly detected. The reported ROM version of 1.1.22 is consistent with production ROMs as shipped. If you connect to Port 2 on the rear at 9600bps during POST, you may see additional messages. Since the front key is in the service position, it goes into a service boot, first trying the CD (looking for a bootloader) and then looking for the file diags on a floppy disk. We have provided the machine with neither, and nothing else is available, so the server drops to an Open Firmware prompt.&lt;p&gt;Open Firmware is the boot environment for all Power Macs starting with the first beige PCI systems. Originating at Sun as OpenBoot, Open Firmware provides an interactive Forth interpreter, which is used for interpreting cross-platform FCode (bytecode) in device ROMs but can also be used for development, and makes available a built-in means of managing and storing settings for installed hardware. In Macs of this generation it was generally invisible to the user except if specifically enabled or requested — remember this for later as well — and the Apple Network Server was the earliest Power Mac (well, derived, anyway) system where Open Firmware was explicitly user-facing. Open Firmware survives today largely in the form of OpenBIOS.&lt;/p&gt;The diags file in question could be theorically any XCOFF binary, but it's specifically looking for this, the Network Server Diagnostic Utility. This came on a floppy disk in the ANS accessory pack. We'll use the NSDU to check the rest of our configuration. We could reboot the server, but we can just start it from the Open Firmware prompt directly with boot fd:diags. You can also see some of the system's current Open Firmware environment variables; we'll have much more to say about those when we finally get to experimenting with the ROMs. Sorry about the screen photographs but the default refresh rate does not agree with my VGA capture box. The NSDU is also a single XCOFF binary. When it starts up it prints a summary of installed hardware and the results from initial POST. It has detected all RAM is parity, detected the CPU speed and internal L1, detected the external L2, detected both power supplies, and correctly shows installed RAM, no PCI cards, and most of the sensors. The only one that's wrong is the Drive Fan reads "Off" but that's because I hadn't remembered to reconnect the disconnected SCSI bus cable to the backplane. We'll now run the complete system test (option 3). The tests scroll up on the screen, here showing the two internal SCSI controllers and the LCD. The video chip also gets exercised for conformance with various test displays. In the end, we have a clean bill of health, both on the screen ... ... and on the LCD. There's one more thing left to do to certify operation: a test boot of AIX from the CD. ANS AIX, codenamed Harpoon, is specific to the Apple Network Server — you can't use a regular AIX CD, and installing Base Operating System packages from such a CD is likely to corrupt your install (don't ask me how I know this). Most systems shipped with this CD in the accessory pack, version 4.1.4.1. 4.1.2 was used on preproduction servers but I've never seen it myself. Apple later issued version 4.1.5, which fixed many bugs and is strongly recommended. Booting from the CD. The LCD is live during an AIX boot, showing the march of AIX bootloader codes. They are the same codes as most IBM servers of this era. Finally, the AIX kernel comes up, asking to define the system console. This proves our hardware (and CD-ROM) both work and that its native AIX can start, which means any weird behaviour after this point is more likely than not due to what we're testing.&lt;p&gt;We're finally ready to begin. Let's enumerate the currently known Network Server ROMs. In these pre-Open Firmware 3 ROMs, the ROM version and the Open Firmware version are the same. For comparison purposes, PCI Power Macs of this era were typically 1.0.5.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Pre-production ROMs. Currently one version is known, 1.1.20.1. These were used to boot Mac OS and AIX (and possibly another operating system I'll mention), but the internal video and SCSI controllers are not supported in Mac OS. This was the version that turned out to be on my flash DIMM.&lt;/item&gt;&lt;item&gt;Production ROMs. Currently one version is known, 1.1.22. These only boot AIX, though systems with these ROMs can also boot NetBSD and certain Linux distributions. I won't talk further about that in this article, but if I were to use a non-AIX operating system on a production ANS, it would almost certainly be NetBSD even though it doesn't currently support internal video or the on-board Ethernet.&lt;/item&gt;&lt;item&gt;Prototype Mac OS ROMs. Currently one version is known, 2.0. These contain ROM drivers for the internal video and SCSI controllers, and are the only known ROMs to fully support all internal devices in Mac OS. This is not currently in my possession — though I'd love to get one! — but at least one person has created replica ROMs from a dump graciously made available by their owner, and then used them to successfully boot their own machine.&lt;/item&gt;&lt;item&gt;Prototype Windows NT ROMs. These ROMs also appear to be required for multiprocessor support. Currently three versions are known, 2.26b6, 2.26b8 (not dumped, referred to on the LinuxPPC-ANS list) and 2.26NT, with relatively small changes between them.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;disk2:aix Device isn't there! can't OPEN: /bandit/53c825@11/sd@2,0:aixOpenFirmware1.1.22 To continue booting from the default boot device type: BOOT&amp;lt;return&amp;gt; ok 0 &amp;gt; dev / ok 0 &amp;gt; ls 004308E0: /PowerPC,604@0 00430B90: /l2-cache@0,0 004313F0: /chosen@0 00431520: /memory@0 00431668: /openprom@0 00431728: /AAPL,ROM@FFC00000 00431968: /options@0 00431E40: /aliases@0 00432080: /packages@0 00432108: /deblocker@0,0 004329A8: /disk-label@0,0 00432F18: /obp-tftp@0,0 00435B28: /mac-files@0,0 00436410: /mac-parts@0,0 00436D30: /aix-boot@0,0 00437488: /fat-files@0,0 00438DF0: /iso-9660-files@0,0 004398E0: /xcoff-loader@0,0 0043A410: /terminal-emulator@0,0 0043A4A8: /bandit@F2000000 0043B500: /53c825@11 0043DDE0: /sd@0,0 0043EA48: /st@0,0 0043F8A8: /53c825@12 00442188: /sd@0,0 00442DF0: /st@0,0 00444288: /gc@10 004446C0: /53c94@10000 00446460: /sd@0,0 00447248: /st@0,0 004480C8: /mace@11000 00449248: /escc@13000 004493A0: /ch-a@13020 00449AD8: /ch-b@13000 0044A210: /awacs@14000 0044A2F8: /swim3@15000 0044BB88: /via-cuda@16000 0044D088: /adb@0,0 0044D178: /keyboard@0,0 0044D950: /mouse@1,0 0044DA00: /pram@0,0 0044DAB0: /rtc@0,0 0044DFE0: /power-mgt@0,0 0044E1B8: /nvram@1D000 00462BC8: /lcd@1C000 00450780: /pci106b,1@B 00450958: /54m30@F 0044E7D0: /bandit@F4000000 00462350: /pci106b,1@B 0044FF28: /hammerhead@F8000000 ok 0 &amp;gt;&lt;/quote&gt;&lt;p&gt;We'll do the first and last of these in the remainder of this article. Since the Bible says the first shall be last and the last first, let's begin with the final known ANS ROM, 2.26NT.&lt;/p&gt;&lt;head rend="h3"&gt;2.26NT Windows NT ROMs&lt;/head&gt;Hancock's late 1996 announcement that the Apple Network Server would optionally run Windows NT caught many industry observers by surprise. Although NT 3.x and 4.x were designed to be architecture-independent and ran on processors as diverse as MIPS and DEC Alpha as well as 32-bit x86, the PowerPC build had been limited to low-volume IBM hardware and never officially ran on Power Macs. Still, it was clear to Apple that NT would be very important in the industry and felt supporting it would broaden the appeal of the server line — or at least soften the impact of its sticker price. Importantly, NT support would not have to wait for Apple's then-expected CHRP Power Macs: reworked ROM support could enable the ANS to boot it "now." (In the end, Jobs eventually scuttled the CHRP initiative to starve the Mac clones; the upcoming New World Macs were ultimately an incompatible blend of CHRP and the earlier PReP standard instead.) When Jobs talked Gil Amelio into canning the ANS as well, the ROM initiative naturally went out the window with it. However, while the existing 2.0 Mac OS ROMs are only known on an unmarked development flash stick similar to mine, these final 2.26NT ROMs appear almost production-ready with fully printed labels, suggesting they had reached a very late stage of development. (The "ESB" tag indicates a prototype designation — consistent with Shiner, the ANS' beer-themed codename during development, ESB stands for "Extra Special Bitter.") These ROMs were kindly sent to me by a former Apple employee at the Elk Grove, CA campus. Sadly this person no longer has the 700 they were running in, but attests to the fact NT did run and apparently even ran well, adding, "I’m pretty certain that the NT ROM was the Apple business systems team trying to find a way to keep their product from being canceled completely. Motorola had just shipped their PowerStack NT machines a few months previously and they were garbage compared to the ANS when it came to field service and expandability." (So true!)&lt;p&gt;The NT ROM DIMM simply replaces the production ROM DIMM in the slot. We'll power it up with the front key set to service just in case.&lt;/p&gt;On the LCD, not only is the version displayed, but as mentioned this is also one of the ROMs that checks for a second CPU (if we had one of the prototype dual-CPU cards, that is — contact me, I'm interested if you've got one to get rid of!).&lt;p&gt;Our first order of business is to immediately dump these ROMs for posterity (they are posted on the group thread at Tinker Different). This can be done without a chip reader by having Open Firmware itself dump the contents in hex over one of the serial ports, and then post-processing the resulting output.&lt;/p&gt;We start by switching the console to a serial port using setenv input-device ttya:57600 and setenv output-device ttya:57600 (ttya is port 2 on the back) followed by reset-all to commit the settings. Then, on a connected terminal program at 57600bps capturing the output (I did something like picocom -b57600 /dev/cu.usbserial-10 | tee out), you can either enter&lt;quote&gt;h# ffc00000 h# 00400000 dump&lt;/quote&gt;&lt;p&gt;which dumps the contents with the addresses, or if you don't need those, you can try something faster but a little more complicated like (suggested by @joevt)&lt;/p&gt;&lt;quote&gt;0 ffc00000 do i 3f and 0= if cr then i l@ 8 u.r 4 +loop cr&lt;/quote&gt;&lt;p&gt;which emits 64 bytes per line. The ANS ROM is already visible in the default memory map, so it can be dumped immediately.&lt;/p&gt;This process is not very quick, but when it finishes you would take the transcript and turn the hex strings back into binary (Perl's pack function is perfect for this), which if properly captured would yield a file exactly 4,194,304 bytes long. Something like this should work on the 64-bytes-per-line output:&lt;quote&gt;#!/usr/bin/perl select(STDOUT); $|++; while(&amp;lt;&amp;gt;) { chomp; chomp; next unless (length == 128); print STDOUT pack("H*", $_); }&lt;/quote&gt;&lt;p&gt;which the Perl golfers will probably have turned into a handful of indecipherable bytes in the comments shortly. After the process is complete, setenv input-device kbd, setenv output-device screen and reset-all will move the console back to the ADB keyboard and VGA port.&lt;/p&gt;&lt;p&gt;There are a number of interesting things about this ROM, though most of it (about the first 3MB) is still identical to the 9500's.&lt;/p&gt;The default boot device remains disk2:aix, but there are apparently NT-specific words in this version of Open Firmware like nt-gen-configs, nt-gen-config-vars, init-nt-vars, maybe-create-nt-part, etc. Their Forth code looks like this:&lt;quote&gt;ok 0 &amp;gt; see nt-gen-configs defer nt-gen-configs : (nt-gen-configs maybe-read-nt-part get-first-str begin while/if _cfgval _cfgvallen encode-string _cfgname count set-option get-next-str repeat ; ok 0 &amp;gt; see nt-gen-config-vars defer nt-gen-config-vars : (nt-gen-config-vars maybe-read-nt-part get-first-str begin while/if _cfgname count _configname pack drop ['] string-var gen-config-var get-next-str repeat ; ok 0 &amp;gt; see maybe-read-nt-part : maybe-read-nt-part init-nt-vars osnv-good? if read-part else nvram-buffer nv-buffer-size erase then ; ok 0 &amp;gt; see init-nt-vars : init-nt-vars nvram-buffer 0= if /osnv dup to nv-buffer-size alloc-mem to nvram-buffer nvram-buffer nv-buffer-size erase nvram-size alloc-mem to _cfgval nvram-size to _cfgval-size _cfgval _cfgval-size erase then ;&lt;/quote&gt;&lt;p&gt;From this you can get the general notion that these allocate a block of NVRAM for NT-specific configuration variables. There are also words for direct mouse support.&lt;/p&gt;&lt;p&gt;If we list out packages, we see other interesting things.&lt;/p&gt;&lt;quote&gt;ok 0 &amp;gt; dev / ok 0 &amp;gt; ls FF8362C0: /PowerPC,604@0 FF836570: /l2-cache@0,0 FF836DA8: /chosen@0 FF836ED8: /memory@0 FF837020: /openprom@0 FF8370E0: /AAPL,ROM@FFC00000 FF8373A0: /options@0 FF837878: /aliases@0 FF837AF0: /packages@0 FF837B78: /deblocker@0,0 FF8383E0: /disk-label@0,0 FF838988: /obp-tftp@0,0 FF83BFA0: /mac-files@0,0 FF83C7A0: /mac-parts@0,0 FF83D078: /aix-boot@0,0 FF83D808: /fat-files@0,0 FF83F608: /iso-9660-files@0,0 FF840390: /xcoff-loader@0,0 FF840DB8: /pe-loader@0,0 FF8416A0: /terminal-emulator@0,0 FF841738: /bandit@F2000000 [...]&lt;/quote&gt;&lt;p&gt;Yes, there is a pe-loader package — as in Portable Executable, the format first introduced in Windows NT 3.1 to replace the old 16-bit New Executable .exe, and today the standard executable format for all modern versions of Windows. Here are some pieces of that:&lt;/p&gt;&lt;quote&gt;ok 0 &amp;gt; see boot : boot "boot " boot|load init-program go ; ok 0 &amp;gt; see boot|load : boot|load _reboot-command pack drop set-diag-mode ['] (init-program) to ^-7DA998 carret word count (load) ; ok 0 &amp;gt; see init-program defer init-program : (init-program) 0 to ^-7DB118 loadaddr "\ " comp 0= if "evaluating Forth source" type loadaddr loadsize evaluate loadaddr loadmapsize do-unmap true to ^-7DB118 else loadaddr 2c@-be F108 = if "evaluating FCode" type loadaddr 1 byte-load loadaddr loadmapsize do-unmap true to ^-7DB118 else loadaddr 2c@-be 1DF = if "loading XCOFF" type 0 0 "xcoff-loader" $open-package "init-program" 2 pick $call-method close-package else loadaddr 2c@-be F001 = if "Loading PE/COFF" type cr 0 0 "pe-loader" $open-package "init-program" 2 pick $call-method close-package else "unrecognized Client Program format" type then then then then ; ok 0 &amp;gt; dev /packages/pe-loader ok 0 &amp;gt; words init-program close open map-space header-size new-load-adr stack-size scthdr.size &amp;gt;pes.rawptr &amp;gt;pes.size_raw &amp;gt;pes.rva &amp;gt;pes.virt_size &amp;gt;pes.name opthdr.size &amp;gt;peo.no_dir &amp;gt;peo.loader_flags &amp;gt;peo.heap_com_size &amp;gt;peo.heap_res_size &amp;gt;peo.stack_com_size &amp;gt;peo.stack_res_size &amp;gt;peo.head_size &amp;gt;peo.image_size &amp;gt;peo.file_algn &amp;gt;peo.scns_algn &amp;gt;peo.image_base &amp;gt;peo.sndata &amp;gt;peo.sntext &amp;gt;peo.entry &amp;gt;peo.bsize &amp;gt;peo.dsize &amp;gt;peo.tsize &amp;gt;peo.magic filehdr.size &amp;gt;pe.nscns &amp;gt;pe.machine ok 0 &amp;gt; see init-program : init-program real? little? 0= or real_base 700000 u&amp;lt; or "load-base" eval 700000 u&amp;lt; or if "false" "real-mode?" $setenv "true" "little-endian?" $setenv @startvec &amp;gt;ramsize @ h#100000 - dup (u.) "real-base" $setenv h#100000 - (u.) "load-base" $setenv cr "RESETing to change Configuration!" type cr force-reboot then loadaddr filehdr.size + &amp;gt;peo.image_base @ dup to new-load-adr "image_base " type u. cr loadaddr filehdr.size + &amp;gt;peo.head_size @ to header-size new-load-adr stack-size - loadsize h#fff + h#-1000 and stack-size + map-space new-load-adr stack-size - stack-size 0 fill loadaddr header-size + new-load-adr loadsize header-size - move new-load-adr loadsize header-size - bounds do i ^dcbf i ^icbi 14 +loop loadaddr loadsize do-unmap 0 4000 map-space install-interrupt-vectors ci-regs h#100 h#deadbeef filll new-load-adr stack-size - FF00 + spsv reg! new-load-adr sasv reg! new-load-adr srr0sv reg! ['] cientry argsv reg! 0 crsv reg! msr@ 17FFF and srr1sv reg! state-valid on ?state-valid ; ok 0 &amp;gt;&lt;/quote&gt;&lt;p&gt;Your eyes deceive you not: when configured to boot NT, this ROM runs the machine little-endian — which at the time would have been a first for a Power Mac as well, though this is the only way that Windows NT on PowerPC ever ran. 32-bit PowerPC has little-endian support through a little-endian bit in the machine state register or by setting a flag on memory pages in the MMU (which is how Virtual PC ran) or at the instruction level with byteswapping, but to this point all official Power Mac payloads had run big.&lt;/p&gt;That means this ROM may be able to run PowerPC Portable Executables directly, so I got out my OEM Windows NT 4.0 kit to see. I ran those words just in case they made a difference and then tried to do a naïve boot directly from the Windows NT 4 CD. This looks something like boot disk0:,\ppc\setupldr (don't forget the colon and the comma). And, well, it can indeed load it and has a sensible image base address — but immediately crashes with a CLAIM failed, suggesting it couldn't map memory for the executable image, even though 32MB of RAM should have been more than enough to start Windows NT Setup. You can see from init_program above that it provides computed values for Open Firmware load-base and real-base, so I imagine they were tailored specifically to boot NT (and NT Setup), but nevertheless I couldn't get past this point.&lt;p&gt;[In the comments, Andrei Warkentin asked if it could boot the veneer from the CD. It parses ...&lt;/p&gt;... but it does not run either.]&lt;p&gt;To be sure, we almost certainly don't have all the pieces together for a successful NT boot yet. One thing I could find no trace of in the ROM was ARC. We talked about the rise and fall of ARC in our SGI Indigo2 refurb weekend, but even though IBM, Sun, HP, Intel and Apple were never members of the Advanced Computing Environment consortium, Microsoft was. As a consequence virtually any machine capable of booting Windows NT would have some means of system specification through ARC (this particular historical vestige persisted until Windows Vista). On DEC Alphas, this was implemented in firmware, which is why you need the right firmware to boot it; for the IBM Power Series workstations and laptops, the ARC console was on floppy disk. It is highly likely the ANS also had an ARC console of its own, and since it doesn't appear to be in the ROM, there must have been a floppy or CD that provided it which we don't have.&lt;/p&gt;&lt;p&gt;Additionally, Windows NT relies on a hardware abstraction layer (HAL) which operates between the physical hardware and the rest of the operating system. The HAL is even more lower-level than device drivers, implementing functions like allowing device drivers to access ports in a more standardized fashion, abstracting away interrupt management, and unifying firmware interfaces and DMA. There are HAL DLLs on the 4.0 CD for various IBM (Types 6015, 6020, 6030, and 6070), FirePower (Powerized MX and ES) and Motorola (PowerStack 2 and Big Bend) PowerPC systems, but none for any Power Mac. The HAL necessarily gets loaded early in the setup process, often from another floppy, and you won't be able to successfully bring up Windows NT without it. Although there are apocryphal references to "halbandit" out there and this name is likely a reference to the ANS HAL, we don't have it either. (While it should be possible to get the Windows NT for Power Mac port running on the ANS, per the maintainer its current HAL relies on Mac OS support, so it wouldn't actually be using this ROM.)&lt;/p&gt;&lt;p&gt;Do you have any of these pieces? Post in the comments, or if you'd prefer to be anonymous, drop me an E-mail at ckaiser at floodgap dawt com.&lt;/p&gt;&lt;p&gt;Even without Jobs' looming axe, NT on the ANS was probably ill-starred anyway no matter how well it ran. The unique persistence of Windows NT on the DEC Alpha was a side-effect of primary architect Dave Cutler strongly basing NT on DEC VMS, an aspect hardly lost on DEC's legal team, to the point where various filenames and directory structures in the NT codebase even directly matched those in VMS. To avoid a lawsuit Microsoft paid off DEC, helped promote VMS, and committed to continued support for NT on Alpha, which remained until the beta phase of Windows 2000. This situation was absolutely not the case with PowerPC: IBM was so irked with Microsoft over OS/2 and NT's adoption of an expanded Windows API instead that its support for RISC NT was never more than half-hearted. Likewise, the only MIPS hardware that ran NT were DECstations — quickly cancelled by DEC in favour of Alpha — and directly from MIPS, the Magnum R4000 — also cancelled to avoid competition with Silicon Graphics' IRIX hardware when SGI bought them out. At that point, and already not favourably predisposed to Microsoft's initiative, IBM didn't see any value in continuing to support Windows NT on PowerPC and Amelio's Apple definitely didn't have the resources to do so themselves.&lt;/p&gt;&lt;head rend="h3"&gt;1.1.20.1 preproduction ROMs&lt;/head&gt;&lt;p&gt;Let's rewind a bit here and talk about booting Mac OS on the ANS, given that's how all this got started in the first place. The stock 1.1.22 ROM blocks booting it at the Open Firmware level:&lt;/p&gt;&lt;quote&gt;ok 0 &amp;gt; dev /AAPL,ROM ok 0 &amp;gt; words load open ok 0 &amp;gt; see open : open "MacOS is not supported. " type false ; ok 0 &amp;gt; see load : load real_base 400000 &amp;lt;&amp;gt; virt_base -800000 &amp;lt;&amp;gt; or real? or little? or if 10 base ! "FFFFFFFF" "real-base" $setenv "FFFFFFFF" "virt-base" $setenv "false" "real-mode?" $setenv "false" "little-endian?" $setenv "boot /AAPL,ROM" !set-restart cr "RESETing to change Configuration!" type cr reset-all then ; ok 0 &amp;gt;&lt;/quote&gt;&lt;p&gt;If you try anyway with boot /AAPL,ROM, it won't work.&lt;/p&gt;You can force it by patching out those Forth words, but even though it will try to start, it will immediately crash and return you to the Open Firmware prompt.&lt;p&gt;Still, repeated reports back in the day swore they could do it. A couple people tried using 9500 ROMs, noting they would get a picture on an IMS Twin Turbo video card, though there was disagreement on whether it could actually boot anything and the different Bandit mapping almost certainly assured this wouldn't get off the ground. A few other people had intermittently acquired remaindered ANS systems from Apple that did indeed boot MacOS (retrospectively they very likely had 2.0 ROMs in them). More interesting, however, were reports that the Network Servers had previously booted Mac OS during development.&lt;/p&gt;One of these early ROMs ended up sitting in a box in my closet for about 20 years. Apple Austin (the address on the box is no longer an Apple building) was the last stand of the Network Server, where a number of systems remained serving content as late as 2005. Per an Apple employee on the LinuxPPC-ANS list in March 2003, "Our team here at Apple decommissioned over 40 Shiners early last year. They used to be the backbone of the Apple Support site [that is, the former www.info.apple.com] serving all the software downloads, all the images for the support site and performing much of the heavy lifting behind the scenes that made our website the highest rated support site in the industry." About twenty of them were sold to list members — I was a starving medical student at the time and couldn't afford either the cash or the space — but I did make a deal to pick up some of the spare parts. I got two 10Mbit Ethernet cards and some 68-pin SCSI interconnects, and also some RAM. I didn't look too closely at what was in the box otherwise. I am told the servers that did not sell were crushed. :(&lt;p&gt;It wasn't until I was looking through my box for a spare ROM to see if I could get it converted to 2.0 that I found this ROM stick in the bottom of the box. It was not labeled and if I hadn't seen a picture of the 2.0 ROM, I probably wouldn't have recognized it for what it was.&lt;/p&gt;This was how the 2.0 ROM looked in the Apple employee's Deep Dish that booted OS 9. Apple used flashable DIMMs exactly like this for Power Mac development generally; the form factor will fit in any beige Power Mac. (We don't know how to flash these yet but I know people are working on it.) Still, the fact it came from the Network Server afterlife meant it probably wasn't any ordinary DIMM, so now let's give it a spin. It comes right up ... and it's a pre-production ROM! This is currently the earliest known ROM available for the Network Server. I have no idea how it got in that box; I didn't request a spare ROM DIMM from them, but it was down at the bottom with the network cards and the other pieces that I did order. I immediately dumped this one also to compare. Our Apple employee with the 2.0 ROMs also had a 1.1.20.1 set, and the hashes match his dump, so this is the same.&lt;quote&gt;disk2:aix Device isn't there! can't OPEN: /bandit/53c825@11/sd@2,0:aixOpenFirmware1.1.20 To continue booting the MacOS type: BYE&amp;lt;return&amp;gt; To continue booting from the default boot device type: BOOT&amp;lt;return&amp;gt; ok 0 &amp;gt; dev / ok 0 &amp;gt; ls FF830648: /PowerPC,604@0 FF8308F8: /l2-cache@0,0 FF831158: /chosen@0 FF831288: /memory@0 FF8313D0: /openprom@0 FF831490: /AAPL,ROM@FFC00000 FF8316F0: /options@0 FF831BD0: /aliases@0 FF831E10: /packages@0 FF831E98: /deblocker@0,0 FF832738: /disk-label@0,0 FF832CA8: /obp-tftp@0,0 FF8358B8: /mac-files@0,0 FF8361A0: /mac-parts@0,0 FF836AC0: /aix-boot@0,0 FF837218: /fat-files@0,0 FF838B80: /iso-9660-files@0,0 FF839670: /xcoff-loader@0,0 FF83A1A0: /terminal-emulator@0,0 FF83A238: /bandit@F2000000 FF83B290: /53c825@11 FF83DB70: /sd@0,0 FF83E7D8: /st@0,0 FF83F638: /53c825@12 FF841F18: /sd@0,0 FF842B80: /st@0,0 FF844018: /gc@10 FF844450: /53c94@10000 FF8461F0: /sd@0,0 FF846FD8: /st@0,0 FF847E58: /mace@11000 FF848FD8: /escc@13000 FF849130: /ch-a@13020 FF849868: /ch-b@13000 FF849FA0: /awacs@14000 FF84A088: /swim3@15000 FF84B918: /via-cuda@16000 FF84CE18: /adb@0,0 FF84CF08: /keyboard@0,0 FF84D6E0: /mouse@1,0 FF84D790: /pram@0,0 FF84D840: /rtc@0,0 FF84DD70: /power-mgt@0,0 FF84DF48: /nvram@1D000 FF8628D8: /lcd@1C000 FF850490: /pci106b,1@B FF850668: /54m30@F FF84E560: /bandit@F4000000 FF862060: /pci106b,1@B FF84FCA8: /hammerhead@F8000000 ok&lt;/quote&gt;&lt;p&gt;This ROM specifically advertises it can boot Mac OS, and there is no block in Open Firmware.&lt;/p&gt;&lt;quote&gt;0 &amp;gt; dev /AAPL,ROM ok 0 &amp;gt; words load open ok 0 &amp;gt; see open : open true ; ok 0 &amp;gt; see load : load real_base 400000 &amp;lt;&amp;gt; virt_base -800000 &amp;lt;&amp;gt; or real? or little? or if 10 base ! "FFFFFFFF" "real-base" $setenv "FFFFFFFF" "virt-base" $setenv "false" "real-mode?" $setenv "false" "little-endian?" $setenv "boot /AAPL,ROM" !set-restart cr "RESETing to change Configuration!" type cr reset-all then ?cr "MacOS is currently unsupported, use at your own risk." type &amp;lt;bye&amp;gt; ; ok 0 &amp;gt;&lt;/quote&gt;&lt;p&gt;However, if you enter bye as directed with a CD in the internal CD-ROM, the screen will go blank and nothing will happen.&lt;/p&gt;&lt;p&gt;The clue comes from those who claimed they got the system partially running with 9500 ROMs: the 9500 has no on-board video and always came from Apple with a video card, so they added a video card. With that, they got a picture on the video card. No Mac OS support for the internal Fast Wide SCSI nor the Cirrus Logic video is implemented in this ROM, and as we mentioned earlier, having never been used in any prior Apple product, the operating system proper doesn't know what they are either. In fact, the Cirrus Logic video is gimped even in AIX — the ANS Hardware Developer Notes say that the video controller provides "only a little-endian window into the packed-pixel frame buffer, hence Big Endian [sic] operating systems are limited to 8 bits per pixel unless low-level transformation routines are written."&lt;/p&gt;&lt;p&gt;For a server that's probably good enough. For a really powerful under-the-desk workstation, that stinks. Let's add a video card.&lt;/p&gt;I chose an IMS Twin Turbo 128MA, nearly the pinnacle of 2D classic Mac performance, and one of the BTO options Apple offered for the 9500. I also put as much high-capacity parity RAM in it as I could get my hands on. The biggest parity FPM DIMMs I have in stock were 64MB. You may need to examine your RAM sticks carefully to make sure you aren't actually putting in non-parity (the stick in the bottom picture is not parity). These two got me 128MB to start. Initially I could only scrape together 192MB of parity RAM from what I had left and the 16MB upgrade kits, so I started with that.&lt;p&gt;For a test boot, I decided to try the external DB-25 BlueSCSI dongle I had left over from when we experimented with Novell NetWare on the Power Macintosh 6100, for two reasons: it already had a bootable image of 7.6 on it I was using for another project, and it also has an image of Cyberpunk, Apple's codename for the very alpha port of NetWare to the Power Mac originally intended for Shiner systems. Recall that this Forth word exists in every known ANS ROM, even the late 2.26 NT ROM, with the notable exception of the 2.0 MacOS ROMs:&lt;/p&gt;&lt;quote&gt;0 &amp;gt; see setenv-netware : setenv-netware "false" "real-mode?" $setenv "ttya:19200" "input-device" $setenv "ttya:19200" "output-device" $setenv ?esb if "scsi-int/sd@2:0" else "scsi-int/sd@3:0" then "boot-device" $setenv ; ok&lt;/quote&gt;&lt;p&gt;I wasn't sure if this version of Cyberpunk, intended for Piltdown Man machines (i.e., the 6100 and allies), would start on it but if any ROM could, I felt sure these beta ROMs had a decent chance. I set the Open Firmware input-device back to the default kbd and the output-device back to the default screen and brought it back up again.&lt;/p&gt;Notice that it will still try to boot AIX as default — you would need to change the boot device to /AAPL,ROM to autoboot Mac OS, and this will be lost if the board NVRAM gets reset.&lt;p&gt;At this point, we plug the monitor into the Twin Turbo card and blindly type bye. Yes, you can set the Open Firmware output-device directly to the video card — something like /bandit@F2000000/IMS,tt128mbA@D would work for slot 1 — but this isn't necessary to boot ...&lt;/p&gt;... because the Toolbox ROM will automatically use the card anyway and we get our long awaited Happy Mac. This is analogous to the situation on a real 9500 where Open Firmware 1.0.5 isn't on the console; by default it's on the serial ports. Another big heaping bowl of foreshadowing for you to keep in mind. I left the Cyberpunk image on SCSI ID 0 to see what it would do, though I was pretty sure it would fail, and it did. This image has System 7.1.2 on it and no PCI Power Mac officially supported anything earlier than 7.5.2. But, rearranging the IDs so that the 7.6 image was on ID 0 and the Cyberpunk image was in ID 1, 7.6 will boot! Let's switch to proper screenshots. Unsurprisingly, 7.6's relatively underpowered System Profiler identifies the system as Gestalt ID 67, which matches the 9500, 9515, 9600 and the WGS 9650, but gives us little more detail than that. For a deeper dive we'll fire up TattleTech which was already on this disk image. TattleTech reports the same Gestalt ID. Cursorily scanning the Gestalt ID list, they all look pretty similar to a Mac of that generation booting 7.6. There is little hint here that this computer is anything other than a 9500. On the other hand, the PCI slot layout is a little different. Like the 9500 and 9600, the ANS has two Bandits (there is even space in the memory map for a third, which remains unimplemented) and thus two PCI busses, but the 9500/9600 assign three slots each to each Bandit (Grand Central handling non-PCI devices is on the first). In the ANS, the first Bandit also carries the internal SCSI and internal video as well as Grand Central, so it only handles two slots, with slot 3 going to the second Bandit. This rearrangement manifests here in TattleTech showing just two slots on the first bus. The ROM checksum also doesn't match. 9500 ROMs contain an Apple checksum of either $96CD923D or $9630C68B (the 9600 might also have $960E4BE9 or $960FC647), but this ROM checksums as $962F6C13. The same checksum appears in the 1.1.22 production ROM, which still contains a substantial portion of the 9500 v2 ROM even though it definitely won't boot Mac OS. This likely represents held-over code that simply no one bothered to remove. We can also see that the two internal SCSI busses are detected, even if they aren't bootable with this ROM, and they are properly probed as a 53C825. The 53C94 used for the external SCSI which we are running from likewise appears. Finally, the built-in AAUI Ethernet is detected as well (MACE, via Grand Central). I point this out specifically because ... ... it doesn't seem to work. While both AAUI dongles I tried showed working LEDs and activity on the network, 7.6 refused to enable the port. This did work in AIX at one point when I used it to sub for stockholm while investigating a hardware fault, but now it won't netboot either from Open Firmware. I'm concluding the MACE embedded in CURIO works but the PHY it connects to must have crapped out in storage. Since we have the Cyberpunk image up, I tried running the PDMLoader just to see. Recall from our NetWare on Power Macintosh article that the PDMLoader is, at least on NuBus Power Macs, what starts the NWstart kernel and enters NetWare. Among other things it provides a fake Open Firmware environment to allow those Macs to resemble a Shiner ESB unit for demonstration purposes, which was the intended target hardware. Early Shiners reportedly could boot it directly. Unsurprisingly, the PDMLoader checks that it was started on a supported Mac and (based on the Gestalt ID) finds our franken-ANS wanting.&lt;p&gt;If we look back at our definition for setenv-netware, however, we can see NetWare was expected to run from a so-called "partition zero" loader. This is like it sounds: a runnable binary occupies partition zero of a bootable disk, usually XCOFF, and is loaded as blocks into memory by Open Firmware and executed. Unfortunately, the Installer we used for Cyberpunk didn't support creating this, and it wouldn't have been necessary for a NuBus Power Mac anyway which doesn't boot like that. As it's a regular XCOFF binary otherwise, I tried putting NWstart onto a plain physical ISO 9660 CD and fed that to 1.1.20.1, but ...&lt;/p&gt;&lt;quote&gt;disk2:aix Device isn't there! can't OPEN: /bandit/53c825@11/sd@2,0:aixOpenFirmware1.1.20 To continue booting the MacOS type: BYE&amp;lt;return&amp;gt; To continue booting from the default boot device type: BOOT&amp;lt;return&amp;gt; ok 0 &amp;gt; boot disk0:,\NWSTART. disk0:,\NWSTART. loading XCOFF tsize=2A14A1 dsize=90028 bsize=10E17C entry=843EC SECTIONS: .pad 00000000 00000000 00000E14 000001EC .text 00000000 00000000 002A14A1 00001000 .pad 00000000 00000000 00000B5F 002A24A1 .data 00000000 00000000 00090028 002A3000 .bss 00090028 00090028 0010E17C 00000000 .pad 00000000 00000000 00000FD8 00333028 .loader 00000000 00000000 0003BC04 00334000 loading .text, done.. loading .dataCLAIM failed ok&lt;/quote&gt;&lt;p&gt;... while the ROM can read the file from disc and it will load, it halts with the same memory claim error I got trying it on the 500 with production ROMs, even after fiddling with the load and real base values to accommodate a large kernel. It's possible this kernel won't run outside of the PDMLoader environment and the Shiners used a different one, but that's not on the CD I have. Oh well.&lt;/p&gt;Since the on-board Ethernet was shot, I decided to see if I could get it working with one of the Ethernet cards I ordered from Apple Austin way back when. This is a 10Mbit "Apple Ethernet PCI" card but not the same as the more typical one found in regular Power Macs — this particular card (820-0765-A, 630-1798, MM4709Z/A) is specific to the Apple Network Server. It has 10baseT, 10base2 and AAUI ports and is based on the DEC 21041 "Tulip" NIC, and is also distinct from the ANS 10/100 card (M3906Z/A). I installed the card in slot 6 so it would be on the other Bandit. Rummaging through the box with the Ethernet cards in it, I also found some more 16MB sticks and bumped the parity RAM to 224MB at the same time.&lt;p&gt;Unfortunately Mac OS 7.6 doesn't see the card; it isn't even offered as a choice. This seemed like a good time to try installing Mac OS 9, first because it might have updated drivers, and second because I wanted to see if 9.1 would work in any event. I ended up copying the 7.6 screenshots to the main server with LocalTalk PhoneNET and a really long telephone cable, which my wife graciously chose to ignore temporarily as well.&lt;/p&gt;Incidentally, a shout-out to my trusty Power Macintosh 7300 that batch-converts these and other PICT screenshots to PNG using Graphic Converter. To start clean, I powered off the box, pulled the plug and turned the rear key for a full reset. While I waited for that to finish, I set up a new microSD card with an empty hard disk image and copied in an ISO of Mac OS 9.1. With power restored and the BlueSCSI reconnected, the CD image booted up — a gratifying sign that Mac OS 9 was going to work just fine — and I formatted the virtual hard disk in Drive Setup. Even though it was over the slow 5MB/sec external SCSI, the installation went surprisingly quickly, likely because the emulated "CD" it was installing from was so fast. When it finished, I restarted the ANS ... and got a black screen on both the video card and the onboard VGA, even though I could see activity on the BlueSCSI and heard alert sounds. The ANS also properly responded to me pressing RESET and RETURN to cleanly shut it down, just like a Mac should. I reset the board again and it rebooted normally with bye from the blind console. We're going to come back to this really soon, because now I was starting to doubt the logic board despite all our testing earlier. 9.1 System Profiler again identifies it with Gestalt ID 67. Everything shows up here that we expect to, including the CPU, clock speed, RAM size and L2 cache. We also see our Twin Turbo and Apple Ethernet PCI cards. And, to my profound pleasure, it shows up (as "Ethernet slot SLOT.&amp;gt;4" [sic], even though I put it in slot 6, because it's slot 4 to the second Bandit) and can be selected. We are now able to mount our usual assisting Netatalk server over the Ethernet, which replaces one long cable with another long cable, but it's all in the name of science! I did try the MACE Ethernet one more time here, and 9.1 doesn't throw an error, but it still doesn't work. As a transfer test I pulled Gauge Pro off the server. It transferred completely and quickly, so I ran it to see what it thought about the hardware, and it didn't seem to find anything unusual.&lt;p&gt;So, about those reboots. At this point I shut down the machine and found the same thing happened when I tried to start it up again: a black screen, rectified by another complete board reset, but the Mac OS still seemed to boot headless and regardless. After the third such attempt, and out of ideas, I decided to foul the boot completely and see what was going on over the serial port. This can be done by letting it boot in regular mode, then for the next boot ensure the floppy drive is empty and turn the key to service, which will forget the boot setting from beforehand and try to start diagnostics. Lo and behold ...&lt;/p&gt;&lt;quote&gt;fd:diags NO DISK can't OPEN: /bandit/gc/swim3:diagsOpenFirmware1.1.20 To continue booting the MacOS type: BYE&amp;lt;return&amp;gt; To continue booting from the default boot device type: BOOT&amp;lt;return&amp;gt; ok 0 &amp;gt; printenv security-#badlogins 1 security-password security-mode none little-endian? false false real-mode? false false auto-boot? true true diag-switch? false false fcode-debug? false false oem-banner? false false oem-logo? false false use-nvramrc? true false f-segment? false true real-base -1 -1 real-size 100000 100000 virt-base -1 -1 virt-size 100000 100000 load-base 4000 4000 pci-probe-list -1 -1 screen-#columns 64 64 screen-#rows 28 28 selftest-#megs 0 0 boot-device /AAPL,ROM disk2:aix boot-file diag-device fd:diags cd fd:diags /AAPL,ROM diag-file input-device ttya kbd output-device ttya screen oem-banner oem-logo z 2C + 8CC '&amp;amp; 8 + BRpatchyn then ;;l-method else $call-parent then ; boot-command boot boot ok 0 &amp;gt;&lt;/quote&gt;&lt;p&gt;... the serial port was active. Instead of kbd and screen (or the TT video card directly), I could see the input and output devices had been set to ttya. I didn't do that — Mac OS did that. Its fingerprints can be found in the apparently nonsense line of text between oem-logo and boot-command, which is in fact an NVRAMRC expected to run at startup to wallpaper firmware bugs.&lt;/p&gt;&lt;p&gt;Now it made sense what was going on. Mac OS thought this was a real 9500, and patched its Open Firmware variables accordingly. The default settings for the Open Firmware 1.0.5 console point to the serial port, but on a real 9500 where Open Firmware wasn't intended as a user-facing interface, the ROM would simply ignore them and continue the boot with the video card and ADB HIDs. Not so on the ANS, where Open Firmware is meant to be interacted with directly: it actually obeys these settings! While Mac OS still brought ADB up regardless, neither the video card nor the onboard video would be enabled, and so the screen would stay black. (NetBSD/macppc explains a related phenomenon.)&lt;/p&gt;&lt;p&gt;However, even after I reset the input-device and output-device to kbd and screen, I still got no display. But from a cold board reset we wouldn't have an NVRAMRC either, so I also added setenv use-nvramrc? false, and now we reboot successfully! The PRAM settings persisted as well.&lt;/p&gt;&lt;p&gt;This means our logic board is likely not at fault, but I do consider this some sort of bug, especially because I don't want to have to constantly rescue it from a serial port just to restart the operating system. Fortunately there's a tool out there we can repurpose to get around the problem.&lt;/p&gt;Paul Mackerras, now working at IBM down under and well-known to us in the OpenPOWER community, years earlier had written a control panel utility called Boot Variables. This CDEV very simply gives you a graphical Mac OS interface to what's stored in Open Firmware. To get this back up I would have had to fix the Mac OS patches, so you can see that the new (tainted) settings are written on startup, not shutdown. This is good news because if we undo the damage beforehand, we'll shutdown and/or reboot normally.&lt;p&gt;Boot Variables lets you save the current contents or load them from a file. If we save the current contents, we can see the NVRAMRC is rather lengthy (extracting the text from the binary dump Boot Variables generates):&lt;/p&gt;&lt;quote&gt;boot: '&amp;amp; get-token drop ; : &amp;gt;&amp;amp; dup @ 6 &amp;lt;&amp;lt; 6 &amp;gt;&amp;gt;a -4 and + ; : &amp;amp; na+ &amp;gt;&amp;amp; ; 6ED '&amp;amp; execute 0 value mi : mmr " map-range" mi if my-self $call-method else $call-parent then ; 89B '&amp;amp; ' mmr BRpatch : mcm -1 to mi $call-method 0 to mi ; 8CB '&amp;amp; 1E na+ ' mcm BLpatch : maa -1 to mi 1D swap ; 8C9 '&amp;amp; 5 na+ ' maa BLpatch 8C9 '&amp;amp; 134 + ' 1 BLpatch 8CD '&amp;amp; 184 + dup 14 + &amp;gt;&amp;amp; BRpatch 8C6 '&amp;amp; 7C + ' u&amp;lt; BLpatch 0 value yn : y yn 0= if dup @ to yn then ; 8CB '&amp;amp; ' y BRpatch ' y 28 + 8CB '&amp;amp; 8 + BRpatch : z yn ?dup if over ! 0 to yn then ; 8CC '&amp;amp; ' z BRpatch ' z 2C + 8CC '&amp;amp; 8 + BRpatch&lt;/quote&gt;&lt;p&gt;This does a lot of low-level patching, and while it's not exactly clear what part the ANS doesn't like, the script is also rather unnecessary since it boots fine without it.&lt;/p&gt;Boot Variables can also write and restart the machine in one step with your new settings. In fact, if you open a Boot Variables dump with the Option key down, it will load those settings and reboot immediately with them, so we can just reboot that way — not exactly an ideal solution, but it works. Since the source code is available for Boot Variables, I'm tempted to write a Shutdown Items version that will do these steps automagically without prompting. In the meantime you can download it from the NetBSD archives, since it has obvious utility for NetBSD/macppc.&lt;p&gt;Because these steps are a bit of a pain, I suspected (and still do) that the version of Mac OS Apple exhibited during the ANS beta test was likely patched to work around the problem. That's yet to show up, though, if it even exists.&lt;/p&gt;&lt;p&gt;The former Apple employee who got me the 2.26NT ROM also mentioned he'd gotten Rhapsody running on one of their orphaned 700s. This would have had obvious political overtones within Apple at the time, and his boss told him not to tell anybody. Interestingly, the 2.26 ROMs do have strings in them claiming they can boot MacOS:&lt;/p&gt;&lt;quote&gt;% strings rom1122.bin | grep -i macos [...] driver,AAPL,MacOS,PowerPC MacOS is not supported. % strings rom226b6.bin | grep -i macos driver,AAPL,MacOS,PowerPC [...] MacOS is not supported. +MacOS is unsupported, use at your own risk. :MacOS requires PCI video card and external SCSI boot disk. % strings rom226nt.bin | grep -i macos driver,AAPL,MacOS,PowerPC [...] MacOS is not supported. +MacOS is unsupported, use at your own risk. :MacOS requires PCI video card and external SCSI boot disk.&lt;/quote&gt;&lt;p&gt;Despite running the system little when (trying to) boot NT, the 2.26NT ROM is of course perfectly capable of running big, and indeed must in order to boot AIX. Those strings appear to be false flags, though, because like the production 1.1.22 ROMs it too is blocked from booting Mac OS at the Open Firmware level:&lt;/p&gt;&lt;quote&gt;disk2:aix can't OPEN: /bandit/53c825@11/sd@2,0:aixOpenFirmware2.26 To continue booting from the default boot device type: BOOT&amp;lt;return&amp;gt; ok 0 &amp;gt; dev /AAPL,ROM ok 0 &amp;gt; words load open ok 0 &amp;gt; see open : open "MacOS is not supported. " type false ; ok 0 &amp;gt; see load : load real_base 400000 &amp;lt;&amp;gt; virt_base -800000 &amp;lt;&amp;gt; or real? or little? or if 10 base ! "FFFFFFFF" "real-base" $setenv "FFFFFFFF" "virt-base" $setenv "false" "real-mode?" $setenv "false" "little-endian?" $setenv "boot /AAPL,ROM" !set-restart cr "RESETing to change Configuration!" type cr reset-all then ?cr "MacOS is unsupported, use at your own risk." type ?cr "MacOS requires PCI video card and external SCSI boot disk." type &amp;lt;bye&amp;gt; ; ok 0 &amp;gt; boot /AAPL,ROM /AAPL,ROM MacOS is not supported. can't OPEN: /AAPL,ROM ok 0 &amp;gt;&lt;/quote&gt;&lt;p&gt;... and it will also hang if you patch out the words anyway.&lt;/p&gt;&lt;p&gt;No matter whatever hacking I tried, it would not go past this point either. It is noteworthy, however, that it claims it would boot with a PCI video card and external disk — it does not — which is exactly our successful configuration for 1.1.20.1. Given these limitations, it seems most likely that our Apple employee did this on a 2.0 ROM system (i.e., the "real" ANS Mac OS ROM), but let's see if the pre-production ROMs can pull off the same trick. Currently I run Mac OS X Server v1.2 (i.e., Rhapsody 5.5) on a WallStreet PowerBook G3, probably the best laptop for doing so, but all versions have been reported to run on beige PCI Power Macs including the 9500. However, my previous experience with Rhapsody was that it rebooted multiple times during the install, and I was concerned this would be a problem with our rickety restart situation. So ... let's have the Wally install it to the BlueSCSI for the 700, and then see if the 700 will boot it. The Wally is also technically unsupported, but you can get around that in the Installer, and the installation created is universal. The installation process ran a lot more slowly than Mac OS 9's, even with the Mac OS X Server v1.2 CD images on the BlueSCSI. When it completed, I took the finished hard disk and the installer CD disk image back to the 700. The 700 booted the CD just fine — it's just Mac OS 9, after all — but its Startup Disk control panel didn't see the Rhapsody disk. I rebooted from the Mac OS 9.1 hard disk image but with the Rhapsody install also present on SCSI ID 1. While both Drive Setup and SCSIProbe saw it, neither mounted it (not even forcibly with SCSIProbe), and Startup Disk still failed to see it.&lt;/p&gt;&lt;p&gt;Apple made a tool to deal with this and other related startup situations called System Disk. Distinct from the built-in Startup Disk CDEV, this is a utility application that lets you pick your boot volume and as a nice side effect can be used to edit Open Firmware variables too. It comes as a self-mounting disk image.&lt;/p&gt;System Disk is not supported on some systems and we should not be surprised it is not supported on this one either. That said, it alone is able to see the Rhapsody volume and can tell us what we need to know. It has the boot and output devices completely wrong — scsi-int would be the internal SCSI, not the external, and /chaos/control references built-in graphics in models like the Power Mac 7300 and 8600 — and this version of Open Firmware lacks the words O or bootr, but we can see where it expects to load the Mach kernel from (partition 8) using its own "partition zero" bootloader. This information is enough to come up with a command line to try booting it manually, but after all that I couldn't get it to start; it gives the same CLAIM failure message that's doomed our other attempts. Since I wasn't able to get it any further, it doesn't seem like trying real OS X out would go anywhere either. They may simply not work with this ROM.&lt;p&gt;Overall, however, the machine boots OS 9.1 well enough as long as you deal with the reboot-and-shutdown situation. It's a bit overkill to do this entirely over the external SCSI but at least doing it with flash media is far faster than a regular hard disk or CD-ROM, and as far as size goes I suppose it's no worse than using an SGI Crimson to browse your filesystem. If this is all you have to boot Mac OS on the ANS, and you really want to boot Mac OS on the ANS instead of indulging in the jackbooted bliss of AIX, it's perfectly cromulent.&lt;/p&gt;&lt;head rend="h3"&gt;The current situation&lt;/head&gt;&lt;p&gt;The pre-production ROMs work. Still, I'm hoping to get a 2.0 ROM in the near future and working with someone on doing just that. Even so, if you're an Apple employee with one of these ANS ROMs you need to get rid of, let's talk! The 2.0 ROM should solve our remaining issues with Mac OS 9, probably enable us to boot Rhapsody, and possibly even get early versions of Mac OS X working on the Apple Network Server too.&lt;/p&gt;&lt;p&gt;Similarly, if you know anything about "halbandit" or can provide the HAL or ARC console for the ANS' spin of Windows NT, that would be great! And anyone with knowledge of how Cyberpunk/NetWare was supposed to boot on Shiner ...&lt;/p&gt;&lt;p&gt;If you'd prefer not to post in the comments or wish to remain publicly anonymous, you can contact me at ckaiser at floodgap dawt com.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://oldvcr.blogspot.com/2026/01/hands-on-with-two-apple-network-server.html"/><published>2026-01-25T08:06:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46751826</id><title>Introduction to PostgreSQL Indexes</title><updated>2026-01-25T18:15:58.461920+00:00</updated><content>&lt;doc fingerprint="791c02166dd40ba4"&gt;
  &lt;main&gt;
    &lt;p&gt;20 minutes&lt;/p&gt;
    &lt;head rend="h1"&gt;Introduction to PostgreSQL Indexes&lt;/head&gt;
    &lt;head rend="h2"&gt;Who’s this for&lt;/head&gt;
    &lt;p&gt;This text is for developers that have an intuitive knowledge of what database indexes are, but don’t necessarily know how they work internaly, what are the tradeoffs associated with indexes, what are the types of indexes provided by postgres and how you can use some of its more advanced options to make them more optimized for your use case.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basics&lt;/head&gt;
    &lt;p&gt;Indexes are special database objects primarily designed to increase the speed of data access, by allowing the database to read less data from the disk. They can also be used to enforce constraints like primary keys, unique keys and exclusion. Indexes are important for performance but do not speedup a query unless the query matches the columns and data types in the index. Also, as a very rough rule of thumb, an index will only help if less than 15-20% of the table will be returned in the query, otherwise the query planner, a part of postgres used to determine how the query is going to be executed, might prefer a sequential scan. In fact, reality is much more complex than this rule of thumb. The query planner uses statistics and predefined costs associated with each type of scan to do its job, but we’re only going approach the query planner behavior tangentially in this article. So, if your query returns a large percentage of the table, consider refactoring it, using summary tables or other techniques before throwing an index at the problem. With that in mind, let’s give a closer look at how Postgres stores your data in the disk and how indexes help to speedup querying this data.&lt;/p&gt;
    &lt;p&gt;There are six types of indexes available in the default postgres installation and more types available through extensions. Typically, they work by associating a key value with a data location in one or more rows of the table containing that key. Each line is identified by a TID, or tuple id.&lt;/p&gt;
    &lt;head rend="h3"&gt;How data is stored in disk&lt;/head&gt;
    &lt;p&gt;To understand indexes, it is important to first understand how postgres stores table data on disk. Every table in postgres has one or more corresponding files on disk, depending on its size. This set of files is called a heap and it is divided into 8kb pagesh. All table rows, internally referred to as “tuples”, are saved in these files and do not have a specific order. The index is a tree structure that links the indexes columns to the row locators, also known as ctid, in the heap. We’ll zoom into the index internals later.&lt;/p&gt;
    &lt;p&gt;To see the heap files we can use a few postgres internal tables to see where they’re located in the disk. First, we can enter psql and use &lt;code&gt;show data_directory&lt;/code&gt; to show the directory Postgres uses to store databases physical files.&lt;/p&gt;
    &lt;code&gt; show data_directory;

         data_directory          
---------------------------------
 /opt/homebrew/var/postgresql@16&lt;/code&gt;
    &lt;p&gt;Now we can use the internal &lt;code&gt;pg_class&lt;/code&gt; to find the file where the heap table is stored:&lt;/p&gt;
    &lt;code&gt;create table foo (id int, name text);


select oid, datname
from pg_database
where datname = 'my_database';                                                                                

  oid  |         datname        
-------+-------------------------
 71122 | my_database
(1 row)&lt;/code&gt;
    &lt;code&gt;select relfilenode from pg_class where relname = 'foo';                                                                                                  
 relfilenode
-------------
       71123&lt;/code&gt;
    &lt;p&gt;Finally, we can check the file on disk by running this command in the shell (ls $PGDATA/base/&amp;lt;database_oid&amp;gt;/&amp;lt;table_oid&amp;gt;):&lt;/p&gt;
    &lt;code&gt;ls -lrt /opt/homebrew/var/postgresql@16/base/71122/71123
-rw-------  1 dlt  admin  0 16 Aug 14:20 /opt/homebrew/var/postgresql@16/base/71122/71123&lt;/code&gt;
    &lt;p&gt;The file has size 0 because we haven’t done any INSERTs in this table yet.&lt;/p&gt;
    &lt;p&gt;Let’s add a couple of rows to our table:&lt;/p&gt;
    &lt;code&gt;insert into foo (id, name) values (1, 'Ronaldo');
INSERT 0 1
insert into foo (id, name) values (2, 'Romario');
INSERT 0 1&lt;/code&gt;
    &lt;p&gt;We can add the &lt;code&gt;ctid&lt;/code&gt; field to the query to retrieve the ctid of each line. The ctid is an internal field that has the address of the line in the heap. Think of it as a pointer to the row location in the heap. It consists of a tuple in the format (m, n) where m is the block id and n is the tuple offset. “ctid” stands for “current tuple id”. Here you can note that the row with id one is stored in the page 0, offset 1.&lt;/p&gt;
    &lt;code&gt;select ctid, * from foo;
 ctid  | id |  name   
-------+----+---------
 (0,1) |  1 | Ronaldo
 (0,2) |  2 | Romario
(2 rows)&lt;/code&gt;
    &lt;head rend="h3"&gt;How indexes speedup access to data&lt;/head&gt;
    &lt;p&gt;Let’s add more players to the table so that the total rows is one million:&lt;/p&gt;
    &lt;code&gt;insert into foo (id, name);
select generate_series(3, 1000000), 'Player ' || generate_series(3, 1000000);&lt;/code&gt;
    &lt;p&gt;After adding more rows to the table its corresponding file is 30MB. Internally, it is divided into 8kb pages.&lt;/p&gt;
    &lt;code&gt;ls -lrtah /opt/homebrew/var/postgresql@16/base/71122/71123
-rw-------  1 dlt  admin    30M 16 Aug 16:32 /opt/homebrew/var/postgresql@16/base/71122/71133&lt;/code&gt;
    &lt;p&gt;When we query a table without an index, Postgres reads all tuples in every page and apply a filter. For example, let’s analyze the command below that searches for rows whose &lt;code&gt;name&lt;/code&gt; column value is equal to “Ronaldo” and show how the database performed this search. We use the explain command with the options &lt;code&gt;(analyse, buffers)&lt;/code&gt;. &lt;code&gt;analyse&lt;/code&gt; will actually execute the query instead of just using cost estimates, and the &lt;code&gt;buffers&lt;/code&gt; option shows how much IO work was done.&lt;/p&gt;
    &lt;code&gt; explain (analyze, buffers) select * from foo where name = 'Ronaldo';
                                                     QUERY PLAN
---------------------------------------------------------------------------------------------------------------------
 Gather  (cost=1000.00..12577.43 rows=1 width=18) (actual time=0.307..264.991 rows=1 loops=1)
   Workers Planned: 2
   Workers Launched: 2
   Buffers: shared hit=97 read=6272
   -&amp;gt;  Parallel Seq Scan on foo  (cost=0.00..11577.33 rows=1 width=18) (actual time=169.520..256.639 rows=0 loops=3)
         Filter: (name = 'Ronaldo'::text)
         Rows Removed by Filter: 333333
         Buffers: shared hit=97 read=6272
 Planning Time: 0.143 ms
 Execution Time: 265.021 ms&lt;/code&gt;
    &lt;p&gt;Note the in output the line starting with " -&amp;gt; Parallel Seq scan on foo". This line denotes that the database performed a sequential search and read all the rows in the table. The execution time for this query was 265.021ms. Also note the line that says “Buffers: shared hit=97 read=6272”. This mean that we needed to read 97 pages from memory, and 6272 pages from disk.&lt;/p&gt;
    &lt;p&gt;Now let’s add an index on the name column and see how the same query performs. We’re using the command &lt;code&gt;create index concurrently&lt;/code&gt; because we don’t want to block the table for writes.&lt;/p&gt;
    &lt;code&gt;create index concurrently on foo(name);
CREATE INDEX

explain (analyze, buffers) select * from foo where name = 'Ronaldo';
                                                    QUERY PLAN
-------------------------------------------------------------------------------------------------------------------
 Index Scan using foo_name_idx on foo  (cost=0.42..8.44 rows=1 width=18) (actual time=0.047..0.049 rows=1 loops=1)
   Index Cond: (name = 'Ronaldo'::text)
   Buffers: shared hit=4
 Planning Time: 0.129 ms
 Execution Time: 0.077 ms
(5 rows)&lt;/code&gt;
    &lt;p&gt;Here we see that the index was used and that in this case the execution time was reduced from 264.21 to 0.074 milliseconds, and the database only needed to read 4 pages! The reduction in execution time happens because, now, instead of reading all the rows in the table, the database uses the index. The index is a tree structure mapping the value “Ronaldo” to the ctid(s) of the rows that have this value in the &lt;code&gt;name&lt;/code&gt; column (in our example we only have one such row). The ctid is then used to quickly locate these rows on the heap.&lt;/p&gt;
    &lt;p&gt;If we use &lt;code&gt;\di+&lt;/code&gt; to show the indexes in our database we can see that the index we’ve created occupies &lt;code&gt;30MB&lt;/code&gt;, roughly the same size as the &lt;code&gt;foo&lt;/code&gt; table.&lt;/p&gt;
    &lt;code&gt;\di+

                                         List of relations
 Schema |     Name     | Type  | Owner | Table | Persistence | Access method | Size  | Description
--------+--------------+-------+-------+-------+-------------+---------------+-------+-------------
 public | foo_name_idx | index | dlt   | foo   | permanent   | btree         | 30 MB |
(1 row)&lt;/code&gt;
    &lt;head rend="h2"&gt;Costs associated with indexes&lt;/head&gt;
    &lt;p&gt;It is important to highlight that the extra speed brought by indices is associated with several costs that must be considered when deciding where and how to apply them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disk Space&lt;/head&gt;
    &lt;p&gt;Indexes are stored in a separate area of the heap and take up additional disk space. The more indexes a table has, the greater the amount of disk space required to store them. This incurs in additional storage costs for your database and for backups, increased replication traffic, and increased backup and failover recovery times. Bear in mind that its not uncommon for btree indexes to be larger than the table itself. Learning about partial indexes, and multicolumn indexes, as well as about other more space efficient index types such as BRIN can be helpful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Write operations&lt;/head&gt;
    &lt;p&gt;Also, there is a maintenance cost in writing operations such as UPDATE, INSERT and DELETE, if a field that is part of an index is modified, the corresponding index needs to be updated, which can add significant overhead to the writing process.&lt;/p&gt;
    &lt;head rend="h3"&gt;Query planner&lt;/head&gt;
    &lt;p&gt;The query planner (also known as query optimizer) is the component responsible for determining the best execution strategy for a query. With more indexes available, the query planner has more options to consider, which can increase the time needed to plan the query, especially in systems with many complex queries or where there are many indexes available.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memory usage&lt;/head&gt;
    &lt;p&gt;PostgreSQL maintains a portion of frequently accessed data and index pages in memory in its shared buffers. When an index is used, the relevant index pages are loaded into shared buffers to speed up access. The more indexes you have and the more they are used, the more shared buffer memory is necessary. Since shared buffers are limited and are also used for caching data pages, filling the shared buffers with indexes can lead to less efficient caching of table data. It’s also good to keep in mind that the whole indexed column is copied in every node of the btree, since there’s a limit in node size capacity, the larger the indexed column the deeper the tree will be.&lt;/p&gt;
    &lt;p&gt;Another aspect of memory usage is that PostgreSQL uses work memory when it executes queries that involves sorting or complex index scans (involving multi-column or covering indexes). Larger indexes require more memory for these operations. Also, indexes require memory to store some metadata about their structure, column names and statistics in the system catalog cache. And finally indexes require memory for maintainance operations like vacuuming and reindexing operations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Types of Indexes&lt;/head&gt;
    &lt;head rend="h3"&gt;Btree&lt;/head&gt;
    &lt;p&gt;The B-Tree is a very powerful data structure, present not only in Postgres but in almost every database management system, since it is a very good general purpose index. It was invented by Rudolf Bayer and Edward M.McCreight while working at Boeing. Nobody really knows if the “B” in B-tree stands for Bayer, Boeing, balanced or better, and it doesn’t really matter. What really matters is that it enables us to search elements in the tree in O(log n) time. If you’re not familiar with Big-O notation, all you need to know is that is is really fast - you only need to make 20 comparisons in order to find an element in a set with 1 million items. Moreover, it can maintain O(log n) time complexity for data sets that are larger than the RAM available on a computer. This means that disks can be used to extend RAM, thanks to the btree efficient prevention of disk page accesses to find the desired data. In PostgreSQL the btree is the most common type of index and its the default, it’s also used to support system and TOAST indexes. Even an empty database has hundreds of btree indexes. It is the only index type that can be used for primary and unique key constraints.&lt;/p&gt;
    &lt;p&gt;In contrast with a binary tree, the BTree is a balanced tree and all of its leave nodes have the same distance from the root. The root nodes and inner nodes have pointers to lower levels, and the leaf nodes have the keys and pointers to the heap. Postgres btrees also have pointers to the left and right nodes for easier forward and backward scanning. Nodes can have multiple keys and these keys are sorted so that it’s easy to walk in ordered directions and to perform ORDER BY and JOIN operations. The values are only stored in the leaf nodes, this makes the tree more compact and facilitates a full traversal of the objects in a tree with just a linear pass through all the leaf nodes. This is just a simplified description of PostgreSQL Btree indexes, if you want to get into the low level details, I suggest you to read the README and the paper that inspired them. Below there’s a simplified illustration of a Postgres Btree.&lt;/p&gt;
    &lt;head rend="h4"&gt;Using multiple indexes&lt;/head&gt;
    &lt;p&gt;Postgres can use multiple indexes to handle cases that cannot be handled by single index scans, by forming &lt;code&gt;AND&lt;/code&gt; and &lt;code&gt;OR&lt;/code&gt; conditions across several index scans with the support of bitmaps. The bitmaps are ANDed or ORed together as needed by the query and finally the table rows are visited and returned. Let’s say we have a query like this:&lt;/p&gt;
    &lt;code&gt;select * from users where age = 30 and login_count = 100;&lt;/code&gt;
    &lt;p&gt;If the &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;login_count&lt;/code&gt; columns are indexed, postgres scans index &lt;code&gt;age&lt;/code&gt; for all pages with &lt;code&gt;age=30&lt;/code&gt; and makes a bitmap where the pages that might contain rows with &lt;code&gt;age=30&lt;/code&gt; are true. In a similar way, it builds a bitmap using the &lt;code&gt;login_count&lt;/code&gt; index. It then ANDs the two bitmaps to form a third bitmap, and performs a table scan, only reading the pages that might contain candidate values, and only adding the rows where &lt;code&gt;age=30 and login_count=100&lt;/code&gt; to the result set.&lt;/p&gt;
    &lt;head rend="h4"&gt;Multi-column indexes&lt;/head&gt;
    &lt;p&gt;Multi-column indexes are an alternative for using multiple indexes. They’re generaly going to be smaller and faster than using multiple indexes, but they’ll also be less flexible. That’s because the order of the columns matter, because the database can search for a subset of the indexed columns, as long as they are the leftmost columns. For example, if you have an index on column &lt;code&gt;a&lt;/code&gt; and another index on column &lt;code&gt;b&lt;/code&gt;, these indexes will serve all the of queries below:&lt;/p&gt;
    &lt;code&gt;select * from my_table where a = 42 and b = 420;

select * from my_table where a = 43;

select * from my_table where b = 99;&lt;/code&gt;
    &lt;p&gt;On the other hand, only the first two queries would use an index if you created a multi-column index on (a, b) with a command like &lt;code&gt;create index on my_table(a, b)&lt;/code&gt;; So, when building multi-column indexes choose the order of the columns well so that your index can be used by the most queries possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Partial indexes&lt;/head&gt;
    &lt;p&gt;Partial indexes allow you to use a conditional expression to control what subset of rows will be indexed, this can bring you many benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;your index can be smaller and more likely fit in RAM.&lt;/item&gt;
      &lt;item&gt;your index is shallower, so lookups are quicker&lt;/item&gt;
      &lt;item&gt;less overhead for index/update/delete (but can also mean more overhead if the column you’re using to filter rows in/out of the index is updated very frequently triggering constant index maintenance)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They’re mostly useful in situations where you don’t care about some rows, or when you’re indexing on a column where the proportion of one value is much greater than others. I’ll give two examples below.&lt;/p&gt;
    &lt;head rend="h5"&gt;When you don’t care about some rows&lt;/head&gt;
    &lt;p&gt;Let’s say you have a rules table where the rows can be marked as enabled/disabled, the vast majority of the rows are disabled and in your queries you only care about enabled rows. In this case, you would have a partial index, filtering out the disable rows like this:&lt;/p&gt;
    &lt;code&gt;create index on rules(status) where status = 'enabled';&lt;/code&gt;
    &lt;head rend="h5"&gt;When the distribution of values is skewed&lt;/head&gt;
    &lt;p&gt;Now imagine you’re building a todo application and the status column value can be either &lt;code&gt;TODO&lt;/code&gt;, &lt;code&gt;DOING&lt;/code&gt;, and &lt;code&gt;DONE&lt;/code&gt;. Suppose you have 1M rows and this is the current distribution of rows in each status:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Rows&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TODO&lt;/cell&gt;
        &lt;cell&gt;90%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DOING&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DONE&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Since postgres keeps statistics about the distribution of values in your table columns and knows that the vast majority of the rows are in the &lt;code&gt;TODO&lt;/code&gt; status, it would choose to do a sequential scan on the &lt;code&gt;tasks&lt;/code&gt; table when you have &lt;code&gt;status='TODO'&lt;/code&gt; in the &lt;code&gt;WHERE&lt;/code&gt; clause of your query, even if you have an index on status, leaving most part of the index unused and wasting space. In this case, a partial scan such as the one below is recommended:&lt;/p&gt;
    &lt;code&gt;create index on tasks(status) where status &amp;lt;&amp;gt; 'TODO';&lt;/code&gt;
    &lt;head rend="h4"&gt;Covering indexes&lt;/head&gt;
    &lt;p&gt;If you have a query that selects only columns in an index, Postgres has all information needed by the query in the index and doesn’t need to fetch pages from the heap to return the result. This optimization is called &lt;code&gt;index-only scan&lt;/code&gt;. To understand how it works, consider the following scenario:&lt;/p&gt;
    &lt;code&gt;create table bar (a int, b int, c int);
create index abc_idx on bar(a, b);

/* query 1 */
select a, b from bar;

/* query 2 */
select a, b, c from bar;&lt;/code&gt;
    &lt;p&gt;In the first query, postgres can do an index-only scan and avoid fetching data from the heap because the values &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are present in the index. In the second query, since &lt;code&gt;c&lt;/code&gt; isn’t in the index, posgres needs to follow the reference to the heap to fetch its value. In the first query we allowed postgres do to an index-only scan with the help of a multi-column index, but we could also achieve the same result by using a covering index. The syntax for creating a covering index looks like this:&lt;/p&gt;
    &lt;code&gt;create index abc_cov_idx on bar(a, b) including c;&lt;/code&gt;
    &lt;p&gt;This is more space efficient than creating a multi-column index on (a, b, c), because c will only be inserted at the leaf nodes of the btree. Also, we might want to use a covering index in cases where we want an unique index and &lt;code&gt;c&lt;/code&gt; would “break” the uniqueness of the index.&lt;/p&gt;
    &lt;head rend="h4"&gt;Expression indexes&lt;/head&gt;
    &lt;p&gt;Expression indexes to index the result of an expression or function, rather than just the raw column values. This can be extremely useful when you frequently query based on a transformed version of your data. It is necessary if you use a function as part of a where clause as in the example below:&lt;/p&gt;
    &lt;code&gt;CREATE TABLE customers (
    id SERIAL PRIMARY KEY,
    name TEXT
);

CREATE INDEX idx_name ON customers(name);
SELECT * FROM customers WHERE LOWER(name) = 'john doe';&lt;/code&gt;
    &lt;p&gt;In this example above, Postgres won’t use the index because it was was built against the &lt;code&gt;name&lt;/code&gt; column. In order to make it work, the index key has to call the &lt;code&gt;lower&lt;/code&gt; function just like it’s used in the where clase. To fix it, do:&lt;/p&gt;
    &lt;p&gt;Now, when you run a query like this:&lt;/p&gt;
    &lt;code&gt;CREATE INDEX idx_lower_name ON customers (lower(name));&lt;/code&gt;
    &lt;p&gt;Now PostgreSQL can use the expression index to efficiently find the matching rows.&lt;/p&gt;
    &lt;p&gt;Expression indexes can be created using various types of expressions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Built-in functions: Like &lt;code&gt;lower()&lt;/code&gt;,&lt;code&gt;upper()&lt;/code&gt;, etc.&lt;/item&gt;
      &lt;item&gt;User-defined functions: As long as they are immutable.&lt;/item&gt;
      &lt;item&gt;String concatenations: Like &lt;code&gt;first_name || ' ' || last_name&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Hash&lt;/head&gt;
    &lt;p&gt;The hash index differs from B-Tree in strucutre, it is much more alike a hashmap data structure present in most programming languages (e.g. dict in Python, array in php, HashMap in java, etc). Instead of adding the full column value to the index, a 32bit hash code is derived from it and added to the hash. This makes hash indexes much smaller than btrees when indexing longer data such as UUIDs, URLs, etc. Any data type can be indexed with the help of postgres hashing functions. If you type &lt;code&gt;\df hash*&lt;/code&gt; and press TAB in psql, you’ll see that there are more then 50 hash related functions. Although it gracefully handles hash conflicts, it works better for even distribution of hash values and is most suited to unique or mostly unique data. Under the correct conditions it will not only be smaller than btree indexes, but also it will be faster for reads when compared with btress. Here’s what the official docs says about it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“In a B-tree index, searches must descend through the tree until the leaf page is found. In tables with millions of rows, this descent can increase access time to data. The equivalent of a leaf page in a hash index is referred to as a bucket page. In contrast, a hash index allows accessing the bucket pages directly, thereby potentially reducing index access time in larger tables. This reduction in “logical I/O” becomes even more pronounced on indexes/data larger than shared_buffers/RAM.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As for its limitations, it only supports equality operations and isn’t going to be helpful if you need to order by the indexed field. It also doesn’t support multi-column indexes and checking for uniqueness. For a in-depth analysis of how hash indexes fare in relation to btree, check Evgeniy Demin’s blog post on the subject.&lt;/p&gt;
    &lt;head rend="h3"&gt;BRIN&lt;/head&gt;
    &lt;p&gt;BRIN stands for Block Range Index and its name tells a lot about how it is implemented. Nodes in BRIN indexes store the minimum and maximum values of a range of values present in the page referred by the index. This makes the index more compact and cache friendly, but restricts the use cases for it. If you have a very large in a work load that is heavy on writes and low on deletes and updates. You can think of a BRIN index as an optimizer for sequential scans of large amounts of data in very large databases, and is a good optimization to try before partitioning a table. For a BRIN index to work well, the index key should be a column that strongly correlates to the location of the row in the heap.Some good use cases for BRIN are append-only tables and tables storing time series data.&lt;/p&gt;
    &lt;p&gt;BRIN won’t work well for tables where the rows are updated constantly, due to the nature of MVCC that duplicates rows and stores them in a different part of the heap. This tuple duplication and moving affect the correlation negatively and reduces the effectiveness of the index. Using extensions such as pg_repack or pg_squeeze isn’t recommended for tables that use BRIN indexes, since they change the internal data layour fo the table and mess up the correlation. Also, this index is lossy in the sense that the index leaf nodes point to pages taht might contain a value within a particular range. For this reason a BRIN is more helpful if you need to return large subset of data, and a btree would be more read performant for queries that only return one or few rows. You can make the index more or less lossy by adjusting the &lt;code&gt;page_per_range&lt;/code&gt; configuration, the trade off will be index size.&lt;/p&gt;
    &lt;head rend="h3"&gt;GIN&lt;/head&gt;
    &lt;p&gt;Generalized inverted index is appropriate for when you want to search for an item in composite data, such as finding a word in a blob of text, an item in an array or an object in a JSON. The GIN is generalized in the sense that it doesn’t need to know how it will acelerate the search for some item. Instead, there’s a set of custom strategies specific for each data type. Please note that in order to index an JSON value it needs to be stored in a JSONB column. Similarly, if you’re indexing text it’s better to store it as (or convert it to) tsvector or use the pg_trgm extension.&lt;/p&gt;
    &lt;head rend="h3"&gt;GiST &amp;amp; SP-GiST&lt;/head&gt;
    &lt;p&gt;The Generalized Search Tree and the Space-Partitioned Generalized Search Tree are tree structures that can be use as a base template to implement indexes for specific data types. You can think of them as framework for building indexes. The GiST is a balanced tree and the SP-GiST allow for the development of non-balanced data structures. They are useful for indexing points and geometric types, inet, ranges and text vectors. You can find an extensive list of the built-in strategies shipped with postgres in the official documentation. If you need an index to enable full-text search in your application, you’ll have to choose between GIN and GiST. Roughly speaking, GIN is faster for lookups but it’s bigger and has greater building and maintainance costs. So the right index type for you will depend on your application requirements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Understanding and effectively using indexes is crucial for optimizing database performance in PostgreSQL. While indexes can greatly speed up query execution and improve overall efficiency, it’s important to be mindful of their impact on write operations and storage. By carefully selecting the appropriate types of indexes based on your specific use cases you can ensure that your PostgreSQL database remains both fast and efficient. I hope this article taught you at least one or two things you didn’t know about Postgres indexes, and that you’re better equiped to deal with different scenarios involving databases from now on.&lt;/p&gt;
    &lt;p&gt;4119 Words&lt;/p&gt;
    &lt;p&gt;2024-09-11 08:07&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dlt.github.io/blog/posts/introduction-to-postgresql-indexes/"/><published>2026-01-25T08:07:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46751899</id><title>Deutsche Telekom is throttling the internet</title><updated>2026-01-25T18:15:57.266388+00:00</updated><content>&lt;doc fingerprint="66133f28ed9a317f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Deutsche Telekom is throttling the internet. Let's do something about it!&lt;/head&gt;
    &lt;p&gt;If you are a customer of Deutsche Telekom and some websites just won't load, then we might have the solution to your problem!&lt;/p&gt;
    &lt;head rend="h2"&gt;Short Explanation!&lt;/head&gt;
    &lt;head rend="h2"&gt;What is this about?&lt;/head&gt;
    &lt;p&gt;Epicenter.works, the Society for Civil Rights, the Federation of German Consumer Organizations, and Stanford Professor Barbara van Schewick are filing an official complaint with the Federal Network Agency against Deutsche Telekom’s unfair business practices.&lt;/p&gt;
    &lt;p&gt;Deutsche Telekom is creating artificial bottlenecks at access points to its network. Financially strong services that pay Telekom get through quickly and work perfectly. Services that cannot afford this are slowed down and often load slowly or not at all.&lt;/p&gt;
    &lt;p&gt;This means Telekom decides which services we can use without issues, violating net neutrality. We are filing a complaint with the Federal Network Agency to stop this unfair practice together!&lt;/p&gt;
    &lt;head rend="h2"&gt;Testimonials&lt;/head&gt;
    &lt;head rend="h2"&gt;How can you help the project?&lt;/head&gt;
    &lt;p&gt;Are you a Deutsche Telekom customer and want to help? Get in touch with usâevery experience counts! Maybe you even have networking expertise and measurement data that could be relevant? Whether with or without measurements, weâd love to hear from you at netzbremse@epicenter.works.&lt;/p&gt;
    &lt;p&gt;Do you have experience with interconnection agreements with Deutsche Telekom and want to talk to us confidentially? Contact us via email or one of the following channels:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Signal: +43 670 404 98 89&lt;/item&gt;
      &lt;item&gt;Threema (ID: BXJMX4R5)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Donate now for a free internet!Donate now for a free internet!&lt;/head&gt;
    &lt;head rend="h2"&gt;Talk at Chaos Communication Congress 38C3&lt;/head&gt;
    &lt;head rend="h2"&gt;Media Coverage&lt;/head&gt;
    &lt;p&gt;VerbraucherschÃ¼tzer werfen der Telekom vor, bewusst EngpÃ¤sse im Internet zu schaffen, um zusÃ¤tzlich Geld fÃ¼r schnellere ZugÃ¤nge zu kassieren. Sie sehen darin eine Verletzung von EU-Recht. Die Telekom widerspricht. Von Markus Reher.&lt;/p&gt;
    &lt;p&gt;Ein BÃ¼ndnis aus mehreren Organisationen hat bei der Bundesnetzagentur eine Beschwerde wegen angeblicher Verletzungen von NetzneutralitÃ¤tspflichten eingereicht.&lt;/p&gt;
    &lt;p&gt;VerbraucherschÃ¼tzer werfen der Telekom vor, mit schlechtem Netz zusÃ¤tzliches Geld zu machen.&lt;/p&gt;
    &lt;p&gt;Ein BÃ¼ndnis mehrerer Organisationen wirft der Telekom vor, kÃ¼nstliche EngpÃ¤sse im Netz zu schaffen und damit Geld zu verdienen. Der Konzern weist die VorwÃ¼rfe zurÃ¼ck und holt zur Gegenkritik aus.&lt;/p&gt;
    &lt;p&gt;VerbraucherschÃ¼tzer werfen der Telekom vor, mit schlechtem Netz zusÃ¤tzliches Geld zu machen.&lt;/p&gt;
    &lt;p&gt;Die Telekom drosselt das Netz", beklagen die Macher einer neuen Kampagne. Das verstoÃe gegen die NetzneutralitÃ¤t. Eine offizielle Beschwerde soll bald folgen.&lt;/p&gt;
    &lt;p&gt;“Die Telekom drosselt das Netz”, beklagen die Macher einer neuen Kampagne. Das verstoÃe gegen die NetzneutralitÃ¤t. Eine offizielle Beschwerde soll bald folgen.&lt;/p&gt;
    &lt;p&gt;Die DeutÂsche Telekom muss endlich die Peering-KapaÂzitÃ¤ten zu anderen Internet-Knoten erhÃ¶hen.&lt;/p&gt;
    &lt;p&gt;VerbrauÂcherÂschÃ¼tzer wollen bei der BundesÂnetzÂagentur Beschwerde gegen Peering-Probleme im Telekom-Netz einreiÂchen.&lt;/p&gt;
    &lt;p&gt;Nicht nur als KrisenlÃ¶sung sucht das Deutsche Forschungsnetz den direkten Anschluss zur Deutschen Telekom. Die wollte sich aber zuerst auf nichts einlassen.&lt;/p&gt;
    &lt;p&gt;Schon seit mindestens Mai 2015 gibt es sie, eine Option fÃ¼r Hetzner-Kunden, die ihre Server fÃ¼r Kunden der Telekom zwischen 19 und 22 Uhr besser erreichbar machen wollen.&lt;/p&gt;
    &lt;p&gt;Die Telekom sieht sich mit schweren VorwÃ¼rfen konfrontiert, nach denen sie absichtlich gegen die NetzneutralitÃ¤t verstoÃen soll. Ãberzeugt davon ist nicht nur die Verbraucherzentrale.&lt;/p&gt;
    &lt;p&gt;Die Verbraucherzentrale sucht Betroffene, die Probleme im Netz der Telekom haben. Der Vorwurf: Verletzung der NetzneutralitÃ¤t.&lt;/p&gt;
    &lt;p&gt;Verbraucher und Organisationen wehren sich gegen Netzdrosselung von Telekom. Erste Beschwerden gehen ein.&lt;/p&gt;
    &lt;p&gt;Der Verbraucherschutz schlÃ¤gt Alarm: Die Deutsche Telekom bevorzugt bei der Internet-Geschwindigkeit offenbar Dienste und Webseiten, die fÃ¼r mehr Tempo zahlen, wÃ¤hrend sie andere drosselt. User sollen das nun bestÃ¤tigen.&lt;/p&gt;
    &lt;p&gt;Macht die Telekom Teile des Internets absichtlich langsam? Dieser Vorwurf hat es in sich und wird vom Verband der Verbraucherzentralen erhoben â ein Einblick in das Prinzip des Peerings und die Frage, wie die Telekom hier seit Jahren fÃ¼r Frust sorgt.&lt;/p&gt;
    &lt;p&gt;Macht die Telekom ihr Netz absichtlich langsamer? Der Verbraucherschutz wirft dem Unternehmen vor, die NetzneutralitÃ¤t absichtlich zu verletzen. Die Telekom verlange von Anbietern Zahlungen fÃ¼r bevorzugten Datentransfer und bremse andere Dienste aus.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://netzbremse.de/en/"/><published>2026-01-25T08:22:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46751906</id><title>Sony Data Discman</title><updated>2026-01-25T18:15:57.135664+00:00</updated><content>&lt;doc fingerprint="bb65194d5c9f2bbb"&gt;
  &lt;main&gt;
    &lt;p&gt;Introduction&lt;/p&gt;
    &lt;p&gt;Back in 1992 I worked at an Electronics Boutique that was an outlet location for the company. We sold regular merchandise but also had an outlet section for clearance stuff aggregated from other stores. This thoroughly confused customers who expected every item in the store to be discounted. The job involved a lot of explaining "no I'm sorry this game that literally launched today is not on clearance". Working retail is a great way to lose faith in the collective intelligence of our species.&lt;/p&gt;
    &lt;p&gt;One day we received several Sony Data Discman Electronic Book Player DD-1EX players that we were supposed to clear out. The original sticker price was $500 but they were marked down to roughly 1% of that. Largely out of curiosity I picked one up along with whatever software we had for it (also at a massive discount). I can't say I've used it for more than an hour. It's a very nice device that serves no practical or entertainment function whatsoever.&lt;/p&gt;
    &lt;p&gt;Using old catalogs as a reference, these were originally listed in the spring 1992 catalog. Here it is:&lt;/p&gt;
    &lt;p&gt;These did not appear in the summer 1992 catalog just a couple months later. Since I started working there during the 1992 holiday season the timeline works out. They must have hit the shelves in early 1992, not sold, then been marked down every month until they were rounded-up and shipped to our location.&lt;/p&gt;
    &lt;p&gt;Let's take a peek at it..&lt;/p&gt;
    &lt;p&gt;Gallery&lt;/p&gt;
    &lt;p&gt;Disclaimer: I am terrible at taking pictures.&lt;/p&gt;
    &lt;p&gt;Sony didn't nickel-and-dime consumers on accessories here. The package came with: the reader (duh), AC adapter, rechargeable battery, and another battery pack that holds AAs. Years later they refused to include an AC adapter in the PlayStation Classic.&lt;/p&gt;
    &lt;p&gt;The reader itself is fairly nice looking. It feels like a miniature laptop. It's a tad on the heavy side but also feels extremely durable. Looking at all the buttons and size of the screen makes me think this had a lot of potential beyond just electronic books. However, it lacks any mechanism to save data. In the early 90s it's not like SD-RAM cards were available. Miniature hard drive? Forget it. It has 90% of what it needs to be a PDA but the technology just wasn't there to get the last 10% in.&lt;/p&gt;
    &lt;p&gt;There's a QWERTY keyboard because all of the books are searchable. The directional pad is there to navigate through menus. Looking at it again just makes me irritated that I don't have any games for this (of course I doubt any were made). This would make a cool little text adventure player.&lt;/p&gt;
    &lt;p&gt;The electronic books are mini CDs in a caddy. I guess that means we can rip them (more on this soon).&lt;/p&gt;
    &lt;p&gt;Bad Screenshots&lt;/p&gt;
    &lt;p&gt;I picked up every electronic book we had in stock. The player has an output jack than can be connected to anything with an A/V input (well, just the "V" part is needed). These screenshots are from the A/V out.&lt;/p&gt;
    &lt;p&gt;The splash screen reminding you that this is for private use only. I guess I'm technically violating that, whatever.&lt;/p&gt;
    &lt;p&gt;Although I don't know the exact date this electronic book reader was produced, the bundled encyclopedia gives some hints. It still lists U.S.S.R. as a country so it had to be authored prior to Christmas day 1991.&lt;/p&gt;
    &lt;p&gt;Early 90s software developer salary in the career guide:&lt;/p&gt;
    &lt;p&gt;Thinking of traveling the world? Well, this handy translator is all you need. Someone once told me that if you ever got lost in a strange foreign country you should claim to be a Swedish citizen. Something about Sweden having an embassy in every country and nobody holding a grudge against them. I couldn't find a translation for "I'm a Swedish citizen please don't turn me over to the secret police" in this guide.&lt;/p&gt;
    &lt;p&gt;The least useful book (to me at least) is the crossword dictionary. You can search for word endings or a list of complete words but that's it.&lt;/p&gt;
    &lt;p&gt;The wellness encyclopedia is the perfect gift for a hypochondriac.&lt;/p&gt;
    &lt;p&gt;Since I won't pay more than $3.99 for a bottle of wine I found this guide relatively useless.&lt;/p&gt;
    &lt;p&gt;Ripping the CDs&lt;/p&gt;
    &lt;p&gt;If you rip the CDs you'll find that some of them contain an emulator for the Discman. Here's the wine guide main screen:&lt;/p&gt;
    &lt;p&gt;It seems like the books are fully functional in this emulator:&lt;/p&gt;
    &lt;p&gt;The career guide also comes with an emulator. You can use it to look for jobs that didn't exist in 1990 I guess:&lt;/p&gt;
    &lt;p&gt;Some CDs, like the encyclopedia, don't have the emulator bundled. However, if you copy the data files around it's trivial to launch it in the emulator bundled on the other CDs:&lt;/p&gt;
    &lt;p&gt;iso Downloads&lt;/p&gt;
    &lt;p&gt;Grab these before I receive a takedown notice.. I mean, these were completely obsolete before Wikipedia existed and are even worse after. I doubt that will stop anyone though. One of the reasons I deleted my YouTube videos was takedown notices from Sony over the intro to Dragon's Lair. Some rapper who sold &amp;lt;100 albums, but is apparently signed with Sony, sampled the intro of Dragon's Lair. In Sony's mind that means they own all rights to it. I don't know how you rap over the Dragon's Lair intro and I don't care to learn. EA also sent me a takedown notice over a Dragon's Lair video, a game they neither wrote nor own the rights to. As far as I can tell they at some point were the distributor for an early iOS version of Dragon's Lair that is no longer available. So to make a long story short, I'm sure these will be offline soon. When that happens I'll try moving them to archive.org since they are somehow able to get away with posting anything.&lt;/p&gt;
    &lt;p&gt;Career encyclopedia (SROM30_VERSION1)&lt;/p&gt;
    &lt;p&gt;Crossword dictionary (SROM13V1_07D)&lt;/p&gt;
    &lt;p&gt;Wellness encyclopedia (SROM25_V1_0)&lt;/p&gt;
    &lt;p&gt;World translator (SROM29_VERSION1)&lt;/p&gt;
    &lt;p&gt;Usual disclaimer that you are downloading isos with executable files from a total rando's site. I am 100% not responsible for any awful thing that happens if you download these and run the files on them.&lt;/p&gt;
    &lt;p&gt;Related&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://huguesjohnson.com/random/sony-ebook/"/><published>2026-01-25T08:23:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46752151</id><title>A flawed paper in Management Science has been cited more than 6,000 times</title><updated>2026-01-25T18:15:57.005520+00:00</updated><content/><link href="https://statmodeling.stat.columbia.edu/2026/01/22/aking/"/><published>2026-01-25T09:04:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46752261</id><title>Jurassic Park - Tablet device on Nedry's desk? (2012)</title><updated>2026-01-25T18:15:56.818600+00:00</updated><content/><link href="https://www.therpf.com/forums/threads/jurassic-park-tablet-device-on-nedrys-desk.169883/"/><published>2026-01-25T09:22:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46752841</id><title>Bridging the Gap Between PLECS and SPICE</title><updated>2026-01-25T18:15:56.600398+00:00</updated><content>&lt;doc fingerprint="351921528b158f82"&gt;
  &lt;main&gt;
    &lt;p&gt;Three years ago, we set out to bring SPICE simulation into PLECS. PLECS Spice is finally here.&lt;/p&gt;
    &lt;p&gt;PLECS Spice brings SPICE device-level simulation directly into PLECS. Available with PLECS 5.0, both system-level and device-level analysis can be performed within a single tool, eliminating the need to maintain duplicate models across separate softwares.&lt;/p&gt;
    &lt;head rend="h2"&gt;Separate Tools, Duplicate Work&lt;/head&gt;
    &lt;p&gt;Power electronics design has long faced a fundamental trade-off: system-level simulation tools deliver the speed and robustness needed for controller development and overall system analysis, but sacrifice the device-level detail necessary to validate component selection before procurement.&lt;/p&gt;
    &lt;p&gt;For over 20 years, Plexim has promoted a top-down design philosophy, enabling engineers to model complete power electronic systems using ideal switches and behavioral components. By avoiding the computational burden of simulating detailed switching transients, PLECS enables rapid validation of system-level requirements like efficiency, control performance and thermal behavior.&lt;/p&gt;
    &lt;p&gt;Conversely, traditional SPICE simulators embody an inherently bottom-up approach. They excel at validating device-level requirements through detailed semiconductor models, capturing switching losses, voltage overshoots and parasitic effects with high fidelity. This comes at a cost: system-level integration becomes computationally prohibitive.&lt;/p&gt;
    &lt;p&gt;This divide has forced engineers into parallel workflows using separate software platforms with different modeling approaches and incompatible component libraries. Moving from a system-level PLECS model to SPICE for device validation requires recreating the model, an error-prone and time-consuming process.&lt;/p&gt;
    &lt;head rend="h2"&gt;PLECS Spice&lt;/head&gt;
    &lt;p&gt;To solve this problem, Plexim has developed PLECS Spice, an extension that brings SPICE device-level simulation capabilities directly into PLECS. PLECS Spice can simulate hybrid systems containing both standard PLECS and SPICE circuits. This allows a schematic to be progressively refined by replacing the ideal switches in a circuit of interest, such as the power stage, with detailed SPICE netlists. Controls and other subsystems can remain unchanged. Because the entire workflow stays within PLECS, engineers can easily toggle between ideal and detailed configurations to compare results. This creates a true top-down workflow where device-level detail is added selectively, only where needed. With PLECS Spice, there is no longer a need to build the same model twice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Under the Hood&lt;/head&gt;
    &lt;p&gt;The PLECS Spice extension adds four key ingredients that transform PLECS into a fully-featured hybrid simulation platform that can simulate standard PLECS and SPICE models together.&lt;/p&gt;
    &lt;head rend="h3"&gt;Netlist Parser&lt;/head&gt;
    &lt;p&gt;SPICE models are typically distributed as netlists. Simply put, these are text files that describe a circuit topology, component interconnections and parameter values. A key capability of PLECS Spice is its parser’s support for multiple netlist dialects. Different SPICE implementations use distinct syntax conventions, making netlists from various vendors incompatible. The PLECS Spice parser handles these variations automatically, enabling engineers to integrate models provided by different semiconductor manufacturers directly into their schematics. Little to no manual conversion or syntax adaptation is needed, regardless of the dialect.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compact Models&lt;/head&gt;
    &lt;p&gt;Netlists provided by manufacturers often rely on well-established semiconductor device models. These compact models combine physics-based modeling with empirical corrections to capture fundamental electrical behavior while maintaining reasonable complexity. PLECS Spice includes optimized implementations of compact models such as diodes, MOSFETs, BJTs, and switches. Each model defines a set of parameters that can be tuned to match the electrical response of specific physical devices. In PLECS Spice, classical compact models have been improved to guarantee continuity of key physical quantities, enhancing numerical stability. By tightly integrating these models into the solver, PLECS Spice achieves both computational efficiency and robust convergence even in the presence of highly nonlinear semiconductor characteristics.&lt;/p&gt;
    &lt;head rend="h3"&gt;Modified Nodal Analysis&lt;/head&gt;
    &lt;p&gt;Standard PLECS uses piecewise state-space equations to simulate electrical models. This approach is computationally efficient for circuits with mostly linear components but struggles with the strong nonlinearities present in detailed semiconductor models. To handle these nonlinearities, SPICE uses Modified Nodal Analysis (MNA), a formulation that produces differential algebraic equations (DAEs).&lt;/p&gt;
    &lt;p&gt;MNA constructs the circuit equations by applying Kirchhoff’s current law at each node and substituting component branch equations. Energy storage elements introduce differential equations, while the network topology and sources introduce algebraic constraints. The result is a coupled system where nodal voltages, source currents, and energy storage currents must satisfy both differential and algebraic equations simultaneously. This integrated treatment of constraints and dynamics is what makes MNA particularly robust for nonlinear semiconductor models.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mixed-Formulation Solver&lt;/head&gt;
    &lt;p&gt;PLECS Spice employs third-order implicit Runge-Kutta methods augmented with circuit-tailored convergence helpers to solve the DAEs produced by MNA. These one-step methods have a crucial advantage for mixed-signal schematics that contain both SPICE and standard PLECS electrical circuits: they are inherently self-starting. In other words, they do not rely on information from previous time steps. When events such as topology changes or zero-crossings occur, the solver must compute the next time step using only the current state. This self-starting property makes one-step methods particularly well-suited for hybrid systems with frequent discontinuities.&lt;/p&gt;
    &lt;p&gt;The solver can simulate complex systems that combine standard PLECS and SPICE models in a single schematic. The only rule is that when an electrical circuit contains a netlist, it must be solved using MNA, and therefore all its components must be compatible with SPICE. But other electrical circuits can remain in the standard PLECS formulation. Circuits of different types connect through the control domain using sources and meters. This enables a powerful top-down workflow: engineers can refine specific circuits of interest by converting them to SPICE netlists while keeping other subsystems and controls unchanged in standard PLECS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Application Example&lt;/head&gt;
    &lt;p&gt;Mixed-signal simulation is particularly valuable when control strategies and device physics must be considered together. The soft switching operation of a Dual Active Bridge (DAB) converter, whose analysis requires taking into consideration both controls and circuit design aspects, serves as a perfect case study for the workflow enabled by PLECS Spice.&lt;/p&gt;
    &lt;p&gt;A DAB is a bidirectional DC-DC topology comprising identical primary and secondary bridges (typically full bridges) separated by a high-frequency transformer and an energy transfer inductance (representing leakage plus external inductance). It is widely employed in high-power, high-density applications requiring bidirectional power flow between two galvanically isolated sides, such as EV chargers and energy storage systems.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Soft Switching Challenge&lt;/head&gt;
    &lt;p&gt;Magnetic components are often the primary limitation to increasing power density. Their size can be reduced by increasing switching frequency. State-of-the-art designs have reached the hundreds of kHz range. However, at these frequencies, switching losses represent a significant part of the overall converter losses. Without careful design, the volume advantage of a smaller transformer could be negated by the increased size needed of the cooling system.&lt;/p&gt;
    &lt;p&gt;To resolve this dilemma, soft switching offers a compelling solution. Given the high switching frequencies, MOSFETs are the standard choice for modern DABs. However, their dominant loss mechanism stems from the charge stored in the parasitic output capacitance (). When the device blocks voltage, this capacitance stores the energy&lt;/p&gt;
    &lt;p&gt;which depends on the drain-source voltage. When a MOSFET is turned on, the stored charge must be evacuated. In hard switching, the closing channel effectively shorts the capacitance, dissipating the stored energy as heat within the semiconductor. At high frequencies, this thermal penalty becomes unsustainable.&lt;/p&gt;
    &lt;p&gt;Here, the DAB offers a distinct advantage. In a full-bridge topology, each leg contains a top and bottom switch that operate complementarily: when one conducts, the other blocks. In practice, a short interval called dead time is introduced between turning off one switch and turning on its complement. Its primary role is to prevent a short circuit across the DC link, but a DAB can also exploit this interval of time for soft switching. During dead time, the inductor current continues to flow. With both switches off, the only path available is through the parasitic capacitances. This discharges the output capacitance of the incoming MOSFET (the switch about to turn on), causing its drain-source voltage to fall. If the dead time is sufficient, reaches zero before the gate signal arrives. The soft switching challenge lies in properly tuning this interval.&lt;/p&gt;
    &lt;p&gt;To achieve such Zero Voltage Switching (ZVS), the relationship between the device output capacitance, the resonant path and the gate drive, must be carefully adjusted. Crucially, these hardware choices cannot be made in isolation. They must inform the control design. This is because robust ZVS depends on several dynamic factors: the converter’s operating point (voltage and power), the gate driving scheme (specifically the dead times) and the degrees of freedom utilized by the modulation strategy.&lt;/p&gt;
    &lt;p&gt;Standard PLECS simulations allow for precise tuning of the operating point within the ZVS region, represented by the blue area in the ZVS range figure. However, because ideal switches are inherently hard switching, they do not simulate the transients required for a detailed analysis of ZVS. The effects of using two different dead times are compared in the figures above. In both tests, the low side gate signal turns off a conducting MOSFET. After the dead time, the complementary MOSFET is turned on by the high side gate signal. In the first experiment, a dead time of 15 ns is used between the two events. In the second, it is set to 50 ns. Yet, the resulting voltage and current waveforms show no visible response to this parameter change.&lt;/p&gt;
    &lt;p&gt;In this case, the ideal model fails to indicate whether the timing achieves soft switching or leads to hard switching transients. The reason is that ideal switches lack parasitic capacitances. Without , the antiparallel diode of the complementary switch conducts immediately, making the simulated waveforms insensitive to the dead time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Validating ZVS with Device-Level Detail&lt;/head&gt;
    &lt;p&gt;PLECS Spice enables precisely the analysis that ideal models cannot provide. Using configurable subsystems, engineers can add a detailed SPICE configuration alongside the ideal model, allowing them to toggle between fast system-level analysis and high-fidelity device validation without modifying the circuit topology or control logic.&lt;/p&gt;
    &lt;p&gt;The ideal switch configuration shown in the first figure consists of a standard PLECS MOSFET with antiparallel diode, driven directly by a control signal. The detailed configuration in the second figure provides a device-level description. It uses manufacturer-provided MOSFET and diode netlists that capture parasitic capacitances and charge dynamics. The control signal is converted to a gate-source voltage through a controlled voltage source. Separate on and off gate resistances ( and ) control switching speed. Engineers can switch between one configuration to the other between two simulations, while the control logic, operating point and all other subsystems remain unchanged.&lt;/p&gt;
    &lt;p&gt;With detailed device models in place, the impact of dead time on ZVS becomes immediately visible. The figure below shows the switching transient with a 15 ns dead time. The drain-source voltage remains high when the gate signal is applied, and only begins falling as channel current rises. This overlap between voltage and current is the signature of hard switching: the channel conducts before the output capacitance fully discharges, dissipating the stored energy as heat in the semiconductor. The insufficient dead time prevents the resonant discharge mechanism from completing.&lt;/p&gt;
    &lt;p&gt;By contrast, the second figure demonstrates successful ZVS with a 50 ns dead time. Here, completes its resonant transition to zero before the gate signal arrives. The channel opens with zero voltage across it, eliminating capacitive turn-on losses. Channel current then begins to flow, carrying the inductor current through the device. This extended dead time provides a sufficient interval for the inductor current to transfer energy from the MOSFET’s output capacitance, fulfilling the conditions for soft switching.&lt;/p&gt;
    &lt;p&gt;This example demonstrates how PLECS Spice enables validation of ZVS by bringing together three tightly coupled aspects within a single model. The control strategy establishes the operating point and determines the available inductor current for resonant transitions. Gate drive timing sets the window for capacitor discharge. The device physics, captured in the SPICE netlist, determines how quickly that discharge occurs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;PLECS Spice marks a significant step towards a unified power electronics design workflow. By integrating SPICE simulation directly into the PLECS environment, engineers no longer need to choose between system-level insights and device-level accuracy. The ability to seamlessly transition between ideal and detailed models within a single schematic eliminates the redundant and error-prone process of rebuilding circuits in separate tools. This empowers engineers to adopt a true top-down design philosophy, starting with a system-level view and progressively adding detail where it matters most. As power electronic systems grow in complexity, this unified approach will be crucial for accelerating innovation and reducing time-to-market.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://erickschulz.dev/posts/plecs-spice/"/><published>2026-01-25T10:44:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46752907</id><title>150k lines of vibe coded Elixir: The Good, the Bad and the Ugly</title><updated>2026-01-25T18:15:56.337480+00:00</updated><content>&lt;doc fingerprint="f72c54daabbd7c64"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;150,000 Lines of Vibe Coded Elixir: The Good, The Bad, and The Ugly&lt;/head&gt;
    &lt;p&gt;TL;DR:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Good: AI is great at Elixir. It gets better as your codebase grows.&lt;/item&gt;
      &lt;item&gt;Bad: It defaults to defensive, imperative code. You need to be strict about what good Elixir looks like.&lt;/item&gt;
      &lt;item&gt;Ugly: It can’t debug concurrent test failures. It doesn’t understand that each test runs in an isolated transaction, or that processes have independent lifecycles. It spirals until you step in.&lt;/item&gt;
      &lt;item&gt;Bottom Line: Even with the drawbacks, the productivity gains are off the charts. I expect it will only get better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;BoothIQ is a universal badge scanner for trade shows. AI writes 100% of our code. We have 150,000 lines of vibe coded Elixir running in production. Here’s what worked and what didn’t.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Good&lt;/head&gt;
    &lt;head rend="h3"&gt;Elixir is Small: It Gets It Right the First Time&lt;/head&gt;
    &lt;p&gt;Elixir is a small language. Few operators. Small standard library. Only so many ways to control flow. It hasn’t been around for decades. It hasn’t piled up paradigms like .NET or Java, where functional and OOP fight for space.&lt;/p&gt;
    &lt;p&gt;This matters. AI is bad at decisions. If you want your agent to succeed, have it make fewer decisions. With Elixir, Claude doesn’t need to pick between OOP and functional. It doesn’t need to navigate old syntax next to new patterns. There’s one way to skin the cat. Claude finds it.&lt;/p&gt;
    &lt;p&gt;This matters more if you’re adding AI to an existing codebase. In languages where paradigms came and went—often with whatever developer pushed them—Claude tries to match the existing code. The existing code is inconsistent. So Claude is inconsistent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Elixir is Terse: Longer Sessions, Fewer Compactions&lt;/head&gt;
    &lt;p&gt;Small and terse are related but different. Small means few concepts. Terse means fewer tokens to express the same thing. Go is small but not terse—few concepts, but verbose syntax and explicit error handling everywhere. Elixir is both. We got lucky.&lt;/p&gt;
    &lt;p&gt;Context windows are a real constraint. Elixir uses fewer tokens than most languages. No braces. No semicolons. No verbose boilerplate. I can stay in a working session longer. More iterations. Fewer compactions—those moments when the AI summarizes and forgets earlier context. More context in memory.&lt;/p&gt;
    &lt;p&gt;When I built the React Native version of our app, I hit compactions constantly. JavaScript is small-ish, but it’s not terse. It burns tokens to do what Elixir does with fewer.&lt;/p&gt;
    &lt;p&gt;I also see more compactions when working on heavy HTML and Tailwind in LiveView. Adding, updating, or editing large sections of markup at once. HTML and HEEx templates are token-heavy. But even then, it’s less painful than JavaScript-heavy work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tidewave: Longer Unassisted Runs&lt;/head&gt;
    &lt;p&gt;Tidewave supercharges Elixir-specific context. It lets the agent read logs from the running app—debug, info, error, warning—so you don’t copy/paste logs around. It can query the dev database, see Ecto schemas, and view package documentation. Fewer hallucinations. Longer unassisted runs. The agent can check and validate its own assumptions without human intervention.&lt;/p&gt;
    &lt;head rend="h3"&gt;Immutability: Fewer Decisions, Less Code&lt;/head&gt;
    &lt;p&gt;If a variable gets mutated by a function call, AI now has three problems instead of one. The actual feature you want implemented. Whether to work around the mutation or update other call sites to stop mutating. And the mutated data itself—what is it, what was it, what will it be, what can it be?&lt;/p&gt;
    &lt;p&gt;AI ponders all of this and contorts itself into an overly defensive mess. It writes nonsense validation checks and if-statements on mutated data. Defensive code that wouldn’t exist in an immutable language.&lt;/p&gt;
    &lt;p&gt;In Elixir, the data is what it is. It’s not going to change. Fewer decisions. Less code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Frontend: Higher Quality, Less Time&lt;/head&gt;
    &lt;p&gt;I prompt high-level changes—“give the top section more padding”—and Claude does it faster than I could. It’s especially good at modifying or moving large chunks of page structure. Mobile-first views? Easy. Way faster than me, and it’s a better designer than me too.&lt;/p&gt;
    &lt;p&gt;The quality floor has gone way up. You can’t hide behind “I’m not a designer” anymore.&lt;/p&gt;
    &lt;head rend="h3"&gt;Git Worktrees: Build Multiple Features in Parallel&lt;/head&gt;
    &lt;p&gt;I use three git worktrees, so I can work on up to three features at any given time. Typically a main feature, a slightly less important one, and the third reserved for quick fixes, low priority stuff, or quick experiments.&lt;/p&gt;
    &lt;p&gt;Three is about the limit. Any more and context switching between features becomes the bottleneck.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Bad&lt;/head&gt;
    &lt;head rend="h3"&gt;AI Can’t Organize: Architecture Is Still On You&lt;/head&gt;
    &lt;p&gt;AI is exceptional at churning out lines of code. It’s significantly less exceptional at deciding where those lines should go. It defaults to creating new files everywhere. It repeats code it’s already written. It introduces inconsistencies.&lt;/p&gt;
    &lt;p&gt;This is the “mess” people describe in vibe code projects as they grow. You still need a human making structural decisions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trained on Imperative: It Writes Defensive Code&lt;/head&gt;
    &lt;p&gt; AI trained mostly on imperative code. Ruby, Python, JavaScript, C#. Elixir looks like Ruby. So Claude writes Ruby-style Elixir—&lt;code&gt;if/then/else&lt;/code&gt; chains, defensive nil-checking, early returns that don’t make sense in a functional context.&lt;/p&gt;
    &lt;p&gt;Elixir wants you to be assertive. Pattern match on what you expect. Let it crash if something’s wrong. The process restarts in a good state. This is foreign to most code Claude trained on.&lt;/p&gt;
    &lt;p&gt;This gets better as the codebase grows. Claude sees more assertive patterns. It starts to infer the style. But it still defaults to defensive. I still correct it regularly. Be strict about what good Elixir looks like.&lt;/p&gt;
    &lt;head rend="h3"&gt;Git Operations: Keep It Out of Context&lt;/head&gt;
    &lt;p&gt;Every git operation takes context window space. Checking status. Writing commit messages. Describing PRs. That space could go to actual work. Git context goes stale fast—a commit message from 20 minutes ago is worthless after three more changes.&lt;/p&gt;
    &lt;p&gt;When I’m babysitting a feature, I commit manually. Every point I’m happy with. It’s fast. It’s cheap version control. It doesn’t burn context.&lt;/p&gt;
    &lt;p&gt;Claude Code has “checkpoints” now. Internal version control that protects vibe coders without explicit commits. That’s better than AI managing git directly.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Ugly&lt;/head&gt;
    &lt;head rend="h3"&gt;OTP and Async: It Chases Ghosts&lt;/head&gt;
    &lt;p&gt;Claude is useless for debugging OTP, Task, or async issues. It doesn’t understand how processes, the actor model, and GenServers work together. When it tries to introspect the running system, it feeds itself bad data. It gets very lost.&lt;/p&gt;
    &lt;p&gt;It can course correct when you point out where it went wrong. But on its own, it chases ghosts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecto Sandbox: It Chases Red Herrings&lt;/head&gt;
    &lt;p&gt;In Elixir tests, each test runs in a database transaction that rolls back at the end. Tests run async without hitting each other. No test data persists.&lt;/p&gt;
    &lt;p&gt;Claude doesn’t understand this. It uses Tidewave’s dev DB connection and thinks it’s looking at the test DB—which is always empty. A test fails. Claude queries the database. Finds nothing. Thinks there’s a data problem.&lt;/p&gt;
    &lt;p&gt;I’ve watched Claude try to seed the test database so a test will pass. That’s clearly wrong.&lt;/p&gt;
    &lt;p&gt;Other times, two tests insert or query the same schema. Claude doesn’t understand transaction isolation—tests can’t see each other’s data. It confuses itself and recommends disabling async tests altogether. Manageable once you watch for it. But ugly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bottom Line&lt;/head&gt;
    &lt;p&gt;AI writing all the code has been a massive win. The friction exists, but it’s manageable and doesn’t interfere much with day-to-day work. By far the most important thing: have a consistent, coherent codebase architecture. Without it, you’ll quickly end up with spaghetti code.&lt;/p&gt;
    &lt;p&gt;The goal for this year: automate myself out of a job. That means giving Claude more control over the entire software development lifecycle—from a simple problem statement to a fully tested, working PR that only needs a quick glance before it’s merged and deployed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://getboothiq.com/blog/150k-lines-vibe-coded-elixir-good-bad-ugly"/><published>2026-01-25T10:54:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46753078</id><title>Show HN: TUI for managing XDG default applications</title><updated>2026-01-25T18:15:55.941834+00:00</updated><content>&lt;doc fingerprint="fe9cba7b7ba6b9a3"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;xdgctl&lt;/code&gt; is a TUI for managing XDG default applications. View and set defaults for file categories without using &lt;code&gt;xdg-mime&lt;/code&gt; directly.&lt;/p&gt;
    &lt;p&gt;Built with C using GLib/GIO and termbox2.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;xdgctl.mp4&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browse by category (Browsers, Text Editors, etc.)&lt;/item&gt;
      &lt;item&gt;Current default marked with &lt;code&gt;*&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Up/Down&lt;/cell&gt;
        &lt;cell&gt;Navigate through categories or applications&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Right/Tab&lt;/cell&gt;
        &lt;cell&gt;Switch from category list to application list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Left&lt;/cell&gt;
        &lt;cell&gt;Switch back to category list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Enter&lt;/cell&gt;
        &lt;cell&gt;Set selected application as default for current category&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Esc / q&lt;/cell&gt;
        &lt;cell&gt;Quit the application&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To build &lt;code&gt;xdgctl&lt;/code&gt;, you need the following development libraries:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;glib-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gio-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gio-unix-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clang&lt;/code&gt;or&lt;code&gt;gcc&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# On Void Linux
sudo xbps-install glibc-devel&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/mitjafelicijan/xdgctl.git
cd xdgctl

# Build
make
sudo make install

# Using prefix
sudo make PREFIX=/usr/local install
make PREFIX=~/.local install&lt;/code&gt;
    &lt;p&gt;If you manually add new applications to your &lt;code&gt;~/.local/share/applications&lt;/code&gt; directory, you might need to run &lt;code&gt;update-desktop-database&lt;/code&gt; again.&lt;/p&gt;
    &lt;code&gt;ls /usr/share/applications
ls ~/.local/share/applications&lt;/code&gt;
    &lt;code&gt;xdg-mime query default text/plain
xdg-mime query default text/html
xdg-mime query default x-scheme-handler/http
xdg-mime query default x-scheme-handler/https
xdg-mime query default inode/directory&lt;/code&gt;
    &lt;code&gt;xdg-mime default brave.desktop x-scheme-handler/http
xdg-mime default brave.desktop x-scheme-handler/https&lt;/code&gt;
    &lt;code&gt;# ~/.local/share/applications/brave.desktop
[Desktop Entry]
Exec=/home/m/Applications/brave
Type=Application
Categories=Applications
Name=Brave Browser
MimeType=text/html;text/xml;application/xhtml+xml;x-scheme-handler/http;x-scheme-handler/https;&lt;/code&gt;
    &lt;code&gt;update-desktop-database ~/.local/share/applications
less ~/.config/mimeapps.list
less /usr/share/applications/mimeapps.list&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/mitjafelicijan/xdgctl"/><published>2026-01-25T11:19:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46753301</id><title>Show HN: Bonsplit – Tabs and splits for native macOS apps</title><updated>2026-01-25T18:15:55.564634+00:00</updated><content>&lt;doc fingerprint="8d0973528a96f1e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Bonsplit is a custom tab bar and layout split library for macOS apps. Enjoy out of the box 120fps animations, drag-and-drop reordering, SwiftUI support &amp;amp; keyboard navigation.&lt;/p&gt;
    &lt;quote&gt;.package(url: "https://github.com/almonk/bonsplit.git", from: "1.0.0")&lt;/quote&gt;
    &lt;p&gt;### Features&lt;/p&gt;
    &lt;p&gt;Create tabs with optional icons and dirty indicators. Target specific panes or use the focused pane.&lt;/p&gt;
    &lt;quote&gt;let tabId = controller.createTab(title: "Document.swift",icon: "swift",isDirty: false,inPane: paneId)&lt;/quote&gt;
    &lt;p&gt;Create tabs with optional icons and dirty indicators. Target specific panes or use the focused pane.&lt;/p&gt;
    &lt;quote&gt;let tabId = controller.createTab(title: "Document.swift",icon: "swift",isDirty: false,inPane: paneId)&lt;/quote&gt;
    &lt;p&gt;Split any pane horizontally or vertically. New panes are empty by default, giving you full control.&lt;/p&gt;
    &lt;quote&gt;// Split focused pane horizontallylet newPaneId = controller.splitPane(orientation: .horizontal)// Split with a tab already in the new panecontroller.splitPane(orientation: .vertical,withTab: Tab(title: "New", icon: "doc"))&lt;/quote&gt;
    &lt;p&gt;Split any pane horizontally or vertically. New panes are empty by default, giving you full control.&lt;/p&gt;
    &lt;quote&gt;// Split focused pane horizontallylet newPaneId = controller.splitPane(orientation: .horizontal)// Split with a tab already in the new panecontroller.splitPane(orientation: .vertical,withTab: Tab(title: "New", icon: "doc"))&lt;/quote&gt;
    &lt;p&gt;Update tab properties at any time. Changes animate smoothly.&lt;/p&gt;
    &lt;quote&gt;// Mark document as modifiedcontroller.updateTab(tabId, isDirty: true)// Rename tabcontroller.updateTab(tabId, title: "NewName.swift")// Change iconcontroller.updateTab(tabId, icon: "doc.text")&lt;/quote&gt;
    &lt;p&gt;Update tab properties at any time. Changes animate smoothly.&lt;/p&gt;
    &lt;quote&gt;// Mark document as modifiedcontroller.updateTab(tabId, isDirty: true)// Rename tabcontroller.updateTab(tabId, title: "NewName.swift")// Change iconcontroller.updateTab(tabId, icon: "doc.text")&lt;/quote&gt;
    &lt;p&gt;Programmatically navigate between panes using directional navigation.&lt;/p&gt;
    &lt;quote&gt;// Move focus between panescontroller.navigateFocus(direction: .left)controller.navigateFocus(direction: .right)controller.navigateFocus(direction: .up)controller.navigateFocus(direction: .down)// Or focus a specific panecontroller.focusPane(paneId)&lt;/quote&gt;
    &lt;p&gt;Programmatically navigate between panes using directional navigation.&lt;/p&gt;
    &lt;quote&gt;// Move focus between panescontroller.navigateFocus(direction: .left)controller.navigateFocus(direction: .right)controller.navigateFocus(direction: .up)controller.navigateFocus(direction: .down)// Or focus a specific panecontroller.focusPane(paneId)&lt;/quote&gt;
    &lt;p&gt;### Read this, agents...&lt;/p&gt;
    &lt;p&gt;Complete reference for all Bonsplit classes, methods, and configuration options.&lt;/p&gt;
    &lt;p&gt;The main controller for managing tabs and panes. Create an instance and pass it to BonsplitView.&lt;/p&gt;
    &lt;p&gt;Implement this protocol to receive callbacks about tab bar events. All methods have default implementations and are optional.&lt;/p&gt;
    &lt;p&gt;Configure behavior and appearance. Pass to BonsplitController on initialization.&lt;/p&gt;
    &lt;code&gt;allowSplits&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable split buttons and drag-to-split&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCloseTabs&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Show close buttons on tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCloseLastPane&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Allow closing the last remaining pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;false&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowTabReordering&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable drag-to-reorder tabs within a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCrossPaneTabMove&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable moving tabs between panes via drag&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;autoCloseEmptyPanes&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Automatically close panes when their last tab is closed&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;contentViewLifecycle&lt;/code&gt;
    &lt;code&gt;ContentViewLifecycle&lt;/code&gt;
    &lt;p&gt;How tab content views are managed when switching tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;.recreateOnSwitch&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;newTabPosition&lt;/code&gt;
    &lt;code&gt;NewTabPosition&lt;/code&gt;
    &lt;p&gt;Where new tabs are inserted in the tab list&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;.current&lt;/code&gt;&lt;/p&gt;
    &lt;quote&gt;let config = BonsplitConfiguration(allowSplits: true,allowCloseTabs: true,allowCloseLastPane: false,autoCloseEmptyPanes: true,contentViewLifecycle: .keepAllAlive,newTabPosition: .current)let controller = BonsplitController(configuration: config)&lt;/quote&gt;
    &lt;p&gt;Controls how tab content views are managed when switching between tabs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
        &lt;cell role="head"&gt;State&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;.recreateOnSwitch&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
        &lt;cell&gt;Simple content&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;.keepAllAlive&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Higher&lt;/cell&gt;
        &lt;cell&gt;Full&lt;/cell&gt;
        &lt;cell&gt;Complex views, forms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Controls where new tabs are inserted in the tab list.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;.current&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Insert after currently focused tab, or at end if none&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;.end&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Always insert at the end of the tab list&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;tabBarHeight&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Height of the tab bar&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;33&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabMinWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum width of a tab&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;140&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabMaxWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Maximum width of a tab&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;220&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabSpacing&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Spacing between tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;0&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;minimumPaneWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum width of a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;100&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;minimumPaneHeight&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum height of a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;100&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;showSplitButtons&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Show split buttons in the tab bar&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;animationDuration&lt;/code&gt;
    &lt;code&gt;Double&lt;/code&gt;
    &lt;p&gt;Duration of animations in seconds&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;0.15&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;enableAnimations&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable or disable all animations&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;.default&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Default configuration with all features enabled&lt;/p&gt;
    &lt;code&gt;.singlePane&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Single pane mode with splits disabled&lt;/p&gt;
    &lt;code&gt;.readOnly&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Read-only mode with all modifications disabled&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bonsplit.alasdairmonk.com"/><published>2026-01-25T11:56:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46753336</id><title>Nango (YC W23, Dev Infrastructure) Is Hiring Remotely</title><updated>2026-01-25T18:15:55.279321+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jobs.ashbyhq.com/Nango"/><published>2026-01-25T12:02:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46753484</id><title>Doom has been ported to an earbud</title><updated>2026-01-25T18:15:53.535096+00:00</updated><content>&lt;doc fingerprint="a6567235022113ff"&gt;
  &lt;main&gt;
    &lt;p&gt;It's almost your turn, get ready!&lt;/p&gt;
    &lt;p&gt;Player queue&lt;/p&gt;
    &lt;p&gt;Your position&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;Players queued&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;Wait time&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;You know the 1993 classic DOOM? I made it run on an earbud, then I connected it to the internet and made it possible for visitors like you to sit in a queue for hours play the game remotely!.&lt;/p&gt;
    &lt;p&gt;Yeah but it won't just run on any old earbud, this only works with the Pinebuds Pro, the only earbuds with open source firmware.&lt;/p&gt;
    &lt;p&gt;You sure can! There are two relevant repos:&lt;/p&gt;
    &lt;p&gt;This was a necessary optimisation to avoid paying outgoing bandwidth fees, once you're 5th in the queue, the twitch player will switch to a low-latency MJPEG stream.&lt;/p&gt;
    &lt;p&gt;shhhh don't look don't look it's ok just join the queue&lt;/p&gt;
    &lt;p&gt;Let's switch to a more readable font first.&lt;/p&gt;
    &lt;p&gt; I'll put out an article / video diving deeper into this later, but here are a few bits of info:&lt;lb/&gt; This project is made up of four parts: &lt;/p&gt;
    &lt;p&gt;The firmware pushes up against a few hardware limitations:&lt;/p&gt;
    &lt;p&gt; Earbuds don't have displays, so the only way to transfer data to/from them is either via bluetooth, or the UART contact pads.&lt;lb/&gt; Bluetooth is pretty slow, you'd be lucky to get a consistent 1mbps connection, UART is easily the better option.&lt;lb/&gt; DOOM's framebuffer is (width * height) bytes, 320 * 200 = 96kB. (doom's internal framebuffer is 8-bit not 24-bit)&lt;lb/&gt; The UART connection provides us with 2.4mbps of usable bandwidth. 2,400,000 / 8 / 96,000 gives us... 3 frames per second.&lt;lb/&gt; Clearly we need to compress the video stream. Modern video codecs like h264 consume way too much CPU and RAM.&lt;lb/&gt; The only feasible approach is sending the video as an MJPEG stream. MJPEG is a stream of JPEG images shown one after the other.&lt;lb/&gt; I found an excellent JPEG encoder for embedded devices here, thanks Larry!&lt;lb/&gt; A conservative estimate for the average HIGH quality JPEG frame is around 13.5KB, but most scenes (without enemies) are around 11kb.&lt;lb/&gt; Theoretical maximum FPS:&lt;lb/&gt; - Optimistic: `2,400,000 / (11,000 * 8)` = 27.3 FPS&lt;lb/&gt; - Conservative: `2,400,000 / (13,500 * 8)` = 22.2 FPS &lt;/p&gt;
    &lt;p&gt; The stock open source firmware has the CPU set to 100mhz, so I cranked that up to 300mhz and disabled low power mode.&lt;lb/&gt; The Cortex-M4F running at 300mhz is actually more than enough for DOOM, however it struggles with JPEG encoding.&lt;lb/&gt; This is why it maxes out at ~18fps, I don't think there's much else I can do to speed it up. &lt;/p&gt;
    &lt;p&gt; By default, we only have access to 768KB of RAM, after disabling the co-processor it gets bumped up to the advertised 992KB.&lt;lb/&gt; DOOM requires 4MB of RAM, though there are plenty of optimisations that can reduce this amount.&lt;lb/&gt; Pre-generating lookup tables, making variables const, reading const variables from flash, disabling DOOM's caching system, removing unneeded variables. It all adds up! &lt;/p&gt;
    &lt;p&gt; The shareware DOOM 1 wad (assets file) is 4.2MB and the earbuds can only store 4MB of data.&lt;lb/&gt; Thankfully, fragglet, a well-known doom modder, has already solved this issue for me.&lt;lb/&gt; Squashware is his trimmed-down DOOM 1 wad that is only 1.7MB in size.&lt;lb/&gt; With this wad file, everything comfortably fits in flash. &lt;/p&gt;
    &lt;p&gt;I thought you'd never ask! (please hire me)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://doombuds.com"/><published>2026-01-25T12:22:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46753611</id><title>Alarm overload is undermining safety at sea as crews face thousands of alerts</title><updated>2026-01-25T18:15:53.217231+00:00</updated><content>&lt;doc fingerprint="3452211c23d1e7ad"&gt;
  &lt;main&gt;
    &lt;p&gt;New research from Lloyd’s Register (LR) has revealed that excessive and nuisance shipboard alarm systems are routinely overwhelming crews and, in many cases, actively undermining safety at sea.&lt;/p&gt;
    &lt;p&gt;The findings, published today in Effective Alarm Management in the Maritime Industry are based on data collected from 11 operational vessels, spanning over 2,000 days and more than 40 million alarm-related events.&lt;/p&gt;
    &lt;p&gt;The study shows that many ships generate thousands of alarms every day, many of which provide little or no operational value. The result is widespread alarm fatigue, disrupted rest periods and a growing erosion of trust in systems that are intended to protect both crews and assets.&lt;/p&gt;
    &lt;p&gt;The research applied recognised industrial best practice, including IEC 62682 and EEMUA 191, to maritime operations for the first time at this scale. It found that fewer than half of the vessels studied met the recommended benchmark of fewer than 30 alarms per hour, while on ships with unattended machinery spaces alarms disrupted 63% of rest periods. In some cases, cruise ships experienced up to 2,600 alarms per day, with peak rates reaching 4,691 alarms in just ten minutes.&lt;/p&gt;
    &lt;p&gt;Crews, overwhelmed by the volume of alerts, are forced to silence alarms without acknowledgement or physically bypass alarm circuits, normalising unsafe practices and eroding trust in critical safety systems.&lt;/p&gt;
    &lt;p&gt;Effective Alarm Management in the Maritime Industry: Insights from 40 million vessel alarms builds on LR’s Effective Alarm Management in the Maritime Industry report (released in September 2024) by moving beyond diagnosis to demonstrate what can be achieved in practice. A pilot project on an operational cruise ship reduced total alarm numbers by almost 50 per cent over a six-month period, without new technology or major system redesign. Improvements were delivered through traditional marine engineering interventions, including correcting valve installations, replacing faulty sensors and tuning existing systems.&lt;/p&gt;
    &lt;p&gt;LR’s analysis also demonstrates that addressing the 10 most frequent alarms could reduce overall loads by nearly 40 per cent.&lt;/p&gt;
    &lt;p&gt;The report calls for greater adoption of objective alarm performance assessment, stronger consideration of human factors in system design and operation throughout the vessel lifecycle, and regulatory frameworks that support consistent, enforceable standards.&lt;/p&gt;
    &lt;p&gt;Duncan Duffy, LR’s Global Head of Technology, said: “Our research found that alarm systems, when poorly managed, have themselves become a safety risk. Without decisive industry action, alarm fatigue will continue to undermine situational awareness and increase the likelihood of serious incidents.&lt;/p&gt;
    &lt;p&gt;“If the maritime industry is serious about safety, it must commit to continuous performance measurement, objective evaluation, and a human-centred approach to alarm system design. Only then can alarm systems fulfil their intended purpose—supporting crews, safeguarding lives, and ensuring safer voyages for all.”&lt;/p&gt;
    &lt;p&gt;The research is part of LR’s Digital Transformation Research programme, specifically designed to provide in-depth analysis of key opportunities and challenges for maritime digitalisation.&lt;/p&gt;
    &lt;p&gt;For more information and to download the full report, visit the link below:&lt;lb/&gt;LR Alarm Management &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.lr.org/en/knowledge/press-room/press-listing/press-release/2026/alarm-overload-is-undermining-safety-at-sea-as-new-research-shows-crews-face-tens-of-thousands-of-daily-alerts/"/><published>2026-01-25T12:40:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46753708</id><title>Web-based image editor modeled after Deluxe Paint</title><updated>2026-01-25T18:15:52.836673+00:00</updated><content>&lt;doc fingerprint="56223e185d452d87"&gt;
  &lt;main&gt;
    &lt;p&gt;Webbased image editor modeled after the legendary Deluxe Paint with a focus on retro Amiga file formats. Next to modern image formats, DPaint.js can read and write Amiga icon files and IFF ILBM images.&lt;/p&gt;
    &lt;p&gt;Online version available at https://www.stef.be/dpaint/&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fully Featured image editor with a.o. &lt;list rend="ul"&gt;&lt;item&gt;Layers&lt;/item&gt;&lt;item&gt;Selections&lt;/item&gt;&lt;item&gt;Masking&lt;/item&gt;&lt;item&gt;Transformation tools&lt;/item&gt;&lt;item&gt;Effects and filters&lt;/item&gt;&lt;item&gt;Multiple undo/redo&lt;/item&gt;&lt;item&gt;Copy/Paste from any other image program or image source&lt;/item&gt;&lt;item&gt;Customizable dither tools&lt;/item&gt;&lt;item&gt;Color Cycling&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Heavy focus on colour reduction with fine-grained dithering options&lt;/item&gt;
      &lt;item&gt;Amiga focus &lt;list rend="ul"&gt;&lt;item&gt;Read/write/convert Amiga icon files (all formats)&lt;/item&gt;&lt;item&gt;Reads IFF ILBM images (all formats including HAM and 24-bit)&lt;/item&gt;&lt;item&gt;Writes IFF ILBM images (up to 256 colors)&lt;/item&gt;&lt;item&gt;Read and write directly from Amiga Disk Files (ADF)&lt;/item&gt;&lt;item&gt;Embedded Amiga Emulator to preview your work in the real Deluxe Paint.&lt;/item&gt;&lt;item&gt;Limit the palette to 12 bit for Amiga OCS/ECS mode, or 9 bit for Atari ST mode.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Deluxe Paint Legacy &lt;list rend="ul"&gt;&lt;item&gt;Supports PBM files as used by the PC version of Deluxe Paint (Thanks to Michael Smith)&lt;/item&gt;&lt;item&gt;Supports Deluxe Paint Atari ST compression modes (Thanks to Nicolas Ramz)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It runs in your browser, works on any system and works fine on touch-screen devices like iPads.&lt;lb/&gt; It is written in 100% plain JavaScript and has no dependencies.&lt;lb/&gt; It's 100% free, no ads, no tracking, no accounts, no nothing.&lt;lb/&gt; All processing is done in your browser, no data is sent to any server.&lt;/p&gt;
    &lt;p&gt;The only part that is not included in this repository is the Amiga Emulator Files. (The emulator is based on the Scripted Amiga Emulator)&lt;/p&gt;
    &lt;p&gt;DPaint.js doesn't need building.&lt;lb/&gt; It also has zero dependencies so there's no need to install anything.&lt;lb/&gt; DPaint.js is written using ES6 modules and runs out of the box in modern browsers.&lt;lb/&gt; Just serve "index.html" from a webserver and you're good to go.&lt;/p&gt;
    &lt;p&gt;There's an optional build step to create a compact version of DPaint.js if you like.&lt;lb/&gt; I'm using Parcel.js for this.&lt;lb/&gt; For convenience, I've included a "package.json" file.&lt;lb/&gt; open a terminal and run &lt;code&gt;npm install&lt;/code&gt; to install Parcel.js and its dependencies.
Then run &lt;code&gt;npm run build&lt;/code&gt; to create a compact version of DPaint.js in the "dist" folder.&lt;/p&gt;
    &lt;p&gt;Documentation can be found at https://www.stef.be/dpaint/docs/&lt;/p&gt;
    &lt;p&gt;Dpaint.js is a web application, not an app that you install on your computer. That being said: DPaint.js has no online dependencies and runs fine offline if you want. One caveat: you have to serve the index.html file from a webserver, not just open it in your browser.&lt;lb/&gt; A quick way to do this is - for example - using the Spark app.&lt;lb/&gt; Download the binary for your platform, drop the Spark executable in the folder where you downloaded the Dpaint.js source files and run it. If you then point your browser to http://localhost:8080/ it should work.&lt;/p&gt;
    &lt;p&gt;If you are using Chrome, you can also "install" dpaint.js as app.&lt;lb/&gt; It will then show up your Chrome apps and work offline.&lt;/p&gt;
    &lt;p&gt;Current version is still alpha.&lt;lb/&gt; I'm sure there are bugs and missing features.&lt;lb/&gt; Bug reports and pull requests are welcome.&lt;/p&gt;
    &lt;p&gt;Planned for the next release, already in the works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Color Cycling&lt;/del&gt;(done)&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Animation support (GIf and Amiga ANIM files)&lt;/del&gt;(done)&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Shading/transparency tools that stay within the palette.&lt;/del&gt;(done)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Planned for a future release if there's a need for it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for non-square pixel modes such as HiRes and Interlaced&lt;/item&gt;
      &lt;item&gt;PSD import and export&lt;/item&gt;
      &lt;item&gt;SpriteSheet support&lt;/item&gt;
      &lt;item&gt;Write HAM,SHAM and Dynamic HiRes images&lt;/item&gt;
      &lt;item&gt;Commodore 64 graphics modes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please note that the Brave browser is using "farbling" that introduces random image noise in certain conditions. They claim this is to protect your privacy. Although I totally understand the sentiment, In my opinion a browser should not actively alter the content of a webpage or intentionally break functionality.&lt;lb/&gt; But hey, who am I to speak, it's a free world. Just be aware that if you are using Brave, you will run into issues, so please "lower your shields" for this app in Brave or use another browser.&lt;/p&gt;
    &lt;p&gt;Dpaint.js supports Color-Cycling - a long lost art of "animating" a static image by only rotating some colors in the palette. See an example here:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;The_Vision_cycle.mp4&lt;/head&gt;
    &lt;p&gt;Open the layered source file of the above image directly in Dpaint.js&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/steffest/DPaint-js"/><published>2026-01-25T12:54:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46754427</id><title>Wine-Staging 11.1 Adds Patches for Enabling Recent Photoshop Versions on Linux</title><updated>2026-01-25T18:15:52.517026+00:00</updated><content>&lt;doc fingerprint="de54f53ba99c238b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Wine-Staging 11.1 Adds Patches For Enabling Recent Adobe Photoshop Versions On Linux&lt;/head&gt;
    &lt;p&gt; Following yesterday's release of Wine 11.1 for kicking off the new post-11.0 development cycle, Wine-Staging 11.1 is now available for this experimental/testing version of Wine that present is around 254 patches over the upstream Wine state. &lt;lb/&gt;Besides re-basing those 250+ patches to the latest Wine Git state, the latest VKD3D Git code is also pulled into Wine-Staging 11.1. There is some new feature work with Wine-Staging... Landing the new patches for enabling the recent Adobe Photoshop versions to successfully install and run under Wine on Linux.&lt;lb/&gt;As covered last week, the latest Adobe Photoshop installer is working and the app running with some new patches against the Wine code for its MSXML3 and MSHTML code. Those patches were cited in Bug 47015 for upstream Wine around the Photoshop Creative Cloud 2019 screen hitting MSXML3 errors.&lt;lb/&gt;Upstream Wine Git hasn't yet taken those patches but now they are in Wine-Staging 11.1 for further testing by the community.&lt;lb/&gt;Hopefully the testing will go well and those patches will get picked up by an upstream Wine 11.x bi-weekly development release in the near future.&lt;lb/&gt;Besides those MSHTML/MSXML3 work to benefit Adobe software, there aren't any other new patches in Wine-Staging 11.1. Those looking for new binaries to test it out can find them via WineHQ.org.&lt;/p&gt;
    &lt;p&gt;Besides re-basing those 250+ patches to the latest Wine Git state, the latest VKD3D Git code is also pulled into Wine-Staging 11.1. There is some new feature work with Wine-Staging... Landing the new patches for enabling the recent Adobe Photoshop versions to successfully install and run under Wine on Linux.&lt;/p&gt;
    &lt;p&gt;As covered last week, the latest Adobe Photoshop installer is working and the app running with some new patches against the Wine code for its MSXML3 and MSHTML code. Those patches were cited in Bug 47015 for upstream Wine around the Photoshop Creative Cloud 2019 screen hitting MSXML3 errors.&lt;/p&gt;
    &lt;p&gt;Upstream Wine Git hasn't yet taken those patches but now they are in Wine-Staging 11.1 for further testing by the community.&lt;/p&gt;
    &lt;p&gt;Hopefully the testing will go well and those patches will get picked up by an upstream Wine 11.x bi-weekly development release in the near future.&lt;/p&gt;
    &lt;p&gt;Besides those MSHTML/MSXML3 work to benefit Adobe software, there aren't any other new patches in Wine-Staging 11.1. Those looking for new binaries to test it out can find them via WineHQ.org.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.phoronix.com/news/Wine-Staging-11.1"/><published>2026-01-25T14:42:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46754724</id><title>Show HN: Netfence – Like Envoy for eBPF Filters</title><updated>2026-01-25T18:15:51.737528+00:00</updated><content>&lt;doc fingerprint="780d8d538f3988c3"&gt;
  &lt;main&gt;
    &lt;p&gt;Like Envoy xDS, but for eBPF filters.&lt;/p&gt;
    &lt;p&gt;Netfence runs as a daemon on your VM/container hosts and automatically injects eBPF filter programs into cgroups and network interfaces, with a built-in DNS server that resolves allowed domains and populates the IP allowlist.&lt;/p&gt;
    &lt;p&gt;Netfence daemons connect to a central control plane that you implement via gRPC to synchronize allowlists/denylists with your backend.&lt;/p&gt;
    &lt;p&gt;Your control plane pushes network rules like &lt;code&gt;ALLOW *.pypi.org&lt;/code&gt; or &lt;code&gt;ALLOW 10.0.0.0/16&lt;/code&gt; to attached interfaces/cgroups. When a VM/container queries DNS, Netfence resolves it, adds the IPs to the eBPF filter, and drops traffic to unknown IPs before it leaves the host without any performance penalty.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Attach eBPF filters to network interfaces (TC) or cgroups&lt;/item&gt;
      &lt;item&gt;Policy modes: disabled, allowlist, denylist, block-all&lt;/item&gt;
      &lt;item&gt;IPv4 and IPv6 CIDR support with optional TTLs&lt;/item&gt;
      &lt;item&gt;Per-attachment DNS server with domain allowlist/denylist&lt;/item&gt;
      &lt;item&gt;Domain rules support subdomains with specificity-based matching (more specific rules win)&lt;/item&gt;
      &lt;item&gt;Resolved domains auto-populate IP filter&lt;/item&gt;
      &lt;item&gt;Metadata on daemons and attachments for associating with VM ID, tenant, etc.&lt;/item&gt;
      &lt;item&gt;Support for proxying DNS queries to the control plane to make DNS decisions per-attachment&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;+------------------+         +-------------------------+
|  Your Control    |&amp;lt;-------&amp;gt;|  Daemon (per host)      |
|  Plane (gRPC)    |  stream |                         |
+------------------+         |  +-------------------+  |
                             |  | DNS Server        |  |
                             |  | (per-attachment)  |  |
                             |  +-------------------+  |
                             +-------------------------+
                                        |
                                 +------+------+
                                 |             |
                              TC Filter    Cgroup Filter
                              (veth, eth)  (containers)
&lt;/code&gt;
    &lt;p&gt;Each attachment gets a unique DNS address (port) provisioned by the daemon. Containers/VMs should be configured to use their assigned DNS address.&lt;/p&gt;
    &lt;p&gt;Run the daemon, which:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Exposes a local gRPC API (&lt;code&gt;DaemonService&lt;/code&gt;) for attaching/detaching filters&lt;/item&gt;
      &lt;item&gt;Connects to your control plane via bidirectional stream (&lt;code&gt;ControlPlane.Connect&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Loads and manages eBPF programs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Start the daemon:&lt;/p&gt;
    &lt;code&gt;# Start with default config
netfenced start

# Start with custom config file
netfenced start --config /etc/netfence/config.yaml&lt;/code&gt;
    &lt;p&gt;Check daemon status:&lt;/p&gt;
    &lt;code&gt;netfenced status&lt;/code&gt;
    &lt;p&gt;Your orchestration system calls the daemon's local API.&lt;/p&gt;
    &lt;p&gt;RPC:&lt;/p&gt;
    &lt;code&gt;DaemonService.Attach(interface_name: "veth123", metadata: {vm_id: "abc"})
// or
DaemonService.Attach(cgroup_path: "/sys/fs/cgroup/...", metadata: {container_id: "xyz"})
&lt;/code&gt;
    &lt;p&gt;CLI:&lt;/p&gt;
    &lt;code&gt;# Attach to a network interface (TC)
netfenced attach --interface veth123 --metadata vm_id=abc

# Attach to a cgroup
netfenced attach --cgroup /sys/fs/cgroup/... --metadata container_id=xyz

# Attach with metadata
netfenced attach --interface eth0 --metadata tenant=acme,env=prod&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Daemon attaches eBPF filter to the target&lt;/item&gt;
      &lt;item&gt;Daemon sends &lt;code&gt;Subscribed{id, target, type, metadata}&lt;/code&gt;to control plane and waits for&lt;code&gt;SubscribedAck&lt;/code&gt;with initial config (mode, CIDRs, DNS rules)&lt;/item&gt;
      &lt;item&gt;If the control plane doesn't respond within the timeout (default 5s, configurable via &lt;code&gt;control_plane.subscribe_ack_timeout&lt;/code&gt;), the attachment is rolled back and the attach call fails&lt;/item&gt;
      &lt;item&gt;Daemon watches for target removal and sends &lt;code&gt;Unsubscribed&lt;/code&gt;automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;RPC:&lt;/p&gt;
    &lt;code&gt;DaemonService.Detach(id)
&lt;/code&gt;
    &lt;p&gt;CLI:&lt;/p&gt;
    &lt;code&gt;netfenced detach --id &amp;lt;attachment-id&amp;gt;&lt;/code&gt;
    &lt;p&gt;List attachments:&lt;/p&gt;
    &lt;code&gt;netfenced list
netfenced list --all  # fetch all pages&lt;/code&gt;
    &lt;p&gt;Implement &lt;code&gt;ControlPlane.Connect&lt;/code&gt; RPC - a bidirectional stream:&lt;/p&gt;
    &lt;p&gt;Receive from daemon:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SyncRequest&lt;/code&gt;on connect/reconnect (lists current attachments)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Subscribed&lt;/code&gt;when new attachments are added&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Unsubscribed&lt;/code&gt;when attachments are removed&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Heartbeat&lt;/code&gt;with stats&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Send to daemon:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SyncAck&lt;/code&gt;after receiving SyncRequest&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SubscribedAck{mode, cidrs, dns_config}&lt;/code&gt;after receiving Subscribed (required - daemon waits for this)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SetMode{mode}&lt;/code&gt;- change IP filter policy mode&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AllowCIDR{cidr, ttl}&lt;/code&gt;/&lt;code&gt;DenyCIDR&lt;/code&gt;/&lt;code&gt;RemoveCIDR&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SetDnsMode{mode}&lt;/code&gt;- change DNS filtering mode&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AllowDomain{domain}&lt;/code&gt;/&lt;code&gt;DenyDomain&lt;/code&gt;/&lt;code&gt;RemoveDomain&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;BulkUpdate{mode, cidrs, dns_config}&lt;/code&gt;- full state sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the daemon receives &lt;code&gt;Subscribed&lt;/code&gt;, it blocks waiting for &lt;code&gt;SubscribedAck&lt;/code&gt; before returning success to the caller. This ensures the attachment has its initial configuration before traffic flows. Use the metadata to identify which VM/tenant/container this attachment belongs to and respond with the appropriate initial rules.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/danthegoodman1/netfence"/><published>2026-01-25T15:13:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46754944</id><title>A macOS app that blurs your screen when you slouch</title><updated>2026-01-25T18:15:51.333003+00:00</updated><content>&lt;doc fingerprint="719cd7c635d7d38d"&gt;
  &lt;main&gt;
    &lt;p&gt;A macOS app that blurs your screen when you slouch.&lt;/p&gt;
    &lt;p&gt;Posturr uses your Mac's camera and Apple's Vision framework to monitor your posture in real-time. When it detects that you're slouching, it progressively blurs your screen to remind you to sit up straight. Maintain good posture, and the blur clears instantly.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time posture detection - Uses Apple's Vision framework for body pose and face tracking&lt;/item&gt;
      &lt;item&gt;Progressive screen blur - Gentle visual reminder that intensifies with worse posture&lt;/item&gt;
      &lt;item&gt;Menu bar controls - Easy access to settings, calibration, and status from the menu bar&lt;/item&gt;
      &lt;item&gt;Multi-display support - Works across all connected monitors&lt;/item&gt;
      &lt;item&gt;Privacy-focused - All processing happens locally on your Mac&lt;/item&gt;
      &lt;item&gt;Lightweight - Runs as a background app with minimal resource usage&lt;/item&gt;
      &lt;item&gt;No account required - No signup, no cloud, no tracking&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest &lt;code&gt;Posturr-vX.X.X.zip&lt;/code&gt;from the Releases page&lt;/item&gt;
      &lt;item&gt;Unzip the downloaded file&lt;/item&gt;
      &lt;item&gt;Drag &lt;code&gt;Posturr.app&lt;/code&gt;to your Applications folder&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since Posturr is not signed with an Apple Developer certificate, macOS Gatekeeper will initially block it:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Right-click (or Control-click) on &lt;code&gt;Posturr.app&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Select "Open" from the context menu&lt;/item&gt;
      &lt;item&gt;Click "Open" in the dialog that appears&lt;/item&gt;
      &lt;item&gt;Grant camera access when prompted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You only need to do this once. After the first launch, you can open Posturr normally.&lt;/p&gt;
    &lt;p&gt;Posturr requires camera access to monitor your posture. When you first launch the app, macOS will ask for permission. Click "OK" to grant access.&lt;/p&gt;
    &lt;p&gt;If you accidentally denied permission, you can grant it later:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open System Settings &amp;gt; Privacy &amp;amp; Security &amp;gt; Camera&lt;/item&gt;
      &lt;item&gt;Find Posturr and enable the toggle&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once launched, Posturr appears in your menu bar with a person icon. The app continuously monitors your posture and applies screen blur when slouching is detected.&lt;/p&gt;
    &lt;p&gt;Click the menu bar icon to access:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Status - Shows current state (Monitoring, Slouching, Good Posture, etc.)&lt;/item&gt;
      &lt;item&gt;Enabled - Toggle posture monitoring on/off&lt;/item&gt;
      &lt;item&gt;Recalibrate - Reset your baseline posture (sit up straight, then click)&lt;/item&gt;
      &lt;item&gt;Sensitivity - Adjust how sensitive the slouch detection is (Low, Medium, High, Very High)&lt;/item&gt;
      &lt;item&gt;Dead Zone - Set the tolerance before blur kicks in (None, Small, Medium, Large)&lt;/item&gt;
      &lt;item&gt;Quit - Exit the application (or press Escape anywhere)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Position your camera at eye level when possible&lt;/item&gt;
      &lt;item&gt;Ensure adequate lighting on your face&lt;/item&gt;
      &lt;item&gt;Sit at a consistent distance from your screen&lt;/item&gt;
      &lt;item&gt;The app works best when your shoulders are visible&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr uses Apple's Vision framework to detect body pose landmarks:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Body Pose Detection: Tracks nose, shoulders, and their relative positions&lt;/item&gt;
      &lt;item&gt;Face Detection Fallback: When full body isn't visible, tracks face position&lt;/item&gt;
      &lt;item&gt;Posture Analysis: Measures the vertical distance between nose and shoulders&lt;/item&gt;
      &lt;item&gt;Blur Response: Applies screen blur proportional to posture deviation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The screen blur uses macOS's private CoreGraphics API for efficient, system-level blur that covers all windows and displays.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 (Ventura) or later&lt;/item&gt;
      &lt;item&gt;Xcode Command Line Tools (&lt;code&gt;xcode-select --install&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/yourusername/posturr.git
cd posturr
./build.sh&lt;/code&gt;
    &lt;p&gt;The built app will be in &lt;code&gt;build/Posturr.app&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;# Standard build
./build.sh

# Build with release archive (.zip)
./build.sh --release&lt;/code&gt;
    &lt;code&gt;swiftc -O \
    -framework AppKit \
    -framework AVFoundation \
    -framework Vision \
    -framework CoreImage \
    -o Posturr \
    main.swift&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No code signing: Requires manual Gatekeeper bypass on first launch&lt;/item&gt;
      &lt;item&gt;Camera dependency: Requires a working camera with adequate lighting&lt;/item&gt;
      &lt;item&gt;Detection accuracy: Works best with clear view of upper body/face&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr exposes a file-based command interface for external control:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;capture&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Take a photo and analyze pose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;blur &amp;lt;0-64&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set blur level manually&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;quit&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Exit the application&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Write commands to &lt;code&gt;/tmp/posturr-command&lt;/code&gt;. Responses appear in &lt;code&gt;/tmp/posturr-response&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 (Ventura) or later&lt;/item&gt;
      &lt;item&gt;Camera (built-in or external)&lt;/item&gt;
      &lt;item&gt;Approximately 10MB disk space&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr processes all video data locally on your Mac. No images or data are ever sent to external servers. The camera feed is used solely for posture detection and is never stored or transmitted.&lt;/p&gt;
    &lt;p&gt;MIT License - see LICENSE for details.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! Please feel free to submit issues and pull requests.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built with Apple's Vision framework for body pose detection&lt;/item&gt;
      &lt;item&gt;Uses private CoreGraphics API for efficient screen blur&lt;/item&gt;
      &lt;item&gt;Inspired by the need for better posture during long coding sessions&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/tldev/posturr"/><published>2026-01-25T15:34:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46755115</id><title>Using PostgreSQL as a Dead Letter Queue for Event-Driven Systems</title><updated>2026-01-25T18:15:51.103680+00:00</updated><content>&lt;doc fingerprint="3f4c263785a28d80"&gt;
  &lt;main&gt;
    &lt;p&gt;While I was working on a project with Wayfair, I got the opportunity to work on a system that generated daily business reports aggregated from multiple data sources flowing through event streams across Wayfair. At a high level, Kafka consumers listened to these events, hydrated them with additional data by calling downstream services, and finally persisted the enriched events into a durable datastoreâCloudSQL PostgreSQL on GCP.&lt;/p&gt;
    &lt;p&gt;When everything was healthy, the pipeline worked exactly as expected. Events flowed in, got enriched, and were stored reliably. The real challenge started when things went wrong, which, in distributed systems, is not an exception but a certainty.&lt;/p&gt;
    &lt;p&gt;There were multiple failure scenarios we had to deal with. Sometimes the APIs we depended on for hydration were down or slow. Sometimes the consumer itself crashed midway through processing. In other cases, events arrived with missing or malformed fields that could not be processed safely. These were all situations outside our direct control, but they still needed to be handled gracefully.&lt;/p&gt;
    &lt;p&gt;This is where the concept of a Dead Letter Queue came into the picture. Whenever we knew an event could not be processed successfully, instead of dropping it or blocking the entire consumer, we redirected it to a DLQ so it could be inspected and potentially reprocessed later.&lt;/p&gt;
    &lt;p&gt;Our first instinct was to use Kafka itself as a DLQ. While this is a common pattern, it quickly became clear that it wasn't a great fit for our needs. Kafka is excellent for moving data, but once messages land in a DLQ topic, they are not particularly easy to inspect. Querying by failure reason, retrying a specific subset of events, or even answering simple questions like "what failed yesterday and why?" required extra tooling and custom consumers. For a system that powered business-critical daily reports, this lack of visibility was a serious drawback.&lt;/p&gt;
    &lt;p&gt;That's when we decided to treat PostgreSQL itself as the Dead Letter Queue.&lt;/p&gt;
    &lt;p&gt;Instead of publishing failed events to another Kafka topic, we persisted them directly into a DLQ table in PostgreSQL. We were already using CloudSQL as our durable store, so operationally this added very little complexity. Conceptually, it also made failures first-class citizens in the system rather than opaque messages lost in a stream.&lt;/p&gt;
    &lt;p&gt; Whenever an event failed processingâdue to an API failure, consumer crash, schema mismatch, or validation errorâwe stored the raw event payload along with contextual information about the failure. Each record carried a simple status field. When the event first landed in the DLQ, it was marked as &lt;code&gt;PENDING&lt;/code&gt;. Once it was successfully reprocessed, the status was updated to &lt;code&gt;SUCCEEDED&lt;/code&gt;. Keeping the state model intentionally minimal made it easy to reason about the lifecycle of a failed event.
                    &lt;/p&gt;
    &lt;head rend="h3"&gt;DLQ Table Schema and Indexing Strategy&lt;/head&gt;
    &lt;p&gt;To support inspection, retries, and long-term operability, the DLQ table was designed to be simple, query-friendly, and retry-aware.&lt;/p&gt;
    &lt;head rend="h4"&gt;Table Schema&lt;/head&gt;
    &lt;code&gt;CREATE TABLE dlq_events (
    id BIGSERIAL PRIMARY KEY,
    event_type VARCHAR(255) NOT NULL,
    payload JSONB NOT NULL,
    error_reason TEXT NOT NULL,
    error_stacktrace TEXT,
    status VARCHAR(20) NOT NULL, -- PENDING / SUCCEEDED
    retry_count INT NOT NULL DEFAULT 0,
    retry_after TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);&lt;/code&gt;
    &lt;head rend="h4"&gt;Key Design Considerations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;payload&lt;/code&gt;is stored as&lt;code&gt;JSONB&lt;/code&gt;to preserve the raw event without enforcing a rigid schema.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;status&lt;/code&gt;keeps the lifecycle simple and explicit.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;retry_after&lt;/code&gt;prevents aggressive retries when downstream systems are unstable.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;retry_count&lt;/code&gt;allows retry limits to be enforced without external state.&lt;/item&gt;
      &lt;item&gt;Timestamps make auditing and operational analysis straightforward.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Indexes&lt;/head&gt;
    &lt;code&gt;CREATE INDEX idx_dlq_status
ON dlq_events (status);

CREATE INDEX idx_dlq_status_retry_after
ON dlq_events (status, retry_after);

CREATE INDEX idx_dlq_event_type
ON dlq_events (event_type);

CREATE INDEX idx_dlq_created_at
ON dlq_events (created_at);&lt;/code&gt;
    &lt;p&gt;These indexes allow the retry scheduler to efficiently locate eligible events while still supporting fast debugging and time-based analysis without full table scans.&lt;/p&gt;
    &lt;head rend="h3"&gt;DLQ Retry Mechanism with ShedLock&lt;/head&gt;
    &lt;p&gt;Persisting failed events solved the visibility problem, but we still needed a safe and reliable way to retry them.&lt;/p&gt;
    &lt;p&gt; For this, we introduced a DLQ retry scheduler backed by ShedLock. The scheduler periodically scans the DLQ table for &lt;code&gt;PENDING&lt;/code&gt; events that are eligible for retry and attempts to process them again. Since the service runs on multiple instances, ShedLock ensures that only one instance executes the retry job at any given time. This eliminates duplicate retries without requiring custom leader-election logic.
                    &lt;/p&gt;
    &lt;head rend="h4"&gt;Retry Configuration&lt;/head&gt;
    &lt;code&gt;dlq:
  retry:
    enabled: true
    max-retries: 240
    batch-size: 50
    fixed-rate: 21600000 # 6 hours in milliseconds&lt;/code&gt;
    &lt;head rend="h4"&gt;How Retries Work&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The scheduler runs every six hours.&lt;/item&gt;
      &lt;item&gt;Up to fifty eligible events are picked up per run.&lt;/item&gt;
      &lt;item&gt;Events exceeding the maximum retry count are skipped.&lt;/item&gt;
      &lt;item&gt;Successful retries immediately transition the event status to &lt;code&gt;SUCCEEDED&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Failures remain in &lt;code&gt;PENDING&lt;/code&gt;and are retried in subsequent runs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Query Implementation&lt;/head&gt;
    &lt;p&gt; The retry scheduler uses a SQL query with &lt;code&gt;FOR UPDATE SKIP LOCKED&lt;/code&gt; to safely select eligible events across multiple instances. This PostgreSQL feature ensures that even if multiple scheduler instances run simultaneously, each will pick up different rows without blocking each other:
                    &lt;/p&gt;
    &lt;code&gt;@QueryHints(@QueryHint(name = "jakarta.persistence.lock.timeout", value = "-2"))
@Query(
    value = "SELECT * FROM dlq_table "
        + "WHERE messagetype = :messageType "
        + "AND retries &amp;lt; :maxRetries "
        + "AND (replay_status IS NULL OR replay_status NOT IN ('COMPLETED')) "
        + "ORDER BY created_at ASC "
        + "FOR UPDATE SKIP LOCKED",
    nativeQuery = true
)&lt;/code&gt;
    &lt;p&gt; The &lt;code&gt;FOR UPDATE SKIP LOCKED&lt;/code&gt; clause is crucial here. It allows each instance to lock and process different rows concurrently, preventing duplicate processing while maintaining high throughput. The query hint sets the lock timeout to &lt;code&gt;-2&lt;/code&gt;, which means "wait indefinitely" but combined with &lt;code&gt;SKIP LOCKED&lt;/code&gt;, it effectively means "skip any rows that are already locked by another transaction."
                    &lt;/p&gt;
    &lt;p&gt;This setup allowed the system to tolerate long downstream outages while avoiding retry storms and unnecessary load on dependent services.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operational Benefits&lt;/head&gt;
    &lt;p&gt;With this approach, failures became predictable and observable rather than disruptive. Engineers could inspect failures using plain SQL, identify patterns, and reprocess only the events that mattered. If a downstream dependency was unavailable for hours or even days, events safely accumulated in the DLQ and were retried later without human intervention. If an event was fundamentally bad, it stayed visible instead of being silently dropped.&lt;/p&gt;
    &lt;p&gt;Most importantly, this design reduced operational stress. Failures were no longer something to fear; they were an expected part of the system with a clear, auditable recovery path.&lt;/p&gt;
    &lt;head rend="h3"&gt;My Thoughts&lt;/head&gt;
    &lt;p&gt;The goal was never to replace Kafka with PostgreSQL. Kafka remained the backbone for high-throughput event ingestion, while PostgreSQL handled what it does bestâdurability, querying, and observability around failures. By letting each system play to its strengths, we ended up with a pipeline that was resilient, debuggable, and easy to operate.&lt;/p&gt;
    &lt;p&gt;In the end, using PostgreSQL as a Dead Letter Queue turned failure handling into something boring and predictable. And in production systems, boring is exactly what you want.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.diljitpr.net/blog-post-postgresql-dlq"/><published>2026-01-25T15:51:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46755975</id><title>Will Your AI Teammate Bring Bagels to Standup?</title><updated>2026-01-25T18:15:50.851951+00:00</updated><content>&lt;doc fingerprint="b057119e68c60007"&gt;
  &lt;main&gt;
    &lt;p&gt;The phrase “AI teammate” has dominated marketing around AI and agentic workplace collaboration tools for years. It’s warm, approachable, and rolls off the tongue with the careful calculation of a thousand focus groups. Asana has become the standard-bearer for this terminology, with their AI Teammates product line of “collaborative AI agents,” joined by Atlassian, who published an entire “Team Playbook” entry titled “Your AI Teammate,” walking through how to identify tasks suitable for AI automation and general instructions for how to build custom agents.&lt;/p&gt;
    &lt;p&gt;This week Anthropic made waves by dropping Cowork, which they’re marketing as “Claude Code for the rest of your work.” This research preview for macOS users is designed to perform entire workflows by accessing a folder on your computer, which it can then manipulate. Super useful (if you’re not put off by the potential security risk, of course). What is more, the name “cowork” seems to fit this tool’s intended function. Even Simon Willison approves, noting:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Cowork is a pretty solid choice on the name front!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While AI enthusiasts rejoice, I couldn’t help but be reminded of Sarah Franklin, CEO of Lattice, and the “digital workers” she predicted in an infamous 2024 LinkedIn post. For those who can’t remember that far back, suffice it to say that the internet did not like the term one bit. In fact, according to Sam Forsdick at Raconteur, Lattice “reportedly cancelled its AI employee features following the backlash.”&lt;/p&gt;
    &lt;p&gt;I strongly suspect that if Franklin posted this thought piece today it would barely make a ripple, and it’s worth discussing why. Consider our options. Maybe Franklin is still wrong. After all, ”digital worker” is not synonymous with teammate, coworker, or any of the bevvy of other labels vendors are testing out to describe these AI and agentic tools. Maybe it is the wrong layer of the stack. HR systems handle deeply sensitive personal information and are bound by legal regulations. But I suspect that Franklin was simply too early to the AI teammate party. Technology and model improvements aside, no one, and perhaps HR software users in particular, were not prepared to hear that they will soon have AI colleagues in 2024.&lt;/p&gt;
    &lt;p&gt;But let’s back up. Because the “AI teammate” and “AI coworker” is more than just marketing goo. These terms reveal how vendors want us to think about where agents fit into the future of work. And if you dig deep enough into the weeds of Hacker News posts, enterprise software announcements, and VC think pieces, you’ll begin to get a handle on why we’ve landed here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Etymology of the AI Teammate&lt;/head&gt;
    &lt;p&gt;Are teammates and coworkers synonymous? No, (more on this below), but these and sibling terms—“AI colleague” is another, albeit less popular one—signal a similar move in the market. Consider the evolution: we started with “automation,” which sounded like impersonal factories. Then came “AI assistant,” which felt servile but safe. Then “copilot,” borrowing authority from aviation while keeping humans firmly in the pilot’s seat. And now “teammate” and “cowork[er]”—a horizontal framing that suggests equality rather than hierarchy.&lt;/p&gt;
    &lt;p&gt;This linguistic shift is deliberate, intended to signal addition rather than subtraction. You’re not losing a job to AI, you’re gaining a colleague. The headcount stays the same; it’s just that some of those heads are now silicon.&lt;/p&gt;
    &lt;p&gt;The rise of these horizontal terms is, ultimately, a story about how we relate to the machines that are increasingly doing our work. The “teamwork makes the dream work” framing is seductive because it suggests partnership rather than replacement, collaboration rather than automation. It’s easier to accept help from a “teammate” than to admit you’ve been replaced by software.&lt;/p&gt;
    &lt;p&gt;But the framing also obscures important questions about control, accuracy, and trust. A teammate who “hallucinates,” who “fails at 70% of basic tasks”—that’s not quite the collaborative partner the marketing suggests.&lt;/p&gt;
    &lt;p&gt;So how can we determine whether the AI tools gathered around your company’s metaphorical water cooler are teammates, coworkers, or something else entirely?&lt;/p&gt;
    &lt;p&gt;For starters, AI teammates are generally positioned as collaborators. They occupy a liminal space: not quite as passive as a copilot, not quite as independent as a fully autonomous agent. They reside in the Goldilocks zone of AI tools, possessing just enough autonomy to be useful but not so much that it triggers fears of human replacement.&lt;/p&gt;
    &lt;p&gt;This perhaps explains another notable distinction from earlier generations of AI tools. Teammates and coworkers are anonymous, in contrast to specifically named tools like Devin (Cognition) and Claude (Anthropic). According to Asana’s marketing, these AI teammates are “quick to deliver results and impact across mission-critical functions,” and have job titles like “Campaign Strategist,” “IT Ticketing Specialist,” and “Bug Investigator,” sounding vaguely like corporate-superhero monikers. The shift away from proper nouns is itself revealing—it normalizes AI as a category of worker rather than a singular novelty.&lt;/p&gt;
    &lt;p&gt;Of course, Asana didn’t just stumble into this framing. According to TechCrunch‘s coverage of the June 2024 launch, Paige Costello, then Asana’s head of AI, was explicit about the linguistic choice of “AI teammate”:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We believe that the future of work is humans not just working with humans, but humans also working with AI … And we believe in that world, that it’s going to be just as important to understand what you asked the AI to do, what it did and how much it cost to make that happen.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Perhaps no companies have embraced this type of collaboration-focused terminology more completely than Teammates.ai and Coworker.ai—startups that literally put it in their names. Teammates, originally known as Uktob.ai, rebranded in early 2025 to launch what they call “a category-defining platform that drastically transforms the way businesses operate,” while Coworker markets itself as “The AI agent for complex work.” When you name your company after the metaphor, you’re betting the farm on its staying power.&lt;/p&gt;
    &lt;p&gt;While many vendors are sanguine, it’s reasonable to be suspicious of this horizontal framing because for all it’s descriptive potential it also obscures important questions about control, accuracy, and trust. A teammate who “hallucinates,” who “fails at 70% of basic tasks”—that’s not quite the collaborative partner the marketing suggests.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Teammate-Coworker Distinction&lt;/head&gt;
    &lt;p&gt;Before going further, it’s worth pausing on a distinction that’s easy to gloss over: teammate and coworker are not the same, and the choice between them carries different implications for how we’re meant to relate to AI.&lt;/p&gt;
    &lt;p&gt;A teammate is someone who shares your objectives. You’re playing the same game, working toward the same goal, invested in each other’s success. The teammate framing is intimate and aspirational—it suggests mutual dependence, shared stakes, and collective victory or defeat. A teammate has your back. A teammate will help you win.&lt;/p&gt;
    &lt;p&gt;A coworker, by contrast, is someone who shares your workspace—not necessarily your goals, your projects, or even your lunch table, but the same general orbit of fluorescent lights and coffee machines. The coworker framing is less intimate and more realistic. You might love your teammates, but you tolerate your coworkers. The coworker relationship is defined by proximity and organizational accident rather than chosen collaboration.&lt;/p&gt;
    &lt;p&gt;And perhaps that’s the quiet genius of Anthropic’s “Cowork” branding: it sets expectations at a realistic level. Your AI coworker isn’t promising to help you score the winning goal—it’s just promising to show up, do its job in the adjacent cubicle, and not microwave fish in the break room. To quote Vidyoot Senthil, software engineer at Ergo, “Your AI Coworker Should Be Boring.”&lt;/p&gt;
    &lt;p&gt;Interestingly, the term “AI coworker” has been popular on Hacker News for years. Way back in 2016, TechCrunch ran the headline: “Meet Aiden, your new AI coworker.” Although folks like Connie Loizos have argued that companies should “stop calling your AI a co-worker,” this request seems to have largely failed.&lt;/p&gt;
    &lt;head rend="h2"&gt;So… Are These Just Agents?&lt;/head&gt;
    &lt;p&gt;The AI teammate appears to be, in many cases, a friendlier stand-in for the term “agent”—itself a confusing enough term, as Willison reminds us. So why the rebrand?&lt;/p&gt;
    &lt;p&gt;Google’s Jerop Kipruto, Senior Software Engineer, and Ryan J. Salva, Senior Director of Product Management, published a blog post entitled “Meet your new AI coding teammate: Gemini CLI GitHub Actions” explaining that this teammate is:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;a no-cost, powerful AI coding teammate for your repository. It acts both as an autonomous agent for critical routine coding tasks, and an on-demand collaborator you can quickly delegate work to.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Kipruto and Savla blur the distinction between agent and collaborator, suggesting that it is precisely this overlap that makes Gemini CLI GitHub Actions a teammate. The distinction here is instructive: an agent operates autonomously on your behalf, while a teammate operates collaboratively alongside you. The agent framing emphasizes capability and independence; the teammate framing emphasizes relationship and integration. In practice, the underlying technology may be identical—but the framing shapes how users understand their role relative to the AI.&lt;/p&gt;
    &lt;p&gt;And that explains is why the teammate metaphor may be so appealing. It represents a deliberate retreat from earlier, more threatening terminology. In a world where AI’s relationship to human labor remains deeply uncertain and anxiety inflicting, the teammate offers a comforting narrative of collaboration over competition.&lt;/p&gt;
    &lt;head rend="h2"&gt;The $6 Trillion Teammate&lt;/head&gt;
    &lt;p&gt;If you really want to understand why “AI teammate” has become the vocabulary du jour, follow the money.&lt;/p&gt;
    &lt;p&gt;The World Economic Forum published a piece last year framing AI teammates as a golden investment opportunity:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;AI teammates could present a $6 trillion global opportunity by accelerating productivity and boosting skills and creativity.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The WEF article goes on to tout their virtues for “collaborative intelligence,” arguing that,&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;AI teammates will become a ‘when’ not an ‘if’ question as we attempt to build a better future for all.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yeah maybe up in Davos. But not necessarily in the real world, where not everyone sitting next to you is a CEO or a head of state that flew in on a private jet.&lt;/p&gt;
    &lt;p&gt;Specific figures aside, the money matters when it comes to investing in AI tools. I was not at all surprised to see “ROI” listed as Atlassian’s #1 benefit of the AI teammate play, since demonstrating value is something more end users are demanding now that the blush is off the AI rose. The teammate framing, whatever its emotional appeal, ultimately needs to justify itself in spreadsheet terms.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Hacker News and Reddit Reveal About the Terminology&lt;/head&gt;
    &lt;p&gt;When I first looked into the AI teammate/ coworker idea I wondered if developers would feel more skeptical of it than less technical users. After all, When Emilia David reported on Asana’s AI Teammates launch for The Verge, her headline pointed to the dystopian flip side, warning that the newfangled “‘AI teammate’ can tell people what to do at work.” This clickbaity headline feels quaint by 2026 standards, especially since developers are using AI tools specifically to create PRs. For instance, Greg Foster, Co-founder of Graphite, has written up best practices for improving “AI-generated pull request descriptions.” The work isn’t new, but the bureaucratic task of documenting and assigning it can be automated—and that surely is.&lt;/p&gt;
    &lt;p&gt;To that end, searching for “AI teammate” and “AI Coworker” on Hacker News yields plenty of results, particularly from startups in the agent space that use these phrases in their Show HN and Launch HN posts. But here’s what’s interesting: most of these companies quietly avoid the terminology on their actual marketing websites.&lt;/p&gt;
    &lt;p&gt;Take chatlily.ai’s recent Show HN announcement “We shipped an AI coworker as Claude Cowork launched,” or Shivon AI, which posted a Show HN in mid-2024 titled “AI Teammate for Recruiters—Automates Job Posts, ATS, Emails and CRM.” Visit the actual websites, chatlily.ai and beta.shivonai.com, and these terms are absent from the homepage. Similarly, Promptless is described on Hacker News as “an AI teammate that proactively updates docs while you ship software.” Yet their marketing website leads with “AI Agents.”&lt;/p&gt;
    &lt;p&gt;Reddit bears out this trend as well. I’ve found rampant conversations about AI teammates, such as one Redditor expressing enthusiasm about the “desktop AI teammate” Energent.ai, and an engaging AMA with Asana’s Nik Greenberg, Principal Product Manager, and Bradley Portnoy, Senior Engineering Manager.&lt;/p&gt;
    &lt;p&gt;This pattern suggests a telling split: while AI teammate tests well with human audiences in conversational contexts, it may feel too informal or imprecise for the buttoned-up world of B2B marketing pages. The teammate is welcome at the hackathon; for the enterprise sales deck, it needs to put on a suit and become an “agent.”&lt;/p&gt;
    &lt;head rend="h2"&gt;The AI Engineer Alternative&lt;/head&gt;
    &lt;p&gt;One significant holdout amid the crush of AI teammate boosters is Shawn “swyx” Wang. As founder of the AI Engineer World’s Fair and Latent Space, he has become a hype man for the competing term: the “AI Engineer.” In his writeup of Factory.ai, a company invested in popularizing the term “autonomous coding droids,” swyx draws an implicit contrast:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Unlike products marketing themselves as ‘your AI teammate’, the Factory platform is built by ‘droids’ that you can spin up. These are basically different agent personas: you can have a coding droid, a knowledge retrieval droid, a reliability droid, etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Swyx’s critique of the teammate language here is implicit, but I suspect it connects to what he termed “sloppy thinking” around AI in the workplace during a RedMonk conversation I recorded with him last year. As he explains:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;People were saying things like software is going to go away because AI is going to write all software. That’s very sloppy thinking because obviously this is kind of thing that is said by people who don’t actually do the work.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The teammate framing, from this perspective, might itself be a form of sloppy thinking—an attempt to make AI palatable by obscuring its actual capabilities and limitations behind warm relational language. Swyx has laid out his vision for the “AI Engineer” across three types:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[First] it’s a software engineer that is enhanced by AI, so they use AI coding tools. The second one is a software engineer building AI products. And the third is a non-human software engineer that is completely AI.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That third type—the fully autonomous AI—is notable as the closest analogue to what others are calling a teammate. This type of AI Engineer takes the human out of the loop entirely, but remains categorically within the “AI Engineer” remit. I suspect the trouble others have had with swyx’s three-part definition is that as these tools expand and grow, each type of AI Engineer is going to need its own label. Clearly the terminological pressure isn’t going away.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Future: From Teammate to… Boss?&lt;/head&gt;
    &lt;p&gt;If you think the AI teammate framing is already a bit much, buckle up because apparently we’re all in line for a promotion!&lt;/p&gt;
    &lt;p&gt;Microsoft’s 2025 Work Trend Index introduces the concept of the human as “agent boss“—a term that suggests your new AI teammates will need managing, feedback, and presumably annual reviews. What interests me about Microsoft’s framing is that they use workplace hierarchy labels (“assistant,” “colleague,” “boss”) to characterize the evolving relationship between agents and human workers:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We see the journey to the Frontier Firm playing out in three phases. First, AI acts as an assistant, removing the drudgery of work and helping people do the same work better and faster. In phase 2, agents join teams as “digital colleagues,” taking on specific tasks at human direction—for instance, a researcher agent creating a go-to-market plan. These agents equip employees with new skills that help scale their impact—freeing them to do new and more valuable work. In phase 3, humans set direction for agents that run entire business processes and workflows, checking in as needed.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Many vendors would like to see this as the logical endpoint of the teammate metaphor. Not just working alongside AI, but supervising it. Which raises an interesting question: if the AI teammate needs a boss, is it really a teammate or just a very sophisticated direct report?&lt;/p&gt;
    &lt;p&gt;The next few years will likely determine whether AI teammate becomes as embedded in corporate vocabulary as “cloud computing” or “agile methodology.” Either way, the phenomenon tells us something important about this moment in tech history. We’re still figuring out how to talk about machines that might be genuinely good at their jobs because we haven’t quite figured out what that means for ours.&lt;/p&gt;
    &lt;p&gt;For now, the anthropomorphized AI teammate and AI coworker are here, ready to join your standup, analyze your workflows, and occasionally tell you what to do. Whether you accept that help or long for the simpler days of asking Rachel Stephens to fix your Excel is entirely up to you.&lt;/p&gt;
    &lt;p&gt;Just don’t expect it to bring bagels.&lt;/p&gt;
    &lt;p&gt;Disclaimer: Google, Microsoft, GitHub, and Atlassian are RedMonk clients.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://redmonk.com/kholterhoff/2026/01/16/will-your-ai-teammate-bring-bagels-to-standup/"/><published>2026-01-25T17:21:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46756117</id><title>ICE Using Palantir Tool That Feeds on Medicaid Data</title><updated>2026-01-25T18:15:50.566331+00:00</updated><content>&lt;doc fingerprint="ad964554d5aa1139"&gt;
  &lt;main&gt;
    &lt;p&gt;EFF last summer asked a federal judge to block the federal government from using Medicaid data to identify and deport immigrants.&lt;/p&gt;
    &lt;p&gt;We also warned about the danger of the Trump administration consolidating all of the government’s information into a single searchable, AI-driven interface with help from Palantir, a company that has a shaky-at-best record on privacy and human rights.&lt;/p&gt;
    &lt;p&gt;Now we have the first evidence that our concerns have become reality.&lt;/p&gt;
    &lt;p&gt;“Palantir is working on a tool for Immigration and Customs Enforcement (ICE) that populates a map with potential deportation targets, brings up a dossier on each person, and provides a “confidence score” on the person’s current address,” 404 Media reports today. “ICE is using it to find locations where lots of people it might detain could be based.”&lt;/p&gt;
    &lt;p&gt;The tool – dubbed Enhanced Leads Identification &amp;amp; Targeting for Enforcement (ELITE) – receives peoples’ addresses from the Department of Health and Human Services (which includes Medicaid) and other sources, 404 Media reports based on court testimony in Oregon by law enforcement agents, among other sources.&lt;/p&gt;
    &lt;p&gt;This revelation comes as ICE – which has gone on a surveillance technology shopping spree – floods Minneapolis with agents, violently running roughshod over the civil rights of immigrants and U.S. citizens alike; President Trump has threatened to use the Insurrection Act of 1807 to deploy military troops against protestors there. Other localities are preparing for the possibility of similar surges.&lt;/p&gt;
    &lt;p&gt;Different government agencies necessarily collect information to provide essential services or collect taxes, but the danger comes when the government begins pooling that data and using it for reasons unrelated to the purpose it was collected.&lt;/p&gt;
    &lt;p&gt;This kind of consolidation of government records provides enormous government power that can be abused. Different government agencies necessarily collect information to provide essential services or collect taxes, but the danger comes when the government begins pooling that data and using it for reasons unrelated to the purpose it was collected.&lt;/p&gt;
    &lt;p&gt;As EFF Executive Director Cindy Cohn wrote in a Mercury News op-ed last August, “While couched in the benign language of eliminating government ‘data silos,’ this plan runs roughshod over your privacy and security. It’s a throwback to the rightly mocked ‘Total Information Awareness’ plans of the early 2000s that were, at least publicly, stopped after massive outcry from the public and from key members of Congress. It’s time to cry out again.”&lt;/p&gt;
    &lt;p&gt;In addition to the amicus brief we co-authored challenging ICE’s grab for Medicaid data, EFF has successfully sued over DOGE agents grabbing personal data from the U.S. Office of Personnel Management, filed an amicus brief in a suit challenging ICE’s grab for taxpayer data, and sued the departments of State and Homeland Security to halt a mass surveillance program to monitor constitutionally protected speech by noncitizens lawfully present in the U.S.&lt;/p&gt;
    &lt;p&gt;But litigation isn’t enough. People need to keep raising concerns via public discourse and Congress should act immediately to put brakes on this runaway train that threatens to crush the privacy and security of each and every person in America.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.eff.org/deeplinks/2026/01/report-ice-using-palantir-tool-feeds-medicaid-data"/><published>2026-01-25T17:36:19+00:00</published></entry></feed>