<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-06T04:12:18.997445+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46160773</id><title>Most technical problems are people problems</title><updated>2025-12-06T04:12:28.445619+00:00</updated><content>&lt;doc fingerprint="2ed1d553662c65da"&gt;
  &lt;main&gt;
    &lt;p&gt;I once worked at a company which had an enormous amount of technical debt - millions of lines of code, no unit tests, based on frameworks that were well over a decade out of date. On one specific project, we had a market need to get some Windows-only modules running on Linux, and rather than cross-compiling, another team had simply copied &amp;amp; pasted a few hundred thousand lines of code, swapping Windows-specific components for Linux-specific.&lt;/p&gt;
    &lt;p&gt;For the non-technical reader, this is an enormous problem because now two versions of the code exist. So, all features &amp;amp; bug fixes must be solved in two separate codebases that will grow apart over time. When I heard about this, a young &amp;amp; naive version of me set out to fix the situation....&lt;/p&gt;
    &lt;head rend="h2"&gt;People Problems&lt;/head&gt;
    &lt;p&gt;Tech debt projects are always a hard sell to management, because even if everything goes flawlessly, the code just does roughly what it did before. This project was no exception, and the optics weren't great. I did as many engineers do and "ignored the politics", put my head down, and got it done. But, the project went long, and I lost management's trust in the process.&lt;/p&gt;
    &lt;p&gt;I realized I was essentially trying to solve a people problem with a technical solution. Most of the developers at this company were happy doing the same thing today that they did yesterday...and five years ago. As Andrew Harmel-Law points out, code tends to follow the personalities of the people that wrote it. Personality types who intensely dislike change tend not to design their code with future change in mind.&lt;/p&gt;
    &lt;p&gt;Most technical problems are really people problems. Think about it. Why does technical debt exist? Because requirements weren't properly clarified before work began. Because a salesperson promised an unrealistic deadline to a customer. Because a developer chose an outdated technology because it was comfortable. Because management was too reactive and cancelled a project mid-flight. Because someone's ego wouldn't let them see a better way of doing things.&lt;/p&gt;
    &lt;p&gt;The core issue with the project was that admitting the need for refactoring was also to admit that the way the company was building software was broken and that individual skillsets were sorely out of date. My small team was trying to fix one module of many, while other developers were writing code as they had been for decades. I had one developer openly tell me, "I don't want to learn anything new." I realized that you'll never clean up tech debt faster than others create it. It is like triage in an emergency room, you must stop the bleeding first, then you can fix whatever is broken.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Ideal World&lt;/head&gt;
    &lt;p&gt;The project also disabused me of the engineer's ideal of a world in which engineering problems can be solved in a vacuum - staying out of "politics" and letting the work speak for itself - a world where deadlines don't exist...and let's be honest, neither do customers. This ideal world rarely exists. The vast majority of projects have non-technical stakeholders, and telling them "just trust me; we're working on it" doesn't cut it. I realized that the perception that your team is getting a lot done is just as important as getting a lot done.&lt;/p&gt;
    &lt;p&gt;Non-technical people do not intuitively understand the level of effort required or the need for tech debt cleanup; it must be communicated effectively by engineering - in both initial estimates &amp;amp; project updates. Unless leadership has an engineering background, the value of the technical debt work likely needs to be quantified and shown as business value.&lt;/p&gt;
    &lt;head rend="h2"&gt;Heads Up&lt;/head&gt;
    &lt;p&gt;Perhaps these are the lessons that prep one for more senior positions. In my opinion, anyone above senior engineer level needs to know how to collaborate cross-functionally, regardless of whether they choose a technical or management track. Schools teach Computer Science, not navigating personalities, egos, and personal blindspots.&lt;/p&gt;
    &lt;p&gt;I have worked with some incredible engineers, better than myself - the type that have deep technical knowledge on just about any technology you bring up. When I was younger, I wanted to be that engineer - the "engineer's engineer". But I realize now, that is not my personality. I'm too ADD to be completely heads down. :)&lt;/p&gt;
    &lt;p&gt;For all of their (considerable) strengths, more often than not, those engineers shy away from the interpersonal. They can be incredibly productive ICs, but may fail with bigger initiatives because they are only one person - a single processor core can only go so fast. Perhaps equally valuable is the "heads up coder" - the person who is deeply technical, but also able to pick their head up &amp;amp; see project risks coming (technical &amp;amp; otherwise) and steer the team around them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.joeschrag.com/2023/11/most-technical-problems-are-really.html"/><published>2025-12-05T13:07:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46162656</id><title>Cloudflare outage on December 5, 2025</title><updated>2025-12-06T04:12:28.219228+00:00</updated><content>&lt;doc fingerprint="e418bb5fc591a8cb"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;On December 5, 2025, at 08:47 UTC (all times in this blog are UTC), a portion of Cloudflareâs network began experiencing significant failures. The incident was resolved at 09:12 (~25 minutes total impact), when all services were fully restored.&lt;/p&gt;
      &lt;p&gt;A subset of customers were impacted, accounting for approximately 28% of all HTTP traffic served by Cloudflare. Several factors needed to combine for an individual customer to be affected as described below.&lt;/p&gt;
      &lt;p&gt;The issue was not caused, directly or indirectly, by a cyber attack on Cloudflareâs systems or malicious activity of any kind. Instead, it was triggered by changes being made to our body parsing logic while attempting to detect and mitigate an industry-wide vulnerability disclosed this week in React Server Components.&lt;/p&gt;
      &lt;p&gt;Any outage of our systems is unacceptable, and we know we have let the Internet down again following the incident on November 18. We will be publishing details next week about the work we are doing to stop these types of incidents from occurring.&lt;/p&gt;
      &lt;p&gt;The graph below shows HTTP 500 errors served by our network during the incident timeframe (red line at the bottom), compared to unaffected total Cloudflare traffic (green line at the top).&lt;/p&gt;
      &lt;p&gt;Cloudflare's Web Application Firewall (WAF) provides customers with protection against malicious payloads, allowing them to be detected and blocked. To do this, Cloudflareâs proxy buffers HTTP request body content in memory for analysis. Before today, the buffer size was set to 128KB.&lt;/p&gt;
      &lt;p&gt;As part of our ongoing work to protect customers who use React against a critical vulnerability, CVE-2025-55182, we started rolling out an increase to our buffer size to 1MB, the default limit allowed by Next.js applications, to make sure as many customers as possible were protected.&lt;/p&gt;
      &lt;p&gt;This first change was being rolled out using our gradual deployment system. During rollout, we noticed that our internal WAF testing tool did not support the increased buffer size. As this internal test tool was not needed at that time and had no effect on customer traffic, we made a second change to turn it off.&lt;/p&gt;
      &lt;p&gt;This second change of turning off our WAF testing tool was implemented using our global configuration system. This system does not perform gradual rollouts, but rather propagates changes within seconds to the entire fleet of servers in our network and is under review following the outage we experienced on November 18.Â &lt;/p&gt;
      &lt;p&gt;Unfortunately, in our FL1 version of our proxy, under certain circumstances, the second change of turning off our WAF rule testing tool caused an error state that resulted in 500 HTTP error codes to be served from our network.&lt;/p&gt;
      &lt;p&gt;As soon as the change propagated to our network, code execution in our FL1 proxy reached a bug in our rules module which led to the following Lua exception: &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;[lua] Failed to run module rulesets callback late_routing: /usr/local/nginx-fl/lua/modules/init.lua:314: attempt to index field 'execute' (a nil value)&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;resulting in HTTP code 500 errors being issued.&lt;/p&gt;
      &lt;p&gt;The issue was identified shortly after the change was applied, and was reverted at 09:12, after which all traffic was served correctly.&lt;/p&gt;
      &lt;p&gt;Customers that have their web assets served by our older FL1 proxy AND had the Cloudflare Managed Ruleset deployed were impacted. All requests for websites in this state returned an HTTP 500 error, with the small exception of some test endpoints such as &lt;code&gt;/cdn-cgi/trace&lt;/code&gt;.&lt;/p&gt;
      &lt;p&gt;Customers that did not have the configuration above applied were not impacted. Customer traffic served by our China network was also not impacted.&lt;/p&gt;
      &lt;p&gt;Cloudflareâs rulesets system consists of sets of rules which are evaluated for each request entering our system. A rule consists of a filter, which selects some traffic, and an action which applies an effect to that traffic. Typical actions are â&lt;code&gt;block&lt;/code&gt;â, â&lt;code&gt;log&lt;/code&gt;â, or â&lt;code&gt;skip&lt;/code&gt;â. Another type of action is â&lt;code&gt;execute&lt;/code&gt;â, which is used to trigger evaluation of another ruleset.&lt;/p&gt;
      &lt;p&gt;Our internal logging system uses this feature to evaluate new rules before we make them available to the public. A top level ruleset will execute another ruleset containing test rules. It was these test rules that we were attempting to disable.&lt;/p&gt;
      &lt;p&gt;We have a killswitch subsystem as part of the rulesets system which is intended to allow a rule which is misbehaving to be disabled quickly. This killswitch system receives information from our global configuration system mentioned in the prior sections. We have used this killswitch system on a number of occasions in the past to mitigate incidents and have a well-defined Standard Operating Procedure, which was followed in this incident.&lt;/p&gt;
      &lt;p&gt;However, we have never before applied a killswitch to a rule with an action of â&lt;code&gt;execute&lt;/code&gt;â. When the killswitch was applied, the code correctly skipped the evaluation of the execute action, and didnât evaluate the sub-ruleset pointed to by it. However, an error was then encountered while processing the overall results of evaluating the ruleset:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;if rule_result.action == "execute" then
  rule_result.execute.results = ruleset_results[tonumber(rule_result.execute.results_index)]
end&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This code expects that, if the ruleset has action=âexecuteâ, the ârule_result.executeâ object will exist. However, because the rule had been skipped, the rule_result.execute object did not exist, and Lua returned an error due to attempting to look up a value in a nil value.&lt;/p&gt;
      &lt;p&gt;This is a straightforward error in the code, which had existed undetected for many years. This type of code error is prevented by languages with strong type systems. In our replacement for this code in our new FL2 proxy, which is written in Rust, the error did not occur.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;What about the changes being made after the incident on November 18, 2025?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We made an unrelated change that caused a similar, longer availability incident two weeks ago on November 18, 2025. In both cases, a deployment to help mitigate a security issue for our customers propagated to our entire network and led to errors for nearly all of our customer base.&lt;/p&gt;
      &lt;p&gt;We have spoken directly with hundreds of customers following that incident and shared our plans to make changes to prevent single updates from causing widespread impact like this. We believe these changes would have helped prevent the impact of todayâs incident but, unfortunately, we have not finished deploying them yet.&lt;/p&gt;
      &lt;p&gt;We know it is disappointing that this work has not been completed yet. It remains our first priority across the organization. In particular, the projects outlined below should help contain the impact of these kinds of changes:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Enhanced Rollouts &amp;amp; Versioning: Similar to how we slowly deploy software with strict health validation, data used for rapid threat response and general configuration needs to have the same safety and blast mitigation features. This includes health validation and quick rollback capabilities among other things.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Streamlined break glass capabilities: Ensure that critical operations can still be achieved in the face of additional types of failures. This applies to internal services as well as all standard methods of interaction with the Cloudflare control plane used by all Cloudflare customers.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;"Fail-Open" Error Handling: As part of the resilience effort, we are replacing the incorrectly applied hard-fail logic across all critical Cloudflare data-plane components. If a configuration file is corrupt or out-of-range (e.g., exceeding feature caps), the system will log the error and default to a known-good state or pass traffic without scoring, rather than dropping requests. Some services will likely give the customer the option to fail open or closed in certain scenarios. This will include drift-prevention capabilities to ensure this is enforced continuously.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Before the end of next week we will publish a detailed breakdown of all the resiliency projects underway, including the ones listed above. While that work is underway, we are locking down all changes to our network in order to ensure we have better mitigation and rollback systems before we begin again.&lt;/p&gt;
      &lt;p&gt;These kinds of incidents, and how closely they are clustered together, are not acceptable for a network like ours. On behalf of the team at Cloudflare we want to apologize for the impact and pain this has caused again to our customers and the Internet as a whole.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Time (UTC)&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Status&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Description&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;08:47&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;INCIDENT start&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Configuration change deployed and propagated to the network&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;08:48&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Full impact&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Change fully propagated&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;08:50&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;INCIDENT declared&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Automated alerts&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;09:11&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Change reverted&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Configuration change reverted and propagation start&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;09:12&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;INCIDENT end&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Revert fully propagated, all traffic restored&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/5-december-2025-outage/"/><published>2025-12-05T15:35:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46163121</id><title>I'm Peter Roberts, immigration attorney who does work for YC and startups. AMA</title><updated>2025-12-06T04:12:27.457169+00:00</updated><content>&lt;doc fingerprint="d376f89b34beef3b"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;As usual, there are countless immigration topics and I'll be guided by whatever you're concerned with. Please remember that I can't provide legal advice on specific cases for obvious liability reasons because I won't have access to all the facts. Please stick to a factual discussion in your questions and comments and I'll do the same in my answers!&lt;/p&gt;
      &lt;p&gt;Previous threads we've done: https://news.ycombinator.com/submitted?id=proberts.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46163121"/><published>2025-12-05T16:04:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46163308</id><title>Gemini 3 Pro: the frontier of vision AI</title><updated>2025-12-06T04:12:27.106350+00:00</updated><content>&lt;doc fingerprint="78df88171d739a97"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Gemini 3 Pro: the frontier of vision AI&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro represents a generational leap from simple recognition to true visual and spatial reasoning. It is our most capable multimodal model ever, delivering state-of-the-art performance across document, spatial, screen and video understanding.&lt;/p&gt;
    &lt;p&gt;This model sets new highs on vision benchmarks such as MMMU Pro and Video MMMU for complex visual reasoning, as well as use-case-specific benchmarks across document, spatial, screen and long video understanding.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Document understanding&lt;/head&gt;
    &lt;p&gt;Real-world documents are messy, unstructured, and difficult to parse — often filled with interleaved images, illegible handwritten text, nested tables, complex mathematical notation and non-linear layouts. Gemini 3 Pro represents a major leap forward in this domain, excelling across the entire document processing pipeline — from highly accurate Optical Character Recognition (OCR) to complex visual reasoning.&lt;/p&gt;
    &lt;head rend="h3"&gt;Intelligent perception&lt;/head&gt;
    &lt;p&gt;To truly understand a document, a model must accurately detect and recognize text, tables, math formulas, figures and charts regardless of noise or format.&lt;/p&gt;
    &lt;p&gt;A fundamental capability is "derendering" — the ability to reverse-engineer a visual document back into structured code (HTML, LaTeX, Markdown) that would recreate it. As illustrated below, Gemini 3 demonstrates accurate perception across diverse modalities including converting an 18th-century merchant log into a complex table, or transforming a raw image with mathematical annotation into precise LaTeX code.&lt;/p&gt;
    &lt;p&gt;Example 1: Handwritten Complex Table from 18th century Albany Merchant’s Handbook&lt;/p&gt;
    &lt;p&gt;Example 2: Reconstructing equations from an image&lt;/p&gt;
    &lt;p&gt;Example 3: Reconstructing Florence Nightingale's original Polar Area Diagram into an interactive chart (with a toggle!)&lt;/p&gt;
    &lt;head rend="h3"&gt;Sophisticated reasoning&lt;/head&gt;
    &lt;p&gt;Users can rely on Gemini 3 to perform complex, multi-step reasoning across tables and charts — even in long reports. In fact, the model notably outperforms the human baseline on the CharXiv Reasoning benchmark (80.5%).&lt;/p&gt;
    &lt;p&gt;To illustrate this, imagine a user analyzing the 62-page U.S. Census Bureau "Income in the United States: 2022" report with the following prompt: “Compare the 2021–2022 percent change in the Gini index for "Money Income" versus "Post-Tax Income", and what caused the divergence in the post-tax measure, and in terms of "Money Income", does it show the lowest quintile's share rising or falling?”&lt;/p&gt;
    &lt;p&gt;Swipe through the images below to see the model's step-by-step reasoning.&lt;/p&gt;
    &lt;p&gt;Visual Extraction: To answer the Gini Index Comparison question, Gemini located and cross-referenced this info in Figure 3 about “Money Income decreased by 1.2 percent” and in Table B-3 about “Post-Tax Income increased by 3.2 percent”&lt;/p&gt;
    &lt;p&gt;Causal Logic: Crucially, Gemini 3 does not stop at the numbers; it correlates this gap with the text’s policy analysis, correctly identifying Lapse of ARPA Policies and the end of Stimulus Payments are the main causes.&lt;/p&gt;
    &lt;p&gt;Numerical Comparison: To compare the lowest quantile’s share rising or falling, Gemini3 looked at table A-3, and compared the number of 2.9 and 3.0, and concluded that “the share of aggregate household income held by the lowest quintile was rising.”&lt;/p&gt;
    &lt;p&gt;Final Model Answer&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Spatial understanding&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro is our strongest spatial understanding model so far. Combined with its strong reasoning, this enables the model to make sense of the physical world.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pointing capability: Gemini 3 has the ability to point at specific locations in images by outputting pixel-precise coordinates. Sequences of 2D points can be strung together to perform complex tasks, such as estimating human poses or reflecting trajectories over time.&lt;/item&gt;
      &lt;item&gt;Open vocabulary references: Gemini 3 identifies objects and their intent using an open vocabulary. The most direct application is robotics: the user can ask a robot to generate spatially grounded plans like, “Given this messy table, come up with a plan on how to sort the trash.” This also extends to AR/XR devices, where the user can request an AI assistant to “Point to the screw according to the user manual.”&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;3. Screen understanding&lt;/head&gt;
    &lt;p&gt;Gemini 3.0 Pro’s spatial understanding really shines through its screen understanding of desktop and mobile OS screens. This reliability helps make computer use agents robust enough to automate repetitive tasks. UI understanding capabilities can also enable tasks like QA testing, user onboarding and UX analytics. The following computer use demo shows the model perceiving and clicking with high precision.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Video understanding&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro takes a massive leap forward in how AI understands video, the most complex data format we interact with. It is dense, dynamic, multimodal and rich with context.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;High frame rate understanding: We have optimized the model to be much stronger at understanding fast-paced actions when sampling at &amp;gt;1 frames-per-second. Gemini 3 Pro can capture rapid details — vital for tasks like analyzing golf swing mechanics.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By processing video at 10 FPS—10x the default speed—Gemini 3 Pro catches every swing and shift in weight, unlocking deep insights into player mechanics.&lt;/p&gt;
    &lt;p&gt;2. Video reasoning with “thinking” mode: We upgraded "thinking" mode to go beyond object recognition toward true video reasoning. The model can now better trace complex cause-and-effect relationships over time. Instead of just identifying what is happening, it understands why it is happening.&lt;/p&gt;
    &lt;p&gt;3. Turning long videos into action: Gemini 3 Pro bridges the gap between video and code. It can extract knowledge from long-form content and immediately translate it into functioning apps or structured code.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Real-world applications&lt;/head&gt;
    &lt;p&gt;Here are a few ways we think various fields will benefit from Gemini 3’s capabilities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Education&lt;/head&gt;
    &lt;p&gt;Gemini 3.0 Pro’s enhanced vision capabilities drive significant gains in the education field, particularly for diagram-heavy questions central to math and science. It successfully tackles the full spectrum of multimodal reasoning problems found from middle school through post-secondary curriculums. This includes visual reasoning puzzles (like Math Kangaroo) and complex chemistry and physics diagrams.&lt;/p&gt;
    &lt;p&gt;Gemini 3’s visual intelligence also powers the generative capabilities of Nano Banana Pro. By combining advanced reasoning with precise generation, the model, for example, can help users identify exactly where they went wrong in a homework problem.&lt;/p&gt;
    &lt;p&gt;Prompt: “Here is a photo of my homework attempt. Please check my steps and tell me where I went wrong. Instead of explaining in text, show me visually on my image.” (Note: Student work is shown in blue; model corrections are shown in red). [See prompt in Google AI Studio]&lt;/p&gt;
    &lt;head rend="h3"&gt;Medical and biomedical imaging&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro 1 stands as our most capable general model for medical and biomedical imagery understanding, achieving state-of-the-art performance across major public benchmarks in MedXpertQA-MM (a difficult expert-level medical reasoning exam), VQA-RAD (radiology imagery Q&amp;amp;A) and MicroVQA (multimodal reasoning benchmarks for microscopy based biological research).&lt;/p&gt;
    &lt;p&gt;Input image from MicroVQA - a benchmark for microscopy-based biological research&lt;/p&gt;
    &lt;head rend="h3"&gt;Law and finance&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro’s enhanced document understanding helps professionals in finance and law tackle highly complex workflows. Finance platforms can seamlessly analyze dense reports filled with charts and tables, while legal platforms benefit from the model's sophisticated document reasoning.&lt;/p&gt;
    &lt;head rend="h2"&gt;6. Media resolution control&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro improves the way it processes visual inputs by preserving the native aspect ratio of images. This drives significant quality improvements across the board.&lt;lb/&gt;Additionally, developers gain granular control over performance and cost via the new media_resolution parameter. This allows you to tune visual token usage to balance fidelity against consumption:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;High resolution: Maximizes fidelity for tasks requiring fine detail, such as dense OCR or complex document understanding.&lt;/item&gt;
      &lt;item&gt;Low resolution: Optimizes for cost and latency on simpler tasks, such as general scene recognition or long-context tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For specific recommendations, refer to our Gemini 3.0 Documentation Guide.&lt;/p&gt;
    &lt;head rend="h2"&gt;Build with Gemini 3 Pro&lt;/head&gt;
    &lt;p&gt;We are excited to see what you build with these new capabilities. To get started, check out our developer documentation or play with the model in Google AI Studio today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/technology/developers/gemini-3-pro-vision/"/><published>2025-12-05T16:15:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46163609</id><title>Patterns for Defensive Programming in Rust</title><updated>2025-12-06T04:12:26.807050+00:00</updated><content>&lt;doc fingerprint="2c79df8a6ea533d8"&gt;
  &lt;main&gt;
    &lt;p&gt;I have a hobby.&lt;/p&gt;
    &lt;p&gt;Whenever I see the comment &lt;code&gt;// this should never happen&lt;/code&gt; in code, I try to find out the exact conditions under which it could happen.
And in 90% of cases, I find a way to do just that.
More often than not, the developer just hasn’t considered all edge cases or future code changes.&lt;/p&gt;
    &lt;p&gt;In fact, the reason why I like this comment so much is that it often marks the exact spot where strong guarantees fall apart. Often, violating implicit invariants that aren’t enforced by the compiler are the root cause.&lt;/p&gt;
    &lt;p&gt;Yes, the compiler prevents memory safety issues, and the standard library is best-in-class. But even the standard library has its warts and bugs in business logic can still happen.&lt;/p&gt;
    &lt;p&gt;All we can work with are hard-learned patterns to write more defensive Rust code, learned throughout years of shipping Rust code to production. I’m not talking about design patterns here, but rather small idioms, which are rarely documented, but make a big difference in the overall code quality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code Smell: Indexing Into a Vector&lt;/head&gt;
    &lt;p&gt;Here’s some innocent-looking code:&lt;/p&gt;
    &lt;code&gt;if !matching_users.is_empty   
&lt;/code&gt;
    &lt;p&gt;What if you refactor it and forget to keep the &lt;code&gt;is_empty()&lt;/code&gt; check?
The problem is that the vector indexing is decoupled from checking the length.
So &lt;code&gt;matching_users[0]&lt;/code&gt; can panic at runtime if the vector is empty.&lt;/p&gt;
    &lt;p&gt;Checking the length and indexing are two separate operations, which can be changed independently. That’s our first implicit invariant that’s not enforced by the compiler.&lt;/p&gt;
    &lt;p&gt;If we use slice pattern matching instead, we’ll only get access to the element if the correct &lt;code&gt;match&lt;/code&gt; arm is executed.&lt;/p&gt;
    &lt;code&gt;match matching_users.as_slice   
&lt;/code&gt;
    &lt;p&gt;Note how this automatically uncovered one more edge case: what if the list is empty? We hadn’t explicitly considered this case before. The compiler-enforced pattern matching requires us to think about all possible states! This is a common pattern in all robust Rust code: putting the compiler in charge of enforcing invariants.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code Smell: Lazy use of &lt;code&gt;Default&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;When initializing an object with many fields, it’s tempting to use &lt;code&gt;..Default::default()&lt;/code&gt; to fill in the rest.
In practice, this is a common source of bugs.
You might forget to explicitly set a new field later when you add it to the struct (thus using the default value instead, which might not be what you want), or you might not be aware of all the fields that are being set to default values.&lt;/p&gt;
    &lt;p&gt;Instead of this:&lt;/p&gt;
    &lt;code&gt;let foo = Foo ;
&lt;/code&gt;
    &lt;p&gt;Do this:&lt;/p&gt;
    &lt;code&gt;let foo = Foo ;
&lt;/code&gt;
    &lt;p&gt;Yes, it’s slightly more verbose, but what you gain is that the compiler will force you to handle all fields explicitly. Now when you add a new field to &lt;code&gt;Foo&lt;/code&gt;, the compiler will remind you to set it here as well and reflect on which value makes sense.&lt;/p&gt;
    &lt;p&gt;If you still prefer to use &lt;code&gt;Default&lt;/code&gt; but don’t want to lose compiler checks, you can also destructure the default instance:&lt;/p&gt;
    &lt;code&gt;let Foo   =  default;
&lt;/code&gt;
    &lt;p&gt;This way, you get all the default values assigned to local variables and you can still override what you need:&lt;/p&gt;
    &lt;code&gt;let foo = Foo ;
&lt;/code&gt;
    &lt;p&gt;This pattern gives you the best of both worlds:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You get default values without duplicating default logic&lt;/item&gt;
      &lt;item&gt;The compiler will complain when new fields are added to the struct&lt;/item&gt;
      &lt;item&gt;Your code automatically adapts when default values change&lt;/item&gt;
      &lt;item&gt;It’s clear which fields use defaults and which have custom values&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Code Smell: Fragile Trait Implementations&lt;/head&gt;
    &lt;p&gt;Completely destructuring a struct into its components can also be a defensive strategy for API adherence. For example, let’s say you’re building a pizza ordering system and have an order type like this:&lt;/p&gt;
    &lt;p&gt;For your order tracking system, you want to compare orders based on what’s actually on the pizza - the &lt;code&gt;size&lt;/code&gt;, &lt;code&gt;toppings&lt;/code&gt;, and &lt;code&gt;crust_type&lt;/code&gt;. The &lt;code&gt;ordered_at&lt;/code&gt; timestamp shouldn’t affect whether two orders are considered the same.&lt;/p&gt;
    &lt;p&gt;Here’s the problem with the obvious approach:&lt;/p&gt;
    &lt;p&gt;Now imagine your team adds a field for customization options:&lt;/p&gt;
    &lt;p&gt;Your &lt;code&gt;PartialEq&lt;/code&gt; implementation still compiles, but is it correct?
Should &lt;code&gt;extra_cheese&lt;/code&gt; be part of the equality check?
Probably yes - a pizza with extra cheese is a different order!
But you’ll never know because the compiler won’t remind you to think about it.&lt;/p&gt;
    &lt;p&gt;Here’s the defensive approach using destructuring:&lt;/p&gt;
    &lt;p&gt;Now when someone adds the &lt;code&gt;extra_cheese&lt;/code&gt; field, this code won’t compile anymore.
The compiler forces you to decide: should &lt;code&gt;extra_cheese&lt;/code&gt; be included in the comparison or explicitly ignored with &lt;code&gt;extra_cheese: _&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;This pattern works for any trait implementation where you need to handle struct fields: &lt;code&gt;Hash&lt;/code&gt;, &lt;code&gt;Debug&lt;/code&gt;, &lt;code&gt;Clone&lt;/code&gt;, etc.
It’s especially valuable in codebases where structs evolve frequently as requirements change.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code Smell: &lt;code&gt;From&lt;/code&gt; Impls That Are Really &lt;code&gt;TryFrom&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Sometimes there’s no conversion that will work 100% of the time. That’s fine. When that’s the case, resist the temptation to offer a &lt;code&gt;From&lt;/code&gt; implementation out of habit; use &lt;code&gt;TryFrom&lt;/code&gt; instead.&lt;/p&gt;
    &lt;p&gt;Here’s an example of &lt;code&gt;TryFrom&lt;/code&gt; in disguise:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;unwrap_or_else&lt;/code&gt; is a hint that this conversion can fail in some way.
We set a default value instead, but is it really the right thing to do for all callers?
This should be a &lt;code&gt;TryFrom&lt;/code&gt; implementation instead, making the fallible nature explicit.
We fail fast instead of continuing with a potentially flawed business logic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code Smell: Non-Exhaustive Matches&lt;/head&gt;
    &lt;p&gt;It’s tempting to use &lt;code&gt;match&lt;/code&gt; in combination with a catch-all pattern like &lt;code&gt;_ =&amp;gt; {}&lt;/code&gt;, but this can haunt you later.
The problem is that you might forget to handle a new case that was added later.&lt;/p&gt;
    &lt;p&gt;Instead of:&lt;/p&gt;
    &lt;code&gt;match self  
&lt;/code&gt;
    &lt;p&gt;Use:&lt;/p&gt;
    &lt;code&gt;match self  
&lt;/code&gt;
    &lt;p&gt;By spelling out all variants explicitly, the compiler will warn you when a new variant is added, forcing you to handle it. Another case of putting the compiler to work.&lt;/p&gt;
    &lt;p&gt;If the code for two variants is the same, you can group them:&lt;/p&gt;
    &lt;code&gt;match self  
&lt;/code&gt;
    &lt;head rend="h2"&gt;Code Smell: &lt;code&gt;_&lt;/code&gt; Placeholders for Unused Variables&lt;/head&gt;
    &lt;p&gt;Using &lt;code&gt;_&lt;/code&gt; as a placeholder for unused variables can lead to confusion.
For example, you might get confused about which variable was skipped.
That’s especially true for boolean flags:&lt;/p&gt;
    &lt;code&gt;match self  
&lt;/code&gt;
    &lt;p&gt;In the above example, it’s not clear which variables were skipped and why. Better to use descriptive names for the variables that are not used:&lt;/p&gt;
    &lt;code&gt;match self  
&lt;/code&gt;
    &lt;p&gt;Even if you don’t use the variables, it’s clear what they represent and the code becomes more readable and easier to review without inline type hints.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pattern: Temporary Mutability&lt;/head&gt;
    &lt;p&gt;If you only want your data to be mutable temporarily, make that explicit.&lt;/p&gt;
    &lt;code&gt;let mut data = get_vec;
data.sort;
let data = data;  // Shadow to make immutable

// Here `data` is immutable.
&lt;/code&gt;
    &lt;p&gt;This pattern is often called “temporary mutability” and helps prevent accidental modifications after initialization. See the Rust unofficial patterns book for more details.&lt;/p&gt;
    &lt;p&gt;You can go one step further and do the initialization part in a scope block:&lt;/p&gt;
    &lt;code&gt;let data = ;
// Here `data` is immutable
&lt;/code&gt;
    &lt;p&gt;This way, the mutable variable is confined to the inner scope, making it clear that it’s only used for initialization. In case you use any temporary variables during initialization, they won’t leak into the outer scope. In our case above, there were none, but imagine if we had a temporary vector to hold intermediate results:&lt;/p&gt;
    &lt;code&gt;let data = ;
&lt;/code&gt;
    &lt;p&gt;Here, &lt;code&gt;temp&lt;/code&gt; is only accessible within the inner scope, which prevents it from accidental use later on.&lt;/p&gt;
    &lt;p&gt;This is especially useful when you have multiple temporary variables during initialization that you don’t want accessible in the rest of the function. The scope makes it crystal clear that these variables are only meant for initialization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pattern: Defensively Handle Constructors&lt;/head&gt;
    &lt;p&gt;Tip for libraries&lt;/p&gt;
    &lt;p&gt;The following pattern is only truly helpful for libraries and APIs that need to be robust against future changes. In such a case, you want to ensure that all instances of a type are created through a constructor function that enforces validation logic. Because without that, future refactorings can easily lead to invalid states.&lt;/p&gt;
    &lt;p&gt;For application code, it’s probably best to keep things simple. You typically have all the call sites under control and can ensure that validation logic is always called.&lt;/p&gt;
    &lt;p&gt;Let’s say you have a simple type like the following:&lt;/p&gt;
    &lt;p&gt;Now you want to add validation logic to ensure invalid states are never created. One pattern is to return a &lt;code&gt;Result&lt;/code&gt; from the constructor:&lt;/p&gt;
    &lt;p&gt;But nothing stops someone from bypassing your validation by creating an instance directly:&lt;/p&gt;
    &lt;code&gt;let s = S ;
&lt;/code&gt;
    &lt;p&gt;This should not be possible! It is our implicit invariant that’s not enforced by the compiler: the validation logic is decoupled from struct construction. These are two separate operations, which can be changed independently and the compiler won’t complain.&lt;/p&gt;
    &lt;p&gt;To force external code to go through your constructor, add a private field:&lt;/p&gt;
    &lt;p&gt;Now code outside your module cannot construct &lt;code&gt;S&lt;/code&gt; directly because it cannot access the &lt;code&gt;_private&lt;/code&gt; field.
The compiler enforces that all construction must go through your &lt;code&gt;new()&lt;/code&gt; method, which includes your validation logic!&lt;/p&gt;
    &lt;p&gt;Why the underscore in &lt;code&gt;_private&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Note that the underscore prefix is just a naming convention to indicate the field is intentionally unused; it’s the lack of &lt;code&gt;pub&lt;/code&gt; that makes it private and prevents external construction.&lt;/p&gt;
    &lt;p&gt;For libraries that need to evolve over time, you can also use the &lt;code&gt;#[non_exhaustive]&lt;/code&gt; attribute instead:&lt;/p&gt;
    &lt;p&gt;This has the same effect of preventing construction outside your crate, but also signals to users that you might add more fields in the future. The compiler will prevent them from using struct literal syntax, forcing them to use your constructor.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Should you use #[non_exhaustive]&lt;/code&gt; or &lt;code&gt;_private&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;There’s a big difference between these two approaches:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;#[non_exhaustive]&lt;/code&gt;only works across crate boundaries. It prevents construction outside your crate.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;_private&lt;/code&gt;works at the module boundary. It prevents construction outside the module, but within the same crate.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On top of that, some developers find &lt;code&gt;_private: ()&lt;/code&gt; more explicit about intent: “this struct has a private field that prevents construction.”&lt;/p&gt;
    &lt;p&gt;With &lt;code&gt;#[non_exhaustive]&lt;/code&gt;, the primary intent is signaling that fields might be added in the future, and preventing construction is more of a side effect.&lt;/p&gt;
    &lt;p&gt;But what about code within the same module? With the patterns above, code in the same module can still bypass your validation:&lt;/p&gt;
    &lt;code&gt;// Still compiles in the same module!
let s = S ;
&lt;/code&gt;
    &lt;p&gt;Rust’s privacy works at the module level, not the type level. Anything in the same module can access private items.&lt;/p&gt;
    &lt;p&gt;If you need to enforce constructor usage even within your own module, you need a more defensive approach using nested private modules:&lt;/p&gt;
    &lt;code&gt; 

// Re-export for public use
pub use  S;
&lt;/code&gt;
    &lt;p&gt;Now even code in your outer module cannot construct &lt;code&gt;S&lt;/code&gt; directly because &lt;code&gt;Seal&lt;/code&gt; is trapped in the private &lt;code&gt;inner&lt;/code&gt; module.
Only the &lt;code&gt;new()&lt;/code&gt; method, which lives in the same module as &lt;code&gt;Seal&lt;/code&gt;, can construct it.
The compiler guarantees that all construction, even internal construction, goes through your validation logic.&lt;/p&gt;
    &lt;p&gt;You could still access the public fields directly, though.&lt;/p&gt;
    &lt;code&gt;let s =  new.unwrap;
s.field1 = "".to_string; // Still possible to mutate fields directly
&lt;/code&gt;
    &lt;p&gt;To prevent that, you can make the fields private and provide getter methods instead:&lt;/p&gt;
    &lt;p&gt;Now the only way to create an instance of &lt;code&gt;S&lt;/code&gt; is through the &lt;code&gt;new()&lt;/code&gt; method, and the only way to access its fields is through the getter methods.&lt;/p&gt;
    &lt;head rend="h3"&gt;When to Use Each&lt;/head&gt;
    &lt;p&gt;To enforce validation through constructors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For external code: Add a private field like &lt;code&gt;_private: ()&lt;/code&gt;or use&lt;code&gt;#[non_exhaustive]&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;For internal code: Use nested private modules with a private “seal” type&lt;/item&gt;
      &lt;item&gt;Choose based on your needs: Most code only needs to prevent external construction; forcing internal construction is more defensive but also more complex&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key insight is that by making construction impossible without access to a private type, you turn your validation logic from a convention into a guarantee enforced by the compiler. So let’s put that compiler to work!&lt;/p&gt;
    &lt;head rend="h2"&gt;Pattern: Use &lt;code&gt;#[must_use]&lt;/code&gt; on Important Types&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;#[must_use]&lt;/code&gt; attribute is often neglected.
That’s sad, because it’s such a simple yet powerful mechanism to prevent callers from accidentally ignoring important return values.&lt;/p&gt;
    &lt;p&gt;Now if someone creates a &lt;code&gt;Config&lt;/code&gt; but forgets to use it, the compiler will warn them
(even with a custom message!):&lt;/p&gt;
    &lt;code&gt;let config =  new;
// Warning: Configuration must be applied to take effect
config.with_timeout; 

// Correct usage:
let config =  new 
    .with_timeout;
apply_config;
&lt;/code&gt;
    &lt;p&gt;This is especially useful for guard types that need to be held for their lifetime and results from operations that must be checked. The standard library uses this extensively. For example, &lt;code&gt;Result&lt;/code&gt; is marked with &lt;code&gt;#[must_use]&lt;/code&gt;, which is why you get warnings if you don’t handle errors.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code Smell: Boolean Parameters&lt;/head&gt;
    &lt;p&gt;Boolean parameters make code hard to read at the call site and are error-prone. We all know the scenario where we’re sure this will be the last boolean parameter we’ll ever add to a function.&lt;/p&gt;
    &lt;code&gt;// Too many boolean parameters
 

// At the call site, what do these booleans mean?
process_data;  // What does this do?
&lt;/code&gt;
    &lt;p&gt;It’s impossible to understand what this code does without looking at the function signature. Even worse, it’s easy to accidentally swap the boolean values.&lt;/p&gt;
    &lt;p&gt;Instead, use enums to make the intent explicit:&lt;/p&gt;
    &lt;code&gt; 

 

 

 

// Now the call site is self-documenting
process_data;
&lt;/code&gt;
    &lt;p&gt;This is much more readable and the compiler will catch mistakes if you pass the wrong enum type. You will notice that the enum variants can be more descriptive than just &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt;.
And more often than not, there are more than two meaningful options; especially for programs which grow over time.&lt;/p&gt;
    &lt;p&gt;For functions with many options, you can configure them using a parameter struct:&lt;/p&gt;
    &lt;code&gt; 

 

 

// Usage with preset configurations
process_data;

// Or customize for specific needs
process_data;
&lt;/code&gt;
    &lt;p&gt;This approach scales much better as your function evolves. Adding new parameters doesn’t break existing call sites, and you can easily add defaults or make certain fields optional. The preset methods also document common use cases and make it easy to use the right configuration for different scenarios.&lt;/p&gt;
    &lt;p&gt;Rust is often criticized for not having named parameters, but using a parameter struct is arguably even better for larger functions with many options.&lt;/p&gt;
    &lt;head rend="h2"&gt;Clippy Lints for Defensive Programming&lt;/head&gt;
    &lt;p&gt;Many of these patterns can be enforced automatically using Clippy lints. Here are the most relevant ones:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Lint&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;clippy::indexing_slicing&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Prevents direct indexing into slices and vectors&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;clippy::fallible_impl_from&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Warns about &lt;code&gt;From&lt;/code&gt; implementations that can panic and should be &lt;code&gt;TryFrom&lt;/code&gt; instead.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;clippy::wildcard_enum_match_arm&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Disallows wildcard &lt;code&gt;_&lt;/code&gt; patterns.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;clippy::unneeded_field_pattern&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Identifies when you’re ignoring too many struct fields with &lt;code&gt;..&lt;/code&gt; unnecessarily.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;clippy::fn_params_excessive_bools&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Warns when a function has too many boolean parameters (4 or more by default).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;clippy::must_use_candidate&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Suggests adding &lt;code&gt;#[must_use]&lt;/code&gt; to types that are good candidates for it.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You can enable these in your project by adding them at the top of your crate, e.g.&lt;/p&gt;
    &lt;p&gt;Or in your &lt;code&gt;Cargo.toml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[]
= "deny"
  = "deny"
  = "deny"
  = "deny"
  = "deny"
  = "deny"
  &lt;/code&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Defensive programming in Rust is about leveraging the type system and compiler to catch bugs before they happen. By following these patterns, you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make implicit invariants explicit and compiler-checked&lt;/item&gt;
      &lt;item&gt;Future-proof your code against refactoring mistakes&lt;/item&gt;
      &lt;item&gt;Reduce the surface area for bugs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s a skill that doesn’t come naturally and it’s not covered in most Rust books, but knowing these patterns can make the difference between code that works but is brittle, and code that is robust and maintainable for years to come.&lt;/p&gt;
    &lt;p&gt;Remember: if you find yourself writing &lt;code&gt;// this should never happen&lt;/code&gt;, take a step back and ask how the compiler could enforce that invariant for you instead.
The best bug is the one that never compiles in the first place.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://corrode.dev/blog/defensive-programming/"/><published>2025-12-05T16:34:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46163977</id><title>Onlook (YC W25) the Cursor for Designers Is Hiring a Founding Fullstack Engineer</title><updated>2025-12-06T04:12:26.392741+00:00</updated><content>&lt;doc fingerprint="be519058de7ce427"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hey HN! I'm Daniel, building Onlook, the Cursor for Designers. We built an open-source collaborative canvas for code that lets designers and developers craft incredible web experiences together.&lt;/p&gt;
      &lt;p&gt;Since launching, Onlook hit #1 on Hacker News, was the #1 trending repo on GitHub—above DeepSeek + Anthropic—and has earned over 23,000 GitHub stars. We’re looking to bring on Onlook’s first Founding Engineers.&lt;/p&gt;
      &lt;p&gt;This role requires autonomy - you’ll be setting standards for one of the fastest-growing open source projects backed by YC ever. You’ll help design and build an uncompromising visual IDE loved by tens of thousands of designers and engineers around the world, and you'll have a heavy influence on the direction of where we take the company.&lt;/p&gt;
      &lt;p&gt;You’re a full-stack engineer based in the U.S. who is ultra comfortable in Typescript, NextJS, React, and Tailwind, and ready to jump-in and build.&lt;/p&gt;
      &lt;p&gt;The most important things we look for:&lt;/p&gt;
      &lt;p&gt;• Olympic-level dedication – you want to be the best in the world at what you do.&lt;/p&gt;
      &lt;p&gt;• Ownership – you like autonomy and control over the destiny of the company.&lt;/p&gt;
      &lt;p&gt;• Speed – you’re comfortable shipping and iterating quickly with feedback.&lt;/p&gt;
      &lt;p&gt;• Craft – you’re opinionated and are willing to defend your opinions.&lt;/p&gt;
      &lt;p&gt;Ideally, you:&lt;/p&gt;
      &lt;p&gt;• Are looking for a fast-paced, early startup environment.&lt;/p&gt;
      &lt;p&gt;• Are willing to put in long hours and go the extra mile.&lt;/p&gt;
      &lt;p&gt;• Are comfortable with any part of the stack, front-end, back-end, or database.&lt;/p&gt;
      &lt;p&gt;• Believe in open source and are ok with your work being very public.&lt;/p&gt;
      &lt;p&gt;The comp range for this role is $130k-200k, 1-5% equity, great healthcare + other perks, and an awesome office if you happen to be in SF. We're open to remote / hybrid candidates.&lt;/p&gt;
      &lt;p&gt;If you’d like to stand out, please share a project or piece of work that you’re most proud of. We love seeing people’s work. If you have a personal website, please include that as well.&lt;/p&gt;
      &lt;p&gt;If you're interested, email daniel@onlook.com with your Github / LinkedIn / Website or work samples and why you'd be a great addition to the team, or apply here: https://www.ycombinator.com/companies/onlook/jobs/e4gHv1n-fo...&lt;/p&gt;
      &lt;p&gt;Excited to meet, and build alongside you!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46163977"/><published>2025-12-05T17:00:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46166708</id><title>Fizz Buzz in CSS</title><updated>2025-12-06T04:12:26.034859+00:00</updated><content>&lt;doc fingerprint="8e5cd03f5f84b2b9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fizz Buzz in CSS&lt;/head&gt;
    &lt;p&gt;What is the smallest CSS we can write to produce the Fizz Buzz sequence? One could of course do this with no CSS at all, simply by placing the entire sequence as plain text in the HTML body. So to make the problem precise and keep it interesting, we require that every number and word that appears in the output must come directly from the CSS. Placing any part of the output numbers or words outside the stylesheet or using JavaScript is not allowed. With this constraint, I think it can be done in four lines of CSS as shown below:&lt;/p&gt;
    &lt;code&gt;
 li { counter-increment: n }
li:not(:nth-child(5n))::before { content: counter(n) }
li:nth-child(3n)::before { content: "Fizz" }
li:nth-child(5n)::after { content: "Buzz" }
&lt;/code&gt;
    &lt;p&gt;Here is a complete working example: css-fizz-buzz.html.&lt;/p&gt;
    &lt;p&gt;I am neither a web developer nor a code-golfer. Seasoned code-golfers looking for a challenge can probably shrink this solution further. However, such wizards are also likely to scoff at any mention of counting lines of code, since CSS can be collapsed into a single line. The number of characters is probably more meaningful. The code can also be minified slightly by removing all whitespace:&lt;/p&gt;
    &lt;code&gt;$ curl -sS https://susam.net/css-fizz-buzz.html | sed -n '/counter/,/after/p' | tr -d '[:space:]'
li{counter-increment:n}li:not(:nth-child(5n))::before{content:counter(n)}li:nth-child(3n)::before{content:"Fizz"}li:nth-child(5n)::after{content:"Buzz"}&lt;/code&gt;
    &lt;p&gt;This minified version is composed of 152 characters:&lt;/p&gt;
    &lt;code&gt;$ curl -sS https://susam.net/css-fizz-buzz.html | sed -n '/counter/,/after/p' | tr -d '[:space:]' | wc -c
152&lt;/code&gt;
    &lt;p&gt;If you manage to create a shorter solution, please do leave a comment.&lt;/p&gt;
    &lt;p&gt;See also: Fizz Buzz with Cosines.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://susam.net/fizz-buzz-in-css.html"/><published>2025-12-05T20:18:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46167349</id><title>The missing standard library for multithreading in JavaScript</title><updated>2025-12-06T04:12:25.537979+00:00</updated><content>&lt;doc fingerprint="c2063f2068bab0a8"&gt;
  &lt;main&gt;
    &lt;p&gt;Multithreading is a TypeScript library that brings robust, Rust-inspired concurrency primitives to the JavaScript ecosystem. It provides a thread-pool architecture, strict memory safety semantics, and synchronization primitives like Mutexes, Read-Write Locks, and Condition Variables.&lt;/p&gt;
    &lt;p&gt;This library is designed to abstract away the complexity of managing &lt;code&gt;WebWorkers&lt;/code&gt;, serialization, and &lt;code&gt;SharedArrayBuffer&lt;/code&gt; complexities, allowing developers to write multi-threaded code that looks and feels like standard asynchronous JavaScript.&lt;/p&gt;
    &lt;code&gt;npm install multithreading&lt;/code&gt;
    &lt;p&gt;JavaScript is traditionally single-threaded. To achieve true parallelism, this library uses Web Workers. However, unlike standard Workers, this library offers:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Managed Worker Pool: Automatically manages a pool of threads based on hardware concurrency.&lt;/item&gt;
      &lt;item&gt;Shared Memory Primitives: Tools to safely share state between threads without race conditions.&lt;/item&gt;
      &lt;item&gt;Scoped Imports: Support for importing external modules and relative files directly within worker tasks.&lt;/item&gt;
      &lt;item&gt;Move Semantics: Explicit data ownership transfer to prevent cloning overhead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The entry point for most operations is the &lt;code&gt;spawn&lt;/code&gt; function. This submits a task to the thread pool and returns a handle to await the result.&lt;/p&gt;
    &lt;code&gt;import { spawn } from "multithreading";

// Spawn a task on a background thread
const handle = spawn(() =&amp;gt; {
  // This code runs in a separate worker
  const result = Math.random();
  return result;
});

// Wait for the result
const result = await handle.join();

if (result.ok) {
  console.log("Result:", result.value); // 0.6378467071314606
} else {
  console.error("Worker error:", result.error);
}&lt;/code&gt;
    &lt;p&gt;Because Web Workers run in a completely isolated context, functions passed to &lt;code&gt;spawn&lt;/code&gt; cannot capture variables from their outer scope. If you attempt to use a variable inside the worker that was defined outside of it, the code will fail.&lt;/p&gt;
    &lt;p&gt;To get data from your main thread into the worker, you have to use the &lt;code&gt;move()&lt;/code&gt; function.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;move&lt;/code&gt; function accepts variadic arguments. These arguments are passed to the worker function in the order they were provided. Despite the name, &lt;code&gt;move&lt;/code&gt; handles data in two ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Transferable Objects (e.g., &lt;code&gt;ArrayBuffer&lt;/code&gt;,&lt;code&gt;Uint32Array&lt;/code&gt;): These are "moved" (zero-copy). Ownership transfers to the worker, and the original becomes unusable in the main thread.&lt;/item&gt;
      &lt;item&gt;Non-Transferable Objects (e.g., JSON, numbers, strings): These are cloned via structured cloning. They remain usable in the main thread.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import { spawn, move } from "multithreading";

// Will be transfered
const largeData = new Uint8Array(1024 * 1024 * 10); // 10MB
// Will be cloned
const metaData = { id: 1 };

// We pass arguments as a comma-separated list.
const handle = spawn(move(largeData, metaData), (data, meta) =&amp;gt; {
  console.log("Processing ID:", meta.id);
  return data.byteLength;
});

await handle.join();&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;SharedJsonBuffer&lt;/code&gt; enables Mutex-protected shared memory for JSON objects, eliminating the overhead of &lt;code&gt;postMessage&lt;/code&gt; data copying. Unlike standard buffers, it handles serialization automatically. It supports partial updates, re-serializing only changed bytes rather than the entire object tree for high-performance state synchronization.&lt;/p&gt;
    &lt;code&gt;import { move, Mutex, SharedJsonBuffer, spawn } from "multithreading";

const sharedState = new Mutex(new SharedJsonBuffer({
  score: 0,
  players: ["Main Thread"],
  level: {
    id: 1,
    title: "Start",
  },
}));

await spawn(move(sharedState), async (lock) =&amp;gt; {
  using guard = await lock.acquire();

  const state = guard.value;

  console.log(`Current Score: ${state.score}`);

  // Modify the data
  state.score += 100;
  state.players.push("Worker1");

  // End of scope: Lock is automatically released here
}).join();

// Verify on main thread
using guard = await sharedState.acquire();

console.log(guard.value); // { score: 100, players: ["Main Thread", "Worker1"], ... }&lt;/code&gt;
    &lt;p&gt;When multiple threads access shared memory (via &lt;code&gt;SharedArrayBuffer&lt;/code&gt;), race conditions occur. This library provides primitives to synchronize access safely.&lt;/p&gt;
    &lt;p&gt;Best Practice: It is highly recommended to use the asynchronous methods (e.g., &lt;code&gt;acquire&lt;/code&gt;, &lt;code&gt;read&lt;/code&gt;, &lt;code&gt;write&lt;/code&gt;, &lt;code&gt;wait&lt;/code&gt;) rather than their synchronous counterparts. Synchronous blocking halts the entire Worker thread, potentially pausing other tasks sharing that worker.&lt;/p&gt;
    &lt;p&gt;A &lt;code&gt;Mutex&lt;/code&gt; ensures that only one thread can access a specific piece of data at a time.&lt;/p&gt;
    &lt;p&gt;This library leverages the Explicit Resource Management proposal (&lt;code&gt;using&lt;/code&gt; keyword). When you acquire a lock, it returns a guard. When that guard goes out of scope, the lock is automatically released.&lt;/p&gt;
    &lt;code&gt;import { spawn, move, Mutex } from "multithreading";

const buffer = new SharedArrayBuffer(4);
const counterMutex = new Mutex(new Int32Array(buffer));

spawn(move(counterMutex), async (mutex) =&amp;gt; {
  // 'using' automatically calls dispose() at the end of the scope
  using guard = await mutex.acquire();
  
  guard.value[0]++;
  
  // End of scope: Lock is automatically released here
});&lt;/code&gt;
    &lt;p&gt;If you are using Bun (which doesn't natively support &lt;code&gt;using&lt;/code&gt; and uses a transpiler which is incompatible with this library) or prefer standard JavaScript syntax, you must manually release the lock using &lt;code&gt;drop()&lt;/code&gt;. Always use a &lt;code&gt;try...finally&lt;/code&gt; block to ensure the lock is released even if an error occurs.&lt;/p&gt;
    &lt;code&gt;import { spawn, move, Mutex } from "multithreading";

const buffer = new SharedArrayBuffer(4);
const counterMutex = new Mutex(new Int32Array(buffer));

spawn(move(counterMutex), async (mutex) =&amp;gt; {
  // Note that we have to import drop here, otherwise it wouldn't be available
  const { drop } = await import("multithreading");

  // 1. Acquire the lock manually
  const guard = await mutex.acquire();

  try {
    // 2. Critical Section
    guard.value[0]++;
  } finally {
    // 3. Explicitly release the lock
    drop(guard);
  }
});&lt;/code&gt;
    &lt;p&gt;A &lt;code&gt;RwLock&lt;/code&gt; is optimized for scenarios where data is read often but written rarely. It allows multiple simultaneous readers but only one writer.&lt;/p&gt;
    &lt;code&gt;import { spawn, move, RwLock } from "multithreading";

const lock = new RwLock(new Int32Array(new SharedArrayBuffer(4)));

// Spawning a Writer
spawn(move(lock), async (l) =&amp;gt; {
  // Blocks until all readers are finished (asynchronously)
  using guard = await l.write(); 
  guard.value[0] = 42;
});

// Spawning Readers
spawn(move(lock), async (l) =&amp;gt; {
  // Multiple threads can hold this lock simultaneously
  using guard = await l.read(); 
  console.log(guard.value[0]);
});&lt;/code&gt;
    &lt;p&gt;A &lt;code&gt;Semaphore&lt;/code&gt; limits the number of threads that can access a resource simultaneously. Unlike a Mutex (which allows exactly 1 owner), a Semaphore allows &lt;code&gt;N&lt;/code&gt; owners. This is essential for rate limiting, managing connection pools, or bounding concurrency.&lt;/p&gt;
    &lt;code&gt;import { spawn, move, Semaphore } from "multithreading";

// Initialize with 3 permits (allowing 3 concurrent tasks)
const semaphore = new Semaphore(3);

for (let i = 0; i &amp;lt; 10; i++) {
  spawn(move(semaphore), async (sem) =&amp;gt; {
    console.log("Waiting for slot...");
    
    // Will wait (async) if 3 threads are already working
    using _ = await sem.acquire(); 
    
    console.log("Acquired slot! Working...");

    await new Promise(r =&amp;gt; setTimeout(r, 1000));
    
    // Guard is disposed automatically, releasing the permit for the next thread
  });
}&lt;/code&gt;
    &lt;p&gt;Like the Mutex, if you cannot use the &lt;code&gt;using&lt;/code&gt; keyword, you can manually manage the lifecycle.&lt;/p&gt;
    &lt;code&gt;spawn(move(semaphore), async (sem) =&amp;gt; {
  const { drop } = await import("multithreading");
  // Acquire 2 permits at once
  const guard = await sem.acquire(2);
  
  try {
    // Critical Section
  } finally {
    // Release the 2 permits
    drop(guard);
  }
});&lt;/code&gt;
    &lt;p&gt;A &lt;code&gt;Condvar&lt;/code&gt; allows threads to wait for a specific condition to become true. It saves CPU resources by putting the task to sleep until it is notified, rather than constantly checking a value.&lt;/p&gt;
    &lt;code&gt;import { spawn, move, Mutex, Condvar } from "multithreading";

const mutex = new Mutex(new Int32Array(new SharedArrayBuffer(4)));
const cv = new Condvar();

spawn(move(mutex, cv), async (lock, cond) =&amp;gt; {
  using guard = await lock.acquire();
  
  // Wait until value is not 0
  while (guard.value[0] === 0) {
    // wait() unlocks the mutex, waits for notification, then re-locks.
    await cond.wait(guard);
  }
  
  console.log("Received signal, value is:", guard.value[0]);
});&lt;/code&gt;
    &lt;p&gt;For higher-level communication, this library provides a Multi-Producer, Multi-Consumer (MPMC) bounded channel. This primitive mimics Rust's &lt;code&gt;std::sync::mpsc&lt;/code&gt; but allows for multiple consumers. It acts as a thread-safe queue that handles backpressure, blocking receivers when empty and blocking senders when full.&lt;/p&gt;
    &lt;p&gt;Channels are the preferred way to coordinate complex workflows (like job queues or pipelines) between workers without manually managing locks.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arbitrary JSON Data: Channels are backed by &lt;code&gt;SharedJsonBuffer&lt;/code&gt;, allowing you to send any JSON-serializable value (objects, arrays, strings, numbers, booleans) through the channel, not just raw integers.&lt;/item&gt;
      &lt;item&gt;Bounded: You define a capacity. If the channel is full, &lt;code&gt;send()&lt;/code&gt;waits. If empty,&lt;code&gt;recv()&lt;/code&gt;waits.&lt;/item&gt;
      &lt;item&gt;Clonable: Both &lt;code&gt;Sender&lt;/code&gt;and&lt;code&gt;Receiver&lt;/code&gt;can be cloned and moved to different workers.&lt;/item&gt;
      &lt;item&gt;Reference Counted: The channel automatically closes when all Senders are dropped (indicating no more data will arrive) or all Receivers are dropped.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;import { spawn, move, channel } from "multithreading";

// Create a channel that holds objects
const [tx, rx] = channel&amp;lt;{ hello: string }&amp;gt;();

// Producer Thread
spawn(move(tx), async (sender) =&amp;gt; {
  await sender.send({ hello: "world" });
  await sender.send({ hello: "multithreading" });
  // Sender is destroyed here, automatically closing the channel
});

// Consumer Thread
spawn(move(rx.clone()), async (rx) =&amp;gt; {
  for await (const value of rx) {
    console.log(value); // { hello: "world" }
  }
});

// Because we cloned rx, we can also receive on the main thread 
for await (const value of rx) {
  console.log(value); // { hello: "world" }
}&lt;/code&gt;
    &lt;p&gt;One of the most difficult aspects of Web Workers is handling imports. This library handles this automatically, enabling you to use dynamic &lt;code&gt;await import()&lt;/code&gt; calls inside your spawned functions.&lt;/p&gt;
    &lt;p&gt;You can import:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;External Libraries: Packages from npm/CDN (depending on environment).&lt;/item&gt;
      &lt;item&gt;Relative Files: Files relative to the file calling &lt;code&gt;spawn&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: The function passed to &lt;code&gt;spawn&lt;/code&gt; must be self-contained or explicitly import what it needs. It cannot access variables from the outer scope unless they are passed via &lt;code&gt;move()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Assume you have a file structure:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;main.ts&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;utils.ts&lt;/code&gt;(contains&lt;code&gt;export const magicNumber = 42;&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;// main.ts
import { spawn } from "multithreading";

spawn(async () =&amp;gt; {
  // 1. Importing a relative file
  // This path is relative to 'main.ts' (the caller location)
  const utils = await import("./utils.ts");
  // 2. Importing an external library (e.g., from a URL or node_modules resolution)
  const _ = await import("lodash");

  console.log("Magic number from relative file:", utils.magicNumber);
  console.log("Random number via lodash:", _.default.random(1, 100));
  
  return utils.magicNumber;
});&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;spawn(fn)&lt;/code&gt;: Runs a function in a worker.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;spawn(move(arg1, arg2, ...), fn)&lt;/code&gt;: Runs a function in a worker with specific arguments transferred or copied.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;initRuntime(config)&lt;/code&gt;: Initializes the thread pool (optional, lazy loaded by default).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;shutdown()&lt;/code&gt;: Terminates all workers in the pool.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;move(...args)&lt;/code&gt;: Marks arguments for transfer (ownership move) rather than structured clone. Accepts a variable number of arguments which map to the arguments of the worker function.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;drop(resource)&lt;/code&gt;: Explicitly disposes of a resource (calls&lt;code&gt;[Symbol.dispose]&lt;/code&gt;). This is required for manual lock management in environments like Bun.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SharedJsonBuffer&lt;/code&gt;: A class for storing JSON objects in shared memory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;channel&amp;lt;T&amp;gt;(capacity)&lt;/code&gt;: Creates a new channel. Returns&lt;code&gt;[Sender&amp;lt;T&amp;gt;, Receiver&amp;lt;T&amp;gt;]&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Sender&amp;lt;T&amp;gt;&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;send(value)&lt;/code&gt;: Async. Returns&lt;code&gt;Promise&amp;lt;Result&amp;lt;void, Error&amp;gt;&amp;gt;&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;code&gt;sendSync(value)&lt;/code&gt;: Blocking. Returns&lt;code&gt;Result&amp;lt;void, Error&amp;gt;&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;code&gt;clone()&lt;/code&gt;: Creates a new handle to the same channel (increments ref count).&lt;/item&gt;&lt;item&gt;&lt;code&gt;close()&lt;/code&gt;: Manually closes the channel for everyone.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Receiver&amp;lt;T&amp;gt;&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;recv()&lt;/code&gt;: Async. Returns&lt;code&gt;Promise&amp;lt;Result&amp;lt;T, Error&amp;gt;&amp;gt;&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;code&gt;recvSync()&lt;/code&gt;: Blocking. Returns&lt;code&gt;Result&amp;lt;T, Error&amp;gt;&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;code&gt;clone()&lt;/code&gt;: Creates a new handle to the same channel.&lt;/item&gt;&lt;item&gt;&lt;code&gt;close()&lt;/code&gt;: Manually drops this handle.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Mutex&amp;lt;T&amp;gt;&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;acquire()&lt;/code&gt;: Async lock (Recommended). Returns&lt;code&gt;Promise&amp;lt;MutexGuard&amp;gt;&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;code&gt;tryLock()&lt;/code&gt;: Non-blocking attempt. Returns boolean.&lt;/item&gt;&lt;item&gt;&lt;code&gt;acquireSync()&lt;/code&gt;: Blocking lock (Halts Worker). Returns&lt;code&gt;MutexGuard&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;RwLock&amp;lt;T&amp;gt;&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;read()&lt;/code&gt;: Async shared read access (Recommended).&lt;/item&gt;&lt;item&gt;&lt;code&gt;write()&lt;/code&gt;: Async exclusive write access (Recommended).&lt;/item&gt;&lt;item&gt;&lt;code&gt;readSync()&lt;/code&gt;/&lt;code&gt;writeSync()&lt;/code&gt;: Synchronous/Blocking variants.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Semaphore&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;acquire(amount?)&lt;/code&gt;: Async wait for&lt;code&gt;n&lt;/code&gt;permits. Returns&lt;code&gt;SemaphoreGuard&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;code&gt;tryAcquire(amount?)&lt;/code&gt;: Non-blocking. Returns&lt;code&gt;SemaphoreGuard&lt;/code&gt;or&lt;code&gt;null&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;&lt;code&gt;acquireSync(amount?)&lt;/code&gt;: Blocking wait. Returns&lt;code&gt;SemaphoreGuard&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Condvar&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;wait(guard)&lt;/code&gt;: Async wait (Recommended). Yields execution.&lt;/item&gt;&lt;item&gt;&lt;code&gt;notifyOne()&lt;/code&gt;: Wake one waiting thread.&lt;/item&gt;&lt;item&gt;&lt;code&gt;notifyAll()&lt;/code&gt;: Wake all waiting threads.&lt;/item&gt;&lt;item&gt;&lt;code&gt;waitSync(guard)&lt;/code&gt;: Blocking wait (Halts Worker).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For advanced users interested in the internal mechanics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Serialization Protocol: The library uses a custom "Envelope" protocol (&lt;code&gt;PayloadType.RAW&lt;/code&gt;vs&lt;code&gt;PayloadType.LIB&lt;/code&gt;). This allows complex objects like&lt;code&gt;Mutex&lt;/code&gt;handles to be serialized, sent to a worker, and rehydrated into a functional object connected to the same&lt;code&gt;SharedArrayBuffer&lt;/code&gt;on the other side.&lt;/item&gt;
      &lt;item&gt;Atomics: Synchronization is built on &lt;code&gt;Int32Array&lt;/code&gt;backed by&lt;code&gt;SharedArrayBuffer&lt;/code&gt;using&lt;code&gt;Atomics.wait&lt;/code&gt;and&lt;code&gt;Atomics.notify&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Import Patching: The &lt;code&gt;spawn&lt;/code&gt;function analyzes the stack trace to determine the caller's file path. It then regex-patches&lt;code&gt;import()&lt;/code&gt;statements within the worker code string to ensure relative paths resolve correctly against the caller's location, rather than the worker's location.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/W4G1/multithreading"/><published>2025-12-05T21:09:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46167500</id><title>Perpetual futures, explained</title><updated>2025-12-06T04:12:25.340345+00:00</updated><content>&lt;doc fingerprint="e285119581a6a7b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Programming note: Bits about Money is supported by our readers. I generally forecast about one issue a month, and haven't kept that pace that this year. As a result, I'm working on about 3-4 for December.&lt;/p&gt;
    &lt;p&gt;Much financial innovation is in the ultimate service of the real economy. Then, we have our friends in crypto, who occasionally do intellectually interesting things which do not have a locus in the real economy. One of those things is perpetual futures (hereafter, perps), which I find fascinating and worthy of study, the same way that a virologist just loves geeking out about furin cleavage sites.&lt;/p&gt;
    &lt;p&gt;You may have read a lot about stablecoins recently. I may write about them (again; see past BAM issue) in the future, as there has in recent years been some uptake of them for payments. But it is useful to understand that a plurality of stablecoins collateralize perps. Some observers are occasionally strategic in whether they acknowledge this, but for payments use cases, it does not require a lot of stock to facilitate massive flows. And so of the $300 billion or so in stablecoins presently outstanding, about a quarter sit on exchanges. The majority of that is collateralizing perp positions.&lt;/p&gt;
    &lt;p&gt;Perps are the dominant way crypto trades, in terms of volume. (It bounces around but is typically 6-8 times larger than spot.) This is similar to most traditional markets: where derivatives are available, derivative volume swamps spot volume. The degree to which depends on the market, Schelling points, user culture, and similar. For example, in India, most retail investing in equity is actually through derivatives; this is not true of the U.S. In the U.S., most retail equity exposure is through the spot market, directly holding stocks or indirectly through ETFs or mutual funds. Most trading volume of the stock indexes, however, is via derivatives.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beginning with the problem&lt;/head&gt;
    &lt;p&gt;The large crypto exchanges are primarily casinos, who use the crypto markets as a source of numbers, in the same way a traditional casino might use a roulette wheel or set of dice. The function of a casino is for a patron to enter it with money and, statistically speaking, exit it with less. Physical casinos are often huge capital investments with large ongoing costs, including the return on that speculative capital. If they could choose to be less capital intensive, they would do so, but they are partially constrained by market forces and partially by regulation.&lt;/p&gt;
    &lt;p&gt;A crypto exchange is also capital intensive, not because the website or API took much investment (relatively low, by the standards of financial software) and not because they have a physical plant, but because trust is expensive. Bettors, and the more sophisticated market makers, who are the primary source of action for bettors, need to trust that the casino will actually be able to pay out winnings. That means the casino needs to keep assets (generally, mostly crypto, but including a smattering of cash for those casinos which are anomalously well-regarded by the financial industry) on hand exceeding customer account balances.&lt;/p&gt;
    &lt;p&gt;Those assets are… sitting there, doing nothing productive. And there is an implicit cost of capital associated with them, whether nominal (and borne by a gambler) or material (and borne by a sophisticated market making firm, crypto exchange, or the crypto exchange’s affiliate which trades against customers [0]).&lt;/p&gt;
    &lt;p&gt;Perpetual futures exist to provide the risk gamblers seek while decreasing the total capital requirement (shared by the exchange and market makers) to profitably run the enterprise.&lt;/p&gt;
    &lt;head rend="h2"&gt;Perps predate crypto but found a home there&lt;/head&gt;
    &lt;p&gt;In the commodities futures markets, you can contract to either buy or sell some standardized, valuable thing at a defined time in the future. The overwhelming majority of contracts do not result in taking delivery; they’re cancelled by an offsetting contract before that specified date.&lt;/p&gt;
    &lt;p&gt;Given that speculation and hedging are such core use cases for futures, the financial industry introduced a refinement: cash-settled futures. Now there is a reference price for the valuable thing, with a great deal of intellectual effort put into making that reference price robust and fair (not always successfully). Instead of someone notionally taking physical delivery of pork bellies or barrels of oil, people who are net short the future pay people who are net long the future on delivery day. (The mechanisms of this clearing are fascinating but outside today’s scope.)&lt;/p&gt;
    &lt;p&gt;Back in the early nineties economist Robert Shiller proposed a refinement to cash settled futures: if you don’t actually want pork bellies or oil barrels for consumption in April, and we accept that almost no futures participants actually do, why bother closing out the contracts in April? Why fragment the liquidity for contracts between April, May, June, etc? Just keep the market going perpetually.&lt;/p&gt;
    &lt;p&gt;This achieved its first widespread popular use in crypto (Bitmex is generally credited as being the popularizer), and hereafter we’ll describe the standard crypto implementation. There are, of course, variations available.&lt;/p&gt;
    &lt;head rend="h2"&gt;Multiple settlements a day&lt;/head&gt;
    &lt;p&gt;Instead of all of a particular futures vintage settling on the same day, perps settle multiple times a day for a particular market on a particular exchange. The mechanism for this is the funding rate. At a high level: winners get paid by losers every e.g. 4 hours and then the game continues, unless you’ve been blown out due to becoming overleveraged or for other reasons (discussed in a moment).&lt;/p&gt;
    &lt;p&gt;Consider a toy example: a retail user buys 0.1 Bitcoin via a perp. The price on their screen, which they understand to be for Bitcoin, might be $86,000 each, and so they might pay $8,600 cash. Should the price rise to $90,000 before the next settlement, they will get +/- $400 of winnings credited to their account, and their account will continue to reflect exposure to 0.1 units of Bitcoin via the perp. They might choose to sell their future at this point (or any other). They’ll have paid one commission (and a spread) to buy, one (of each) to sell, and perhaps they’ll leave the casino with their winnings, or perhaps they’ll play another game.&lt;/p&gt;
    &lt;p&gt;Where did the money come from? Someone else was symmetrically short exposure to Bitcoin via a perp. It is, with some very important caveats incoming, a closed system: since no good or service is being produced except the speculation, winning money means someone else lost.&lt;/p&gt;
    &lt;p&gt;One fun wrinkle for funding rates: some exchanges cap the amount the rate can be for a single settlement period. This is similar in intent to traditional markets’ usage of circuit breakers: designed to automatically blunt out-of-control feedback loops. It is dissimilar in that it cannot actually break circuits: changes to funding rate can delay realization of losses but can’t prevent them, since they don’t prevent the realization of symmetrical gains.&lt;/p&gt;
    &lt;p&gt;Perp funding rates also embed an interest rate component. This might get quoted as 3 bps a day, or 1 bps every eight hours, or similar. However, because of the impact of leverage, gamblers are paying more than you might expect: at 10X leverage that’s 30 bps a day. Consumer finance legislation standardizes borrowing costs as APR rather than basis points per day so that an unscrupulous lender can’t bury a 200% APR in the fine print.&lt;/p&gt;
    &lt;head rend="h2"&gt;Convergence in prices via the basis trade&lt;/head&gt;
    &lt;p&gt;Prices for perps do not, as a fact of nature, exactly match the underlying. That is a feature for some users.&lt;/p&gt;
    &lt;p&gt;In general, when the market is exuberant, the perp will trade above spot (the underlying market). To close the gap, a sophisticated market participant should do the basis trade: make offsetting trades in perps and spot (short the perp and buy spot, here, in equal size). Because the funding rate is set against a reference price for the underlying, longs will be paying shorts more (as a percentage of the perp’s current market price). For some of them, that’s fine: the price of gambling went up, oh well. For others, that’s a market incentive to close out the long position, which involves selling it, which will decrease the price at the margin (in the direction of spot).&lt;/p&gt;
    &lt;p&gt;The market maker can wait for price convergence; if it happens, they can close the trade at a profit, while having been paid to maintain the trade. If the perp continues to trade rich, they can just continue getting the increased funding cost. To the extent this is higher than their own cost of capital, this can be extremely lucrative.&lt;/p&gt;
    &lt;p&gt;Flip the polarities of these to understand the other direction.&lt;/p&gt;
    &lt;p&gt;The basis trade, classically executed, is delta neutral: one isn’t exposed to the underlying itself. You don’t need any belief in Bitcoin’s future adoption story, fundamentals, market sentiment, halvings, none of that. You’re getting paid to provide the gambling environment, including a really important feature: the perp price needs to stay reasonably close to the spot price, close enough to continue attracting people who want to gamble. You are also renting access to your capital for leverage.&lt;/p&gt;
    &lt;p&gt;You are also underwriting the exchange: if they blow up, your collateral becoming a claim against the bankruptcy estate is the happy scenario. (As one motivating example: Galois Capital, a crypto hedge fund doing basis trades, had ~40% of its assets on FTX when it went down. They then wound down the fund, selling the bankruptcy claim for 16 cents on the dollar.)&lt;/p&gt;
    &lt;p&gt;Recall that the market can’t function without a system of trust saying that someone is good for it if a bettor wins. Here, the market maker is good for it, via the collateral it kept on the exchange.&lt;/p&gt;
    &lt;p&gt;Many market makers function across many different crypto exchanges. This is one reason they’re so interested in capital efficiency: fully collateralizing all potential positions they could take across the universe of venues they trade on would be prohibitively capital intensive, and if they do not pre-deploy capital, they miss profitable trading opportunities. [1]&lt;/p&gt;
    &lt;head rend="h2"&gt;Leverage and liquidations&lt;/head&gt;
    &lt;p&gt;Gamblers like risk; it amps up the fun. Since one has many casinos to choose from in crypto, the ones which only “regular” exposure to Bitcoin (via spot or perps) would be offering a less-fun product for many users than the ones which offer leverage. How much leverage? More leverage is always the answer to that question, until predictable consequences start happening.&lt;/p&gt;
    &lt;p&gt;In a standard U.S. brokerage account, Regulation T has, for almost 100 years now, set maximum leverage limits (by setting minimums for margins). These are 2X at position opening time and 4X “maintenance” (before one closes out the position). Your brokerage would be obligated to forcibly close your position if volatility causes you to exceed those limits.&lt;/p&gt;
    &lt;p&gt;As a simplified example, if you have $50k of cash, you’d be allowed to buy $100k of stock. You now have $50k of equity and a $50k loan: 2x leverage. Should the value of that stock decline to about $67k, you still owe the $50k loan, and so only have $17k remaining equity. You’re now on the precipice of being 4X leveraged, and should expect a margin call very soon, if your broker hasn’t “blown you out of the trade” already.&lt;/p&gt;
    &lt;p&gt;What part of that is relevant to crypto? For the moment, just focus on that number: 4X.&lt;/p&gt;
    &lt;p&gt;Perps are offered at 1X (non-levered exposure). But they’re routinely offered at 20X, 50X, and 100X. SBF, during his press tour / regulatory blitz about being a responsible financial magnate fleecing the customers in an orderly fashion, voluntarily self-limited FTX to 20X.&lt;/p&gt;
    &lt;p&gt;One reason perps are structurally better for exchanges and market makers is that they simplify the business of blowing out leveraged traders. The exact mechanics depend on the exchange, the amount, etc, but generally speaking you can either force the customer to enter a closing trade or you can assign their position to someone willing to bear the risk in return for a discount.&lt;/p&gt;
    &lt;p&gt;Blowing out losing traders is lucrative for exchanges except when it catastrophically isn’t. It is a priced service in many places. The price is quoted to be low (“a nominal fee of 0.5%” is one way Binance describes it) but, since it is calculated from the amount at risk, it can be a large portion of the money lost. If the account’s negative balance is less than the liquidation fee, wonderful, thanks for playing and the exchange / “the insurance fund” keeps the rest, as a tip.&lt;/p&gt;
    &lt;p&gt;In the case where the amount an account is negative by is more than the fee, that “insurance fund” can choose to pay the winners on behalf of the liquidated user, at management’s discretion. Management will usually decide to do this, because a casino with a reputation for not paying winners will not long remain a casino.&lt;/p&gt;
    &lt;p&gt;But tail risk is a real thing. The capital efficiency has a price: there physically does not exist enough money in the system to pay all winners given sufficiently dramatic price moves. Forced liquidations happen. Sophisticated participants withdraw liquidity (for reasons we’ll soon discuss) or the exchange becomes overwhelmed technically / operationally. The forced liquidations eat through the diminished / unreplenished liquidity in the book, and the magnitude of the move increases.&lt;/p&gt;
    &lt;p&gt;Then crypto gets reminded about automatic deleveraging (ADL), a detail to perp contracts that few participants understand.&lt;/p&gt;
    &lt;head rend="h2"&gt;We have altered the terms of your unregulated futures investment contract.&lt;/head&gt;
    &lt;p&gt;(Pray we do not alter them further.)&lt;/p&gt;
    &lt;p&gt;Risk in perps has to be symmetric: if (accounting for leverage) there are 100,000 units of Somecoin exposure long, then there are 100,000 units of Somecoin exposure short. This does not imply that the shorts or longs are sufficiently capitalized to actually pay for all the exposure in all instances.&lt;/p&gt;
    &lt;p&gt;In cases where management deems paying winners from the insurance fund would be too costly and/or impossible, they automatically deleverage some winners. In theory, there is a published process for doing this, because it would be confidence-costing to ADL non-affiliated accounts but pay out affiliated accounts, one’s friends or particularly important counterparties, etc. In theory.&lt;/p&gt;
    &lt;p&gt;In theory, one likely ADLs accounts which were quite levered before ones which were less levered, and one ADLs accounts which had high profits before ones with lower profits. In theory. [2]&lt;/p&gt;
    &lt;p&gt;So perhaps you understood, prior to a 20% move, that you were 4X leveraged. You just earned 80%, right? Ah, except you were only 2X leveraged, so you earned 40%. Why were you retroactively only 2X? That’s what automatic deleveraging means. Why couldn’t you get the other 40% you feel entitled to? Because the collective group of losers doesn’t have enough to pay you your winnings and the insurance fund was insufficient or deemed insufficient by management.&lt;/p&gt;
    &lt;p&gt;ADL is particularly painful for sophisticated market participants doing e.g. a basis trade, because they thought e.g. they were 100 units short via perps and 100 units long somewhere else via spot. If it turns out they were actually 50 units short via perps, but 100 units long, their net exposure is +50 units, and they have very possibly just gotten absolutely shellacked.&lt;/p&gt;
    &lt;p&gt;In theory, this can happen to the upside or the downside. In practice in crypto, this seems to usually happen after sharp decreases in prices, not sharp increases. For example, October 2025 saw widespread ADLing as (more than) $19 billion of liquidations happened, across a variety of assets. Alameda’s CEO Caroline Ellison testified that they lost over $100 million during the collapse of Terra’s stablecoin in 2022, but since FTX’s insurance fund was made up; when leveraged traders lost money, their positions were frequently taken up by Alameda. That was quite lucrative much of the time, but catastrophically expensive during e.g. the Terra blowup. Alameda was a good loser and paid the winners, though: with other customers’ assets that they “borrowed.”&lt;/p&gt;
    &lt;head rend="h2"&gt;An aside about liquidations&lt;/head&gt;
    &lt;p&gt;In the traditional markets, if one’s brokerage deems one’s assets are unlikely to be able to cover the margin loan from the brokerage one has used, one’s brokerage will issue a margin call. Historically that gave one a relatively short period (typically, a few days) to post additional collateral, either by moving in cash, by transferring assets from another brokerage, or by experiencing appreciation in the value of one’s assets. Brokerages have the option, and in some cases the requirement, to manage risk after or during a margin call by forcing trades on behalf of the customer to close positions.&lt;/p&gt;
    &lt;p&gt;It sometimes surprises crypto natives that, in the case where one’s brokerage account goes negative and all assets are sold, with a negative remaining balance, the traditional markets largely still expect you to pay that balance. This contrasts with crypto, where the market expectation for many years was that the customer was Daffy Duck with a gmail address and a pseudonymous set of numbered accounts recorded on a blockchain, and dunning them was a waste of time. Crypto exchanges have mostly, in the intervening years, either stepped up their game regarding KYC or pretended to do so, but the market expectation is still that a defaulting user will basically never successfully recover. (Note that the legal obligation to pay is not coextensive with users actually paying. The retail speculators with $25,000 of capital that the pattern day trade rules are worried about will often not have $5,000 to cover a deficiency. On the other end of the scale, when a hedge fund blows up, the fund entity is wiped out, but its limited partners—pension funds, endowments, family offices—are not on the hook to the prime broker, and nobody expects the general partner to start selling their house to make up the difference.)&lt;/p&gt;
    &lt;p&gt;So who bears the loss when the customer doesn’t, can’t, or won’t? The waterfall depends on market, product type, and geography, but as a sketch: brokerages bear the loss first, out of their own capital. They’re generally required to keep a reserve for this purpose.&lt;/p&gt;
    &lt;p&gt;A brokerage will, in the ordinary course of business, have obligations to other parties which would be endangered if they were catastrophically mismanaged and could not successfully manage risk during a downturn. (It’s been known to happen, and even can be associated with assets rather than liabilities.) In this case, most of those counterparties are partially insulated by structures designed to insure the peer group. These include e.g. clearing pools, guaranty funds capitalized by the member firms of a clearinghouse, the clearinghouse’s own capital, and perhaps mutualized insurance pools. That is the rough ordering of the waterfall, which varies depending geography/product/market.&lt;/p&gt;
    &lt;p&gt;One can imagine a true catastrophe which burns through each of those layers of protection, and in that case, the clearinghouse might be forced to assess members or allocate losses across survivors. That would be a very, very bad day, but contracts exist to be followed on very bad days.&lt;/p&gt;
    &lt;p&gt;One commonality with crypto, though: this system is also not fully capitalized against all possible events at all times. Unlike crypto, which for contingent reasons pays some lip service to being averse to credit even as it embraces leveraged trading, the traditional industry relies extensively on underwriting risk of various participants.&lt;/p&gt;
    &lt;head rend="h2"&gt;Will crypto successfully “export” perps?&lt;/head&gt;
    &lt;p&gt;Many crypto advocates believe that they have something which the traditional finance industry desperately needs. Perps are crypto’s most popular and lucrative product, but they probably won’t be adopted materially in traditional markets.&lt;/p&gt;
    &lt;p&gt;Existing derivatives products already work reasonably well at solving the cost of capital issue. Liquidations are not the business model of traditional brokerages. And learning, on a day when markets are 20% down, that you might be hedged or you might be bankrupt, is not a prospect which fills traditional finance professionals with the warm fuzzies.&lt;/p&gt;
    &lt;p&gt;And now you understand the crypto markets a bit better.&lt;/p&gt;
    &lt;p&gt;[0] Brokers trading with their own customers can happen in the ordinary course of business, but has been progressively discouraged in traditional finance, as it enables frontrunning.&lt;/p&gt;
    &lt;p&gt;Frontrunning, while it is understood in the popular parlance to mean “trading before someone else can trade” and often brought up in discussions of high frequency trading using very fast computers, does not historically mean that. It historically describes a single abusive practice: a broker could basically use the slowness of traditional financial IT systems to give conditional post-facto treatment to customer orders, taking the other side of them (if profitable) or not (if not). Frontrunning basically disappeared because customers now get order confirms almost instantly by computer not at end of day via a phone call. The confirm has the price the trade executed at on it.&lt;/p&gt;
    &lt;p&gt;In classic frontrunning, you sent the customer’s order to the market (at some price X), waited a bit, and then observed a later price Y. If Y was worse for the customer than X, well, them’s the breaks on Wall Street. If Y was better, you congratulated the customer on their investing acumen, and informed them that they had successfully transacted at Z, a price of your choosing between X and Y. You then fraudulently inserted a recorded transaction between the customer and yourself earlier in the day, at price Z, and assigned the transaction which happened at X to your own account, not to the customer’s account.&lt;/p&gt;
    &lt;p&gt;Frontrunning was a lucrative scam while it lasted, because (effectively) the customer takes 100% of the risk of the trade but the broker gets any percentage they want of the first day’s profits. This is potentially so lucrative that smart money (and some investors in his funds!) thought Madoff was doing it, thus generating the better-than-market stable returns for over a decade through malfeasance. Of frontrunning Madoff was entirely innocent.&lt;/p&gt;
    &lt;p&gt;Some more principled crypto participants have attempted to discourage exchanges from trading with their own customers. They have mostly been unsuccessful: Merit Peak Limited is Binance’s captive entity which does this. It also is occasionally described by U.S. federal agencies as running a sideline in money laundering, Alameda Research was FTX’s affiliated trading fund. Their management was criminally convicted of money laundering. etc, etc.&lt;/p&gt;
    &lt;p&gt;One of the reasons this behavior is so adaptive is because the billions of dollars sloshing around can be described to banks as “proprietary trading” and “running an OTC desk”, and an inattentive bank (like, say, Silvergate, as recounted here) might miss the customer fund flows they would have been formally unwilling to facilitate. This is a useful feature for sophisticated crypto participants, and so some of them do not draw attention to the elephant in the room, even though it is averse to their interests.&lt;/p&gt;
    &lt;p&gt;[1] Not all crypto trades are pre-funded. Crypto OTC transactions sometimes settle on T+1, with the OTC desk essentially extending credit in the fashion that a prime broker would in traditional markets. But most transactions on exchanges have to be paid immediately in cash already at the venue. This is very different from traditional equity market structure, where venues don’t typically receive funds flow at all, and settling/clearing happens after the fact, generally by a day or two.&lt;/p&gt;
    &lt;p&gt;[2] I note, for the benefit of readers of footnote 0, that there is often a substantial gap between the time when market dislocation happens and when a trader is informed they were ADLed. The implications of this are left as an exercise to the reader.&lt;/p&gt;
    &lt;head rend="h2"&gt;Want more essays in your inbox?&lt;/head&gt;
    &lt;p&gt;I write about the intersection of tech and finance, approximately biweekly. It's free.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bitsaboutmoney.com/archive/perpetual-futures-explained/"/><published>2025-12-05T21:23:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46167552</id><title>Leaving Intel</title><updated>2025-12-06T04:12:25.017621+00:00</updated><content>&lt;doc fingerprint="26d54338ddba5263"&gt;
  &lt;main&gt;
    &lt;p&gt;I've resigned from Intel and accepted a new opportunity. If you are an Intel employee, you might have seen my fairly long email that summarized what I did in my 3.5 years. Much of this is public:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI flame graphs and released them as open source&lt;/item&gt;
      &lt;item&gt;GPU subsecond-offset heatmap&lt;/item&gt;
      &lt;item&gt;Worked with Linux distros to enable stack walking&lt;/item&gt;
      &lt;item&gt;Was interviewed by the WSJ about eBPF for security monitoring&lt;/item&gt;
      &lt;item&gt;Provided leadership on the eBPF Technical Steering Committee (BSC)&lt;/item&gt;
      &lt;item&gt;Co-chaired USENIX SREcon APAC 2023&lt;/item&gt;
      &lt;item&gt;Gave 6 conference keynotes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It's still early days for AI flame graphs. Right now when I browse CPU performance case studies on the Internet, I'll often see a CPU flame graph as part of the analysis. We're a long way from that kind of adoption for GPUs (and it doesn't help that our open source version is Intel only), but I think as GPU code becomes more complex, with more layers, the need for AI flame graphs will keep increasing.&lt;/p&gt;
    &lt;p&gt;I also supported cloud computing, participating in 110 customer meetings, and created a company-wide strategy to win back the cloud with 33 specific recommendations, in collaboration with others across 6 organizations. It is some of my best work and features a visual map of interactions between all 19 relevant teams, described by Intel long-timers as the first time they have ever seen such a cross-company map. (This strategy, summarized in a slide deck, is internal only.)&lt;/p&gt;
    &lt;p&gt;I always wish I did more, in any job, but I'm glad to have contributed this much especially given the context: I overlapped with Intel's toughest 3 years in history, and I had a hiring freeze for my first 15 months.&lt;/p&gt;
    &lt;p&gt;My fond memories from Intel include meeting Linus at an Intel event who said "everyone is using fleme graphs these days" (Finnish accent), meeting Pat Gelsinger who knew about my work and introduced me to everyone at an exec all hands, surfing lessons at an Intel Australia and HP offsite (mp4), and meeting Harshad Sane (Intel cloud support engineer) who helped me when I was at Netflix and now has joined Netflix himself -- we've swapped ends of the meeting table. I also enjoyed meeting Intel's hardware fellows and senior fellows who were happy to help me understand processor internals. (Unrelated to Intel, but if you're a Who fan like me, I recently met some other people as well!)&lt;/p&gt;
    &lt;p&gt;My next few years at Intel would have focused on execution of those 33 recommendations, which Intel can continue to do in my absence. Most of my recommendations aren't easy, however, and require accepting change, ELT/CEO approval, and multiple quarters of investment. I won't be there to push them, but other employees can (my CloudTeams strategy is in the inbox of various ELT, and in a shared folder with all my presentations, code, and weekly status reports). This work will hopefully live on and keep making Intel stronger. Good luck.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.brendangregg.com/blog//2025-12-05/leaving-intel.html"/><published>2025-12-05T21:27:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46167621</id><title>Frank Gehry has died</title><updated>2025-12-06T04:12:24.507524+00:00</updated><content>&lt;doc fingerprint="76ebd040bfff3a58"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Legendary architect Frank Gehry dies aged 96&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Published&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Frank Gehry, one of the most influential architects of the last century, has died aged 96.&lt;/p&gt;
    &lt;p&gt;Gehry was acclaimed for his avant garde, experimental style of architecture. His titanium-covered design of the Guggenheim Museum in Bilbao, Spain, catapulted him to fame in 1997.&lt;/p&gt;
    &lt;p&gt;His breakthrough in the architectural world came years earlier when he redesigned his own home in Santa Monica, California, using materials like chain-link fencing, plywood, and corrugated steel.&lt;/p&gt;
    &lt;p&gt;His death was confirmed by his chief of staff Meaghan Lloyd. He is survived by two daughters from his first marriage, Leslie and Brina, as well as his wife, Berta Isabel Aguilera, and their two sons, Alejandro and Samuel.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Frank Gehry's most iconic work - in pictures&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Published7 hours ago&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Born in Toronto in 1929, Gehry moved to Los Angeles as a teenager to study architecture at the University of Southern California, before completing further study at the Harvard Graduate School of Design in 1956 and 1957.&lt;/p&gt;
    &lt;p&gt;After starting his own firm, he broke from traditional architectural principles of symmetry, using unconventional geometric shapes and unfinished materials in a style now known as deconstructivism.&lt;/p&gt;
    &lt;p&gt;Through blending unexpected materials and sheathing buildings in stainless steel to create curvy exteriors, Gehry created buildings that took on arresting sculptural shapes.&lt;/p&gt;
    &lt;p&gt;Later in his career, Gehry used 3D modelling similar to that used by aerospace engineers to shape windy buildings, a practice largely avoided by other architects because of the complexity and costliness of construction.&lt;/p&gt;
    &lt;p&gt;In 1989, at the age of 60, Gehry was awarded the industry's top accolade, the Pritzker Architecture prize, for lifetime achievement.&lt;/p&gt;
    &lt;p&gt;The Pritzker jury said his work possessed a "highly refined, sophisticated and adventurous aesthetic".&lt;/p&gt;
    &lt;p&gt;"His designs, if compared to American music, could best be likened to Jazz, replete with improvisation and a lively unpredictable spirit," the panel said at the time.&lt;/p&gt;
    &lt;p&gt;Gehry's international breakthrough with the Guggenheim transformed the city of Bilbao, boosting tourism to the city and the local economy. Crafted out of titanium sheets, limestone, and glass, the museum was instantly celebrated as a modern marvel.&lt;/p&gt;
    &lt;p&gt;Architect Philip Johnson, Gehry's American contemporary, described the structure as "the greatest building of our time".&lt;/p&gt;
    &lt;p&gt;Other cities tried to replicate its success, branded the "Bilbao effect", where investment in daring art could revitalise ailing economies.&lt;/p&gt;
    &lt;p&gt;The cultural phenomenon was parodied in a 2005 episode of The Simpsons, in which the fictional town of Springfield invites Gehry, who voiced himself in the cartoon TV show, to design a new concert hall.&lt;/p&gt;
    &lt;p&gt;In the episode, the shape of the concert hall is jokingly inspired by a letter Gehry had scrunched up.&lt;/p&gt;
    &lt;p&gt;The guest appearance later "haunted" Gehry, who told the Observer in 2011 that people sincerely believed his real-life designs were inspired by crumpled paper instead of complex computations.&lt;/p&gt;
    &lt;head rend="h2"&gt;'Pushing the envelope'&lt;/head&gt;
    &lt;p&gt;His work in Bilbao put him in high demand, and he went on to design iconic structures in cities all over the world: the Jay Pritzker Pavilion in Chicago's Millennium Park, the Gehry Tower in Germany, and the Louis Vuitton Foundation in Paris.&lt;/p&gt;
    &lt;p&gt;"He bestowed upon Paris and upon France his greatest masterpiece," said Bernard Arnault, the CEO of LVMH, the worlds largest luxury goods company which owns Louis Vuitton.&lt;/p&gt;
    &lt;p&gt;With a largely unpredictable style, no two of his works look the same. Prague's Dancing House, finished in 1996, looks like a glass building folding in on itself; his Hotel Marques in Spain, built in 2006, features thin sheets of wavy, multicoloured metal; his design for a business school in Sydney looks like a brown paper bag.&lt;/p&gt;
    &lt;p&gt;Gehry also designed the Walt Disney Concert Hall in Los Angeles, layered in metal resembling sails billowing in the wind. After it opened in 2003, critics described it as a "pile of broken crockery", a "fortune cookie gone berserk" and an "emptied waste basket".&lt;/p&gt;
    &lt;p&gt;In a 2007 interview with the New Yorker, Gehry shrugged off the concert hall's critics: "At least they're looking!" he quipped.&lt;/p&gt;
    &lt;p&gt;Tributes are celebrating his eagerness to discard convention - and forge his own creative legacy.&lt;/p&gt;
    &lt;p&gt;Paul Goldberger, author of Building Art: The Life and Work of Frank Gehry, came to know Gehry closely, and said he wanted to work "until the day he died".&lt;/p&gt;
    &lt;p&gt;"He was one of the very few architects of our time to engage people emotionally," Goldberger told BBC Radio 4's The World Tonight.&lt;/p&gt;
    &lt;p&gt;"He was all about pushing the envelope... wanting to use the most advanced technology to do the most adventurous things."&lt;/p&gt;
    &lt;p&gt;In a statement, Canadian Prime Minister Mark Carney extended his "deepest condolences" to Gehry's family and the "many admirers of his work".&lt;/p&gt;
    &lt;p&gt;He added: "His unmistakable vision lives on in iconic buildings around the world."&lt;/p&gt;
    &lt;p&gt;Bilbao's Guggenheim Museum posted a video tribute to Gehry.&lt;/p&gt;
    &lt;p&gt;"We will be forever grateful," the museum wrote on Instagram, "his spirit and legacy will always remain connected to Bilbao".&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.co.uk/news/articles/c5y2p22z9gno"/><published>2025-12-05T21:31:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46168057</id><title>Adenosine on the common path of rapid antidepressant action: The coffee paradox</title><updated>2025-12-06T04:12:24.141269+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://genomicpress.kglmeridian.com/view/journals/brainmed/aop/article-10.61373-bm025c.0134/article-10.61373-bm025c.0134.xml"/><published>2025-12-05T22:10:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46169224</id><title>Sam Altman’s DRAM Deal</title><updated>2025-12-06T04:12:23.958565+00:00</updated><content>&lt;doc fingerprint="a830918429a057fb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sam Altman’s Dirty DRAM Deal&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Moore's Law Is Dead&lt;/item&gt;
      &lt;item&gt;Nov 24&lt;/item&gt;
      &lt;item&gt;9 min read&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Or: How the AI Bubble, Panic, and Unpreparedness Stole Christmas&lt;/head&gt;
    &lt;p&gt;Written by Tom of Moore’s Law Is Dead&lt;/p&gt;
    &lt;p&gt;Special Assistance by KarbinCry &amp;amp; kari-no-sugata&lt;/p&gt;
    &lt;p&gt;Based on this Video: https://youtu.be/BORRBce5TGw&lt;/p&gt;
    &lt;head rend="h4"&gt;Introduction — The Day the RAM Market Snapped&lt;/head&gt;
    &lt;p&gt;At the beginning of November, I ordered a 32GB DDR5 kit for pairing with a Minisforum BD790i X3D motherboard, and three weeks later those very same sticks of DDR5 are now listed for a staggering $330– a 156% increase in price from less than a month ago! At this rate, it seems likely that by Christmas, that DDR5 kit alone could be worth more than the entire Zen 4 X3D platform I planned to pair it with! How could this happen, and more specifically – how could this happen THIS quickly? Well, buckle up! I am about to tell you the story of Sam Altman’s Dirty DRAM Deal, or: How the AI bubble, panic, and unpreparedness stole Christmas...&lt;/p&gt;
    &lt;p&gt;But before I dive in, let me make it clear that my RAM kit’s 156% jump in price isn’t a fluke or some extreme example of what's going on right now. Nope, and in fact, I'd like to provide two more examples of how how impossible it is becoming to get ahold of RAM - these were provided by a couple of our sources within the industry:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;One source that works at a US Retailer, stated that a RAM Manufacturer called them in order to inquire if they might buy RAM from them to stock up for their other customers. This would be like Corsair asking a Best Buy if they had any RAM around.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Another source that works at a Prebuilt PC company, was recently given an estimate for when they would receive RAM orders if they placed them now…and they were told December…of 2026!&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So what happened? Well, it all comes down to three perfectly synergistic events:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;OpenAI executed two unprecedented RAM deals that took everyone by surprise.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The secrecy and size of the deals triggered full-scale panic buying from everyone else.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The market had almost zero safety stock left due to tariffs, worry about decreasing RAM prices over the summer, and stalled equipment transfers.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Below, we’re going to walk through each of these factors — and then I’m going to warn you about which hardware categories will be hit the hardest, which products are already being cancelled, and what you should buy right now before the shelves turn into a repeat of 2021–2022...because this is doomed to turn into much more than just RAM scarcity...&lt;/p&gt;
    &lt;head rend="h4"&gt;Part I —OpenAI wasn’t Very “Open”&lt;/head&gt;
    &lt;p&gt;On October 1st OpenAI signed two simultaneous deals with Samsung and SK Hynix for 40% of the worlds DRAM supply. Now, did OpenAI’s competition suspect some big RAM deals could be signed in late 2025? Yes. Ok, but did they think it would be deals this huge and with multiple companies? NO! In fact, if you go back and read reporting on Sam Altman’s now infamous trip to South Korea on October 1st, even just mere hours before the massive deals with Samsung and SK Hynix were simultaneously signed – most reporting simply mentioned vague reports about Sam talking to Samsung, SK Hynix, TSMC, and Foxconn. But the reporting at the time was soft, almost dismissive — “exploring ties,” “seeking cooperation,” “probing for partnerships.” Nobody hinted that OpenAI was about to swallow up to 40% of global DRAM output – even on morning before it happened! Nobody saw this coming - this is clear in the lack of reporting about the deals before they were announced, and every MLID Source who works in DRAM manufacturing and distribution insist this took everyone in the industry by surprise.&lt;/p&gt;
    &lt;p&gt;To be clear - the shock wasn’t that OpenAI made a big deal, no, it was that they made two massive deals this big, at the same time, with Samsung and SK Hynix simultaneously! In fact, according to our sources - both companies had no idea how big each other's deal was, nor how close to simultaneous they were. And this secrecy mattered. It mattered a lot.&lt;/p&gt;
    &lt;p&gt;Had Samsung known SK Hynix was about to commit a similar chunk of supply — or vice-versa — the pricing and terms would have likely been different. It’s entirely conceivable they wouldn’t have both agreed to supply such a substantial part of global supply if they had known more...but at the end of the day - OpenAI did succeed in keeping the circles tight, locking down the NDAs, and leveraging the fact that these companies assumed the other wasn’t giving up this much wafer volume simultaneously…in order to make a surgical strike on the global RAM supply chain…and it's worked so far...&lt;/p&gt;
    &lt;head rend="h4"&gt;Part II — Instant Panic: How did we miss this?&lt;/head&gt;
    &lt;p&gt;Imagine you're running a hyper scaler, or maybe you’re a major OEM, or perhaps pretend that you are simply one of OpenAI’s chief competitors: On October 1st of 2025, you would have woken up to the news that OpenAI had just cornered the memory market more aggressively than any company in the last decade, and you hadn't heard even a murmur that this was coming beforehand! Well, you would probably make some follow-up calls to colleagues in the industry, and then also quickly hear rumors that it wasn't just you - also the two largest suppliers didn’t even see each other’s simultaneous cooperation with OpenAI coming either! You wouldn't go: “Well, that’s an interesting coincidence”, no, you would say: “WHAT ELSE IS GOING ON THAT WE DON’T KNOW ABOUT?”&lt;/p&gt;
    &lt;p&gt;Again – it’s not the size of the deals that's solely the issue here, no, it’s also the unexpectedness and brazenness of them. On October 1st silicon valley executives and procurement managers panicked over concerns like these:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;What other deals don’t we know about? Is this just the first of many?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;None of our DRAM suppliers warned us ahead of time! We have to assume they also won't in the future, and that it’s possible all of global DRAM could be bought up without us getting a single warning!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We know OpenAI’s competitors are already panic-buying! If we don’t move now, we might be locked out of the market until 2028!&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;OpenAI’s competitors, OEMs, and cloud providers scrambled to secure whatever inventory remained out of self-defense, and self-defense in a world that was entirely defenseless due to the accelerant I’ll now explain in Part III...&lt;/p&gt;
    &lt;head rend="h4"&gt;Part III — There Wasn't any Safety Stock&lt;/head&gt;
    &lt;p&gt;Normally, the DRAM market has buffers: warehouses of emergency stock, excess wafer starts, older DRAM manufacturing machinery being sold off to budget brands while the big brands upgrade their production lines…but not in 2025, in 2025 those would-be buffers were depleted for three separate reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Tariff Chaos. Companies had deliberately reduced how much DRAM they ordered for their safety stock over the summer of 2025 because tariffs were changing almost weekly. Every RAM purchase risked being made at the wrong moment – and so fewer purchases were made.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prices had been falling all summer. Because of the hesitancy to purchase as much safety stock as usual, RAM prices were also genuinely falling over time. And, obviously when memory is getting cheaper month over month, the last thing you’d feel is pressured to buy a commodity that could be cheaper the next month…so everyone waited.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Secondary RAM Manufacturing Had Stalled. Budget brands normally buy older DRAM fabrication equipment from mega-producers like Samsung when Samsung upgrades their DRAM lines to the latest and greatest equipment. This allows the DRAM market to expand more than it would otherwise because it makes any upgrading of the fanciest production lines to still be additive change to the market. However, Korean memory firms have been terrified that reselling old equipment to China-adjacent OEMs might trigger U.S. retaliation…and so those machines have been sitting idle in warehouses since early spring.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yep, there was no cushion. OpenAI hit the market at the exact moment it was least prepared.&lt;/p&gt;
    &lt;head rend="h4"&gt;Part IV — Artificial Scarcity&lt;/head&gt;
    &lt;p&gt;And now time for the biggest twist of all, a twist that’s actually public information, and therefore should be getting discussed by far more people in this writer's opinion: OpenAI isn’t even bothering to buy finished memory modules! No, their deals are unprecedentedly only for raw wafers — uncut, unfinished, and not even allocated to a specific DRAM standard yet. It’s not even clear if they have decided yet on how or when they will finish them into RAM sticks or HBM! Right now it seems like these wafers will just be stockpiled in warehouses – like a kid who hides the toybox because they’re afraid nobody wants to play with them, and thus selfishly feels nobody but them should get the toys!&lt;/p&gt;
    &lt;p&gt;And let’s just say it: Here is the uncomfortable truth Sam Altman is always loath to admit in interviews: OpenAI is worried about losing its lead. The last 18 months have seen competitors catching up fast — Anthropic, Meta, xAI, and specifically Google’s Gemini 3 has gotten a ton of praise just in the past week. Everyone’s chasing training capacity. Everyone needs memory. DRAM is the lifeblood of scaling inference and training throughput. Cutting supply to your rivals is not a conspiracy theory. It’s a business tactic as old as business itself. And so, when you consider how secretive OpenAI was about their deals with Samsung and SK Hynix, but additionally how unready they were to immediately utilize their warehouses of DRAM wafers – it sure seems like a primary goal of these deals was to deprive the market, and not just an attempt to protect OpenAI's own supply…&lt;/p&gt;
    &lt;head rend="h4"&gt;Part V — What will be cancelled? What should you buy now?&lt;/head&gt;
    &lt;p&gt;Alright, now that we are done explaining the how, let’s get to the “now what?” – because even if the RAM shortage miraculously improves immediately behind the scenes – even if the AI Bubble instantly popped or 10 companies started tooling up for more DRAM capacity this second (and many are, to be fair), at a minimum the next six to nine months are already screwed. See above: DRAM manufactures are quoting 13-Month lead times for DDR5! This is not a temporary blip. This could be a once-in-a-generation shock. So what gets hit first? What gets hit hardest? Well, below is an E through S-Tier ranking of which products are "the most screwed":&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;S-Tier (Already Screwed – Too Late to Buy) -&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;RAM itself, obviously. RAM prices have “exploded”. The detonation is in the past.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A-Tier (Almost Screwed – Don’t Wait to Buy!!!)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;SSDs. These tends to follow DRAM pricing with a lag.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Small Prebuilt PC Companies. That lack large buffers of inventory.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;RADEON GPUs. AMD doesn’t bundle RAM in their BOM kits to AIBs the way Nvidia does. In fact, the RX 9070 GRE 16GB this channel leaked months ago is almost certainly cancelled according to our sources.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;XBOX. Microsoft didn’t plan. Prices may rise and/or supply may dwindle in 2026.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;B-Tier (Eventually Screwed – Don’t wait much longer to buy!)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Nvidia GPUs. Nvidia maintains large memory inventories for its board partners, giving them a buffer. But high-capacity GPUs (like a hypothetical 24GB 5080 SUPER) are on ice for now because those stores were never sufficiently built up. In fact, Nvidia is quietly telling partners that their SUPER refresh “might” launch Q3 2026 – although most partners think it’s just a placeholder for when Nvidia expects new capacity to come online, and thus SUPER may never launch.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;C-Tier (Think about buying soon)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Laptops and phones. These companies negotiate immense long-term contracts, so they’re not hit immediately. But once their stockpiles run dry, watch out!&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;D-Tier (Consider buying soon, but there’s no rush)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;PlayStation. Sony planned better than almost anyone else. They bought aggressively during the summer price trough, which is why they can afford a Black Friday discount while everyone else is raising prices.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;E-Tier (Prices might actually drop)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Anything without RAM. Specifically CPUs that do not come with coolers could see price drops over time since there could be a drop in demand for CPUs if nobody has the RAM to feed them in systems.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;???-Tier —Steam Machine. Valve keeps things quiet, but the big unknown is whether they pre-bought RAM months ago before announcing their much-hyped Steam Machine. If they did already stockpile an ample supply of DDR5 - then Steam Machine should launch fine, but supply could dry up temporarily at some point while they wait for prices to drop. However, if they didn’t plan ahead - expect a high launch price and very little resupply…it might even need to be cancelled or there might need to be a variant offered without RAM included (BYO RAM Edition!).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And that’s it! This last bit was the most important part of the article in this writer's opinion – an attempt at helping you avoid getting burned. Well, actually, there is one other important reason for this article’s existence I'll tack onto the end – a hope that other people start digging into what’s going on at OpenAI. I mean seriously – do we even have a single reliable audit of their financials to back up them outrageously spending this much money…for this? Heck, I’ve even heard from numerous sources that OpenAI is “buying up the manufacturing equipment as well” – and without mountains of concrete proof, and/or more input from additional sources on what that really means…I don’t feel I can touch that hot potato without getting burned…but I hope someone else will…&lt;/p&gt;
    &lt;p&gt;Sources:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.mooreslawisdead.com/post/sam-altman-s-dirty-dram-deal"/><published>2025-12-06T00:24:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46169330</id><title>Extra Instructions Of The 65XX Series CPU (1996)</title><updated>2025-12-06T04:12:23.542802+00:00</updated><link href="http://www.ffd2.com/fridge/docs/6502-NMOS.extra.opcodes"/><published>2025-12-06T00:38:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46169554</id><title>YouTube caught making AI-edits to videos and adding misleading AI summaries</title><updated>2025-12-06T04:12:23.352436+00:00</updated><content>&lt;doc fingerprint="bc955230b9bddc79"&gt;
  &lt;main&gt;
    &lt;p&gt;YouTube has begun quietly using artificial intelligence to enhance videos by some of its top creators—without notifying them or their audiences.&lt;/p&gt;
    &lt;p&gt;The practice was first noticed by two well-known American YouTubers popular among music enthusiasts: Rick Beato and Rhett Shull. Both run channels with millions of subscribers. Beato, a music educator and producer with more than 5 million subscribers, said he realized something was “off” in one of his recent videos.&lt;/p&gt;
    &lt;p&gt;“I thought I was imagining it, but my hair looked strange, and it almost looked like I was wearing makeup,” he said in a post.&lt;/p&gt;
    &lt;head rend="h3"&gt;Subtle changes, big questions&lt;/head&gt;
    &lt;p&gt;It turns out YouTube has been experimenting in recent months with AI-powered video enhancement, even altering YouTube Shorts without creator approval. The changes are subtle: sharper shirt folds, smoother or more highlighted skin, even slightly altered ears. Most viewers would not notice—but Beato and Shull said the edits made their videos feel unnatural.&lt;/p&gt;
    &lt;p&gt;Shull, a guitarist and creator, posted a video about the issue. “It looks like something AI-generated,” he said. “It bothers me because it could erode the trust I have with my audience.”&lt;/p&gt;
    &lt;p&gt;Complaints about odd changes to Shorts surfaced on social media as early as June, but only after Beato and Shull spoke out did YouTube confirm the experiment.&lt;/p&gt;
    &lt;head rend="h3"&gt;YouTube admits to ‘limited test’&lt;/head&gt;
    &lt;p&gt;Rene Ritchie, YouTube’s creator liaison, acknowledged in a post on X that the company was running “a small experiment on select Shorts, using traditional machine learning to clarify, reduce noise and improve overall video clarity—similar to what modern smartphones do when shooting video.”&lt;/p&gt;
    &lt;p&gt;That explanation drew further criticism. Samuel Woolley, a disinformation expert at the University of Pittsburgh, said the company’s wording was misleading. “Machine learning is a subset of artificial intelligence,” he said. “This is AI.”&lt;/p&gt;
    &lt;p&gt;The controversy highlights a wider trend in which more of what people see online is pre-processed by AI before reaching them. Smartphone makers like Samsung and Google have long used AI to “enhance” images. Samsung previously admitted to using AI to sharpen moon photos, while Google’s Pixel “Best Take” feature stitches together facial expressions from multiple shots to create a single “perfect” group picture.&lt;/p&gt;
    &lt;p&gt;“What’s happening here is that a company is altering creators’ content and distributing it to the public without their consent,” Woolley said. Unlike Photoshop filters or social media effects, he warned, YouTube’s AI edits add another hidden layer between audiences and the media they consume—raising concerns about authenticity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Creators respond&lt;/head&gt;
    &lt;p&gt;While Woolley warned of eroding public trust, Beato remained more optimistic. “YouTube is always working on new tools and experimenting,” he said. “They’re an industry leader, and I have nothing bad to say about them. YouTube changed my life.”&lt;/p&gt;
    &lt;p&gt;Still, critics say even minor retouching without disclosure sets a troubling precedent. YouTube is home not only to entertainment, but also to news, education, and informational content—areas where accuracy and authenticity matter.&lt;/p&gt;
    &lt;p&gt;The quiet rollout suggests a future in which AI may increasingly reshape digital media before users even press play.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ynetnews.com/tech-and-digital/article/bj1qbwcklg"/><published>2025-12-06T01:15:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46170027</id><title>EU hits X with €120M fine for breaching the Digital Services Act</title><updated>2025-12-06T04:12:22.189103+00:00</updated><content>&lt;doc fingerprint="a89c0a7d798d331c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;EU hits X with €120 million fine over breaking digital rules&lt;/head&gt;December 5, 2025&lt;p&gt;The European Union has fined Elon Musk's social media platform X €120 million ($140 million) for breaching its transparency rules set out in its Digital Services Act (DSA).&lt;/p&gt;&lt;p&gt;"Deceiving users with blue checkmarks, obscuring information on ads and shutting out researchers have no place online in the EU," said European Commission Vice President Henna Virkkunen.&lt;/p&gt;&lt;p&gt;Pre-empting the announcement on Thursday night, United States Vice President JD Vance that "the EU should be supporting free speech not attacking American companies over garbage."&lt;/p&gt;&lt;p&gt;Virkkunen said the ruling had "nothing to do with censorship," adding: "If you comply with our rules, you don't get a fine. It's as simple as that."&lt;/p&gt;&lt;head rend="h2"&gt;Why has the EU fined X?&lt;/head&gt;&lt;p&gt;The heavy fine is the first such punishment issued by the European Commission, which began its investigations into X in December 2023.&lt;/p&gt;&lt;p&gt;Among other things, Brussels accuses the platform of using the white and blue checkmarks for paid user accounts to falsely suggest that these accounts are authentic and verified.&lt;/p&gt;&lt;p&gt;The Commission also criticized that it is not always clear who is behind advertising on X.&lt;/p&gt;&lt;p&gt;DSA fines can be as high as 6% of a company's annual global revenue, but the Commission said X's annual turnover didn't play a direct role in the calculation of the fine.&lt;/p&gt;&lt;p&gt;According to the EU decision, the penalty consists of three parts: €45 million for the verification checkmarks, €35 million for the lack of advertising transparency, and €40 million for the lack of data access for researchers.&lt;/p&gt;&lt;p&gt;"We are not here to impose the highest fines," insisted Virkkunen. "We are here to make sure that our digital legislation is enforced."&lt;/p&gt;&lt;p&gt;Additional "very broad" investigations into Musk's platform are ongoing, according to the EU, including into whether X has taken sufficient action to combat the spread of illegal content and manipulated information.&lt;/p&gt;&lt;head rend="h2"&gt;How has the US government reacted to the EU fining X?&lt;/head&gt;&lt;p&gt;But the US government has criticized the bloc for "regulatory suffocation" and threatened trade consequences, with US Commerce Secretary Howard Lutnick last week pressing the EU to rethink its rules if it wants lower steel duties.&lt;/p&gt;&lt;p&gt;Brendan Carr, the chairman of the Federal Communications Commission (FCC), the US agency which regulates national and international radio, television, wire, satellite, and cable communications, condemned the EU.&lt;/p&gt;&lt;p&gt;In a post on X, Carr accused the bloc of "fining a successful US tech company for being a successful US tech company."&lt;/p&gt;&lt;p&gt;"Europe is taxing Americans to subsidize a continent held back by Europe’s own suffocating regulations," Carr said.&lt;/p&gt;&lt;p&gt;Edited by: Roshni Majumdar&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dw.com/en/eu-imposes-120-million-fine-on-elon-musks-x-for-breaking-digital-rules/a-75033724"/><published>2025-12-06T02:32:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46170060</id><title>I cracked a $200 software protection with xcopy</title><updated>2025-12-06T04:12:20.448765+00:00</updated><content>&lt;doc fingerprint="ed16103829ed7681"&gt;
  &lt;main&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Published on&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;i cracked a $200 software protection in a day with xcopy&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Authors&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;list rend="dl"&gt;
              &lt;item rend="dt-2"&gt;Name&lt;/item&gt;
              &lt;item rend="dd-2"&gt;vmfunc&lt;/item&gt;
              &lt;item rend="dd-3"&gt;@vmfunc&lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
          &lt;item&gt;computer wizard&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;disclaimer: this is educational security research only. i do not condone piracy. i purchased a legitimate license for this software and conducted this analysis on my own property. this writeup exists to document protection implementation flaws, not to enable theft. support developers - buy their software.&lt;/p&gt;
    &lt;p&gt;github repo: vmfunc/enigma&lt;/p&gt;
    &lt;head rend="h2"&gt;tl;dr&lt;/head&gt;
    &lt;p&gt;i spent a day analyzing enigma protector - a $200 commercial software protection system used by thousands of vendors. RSA cryptographic signatures, hardware-bound licensing, anti-debugging, VM-based code obfuscation. serious enterprise security theater.&lt;/p&gt;
    &lt;p&gt;then i noticed the protected installer extracts a completely unprotected payload to disk.&lt;/p&gt;
    &lt;code&gt;xcopy /E "C:\Program Files\...\product" .\crack\&lt;/code&gt;
    &lt;p&gt;that’s the entire crack. copy the installed files. they run on any machine. no keygen needed, no binary patching, no cryptanalysis.&lt;/p&gt;
    &lt;p&gt;$200 protection defeated by a command that shipped with DOS 3.2 in 1986.&lt;/p&gt;
    &lt;p&gt;this is a case study in why threat modeling matters more than fancy cryptography, and why “military-grade encryption” means nothing when you leave the back door wide open.&lt;/p&gt;
    &lt;head rend="h2"&gt;target overview&lt;/head&gt;
    &lt;p&gt;bass bully premium - a VST3 synthesizer plugin. protected by enigma protector, a commercial software protection system that costs $250+ and promises serious security.&lt;/p&gt;
    &lt;p&gt;from their marketing:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Enigma Protector is a powerful tool designed to protect executable files from illegal copying, hacking, modification and analysis.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;we’ll see about that.&lt;/p&gt;
    &lt;p&gt;we have one known valid license:&lt;/p&gt;
    &lt;code&gt;Key:  GLUJ-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-V99KP3
HWID: 3148CC-XXXXXX
Name: Bass Bully&lt;/code&gt;
    &lt;p&gt;our goal: understand the protection and build a proper crack&lt;/p&gt;
    &lt;head rend="h2"&gt;static analysis&lt;/head&gt;
    &lt;head rend="h3"&gt;the PE headers&lt;/head&gt;
    &lt;p&gt;first, let’s look at what we’re dealing with:&lt;/p&gt;
    &lt;code&gt;import pefile

pe = pefile.PE(r"Bass Bully Premium_Installer_win64.exe")

print(f"Machine:     {'x64' if pe.FILE_HEADER.Machine == 0x8664 else 'x86'}")
print(f"Sections:    {pe.FILE_HEADER.NumberOfSections}")
print(f"Entry Point: 0x{pe.OPTIONAL_HEADER.AddressOfEntryPoint:X}")
print(f"Image Base:  0x{pe.OPTIONAL_HEADER.ImageBase:X}")&lt;/code&gt;
    &lt;code&gt;Machine:     x64
Sections:    9
Entry Point: 0x16485D0
Image Base:  0x140000000&lt;/code&gt;
    &lt;p&gt;that entry point is suspicious. &lt;code&gt;0x16485D0&lt;/code&gt; is way into the binary.. typical of packed executables where the real entry point is hidden. normal programs start around &lt;code&gt;0x1000&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;string hunting&lt;/head&gt;
    &lt;code&gt;with open(pe_path, 'rb') as f:
    data = f.read()

for target in [b'Enigma Protector', b'enigmaprotector']:
    offset = 0
    while (idx := data.find(target, offset)) != -1:
        print(f"0x{idx:08X}: {target.decode()}")
        offset = idx + 1&lt;/code&gt;
    &lt;code&gt;0x0040972B: Enigma Protector
0x00409746: Enigma Protector
0x00409786: Enigma Protector
0x00409BA8: Enigma Protector
0x00409BC3: Enigma Protector
0x0040A038: Enigma Protector
0x0040A053: Enigma Protector
0x004099BF: enigmaprotector
0x00409DDA: enigmaprotector&lt;/code&gt;
    &lt;p&gt;confirmed: enigma protector. now we know what we’re dealing with.&lt;/p&gt;
    &lt;head rend="h3"&gt;what about the network?&lt;/head&gt;
    &lt;p&gt;does this even phone home..?&lt;/p&gt;
    &lt;code&gt;imports = [entry.dll.decode() for entry in pe.DIRECTORY_ENTRY_IMPORT]&lt;/code&gt;
    &lt;code&gt;kernel32.dll, user32.dll, advapi32.dll, oleaut32.dll, gdi32.dll,
shell32.dll, version.dll, ole32.dll, COMDLG32.dll, MSVCP140.dll, ...&lt;/code&gt;
    &lt;p&gt;no &lt;code&gt;winhttp.dll&lt;/code&gt;, &lt;code&gt;wininet.dll&lt;/code&gt;, or &lt;code&gt;ws2_32.dll&lt;/code&gt;. offline validation only. all crypto is local, so theoretically extractable.&lt;/p&gt;
    &lt;p&gt;this is good news!! online validation would require MITM or server emulation. offline means everything we need is in the binary.&lt;/p&gt;
    &lt;head rend="h2"&gt;the enigma protector internals&lt;/head&gt;
    &lt;p&gt;enigma protector is a commercial protection system that provides:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;code virtualization - transforms x86/x64 into proprietary VM bytecode&lt;/item&gt;
      &lt;item&gt;anti-debugging - &lt;code&gt;IsDebuggerPresent&lt;/code&gt;, timing checks, hardware BP detection&lt;/item&gt;
      &lt;item&gt;anti-tampering - CRC checks on packed sections&lt;/item&gt;
      &lt;item&gt;registration API - HWID-bound licensing with RSA signatures&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;you can read about all these features on their documentation page. they’re pretty thorough about explaining what they protect against. they just didn’t think someone would… not use it on the payload.&lt;/p&gt;
    &lt;head rend="h3"&gt;the registration API&lt;/head&gt;
    &lt;p&gt;according to enigma’s SDK, these functions are exposed:&lt;/p&gt;
    &lt;code&gt;int EP_RegCheckKey(const char* name, const char* key);
const char* EP_RegHardwareID(void);
void EP_RegSaveKey(const char* name, const char* key);
void EP_RegLoadKey(char* name, char* key);&lt;/code&gt;
    &lt;p&gt;these aren’t normal exports; they’re resolved dynamically after enigma unpacks itself. you can’t just &lt;code&gt;GetProcAddress&lt;/code&gt; them from outside. you have to either:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;hook them at runtime after unpacking&lt;/item&gt;
      &lt;item&gt;pattern scan the unpacked memory&lt;/item&gt;
      &lt;item&gt;be an absolute clown and just not use them in your payload (definitely not foreshadowing)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;the entry point&lt;/head&gt;
    &lt;p&gt;using capstone to disassemble the entry point:&lt;/p&gt;
    &lt;code&gt;from capstone import Cs, CS_ARCH_X86, CS_MODE_64

entry_rva = pe.OPTIONAL_HEADER.AddressOfEntryPoint
entry_offset = pe.get_offset_from_rva(entry_rva)

with open(pe_path, 'rb') as f:
    f.seek(entry_offset)
    code = f.read(64)

md = Cs(CS_ARCH_X86, CS_MODE_64)
base = pe.OPTIONAL_HEADER.ImageBase + entry_rva

for insn in md.disasm(code, base):
    print(f"0x{insn.address:X}: {insn.mnemonic:8} {insn.op_str}")&lt;/code&gt;
    &lt;code&gt;0x1416485D0: jmp      0x1416485da      ; skip garbage bytes
0x1416485D2: add      byte ptr [rsi + 0x40], dl
0x1416485D8: add      byte ptr [rax], al
0x1416485DA: push     rax              ; real code starts here
0x1416485DB: push     rcx
0x1416485DC: push     rdx
0x1416485DD: push     rbx
0x1416485DE: push     rbp
0x1416485DF: push     rsi
0x1416485E0: push     rdi
0x1416485E1: push     r8
0x1416485E3: push     r9&lt;/code&gt;
    &lt;p&gt;the jmp-over-garbage pattern is classic anti-disassembly. linear disassemblers will try to decode the garbage bytes between &lt;code&gt;jmp&lt;/code&gt; and its target, producing nonsense. the real unpacker starts at &lt;code&gt;0x1416485DA&lt;/code&gt; with a standard register preservation sequence before calling the enigma loader.&lt;/p&gt;
    &lt;head rend="h2"&gt;phase 3: key format analysis&lt;/head&gt;
    &lt;p&gt;we have a known valid key. let’s understand its structure before we try to break it.&lt;/p&gt;
    &lt;code&gt;GLUJ-XXXX-XXXX-XXXX-XXXX-XXXX-XXXX-V99KP3&lt;/code&gt;
    &lt;head rend="h3"&gt;structure breakdown&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8 groups separated by dashes&lt;/item&gt;
      &lt;item&gt;groups 0-6: 4 characters each&lt;/item&gt;
      &lt;item&gt;group 7: 6 characters (larger - likely checksum/signature)&lt;/item&gt;
      &lt;item&gt;character set: 0-9, A-Z (base36)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;base36 decoding&lt;/head&gt;
    &lt;code&gt;key = "GLUJ-QE58-U3Z4-RQTJ-K7GJ-JXZ5-CVK5-V99KP3"
groups = key.split('-')

for i, group in enumerate(groups):
    val = int(group, 36)
    bits = val.bit_length()
    print(f"[{i}] {group:6} = {val:10} (0x{val:08X}) {bits:2} bits")&lt;/code&gt;
    &lt;code&gt;[0] GLUJ   =     774811 (0x000BD29B) 20 bits
[1] QE58   =    1231388 (0x0012CA1C) 21 bits
[2] U3Z4   =    1404832 (0x00156FA0) 21 bits
[3] RQTJ   =    1294471 (0x0013C087) 21 bits
[4] K7GJ   =     942787 (0x000E62C3) 20 bits
[5] JXZ5   =     930497 (0x000E32C1) 20 bits
[6] CVK5   =     600773 (0x00092AC5) 20 bits
[7] V99KP3 = 1890014727 (0x70A75607) 31 bits  &amp;lt;- significantly larger&lt;/code&gt;
    &lt;p&gt;interesting. group 7 is way bigger than the others. that’s probably a truncated cryptographic signature.&lt;/p&gt;
    &lt;head rend="h3"&gt;cryptographic structure&lt;/head&gt;
    &lt;p&gt;the key structure appears to be:&lt;/p&gt;
    &lt;code&gt;[    DATA: ~143 bits     ] [ SIGNATURE: 31 bits ]
 Groups 0-6 (7 x ~20 bits)   Group 7 (truncated)&lt;/code&gt;
    &lt;p&gt;enigma uses RSA for signing. the full signature would be much larger, but they truncate it to fit the key format. this means:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;public key is embedded in the protected binary&lt;/item&gt;
      &lt;item&gt;key validation = RSA signature verification&lt;/item&gt;
      &lt;item&gt;keygen would require extracting and factoring the RSA modulus&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;RSA with small key sizes is technically factorable with enough compute. but that’s a lot of work for… well, you’ll see.&lt;/p&gt;
    &lt;head rend="h3"&gt;HWID format&lt;/head&gt;
    &lt;code&gt;hwid = "3148CC-059521"
parts = hwid.split('-')
# Two 24-bit values = 48 bits total hardware fingerprint&lt;/code&gt;
    &lt;p&gt;HWID is derived from hardware characteristics (CPU ID, disk serial, MAC address, etc). the key is cryptographically bound to this value, so a key generated for one machine won’t work on another.&lt;/p&gt;
    &lt;p&gt;this is actually decent protection! if they used it properly! (they didn’t lmao)&lt;/p&gt;
    &lt;head rend="h2"&gt;the pivot&lt;/head&gt;
    &lt;p&gt;at this point i’m preparing to either factor the RSA key or do runtime hooking to bypass validation. then i thought: wait, what are we actually protecting here?&lt;/p&gt;
    &lt;p&gt;let me just check the installed VST real quick…&lt;/p&gt;
    &lt;head rend="h3"&gt;analyzing the installed VST&lt;/head&gt;
    &lt;code&gt;vst_path = r"C:\Program Files\Common Files\VST3\Bass Bully VST\Bass Bully Premium.vst3"
vst_dll = vst_path + r"\Contents\x86_64-win\Bass Bully Premium.vst3"
pe_vst = pefile.PE(vst_dll)

print(f"Size: {os.path.getsize(vst_dll):,} bytes")
print("Imports:")
for entry in pe_vst.DIRECTORY_ENTRY_IMPORT:
    print(f"  {entry.dll.decode()}")&lt;/code&gt;
    &lt;code&gt;Size: 7,092,736 bytes
Imports:
  KERNEL32.dll
  USER32.dll
  GDI32.dll
  SHELL32.dll
  ole32.dll
  OLEAUT32.dll
  MSVCP140.dll
  WINMM.dll
  IMM32.dll
  dxgi.dll
  VCRUNTIME140.dll
  VCRUNTIME140_1.dll
  api-ms-win-crt-runtime-l1-1-0.dll
  ...&lt;/code&gt;
    &lt;p&gt;wait. where are the enigma imports?&lt;/p&gt;
    &lt;head rend="h3"&gt;hunting for protection&lt;/head&gt;
    &lt;code&gt;with open(vst_dll, 'rb') as f:
    data = f.read()

for term in [b'Enigma', b'EP_Reg', b'Registration', b'HWID', b'enigma']:
    count = data.count(term)
    print(f"{term.decode():15} : {count} occurrences")&lt;/code&gt;
    &lt;code&gt;Enigma          : 0 occurrences
EP_Reg          : 0 occurrences
Registration    : 0 occurrences
HWID            : 0 occurrences
enigma          : 0 occurrences&lt;/code&gt;
    &lt;p&gt;zero.&lt;/p&gt;
    &lt;p&gt;hold on.&lt;/p&gt;
    &lt;code&gt;$ strings "Bass Bully Premium.vst3" | grep -i enigma
$ strings "Bass Bully Premium.vst3" | grep -i regist&lt;/code&gt;
    &lt;p&gt;nothing. no output… ?????????&lt;/p&gt;
    &lt;p&gt;you have got to be kidding me.&lt;/p&gt;
    &lt;p&gt;the VST has absolutely no protection. it’s a clean JUCE framework build. no enigma runtime. no license callbacks. no protection whatsoever.&lt;/p&gt;
    &lt;p&gt;they protected the installer. not the payload. THE INSTALLER. NOT THE ACTUAL PRODUCT.&lt;/p&gt;
    &lt;p&gt;i can’t even be mad. this is genuinely hilarious.&lt;/p&gt;
    &lt;head rend="h2"&gt;the vulnerability&lt;/head&gt;
    &lt;p&gt;here’s what’s happening:&lt;/p&gt;
    &lt;code&gt;+-------------------------------------------------------------------+
|                    ENIGMA PROTECTOR                               |
|  +--------------------------------------------------------------+ |
|  |  Installer.exe                                               | |
|  |  [x] RSA key verification                                    | |
|  |  [x] HWID binding                                            | |
|  |  [x] Anti-debug, anti-tamper                                 | |
|  |  [x] Code virtualization                                     | |
|  |                        |                                     | |
|  |                        v                                     | |
|  |  +--------------------------------------------------------+  | |
|  |  |  Payload (extracted on install)                        |  | |
|  |  |  - Bass Bully Premium.vst3  &amp;lt;- ZERO PROTECTION lol     |  | |
|  |  |  - Bass Bully Premium.rom   &amp;lt;- NOT EVEN ENCRYPTED      |  | |
|  |  +--------------------------------------------------------+  | |
|  +--------------------------------------------------------------+ |
+-------------------------------------------------------------------+&lt;/code&gt;
    &lt;p&gt;the entire protection stack only controls whether the installer runs. once files hit disk, the protection is basically useless.&lt;/p&gt;
    &lt;p&gt;this is like putting a vault door on a tent.&lt;/p&gt;
    &lt;head rend="h3"&gt;what they should have done&lt;/head&gt;
    &lt;p&gt;enigma would have been effective if the VST itself checked the license:&lt;/p&gt;
    &lt;code&gt;bool VST_Init() {
    char key[256], name[256];
    EP_RegLoadKey(name, key);

    if (!EP_RegCheckKey(name, key)) {
        ShowTrialNag();
        return false;
    }

    CreateThread(NULL, 0, LicenseWatchdog, NULL, 0, NULL);
    return true;
}&lt;/code&gt;
    &lt;p&gt;instead, the VST has no &lt;code&gt;EP_Reg*&lt;/code&gt; calls. no license checks. no callbacks. nothing. it just… runs.&lt;/p&gt;
    &lt;head rend="h2"&gt;the crack&lt;/head&gt;
    &lt;p&gt;the crack is embarrassingly simple. i spent hours analyzing RSA key formats for this…&lt;/p&gt;
    &lt;head rend="h3"&gt;the very sophisticated exploit&lt;/head&gt;
    &lt;code&gt;xcopy /E "C:\Program Files\Common Files\VST3\Bass Bully VST" .\crack\
copy "C:\ProgramData\Bass Bully VST\Bass Bully Premium\*.rom" .\crack\&lt;/code&gt;
    &lt;p&gt;that’s it. that’s the crack. copy the files. they work on any machine because there’s no license check in the actual product.&lt;/p&gt;
    &lt;p&gt;i wrote a python script to automate it because i have some self-respect:&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env python3
import shutil
from pathlib import Path

VST_SRC = Path(r"C:\Program Files\Common Files\VST3\Bass Bully VST\Bass Bully Premium.vst3")
ROM_SRC = Path(r"C:\ProgramData\Bass Bully VST\Bass Bully Premium\Bass Bully Premium.rom")

def extract():
    out = Path("crack_package")
    out.mkdir(exist_ok=True)
    shutil.copytree(VST_SRC, out / "Bass Bully Premium.vst3", dirs_exist_ok=True)
    shutil.copy2(ROM_SRC, out / "Bass Bully Premium.rom")
    print("[+] done")

if __name__ == "__main__":
    extract()&lt;/code&gt;
    &lt;p&gt;usage:&lt;/p&gt;
    &lt;code&gt;python patcher.py&lt;/code&gt;
    &lt;p&gt;load in fl studio. no registration. no nag. no nothing. because there’s no check.&lt;/p&gt;
    &lt;head rend="h2"&gt;for science: the hook approach&lt;/head&gt;
    &lt;p&gt;we also wrote a DLL that hooks enigma’s validation at runtime. completely unnecessary given the vulnerability, but i’d already done the research so here it is:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;windows.h&amp;gt;
#include &amp;lt;detours.h&amp;gt;

static int (WINAPI *Real_EP_RegCheckKey)(LPCSTR, LPCSTR) = NULL;

int WINAPI Hooked_EP_RegCheckKey(LPCSTR name, LPCSTR key) {
    return 1;
}

BOOL APIENTRY DllMain(HMODULE hModule, DWORD reason, LPVOID lpReserved) {
    if (reason == DLL_PROCESS_ATTACH) {
        Sleep(2000);
        DetourTransactionBegin();
        DetourUpdateThread(GetCurrentThread());
        DetourAttach(&amp;amp;(PVOID&amp;amp;)Real_EP_RegCheckKey, Hooked_EP_RegCheckKey);
        DetourTransactionCommit();
    }
    return TRUE;
}&lt;/code&gt;
    &lt;p&gt;this approach works great. completely unnecessary. the payload has no protection.&lt;/p&gt;
    &lt;head rend="h2"&gt;lessons learned&lt;/head&gt;
    &lt;head rend="h3"&gt;for developers&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;protect the payload, not the installer - if users need the installed files to run your software, those files need runtime protection&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;defense in depth - don’t rely on a single layer. the VST should call&lt;/p&gt;&lt;code&gt;EP_RegCheckKey&lt;/code&gt;on load&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;threat model correctly - ask “what happens after installation?” if the answer is “nothing checks the license”, you have a problem&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;periodic validation - one-time checks are trivially bypassed by file copying&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;for reversers&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;always check the payload first - before diving into complex crypto, verify what you’re actually protecting&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;the simplest attack wins - don’t factor RSA when&lt;/p&gt;&lt;code&gt;xcopy&lt;/code&gt;works&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;protection != security - expensive protection systems are worthless if applied incorrectly&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;sometimes the crack writes itself - not every target requires sophisticated techniques&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;conclusion&lt;/head&gt;
    &lt;p&gt;enigma protector’s $250 protection was defeated by:&lt;/p&gt;
    &lt;code&gt;xcopy /E "C:\Program Files\..." .\crack\&lt;/code&gt;
    &lt;p&gt;the protection itself works fine - RSA signatures, HWID binding, anti-debug. but it only protects the installer. the payload runs completely unprotected.&lt;/p&gt;
    &lt;p&gt;250 dollars for a this…&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ud2.rip/blog/enigma-protector/"/><published>2025-12-06T02:37:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46170302</id><title>Have I been Flocked? – Check if your license plate is being watched</title><updated>2025-12-06T04:12:20.253683+00:00</updated><link href="https://haveibeenflocked.com/"/><published>2025-12-06T03:16:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46170332</id><title>Albert Michelson's Harmonic Analyzer (2014) [pdf]</title><updated>2025-12-06T04:12:19.487218+00:00</updated><content/><link href="https://engineerguy.com/fourier/pdfs/albert-michelsons-harmonic-analyzer.pdf"/><published>2025-12-06T03:21:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46170402</id><title>Nook Browser</title><updated>2025-12-06T04:12:19.427674+00:00</updated><content/><link href="https://browsewithnook.com"/><published>2025-12-06T03:32:56+00:00</published></entry></feed>