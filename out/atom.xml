<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-08-31T19:07:08.643213+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45082587</id><title>Why haven't quantum computers factored 21 yet?</title><updated>2025-08-31T19:07:14.471499+00:00</updated><content>&lt;doc fingerprint="7135b1ea033e1ef8"&gt;
  &lt;main&gt;
    &lt;p&gt;In 2001, quantum computers factored the number 15. It’s now 2025, and quantum computers haven’t yet factored the number 21. It’s sometimes claimed this is proof there’s been no progress in quantum computers. But there’s actually a much more surprising reason 21 hasn’t been factored yet, which jumps out at you when contrasting the operations used to factor 15 and to factor 21.&lt;/p&gt;
    &lt;p&gt;The circuit (the series of quantum logic gates) that was run to factor 15 can be seen in Figure 1b of “Experimental realization of Shor’s quantum factoring algorithm using nuclear magnetic resonance”:&lt;/p&gt;
    &lt;p&gt;The important cost here is the number of entangling gates. This factoring-15 circuit has 6 two-qubit entangling gates (a mix of CNOT and CPHASE gates). It also has 2 Toffoli gates, which each decompose into 6 two-qubit entangling gates. So there’s a total of 21 entangling gates in this circuit.&lt;/p&gt;
    &lt;p&gt;Now, for comparison, here is a circuit for factoring 21. Sorry for rotating it, but I couldn’t get it to fit otherwise. Try counting the Toffolis:&lt;/p&gt;
    &lt;p&gt;(Here’s an OPENQASM2 version of the circuit, so you can test it produces the right distribution if you’re inclined to do so.)&lt;/p&gt;
    &lt;p&gt;In case you lost count: this circuit has 191 cnot gates and 369 Toffoli gates, implying a total of 2405 entangling gates. That’s 115x more entangling gates than the factoring-15 circuit. The factoring-21 circuit is more than one hundred times more expensive than the factoring-15 circuit.&lt;/p&gt;
    &lt;p&gt;When I ask people to guess how many times larger the factoring-21 circuit is, compared to the factoring-15 circuit, there’s a tendency for them to assume it’s 25% larger. Or maybe twice as large. The fact that it’s two orders of magnitude more expensive is shocking. So I’ll try to explain why it happens.&lt;/p&gt;
    &lt;p&gt;(Quick aside: the amount of optimization that has gone into this factoring-21 circuit is probably unrepresentative of what would be possible when factoring big numbers. I think a more plausible amount of optimization would produce a circuit with 500x the cost of the factoring-15 circuit… but a 100x overhead is sufficient to make my point. Regardless, special thanks to Noah Shutty for running expensive computer searches to find the conditional-multiplication-by-4-mod-21 subroutine used by this circuit.)&lt;/p&gt;
    &lt;head rend="h1"&gt;Where does the 100x come from?&lt;/head&gt;
    &lt;p&gt;A key background fact you need to understand is that the dominant cost of a quantum factoring circuit comes from doing a series of conditional modular multiplications under superposition. To factor an $n$-bit number $N$, Shor’s algorithm will conditionally multiply an accumulator by $m_k = g^{2^k} \pmod{N}$ for each $k &amp;lt; 2n$ (where $g$ is a randomly chosen value coprime to $N$). Sometimes people also worry about the frequency basis measurement at the end of the algorithm, which is crucial to the algorithm’s function, but from a cost perspective it’s irrelevant. (It’s negligible due by an optimization called “qubit recycling”, which I also could have used to reduce the qubit count of the factoring-21 circuit, but in this post I’m just counting gates so meh).&lt;/p&gt;
    &lt;p&gt;There are three effects that conspire to make the factoring-15 multiplications substantially cheaper than the factoring-21 multiplications:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;All but two of the factoring-15 multiplications end up multiplying by 1.&lt;/item&gt;
      &lt;item&gt;The first multiplication is always ~free, because its input is known to be 1.&lt;/item&gt;
      &lt;item&gt;The one remaining factoring-15 multiplication can be implemented with only two CSWAPs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s consider the case where $g=2$. In that case, when factoring 15, the constants to conditionally multiply by would be:&lt;/p&gt;
    &lt;code&gt;&amp;gt;&amp;gt;&amp;gt; print([pow(2, 2**k, 15) for k in range(8)])
[2, 4, 1, 1, 1, 1, 1, 1]
&lt;/code&gt;
    &lt;p&gt;First, notice that the last six constants are 1. Multiplications by 1 can be implemented by doing nothing. So the factoring-15 circuit is only paying for 2 of the expected 8 multiplications.&lt;/p&gt;
    &lt;p&gt;Second, notice that the first conditional multiplication (by 2) will either leave the accumulator storing 1 (when its control is off) or storing 2 (when its control is on). This can be achieved much more cheaply by performing a controlled xor of $1 \oplus 2 = 3$ into the accumulator.&lt;/p&gt;
    &lt;p&gt;Third, notice that the only remaining multiplication is a multiplication by 4. Because 15 is one less than a power of 2, multiplying by 2 modulo 15 can be implemented using a circular shift. A multiplication by 4 is just two multiplications by 2, so it can also be implemented by a circular shift. This is a very rare property for a modular multiplication to have, and here it reduces what should be an expensive operation into a pair of conditional swaps. (If you go back and look at the factoring-15 circuit at the top of the post, the 2 three-qubit gates are being used to implement these two conditional swaps.)&lt;/p&gt;
    &lt;p&gt;You may worry that these savings are specific to the choice of $g=2$ and $N=15$. And they are in fact specific to $N=15$. But they aren’t specific to $g=2$. They occur for all possible choices of $g$ when factoring 15.&lt;/p&gt;
    &lt;p&gt;For contrast, let’s now consider what happens when factoring 21. Using $g=2$, the multiplication constants would be:&lt;/p&gt;
    &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;  print([pow(2, 2**k, 21) for k in range(10)])
[2, 4, 16, 4, 16, 4, 16, 4, 16, 4]
&lt;/code&gt;
    &lt;p&gt;This is going to be a lot more expensive.&lt;/p&gt;
    &lt;p&gt;First, there’s no multiplications by 1, so the circuit has to pay for every multiplication instead of only a quarter. That’s a ~4x relative cost blowup vs factoring 15. Second, although the first-one’s-free trick does still apply, proportionally speaking it’s not as good. It cheapens 10% of the multiplications rather than 50%. That’s an extra ~1.8x cost blowup vs factoring 15. Third, the multiplication by 4 and 16 can’t be implemented with two CSWAPs. The best conditionally-multiply-by-4-mod-21 circuit that I know is the one being used in the diagram above, and it uses 41 Toffolis. These more expensive multiplications add a final bonus ~20x cost blowup vs factoring 15.&lt;/p&gt;
    &lt;p&gt;(Aside: multiplication by 16 mod 21 is the inverse of multiplying by 4 mod 21, and the circuits are reversible, so multiplying by 16 uses the same number of Toffolis as multiplying by 4.)&lt;/p&gt;
    &lt;p&gt;These three factors (multiplying-by-one, first-one’s-free, and multiplying-by-swapping) explain the 100x blowup in cost of factoring 21, compared to factoring 15. And this 100x increase in cost explains why no one has factored 21 with a quantum computer yet.&lt;/p&gt;
    &lt;head rend="h1"&gt;Closing remarks&lt;/head&gt;
    &lt;p&gt;Another contributor to the huge time gap between factoring 15 and factoring 21 is that the 2001 factoring of 15 was done with an NMR quantum computer. These computers were known to have inherent scaling issues, and in fact it’s debated whether NMR computers were even properly “quantum”. If the 2001 NMR experiment doesn’t count, I think the actually-did-the-multiplications runner-up is a 2015 experiment done with an ion trap quantum computer (discussed by Scott Aaronson at the time).&lt;/p&gt;
    &lt;p&gt;Yet another contributor is the overhead of quantum error correction. Performing 100x more gates requires 100x lower error, and the most plausible way of achieving that is error corection. Error correction requires redundancy, and could easily add a 100x overhead on qubit count. Accounting for this, I could argue that factoring 21 will be ten thousand times more expensive than factoring 15, rather than “merely” a hundred times more expensive.&lt;/p&gt;
    &lt;p&gt;There are papers that claim to have factored 21 with a quantum computer. For example, here’s one from 2021. But, as far as I know, all such experiments are guilty of using optimizations that imply the code generating the circuit had access to information equivalent to knowing the factors (as explained in “Pretending to factor large numbers on a quantum computer” by Smolin et al). Basically: they don’t do the multiplications, because the multiplications are hard, but the multiplications are what make it factoring instead of simpler forms of period finding. So I don’t count them.&lt;/p&gt;
    &lt;p&gt;There is unfortunately a trickle of bullshit results that claim to be quantum factoring demonstrations. For example, I have a joke paper in this year’s sigbovik proceedings that cheats in a particularly silly way. More seriously, I enjoyed “Replication of Quantum Factorisation Records with an 8-bit Home Computer, an Abacus, and a Dog” making fun of some recent egregious papers. I also recommend Scott Aaronson’s post “Quantum computing motte-and-baileys”, which complains about papers that benchmark “variational” factoring techniques while ignoring the lack of any reason to expect them to scale.&lt;/p&gt;
    &lt;p&gt;Because of the large cost of quantum factoring numbers (that aren’t 15), factoring isn’t yet a good benchmark for tracking the progress of quantum computers. If you want to stay abreast of progress in quantum computing, you should be paying attention to the arrival quantum error correction (such as surface codes getting more reliable as their size is increased) and to architectures solving core scaling challenges (such as lost neutral atoms being continuously replaced).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://algassert.com/post/2500"/></entry><entry><id>https://news.ycombinator.com/item?id=45082595</id><title>F-Droid site certificate expired</title><updated>2025-08-31T19:07:13.990052+00:00</updated><content>&lt;doc fingerprint="7857cdff5f9f3c6f"&gt;
  &lt;main&gt;
    &lt;p&gt;Skip to content GitLab Menu Why GitLab Pricing Contact Sales Explore Why GitLab Pricing Contact Sales Explore Sign in Get free trial Site certificates expired "Your connection isn't private" I got error when access f-droid.org on Edge and Chrome To upload designs, you'll need to enable LFS and have an admin enable hashed storage. More information&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gitlab.com/fdroid/fdroid-website/-/issues/883"/></entry><entry><id>https://news.ycombinator.com/item?id=45082731</id><title>"This telegram must be closely paraphrased before being communicated" Why?</title><updated>2025-08-31T19:07:13.641671+00:00</updated><content>&lt;doc fingerprint="2e9509d36c2fa0d0"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;It appears that it was US military communications doctrine to not send the exact same message twice using different encryption ("none" counting as one type of encryption), and the term of art for changing a message to avoid that was indeed "paraphrase".&lt;/p&gt;
      &lt;p&gt;I managed to dig up a US Army document on Cryptology from roughly that era that appears to discuss paraphrasing. The document in question is Department of the Army Technical Manual TM 32-220(pdf), dated 1950, titled "BASIC CRYPTOGRAPHY". It apparently supersedes previous documents TM-484 from March 1945 and TM 11-485 from June 1944. It would probably be more ideal to look at them, since they are closer to the time you are interested in, but I was not able to find them online.&lt;/p&gt;
      &lt;p&gt;Here's what this declassified manual had to say about "paraphrasing", from Chapter 7, in the section Fundamental Rules of Cryptographic Security, section 84, subsection b, rule 3 (titled "Text of messages")&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;(a) Never repeat in the clear the identical text of a message once sent in cryptographic form, or repeat in cryptographic form the text of a message once sent in the clear. Anything which will enable an alert enemy to compare a given piece of plain text with a cryptogram that supposedly contains this plain text is highly dangerous to the safety of the cryptographic system. Where information must be given out for publicity, or where information is handled by many persons, the plain text version should be very carefully paraphrased before distribution, to minimize the data an enemy might obtain from an accurate comparison of the cryptographic text with the equivalent, original plain text. To paraphrase a message means to rewrite it so as to change its original wording as much as possible without changing the meaning of the message. This is done by altering the positions of sentences in the message, by altering the positions of subject, predicate, and modifying phrases or clauses in the sentence, and by altering as much as possible the diction by the use of synonyms and synonymous expressions. In this process, deletion rather than expansion of the wording of the message is preferable, because if an ordinary message is paraphrased simply by expanding it along its original lines, an expert can easily reduce the paraphrased message to its lowest terms, and the resultant wording will be practically the original message. It is very important to eliminate repeated words or proper names, if at all possible, by the use of carefully selected pronouns; by the use of the words "former," "latter," "first-mentioned," "second-mentioned"; or by other means. After carefully paraphrasing, the message can be sent in the other key or code.&lt;/p&gt;
        &lt;p&gt;(b) Never send the literal plain text or a paraphrased version of the plain text of a message which has been or will be transmitted in cryptographed form except as specifically provided in appropriate regulations&lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;(emphasis mine)&lt;/p&gt;
      &lt;p&gt;In fact the allies would have have known intimately about how this was possible, because this is one of the ways they ended up decrypting the stronger German Enigma cipher. Captured machines using simpler ciphers were used to break those simpler ciphers. The fact that the Germans were encrypting the exact same messages in both ciphers meant the allies could know (for those messages) what both the unencrypted and encrypted messages were, which allowed them to decrypt the stronger cyphers as well, or quickly figure out what today's code was.&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;Though Enigma had some cryptographic weaknesses, in practice it was German procedural flaws, operator mistakes, failure to systematically introduce changes in encipherment procedures, and Allied capture of key tables and hardware that, during the war, enabled Allied cryptologists to succeed.&lt;/p&gt;
      &lt;/quote&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://history.stackexchange.com/questions/79371/this-telegram-must-be-closely-paraphrased-before-being-communicated-to-anyone"/></entry><entry><id>https://news.ycombinator.com/item?id=45083134</id><title>Notes on Managing ADHD</title><updated>2025-08-31T19:07:13.412717+00:00</updated><content>&lt;doc fingerprint="265009540f6314f0"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;The pleasure is in foreseeing it, not in bringing it to term.&lt;/p&gt;
      &lt;p&gt;— Jorge Luis Borges, Selected Non-Fictions&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This post is about managing ADHD. It is divided into two sections: “Strategies” describes the high-level control system, “Tactics” is a list of micro-level improvements (really it should be called “stratagems”, since most are essentially about tricking yourself).&lt;/p&gt;
    &lt;head rend="h1"&gt;Contents&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Strategies&lt;/item&gt;
      &lt;item&gt;Tactics &lt;list rend="ol"&gt;&lt;item&gt;Task Selection&lt;/item&gt;&lt;item&gt;Visual Field Management&lt;/item&gt;&lt;item&gt;Project Check-Ins&lt;/item&gt;&lt;item&gt;Centralize Your Inboxes&lt;/item&gt;&lt;item&gt;Inbox Zero&lt;/item&gt;&lt;item&gt;Inbox Bankruptcy&lt;/item&gt;&lt;item&gt;Do It On Your Own Terms&lt;/item&gt;&lt;item&gt;Replace Interrupts with Polling&lt;/item&gt;&lt;item&gt;Accountability Buddy&lt;/item&gt;&lt;item&gt;Plan First, Do Later&lt;/item&gt;&lt;item&gt;Derailment&lt;/item&gt;&lt;item&gt;Using OCD to Defeat ADHD&lt;/item&gt;&lt;item&gt;The Master of Drudgery&lt;/item&gt;&lt;item&gt;Thrashing&lt;/item&gt;&lt;item&gt;Put Travel in the Calendar&lt;/item&gt;&lt;item&gt;Choice of Tools&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Resources&lt;/item&gt;
      &lt;item&gt;Acknowledgements&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Strategies&lt;/head&gt;
    &lt;p&gt;High-level advice, control systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chemistry First&lt;/head&gt;
    &lt;p&gt;ADHD has a biological cause and drugs are the first-line treatment for good reasons. There is no virtue in trying to beat it through willpower alone.&lt;/p&gt;
    &lt;p&gt;The first-line treatment for ADHD is stimulants. Everything else in this post works best as a complement to, rather than as an alternative to, stimulant medication. In fact most of the strategies described here, I was only able to execute after starting stimulants. For me, chemistry is the critical node in the tech tree: the todo list, the pomodoro timers, etc., all of that was unlocked by the medication.&lt;/p&gt;
    &lt;p&gt;Some people can’t tolerate a specific stimulant. But there are many stimulant and non-stimulant drugs for ADHD. I would prefer to exhaust all the psychiatric options before white-knuckling it.&lt;/p&gt;
    &lt;p&gt;A lot of people don’t want to take medication for shame-based reasons. There is a lot of pill-shaming in the culture. You must learn to ignore it: we are automata, our minds are molecules in salt water.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example: Melatonin&lt;/head&gt;
    &lt;p&gt;As a motivating example for the “salt water automaton” view: I struggled with sleep hygiene for a long time. It felt like WW1: throwing wave after wave of discipline at it and always failing. I would set an alarm, for, say, 10pm, that said: it is time to go to bed. How many times did I obey it? Never. I was always doing something more important.&lt;/p&gt;
    &lt;p&gt;What fixed it? Melatonin. I have an alarm that goes off at 8pm to remind me to take melatonin. The point of the alarm is not, “now you must log off”, which is a very discipline-demanding task. The point of the alarm is simply: take this pill. It takes but a moment. Importantly, I’m not committing to anything other than taking a pill. Thirty, forty minutes later, I want to sleep. That is the key thing: the melatonin has changed my preferences. And then I don’t need willpower to close the sixteen Wikipedia tabs or whatever, because I want to sleep more than I want to scroll, or watch YouTube.&lt;/p&gt;
    &lt;head rend="h3"&gt;Internal and External Change&lt;/head&gt;
    &lt;p&gt;The broader perspective here is that personal growth is a dialogue between internal changes and external changes.&lt;/p&gt;
    &lt;p&gt;Internal changes might come from medication, meditation, therapy, coaching, or practicing habits for a long enough time. External changes are the scaffolding around the brain: using a todo list, and using it effectively. Using a calendar. Clearing your desk so you don’t get distracted by things. Journaling, so that you can introspect and notice patterns: which behaviours leads to a good workday, and which behaviours lead to a day being wasted.&lt;/p&gt;
    &lt;p&gt;Are internal changes more important? Kind of. It’s more a back and forth, where internal changes unlock external changes which unlock further internal changes.&lt;/p&gt;
    &lt;p&gt;Here’s an example: you (having undiagnosed ADHD) try to set a schedule, or use a todo list, or clean your bed every day, but it doesn’t stick. So you get on medication, and the medication lets you form your first habit: which is using a todo list app consistently, checking it every morning. Then, with the todo list as a core part of your exocortex, you start adding recurring tasks, and forming other simple habits: you have a daily recurring task to make your bed, and so every morning when you check the todo list, you see the task, and make your bed, and in time, with your now-functioning dopamine system, you make a habit to make your bed every day, such that you no longer need to have that in the todo list.&lt;/p&gt;
    &lt;p&gt;So the timeline is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Internal change: starting medication unlocks…&lt;/item&gt;
      &lt;item&gt;External change: using a todo list, which provides scaffolding (e.g. daily recurring tasks) for forming new habits, which unlocks&lt;/item&gt;
      &lt;item&gt;Internal change: new habits formed (make bed, brush teeth in the morning)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Taking Ritalin with no plan for what you will do today/tomorrow/this week doesn’t work. Dually, an ambitious todo list will sit idle if your brain won’t let you execute it. So personal growth comes from using both internal and external changes, like a ladder with alternating left-right steps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory&lt;/head&gt;
    &lt;p&gt;A todo list is a neuroprosthesis that augments long-term memory for tasks.&lt;/p&gt;
    &lt;p&gt;I use Todoist on my desktop and my phone. The pro plan is worth it. I don’t really think of it as an app, rather, it’s a cognitive prosthesis.&lt;/p&gt;
    &lt;p&gt;The todo list provides three things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Memory: the list remembers things for me. I’m not at the mercy of my brain randomly pinging me that I forgot to do X or I want to someday do Y. The todo list remembers.&lt;/item&gt;
      &lt;item&gt;Order: the todo list lets you drag and drop tasks around, so you can figure out the ordering in which you’re going to do them.&lt;/item&gt;
      &lt;item&gt;Hierarchy: the todo list lets you break tasks down hierarchically and without limit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of these, the most important is memory. The todolist is an action-oriented long term memory prosthesis.&lt;/p&gt;
    &lt;p&gt;This is especially useful for habit formation: my biggest blocker with forming habits was just remembered that I’d committed to doing something. If you think, i will make the bed every day, you might do it today, tomorrow, and by the third day you forget. You’re failing by simply forgetting to show up, which is a sad way to fail. By making something a recurring task on the todo list, it ensures I will see it. In a sense, the todo list turns many habits into one. You don’t need to remember “I will make my bed every day”, “I will floss my teeth every night”, etc., because the todolist remembers all those things for you. You only need to form a single habit: checking the todo list.&lt;/p&gt;
    &lt;p&gt;Analogously, I often fail to finish projects simply because I forget about them. I start reading a book, but I don’t write it down anywhere (say, in Goodreads) that “I’m reading this book” is something I have committed to. I leave the book on a table where it’s out of sight (and therefore out of mind) for all of my waking hours. I glance at it occasionally and think, oh, yeah, I was reading that book, and then I’m distracted by something else. And weeks later, when I’ve already started another book, I notice the first book, with the bookmark on page 20, abandoned.&lt;/p&gt;
    &lt;p&gt;The todolist prevents this failure mode: you create a project to represent reading the book, and that project is now tracked, and when you open the todo list, you can see it in the list of active projects.&lt;/p&gt;
    &lt;head rend="h3"&gt;How I Use Todoist&lt;/head&gt;
    &lt;p&gt;In Todoist, every task is part of a project (which really should just be called a list). My sidebar looks like this:&lt;/p&gt;
    &lt;p&gt;Tasks is the list for ad-hoc tasks. Mostly chores and things that don’t fit in elsewhere. Unload the dishwasher, reply to this email, etc. The only rule for this list is that everything in it must be scheduled.&lt;/p&gt;
    &lt;p&gt;Groceries is self-explanatory.&lt;/p&gt;
    &lt;p&gt;Ideas is the where every half-formed goal, intention, project idea etc. goes. “Go deeper into metta” and “learn how to use the slide rule” and “go penguin watching in Manly” and “write a journalling app” and “learn PLT Redex”. I put these things here so that they don’t live in my brain. And occasionally I go through the list and promote something into an actual, active project.&lt;/p&gt;
    &lt;p&gt;Blog is like the ideas list specifically ideas for blog posts.&lt;/p&gt;
    &lt;p&gt;Reading List is for media I want to consume. This is divided into: fiction books, non-fiction books, technical books, blog posts, papers, games, films.&lt;/p&gt;
    &lt;p&gt;Cycles is for recurring tasks. This one is divided into sections by period: daily, weekly, and above. The daily recurring tasks are things like “take vitamin D”, “meditate”, and the inbox-clearing task.&lt;/p&gt;
    &lt;p&gt;Projects is a container for actual projects: an objective which takes multiple tasks to accomplish. Why lift projects into lists? Why not just use a top-level task to represent the project’s objective, and nested subtasks to represent the execution steps of the project? Because having the project in the sidebar is one mechanism I use to ensure I don’t forget about it. Every time I glance at the todo list, I can see the list of active projects. I can notice if something has not been worked on for a while, and act on it. Otherwise: out of sight, out of mind.&lt;/p&gt;
    &lt;head rend="h2"&gt;Energy&lt;/head&gt;
    &lt;p&gt;The difficulty class of the tasks you can perform declines throughout the day.&lt;/p&gt;
    &lt;p&gt;There are many metaphors for the concept of mental energy. Spoon theory, for example. The usual metaphor is that “mental energy” is like a battery that is drained through the day, in greater and lesser quantities, and is replenished by sleep.&lt;/p&gt;
    &lt;p&gt;To me, energy is less like a battery and more like voltage. Some machines require a threshold voltage to operate. Below that voltage they don’t just operate slower, they don’t operate at all. Analogously, different categories of activity have different threshold voltages. For me, it’s like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Things I am averse to, the things I intuitively want to put off because they bring up painful emotions, are high-voltage.&lt;/item&gt;
      &lt;item&gt;Creative, open-ended work is high-voltage to start, but once you get started, keeping it going is medium-voltage.&lt;/item&gt;
      &lt;item&gt;Simple chores like cleaning, throwing clothes in the washing machine, etc. are low-voltage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And when I wake up I have the highest possible voltage, and throughout the course of the day the voltage declines. And that’s the key difference from spoon theory: spoons are fungible across time, voltage is not. For each category of activity, there is a span of the day when I can action it.&lt;/p&gt;
    &lt;p&gt;When I wake up, I do my morning routine, get some quick wins, and then I try to tackle the thing I dread the most, as early in the morning as possible, because that’s the time of day when I have the most energy and self-control. I get that done and I move on.&lt;/p&gt;
    &lt;p&gt;(Another reason to do the dreaded tasks first: if you put it off to, say, late morning, well, why not put it off again? And again and again. And then it’s 7pm and you can’t even think about the task, and it’s late, and I don’t have energy, so I couldn’t even do it if I wanted to, so let’s do it tomorrow.)&lt;/p&gt;
    &lt;p&gt;And then, when I have removed that burden, I work on projects. The creative, generative, intellectual things. The things that move some kind of needle, and aren’t just pointless chores.&lt;/p&gt;
    &lt;p&gt;And when I run out of energy to create, I read.&lt;/p&gt;
    &lt;p&gt;And when I run out of energy to read, I clean and go to the gym and do the other things.&lt;/p&gt;
    &lt;p&gt;And when the sun goes down everything starts to unravel: I have zero energy and the lazy dopamine-seeking behaviour comes out. So I take melatonin, and try to be in bed before the instant gratification monkey seizes power.&lt;/p&gt;
    &lt;head rend="h2"&gt;Procrastination&lt;/head&gt;
    &lt;p&gt;Typology of procrastination, approaches.&lt;/p&gt;
    &lt;p&gt;In my ontology there are three types of procrastination:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ADHD Procrastination: you want to do the task, but can’t because of distraction/hyperactivity.&lt;/item&gt;
      &lt;item&gt;Anxious Procrastination: you know you have to do the task, but you don’t want to, because it triggers difficult emotions.&lt;/item&gt;
      &lt;item&gt;Decision Paralysis Procrastination: you don’t know how to execute the task, because it involves a decision and you have difficulty making the decision.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;ADHD Procrastination&lt;/head&gt;
    &lt;p&gt;This is the easiest kind to address. The solution is pharmacological treatment for ADHD + having a productivity system and some tricks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Anxious Procrastination&lt;/head&gt;
    &lt;p&gt;This one is harder. The good thing is you know, cognitively, what you have to do. The hard part is getting over the aversion.&lt;/p&gt;
    &lt;p&gt;In the short term, the way to fix this is to do it scared. Accept the anxiety. Asking for help also works, sometimes you just need someone in the room with you when you hit send on the email. You can also use techniques like CBT to rationally challenge the source of the anxiety and maybe overcome it.&lt;/p&gt;
    &lt;p&gt;In the long term: write down the things you procrastinate one due to anxiety, and find the common through-line, or the common ancestor. By identifying the emotional root cause, you can work on fixing it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Decision Paralysis Procrastination&lt;/head&gt;
    &lt;p&gt;And this is the hardest, because you don’t know, cognitively, what the right choice is, and also you probably have a lot of anxiety/aversion around it. Many things in life are susceptible to this: you have set of choices, there’s good arguments for/against each one, and you have a lot of uncertainty as to the outcomes. And so you ruminate on it endlessly.&lt;/p&gt;
    &lt;p&gt;I don’t have a good general solution for this.&lt;/p&gt;
    &lt;p&gt;Talking to people helps: friends, therapists, Claude. This works because thinking by yourself has diminishing returns: you will quickly exhaust all the thoughts you will have about the problem, and start going in circles. Often people will bring up options/considerations I would never have thought of. Sometimes, if you’re lucky, that’s all it takes: someone mentions an option you had not considered and you realize, oh, it was all so simple.&lt;/p&gt;
    &lt;p&gt;One thing to consider is that thinking in your head is inherently circular, because you have a limited working memory, and you will inevitably start going in circles. Writing things down helps here. Treat the decision, or the emotions behind it, like an object of study, or an engineering problem. Sit down and write an essay about it. Name the arguments, number the bullet points, refer back to things. Make the thoughts into real, physical, manipulable entities.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introspection&lt;/head&gt;
    &lt;p&gt;Journaling is good for detecting maladaptive patterns and tracking your progress.&lt;/p&gt;
    &lt;p&gt;I keep a hierarchical journal in Obsidian. Hierarchical because I have entries for the days, weeks, months, and years. The directory tree looks like this:&lt;/p&gt;
    &lt;code&gt;Journal/
  Daily/
    YYYY/
      MM/
        YYYY-MM-DD.md
  Weekly/
    YYYY/
      YYYY-WW.md
  Monthly/
    YYYY/
      YYYY-MM.md
  Yearly/
    YYYY.md
&lt;/code&gt;
    &lt;p&gt;In the morning I finish yesterday’s journal entry, and begin today’s. Every Sunday I write the review of the week, the first of each month I write the review of the previous month, the first of each year I review the past year. The time allotted to each review is in inverse proportion to its frequency: so a monthly review might take an hour while a yearly review might take up a whole morning.&lt;/p&gt;
    &lt;p&gt;The daily reviews are pretty freeform. Weekly and above there’s more structure. For example, for the weekly reviews I will write a list of the salient things that happened in the week. Then I list on what went well and what went poorly. And then I reflect on how I will change my behaviour to make the next week go better.&lt;/p&gt;
    &lt;p&gt;Journaling is a valuable habit. I started doing it for vague reasons: I wasn’t sure what I wanted to get out of it, and it took a long time (and long stretches of not doing it) until it became a regular, daily habit. I’ve been doing it consistently now for three years, and I can identify the benefits.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The main benefit is that to change bad patterns, you have to notice them. And it is very easy to travel in a fix orbit, day in, day out, and not notice it. Laying it out in writing helps to notice the maladaptive coping mechanisms. Reading back over the journal entries helps you notice: when an event of type X happens, I react with Y.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Today’s journal entry is a good default place for writing ad-hoc notes or thoughts. Often I wanted to write something, but didn’t know where I would file it (how do you even file these little scraps of thought?) and from not knowing where to put it, I would not do it. Nowadays I just begin writing in the journal. Later, if it is valuable to file it away, I do so.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Creating a journal entry in the morning is a good opportunity to go over the goals and priorities for the day and explicitly restate them to myself.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The final benefit is retrospection: I can look at the past and see how my life has changed. And this is often a positive experience, because the things that worried me didn’t come to pass, the things I used to struggle with are now easy, or at least easier.&lt;/p&gt;
        &lt;p&gt;There’s a paradox with productivity: when you grind executive function enough, things that you used to struggle with become quotidian. And so what was once the ceiling becomes the new floor. You no longer feel proud that you did X, Y, Z because that’s just the new normal. It’s like the hedonic treadmill. You might feel that you never get to “productive”. Journaling helps to combat this because you can see how far you’ve come.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Time&lt;/head&gt;
    &lt;p&gt;Manage time at the macro level with calendars, at the micro level with timers.&lt;/p&gt;
    &lt;p&gt;To manage time, you need a calendar (macro) and a timer (micro).&lt;/p&gt;
    &lt;head rend="h3"&gt;Macro&lt;/head&gt;
    &lt;p&gt;At the macro level, I use the calendar very lightly. Mostly for social things (to ensure I don’t forget an event, and that I don’t double-book things). I also use it to schedule the gym: if the goal is to lift, say, five times a week, I schedule five time blocks to lift. Lifting is special because it has a lot of temporal constraints:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I lift exactly n times per week.&lt;/item&gt;
      &lt;item&gt;I lift at most once a day.&lt;/item&gt;
      &lt;item&gt;I lift in the evening, which potentially clashes with social things.&lt;/item&gt;
      &lt;item&gt;There are adjacency constraints, e.g. doing shoulders the day before chest is bad.&lt;/item&gt;
      &lt;item&gt;There is at least one rest day which has to be scheduled strategically (e.g. to have maximal distance between successive deadlift sessions).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But outside these two categories, my calendar is empty.&lt;/p&gt;
    &lt;p&gt;The calendar might be useful to you as a self-binding device. If you keep dragging some project along because you “haven’t made time” for it: consider making a time block in the calendar, and sticking to it. Creating a calendar event is, literally, making time: it’s like calling &lt;code&gt;malloc_time()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Some people use the calendar as their entire todo list. I think this kind of works if your todo list is very coarse grained: “buy groceries” and “go to the dentist”. But I have a very fine-grained todo list, and putting my tasks in the calendar would make it overwhelming.&lt;/p&gt;
    &lt;p&gt;Another problem with calendars is they are too time-bound: if I make a calendar block to do something, and I don’t do it, the calendar doesn’t know it. It just sits there, forgotten, in the past. In a todo list, everything gets dragged along until I explicitly complete it. Along the same lines, the calendar is not good for collecting vague ideas and plans for things you want to do in the future, while todo lists are ideal for this.&lt;/p&gt;
    &lt;head rend="h3"&gt;Micro&lt;/head&gt;
    &lt;p&gt;The problem with todo lists is that they’re timeless: there is no sense of urgency. You look at the list and think, I could do the next task now, or in five minutes, or in an hour. There’s always some time left in the day. Or tomorrow. You need a way to manufacture urgency.&lt;/p&gt;
    &lt;p&gt;If you have ADHD you’ve probably heard of the Pomodoro method, tried it, and bounced off it. The way it’s framed is very neurotypical: it’s scaffolding around doing, but ADHD people often have problems with the doing itself. And so the scaffolding is kind of pointless.&lt;/p&gt;
    &lt;p&gt;The method works well in three kinds of contexts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Overcoming Aversion: when you have a large number of microtasks, each of which takes a few seconds to a few minutes, but the number of them, and the uncertainty factor, makes the sum seem a lot larger. A classic example for me is having to reply to like ten different people. Realistically, each person can be handled in 15s. One or two might require a couple of minutes to compose a longer reply. But often I will avoid those tasks like the plague and drag them across the entire day.&lt;/p&gt;
        &lt;p&gt;The pomodoro method works here because you’re basically trading (up to) 25m of pain for an entire day’s peace and quiet. So you get all the annoying little tasks together, start a timer, and go through them. And usually you’re done in maybe ten minutes. And you feel really good after, because all those annoying little tasks are done.&lt;/p&gt;
        &lt;p&gt;It really is amazing what a little bit of fake urgency can do.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Starting: sometimes the problem is just starting. It is very trite, but it’s true. You have something you want to want to do, but don’t want to do. I want to want to read this book, to learn this topic, to write this blog post, to work on this software project. But I don’t want to do it. The pomodoro method helps you start.&lt;/p&gt;
        &lt;p&gt;You’re not committing to finishing the project. You’re not committing to months or weeks or days or even hours of work. You’re committing to a half hour. And if you work just that half hour: great, promise kept. 30m a day, over the course of a single month, is 15h of work. And often I start a 30m timer and end up working four hours, and maybe that’s a good outcome.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stopping: dually, sometimes the problem is stopping. If you’re trying to advance multiple projects at the same time, if you hyperfocus on one, it eats into the time you allocated for the others. And more broadly, spending too much time on one project can derail all your plans for the day. Maybe you meant to go to the gym at 6pm but you got so stuck in with this project that it’s 8:30pm and you’re still glued to the screen. So the gym suffers, your sleep schedule suffers, etc.&lt;/p&gt;
        &lt;p&gt;Actually stopping when the pomodoro timer goes off can prevent excessive single-mindedness.&lt;/p&gt;
        &lt;p&gt;Additionally, the five-minute break at the end of the pomodoro block is useful. It’s a time to get up from the computer, unround your shoulders, practice mindfulness, essentially, all those little things that you want to do a few times throughout the day.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Tactics&lt;/head&gt;
    &lt;p&gt;Stratagems, tricks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Task Selection&lt;/head&gt;
    &lt;p&gt;To select the next task, pick either the shortest or the most-procrastinated task.&lt;/p&gt;
    &lt;p&gt;I don’t like the word “prioritize”, because it has two subtly different meanings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Weak prioritization” means to sort a list of tasks by some unspecified criterion, that is, to establish an order where some things are prior to another.&lt;/item&gt;
      &lt;item&gt;“Strong prioritization” is to sort a list specifically by importance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;“Weak prioritization” is something everyone should do: it takes a moment to go over the todo list and drag the tasks into more or less the order in which you will do them. This keeps the most relevant tasks near the top, which is where your eyes naturally go to.&lt;/p&gt;
    &lt;p&gt;“Strong prioritization” is a terrible job scheduling algorithm. Importance alone is not good enough.&lt;/p&gt;
    &lt;p&gt;Consider the case where you have a very important task A which takes a long time to finish, and a less important task B which takes 5m to finish. For example, writing an essay versus replying to an email. Which should you do first? I would execute B first, because doing so in turn unblocks B’s successor tasks. If you reply to the email and then get to work on task A, the other person has time to read your email and reply to you. And the conversation moves forward while you are otherwise engaged.&lt;/p&gt;
    &lt;p&gt;Of course, the pathological version of this is where you only action the quick wins: all the minute little chores get done instantly, but the big tasks, requiring long periods of concentration, get postponed perpetually.&lt;/p&gt;
    &lt;p&gt;My task-selection algorithm is basically: do the shortest task first, with two exceptions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Stalled tasks get a priority bump. If I created a task weeks ago, or if I’ve been postponing in for many days in a row, it has to be done now.&lt;/item&gt;
      &lt;item&gt;Content-dependence: if I’m working on a particular project, I’d rather focus on tasks from that project, rather than from the global todo list.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Visual Field Management&lt;/head&gt;
    &lt;p&gt;To remember something, put it in your visual field. Dually: to forget, get it out of sight.&lt;/p&gt;
    &lt;p&gt;Out of sight, out of mind. The corollary: to keep something in mind, put it in your visual field; to keep it out, leave it out.&lt;/p&gt;
    &lt;p&gt;My desk is very spartan: there’s a monitor, a mouse, and a keyboard, and a few trinkets. My desktop is empty. There are no files in it. The dock has only the apps I use frequently. And at a higher level, I try to keep the apartment very clean and orderly. Because everything that’s out of place is a distraction, visual noise. That’s the negative aspect: the things I remove.&lt;/p&gt;
    &lt;p&gt;The positive aspect, the things I keep in my visual field: most of the time, I have two windows open on my computer the todo list occupies the left third of the screen, the right two-thirds are occupied by whatever window I have open at the time, e.g.:&lt;/p&gt;
    &lt;p&gt;And so at a glance, I can see:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What I’m currently working on.&lt;/item&gt;
      &lt;item&gt;What I will work on next.&lt;/item&gt;
      &lt;item&gt;The list of active projects, so that I don’t forget they exist.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Project Check-Ins&lt;/head&gt;
    &lt;p&gt;Keep in regular contact with long-running projects.&lt;/p&gt;
    &lt;p&gt;A common failure mode I have is, I will fail to finish a project because I forget I even started it. Or, relatedly: I will let a project drag on and on until enough time has passed that my interests have shifted, the sun has set on it, and it is now a slog to finish.&lt;/p&gt;
    &lt;p&gt;One reason I do this is that creative/intellectual work often requires (or feels like it requires) long stretches of uninterrupted time. So I procrastinate working on something until I can find such a chunk of time. Which never comes. Time passes and the project begins to slip the moorings of my attention, as other new and shiny things arrive.&lt;/p&gt;
    &lt;p&gt;And sometimes I will pick the project back up after months or years, and I have lost so much context, it’s impossible to know what I even intended. And then you procrastinate even more, because you don’t want to feel the guilty of picking up a project and realizing it has become strange and unfamiliar to you.&lt;/p&gt;
    &lt;p&gt;One way to combat this is to make regular project checkins. This could be a daily or few-times-a-week recurring task on Todoist that just says “spend 30m on this project”.&lt;/p&gt;
    &lt;p&gt;You don’t even have to work on the thing: just allocate fifteen minutes to hold the project in your mind and nothing else. If it’s creative writing, you might open the Word document and just look at it. If it’s a programming project: read the Jira board and look at the code again. Don’t write anything. Just read the code. You will likely come up with a few tasks to do, so write those down. Think. Plan. Build up the structures in your mind, refresh the caches. If you can do, do, otherwise, plan, and if you can’t even do that, read.&lt;/p&gt;
    &lt;p&gt;When you’re doing this regularly, when you’re in regular contact with the project, when the shape of it is clear in your mind, you will have the tasks on the top of your mind, you will no longer feel that you need a giant empty runway of time to work on it, you will be able to work on it in shorter chunks.&lt;/p&gt;
    &lt;p&gt;To manage long-term creative work, keep in regular contact. That doesn’t mean work on them every day, but maybe look at them every day.&lt;/p&gt;
    &lt;p&gt;The pomodoro method works here. Set a timer for just 25m to keep in touch with the project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Centralize Your Inboxes&lt;/head&gt;
    &lt;p&gt;Bring all tasks, broadly defined, into one todo list.&lt;/p&gt;
    &lt;p&gt;Life is full of inboxes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;DMs on Twitter, iMessage, WhatsApp, Signal, Discord, etc.&lt;/item&gt;
      &lt;item&gt;Twitter bookmarks&lt;/item&gt;
      &lt;item&gt;Browser bookmarks&lt;/item&gt;
      &lt;item&gt;Your Downloads folder.&lt;/item&gt;
      &lt;item&gt;Messages in my myGov inbox.&lt;/item&gt;
      &lt;item&gt;The physical mailbox in my apartment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are inboxes because they fill up over time and need action to empty. You can also think of them as little domain-specific task lists. “Centralizing your inboxes” means moving all these tasks from their silos into the one, central todo list.&lt;/p&gt;
    &lt;p&gt;For example, I have a daily task called “catch up” to clear the digital inboxes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go through all my communication apps (email, Discord, Twitter DMs etc) and triage the unread conversations: if something needs replying to, I either reply immediately or make a task to reply later so I don’t forget.&lt;/item&gt;
      &lt;item&gt;File the contents of my Downloads folder.&lt;/item&gt;
      &lt;item&gt;Go through Twitter/browser bookmarks and turn them into tasks (e.g., if I bookmark an article, the task is to read the article).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this way I mostly manage to stay on top of comms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inbox Zero&lt;/head&gt;
    &lt;p&gt;All inboxes should be at zero.&lt;/p&gt;
    &lt;p&gt;You have probably heard of inbox zero. It sounds like LinkedIn-tier advice. But if you struggle with comms, with replying to people in a timely manner (or at all), inbox zero is a good strategy. There are two reasons, briefly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inbox zero has no false negatives: if an inbox is empty, you know you’ve handled everything.&lt;/item&gt;
      &lt;item&gt;Important communications have a way of “camouflaging” themselves among irrelevance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And, like everything: before you make it into a habit, it feels incredibly time-consuming and labour-intensive. But once you make it into a habit, it’s almost effortless.&lt;/p&gt;
    &lt;p&gt;So, I will give you an example. I come in to work, and read four emails. Three could’ve been archived outright, one needed a reply from me. And I said, oh, I’ll get to it in a second. And then I got distracted with other tasks. And throughout the day I kept glancing at the email client, and thinking, yeah, I will get to it. Eventually I got used to those four emails: they are the “new normal”, and what’s normal doesn’t require action. I would think: if those emails are there, and I already looked at them, then it’s probably fine. At the end of the day I looked at the inbox again and saw, wait, no, one of those emails was actually important. That’s the failure mode of inbox greater-than-zero: the important stuff hides among the irrelevant stuff, such that a quick glance at the todo list doesn’t show anything obviously wrong. Dually, with inbox zero, if you see a single email in the inbox, you know there’s work to do.&lt;/p&gt;
    &lt;p&gt;Inbox zero removes ambiguity. If there’s anything in the inbox, you know, unambiguously, you have a task to complete. If there is nothing in the inbox, you know, unambiguously, there is nothing to do. Inbox zero frees you from false negatives, where you think you’ve handled your correspondence but there’s some important email, camouflaged among the trivial ones, that has not been replied to.&lt;/p&gt;
    &lt;p&gt;A problem with doing inbox zero is most communication apps (like Discord, Slack, iMessage etc.) don’t have a concept of an inbox, just the read/unread flag on conversations. Since there’s no separation between the inbox and the archive, it takes more discipline to ensure every conversation is replied to.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inbox Bankruptcy&lt;/head&gt;
    &lt;p&gt;If an inbox is overwhelmed, archive it in a recoverable way.&lt;/p&gt;
    &lt;p&gt;By the time I started to become organized I’d already accumulated thousands of bookmarks, unread emails, files in my downloads folder, papers in my physical inbox, etc. It would have been a Herculean effort to file these things away. So I didn’t. All the disorganized files, I wrapped them up in a folder and threw them in my &lt;code&gt;Attic&lt;/code&gt; folder. Emails? Archived. Bookmarks? Exported to HTML, archived the export, and deleted them from the browser.&lt;/p&gt;
    &lt;p&gt;Ideally you should do this once, at the start.&lt;/p&gt;
    &lt;p&gt;And by archiving things rather than deleting them, you leave open the possibility that as some point in the future, you might be able to action some of those things. Triage the old bookmarks, sort your filesystem, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Do It On Your Own Terms&lt;/head&gt;
    &lt;p&gt;Bring aversion-causing tasks into an environment that you control.&lt;/p&gt;
    &lt;p&gt;If you’re averse to doing something, for emotional reasons, one way to overcome the aversion is to do it as much as possible on your own terms.&lt;/p&gt;
    &lt;p&gt;An example: you have to fill out some government form. You’re averse to it because you worry about making a mistake. And just the thought of opening the form fills you with dread. So, take the boxes in the form, and make a spreadsheet for them. If fonts/colours/emojis/etc. if that makes it feel more personal, or like something you designed and created. Then fill out the form in the spreadsheet. And then copy the values to the form and submit.&lt;/p&gt;
    &lt;p&gt;This helps because instead of performing the task in this external domain where you feel threatened, you’re performing the task in your own domain, in your own terms.&lt;/p&gt;
    &lt;p&gt;Another example: you have an email you have to reply to, and you’re anxious about it. Just opening the email client gives you a bad feeling. Instead, try composing the email elsewhere, say, in a text editor. The change of environment changes the emotional connotation: you’re not replying to an email, you’re writing a text. You might even think of it as a work of fiction, a pseudoepigraphy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replace Interrupts with Polling&lt;/head&gt;
    &lt;p&gt;Turn off notifications, check comms as an explicit task.&lt;/p&gt;
    &lt;p&gt;“Interrupts” means notifications, which arrive at unpredictable and often inconvenient times. “Polling” means manually checking the source of the notifications for things to action.&lt;/p&gt;
    &lt;p&gt;The obvious benefit of replacing interrupts with polling is you don’t get interrupted by a notification. The less obvious benefit is that when notifications are smeared throughout the day, it is easy for them to fall through the cracks. Something comes in when you’re busy, and you swipe it away, and forget about it, and realize days later you forgot to respond to an important message. Polling is focused: you’ve chosen a block of time, you’re committed to going through the notifications systematically. Instead of random islands of interruptions throughout the day, you have a few short, focused blocks of going through your notifications. Often I get an email while I’m on my phone and think, well, I can’t reply, typing on mobile is horrible, I’m on a train, etc. Polling usually happens at my desk so I have no excuses: I’m in the right environment and in the right mental state.&lt;/p&gt;
    &lt;p&gt;This is so trite. “Put your phone on Do Not Disturb and silence notifications”. And yet it works. For a long time I resisted this because I aspire to be the kind of person who gets a message and replies within minutes. But I didn’t notice how much notifications were impairing my focus until one day I accidentally put the phone/desktop on DND and had a wonderfully productive, distraction-free day.&lt;/p&gt;
    &lt;head rend="h2"&gt;Accountability Buddy&lt;/head&gt;
    &lt;p&gt;Get someone to sit next to you while you work.&lt;/p&gt;
    &lt;p&gt;If you’re struggling to work on something, work next to another person. Set a timer and tell them what you’re going to accomplish and when the timer ends tell them how you did. Just being around other people can make it easier to overcome aversion. This is why coworking spaces are useful.&lt;/p&gt;
    &lt;p&gt;If you don’t have a person around, you might try Focusmate. It works for some people.&lt;/p&gt;
    &lt;p&gt;Sometimes I’ll start a conversation with Claude, lay out my plans for the day, and update Claude as I do things. If I’m stuck, or if I need help overcoming procrastination, I can ask Claude for help, and it’s easier to do that in an on-going thread because Claude already has the necessary context, so I don’t have to describe what I’m struggling with ab initio.&lt;/p&gt;
    &lt;head rend="h2"&gt;Plan First, Do Later&lt;/head&gt;
    &lt;p&gt;Separate planning from action, so if you get distracted while acting, you can return to the plans.&lt;/p&gt;
    &lt;p&gt;Separating planning from doing can be useful. Firstly because planning/doing require different kinds of mental energy. When you’re too tired to do, you can often still plan. Secondly because by separating them you can look back and see how useful the plan was, how much you stuck to it, and then get better at planning.&lt;/p&gt;
    &lt;p&gt;Thirdly, and most importantly, because for ADHD people doing can be a source of distractions that impair other tasks. From Driven to Distraction:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The first item on the list referred to a cough drop. As I read it, I asked her about it.&lt;/p&gt;
      &lt;p&gt;“Oh,” she answered, “that is about a cough drop someone left on the dashboard of our car. The other day I saw the cough drop and thought, I’ll have to throw that away. When I arrived at my first stop, I forgot to take the cough drop to a trash can. When I got back into the car, I saw it and thought, I’ll throw it away at the gas station. The gas station came and went and I hadn’t thrown the cough drop away. Well, the whole day went like that, the cough drop still sitting on the dashboard. When I got home, I thought, I’ll take it inside with me and throw it out. In the time it took me to open the car door, I forgot about the cough drop. It was there to greet me when I got in the car the next morning. […]&lt;/p&gt;
      &lt;p&gt;It was such a classic ADD story that I’ve come to call it the “cough drop sign” when a person habitually has trouble following through on plans on a minute-to-minute, even second-to-second, basis. This is not due to procrastination per se as much as it is due to the busyness of the moment interrupting or interfering with one’s memory circuits. You can get up from your chair, go into the kitchen to get a glass of water, and then in the kitchen forget the reason for your being there.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Emphasis mine.&lt;/p&gt;
    &lt;p&gt;When I notice a micro-task like this, my instinct is not to do it, but to put it in the todo list. Then I try to do it immediately. And if I get distracted halfway through, it’s still there, in the todo list.&lt;/p&gt;
    &lt;p&gt;A practical example is something I call the apartment survey. When I clean the apartment, I start by walking around, noticing everything that needs fixing, and creating a little task for it. Even something as simple as “move the book from the coffee table to the bookshelf”. But I don’t start anything until the survey is done. And when the survey is done, I execute it. And if I get distracted halfway through cleaning the apartment, I have the tasks in the list to go back to.&lt;/p&gt;
    &lt;head rend="h2"&gt;Derailment&lt;/head&gt;
    &lt;p&gt;Introspect to find the things that ruin your productivity and avoid them.&lt;/p&gt;
    &lt;p&gt;Through introspection you can discover the behaviours that derail your productivity.&lt;/p&gt;
    &lt;p&gt;Lifting in the morning derails the day. Cardio is fine, but if I lift weights in the morning, the rest of the day I’m running on -40 IQ points. The most cognitively demanding thing I can do is wash the dishes. I’m not sure what the physiology is: maybe it’s exhaustion of the glycogen stores, or fatigue byproducts floating around in my brain, or the CNS is busy rewiring the motor cortex. The point is that I try to do the cognitively-demanding things in the morning and lift in the evening.&lt;/p&gt;
    &lt;p&gt;Motion also does this. I suppose it’s the H in ADHD: hyperactivity. I used to be a big pacer: put on headphones, pace my room back and forth daydreaming for hours and hours. Some days I would pace so much my legs were sore. To think, I have to be in motion. But sometimes I’ve thought enough, and it’s time to do.&lt;/p&gt;
    &lt;p&gt;Music, too, derails me. If I start listening to music very soon I start pacing the room and it’s over. Music is almost like reverse methylphenidate: it makes me restless, mentally hyperactive, and inattentive.&lt;/p&gt;
    &lt;p&gt;So, to be productive I have to not move too much, and be in silence, and not have fried my brain with exercise.&lt;/p&gt;
    &lt;head rend="h2"&gt;Using OCD to Defeat ADHD&lt;/head&gt;
    &lt;p&gt;If being organized makes you feel good, spend more on organizing your productivity system.&lt;/p&gt;
    &lt;p&gt;In a sense, having a really complex productivity system is like trying to use OCD to defeat ADHD, to use high neuroticism to defeat low conscientiousness. There’s an element of truth to that, sure (see mastery of drudgery).&lt;/p&gt;
    &lt;p&gt;But here’s the thing: you have to play to your strengths. You have to. If you are very OCD and you like order and systems and planning but you struggle with doing, then, yeah, it might work, for you, to spend more energy on the trappings of productivity (ensuring your todo list is properly formatted, organized, etc.) if that bleeds over into making it easier to do the real, meaningful things.&lt;/p&gt;
    &lt;p&gt;For example: I like emojis in my todo list. The chores have a 🧼 emoji, the comms tasks have an ✉️ emoji. That kind of thing. Makes it easy to see at a glance what kind of things I have to do, to group them by category. But Todoist doesn’t support emoji icons on tasks, unlike Notion, so adding the emojis takes a bit more effort: I have to open Raycast and search for the emoji I want and paste it into the task title. It adds a little friction each time I create a task, but the benefit is I enjoy using the todo list more.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Master of Drudgery&lt;/head&gt;
    &lt;p&gt;Avoid spending too much productive time on worthless chores.&lt;/p&gt;
    &lt;p&gt;A productivity antipattern: indulging too much in “quick wins”.&lt;/p&gt;
    &lt;p&gt;There’s this running joke, or meme, online, about the kind of person who has this huge, colossal productivity system, but they get nothing done. They have five todo list apps and everything is categorized and indexed and sorted, but their material output is zero. They complete a hundred tasks a day and when you interrogate what those tasks are they are “brush my teeth” or “reorganize my bookshelf”. There’s a lot of truth to that.&lt;/p&gt;
    &lt;p&gt;Every task falls into one of two categories: the quick wins, and everything else. Life is not made of quick wins. Creative, generative, open-ended work requires long periods of focused work. A lot of unpleasant, aversion-causing things have to be done. But the quick wins are infinite: there’s always some micro-chore to do around the house, for example.&lt;/p&gt;
    &lt;p&gt;I don’t have advice specifically on avoiding this. But you should notice if you’re doing it and course-correct.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thrashing&lt;/head&gt;
    &lt;p&gt;Don’t let procrastiation on one task derail everything else.&lt;/p&gt;
    &lt;p&gt;A bad failure mode I have is: I have a task T that I have to do, but I can’t, because of some kind of aversion. But when I try to work on other things, the alarms are going off in my head, telling me to work on T because you’ve been putting this off for so long and life is finite and the years are short and all that. The end result is that because one thing is blocked, everything grinds to a halt. It’s a very annoying state to be in.&lt;/p&gt;
    &lt;p&gt;And I don’t have a perfect solution, but I try to manage it but applying a sense of proportionality, “render unto Caesar” etc. You can’t ignore T forever, dually, you probably won’t solve it in the next ten minutes. But you can timebox T: allocate some block of time every day to try to advance it, or at least to work around it, e.g. to ask a friend for help, for example. And the rest of the day you can dedicate to moving other things forward.&lt;/p&gt;
    &lt;head rend="h2"&gt;Put Travel in the Calendar&lt;/head&gt;
    &lt;p&gt;Calculate travel time ahead of time to avoid being late.&lt;/p&gt;
    &lt;p&gt;I am chronically late. So if I have a calendar event like a party at someone’s home, I will go on Google Maps and measure the travel time (from my home or wherever I’m likely to be) to the destination, and make a time block for that. e.g., if it takes 30m to go to the dentist and back, this is what my calendar looks like:&lt;/p&gt;
    &lt;p&gt;This ensures I leave my home on time. If it’s something especially important I often add 15m to the travel block as a buffer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Choice of Tools&lt;/head&gt;
    &lt;p&gt;Use tools that are effective and you like.&lt;/p&gt;
    &lt;p&gt;What productivity app should I use? Reminders? Linear? Todoist? A bullet journal?&lt;/p&gt;
    &lt;p&gt;Use something that feels good and works. That’s all. Personally I use Todoist. A lot of people think todo list apps are commodities, but when you have an app open for 98% of your screentime, the little subtleties really add up. I’ve tried using Reminders, Linear, as my todo lists, and building my own. My productivity always suffers and I always go back to Todoist.&lt;/p&gt;
    &lt;p&gt;One app is better than two: the more disjoint things you have to pay attention to, the worse it is.&lt;/p&gt;
    &lt;p&gt;If you’re a software engineer I strongly advise against building your own, which is a terrible form of procrastination for creative types.&lt;/p&gt;
    &lt;head rend="h1"&gt;Resources&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How To Do Things describes an ADHD-friendly version of the Pomodoro method. It’s a 50 page PDF with no fluff, so it’s worth buying to support writers who don’t waste the reader’s time.&lt;/item&gt;
      &lt;item&gt;Getting Things Done has a lot of good advice (e.g. dump your entire brain into the todo list) but it’s somewhat neurotypical in that it’s assumed you won’t have any problems actually executing the tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Thanks to Cameron Pinnegar for reviewing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://borretti.me/article/notes-on-managing-adhd"/></entry><entry><id>https://news.ycombinator.com/item?id=45083495</id><title>Replacing a Cache Service with a Database</title><updated>2025-08-31T19:07:13.240057+00:00</updated><content>&lt;doc fingerprint="ff5fd97667b079d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Replacing a cache service with a database&lt;/head&gt;
    &lt;p&gt;I’ve been thinking about this: will we ever replace caches entirely with databases? In this post I will share some ideas and how we are moving towards it. tl;dr we are still not there, yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why do we even use caches?&lt;/head&gt;
    &lt;p&gt;Caches solve one important problem: providing pre-computed data at insanely low latencies, compared to databases. I am talking about typical use cases where we use a cache along with the db (cache aside pattern), where the application always talks with cache and database, tries to keep the cache up to date with the db. There are other patterns where cache itself talks with DBs, but I think this is the more common pattern where application talks to both cache and database.&lt;/p&gt;
    &lt;p&gt;I’d like to keep my systems simple, and try to reduce dependencies, if possible. If databases can provide the same benefits as cache, it can go a long way before we decide to add an external caching service.&lt;/p&gt;
    &lt;p&gt;Instead of using a cache, like Valkey (or Redis), you could just set up a read replica and use it like a cache. Databases already keep some data in-memory (in buffer pool). Caches aren’t expected to be strongly consistent with the DB, and neither are read replicas. As an added benefit, you can use the same SQL queries instead of whatever custom interface the cache provides. Not using a cache would make things operationally so much simpler; and I’d never have to worry about cache invalidation.&lt;/p&gt;
    &lt;p&gt;If you use an embedded database (like SQLite, PGLite) with replication (like Litestream or libSQL), you’d even get zero network latency.&lt;/p&gt;
    &lt;p&gt;However, caches are still very prominent and can’t be replaced with just read replicas. I often think about how we can bridge the gap, but I think the workloads are so different that it’s not going to happen anytime soon. The closest we’ve come, I think, is Noria + MySQL (now ReadySet).&lt;/p&gt;
    &lt;p&gt;So why are caches still preferred? Comparatively, here are a few things caches do better than databases:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Setting up and destroying a cache is cheap; both operationally and cost-wise.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most workloads only cache a subset of the data, and developers have control over what that subset is. It uses fewer resources. With a DB + buffer pool, that level of control doesn’t exist today.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Caches keep pre-computed data. I could do a complex join and then save the results in a cache. How could I achieve the same with a db?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I don’t know of any database that lets me assign priority to specific rows to always keep them in the buffer pool. Caches also provide eviction policies (and TTL), which I can’t do with the DB buffer pool.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Databases are orders of magnitude larger than caches. Using a full read replica that consumes terabytes of storage just to access a few gigabytes of hot data feels wasteful. Some cloud providers won’t even let you use larger SSDs without also upgrading CPU/memory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Cache services can handle hundreds of thousands of concurrent connections, whereas databases generally don’t scale that way. Database connections are expensive.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Cache → Database&lt;/head&gt;
    &lt;p&gt;What needs to happen to close the gap?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Since I’m only interested in a subset of the data, setting up a full read replica feels like overkill. It would be great to have a read replica with just partial data.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I don’t know of any database built to handle hundreds of thousands of read replicas constantly pulling data. Would they even behave sanely if I kept plugging in new replicas as if they were caches? Interestingly, with databases that use disaggregated storage, replicas could pull directly from storage without ever contacting the master.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;IVM (Incremental View Maintenance) is the hot new stuff. They can be used to precompute the results to cache. e.g. Noria saves results of a join query. So we also need some fancy data structures rather than a simple buffer pool. I’d also love to use WASM extensions to aid in pre computation. The trick is making this work without paying the full cost of query planning. And note: pre-computation does not really help if you have multiple data sources.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most mainstream databases don’t let me fetch just the subset I care about. If IVM were easier, and we could combine it with partial read replicas, maybe then a replica could truly replace a cache*.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we look at this from another angle, we could use an IVM engine to populate and update an external cache service; but that might be a topic for another day.&lt;/p&gt;
    &lt;p&gt;Thanks to Gowtham for reading a draft of this.&lt;/p&gt;
    &lt;p&gt;1. This blog is a rehash of a tweet I wrote earlier, which itself was a rehash of a reply I made to Phil Eaton’s tweet. FWIW, my fren thinks the tweet was better than this post.&lt;lb/&gt;2. This of course does not fit all the use cases, majority of them yes&lt;lb/&gt;3. Weirdly, there’s no Wikipedia page for ‘buffer pool’. btw, Andy Pavlo has a killer lecture video on them.&lt;lb/&gt;4. Many new companies doing some insane stuff around IVM: ReadySet, Materialize, and Feldera&lt;lb/&gt;5. If you are new to IVM / Materialized views, then Sophie Alpert has an excellent post on the topic Materialized views are obviously useful.&lt;lb/&gt;*at least for my use cases&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://avi.im/blag/2025/db-cache/"/></entry><entry><id>https://news.ycombinator.com/item?id=45083845</id><title>FDA official demands removal of YouTube videos of himself criticizing vaccines</title><updated>2025-08-31T19:07:13.066889+00:00</updated><content>&lt;doc fingerprint="fd88955d441db641"&gt;
  &lt;main&gt;
    &lt;p&gt;A top official at the Food and Drug Administration (FDA) demanded the removal of YouTube videos of himself that were published by a physician and writer who has been critical of medical misinformation and public health officials in the Trump administration, according to a YouTube notice that was seen by the Guardian.&lt;/p&gt;
    &lt;p&gt;Jonathan Howard, a neurologist and psychiatrist in New York City, received an email from YouTube on Friday night, which stated that Vinay Prasad, who is the FDA’s top vaccine regulator, had demanded the removal of six videos of himself from Howard’s YouTube channel.&lt;/p&gt;
    &lt;p&gt;Howard’s entire channel has now been deleted by YouTube, which cited copyright infringement.&lt;/p&gt;
    &lt;p&gt;The now-defunct channel contained about 350 videos of doctors and commentators, including Prasad, Robert F Kennedy Jr, the secretary of health and human services, and Jay Bhattacharya, the head of the National Institutes of Health, which had been collected by Howard from their social media accounts, interviews and podcasts.&lt;/p&gt;
    &lt;p&gt;Creating the channel, Howard told Guardian in an interview, had been an attempt to “preserve” what these individuals had said during the early years of the pandemic, including comments that Howard said exaggerated the dangers of the Covid vaccine to children and – in some cases – minimized the risk of Covid infection, among other issues.&lt;/p&gt;
    &lt;p&gt;“These videos were nothing more than collections of what other doctors said during the pandemic, including doctors who are extremely influential and who are now the medical establishment,” he said.&lt;/p&gt;
    &lt;p&gt;The Guardian requested a comment from the office of public affairs at the department of health and human services, and attempted to reach Prasad through personal email addresses and by a listed mobile phone number. No one responded to the request for comment.&lt;/p&gt;
    &lt;p&gt;When YouTube notified Howard of the demand request, it included an email address for Prasad, which is identical to the email address that is linked to Prasad’s now inactive podcast, called Plenary Session.&lt;/p&gt;
    &lt;p&gt;Prasad, a former hematologist-oncologist at the University of California San Francisco, is now head of the FDA’s Center for Biologics Evaluation and Research (CBER), which makes him the chief vaccine regulator in the US. He was a vocal critic of Peter Marks, who previously led CBER and was widely respected for his role in Operation Warp Speed, the initiative that developed, manufactured and helped distribute the Covid-19 vaccines. Marks was forced to resign by Kennedy.&lt;/p&gt;
    &lt;p&gt;Prasad has also been critical of the use of Covid boosters in young people and vaccine mandates, and has defended cuts to health agencies and university research.&lt;/p&gt;
    &lt;p&gt;“It’s really important to remember [Prasad’s] past words in order to gauge his current and future credibility, and that was the mission of my YouTube channel, to record what these doctors [Prasad and others] said,” Howard said.&lt;/p&gt;
    &lt;p&gt;Although the videos Howard collected were often only viewed “dozens” of times, Howard included them in his online articles that appeared on the Science Based Medicine blog. Now those video links are dead.&lt;/p&gt;
    &lt;p&gt;He noted that snippets of Prasad’s comments still appeared on anti-vaccine social media accounts, suggesting Prasad was directing his removal demand only at a critic and not anti-vaccine influencers. In the past, Prasad has complained about censorship by social media companies.&lt;/p&gt;
    &lt;p&gt;Howard has been quoted in the New York Times, the Guardian, and other publications and is the author of a forthcoming book Everyone Else Is Lying to You, which he said examines how the medical establishment, which has come into power in Trump’s second term, normalized “quackery” during the Covid pandemic and undermined public health.&lt;/p&gt;
    &lt;p&gt;“I had thought there was a policy that government officials shouldn’t censor opposing perspectives, but I must be mistaken,” said John Moore, a scientist and colleague who is familiar with Howard’s book and videos.&lt;/p&gt;
    &lt;p&gt;Howard told the Guardian he wanted to emphasize that he was not a victim, and that the ordeal of having his YouTube channel deleted was nothing compared with the dire situation facing scientists and researchers whose funding is being cut by public health institutions.&lt;/p&gt;
    &lt;p&gt;Prasad has had a rocky start in his FDA tenure. Jeremy Faust, a doctor at Brigham and Women’s Hospital Department of Emergency Medicine whose Substack newsletter Inside Medicine is widely followed, once described Prasad as having two sides.&lt;/p&gt;
    &lt;p&gt;There was a 2010s Prasad who was a “rigorous and professorial cancer research methodology expert with hundreds of peer-reviewed publications, including well-reasoned analyses that often stood up against some slippery stuff from big pharma”. And there was the “2020s Prasad”, who Faust called “newly famous and admired by the Right … [a] hot-headed firebrand who when asked about how we should move forward from the lessons of Covid-19 pandemic criticized the pro-masking contingent saying, “I don’t believe in forgiveness because, in my opinion, these pieces of shit are still lying.”&lt;/p&gt;
    &lt;p&gt;Prasad briefly resigned this summer after he was the subject of an attack by the rightwing activist Laura Loomer, and then returned to his post at the FDA. He reportedly had a significant role in the FDA’s decision to change rules around the Covid-19 vaccine, limiting its availability this fall to adults over 65 or those with certain medical conditions. Previously, Covid shots were recommended for everyone six months or older.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/us-news/2025/aug/31/fda-official-youtube-videos"/></entry><entry><id>https://news.ycombinator.com/item?id=45083952</id><title>Jujutsu for Everyone</title><updated>2025-08-31T19:07:12.968243+00:00</updated><content>&lt;doc fingerprint="97d01e5b68f7a9ce"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;This is a tutorial for the Jujutsu version control system. It requires no previous experience with Git or any other version control system.&lt;/p&gt;
    &lt;p&gt;At the time of writing, most Jujutsu tutorials are targeted at experienced Git users, teaching them how to transfer their existing Git skills over to Jujutsu. This tutorial is my attempt to fill the void of beginner learning material for Jujutsu. If you are already experienced with Git, I recommend Steve Klabnik's tutorial instead of this one.&lt;/p&gt;
    &lt;p&gt;This tutorial requires you to work in the terminal. Don't worry, there's a chapter covering some terminal basics in case you're not 100% comfortable with that yet. The commands I tell you to run will often only work on Unix-like operating systems like Linux and Mac. If you're on Windows (and can't switch to Linux), consider using WSL.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to read this tutorial&lt;/head&gt;
    &lt;p&gt;The tutorial is split into levels, which are the top-level chapters in the sidebar. The idea is that once you complete a level, you should probably put this tutorial away for a while and practice what you've learned. Once you're comfortable with those skills, come back for the next level.&lt;/p&gt;
    &lt;p&gt;There is one exception to this: If you're here because you need to collaborate with other people, you should complete the levels 1 and 2 right away.&lt;/p&gt;
    &lt;p&gt;Here's an overview of the planned levels:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Level&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;The bare minimum to get started. This is only enough for the simplest use cases where you're working alone. For example, students who track and submit their homework with a Git repository can get by with only this.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;The bare minimum for any sort of collaboration. Students who are working on a group project and professional software developers need to know this. Going further is highly recommended, but you can take a break after this.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;Basic problem solving skills like conflict resolution and restoring files from history. Without this knowledge, it's only a matter of time until you run into trouble. Completing this level is comparable to the skill level of the average software developer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;History rewriting skills. These will allow you to iterate toward a polished version history, which pays dividends long-term. Some projects require you to have these skills in order to meet their quality standards.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;Productivity boosters, advanced workflows, lesser-known CLI functions and a little VCS theory. Completing this level means you have mastered Jujutsu.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;Additional topics that only come up in specific situations: tags, submodules, workspaces etc. Consider skimming the list of topics and come back once you have an actual need for it.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Only a few levels are complete right now, the rest are on the way.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reset your progress&lt;/head&gt;
    &lt;p&gt;Throughout the tutorial, you will build an example repository. Later chapters depend on the state of previous ones. Losing the state of the example repo can therefore block you from making smooth progress. This might happen for several reasons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You use the example repo for practice and experimentation.&lt;/item&gt;
      &lt;item&gt;You switch to a different computer or reinstall the OS.&lt;/item&gt;
      &lt;item&gt;You intentionally delete it to clean up your home directory.&lt;/item&gt;
      &lt;item&gt;The tutorial is updated significantly while you're taking a break.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To solve this problem, there is a script which automates the task of resetting your progress to the start of any chapter. To identify the chapter you want to continue with, the script expects a keyword as an argument. Each chapter includes its precise reset command at the beginning, so you can easily copy-paste it.&lt;/p&gt;
    &lt;p&gt;The script is not complicated, you can verify that it's not doing anything malicious. Basically, it's just the list of commands I tell you to run manually. For convenience, it's included in the expandable text box below. You can also download the script here and then execute it locally once you have inspected it.&lt;/p&gt;
    &lt;head class="admonition-title"&gt;
      &lt;p&gt;Source of reset script&lt;/p&gt;
    &lt;/head&gt;
    &lt;p&gt;Source of reset script&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env bash
set -euxo pipefail

if [ "${1:-x}" = "x" ] ; then
    echo "Please provide the chapter keyword as the first argument."
    exit 1
fi
chapter="$1"

function success() {
    set +x
    echo "✅✅✅ Reset script completed successfully! ✅✅✅"
    exit 0
}

# Ensure existing user configuration does not affect script behavior.
export JJ_CONFIG=/dev/null

rm -rf ~/jj-tutorial

if ! command -v jj &amp;gt; /dev/null ; then
    echo "ERROR: Jujutsu doesn't seem to be installed."
    echo "       Please install it and rerun the script."
    exit 1
fi

if [ "$chapter" = initialize ] ; then success ; fi

mkdir -p ~/jj-tutorial/repo
cd ~/jj-tutorial/repo
jj git init --colocate

jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

if [ "$chapter" = log ] ; then success ; fi

if [ "$chapter" = make_changes ] ; then success ; fi

echo "# jj-tutorial" &amp;gt; README.md
jj log -r 'none()' # trigger snapshot

if [ "$chapter" = commit ] ; then success ; fi

jj commit --message "Add readme with project title

It's common practice for software projects to include a file called
README.md in the root directory of their source code repository. As the
file extension indicates, the content is usually written in markdown,
where the title of the document is written on the first line with a
prefixed \`#\` symbol.
"

if [ "$chapter" = remote ] ; then success ; fi

git init --bare ~/jj-tutorial/remote
jj git remote add origin ~/jj-tutorial/remote
jj bookmark create main --revision @-
jj git push --bookmark main --allow-new

if [ "$chapter" = clone ] ; then success ; fi

cd ~
rm -rf ~/jj-tutorial/repo
jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo
cd ~/jj-tutorial/repo
jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

if [ "$chapter" = github ] ; then success ; fi

if [ "$chapter" = update_bookmark ] ; then success ; fi

printf "\nThis is a toy repository for learning Jujutsu.\n" &amp;gt;&amp;gt; README.md
jj commit -m "Add project description to readme"

jj bookmark move main --to @-

jj git push

if [ "$chapter" = branch ] ; then success ; fi

echo "print('Hello, world!')" &amp;gt; hello.py

jj commit -m "Add Python script for greeting the world

Printing the text \"Hello, world!\" is a classic exercise in introductory
programming courses. It's easy to complete in basically any language and
makes students feel accomplished and curious for more at the same time."

jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo-bob
cd ~/jj-tutorial/repo-bob
jj config set --repo user.name Bob
jj config set --repo user.email bob@local
jj describe --reset-author --no-edit

echo "# jj-tutorial

The file hello.py contains a script that greets the world.
It can be executed with the command 'python hello.py'.
Programming is fun!" &amp;gt; README.md
jj commit -m "Document hello.py in README.md

The file hello.py doesn't exist yet, because Alice is working on that.
Once our changes are combined, this documentation will be accurate."

jj bookmark move main --to @-
jj git push

cd ~/jj-tutorial/repo
jj bookmark move main --to @-
jj git fetch

if [ "$chapter" = show ] ; then success ; fi

if [ "$chapter" = merge ] ; then success ; fi

jj new main@origin @-

jj commit -m "Merge code and documentation for hello-world"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = ignore ] ; then success ; fi

cd ~/jj-tutorial/repo-bob

tar czf submission_alice_bob.tar.gz README.md

echo "
## Submission

Run the following command to create the submission tarball:

~~~sh
tar czf submission_alice_bob.tar.gz [FILE...]
~~~" &amp;gt;&amp;gt; README.md

echo "*.tar.gz" &amp;gt; .gitignore

jj file untrack submission_alice_bob.tar.gz

jj commit -m "Add submission instructions"

if [ "$chapter" = rebase ] ; then success ; fi

jj bookmark move main --to @-
jj git fetch
jj rebase --destination main@origin
jj git push

if [ "$chapter" = more_bookmark ] ; then success ; fi

cd ~/jj-tutorial/repo

echo "for (i = 0; i &amp;lt; 10; i = i + 1):
    print('Hello, world!')" &amp;gt; hello.py

jj commit -m "WIP: Add for loop (need to fix syntax)"

jj git push --change @-

if [ "$chapter" = navigate ] ; then success ; fi

jj git fetch
jj new main

if [ "$chapter" = undo ] ; then success ; fi

echo "print('Hallo, Welt!')" &amp;gt;&amp;gt; hello.py
echo "print('Bonjour, le monde!')" &amp;gt;&amp;gt; hello.py

jj commit -m "code improvements"

jj undo

jj commit -m "Print German and French greetings as well"

jj undo
jj undo
jj undo

jj redo
jj redo
jj redo

if [ "$chapter" = track ] ; then success ; fi

cd ~ # move out of the directory we're about to delete
rm -rf ~/jj-tutorial/repo
jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo
cd ~/jj-tutorial/repo

# roleplay as Alice
jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

echo "print('Hallo, Welt!')" &amp;gt;&amp;gt; hello.py
echo "print('Bonjour, le monde!')" &amp;gt;&amp;gt; hello.py
jj commit -m "Print German and French greetings as well"

jj bookmark move main -t @-
jj git push

jj bookmark track 'glob:push-*@origin'

if [ "$chapter" = conflict ] ; then success ; fi

jj new 'description("WIP: Add for loop")'

echo "for _ in range(10):
    print('Hello, world!')" &amp;gt; hello.py

jj commit -m "Fix loop syntax"

jj new main @-

echo "for _ in range(10):
    print('Hello, world!')
    print('Hallo, Welt!')
    print('Bonjour, le monde!')" &amp;gt; hello.py

jj commit -m "Merge repetition and translation of greeting"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = abandon ] ; then success ; fi

jj commit -m "Experiment: Migrate to shiny new framework"
jj git push --change @-
jj new main
jj commit -m "Experiment: Improve scalability using microservices"
jj git push --change @-
jj new main
jj commit -m "Experiment: Apply SOLID design patterns"
jj git push --change @-
jj new main

jj abandon 'description("Experiment")'

jj git push --deleted

if [ "$chapter" = restore ] ; then success ; fi

rm README.md
jj show &amp;amp;&amp;gt; /dev/null

jj restore README.md

jj restore --from 'description("Fix loop syntax")' hello.py

jj commit -m "Remove translations"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = complete ] ; then success ; fi

set +x
echo "Error: Didn't recognize the chapter keyword: '$chapter'."
exit 1
&lt;/code&gt;
    &lt;head rend="h2"&gt;Stay up to date&lt;/head&gt;
    &lt;p&gt;Both this tutorial and Jujutsu are still evolving. In order to keep your Jujutsu knowledge updated, subscribe to releases of the tutorial's GitHub repo. You will be notified of important changes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A new level becomes available.&lt;/item&gt;
      &lt;item&gt;An existing level is changed significantly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I especially intend to keep this tutorial updated as new version of Jujutsu come out with features and changes that are relevant to the tutorial's content. I consider this tutorial up-to-date with the latest version of Jujutsu (&lt;code&gt;0.32&lt;/code&gt;) as of August 2025.
If that's more than a couple months in the past, I probably stopped updating this tutorial.&lt;/p&gt;
    &lt;p&gt;You can subscribe to these updates by visiting the GitHub repo and clicking on "Watch", "Custom" and then selecting "Releases".&lt;/p&gt;
    &lt;head rend="h2"&gt;Help make this tutorial better&lt;/head&gt;
    &lt;p&gt;If you find a typo, you can suggest a fix directly by clicking on the "edit" icon in the top-right corner. If you have general suggestions for improvement, please open an issue. I am also very interested in experience reports, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Do you have any frustrations with Jujutsu which the tutorial did not help you overcome?&lt;/item&gt;
      &lt;item&gt;Was there a section that wasn't explained clearly? (If you didn't understand something, it's probably the tutorial's fault, not yours!)&lt;/item&gt;
      &lt;item&gt;Did you complete a level but didn't feel like you had the skills that were promised in the level overview?&lt;/item&gt;
      &lt;item&gt;Is there something missing that's not being taught but should?&lt;/item&gt;
      &lt;item&gt;Do you feel like the content could be structured better?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you for helping me improve this tutorial!&lt;/p&gt;
    &lt;head rend="h2"&gt;What is version control and why should you use it?&lt;/head&gt;
    &lt;p&gt;I will assume you're using version control for software development, but it can be used for other things as well. For example, authoring professionally formatted documents with tools like Typst. The source of this tutorial is stored in version control too!&lt;/p&gt;
    &lt;p&gt;What these scenarios have in common is that a large body of work (mostly in the form of text) is slowly being expanded and improved over time. You don't want to lose any of it and you want to be able to go back to previous states of your work. Often, several people need to work on the project at the same time.&lt;/p&gt;
    &lt;p&gt;A general-purpose backup solution can keep a few copies of your files around. A graphical document editor can allow multiple people to edit the text simultaneously. But sometimes, you need a sharper knife. Jujutsu is the sharpest knife available.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Jujutsu instead of Git?&lt;/head&gt;
    &lt;p&gt;Git is by far the most commonly used VCS in the software development industry. So why not use that? Using the most popular thing has undeniable benefits. There is lots of learning material, lots of people can help you with problems, lots of other tools integrate with it etc. Why make life harder on yourself by using a lesser-known alternative?&lt;/p&gt;
    &lt;p&gt;Here's my elevator pitch:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Jujutsu is compatible with Git. You're not actually losing anything by using Jujutsu. You can work with it on any existing project that uses Git for version control without issues. Tools that integrate with Git mostly work just as well with Jujutsu.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jujutsu is easier to learn than Git. (That is, assuming I did a decent job writing this tutorial.) Git is known for its complicated, unintuitive user interface. Jujutsu gives you all the functionality of Git with a lot less complexity. Experienced users of Git usually don't care about this, because they've paid the price of learning Git already. (I was one of these people once.) But you care!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jujutsu is more powerful than Git. Despite the fact that it's easier to learn and more intuitive, it actually has loads of awesome capabilities for power users that completely leave Git in the dust. Don't worry, you don't have to use that power right away. But you can be confident that if your VCS-workflow becomes more demanding in the future, Jujutsu will have your back. This is not a watered-down "we have Git at home" for slow learners!&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Learning Jujutsu instead of Git as your first VCS does have some downsides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;When talking about version control with peers, they will likely use Git-centric vocabulary. Jujutsu shares a lot of Git's concepts, but there are also differences. Translating between the two in conversation can add some mental overhead. (solution: convince your peers to use Jujutsu 😉)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jujutsu is relatively new and doesn't cover 100% of the features of Git yet. When you do run into the rare problem where Jujutsu doesn't have an answer, you can always fall back to use Git directly, which works quite seamlessly. Still, having to use two tools instead of one is slightly annoying. I plan to teach such Git features in this tutorial in later levels. The tutorial should be a one-stop-shop for all Jujutsu users.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The command line interface of Jujutsu is not yet stable. That means in future versions of Jujutsu, some commands might work a little differently or be renamed. I personally don't think this should scare you away. Many people including me have used Jujutsu as a daily driver for a long time. Whenever something did change, my reaction was usually: "Great, that was one of the less-than-perfect parts of Jujutsu! Now it's even more intuitive than before!" Consider subscribing to GitHub releases of this tutorial. You will be notified if new versions of Jujutsu change something in a way that's relevant to what you learned in this tutorial.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Despite some downsides, I think the benefits are well worth it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jj-for-everyone.github.io/"/></entry><entry><id>https://news.ycombinator.com/item?id=45084016</id><title>No clicks, no content: The unsustainable future of AI search</title><updated>2025-08-31T19:07:12.798854+00:00</updated><content>&lt;doc fingerprint="39229ae8958323c1"&gt;
  &lt;main&gt;
    &lt;p&gt;AI companies are causing a content drought that will eventually starve them.&lt;/p&gt;
    &lt;p&gt;In a recent article, The Economist didn’t mince words: “AI is killing the web.” Published last month, the piece raises urgent questions about how artificial intelligence is reshaping the internet as we know it: ChatGPT, Google, and its competitors are rapidly diverting traffic from publishers. Publishers are fighting to survive through lawsuits, partnerships, paywalls, and micropayments. It’s pretty bleak, but unfortunately I think the situation is far worse than it seems.&lt;/p&gt;
    &lt;p&gt;The article focuses mainly on the publishing industry, news and magazine sites that rely primarily on visits to their sites and selling ads. This is hardly new for the publishing industry. Televisions arrived in living rooms in the 60s disrupting print and radio media, in the late 90s and early 2000s the internet further devastated the print business, and social media was stealing attention well before the advent of AI. But it’s not just the publishing industry. There’s a much larger economy being disrupted by generative AI platforms.&lt;/p&gt;
    &lt;p&gt;For the past 25 years, online businesses have relied on people searching Google for information and clicking through to their sites to get the information. For example, a business that sells dirt bikes might create a comprehensive guide to winterize a cottage. People search for information on winterizing their cottage, click through to the dirt bike company’s guide, and are then exposed to the company’s brand, maybe join their email list, and maybe buy their products or services.&lt;/p&gt;
    &lt;p&gt;Now that ChatGPT and Google are serving the information up to people, there’s little reason to click through to the site. If you’ve used Google search lately, you’ll have noticed an AI blurb responding to your query before you even see a list of links. The result: less clicks on the links.&lt;/p&gt;
    &lt;p&gt;So the question follows, if fewer and fewer people are visiting your company site, what’s your incentive to produce and maintain high quality content?&lt;/p&gt;
    &lt;p&gt;Worse yet, ChatGPT and Google rely on the content produced by businesses to train their AI models. If businesses stop producing content, what happens to the answers provided by ChatGPT and Google?&lt;/p&gt;
    &lt;p&gt;Could AI companies be this short sighted?&lt;/p&gt;
    &lt;p&gt;In short: Yes. This is a gold rush mentality. And like any gold rush, there’s little attention paid to the long term. It’s get rich quick and we’ll deal with the consequences later. It’s a race to become the dominant force in AI with no attention paid to the sustainability of their fuel source: the content.&lt;/p&gt;
    &lt;p&gt;However, Google doesn’t fit this profile. They’ve needed businesses and publishers to produce content all along and they know they still do.&lt;/p&gt;
    &lt;p&gt;We, the public, have greatly benefited from the symbiotic relationship between businesses and Google. You ask Google for something and it responds with links to the best content. Businesses want those visitors to their sites and so they want to have the best content. Although Google’s results pages have gotten worse for the public and businesses in recent years (half a page of ads at this point), the situation has largely been a win-win-win for them, businesses, and the public.&lt;/p&gt;
    &lt;p&gt;Businesses produced and maintained quality content, Google rewarded the businesses with visitors while diverting some to their ads, and the public got the information they were searching for. Unfortunately this symbiotic relationship is breaking down. In their effort to stay relevant and compete with ChatGPT, Google is tearing up the contract they’ve had with publishers and businesses for the past 25 years.&lt;/p&gt;
    &lt;p&gt;Google knows this but they seem to be pretending that they don’t. In fact, it seems that they’re scared and they don’t know what else to do. They have no other option.&lt;/p&gt;
    &lt;p&gt;One solution here seems to be regulation. To many, it feels like an injustice that AI companies can scrape information from sites, combine it, and serve it up to their users. The bottom line is that if the content didn’t exist to train their models, the AI companies wouldn’t be able to produce an answer.&lt;/p&gt;
    &lt;p&gt;Unfortunately, lawsuits so far have been going in favor of AI companies. Copyright law doesn’t seem to be a fit here, so perhaps we need new laws. I doubt they’ll come quickly enough though. Google search is rolling out AI Mode right now: no more AI blurb with links underneath. Just a ChatGPT-like interface when you do a Google search. It seems we’re already well into this trap and there doesn’t seem to be an escape.&lt;/p&gt;
    &lt;p&gt;Then again, there’s definitely an economic bubble here. ChatGPT is not profitable despite billions in revenue. The infrastructure is very expensive to run. Perhaps the bubble will burst, the money will dry up, and it won’t be feasible to employ generative AI for general search. Google and its competitors will use it for other things of course, but not for search. It’s hard to see this happening though. The genie is out of the bottle.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bradt.ca/blog/no-clicks-no-content/"/></entry><entry><id>https://news.ycombinator.com/item?id=45084193</id><title>Plastic Before Plastic: How gutta-percha shaped the 19th century</title><updated>2025-08-31T19:07:12.742564+00:00</updated><content/><link href="https://worldhistory.substack.com/p/plastic-before-plastic"/></entry><entry><id>https://news.ycombinator.com/item?id=45084328</id><title>eBPF 101: Your First Step into Kernel Programming</title><updated>2025-08-31T19:07:12.448691+00:00</updated><content>&lt;doc fingerprint="a6b30138f40a37c8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;eBPF 101: Your First Step into Kernel Programming&lt;/head&gt;
    &lt;p&gt;eBPF has revolutionized Linux observability and security by allowing sandboxed programs to run in the kernel without changing kernel source code or loading modules&lt;/p&gt;
    &lt;head rend="h2"&gt;I. What is this eBPF? It looks scary!&lt;/head&gt;
    &lt;p&gt;Have you wanted to write programs that act as drivers for Linux? Wanted programs to run at a kernel level? Wanted to monitor events, internal resources and get better observability? All you need to know is how to make good use of Linux eBPF.&lt;/p&gt;
    &lt;p&gt;eBPF is a technology in the Linux kernel that can run sandboxed programs in a privileged context (in the OS kernel). It is used to efficiently extend the capabilities of the kernel without changing kernel source code.&lt;/p&gt;
    &lt;p&gt;An operating system kernel is hard to modify due to its central role and high requirement towards stability and security. Innovation at the operating system level is lower compared to functionality implemented outside of the operating system. And developing drivers is difficult in general (I have tried that in Windows and failed).&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; link : https://ebpf.io/what-is-ebpf/&lt;/p&gt;
    &lt;p&gt;eBPF changes this formula fundamentally. It allows sandboxed programs to run within the operating system, which means that application developers can run eBPF programs to add additional capabilities to the operating system at runtime. The operating system then guarantees efficiency as if natively compiled with the aid of a Just-In-Time (JIT) compiler and verification engine.&lt;/p&gt;
    &lt;p&gt;This has led to a wave of eBPF-based projects covering a wide array of use cases, improving networking, observability, and security spaces.&lt;/p&gt;
    &lt;p&gt;Let's dive right into some practical scenario where we will build a simple firewall to block traffic from a particular ip like 8.8.8.8. And counts the incoming packets transfered each second. Follow through is you have an Ubuntu machine ready.&lt;/p&gt;
    &lt;head rend="h2"&gt;II. Developing with eBPF made Simple.&lt;/head&gt;
    &lt;p&gt;We need 2 files for a simple ePBF program.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A Python user space script for interacting with eBPF&lt;/item&gt;
      &lt;item&gt;A C code that uses eBPF functions and modules (core logic)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let's download the requirements and setup a python virtual environment for smooth workflow.&lt;/p&gt;
    &lt;p&gt;Initial setup for ubuntu:&lt;/p&gt;
    &lt;code&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y bpfcc-tools libbpfcc-dev
&lt;/code&gt;
    &lt;p&gt;Create a python virtual environment.&lt;/p&gt;
    &lt;code&gt;➜   python3 -m venv venv
➜   source venv/bin/activate
&lt;/code&gt;
    &lt;p&gt;Here's what the 2 files that we are going to create:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;eBPF program that runs in the Linux kernel&lt;/item&gt;
          &lt;item&gt;Counts all incoming packets on a network interface&lt;/item&gt;
          &lt;item&gt;Drops packets destined for IP 8.8.8.8 (Google DNS)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Python control program that: &lt;list rend="ul"&gt;&lt;item&gt;Loads and compiles the eBPF program&lt;/item&gt;&lt;item&gt;Attaches it to a network interface&lt;/item&gt;&lt;item&gt;Monitors and prints packet counts per second&lt;/item&gt;&lt;item&gt;Handles graceful shutdown on SIGTERM/Ctrl+C&lt;/item&gt;&lt;item&gt;Prints debug messages when packets are dropped&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Python control program that: &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To find the network interface try this command.&lt;/p&gt;
    &lt;code&gt;$ ip link show | grep -Po '(?&amp;lt;=: ).*(?=: &amp;lt;)'
lo
wlp0s20f3
docker0

&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;runner.py&lt;/code&gt; script is the user-space controller for our eBPF firewall. It's responsible for loading the eBPF program into the kernel, monitoring its activity, and cleaning up when it's done.&lt;/p&gt;
    &lt;p&gt;First, we import the necessary Python libraries. &lt;code&gt;bcc&lt;/code&gt; is the core library that lets us interact with eBPF, while the others help with handling signals, time, file paths, and network data structures.&lt;/p&gt;
    &lt;code&gt;from bcc import BPF
from time import sleep
from pathlib import Path
import signal
import ctypes
import socket
import struct
&lt;/code&gt;
    &lt;p&gt;To ensure the firewall can be shut down cleanly, we set up a custom signal handler. The &lt;code&gt;TerminateSignal&lt;/code&gt; exception and &lt;code&gt;handle_sigterm&lt;/code&gt; function work together to catch termination signals (like &lt;code&gt;SIGTERM&lt;/code&gt;), allowing the script to proceed to the cleanup steps instead of stopping abruptly.&lt;/p&gt;
    &lt;code&gt;class TerminateSignal(Exception):
    pass

# Signal handler for SIGTERM
def handle_sigterm(signum, frame):
    raise TerminateSignal("Received SIGTERM, terminating...")
&lt;/code&gt;
    &lt;head rend="h3"&gt;Loading and Managing the eBPF Program&lt;/head&gt;
    &lt;p&gt;The eBPF logic itself is written in C in &lt;code&gt;probe.c&lt;/code&gt;. The &lt;code&gt;load_bpf_program&lt;/code&gt; function reads this C code, and the BCC library compiles it into eBPF bytecode and loads it into the kernel. Once loaded, &lt;code&gt;attach_xdp_program&lt;/code&gt; hooks the compiled code to a network interface using XDP (eXpress Data Path), allowing it to process packets at the earliest possible point in the network stack.&lt;/p&gt;
    &lt;code&gt;# Load and compile the eBPF program from the source file
def load_bpf_program():
    bpf_source = Path('probe.c').read_text()
    bpf = BPF(text=bpf_source)
    return bpf

# Attach the eBPF program to the specified interface
def attach_xdp_program(bpf, interface):
    xdp_fn = bpf.load_func("xdp_packet_counter", BPF.XDP)
    bpf.attach_xdp(interface, xdp_fn, 0)
    return bpf
&lt;/code&gt;
    &lt;p&gt;When the script terminates, &lt;code&gt;detach_xdp_program&lt;/code&gt; safely removes the eBPF program from the interface, ensuring the system returns to its normal state.&lt;/p&gt;
    &lt;code&gt;# Detach the eBPF program from the specified interface
def detach_xdp_program(bpf, interface):
    bpf.remove_xdp(interface, 0)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Monitoring and Event Handling&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;main&lt;/code&gt; function orchestrates the entire process. It starts by registering the signal handler and defining the network interface to monitor (&lt;code&gt;wlp0s20f3&lt;/code&gt;).&lt;/p&gt;
    &lt;code&gt;# Main function to execute the script
def main():
    # Register the signal handler for SIGTERM
    signal.signal(signal.SIGTERM, handle_sigterm)

    # Define the network interface to monitor
    INTERFACE = "wlp0s20f3"
&lt;/code&gt;
    &lt;p&gt;Next, it loads and attaches the eBPF program. It then gains access to the &lt;code&gt;packet_count_map&lt;/code&gt; (a shared data structure for counting packets) and opens a &lt;code&gt;perf_buffer&lt;/code&gt; to receive real-time debug events from the kernel, such as notifications about dropped packets.&lt;/p&gt;
    &lt;code&gt;    # Load the eBPF program and attach it to the network interface
    bpf = load_bpf_program()
    attach_xdp_program(bpf, INTERFACE)

    # Access the BPF map and open the perf buffer for debug events
    packet_count_map = bpf.get_table("packet_count_map")
    bpf["debug_events"].open_perf_buffer(print_debug_event)
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;print_debug_event&lt;/code&gt; function is a callback that processes these events. When the eBPF program drops a packet, this function formats the data and prints a message to the console.&lt;/p&gt;
    &lt;code&gt;def print_debug_event(cpu, data, size):
    dest_ip = ctypes.cast(data, ctypes.POINTER(ctypes.c_uint32)).contents.value
    print(f"Packet to {socket.inet_ntoa(struct.pack('!L', dest_ip))} dropped")
&lt;/code&gt;
    &lt;p&gt;The script then enters an infinite loop to monitor packet counts. Every second, it reads the total count from the &lt;code&gt;packet_count_map&lt;/code&gt;, calculates the packets-per-second rate, and prints it. It also polls for any new debug events.&lt;/p&gt;
    &lt;code&gt;    try:
        print("Counting packets, press Ctrl+C to stop...")
        prev_total_packets = 0
        while True:
            sleep(1)
            total_packets = sum(counter.value for counter in packet_count_map.values())
            
            packets_per_second = total_packets - prev_total_packets
            prev_total_packets = total_packets
            print(f"Packets per second: {packets_per_second}")
            bpf.perf_buffer_poll(1)
&lt;/code&gt;
    &lt;head rend="h3"&gt;Graceful Shutdown&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;try...except...finally&lt;/code&gt; block ensures that the program can be stopped cleanly with &lt;code&gt;Ctrl+C&lt;/code&gt; or a &lt;code&gt;SIGTERM&lt;/code&gt; signal. The &lt;code&gt;finally&lt;/code&gt; block guarantees that the eBPF program is always detached from the network interface, preventing resource leaks.&lt;/p&gt;
    &lt;code&gt;    except (KeyboardInterrupt, TerminateSignal) as e:
            print(f"\n{e}. Interrupting eBPF runner.")
    finally:
        print("Detaching eBPF program and exiting.")
        detach_xdp_program(bpf, INTERFACE)
&lt;/code&gt;
    &lt;p&gt;Finally, the &lt;code&gt;if __name__ == "__main__":&lt;/code&gt; guard ensures the &lt;code&gt;main&lt;/code&gt; function runs only when the script is executed directly.&lt;/p&gt;
    &lt;code&gt;# Execute the main function when the script is run directly
if __name__ == "__main__":
    main()
&lt;/code&gt;
    &lt;p&gt;Next, the &lt;code&gt;probe.c&lt;/code&gt; file contains the eBPF program that runs inside the Linux kernel. It uses XDP (eXpress Data Path) to inspect and filter network packets at the earliest possible point—right in the network driver—making it extremely fast.&lt;/p&gt;
    &lt;head rend="h3"&gt;Kernel-Space Setup&lt;/head&gt;
    &lt;p&gt;First, we include kernel headers that provide access to eBPF helpers and network data structures. We then define two key BPF maps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;BPF_ARRAY&lt;/code&gt;: A single-element array named&lt;code&gt;packet_count_map&lt;/code&gt;to store a global packet counter.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;BPF_PERF_OUTPUT&lt;/code&gt;: A perf buffer named&lt;code&gt;debug_events&lt;/code&gt;to send notifications about dropped packets to the user-space script.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;#include &amp;lt;uapi/linux/bpf.h&amp;gt;
#include &amp;lt;uapi/linux/if_ether.h&amp;gt;
#include &amp;lt;uapi/linux/if_packet.h&amp;gt;
#include &amp;lt;uapi/linux/ip.h&amp;gt;
#include &amp;lt;linux/in.h&amp;gt;
#include &amp;lt;bcc/helpers.h&amp;gt;

BPF_ARRAY(packet_count_map, __u64, 1);
BPF_PERF_OUTPUT(debug_events);
&lt;/code&gt;
    &lt;head rend="h3"&gt;The Main XDP Program&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;xdp_packet_counter&lt;/code&gt; function is the entry point for our eBPF program. It runs for every single packet that arrives on the attached network interface.&lt;/p&gt;
    &lt;p&gt;Its first job is to increment the global packet counter. It looks up the counter from &lt;code&gt;packet_count_map&lt;/code&gt; and atomically increments it. Using an atomic operation is crucial to prevent race conditions when multiple CPU cores process packets simultaneously.&lt;/p&gt;
    &lt;code&gt;int xdp_packet_counter(struct xdp_md *ctx) {
    __u32 key = 0;
    __u64 *counter;

    counter = packet_count_map.lookup(&amp;amp;key);
    if (!counter)
        return XDP_ABORTED; // Abort if map lookup fails

    // Atomically increment the counter
    __sync_fetch_and_add(counter, 1);

    // Define the blocked IP and call the filtering function
    __be32 blocked_ip = (8 &amp;lt;&amp;lt; 24) | (8 &amp;lt;&amp;lt; 16) | (8 &amp;lt;&amp;lt; 8) | 8;
    return drop_packet_to_destination(ctx, blocked_ip);
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Packet Filtering Logic&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;drop_packet_to_destination&lt;/code&gt; function contains the firewall's core logic. It carefully inspects the packet to decide whether to drop it or let it pass.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Parse Headers: It starts by getting pointers to the packet's data and performs bounds checks to ensure the Ethernet and IP headers are safely accessible within the packet's memory region. This prevents the eBPF verifier from rejecting the program.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Check Protocol: It checks if the packet is an IP packet. If not, it's immediately passed through with&lt;/p&gt;&lt;code&gt;XDP_PASS&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;static int drop_packet_to_destination(struct xdp_md *ctx, __be32 blocked_ip) {
    void *data_end = (void *)(long)ctx-&amp;gt;data_end;
    void *data = (void *)(long)ctx-&amp;gt;data;
    struct ethhdr *eth = data;

    // Safety check: ensure Ethernet header is within packet bounds
    if ((void *)(eth + 1) &amp;gt; data_end)
        return XDP_PASS;

    // Pass non-IP packets
    if (eth-&amp;gt;h_proto != bpf_htons(ETH_P_IP))
        return XDP_PASS;

    struct iphdr *iph = (struct iphdr *)(data + ETH_HLEN);
    // Safety check: ensure IP header is within packet bounds
    if ((void *)(iph + 1) &amp;gt; data_end)
        return XDP_PASS;

    // If the destination IP matches the blocked IP, drop the packet
    if (iph-&amp;gt;daddr == blocked_ip) {
        __be32 daddr_copy = iph-&amp;gt;daddr;
        debug_events.perf_submit(ctx, &amp;amp;daddr_copy, sizeof(daddr_copy));
        return XDP_DROP;
    }

    return XDP_PASS;
}
&lt;/code&gt;
    &lt;p&gt;The code for this tutorial is taken from this beautiful talk. I recommend you to check it out.&lt;/p&gt;
    &lt;p&gt;Together they form a simple eBPF firewall that counts packets and blocks traffic to a specific IP address. The Python script manages the eBPF program lifecycle while the C code does the actual packet processing in kernel space.&lt;/p&gt;
    &lt;code&gt;$ sudo python3 runner.py 
&lt;/code&gt;
    &lt;p&gt;Results after running the program.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Many tech giants Netflix, Dropbox, Yahoo, LinkedIn, Alibaba, Datadog, Shopify, DoorDash use eBPF for network observability, infrastructure debugging, pod networking/security in Kubernetes, intrusion detection. Its widely used in security monitoring and Incident Response.&lt;/p&gt;
    &lt;p&gt;It will be a big miss if you did not adopt or at least know something about it. I hope this article bridges the gap. For more articles follow the newsletter.&lt;/p&gt;
    &lt;p&gt;LiveReview helps you get great feedback on your PR/MR in a few minutes. Saves hours on every PR by giving fast, automated first-pass reviews. If you're tired of waiting for your peer to review your code or are not confident that they'll provide valid feedback, here's LiveReview for you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://journal.hexmos.com/ebpf-introduction/"/></entry><entry><id>https://news.ycombinator.com/item?id=45084673</id><title>I Don't Have Spotify</title><updated>2025-08-31T19:07:11.854063+00:00</updated><content>&lt;doc fingerprint="416ed44dff1e4ff3"&gt;
  &lt;main&gt;
    &lt;p&gt;I Don't Have Spotify Paste a link from Spotify, YouTube Music, Apple Music, Deezer or SoundCloud to start. Search Search&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://idonthavespotify.sjdonado.com/"/></entry><entry><id>https://news.ycombinator.com/item?id=45084757</id><title>Infisical (YC W23) Is Hiring Solutions Engineers to Scale the OSS Security Stack</title><updated>2025-08-31T19:07:10.864974+00:00</updated><content>&lt;doc fingerprint="3230f69a95b713ae"&gt;
  &lt;main&gt;
    &lt;p&gt;Open-source secrets manager for developers&lt;/p&gt;
    &lt;p&gt;Infisical is the #1 open source secret management platform for developers. In other words, we help organizations manage API-keys, DB access tokens, certificates, and other credentials across all parts of their infra! In fact, we process over 100M of such secrets per day.&lt;/p&gt;
    &lt;p&gt;Our customers range from some of the largest public enterprises to fastest-growing startups (e.g., companies like Hugging Face, Delivery Hero). Developers love us and our community is growing every day! Join us on a mission to make security easier for all developers – starting from secret management.&lt;/p&gt;
    &lt;p&gt;Infisical is looking for a customer success engineer to help grow Infisical’s customer base and ensure great product/onboarding experience. You will be working closely with our CEO and the rest of the engineering team on:&lt;/p&gt;
    &lt;p&gt;Overall, you’re going to be one of the defining pieces of our team as we scale to thousands of customers over the next 18 months.&lt;/p&gt;
    &lt;p&gt;This job will require you to have the following pivotal skills:&lt;/p&gt;
    &lt;p&gt;With this role, you play the defining role in:&lt;/p&gt;
    &lt;p&gt;Our team has worked across transformative tech companies, from Figma to AWS to Red Hat.&lt;/p&gt;
    &lt;p&gt;We have an office in San Francisco, but we are mostly a remote team. We try to get together as often as possible – whether it's for an off-site, conferences, or just get-togethers. This is a full-time role open to anyone in North American time zones.&lt;/p&gt;
    &lt;p&gt;At Infisical, we will treat you well with a competitive salary and equity offer. Depending on your risk tolerance, we would love to talk more with you about the range of options available between the two. For some other benefits (including lunch stipend, work setup budget, etc), please check out our careers page: https://infisical.com/careers.&lt;/p&gt;
    &lt;p&gt;Infisical is the #1 open source secret management platform – used by tens of thousands of developers.&lt;/p&gt;
    &lt;p&gt;We raised $3M from Y Combinator, Gradient Ventures (Google's VC fund), and awesome angel investors like Elad Gil, Arash Ferdowsi (founder/ex-CTO of Dropbox), Paul Copplestone (founder/CEO of Supabase), James Hawkins (founder/CEO of PostHog), Andrew Miklas (founder/ex-CTO of PagerDuty), Diana Hu (GP at Y Combinator), and more.&lt;/p&gt;
    &lt;p&gt;We are default alive, and have signed many customers ranging from fastest growing startups to post-IPO enterprises.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/infisical/jobs/yaEvock-solutions-engineer"/></entry><entry><id>https://news.ycombinator.com/item?id=45084759</id><title>Launch HN: VibeFlow (YC S25) – Web app generator with visual, editable workflows</title><updated>2025-08-31T19:07:10.385894+00:00</updated><content>&lt;doc fingerprint="334ac65b00c5ecf8"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN! We’re Alessia and Elia, the founders of VibeFlow (&lt;/p&gt;https://vibeflow.ai&lt;p&gt;). VibeFlow lets semi-technical people (i.e. people with some technical skill but who are not professional programmers) build full-stack web apps from natural language prompts, while making the underlying business logic clear and editable as a visual workflow. Demo video: &lt;/p&gt;https://www.youtube.com/watch?v=-CwWd3-b1JI&lt;p&gt;.&lt;/p&gt;&lt;p&gt;The problem we’re trying to solve: today, people who want to build apps without coding often have to stitch together multiple tools, e.g. using Lovable for the frontend, n8n for workflows, and Supabase for the database. That creates data silos and leaves builders with fragile apps that break in production, don’t scale, and aren’t safe. We saw YouTube tutorials teaching people how to duct-tape these together just to get a functional app running. As engineers building no-code tools, we realized that people wanted the power of AI-generated UIs but also the ability to see and control their backend workflows and data.&lt;/p&gt;&lt;p&gt;Our solution is to generate the whole app at once, and represent it as a visual workflow. Users describe what they want in English (“I need a chat widget with an AI agent”) and VibeFlow generates both the interface and the logic. That logic shows up as a workflow graph they can edit visually or by giving new instructions.&lt;/p&gt;&lt;p&gt;We use Convex (https://www.convex.dev/) as backend. The generation of the backend code is fully deterministic, we map workflow graphs to code templates. This makes deployments predictable and avoids the hallucinated, black-box code you often get from AI.&lt;/p&gt;&lt;p&gt;Workflow representation: the logic is a directed graph where each node can be customized. We currently support CRUD operations and agent components. Any changes to the graph compiles directly back into code, so you always own the underlying logic.&lt;/p&gt;&lt;p&gt;Frontend: generated via AI and directly linked to workflow outputs, so it always stays in sync with the business logic. Changes to the frontend can be made through a chat interface.&lt;/p&gt;&lt;p&gt;Semi-technical builders can create maintainable apps (not opaque “magic”), and technical folks can still inspect the code and architecture. Compared to Bubble/Webflow, the interface is simpler; compared to Zapier, the workflows have an output code; and compared to AI coding assistants, you get an automatic backend plugged in with no black-box.&lt;/p&gt;&lt;p&gt;You can try it here: https://app.vibeflow.ai/. The demo video is https://youtu.be/-CwWd3-b1JI We'd love to hear from the HN community, whether you're a builder who's struggled with stitching tools together, a developer who's seen the pain points in no-code platforms, or someone curious about where AI-powered app generation is heading - we're eager for your thoughts!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45084759"/></entry><entry><id>https://news.ycombinator.com/item?id=45084905</id><title>Show HN: Anonymous Age Verification</title><updated>2025-08-31T19:07:10.159690+00:00</updated><content>&lt;doc fingerprint="9d00d29a183d9720"&gt;
  &lt;main&gt;
    &lt;p&gt;A zero‑storage, privacy‑preserving age check that leverages banks’ existing KYC — with the user as the transport layer.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Banks sign an age claim, not an identity. They never learn which site you’re visiting.&lt;/item&gt;
      &lt;item&gt;Merchants verify a short‑lived token against their own nonce and a one‑time WebAuthn key. No database required.&lt;/item&gt;
      &lt;item&gt;The user copy/pastes the values between merchant and bank. No redirects, no OAuth, no trackers, no server‑to‑server calls. YOU see and control everything.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;This is a framework / reference design to make anonymous age checks practical using institutions that already have KYC. It’s not “the one true standard” — it’s a clean baseline to critique, pilot, and iterate.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Current age‑verification options are either leaky (share PII), heavy (ID upload &amp;amp; storage), tracky (central IdPs), or pricey (per‑verification fees). Banks already know your age via KYC — we reuse that fact without revealing who you are or where you’re going.&lt;/p&gt;
    &lt;code&gt;[Merchant]
  │  (1) shows nonce Nm + challenge
  ▼
[User/Browser]  —(2) creates fresh WebAuthn key Kt—►
  │                                           
  ├──(3) copy two short strings────────────────► [Bank]
  │     - SHA256(Nm)
  │     - jkt(Kt_public) = SHA256(SPKI(Kt_public))
  │
  ◄────────────────────(4) bank returns signed age token (short‑lived)
  │
  └──(5) paste token + Kt_public back to merchant

Merchant verifies: token signature, matches hashes, and checks WebAuthn assertion with Kt_public.
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Who&lt;/cell&gt;
        &lt;cell role="head"&gt;Sees&lt;/cell&gt;
        &lt;cell role="head"&gt;Never sees&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bank&lt;/cell&gt;
        &lt;cell&gt;Your identity (already KYC’d), token issue time, the two hashes&lt;/cell&gt;
        &lt;cell&gt;Merchant domain, URL cookies, user identity at merchant&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Merchant&lt;/cell&gt;
        &lt;cell&gt;Their own nonce, age threshold claim (e.g., &lt;code&gt;over_18&lt;/code&gt;), Kt_public, token times&lt;/cell&gt;
        &lt;cell&gt;User identity, bank account details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;User&lt;/cell&gt;
        &lt;cell&gt;Everything they copy/paste&lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Merchant → User: Render a page that displays (a) a signed nonce&lt;/p&gt;&lt;code&gt;Nm&lt;/code&gt;and (b) a WebAuthn challenge.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Browser: Create a fresh, ephemeral WebAuthn credential (&lt;/p&gt;&lt;code&gt;Kt&lt;/code&gt;); extract&lt;code&gt;Kt_public&lt;/code&gt;. UV must be required.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;User → Bank: Copy two short strings from the merchant page into the bank’s age‑check page:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;code&gt;merchant_nonce_hash = SHA256(Nm)&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;code&gt;user_key_jkt = SHA256(SPKI(Kt_public))&lt;/code&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bank → User: After strong auth (login + 2FA), bank returns a signed age token (short TTL, e.g., 5 min).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;User → Merchant: Paste the token +&lt;/p&gt;&lt;code&gt;Kt_public&lt;/code&gt;back to the merchant.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Merchant (stateless): Verify HMAC of&lt;/p&gt;&lt;code&gt;Nm&lt;/code&gt;, verify bank signature &amp;amp; fields, match both hashes, verify a WebAuthn assertion with&lt;code&gt;Kt_public&lt;/code&gt;(UV=1). If valid, accept age check.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;No redirects. No iframes. No server‑to‑server calls. The user is the transport layer by design.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;// Create a minimal, verifiable nonce without a DB.
// Nm = base64url(payload) + "." + HMAC_SHA256(secret, payload)
const payload = {
  v: 1,                      // version
  ts: Date.now(),            // ms since epoch
  rnd: crypto.getRandomValues(new Uint8Array(16)) // 128‑bit entropy
};
const body = base64url(JSON.stringify(payload));
const mac = base64url(hmacSHA256(MERCHANT_SECRET, body));
const Nm = `${body}.${mac}`; // display Nm to the user&lt;/code&gt;
    &lt;code&gt;{
  "ctx": "bank.age.v1",
  "iss": "bank.example",     
  "iat": 1735500000,
  "exp": 1735500300,          
  "age_over": { "18": true, "21": false },
  "merchant_nonce_hash": "HkJI…",
  "user_key_jkt": "XyZ1…",
  "jti": "abc123…"
}
// signed with ES256 over a canonical form; banks publish JWKs at:
// https://bank.example/.well-known/age-verification-key.json&lt;/code&gt;
    &lt;code&gt;async function verifyAge({Nm, token, Kt_public, webauthnAssertion}: Input): Promise&amp;lt;boolean&amp;gt; {
  // 1) Verify Nm HMAC &amp;amp; freshness
  const [body, mac] = Nm.split(".");
  if (!timingSafeEq(mac, hmacSHA256(MERCHANT_SECRET, body))) return false;
  const {v, ts} = JSON.parse(atoburl(body));
  if (v !== 1 || (Date.now() - ts) &amp;gt; 5*60*1000) return false; // 5 min

  // 2) Verify bank signature &amp;amp; token times
  const bankKey = await fetchJwkFor(token.iss, token.kid);
  if (!verifyES256(bankKey, token)) return false;
  if (now() &amp;lt; token.iat || now() &amp;gt; token.exp) return false;

  // 3) Match hashes
  if (sha256(Nm) !== token.merchant_nonce_hash) return false;
  if (sha256(spki(Kt_public)) !== token.user_key_jkt) return false;

  // 4) Verify WebAuthn assertion with UV required
  if (!verifyWebAuthn({publicKey: Kt_public, assertion: webauthnAssertion, requireUV: true})) return false;

  // 5) Policy: check acceptable issuers &amp;amp; age thresholds
  if (!TRUSTED_BANKS.has(token.iss)) return false;
  return token.age_over["18"] === true; // or 21+
}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Anonymity by default: No merchant identifiers reach the bank; no bank identifiers reach the merchant.&lt;/item&gt;
      &lt;item&gt;One‑time keys: &lt;code&gt;Kt&lt;/code&gt;is generated per check → prevents cross‑site correlation.&lt;/item&gt;
      &lt;item&gt;Stateless merchant: HMAC’d nonce + WebAuthn means no DB needed.&lt;/item&gt;
      &lt;item&gt;Short‑lived tokens: Minutes‑scale TTL + UV reduces resale value.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If a merchant wants stronger replay deterrence at some privacy cost, a bank may include opt‑in echoes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;ip_prefix&lt;/code&gt;(e.g., /24 for IPv4 or /56 for IPv6)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ua_hash&lt;/code&gt;(SHA256 of normalized UA string) Merchants can choose to enforce these; users/banks can decline.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On first success, offer the user a passkey account immediately. From then on, they just sign in with the passkey; your app’s account state records that age verification was done (no age or PII stored).&lt;/p&gt;
    &lt;p&gt;Benefits&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One‑click returns, no re‑verification friction&lt;/item&gt;
      &lt;item&gt;Strong auth resistant to sharing&lt;/item&gt;
      &lt;item&gt;Compliance evidence at account creation time&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Endpoint: &lt;code&gt;https://&amp;lt;bank-domain&amp;gt;/.well-known/age-verification-key.json&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Format: JWK Set (include current and grace‑period keys)&lt;/item&gt;
      &lt;item&gt;Caching: Reasonable TTL (e.g., 1h) + fast rotation path&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example JWK Set:&lt;/p&gt;
    &lt;code&gt;{ "keys": [{
  "kid": "2024-key-1",
  "kty": "EC",
  "crv": "P-256",
  "x": "…",
  "y": "…",
  "use": "sig",
  "alg": "ES256"
}]}&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Threat&lt;/cell&gt;
        &lt;cell role="head"&gt;Mitigation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Token resale/replay&lt;/cell&gt;
        &lt;cell&gt;Short TTL; &lt;code&gt;user_key_jkt&lt;/code&gt; binds token to Kt; WebAuthn UV assertion required; optional &lt;code&gt;ip_prefix&lt;/code&gt;/&lt;code&gt;ua_hash&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Bank learns merchant&lt;/cell&gt;
        &lt;cell&gt;Only opaque hashes are sent; no origin, no referrer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Merchant learns user identity&lt;/cell&gt;
        &lt;cell&gt;Token contains only age threshold + hashes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Token theft in transit&lt;/cell&gt;
        &lt;cell&gt;User is the transport; token is useless without Kt + live UV assertion.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Key rotation breakage&lt;/cell&gt;
        &lt;cell&gt;Fetch new JWKs on signature fail; keep old keys during grace.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Fake bank UI&lt;/cell&gt;
        &lt;cell&gt;Users should navigate directly to their bank’s domain (or use a bank app). No redirects to spoof.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Shared devices/households&lt;/cell&gt;
        &lt;cell&gt;UV enforces per‑person auth at issuance; merchants may add risk rules.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Is: A lean, comprehensible pattern to decouple age claims from identity, reusing bank KYC without central trackers.&lt;/p&gt;
    &lt;p&gt;Isn’t: A mandate, a full spec, or legal advice. Jurisdictional rules vary; merchants must map “age threshold” fields to local requirements.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bank UX: a tiny page that accepts two strings, authenticates the user, and returns a signed token.&lt;/item&gt;
      &lt;item&gt;Merchant UX: a panel that shows &lt;code&gt;Nm&lt;/code&gt;, a “Create passkey” button, and a paste box for the token.&lt;/item&gt;
      &lt;item&gt;Browser: use the native WebAuthn API; do not request attestation (privacy).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Incentives for banks (flat vs micro‑fees, CSR angle, or consortium model)?&lt;/item&gt;
      &lt;item&gt;Standardizing a community trusted‑banks registry (or allow‑lists per sector)?&lt;/item&gt;
      &lt;item&gt;How to treat teen accounts, guardianship, and regional thresholds (18/21/other)?&lt;/item&gt;
      &lt;item&gt;Should browsers offer a built‑in ‘Age Token’ flow to remove copy/paste friction while keeping the same privacy model?&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hash: SHA‑256&lt;/item&gt;
      &lt;item&gt;Sign: ES256 (P‑256)&lt;/item&gt;
      &lt;item&gt;Nonce: ≥128 bits entropy; HMAC‑SHA256 with secret; include version + timestamp&lt;/item&gt;
      &lt;item&gt;Token TTL: ≤5 minutes&lt;/item&gt;
      &lt;item&gt;WebAuthn: UV required; fresh credential per check; no attestation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PRs welcome! Especially:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tiny bank issuer reference server&lt;/item&gt;
      &lt;item&gt;Merchant verifier lib (TS)&lt;/item&gt;
      &lt;item&gt;Browser helper (nice copy/paste + WebAuthn UX)&lt;/item&gt;
      &lt;item&gt;Threat modeling &amp;amp; red‑team notes&lt;/item&gt;
      &lt;item&gt;Test vectors &amp;amp; conformance cases&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gist.github.com/JWally/bf4681f79c0725eb378ec3c246cf0664"/></entry><entry><id>https://news.ycombinator.com/item?id=45085002</id><title>A 'Third Way' Between Buying or Renting? Swiss Co-Ops Say They've Found It</title><updated>2025-08-31T19:07:10.009716+00:00</updated><content/><link href="https://www.nytimes.com/2025/08/26/realestate/switzerland-rental-coops-nonprofit-lausanne.html"/></entry><entry><id>https://news.ycombinator.com/item?id=45085029</id><title>Use One Big Server (2022)</title><updated>2025-08-31T19:07:09.709527+00:00</updated><content>&lt;doc fingerprint="dfdab7597d822fd7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Use One Big Server&lt;/head&gt;
    &lt;p&gt;A lot of ink is spent on the "monoliths vs. microservices" debate, but the real issue behind this debate is about whether distributed system architecture is worth the developer time and cost overheads. By thinking about the real operational considerations of our systems, we can get some insight into whether we actually need distributed systems for most things.&lt;/p&gt;
    &lt;p&gt;We have all gotten so familiar with virtualization and abstractions between our software and the servers that run it. These days, "serverless" computing is all the rage, and even "bare metal" is a class of virtual machine. However, every piece of software runs on a server. Since we now live in a world of virtualization, most of these servers are a lot bigger and a lot cheaper than we actually think.&lt;/p&gt;
    &lt;head rend="h2"&gt;Meet Your Server&lt;/head&gt;
    &lt;p&gt;This is a picture of a server used by Microsoft Azure with AMD CPUs. Starting from the left, the big metal fixture on the left (with the copper tubes) is a heatsink, and the metal boxes that the copper tubes are attached to are heat exchangers on each CPU. The CPUs are AMD's third generation server CPU, each of which has the following specifications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;64 cores&lt;/item&gt;
      &lt;item&gt;128 threads&lt;/item&gt;
      &lt;item&gt;~2-2.5 GHz clock&lt;/item&gt;
      &lt;item&gt;Cores capable of 4-6 instructions per clock cycle&lt;/item&gt;
      &lt;item&gt;256 MB of L3 cache&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In total, this server has 128 cores with 256 simultaneous threads. With all of the cores working together, this server is capable of 4 TFLOPs of peak double precision computing performance. This server would sit at the top of the top500 supercomputer list in early 2000. It would take until 2007 for this server to leave the top500 list. Each CPU core is substantially more powerful than a single core from 10 years ago, and boasts a much wider computation pipeline.&lt;/p&gt;
    &lt;p&gt;Above and below each CPU is the memory: 16 slots of DDR4-3200 RAM per socket. The largest capacity "cost effective" DIMMs today are 64 GB. Populated cost-efficiently, this server can hold 1 TB of memory. Populated with specialized high-capacity DIMMs (which are generally slower than the smaller DIMMs), this server supports up to 8 TB of memory total. At DDR4-3200, with a total of 16 memory channels, this server will likely see ~200 Gbps of memory throughput across all of its cores.&lt;/p&gt;
    &lt;p&gt;In terms of I/O, each CPU offers 64 PCIe gen 4 lanes. With 128 PCIe lanes total, this server is capable of supporting 30 NVMe SSDs plus a network card. Typical configurations you can buy will offer slots for around 16 SSDs or disks. The final thing I wanted to point out in this picture is in the top right, the network card. This server is likely equipped with a 50-100 Gbps network connection.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Capabilities of One Server&lt;/head&gt;
    &lt;p&gt;One server today is capable of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Serving video files at 400 Gbps (now 800 Gbps)&lt;/item&gt;
      &lt;item&gt;1 million IOPS on a NoSQL database&lt;/item&gt;
      &lt;item&gt;70k IOPS in PostgreSQL&lt;/item&gt;
      &lt;item&gt;500k requests per second to nginx&lt;/item&gt;
      &lt;item&gt;Compiling the linux kernel in 20 seconds&lt;/item&gt;
      &lt;item&gt;Rendering 4k video with x264 at 75 FPS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Among other things. There are a lot of public benchmarks these days, and if you know how your service behaves, you can probably find a similar benchmark.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Cost of One Server&lt;/head&gt;
    &lt;p&gt;In a large hosting provider, OVHCloud, you can rent an HGR-HCI-6 server with similar specifications to the above, with 128 physical cores (256 threads), 512 GB of memory, and 50 Gbps of bandwidth for $1,318/month.&lt;/p&gt;
    &lt;p&gt;Moving to the popular budget option, Hetzner, you can rent a smaller server with 32 physical cores and 128 GB of RAM for about â¬140.00/month. This is a smaller server than the one from OVHCloud (1/4 the size), but it gives you some idea of the price spread between hosting providers.&lt;/p&gt;
    &lt;p&gt;In AWS, one of the largest servers you can rent is the m6a.metal server. It offers 50 Gbps of network bandwidth, 192 vCPUs (96 physical cores), and 768 GB of memory, and costs $8.2944/hour in the US East region. This comes out to $6,055/month. The cloud premium is real!&lt;/p&gt;
    &lt;p&gt;A similar server, with 128 physical cores and 512 GB of memory (as well as appropriate NICs, SSDs, and support contracts), can be purchased from the Dell website for about $40,000. However, if you are going to spend this much on a server, you should probably chat with a salesperson to make sure you are getting the best deal you can. You will also need to pay to host this server and connect it to a network, though.&lt;/p&gt;
    &lt;p&gt;In comparison, buying servers takes about 8 months to break even compared to using cloud servers, and 30 months to break even compared to renting. Of course, buying servers has a lot of drawbacks, and so does renting, so going forward, we will think a little bit about the "cloud premium" and whether you should be willing to pay it (spoiler alert: the answer is "yes, but not as much as the cloud companies want you to pay").&lt;/p&gt;
    &lt;head rend="h2"&gt;Thinking about the Cloud&lt;/head&gt;
    &lt;p&gt;The "cloud era" began in earnest around 2010. At the time, the state of the art CPU was an 8-core Intel Nehalem CPU. Hyperthreading had just begun, so that 8-core CPU offered a whopping 16 threads. Hardware acceleration was about to arrive for AES encryption, and vectors were 128 bits wide. The largest CPUs had 24 MB of cache, and your server could fit a whopping 256 GB of DDR3-1066 memory. If you wanted to store data, Seagate had just begun to offer a 3 TB hard drive. Each core offered 4 FLOPs per cycle, meaning that your 8-core server running at 2.5 GHz offered a blazing fast 80 GFLOPs.&lt;/p&gt;
    &lt;p&gt;The boom in distributed computing rode on this wave: if you wanted to do anything that involved retrieval of data, you needed a lot of disks to get the storage throughput you want. If you wanted to do large computations, you generally needed a lot of CPUs. This meant that you needed to coordinate between a lot of CPUs to get most things done.&lt;/p&gt;
    &lt;p&gt;Since that time began, the size of servers has increased a lot, and SSDs have increased available IOPS by a factor of at least 100, but the size of mainstream VMs and containers hasn't increased much, and we still use virtualized drives that perform more like hard drives than SSDs (although this gap is closing).&lt;/p&gt;
    &lt;head rend="h4"&gt;One Server (Plus a Backup) is Usually Plenty&lt;/head&gt;
    &lt;p&gt;If you are doing anything short of video streaming, and you have under 10k QPS, one server will generally be fine for most web services. For really simple services, one server could even make it to a million QPS or so. Very few web services get this much traffic - if you have one, you know about it. Even if you're serving video, running only one server for your control plane is very reasonable. A benchmark can help you determine where you are. Alternatively, you can use common benchmarks of similar applications, or tables of common performance numbers to estimate how big of a machine you might need.&lt;/p&gt;
    &lt;head rend="h4"&gt;Tall is Better than Wide&lt;/head&gt;
    &lt;p&gt;When you need a cluster of computers, if one server is not enough, using fewer larger servers will often be better than using a large fleet of small machines. There is non-zero overhead to coordinate a cluster, and that overhead is frequently O(n) on each server. To reduce this overhead, you should generally prefer to use a few large servers than to use many small servers. In the case of things like serverless computing, where you allocate tiny short-lived containers, this overhead accounts for a large fraction of the cost of use. On the other extreme end, coordinating a cluster of one computer is trivial.&lt;/p&gt;
    &lt;head rend="h4"&gt;Big Servers and Availability&lt;/head&gt;
    &lt;p&gt;The big drawback of using a single big server is availability. Your server is going to need downtime, and it is going to break. Running a primary and a backup server is usually enough, keeping them in different datacenters. A 2x2 configuration should appease the truly paranoid: two servers in a primary datacenter (or cloud provider) and two servers in a backup datacenter will give you a lot of redundancy. If you want a third backup deployment, you can often make that smaller than your primary and secondary.&lt;/p&gt;
    &lt;p&gt;However, you may still have to be concerned about correlated hardware failures. Hard drives (and now SSDs) have been known to occasionally have correlated failures: if you see one disk fail, you are a lot more likely to see a second failure before getting back up if your disks are from the same manufacturing batch. Services like Backblaze overcome this by using many different models of disks from multiple manufacturers. Hacker news learned this the hard way recently when the primary and backup server went down at the same time.&lt;/p&gt;
    &lt;p&gt;If you are using a hosting provider which rents pre-built servers, it is prudent to rent two different types of servers in each of your primary and backup datacenters. This should avoid almost every failure mode present in modern systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Use the Cloud, but don't be too Cloudy&lt;/head&gt;
    &lt;p&gt;A combination of availability and ease of use is one of the big reasons why I (and most other engineers) like cloud computers. Yes, you pay a significant premium to rent the machines, but your cloud provider has so much experience building servers that you don't even see most failures, and for the other failures, you can get back up and running really quickly by renting a new machine in their nearly-limitless pool of compute. It is their job to make sure that you don't experience downtime, and while they don't always do it perfectly, they are pretty good at it.&lt;/p&gt;
    &lt;p&gt;Hosting providers who are willing to rent you a server are a cheaper alternative to cloud providers, but these providers can sometimes have poor quality and some of them don't understand things like network provisioning and correlated hardware failures. Also, moving from one rented server to a larger one is a lot more annoying than resizing a cloud VM. Cloud servers have a price premium for a good reason.&lt;/p&gt;
    &lt;p&gt;However, when you deal with clouds, your salespeople will generally push you towards "cloud-native" architecture. These are things like microservices in auto-scaling VM groups with legions of load balancers between them, and vendor-lock-in-enhancing products like serverless computing and managed high-availability databases. There is a good reason that cloud salespeople are the ones pushing "cloud architecture" - it's better for them!&lt;/p&gt;
    &lt;p&gt;The conventional wisdom is that using cloud architecture is good because it lets you scale up effortlessly. There are good reasons to use cloud-native architecture, but serving lots of people is not one of them: most services can serve millions of people at a time with one server, and will never give you a surprise five-figure bill.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why Should I Pay for Peak Load?&lt;/head&gt;
    &lt;p&gt;One common criticism of the "one big server" approach is that you now have to pay for your peak usage instead of paying as you go for what you use. Thus, serverless computing or fleets of microservice VMs more closely align your costs with your profit.&lt;/p&gt;
    &lt;p&gt;Unfortunately, since all of your services run on servers (whether you like it or not), someone in that supply chain is charging you based on their peak load. Part of the "cloud premium" for load balancers, serverless computing, and small VMs is based on how much extra capacity your cloud provider needs to build in order to handle their peak load. You're paying for someone's peak load anyway!&lt;/p&gt;
    &lt;p&gt;This means that if your workload is exceptionally bursty - like a simulation that needs to run once and then turn off forever - you should prefer to reach for "cloudy" solutions, but if your workload is not so bursty, you will often have a cheaper system (and an easier time building it) if you go for few large servers. If your cloud provider's usage is more bursty than yours, you are going to pay that premium for no benefit.&lt;/p&gt;
    &lt;p&gt;This premium applies to VMs, too, not just cloud services. However, if you are running a cloud VM 24/7, you can avoid paying the "peak load premium" by using 1-year contracts or negotiating with a salesperson if you are big enough.&lt;/p&gt;
    &lt;p&gt;Generally, the burstier your workload is, the more cloudy your architecture should be.&lt;/p&gt;
    &lt;head rend="h4"&gt;How Much Does it Cost to be Cloudy?&lt;/head&gt;
    &lt;p&gt;Being cloudy is expensive. Generally, I would anticipate a 5-30x price premium depending on what you buy from a cloud company, and depending on the baseline. Not 5-30%, a factor of between 5 and 30.&lt;/p&gt;
    &lt;p&gt;Here is the pricing of AWS lambda: $0.20 per 1M requests + $0.0000166667 per GB-second of RAM. I am using pricing for an x86 CPU here to keep parity with the m6a.metal instance we saw above. Large ARM servers and serverless ARM compute are both cheaper.&lt;/p&gt;
    &lt;p&gt;Assuming your server costs $8.2944/hour, and is capable of 1k QPS with 768 GB of RAM:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;1k QPS is 60k queries per minute, or 3.6M queries per hour&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Each query here gets 0.768 GB-seconds of RAM (amortized)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Replacing this server would cost about $46/hour using serverless computing&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The price premium for serverless computing over the instance is a factor of 5.5. If you can keep that server over 20% utilization, using the server will be cheaper than using serverless computing. This is before any form of savings plan you can apply to that server - if you can rent those big servers from the spot market or if you compare to the price you can get with a 1-year contract, the price premium is even higher.&lt;/p&gt;
    &lt;p&gt;If you compare to the OVHCloud rental price for the same server, the price premium of buying your compute through AWS lambda is a factor of 25&lt;/p&gt;
    &lt;p&gt;If you are considering renting a server from a low-cost hosting provider or using AWS lambda, you should prefer the hosting provider if you can keep the server operating at 5% capacity!&lt;/p&gt;
    &lt;p&gt;Also, note that the actual QPS number doesn't matter: if the $8.2944/hour server is capable of 100k QPS, the query would use 100x less memory-time, meaning that you would arrive at the same 5.5x (or 25x) premium. Of course, you should scale the size of the server to fit your application.&lt;/p&gt;
    &lt;head rend="h2"&gt;Common Objections to One Big Server&lt;/head&gt;
    &lt;p&gt;If you propose using the one big server approach, you will often get pushback from people who are more comfortable with the cloud, prefer to be fashionable, or have legitimate concerns. Use your judgment when you think about it, but most people vastly underestimate how much "cloud architecture" actually costs compared to the underlying compute. Here are some common objections.&lt;/p&gt;
    &lt;head rend="h4"&gt;But if I use Cloud Architecture, I Don't Have to Hire Sysadmins&lt;/head&gt;
    &lt;p&gt;Yes you do. They are just now called "Cloud Ops" and are under a different manager. Also, their ability to read the arcane documentation that comes from cloud companies and keep up with the corresponding torrents of updates and deprecations makes them 5x more expensive than system administrators.&lt;/p&gt;
    &lt;head rend="h4"&gt;But if I use Cloud Architecture, I Don't Have to Do Security Updates&lt;/head&gt;
    &lt;p&gt;Yes you do. You may have to do fewer of them, but the ones you don't have to do are the easy ones to automate. You are still going to share in the pain of auditing libraries you use, and making sure that all of your configurations are secure.&lt;/p&gt;
    &lt;head rend="h4"&gt;But if I use Cloud Architecture, I Don't Have to Worry About it Going Down&lt;/head&gt;
    &lt;p&gt;The "high availability" architectures you get from using cloudy constructs and microservices just about make up for the fragility they add due to complexity. At this point, if you use two different cloud regions or two cloud providers, you can generally assume that is good enough to avoid your service going down. However, cloud providers have often had global outages in the past, and there is no reason to assume that cloud datacenters will be down any less often than your individual servers.&lt;/p&gt;
    &lt;p&gt;Remember that we are trying to prevent correlated failures. Cloud datacenters have a lot of parts that can fail in correlated ways. Hosting providers have many fewer of these parts. Similarly, complex cloud services, like managed databases, have more failure modes than simple ones (VMs).&lt;/p&gt;
    &lt;head rend="h4"&gt;But I can Develop More Quickly if I use Cloud Architecture&lt;/head&gt;
    &lt;p&gt;Then do it, and just keep an eye on the bill and think about when it's worth it to switch. This is probably the strongest argument in favor of using cloudy constructs. However, if you don't think about it as you grow, you will likely end up burning a lot of money on your cloudy architecture long past the time to switch to something more boring.&lt;/p&gt;
    &lt;head rend="h4"&gt;My Workload is Really Bursty&lt;/head&gt;
    &lt;p&gt;Cloud away. That is a great reason to use things like serverless computing. One of the big benefits of cloud architecture constructs is that the scale down really well. If your workload goes through long periods of idleness punctuated with large unpredictable bursts of activity, cloud architecture probably works really well for you.&lt;/p&gt;
    &lt;head rend="h4"&gt;What about CDNs?&lt;/head&gt;
    &lt;p&gt;It's impossible to get the benefits of a CDN, both in latency improvements and bandwidth savings, with one big server. This is also true of other systems that need to be distributed, like backups. Thankfully CDNs and backups are competitive markets, and relatively cheap. These are the kind of thing to buy rather than build.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Note On Microservices and Monoliths&lt;/head&gt;
    &lt;p&gt;Thinking about "one big server" naturally lines up with thinking about monolithic architectures. However, you don't need to use a monolith to use one server. You can run many containers on one big server, with one microservice per container. However, microservice architectures in general add a lot of overhead to a system for dubious gain when you are running on one big server.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;When you experience growing pains, and get close to the limits of your current servers, today's conventional wisdom is to go for sharding and horizontal scaling, or to use a cloud architecture that gives you horizontal scaling "for free." It is often easier and more efficient to scale vertically instead. Using one big server is comparatively cheap, keeps your overheads at a minimum, and actually has a pretty good availability story if you are careful to prevent correlated hardware failures. It's not glamorous and it won't help your resume, but one big server will serve you well.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://specbranch.com/posts/one-big-server/"/></entry><entry><id>https://news.ycombinator.com/item?id=45085156</id><title>How is Ultrassembler so fast?</title><updated>2025-08-31T19:07:09.437811+00:00</updated><content>&lt;doc fingerprint="aa5a930a6c0c0688"&gt;
  &lt;main&gt;
    &lt;p&gt;Ultrassembler is a superfast and complete RISC-V assembler library that I'm writing as a component of the bigger Chata signal processing project.&lt;/p&gt;
    &lt;p&gt;Assemblers take in a platform-dependent assembly language and output that platform's native machine code which runs directly on the processor.&lt;/p&gt;
    &lt;p&gt;"Why would you want to do this?" you might ask. First, existing RISC-V assemblers that conform the the entirety of the specification, &lt;code&gt;as&lt;/code&gt; and &lt;code&gt;llvm-mc&lt;/code&gt;, ship as binaries that you run as standalone programs. This is normally not an issue. However, in Chata's case, it needs to access a RISC-V assembler from within its C++ code. The alternative is to use some ugly C function like &lt;code&gt;system()&lt;/code&gt; to run external software as if it were a human or script running a command in a terminal. &lt;/p&gt;
    &lt;p&gt;Here's an example of what I'm talking about:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;string&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

int main() {
    std::string command = "riscv64-linux-gnu-as code.s -o code.bin";

    int res = std::system(command.data());

    if (res != 0) {
        std::cerr &amp;lt;&amp;lt; "Error executing command: " &amp;lt;&amp;lt; command &amp;lt;&amp;lt; std::endl;
    }
    return res;
}
&lt;/code&gt;
    &lt;p&gt;It gets even worse once you realize you need temporary files and possibly have to artisanally craft the command beforehand. Additionally, invoking the assembler in this manner incurs a significant performance overhead on embedded systems which lack significant processing power. There must be a better way.&lt;/p&gt;
    &lt;p&gt;Enter Ultrassembler.&lt;/p&gt;
    &lt;p&gt;With these two goals of speed and standard conformance in mind, I wrote Ultrassembler as a completely standalone library with GNU &lt;code&gt;as&lt;/code&gt; as the speed and standard conformity benchmark. &lt;/p&gt;
    &lt;p&gt;The results are nothing short of staggering.&lt;/p&gt;
    &lt;p&gt;After months of peformance optimization, Ultrassembler can assemble a test file with about 16 thousand RISC-V instructions over 10 times faster than &lt;code&gt;as&lt;/code&gt;, and around 20 times faster than &lt;code&gt;llvm-mc&lt;/code&gt;. To put it another way, it only takes about 1000 CPU instructions (+-50% depending on platform) to assemble one RISC-V instruction, while it takes 10,000 for &lt;code&gt;as&lt;/code&gt; and 20,000 for &lt;code&gt;llvm-mc&lt;/code&gt;. This happens with plain old C++ code only and no platform-specific assembly code, although integrating assembly could crank up the speed even further.&lt;/p&gt;
    &lt;p&gt;Such performance ensures a good user experience on the platforms where Chata runs, but also as a consequence of this lack of overhead, you could also combine Ultrassembler with fantastic libraries like libriscv to implement low-to-no-cost RISC-V scripting in things like games, or maybe even in your JIT programming language!&lt;/p&gt;
    &lt;p&gt;Let's look at some of the ways I made Ultrassembler this fast so that you can reap the benefits too.&lt;/p&gt;
    &lt;p&gt;WARNING! The code you're about to see here is only current as of this article's publishing. The actual code Ultrassembler uses could be different by the time you read this in the future!&lt;/p&gt;
    &lt;head rend="h1"&gt;Exceptions&lt;/head&gt;
    &lt;p&gt;Exceptions, C++'s first way of handling errors, are slow. Super duper slow. Mega slow. So slow, in fact, that many Programming Furus©️®️™️ say you should never ever use them. They'll infect your code with their slowness and transform you into a slow old hunchback in no time.&lt;/p&gt;
    &lt;p&gt;Or so you would think.&lt;/p&gt;
    &lt;p&gt;C++ exceptions, despite being so derided, are in fact zero-overhead. Huh? Didn't I just say they were super duper slow? Let me explain.&lt;/p&gt;
    &lt;p&gt;It's not clear when exactly exceptions are slow. I had to do some research here. As it turns out, GCC's &lt;code&gt;libstdc++&lt;/code&gt; uses a so-called "zero-overhead" exception system, meaning that in the ideal normal case where the C++ code calls zero exceptions, there is zero performance penalty. But when it does call an exception, it could become very slow depending on how the code is laid out. Most programmers, not knowing this, frequently use exceptions in their normal cases, and as a result, their programs are slow. Such mysterious behavior caught the attention of Programming Furus©️®️™️ and has made exceptions appear somewhat of a curse.&lt;/p&gt;
    &lt;p&gt;This tragic curse turns out to be a heavenly blessing for Ultrassembler. In the normal case, there are zero errors to report as a result of proper usage of RISC-V instructions. But if there's some error somewhere, say somebody put in the wrong register, then Ultrassembler sounds the alarm. Since such mistakes only occur as a result of human error (ex bugs in codegen and Ultrassembler itself) the timeframe in which to report the error can expand to that of a human. As a result, even if an exception triggered by a mistake took a full 1 second (about a million times slower than it does in reality), it doesn't matter because the person percepting the error message can only do so in approximately that second timeframe.&lt;/p&gt;
    &lt;p&gt;"But hold on!" you exclaim. "What about std::expected?" In response to some programs which frequently need to handle errors not seen by humans, C++ added a system to reduce the overhead of calling errors, &lt;code&gt;std::expected&lt;/code&gt;. I tried this in Ultrassembler and the results weren't pretty. It trades off exception speed for normal case speed. Since the normal case is the norm in Ultrassembler, &lt;code&gt;std::expected&lt;/code&gt; incurred at least a 10% performance loss due to the way the &lt;code&gt;std::expected&lt;/code&gt; object wraps two values (the payload and the error code) together. See this C++ standard document for the juicy details.&lt;/p&gt;
    &lt;p&gt;The end result of the use of exceptions is that there is zero performance penalty to optimize out.&lt;/p&gt;
    &lt;head rend="h1"&gt;Fast data structures&lt;/head&gt;
    &lt;p&gt;Between all of the RISC-V instruction set extensions, there are 2000+ individual "instructions" (many instructions are identical to one another with a slight numerical change). There are also hundreds of CSRs and just under a hundred registers. This requires data structures large enough to store the properties of thousands of entries. How do you do that? It's tricky. So, how about I just show you what Ultrassembler uses as of this writing:&lt;/p&gt;
    &lt;code&gt;struct rvregister {
    RegisterType type; //1B
    RegisterID id; //1B
    uint8_t encoding;
    uint8_t padding;
};

const std::array&amp;lt;rvregister, 96&amp;gt; registers;

struct rvinstruction {
    RVInstructionID id; //2B
    RVInstructionFormat type; //1B
    uint8_t opcode;
    uint16_t funct;
    RVInSetMinReqs setreqs; //1B
    rreq regreqs = reg_reqs::any_regs; //1B
    special_snowflake_args ssargs = special_snowflake_args(); //2B
};

// We use a strong typedef to define both rreq and ssflag, but the underlying is a uint8_t in both cases

namespace ssarg {

constexpr ssflag get_imm_for_rs = ssflag(0b00000001);
constexpr ssflag use_frm_for_funct3 = ssflag(0b00000010);
constexpr ssflag special_handling = ssflag(0b00000100);
constexpr ssflag swap_rs1_rs2 = ssflag(0b00001000);
constexpr ssflag use_funct_for_imm = ssflag(0b00010000);
constexpr ssflag no_rs1 = ssflag(0b00100000);
constexpr ssflag has_custom_reg_val = ssflag(0b01000000);

}

struct special_snowflake_args {
    uint8_t custom_reg_val = 0;
    ssflag flags; //1B
};

const std::array&amp;lt;rvinstruction, 2034&amp;gt; instructions;
&lt;/code&gt;
    &lt;p&gt;Let's go over what each &lt;code&gt;struct&lt;/code&gt; does.&lt;/p&gt;
    &lt;head rend="h2"&gt;
      &lt;code&gt;rvregister&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;&lt;code&gt;rvregister&lt;/code&gt; is how Ultrassembler stores the data for all the RISC-V registers. What describes a register? You have its friendly name (like x0 or v20), an alias (like zero or fa1), what kind of register it is (integer, float, or vector?), and what raw encoding it looks like in instructions. You can get away with single bytes to represent the type and encoding. And, that's what we use here to keep data access simple. You could squeeze everything into one or two bytes through clever bitmasking, but after doing so, I couldn't find much of a speedup. This could be situational and so you should not dismiss such a trick.&lt;/p&gt;
    &lt;p&gt;Why not store the name and alias strings? Ultrassembler does not actually reference the name nor the alias anywhere in its code. Why? Strings are very expensive. This fact is not obvious if you have not made software at the level of Ultrassembler, where string comparison and manipulation grind computation to a crawl. So we just don't use strings anywhere. In spite of this, the initializers of &lt;code&gt;const std::array&amp;lt;rvregister, 96&amp;gt; registers&lt;/code&gt; do contain both the name and alias, but the constructors silently discard these data. Such inclusion enables external scripts to look at the array and generate code around it. We'll look at that in the next section. But for now, know that we hate strings.&lt;/p&gt;
    &lt;head rend="h2"&gt;
      &lt;code&gt;rvinstruction&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;&lt;code&gt;rvinstruction&lt;/code&gt; follows a similar idea, with the biggest differences being that it's much bigger, 2000+ entries versus 96, and that there is more information to store per entry. This necessitates some extra memory saving magic because there are so many different instructions. We first need an ID for each instruction to do specific checks if needed. We have almost more than 2048 instructions (subject to future expansion) but less than 4196, so we'll need 2 bytes. There are fewer than 256 "types" of instructions (R, I, S, B, U, J, etc.), so 1 byte is good. Same idea with all the other fields. Similarly to &lt;code&gt;rvregister&lt;/code&gt;, it would be possible to use bitmasking to compress everything, but this might not result in a significant speedup.&lt;/p&gt;
    &lt;head rend="h2"&gt;
      &lt;code&gt;special_snowflake_args&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;In RISC-V, many instructions require special attention because they have a special encoding, do something special, or are otherwise different from the rest of the herd. To avoid hardcoding behavior handling as much as possible, &lt;code&gt;special_snowflake_args&lt;/code&gt; encodes specific properties that many of these special instructions share, such as getting an immediate value instead of a register, swapping the &lt;code&gt;rs1&lt;/code&gt; and &lt;code&gt;rs2&lt;/code&gt; registers (or &lt;code&gt;vs1&lt;/code&gt; and &lt;code&gt;vs2&lt;/code&gt;), or omitting a register entirely. We can encode all these properties in a binary way so we use a custom bitmask system to save all the properties in a single byte. &lt;code&gt;custom_reg_val&lt;/code&gt;, however, is a separate 1-byte field because registers use 5 bits, and only exists in tandem with &lt;code&gt;has_custom_reg_val&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;All together, we are able to use only 20kB of memory to save all the instructions, not withstanding future entries. This fits nicely into many CPU data caches.&lt;/p&gt;
    &lt;head rend="h1"&gt;Preallocated memory pools&lt;/head&gt;
    &lt;p&gt;In C++, by default, containers that dynamically allocate memory do so through the heap. The underlying OS provides the heap through assignment of a certain section of its virtual memory to the program requesting the heap memory. Heap allocation happens transparently most of the time. Unfortunately for us, it matters where exactly that memory is. Memory located far away from everything else (often the case with heap memory) unnecessarily clogs up the CPU's memory cache. Additionally, in C++, requesting that heap memory also requires a syscall every time the container geometrically changes size (roughly speaking, 1B -&amp;gt; 2B -&amp;gt; 4B -&amp;gt; 8B -&amp;gt; ... -&amp;gt; 1MB). Syscalls drastically slow down code execution (more so than yo mama is big) because the OS needs to save all the registers, swap in the kernel's, and run the kernel code, all while clogging up the CPU cache again. Therefore, we need a way to allocate memory close to our variables with zero syscalls.&lt;/p&gt;
    &lt;p&gt;The solution?&lt;/p&gt;
    &lt;p&gt;Preallocated memory pools.&lt;/p&gt;
    &lt;p&gt;C++ offers a totally neato way to use the containers you know and love with a custom crafted memory allocator of your choice.&lt;/p&gt;
    &lt;p&gt;Here's how Ultrassembler does it.&lt;/p&gt;
    &lt;code&gt;constexpr size_t memory_pool_size = 33554432;

template &amp;lt;class T&amp;gt;
class MemoryBank;

typedef std::basic_string&amp;lt;char, std::char_traits&amp;lt;char&amp;gt;, MemoryBank&amp;lt;char&amp;gt;&amp;gt; ultrastring;

template &amp;lt;typename T&amp;gt;
using ultravector = std::vector&amp;lt;T, MemoryBank&amp;lt;T&amp;gt;&amp;gt;;

class GlobalMemoryBank {
    inline static std::array&amp;lt;std::byte, memory_pool_size&amp;gt; pool;
    inline static size_t used = 0; 
    inline static long pagesize = sysconf(_SC_PAGE_SIZE); // This only happens once :)

public:
    void* grab_some_memory(size_t requested);

    void reset();
};

extern GlobalMemoryBank memory_bank;

template &amp;lt;class T&amp;gt;
class MemoryBank {
public:
    using value_type = T;

    MemoryBank() = default;

    [[nodiscard]] T* allocate(size_t requested) {
        std::size_t bytes = requested * sizeof(T);
        return reinterpret_cast&amp;lt;T*&amp;gt;(memory_bank.grab_some_memory(bytes));
    }

    void deallocate(T* ptr, size_t requested) { return; }

    bool operator==(const MemoryBank&amp;amp;) const { return true; }
};

// In another file...

void* GlobalMemoryBank::grab_some_memory(size_t requested) {
    if (requested + used &amp;gt; pool.size()) {
        throw UASError(OutOfMemory, "Out of memory!");
    }
    void* ptr = reinterpret_cast&amp;lt;void*&amp;gt;(pool.data() + used);
    used += requested;
    return ptr;
}

void GlobalMemoryBank::reset() {
    used = 0;
}
&lt;/code&gt;
    &lt;p&gt;Let's go through this section by section.&lt;/p&gt;
    &lt;code&gt;constexpr size_t memory_pool_size = 33554432;

template &amp;lt;class T&amp;gt;
class MemoryBank;

typedef std::basic_string&amp;lt;char, std::char_traits&amp;lt;char&amp;gt;, MemoryBank&amp;lt;char&amp;gt;&amp;gt; ultrastring;

template &amp;lt;typename T&amp;gt;
using ultravector = std::vector&amp;lt;T, MemoryBank&amp;lt;T&amp;gt;&amp;gt;;
&lt;/code&gt;
    &lt;p&gt;This is boilerplate defining how big our memory pool is (in bytes), declaring the regular memory pool class (annoying!), what our special memory pool string is an alias of (a standard string but with the regular memory pool allocator), and the same creation of a vector using the regular memory pool.&lt;/p&gt;
    &lt;code&gt;class GlobalMemoryBank {
    inline static std::array&amp;lt;std::byte, memory_pool_size&amp;gt; pool;
    inline static size_t used = 0;
    inline static long pagesize = sysconf(_SC_PAGE_SIZE);

public:
    void* grab_some_memory(size_t requested);

    void reset();
};

extern GlobalMemoryBank memory_bank;
&lt;/code&gt;
    &lt;p&gt;This class defines the memory pool wrapper that the actual allocator uses. Why? This has to do with how C++ uses custom allocators. When you use a container with a custom allocator, each declaration of that container creates a separate instance of that container and the allocator class. Therefore, if you added the memory pool array as a member of this custom allocator class, each declaration of the container would result in separate instantiations of the underlying memory pool object. This is UNACCEPTABLE for Ultrassembler. Therefore, we instead use a helper class that the allocators call to. As a consequence, it allows us to add memory pool functionality controlled independently of the containers through calls to the helper &lt;code&gt;GlobalMemoryBank&lt;/code&gt; class in the future.&lt;/p&gt;
    &lt;code&gt;template &amp;lt;class T&amp;gt;
class MemoryBank {
public:
    using value_type = T;

    MemoryBank() = default;

    [[nodiscard]] T* allocate(size_t requested) {
        std::size_t bytes = requested * sizeof(T);
        return reinterpret_cast&amp;lt;T*&amp;gt;(memory_bank.grab_some_memory(bytes));
    }

    void deallocate(T* ptr, size_t requested) { return; }

    bool operator==(const MemoryBank&amp;amp;) const { return true; }
};
&lt;/code&gt;
    &lt;p&gt;This is the actual custom allocator object that we pass to C++ containers. The definition of a custom allocator in C++ is simply a class that provides the &lt;code&gt;allocate&lt;/code&gt; and &lt;code&gt;deallocate&lt;/code&gt; functions publicly. That's literally it. There are in fact more potential functions that you could add to handle specific uses, but &lt;code&gt;allocate&lt;/code&gt; and &lt;code&gt;deallocate&lt;/code&gt; are all we need for Ultrassembler. We define this class as a template because the return value of the &lt;code&gt;allocate&lt;/code&gt; function must match the underlying type of the container using the allocator class. We furthermore define the &lt;code&gt;==&lt;/code&gt; operator because C++ requires that two objects using allocators match their allocators. You'll normally never notice this because the default allocator for all C++ containers, &lt;code&gt;std::allocator&lt;/code&gt;, provides all the allocator functions and operator comparison functions, and as a result, handles all comparisons transparently. Ultrassembler only uses equality. Finally, we provide a default constructor &lt;code&gt;MemoryBank() = default;&lt;/code&gt; as this is what the C++ standard expects too from allocator classes.&lt;/p&gt;
    &lt;code&gt;void* GlobalMemoryBank::grab_some_memory(size_t requested) {
    if (requested + used &amp;gt; pool.size()) {
        throw UASError(OutOfMemory, "Out of memory!");
    }
    void* ptr = reinterpret_cast&amp;lt;void*&amp;gt;(pool.data() + used);
    used += requested;
    return ptr;
}

void GlobalMemoryBank::reset() {
    used = 0;
}
&lt;/code&gt;
    &lt;p&gt;These functions implement allocating the memory and resetting the memory bank. Allocating should be obvious. However, resetting might not. As it stands, the memory pool simply gives up if it runs out of memory to allocate. We don't deallocate because such an operation would add extra overhead and subjects us to the issue of memory fragementation. Memory fragmentation is when you deallocate a small object from a large area of allocated memory, leaving a small area of unallocated memory laying in the otherwise allocated area. If you want to allocate a new object, tough luck, you probably can't fit it in this small area. You need to wait for the other objects to deallocate first. This cycle continues until your memory usage looks like Swiss cheese and doesn't support allocating any more objects, leading to a system crash. Normally, the OS kernel handles this problem transparently. Linux for example uses a "buddy allocator" to help deal with it. Memory fragmentation is also less of an issue with huge swaths of memory on modern systems. Our memory pool unfortunately lacks those luxuries of large memory and processing power for buddy allocators. Therefore, we provide the &lt;code&gt;reset&lt;/code&gt; function to start everything over if the software using Ultrassembler receives an &lt;code&gt;OutOfMemory&lt;/code&gt; exception.&lt;/p&gt;
    &lt;p&gt;Our memory pool trick lets Ultrassembler enjoy optimal memory locality and predefined memory usage while also completely eliminating syscalls (almost) and memory leaks, notwithstanding occasional memory bank resets.&lt;/p&gt;
    &lt;head rend="h1"&gt;Value speculation&lt;/head&gt;
    &lt;p&gt;A while ago, I read this fascinating article on something called L1 value speculation. The basic idea is to free the branch predictor by giving it extra work to do guessing the next value in the linked list. If it's right (usually it is) then you get a free speedup.&lt;/p&gt;
    &lt;p&gt;Ultrassembler does something similar. Instead of a linked list, we iterate through an array checking for specific combinations of characters that define the end of a sequence to copy.&lt;/p&gt;
    &lt;code&gt;auto ch = [&amp;amp;]() {
    return data[i];
};

volatile char preview;
while (i &amp;lt; data.size() &amp;amp;&amp;amp; not_at_end(ch()) &amp;amp;&amp;amp; !is_whitespace(ch())) {
    c.inst.push_back(ch());
    i++;
    preview = ch();
}
&lt;/code&gt;
    &lt;p&gt;As built-in strings in C++ are super duper mega slow even with custom allocators, we spend a lot of time on &lt;code&gt;c.inst.push_back(ch());&lt;/code&gt;. There's fortunately a workaround. If the CPU knows that we'll be accessing the next character in the target string, why not queue it up first? This is exactly what &lt;code&gt;volatile char preview;&lt;/code&gt; and &lt;code&gt;preview = ch();&lt;/code&gt; accomplish. We already have an opportunity for speculation with the &lt;code&gt;i++;&lt;/code&gt; and &lt;code&gt;i &amp;lt; data.size();&lt;/code&gt;. Although I'm not 100% sure, my hypothesis on why &lt;code&gt;preview&lt;/code&gt; provides a speedup is that the branch predictor can only handle &lt;code&gt;i &amp;lt; data.size()&lt;/code&gt; and not additionally the character loading of &lt;code&gt;ch()&lt;/code&gt;. Therefore, we should preemptively load &lt;code&gt;ch()&lt;/code&gt; during &lt;code&gt;c.inst.push_back(ch());&lt;/code&gt;. &lt;/p&gt;
    &lt;p&gt;Eagle eyed readers will notice how there is an opportunity for memory overflow if we are at the end of a string and &lt;code&gt;i++;&lt;/code&gt; then &lt;code&gt;preview = ch();&lt;/code&gt; loads a character past the string &lt;code&gt;data&lt;/code&gt;. However, Ultrassembler accounts for this by preemptively adding an extra null character to the input string &lt;code&gt;data&lt;/code&gt; earlier in the code, ensuring that such illegal memory accesses are impossible by definition. &lt;/p&gt;
    &lt;p&gt;This optimization sped up parsing of the instruction names enough that the overall Ultrassembler performance increased by about 10%.&lt;/p&gt;
    &lt;head rend="h1"&gt;(Super) smart searches&lt;/head&gt;
    &lt;p&gt;Here's one weird trick I haven't seen anywhere else.&lt;/p&gt;
    &lt;p&gt;Imagine I provided you these words: apple, apricot, avocado, and banana.&lt;/p&gt;
    &lt;p&gt;Now, what if I told you a mystery word I was looking for among the ones I provided was 7 letters long. You would immediately discard "apple" and "banana" because they're not 7 letters long. Now, I tell you that it starts with "a." You wouldn't discard any at this point because both "apricot" and "avocado" start with the letter a. Finally, I tell you that the second letter is "v." Immediately we know "avocado" is the mystery word because no other word remaining starts with "av."&lt;/p&gt;
    &lt;p&gt;This is the basic idea behind the instruction, register, CSR, and pseudoinstruction lookup systems in Ultrassembler. There's a rub, though. The code for these lookups looks something like this:&lt;/p&gt;
    &lt;code&gt;const uint16_t fast_instr_search(const ultrastring&amp;amp; inst) {
    const auto size = inst.size();

    if (size == 2) {
        if (inst[0] == 's') {
            if (inst[1] == 'd') return 44;
            if (inst[1] == 'w') return 17;
            if (inst[1] == 'b') return 15;
            if (inst[1] == 'h') return 16;
        }
        if (inst[0] == 'o') {
            if (inst[1] == 'r') return 35;
        }
        if (inst[0] == 'l') {
            if (inst[1] == 'd') return 43;
            if (inst[1] == 'w') return 12;
            if (inst[1] == 'b') return 10;
            if (inst[1] == 'h') return 11;
        }
    }

    if (size == 3) {
        etc...
&lt;/code&gt;
    &lt;p&gt;Clearly, there's a lot of work to do if you've got thousands of entries like the instructions array does. There's a fix for that though!&lt;/p&gt;
    &lt;p&gt;Enter codegen.&lt;/p&gt;
    &lt;p&gt;Ultrassembler uses artisan-crafted Python scripts to traverse through the listings and extract the string names for each instruction, register, CSR, and pseudoinstruction. Then, these scripts generate C++ code which performs these precomputed lookups.&lt;/p&gt;
    &lt;p&gt;Here's what the instruction search script looks like. WARNING! If this script looks ugly, it's because Python is one of the worst programming languages out there for anything more than mere supportive, throwaway software like this.&lt;/p&gt;
    &lt;code&gt;input = "src/instructions.cpp"
output = "src/generated/instruction_search.cpp"

import re

content = ""
with open(input, "r") as file:
    content = file.read()

regex = "(?&amp;lt;={)\"([\w.]+)\""

instructions = re.findall(regex, content)

for i in range(len(instructions)):
    instructions[i] = (instructions[i], i, len(instructions[i]))

instructions.sort()

print(instructions)

min_len = min([i[2] for i in instructions])

max_len = max([i[2] for i in instructions])

depth = 0

current_instr = ""

code = "// SPDX-License-Identifier: MPL-2.0\n"
code += "// The generate_instruction_search.py script automatically generated this code. DO NOT MODIFY!\n"
code += "#include \"../instructions.hpp\"\n"
code += "#include \"../ultrassembler.hpp\"\n\n"
code += "namespace ultrassembler_internal {\n\n"
code += "const uint16_t fast_instr_search(const ultrastring&amp;amp; inst) {\n"
code += "    const auto size = inst.size();\n\n"

def ind():
    return "    " * (depth + 2)

def instr_exists(instr, length):
    for i in instructions:
        if i[0] == instr and i[2] == length:
            return True
    return False
    
def prefix_exists(prefix, length):
    for i in instructions:
        if i[0].startswith(prefix) and i[2] == length:
            return True
    return False

potentialchars = ""

for instr in instructions:
    for char in instr[0]:
        if char not in potentialchars:
            potentialchars += char

def process_depth(current_len):
    global code, current_instr, depth
    for letter in potentialchars:
        if instr_exists(current_instr + letter, current_len):
            code += ind() + f"if (inst[{depth}] == '{letter}') return {instructions[[i[0] for i in instructions].index(current_instr + letter)][1]};\n"
        elif prefix_exists(current_instr + letter, current_len):
            code += ind() + f"if (inst[{depth}] == '{letter}') {{\n"
            current_instr += letter
            depth += 1
            process_depth(current_len)
            depth -= 1
            current_instr = current_instr[:-1]
            code += ind() + "}\n"

for i in range(min_len, max_len + 1):
    code += f"    if (size == {i}) {{\n"
    process_depth(i)
    code += "    }\n\n"

code += "    return instr_search_failed;\n"
code += "}\n\n"
code += "} // namespace ultrassembler_internal"

print(code)

with open(output, "w") as file:
    file.write(code)
&lt;/code&gt;
    &lt;p&gt;Let's go through it section by section.&lt;/p&gt;
    &lt;code&gt;input = "src/instructions.cpp"
output = "src/generated/instruction_search.cpp"

import re

content = ""
with open(input, "r") as file:
    content = file.read()
&lt;/code&gt;
    &lt;p&gt;This simply tells the script what file to read and where to generate the code, imports the regex package, and reads the input file.&lt;/p&gt;
    &lt;code&gt;regex = "(?&amp;lt;={)\"([\w.]+)\""

instructions = re.findall(regex, content)

for i in range(len(instructions)):
    instructions[i] = (instructions[i], i, len(instructions[i]))

instructions.sort()

print(instructions)
&lt;/code&gt;
    &lt;p&gt;This regex searches for all instances of quotes in the instruction C++ code. That code looks like this:&lt;/p&gt;
    &lt;code&gt;const std::array&amp;lt;rvinstruction, 2034&amp;gt; instructions = {
        {{"lui", LUI, U, op_LUI, 0b000, RVI, int_reg},
         {"auipc", AUIPC, U, op_AUIPC, 0b000, RVI, int_reg},
         {"jal", JAL, J, op_JAL, 0b000, RVI, int_reg}, etc...
&lt;/code&gt;
    &lt;p&gt;Then, it creates a new array with the instruction name, what position it is in the array, and its length. This might seem redundant at first, but it's helpful later. We then sort all the insructions alphabetically (also important!) and show all of them for debugging/status purposes.&lt;/p&gt;
    &lt;code&gt;min_len = min([i[2] for i in instructions])

max_len = max([i[2] for i in instructions])

depth = 0

current_instr = ""

code = "// SPDX-License-Identifier: MPL-2.0\n"
code += "// The generate_instruction_search.py script automatically generated this code. DO NOT MODIFY!\n"
code += "#include \"../instructions.hpp\"\n"
code += "#include \"../ultrassembler.hpp\"\n\n"
code += "namespace ultrassembler_internal {\n\n"
code += "const uint16_t fast_instr_search(const ultrastring&amp;amp; inst) {\n"
code += "    const auto size = inst.size();\n\n"

def ind():
    return "    " * (depth + 2)

def instr_exists(instr, length):
    for i in instructions:
        if i[0] == instr and i[2] == length:
            return True
    return False
    
def prefix_exists(prefix, length):
    for i in instructions:
        if i[0].startswith(prefix) and i[2] == length:
            return True
    return False

potentialchars = ""

for instr in instructions:
    for char in instr[0]:
        if char not in potentialchars:
            potentialchars += char
&lt;/code&gt;
    &lt;p&gt;This is a lot of boilerplate for the algorithm later to come. We find the shortest and longest instructions. We add the first parts of the generated file. We define an indentation helper for nice formatting. We define additional helper functions to check if a whole instruction exists with a given name and length or if there is an instruction with the provided prefix and length. Finally, we assemble an array with all the characters to search for that the instructions use to avoid unnecessary computation later.&lt;/p&gt;
    &lt;code&gt;def process_depth(current_len):
    global code, current_instr, depth
    for letter in potentialchars:
        if instr_exists(current_instr + letter, current_len):
            code += ind() + f"if (inst[{depth}] == '{letter}') return {instructions[[i[0] for i in instructions].index(current_instr + letter)][1]};\n"
        elif prefix_exists(current_instr + letter, current_len):
            code += ind() + f"if (inst[{depth}] == '{letter}') {{\n"
            current_instr += letter
            depth += 1
            process_depth(current_len)
            depth -= 1
            current_instr = current_instr[:-1]
            code += ind() + "}\n"

for i in range(min_len, max_len + 1):
    code += f"    if (size == {i}) {{\n"
    process_depth(i)
    code += "    }\n\n"
&lt;/code&gt;
    &lt;p&gt;Here's where the magic happens. We process one instruction length depth at a time. Like the algorithm we talked about at the beginning of this section, we start with the shortest possible "words" and work our way to the longest. Each depth step works through a search of all the possible characters and first checks if we have already found an instruction. If there is such an instruction, we add it to the code. Alternatively, if there is no such instruction but there is in fact an instruction that starts with the current sequence, we go down a depth level because we know that eventually, we will find an instruction with an exact match. Once we've gone through all of the possible instructions and depths, we exit the &lt;code&gt;for&lt;/code&gt; loop.&lt;/p&gt;
    &lt;code&gt;code += "    return instr_search_failed;\n"
code += "}\n\n"
code += "} // namespace ultrassembler_internal"

print(code)

with open(output, "w") as file:
    file.write(code)
&lt;/code&gt;
    &lt;p&gt;This completes the generated search function, shows it all for debugging/status purposes, and finally writes the generated code to the output file path.&lt;/p&gt;
    &lt;p&gt;There are no other instances of this kind of codegen that I know of. That's surprising, because codegen allows us to perform lookup of thousands of instructions with near-zero overhead. I estimate each instruction lookup takes on the order of 10 instructions to complete.&lt;/p&gt;
    &lt;p&gt;Here's what the resulting compiled assembly looks like on my x86 PC:&lt;/p&gt;
    &lt;code&gt;0000000000029340 &amp;lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE&amp;gt;:
   29340:	f3 0f 1e fa          	endbr64 
   29344:	48 8b 47 08          	mov    0x8(%rdi),%rax
   29348:	48 83 f8 02          	cmp    $0x2,%rax
   2934c:	0f 84 c6 00 00 00    	je     29418 &amp;lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0xd8&amp;gt;
   29352:	48 83 f8 03          	cmp    $0x3,%rax
   29356:	75 28                	jne    29380 &amp;lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x40&amp;gt;
   29358:	48 8b 17             	mov    (%rdi),%rdx
   2935b:	0f b6 0a             	movzbl (%rdx),%ecx
   2935e:	80 f9 61             	cmp    $0x61,%cl
   29361:	0f 84 79 2b 00 00    	je     2bee0 &amp;lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x2ba0&amp;gt;
   29367:	80 f9 64             	cmp    $0x64,%cl
   2936a:	0f 85 58 10 00 00    	jne    2a3c8 &amp;lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x1088&amp;gt;
   29370:	80 7a 01 69          	cmpb   $0x69,0x1(%rdx)
   29374:	b8 ff ff ff ff       	mov    $0xffffffff,%eax
   29379:	0f 84 09 2f 00 00    	je     2c288 &amp;lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x2f48&amp;gt;
   2937f:	c3                   	ret
   # There are thousands more lines of this!
&lt;/code&gt;
    &lt;p&gt;And RISC-V:&lt;/p&gt;
    &lt;code&gt;000000000007c33c &amp;lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE&amp;gt;:
   7c33c:	7179                	addi	sp,sp,-48
   7c33e:	f406                	sd	ra,40(sp)
   7c340:	e42a                	sd	a0,8(sp)
   7c342:	6522                	ld	a0,8(sp)
   7c344:	00089317          	auipc	t1,0x89
   7c348:	afc33303          	ld	t1,-1284(t1) # 104e40 &amp;lt;_ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcEN22ultrassembler_internal10MemoryBankIcEEE4sizeEv@@Base+0xad9c4&amp;gt;
   7c34c:	9302                	jalr	t1
   7c34e:	ec2a                	sd	a0,24(sp)
   7c350:	6762                	ld	a4,24(sp)
   7c352:	4789                	li	a5,2
   7c354:	22f71c63          	bne	a4,a5,7c58c &amp;lt;_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x250&amp;gt;
   7c358:	4581                	li	a1,0
   7c35a:	6522                	ld	a0,8(sp)
   7c35c:	00089317          	auipc	t1,0x89
   7c360:	c6433303          	ld	t1,-924(t1) # 104fc0 &amp;lt;_ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcEN22ultrassembler_internal10MemoryBankIcEEEixEm@@Base+0xaaef8&amp;gt;
   7c364:	9302                	jalr	t1
   # Also thousands more lines of this!
&lt;/code&gt;
    &lt;head rend="h1"&gt;Compile-time templates&lt;/head&gt;
    &lt;p&gt;This is similar to script codegen but with native C++ only.&lt;/p&gt;
    &lt;p&gt;One of the verification steps in Ultrassembler involves checking that the immediate value of an instruction (for example, &lt;code&gt;addi t0, t1, 100&lt;/code&gt;) fits within some known range. C++ allows us to both cleanly invoke this check for an arbitrary range and do so with little to no runtime overhead to calculate that range.&lt;/p&gt;
    &lt;p&gt;Here's how it works.&lt;/p&gt;
    &lt;code&gt;template &amp;lt;auto bits&amp;gt;
void verify_imm(const auto&amp;amp; imm) {
    using T = decltype(bits);
    if constexpr (std::is_signed_v&amp;lt;T&amp;gt;) {
        if (imm &amp;lt; -(1 &amp;lt;&amp;lt; (bits - 1)) || imm &amp;gt;= (1 &amp;lt;&amp;lt; (bits - 1))) {
            throw UASError(ImmOutOfRange, "Immediate " + to_ultrastring(imm) + " is out of range [" + to_ultrastring(-(1 &amp;lt;&amp;lt; (bits - 1))) + ", " + to_ultrastring((1 &amp;lt;&amp;lt; (bits - 1))) + ")", 0, 0);
        }
    } else if constexpr (std::is_unsigned_v&amp;lt;T&amp;gt;) {
        if (imm &amp;lt; 0 || imm &amp;gt;= (1u &amp;lt;&amp;lt; bits)) {
            throw UASError(ImmOutOfRange, "Immediate " + to_ultrastring(imm) + " is out of range [0, " + to_ultrastring((1u &amp;lt;&amp;lt; bits)) + ")", 0, 0);
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;Each invocation looks something like &lt;code&gt;verify_imm&amp;lt;5u&amp;gt;(imm)&lt;/code&gt;. We supply a numeric literal and the immediate variable to check. C++'s template facilities then check whether we've supplied a signed or unsigned numeric literal, as RISC-V instruction can vary whether they expect signed or unsigned numbers only. We then calculate the lowest possible number (&lt;code&gt;-(1 &amp;lt;&amp;lt; (bits - 1))&lt;/code&gt; for signed and &lt;code&gt;0&lt;/code&gt; for unsigned) and the highest possible number (&lt;code&gt;(1 &amp;lt;&amp;lt; (bits - 1))&lt;/code&gt; for signed and &lt;code&gt;(1u &amp;lt;&amp;lt; bits)&lt;/code&gt; for unsigned) and check the input against that. We then throw an error if it doesn't fit these calculated constraints or return silently if it does.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;if constexpr&lt;/code&gt; tells the compiler to generate each signed or unsigned execution path at compile time depending on what numeric literal we've provided, allowing us to make each function call as pretty and fast as possible.&lt;/p&gt;
    &lt;head rend="h1"&gt;Fast string comparisons&lt;/head&gt;
    &lt;p&gt;For the times where we can't or don't want to use a precomputed string search, Ultrassembler uses an optimized string comparison function to minimize overhead.&lt;/p&gt;
    &lt;code&gt;bool fast_eq(const auto&amp;amp; first, const std::string_view&amp;amp; second) {
    if (first.size() != second.size()) { 
        return false;
    }
    for (size_t i = 0; i &amp;lt; first.size(); i++) {
        if (first[i] != second[i]) {
            [[likely]] return false;
        } else {
            [[unlikely]] continue;
        }
    }
    return true;
}
&lt;/code&gt;
    &lt;p&gt;How does this work? First, we check to make sure the input strings are the same length. It's impossible by definition for them to be the same if they have different lengths. Then, we compare them character by character. Here, we use C++20's &lt;code&gt;[[likely]]&lt;/code&gt; and &lt;code&gt;[[unlikely]]&lt;/code&gt; tags to help the compiler optimize the positioning of each comparison. It's statistically more likely to have a comparison failure than a success because we are usually comparing one input string against many possible options but it can only match with up to one.&lt;/p&gt;
    &lt;head rend="h1"&gt;Reference bigger-than-fundamental objects in function arguments&lt;/head&gt;
    &lt;p&gt;This one surprised me.&lt;/p&gt;
    &lt;p&gt;When you call a C++ function, you can choose to pass your arguments by value, or by reference. By default, C++ uses by value, which means the code internally makes a copy of the argument and provides that copy to the function. If you add a &lt;code&gt;&amp;amp;&lt;/code&gt; to make it a reference instead (there are other ways to do this too) then the code generates a pointer to that original object and passes that pointer to the function. However, unlike pointers, references handle referencing and dereferencing transparently. As an aside, this also means Ultrassembler technically doesn't use pointers... anywhere! Pointers are horrible.&lt;/p&gt;
    &lt;p&gt;One of the most common pieces of C++ optimization advice is to use references whenever possible to avoid the copy overhead incurred by value references. It might surprise you, then, to find out that the following code is vastly faster due to the use of a value argument:&lt;/p&gt;
    &lt;code&gt;size_t parse_this_line(size_t i, const ultrastring&amp;amp; data, assembly_context&amp;amp; c) {
    // code that does "i++;" a lot
}

// later, in a different function:
for (size_t i = 0; i &amp;lt; data.size();) {
    i = parse_this_line(i, data, c);
    // etc...
}
&lt;/code&gt;
    &lt;p&gt;If we had applied the Programming Furus©️®️™️'s advice to pass &lt;code&gt;i&lt;/code&gt; by reference, it would have looked like:&lt;/p&gt;
    &lt;code&gt;void parse_this_line(size_t&amp;amp; i, const ultrastring&amp;amp; data, assembly_context&amp;amp; c) {
    // code that does "i++;" a lot
}

// later, in a different function:
for (size_t i = 0; i &amp;lt; data.size();) {
    parse_this_line(i, data, c);
    // etc...
}
&lt;/code&gt;
    &lt;p&gt;So why is the first one faster? Here's why.&lt;/p&gt;
    &lt;p&gt;Under the hood of all programming languages, you have assembly code which translates to the CPU's machine code. There are also no variables. Instead, you've got registers which hold raw data and raw memory. In most application processors today, the registers are 64 bits wide, and maybe wider for special vector operations which don't matter here. 64 bits happens to match the maximum width of so-called fundamental types in C and C++ which are integers and most common floats. Therefore, we can fit at least one fundamental type into each register.&lt;/p&gt;
    &lt;p&gt;Quick refresher of the registers in RISC-V:&lt;/p&gt;
    &lt;p&gt;Assembly also has little concept of a function call. Internally, all function calls do is clear out the current registers, load them with the function parameters, then jump to the function's address. This means all function calls involve at least one copy per argument, whether it's a fundamental type or a pointer to a fundamental type or a pointer to something else.&lt;/p&gt;
    &lt;code&gt;# Here's what this looks like in RISC-V assembly.
# Say we have a number in register t0, like 69.

addi t0, x0, 69

# We also have a function foobar that takes a single integer argument (like "void foobar(size_t arg)" in C/C++)
# We can copy that register (and therefore its value) to argument register a0 before calling foobar

addi a0, t0, 0

jal foobar

# The copying of this value only took one step!
&lt;/code&gt;
    &lt;p&gt;You can see where we're going. If our goal is to minimize copying, it would be better to copy a fundamental type once than to generate a pointer, copy that, then dereference that pointer to get the underlying value. That is the crux of this subtle optimization trick. The cost to copy one register is less than the cost to copy a register holding a pointer.&lt;/p&gt;
    &lt;p&gt;Note how I've only talked about fundamental types. Any type which does not fit in a single register, AKA many structs, containers, or anything else that isn't a fundamental type, costs more to copy by value in multiple registers than it does to copy a single register holding a pointer. I don't know of any Programming Furu©️®️™️ that makes this distinction clear.&lt;/p&gt;
    &lt;head rend="h1"&gt;Don't do insertions or deletions&lt;/head&gt;
    &lt;p&gt;One of the steps to assemble a jump operation in RISC-V assembly is to calculate the offset of bytes to the jump target. However, this is often impossible unless all other instructions are already assembled. Ultrassembler does its best to avoid insertions or deletions through a clever trick to assemble jump instructions with a placeholder jump offset and then insert the correct offset in-place at the end.&lt;/p&gt;
    &lt;p&gt;Here's how it works:&lt;/p&gt;
    &lt;code&gt;void solve_label_offsets(assembly_context&amp;amp; c) {
    using enum RVInstructionFormat;
    for (size_t i = 0; i &amp;lt; c.label_locs.size(); i++) {
        if (!c.label_locs.at(i).is_dest) {
            for (size_t j = 0; j &amp;lt; c.label_locs.size(); j++) {
                if (c.label_locs.at(j).is_dest &amp;amp;&amp;amp; c.label_locs.at(j).id == c.label_locs.at(i).id) {
                    uint32_t inst = 0;

                    if (c.label_locs.at(i).i_bytes == 2) {
                        inst = reinterpret_cast&amp;lt;uint16_t&amp;amp;&amp;gt;(c.machine_code.at(c.label_locs.at(i).loc));
                    } else if (c.label_locs.at(i).i_bytes == 4) {
                        inst = reinterpret_cast&amp;lt;uint32_t&amp;amp;&amp;gt;(c.machine_code.at(c.label_locs.at(i).loc));
                    }

                    int32_t offset = c.label_locs.at(j).loc - c.label_locs.at(i).loc;

                    if (c.label_locs.at(i).format == Branch) {
                        inst &amp;amp;= 0b00000001111111111111000001111111;
                        inst |= ((offset &amp;gt;&amp;gt; 11) &amp;amp; 0b1) &amp;lt;&amp;lt; 7;      // Add imm[11]
                        inst |= ((offset &amp;gt;&amp;gt; 1) &amp;amp; 0b1111) &amp;lt;&amp;lt; 8;    // Add imm[4:1]
                        inst |= ((offset &amp;gt;&amp;gt; 5) &amp;amp; 0b111111) &amp;lt;&amp;lt; 25; // Add imm[10:5]
                        inst |= ((offset &amp;gt;&amp;gt; 12) &amp;amp; 0b1) &amp;lt;&amp;lt; 31;     // Add imm[12]
                    } else if (c.label_locs.at(i).format == J) {
                        inst &amp;amp;= 0b00000000000000000000111111111111;
                        inst |= ((offset &amp;gt;&amp;gt; 12) &amp;amp; 0b11111111) &amp;lt;&amp;lt; 12;  // Add imm[19:12]
                        inst |= ((offset &amp;gt;&amp;gt; 11) &amp;amp; 0b1) &amp;lt;&amp;lt; 20;         // Add imm[11]
                        inst |= ((offset &amp;gt;&amp;gt; 1) &amp;amp; 0b1111111111) &amp;lt;&amp;lt; 21; // Add imm[10:1]
                        inst |= ((offset &amp;gt;&amp;gt; 20) &amp;amp; 0b1) &amp;lt;&amp;lt; 31;         // Add imm[20]
                    } else if (c.label_locs.at(i).format == CJ) {
                        inst &amp;amp;= 0b1110000000000011;
                        inst |= ((offset &amp;gt;&amp;gt; 5) &amp;amp; 0b1) &amp;lt;&amp;lt; 2;   // Add offset[5]
                        inst |= ((offset &amp;gt;&amp;gt; 1) &amp;amp; 0b111) &amp;lt;&amp;lt; 3; // Add offset[3:1]
                        inst |= ((offset &amp;gt;&amp;gt; 7) &amp;amp; 0b1) &amp;lt;&amp;lt; 6;   // Add offset[7]
                        inst |= ((offset &amp;gt;&amp;gt; 6) &amp;amp; 0b1) &amp;lt;&amp;lt; 7;   // Add offset[6]
                        inst |= ((offset &amp;gt;&amp;gt; 10) &amp;amp; 0b1) &amp;lt;&amp;lt; 8;  // Add offset[10]
                        inst |= ((offset &amp;gt;&amp;gt; 8) &amp;amp; 0b11) &amp;lt;&amp;lt; 9;  // Add offset[9:8]
                        inst |= ((offset &amp;gt;&amp;gt; 4) &amp;amp; 0b1) &amp;lt;&amp;lt; 11;  // Add offset[4]
                        inst |= ((offset &amp;gt;&amp;gt; 11) &amp;amp; 0b1) &amp;lt;&amp;lt; 12; // Add offset[11]
                    } else if (c.label_locs.at(i).format == CB) {
                        inst &amp;amp;= 0b1110001110000011;
                        inst |= ((offset &amp;gt;&amp;gt; 5) &amp;amp; 0b1) &amp;lt;&amp;lt; 2;   // Add offset[5]
                        inst |= ((offset &amp;gt;&amp;gt; 1) &amp;amp; 0b11) &amp;lt;&amp;lt; 3;  // Add offset[2:1]
                        inst |= ((offset &amp;gt;&amp;gt; 6) &amp;amp; 0b11) &amp;lt;&amp;lt; 5;  // Add offset[7:6]
                        inst |= ((offset &amp;gt;&amp;gt; 3) &amp;amp; 0b11) &amp;lt;&amp;lt; 10; // Add offset[4:3]
                        inst |= ((offset &amp;gt;&amp;gt; 8) &amp;amp; 0b1) &amp;lt;&amp;lt; 12;  // Add offset[8]
                    }

                    if (c.label_locs.at(i).i_bytes == 2) {
                        reinterpret_cast&amp;lt;uint16_t&amp;amp;&amp;gt;(c.machine_code.data()[c.label_locs.at(i).loc]) = inst;
                    } else if (c.label_locs.at(i).i_bytes == 4) {
                        reinterpret_cast&amp;lt;uint32_t&amp;amp;&amp;gt;(c.machine_code.data()[c.label_locs.at(i).loc]) = inst;
                    }
                }
            }
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;When we find a jump instruction that needs later TLC, we save its location and some other attributes to a special array. Then, after the rest of the code is done assembling, we go back through each jump instruction and calculate the correct offset and insert that offset in-place in the correct instruction format.&lt;/p&gt;
    &lt;p&gt;I believe this is faster than what some other assemblers do for instructions which jump to a location reachable within the constraints of the offset's size. However, it's not useful for far jumps, which require a separate helper instruction to extend the jump. Ultrassembler doesn't support those yet.&lt;/p&gt;
    &lt;head rend="h1"&gt;More optimizations&lt;/head&gt;
    &lt;p&gt;Here's a few more optimization tricks that aren't quite significant enough for their own sections but deserve a mention anyway.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory padding&lt;/head&gt;
    &lt;p&gt;There are a few strings which Ultrassembler frequently reads and writes. To insure against runtime memory pool allocation overhead, we preemptively allocate a good amount of memory.&lt;/p&gt;
    &lt;code&gt;c.inst.reserve(32);
c.arg1.reserve(32);
c.arg2.reserve(32);
c.arg3.reserve(32);
c.arg4.reserve(32);
c.arg5.reserve(32);
c.arg6.reserve(32);
c.arg_extra.reserve(32);
c.machine_code.reserve(128000);
&lt;/code&gt;
    &lt;p&gt;I found that 32 bytes gave the biggest speedup for small strings, and sizes above a few kB are more appropriate for the machine code output.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inline some functions&lt;/head&gt;
    &lt;p&gt;Sometimes, functions are faster when you mark them &lt;code&gt;inline&lt;/code&gt; to suggest that the code have a copy for each invocation. This tends to work better for smaller functions.&lt;/p&gt;
    &lt;code&gt;inline const uint8_t decode_encoding_length(const uint8_t opcode) {
    if ((opcode &amp;amp; 0b11) != 0b11) {
        return 2;
    } else {
        return 4;
    }
}
&lt;/code&gt;
    &lt;p&gt;Try it and see what works best for your own code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Minimize string stripping copies&lt;/head&gt;
    &lt;p&gt;Here's a special case of minimizing string copying. This function removes the parentheses and optionally the number 0 from a string like "0(t4)":&lt;/p&gt;
    &lt;code&gt;void remove_extraneous_parentheses(ultrastring&amp;amp; str) {
    if (str.back() == ')') {
        str.pop_back();
    }
    if (str.front() == '0') {
        str.erase(0, 1);
    }
    if (str.front() == '(') {
        str.erase(0, 1);
    }
}
&lt;/code&gt;
    &lt;p&gt;Why do we tackle the last character first? When you erase one or more characters from a string, C++ internally copies every individual character after setting the characters to erase to blank. In other words, it looks a little like this:&lt;/p&gt;
    &lt;code&gt;# Erase "foo" from "foobar"

foobar

 oobar

  obar

   bar

b  bar

ba bar

barbar

barba

barb

bar
&lt;/code&gt;
    &lt;p&gt;That's a lot of copies. So it would be great if we can avoid copying more of these characters in the future. Then, we handle the case where the input string is like "(t4)" where there is no 0 at the beginning. Finally is the removal of the front parenthesis.&lt;/p&gt;
    &lt;p&gt;This optimization yielded a surprising speedup (several percent overall) due to how often the case of "0(reg)" shows up in RISC-V assembly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Call small lambda functions frequently&lt;/head&gt;
    &lt;p&gt;These three lambda functions both help make parsing faster and simplify the code:&lt;/p&gt;
    &lt;code&gt;auto is_whitespace = [](const char&amp;amp; c) {
    return c == '\t' || c == ' ';
};
auto ch = [&amp;amp;]() {
    return data[i];
};
auto not_at_end = [](const char&amp;amp; c) {
    return c != '\n' &amp;amp;&amp;amp; c != '#';
};
&lt;/code&gt;
    &lt;p&gt;Why do they work? The simplification part is obvious, but maybe not for speed. One reason might be because the compiler now knows how often we do the same comparisons over and over. If it knows we do the same thing many times, it can optimize with that known fact.&lt;/p&gt;
    &lt;p&gt;Also note how the first and last functions violate the earlier optimization trick regarding passing fundamental types by value. That trick does not entirely apply to lambda functions, which work differently, where they could be inline and incur zero function call overhead. Passing by reference enables the zero function call overhead optimization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Strip out the compilation junk&lt;/head&gt;
    &lt;p&gt;By default, C++ compilers like GCC and Clang add in a lot of junk that we can safely strip out. Here's how we do it in CMake:&lt;/p&gt;
    &lt;code&gt;target_compile_options(objultra PRIVATE -fno-rtti -fno-stack-protector -fomit-frame-pointer)
&lt;/code&gt;
    &lt;head rend="h3"&gt;-fno-rtti&lt;/head&gt;
    &lt;p&gt;RTTI is runtime type identification. Only some software uses this feature but it adds nonzero overhead to all. Therefore, we disable it to eliminate that overhead.&lt;/p&gt;
    &lt;head rend="h3"&gt;-fno-stack-protector&lt;/head&gt;
    &lt;p&gt;The stack protector is a feature that many Programming Furus©️®️™️ peddle to improve security. However, it adds considerable overhead, and does nothing for security outside of a specific attack. Therefore, we disable it to eliminate that overhead.&lt;/p&gt;
    &lt;head rend="h3"&gt;-fomit-frame-pointer&lt;/head&gt;
    &lt;p&gt;The frame pointer is a specific feature on some CPU platforms (like x86). However, it's not actually needed anymore for modern CPUs, and it adds overhead. Therefore, we disable it to eliminate that overhead.&lt;/p&gt;
    &lt;head rend="h2"&gt;Link-time optimization&lt;/head&gt;
    &lt;p&gt;Link-time optimization, or LTO, is a more intelligent way for the compiler to optimize your code than regular optimization passes. It can enable some serious speedups if your code benefits from function inlining or has code across many files. It's been supported for a while now but isn't enabled by default. Here's how to enable it in CMake:&lt;/p&gt;
    &lt;code&gt;include(CheckIPOSupported)
check_ipo_supported(RESULT lto_supported)
if(lto_supported AND NOT NO_LTO)
  set_property(TARGET ${this_target} PROPERTY INTERPROCEDURAL_OPTIMIZATION TRUE)
  if(CMAKE_COMPILER_IS_GNUCXX)
    list(APPEND CMAKE_CXX_COMPILE_OPTIONS_IPO "-flto=auto") # set the thread amount to what is available on the CPU
  endif()
endif()
&lt;/code&gt;
    &lt;p&gt;This has been nothing but a benefit for Ultrassembler.&lt;/p&gt;
    &lt;head rend="h2"&gt;Make structs memory-friendly&lt;/head&gt;
    &lt;p&gt;This struct holds variables that most of the Ultrassembler code uses:&lt;/p&gt;
    &lt;code&gt;struct assembly_context {
    ultrastring inst;
    ultrastring arg1;
    ultrastring arg2;
    ultrastring arg3;
    ultrastring arg4;
    ultrastring arg5;
    ultrastring arg6;
    ultrastring arg_extra;
    ultravector&amp;lt;uint8_t&amp;gt; machine_code;
    ultravector&amp;lt;RVInstructionSet&amp;gt; supported_sets;
    ultravector&amp;lt;std::pair&amp;lt;ultrastring, int&amp;gt;&amp;gt; labels;
    ultravector&amp;lt;label_loc&amp;gt; label_locs;
    ultravector&amp;lt;std::pair&amp;lt;ultrastring, ultrastring&amp;gt;&amp;gt; constants;
    ultravector&amp;lt;directive_options&amp;gt; options;
    int32_t custom_inst = 0;
    uint32_t line = 1;
    uint32_t column = 0;
    uint16_t inst_offset = 0;
};
&lt;/code&gt;
    &lt;p&gt;We order them in descending memory size, from 32 bytes for &lt;code&gt;ultrastring&lt;/code&gt; to 2 for &lt;code&gt;uint16_t&lt;/code&gt;. This packs the members the most efficient way possible for memory usage.&lt;/p&gt;
    &lt;p&gt;Also, these variables are not in the global scope or a namespace because holding them all in a struct enables multithreaded operation. It would be possible to add &lt;code&gt;thread_local&lt;/code&gt; to each one to enable multithreading easily, but in testing, this added enormous overhead compared to a plain old struct.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory locality&lt;/head&gt;
    &lt;p&gt;Memory locality is the general idea that the most frequently accessed memory should be close together. Ultrassembler has many such cases, and we already help ensure memory locality through preallocated memory pools. We go further by ensuring sections of code which frequently work on one area of memory get their own space to work with.&lt;/p&gt;
    &lt;p&gt;Here's an example:&lt;/p&gt;
    &lt;code&gt;void make_inst(assembly_context&amp;amp; c) {
    // boilerplate

    uint32_t inst = 0;

    // code which modifies this inst variable

    reinterpret_cast&amp;lt;uint32_t&amp;amp;&amp;gt;(c.machine_code[c.machine_code.size() - bytes]) = inst;
}
&lt;/code&gt;
    &lt;p&gt;We work on the local &lt;code&gt;inst&lt;/code&gt; variable to prevent far reaches across memory to the &lt;code&gt;c.machine_code&lt;/code&gt; vector. When we're done, we write to &lt;code&gt;c.machine_code&lt;/code&gt; once and invoke only one far memory access as a result.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Congrats if you read all the way here!&lt;/p&gt;
    &lt;p&gt;Hopefully you've learned something new and/or useful. Although I've crafted the optimizations here for Ultrassembler, there's nothing stopping you from applying the same underlying principles to your own code.&lt;/p&gt;
    &lt;p&gt;Check out Ultrassembler: https://github.com/Slackadays/Chata/tree/main/ultrassembler&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jghuff.com/articles/ultrassembler-so-fast/"/></entry><entry><id>https://news.ycombinator.com/item?id=45085318</id><title>Code Is Debt</title><updated>2025-08-31T19:07:09.299690+00:00</updated><content>&lt;doc fingerprint="2670c054ff7f05fc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Code is Debt&lt;/head&gt;
    &lt;p&gt;“Tornike, what do you think of AI coding tools?”&lt;/p&gt;
    &lt;p&gt;I like to answer this frequent question by way of an example. An example of two companies. It goes something like this:&lt;/p&gt;
    &lt;p&gt;Imagine two very similar companies. Both companies generate similar revenue and produce a similar software product. The only difference between these companies is that Company A uses 1 million lines of code and Company B uses 100 thousand lines of code. Which company is better off?&lt;/p&gt;
    &lt;p&gt;Clearly, the company with fewer lines of code is better off. They have fewer lines of code and so they can understand and modify their code more quickly. All other things being equal, less code is better. Put another way code is a form of debt. If you use an AI to generate code, you are effectively getting a debt – a debt of code.&lt;/p&gt;
    &lt;p&gt;Is it worth going into code debt? It depends. Debt can be both good or bad, it might have interest or be interest-free. Debt can also allow faster growth or it can cause your project to implode. In all cases it is important to have easy access to these debt-generating tools. It is still up to you to generate the code debt responsibly.&lt;/p&gt;
    &lt;p&gt;Thanks to Ani Talakhadze for reading drafts of this&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tornikeo.com/code-is-debt/"/></entry><entry><id>https://news.ycombinator.com/item?id=45085373</id><title>Mark Zuckerberg's AI dream team seems to be falling apart</title><updated>2025-08-31T19:07:09.065828+00:00</updated><content>&lt;doc fingerprint="ecfcf729aa82476b"&gt;
  &lt;main&gt;
    &lt;p&gt;Within days of joining Meta, Shengjia Zhao, co-creator of OpenAI’s ChatGPT, had threatened to quit and return to his former employer, in a blow to Mark Zuckerberg’s multibillion-dollar push to build “personal superintelligence.”&lt;/p&gt;
    &lt;p&gt;Zhao went as far as to sign employment paperwork to go back to OpenAI. Shortly afterwards, according to four people familiar with the matter, he was given the title of Meta’s new “chief AI scientist.”&lt;/p&gt;
    &lt;p&gt;The incident underscores Zuckerberg’s turbulent effort to direct the most dramatic reorganisation of Meta’s senior leadership in the group’s 20-year history.&lt;/p&gt;
    &lt;p&gt;One of the few remaining Big Tech founder-CEOs, Zuckerberg has relied on longtime acolytes such as Chief Product Officer Chris Cox to head up his favored departments and build out his upper ranks.&lt;/p&gt;
    &lt;p&gt;But in the battle to dominate AI, the billionaire is shifting towards a new and recently hired generation of executives, including Zhao, former Scale AI CEO Alexandr Wang, and former GitHub chief Nat Friedman.&lt;/p&gt;
    &lt;p&gt;Current staff are adapting to the reinvention of Meta’s AI efforts as the newcomers seek to flex their power while adjusting to the idiosyncrasies of working within a sprawling $1.95 trillion giant with a hands-on chief executive.&lt;/p&gt;
    &lt;p&gt;“There’s a lot of big men on campus,” said one investor who is close with some of Meta’s new AI leaders.&lt;/p&gt;
    &lt;p&gt;Adding to the tumult, a handful of new AI staff have already decided to leave after brief tenures, according to people familiar with the matter.&lt;/p&gt;
    &lt;p&gt;This includes Ethan Knight, a machine-learning scientist who joined the company weeks ago. Another, Avi Verma, a former OpenAI researcher, went through Meta’s onboarding process but never showed up for his first day, according to a person familiar with the matter.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/ai/2025/08/zuckerbergs-ai-hires-disrupt-meta-with-swift-exits-and-threats-to-leave/"/></entry><entry><id>https://news.ycombinator.com/item?id=45085446</id><title>How many HTTP requests/second can a Single Machine handle?</title><updated>2025-08-31T19:07:08.881954+00:00</updated><content>&lt;doc fingerprint="ad6a4e67650f80f4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Load Testing: how many HTTP requests/second can a Single Machine handle?&lt;/head&gt;
    &lt;p&gt;When designing systems and deciding on the architecture, I often hear justifying the use of microservices and other complex solutions because of the predicted performance and scalability needs. Out of curiosity then, let's test the limits of an extremely simple approach, the simplest possible one. Let's test a single instance of an application, with a single instance of a database, deployed to a single machine, and answer the question:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How many HTTP requests per second can a Single Machine handle?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Tests setup&lt;/head&gt;
    &lt;p&gt;To resemble real-world use cases as much as possible, we have the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Java 21-based REST API built with Spring Boot 3 and using Virtual Threads&lt;/item&gt;
      &lt;item&gt;PostgreSQL as a database, loaded with over one million rows of data&lt;/item&gt;
      &lt;item&gt;External volume for the database - it does not write to the local file system (we use DigitalOcean Block Storage)&lt;/item&gt;
      &lt;item&gt;Realistic load characteristics: tests consist primarily of read requests with approximately 20% of writes. They call our REST API which makes use of the PostgreSQL database with a reasonable amount of data (over one million rows)&lt;/item&gt;
      &lt;item&gt;Single Machine in a few versions:&lt;list rend="ul"&gt;&lt;item&gt;1 CPU, 2 GB of memory&lt;/item&gt;&lt;item&gt;2 CPUs, 4 GB of memory&lt;/item&gt;&lt;item&gt;4 CPUs, 8 GB of memory&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Single LoadTest.java file as a testing tool - we run it on 4 test machines, in parallel, since we usually have many http clients, not just one&lt;/item&gt;
      &lt;item&gt;Everything built and running in Docker&lt;/item&gt;
      &lt;item&gt;DigitalOcean as our infrastructure provider&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;lb/&gt;Whole infrastructure setup is automated by one Python script; it is extremely easy to run:&lt;/p&gt;
    &lt;code&gt;bash setup_python_env.bash
source venv/bin/activate
export DO_API_TOKEN=&amp;lt;your DigitalOcean API token&amp;gt;
export SSH_KEY_FINGERPRINT=&amp;lt;your ssh key fingerprint uploaded to DigitalOcean; it gives access to created machines&amp;gt;
python3 prepare_infra.py &amp;lt;machine size: small, medium, large&amp;gt;
&lt;/code&gt;
    &lt;p&gt;&lt;lb/&gt;In the database, we have one table with the following schema:&lt;/p&gt;
    &lt;code&gt;CREATE TABLE account (
  id UUID PRIMARY KEY,
  name TEXT NOT NULL,
  email TEXT UNIQUE,
  created_at TIMESTAMP NOT NULL DEFAULT NOW(),
  version BIGINT NOT NULL
);

CREATE INDEX account_name ON account(name);
&lt;/code&gt;
    &lt;p&gt;We call &lt;code&gt;POST: /accounts/generate-test-data&lt;/code&gt; endpoint to generate &lt;code&gt;random 1_250_000 rows&lt;/code&gt; of it.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;LoadTest&lt;/code&gt; calls the following endpoints:&lt;/p&gt;
    &lt;code&gt;GET: /accounts/{id}
GET: /accounts/count?name={name}
// randomly executes either insert or delete
POST: /accounts/execute-random-write
&lt;/code&gt;
    &lt;p&gt;For &lt;code&gt;GET: /accounts/{id}&lt;/code&gt;, we will see some responses with 404 status as we sometimes try to GET nonexistent accounts.&lt;/p&gt;
    &lt;p&gt;To make it even more realistic, there is a simple security mechanism. For all requests, we require a secret value in the query string (SecurityFilter.java):&lt;/p&gt;
    &lt;code&gt;...
// Keep in sync with LoadTest!
private static final String SECRET_QUERY_STRING = "17e57c8c-60ea-4b4a-8d48-5967f03b942c";
private static final Logger log = LoggerFactory.getLogger(SecurityFilter.class);

...

var authorized = Optional.ofNullable(httpRequest.getQueryString())
  .map(q -&amp;gt; q.contains(SECRET_QUERY_STRING))
  .orElse(false);

if (authorized) {
  // pass
} else {
  log.warn("Somebody tried to poke around! Their request:");
  log.warn("Method: {}, url: {}, query: {}", httpRequest.getMethod(), httpRequest.getRequestURI(), httpRequest.getQueryString());
  var httpResponse = (HttpServletResponse) response;
  httpResponse.setStatus(404);
  httpResponse.getWriter().write("Don't know anything about it");
}
...
&lt;/code&gt;
    &lt;p&gt;&lt;lb/&gt;Having all these details in mind, let's run some tests and examine the results!&lt;/p&gt;
    &lt;head rend="h2"&gt;Test results&lt;/head&gt;
    &lt;p&gt;All tests were run on 4 test machines, in parallel, with 2 CPUs and 2 GB of memory, on the DigitalOcean infrastructure. Most tests took ~ 15 seconds: every second, for 15 seconds, a certain number of requests was issued.&lt;/p&gt;
    &lt;p&gt;Tests were mainly executed in four profiles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;low_load: 20 requests per second - 4 machines x 5 RPS&lt;/item&gt;
      &lt;item&gt;average_load: 200 requests per second - 4 machines x 50 RPS&lt;/item&gt;
      &lt;item&gt;high_load: 1000 requests per second - 4 machines x 250 RPS&lt;/item&gt;
      &lt;item&gt;very_high_load: 4000 requests per second - 4 machines x 1000 RPS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To test sustained load and see whether we experience a performance degradation, I have also run a few long variations of these profiles for ~ 10 minutes: every second, for 600 seconds, a certain number of requests was issued.&lt;/p&gt;
    &lt;p&gt;All test results shown below come from 1 test machine. Therefore, we need to multiply the request rate by 4, as tests were always run on 4 machines in parallel.&lt;/p&gt;
    &lt;head rend="h3"&gt;Small machine - 1 CPU, 2 GB of memory&lt;/head&gt;
    &lt;p&gt;low_load: not worth showing, since average load performed so well.&lt;/p&gt;
    &lt;p&gt;average_load:&lt;/p&gt;
    &lt;code&gt;...

750 requests with 50 per second rate took PT15.303S

...

Tests executed on: 4 machines, in parallel
Executed requests on 1 machine: 750, with 50/s rate
Requests with connect timeout [5000]: 0, as percentage: 0
Requests with request timeout [5000]: 0, as percentage: 0

Min: 0.002 s
Max: 0.153 s
Mean: 0.01 s

Percentile 10: 0.003 s
Percentile 25: 0.004 s
Percentile 50 (Median): 0.007 s
Percentile 75: 0.012 s
Percentile 90: 0.019 s
Percentile 95: 0.029 s
Percentile 99: 0.06 s
Percentile 999: 0.153 s

...

POST: /accounts/execute-random-write
Requests: 138, which is 18% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=138}

...

GET: /accounts/{id}
Requests: 324, which is 43% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {404=152, 200=172}

...

GET: /accounts/count?name={name}
Requests: 288, which is 38% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=288}

...
&lt;/code&gt;
    &lt;p&gt;To check whether performance does not decrease over time, I have also run an average_long_load test:&lt;/p&gt;
    &lt;code&gt;...

30000 requests with 50 per second rate took PT10M0.605S

...

Tests executed on: 4 machines, in parallel
Executed requests on 1 machine: 30000, with 50/s rate
Requests with connect timeout [5000]: 0, as percentage: 0
Requests with request timeout [5000]: 0, as percentage: 0

Min: 0.0 s
Max: 0.11 s
Mean: 0.002 s

Percentile 10: 0.001 s
Percentile 25: 0.002 s
Percentile 50 (Median): 0.002 s
Percentile 75: 0.003 s
Percentile 90: 0.004 s
Percentile 95: 0.005 s
Percentile 99: 0.008 s
Percentile 999: 0.016 s

...

POST: /accounts/execute-random-write
Requests: 6050, which is 20% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=6050}

...

GET: /accounts/{id}
Requests: 11940, which is 40% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {404=5972, 200=5968}

...

GET: /accounts/count?name={name}
Requests: 12010, which is 40% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=12010}

...
&lt;/code&gt;
    &lt;p&gt;As we can see, no issues there.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;high_load:&lt;/p&gt;
    &lt;code&gt;...

3750 requests with 250 per second rate took PT15.371S

...

Tests executed on: 4 machines, in parallel
Executed requests on 1 machine: 3750, with 250/s rate
Requests with connect timeout [5000]: 0, as percentage: 0
Requests with request timeout [5000]: 0, as percentage: 0

Min: 0.001 s
Max: 0.2 s
Mean: 0.013 s

Percentile 10: 0.003 s
Percentile 25: 0.005 s
Percentile 50 (Median): 0.009 s
Percentile 75: 0.017 s
Percentile 90: 0.026 s
Percentile 95: 0.034 s
Percentile 99: 0.099 s
Percentile 999: 0.157 s

...

POST: /accounts/execute-random-write
Requests: 753, which is 20% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=753}

...

GET: /accounts/{id}
Requests: 1483, which is 40% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {404=750, 200=733}

...

GET: /accounts/count?name={name}
Requests: 1514, which is 40% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=1514}

...
&lt;/code&gt;
    &lt;p&gt;&lt;lb/&gt;very_high_load - this is where a small machine reached its limits, we got many timeouts:&lt;/p&gt;
    &lt;code&gt;...

15000 requests with 1000 per second rate took PT25.557S

...

Tests executed on: 4 machines, in parallel
Executed requests on 1 machine: 15000, with 1000/s rate
Requests with connect timeout [5000]: 4215, as percentage: 28
Requests with request timeout [5000]: 7730, as percentage: 51

Min: 0.007 s
Max: 5.052 s
Mean: 4.413 s

Percentile 10: 1.861 s
Percentile 25: 4.999 s
Percentile 50 (Median): 5.0 s
Percentile 75: 5.0 s
Percentile 90: 5.001 s
Percentile 95: 5.001 s
Percentile 99: 5.012 s
Percentile 999: 5.037 s

...

POST: /accounts/execute-random-write
Requests: 2974, which is 20% of all requests
Connect timeouts: 834
Request timeouts: 1554
Requests by status: {200=586}

...

GET: /accounts/{id}
Requests: 6088, which is 41% of all requests
Connect timeouts: 1730
Request timeouts: 3152
Requests by status: {404=599, 200=607}

...

GET: /accounts/count?name={name}
Requests: 5938, which is 40% of all requests
Connect timeouts: 1651
Request timeouts: 3024
Requests by status: {200=1263}

...
&lt;/code&gt;
    &lt;head rend="h3"&gt;Medium machine - 2 CPUs, 4 GB of memory&lt;/head&gt;
    &lt;p&gt;high_load:&lt;/p&gt;
    &lt;code&gt;...

3750 requests with 250 per second rate took PT15.336S

...

Tests executed on: 4 machines, in parallel
Executed requests on 1 machine: 3750, with 250/s rate
Requests with connect timeout [5000]: 0, as percentage: 0
Requests with request timeout [5000]: 0, as percentage: 0

Min: 0.001 s
Max: 0.135 s
Mean: 0.004 s

Percentile 10: 0.002 s
Percentile 25: 0.002 s
Percentile 50 (Median): 0.003 s
Percentile 75: 0.005 s
Percentile 90: 0.007 s
Percentile 95: 0.01 s
Percentile 99: 0.023 s
Percentile 999: 0.072 s

...

POST: /accounts/execute-random-write
Requests: 772, which is 21% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=772}

...

GET: /accounts/{id}
Requests: 1457, which is 39% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {404=706, 200=751}

...

GET: /accounts/count?name={name}
Requests: 1521, which is 41% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=1521}

...
&lt;/code&gt;
    &lt;p&gt;Again, to see whether we can sustain this load over a longer period of time I have executed a high_long_load test:&lt;/p&gt;
    &lt;code&gt;...

150000 requests with 250 per second rate took PT10M0.701S

...

Tests executed on: 4 machines, in parallel
Executed requests on 1 machine: 150000, with 250/s rate
Requests with connect timeout [5000]: 0, as percentage: 0
Requests with request timeout [5000]: 0, as percentage: 0

Min: 0.0 s
Max: 0.217 s
Mean: 0.003 s

Percentile 10: 0.001 s
Percentile 25: 0.002 s
Percentile 50 (Median): 0.002 s
Percentile 75: 0.004 s
Percentile 90: 0.005 s
Percentile 95: 0.007 s
Percentile 99: 0.018 s
Percentile 999: 0.129 s

...

POST: /accounts/execute-random-write
Requests: 30277, which is 20% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=30277}

...

GET: /accounts/{id}
Requests: 59880, which is 40% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {404=30005, 200=29875}

...

GET: /accounts/count?name={name}
Requests: 59843, which is 40% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=59843}

...
&lt;/code&gt;
    &lt;p&gt;No problems there.&lt;/p&gt;
    &lt;p&gt;very_high_load - it also failed there, but notice much better times and significantly fewer timeouts:&lt;/p&gt;
    &lt;code&gt;...

15000 requests with 1000 per second rate took PT22.588S

...

Tests executed on: 4 machines, in parallel
Executed requests on 1 machine: 15000, with 1000/s rate
Requests with connect timeout [5000]: 1896, as percentage: 12
Requests with request timeout [5000]: 68, as percentage: 0

Min: 0.008 s
Max: 5.032 s
Mean: 1.97 s

Percentile 10: 0.21 s
Percentile 25: 0.437 s
Percentile 50 (Median): 1.038 s
Percentile 75: 4.125 s
Percentile 90: 5.0 s
Percentile 95: 5.0 s
Percentile 99: 5.001 s
Percentile 999: 5.018 s

...

POST: /accounts/execute-random-write
Requests: 2998, which is 20% of all requests
Connect timeouts: 348
Request timeouts: 16
Requests by status: {200=2634}

...

GET: /accounts/{id}
Requests: 6019, which is 40% of all requests
Connect timeouts: 767
Request timeouts: 28
Requests by status: {404=2640, 200=2584}

...

GET: /accounts/count?name={name}
Requests: 5983, which is 40% of all requests
Connect timeouts: 781
Request timeouts: 24
Requests by status: {200=5178}

...
&lt;/code&gt;
    &lt;head rend="h3"&gt;Large machine - 4 CPUs, 8 GB of memory&lt;/head&gt;
    &lt;p&gt;very_high_load:&lt;/p&gt;
    &lt;code&gt;...

15000 requests with 1000 per second rate took PT15.32S

...

Tests executed on: 4 machines, in parallel
Executed requests on 1 machine: 15000, with 1000/s rate
Requests with connect timeout [5000]: 0, as percentage: 0
Requests with request timeout [5000]: 0, as percentage: 0

Min: 0.0 s
Max: 1.05 s
Mean: 0.058 s

Percentile 10: 0.002 s
Percentile 25: 0.002 s
Percentile 50 (Median): 0.005 s
Percentile 75: 0.053 s
Percentile 90: 0.124 s
Percentile 95: 0.353 s
Percentile 99: 0.746 s
Percentile 999: 0.879 s

...

POST: /accounts/execute-random-write
Requests: 3047, which is 20% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=3047}

...

GET: /accounts/{id}
Requests: 6047, which is 40% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {404=2982, 200=3065}

...

GET: /accounts/count?name={name}
Requests: 5906, which is 39% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=5906}

...
&lt;/code&gt;
    &lt;p&gt;&lt;lb/&gt;Last results seemed a little too good to be true, so to double-check, I decided to repeat this test, but for a longer time period. very_high_long_load results:&lt;/p&gt;
    &lt;code&gt;...

600000 requests with 1000 per second rate took PT10M11.923S

...

Tests executed on: 4 machines, in parallel
Executed requests on 1 machine: 600000, with 1000/s rate
Requests with connect timeout [5000]: 5197, as percentage: 0
Requests with request timeout [5000]: 10180, as percentage: 1

Min: 0.0 s
Max: 5.05 s
Mean: 0.158 s

Percentile 10: 0.002 s
Percentile 25: 0.002 s
Percentile 50 (Median): 0.004 s
Percentile 75: 0.01 s
Percentile 90: 0.033 s
Percentile 95: 0.132 s
Percentile 99: 5.0 s
Percentile 999: 5.002 s

...

POST: /accounts/execute-random-write
Requests: 119950, which is 20% of all requests
Connect timeouts: 1065
Request timeouts: 2108
Requests by status: {200=116777}

...

GET: /accounts/{id}
Requests: 240329, which is 40% of all requests
Connect timeouts: 2117
Request timeouts: 4045
Requests by status: {404=117141, 200=117026}

...

GET: /accounts/count?name={name}
Requests: 239721, which is 40% of all requests
Connect timeouts: 2015
Request timeouts: 4027
Requests by status: {200=233679}

...
&lt;/code&gt;
    &lt;p&gt;As we can see, there is a performance degradation. Times are still very good, but we had more than 1% of timeouts; we only made it in 95 percentile, not 999 as previously. We most likely are on the edge of running out of resources and probably need to reduce this load from 4000 RPS to 2000 - 3000 RPS to make it sustainable for a longer period of time. Of course, I did exactly that; here are the results of a 3000 RPS test running for 10 minutes:&lt;/p&gt;
    &lt;code&gt;...

450000 requests with 750 per second rate took PT10M30.998S

...

Tests executed on: 4 machines, in parallel
Executed requests on 1 machine: 450000, with 750/s rate
Requests with connect timeout [5000]: 0, as percentage: 0
Requests with request timeout [5000]: 0, as percentage: 0

Min: 0.0 s
Max: 1.921 s
Mean: 0.016 s

Percentile 10: 0.001 s
Percentile 25: 0.001 s
Percentile 50 (Median): 0.002 s
Percentile 75: 0.003 s
Percentile 90: 0.004 s
Percentile 95: 0.006 s
Percentile 99: 0.616 s
Percentile 999: 1.419 s

...

POST: /accounts/execute-random-write
Requests: 89991, which is 20% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=89991}

...

GET: /accounts/{id}
Requests: 180012, which is 40% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {404=90140, 200=89872}

...

GET: /accounts/count?name={name}
Requests: 179997, which is 40% of all requests
Connect timeouts: 0
Request timeouts: 0
Requests by status: {200=179997}

...
&lt;/code&gt;
    &lt;p&gt;No timeouts, 99 percentile under 1 second and 999 percentile under 1.5, which is amazing. Out of curiosity, I have also pulled out some cpu/memory stats from Docker (we have 4 CPUs, so 400% CPU is available):&lt;/p&gt;
    &lt;code&gt;...

Date: 2024-03-25T16:51:11Z
CONTAINER ID   NAME         CPU %     MEM USAGE / LIMIT     MEM %     NET I/O   BLOCK I/O     PIDS
accd79ae0eb8   single-app   77.67%    652.4MiB / 7.763GiB   8.21%     0B / 0B   0B / 6.5MB    38
2dbb4ee8610e   single-db    47.62%    595.3MiB / 7.763GiB   7.49%     0B / 0B   0B / 16.6GB   20

Date: 2024-03-25T16:51:34Z
CONTAINER ID   NAME         CPU %     MEM USAGE / LIMIT     MEM %     NET I/O   BLOCK I/O     PIDS
accd79ae0eb8   single-app   83.01%    652.7MiB / 7.763GiB   8.21%     0B / 0B   0B / 6.55MB   38
2dbb4ee8610e   single-db    57.63%    595.5MiB / 7.763GiB   7.49%     0B / 0B   0B / 16.8GB   20

...

Date: 2024-03-25T16:53:49Z
CONTAINER ID   NAME         CPU %     MEM USAGE / LIMIT     MEM %     NET I/O   BLOCK I/O     PIDS
accd79ae0eb8   single-app   101.57%   652.5MiB / 7.763GiB   8.21%     0B / 0B   0B / 6.89MB   38
2dbb4ee8610e   single-db    70.83%    600.4MiB / 7.763GiB   7.55%     0B / 0B   0B / 17.7GB   21

Date: 2024-03-25T16:54:12Z
CONTAINER ID   NAME         CPU %     MEM USAGE / LIMIT     MEM %     NET I/O   BLOCK I/O     PIDS
accd79ae0eb8   single-app   53.55%    652.4MiB / 7.763GiB   8.21%     0B / 0B   0B / 6.93MB   38
2dbb4ee8610e   single-db    37.06%    599.3MiB / 7.763GiB   7.54%     0B / 0B   0B / 17.9GB   20

...
&lt;/code&gt;
    &lt;head rend="h2"&gt;Summing it up&lt;/head&gt;
    &lt;p&gt;As we have seen, a single machine, with a single database, can handle a lot - way more than most of us will ever need. Here is a summary of the test results:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Small machine - 1 CPU, 2 GB of memory&lt;list rend="ul"&gt;&lt;item&gt;Can handle sustained load of 200 - 300 RPS&lt;/item&gt;&lt;item&gt;For 15 seconds, it was able to handle 1000 RPS with stats:&lt;list rend="ul"&gt;&lt;item&gt;Min: 0.001s, Max: 0.2s, Mean: 0.013s&lt;/item&gt;&lt;item&gt;Percentile 90: 0.026s, Percentile 95: 0.034s&lt;/item&gt;&lt;item&gt;Percentile 99: 0.099s&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Medium machine - 2 CPUs, 4 GB of memory&lt;list rend="ul"&gt;&lt;item&gt;Can handle sustained load of 500 - 1000 RPS&lt;/item&gt;&lt;item&gt;For 15 seconds, it was able to handle 1000 RPS with stats:&lt;list rend="ul"&gt;&lt;item&gt;Min: 0.001s, Max: 0.135s, Mean: 0.004s&lt;/item&gt;&lt;item&gt;Percentile 90: 0.007s, Percentile 95: 0.01s&lt;/item&gt;&lt;item&gt;Percentile 99: 0.023s&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Large machine - 4 CPUs, 8 GB of memory&lt;list rend="ul"&gt;&lt;item&gt;Can handle sustained load of 2000 - 3000 RPS&lt;/item&gt;&lt;item&gt;For 15 seconds, it was able to handle 4000 RPS with stats:&lt;list rend="ul"&gt;&lt;item&gt;Min: 0.0s, (less than 1ms), Max: 1.05s, Mean: 0.058s&lt;/item&gt;&lt;item&gt;Percentile 90: 0.124s, Percentile 95: 0.353s&lt;/item&gt;&lt;item&gt;Percentile 99: 0.746s&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Huge machine - 8 CPUs, 16 GB of memory (not tested)&lt;list rend="ul"&gt;&lt;item&gt;Most likely can handle sustained load of 4000 - 6000 RPS&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;lb/&gt;Of course, there are other, non-performance related, reasons for having more than one machine - mostly associated with resilience and redundancy in case of failures. Nevertheless, remember these results the next time someone tries to persuade you into implementing a complex solution, architecture and infrastructure, for a system expected to handle at most 5 requests per second.&lt;/p&gt;
    &lt;p&gt;Keep things simple!&lt;/p&gt;
    &lt;head rend="h3"&gt;Links&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Related video on my YouTube channel: https://www.youtube.com/watch?v=NsdDIBll-Lw&lt;/item&gt;
      &lt;item&gt;Source code, so you can experiment and run tests on your own: https://github.com/BinaryIgor/code-examples/tree/master/single-machine-tests&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://binaryigor.com/how-many-http-requests-can-a-single-machine-handle.html"/></entry></feed>