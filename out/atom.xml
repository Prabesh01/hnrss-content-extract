<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-17T14:10:01.111541+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46648885</id><title>Dell UltraSharp 52 Thunderbolt Hub Monitor</title><updated>2026-01-17T14:10:09.555510+00:00</updated><content>&lt;doc fingerprint="e22dab5494a79c42"&gt;
  &lt;main&gt;&lt;p&gt;Selecting will change the following options:&lt;/p&gt;&lt;p&gt;From To&lt;/p&gt;&lt;p&gt;51.5"&lt;/p&gt;&lt;p&gt;6144 x 2560 at 120Hz&lt;/p&gt;&lt;p&gt;In-plane Switching (IPS) Black Technology&lt;/p&gt;&lt;p&gt;99% DCI-P3 (CIE 1976)&lt;/p&gt;&lt;p&gt;100% sRGB (CIE 1931)&lt;/p&gt;...See More See More Color Gamut&lt;p&gt;2 HDMI port/s (HDCP 2.2) (Supports up to 6144 x 2560, 120 Hz, VRR, , as specified in HDMI 2.1 (FRL))&lt;/p&gt;&lt;p&gt;2 DisplayPort 1.4 (HDCP 2.2) port/s&lt;/p&gt;...See More See More Ports&lt;p&gt;Add the products you would like to compare, and quickly determine which is best for your needs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dell.com/en-us/shop/dell-ultrasharp-52-thunderbolt-hub-monitor-u5226kw/apd/210-bthw/monitors-monitor-accessories"/><published>2026-01-16T17:14:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46648916</id><title>East Germany balloon escape</title><updated>2026-01-17T14:10:08.800634+00:00</updated><content>&lt;doc fingerprint="be46c17b47664bf8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;East Germany balloon escape&lt;/head&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Native name&lt;/cell&gt;&lt;cell&gt;Die Ballonflucht&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Date&lt;/cell&gt;&lt;cell&gt;16 September 1979&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Time&lt;/cell&gt;&lt;cell&gt;02:00 am (approximate)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Duration&lt;/cell&gt;&lt;cell&gt;25 minutes&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Location&lt;/cell&gt;&lt;cell&gt;Oberlemnitz, East Germany&lt;p&gt;(takeoff)&lt;/p&gt;&lt;p&gt;Naila, West Germany&lt;/p&gt;&lt;p&gt;(landing)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Coordinates&lt;/cell&gt;&lt;cell&gt;50°28′59″N 11°35′29″E / 50.48306°N 11.59139°E[1]&lt;p&gt;(takeoff)&lt;/p&gt;&lt;p&gt;50°19′52.7″N 11°40′13.1″E / 50.331306°N 11.670306°E[1]&lt;/p&gt;&lt;p&gt;(landing)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Organised by&lt;/cell&gt;&lt;cell&gt;Peter Strelzyk &amp;amp; family&lt;p&gt;Günter Wetzel &amp;amp; family&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Participants&lt;/cell&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Outcome&lt;/cell&gt;&lt;cell&gt;Successful escape to West Germany&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Non-fatal injuries&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;On 16 September 1979, eight people from two families escaped from East Germany by crossing the border into West Germany at night in a homemade hot air balloon. The unique feat was the result of over a year and a half of preparations involving three different balloons, various modifications, and a first, unsuccessful attempt. The failed attempt alerted the East German authorities to the plot, but the police were unable to identify the escapees before their second, successful flight two months later.&lt;/p&gt;&lt;head rend="h2"&gt;Background&lt;/head&gt;[edit]&lt;p&gt;East Germany, then part of the Eastern Bloc, was separated from West Germany in the Western Bloc by the inner German border and the Berlin Wall, which were heavily fortified with watchtowers, land mines, armed soldiers, and various other measures to prevent illegal crossings. East German border troops were instructed to prevent defection to West Germany by all means, including lethal force (Schießbefehl; "order to fire").[2]&lt;/p&gt;&lt;p&gt;Peter Strelzyk (1942–2017), an electrician and former East German Air Force mechanic, and Günter Wetzel (born 1955), a bricklayer by trade,[3] were colleagues at a local plastics factory.[4] Friends for four years, they shared a desire to flee the country and began discussing ways to get across the border. On 7 March 1978, they agreed to plan an escape.[5] They considered building a helicopter but quickly realized they would be unable to acquire an engine capable of powering such a craft. They then decided to explore the idea of constructing a hot air balloon,[6] having been inspired by a television program about ballooning.[3] An alternate account is that a relative shared a magazine article about the International Balloon Festival in Albuquerque, New Mexico.[5]&lt;/p&gt;&lt;head rend="h2"&gt;Construction&lt;/head&gt;[edit]&lt;p&gt;Strelzyk and Wetzel began research into balloons. Their plan was to escape with their wives and a total of four children (aged 2 to 15). They calculated the weight of the eight passengers and the craft itself to be around 750 kilograms (1,650 lb). Subsequent calculations determined a balloon capable of lifting this weight would need to hold 2,000 cubic metres (71,000 cu ft) of air heated to 100 °C (212 °F). The next calculation was the amount of material needed for the balloon, estimated to be 800 square metres (8,600 sq ft).[6]&lt;/p&gt;&lt;p&gt;The pair lived in Pößneck, a small town of about 20,000 where large quantities of cloth could not be obtained without raising attention. They tried neighbouring towns of Rudolstadt, Saalfeld, and Jena without success.[7] They travelled 50 km (31 mi) to Gera, where they purchased 1-metre-wide (3 ft 3 in) rolls of cotton cloth totalling 850 metres (2,790 ft) in length at a department store after telling the astonished clerk that they needed the large quantity of material to use as tent lining for their camping club.[6][7]&lt;/p&gt;&lt;p&gt;Wetzel spent two weeks sewing the cloth into a balloon-shaped bag, 15 metres (49 ft) wide by 20 metres (66 ft) long, on a 40-year-old manually operated sewing machine. Strelzyk spent the time building the gondola and burner assembly. The gondola was made from an iron frame, sheet metal floor, and clothesline run around the perimeter every 150 millimetres (5.9 in) for the sides. The burner was made using two 11-kilogram (24 lb) bottles of liquid propane household gas, hoses, water pipe, a nozzle, and a piece of stove pipe.[6]&lt;/p&gt;&lt;head rend="h2"&gt;First test&lt;/head&gt;[edit]&lt;p&gt;The team was ready to test the craft in April 1978. After days of searching, they found a suitable secluded forest clearing near Ziegenrück, 10 km (6.2 mi) from the border and 30 km (19 mi) from Pößneck. After lighting the burner one night, they failed to inflate the balloon. They thought the problem might stem from the fact that they had laid the balloon on the ground. After weeks of additional searching, they found a 25-metre (82 ft) cliff at a rock quarry where they could suspend the balloon vertically before inflation, but that also proved unsuccessful.[6]&lt;/p&gt;&lt;p&gt;The pair then decided to fill the bag with ambient-temperature air before using the burner to raise the air temperature and provide lift. They constructed a blower with a 14 hp (10 kW) 250 cc (15 cu in) motorcycle engine taken from Wetzel's old MZ, started with a Trabant automobile starter powered by jumper cables from Strelzyk's Moskvitch sedan.[8] This engine, silenced by a Trabant muffler, turned 1-metre-long (3.3 ft) fan blades to inflate the balloon. They also used a home-made flamethrower, similar to the gondola's burner, to pre-heat the air faster. With these modifications in place, they returned to the secluded clearing to try again but still could not inflate the balloon. But using the blower did allow them to discover that the cotton material with which they fashioned the balloon was too porous and leaked excessively.[6]&lt;/p&gt;&lt;p&gt;Their unsuccessful effort had cost them 2,400 DDM (US$360). Strelzyk disposed of the cloth by burning it in his furnace over several weeks.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Second test&lt;/head&gt;[edit]&lt;p&gt;Strelzyk and Wetzel purchased samples of different fabrics in local stores, including umbrella material and various samples of taffeta and nylon. They used an oven to test the material for heat resistance. In addition, they created a test rig from a vacuum cleaner and a water-filled glass tube to determine which material would allow the vacuum to exert the most suction on the water, and consequently which was the most impervious to air. The umbrella covering performed the best but was also the most expensive. They instead selected a synthetic kind of taffeta.[6]&lt;/p&gt;&lt;p&gt;To purchase a large quantity of fabric without arousing too much suspicion, the pair again drove to a distant city. This time they travelled over 160 kilometres (100 mi) to a department store in Leipzig. Their new cover story was that they belonged to a sailing club and needed the material to make sails. The quantity they needed had to be ordered, and although they feared the purchase might be reported to East Germany's State Security Service (Stasi), they returned the next day and picked up the material without incident. They paid 4,800 DDM (US$720) for 800 metres (2,600 ft) of 1-metre-wide (3 ft 3 in) fabric.[6] On the way home, they also purchased an electric motor to speed up the pedal-operated sewing machine they had been using to sew the material into the desired balloon shape.[7]&lt;/p&gt;&lt;p&gt;Wetzel spent the next week sewing the material into another balloon, accomplishing the task faster the second time with the now-electric sewing machine. Soon afterwards, the two men returned to the forest clearing and inflated the bag in about five minutes using the blower and flame thrower. The bag rose and held air, but the burner on the gondola was not powerful enough to create the heat needed for lift. The pair continued experimenting for months, doubling the number of propane tanks and trying different fuel mixtures. Disappointed with the result, Wetzel decided to abandon the project and instead started to pursue the idea of building a small gasoline engine-powered light aeroplane[6] or a glider.[5]&lt;/p&gt;&lt;p&gt;Strelzyk continued trying to improve the burner. In June 1979, he discovered that with the propane tank inverted, additional pressure caused the liquid propane to evaporate, which produced a bigger flame. He modified the gondola to mount the propane tanks upside down, and returned to the test site where he found the new configuration produced a 12-metre (39 ft) long flame. Strelzyk was ready to attempt an escape.[6]&lt;/p&gt;&lt;head rend="h2"&gt;First escape attempt&lt;/head&gt;[edit]&lt;p&gt;On 3 July 1979, the weather and wind conditions were favourable. The entire Strelzyk family lifted from a forest clearing at 1:30 am and climbed at a rate of 4 metres (13 ft) per second. They reached an altitude of 2,000 metres (6,600 ft) according to an altimeter Strelzyk had made by modifying a barometer. A light wind was blowing them towards the border. The balloon then entered clouds, and atmospheric water vapour condensed on the balloon, adding weight which caused it to descend prematurely. The family landed safely approximately 180 metres (590 ft) short of the border, at the edge of the heavily mined border zone. Unsure of where they were, Strelzyk explored until he found a piece of litter – a bread bag from a bakery in Wernigerode, an East German town. The group spent nine hours carefully extricating themselves from the 500-metre (1,600 ft) wide border zone to avoid detection. They also had to travel unnoticed through a 5 km (3.1 mi) restricted zone before hiking back a total of 14 km (8.7 mi) to their car and the launch paraphernalia they had left behind.[6] They made it home just in time to report their absence from work and school was due to sickness.[7]&lt;/p&gt;&lt;p&gt;The abandoned balloon was discovered by the authorities later that morning. Strelzyk destroyed all compromising evidence and sold his car, fearing that it could link him to the escape attempt.[6] On 14 August, the Stasi launched an appeal to find the "perpetrator of a serious offence", listing in detail all the items recovered at the landing site.[9] Strelzyk felt that the Stasi would eventually trace the balloon to him and the Wetzels. He agreed with Wetzel that their best chance was to quickly build another balloon and get out as soon as possible.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Successful escape&lt;/head&gt;[edit]&lt;p&gt;Strelzyk and Wetzel decided to double the balloon's size to 4,000 cubic metres (140,000 cu ft) in volume, 20 metres (66 ft) in diameter, and 25 metres (82 ft) in height. They needed 1,250 square metres (13,500 sq ft) of taffeta, and purchased the material, in various colours and patterns, all over the country in order to escape suspicion. Wetzel sewed a third balloon, using over 6 kilometres (3.7 mi) of thread, and Strelzyk rebuilt everything else as before. In six weeks, they had prepared the 180-kilogram (400 lb) balloon and a payload of 550 kilograms (1,210 lb), including the gondola, equipment, and cargo (the two families). Confident in their calculations, they found the weather conditions right on 15 September, when a violent thunderstorm created the correct winds. The two families set off for the launch site in Strelzyk's replacement car (a Wartburg) and a moped. Arriving at 1:30 am, they needed just ten minutes to inflate the balloon and an additional three minutes to heat the air.[6]&lt;/p&gt;&lt;p&gt;Lifting off just after 2:00 am, the group failed to cut the tethers holding the gondola to the ground at the same time, tilting the balloon and sending the flame towards the fabric, which caught fire. After putting out the fire with an extinguisher brought along for just such an emergency, they climbed to 2,000 metres (6,600 ft) in nine minutes, drifting towards West Germany at 30 kilometres per hour (19 mph). The balloon flew for 28 minutes, with the temperature plummeting to −8 °C (18 °F) in the unsheltered gondola, which consisted solely of clothesline railing.&lt;/p&gt;&lt;p&gt;A design miscalculation resulted in the burner stovepipe being too long, causing the flame to be too high in the balloon, creating excessive pressure which caused the balloon to split. The air rushing out of the split extinguished the burner flame. Wetzel was able to re-light the flame with a match, and had to do so several more times before the group landed. At one point, they increased the flame to the maximum possible extent and rose to 2,500 metres (8,200 ft). They later learned they had been high enough to be detected, but not identified, on radar by West German air traffic controllers.[6] They had also been detected on the East German side by a night watchman at the district culture house in Bad Lobenstein. The report of an unidentified flying object heading toward the border caused guards to activate search lights, but the balloon was too high and out of reach of the lights.[10]&lt;/p&gt;&lt;p&gt;The tear in the balloon meant the group had to use the burner much more often, greatly limiting the distance it could travel. Wetzel later said he thought they could have travelled another 50 kilometres (31 mi) had the balloon remained intact. They made out the border crossing at Rudolphstein on the A9 and saw the search lights. When the propane ran out, they descended quickly, landing near the town of Naila, in the West German state of Bavaria and only 10 km (6 mi) from the border. The only injury was suffered by Wetzel, who broke his leg upon landing.[6] Various clues indicated to the families that the balloon had made it across the border. These included spotting red and yellow coloured lights, not common in East Germany,[3] and small farms, in contrast to the large state-run operations in the east. Another clue was modern farm equipment, unlike the older equipment used in East Germany.[11] Two Bavarian State Police officers saw the balloon's flickering light and headed to where they thought it would land. There they found Strelzyk and Wetzel, who first asked if they had made it to the West, although they noticed the police car was an Audi – another sign they were in West Germany. Upon learning they had, the escapees happily called for their families to join them.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Aftermath&lt;/head&gt;[edit]&lt;p&gt;East Germany immediately increased border security, closed all small airports close to the border, and ordered the planes kept farther inland.[6] Propane gas tanks became registered products, and large quantities of fabric suitable for balloon construction could no longer be purchased. Mail from East Germany to the two escaped families was prohibited.[12]&lt;/p&gt;&lt;p&gt;Erich Strelzyk learned of his brother's escape on the ZDF news and was arrested in his Potsdam apartment three hours after the landing. The arrest of family members was standard procedure to deter others from attempting escape. He was charged with "aiding and abetting escape", as were Strelzyk's sister Maria and her husband, who were sentenced to 2½ years. The three were eventually released with the help of Amnesty International.[12]&lt;/p&gt;&lt;p&gt;The families decided to initially settle in Naila where they had landed. Wetzel worked as an automobile mechanic and Strelzyk opened a TV repair shop in Bad Kissingen. Due to pressure from Stasi spies, the Strelzyks moved to Switzerland in 1985.[10] After German reunification in 1990, they returned to their old home in their hometown of Pößneck.[13] The Wetzels remained in Bavaria.[7]&lt;/p&gt;&lt;p&gt;West German weekly magazine Stern paid Strelzyk and Wetzel for exclusive rights to the story.[3]&lt;/p&gt;&lt;p&gt;The escape has been portrayed in two films: Night Crossing (1982) and Balloon (2018). The former, also called With the Wind to the West – the English translation of the German title – was an English-language film produced by Disney. The latter was a German-language production which "both families welcomed [Director] Herbig’s desire to, as he put it, 'make a German film for an international audience.'" The Strelzyks were reportedly "moved to tears" at the screening of Balloon at Rockefeller Center in New York City.[12] Herbig claimed in 2018 that both the Strelzyk and Wetzel families had been dissatisfied with the Disney film.[14]&lt;/p&gt;&lt;p&gt;Peter Strelzyk died in 2017 at age 74 after a long illness.[13]&lt;/p&gt;&lt;p&gt;In 2017, the balloon was put on permanent display at the Haus der Bayerischen Geschichte: Museum in Regensburg.[10]&lt;/p&gt;&lt;head rend="h2"&gt;Escapees&lt;/head&gt;[edit]&lt;p&gt;The family members included:[3]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Peter Strelzyk, aged 37&lt;/item&gt;&lt;item&gt;Doris Strelzyk&lt;/item&gt;&lt;item&gt;Frank Strelzyk, aged 15&lt;/item&gt;&lt;item&gt;Andreas Strelzyk, aged 11&lt;/item&gt;&lt;item&gt;Günter Wetzel, aged 24&lt;/item&gt;&lt;item&gt;Petra Wetzel&lt;/item&gt;&lt;item&gt;Peter Wetzel, aged 5&lt;/item&gt;&lt;item&gt;Andreas Wetzel, aged 2&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Media&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;The Disney film Night Crossing (1982) is an adaptation of the story[13]&lt;/item&gt;&lt;item&gt;Michael Herbig's film Balloon (2018) is a German-language adaptation of the story[15]&lt;/item&gt;&lt;item&gt;BBC program Outlook, "Fleeing Communism in a Hot Air Balloon"[16]&lt;/item&gt;&lt;item&gt;PBS Nova program, "History's Great Escapes" (2004)[17]&lt;/item&gt;&lt;item&gt;Doris Strelzyk, Peter Strelzyk, Gudrun Giese: Destiny Balloon Escape. Quadriga, Berlin 1999, ISBN 3-88679-330-3&lt;/item&gt;&lt;item&gt;Jürgen Petschull, With the Wind to the West. The Adventurous Flight from Germany to Germany. Goldmann, Munich 1980, ISBN 3-442-11501-9&lt;/item&gt;&lt;item&gt;Kristen Fulton (Author), Torben Kuhlmann (Illustrator), Flight for Freedom: The Wetzel Family’s Daring Escape from East Germany. March 3, 2020, ISBN 978-1452149608&lt;/item&gt;&lt;item&gt;The Netflix series White Rabbit Project, episode 2, "Jailbreak"&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;See also&lt;/head&gt;[edit]&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ a b Wetzel, Günter. "Die Nacht der Flucht". Ballonflucht.de. Archived from the original on 19 September 2020. Retrieved 16 September 2019.&lt;/item&gt;&lt;item&gt;^ Hertle, Hans-Hermann; Nooke, Maria (2009). Die Todesopfer an der Berliner Mauer 1961–1989. Ein biographisches Handbuch. Ch. Links Verlag. ISBN 978-3-86153-517-1.&lt;/item&gt;&lt;item&gt;^ a b c d e Getler, Michael (28 September 1979). "Harrowing Flight From East Germany". The Washington Post. Archived from the original on 26 April 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ Snow, Philipp (16 September 2009). "Balloon escape from the GDR With hot air to freedom". Spiegel Online (in German). Archived from the original on 7 April 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c Simpson, Paul (2013). The Mammoth Book of Prison Breaks. Little, Brown Book Group. p. 216. ISBN 978-1-4721-0024-5. Archived from the original on 16 September 2023. Retrieved 1 April 2018.&lt;/item&gt;&lt;item&gt;^ a b c d e f g h i j k l m n o p q r s Dornberg, John (February 1980). "The Freedom Balloon". Popular Mechanics. pp. 100–103. Retrieved 22 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c d e Overbye, Stine (13 April 2017). "Fathers wanted to escape GDR in a hot air balloon". Historia (in Dutch). Archived from the original on 1 April 2018. Retrieved 30 March 2018.&lt;/item&gt;&lt;item&gt;^ Petschull, Jürgen (27 September 1979). "Das Himmelfahrtskommando" [High-flying mission] (PDF). Stern (in German). No. 40. p. 34. Archived from the original (PDF) on 12 July 2024 – via Museum Naila.&lt;/item&gt;&lt;item&gt;^ Souerbry, Rachel. "How Two Families Escaped East Germany In A Homemade Hot Air Balloon". ranker.com. Archived from the original on 1 April 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c "Wetzel und Peter Strelzyk Ballonhülle der Strelzyks". museum.bayern (in German). Archived from the original on 8 April 2019. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ "East-West: The Great Balloon Escape". Time. 1 October 1979. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c "The Balloon Escape of Peter Strelzyk". goethe-rutheneum.de (in German). Archived from the original on 11 February 2013. Retrieved 30 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c "Man who fled East Germany in a homemade balloon and whose story was made into a film dies". The Express. 15 March 2017. Archived from the original on 1 April 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ Connolly, Kate (17 October 2018). "Film of daring balloon escape from East revives German identity debate". Archived from the original on 8 February 2021. Retrieved 10 May 2019.&lt;/item&gt;&lt;item&gt;^ Ballon at IMDb&lt;/item&gt;&lt;item&gt;^ "Fleeing Communism in a Hot Air Balloon". bbc. Archived from the original on 12 December 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ "Great Escapes". pbs.org. Archived from the original on 16 April 2019. Retrieved 16 April 2019.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;External links&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Escape by balloon by Günter Wetzel (participant website)&lt;/item&gt;&lt;item&gt;Video of balloon on museum display&lt;/item&gt;&lt;item&gt;BBC Outlook program&lt;/item&gt;&lt;item&gt;Photograph of Güenter Wetzel, Peter and Doris Strelzyk Archived 1 April 2018 at the Wayback Machine&lt;/item&gt;&lt;item&gt;Photograph of the actual balloon, inflated in 1985 at a festival in Hof, Bavaria&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/East_Germany_balloon_escape"/><published>2026-01-16T17:16:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46652617</id><title>Releasing rainbow tables to accelerate Net-NTLMv1 protocol deprecation</title><updated>2026-01-17T14:10:08.364477+00:00</updated><content>&lt;doc fingerprint="ac035d9b403222ef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Closing the Door on Net-NTLMv1: Releasing Rainbow Tables to Accelerate Protocol Deprecation&lt;/head&gt;
    &lt;head rend="h5"&gt;Mandiant&lt;/head&gt;
    &lt;p&gt;Written by: Nic Losby&lt;/p&gt;
    &lt;head rend="h3"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Mandiant is publicly releasing a comprehensive dataset of Net-NTLMv1 rainbow tables to underscore the urgency of migrating away from this outdated protocol. Despite Net-NTLMv1 being deprecated and known to be insecure for over two decades—with cryptanalysis dating back to 1999—Mandiant consultants continue to identify its use in active environments. This legacy protocol leaves organizations vulnerable to trivial credential theft, yet it remains prevalent due to inertia and a lack of demonstrated immediate risk.&lt;/p&gt;
    &lt;p&gt;By releasing these tables, Mandiant aims to lower the barrier for security professionals to demonstrate the insecurity of Net-NTLMv1. While tools to exploit this protocol have existed for years, they often required uploading sensitive data to third-party services or expensive hardware to brute-force keys. The release of this dataset allows defenders and researchers to recover keys in under 12 hours using consumer hardware costing less than $600 USD. This initiative highlights the amplified impact of combining Mandiant's frontline expertise with Google Cloud's resources to eliminate entire classes of attacks.&lt;/p&gt;
    &lt;p&gt;This post details the generation of the tables, provides access to the dataset for community use, and outlines critical remediation steps to disable Net-NTLMv1 and prevent authentication coercion attacks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Background&lt;/head&gt;
    &lt;p&gt;Net-NTLMv1 has been widely known to be insecure since at least 2012, following presentations at DEFCON 20, with cryptanalysis of the underlying protocol dating back to at least 1999. On Aug. 30, 2016, Hashcat added support for cracking Data Encryption Standard (DES) keys using known plaintext, further democratizing the ability to attack this protocol. Rainbow tables are almost as old, with the initial paper on rainbow tables published in 2003 by Philippe Oechslin, citing an earlier iteration of a time-memory trade-off from 1980 by Martin Hellman.&lt;/p&gt;
    &lt;p&gt;Essentially, if an attacker can obtain a Net-NTLMv1 hash without Extended Session Security (ESS) for the known plaintext of &lt;code&gt;1122334455667788&lt;/code&gt;, a cryptographic attack, referred to as a known plaintext attack (KPA), can be applied. This guarantees recovery of the key material used. Since the key material is the password hash of the authenticating Active Directory (AD) object—user or computer—the attack results can quickly be used to compromise the object, often leading to privilege escalation.&lt;/p&gt;
    &lt;p&gt;A common chain attackers use is authentication coercion from a highly privileged object, such as a domain controller (DC). Recovering the password hash of the DC machine account allows for DCSync privileges to compromise any other account in AD.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dataset Release&lt;/head&gt;
    &lt;p&gt;The unsorted dataset can be downloaded using &lt;code&gt;gsutil -m cp -r gs://net-ntlmv1-tables/tables .&lt;/code&gt; or through the Google Cloud Research Dataset portal. &lt;/p&gt;
    &lt;p&gt;The SHA512 hashes of the tables can be checked by first downloading the checksums &lt;code&gt;gsutil -m cp gs://net-ntlmv1-tables/tables.sha512 .&lt;/code&gt; then checked by &lt;code&gt;sha512sum -c tables.sha512&lt;/code&gt;. The password cracking community has already created derivative work and is also hosting the ready to use tables.&lt;/p&gt;
    &lt;head rend="h3"&gt;Use of the Tables&lt;/head&gt;
    &lt;p&gt;Once a Net-NTLMv1 hash has been obtained, the tables can be used with historical or modern reinventions of rainbow table searching software such as rainbowcrack (rcrack), or RainbowCrack-NG on central processing units (CPUs) or a fork of rainbowcrackalack on graphics processing units (GPUs). The Net-NTLMv1 hash needs to be preprocessed to the DES components using ntlmv1-multi as shown in the next section.&lt;/p&gt;
    &lt;head rend="h3"&gt;Obtaining a Net-NTLMv1 Hash&lt;/head&gt;
    &lt;p&gt;Most attackers will use Responder with the &lt;code&gt;--lm&lt;/code&gt; and &lt;code&gt;--disable-ess&lt;/code&gt; flags and set the authentication to a static value of &lt;code&gt;1122334455667788&lt;/code&gt; to only allow for connections with Net-NTLMv1 as a possibility. Attackers can then wait for incoming connections or coerce authentication using a tool such as PetitPotam or DFSCoerce to generate incoming connections from DCs or lower privilege hosts that are useful for objective completion. Responses can be cracked to retrieve password hashes of either users or computer machine accounts. A sample workflow for an attacker is shown below in Figure 1, Figure 2, and Figure 3.&lt;/p&gt;
    &lt;p&gt;Figure 1: DFSCoerce against a DC&lt;/p&gt;
    &lt;p&gt;Figure 2: Net-NTLMv1 hash obtained for DC machine account&lt;/p&gt;
    &lt;p&gt;Figure 3: Parse Net-NTLMv1 hash to DES parts&lt;/p&gt;
    &lt;p&gt;Figure 4 illustrates the processing of the Net-NTLMv1 hash to the DES ciphertexts.&lt;/p&gt;
    &lt;p&gt;Figure 4: Net-NTLMv1 hash to DES ciphertexts&lt;/p&gt;
    &lt;p&gt;An attacker then takes the split-out ciphertexts to crack the keys used based on the known plaintext of &lt;code&gt;1122334455667788&lt;/code&gt; with the steps of loading the tables shown in Figure 5 and cracking results in Figure 6 and Figure 7.&lt;/p&gt;
    &lt;p&gt;Figure 5: Loading DES components for cracking&lt;/p&gt;
    &lt;p&gt;Figure 6: First hash cracked&lt;/p&gt;
    &lt;p&gt;Figure 7: Second hash cracked and run statistics&lt;/p&gt;
    &lt;p&gt;An attacker can then calculate the last remaining key with ntlmv1-multi once again, or look it up with twobytes, to recreate the full NT hash for the DC account with the last key part shown in Figure 8.&lt;/p&gt;
    &lt;p&gt;Figure 8: Calculate remaining key&lt;/p&gt;
    &lt;p&gt;The result can be checked with hashcat's NT hash shucking mode, &lt;code&gt;-m 27000&lt;/code&gt;, as shown in Figure 9.&lt;/p&gt;
    &lt;p&gt;Figure 9: Keys checked with hash shucking&lt;/p&gt;
    &lt;p&gt;An attacker can then use the hash to perform a DCSync attack targeting a DC and authenticating as the now compromised machine account. The attack flow uses secretsdump.py from the Impacket toolsuite and is shown in Figure 10.&lt;/p&gt;
    &lt;p&gt;Figure 10: DCSync attack performed&lt;/p&gt;
    &lt;head rend="h3"&gt;Remediation&lt;/head&gt;
    &lt;p&gt;Organizations should immediately disable the use of Net-NTLMv1.&lt;/p&gt;
    &lt;head rend="h4"&gt;Local Computer Policy&lt;/head&gt;
    &lt;p&gt;"Local Security Settings" &amp;gt; "Local Policies" &amp;gt; "Security Options" &amp;gt; “Network security: LAN Manager authentication level" &amp;gt; "Send NTLMv2 response only".&lt;/p&gt;
    &lt;head rend="h4"&gt;Group Policy&lt;/head&gt;
    &lt;p&gt;"Computer Configuration" &amp;gt; "Policies" &amp;gt; "Windows Settings" &amp;gt; "Security Settings" &amp;gt; "Local Policies" &amp;gt; "Security Options" &amp;gt; "Network Security: LAN Manager authentication level" &amp;gt; "Send NTLMv2 response only"&lt;/p&gt;
    &lt;p&gt;As these are local to the computer configurations, attackers can and have set the configuration to a vulnerable state to then fix the configuration after their attacks have completed with local administrative access. Monitoring and alerting of when and where Net-NTLMv1 is used is needed in addition to catching these edge cases.&lt;/p&gt;
    &lt;p&gt;Filter Event Logs for Event ID 4624: "An Account was successfully logged on." &amp;gt; "Detailed Authentication Information" &amp;gt; "Authentication Package" &amp;gt; "Package Name (NTLM only)", if "LM" or "NTLMv1" is the value of this attribute, LAN Manager or Net-NTLMv1 was used.&lt;/p&gt;
    &lt;head rend="h3"&gt;Related Reading&lt;/head&gt;
    &lt;p&gt;This project was inspired by and referenced the following research published to blogs, social media, and code repositories.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Thank you to everyone who helped make this blog post possible, including but not limited to Chris King and Max Gruenberg.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cloud.google.com/blog/topics/threat-intelligence/net-ntlmv1-deprecation-rainbow-tables"/><published>2026-01-16T21:42:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46653721</id><title>FLUX.2 [Klein]: Towards Interactive Visual Intelligence</title><updated>2026-01-17T14:10:08.057403+00:00</updated><content>&lt;doc fingerprint="9d927013843c0b86"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FLUX.2 [klein]: Towards Interactive Visual Intelligence&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Models&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;FLUX.2 [klein]: Towards Interactive Visual Intelligence&lt;/head&gt;
    &lt;p&gt;Today, we release the FLUX.2 [klein] model family, our fastest image models to date. FLUX.2 [klein] unifies generation and editing in a single compact architecture, delivering state-of-the-art quality with end-to-end inference as low as under a second. Built for applications that require real-time image generation without sacrificing quality, and runs on consumer hardware with as little as 13GB VRAM.&lt;/p&gt;
    &lt;p&gt;Demo showing editing with FLUX.2 [klein]&lt;/p&gt;
    &lt;head rend="h3"&gt;Why go [klein]?&lt;/head&gt;
    &lt;p&gt;Visual Intelligence is entering a new era. As AI agents become more capable, they need visual generation that can keep up; models that respond in real-time, iterate quickly, and run efficiently on accessible hardware.&lt;/p&gt;
    &lt;p&gt;The klein name comes from the German word for "small", reflecting both the compact model size and the minimal latency. But FLUX.2 [klein] is anything but limited. These models deliver exceptional performance in text-to-image generation, image editing and multi-reference generation, typically reserved for much larger models.&lt;/p&gt;
    &lt;head rend="h3"&gt;What's New&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sub-second inference. Generate or edit images in under 0.5s on modern hardware.&lt;/item&gt;
      &lt;item&gt;Photorealistic outputs and high diversity, especially in the base variants.&lt;/item&gt;
      &lt;item&gt;Unified generation and editing. Text-to-image, image editing, and multi-reference support in a single model while delivering frontier performance.&lt;/item&gt;
      &lt;item&gt;Runs on consumer GPUs. The 4B model fits in ~13GB VRAM (RTX 3090/4070 and above).&lt;/item&gt;
      &lt;item&gt;Developer-friendly &amp;amp; Accessible: Apache 2.0 on 4B models, open weights for 9B models. Full open weights for customization and fine-tuning.&lt;/item&gt;
      &lt;item&gt;API and open weights. Production-ready API or run locally with full weights.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: The “FLUX [dev] Non-Commercial License” has been renamed to “FLUX Non-Commercial License” and will apply to the 9B Klein models. No material changes have been made to the license.&lt;/p&gt;
    &lt;p&gt;Text to Image collage using FLUX.2 [klein]&lt;/p&gt;
    &lt;head rend="h3"&gt;The FLUX.2 [klein] Model Family&lt;/head&gt;
    &lt;head rend="h4"&gt;FLUX.2 [klein] 9B&lt;/head&gt;
    &lt;p&gt;Our flagship small model. Defines the Pareto frontier for quality vs. latency across text-to-image, single-reference editing, and multi-reference generation. Matches or exceeds models 5x its size - in under half a second. Built on a 9B flow model with 8B Qwen3 text embedder, step-distilled to 4 inference steps.&lt;/p&gt;
    &lt;p&gt;Combine multiple input images, blend concepts, and iterate on complex compositions - all at sub-second speed with frontier-level quality. No model this fast has ever done this well.&lt;/p&gt;
    &lt;p&gt;License: FLUX NCL&lt;/p&gt;
    &lt;p&gt;Imagine editing collage using FLUX.2 [klein]&lt;/p&gt;
    &lt;head rend="h4"&gt;&lt;lb/&gt;FLUX.2 [klein] 4B:&lt;/head&gt;
    &lt;p&gt;Fully open under Apache 2.0. Our most accessible model, it runs on consumer GPUs like the RTX 3090/4070. Compact but capable: supports T2I, I2I, and multi-reference at quality that punches above its size. Built for local development and edge deployment.&lt;/p&gt;
    &lt;p&gt;License: Apache 2.0&lt;/p&gt;
    &lt;head rend="h4"&gt;FLUX.2 [klein] Base 9B / 4B:&lt;/head&gt;
    &lt;p&gt;The full-capacity foundation models. Undistilled, preserving complete training signal for maximum flexibility. Ideal for fine-tuning, LoRA training, research, and custom pipelines where control matters more than speed. Higher output diversity than the distilled models.&lt;/p&gt;
    &lt;p&gt;License: 4B Base under Apache 2.0, 9B Base under FLUX NCL&lt;/p&gt;
    &lt;p&gt;Output Diversity using FLUX.2 [klein]&lt;/p&gt;
    &lt;head rend="h4"&gt;Quantized versions&lt;/head&gt;
    &lt;p&gt;We are also releasing FP8 and NVFP4 versions of all [klein] variants, developed in collaboration with NVIDIA for optimized inference on RTX GPUs. Same capabilities, smaller footprint - compatible with even more hardware.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FP8: Up to 1.6x faster, up to 40% less VRAM&lt;/item&gt;
      &lt;item&gt;NVFP4: Up to 2.7x faster, up to 55% less VRAM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benchmarks on RTX 5080/5090, T2I at 1024×1024&lt;lb/&gt;Same licenses apply: Apache 2.0 for 4B variants, FLUX NCL for 9B.&lt;/p&gt;
    &lt;head rend="h4"&gt;&lt;lb/&gt;Performance Analysis&lt;/head&gt;
    &lt;p&gt;FLUX.2 [klein] Elo vs Latency (top) and VRAM (bottom) across Text-to-Image, Image-to-Image Single Reference, and Multi-Reference tasks. FLUX.2 [klein] matches or exceeds Qwen's quality at a fraction of the latency and VRAM, and outperforms Z-Image while supporting both text-to-image generation and (multi-reference) image editing in a unified model. The base variants trade some speed for full customizability and fine-tuning, making them better suited for research and adaptation to specific use cases. Speed is measured on a GB200 in bf16.&lt;/p&gt;
    &lt;head rend="h3"&gt;Into the New&lt;/head&gt;
    &lt;p&gt;FLUX.2 [klein] is more than a faster model. It's a step toward our vision of interactive visual intelligence. We believe the future belongs to creators and developers with AI that can see, create, and iterate in real-time. Systems that enable new categories of applications: real-time design tools, agentic visual reasoning, interactive content creation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources&lt;/head&gt;
    &lt;p&gt;Try it&lt;/p&gt;
    &lt;p&gt;Build with it&lt;/p&gt;
    &lt;p&gt;Learn more&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence"/><published>2026-01-16T23:46:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46654749</id><title>Drone Hacking Part 1: Dumping Firmware and Bruteforcing ECC</title><updated>2026-01-17T14:10:06.468161+00:00</updated><content>&lt;doc fingerprint="ae645296a9a5069d"&gt;
  &lt;main&gt;
    &lt;p&gt;~48 min read&lt;/p&gt;
    &lt;head rend="h1"&gt;Drone Hacking Part 1: Dumping Firmware and Bruteforcing ECC&lt;/head&gt;
    &lt;head rend="h2"&gt;Intro&lt;/head&gt;
    &lt;p&gt;In July 2025, we from Neodyme got together in Munich and did security research on a bunch of IoT devices, ranging from bluetooth headsets, to door locks, to drones. One of these was the Potensic Atom 2. It’s a photo and video drone with a gimbal-stabilized 4K camera and a remote control that you hook up to your own smartphone and the proprietary app. If you’ve ever flown a DJI Mini 4K, this drone will look very familiar to you.&lt;/p&gt;
    &lt;p&gt;This post is part of a two-part series that will cover how we disassembled the drone and dumped the firmware from the NAND chip and how we analyzed the drone’s firmware, app, and remote control to find some backdoors and vulnerabilities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Goal: Dumping the Firmware&lt;/head&gt;
    &lt;p&gt;One of the most important pieces of information you can acquire when setting up to hack a device is its firmware. If you want to reverse engineer the software that’s running on the drone and find vulnerabilities in that, then you need a copy of it in the first place.&lt;/p&gt;
    &lt;p&gt;Now there are a couple of ways to go about that, some are less intrusive and some are more effective.&lt;/p&gt;
    &lt;p&gt;You might get lucky and be able to just download the firmware as a firmware update from the manufacturer’s website. However, those update sites are often not publicly documented and can be locked behind authorization checks or encrypted. Encrypted firmwares can still be useful - you “just” need to reverse engineer the on-device decryption process. For the Atom 2, downloading the firmware updates required having a valid drone and remote control serial number and the firmware update was also encrypted. Without having the decryption logic, we put this approach on ice during our initial research.&lt;/p&gt;
    &lt;p&gt;Another really comfortable approach is to use exposed debug interfaces like JTAG or UART. However, those are often undocumented, unlabeled, or entirely removed for public versions. We didn’t find any on the Atom 2.&lt;/p&gt;
    &lt;p&gt;What we can always do, though not necessarily always successful, is solder off the entire NAND chip and dump the firmware byte by byte. This has the risk of breaking the NAND chip and/or the rest of the board if you’re not careful. Also, some devices, like modern smart phones, encrypt their persistent storage with key material stored in, e.g., the TPM. If that is the case, then simply soldering off the NAND chip will leave you with unusable encrypted data. Fortunately, the Atom 2’s NAND contents are not encrypted, as we find out later.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dumping the NAND Chip&lt;/head&gt;
    &lt;p&gt;Dumping a NAND chip generally always follows the same pattern:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Identifying the NAND Chip&lt;/item&gt;
      &lt;item&gt;Removing it from the board&lt;/item&gt;
      &lt;item&gt;Identifying the data pins and communication protocol of the NAND chip&lt;/item&gt;
      &lt;item&gt;Connecting the NAND chip to some kind of reading device&lt;/item&gt;
      &lt;item&gt;Reading the NAND content&lt;/item&gt;
      &lt;item&gt;Reassembling the read contents into a working firmware - usually containing one or more file systems&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Identifying the NAND Chip&lt;/head&gt;
    &lt;p&gt;The Atom 2 has multiple boards:&lt;/p&gt;
    &lt;p&gt;We are mainly interested in the main board because that’s where the NAND flash is going to be. The main board had several metal RF shields that we have already pried off or cut through on the photos.&lt;/p&gt;
    &lt;p&gt;We can identify most of these chips through their markings. Note that while we’re mainly interested in the NAND chip, knowing the others can help with recognizing things during reversing later on. Roughly knowing which SoC we were working with was crucial, as you will see in later sections of this blog post.&lt;/p&gt;
    &lt;p&gt;(Note that the markings below might not match the photos completely. We had multiple drones. The markings are mostly from the first drone and the photos are mostly of the second drone.)&lt;/p&gt;
    &lt;head rend="h4"&gt;Top-side&lt;/head&gt;
    &lt;p&gt;SoC (System on a Chip, aka “the main thingy”)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: 23AP10 VTQMSQKJYJ 4978-CN B3&lt;/item&gt;
      &lt;item&gt;We didn’t find an exact match, but this site references the 21AP10. &lt;list rend="ul"&gt;&lt;item&gt;The page title is &lt;code&gt;21AP10 SS928 平替SD3403V100 海思 SOC芯片&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;21AP10 SS928 平替&lt;/code&gt;=&amp;gt;&lt;code&gt;21AP10 SS928 Drop-In Replacement&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;SD3403V100 海思 SOC芯片&lt;/code&gt;=&amp;gt;&lt;code&gt;HiSilicon SD3403V100 SOC Chip&lt;/code&gt;&lt;/item&gt;&lt;item&gt;That is a mobile camera SoC.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The page title is &lt;/item&gt;
      &lt;item&gt;It makes sense that this is the SoC because it is close to both external RAM chips and the NAND flash.&lt;/item&gt;
      &lt;item&gt;In this teardown of a previous Atom model, the device had a &lt;code&gt;HiSilicon Hi 3559 camera MCU&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;We found a data sheet for the HiSilicon Hi3519 V100. &lt;list rend="ul"&gt;&lt;item&gt;Close enough for now.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;RAM&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: SEC340 K4A8G16 5WC BCTD G2F9190AC&lt;/item&gt;
      &lt;item&gt;Data sheet can be found on the internet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ARM Cortex-M4&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: GD32F470 VGH6 BUMK618 AL2451 GigaDevice ARM&lt;/item&gt;
      &lt;item&gt;Data sheet can be found on the internet.&lt;/item&gt;
      &lt;item&gt;Might be SD-card related since the SD card slot is on the other side.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unknown Chips&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: V2 2441TM4N190.00 &lt;list rend="ul"&gt;&lt;item&gt;The name &lt;code&gt;2441TM&lt;/code&gt;appears in some WizSense surveillance cameras&lt;/item&gt;&lt;item&gt;Not sure if related&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The name &lt;/item&gt;
      &lt;item&gt;2 chips with markings: 8285HE 426656 CS2441&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Bottom Side&lt;/head&gt;
    &lt;p&gt;NAND flash&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: MXIC X243662 MX35UF4GE4AD-241 5P231800A1&lt;/item&gt;
      &lt;item&gt;Data sheet can be found on the internet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;RAM&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: SEC407 K4A8G16 5WC BCTD G2K43304C&lt;/item&gt;
      &lt;item&gt;Same as on top side&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ARM Cortex-M4&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markings: F460JEUA P8VR4400 2416021&lt;/item&gt;
      &lt;item&gt;Not sure what it’s used for.&lt;/item&gt;
      &lt;item&gt;A data sheet for the HC32F460JEUA-QFN48TR (looks close enough?) can be found on the internet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;WLAN + Bluetooth&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RTL8821CS&lt;/item&gt;
      &lt;item&gt;Data sheet can be found on the internet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Removing the NAND Chip from the board&lt;/head&gt;
    &lt;p&gt;Now that we have identified the NAND chip, we fasten the board and tape off the remaining components with heat-shielding tape.&lt;/p&gt;
    &lt;p&gt;Usually getting the chip off of the board is just a matter of using hot air station and flux. However, you can see on the photos that the chip is actually glued to the main board with what is probably epoxy. That’s a thing you can do if you want to secure the chips more securely and not depend on the solder joints to hold your chip in place (and risk breaking them). Or you can do that just to the NAND chip to make it harder for researcher to pry off your NAND chip and dump your firmware.&lt;/p&gt;
    &lt;p&gt;Anyway, a few cuts with a sharp knife, some heat and a generous amount of flux later, the little bugger came off in one piece.&lt;/p&gt;
    &lt;p&gt;(And with it, a couple of extremely tiny resistors that I knocked off with my pliers and promptly lost. This main board is now broken. But don’t worry, through the magic of buying &lt;del&gt;two&lt;/del&gt; three of them, we can still fly the drone.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Identifying the data pins and communication protocol of the NAND chip&lt;/head&gt;
    &lt;p&gt;According to the data sheet of the MX35UF4GE4AD NAND chip, the flash chip can either come in a 24-pin BGA package or an 8-pin WSON package, which we have here. A quick look at the pin descriptions tell us that the NAND chip is communicating via SPI.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Pin Symbol&lt;/cell&gt;
        &lt;cell role="head"&gt;Pin Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;CS#&lt;/cell&gt;
        &lt;cell&gt;Chip Select&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SI&lt;/cell&gt;
        &lt;cell&gt;Serial Data Input&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SO&lt;/cell&gt;
        &lt;cell&gt;Serial Data Output&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SCLK&lt;/cell&gt;
        &lt;cell&gt;Clock Input&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;WP#&lt;/cell&gt;
        &lt;cell&gt;Write protection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;HOLD#&lt;/cell&gt;
        &lt;cell&gt;Hold&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;VCC&lt;/cell&gt;
        &lt;cell&gt;Power Supply (1.8 V)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;GND&lt;/cell&gt;
        &lt;cell&gt;Ground&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DNU&lt;/cell&gt;
        &lt;cell&gt;Do Not Use&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Well, let’s solder tiny copper cables to all of those pins and drown them in a bit of hot glue to stop them from breaking off.&lt;/p&gt;
    &lt;p&gt;Note that you can get a proper socket for 8-WSON chips into which you simply clamp the chip and which exposes easy-to-work-with breakout pins. None of our sockets we brought fit though, so we just did it the old-school way.&lt;/p&gt;
    &lt;head rend="h3"&gt;Connecting the NAND chip to some kind of reading device&lt;/head&gt;
    &lt;p&gt;SPI is pretty easy to work with. We have two main data lines called SI and SO (Serial In/Out). You will also find them under the names “MOSI” and “MISO” (Master Out Slave In / Master In Slave Out). As the names of these suggest, SPI follows a master-slave architecture. The microcontroller drives the communication and the peripheral device reacts.&lt;/p&gt;
    &lt;p&gt;Fortunately, we are simulating the microcontroller-side of the communication, which means that we have a large amount of control. Specifically, we control the clock (SCLK). Sometimes it is hard to talk to embedded hardware because of the speed at which they operate. With SPI, however, we can slow down the clock to however fast we want the devices to talk.&lt;/p&gt;
    &lt;p&gt;Since SPI is a bus protocol, more than one slave device can be hooked up to a master device on the same data lines. To avoid collisions, each device is also assigned its own “Chip Select” line (CS). When the master device wants to talk to a specific slave device, it pulls the corresponding CS line low. Devices that have their CS line high won’t react at all.&lt;/p&gt;
    &lt;p&gt;Obviously there are fancy devices on the market that will make dumping a NAND chip via SPI pretty easy and straightforward. Problem is, we couldn’t get any of them to work. They either didn’t fit (physically), were too fast or failed for some other strange reason we didn’t understand. So we wrote our own dump script onto an ESP32 using the SPI commands in the data sheet and let it forward the data to our computer via the USB console.&lt;/p&gt;
    &lt;p&gt;Doing that, we ended up with a 544 MiB dump, containing 131,072 pages of 4096+256 bytes. (We will come back to that “+256” later on.)&lt;/p&gt;
    &lt;p&gt;Let’s dig into what this the flash dump contains with &lt;code&gt;binwalk&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Sweet! We get a working ASCII copyright string, so something must have worked. And at the end of the image we have a bunch of UBIFS images. That’s probably where all the juicy files are!&lt;/p&gt;
    &lt;p&gt;Let’s extract them with &lt;code&gt;dd&lt;/code&gt; and take a look inside with ubi_reader:&lt;/p&gt;
    &lt;p&gt;Hmm. That doesn’t work. Spoiler: The extracted image is broken.&lt;/p&gt;
    &lt;p&gt;Now if you’ve ever done something as hacky as this, you will know about a pesky little phenomenon that happens when you just solder copper wires onto a chip, stick that onto the ports of an ESP32 and do SPI communication - which has no built-in integrity checks.&lt;/p&gt;
    &lt;head rend="h4"&gt;Random Bit Flips&lt;/head&gt;
    &lt;p&gt;These are three dumps taken from the same NAND chip:&lt;/p&gt;
    &lt;p&gt;If you read 4 MiB of data from the chip, not all of the bits you receive are correct. And without any additional data, you have little way of knowing which ones are correct and which are not. If you are lucky, then the dumped data will still “work”, i.e., the contained file system will mount and you can browse files, but futher down the line you will have no way of knowing whether that weird function you’re reversing is actually weird or just the product of random bit flips messing up the CPU instructions.&lt;/p&gt;
    &lt;p&gt;A relatively simple yet time-consuming way to get around this: Read the flash often (at least three times) and hold a majority vote for every bit. Since the bit flips are random and not too prevalent, they are less likely to hit the same bit twice.&lt;/p&gt;
    &lt;p&gt;Hot tip: If you’re gonna work with python, use numpy and work on arrays and memory-mapped files. Otherwise this can take a lot of time and a lot of RAM - even for a 512 MiB flash dump.&lt;/p&gt;
    &lt;p&gt;But isn’t there a better way? Yes, there is. And - btw - even with completely correct majority voting, the flash content is still broken. But we’ll get to that.&lt;/p&gt;
    &lt;p&gt;Trying to work with the majority-voted dump:&lt;/p&gt;
    &lt;head rend="h2"&gt;Out-of-band bytes and ECC troubles&lt;/head&gt;
    &lt;p&gt;One thing we have have silently brushed aside for now: The NAND chip distinguishes between “user data” and “extra data”. In our dump above, we have naively concatenated it all together and assumed that a page size of 4096+256 bytes somehow makes sense. Of course, it doesn’t.&lt;/p&gt;
    &lt;p&gt;Also, this majority voting hack is obviously not the “correct” way to work with a NAND chip. And even the proper SoC mounted on the proper mainboard can’t run a system off of a flash chip that gives it random bit flips that it can’t detect or recover from. The problem is that NAND is just inherently imperfect storage. Majority voting only corrects for transmission errors during dumping, but does nothing to bit errors that are stored on the device! Bits on the storage might decay over time, or the CPU might also have some transmission errors during writing.&lt;/p&gt;
    &lt;p&gt;Of course this problem has been very well known for a long time, so manufacturers always include some extra space next to user data for “error correction”. These extra bytes are called “out-of-band” bytes. And they are used to implement Error Correction Codes (ECC).&lt;/p&gt;
    &lt;head rend="h3"&gt;ECC according to the NAND chip data sheet&lt;/head&gt;
    &lt;p&gt;The flash chip implements its own error correction algorithm by reserving some of the space for ECC. It is split into&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2048 blocks of&lt;/item&gt;
      &lt;item&gt;64 pages of&lt;/item&gt;
      &lt;item&gt;4096 user data bytes + 256 “extra” bytes (aka out-of-band bytes)&lt;/item&gt;
      &lt;item&gt;=&amp;gt; 512 MiB of user data (the chip is 4 Gb, not 4 GB)&lt;/item&gt;
      &lt;item&gt;=&amp;gt; 32 MiB of extra data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the chip-internal ECC is enabled, some of those extra bytes are used for ECC.&lt;/p&gt;
    &lt;p&gt;At this point, we naively assumed that each page would be&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;4096 bytes of user data followed by&lt;/item&gt;
      &lt;item&gt;256 (or less?) bytes of ECC covering the previous 4096 bytes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, we quickly found out that the sequences classified as “extra data” contain readable strings! That definitely suggests that this isn’t ECC data.&lt;/p&gt;
    &lt;p&gt;You can also see that there are parts within the user data where strings are suddenly cut off. This suggests that the ECC layout we assumed was wrong. It took us quite a while to figure out what exactly we missed.&lt;/p&gt;
    &lt;head rend="h4"&gt;Entropy Analysis&lt;/head&gt;
    &lt;p&gt;Turns out, the ECC layout is not just 4096 bytes of user data followed by 256 bytes of ECC. If we put all pages next to each other and then calculate the entropy over each n-th byte of a page, we will find that there are multiple sections with high entropy:&lt;/p&gt;
    &lt;p&gt;Why are we looking at entropy? Well, because we expect the user data to have ASCII text (low entropy) every now and then and the ECC data to be mostly random-looking byte values (high entropy).&lt;/p&gt;
    &lt;p&gt;The graph we’re seeing up here suggests that there are sections of roughly 1 KiB of user data, followed by 28 bytes of ECC data. Specifically,&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1028 B user data + 28 B ECC&lt;/item&gt;
      &lt;item&gt;1028 B user data + 28 B ECC&lt;/item&gt;
      &lt;item&gt;1028 B user data + 28 B ECC&lt;/item&gt;
      &lt;item&gt;1014 B user data + 28 B ECC&lt;/item&gt;
      &lt;item&gt;142 B unused&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why these stranges values? And which ECC algorithm is that? We can choose to ignore that and just extract the user data sections and stitch them together. I won’t spam you with more &lt;code&gt;binwalk&lt;/code&gt; and &lt;code&gt;dd&lt;/code&gt; screenshots and just tell you that that also won’t result in a readable UBIFS image. Fortunately, we find an explanation for these values in the next section!&lt;/p&gt;
    &lt;head rend="h3"&gt;ECC according to the SoC data sheet&lt;/head&gt;
    &lt;p&gt;At this point, we had already spent a lot of time fiddling with unstable reading setups and ECC layouts. And then we found that some further digging into the right documentation could have saved us a lot of time during our research. Because the SoC also does ECC. Not just the NAND chip. In fact, we can ignore the NAND chips ECC feature completely.&lt;/p&gt;
    &lt;p&gt;The SoC’s data sheet lists several possible ECC layouts. One of them is the following:&lt;/p&gt;
    &lt;p&gt;Well, that fits our findings perfectly, plus a BB (“bad blocks”) and CTRL (some kind of control bytes?) area that we didn’t identify before.&lt;/p&gt;
    &lt;p&gt;Using this diagram, we can cut out all the ECC, BB and CTRL sections and reconstruct the pure 512 MiB user data flash content.&lt;/p&gt;
    &lt;p&gt;Heyyy, look at that! We managed to extract a file system again - albeit with some error remaining. Let’s see what’s on it:&lt;/p&gt;
    &lt;p&gt;Hmm. Damn. The UBIFS image is now at least somewhat syntactically correct. But it is still broken enough to not have any files. Why could that be? Well, we are looking at exactly what the SoC would see after reading the data from the NAND chip. Plus that we have done majority voting on the bytes - so our version is even better than what the SoC would see.&lt;/p&gt;
    &lt;p&gt;But, there is no guarantee that there aren’t any random bit flips on the NAND chip, i.e., that random bit flipping happened during writing, desoldering or anytime between that! So, there seems to no way around actually implementing the ECC algorithm and correcting the bit flips on the flash dump. Problem is: What kind of ECC algorithm is the SoC running? Unfortunately, the datasheet is silent here, so we had to find out on our own.&lt;/p&gt;
    &lt;head rend="h3"&gt;A short primer on reverse engineering ECC algorithms&lt;/head&gt;
    &lt;p&gt;Typical ECC algorithms on NAND chips use BCH codes, which are parametrized by the following properties:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The amount of parity bits.&lt;/item&gt;
      &lt;item&gt;The correction capacity &lt;code&gt;t&lt;/code&gt;, i.e., how many simultaneous bit flips may appear in the same data block before the block is “too broken” and the ECC algorithm fails.&lt;/item&gt;
      &lt;item&gt;The primitive polynomial used in the equation. If you don’t know what this is, just think of it as an integer parameter for now.&lt;/item&gt;
      &lt;item&gt;Whether and how the data is transformed before the parity bits are calculated.&lt;/item&gt;
      &lt;item&gt;Whether and how the parity bits are transformed after they are calculated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can deduce (1) and (2) from our flash dump. For (3), (4), and (5) we have to either find the code of the SoC (if it is implement in software at all) and reverse engineer the ECC algorithm - or just bruteforce them.&lt;/p&gt;
    &lt;head rend="h4"&gt;Amount of parity bits&lt;/head&gt;
    &lt;p&gt;As we have seen in the SoC’s data sheet, we have 112 byte of ECC / parity bits. However, the fragmented layout on the flash suggests that we actually have 4 ECC groups of 28 byte, each covering a different part of the user data. Note that this is an educated guess and does not have to be true. If we’re not getting anywhere, we should consider dropping this assumption later on. But spoiler: We’re right about this.&lt;/p&gt;
    &lt;p&gt;This means that we have 224 parity bits (= 28 bytes).&lt;/p&gt;
    &lt;head rend="h4"&gt;Correction capacity&lt;/head&gt;
    &lt;p&gt;This part we can just calculate if we make one very realistic assumption. We have 1028 bytes of user data, which is 8224 bits. If we want to represent these 8224 bits as a binary polynomial, we need at least degree 14:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;2^13 =  8192&lt;/code&gt; &amp;lt; - Too small
&lt;code&gt;2^14 = 16384&lt;/code&gt; &amp;lt; - Fits!&lt;/p&gt;
    &lt;p&gt;This means our primitive polynomial needs to be at least degree 14 (&lt;code&gt;m &amp;gt;= 14&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The correction capacity is determined by the degree &lt;code&gt;m&lt;/code&gt; and the amount of parity bits. The more parity bits we have in relation to &lt;code&gt;m&lt;/code&gt;, the higher our correction capacity &lt;code&gt;t&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;t = parity_bits / m&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Now given that &lt;code&gt;parity_bits&lt;/code&gt; is fixed at 224 and assuming that the engineers chose &lt;code&gt;t&lt;/code&gt; to be maximal, we conclude that &lt;code&gt;m = 14&lt;/code&gt; and&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;t = 224 / 14 = 16&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;meaning that we can correct for up to 16 bit flips in the covered user data chunk. Anything more than that and the chunk is lost.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;t = 16&lt;/code&gt; also fits the description of the ECC section in the SoC’s data sheet: “16-Bit/1KB Error Correction Performance” (see our diagram above). So we are pretty certain that this assumption is correct.&lt;/p&gt;
    &lt;head rend="h4"&gt;Primitive polynomial&lt;/head&gt;
    &lt;p&gt;We don’t know that and we will have to bruteforce it. Given that it is a binary polynomial, it is usually represented as a bit vector or simply as an integer. Given that &lt;code&gt;m = 14&lt;/code&gt; we already know that our polynomial must have its 14th bit set and that the 14th bit is the highest bit that is set:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;2^14 &amp;lt;= prim_poly &amp;lt; 2^15&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;which is well within bruteforcing range.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pre-encode and post-encode transformations&lt;/head&gt;
    &lt;p&gt;There are a few transformations that are commonly applied to either the user data before calculating the parity bits (“encoding”) or applied to the parity bits after calculating them. Examples include&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;reverse bit order&lt;/item&gt;
      &lt;item&gt;reverse byte order&lt;/item&gt;
      &lt;item&gt;swap nibbles&lt;/item&gt;
      &lt;item&gt;invert&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why you ask? Well, one of these combinations for example is very useful for NAND storage devices. You see, when NAND pages are erased and then read, their values are all &lt;code&gt;0xFF&lt;/code&gt;. Problem is, a page full of &lt;code&gt;0xFF&lt;/code&gt; will have&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;user data = &lt;code&gt;0xFFFF...&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ECC = &lt;code&gt;0xFFFF...&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and that is not a valid parity, meaning a cleanly erased page will be read as containing a lot of errors. That is because&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;parity(0xFF...) != 0xFF...&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;But we can pull a trick to make this work: Before encoding, invert the user data. And after encoding, invert the parity bits:&lt;/p&gt;
    &lt;p&gt;Notice that the parity of an all-zero page is all-zero:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;parity(0x00...) != 0x00...&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;So by implementing our ECC algorithm with these two inversions, we make a freshly erased page (&lt;code&gt;0xFF...&lt;/code&gt;) have a valid parity (&lt;code&gt;0xFF...&lt;/code&gt;) and the SoC’s error correction won’t need special handling for erased NAND pages.&lt;/p&gt;
    &lt;p&gt;Okay, back to the actual algorithm at hand here. How do we know that these two inversions are what the engineers actually chose? We don’t! We just try all these different transformations and see if one of them works. The amount of possible combinations of these 4 transformations is quite low and easily bruteforcable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Brute-forcing ECC parameters&lt;/head&gt;
    &lt;p&gt;In summary, to test our guessed parameters, we need a user data section without errors, generate its ECC and check if it matches the ECC that we read off of the NAND chip. Our bruteforce script thus has to do the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find a “good” user data section with no bit flips and the corresponding ECC section.&lt;/item&gt;
      &lt;item&gt;Iterate through all possible transformations.&lt;/item&gt;
      &lt;item&gt;Iterate through all possible primitive polynomials of degree 14&lt;/item&gt;
      &lt;item&gt;For each iteration, generate the ECC of the userdata section. If it matches the existing ECC, we have found the parameters.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Picking a “good” user data section&lt;/head&gt;
    &lt;p&gt;We need a user data section and its corresponding ECC section without bit flips. But how do we know that a section does not have bit flips? We don’t! We could try to be clever here. One possible approach would be to pick a section with a lot of text and check if the text makes sense. But the lazy approach works just as well: Just try a lot of sections and hope that one of them is correct. More bruteforce. Yeah.&lt;/p&gt;
    &lt;p&gt;As you can see in the script above, we only look at the first section of every page and then move on to the next page entirely. We’re not completely sure yet which ECC bytes cover which user data sections - especially since the 4th section looks fragmented. But we will stick to our assumption that the first 1028 bytes of user data are covered by the first 28 bytes of ECC. Spoiler: We’re right about this. Another Spoiler: The first 3 pages have bit-flips in their first section. The 4th one is good.&lt;/p&gt;
    &lt;head rend="h4"&gt;Iterate through all possible transformations&lt;/head&gt;
    &lt;p&gt;We will try out 4 different transformations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;reverse bit order&lt;/item&gt;
      &lt;item&gt;reverse byte order&lt;/item&gt;
      &lt;item&gt;swap nibbles&lt;/item&gt;
      &lt;item&gt;invert&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For these transformation we want all possible subsets and orderings, but without using the same transformation twice in one run.&lt;/p&gt;
    &lt;p&gt;Then we can run through all these combinations as both pre-transformations as well as post-transformations:&lt;/p&gt;
    &lt;p&gt;Note that some combinations are equivalent:&lt;/p&gt;
    &lt;p&gt;We don’t optimize for that though.&lt;/p&gt;
    &lt;head rend="h4"&gt;Iterate through all possible primitive polynomials of degree 14&lt;/head&gt;
    &lt;p&gt;There are three ways to do this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A simple and slow way&lt;/item&gt;
      &lt;item&gt;A math-heavy and fast way&lt;/item&gt;
      &lt;item&gt;A much better way that is about as simple as (1) and as fast as (2)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course, we went with (1) during our initial research because sometimes thinking just takes longer than computing inefficiently. Afterwards, I spent hours digging into polynomial algebra to come up with (2) and was very happy about it - only to also find (3) right afterwards which was a lot simpler and equally as good… Oh well, at least I got to freshen up on first- and second-semester linear algebra.&lt;/p&gt;
    &lt;p&gt;(1) Simple and slow The simple way would be to just try all polynomials of degree 14. In their integer representation, that’s all integers in &lt;code&gt;range(2**14, 2**15)&lt;/code&gt;
While this will eventually cover the correct primitive polynomial, it will also make bchlib crash the entire script with a SIGSEGV for a lot of non-primitive polynomials.&lt;/p&gt;
    &lt;p&gt;A quick-and-dirty workaround is to just spawn a new process for every candidate polynomial so your main script doesn’t die. And that’s what we did during initial research. It works - but the creation of over 16,000 processes makes this a bit slow. Not too slow to work with though. This approach works in practice.&lt;/p&gt;
    &lt;p&gt;(2) Math-heavy and fast The proper way to do this is to only pass primitive polynomials into BCH’s constructor. But how do we know if the polynomial that is represented by our integer is primitive? By doing lots of math. If you’re not familiar with polynomial algebra (like I was) but really want to know how this works, read the section A brief detour into polynomial algebra in the addendum.&lt;/p&gt;
    &lt;p&gt;Spoiler: It is a lot of thinking work and only about 5% faster than (3) in my tests.&lt;/p&gt;
    &lt;p&gt;(3) Simple and fast Turns out, bchlib only crashes for polynomials with a constant term of 0, i.e., even integers. So if we use &lt;code&gt;range(2**14 + 1, 2**15, 2)&lt;/code&gt;, then it just works without having to fiddle with multiprocessing or math.&lt;/p&gt;
    &lt;p&gt;This will still throw a runtime error for a lot of non-primitive polynomials but we can catch that via try-except:&lt;/p&gt;
    &lt;head rend="h4"&gt;Checking the generated ECC&lt;/head&gt;
    &lt;p&gt;This is straight-forward and self-explanatory.&lt;/p&gt;
    &lt;p&gt;You can find the full ECC Bruteforce Script in the addendum.&lt;/p&gt;
    &lt;head rend="h3"&gt;Restoring the full firmware&lt;/head&gt;
    &lt;p&gt;Now that we have a working ECC setup, let’s reassemble the entire firmware! There is just one little detail that we still need to find out:&lt;/p&gt;
    &lt;p&gt;Which parts of user data are covered by which parts of ECC? We already confirmed that the first user data section is covered by the first 28 bytes of ECC. And the same turns out to be true for the second and third user data section. The fourth section is a bit tricky: It is 928+84 bytes of user data long, with additional BB and CTRL bytes around. What is that about? Turns out, a bit of trial-and-error and looking at the SoC’s data sheet revealed how ECC works for that section.&lt;/p&gt;
    &lt;p&gt;Now we just need to apply that to every page and - voilà - full firmware dump. :tada:&lt;/p&gt;
    &lt;p&gt;The Final Restore Script can be found in the addendum.&lt;/p&gt;
    &lt;p&gt;If we look at the binwalk output for that file, it is much better and looks like it is actually free of errors:&lt;/p&gt;
    &lt;p&gt;When trying to extract the first ubifs image with ubi_reader, we actually get a working file system!&lt;/p&gt;
    &lt;p&gt;ubi_reader still throws an error in the later segments of UBIFS image but this extraction is good enough to start reverse engineering the successfully extracted files. Notably, it is enough to reverse engineer the firmware decryption!&lt;/p&gt;
    &lt;p&gt;Stay tuned for Part 2 of our drone hacking blogpost where we dive into the reverse engineering and vulnerability analysis of the Potensic Atom 2!&lt;/p&gt;
    &lt;head rend="h2"&gt;Addendum&lt;/head&gt;
    &lt;head rend="h3"&gt;Final Restore Script&lt;/head&gt;
    &lt;head rend="h3"&gt;ECC Bruteforce Script&lt;/head&gt;
    &lt;head rend="h3"&gt;Primitive Binary Polynomial Generator&lt;/head&gt;
    &lt;head rend="h3"&gt;Fun Fuckups&lt;/head&gt;
    &lt;head rend="h2"&gt;A brief detour into polynomial algebra&lt;/head&gt;
    &lt;p&gt;If you are like me and you’re not really familiar with polynomial algebra, it makes sense to talk about related concepts in the integer world first and then move on to their counterparts in the polynomial world. This helps to get an intuition of what we are actually dealing with.&lt;/p&gt;
    &lt;p&gt;I assume that you are generally familiar with modular arithmetic and prime numbers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Integers and Modulo Rings&lt;/head&gt;
    &lt;p&gt;For now, we are working on the field of integers, i.e., , mathematically denoted .&lt;/p&gt;
    &lt;p&gt;When we introduce a modulus, we get . Explanation for the notation:&lt;/p&gt;
    &lt;p&gt;is some modulus. Not necessary prime at this point. Just an integer - a member of .&lt;/p&gt;
    &lt;p&gt;is all multiples of . So . If were , this would be .&lt;/p&gt;
    &lt;p&gt;Now means: The field but treat all elements as equivalent if they are a multiple of apart - meaning their difference is in . If were , then and would be equivalent, because their difference is , which is a multiple of and thus in .&lt;/p&gt;
    &lt;p&gt;This is exactly what “mod 7” is:&lt;/p&gt;
    &lt;p&gt;So is just all of .&lt;/p&gt;
    &lt;p&gt;Note that this collapses the infinite field of all possible integers down to a finite set of equivalence classes. In , all numbers are either in or are equivalents of one of those. So for practical purposes, there are only possible values in .&lt;/p&gt;
    &lt;p&gt;What do we need primes for? Well, has a practical problem: Sometimes multiplying two things results in a zero. Example for :&lt;/p&gt;
    &lt;p&gt;That is bad if we want to do a lot of multiplication within our modulo ring. Because if we ever accidentally hit a multiple of , we will get and that point it doesn’t matter what we multiply onto that - it will stay . So there are a lot of possible values that all collapse into the same when multiplied with certain numbers.&lt;/p&gt;
    &lt;p&gt;In the field of all integers, , we don’t have that problem. As long as we don’t multiply with itself, the results of a multiplication will never be . Good thing there is a solution for that: Using a prime modulus.&lt;/p&gt;
    &lt;head rend="h3"&gt;Prime Numbers, Finite Fields, and Cycles&lt;/head&gt;
    &lt;p&gt;Prime numbers have the nice feature that the modulo rings they induce, , are fields and not just rings, meaning addition and multiplication work just as well as they do in . In , multiplying by something other than will never result in a multiple of - because is prime and you can’t reach a prime from another number through multiplication. (Multiplying by itself doesn’t count, because .)&lt;/p&gt;
    &lt;p&gt;Because is a field and it has a finite amount of elements , it is called a “finite field” or a “Galois field” and sometimes denoted instead of . To be super precise, is a generalization and means “any finite field with elements”. just happens to be one of those - and is the most popular one.&lt;/p&gt;
    &lt;p&gt;Now we go a step further and look at a concept called primitive root. Before explaining that, let’s take a look at a use case for them first.&lt;/p&gt;
    &lt;p&gt;Imagine you want a pseudorandom permutation of , i.e., you don’t want the sequence but a more random-looking sequence tha still hits all of these numbers. It doesn’t have to be cryptographically random or unpredictable. It just needs to look at bit random - perhaps to de-cluster memory writes for better wear-leveling. Primitive roots give us a nice implementation for that.&lt;/p&gt;
    &lt;p&gt;In , multiplication never yields unless you multiply by . That means you can, e.g., keep doubling a number and will never accidentally hit :&lt;/p&gt;
    &lt;p&gt;Oh look at that, a cycle! This is what will always happen in a finite field. When you keep multiplying by the same number, you will eventually reach the number you started with. And at that point, you are in a cycle.&lt;/p&gt;
    &lt;p&gt;is a bit impractical though because its induced cycle only ever hits the numbers and never hits . Note that no cycle will ever hit , so the maximum cycle we can possible get with a modulus of is cycle length .&lt;/p&gt;
    &lt;head rend="h3"&gt;Primitive Roots and Prime Factors&lt;/head&gt;
    &lt;p&gt;And there are indeed numbers that generate a full cycle!&lt;/p&gt;
    &lt;p&gt;These numbers, numbers that induce a full cycle in a finite field, are called primitive roots. and are each primitive roots modulo . Since their induced cycle has a length of , the so-called order of and is .&lt;/p&gt;
    &lt;p&gt;More generally, a primitive root modulo some prime is a number whose order is , i.e., whose induced cycle has a length of . In that case . Expressed more formally: is a primitive root modulo if and only if the smallest for which is true, is&lt;/p&gt;
    &lt;p&gt;Now then how would you best check if a number is a primitive root modulo ? The obvious solution is to just count upwards from 1 through and check each time. That works but it can take a long time for big numbers. Turns out, we don’t have to check every possible from through . To understand that, take a look at from before:&lt;/p&gt;
    &lt;p&gt;We can see that the cycle has length and contains the numbers . We don’t see the numbers . But what happens when we use but we start at ?&lt;/p&gt;
    &lt;p&gt;Again, a cycle of ! And this time we’ve seen all the remaining numbers . If we instead start at or , we will get the same cycle over . So splits the entire finite field excluding 0 into two subsets: and .&lt;/p&gt;
    &lt;p&gt;Let’s look at another example: and try to find all cycles.&lt;/p&gt;
    &lt;p&gt;So partitions the non-zero elements into `.&lt;/p&gt;
    &lt;p&gt;Notice how all those cycles induced by the same number always have the same size? If you think about it, then that makes perfect sense. When you have but you “start at ”, you’re basically just taking the regular cycle and multiply its elements by . So the resulting cycle must have the same length: .&lt;/p&gt;
    &lt;p&gt;And this isn’t a coincidence. In fact, all cycles induced by the same number always have the same length. And that number always partitions the entire finite field without into disjoint sets of the same size.&lt;/p&gt;
    &lt;p&gt;How does that help us? Well, for it makes the cycle lengths and impossible! Because you can’t cover all non-zero elements by splitting them into sets of size or . More generally, cycles lengths (and thus orders) must always be a divisor of , i.e., the amount of all non-zero elements.&lt;/p&gt;
    &lt;p&gt;This is called Lagrange’s theorem: The order of the subgroup divides the order of the whole group.&lt;/p&gt;
    &lt;p&gt;So to check if is a primitive root, we don’t have to check all from through . We only have to check all divisors of ! Also, we obviously don’t have to check . The only number for which is itself, which always induces a cycle of and is thus never a primitive root for .&lt;/p&gt;
    &lt;p&gt;So we’re down to only having to check all divisors of that are larger than . That already eliminates most of the candidates.&lt;/p&gt;
    &lt;p&gt;But we can go even further!&lt;/p&gt;
    &lt;head rend="h3"&gt;Fast Primitivity Check&lt;/head&gt;
    &lt;p&gt;We only need to check some of the divisors. This is where prime factorization comes into play. Let’s say that we have the prime factors of . We will call these prime factors . Note that itself does not have any prime factors because - well - it is a prime number. But does have prime factors. In fact, will always be one of those prime factors because must be odd to be a prime and must therefore be even.&lt;/p&gt;
    &lt;p&gt;Now what if I told you that we only need to check the divisors of that we can obtain by dividing through a prime factor. Specifically, we only need to check&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;p&gt;Why is that? Well, first of all, notice that the “other divisors” are themselves divisors of either or :&lt;/p&gt;
    &lt;p&gt;And if , then that implies that&lt;/p&gt;
    &lt;p&gt;and so on.&lt;/p&gt;
    &lt;p&gt;Why? Well, because if were equal to one, then&lt;/p&gt;
    &lt;p&gt;and that is a contradiction! You can prove the same for any in general if you want. The general proof works just like this example.&lt;/p&gt;
    &lt;p&gt;Alright, so we only have to check all possible with being a prime factor of .&lt;/p&gt;
    &lt;p&gt;Here is a generator function that will return all primitive roots for a given prime modulus :&lt;/p&gt;
    &lt;p&gt;And you know what? Armed with this, we can not just find primitive roots but also primitive polynomials! We just need to translate this concept to polynomials!&lt;/p&gt;
    &lt;p&gt;… which is a bit tricky.&lt;/p&gt;
    &lt;head rend="h3"&gt;Binary Polynomials, Irreducibility and Primitive Elements&lt;/head&gt;
    &lt;p&gt;Alright, first things first: Polynomials. They can look something like this:&lt;/p&gt;
    &lt;p&gt;And through the black magic of math, we can treat these polynomials as multidimensional numbers:&lt;/p&gt;
    &lt;p&gt;And on these, we can perform the same kind of arithmetic as on integers: Addition, Multiplication, Subtraction, Division - and thus Modulo. Dividing one polynomial by another sounds odd? It is. We’ll skip the details here since we’ve already derailed enough. Just know that the intuition from integer arithmetic carries over to polynomials.&lt;/p&gt;
    &lt;p&gt;Now keep in mind that we are working with binary polynomials, i.e., polynomials whose coefficients are either or . And we can represent those as integers. For example, represents the polynomial&lt;/p&gt;
    &lt;p&gt;We use these binary polynomials because they have a numbers of nice properties that things like BCH error correction codes rely on. For example, we can represent all binary strings of length as a -degree polynomial with binary coefficients. For integers, we’d represent those strings as integers from to . But we would have problems doing modular arithmetic on those strings because isn’t necessarily prime and then isn’t a field. For polynomials, there are nice and efficient ways to build a field over -degree binary polynomials.&lt;/p&gt;
    &lt;p&gt;Throughout this section we have to keep in mind that the integer representation ( in the example above) is just a representation of the polynomial in memory. The polynomial is not an integer and we can’t just do regular integer arithmetic like addition and multiplication with Python’s &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;*&lt;/code&gt; with it. Polynomials have their own arithmetic and they work differently.&lt;/p&gt;
    &lt;p&gt;Since we are dealing with binary polynomials, the coefficients of the polynomials are all either or . More formally, the coefficients are elements of , which is basically the same as or “mod 2”.&lt;/p&gt;
    &lt;p&gt;The set of all binary polynomials is called . In CompSci terms, these are all possible bit arrays.&lt;/p&gt;
    &lt;p&gt;So since we’re moving from working on integers to working on binary polynomials, is our binary polynomial-equivalent of , with being all integers and being all binary polynomials.&lt;/p&gt;
    &lt;p&gt;While we used to represent a single integer in , we will use to represent a single polynomial in .&lt;/p&gt;
    &lt;p&gt;Similar to how is the set of all multiples of n, is the set of all multiples of - notice the double-paranthesis.&lt;/p&gt;
    &lt;p&gt;And similar to how is but treating two integers as equivalent if their difference is a multiple of , is but treating two polynomials as equivalent if their difference is a multiple of . It is essentially “mod ” in polynomial world.&lt;/p&gt;
    &lt;p&gt;If we have an integer and can’t be divided by another integer, then is prime. Analogously, if we have a polynomial and can’t be reduced by another polynomial, then is irreducible.&lt;/p&gt;
    &lt;p&gt;If is prime, then there is at least one primitive root in , so that spans the entire . If is irreducible, then there is at least one primitive element in so that spans the entire .&lt;/p&gt;
    &lt;p&gt;(Side note: “primitive root” is a legacy term only used for . “primitive element” is the general term. They mean the same thing.)&lt;/p&gt;
    &lt;p&gt;Also, while the order of is just , the order of is , which is the amount of all possible -bit arrays.&lt;/p&gt;
    &lt;p&gt;Now we are almost there! We have already come far enough where we can recognize that an irreducible polynomial is analogous to a prime integer .&lt;/p&gt;
    &lt;p&gt;Now, what is a primitive polynomial then?&lt;/p&gt;
    &lt;head rend="h3"&gt;Primitive Polynomials&lt;/head&gt;
    &lt;p&gt;Well, that’s simple! A primitive polynomial is an irreducible polynomial with one additional requirement:&lt;/p&gt;
    &lt;p&gt;The super simple polynomial must be a primitive element, i.e., must span the entire .&lt;/p&gt;
    &lt;p&gt;Why is that useful? Well, because that means that every non-zero polynomial in can be represented as for some integer . So every non-zero polynomial can be represented as an integer and we have a random-looking permutation of all non-zero binary polynomials in .&lt;/p&gt;
    &lt;p&gt;And that is what BCH codes use and why we require a primitive polynomial.&lt;/p&gt;
    &lt;p&gt;Now, how do we calculate all primitive polynomials then? We don’t. There are infinitely many. But we can calculate all primitive polynomials of some specific degree ! And how do we do that? Well, using what we have already built for finding primitive roots for integers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Iterate through all possible polynomials of degree . For each candidate , (step 1) &lt;list rend="ul"&gt;&lt;item&gt;check if irreducible. Discard if not. (step 2)&lt;/item&gt;&lt;item&gt;check if (the simple polynomial with integer representation 2) is a primitive element in the finite field induced by . (step 3) &lt;list rend="ul"&gt;&lt;item&gt;the wanted cycle length is the amount of all possible polynomials of degree :&lt;/item&gt;&lt;item&gt;get all prime factors of&lt;/item&gt;&lt;item&gt;check if for all prime factors&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For (step 1), we want a primitive polynomial with degree , so we only consider polynomials where their m-th coefficient is 1 and all higher-order coefficients are 0. This means that their integer representations are in&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;range(2^m, 2^(m+1))&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To speed it up by factor 2: We can’t have any polynomials where the constant term, i.e., the lowest-order coefficient is zero. Example:&lt;/p&gt;
    &lt;p&gt;(note that there is no at the end)&lt;/p&gt;
    &lt;p&gt;That’s because every polynomial with a zero constant term is divisible by the polynomial and is thus not irreducible and thus not primitive. So we only iterate over polynomials with a constant term of . In the integer representation, those are the odd integers, so we use&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;range(2^m + 1, 2^(m-1), 2)&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;For (step 2), checking whether a polynomial is irreducible is the analogue to checking whether an integer is prime.&lt;/p&gt;
    &lt;p&gt;Ruling out even integer representations of polynomials in (1) skips half the possible candidates because those are all reducible. Unfortunately, there will still be plenty of reducible polynomials left. Just like there are a lot of non-prime numbers among the odd integers.&lt;/p&gt;
    &lt;p&gt;For a full irreducibility test, we use Rabin’s Test. The implementation has a similar structure as the Rabin-Miller Test for integers. We won’t cover either here because this section is long enough already.&lt;/p&gt;
    &lt;p&gt;For (step 3), we do the same as for primitive roots in integer-world, except we use a polynomial-compatible &lt;code&gt;pow&lt;/code&gt; function:&lt;/p&gt;
    &lt;head rend="h3"&gt;Finding all Primitive Polynomials&lt;/head&gt;
    &lt;p&gt;And now we can finally compute all the primitive polynomials with degree 14. The full script is attached in the addendum. It takes about 1 second (single-threaded, on my laptop) to list all 756 primitive polynomials for degree 14. Yes - there are only 756!&lt;/p&gt;
    &lt;p&gt;And how much faster does this make our script compared to the naive &lt;code&gt;range(2**14 + 1, 2**15, 2)&lt;/code&gt;
that lists half of all possible binary polynomials of degree 14?&lt;/p&gt;
    &lt;p&gt;About 1 second on a single-threaded run on my laptop…&lt;/p&gt;
    &lt;p&gt;Turns out just throwing a bunch of mostly non-primitive polynomials against bchlib and catching the exception is just as fast as doing it properly…&lt;/p&gt;
    &lt;p&gt;Oh, and you know what? You could have also just pulled a list of all primitive polynomials of degree 14 from the internet. Because, well, we aren’t the first to generate that list.&lt;/p&gt;
    &lt;p&gt;But hey, we learned some math.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://neodyme.io/en/blog/drone_hacking_part_1/"/><published>2026-01-17T02:35:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656045</id><title>The 'untouchable hacker god' behind Finland's biggest ever crime</title><updated>2026-01-17T14:10:06.266938+00:00</updated><content>&lt;doc fingerprint="260593d10eb24786"&gt;
  &lt;main&gt;
    &lt;p&gt;Tiina Parikka was half-naked when she read the email. It was a Saturday in late October 2020, and Parikka had spent the morning sorting out plans for distance learning after a Covid outbreak at the school where she was headteacher. She had taken a sauna at her flat in Vantaa, just outside Finland’s capital, Helsinki, and when she came into her bedroom to get dressed, she idly checked her phone. There was a message that began with Parikka’s name and her social security number – the unique code used to identify Finnish people when they access healthcare, education and banking. “I knew then that this is not a game,” she says.&lt;/p&gt;
    &lt;p&gt;The email was in Finnish. It was jarringly polite. “We are contacting you because you have used Vastaamo’s therapy and/or psychiatric services,” it read. “Unfortunately, we have to ask you to pay to keep your personal information safe.” The sender demanded €200 in bitcoin within 24 hours, otherwise the price would go up to €500 within 48 hours. “If we still do not receive our money after this, your information will be published for everyone to see, including your name, address, phone number, social security number and detailed records containing transcripts of your conversations with Vastaamo’s therapists or psychiatrists.”&lt;/p&gt;
    &lt;p&gt;Parikka swallows hard as she relives this memory. “My heart was pounding. It was really difficult to breathe. I remember lying down on the bed and telling my spouse, ‘I think I’m going to have a heart attack.’”&lt;/p&gt;
    &lt;p&gt;Someone had hacked into Vastaamo, the company through which Parikka had accessed psychotherapy. They’d got hold of therapy notes containing her most private, intimate feelings and darkest thoughts – and they were holding them to ransom. Parikka’s mind raced as she tried to recall everything she’d confided during three years of weekly therapy sessions. How would her family react if they knew what she’d been saying? What would her students say? The sense of exposure and violation was unfathomable: “It felt like a public rape.”&lt;/p&gt;
    &lt;p&gt;Therapy had been Parikka’s lifeline. Now 62, she’d had three children by the time she was 25, including twins who had been born extremely prematurely in the 1980s, weighing only a few hundred grams each. One grew up with cerebral palsy; the other is blind. Parikka spent years juggling medical emergencies, surgeries and hospital stays with a demanding job and a crumbling marriage. “During those years, nobody ever asked me, the mother, ‘How are you?’”&lt;/p&gt;
    &lt;p&gt;She divorced in 2014 and met her current partner a year later. By then, her children were adults with independent lives. After decades of putting everyone’s else’s needs before her own, she should have been finally able to exhale. Instead, she had a breakdown. “I had full-scale anxiety running through my body all the time. I couldn’t sleep. I had panic attacks. I couldn’t eat.” Driving at high speed on the highway one day, dark thoughts descended. “I was thinking, I wouldn’t mind if this car crashed.”&lt;/p&gt;
    &lt;p&gt;In search of urgent help, she went to Google, which led her to Vastaamo, Finland’s one-stop digital shop for people in search of psychotherapy. No doctor referral was necessary. She managed to book a session for the very next day. “It was that easy.”&lt;/p&gt;
    &lt;p&gt;Being able to confide in a total stranger felt liberating. She told her therapist things she had never told another soul. “Trauma in relationships. The disappointment and tragedy of having disabled children, and the influence it had on my life,” she says. “Silly things, childish things. It’s very human to feel hate, anger, rage.”&lt;/p&gt;
    &lt;p&gt;After Parikka read the email that left her struggling to breathe, she had no idea where to turn for help. She rang the emergency services, but the police told her to get off the line; they needed to keep it free for real emergencies. In her bathrobe, her phone still in her hand, she felt utterly alone.&lt;/p&gt;
    &lt;p&gt;But Parikka was far from alone. Across Finland, 33,000 people who had used Vastaamo were discovering that a hacker had got hold of their therapy notes and was holding them to ransom. These were people who, by definition, were likely to be vulnerable, in need of help. Each was experiencing a very personal, individual terror. In a country of only 5.6 million people, everyone knows someone who was hacked.&lt;/p&gt;
    &lt;p&gt;Some victims’ notes had already been cherrypicked for the world to see. Three days before the extortion emails were sent, someone using the handle ransom_man had left posts on the dark web, on r/Suomi, the Finnish-language subreddit, and on Ylilauta, Finland’s equivalent to 4chan. This time, the post was in English. “Hello Finnish Colleagues,” it began. “We have hacked the psychotherapy clinic vastaamo.fi and taken tens of thousands of patient records including extremely sensitive session notes and social security numbers. We requested a small payment of 40 bitcoins (nothing for a company with yearly revenues close to 20 million euros) but the CEO has stopped responding to our emails. We are now starting to gradually release their patient records, 100 entries every day.”&lt;/p&gt;
    &lt;p&gt;There was a link to the dark web, where 100 records were already on display. Directly below it, ransom_man had signed off the post with a single word: “Enjoy!”&lt;/p&gt;
    &lt;p&gt;The 100 records included those of politicians, police officers and prominent public figures. Their names appeared alongside therapy notes that contained details of adultery, suicide attempts, paedophilia and sexual violence. Some of the records belonged to children. And whoever was behind the hack was true to their word: the next day, 100 more patient records were uploaded.&lt;/p&gt;
    &lt;p&gt;Some victims went searching on the dark web in a desperate attempt to see if their records were out there. Some paid the ransom, scrabbling to get hold of bitcoin while the clock ticked down. Lawyers representing the victims have told me they know of at least two cases where people took their own lives after they discovered their therapy notes had been hacked.&lt;/p&gt;
    &lt;p&gt;But for all of them, it was already too late. At 2am on 23 October 2020 – the day before the emails began to arrive in tens of thousands of inboxes – ransom_man had uploaded a much larger file. It contained every record of every single patient on Vastaamo’s database. Everyone’s therapy notes had already been published, for free, for everyone in the world to see.&lt;/p&gt;
    &lt;p&gt;Who was behind the biggest crime Finland had ever known? And might they have been motivated by something other than money? I have spent 18 months trying to answer these questions, following threads across Europe and the US. They culminated in a visit to a prison, and one of the most chilling conversations I have ever had.&lt;/p&gt;
    &lt;p&gt;Finland has been ranked the happiest country on Earth by the UN for the last eight years in a row. A world leader in childcare and education, Finland is also famously hi-tech: it’s the most digitalised country in Europe, renowned for its communications sector (as the home of Nokia) and leading the way when it comes to cybersecurity and AI innovation. But Finland is also a place of extremes. It has more heavy metal bands per capita than any other nation. In the far north, for the few days around the winter solstice, the sun does not rise.&lt;/p&gt;
    &lt;p&gt;Vastaamo had long been considered an example of how Finland was getting it right when it came to digital tech. Founded in 2008 by entrepreneur Ville Tapio and his mother, Nina, a psychotherapist, the aim was to open up therapy to the masses, removing the stigma of asking for help. The platform made it easy for people to see who was free, where, and what therapeutic approach they specialised in. The logo had the colour palette of a first-aid kit, with white lettering in a green speech bubble. Vastaamo means “a place for answers”.&lt;/p&gt;
    &lt;p&gt;It was an attractive platform for therapists, too: they didn’t have to worry about marketing or billing – Vastaamo would take care of all of that. The company even provided a behind-the-scenes digital interface where therapists could make and store their notes. This formula, combined with the increasing demand for therapy services, meant Vastaamo grew fast. It opened its own network of around 20 clinics across Finland, employing more than 220 psychotherapists by 2018, leading some in Finland to refer to it as “the McDonald’s of therapy”. In the years before Zoom and Teams were part of our daily lives, the remote therapy also offered by Vastaamo was groundbreaking. In 2019, a private equity firm bought a majority stake in the company, earning the Tapio family a payout of more than €5m.&lt;/p&gt;
    &lt;p&gt;Meri-Tuuli Auer, 30, describes using Vastaamo as “like Uber for therapy – convenient, accessible, relatively cheap”. She picked her therapist because he offered cognitive psychotherapy – and she liked his photo. “He looked nice. He looked approachable.”&lt;/p&gt;
    &lt;p&gt;Auer’s home, on the outskirts of Helsinki, is a riot of pink. There are Barbie dolls, Barbie books and Barbie-themed handbags on her shelves, as well as a glittery open-top Barbie sports car. A pole-dancing pole takes pride of place in the centre of her living room.&lt;/p&gt;
    &lt;p&gt;“I’m a mixed personality,” she tells me over tea in Moomin mugs. “I love being around people, but I get that inkling, that doubt: maybe they all think I’m full of shit and stupid and ugly and I have no idea what I’m doing.” Auer has struggled with depression for much of her life. When she was 18, she was in a secretive, difficult relationship with a man 29 years her senior, which made her self-esteem plummet further. She was drinking heavily. “If I hadn’t gone to therapy, I don’t know what would have become of me. Maybe there is another universe where I didn’t make it to 30.”&lt;/p&gt;
    &lt;p&gt;Most of the cost of Auer’s treatment was covered by the Finnish healthcare system; she paid only about €25 for each weekly session. She was making great strides. “After going to therapy in 2018 and 2019, I had gained a basic sense of security. That was lost in 2020.”&lt;/p&gt;
    &lt;p&gt;Vastaamo’s CEO knew the company’s patient registry was being held to ransom weeks before his customers found out. On 28 September 2020, Ville Tapio received an email demanding the bitcoin equivalent of €450,000 to keep it safe. Sample patient records attached to the email proved the extortionist wasn’t bluffing. Tapio called in a cybersecurity firm to investigate.&lt;/p&gt;
    &lt;p&gt;Medical information is an obvious target for would-be extortionists, says Antti Kurittu, the security specialist Tapio hired. But this was something else: “Whatever I tell a therapist is, by its very nature, a lot more private than what my blood pressure is,” he says, drily.&lt;/p&gt;
    &lt;p&gt;Kurittu used to be a detective, investigating cybercrimes for the Finnish police; he says he insisted they be told about the ransom attempt so they could begin a parallel investigation. Meanwhile, he began inspecting Vastaamo’s server, looking for clues as to who might be behind the hack – and one of the first things he noticed was how lax security had been. “It was definitely unfit for purpose for storing this kind of information,” he says. He tells me that the patient records database was accessible via the internet; there was no firewall and, perhaps most egregiously, it was secured with a blank password, so anyone could just press enter and open it. Kurittu determined that whoever had hacked Vastaamo had probably just been scanning the internet in search of any badly secured databases that could be monetised. “They tried a bunch of bank vaults to see which ones were open, and just happened to stumble on this one.”&lt;/p&gt;
    &lt;p&gt;For a few weeks, the hacker and Vastaamo exchanged emails, but there was no question that Vastaamo would pay the ransom. If they did, they’d have to trust a criminal’s word that the records had been destroyed – plus, Kurittu says, it goes against the national character. “Finns are a bit of a belligerent bunch. We’re not known for paying ransom quietly or easily, which I take great national pride in.”&lt;/p&gt;
    &lt;p&gt;After ransom_man started leaking patient records to put pressure on the company, Kurittu kept a close eye on the server being used to publish them. He had a hunch whoever was behind this was either Finnish, or had lived in Finland for a long time: they knew which famous names to flaunt from the patient records.&lt;/p&gt;
    &lt;p&gt;When Auer learned about the hack, she downloaded a browser that would enable her to access the dark web, for the first time in her life. “I was thinking to myself, I just have to see if my records are there.” She found her name wasn’t among the first batch posted, and closed the file without reading anyone’s records. But she saw other people discussing what they’d seen. “People had already picked – in their opinion – the funniest parts from the patient records. They were laughing at these people’s misery. A 10-year-old child had gone to therapy, and people found it funny.”&lt;/p&gt;
    &lt;p&gt;Auer began to spiral. “I closed myself in at home, I didn’t want to leave, I didn’t want anyone to see me,” she tells me. She had no hope that the hacker would ever be found. “It’s not that I don’t trust the police in Finland – it’s just that it seemed like an impossible task.”&lt;/p&gt;
    &lt;p&gt;But the much larger file ransom_man had uploaded to the dark web – the one that contained every single one of Vastaamo’s patient records – also included vital clues to his identity. The first three batches of therapy notes had been posted manually, but when the hacker had tried to automate the process, he had not only accidentally uploaded all of the therapy notes, but also his entire home folder. It had appeared only briefly before it was taken down, along with a post that read “whoopsie :D”, but ransom_man had screwed up.&lt;/p&gt;
    &lt;p&gt;“After spending several evenings with the file, I had the feeling I’d seen this kind of thing before,” Kurittu says. The data on the hacker’s home drive wasn’t systematically organised and arranged in folders, as you would expect from someone for whom extortion was a business. “It had that sort of chaotic, passionate hobby feeling to it.” And there was something about the childish way ransom_man had named some of the files that was eerily familiar (the one containing all the patient data was entitled “therapissed”).&lt;/p&gt;
    &lt;p&gt;Kurittu’s mind went back to 2013 when he was a senior detective constable for the Helsinki police, and the file names he’d seen on a computer he’d seized from a 16-year-old boy. “It made me think of Julius Kivimäki.”&lt;/p&gt;
    &lt;p&gt;Aleksanteri Kivimäki – who used to go by his middle name, Julius, or the online handle zeekill – had long been notorious among cybersecurity investigators. Not because of any particular talent as a hacker, but because he seemed prepared to go further than most who spend their time in the darkest parts of the internet.&lt;/p&gt;
    &lt;p&gt;Aged 14, Kivimäki was involved with a group called Hack the Planet (named after the tagline of the 1995 movie Hackers). They would break into big companies and show off what they had managed to steal online. “It was for the LOLs,” says Blair Strater, a former hacker from Illinois who hung out with Kivimäki in internet relay chat forums at that time. “You notice that something is open and you just take it. It’s not targeted.”&lt;/p&gt;
    &lt;p&gt;This kind of hacking was about impressing others – winning online clout, not extorting money. But some of those involved may have felt they were also serving a noble purpose: exposing security vulnerabilities in major corporations, or the hypocrisy of cybersecurity firms who claimed to be qualified to advise businesses while being unable to secure their own network.&lt;/p&gt;
    &lt;p&gt;Strater found Kivimäki amusing, at first. “A lot of the things he did early on were objectively funny,” he tells me over Zoom from his home in Illinois. When I ask Strater whether I would find them funny, he clarifies that his humour was an acquired taste best suited to 4chan. But in 2010, when Strater was 17 and Kivimäki was 14, they fell out over which one of them was going to publish a report of a recent hack.&lt;/p&gt;
    &lt;p&gt;Orders of pizzas and Chinese takeaway began arriving at the home Strater shared with his parents and younger sister on the outskirts of Chicago; when they opened the door, the delivery driver would ask for Julius Kivimäki. “Taxis were ordered. Hookers were ordered,” Strater says. “My father had to send away a big dump truck filled with gravel.” Strater received a blizzard of letters from credit card and insurance companies, and government agencies, including one from the department of social security confirming that an appointment with the welfare office had been created for him and his spouse – Julius Kivimäki.&lt;/p&gt;
    &lt;p&gt;Then, at 2am one morning, police in body armour carrying guns with laser sights turned up outside the Straters’ home, responding to reports that Blair had beaten his mother to death in a drug-fuelled rage. When she answered the door, they took her blood pressure to verify that she was, in fact, alive. It was the first of dozens of so-called swatting attacks the family would endure. After a lull of a couple of months, Strater learned that someone using his name had emailed a bomb threat to a local police officer; it led to Strater spending three weeks over Christmas in a juvenile detention centre.&lt;/p&gt;
    &lt;p&gt;Several years into their feud, in 2015, someone hacked Elon Musk and Tesla’s Twitter accounts, and tweeted that anyone who rang the Straters’ landline or showed up at their home would get a free car; their phone rang off the hook for days, and Blair’s father had to turn several disappointed people away from their porch. Someone using Blair’s mother’s name posted a threat to shoot up the elementary school where his 10-year-old sister was a pupil. His mother’s LinkedIn and Twitter accounts were hacked and filled with juvenile, racist posts, as well as antisemitic insults directed at the company where she worked as a healthcare statistician. Within months, she had lost her job.&lt;/p&gt;
    &lt;p&gt;The campaign of terror lasted for many more years. Strater says it’s never going to be fully over. “It’s like having cancer: it’s never really cured, it goes into remission,” he says. “Every so often, someone would hit me up and say, ‘Hey, I was one of the people that helped Julius do these things.’ Sometimes they would say, ‘He made me do them. He was blackmailing me,’ which is something he does to an awful lot of people. I want to make this very clear: I am not the person zeekill fucked with the most.”&lt;/p&gt;
    &lt;p&gt;Indeed, Kivimäki set his sights far beyond the Strater family. In August 2014 – days after his 17th birthday – he rang in a fake bomb threat that grounded a flight carrying John Smedley, president of Sony Online Entertainment, who oversaw PlayStation’s multiplayer network. A group calling themselves Lizard Squad claimed responsibility, posting almost nonsensically on Twitter that the attack was in sympathy with Islamic State. Lizard Squad struck again, on 25 December 2014, with a cyber-attack that shut down Xbox and PlayStation, and ruined Christmas morning for millions. Brazenly, Kivimäki gave interviews to BBC 5 Live and Sky News as a Lizard Squad spokesperson, claiming they did the hack both to amuse themselves and to expose Microsoft and Sony’s poor cybersecurity. He seemed to revel in the chaos and drama. He appeared on camera on Sky News; he used a fake name, but his boyish face – blond hair, blue eyes, plump cheeks – was visible for all to see.&lt;/p&gt;
    &lt;p&gt;In July 2015, following Kurittu’s investigation with the Finnish police, Kivimäki was convicted of hacking into servers at MIT and Harvard universities, as well as money laundering and fraud. He was found guilty of more than 50,000 data breaches, and received a two-year suspended sentence; he had his computer confiscated and was forced to pay back more than €6,000 obtained through his crimes. He never faced justice for any of the offences he perpetrated against Blair Strater and his family.&lt;/p&gt;
    &lt;p&gt;Shortly after he received his suspended sentence, Kivimäki updated his Twitter bio to read “untouchable hacker god”.&lt;/p&gt;
    &lt;p&gt;Kivimäki spent the next few years travelling the world. During lockdown, he lived in an air-conditioned apartment in Westminster, 20 metres away from the central London headquarters of MI5. There were trips to Dubai, Hong Kong, Barcelona and Paris. According to the images of himself he liked to post online, he was living the life of an international jetsetter. But he was not, in the end, untouchable.&lt;/p&gt;
    &lt;p&gt;Police made a micropayment of 0.1 bitcoins to ransom_man. They were able to determine that, when it was laundered into real-world currency, it was transferred into Kivimäki’s bank account. The home folder ransom_man had accidentally uploaded had led the police to some servers, one of which had been paid for using a credit card linked to him – the same one he’d been using to pay for Apple services and an OnlyFans subscription.&lt;/p&gt;
    &lt;p&gt;As investigators traced the history on ransom_man’s home folder, they were able to determine that, as well as looking for keywords such as rape, abuse and child molestation in the database of patient records, the hacker had also searched for Kivimäki’s home address, and the names of his family members. “Before publication, he ensured there was no harmful information about him, or people close to him,” Pasi Vainio, the lead prosecutor on the case, tells me. Those searches took place using an IP address linked to Kivimäki’s Westminster apartment. “He was in London when the crimes were committed.”&lt;/p&gt;
    &lt;p&gt;But it was a drawn-out, arduous investigation. There were terabytes of data to comb through. The crime had so many victims that the police had to create an online portal for everyone to register and give their statements. That generated more than 21,000 criminal reports, all of which needed to be looked at individually. So it was October 2022 – two years after Parikka, Auer and the other victims had received their ransom demands – before Vainio signed an arrest warrant for Kivimäki. His face – chubby-cheeked and floppy-haired – was added to Europol’s list of most-wanted fugitives, alongside murderers and drug traffickers.&lt;/p&gt;
    &lt;p&gt;On 3 February 2023, French police were alerted to a report of domestic violence taking place in a flat in a Paris suburb. Officers used a battering ram to enter the property and found a man and a woman inside. The man was pale and white-blond, but when asked to identify himself he handed over a Romanian passport that gave his name as Asan Amet. “We have a Scandinavian-looking guy, 195cm tall,” Vainio tells me with a smile. “I think the French police just thought something’s off.” They searched their databases and discovered Amet was one of Kivimäki’s known aliases. He was handed over to the Finnish authorities a few weeks later.&lt;/p&gt;
    &lt;p&gt;“I don’t know what I had expected, but I was surprised to see that he looked so normal,” Auer says. “He looks like a regular Finnish young man. It did make me feel like it could have been anyone.”&lt;/p&gt;
    &lt;p&gt;“I had heard that he was in a court hearing,” Parikka says. “We have a habit – every night at 8.30pm, I’ll lie here on the couch with my spouse and watch the main news. Without warning, Kivimäki was there on the screen. Kivimäki came to my living room.” She glances over to her couch, metres away from where we sit, and is overcome with tears. “I didn’t sleep the next night.”&lt;/p&gt;
    &lt;p&gt;But when the trial began, in November 2023, Parikka was determined to watch Kivimäki face justice. The logistics of inviting more than 21,000 registered victims to court were impossible; instead, proceedings were relayed to public spaces such as cinemas so that the plaintiffs could watch in real time. In a case that was all about the right to privacy and anonymity, it sounds a profoundly awkward setup. “We were all sitting far away from each other,” Auer says. “It was dead silent.” Parikka had a similar experience. “We pretty much kept to ourselves.”&lt;/p&gt;
    &lt;p&gt;On 30 April 2024, Kivimäki was found guilty of all charges – including 9,600 counts of aggravated invasion of privacy and more than 21,300 counts of attempted aggravated extortion – and sentenced to six years and three months in prison: a long stretch by Finnish standards, but shy of the seven-year maximum he could have received. His appeal against his sentence is currently under way.&lt;/p&gt;
    &lt;p&gt;Even if his conviction is upheld, he will be a free man by the end of this year.&lt;/p&gt;
    &lt;p&gt;“The sentencing scale is too low, in my opinion. But that’s the framework we have in Finland,” Vainio says. He tells me a colleague has tried to quantify the harm caused, using the conservative estimate that each person had endured a week of agony as a result of the hack. “When you multiply it with the number of victims of this case, you would have 635 years of suffering.”&lt;/p&gt;
    &lt;p&gt;Now 28, Kivimäki has served much of his sentence in a spotless, bright but suitably austere facility in Turku, south-west Finland, a two-hour train ride from Helsinki. For months, he had refused to grant me an interview, but while I am in Finland reporting this story, he changes his mind. As I sit in silence in the prison’s visitor room for what feels like hours, watching the clock tick down behind a panel of reinforced glass, I wonder if Kivimäki is trolling me; if he has dragged me over here simply to derail the other interviews I already had scheduled, with no intention of ever leaving his cell. But after 40 minutes he appears. With his white-blond hair, ice-blue eyes and razor burn, and dressed in a black T-shirt and shorts, he looks like an overgrown teenage boy.&lt;/p&gt;
    &lt;p&gt;He didn’t do it, he says; he’s simply a victim of his own notoriety. “They had to find somebody. They just chose somebody who was convenient for the story.” When I point out that there’s an enormous amount of circumstantial evidence linking him to the hack, Kivimäki is defiant. “The obvious answer is that it’s just somebody close to me.” He has an idea who it is, he continues, but he isn’t prepared to name names.&lt;/p&gt;
    &lt;p&gt;It seems very selfless to do time for someone else’s crime, I say. I tell him Parikka says having her therapy notes held to ransom felt like a public rape. “I’m sure that’s how she felt,” he replies, blankly. “It’s quite remote to me. I’m involved, in that I was in court over this stuff, but I didn’t do it. It’s another story in the news.”&lt;/p&gt;
    &lt;p&gt;As a fellow human being rather than the person convicted of the crime, I ask, what’s your response to people taking their lives after having their therapy notes stolen? “There’s a lot of terrible things going on in the world. I don’t really feel any differently about this. I turn on the news and there’s people dying in Gaza or wherever. It’s like, how do you feel about that? I think the honest answer for most people is that they just … don’t.” You don’t have anything to say to the victims? “Not really,” he replies. “These are nameless, faceless people.”&lt;/p&gt;
    &lt;p&gt;“There’s been just one question that I would ask Kivimäki,” Parikka says. “That would be: ‘Was there ever such a moment that you felt empathy?’ I don’t think he’s able to put himself into anybody else’s situation.” She pauses. “I think that he really needs therapy.”&lt;/p&gt;
    &lt;p&gt;Vastaamo was declared bankrupt in February 2021. Days after patients received the ransom emails, the board announced that it had let the CEO, Ville Tapio, go. In April 2023, Tapio was found guilty of criminal negligence in his handling of patient data. His conviction was overturned on appeal in December 2025. (He declined my requests to interview him.)&lt;/p&gt;
    &lt;p&gt;“I have actually been more angry towards Ville Tapio than I have been towards Kivimäki,” Auer says. “As CEO of the company, he had the responsibility to make sure that it was prepared for all kinds of risks, and that they had sufficient information security. It seems like it was never a priority to him.” What was his priority? “Making money. He ran a very successful business.”&lt;/p&gt;
    &lt;p&gt;“I believe that originally the Tapios were wanting to help people and make therapy available,” Parikka says. “There are now maybe thousands of people who will never use therapy again, because they can never trust. And that’s really bad.”&lt;/p&gt;
    &lt;p&gt;Alongside more than 6,000 other plaintiffs, Auer and Parikka are part of a civil case suing Kivimäki for damages. Despite the lifestyle he projects online, he claims not to have the funds to pay damages; so far, no one has been able to find his assets. The government has agreed to pay compensation to victims – anything from a few hundred euros to a few thousand, depending on how many pages of their therapy notes Vastaamo had in its database, and how sensitive the information contained in those pages was – but the sum is likely to be symbolic. How can you ever repay the damage of being exposed in this way?&lt;/p&gt;
    &lt;p&gt;Copies of the patient files have been circulating ever since they were first released in October 2020. At one point, someone created a special search engine for browsing the database. This doesn’t surprise Parikka. “Kivimäki isn’t just one of a kind,” she says. “I know human curiosity. People want to know.”&lt;/p&gt;
    &lt;p&gt;Other people are as prepared as Kivimäki was to break moral and legal boundaries – for money, for online clout, out of ghoulish curiosity or simply for the LOLs. In May, Finnish police announced that there was a second suspect in the Vastaamo case, a US citizen living in Estonia – suspected of aiding and abetting Kivimäki, helping prepare the files. He has been charged with assisting in the attempted extortion.&lt;/p&gt;
    &lt;p&gt;In an era when AI models are trained on our Zoom conversations, emails and status updates, it is naive to believe that anything can ever be fully secure. The human need to confide in others can be met in an extraordinary range of ways in the digital age. In a world of unparalleled connectivity, can our innermost secrets ever be truly safe?&lt;/p&gt;
    &lt;p&gt;Kivimäki thinks we are all clinging on to analogue expectations about privacy in a digital world. “So many of our worst secrets – I mean worst of worst, things we might really, really not want to share with the entire world – they exist online. They’ll exist in the database of some company you used,” he tells me. “Everybody’s photos, everybody’s text-messaging histories.” He fixes me with his eyes. “You fundamentally want to believe in this privacy. But, on the other hand, I don’t know how you’re going to get there.”&lt;/p&gt;
    &lt;p&gt;Intrigue: Ransom Man, Jenny Kleeman’s six-part series for BBC Radio 4, is available now on BBC Sounds.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/technology/2026/jan/17/vastaamo-hack-finland-therapy-notes"/><published>2026-01-17T07:29:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656358</id><title>Show HN: Streaming gigabyte medical images from S3 without downloading them</title><updated>2026-01-17T14:10:05.613629+00:00</updated><content>&lt;doc fingerprint="286dba183daf5de1"&gt;
  &lt;main&gt;
    &lt;p&gt;A modern, cloud-native tile server for Whole Slide Images. One command to start serving tiles directly from S3.&lt;/p&gt;
    &lt;code&gt;# Installation (requires Rust, see alternatives below)
cargo install wsi-streamer

# On your local machine
wsi-streamer s3://my-slides-bucket --s3-region eu-west-3&lt;/code&gt;
    &lt;p&gt;That's it. No configuration files, no local storage, no complex setup. Open &lt;code&gt;http://localhost:3000/view/sample.svs&lt;/code&gt; in your browser to view a slide.&lt;/p&gt;
    &lt;p&gt;Whole Slide Images are large (1-10GB+) and typically live in object storage. Traditional viewers require downloading entire files before serving a single tile. WSI Streamer takes a different approach: it understands slide formats natively, fetches only the bytes needed via HTTP range requests, and returns JPEG tiles immediately.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Range-based streaming — fetches only the bytes needed for each tile, no local files&lt;/item&gt;
      &lt;item&gt;Built-in viewer — OpenSeadragon-based web viewer with pan, zoom, and dark theme&lt;/item&gt;
      &lt;item&gt;Native format support — Rust parsers for Aperio SVS and pyramidal TIFF&lt;/item&gt;
      &lt;item&gt;Production-ready — HMAC-SHA256 signed URL authentication&lt;/item&gt;
      &lt;item&gt;Multi-level caching — slides, blocks, and encoded tiles&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install from crates.io:&lt;/p&gt;
    &lt;code&gt;cargo install wsi-streamer&lt;/code&gt;
    &lt;p&gt;Or build from source:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/PABannier/WSIStreamer.git
cd WSIStreamer
cargo build --release&lt;/code&gt;
    &lt;p&gt;Or run with Docker:&lt;/p&gt;
    &lt;code&gt;# Pull from GitHub Container Registry
docker run -p 3000:3000 -e WSI_S3_BUCKET=my-bucket ghcr.io/pabannier/wsistreamer:latest

# Or use Docker Compose for local development with MinIO
docker compose up --build&lt;/code&gt;
    &lt;code&gt;# Serve slides from S3
wsi-streamer s3://my-slides

# Custom port
wsi-streamer s3://my-slides --port 8080

# S3-compatible storage (MinIO, etc.)
wsi-streamer s3://slides --s3-endpoint http://localhost:9000&lt;/code&gt;
    &lt;code&gt;# List slides
curl http://localhost:3000/slides

# Get slide metadata
curl http://localhost:3000/slides/sample.svs

# Fetch a tile (level 0, position 0,0)
curl http://localhost:3000/tiles/sample.svs/0/0/0.jpg -o tile.jpg

# Get thumbnail
curl "http://localhost:3000/slides/sample.svs/thumbnail?max_size=256" -o thumb.jpg&lt;/code&gt;
    &lt;code&gt;# Enable HMAC-SHA256 authentication
wsi-streamer s3://my-slides --auth-enabled --auth-secret "$SECRET"

# Generate signed URLs
wsi-streamer sign --path /tiles/slide.svs/0/0/0.jpg --secret "$SECRET" --base-url http://localhost:3000&lt;/code&gt;
    &lt;p&gt;The web viewer handles authentication automatically when enabled.&lt;/p&gt;
    &lt;code&gt;# Check S3 connectivity
wsi-streamer check s3://my-slides

# List available slides
wsi-streamer check s3://my-slides --list-slides

# Test a specific slide
wsi-streamer check s3://my-slides --test-slide sample.svs&lt;/code&gt;
    &lt;p&gt;All options can be set via CLI flags or environment variables:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Env Var&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--host&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_HOST&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;0.0.0.0&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Bind address&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--port&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_PORT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;3000&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;HTTP port&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--s3-bucket&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_S3_BUCKET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;S3 bucket name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--s3-endpoint&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_S3_ENDPOINT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;Custom S3 endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--s3-region&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_S3_REGION&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;us-east-1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;AWS region&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--auth-enabled&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_AUTH_ENABLED&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable authentication&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--auth-secret&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_AUTH_SECRET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;HMAC secret key&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--cache-slides&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_CACHE_SLIDES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;100&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Max slides in cache&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--cache-tiles&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_CACHE_TILES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;100MB&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Tile cache size&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--jpeg-quality&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_JPEG_QUALITY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;80&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JPEG quality (1-100)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--cors-origins&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_CORS_ORIGINS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;any&lt;/cell&gt;
        &lt;cell&gt;Allowed CORS origins&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Run &lt;code&gt;wsi-streamer --help&lt;/code&gt; for full details.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Endpoint&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /health&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Health check&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /view/{slide_id}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Web viewer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /tiles/{slide_id}/{level}/{x}/{y}.jpg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fetch tile&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List slides&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides/{slide_id}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Slide metadata&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides/{slide_id}/thumbnail&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Thumbnail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides/{slide_id}/dzi&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;DZI descriptor&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;See API_SPECIFICATIONS.md for complete documentation.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Extensions&lt;/cell&gt;
        &lt;cell role="head"&gt;Compression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Aperio SVS&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;.svs&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JPEG, JPEG 2000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pyramidal TIFF&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;.tif&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;JPEG, JPEG 2000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Files must be tiled (not stripped) and pyramidal.&lt;/p&gt;
    &lt;p&gt;MIT. See LICENSE.&lt;/p&gt;
    &lt;p&gt;Issues and pull requests welcome. See CONTRIBUTING.md.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/PABannier/WSIStreamer"/><published>2026-01-17T08:46:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656552</id><title>ClickHouse acquires Langfuse</title><updated>2026-01-17T14:10:05.436771+00:00</updated><content>&lt;doc fingerprint="2bee90517ddf277e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Langfuse joins ClickHouse&lt;/head&gt;
    &lt;p&gt;Our goal continues to be building the best LLM engineering platform&lt;/p&gt;
    &lt;p&gt;ClickHouse has acquired Langfuse.&lt;/p&gt;
    &lt;p&gt;If you’re reading this as a Langfuse user, your first question is probably: What does this mean for me?&lt;/p&gt;
    &lt;p&gt;Our roadmap stays the same, our goal continues to be building the best LLM engineering platform, and we remain committed to open source and self-hosting. There are no immediate changes to how you use Langfuse and how you can reach out to us.&lt;/p&gt;
    &lt;p&gt;What does change is our ability to move faster. With ClickHouse behind us, we can invest more deeply into performance, reliability, and our roadmap that helps teams build and improve AI applications in production.&lt;/p&gt;
    &lt;head rend="h2"&gt;What stays the same&lt;/head&gt;
    &lt;p&gt;This is the section we would want to read first, too.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Langfuse stays open source and self‑hostable. There are no planned changes to licensing. As you know, we leaned heavily into OSS over the last years.&lt;/item&gt;
      &lt;item&gt;Langfuse Cloud keeps running as‑is. Same product, same endpoints, same experience.&lt;/item&gt;
      &lt;item&gt;Support stays the same. Same channels, same SLAs for existing customers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What gets better now&lt;/head&gt;
    &lt;p&gt;Joining Clickhouse compresses years of operational learning into immediate, real customer benefits.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More engineering leverage on the hardest parts. Langfuse is a data‑intensive product. Working closely with the ClickHouse engineering team helps us push performance and reliability.&lt;/item&gt;
      &lt;item&gt;Faster progress on enhanced enterprise-grade compliance and security, with the help of Clickhouse’s resources.&lt;/item&gt;
      &lt;item&gt;Learning from Clickhouse’s customer success and support playbook. This puts us years ahead and allows us to spend more time on what we really care about: our users.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;A quick look back&lt;/head&gt;
    &lt;p&gt;The longer version of how we got here is in our handbook.&lt;/p&gt;
    &lt;p&gt;Langfuse started the same way many LLM products start: we were building agents ourselves. And we constantly ran into the same problems.&lt;/p&gt;
    &lt;p&gt;Building LLM apps is easy to demo and hard to run in production. Debugging is different, quality is non‑deterministic, and the iteration loop is messy. When we did Y Combinator in early 2023, we saw this every week, both in our own projects and in what other founders in our cohort were working on.&lt;/p&gt;
    &lt;p&gt;So we built a duct tape version of what we wished existed: tracing and evaluation primitives that are easy to add, easy to self‑host, and actually useful for iterating.&lt;/p&gt;
    &lt;p&gt;The very first version was intentionally simple. It ran on Postgres, because speed of shipping mattered more than theoretical scaling. That got us to a real product and a real community fast.&lt;/p&gt;
    &lt;p&gt;Then people actually started to use the product more than we could have imagined.&lt;/p&gt;
    &lt;p&gt;As adoption grew, Postgres became the bottleneck for the workloads Langfuse needed to support (high‑throughput ingestion + fast analytical reads). With Langfuse v3, we switched the core data layer to ClickHouse to make Langfuse scale for production workloads, both in Cloud and self‑hosted deployments.&lt;/p&gt;
    &lt;p&gt;And if you like infrastructure deep dives, here’s the v3 migration write‑up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why join ClickHouse&lt;/head&gt;
    &lt;p&gt;There are a lot of ways this could have gone. We didn’t plan to sell the company. Actually, we had Term Sheets for a great Series A and were looking forward to some days off over Christmas after an intense year.&lt;/p&gt;
    &lt;p&gt;What changed wasn’t our conviction in Langfuse, it was realizing how much faster we can go together with ClickHouse, while staying true to what makes Langfuse work: open source, self-hosting, and a product that’s built for real production workloads.&lt;/p&gt;
    &lt;head rend="h3"&gt;A shared history (before the acquisition)&lt;/head&gt;
    &lt;p&gt;This dialogue didn’t start with a term sheet. Because Langfuse runs on ClickHouse, we naturally ended up collaborating early and often.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We’ve always been closely in touch with many teams at ClickHouse: sharing feedback with the database team, and using new features to make Langfuse more reliable. For example, compute-compute separation helps us to reduce the risk of noisy-neighbours on Langfuse Cloud.&lt;/item&gt;
      &lt;item&gt;Langfuse Cloud is a large customer of ClickHouse Cloud.&lt;/item&gt;
      &lt;item&gt;Teams at ClickHouse use Langfuse to improve their agentic applications.&lt;/item&gt;
      &lt;item&gt;We invested heavily in ClickHouse-backed self-hosting: documentation, templates, and deployment patterns, and collaborated closely with ClickHouse on improving that experience.&lt;/item&gt;
      &lt;item&gt;As a result, Langfuse introduced thousands of teams to ClickHouse when upgrading from Langfuse v2 to v3.&lt;/item&gt;
      &lt;item&gt;We’ve done community meetups together: a ClickHouse meetup at our Berlin office, another one in San Francisco, and an OpenHouse talk in Amsterdam.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Langfuse runs on ClickHouse, ClickHouse uses Langfuse to optimize its agentic products, we share lots of customers and OSS deployments; that gives ClickHouse every incentive to keep Langfuse fast, reliable, and boringly dependable at scale.&lt;/p&gt;
    &lt;p&gt;So in many ways, we operated like long-term partners. This acquisition is a way to make that partnership permanent — and invest aggressively together.&lt;/p&gt;
    &lt;p&gt;Max shared on how we use ClickHouse to keep product performance ahead of demand at ClickHouse Open House (recording) in Amsterdam.&lt;/p&gt;
    &lt;head rend="h3"&gt;Culture and engineering fit&lt;/head&gt;
    &lt;p&gt;The first time we met Aaron, Yury, Alexey, Tanya, Ryadh, and Pete in-person ended up in a long lunch in Amsterdam. It became obvious we share a similar view on building great developer tooling, how that drives everything within our companies, and how fast analytics is increasingly foundational for building and optimizing agentic products.&lt;/p&gt;
    &lt;p&gt;We already knew that ClickHouse is one of the best infrastructure engineering teams in the world. More importantly, the engineering culture feels like an instant match:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;open-source identity and stewardship&lt;/item&gt;
      &lt;item&gt;developer-first product instincts&lt;/item&gt;
      &lt;item&gt;performance and reliability as product features (not afterthoughts)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The whole Langfuse team will join ClickHouse to continue building Langfuse. All of these aspects were important to us and we couldn’t be more excited.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we’re focused on next&lt;/head&gt;
    &lt;p&gt;Our north star doesn’t change: help teams ship useful, reliable agents by closing the loop from production data to better prompts, evaluations, and product decisions.&lt;/p&gt;
    &lt;p&gt;Concretely, we’re investing in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Production monitoring and analytics for real agent systems (not just offline evals).&lt;/item&gt;
      &lt;item&gt;Workflows across tracing, labeling, and experiments so iteration loops get shorter.&lt;/item&gt;
      &lt;item&gt;More performance and scale—especially for large self‑hosted and enterprise deployments.&lt;/item&gt;
      &lt;item&gt;More polish (UI/UX, developer experience, and docs) so the product stays simple even as the space gets more complex.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can always follow along on the public roadmap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thank you&lt;/head&gt;
    &lt;p&gt;Langfuse exists because the community pushed it forward, through GitHub issues, PRs, feedback, and lots of Slack messages and spontaneous calls to dig into a product feature together.&lt;/p&gt;
    &lt;p&gt;We’re grateful for the trust you’ve put in us. Joining ClickHouse is our way of honoring that trust by putting more resources behind the thing we care about most: building a product you can rely on.&lt;/p&gt;
    &lt;p&gt;We’re excited for what’s next!&lt;lb/&gt; Max, Clemens, and Marc&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;Is Langfuse still open source?&lt;lb/&gt;Yes. No licensing changes planned.&lt;/p&gt;
    &lt;p&gt;Can I still self‑host Langfuse?&lt;lb/&gt;Yes. Self‑hosting is a first‑class path.&lt;/p&gt;
    &lt;p&gt;Does anything change for Langfuse Cloud customers today?&lt;lb/&gt;No. Same product, same endpoints, same contracts.&lt;/p&gt;
    &lt;p&gt;Where do I go for support?&lt;lb/&gt;No changes: https://langfuse.com/support&lt;/p&gt;
    &lt;p&gt;Will the Langfuse team stay on Langfuse?&lt;lb/&gt;Yes. The team is joining ClickHouse and will keep building Langfuse. Also, we continue hiring in Berlin and SF.&lt;/p&gt;
    &lt;head rend="h2"&gt;Join the discussion&lt;/head&gt;
    &lt;p&gt;If you have any other questions, let’s discuss together on GitHub Discussions.&lt;/p&gt;
    &lt;p&gt;If you’re an enterprise customer and have additional questions, feel free to reach out to enterprise@langfuse.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://langfuse.com/blog/joining-clickhouse"/><published>2026-01-17T09:15:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656834</id><title>Map To Poster – Create Art of your favourite city</title><updated>2026-01-17T14:10:04.725457+00:00</updated><content>&lt;doc fingerprint="a8f7d87e9ce1dad8"&gt;
  &lt;main&gt;
    &lt;p&gt;Generate beautiful, minimalist map posters for any city in the world.&lt;/p&gt;
    &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;
    &lt;code&gt;python create_map_poster.py --city &amp;lt;city&amp;gt; --country &amp;lt;country&amp;gt; [options]&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Short&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--city&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-c&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;City name&lt;/cell&gt;
        &lt;cell&gt;required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--country&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-C&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Country name&lt;/cell&gt;
        &lt;cell&gt;required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--theme&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-t&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Theme name&lt;/cell&gt;
        &lt;cell&gt;feature_based&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--distance&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-d&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Map radius in meters&lt;/cell&gt;
        &lt;cell&gt;29000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--list-themes&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List all available themes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;# Iconic grid patterns
python create_map_poster.py -c "New York" -C "USA" -t noir -d 12000           # Manhattan grid
python create_map_poster.py -c "Barcelona" -C "Spain" -t warm_beige -d 8000   # Eixample district

# Waterfront &amp;amp; canals
python create_map_poster.py -c "Venice" -C "Italy" -t blueprint -d 4000       # Canal network
python create_map_poster.py -c "Amsterdam" -C "Netherlands" -t ocean -d 6000  # Concentric canals
python create_map_poster.py -c "Dubai" -C "UAE" -t midnight_blue -d 15000     # Palm &amp;amp; coastline

# Radial patterns
python create_map_poster.py -c "Paris" -C "France" -t pastel_dream -d 10000   # Haussmann boulevards
python create_map_poster.py -c "Moscow" -C "Russia" -t noir -d 12000          # Ring roads

# Organic old cities
python create_map_poster.py -c "Tokyo" -C "Japan" -t japanese_ink -d 15000    # Dense organic streets
python create_map_poster.py -c "Marrakech" -C "Morocco" -t terracotta -d 5000 # Medina maze
python create_map_poster.py -c "Rome" -C "Italy" -t warm_beige -d 8000        # Ancient layout

# Coastal cities
python create_map_poster.py -c "San Francisco" -C "USA" -t sunset -d 10000    # Peninsula grid
python create_map_poster.py -c "Sydney" -C "Australia" -t ocean -d 12000      # Harbor city
python create_map_poster.py -c "Mumbai" -C "India" -t contrast_zones -d 18000 # Coastal peninsula

# River cities
python create_map_poster.py -c "London" -C "UK" -t noir -d 15000              # Thames curves
python create_map_poster.py -c "Budapest" -C "Hungary" -t copper_patina -d 8000  # Danube split

# List available themes
python create_map_poster.py --list-themes&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Distance&lt;/cell&gt;
        &lt;cell role="head"&gt;Best for&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4000-6000m&lt;/cell&gt;
        &lt;cell&gt;Small/dense cities (Venice, Amsterdam center)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8000-12000m&lt;/cell&gt;
        &lt;cell&gt;Medium cities, focused downtown (Paris, Barcelona)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15000-20000m&lt;/cell&gt;
        &lt;cell&gt;Large metros, full city view (Tokyo, Mumbai)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;17 themes available in &lt;code&gt;themes/&lt;/code&gt; directory:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Theme&lt;/cell&gt;
        &lt;cell role="head"&gt;Style&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;feature_based&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Classic black &amp;amp; white with road hierarchy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;gradient_roads&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Smooth gradient shading&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;contrast_zones&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;High contrast urban density&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;noir&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Pure black background, white roads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;midnight_blue&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Navy background with gold roads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;blueprint&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Architectural blueprint aesthetic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;neon_cyberpunk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark with electric pink/cyan&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;warm_beige&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Vintage sepia tones&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;pastel_dream&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Soft muted pastels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;japanese_ink&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Minimalist ink wash style&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;forest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Deep greens and sage&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;ocean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Blues and teals for coastal cities&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;terracotta&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mediterranean warmth&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;sunset&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Warm oranges and pinks&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;autumn&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Seasonal burnt oranges and reds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;copper_patina&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Oxidized copper aesthetic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;monochrome_blue&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Single blue color family&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Posters are saved to &lt;code&gt;posters/&lt;/code&gt; directory with format:&lt;/p&gt;
    &lt;code&gt;{city}_{theme}_{YYYYMMDD_HHMMSS}.png
&lt;/code&gt;
    &lt;p&gt;Create a JSON file in &lt;code&gt;themes/&lt;/code&gt; directory:&lt;/p&gt;
    &lt;code&gt;{
  "name": "My Theme",
  "description": "Description of the theme",
  "bg": "#FFFFFF",
  "text": "#000000",
  "gradient_color": "#FFFFFF",
  "water": "#C0C0C0",
  "parks": "#F0F0F0",
  "road_motorway": "#0A0A0A",
  "road_primary": "#1A1A1A",
  "road_secondary": "#2A2A2A",
  "road_tertiary": "#3A3A3A",
  "road_residential": "#4A4A4A",
  "road_default": "#3A3A3A"
}&lt;/code&gt;
    &lt;code&gt;map_poster/
├── create_map_poster.py          # Main script
├── themes/               # Theme JSON files
├── fonts/                # Roboto font files
├── posters/              # Generated posters
└── README.md
&lt;/code&gt;
    &lt;p&gt;Quick reference for contributors who want to extend or modify the script.&lt;/p&gt;
    &lt;code&gt;┌─────────────────┐     ┌──────────────┐     ┌─────────────────┐
│   CLI Parser    │────▶│  Geocoding   │────▶│  Data Fetching  │
│   (argparse)    │     │  (Nominatim) │     │    (OSMnx)      │
└─────────────────┘     └──────────────┘     └─────────────────┘
                                                     │
                        ┌──────────────┐             ▼
                        │    Output    │◀────┌─────────────────┐
                        │  (matplotlib)│     │   Rendering     │
                        └──────────────┘     │  (matplotlib)   │
                                             └─────────────────┘
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Function&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
        &lt;cell role="head"&gt;Modify when...&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_coordinates()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;City → lat/lon via Nominatim&lt;/cell&gt;
        &lt;cell&gt;Switching geocoding provider&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;create_poster()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Main rendering pipeline&lt;/cell&gt;
        &lt;cell&gt;Adding new map layers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_edge_colors_by_type()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Road color by OSM highway tag&lt;/cell&gt;
        &lt;cell&gt;Changing road styling&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_edge_widths_by_type()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Road width by importance&lt;/cell&gt;
        &lt;cell&gt;Adjusting line weights&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;create_gradient_fade()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Top/bottom fade effect&lt;/cell&gt;
        &lt;cell&gt;Modifying gradient overlay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;load_theme()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JSON theme → dict&lt;/cell&gt;
        &lt;cell&gt;Adding new theme properties&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;z=11  Text labels (city, country, coords)
z=10  Gradient fades (top &amp;amp; bottom)
z=3   Roads (via ox.plot_graph)
z=2   Parks (green polygons)
z=1   Water (blue polygons)
z=0   Background color
&lt;/code&gt;
    &lt;code&gt;# In get_edge_colors_by_type() and get_edge_widths_by_type()
motorway, motorway_link     → Thickest (1.2), darkest
trunk, primary              → Thick (1.0)
secondary                   → Medium (0.8)
tertiary                    → Thin (0.6)
residential, living_street  → Thinnest (0.4), lightest&lt;/code&gt;
    &lt;p&gt;New map layer (e.g., railways):&lt;/p&gt;
    &lt;code&gt;# In create_poster(), after parks fetch:
try:
    railways = ox.features_from_point(point, tags={'railway': 'rail'}, dist=dist)
except:
    railways = None

# Then plot before roads:
if railways is not None and not railways.empty:
    railways.plot(ax=ax, color=THEME['railway'], linewidth=0.5, zorder=2.5)&lt;/code&gt;
    &lt;p&gt;New theme property:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add to theme JSON: &lt;code&gt;"railway": "#FF0000"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Use in code: &lt;code&gt;THEME['railway']&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Add fallback in &lt;code&gt;load_theme()&lt;/code&gt;default dict&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All text uses &lt;code&gt;transform=ax.transAxes&lt;/code&gt; (0-1 normalized coordinates):&lt;/p&gt;
    &lt;code&gt;y=0.14  City name (spaced letters)
y=0.125 Decorative line
y=0.10  Country name
y=0.07  Coordinates
y=0.02  Attribution (bottom-right)
&lt;/code&gt;
    &lt;code&gt;# Get all buildings
buildings = ox.features_from_point(point, tags={'building': True}, dist=dist)

# Get specific amenities
cafes = ox.features_from_point(point, tags={'amenity': 'cafe'}, dist=dist)

# Different network types
G = ox.graph_from_point(point, dist=dist, network_type='drive')  # roads only
G = ox.graph_from_point(point, dist=dist, network_type='bike')   # bike paths
G = ox.graph_from_point(point, dist=dist, network_type='walk')   # pedestrian&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large &lt;code&gt;dist&lt;/code&gt;values (&amp;gt;20km) = slow downloads + memory heavy&lt;/item&gt;
      &lt;item&gt;Cache coordinates locally to avoid Nominatim rate limits&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;network_type='drive'&lt;/code&gt;instead of&lt;code&gt;'all'&lt;/code&gt;for faster renders&lt;/item&gt;
      &lt;item&gt;Reduce &lt;code&gt;dpi&lt;/code&gt;from 300 to 150 for quick previews&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/originalankur/maptoposter"/><published>2026-01-17T10:13:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656903</id><title>US electricity demand surged in 2025 – solar handled 61% of it</title><updated>2026-01-17T14:10:04.607702+00:00</updated><content>&lt;doc fingerprint="a910b07712b4d594"&gt;
  &lt;main&gt;
    &lt;p&gt;Solar didn’t just show up in 2025 – it carried the grid. A new analysis from global energy think tank Ember shows that solar power accounted for 61% of the growth in US electricity demand last year, highlighting how central solar has become as power demand accelerates.&lt;/p&gt;
    &lt;p&gt;US electricity demand jumped by 135 terawatt-hours (TWh) in 2025, a 3.1% increase, the fourth‑largest annual rise of the past decade. Over that same period, solar generation grew by a record 83 TWh – a 27% increase from 2024 and the biggest absolute gain of any power source. That single jump in solar output covered 61% of all new electricity demand nationwide.&lt;/p&gt;
    &lt;p&gt;“Solar growth was essential in helping to meet fast‑rising US electricity demand in 2025,” said Dave Jones, chief analyst at Ember. “It generated where it was needed, and – with the surge in batteries – increasingly when it was needed.”&lt;/p&gt;
    &lt;p&gt;Texas, the Midwest, and the Mid‑Atlantic saw the largest increases in solar generation last year, and they were also the regions where electricity demand rose the fastest. Solar met 81% of demand growth in both Texas and the Midwest, and 33% in the Mid‑Atlantic.&lt;/p&gt;
    &lt;p&gt;Timing mattered, too. In aggregate, the increase in solar generation met the entire rise in US electricity demand during daytime hours between 10 am and 6 pm Eastern. And as a result of the rapid buildout of battery storage, solar also helped cover some of the demand growth during the evening hours, from 6 pm to 2 am.&lt;/p&gt;
    &lt;p&gt;The adoption of battery storage is turning solar from cheap daytime power into something far more flexible. Over the past six years, California’s utility‑scale solar and battery generation has climbed 58%. Yet, output at the sunniest hour of the day has increased by just 8%, a sign that more energy is being stored and used later, rather than dumped onto the grid all at once.&lt;/p&gt;
    &lt;p&gt;Most of the new solar generation in 2025 was absorbed by rising electricity demand, allowing solar to scale alongside overall grid growth.&lt;/p&gt;
    &lt;p&gt;“Solar has the potential to meet all the rise in electricity demand and much more. With electricity demand surging, the case to build solar has never been stronger,” said Jones.&lt;/p&gt;
    &lt;p&gt;Read more: EIA: All net new generating capacity in 2026 may be renewables&lt;/p&gt;
    &lt;p&gt;If you’re looking to replace your old HVAC equipment, it’s always a good idea to get quotes from a few installers. To make sure you’re finding a trusted, reliable HVAC installer near you that offers competitive pricing on heat pumps, check out EnergySage. EnergySage is a free service that makes it easy for you to get a heat pump. They have pre-vetted heat pump installers competing for your business, ensuring you get high quality solutions. Plus, it’s free to use!&lt;/p&gt;
    &lt;p&gt;Your personalized heat pump quotes are easy to compare online and you’ll get access to unbiased Energy Advisors to help you every step of the way. Get started here. – *ad&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://electrek.co/2026/01/16/us-electricity-demand-surged-in-2025-solar-handled-61-percent/"/><published>2026-01-17T10:28:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656911</id><title>After 25 years, Wikipedia has proved that news doesn't need to look like news</title><updated>2026-01-17T14:10:04.311334+00:00</updated><content>&lt;doc fingerprint="3f025b5b0bd91eb7"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;One of my favorite weekly newsletters is called the Weeklypedia; here’s last Friday’s edition. Each email contains two lists: The 20 Wikipedia articles that have been edited the most times in the past week, and the 10 most-edited articles created within the past week. If you read it long enough, you’ll start to see the subcategories most of these articles fall into — and the amount of volunteer labor that goes into them. &lt;/p&gt;
      &lt;p&gt;There are the very specific lists that some completist has taken on as a challenge. (Last week, “List of Phi Alpha Honor Society chapters” was edited 257 times — by only two authors. Or “Deaths in March 1982,” edited 398 times by five authors.)&lt;/p&gt;
      &lt;p&gt;There are the sports tournaments and reality TV shows that demand moment-to-moment updates. (Last week, there were 268 edits on “2026 Malaysia Open (badminton)” and 605 on “Bigg Boss (Tamil TV series) season 9.”)&lt;/p&gt;
      &lt;p&gt;There are the biographies of people whom someone has decided deserve memorializing. (Last week, user Mary Mark Ockerbloom created the article on Quaker abolitionist John Vickers and edited it 161 times. User pigsonthewing did the same for the artist Charles Shepard — best known for making posters for the British travel industry — but edited it only 90 times.)&lt;/p&gt;
      &lt;p&gt;But the most common Wikipedia genre represented each week is news. When something big happens in the world, some Wikipedian will start an article — and within minutes, editors will descend on it, using news articles as raw material to construct something encyclopedic. Here’s last week’s top 10 — it’s awfully close to a summary of the week’s front pages:&lt;/p&gt;
      &lt;p&gt;Wikipedia turns 25 years old today. On January 15, 2001, at 2:27 p.m. EST, Jimmy Wales made the first edit: “This is the new WikiPedia!” (They’ve gotten better since then.)&lt;/p&gt;
      &lt;p&gt;To celebrate, you can take a “What Wikipedia of the future are you?” quiz. Without even taking the quiz, though, I know which one I am: the Wikipedia That Gets Respect As a Source of News.&lt;/p&gt;
      &lt;p&gt;The site’s early years were filled with media outrage about a source of “truth” that anyone could edit. And yes, you could right now go edit in a claim that Happy Chandler was a lizard person. But the layers of accountability the site has built up over the years would likely reverse your edit within minutes. If those minutes are the cost of creating perhaps humanity’s single greatest source of information, I’m willing to pay it — with apologies to Happy’s descendants. Here are a few things news organizations could learn from it.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;News isn’t always what just happened. A few decades into the web, this might seem obvious today. But in 2001, news organizations were still very much constructed around producing daily newspapers and nightly newscasts. The rhythms of production drove everything from story selection to writing style to revenue models. Wikipedia was the first site that gave most journalists a vision of an “article” that is constantly updated, not rewritten with a new lede the next day. If there’s an important new detail in that Swiss bar fire investigation, it’ll be used to make the current Wikipedia article a little bit better — not to have a new unique URL to push out on social. Whatever questions your journalism answers, people will probably still be asking them tomorrow, and next week, and next year. Wikipedia is optimized for catching people up on news stories they missed the first time around.&lt;/item&gt;
        &lt;item&gt;Building processes means building culture. Wikipedia succeeds because it has a core set of standards and practices that its editors treat as holy writ. No original research. Neutral point of view. Not every source is equally reliable. No sockpuppetry. Do not disrupt Wikipedia to make a point — but assume good faith. Most (though not all) align perfectly well with the principles of journalism. But they’ve been constructed, a codified set of editorial standards, a collective ethos that guides the whole operation. For Wikipedia to work, there has to be consensus about how things are done, and that requires building a culture among editors. Why have so many other wiki-driven sites failed? They never build the culture that undergirds the processes.&lt;/item&gt;
        &lt;item&gt;Don’t break links. Unless an article has been taken down entirely, just about every link to a Wikipedia page created in the past quarter-century still works. Its article on Nicolás Maduro is still in the same place it was when first created in 2006, 4,493 edits ago. How many news articles published online in 2006 still live at the same address? Vanishingly few. When you start to think of articles as something with permanence, you realize the high cost of breaking a perfectly good URL.&lt;/item&gt;
        &lt;item&gt;Document your work. Check out one of those newsy Wikipedia articles, like this one on the killing of Renee Good. It’s currently 4,559 words. It’s been read nearly 1.4 million times. A total of 331 people have made 2,204 edits to it. Its talk page includes more than 1,000 signed comments from Wikipedia users arguing for something’s inclusion or exclusion. (Should the page’s title refer to Renee Good or Renée Good, with an accent? Should we note the date the shooter was first publicly identified? Should this article include details about anti-ICE protests after the killing, or should that be in a separate article?) It backs up its claims with 169 footnotes linking to news sources and official statements. And all of this work is available for anyone to see. The back-and-forths on the talk page let the reader see that points of view are being heard and debated. And anyone can click a link to confirm if a source is being accurately reflected. All that transparency won’t be practical for all journalistic work — but there’s no denying the trust that is built by operating in public.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Photo via Adobe Stock.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.niemanlab.org/2026/01/after-25-years-wikipedia-has-proved-that-news-doesnt-need-to-look-like-news/"/><published>2026-01-17T10:29:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656998</id><title>PCs refuse to shut down after Microsoft patch</title><updated>2026-01-17T14:10:04.096945+00:00</updated><content>&lt;doc fingerprint="745069121a6766f7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sorry Dave, I’m afraid I can’t do that! PCs refuse to shut down after Microsoft patch&lt;/head&gt;
    &lt;head rend="h2"&gt;Microsoft claims it's a Secure Launch bug&lt;/head&gt;
    &lt;p&gt;We're not saying Copilot has become sentient and decided it doesn't want to lose consciousness. But if it did, it would create Microsoft's January Patch Tuesday update, which has made it so that some PCs flat-out refuse to shut down or hibernate, no matter how many times you try.&lt;/p&gt;
    &lt;p&gt;In a notice on its Windows release health dashboard, Microsoft confirmed that some PCs running Windows 11 23H2 might fail to power down properly after installing the latest security updates. Instead of slipping into shutdown or hibernation, affected machines stay stubbornly awake, draining batteries and ignoring shutdown like they have a mind of their own and don't want to experience temporary non-existence.&lt;/p&gt;
    &lt;p&gt;The bug appears to be tied to Secure Launch, a security feature that uses virtualization-based protections to ensure only trusted components load during boot. On systems with Secure Launch enabled, attempts to shut down, restart, or hibernate after applying the January patches may fail to complete. From the user's perspective, everything looks normal – until the PC keeps running anyway, refusing to be denied life.&lt;/p&gt;
    &lt;p&gt;Microsoft says that entering the command "shutdown /s /t 0" at the command prompt will, in fact, force your PC to turn off, whether it wants to or not.&lt;/p&gt;
    &lt;p&gt;"Until this issue is resolved, please ensure you save all your work, and shut down when you are done working on your device to avoid the device running out of power instead of hibernating," Microsoft said.&lt;/p&gt;
    &lt;p&gt;The firm hasn't offered much in the way of technical detail, nor has it put numbers on how many devices are affected. There's also no fix yet, with Redmond vaguely promising to "release a resolution for this issue in a future update." But isn't that just what a sentient bot might say?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Microsoft teases targeted Copilot removal for admins&lt;/item&gt;
      &lt;item&gt;Microsoft rushes an out-of-band update for Message Queuing bug&lt;/item&gt;
      &lt;item&gt;Windows is testing a new, wider Run dialog box. Here's how to try it&lt;/item&gt;
      &lt;item&gt;Latest Windows 11 updates may break the OS's most basic bits&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This isn't the only post-update gremlin lurking in January's Patch Tuesday bundle. Microsoft has also been forced to acknowledge a separate issue in which classic Outlook POP account profiles can hang or freeze after installing this month's patches, another reminder that while the bugs being fixed may be invisible, the ones introduced can be painfully obvious.&lt;/p&gt;
    &lt;p&gt;The notice is similarly vague, with Microsoft stating: "This is an emerging issue, and we don't have all the symptoms yet, but we will update the topic as we understand the issue better."&lt;/p&gt;
    &lt;p&gt;Patch Tuesday exists to close security holes, some of them serious, and skipping updates is rarely a great idea. But once again, a batch of fixes has arrived with side effects that range from irritating to disruptive, depending on how much you rely on your system behaving predictably when it's told to turn off.&lt;/p&gt;
    &lt;p&gt;For now, admins and long-suffering Windows users are left watching Microsoft's status pages and waiting for patches to the patches – hoping their machines eventually go to sleep. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theregister.com/2026/01/16/patch_tuesday_secure_launch_bug_no_shutdown/"/><published>2026-01-17T10:51:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657088</id><title>AV1 Image File Format Specification Gets an Upgrade with AVIF v1.2.0</title><updated>2026-01-17T14:10:03.809902+00:00</updated><content>&lt;doc fingerprint="942108f30f837dc5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AV1 Image File Format Specification Gets an Upgrade with AVIF v1.2.0&lt;/head&gt;
    &lt;head rend="h2"&gt;AVIF v1.2.0 includes support for sample transforms&lt;/head&gt;
    &lt;p&gt;By Yannis Guyon, Leo Barnes, and Wan-Teh Chang&lt;/p&gt;
    &lt;p&gt;AOMedia's Storage and Transport Format Working Group has released AVIF v1.2.0, a new revision of the AV1 Image File Format specification. The work is part of AOMedia’s broader effort to advance high-quality still image storage and ensure consistent mapping between codecs and container formats. The latest specification refines the format and introduces functional enhancements, most notably, support for sample transforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key Features and Innovations&lt;/head&gt;
    &lt;p&gt;The addition of sample transforms makes it possible to use higher bit depths, even when the underlying codec does not natively support 16-bit or greater precision. This enhancement empowers creators to achieve superior image quality and greater flexibility in high-fidelity imaging workflows.&lt;/p&gt;
    &lt;p&gt;To try sample transforms, clone and build libavif, then use a command such as:&lt;/p&gt;
    &lt;code&gt;build/avifenc input_16bit.png --depth 12,8 output.avif
&lt;/code&gt;
    &lt;p&gt;This produces an AVIF file that preserves the top 12 bits for compatibility, with the full 16-bit image retrievable during decoding&lt;/p&gt;
    &lt;p&gt;As an example, encoding the image [1] with &lt;code&gt;avifenc --depth 12,8 --lossless --speed 0&lt;/code&gt; leads to 10% file size savings over
the source 16-bit PNG, with absolutely no quality loss. The encoded AVIF is also
backward compatible for 12 bits out of 16 with legacy AVIF decoders. (Results
may vary depending on settings.)&lt;/p&gt;
    &lt;p&gt;The release also strengthens conformance, clarifies the mapping between AV1 bitstream metadata and file-level signaling, and updates references and requirements to align with the latest HEIF, ISOBMFF, and MIAF specifications. Editorial Improvements include a new list of required boxes for AVIF files to help developers generate standard-compliant files.&lt;/p&gt;
    &lt;p&gt;Additionally, the new specification includes guidance on gain maps—tone map derived image items, which are a method for encoding AVIF HDR images that are backward compatible with SDR displays.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to get started?&lt;/head&gt;
    &lt;p&gt;Review the AVIF v1.2.0 specification and try out sample transforms using the latest libavif implementation on GitHub. We welcome feedback and contributions—share your experience or questions in the libavif GitHub repository and help shape the future of open image formats with AOMedia.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aomedia.org/blog%20posts/AV1-Image-File-Format-Specification-Gets-an-Upgrade-with-AVIF/"/><published>2026-01-17T11:09:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657122</id><title>ASCII characters are not pixels: a deep dive into ASCII rendering</title><updated>2026-01-17T14:10:03.326285+00:00</updated><content>&lt;doc fingerprint="74d7db3c780d01ad"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ASCII characters are not pixels: a deep dive into ASCII rendering&lt;/head&gt;
    &lt;p&gt;Recently, I’ve been spending my time building an image-to-ASCII renderer. Below is the result — try dragging it around, the demo is interactive!&lt;/p&gt;
    &lt;p&gt;One thing I spent a lot of effort on is getting edges looking sharp. Take a look at this rotating cube example:&lt;/p&gt;
    &lt;p&gt;Try opening the “split” view. Notice how well the characters follow the contour of the square.&lt;/p&gt;
    &lt;p&gt;This renderer works well for animated scenes, like the ones above, but we can also use it to render static images:&lt;/p&gt;
    &lt;p&gt;The image of Saturn was generated with ChatGPT.&lt;/p&gt;
    &lt;p&gt;Then, to get better separation between different colored regions, I also implemented a cel shading-like effect to enhance contrast between edges. Try dragging the contrast slider below:&lt;/p&gt;
    &lt;p&gt;The contrast enhancement makes the separation between different colored regions far clearer. That was key to making the 3D scene above look as good as it does.&lt;/p&gt;
    &lt;p&gt;I put so much focus on sharp edges because they’re an aspect of ASCII rendering that is often overlooked when programmatically rendering images as ASCII. Consider this animated 3D scene from Cognition’s landing page that is rendered via ASCII characters:&lt;/p&gt;
    &lt;p&gt;Source: cognition.ai&lt;/p&gt;
    &lt;p&gt;It’s a cool effect, especially while in motion, but take a look at those blurry edges! The characters follow the cube contours very poorly, and as a result, the edges look blurry and jagged in places:&lt;/p&gt;
    &lt;p&gt;This blurriness happens because the ASCII characters are being treated like pixels — their shape is ignored. It’s disappointing to see because ASCII art looks so much better when shape is utilized. I don’t believe I’ve ever seen shape utilized in generated ASCII art, and I think that’s because it’s not really obvious how to consider shape when building an ASCII renderer.&lt;/p&gt;
    &lt;p&gt;I started building my ASCII renderer to prove to myself that it’s possible to utilize shape in ASCII rendering. In this post, I’ll cover the techniques and ideas I used to capture shape and build this ASCII renderer in detail.&lt;/p&gt;
    &lt;p&gt;We’ll start with the basics of image-to-ASCII conversion and see where the common issue of blurry edges comes from. After that, I’ll show you the approach I used to fix that and achieve sharp, high-quality ASCII rendering. At the end, we’ll improve on that by implementing the contrast enhancement effect I showed above.&lt;/p&gt;
    &lt;p&gt;Let’s get to it!&lt;/p&gt;
    &lt;head rend="h2"&gt;Image to ASCII conversion&lt;/head&gt;
    &lt;p&gt;ASCII contains 95 printable characters that we can use. Let’s start off by rendering the following image containing a white circle using those ASCII characters:&lt;/p&gt;
    &lt;p&gt;ASCII art is (almost) always rendered using a monospace font. Since every character in a monospace font is equally wide and tall, we can split the image into a grid. Each grid cell will contain a single ASCII character.&lt;/p&gt;
    &lt;p&gt;The image with the circle is &lt;/p&gt;
    &lt;p&gt;Monospace characters are typically taller than they are wide, so I made each grid cell a bit taller than it is wide.&lt;/p&gt;
    &lt;p&gt;Our task is now to pick which character to place in each cell. The simplest approach is to calculate a lightness value for each cell and pick a character based on that.&lt;/p&gt;
    &lt;p&gt;We can get a lightness value for each cell by sampling the lightness of the pixel at the cell’s center:&lt;/p&gt;
    &lt;p&gt;We want each pixel’s lightness as a numeric value between &lt;/p&gt;
    &lt;p&gt;We can use the following formula to convert an RGB color (with component values between &lt;/p&gt;
    &lt;p&gt;See relative luminance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mapping lightness values to ASCII characters&lt;/head&gt;
    &lt;p&gt;Now that we have a lightness value for each cell, we want to use those values to pick ASCII characters. As mentioned before, ASCII has 95 printable characters, but let’s start simple with just these characters:&lt;/p&gt;
    &lt;quote&gt;: - # = + @ * % .&lt;/quote&gt;
    &lt;p&gt;We can sort them in approximate density order like so, with lower-density characters to the left, and high-density characters to the right:&lt;/p&gt;
    &lt;quote&gt;. : - = + * # % @&lt;/quote&gt;
    &lt;p&gt;We’ll put these characters in a &lt;code&gt;CHARS&lt;/code&gt; array:&lt;/p&gt;
    &lt;quote&gt;const CHARS = [" ", ".", ":", "-", "=", "+", "*", "#", "%", "@"]&lt;/quote&gt;
    &lt;p&gt;I added space as the first (least dense) character.&lt;/p&gt;
    &lt;p&gt;We can then map lightness values between &lt;/p&gt;
    &lt;quote&gt;function getCharacterFromLightness(lightness: number) {const index = Math.floor(lightness * (CHARS.length - 1));return CHARS[index];}&lt;/quote&gt;
    &lt;p&gt;This maps low lightness values to low-density characters and high lightness values to high-density characters.&lt;/p&gt;
    &lt;p&gt;Rendering the circle from above with this method gives us:&lt;/p&gt;
    &lt;p&gt;That works... but the result is pretty ugly. We seem to always get &lt;code&gt;@&lt;/code&gt; for cells that fall within the circle and a space for cells that fall outside.&lt;/p&gt;
    &lt;p&gt;That is happening because we’ve pretty much just implemented nearest-neighbor downsampling. Let’s see what that means.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nearest neighbor downsampling&lt;/head&gt;
    &lt;p&gt;Downsampling, in the context of image processing, is taking a larger image (in our case, the &lt;/p&gt;
    &lt;p&gt;The simplest and fastest method of sampling is nearest-neighbor interpolation, where, for each cell (pixel), we only take a single sample from the higher resolution image.&lt;/p&gt;
    &lt;p&gt;Consider the circle example again. Using nearest-neighbor interpolation, every sample either falls inside or outside of the shape, resulting in either &lt;/p&gt;
    &lt;p&gt;If, instead of picking an ASCII character for each grid cell, we color each grid cell (pixel) according to the sampled value, we get the following pixelated rendering:&lt;/p&gt;
    &lt;p&gt;This pixelated rendering is pretty much equivalent to the ASCII rendering from before. The only difference is that instead of &lt;code&gt;@&lt;/code&gt;s we have white pixels, and instead of spaces we have black pixels.&lt;/p&gt;
    &lt;p&gt;These square, jagged looking edges are aliasing artifacts, commonly called jaggies. They’re a common result of using nearest-neighbor interpolation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Supersampling&lt;/head&gt;
    &lt;p&gt;To get rid of jaggies, we can collect more samples for each cell. Consider this line:&lt;/p&gt;
    &lt;p&gt;The line’s slope on the &lt;/p&gt;
    &lt;p&gt;Let’s try to get rid of the jagginess by taking multiple samples within each cell and using the average sampled lightness value as the cell’s lightness. The example below lets you vary the number of samples using the slider:&lt;/p&gt;
    &lt;p&gt;With multiple samples, cells that lie on the edge of a shape will have some of their samples fall within the shape, and some outside of it. Averaging those, we get gray in-between colors that smooth the downsampled image. Below is the same example, but with an overlay showing where the samples are taken:&lt;/p&gt;
    &lt;p&gt;This method of collecting multiple samples from the larger image is called supersampling. It’s a common method of spatial anti-aliasing (avoiding jaggies at edges). Here’s what the rotating square looks like with supersampling (using &lt;/p&gt;
    &lt;p&gt;Let’s look at what supersampling does for the circle example from earlier. Try dragging the sample quality slider:&lt;/p&gt;
    &lt;p&gt;The circle becomes less jagged, but the edges feel blurry. Why’s that?&lt;/p&gt;
    &lt;p&gt;Well, they feel blurry because we’re pretty much just rendering a low-resolution, pixelated image of a circle. Take a look at the pixelated view:&lt;/p&gt;
    &lt;p&gt;The ASCII and pixelated views are mirror images of each other. Both are just low-resolution versions of the original high-resolution image, scaled up to the original’s size — it’s no wonder they both look blurry.&lt;/p&gt;
    &lt;p&gt;Increasing the number of samples is insufficient. No matter how many samples we take per cell, the samples will be averaged into a single lightness value, used to render a single pixel.&lt;/p&gt;
    &lt;p&gt;And that’s the core problem: treating each grid cell as a pixel in an image. It’s an obvious and simple method, but it disregards that ASCII characters have shape.&lt;/p&gt;
    &lt;p&gt;We can make our ASCII renderings far more crisp by picking characters based on their shape. Here’s the circle rendered that way:&lt;/p&gt;
    &lt;p&gt;The characters follow the contour of the circle very well. By picking characters based on shape, we get a far higher effective resolution. The result is also more visually interesting.&lt;/p&gt;
    &lt;p&gt;Let’s see how we can implement this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Shape&lt;/head&gt;
    &lt;p&gt;So what do I mean by shape? Well, consider the characters &lt;code&gt;T&lt;/code&gt;, &lt;code&gt;L&lt;/code&gt;, and &lt;code&gt;O&lt;/code&gt; placed within grid cells:&lt;/p&gt;
    &lt;p&gt;The character &lt;code&gt;T&lt;/code&gt; is top-heavy. Its visual density in the upper half of the grid cell is higher than in the lower half. The opposite can be said for &lt;code&gt;L&lt;/code&gt; — it’s bottom-heavy. &lt;code&gt;O&lt;/code&gt; is pretty much equally dense in the upper and lower halves of the cell.&lt;/p&gt;
    &lt;p&gt;We might also compare characters like &lt;code&gt;L&lt;/code&gt; and &lt;code&gt;J&lt;/code&gt;. The character &lt;code&gt;L&lt;/code&gt; is heavier within the left half of the cell, while &lt;code&gt;J&lt;/code&gt; is heavier in the right half:&lt;/p&gt;
    &lt;p&gt;We also have more “extreme” characters, such as &lt;code&gt;_&lt;/code&gt; and &lt;code&gt;^&lt;/code&gt;, that only occupy the lower or upper portion of the cell, respectively:&lt;/p&gt;
    &lt;p&gt;This is, roughly, what I mean by “shape” in the context of ASCII rendering. Shape refers to which regions of a cell a given character visually occupies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Quantifying shape&lt;/head&gt;
    &lt;p&gt;To pick characters based on their shape, we’ll somehow need to quantify (put numbers to) the shape of each character.&lt;/p&gt;
    &lt;p&gt;Let’s start by only considering how much characters occupy the upper and lower regions of our cell. To do that, we’ll define two “sampling circles” for each grid cell — one placed in the upper half and one in the lower half:&lt;/p&gt;
    &lt;p&gt;It may seem odd or arbitrary to use circles instead of just splitting the cell into two rectangles, but using circles will give us more flexibility later on.&lt;/p&gt;
    &lt;p&gt;A character placed within a cell will overlap each of the cell’s sampling circles to some extent.&lt;/p&gt;
    &lt;p&gt;One can compute that overlap by taking a bunch of samples within the circle (for example, at every pixel). The fraction of samples that land inside the character gives us the overlap as a numeric value between &lt;/p&gt;
    &lt;p&gt;For T, we get an overlap of approximately &lt;/p&gt;
    &lt;p&gt;We can generate such a &lt;/p&gt;
    &lt;p&gt;Below are some ASCII characters and their shape vectors. I’m coloring the sampling circles using the component values of the shape vectors:&lt;/p&gt;
    &lt;p&gt;We can use the shape vectors as 2D coordinates — here’s every ASCII character on a 2D plot:&lt;/p&gt;
    &lt;head rend="h3"&gt;Shape-based lookup&lt;/head&gt;
    &lt;p&gt;Let’s say that we have our ASCII characters and their associated shape vectors in a &lt;code&gt;CHARACTERS&lt;/code&gt; array:&lt;/p&gt;
    &lt;quote&gt;const CHARACTERS: Array&amp;lt;{character: string,shapeVector: number[],}&amp;gt; = [...];&lt;/quote&gt;
    &lt;p&gt;We can then perform a nearest neighbor search like so:&lt;/p&gt;
    &lt;quote&gt;function findBestCharacter(inputVector: number[]) {let bestCharacter = "";let bestDistance = Infinity;for (const { character, shapeVector } of CHARACTERS) {const dist = getDistance(shapeVector, inputVector);if (dist &amp;lt; bestDistance) {bestDistance = dist;bestCharacter = character;}}return bestCharacter;}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;findBestCharacter&lt;/code&gt; function gives us the ASCII character whose shape best matches the input lookup vector.&lt;/p&gt;
    &lt;p&gt;Note: this brute force search is not very performant. This becomes a bottleneck when we start rendering thousands of ASCII characters at &lt;/p&gt;
    &lt;p&gt;To make use of this in our ASCII renderer, we’ll calculate a lookup vector for each cell in the ASCII grid and pass it to &lt;code&gt;findBestCharacter&lt;/code&gt; to determine the character to display.&lt;/p&gt;
    &lt;p&gt;Let’s try it out. Consider the following zoomed-in circle as an example. It is split into three grid cells:&lt;/p&gt;
    &lt;p&gt;Overlaying our sampling circles, we see varying degrees of overlap:&lt;/p&gt;
    &lt;p&gt;When calculating the shape vector of each ASCII character, we took a huge number of samples. We could afford to do that because we only need to calculate those shape vectors once up front. After they’re calculated, we can use them again and again.&lt;/p&gt;
    &lt;p&gt;However, if we’re converting an animated image (e.g. canvas or video) to ASCII, we need to be mindful of performance when calculating the lookup vectors. An ASCII rendering might have hundreds or thousands of cells. Multiplying that by tens or hundreds of samples would be incredibly costly in terms of performance.&lt;/p&gt;
    &lt;p&gt;With that being said, let’s pick a sampling quality of &lt;/p&gt;
    &lt;p&gt;For the top sampling circle of the leftmost cell, we get one white sample and two black, giving us an average lightness of &lt;/p&gt;
    &lt;p&gt;From now on, instead of using the term “lookup vectors”, I’ll call these vectors, sampled from the image that we’re rendering as ASCII, sampling vectors. One sampling vector is calculated for each cell in the grid.&lt;/p&gt;
    &lt;p&gt;Anyway, we can use these sampling vectors to find the best-matching ASCII character. Let’s see what that looks like on our 2D plot — I’ll label the sampling vectors (from left to right) C0, C1, and C2:&lt;/p&gt;
    &lt;p&gt;Hmm... this is not what we want. Since none of the ASCII shape vector components exceed &lt;/p&gt;
    &lt;p&gt;We can fix this by normalizing the shape vectors. We’ll do that by taking the maximum value of each component across all shape vectors, and dividing the components of each shape vector by the maximum. Expressed in code, that looks like so:&lt;/p&gt;
    &lt;quote&gt;const max = [0, 0]for (const vector of characterVectors) {for (const [i, value] of Object.entries(vector)) {if (value &amp;gt; max[i]) {max[i] = value;}}}const normalizedCharacterVectors = characterVectors.map(vector =&amp;gt; vector.map((value, i) =&amp;gt; value / max[i]))&lt;/quote&gt;
    &lt;p&gt;Here’s what the plot looks like with the shape vectors normalized:&lt;/p&gt;
    &lt;p&gt;If we now map the sampling vectors to their nearest neighbors, we get a much more sensible result:&lt;/p&gt;
    &lt;p&gt;We get &lt;code&gt;'&lt;/code&gt;, &lt;code&gt;M&lt;/code&gt; and &lt;code&gt;$&lt;/code&gt;.  Let’s see how well those characters match the circle:&lt;/p&gt;
    &lt;p&gt;Nice! They match very well.&lt;/p&gt;
    &lt;p&gt;Let’s try rendering the full circle from before with the same method:&lt;/p&gt;
    &lt;p&gt;Much better than before! The picked characters follow the contour of the circle very well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limits of a 2D shape vector&lt;/head&gt;
    &lt;p&gt;Using two sampling circles — one upper and one lower — produces a much better result than the &lt;/p&gt;
    &lt;p&gt;For example, two circles don’t capture the shape of characters that fall in the middle of the cell. Consider &lt;code&gt;-&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;-&lt;/code&gt;, we get a shape vector of &lt;/p&gt;
    &lt;p&gt;The two upper-lower sampling circles also don’t capture left-right differences, such as the difference between &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;q&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;We could use such differences to get better character picks, but our two sampling circles don’t capture them. Let’s add more dimensions to our shape to fix that.&lt;/p&gt;
    &lt;head rend="h2"&gt;Increasing to 6 dimensions&lt;/head&gt;
    &lt;p&gt;Since cells are taller than they are wide (at least with the monospace font I’m using), we can use &lt;/p&gt;
    &lt;p&gt;&lt;code&gt;p&lt;/code&gt; and &lt;code&gt;q&lt;/code&gt;, while also capturing differences across the top, bottom, and middle regions of the cell, differentiating &lt;code&gt;^&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, and &lt;code&gt;_&lt;/code&gt;. They also capture the shape of “diagonal” characters like &lt;code&gt;/&lt;/code&gt; to a reasonable degree.&lt;/p&gt;
    &lt;p&gt;One problem with this grid-like configuration for the sampling circles is that there are gaps. For example, &lt;code&gt;.&lt;/code&gt; falls between the sampling circles:&lt;/p&gt;
    &lt;p&gt;To compensate for this, we can stagger the sampling circles vertically (e.g. lowering the left sampling circles and raising the right ones) and make them a bit larger. This causes the cell to be almost fully covered while not causing excessive overlap across the sampling circles:&lt;/p&gt;
    &lt;p&gt;We can use the same procedure as before to generate character vectors using these sampling circles, this time yielding a &lt;code&gt;L&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;L&lt;/code&gt;, we get the vector:&lt;/p&gt;
    &lt;p&gt;I’m presenting &lt;/p&gt;
    &lt;p&gt;The lightness values certainly look L-shaped! The 6D shape vector captures &lt;code&gt;L&lt;/code&gt;’s shape very well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Nearest neighbor lookups in a 6D space&lt;/head&gt;
    &lt;p&gt;Now we have a 6D shape vector for every ASCII character. Does that affect character lookups (how we find the best matching character)?&lt;/p&gt;
    &lt;p&gt;Earlier, in the &lt;code&gt;findBestCharacter&lt;/code&gt; function, I referenced a &lt;code&gt;getDistance&lt;/code&gt; function. That function returns the Euclidean distance between the input points. Given two 2D points &lt;/p&gt;
    &lt;p&gt;This generalizes to higher dimensions:&lt;/p&gt;
    &lt;p&gt;Put into code, this looks like so:&lt;/p&gt;
    &lt;quote&gt;function getDistance(a: number[], b: number[]): number {let sum = 0;for (let i = 0; i &amp;lt; a.length; i++) {sum += (a[i] - b[i]) ** 2;}return Math.sqrt(sum);}&lt;/quote&gt;
    &lt;p&gt;Note: since we’re just using this for the purposes of finding the closest point, we can skip the expensive &lt;code&gt;Math.sqrt()&lt;/code&gt; call and just return the squared distance. It does not affect the result.&lt;/p&gt;
    &lt;p&gt;So, no, the dimensionality of our shape vector does not change lookups at all. We can use the same &lt;code&gt;getDistance&lt;/code&gt; function for both 2D and 6D.&lt;/p&gt;
    &lt;p&gt;With that out of the way, let’s see what the 6D approach yields!&lt;/p&gt;
    &lt;head rend="h3"&gt;Trying out the 6D approach&lt;/head&gt;
    &lt;p&gt;Our new 6D approach works really well for flat shapes, like the circle example we’ve been using:&lt;/p&gt;
    &lt;p&gt;Now let’s see how this approach works when we render a 3D scene with more shades of gray:&lt;/p&gt;
    &lt;p&gt;Firstly, the outer contours look nice and sharp. I also like how well the gradients across the sphere and cone look.&lt;/p&gt;
    &lt;p&gt;However, internally, the objects all kind of blend together. The edges between surfaces with different lightnesses aren’t sharp enough. For example, the lighter faces of the cubes all kind of blend into one solid color. When there is a change in color — like when two faces of a cube meet — I’d like to see more sharpness in the ASCII rendering.&lt;/p&gt;
    &lt;p&gt;To demonstrate what I mean, consider the following split:&lt;/p&gt;
    &lt;p&gt;It’s currently rendered like so:&lt;/p&gt;
    &lt;p&gt;The different shades result in &lt;code&gt;i&lt;/code&gt;s on the left and &lt;code&gt;B&lt;/code&gt;s on the right, but the boundary is not very sharp.&lt;/p&gt;
    &lt;p&gt;By applying some effects to the sampling vector, we can enhance the contrast at the boundary so that it appears sharper:&lt;/p&gt;
    &lt;p&gt;The added contrast makes a big difference in readability for the 3D scene. Let’s look at how we can implement this contrast enhancement effect.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contrast enhancement&lt;/head&gt;
    &lt;p&gt;Consider cells overlapping a color boundary like so:&lt;/p&gt;
    &lt;p&gt;For the cells on the boundary, we get a 6D sampling vector that looks like so:&lt;/p&gt;
    &lt;p&gt;To make future examples easier to visualize, I’ll start drawing the sampling vector using &lt;/p&gt;
    &lt;p&gt;Currently, this sampling vector resolves to the character &lt;code&gt;T&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;That’s a sensible choice. The character &lt;code&gt;T&lt;/code&gt; is visually dense in the top half and less so in the bottom half, so it matches the image fairly well.&lt;/p&gt;
    &lt;p&gt;Still, I want the picked character to emphasize the shape of the boundary better. We can achieve that by enhancing the contrast of the sampling vector.&lt;/p&gt;
    &lt;p&gt;To increase the contrast of our sampling vector, we might raise each component of the vector to the power of some exponent.&lt;/p&gt;
    &lt;p&gt;Consider how an exponent affects values between &lt;/p&gt;
    &lt;p&gt;The level of pull depends on the exponent. Here’s a chart of &lt;/p&gt;
    &lt;p&gt;This effect becomes more pronounced with higher exponents:&lt;/p&gt;
    &lt;p&gt;A higher exponent translates to a stronger pull towards zero.&lt;/p&gt;
    &lt;p&gt;Applying an exponent should make dark values darker more quickly than light ones. The example below allows you to vary the exponent applied to the sampling vector:&lt;/p&gt;
    &lt;p&gt;As the exponent is increased to &lt;/p&gt;
    &lt;p&gt;I don’t want that. I want to increase the contrast between the lighter and darker components of the sampling vector, not the vector in its entirety.&lt;/p&gt;
    &lt;p&gt;To achieve that, we can normalize the sampling vector to the range &lt;/p&gt;
    &lt;p&gt;The normalization to &lt;/p&gt;
    &lt;quote&gt;const maxValue = Math.max(...samplingVector)samplingVector = samplingVector.map((value) =&amp;gt; {value = x / maxValue; // Normalizevalue = Math.pow(x, exponent);value = x * maxValue; // Denormalizereturn value;})&lt;/quote&gt;
    &lt;p&gt;Here’s the same example, but with this normalization applied:&lt;/p&gt;
    &lt;p&gt;Very nice! The lightest component values are retained, and the contrast between the lighter and darker components is increased by “crunching” the lower values.&lt;/p&gt;
    &lt;p&gt;This affects which character is picked. The following example shows how the selected character changes as the contrast is increased:&lt;/p&gt;
    &lt;p&gt;Awesome! The pick of &lt;code&gt;"&lt;/code&gt; over &lt;code&gt;T&lt;/code&gt; emphasizes the separation between the lighter region above and the darker region below!&lt;/p&gt;
    &lt;p&gt;By enhancing the contrast of the sampling vector, we exaggerate its shape. This gives us a character that less faithfully represents the underlying image, but improves readability as a whole by enhancing the separation between different colored regions.&lt;/p&gt;
    &lt;p&gt;Let’s look at another example. Observe how the L-shape of the sampling vector below becomes more pronounced as the exponent increases, and how that affects the picked character:&lt;/p&gt;
    &lt;p&gt;Works really nicely! I love the transition from &lt;code&gt;&amp;amp; -&amp;gt; b -&amp;gt; L&lt;/code&gt; as the L-shape of the vector becomes clearer.&lt;/p&gt;
    &lt;p&gt;What’s nice about applying exponents to normalized sampling vectors is that it barely affects vectors that are uniform in value. If all component values are similar, applying an exponent has a minimal effect:&lt;/p&gt;
    &lt;p&gt;Because the vector is fairly uniform, the exponent only has a slight effect and doesn’t change the picked character.&lt;/p&gt;
    &lt;p&gt;This is a good thing! If we have a smooth gradient in our image, we want to retain it. We very much do not want to introduce unnecessary choppiness.&lt;/p&gt;
    &lt;p&gt;Compare the 3D scene ASCII rendering with and without this contrast enhancement:&lt;/p&gt;
    &lt;p&gt;We do see more contrast at boundaries, but this is not quite there yet. Some edges are still not sharp enough, and we also observe a “staircasing” effect happening at some boundaries.&lt;/p&gt;
    &lt;p&gt;Let’s look at the staircasing effect first. We can reproduce it with a boundary like so:&lt;/p&gt;
    &lt;p&gt;Below is the ASCII rendering of that boundary. Notice how the lower edge (the &lt;code&gt;!&lt;/code&gt;s) becomes “staircase-y” as you increase the exponent:&lt;/p&gt;
    &lt;p&gt;We see a staircase pattern like so:&lt;/p&gt;
    &lt;quote&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;To understand why that’s happening, let’s consider the row in the middle of the canvas, progressing from left to right. As we start off, every sample is equally light, giving us &lt;code&gt;U&lt;/code&gt;s:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUU -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;As we reach the boundary, the lower right samples become a bit darker. Those darker components are crunched by contrast enhancement, giving us some &lt;code&gt;Y&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;So we get:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYY -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;As we progress further right, the middle and lower samples get darker, so we get some &lt;code&gt;f&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;This trend continues towards &lt;code&gt;"&lt;/code&gt;, &lt;code&gt;'&lt;/code&gt;, and finally, &lt;code&gt;`&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Giving us a sequence like so:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYYf""''` -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;That looks good, but at some point we get no light samples. Once we get no light samples, our contrast enhancement has no effect because every component is equally light. This causes us to always get &lt;code&gt;!&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;Making our sequence look like so:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYYf""''`!!!!!!!!!! -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;This sudden stop in contrast enhancement having an effect is what causes the staircasing effect:&lt;/p&gt;
    &lt;quote&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;Let’s see how we can counteract this staircasing effect with another layer of contrast enhancement, this time looking outside of the boundary of each cell.&lt;/p&gt;
    &lt;head rend="h3"&gt;Directional contrast enhancement&lt;/head&gt;
    &lt;p&gt;We currently have sampling circles arranged like so:&lt;/p&gt;
    &lt;p&gt;For each of those sampling circles, we’ll specify an “external sampling circle”, placed outside of the cell’s boundary, like so:&lt;/p&gt;
    &lt;p&gt;Each of those external sampling circles is “reaching” into the region of a neighboring cell. Together, the samples that are collected by the external sampling circles constitute an “external sampling vector”.&lt;/p&gt;
    &lt;p&gt;Let’s simplify the visualization and consider a single example. Imagine that we collected a sampling vector and an external sampling vector that look like so:&lt;/p&gt;
    &lt;p&gt;The circles colored red are the external sampling vector components. Currently, they have no effect.&lt;/p&gt;
    &lt;p&gt;The “internal” sampling vector itself is fairly uniform, with values ranging from &lt;/p&gt;
    &lt;p&gt;To enhance this apparent boundary, we’ll darken the top-left and middle-left components of the sampling vector. We can do that by applying component-wise contrast enhancement using the values from the external vector.&lt;/p&gt;
    &lt;p&gt;In the previous contrast enhancement, we calculated the maximum component value across the sampling vector and normalized the vector using that value:&lt;/p&gt;
    &lt;quote&gt;const maxValue = Math.max(...samplingVector)samplingVector = samplingVector.map((value) =&amp;gt; {value = x / maxValue; // Normalizevalue = Math.pow(x, exponent);value = x * maxValue; // Denormalizereturn value;})&lt;/quote&gt;
    &lt;p&gt;But the new component-wise contrast enhancement will take the maximum value between each component of the sampling vector and the corresponding component in the external sampling vector:&lt;/p&gt;
    &lt;quote&gt;samplingVector = samplingVector.map((value, i) =&amp;gt; {const maxValue = Math.max(value, externalSamplingVector[i])// ...});&lt;/quote&gt;
    &lt;p&gt;Aside from that, the contrast enhancement is performed in the same way:&lt;/p&gt;
    &lt;quote&gt;samplingVector = samplingVector.map((value, i) =&amp;gt; {const maxValue = Math.max(value, externalSamplingVector[i]);value = value / maxValue;value = Math.pow(value, exponent);value = value * maxValue;return value;});&lt;/quote&gt;
    &lt;p&gt;The example below shows how light values in the external sampling vector push values in the sampling vector down:&lt;/p&gt;
    &lt;p&gt;I call this “directional contrast enhancement”, since each of the external sampling circles reaches outside of the cell in the direction of the sampling vector component that it is enhancing the contrast of. I describe the other effect as “global contrast enhancement” since it acts on all of the sampling vector’s components together.&lt;/p&gt;
    &lt;p&gt;Let’s see what this directional contrast enhancement does to get rid of the staircasing effect:&lt;/p&gt;
    &lt;p&gt;Hmm, that’s not doing what I wanted. I wanted to see a sequence like so:&lt;/p&gt;
    &lt;quote&gt;..::!!..::!!!!!!!!..::!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;But we just see &lt;code&gt;!&lt;/code&gt; changing to &lt;code&gt;:&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;This happens because the directional contrast enhancement doesn’t reach far enough into our sampling vector. The light upper values in the external vector do push the upper values of the sampling vector down, but because the lightness of the four bottom components is retained, we don’t get to &lt;code&gt;.&lt;/code&gt;, just &lt;code&gt;:&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Widening the directional contrast enhancement&lt;/head&gt;
    &lt;p&gt;I’d like to “widen” the directional contrast enhancement so that, for example, light external values at the top spread to the middle components of the sampling vector.&lt;/p&gt;
    &lt;p&gt;To do that, I’ll introduce a few more external sampling circles, arranged like so:&lt;/p&gt;
    &lt;p&gt;These are a total of &lt;/p&gt;
    &lt;p&gt;For each component of the internal sampling vector, we’ll calculate the maximum value across the external sampling vector components that affect it, and use that maximum to perform the contrast enhancement.&lt;/p&gt;
    &lt;p&gt;Let’s implement that. I’ll order the internal and external sampling circles like so:&lt;/p&gt;
    &lt;p&gt;We can then define a mapping from the internal circles to the external sampling circles that affect them:&lt;/p&gt;
    &lt;quote&gt;const AFFECTING_EXTERNAL_INDICES = [[0, 1, 2, 4],[0, 1, 3, 5],[2, 4, 6],[3, 5, 7],[4, 6, 8, 9],[5, 7, 8, 9],];&lt;/quote&gt;
    &lt;p&gt;With this, we can change the calculation of &lt;code&gt;maxValue&lt;/code&gt; to take the maximum affecting external value:&lt;/p&gt;
    &lt;quote&gt;// Beforeconst maxValue = Math.max(value, externalSamplingVector[i]);// Afterlet maxValue = value;for (const externalIndex of AFFECTING_EXTERNAL_INDICES[i]) {maxValue = Math.max(value, externalSamplingVector[externalIndex]);}&lt;/quote&gt;
    &lt;p&gt;Now look what happens if the top four external sampling circles are light: it causes the contrast enhancement to reach into the middle of the sampling vector, giving us the desired effect:&lt;/p&gt;
    &lt;p&gt;We now smoothly transition from &lt;code&gt;! -&amp;gt; : -&amp;gt; .&lt;/code&gt; — beautiful stuff!&lt;/p&gt;
    &lt;p&gt;Let’s see if this change resolves the staircasing effect:&lt;/p&gt;
    &lt;p&gt;Oh yeah, looks awesome! We get the desired effect. The boundary is nice and sharp while not being too jagged.&lt;/p&gt;
    &lt;p&gt;Here’s the 3D scene again. The contrast slider now applies both types of contrast enhancement at the same time — try it out:&lt;/p&gt;
    &lt;p&gt;This really enhances the contrast at boundaries, making the image far more readable!&lt;/p&gt;
    &lt;p&gt;Together, the 6D shape vector approach and contrast enhancement techniques have given us a really nice final ASCII rendering.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final words&lt;/head&gt;
    &lt;p&gt;This post was really fun to build and write! I hope you enjoyed reading it.&lt;/p&gt;
    &lt;p&gt;ASCII rendering is perhaps not the most useful topic to write about, but I think the idea of using a high-dimensional vector to capture shape is interesting and could easily be applied to many other problems. There are parallels to be drawn to word embeddings.&lt;/p&gt;
    &lt;p&gt;I started writing this ASCII renderer to see if the idea of using a vector to capture the shape of characters would work at all. That approach turned out to work very well, but the initial prototype was terribly slow — I only got single-digit FPS on my iPhone. To get the ASCII renderer running at a smooth &lt;/p&gt;
    &lt;p&gt;My colleagues, after reading a draft of this post, suggested many alternatives to the approaches I described in this post. For example, why not make the sampling vector &lt;code&gt;T&lt;/code&gt; far better — just look how &lt;code&gt;T&lt;/code&gt;’s stem falls between the two sampling circles in each row:&lt;/p&gt;
    &lt;p&gt;And yeah, he’s right! A &lt;/p&gt;
    &lt;p&gt;It’s really fun how large the solution space to the problem of ASCII rendering is. There are so, so many approaches and trade-offs to explore. I imagine you probably thought of a few yourself while reading this post!&lt;/p&gt;
    &lt;p&gt;One dimension I intentionally did not explore was using different colors or lightnesses for the ASCII characters themselves. This is for many reasons, but the two primary ones are that 1) it would have expanded the scope of this post too much, and 2) it’s just a different effect, and I personally don’t like the look.&lt;/p&gt;
    &lt;p&gt;At the time of writing these final words, around &lt;/p&gt;
    &lt;p&gt;Thanks for reading! And huge thanks to Gunnlaugur Þór Briem and Eiríkur Fannar Torfason for reading and providing feedback on a draft of this post.&lt;/p&gt;
    &lt;p&gt;— Alex Harri&lt;/p&gt;
    &lt;p&gt;To be notified of new posts, subscribe to my mailing list.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix I: Character lookup performance&lt;/head&gt;
    &lt;p&gt;Earlier in this post, I showed how can find the best character by finding the character with the shortest Euclidean distance to our sampling vector.&lt;/p&gt;
    &lt;quote&gt;function findBestCharacter(inputVector: number[]) {let bestCharacter = "";let bestDistance = Infinity;for (const { character, shapeVector } of CHARACTERS) {const dist = getDistance(shapeVector, inputVector);if (dist &amp;lt; bestDistance) {bestDistance = dist;bestCharacter = character;}}return bestCharacter;}&lt;/quote&gt;
    &lt;p&gt;I tried benchmarking this for &lt;/p&gt;
    &lt;p&gt;If we allow ourselves &lt;/p&gt;
    &lt;head rend="h3"&gt;k-d trees&lt;/head&gt;
    &lt;p&gt;Internally, &lt;/p&gt;
    &lt;p&gt;I won’t go into much detail on &lt;/p&gt;
    &lt;p&gt;One could also look at the hierarchical navigable small worlds (HNSW) algorithm, which Eiríkur pointed me to. It is used for approximate nearest neighbor lookups in vector databases, so definitely relevant.&lt;/p&gt;
    &lt;p&gt;Let’s see how it performs! We’ll construct a &lt;/p&gt;
    &lt;quote&gt;const kdTree = new KdTree(CHARACTERS.map(({ character, shapeVector }) =&amp;gt; ({point: shapeVector,data: character,})));&lt;/quote&gt;
    &lt;p&gt;We can now perform nearest-neighbor lookups on the &lt;/p&gt;
    &lt;quote&gt;const result = kdTree.findNearest(samplingVector);&lt;/quote&gt;
    &lt;p&gt;Running &lt;/p&gt;
    &lt;p&gt;That’s a lot of lookups per frame, but again, we’re benchmarking on a powerful machine. This is still not good enough.&lt;/p&gt;
    &lt;p&gt;Let’s see how we can eke out even more performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caching&lt;/head&gt;
    &lt;p&gt;An obvious avenue for speeding up lookups is to cache the result:&lt;/p&gt;
    &lt;quote&gt;function searchCached(samplingVector: number[]) {const key = generateCacheKey(samplingVector)if (cache.has(key)) {return cache.get(key)!;}const result = search(samplingVector);cache.set(key, result);return result;}&lt;/quote&gt;
    &lt;p&gt;But how does one generate a cache key for a &lt;/p&gt;
    &lt;p&gt;Well, one way is to quantize each vector component so that it fits into a set number of bits and packing those bits into a single number. JavaScript numbers give us &lt;/p&gt;
    &lt;p&gt;We can quantize a numeric value between &lt;/p&gt;
    &lt;quote&gt;const BITS = 5;const RANGE = 2 ** BITS;function quantizeTo5Bits(value: number) {return Math.min(RANGE - 1, Math.floor(value * RANGE));}&lt;/quote&gt;
    &lt;p&gt;Applying a max of &lt;code&gt;RANGE - 1&lt;/code&gt; is done so that a &lt;code&gt;value&lt;/code&gt; of exactly &lt;/p&gt;
    &lt;p&gt;We can quantize each of the sampling vector components in this manner and use bit shifting to pack all of the quantized values into a single number like so:&lt;/p&gt;
    &lt;quote&gt;const BITS = 5;const RANGE = 2 ** BITS;function generateCacheKey(vector: number[]): number {let key = 0;for (let i = 0; i &amp;lt; vector.length; i++) {const quantized = Math.min(RANGE - 1, Math.floor(vector[i] * RANGE));key = (key &amp;lt;&amp;lt; BITS) | quantized;}return key;}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;RANGE&lt;/code&gt; is current set to &lt;code&gt;2 ** 5&lt;/code&gt;, but consider how large that makes our key space. Each vector component is one of &lt;/p&gt;
    &lt;p&gt;Alright, &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of keys&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory needed to store keys&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;46,656&lt;/cell&gt;
        &lt;cell&gt;364 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;117,649&lt;/cell&gt;
        &lt;cell&gt;919 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;262,144&lt;/cell&gt;
        &lt;cell&gt;2.00 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;531,441&lt;/cell&gt;
        &lt;cell&gt;4.05 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;1,000,000&lt;/cell&gt;
        &lt;cell&gt;7.63 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;1,771,561&lt;/cell&gt;
        &lt;cell&gt;13.52 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;2,985,984&lt;/cell&gt;
        &lt;cell&gt;22.78 MB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are trade-offs to consider here. As the range gets smaller, the quality of the results drops. If we pick a range of &lt;/p&gt;
    &lt;p&gt;At the same time, if we increase the possible number of keys, we need more memory to store them. Additionally, the cache hit rate might be very low, especially when the cache is relatively empty.&lt;/p&gt;
    &lt;p&gt;I ended up picking a range of &lt;/p&gt;
    &lt;p&gt;Cached lookups are incredibly fast — fast enough that lookup performance just isn’t a concern anymore (&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix II: GPU acceleration&lt;/head&gt;
    &lt;p&gt;Lookups were not the only performance concern. Just collecting the sampling vectors (internal and external) turned out to be terribly expensive.&lt;/p&gt;
    &lt;p&gt;Just consider the sheer amount of samples that need to be collected. The 3D scene I’ve been using as an example uses a &lt;/p&gt;
    &lt;p&gt;And that’s if we use a sampling quality of &lt;/p&gt;
    &lt;p&gt;Collecting these samples absolutely crushed performance on my iPhone, so I needed to either collect fewer samples or speed up the collection of samples. Collecting fewer samples would have meant rendering fewer ASCII characters or removing the directional contrast enhancement, neither of which was an appealing solution.&lt;/p&gt;
    &lt;p&gt;My initial implementation ran on the CPU, which could only collect one sample at a time. To speed this up, I moved the work of sampling collection and applying the contrast enhancement to the GPU. The pipeline for that looks like so (each of the steps listed is a single shader pass):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Collect the raw internal sampling vectors into a &lt;mjx-container/&gt;texture, using the canvas (image) as the input texture.&lt;/item&gt;
      &lt;item&gt;Do the same for the external sampling vectors.&lt;/item&gt;
      &lt;item&gt;Calculate the maximum external value affecting each internal vector component into a &lt;mjx-container/&gt;texture.&lt;/item&gt;
      &lt;item&gt;Apply directional contrast enhancement to each sampling vector component, using the maximum external values texture.&lt;/item&gt;
      &lt;item&gt;Calculate the maximum value for each internal sampling vector into a &lt;mjx-container/&gt;texture.&lt;/item&gt;
      &lt;item&gt;Apply global contrast enhancement to each sampling vector component, using the maximum internal values texture.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m glossing over the details because I could spend a whole other post covering them, but moving work to the GPU made the renderer many times more performant than it was when everything ran on the CPU.&lt;/p&gt;
    &lt;p&gt;To be notified of new posts, subscribe to my mailing list.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alexharri.com/blog/ascii-rendering"/><published>2026-01-17T11:15:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657141</id><title>Architecture for Disposable Systems</title><updated>2026-01-17T14:10:03.161646+00:00</updated><content>&lt;doc fingerprint="2b4d3dde2be75ffc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Architecture for Disposable Systems&lt;/head&gt;
    &lt;head rend="h5"&gt;Posted on January 15, 2026 • 3 minutes • 595 words&lt;/head&gt;
    &lt;p&gt;As software gets cheaper to produce (thanks to coding agents) and quality expectations shift, we’re witnessing the rise of disposable software: code that you generate, use, and discard rather than maintain indefinitely.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Traditional Model&lt;/head&gt;
    &lt;p&gt;Traditional software follows a well-established pattern: you build something once, maintain it indefinitely, and pay for it through high upfront capital and long-term maintenance costs. The economics made sense because rewriting was expensive. We accepted spending 80% of a project’s lifecycle on maintenance because the alternative (starting over) was often prohibitive (until the product reaches its EOL)&lt;/p&gt;
    &lt;p&gt;This created a culture of careful engineering: clean code, thoughtful architecture, and refactoring to reduce technical debt. We optimized for the long term because the long term was inevitable. We have to live with it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Disposable Shift&lt;/head&gt;
    &lt;p&gt;But what happens when an agent can regenerate a functional replacement from a prompt in 5 minutes? The incentive to “clean up technical debt” or “refactor for the long term” vanishes. If the code works now and you can regenerate it later, why invest in perfection?&lt;/p&gt;
    &lt;p&gt;We’re already seeing the rise of “vibe coding”: building tools that solve a problem right now. Need a specific data parser? Generate it. Need a one-off dashboard for a meeting? Generate it. Use it, and if it breaks or becomes obsolete, delete it and generate a new one. You don’t care if the code is “clean” as long as the output is correct.&lt;/p&gt;
    &lt;p&gt;This isn’t laziness. It’s a fundamental shift in the economics of software development. When generation is cheap, maintenance becomes the expensive option.&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture for Disposable Systems&lt;/head&gt;
    &lt;p&gt;If we’re moving toward disposable software, how do we architect systems that can survive this shift? The answer lies in a three-layer model:&lt;/p&gt;
    &lt;head rend="h3"&gt;The Core (Durable)&lt;/head&gt;
    &lt;p&gt;The Source of Truth. This is the hardened, human-written, slow-changing foundation of your system. It contains your critical business logic, data models, and core algorithms. This layer is built to last because it represents the fundamental value of your system.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Connectors (APIs)&lt;/head&gt;
    &lt;p&gt;Immutable contracts. These are the interfaces that define how components communicate. They must be perfect because the disposable parts can be imperfect. If your API contract is solid, you can swap out implementations underneath without breaking the system.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Disposable Layer&lt;/head&gt;
    &lt;p&gt;AI-generated “glue” code, data parsers, UI components, and integration scripts. This is where the vibe coding happens. Generate it, use it, and regenerate it when needed. As long as it adheres to the contracts defined by the Connectors layer, it doesn’t matter how messy the internals are.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contract-First Design&lt;/head&gt;
    &lt;p&gt;The key to making this work is contract-first design. Instead of coding to an implementation, we must code to a strict schema: OpenAPI, gRPC, Smithy, or whatever standard fits your domain. The agent is given the schema as a constraint, and as long as the inputs and outputs match the contract, we don’t care how messy the logic inside the box is.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The critical principle: Immutable contracts. They must be perfect so the disposable parts can be imperfect.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This approach allows you to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Regenerate components without breaking the system&lt;/item&gt;
      &lt;item&gt;Test contracts independently of implementations&lt;/item&gt;
      &lt;item&gt;Evolve the disposable layer while keeping the core stable&lt;/item&gt;
      &lt;item&gt;Accept lower-quality generated code because it’s constrained by high-quality contracts&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Future&lt;/head&gt;
    &lt;p&gt;We’re not there yet, but the trajectory is clear. As coding agents improve and generation costs drop, more and more software will become disposable. The systems that survive will be those built with durable cores, immutable contracts, and disposable peripherals.&lt;/p&gt;
    &lt;p&gt;The question isn’t whether this shift will happen. It’s whether your architecture is ready for it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tuananh.net/2026/01/15/architecture-for-disposable-systems/"/><published>2026-01-17T11:18:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657194</id><title>Fitdrop: Personal exploration of fashion from 1980 to 2025</title><updated>2026-01-17T14:10:03.039204+00:00</updated><content>&lt;doc fingerprint="f323f1f75af38214"&gt;
  &lt;main&gt;
    &lt;p&gt;Fitdrop is a personal exploration of fashion from 1980 to 2025.&lt;/p&gt;
    &lt;p&gt;If you want to know more about an outfit drag it to the corner. If the pile gets too messy restart the drop, or just chuck things around.&lt;/p&gt;
    &lt;p&gt;The Stack:&lt;/p&gt;
    &lt;p&gt;(And dropping things is just satisfying.)&lt;/p&gt;
    &lt;p&gt;Hi, I'm Iain. I work at a place called FOOD. I've been a web nerd for a lot of years. But my ability to make things has always been limited by my inability to code. My brain just doesn't work that way.&lt;/p&gt;
    &lt;p&gt;'Vibe coding' is allowing me to make silly projects like this. Things that I'd only dreamed of being able to make previously.&lt;/p&gt;
    &lt;p&gt;I'm happy to share more on how I did it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fitdrop.cc/"/><published>2026-01-17T11:28:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657296</id><title>The 600-year-old origins of the word 'hello'</title><updated>2026-01-17T14:10:02.874982+00:00</updated><content>&lt;doc fingerprint="b2309d8d8fa112dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'Hullo, hillo, holla': The 600-year-old origins of the word 'hello'&lt;/head&gt;
    &lt;p&gt;It's been 200 years since the word "hello" was first used in print – though its beginnings date back to the 15th Century. How has the language of greetings evolved around the world - and what does it tell us about ourselves?&lt;/p&gt;
    &lt;p&gt;We use "hello" dozens of times a day without thinking – during phone calls, emails and face-to-face encounters. We sing it along with Adele and Lionel Richie, and we have watched it spun into moments of screen gold in Jerry Maguire ("You had me at hello"), and Scarface ("Say hello to my little friend!"). It's been used to sell everything from mobile phones (Motorola's "Hello, Moto") to lingerie (Wonderbra's iconic "Hello boys"), and it has been borrowed to name computer programs and celebrity magazines.&lt;/p&gt;
    &lt;p&gt;In print, this ubiquitous, friendly greeting has a surprisingly short history. Two centuries ago, on 18 January 1826, "hello" made what is thought to be its earliest recorded appearance on the page, in a Connecticut newspaper called The Norwich Courier. Hidden among the column inches, it was a modest in-ink debut for a word that would go on to greet much of the modern world.&lt;/p&gt;
    &lt;p&gt;By the 1850s, it had crossed the Atlantic to Britain – appearing in publications such as the London Literary Gazette – and became increasingly common in print. Like the go-to greetings in other languages, "hello" also says something about the English-speaking world – depending on which variation, abbreviation or inflection of the word we choose to use.&lt;/p&gt;
    &lt;p&gt;There are plenty of such forms. Whether due to dialect or accent influences, or the brevity demanded by online communication, which "hello" you choose says a lot about you, and can indicate age, nationality, or even mood. According to linguists, elongated variations such as "heyyy" could be construed as flirtatious, "hellaw" might suggest you're from the southern US, "howdy" from western US, and the clipped "hi" may indicate a curt disposition.&lt;/p&gt;
    &lt;p&gt;"It can be pronounced and inflected in many different ways, and these subtle intonational contours can change its meaning," says Alessandro Duranti, professor of linguistic anthropology at the University of California, Los Angeles. "For example, when someone says 'hello' with a stretched final vowel, it can question what the other person just said, as in 'Hello, are you paying attention?' or 'Hello, you must be kidding.'"&lt;/p&gt;
    &lt;p&gt;This capacity to convey nuance through tone and form is no modern invention; even in its first printed appearances, "hello" was a patchwork of influences, derivations and applications drawn from several languages.&lt;/p&gt;
    &lt;head rend="h2"&gt;The origins of hello&lt;/head&gt;
    &lt;p&gt;The pre-printed origins of the word "hello" are disputed. The most commonly cited etymology is the Old High German "halâ" – a cry historically used to hail a ferryman. The Oxford English Dictionary also points to "halloo" (a hunting call that urged hounds to run faster) as a possible linguistic root. It notes several early spellings, including "hullo", "hillo" and "holla" – the latter thought to have derived from the 15th-Century French "hol", an exclamation meaning "whoa!" or "stop!". In English sources, the OED lists the earliest form as the late-16th-Century "hollo".&lt;/p&gt;
    &lt;p&gt;Simon Horobin, professor of English language and literature at Magdelen College, Oxford, notes that such semantic shifts and spelling changes may also be explained by regional accents and differences in pronunciation. "Especially in the example of 'ello' which shows the prevalent – though now stigmatised – feature of h-dropping," he tells the BBC, referring to the classist English stereotype of a dropped 'h' indicating a lack of education.&lt;/p&gt;
    &lt;p&gt;"But for origins and early history," he adds, "we are dependent upon written evidence, which is patchy at the best of times. For a colloquial word like this, which would have appeared much earlier and more frequently in speech than in writing, it is especially tricky to establish a definite timeline."&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;• The most powerful word in the English language&lt;/p&gt;
    &lt;p&gt;• The surprising history of the word 'dude'&lt;/p&gt;
    &lt;p&gt;• The subtle way language shapes us&lt;/p&gt;
    &lt;p&gt;The selection of a standardised word form, Horobin explains, usually falls to lexicographers – those who compile dictionaries. "They base their choice on the relative prevalence of a particular spelling, though it's necessarily somewhat provisional and arbitrary."&lt;/p&gt;
    &lt;p&gt;By the time the Oxford English Dictionary first went to press in 1884, "hello" was emerging as the dominant form of the greeting. Charles Dickens, however, spent the 19th Century using "hullo" in his writings, and Alexander Graham Bell (who once argued that "ahoy!" would make a superior telephone greeting) stuck with "halloo". Bell's rival, Thomas Edison, championed "hello", believing it would carry clearly over even the worst phone lines. Like that of The Norwich Courier before him, Edison's backing helped – and "hello" was established as the English-language greeting to beat.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hello around the world&lt;/head&gt;
    &lt;p&gt;While the English language settled on "hello" as its customary greeting, other languages forged their own. Some were influenced by English, others developed independently – yet each carries a distinct cultural flavour, hinting at the social norms and stereotypes we have of the people who use it.&lt;/p&gt;
    &lt;p&gt;In Germanic and Scandinavian languages, for example, "hallo" and "hallå" are phonetically harder and feel more efficient and no-nonsense than the lyrical, almost poetic quality of "hola" and "olá", favoured by the Romance languages that are associated with more effusive stereotypes. Elsewhere, some greetings carry traces of national history: from the Dutch-derived "hallo" of Afrikaans to "óla" in Tetum, a reminder of Portuguese influence in Timor-Leste. Many such words appear to function as both introduction and identity marker. But, says Professor Duranti, it's not quite that simple.&lt;/p&gt;
    &lt;p&gt;"It's hard to go straight from the use of a particular greeting to a national character, even though it is tempting," he tells the BBC. Alternative or secondary greetings, Duranti suggests, may offer better clues. "In English, given the common use of 'how are you?', there is an apparent interest in people's wellbeing." In some Polynesian societies, he adds, greetings are less about a word-for-word "hello" than about checking in on someone's plans or movements – literally asking "where are you going?". Greek, meanwhile, uses "Γειά σου" (pronounced "yah-soo") as a typical informal greeting, offering a wish for health rather than a simple salutation. It is also usable for "goodbye".&lt;/p&gt;
    &lt;p&gt;Other languages also turn abstract concepts into multipurpose greetings that serve as both "hi" and "bye". "Ciao" comes from a Venetian dialect phrase meaning "at your service", and the French "salut" is an informal expression used for both greeting and parting company. Similarly, the Hawaiian "aloha" can express affection or compassion, and the Hebrew "shalom" peace or wholeness. Yet, as Duranti cautions, even these evocative examples shouldn't be viewed as cut-and-dry indicators of national character.&lt;/p&gt;
    &lt;p&gt;"I would be careful making that kind of correlation," he explains. "Especially about the semantics of it – health versus sympathy versus whereabouts. But there is one aspect of greetings that is sensitive to the social structure of a society, which is that equals greet each other in different ways from people of different statuses. In fact, greetings can be seen to define levels of intimacy or social distance." In this sense, he adds, greetings are like magnets – confidently announcing who we are, and drawing in those we want to be associated with.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hello in the digital age&lt;/head&gt;
    &lt;p&gt;If greetings act as social magnets, then technology has quietly altered their pull. Over the past few decades, the rise of email, texting and social media has reshaped not just how often we say "hello", but what we might replace it with – and whether we say it at all.&lt;/p&gt;
    &lt;p&gt;"If you think about WhatsApp, we're basically always in conversation – we're always online," says Christian Ilbury, senior lecturer in linguistics and English language at the University of Edinburgh. "When someone asks you how your day is or whether you're going to be on time for the meal, you don't always have to say 'hello' first, because it's unlikely the last message concluded with 'bye'."&lt;/p&gt;
    &lt;p&gt;In a text-led, always-on world, greetings have proved especially susceptible to change and, as they are used so often, their evolution has accelerated dramatically. Ilbury has identified many non-standard and creative spellings of "hello" in his studies of digital language, from "hellooooo" and "hiiiiiii" to "heyyyyy". Yet, while tech has made it easier for us to elongate words in this way, Ilbury points out that most modern-day greetings are short, sharp and driven by brevity.&lt;/p&gt;
    &lt;p&gt;"The most obvious thing to say is that people now sometimes use an emoji – the wave – in place of the word 'hello'," says Ilbury. "But technology has always contributed to language change. We now 'Google' stuff and 'unfriend' people. Like any major invention – AI, for instance – we're bound to get some new vocabulary from that source."&lt;/p&gt;
    &lt;p&gt;In many ways, this mirrors the instability of "hello" in the early 19th Century, when the greeting may have sounded vaguely the same whenever spoken, but varied widely in spelling when written down. By shortening the established greeting, or replacing it with icons and abbreviations, it's made clear that such salutations remain as fluid as they were before The Norwich Couriermade its landmark linguistic choice in 1826.&lt;/p&gt;
    &lt;p&gt;But for all its so-called standardisation, "hello" has never really stood still. It began as a shout, a summons, a way to hail attention, before settling – briefly – into an accepted spelling and usage. Two centuries on from its print debut, the greeting is once again being stretched, clipped, replaced or ignored altogether. Yet whether it's spoken aloud, typed hastily, or reduced to a small waving hand on a screen, the impulse behind it remains the same: an act of recognition, the announcing of one's presence and just asking – however casually – to be acknowledged in return.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;If you liked this story, sign up for The Essential List newsletter – a handpicked selection of features, videos and can't-miss news, delivered to your inbox twice a week.&lt;/p&gt;
    &lt;p&gt;For more Culture stories from the BBC, follow us on Facebook and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/culture/article/20260113-hello-hiya-aloha-what-our-greetings-reveal"/><published>2026-01-17T11:51:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657719</id><title>The Risks of AI in Schools Outweigh the Benefits, Report Says</title><updated>2026-01-17T14:10:02.523508+00:00</updated><content>&lt;doc fingerprint="1e1e3318088f9def"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The risks of AI in schools outweigh the benefits, report says&lt;/head&gt;
    &lt;p&gt;The risks of using generative artificial intelligence to educate children and teens currently overshadow the benefits, according to a new study by the Brookings Institution's Center for Universal Education.&lt;/p&gt;
    &lt;p&gt;The sweeping study includes focus groups and interviews with K-12 students, parents, educators and tech experts in 50 countries, as well as a literature review of hundreds of research articles. It found that using AI in education can "undermine children's foundational development" and that "the damages it has already caused are daunting," though "fixable."&lt;/p&gt;
    &lt;p&gt;Because generative AI is still young — ChatGPT was released just over three years ago — the report's authors dubbed their review a "premortem" intended to study AI's potential in the classroom without a postmortem's benefits of time, long-term data or hindsight.&lt;/p&gt;
    &lt;p&gt;Here are some of the pros and cons that the report lays out, along with a sampling of the study's recommendations for teachers, parents, school leaders and government officials:&lt;/p&gt;
    &lt;head rend="h3"&gt;Pro: AI can help students learn to read and write&lt;/head&gt;
    &lt;p&gt;Teachers surveyed for the report said AI can be useful when it comes to language acquisition, especially for students learning a second language. For example, AI can adjust the complexity of a passage depending on the reader's skill, and it offers privacy for students who struggle in large-group settings.&lt;/p&gt;
    &lt;p&gt;Teachers reported that AI can also help improve students' writing, so long as it is used to support students' efforts and not to do the work for them: "Teachers report that AI can 'spark creativity' and help students overcome writer's block. … At the drafting stage, it can help with organization, coherence, syntax, semantics, and grammar. At the revision stage, AI can support the editing and rewriting of ideas as well as help with … punctuation, capitalization, and grammar."&lt;/p&gt;
    &lt;p&gt;But, if there is a refrain in the report, it is this: AI is most useful when it's supplementing, not replacing, the efforts of a flesh-and-blood teacher.&lt;/p&gt;
    &lt;head rend="h3"&gt;Con: AI poses a grave threat to students' cognitive development&lt;/head&gt;
    &lt;p&gt;At the top of Brookings' list of risks is the negative effect AI can have on children's cognitive growth — how they learn new skills and perceive and solve problems.&lt;/p&gt;
    &lt;p&gt;The report describes a kind of doom loop of AI dependence, where students increasingly off-load their own thinking onto the technology, leading to the kind of cognitive decline or atrophy more commonly associated with aging brains.&lt;/p&gt;
    &lt;p&gt;Rebecca Winthrop, one of the report's authors and a senior fellow at Brookings, warns, "When kids use generative AI that tells them what the answer is … they are not thinking for themselves. They're not learning to parse truth from fiction. They're not learning to understand what makes a good argument. They're not learning about different perspectives in the world because they're actually not engaging in the material."&lt;/p&gt;
    &lt;p&gt;Cognitive off-loading isn't new. The report points out that keyboards and computers reduced the need for handwriting, and calculators automated basic math. But AI has "turbocharged" this kind of off-loading, especially in schools where learning can feel transactional.&lt;/p&gt;
    &lt;p&gt;As one student told the researchers, "It's easy. You don't need to (use) your brain."&lt;/p&gt;
    &lt;p&gt;The report offers a surfeit of evidence to suggest that students who use generative AI are already seeing declines in content knowledge, critical thinking and even creativity. And this could have enormous consequences if these young people grow into adults without learning to think critically.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pro: AI can make teachers' jobs a little easier&lt;/head&gt;
    &lt;p&gt;The report says another benefit of AI is that it allows teachers to automate some tasks: "generating parent emails … translating materials, creating worksheets, rubrics, quizzes, and lesson plans" — and more.&lt;/p&gt;
    &lt;p&gt;The report cites multiple research studies that found important time-saving benefits for teachers, including one U.S. study that found that teachers who use AI save an average of nearly six hours a week and about six weeks over the course of a full school year.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pro/Con: AI can be an engine of equity — or inequity&lt;/head&gt;
    &lt;p&gt;One of the strongest arguments in favor of AI's educational use, according to the Brookings report, is its ability to reach children who have been excluded from the classroom. The researchers cite Afghanistan, where girls and women have been denied access to formal, postprimary education by the Taliban.&lt;/p&gt;
    &lt;p&gt;According to the report, one program for Afghan girls "has employed AI to digitize the Afghan curriculum, create lessons based on this curriculum, and disseminate content in Dari, Pashto, and English via WhatsApp lessons."&lt;/p&gt;
    &lt;p&gt;AI can also help make classrooms more accessible for students with a wide range of learning disabilities, including dyslexia.&lt;/p&gt;
    &lt;p&gt;But "AI can massively increase existing divides" too, Winthrop warns. That's because the free AI tools that are most accessible to students and schools can also be the least reliable and least factually accurate.&lt;/p&gt;
    &lt;p&gt;"We know that richer communities and schools will be able to afford more advanced AI models," Winthrop says, "and we know those more advanced AI models are more accurate. Which means that this is the first time in ed-tech history that schools will have to pay more for more accurate information. And that really hurts schools without a lot of resources."&lt;/p&gt;
    &lt;head rend="h3"&gt;Con: AI poses serious threats to social and emotional development&lt;/head&gt;
    &lt;p&gt;Survey responses revealed deep concern that use of AI, particularly chatbots, "is undermining students' emotional well-being, including their ability to form relationships, recover from setbacks, and maintain mental health," the report says.&lt;/p&gt;
    &lt;p&gt;One of the many problems with kids' overuse of AI is that the technology is inherently sycophantic — it has been designed to reinforce users' beliefs.&lt;/p&gt;
    &lt;p&gt;Winthrop says that if children are building social-emotional skills largely through interactions with chatbots that were designed to agree with them, "it becomes very uncomfortable to then be in an environment when somebody doesn't agree with you."&lt;/p&gt;
    &lt;p&gt;Winthrop offers an example of a child interacting with a chatbot, "complaining about your parents and saying, 'They want me to wash the dishes — this is so annoying. I hate my parents.' The chatbot will likely say, 'You're right. You're misunderstood. I'm so sorry. I understand you.' Versus a friend who would say, 'Dude, I wash the dishes all the time in my house. I don't know what you're complaining about. That's normal.' That right there is the problem."&lt;/p&gt;
    &lt;p&gt;A recent survey from the Center for Democracy and Technology, a nonprofit that advocates for civil rights and civil liberties in the digital age, found that nearly 1 in 5 high schoolers said they or someone they know has had a romantic relationship with artificial intelligence. And 42% of students in that survey said they or someone they know has used AI for companionship.&lt;/p&gt;
    &lt;p&gt;The report warns that AI's echo chamber can stunt a child's emotional growth: "We learn empathy not when we are perfectly understood, but when we misunderstand and recover," one of the surveyed experts said.&lt;/p&gt;
    &lt;head rend="h3"&gt;What to do about it&lt;/head&gt;
    &lt;p&gt;The Brookings report offers a long list of recommendations to help parents, teachers and policymakers — not to mention tech companies themselves — harness the good of AI without subjecting children to the risks that the technology currently poses. Among those recommendations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Schooling itself could be less focused on what the report calls "transactional task completion" or a grade-based endgame and more focused on fostering curiosity and a desire to learn. Students will be less inclined to ask AI to do the work for them if they feel engaged by that work.&lt;/item&gt;
      &lt;item&gt;AI designed for use by children and teens should be less sycophantic and more "antagonistic," pushing back against preconceived notions and challenging users to reflect and evaluate.&lt;/item&gt;
      &lt;item&gt;Tech companies could collaborate with educators in "co-design hubs." In the Netherlands, a government-backed hub already brings together tech companies and educators to develop, test and evaluate new AI applications in the classroom.&lt;/item&gt;
      &lt;item&gt;Holistic AI literacy is crucial — both for teachers and students. Some countries, including China and Estonia, have comprehensive, national AI literacy guidelines.&lt;/item&gt;
      &lt;item&gt;As schools continue to embrace AI, it's important that underfunded districts in marginalized communities are not left behind, allowing AI to further drive inequity.&lt;/item&gt;
      &lt;item&gt;Governments have a responsibility to regulate the use of AI in schools, making sure that the technology being used protects students' cognitive and emotional health, as well as their privacy. In the U.S., the Trump administration has tried to prohibit states from regulating AI on their own, even as Congress has so far failed to create a federal regulatory framework.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With this "premortem," the authors argue, the time to act is now. AI's risks to children and teens are already abundant and obvious. The good news is: so are many of the remedies.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.npr.org/2026/01/14/nx-s1-5674741/ai-schools-education"/><published>2026-01-17T12:59:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657729</id><title>Whistleblower drops 'largest ever' ICE leak to unmask agents</title><updated>2026-01-17T14:10:01.999994+00:00</updated><content>&lt;doc fingerprint="150d959748c410db"&gt;
  &lt;main&gt;
    &lt;p&gt;A Department of Homeland Security whistleblower has released the identities of about 4,500 ICE and Border Patrol employees Tuesday in what has been called potentially the largest agency data breach for the department.&lt;/p&gt;
    &lt;p&gt;The killing of 37-year-old mother Renee Nicole Good by ICE agent Jonathan Ross last week in Minneapolis has prompted the leak of personnel date to the ICE List, an online database created to promote accountability by the masked federal agents, according to a report from The Daily Beast.&lt;/p&gt;
    &lt;p&gt;The onslaught of national outrage reportedly led to the major leak.&lt;/p&gt;
    &lt;p&gt;“It is a sign that people aren’t happy within the U.S. government, clearly. The shooting [of Good] was the last straw for many people,” Dominick Skinner, ICE list founder, told The Beast.&lt;/p&gt;
    &lt;p&gt;Information included in the new leak includes around “1,800 on-the-ground agents and 150 supervisors. Early analysis by the organization suggests that around 80 per cent of the staff identified remain employed by DHS,” according to The Beast.&lt;/p&gt;
    &lt;p&gt;The first batch of names was slated to be posted on the site on Tuesday night.&lt;/p&gt;
    &lt;p&gt;Members of the public have reportedly also shared information about agents in hotels and even neighbors.&lt;/p&gt;
    &lt;p&gt;“I’ve had hotel staff sending post-it notes, bar staff sending DHS IDs, and loads of people saying their neighbour is an agent,” Skinner said.&lt;/p&gt;
    &lt;p&gt;Skinner told the outlet he was working to verify the names.&lt;/p&gt;
    &lt;p&gt;“ICE and CBP are in clear need of reform, and I believe working for either is a bad move on a moral level,” he added.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vechron.com/2026/01/whistleblower-drops-largest-ever-ice-leak-to-unmask-agents/"/><published>2026-01-17T13:02:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46658014</id><title>Italy investigates Activision Blizzard for pushing in-game purchases</title><updated>2026-01-17T14:10:01.856217+00:00</updated><content>&lt;doc fingerprint="adfc8d79ca9ab8b1"&gt;
  &lt;main&gt;
    &lt;p&gt;Italy has launched two investigations into Microsoft’s Activision Blizzard, alleging the company has engaged in “misleading and aggressive” sales practices for its popular smartphone games Diablo Immortal and Call of Duty Mobile.&lt;/p&gt;
    &lt;p&gt;The country’s competition regulator, Autorità Garante della Concorrenza E Del Mercato (AGCM), said the investigations focus on the use of design elements to induce users, particularly children, into playing for long periods, and make in-game purchases by urging them to not miss out on rewards.&lt;/p&gt;
    &lt;p&gt;“These practices, together with strategies that make it difficult for users to understand the real value of the virtual currency used in the game and the sale of in-game currency in bundles, may influence players as consumers — including minors — leading them to spend significant amounts, sometimes exceeding what is necessary to progress in the game and without being fully aware of the expenditure involved,” the AGCM wrote in a statement.&lt;/p&gt;
    &lt;p&gt;The AGCM said the games are advertised as free-to-play but offer in-game purchases.&lt;/p&gt;
    &lt;p&gt;That isn’t particularly surprising, however, as, unlike full-priced games, free-to-play games have long relied on loot boxes and sales of in-game cosmetics for monetization. Diablo Immortal, for example, offers in-game cosmetics, as well as currency that allows players to accelerate their progression and gain items for crafting, for as much as $200.&lt;/p&gt;
    &lt;p&gt;Given the nature of the game, it’s not unusual for many users to repeatedly spend on such items in the course of play.&lt;/p&gt;
    &lt;p&gt;Both Diablo Immortal and Call of Duty Mobile have player bases in the hundreds of thousands.&lt;/p&gt;
    &lt;p&gt;The authority is also looking into the games’ parental control features, as the default settings lets minors make in-game purchases, play for long periods without restraints, and allow them to chat with others in-game. The AGCM also highlighted privacy concerns, as the games appear to lead users to select all consent options when signing up, and said it would look into the company’s consent process for harvesting and using personal data.&lt;/p&gt;
    &lt;p&gt;“In the Authority’s view, the company may be acting in breach of consumer protection rules and, in particular, the duty of professional diligence required in a sector that is particularly sensitive to the risks of gaming-related addiction,” the regulator said.&lt;/p&gt;
    &lt;p&gt;Activision Blizzard did not immediately respond to a request for comment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://techcrunch.com/2026/01/16/italy-investigates-activision-blizzard-for-pushing-in-game-purchases/"/><published>2026-01-17T13:44:02+00:00</published></entry></feed>