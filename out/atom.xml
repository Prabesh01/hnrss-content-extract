<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-29T17:10:50.697663+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45745566</id><title>Show HN: Learn German with Games</title><updated>2025-10-29T17:10:58.117922+00:00</updated><content>&lt;doc fingerprint="2ce7b8cb946d6548"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Choose Your Learning Adventure&lt;/head&gt;
    &lt;p&gt;Select a game below and start mastering German in an engaging, interactive way!&lt;/p&gt;
    &lt;head rend="h3"&gt;Numbers to Words Game&lt;/head&gt;
    &lt;p&gt;See a number and type the German word - perfect for learning German number vocabulary&lt;/p&gt;
    &lt;head rend="h3"&gt;Words to Numbers Game&lt;/head&gt;
    &lt;p&gt;Practice recognizing German number words and converting them to digits&lt;/p&gt;
    &lt;head rend="h3"&gt;German Time Game&lt;/head&gt;
    &lt;p&gt;Learn to tell time in German by reading analog clocks and typing time expressions&lt;/p&gt;
    &lt;head rend="h3"&gt;Time Short Form Game&lt;/head&gt;
    &lt;p&gt;Practice German time with short forms: nach, vor, halb, viertel, and punkt&lt;/p&gt;
    &lt;head rend="h3"&gt;Guess the Artikel&lt;/head&gt;
    &lt;p&gt;Master German artikels (der, die, das) by guessing the correct artikel for each noun&lt;/p&gt;
    &lt;head rend="h3"&gt;Guess the Word&lt;/head&gt;
    &lt;p&gt;Translate German nouns to English - see a German word with its article and type the English meaning&lt;/p&gt;
    &lt;head rend="h3"&gt;English Nouns to German&lt;/head&gt;
    &lt;p&gt;See an English word and type the German translation with its artikel&lt;/p&gt;
    &lt;head rend="h3"&gt;Verb Conjugation&lt;/head&gt;
    &lt;p&gt;Practice conjugating German verbs in present tense for all persons - ich, du, er/sie/es, wir, ihr, sie/Sie&lt;/p&gt;
    &lt;head rend="h3"&gt;German Verbs to English&lt;/head&gt;
    &lt;p&gt;See a German verb and type its English meaning - perfect for building vocabulary&lt;/p&gt;
    &lt;head rend="h3"&gt;English Verbs to German&lt;/head&gt;
    &lt;p&gt;See an English verb meaning and type the German infinitive form - reverse translation practice&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.learngermanwithgames.com/"/><published>2025-10-29T11:50:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45745995</id><title>Berkeley Out-of-Order RISC-V Processor (Boom) (2020)</title><updated>2025-10-29T17:10:57.753544+00:00</updated><content>&lt;doc fingerprint="f35afe91c75d4326"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Berkeley Out-of-Order Machine (BOOM)Â¶&lt;/head&gt;
    &lt;p&gt;The Berkeley Out-of-Order Machine (BOOM) is heavily inspired by the MIPS R10000 [1] and the Alpha 21264 [2] outâofâorder processors. Like the MIPS R10000 and the Alpha 21264, BOOM is a unified physical register file design (also known as âexplicit register renamingâ).&lt;/p&gt;
    &lt;p&gt;BOOM implements the open-source RISC-V ISA and utilizes the Chisel hardware construction language to construct generator for the core. A generator can be thought of a generialized RTL design. A standard RTL design can be viewed as a single instance of a generator design. Thus, BOOM is a family of out-of-order designs rather than a single instance of a core. Additionally, to build an SoC with a BOOM core, BOOM utilizes the Rocket Chip SoC generator as a library to reuse different micro-architecture structures (TLBs, PTWs, etc).&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[1]&lt;/cell&gt;
        &lt;cell&gt;Yeager, Kenneth C. âThe MIPS R10000 superscalar microprocessor.â IEEE micro 16.2 (1996): 28-41.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[2]&lt;/cell&gt;
        &lt;cell&gt;Kessler, Richard E. âThe alpha 21264 microprocessor.â IEEE micro 19.2 (1999): 24-36.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://docs.boom-core.org/en/latest/sections/intro-overview/boom.html"/><published>2025-10-29T12:33:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45746478</id><title>From VS Code to Helix</title><updated>2025-10-29T17:10:57.394154+00:00</updated><content>&lt;doc fingerprint="7708325733a03a9d"&gt;
  &lt;main&gt;
    &lt;p&gt;I created the website you’re reading with VS Code. Behind the scenes I use Astro, a static site generator that gets out of the way while providing nice conveniences.&lt;/p&gt;
    &lt;p&gt;Using VS Code was a no-brainer: everyone in the industry seems to at least be familiar with it, every project can be opened with it, and most projects can get enhancements and syntactic helpers in a few clicks. In short: VS Code is free, easy to use, and widely adopted.&lt;/p&gt;
    &lt;p&gt;A Rustacean colleague kept singing Helix’s praises. I discarded it because he’s much smarter than I am, and I only ever use vim when I need to fiddle with files on a server. I like when things “Just Work” and didn’t want to bother learning how to use Helix nor how to configure it.&lt;/p&gt;
    &lt;p&gt;Today it has become my daily driver. Why did I change my mind? What was preventing me from using it before? And how difficult was it to get there?&lt;/p&gt;
    &lt;head rend="h2"&gt;Automation is a double-edged sword&lt;/head&gt;
    &lt;p&gt;Automation and technology make work easier, this is why we produce technology in the first place. But it also means you grow more dependent on the tech you use. If the tech is produced transparently by an international team or a team you trust, it’s fine. But if it’s produced by a single large entity that can screw you over, it’s dangerous.&lt;/p&gt;
    &lt;p&gt;VS Code might be open source, but in practice it’s produced by Microsoft. Microsoft has a problematic relationship to consent and is shoving AI products down everyone’s throat. I’d rather use tools that respect me and my decisions, and I’d rather not get my tools produced by already monopolistic organizations.&lt;/p&gt;
    &lt;p&gt;Microsoft is also based in the USA, and the political climate over there makes me want to depend as little as possible on American tools. I know that’s a long, uphill battle, but we have to start somewhere.&lt;/p&gt;
    &lt;p&gt;I’m not advocating for a ban against American tech in general, but for more balance in our supply chain. I’m also not advocating for European tech either: I’d rather get open source tools from international teams competing in a race to the top, rather than from teams in a single jurisdiction. What is happening in the USA could happen in Europe too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why I feared using Helix&lt;/head&gt;
    &lt;p&gt;I’ve never found vim particularly pleasant to use but it’s everywhere, so I figured I might just get used to it. But one of the things I never liked about vim is the number of moving pieces. By default, vim and neovim are very bare bones. They can be extended and completely modified with plugins, but I really don’t like the idea of having extremely customize tools.&lt;/p&gt;
    &lt;p&gt;I’d rather have the same editor as everyone else, with a few knobs for minor preferences. I am subject to choice paralysis, so making me configure an editor before I’ve even started editing is the best way to tank my productivity.&lt;/p&gt;
    &lt;p&gt;When my colleague told me about Helix, two things struck me as improvements over vim.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Helix’s philosophy is that everything should work out of the box. There are a few configs and themes, but everything should work similarly from one Helix to another. All the language-specific logic is handled in Language Servers that implement the Language Server Protocol standard.&lt;/item&gt;
      &lt;item&gt;In Helix, first you select text, and then you perform operations onto it. So you can visually tell what is going to be changed before you apply the change. It fits my mental model much better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But there are major drawbacks to Helix too:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;After decades of vim, I was scared to re-learn everything. In practice this wasn’t a problem at all because of the very visual way Helix works.&lt;/item&gt;
      &lt;item&gt;VS Code “Just Works”, and Helix sounded like more work than the few clicks from VS Code’s extension store. This is true, but not as bad as I had anticipated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After a single week of usage, Helix was already very comfortable to navigate. After a few weeks, most of the wrinkles have been ironed out and I use it as my primary editor. So how did I overcome those fears?&lt;/p&gt;
    &lt;head rend="h2"&gt;What Helped&lt;/head&gt;
    &lt;head rend="h3"&gt;Just Do It&lt;/head&gt;
    &lt;p&gt;I tried Helix. It can sound silly, but the very first step to get into Helix was not to overthink it. I just installed it on my mac with &lt;code&gt;brew install helix&lt;/code&gt; and gave it a go. I was not too familiar with it, so I looked up the official documentation and noticed there was a tutorial.&lt;/p&gt;
    &lt;p&gt;This tutorial alone is what convinced me to try harder. It’s an interactive and well written way to learn how to move and perform basic operations in Helix. I quickly learned how to move around, select things, surround them with braces or parenthesis. I could see what I was about to do before doing it. This has been epiphany. Helix just worked the way I wanted.&lt;/p&gt;
    &lt;p&gt;Better: I could get things done faster than in VS Code after a few minutes of learning. Being a lazy person, I never bothered looking up VS Code shortcuts. Because the learning curve for Helix is slightly steeper, you have to learn those shortcuts that make moving around feel so easy.&lt;/p&gt;
    &lt;p&gt;Not only did I quickly get used to Helix key bindings: my vim muscle-memory didn’t get in the way at all!&lt;/p&gt;
    &lt;head rend="h3"&gt;Better docs&lt;/head&gt;
    &lt;p&gt;The built-in tutorial is a very pragmatic way to get started. You get results fast, you learn hands on, and it’s not that long. But if you want to go further, you have to look for docs. Helix has officials docs. They seem to be fairly complete, but they’re also impenetrable as a new user. They focus on what the editor supports and not on what I will want to do with it.&lt;/p&gt;
    &lt;p&gt;After a bit of browsing online, I’ve stumbled upon this third-party documentation website. The domain didn’t inspire me a lot of confidence, but the docs are really good. They are clearly laid out, use-case oriented, and they make the most of Astro Starlight to provide a great reading experience. The author tried to upstream these docs, but that won’t happen. It looks like they are upstreaming their docs to the current website. I hope this will improve the quality of upstream docs eventually.&lt;/p&gt;
    &lt;p&gt;After learning the basics and finding my way through the docs, it was time to ensure Helix was set up to help me where I needed it most.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting the most of Markdown and Astro in Helix&lt;/head&gt;
    &lt;p&gt;In my free time, I mostly use my editor for three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write notes in markdown&lt;/item&gt;
      &lt;item&gt;Tweak my website with Astro&lt;/item&gt;
      &lt;item&gt;Edit yaml to faff around my Kubernetes cluster&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Helix is a “stupid” text editor. It doesn’t know much about what you’re typing. But it supports Language Servers that implement the Language Server Protocol. Language Servers understand the document you’re editing. They explain to Helix what you’re editing, whether you’re in a TypeScript function, typing a markdown link, etc. With that information, Helix and the Language Server can provide code completion hints, errors &amp;amp; warnings, and easier navigation in your code.&lt;/p&gt;
    &lt;p&gt;In addition to Language Servers, Helix also supports plugging code formatters. Those are pieces of software that will read the document and ensure that it is consistently formatted. It will check that all indentations use spaces and not tabs, that there is a consistent number of space when indenting, that brackets are on the same line as the function, etc. In short: it will make the code pretty.&lt;/p&gt;
    &lt;head rend="h3"&gt;Markdown&lt;/head&gt;
    &lt;p&gt;Markdown is not really a programming language, so it might seem surprising to configure a Language Server for it. But if you remember what we said earlier, Language Servers can provide code completion, which is useful when creating links for example. Marksman does exactly that!&lt;/p&gt;
    &lt;p&gt;Since Helix is pre-configured to use marksman for markdown files we only need to install marksman and make sure it’s in our &lt;code&gt;PATH&lt;/code&gt;. Installing it with homebrew is enough.&lt;/p&gt;
    &lt;p&gt;We can check that Helix is happy with it with the following command&lt;/p&gt;
    &lt;p&gt;But Language Servers can also help Helix display errors and warnings, and “code suggestions” to help fix the issues. It means Language Servers are a perfect fit for… grammar checkers! Several grammar checkers exist. The most notable are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LTEX+, the Language Server used by Language Tool. It supports several languages must is quite resource hungry.&lt;/item&gt;
      &lt;item&gt;Harper, a grammar checker Language Server developed by Automattic, the people behind WordPress, Tumblr, WooCommerce, Beeper and more. Harper only support English and its variants, but they intend to support more languages in the future.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I mostly write in English and want to keep a minimalistic setup. Automattic is well funded, and I’m confident they will keep working on Harper to improve it. Since grammar checker LSPs can easily be changed, I’ve decided to go with Harper for now.&lt;/p&gt;
    &lt;p&gt;To install it, homebrew does the job as always:&lt;/p&gt;
    &lt;p&gt;Then I edited my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; to add Harper as a secondary Language Server in addition to marksman&lt;/p&gt;
    &lt;p&gt;Finally I can add a markdown linter to ensure my markdown is formatted properly. Several options exist, and markdownlint is one of the most popular. My colleagues recommended the new kid on the block, a Blazing Fast equivalent: rumdl.&lt;/p&gt;
    &lt;p&gt;Installing rumdl was pretty simple on my mac. I only had to add the repository of the maintainer, and install rumdl from it.&lt;/p&gt;
    &lt;p&gt;After that I added a new &lt;code&gt;language-server&lt;/code&gt; to my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; and added it to the language servers to use for the markdown &lt;code&gt;language&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Since my website already contained a &lt;code&gt;.markdownlint.yaml&lt;/code&gt; I could import it to the rumdl format with&lt;/p&gt;
    &lt;p&gt;You might have noticed that I’ve added a little quality of life improvement: soft-wrap at 80 characters.&lt;/p&gt;
    &lt;p&gt;Now if you add this to your own &lt;code&gt;config.toml&lt;/code&gt; you will notice that the text is completely left aligned. This is not a problem on small screens, but it rapidly gets annoying on wider screens.&lt;/p&gt;
    &lt;p&gt;Helix doesn’t support centering the editor. There is a PR tackling the problem but it has been stale for most of the year. The maintainers are overwhelmed by the number of PRs making it their way, and it’s not clear if or when this PR will be merged.&lt;/p&gt;
    &lt;p&gt;In the meantime, a workaround exists, with a few caveats. It is possible to add spaces to the left gutter (the column with the line numbers) so it pushes the content towards the center of the screen.&lt;/p&gt;
    &lt;p&gt;To figure out how many spaces are needed, you need to get your terminal width with &lt;code&gt;stty&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;In my case, when in full screen, my terminal is 243 characters wide. I need to remove the content column with from it, and divide everything by 2 to get the space needed on each side. In my case for a 243 character wide terminal with a text width of 80 characters:&lt;/p&gt;
    &lt;p&gt;As is, I would add 203 spaces to my left gutter to push the rest of the gutter and the content to the right. But the gutter itself has a width of 4 characters, that I need to remove from the total. So I need to subtract them from the total, which leaves me with &lt;code&gt;76&lt;/code&gt; characters to add.&lt;/p&gt;
    &lt;p&gt;I can open my &lt;code&gt;~/.config/helix/config.toml&lt;/code&gt; to add a new key binding that will automatically add or remove those spaces from the left gutter when needed, to shift the content towards the center.&lt;/p&gt;
    &lt;p&gt;Now when in normal mode, pressing Space then t then z will add/remove the spaces. Of course this workaround only works when the terminal runs in full screen mode.&lt;/p&gt;
    &lt;head rend="h3"&gt;Astro&lt;/head&gt;
    &lt;p&gt;Astro works like a charm in VS Code. The team behind it provides a Language Server and a TypeScript plugin to enable code completion and syntax highlighting.&lt;/p&gt;
    &lt;p&gt;I only had to install those globally with&lt;/p&gt;
    &lt;p&gt;Now we need to add a few lines to our &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; to tell it how to use the language server&lt;/p&gt;
    &lt;p&gt;We can check that the Astro Language Server can be used by helix with&lt;/p&gt;
    &lt;p&gt;I also like to get a formatter to automatically make my code consistent and pretty for me when I save a file. One of the most popular code formaters out there is Prettier. I’ve decided to go with the fast and easy formatter dprint instead.&lt;/p&gt;
    &lt;p&gt;I installed it with&lt;/p&gt;
    &lt;p&gt;Then in the projects I want to use dprint in, I do&lt;/p&gt;
    &lt;p&gt;I might edit the &lt;code&gt;dprint.json&lt;/code&gt; file to my liking. Finally, I configure Helix to use dprint globally for all Astro projects by appending a few lines in my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;One final check, and I can see that Helix is ready to use the formatter as well&lt;/p&gt;
    &lt;head rend="h3"&gt;YAML&lt;/head&gt;
    &lt;p&gt;For yaml, it’s simple and straightforward: Helix is preconfigured to use &lt;code&gt;yaml-language-server&lt;/code&gt; as soon as it’s in the PATH. I just need to install it with&lt;/p&gt;
    &lt;head rend="h2"&gt;Is it worth it?&lt;/head&gt;
    &lt;p&gt;Helix really grew on me. I find it particularly easy and fast to edit code with it. It takes a tiny bit more work to get the language support than it does in VS Code, but it’s nothing insurmountable. There is a slightly steeper learning curve than for VS Code, but I consider it to be a good thing. It forced me to learn how to move around and edit efficiently, because there is no way to do it inefficiently. Helix remains intuitive once you’ve learned the basics.&lt;/p&gt;
    &lt;p&gt;I am a GNOME enthusiast, and I adhere to the same principles: I like when my apps work out of the box, and when I have little to do to configure them. This is a strong stance that often attracts a vocal opposition. I like products that follow those principles better than those who don’t.&lt;/p&gt;
    &lt;p&gt;With that said, Helix sometimes feels like it is maintained by one or two people who have a strong vision, but who struggle to onboard more maintainers. As of writing, Helix has more than 350 PRs open. Quite a few bring interesting features, but the maintainers don’t have enough time to review them.&lt;/p&gt;
    &lt;p&gt;Those 350 PRs mean there is a lot of energy and goodwill around the project. People are willing to contribute. Right now, all that energy is gated, resulting in frustration both from the contributors who feel like they’re working in the void, and the maintainers who feel like there at the receiving end of a fire hose.&lt;/p&gt;
    &lt;p&gt;A solution to make everyone happier without sacrificing the quality of the project would be to work on a Contributor Ladder. CHAOSS’ Dr Dawn Foster published a blog post about it, listing interesting resources at the end.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ergaster.org/posts/2025/10/29-vscode-to-helix/"/><published>2025-10-29T13:19:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45746726</id><title>Recreating a Homebrew Game System from 1987</title><updated>2025-10-29T17:10:57.326127+00:00</updated><content>&lt;doc fingerprint="345a297f4904c8c5"&gt;
  &lt;main&gt;
    &lt;p&gt;The specifications are as follows:&lt;/p&gt;
    &lt;p&gt;Controller input and audio output are handled by either an Intel 8255 or Zilog Z80PIO I/O controller. There are two sockets on the PCB for either controller, depending on which is easier for you to obtain. These two ICs have to be controlled slightly differently by software, but it's possible to write games that are compatible with both, as demonstrated by the games written by Inufuto.&lt;/p&gt;
    &lt;p&gt;The controller interface is designed for two-button Sega Master System controllers and will also work with Mega Drive/Genesis controllers. Standard one-button joysticks will also work, aside from the lack of a second button.&lt;/p&gt;
    &lt;p&gt;The composite sync signal is generated with an EPROM, an unconventional method of simplifying the circuitry. Different ROMs have different data access times, so you may need to experiment with one or two models of ROM before you'll find one that produces a glitchless video signal, due to the high speed at which the raster generator steps through the ROM's address bus.&lt;/p&gt;
    &lt;p&gt;Fortunately, any size of ROM between 4KB (2732) and 64KB (27512) can be used, so long as the 4KB binary data file (available for download further down this page) is written to the upper 4KB of higher capacity ROMs. During testing, I found that a 150ns ROM worked well, while a 450ns ROM was too slow.&lt;/p&gt;
    &lt;p&gt;If the prospect of making a lot of cartridges doesn't appeal to you, I've designed a multi-cartridge that holds sixteen 32KB games on one 27C040 ROM. Game selection on the multi-cartridge is performed with DIP switches.&lt;/p&gt;
    &lt;p&gt;The maximum file size for games is 32KB, but I've designed an experimental bank-switching cartridge PCB (not tested yet!) that should allow games of up to 256KB to be accessed through two configurable 16KB page registers on the cartridge.&lt;/p&gt;
    &lt;p&gt;8255:&lt;/p&gt;
    &lt;p&gt;There's a selection of tools available for programming the Z80 TV Game in C:&lt;/p&gt;
    &lt;p&gt; Schematic - Console &lt;lb/&gt; PDF document, 941 KB &lt;/p&gt;
    &lt;p&gt; PCB Gerbers - Console &lt;lb/&gt; ZIP archive, 744 KB &lt;/p&gt;
    &lt;p&gt; KiCad Files - Console &lt;lb/&gt; ZIP archive, 1.33 MB - Useful if you want to make modifications to the PCB. Made with KiCad 9. &lt;/p&gt;
    &lt;p&gt; Interactive Bill of Materials - 32KB ROM Cartridge &lt;lb/&gt; HTML document, 338 KB &lt;/p&gt;
    &lt;p&gt; Schematic - 32KB ROM Cartridge &lt;lb/&gt; PDF document, 127 KB &lt;/p&gt;
    &lt;p&gt; PCB Gerbers - 32KB ROM Cartridge &lt;lb/&gt; ZIP archive, 170 KB &lt;/p&gt;
    &lt;p&gt; KiCad Files - 32KB ROM Cartridge &lt;lb/&gt; ZIP archive, 532 KB - Useful if you want to make modifications to the PCB. Made with KiCad 9. &lt;/p&gt;
    &lt;p&gt; Interactive Bill of Materials - 32KB x 16 Multi-Cartridge &lt;lb/&gt; HTML document, 350 KB &lt;/p&gt;
    &lt;p&gt; Schematic - 32KB x 16 Multi-Cartridge &lt;lb/&gt; PDF document, 151 KB &lt;/p&gt;
    &lt;p&gt; PCB Gerbers - 32KB x 16 Multi-Cartridge &lt;lb/&gt; ZIP archive, 191 KB &lt;/p&gt;
    &lt;p&gt; KiCad Files - 32KB x 16 Multi-Cartridge &lt;lb/&gt; ZIP archive, 564 KB - Useful if you want to make modifications to the PCB. Made with KiCad 9. &lt;/p&gt;
    &lt;p&gt; Interactive Bill of Materials - Experimental 256KB ROM Cartridge &lt;lb/&gt; HTML document, 316 KB &lt;/p&gt;
    &lt;p&gt; Schematic - Experimental 256KB ROM Cartridge &lt;lb/&gt; PDF document, 221 KB &lt;/p&gt;
    &lt;p&gt; PCB Gerbers - Experimental 256KB ROM Cartridge &lt;lb/&gt; ZIP archive, 209 KB - Please note that the 256KB cartridge hasn't yet been tested! &lt;/p&gt;
    &lt;p&gt; KiCad Files - Experimental 256KB ROM Cartridge &lt;lb/&gt; ZIP archive, 603 KB - Useful if you want to make modifications to the PCB. Made with KiCad 9. &lt;/p&gt;
    &lt;p&gt; Custom Fonts &lt;lb/&gt; ZIP archive, 8.90 MB - Custom fonts used for the KiCad files. Only needed if you want to modify these files. &lt;/p&gt;
    &lt;p&gt; Original Schematics &lt;lb/&gt; ZIP archive, 1.14 MB - Mr. Isizu's original schematics for the Z80 TV Game, with the 74LS122 timing circuit corrected. Includes the 1980's hand-drawn schematic, which has a different memory map to the 2000's CAD schematic that this PCB, emulators, C devtools, etc. are based on. &lt;/p&gt;
    &lt;p&gt; Game ROMs &lt;lb/&gt; ZIP archive, 922 KB - All the games I know to exist for the Z80 TV Game thus far. Includes two combined ROMs for those who would rather have all 26 games on 2 multi-cartridges. If you know of any games that aren't mentioned on this page (or you've written a new game), please let me know! My email address is on the home page. &lt;/p&gt;
    &lt;p&gt; 32KB Cartridge Dimensions &lt;lb/&gt; PDF document, 61.3 KB - Useful for designing a 3D printed cartridge enclosure. Note that the standard PCB thickness used by most manufacturers is 1.6mm. &lt;/p&gt;
    &lt;p&gt; 32KB x 16 Multi-Cartridge Dimensions &lt;lb/&gt; PDF document, 67.1 KB - Useful for designing a 3D printed cartridge enclosure. Note that the standard PCB thickness used by most manufacturers is 1.6mm. &lt;/p&gt;
    &lt;p&gt; Z80 TV Game Logo (1920 x 846) (Variant 1) &lt;lb/&gt; PNG image, 1.21 MB - The logo seen at the top of the page in full resolution. &lt;/p&gt;
    &lt;p&gt; Z80 TV Game Logo (1920 x 846) (Variant 2) &lt;lb/&gt; PNG image, 1.08 MB - The logo seen at the top of the page in full resolution. &lt;/p&gt;
    &lt;p&gt;Inufuto: Developer of Cate, a multi-platform compiler that can generate software for the Z80 TV Game. All 20 of the games he has created with it thus far have Z80 TV Game versions. Inufuto has also designed a PCB version of the Z80 TV Game that outputs VGA video via a Raspberry Pi Pico.&lt;/p&gt;
    &lt;p&gt;Takeda Toshiya: Developer of eZ80TVGAME, a Z80 TV Game emulator for Windows.&lt;/p&gt;
    &lt;p&gt;lsluk: Developer of vdmgr, a multi-platform emulator for Windows that supports the Z80 TV Game.&lt;/p&gt;
    &lt;p&gt;Last updated on Oct 26, 2025. &lt;lb/&gt;This page was first uploaded on Oct 26, 2025. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alex-j-lowry.github.io/z80tvg.html"/><published>2025-10-29T13:42:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747018</id><title>Kafka is Fast – I'll use Postgres</title><updated>2025-10-29T17:10:56.888152+00:00</updated><content>&lt;doc fingerprint="6f5090de6e0009c6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Intro&lt;/head&gt;
    &lt;p&gt;I feel like the tech world lives in two camps.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;One camp chases buzzwords.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This camp tends to adopt whatever’s popular without thinking hard about whether it’s appropriate. They tend to fall for all the purported benefits the sales pitch gives them - real-time, infinitely scale, cutting-edge, cloud-native, serverless, zero-trust, AI-powered, etc.&lt;/p&gt;
    &lt;p&gt;You see this everywhere in the Kafka world: Streaming Lakehouse™️, Kappa™️ Architecture, Streaming AI Agents1.&lt;/p&gt;
    &lt;p&gt;This phenomenon is sometimes known as resume-driven design. Modern practices actively encourage this. Consultants push “innovative architectures” stuffed with vendor tech via “insight” reports2. System design interviews expect you to design Google-scale architectures that are inevitably at a scale 100x higher than the company you’re interviewing for would ever need. Career progression rewards you for replatforming to the Hot New Stack™️, not for being resourceful.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The other camp chases common sense&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This camp is far more pragmatic. They strip away unnecessary complexity and steer clear of overengineered solutions. They reason from first principles before making technology choices. They resist marketing hype and approach vendor claims with healthy skepticism.&lt;/p&gt;
    &lt;p&gt;Historically, it has felt like Camp 1 definitively held the upper hand in sheer numbers and noise. Today, it feels like the pendulum may be beginning to swing back, at least a tiny bit. Two recent trends are on the side of Camp 2:&lt;/p&gt;
    &lt;p&gt;Trend 1 - the “Small Data” movement. People are realizing two things - their data isn’t that big and their computers are becoming big too. You can rent a 128-core, 4 TB of RAM instance from AWS. AMD just released 192-core CPUs this summer. That ought to be enough for anybody.3&lt;/p&gt;
    &lt;p&gt;Trend 2 - the Postgres Renaissance. The space is seeing incredible growth and investment4. In the last 2 years, the phrase “Just Use Postgres (for everything)” has gained a ton of popularity. The basic premise is that you shouldn’t complicate things with new tech when you don’t need to, and that Postgres alone solves most problems pretty well. Postgres competes with purpose-built solutions like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Elasticsearch (functionality supported by Postgres’ &lt;code&gt;tsvector&lt;/code&gt;/&lt;code&gt;tsquery&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;MongoDB (&lt;code&gt;jsonb&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Redis (&lt;code&gt;CREATE UNLOGGED TABLE&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;AI Vector Databases (&lt;code&gt;pgvector&lt;/code&gt;,&lt;code&gt;pgai&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Snowflake (&lt;code&gt;pg_mooncake&lt;/code&gt;,&lt;code&gt;pg_duckdb&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and… Kafka (this blog).&lt;/p&gt;
    &lt;p&gt;The claim isn’t that Postgres is functionally equivalent to any of these specialized systems. The claim is that it handles 80%+ of their use cases with 20% of the development effort. (Pareto Principle)&lt;/p&gt;
    &lt;p&gt;When you combine the two trends, the appeal becomes obvious. Postgres is a battle-tested, well-known system that is simple, scalable and reliable. Pair it with today’s powerful hardware and you quickly begin to realize that, more often than not, you do not need the state-of-the-art highly optimized and complex distributed system in order to handle your organization’s scale.&lt;/p&gt;
    &lt;p&gt;Despite being somebody who is biased towards Kafka, I tend to agree. Kafka is similar to Postgres in that it’s stable, mature, battle-tested and boasts a strong community. It also scales a lot further. Despite that, I don’t think it’s the right choice for a lot of cases. Very often I see it get adopted where it doesn’t make sense.&lt;/p&gt;
    &lt;p&gt;A 500 KB/s workload should not use Kafka. There is a scalability cargo cult in tech that always wants to choose “the best possible” tech for a problem - but this misses the forest for the trees. The “best possible” solution frequently isn’t a technical question - it’s a practical one. Adriano makes an airtight case for why you should opt for simple tech in his PG as Queue blog (2023) that originally inspired me to write this.&lt;/p&gt;
    &lt;p&gt;Enough background. In this article, we will do three simple things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Benchmark how far Postgres can scale for pub/sub messaging - # PG as a Pub/Sub&lt;/item&gt;
      &lt;item&gt;Benchmark how far Postgres can scale for queueing - # PG as a Queue&lt;/item&gt;
      &lt;item&gt;Concisely touch upon when Postgres can be a fit for these use cases - # Should You Use Postgres?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I am not aiming for an exhaustive in-depth evaluation. Benchmarks are messy af. Rather, my goal is to publish some reasonable data points which can start a discussion.&lt;/p&gt;
    &lt;p&gt;(while this article is for Postgres, feel free to replace it with your database of choice)&lt;/p&gt;
    &lt;head rend="h1"&gt;Results TL;DR&lt;/head&gt;
    &lt;p&gt;If you’d like to skip straight to the results, here they are:&lt;/p&gt;
    &lt;head&gt;🔥 The Benchmark Results&lt;/head&gt;
    &lt;head rend="h3"&gt;Pub-Sub Results&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Setup&lt;/cell&gt;
        &lt;cell role="head"&gt;✍️ Write&lt;/cell&gt;
        &lt;cell role="head"&gt;📖 Read&lt;/cell&gt;
        &lt;cell role="head"&gt;🔭 e2e Latency5 (p99)&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1× c7i.xlarge&lt;/cell&gt;
        &lt;cell&gt;4.8 MiB/s&lt;p&gt;5036 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;24.6 MiB/s&lt;p&gt;25 183 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;60 ms&lt;/cell&gt;
        &lt;cell&gt;~60 % CPU; 4 partitions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;3× c7i.xlarge (replicated)&lt;/cell&gt;
        &lt;cell&gt;4.9 MiB/s&lt;p&gt;5015 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;24.5 MiB/s&lt;p&gt;25 073 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;186 ms&lt;/cell&gt;
        &lt;cell&gt;~65 % CPU; cross-AZ RF≈2.5; 4 partitions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1× c7i.24xlarge&lt;/cell&gt;
        &lt;cell&gt;238 MiB/s&lt;p&gt;243,000 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;1.16 GiB/s&lt;p&gt;1,200,000 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;853 ms&lt;/cell&gt;
        &lt;cell&gt;~10 % CPU (idle); 30 partitions&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Queue Results&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Setup&lt;/cell&gt;
        &lt;cell role="head"&gt;📬 Throughput (read + write)&lt;/cell&gt;
        &lt;cell role="head"&gt;🔭 e2e Latency5 (p99)&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1× c7i.xlarge&lt;/cell&gt;
        &lt;cell&gt;2.81 MiB/s&lt;p&gt;2885 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;17.7 ms&lt;/cell&gt;
        &lt;cell&gt;~60 % CPU; read-client bottleneck&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3× c7i.xlarge (replicated)&lt;/cell&gt;
        &lt;cell&gt;2.34 MiB/s&lt;p&gt;2397 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;920 ms ⚠️6&lt;/cell&gt;
        &lt;cell&gt;replication lag inflated E2E latency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1× c7i.24xlarge&lt;/cell&gt;
        &lt;cell&gt;19.7 MiB/s&lt;p&gt;20,144 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;930 ms ⚠️6&lt;/cell&gt;
        &lt;cell&gt;~50 % CPU; single-table bottleneck&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Make sure to at least read the last section of the article where we philosophize - # Should You Use Postgres?&lt;/p&gt;
    &lt;head rend="h1"&gt;PG as a Pub/Sub&lt;/head&gt;
    &lt;p&gt;There are dozens of blogs out there using Postgres as a queue, but interestingly enough I haven’t seen one use it as a pub-sub messaging system.&lt;/p&gt;
    &lt;p&gt;A quick distinction between the two because I often see them get confused:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Queues are meant for point-to-point communication. They’re widely used for asynchronous background jobs: worker apps (clients) process a task in the queue like sending an e-mail or pushing a notification. The event is consumed once and it’s done with. A message is immediately deleted (popped) off the queue once it’s consumed. Queues do not have strict ordering guarantees7.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pub-sub messaging differs from the queue in that it is meant for one-to-many communication. This inherently means there is a large read fanout - more than one reader client is interested in any given message. Good pub-sub systems decouple readers from writers by storing data on disks. This allows them to not impose a max queue depth limit - something in-memory queues need to do in order to prevent them from going OOM.&lt;/p&gt;
        &lt;p&gt;There is also a general expectation that there is strict order - events should be read in the same order that they arrived in the system.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Postgres’ main competitor here is Kafka, which is the standard in pub-sub today. Various (mostly-proprietary) alternatives exist.8&lt;/p&gt;
    &lt;p&gt;Kafka uses the Log data structure to hold messages. You’ll see my benchmark basically reconstructs a log from Postgres primitives.&lt;/p&gt;
    &lt;p&gt;Postgres doesn’t seem to have any popular libraries for pub-sub9 use cases, so I had to write my own. The Kafka-inspired workflow I opted for is this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Writers produce batches of messages per statement10 (&lt;code&gt;INSERT INTO&lt;/code&gt;). Each transaction carries one batch insert and targets a single&lt;code&gt;topicpartition&lt;/code&gt;table11&lt;/item&gt;
      &lt;item&gt;Each writer is sticky to one table, but in aggregate they produce to multiple tables.&lt;/item&gt;
      &lt;item&gt;Each message has a unique monotonically-increasing offset number. A specific row in a special &lt;code&gt;log_counter&lt;/code&gt;table denotes the latest offset for a given&lt;code&gt;topicpartition&lt;/code&gt;table.&lt;/item&gt;
      &lt;item&gt;Write transactions atomically update both the &lt;code&gt;topicpartition&lt;/code&gt;data and the&lt;code&gt;log_counter&lt;/code&gt;row. This ensures consistent offset tracking across concurrent writers.&lt;/item&gt;
      &lt;item&gt;Readers poll for new messages. They consume the &lt;code&gt;topicpartition&lt;/code&gt;table(s) sequentially, starting from the lowest offset and progressively reading up.&lt;/item&gt;
      &lt;item&gt;Readers are split into consumer groups. Each group performs separate, independent reads and makes progress on the &lt;code&gt;topicpartition&lt;/code&gt;tables.&lt;/item&gt;
      &lt;item&gt;Each group contains 1 reader per &lt;code&gt;topicpartition&lt;/code&gt;table.&lt;/item&gt;
      &lt;item&gt;Readers store their progress in a &lt;code&gt;consumer_offsets&lt;/code&gt;table, with a row for each&lt;code&gt;topicpartition,group&lt;/code&gt;pair.&lt;/item&gt;
      &lt;item&gt;Each reader updates the latest processed offset (claiming the records), selects the records and processes them inside a single transaction.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This ensures Kafka-like semantics - gapless, monotonically-increasing offsets and at-least-once/at-most-once processing. This test in particular uses at-least-once semantics, but neither choice should impact the benchmark results.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pub-Sub Setup&lt;/head&gt;
    &lt;head rend="h4"&gt;Table&lt;/head&gt;
    &lt;head rend="h4"&gt;Writes&lt;/head&gt;
    &lt;p&gt;The benchmark runs &lt;code&gt;N&lt;/code&gt; writer goroutines. These represent writer clients.
Each one loops and atomically inserts &lt;code&gt;$BATCH_SIZE&lt;/code&gt; records while updating the latest offset:&lt;/p&gt;
    &lt;head rend="h4"&gt;Reads&lt;/head&gt;
    &lt;p&gt;The benchmark also runs &lt;code&gt;N&lt;/code&gt; reader goroutines. Each reader is assigned a particular consumer group and partition. The group as a whole reads all partitions while each reader in the group reads only one partition at a time.&lt;/p&gt;
    &lt;p&gt;The reader loops, opens a transaction, optimistically claims &lt;code&gt;$BATCH_SIZE&lt;/code&gt; records (by advancing the offset mark beyond them), selects them and processes the records.
If successful, it commits the transaction and through that advances the offset for the group.&lt;/p&gt;
    &lt;p&gt;It is a pull-based read (just like Kafka), rather than push-based. If the reader has no records to poll, it sleeps for a bit.&lt;/p&gt;
    &lt;p&gt;First it opens a transaction:&lt;/p&gt;
    &lt;p&gt;Then it claims the offsets:&lt;/p&gt;
    &lt;p&gt;Followed by selecting the claimed records:&lt;/p&gt;
    &lt;p&gt;Finally, the data gets processed by the business logic (no-op in our benchmark) and the transaction is closed:&lt;/p&gt;
    &lt;p&gt;If you’re wondering “why no &lt;code&gt;NOTIFY/LISTEN&lt;/code&gt;?” - my understanding of that feature is that it’s an optimization and cannot be fully relied upon, so polling is required either way12. Given that, I just copied Kafka’s relatively simple design.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pub-Sub Results&lt;/head&gt;
    &lt;p&gt;The full code and detailed results are all published on GitHub at stanislavkozlovski/pg-queue-pubsub-benchmark. I ran three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The results are the average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.xlarge Postgres server /w 25GB gp3 9000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;mostly default Postgres settings (synchronous commit, fsync); &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row’s payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;4 topicpartition tables&lt;/item&gt;
      &lt;item&gt;10 writers (2 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;20 reader clients total (4 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 100 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 5036 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 4.8 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 38.7ms p99 / 6.2ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 25,183 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 24.6 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 27.3ms p99 (varied 8.9ms-47ms b/w runs); 4.67ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 60ms p99 / 10.6ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~60% CPU;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;disk was at ~1200 writes/s with iostat claiming 46 MiB/s&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are pretty good results. It’s funny to think that the majority of people run a complex distributed system like Kafka for similar workloads13.&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Tri-Node&lt;/head&gt;
    &lt;p&gt;Now, a replicated setup to more accurately mimic the durability and availability guarantees of Kafka.&lt;/p&gt;
    &lt;p&gt;The average of two 5-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3x c7i.xlarge Postgres servers /w 25GB gp3 9000 IOPS EBS volume &lt;list rend="ul"&gt;&lt;item&gt;each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)&lt;/item&gt;&lt;item&gt;one &lt;code&gt;sync&lt;/code&gt;replica and one&lt;code&gt;potential&lt;/code&gt;14 replica&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;a few custom Postgres settings like &lt;code&gt;wal_compression&lt;/code&gt;,&lt;code&gt;max_worker_processes&lt;/code&gt;,&lt;code&gt;max_parallel_workers&lt;/code&gt;,&lt;code&gt;max_parallel_workers_per_gather&lt;/code&gt;and of course -&lt;code&gt;hot_standby&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row’s payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;4 topicpartition tables&lt;/item&gt;
      &lt;item&gt;10 writers (2 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;readers only access the primary DB15; readers are in the same AZ as the primary;&lt;/item&gt;
      &lt;item&gt;20 reader clients total (4 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 100 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 5015 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 4.9 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 153.45ms p99 / 6.8ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 25,073 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 24.5 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 57ms p99; 4.91ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 186ms p99 / 12ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~65% CPU;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;disk was at ~1200 writes/s with iostat claiming 46 MiB/s&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now these are astonishing results! Throughput was not impacted at all. Latency increased but not extremely. Our p99 e2e latency 3x’d (60ms vs 185ms), but the p95 barely moved from 10.6ms to 12ms.&lt;/p&gt;
    &lt;p&gt;This shows that a simple 3-node Postgres cluster can pretty easily sustain what is a very common Kafka workload - 5 MB/s ingest and 25 MB/s egress. Not only that, but for a cheap cost too. Just $11,514 per year.16&lt;/p&gt;
    &lt;p&gt;Typically, you’d expect Postgres to run more expensive than Kafka at a certain scale, simply because it wasn’t designed to be efficient for this use case. Not here though. Running Kafka yourself would cost the same. Running the same workload through a Kafka vendor will cost you at least $50,000 a year. 🤯&lt;/p&gt;
    &lt;p&gt;By the way, in Kafka it’s customary to apply client-side compression on your data. If we assume your messages were 5 KB in size and your clients applied a pretty regular compression ratio of 4x17 - Postgres is actually handling 20 MB/s ingress and 100 MB/s egress.&lt;/p&gt;
    &lt;head rend="h3"&gt;96 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;Ok, let’s see how far Postgres will go.&lt;/p&gt;
    &lt;p&gt;The results are the average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.24xlarge (96 vCPU, 192 GiB RAM) Postgres server instance /w 250GB io2 12,000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;modified Postgres settings (&lt;code&gt;huge_pages&lt;/code&gt;on, other settings scaled to match the machine);&lt;list rend="ul"&gt;&lt;item&gt;still kept fsync &amp;amp; synchronous_commit on for durability.&lt;/item&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row’s payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;30 topicpartition tables&lt;/item&gt;
      &lt;item&gt;100 writers (~3.33 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;150 reader clients total (5 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 200 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 243,000 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 238 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 138ms p99 / 47ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 1,200,000 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 1.16 GiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 24.6ms p99&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 853ms p99 / 242ms p95 / 23.4ms p50&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~10% CPU (basically idle);&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;bottleneck: The bottleneck was the write rate per partition. It seems like the test wasn’t able to write at a higher rate than 8 MiB/s (8k msg/s) per table with this design. I didn’t push further, but I do wonder now as I write this - how far would writes have scaled?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Reads were trivial to scale. Adding more consumer groups was trivial - I tried with 10x fanout and still ran at low CPU. I didn’t include it because I didn’t feel the need to push to an unrealistic read-fanout extreme.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;240 MiB/s ingress and 1.16 GiB/s egress are pretty good! The 96 vCPU machine was overkill for this test - it could have done a lot more, or we could have simply opted for a smaller machine. For what it’s worth, I do think it’s worth it to deploy a separate Kafka cluster at this scale. Kafka can save you a lot of money here because it can be more efficient in how it handles cross-zone network traffic with features like Diskless Kafka.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pub-Sub Test Summary&lt;/head&gt;
    &lt;p&gt;The summarized table with the three test results can be found here → 👉 stanislavkozlovski/pg-queue-pubsub-benchmark&lt;/p&gt;
    &lt;p&gt;These tests seem to show that Postgres is pretty competitive with Kafka at low scale.&lt;/p&gt;
    &lt;p&gt;You may have noticed none of these tests were particularly long-running. From my understanding, the value in longer-running tests is to test table vacuuming in Postgres, as that can have negative performance effects. In the pub-sub section, vacuuming doesn’t apply because the tables are append-only. My other reasoning for running shorter tests was to keep costs in check and not spend too much time18.&lt;/p&gt;
    &lt;p&gt;In any case, no benchmark is perfect. My goal wasn’t to indisputably prove &lt;code&gt;$MY_CLAIM&lt;/code&gt;. Rather, I want to start a discussion by showing that what’s possible is likely larger than what most people assume. I certainly didn’t assume I’d get such good numbers, especially with the pub-sub part.&lt;/p&gt;
    &lt;head rend="h1"&gt;PG as a Queue&lt;/head&gt;
    &lt;p&gt;In Postgres, a queue can be implemented with &lt;code&gt;SELECT FOR UPDATE SKIP LOCKED&lt;/code&gt;. This command selects an unlocked row and locks it. It also skips reading already-locked rows. That’s how mutual exclusion is achieved - a worker can’t get other workers’ jobs.&lt;/p&gt;
    &lt;p&gt;Postgres has a very popular pgmq library that offers a slick queue API. To keep it simple and understand the end-to-end flow better, I decided to write my own queue. The basic version of it is very easy. My workflow is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;add job (&lt;code&gt;INSERT&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;lock row &amp;amp; take job (&lt;code&gt;SELECT FOR UPDATE SKIP LOCKED&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;process job (&lt;code&gt;{your business logic}&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;mark job as “done” (&lt;code&gt;UPDATE&lt;/code&gt;a field or&lt;code&gt;DELETE &amp;amp; INSERT&lt;/code&gt;the row into a separate table)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Postgres competes with RabbitMQ, AWS SQS, NATS, Redis19 and to an extent Kafka20 here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Queue Setup&lt;/head&gt;
    &lt;head rend="h4"&gt;Table&lt;/head&gt;
    &lt;p&gt;We use a simple &lt;code&gt;queue&lt;/code&gt; table. When an element is processed off the queue, it’s moved into the archive table.&lt;/p&gt;
    &lt;head rend="h4"&gt;Writes&lt;/head&gt;
    &lt;p&gt;We again run &lt;code&gt;N&lt;/code&gt; writer client goroutines.
Each one simply loops and sequentially inserts a single random item into the table:&lt;/p&gt;
    &lt;p&gt;It only inserts one message per statement, which is pretty inefficient at scale.&lt;/p&gt;
    &lt;head rend="h4"&gt;Reads&lt;/head&gt;
    &lt;p&gt;We again run &lt;code&gt;M&lt;/code&gt; reader client goroutines. Each reader loops and processes one message.
The processing is done inside a database transaction.&lt;/p&gt;
    &lt;p&gt;Each reader again only works with one message at a time per transaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Queue Results&lt;/head&gt;
    &lt;p&gt;I again ran the same three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The results are the average of two 15-minute tests. I also ran three 2-minute tests. They all performed similarly. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.xlarge Postgres server /w 25GB gp3 9000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;all default Postgres settings21&lt;/item&gt;
      &lt;item&gt;each row’s payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;10 writer clients, 15 reader clients&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 2885 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 2.81 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 2.46ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 4.2ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency5: 17.72ms p99&lt;/item&gt;
      &lt;item&gt;server kept at ~60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What I found Postgres wasn’t good at was handling client count. The bottleneck in this setup was the read clients. Each client could not read more than ~192 messages a second because of its median read latency and sequential read nature.&lt;/p&gt;
    &lt;p&gt;Increasing client count boosted throughput but violated my ~60% CPU target. Trying to run 50 write and 50 read clients got to 4000 msg/s without increasing the queue depth but pegged the server’s CPU to 100%. I wanted to keep the benchmarks realistic for what you may run in production, rather than maxing out what a machine can do. This would be easily alleviated with a connection pooler (standard across all prod PG deployments) or a larger machine.&lt;/p&gt;
    &lt;p&gt;Another thing worth mentioning is that the workload could sustain a lot more writes than reads. If I didn’t throttle the benchmark, it would write at 12,000 msg/s and read at 2,800 msg/s. In the spirit of simplicity, I didn’t debug further and instead throttled my writes to see at what point I could get a stable 1:1 workload.&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Tri-Node&lt;/head&gt;
    &lt;p&gt;A single 10-minute test. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3x c7i.xlarge Postgres servers /w 25GB gp3 9000 IOPS EBS volume &lt;list rend="ul"&gt;&lt;item&gt;each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)&lt;/item&gt;&lt;item&gt;one &lt;code&gt;sync&lt;/code&gt;replica and one&lt;code&gt;potential&lt;/code&gt;14 replica&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;a few custom Postgres settings like &lt;code&gt;wal_compression&lt;/code&gt;,&lt;code&gt;max_worker_processes&lt;/code&gt;,&lt;code&gt;max_parallel_workers&lt;/code&gt;,&lt;code&gt;max_parallel_workers_per_gather&lt;/code&gt;and of course -&lt;code&gt;hot_standby&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;each row’s payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;10 writer clients, 15 reader clients&lt;/item&gt;
      &lt;item&gt;readers only access the primary DB15; readers are in the same AZ as the primary;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 2397 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 2.34 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 3.3ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 7.6ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency5: 920ms p99 ⚠️6; 536ms p95; 7ms p50&lt;/item&gt;
      &lt;item&gt;server kept at ~60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As expected, throughput and latency were impacted somewhat. But not that much. It’s still over 2000 messages a second, which is pretty good for an HA queue!&lt;/p&gt;
    &lt;head rend="h3"&gt;96 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.24xlarge Postgres server instance /w 250GB io2 12,000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;modified Postgres settings (&lt;code&gt;huge_pages&lt;/code&gt;on, other settings scaled to match the machine);&lt;list rend="ul"&gt;&lt;item&gt;still kept fsync &amp;amp; synchronous_commit on for durability.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row’s payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;100 writer clients, 200 reader clients&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 20,144 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 19.67 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 9.42ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 22.6ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency: 930ms p99 ⚠️6; 709ms p95; 12.6ms p50&lt;/item&gt;
      &lt;item&gt;server at 40-60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This run wasn’t that impressive. There is some bottleneck in the single-table queue approach at this scale which I didn’t bother figuring out. I figured that it wasn’t important to reach absurd numbers on a single table, since all realistic scenarios would have multiple queues and never reach 20,000 msg/s on a single one. The 96 vCPU instance would likely scale far further were we to run a few separate queue tables in parallel.&lt;/p&gt;
    &lt;head rend="h3"&gt;Queue Test Summary&lt;/head&gt;
    &lt;p&gt;The summarized table with the three test results can be found here → 👉 stanislavkozlovski/pg-queue-pubsub-benchmark&lt;/p&gt;
    &lt;p&gt;Even a modest Postgres node can durably push thousands of queue ops/sec, which already covers the scale 99% of companies ever hit with a single queue. As I said earlier, the last 2 years have seen the Just Use Postgres slogan become mainstream. The &lt;code&gt;pgmq&lt;/code&gt; library’s star history captures this trend perfectly:
&lt;/p&gt;
    &lt;head rend="h1"&gt;Should You Use Postgres?&lt;/head&gt;
    &lt;p&gt;Most of the time - yes. You should always default to Postgres until the constraints prove you wrong.&lt;/p&gt;
    &lt;p&gt;Kafka is obviously better optimized for pub-sub workloads. Queue systems are obviously better optimized for queue workloads. The point is that picking a technology based on technical optimization alone is a flawed approach. To throw an analogy:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;a Formula One car is optimized to drive faster, but I still use a sedan to go to work. I am way more comfortable driving my sedan than an F1 car.&lt;/p&gt;
      &lt;p&gt;(seriously, see the steering wheel on these things)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Postgres sedan comes with many quality-of-life comforts that the F1 Kafka does not:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ability to debug messages with regular SQL&lt;/item&gt;
      &lt;item&gt;ability to delete, re-order or edit messages in place&lt;/item&gt;
      &lt;item&gt;ability to join pub-sub data with regular tables&lt;/item&gt;
      &lt;item&gt;ability to trivially read specific data via rich SQL queries (&lt;code&gt;ID=54&lt;/code&gt;,&lt;code&gt;name="John"&lt;/code&gt;,&lt;code&gt;cost&amp;gt;1000&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Giving up these comforts is a justified sacrifice for your F1 car to go at 378 kmh (235 mph), but masochistic if you plan on driving at 25kmh (15 mph).&lt;/p&gt;
    &lt;p&gt;Donald Knuth warned us in 1974 - premature optimization is the root of all evil. Deploying Kafka at small scale is premature optimization. The point of this article is to show you that this “small scale” number has grown further than what people remember it to be - it can comfortably mean many megabytes per second.&lt;/p&gt;
    &lt;p&gt;We are in a Postgres Renaissance for a reason: Postgres is frequently good enough. Modern NVMEs and cheap RAM allow it to scale absurdly high.&lt;/p&gt;
    &lt;p&gt;What’s the alternative?&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Solutions for Everything?&lt;/head&gt;
    &lt;p&gt;Naive engineers tend to adopt a specialized technology at the slightest hint of a need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Need a cache? Redis, of course!&lt;/item&gt;
      &lt;item&gt;Search? Let’s deploy Elasticsearch!&lt;/item&gt;
      &lt;item&gt;Offline data analysis? BigQuery or Snowflake - that’s what our data analysts used at their last job.&lt;/item&gt;
      &lt;item&gt;No schemas? We need a NoSQL database like MongoDB.&lt;/item&gt;
      &lt;item&gt;Have to crunch some numbers on S3? Let’s use Spark!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A good engineer thinks through the bigger picture.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Does this new technology move the needle?&lt;/item&gt;
      &lt;item&gt;Is shaving a few milliseconds off our query worth the extra organizational complexity introduced with the change?&lt;/item&gt;
      &lt;item&gt;Will our users notice?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At small scale, these systems hurt you more than they benefit you. Distributed systems - both in terms of node count and system cardinality - should be respected, feared, avoided and employed only as a weapon of last resort against particularly gnarly problems. Everything with a distributed system becomes more challenging and time-consuming.&lt;/p&gt;
    &lt;p&gt;The problem is the organizational overhead. The organizational overhead of adopting a new system, learning its nuances, configs, establishing monitoring, establishing processes around deployments and upgrades, attaining operational expertise on how to manage it, creating runbooks, testing it, debugging it, adopting its clients and API, using its UI, keeping up with its ecosystem, etc.&lt;/p&gt;
    &lt;p&gt;All of these are real organizational costs that can take months to get right, even if the system in question isn’t difficult (a lot are). Managed SaaS offerings trade off some of the organizational overhead for greater financial costs - but they still don’t remove it all. And until you reach the scale where the technology is necessary, you pay these extra {financial, organizational} costs for zero significant gain.&lt;/p&gt;
    &lt;p&gt;If the same can be done with tech for which you’ve already paid the organizational costs for (e.g Postgres), adopting something else prematurely is most definitely an anti-pattern. You don’t need web-scale technologies when you don’t have web-scale problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;MVI (a better alternative)&lt;/head&gt;
    &lt;p&gt;What I think is a better approach is to search for the minimum viable infrastructure (MVI): build the smallest amount of system while still providing value.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;choose good-enough technology your org is already familiar with &lt;list rend="ul"&gt;&lt;item&gt;good-enough == meets your users’ needs without being too slow/expensive/insecure&lt;/item&gt;&lt;item&gt;familiar == your org has prior experience, has runbooks/ops setups, monitoring, UI, etc&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;solve a real problem with it&lt;/item&gt;
      &lt;item&gt;use the minimum set of features &lt;list rend="ul"&gt;&lt;item&gt;the fewer features you use, the more flexibility you have to move off the infra in question in the future (e.g if locked in with a vendor)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bonus points if that technology:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;is widely adopted so finding good engineers for it is trivial (Postgres - check)&lt;/item&gt;
      &lt;item&gt;has a strong and growing network effect (Postgres - check)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MVI approach reduces the surface area of your infra. The fewer moving parts you have, the fewer failure modes you worry about and the less glue code you have to maintain.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it’s human nature to go against this. Just like startups suffer due to MVP bloat (one more feature!), infra teams suffer due to MVI bloat (one more system!)&lt;/p&gt;
    &lt;head rend="h2"&gt;Why are we like this?&lt;/head&gt;
    &lt;p&gt;I won’t pretend to be able to map out the exact path-dependent outcome, but my guess is this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;the zero interest rate era gave us abundant speculative money that was invested in any company that could grow fast&lt;/item&gt;
      &lt;item&gt;a lot of viral internet companies were growing at speeds that led old infra to become obsolete fast&lt;/item&gt;
      &lt;item&gt;this prompted the next wave of ZIRP investment - specialized data infrastructure companies (in a gold rush, sell shovels!); some of these data infra startups spun off directly from the high-growth companies themselves&lt;/item&gt;
      &lt;item&gt;each well-funded data infra vendor is financially motivated to evangelize their product and have you adopt it even when you don’t need to (Everyone is Talking Their Book). They had deep pockets for marketing and used them.&lt;/item&gt;
      &lt;item&gt;innovative infrastructure software got engineered. It was exciting - so engineers got nerd-sniped into it&lt;/item&gt;
      &lt;item&gt;a web-scale craze/cargo cult developed, where everybody believed they need to be able to scale from zero to millions of RPS because they may go viral any day.&lt;/item&gt;
      &lt;item&gt;a trend developed to copy whatever solutions the most successful, largest digital-native companies were using (Amazon, Google, Uber, etc.)&lt;/item&gt;
      &lt;item&gt;the trend became a self-perpetuating prophecy: these technologies became a sought-after skill on resumes &lt;list rend="ul"&gt;&lt;item&gt;system design interview questions were adapted to test for knowledge of these systems&lt;/item&gt;&lt;item&gt;within an organization, engineers (knowingly or not) pushed for projects that are exciting and helped build their resumes;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This trend continues to grow while there is no strong competing force that is sufficiently motivated to push the opposite view. Even engineers inside a company, who ought to be motivated to keep things simple, have strong incentives to pursue extra complexity. It benefits their career by giving them a project to use as ammo for their next promotion and improves their resume (cool tech/story on there) for their next job-hop. Plus it’s simply more fun.&lt;/p&gt;
    &lt;p&gt;This is why I think we, as an industry, don’t always use the simplest solution available.&lt;/p&gt;
    &lt;p&gt;In most cases, Postgres is that simplest solution that is available.&lt;/p&gt;
    &lt;head rend="h2"&gt;But It Won’t Scale!&lt;/head&gt;
    &lt;p&gt;I want to wrap this article up, but one rebuttal I can’t miss addressing is the “it won’t scale argument”.&lt;/p&gt;
    &lt;p&gt;The argument goes something like this: “in today’s age we can go viral at a moment’s notice; these viral moments are very valuable for our business so we need to aggressively design in a way that keeps our app stable under traffic spikes”&lt;/p&gt;
    &lt;p&gt;I have three arguments against this:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Postgres Scales&lt;/head&gt;
    &lt;p&gt;As of 2025, OpenAI still uses an unsharded Postgres architecture with only one primary instance for writes22. OpenAI is the poster-child of rapid viral growth. They hold the record for the fastest startup to reach 100 million users.&lt;/p&gt;
    &lt;p&gt;Bohan Zhang, a member of OpenAI’s infrastructure team and co-founder of OtterTune (a Postgres tuning service), can be quoted as saying23:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“At OpenAI, we utilize an unsharded architecture with one writer and multiple readers, demonstrating that PostgreSQL can scale gracefully under massive read loads.”&lt;/p&gt;
      &lt;p&gt;“The main message of my talk was that if you are not too write heavy, you can scale Postgres to a very high read throughput with read replicas using only a single master! That is exactly the message that needs to be spelled out as that covers the vast majority of apps.”&lt;/p&gt;
      &lt;p&gt;“Postgres is probably the default choice for developers right now. You can use Postgres for a very long time. If you are building a startup with read-heavy workloads, just start with Postgres. If you hit a scalability issue, increase the instance size. You can scale it to a very large scale. If in the future the database becomes a bottleneck, congratulations. You have built a successful startup. It’s a good problem to have.”&lt;/p&gt;
      &lt;p&gt;(slightly edited for clarity and grammar)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Despite their rapid growth to a user base of more than 800 million, OpenAI has still NOT opted for a web-scale distributed database. If they haven’t… why does your unproven project need to?&lt;/p&gt;
    &lt;head rend="h3"&gt;2. You Have More Time To Scale Than You Think&lt;/head&gt;
    &lt;p&gt;Let’s say it’s a good principle to design/test for ~10x your scale. Here are the years of consistent growth rate it takes to get to 10x your current scale:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;annual growth&lt;/cell&gt;
        &lt;cell role="head"&gt;years to hit 10× scale&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10 %&lt;/cell&gt;
        &lt;cell&gt;24.16 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25 %&lt;/cell&gt;
        &lt;cell&gt;10.32 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;50 %&lt;/cell&gt;
        &lt;cell&gt;5.68 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;75 %&lt;/cell&gt;
        &lt;cell&gt;4.11 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;100 %&lt;/cell&gt;
        &lt;cell&gt;3.32 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;150 %&lt;/cell&gt;
        &lt;cell&gt;2.51 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;200 %&lt;/cell&gt;
        &lt;cell&gt;2.10 y&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It goes to show that even at extreme growth levels, you still have years to migrate between solutions. The majority of developers, though, work at companies in the 0-50% growth rate. They are more likely to have moved on to another job by the time the solution needs to change (if ever).&lt;/p&gt;
    &lt;head rend="h3"&gt;3. It’s Overdesign&lt;/head&gt;
    &lt;p&gt;In an ideal world, you would build for scale and any other future problem you may hit in 10 years.&lt;/p&gt;
    &lt;p&gt;In the real world, you have finite bandwidth, so you have to build for the most immediate, highest ROI problem.&lt;/p&gt;
    &lt;p&gt;Commenter snej on lobste.rs captured it well:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Planning your infrastructure around being able to handle that is sort of like buying a huge Marshall stack as your first guitar amp because your garage band might get invited to open for Coldplay.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Just use Postgres until it breaks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disclaimers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Title inspiration comes from a great recent piece - “Redis is fast - I’ll cache in Postgres”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I’m a complete Postgres noob. You may see a lot of dumb mistakes here. Feel free to call me out on them - I’m happy to learn. I used AI to help a lot with some of the PG tools to use. This both shows how inexperienced I am in the context and how easy it is to start. I am generally skeptical of AI’s promise (in the short-term), but there’s no denying it has made a large dent in democratizing niche/low-level knowledge.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’d like to reach out to me, you can find me on LinkedIn or X (Twitter).&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Don’t worry if you don’t fully understand these terms. I work full-time in the industry that spews these things and I don’t have a great grasp either. It’s marketing slop. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gartner and others push embarrassing recommendations that aren’t tech driven. It’s frequently the opposite - they’re profit driven. Gartner makes $6.72B purely off a consulting service that charges organizations $50k per seat solely for access to reports that recommend these slop architectures. It’s not crazy to believe, hence many people are converging with the idea that it is a pay-to-win racket model. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Seriously, the improvement in hardware is something I find most senior engineers haven’t properly appreciated. Newest gen AMD CPUs boast 192 cores. Modern SSDs can do 5.5 million random reads a second, or ~28GB/s sequential reads. Both are a 10-20x improvement over the last 10 years alone. Single nodes are more powerful than ever. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Just in the last 6 months - Snowflake acquired Crunchy Data for ~$250M, Databricks acquired Neon for ~$1 billion; In the last 12 months, Supabase more than 5x’d its valuation from ($900M to $5B), raising $380M across three series (!!!). Within a single year! ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;End-to-end latency here is defined as&lt;/p&gt;&lt;code&gt;now() - event_create_time&lt;/code&gt;; In essence, it tracks how long a brand new persisted event takes to get consumed. It helps show cases where queue lag spikes like when consumers temporarily fall behind the write rate. ↩ ↩2 ↩3 ↩4 ↩5 ↩6 ↩7&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Some queue tests showed higher E2E latencies which I believe was due to a bug. In the pub-sub tests, I ensured readers start before the writers via a 1000ms sleep. For the queue tests, though, I didn’t do this. The result is that queue tests immediately spike queue depth at startup because the writers manage to get a head start before the readers. I believe the E2E latency is artificially high because of this flaw in the test. ↩ ↩2 ↩3 ↩4&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Actually, things are ordered in the happy path. Only during retries can you get out of order processing. e.g at t=0, client A reads task N; At t=1, client B reads task N+1 and processes it successfully; At t=2, A fails and is unable to process task N; At t=3, client B takes the next available task - which is N. B therefore executes the tasks in order [N+1, N], whereas proper order would have been [N, N+1] ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Open-source projects include Apache Pulsar (open source), RedPanda (source-available), AutoMQ (a fork of Kafka) and a lot of proprietary offerings - AWS Kinesis, Google Pub/Sub, Azure Event Hubs, Confluent Kora, Confluent WarpStream, Bufstream to name a few. What’s common in 90% of these projects is that they all implement the Apache Kafka API, making Kafka undoubtedly the protocol standard in the space. There’s also an open-source project which exposes a Kafka API on top of a pluggable Postgres or S3 backend - Tansu (Rust, btw :] ) ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The most popular library I could find is pg-pubsub with 106 stars as of writing (Oct 2025). Its last commit was 3 months ago. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Batching messages per client is very important for scalability here. It is one of Kafka’s least-talked-about performance “tricks”. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;These tables act as different log data structures. You can see them as separate topics, or partitions of one topic (shards). ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Postgres stores all&lt;/p&gt;&lt;code&gt;NOTIFY&lt;/code&gt;events in a single, global queue. If this queue becomes full, transactions calling&lt;code&gt;NOTIFY&lt;/code&gt;will fail when committing. (src) ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A report by RedPanda found that ~55% of respondents use Kafka for &amp;lt; 1 MB/s. Kafka-vendor Aiven similarly shared that 50% of their Kafka deployments have an ingest rate of below 10 MB/s. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This replication is equivalent to RF=2 in Kafka with one extra non-synchronous replica. Call it RF=2.5. The client receives a response when the one&lt;/p&gt;&lt;code&gt;sync&lt;/code&gt;replica confirms the change. The other&lt;code&gt;potential&lt;/code&gt;replica is replicating asynchronously without blocking the write path. It will become promoted to&lt;code&gt;sync&lt;/code&gt;if the other one was to die. ↩ ↩2&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The tests didn’t direct any read traffic to the standbys. This caused extra load on the primary - most production workloads would read from the standbys. Despite that, the results were still good! In my tests, I found that the extra read workload didn’t seem to have a negative effect on the database - it seems such tail reads were served exclusively from cache. ↩ ↩2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The node and its disk cost $1826 per year. Since we run three of those, it’s $5478/yr. The networking in AWS costs $0.02/GB and our setup is replicating 4.9MB/s across two instances - that results in 294.74TB cross-zone networking per year. That’s $6036 per year in replication networking. Assuming your clients are in the same zone as the database they’re writing to / reading from, that networking is free. That results in an annual cost of $11,514. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We can realistically achieve a 10x+ compression ratio if operating on compressible data like logs (something Kafka is used for frequently). The only gotcha is that we need to compress larger batches - eg 25KB+ - so that requires a bit of a re-design in the pub-sub data model. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I had already spent enough business days working on this benchmark and re-running tests numerous, numerous times as I iterated on the benchmark and the methodology. On the larger instances, the cost accumulates fast and running longer tests at high MB/s rates requires deploying much larger and more expensive disks in order to store all the accumulated data. The effort spent matches the goal I have with the article. If any Postgres vendor wants to sponsor a more thorough investigation - let me know! ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Surprisingly (to me), Redis is a very popular queue-like backend choice for background jobs. Most popular open-source libraries use it. Although I’m sure Postgres can do just as good a job, many devs will prefer to use an established library rather than build one from scratch or use something less well-maintained. I do think PG-backed libraries should get developed though! ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kafka has historically never been a queue. To use it as one, you had to develop some difficult workarounds. Today, however, it is in the middle of implementing a first-class Queue-like interface (currently in Preview) ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most importantly, synchronous commit and fsync are both on. This means every write is durably persisted to disk. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The optimizations they did to support this scale are cool, but not novel. See these two talks at a) PGConf.dev 2025 (my transcript) and b) POSETTE (my transcript) ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;From the talks PGConf.dev 2025 (my transcript) and POSETTE (my transcript) ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks"/><published>2025-10-29T14:06:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747112</id><title>I made a 10¢ MCU Talk</title><updated>2025-10-29T17:10:56.457752+00:00</updated><content>&lt;doc fingerprint="a50cd3b339220607"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;TLDR: Yes, you can fit about 7 seconds of audio into 16K of flash and still have room for code. And you can even play LPC encoded audio on a 10 cent MCU.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There’s quite a lot more detail in this video (and of course you can hear the audio!).&lt;/p&gt;
    &lt;p&gt;In the previous project, I had this ultra-cheap CH32V003 microcontroller playing simple tunes on a tiny SMD buzzer. It was just toggling a GPIO pin at musical note frequencies – 1-bit audio output – and it sounded surprisingly decent. That was a fun start, but now it’s time to push this little $0.10 MCU even further: can we make it actually talk?&lt;/p&gt;
    &lt;p&gt;Spoiler: Yes, we can! (well, there wouldn’t be much of a blog post if we couldn’t) This 8-pin RISC-V chip is now producing sampled audio data and spoken words. We’re really stretching the limits of what you can fit in 16 KB of flash.&lt;/p&gt;
    &lt;head rend="h2"&gt;From Beeps to Actual Audio&lt;/head&gt;
    &lt;p&gt;Moving from simple beeps to real audio meant using the microcontroller’s PWM output as a rudimentary DAC. Instead of just on/off beeping, I’m driving a waveform at an 8 kHz sample rate using a high-frequency PWM on the output pin. The hardware is the same tiny board as before – but I’ve swapped the small SMD buzzer for a small speaker. The buzer works too, but it’s quieter and very tinny.&lt;/p&gt;
    &lt;p&gt;The sample I wanted to test with is just over 6 seconds in length - it’s the iconic “Open the pod bay doors HAL…” sequence from 2001.&lt;/p&gt;
    &lt;p&gt;If we keep this audio at 16-bit PCM, 8kHZ, we’d need about 96KB – way beyond our 16 KB flash! And remember, that 16 KB has to hold both the audio data and our playback code. Clearly some aggressive compression is required.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Sample Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Bits/Sample&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Fits in 16KB?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CD Quality&lt;/cell&gt;
        &lt;cell&gt;44.1 kHz&lt;/cell&gt;
        &lt;cell&gt;16-bit&lt;/cell&gt;
        &lt;cell&gt;529 KB&lt;/cell&gt;
        &lt;cell&gt;❌ 33× too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Phone Quality&lt;/cell&gt;
        &lt;cell&gt;16 kHz&lt;/cell&gt;
        &lt;cell&gt;16-bit&lt;/cell&gt;
        &lt;cell&gt;192 KB&lt;/cell&gt;
        &lt;cell&gt;❌ 12× too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Basic PCM&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;8-bit&lt;/cell&gt;
        &lt;cell&gt;48 KB&lt;/cell&gt;
        &lt;cell&gt;❌ 3× too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;4-bit ADPCM (IMA)&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;4-bit&lt;/cell&gt;
        &lt;cell&gt;24 KB&lt;/cell&gt;
        &lt;cell&gt;❌ 1.5× too big&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;QOA (Quite OK Audio)&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;3.2-bit&lt;/cell&gt;
        &lt;cell&gt;19 KB&lt;/cell&gt;
        &lt;cell&gt;❌ Still too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2-bit ADPCM&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;2-bit&lt;/cell&gt;
        &lt;cell&gt;12 KB&lt;/cell&gt;
        &lt;cell&gt;✅ Fits!&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I considered a few encoding options for compressing the audio.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8-bit PCM: Simply using 8-bit samples at 8 kHz cuts size in half (to ~47 KB for 6s), but that’s still about 3× too large for our flash.&lt;/item&gt;
      &lt;item&gt;4-bit ADPCM: Adaptive Differential PCM is a simple lossy compression that could quarter the size. In theory 6 seconds would be ~24 KB – much closer to fitting,&lt;/item&gt;
      &lt;item&gt;“Quite OK Audio” (QOA): This is nice codec that packs audio into about 3.2 bits per sample (roughly 1/5 the size of 16-bit PCM)&lt;/item&gt;
      &lt;item&gt;2-bit ADPCM: Going even further with ADPCM, using only 2 bits per sample gives a 4:1 compression relative to 8-bit audio – that’s 75% storage savings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2-bit ADPCM is definitely the winner here. Our 6-second clip shrinks to under 12 KB, which comfortably fits in flash with room for code. This looked like the winner, provided the audio quality was acceptable. The decoder for 2-bit ADPCM is also very lightweight (my implementation compiled to under just over 1K of code - 1340 bytes!). It’s definitely low quality - but it actually sounds surprisingly ok.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does 2-bit ADPCM work?&lt;/head&gt;
    &lt;p&gt;It’s actually a very simple algorithm. Both the encoder and decoder maintain a predicted signal value and a step size index into a predefined table. Each 2-bit code tells the decoder how to adjust the current prediction and the step size index. In essence, we’re coding the difference between the real audio and our prediction, with only four possible levels (since 2 bits gives 4 values). After each sample, the algorithm adapts: if the prediction error was large, we move to a bigger step size (to allow larger changes); if the error was small, we use a smaller step size for finer resolution. This adaptive step is what makes it ADPCM (Adaptive Differential PCM).&lt;/p&gt;
    &lt;p&gt;Our codes are as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;00&lt;/code&gt;(0): Go down by 1 step - subtract the step size from our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;01&lt;/code&gt;(1): Go up by 1 step - add the step size to our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;10&lt;/code&gt;(2): Go down by 2 steps - subtract the 2 x step size from our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;11&lt;/code&gt;(3): Go up by 2 steps - add the 2 x step size to our current prediction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even with this very high level of compression, the predicted waveform manages to track the original audio surprisingly well. The above graph shows a small snippet of the audio: the blue line is the original waveform and the yellow line is the ADPCM decoder’s output.&lt;/p&gt;
    &lt;p&gt;They’re not identical (and we wouldn’t expect them to be), but the general shape is preserved. When you play it back through the little speaker, it’s recognizable and surprisingly good.&lt;/p&gt;
    &lt;p&gt;To make my life easier, I built a quick conversion tool to encode WAV files into this 2-bit ADPCM format. The tool lets me drag-and-drop a WAV, and it gives you the files with the data that can ve dropped into the firmware code.&lt;/p&gt;
    &lt;head rend="h2"&gt;LPC Speech Synthesis&lt;/head&gt;
    &lt;p&gt;Six seconds of audio is cool, but what about longer phrases or even arbitrary speech? Storing anything much longer with raw or ADPCM audio would quickly fill the 16K of flash.&lt;/p&gt;
    &lt;p&gt;For my second experiment, I tried something different: instead of recorded waveform audio, I used an old-school speech synthesis approach. This leverages the fact that spoken language can be encoded very compactly by modeling the human voice, rather than storing the raw sound. Specifically, I integrated a library called Talkie.&lt;/p&gt;
    &lt;p&gt;Talkie is a software implementation of the Texas Instruments LPC speech synthesis architecture from the late 1970s. This was implemented in a variety of chips, most commonly the TMS5220 and TMS5100 speech chips.&lt;/p&gt;
    &lt;p&gt;These were used in things like the original Speak &amp;amp; Spell, arcade games like early Star Wars, and speech add-ons for home computers (e.g. the BBC Micro).&lt;/p&gt;
    &lt;p&gt;The Talkie library (originally by Peter Knight, later added to by Adafruit) comes with a big set of examples and vocabulary. It’s also possible to extract examples from old ROMs from arcade games.&lt;/p&gt;
    &lt;p&gt;Each phrase or word only takes a few hundred bytes or even less, so you can fit quite a lot of speech into a few kilobytes of flash. The trade-off is that the voice has a very computer-esque timbre – think of the Speak &amp;amp; Spell’s voice. It’s clearly synthetic, but still understandable.&lt;/p&gt;
    &lt;p&gt;To say custom sentences not in the library, you either concatenate the available words/phonemes (which can be clunky), or you need to generate new LPC data. The original tools for this are a bit obscure – there’s BlueWizard (a classic Mac app) and PythonWizard (a command-line tool with TK GUI) which can analyze WAV files and produce LPC data.&lt;/p&gt;
    &lt;p&gt;I gave both a try with some success (and a few headaches setting them up). In the end, I cheated a bit and used an AI coding assistant to help me create a streamlined online tool for this.&lt;/p&gt;
    &lt;p&gt;The result is a little web app where I can upload a recording of, say, my own voice, and it outputs the LPC data. It even lets me play back the synthesized voice in-browser to check it.&lt;/p&gt;
    &lt;p&gt;So there we have it – our 10¢ microcontroller now has a voice! By using 2-bit ADPCM compression, we can store short audio clips (up to around 8 seconds) even in 16 KB of flash, and play them back via PWM with decent fidelity.&lt;/p&gt;
    &lt;p&gt;And with the Talkie LPC speech synthesis, we can make the device “speak” lots of words and phrases with only a tiny memory footprint.&lt;/p&gt;
    &lt;p&gt;If you want to hear it for yourself, check out the video demo linked at the top of this post. In the video, you’ll see (and hear) the WarGames clip and the Star Wars quotes running on the hardware. It’s honestly amazing what these cheap little MCUs can do. We’re really pushing the boundaries of cheap hardware here.&lt;/p&gt;
    &lt;p&gt;You can find all my code on GitHub in this repository.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.atomic14.com/2025/10/29/CH32V003-talking"/><published>2025-10-29T14:12:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747402</id><title>Show HN: HUD-like live annotation and sketching app for macOS</title><updated>2025-10-29T17:10:56.175575+00:00</updated><content>&lt;doc fingerprint="354456b7533f15ed"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Annotate, explain, create - anywhere on your screen&lt;/head&gt;&lt;p&gt;Draw Over It gives presenters, trainers, and any professional an always-ready overlay for live markups. Open it with a hotkey, sketch directly on top of any app, and jump back to work without leaving a trace.&lt;/p&gt;Mac App Store&lt;head rend="h2"&gt;Always-on overlay for live sessions&lt;/head&gt;&lt;p&gt;Stay in flow while you mark up anything on screen. Draw Over It floats above your desktop and hides with a single hotkey.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Overlay the canvas on any app instantly with the menu bar controls or just ââ¥âD combination.&lt;/item&gt;&lt;item&gt;Access the HUD toolkit easily with a right-click.&lt;/item&gt;&lt;item&gt;With just a click, blur everything else to keep attention on your key points.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Tools built for clear annotations&lt;/head&gt;&lt;p&gt;Summon the SwiftUI HUD with a right-click and reach pens, highlighters, shapes, blur, and session controls without leaving the overlay.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Swap between pens, highlighters, rectangles, and circles with one click.&lt;/item&gt;&lt;item&gt;Configure pen widths from 1â64 pt, opacity, fill modes, and color presets.&lt;/item&gt;&lt;item&gt;Use quick presets for Pen sizes, highlight box, and highlighter to stay in rhythm.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Capture your ideas close to the source&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Save the canvas at any time so you can get back to that state quickly.&lt;/item&gt;&lt;item&gt;Export the canvas state as transparent PNGs ready for sharing.&lt;/item&gt;&lt;item&gt;The entire application is available in 14 languages.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Switch tools, tweak width or opacity, and pick presets in one floating palette.&lt;/p&gt;&lt;p&gt;Easily annotate any document, presentation or codebase&lt;/p&gt;&lt;p&gt;Export your canvas to a PNG with a single click&lt;/p&gt;&lt;p&gt;Visualise your ideas close to the source&lt;/p&gt;&lt;p&gt;Easy to use tooling always at your fingertips&lt;/p&gt;&lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;&lt;head rend="h3"&gt;How do I toggle the overlay?&lt;/head&gt;&lt;p&gt;Use ââ¥âD or the status bar menu. The overlay floats above every window until you hide it.&lt;/p&gt;&lt;head rend="h3"&gt;How do I open the tool HUD?&lt;/head&gt;&lt;p&gt;Right-click the overlay or press ââ¥âH. The HUD appears near your pointer for quick adjustments.&lt;/p&gt;&lt;head rend="h3"&gt;Can I erase or undo mistakes?&lt;/head&gt;&lt;p&gt;Hold Option to switch to the eraser or press ââ¥âZ to undo. Redo lives in the HUD and app menus.&lt;/p&gt;&lt;head rend="h3"&gt;Where are exports saved?&lt;/head&gt;&lt;p&gt;Transparent PNG exports live in ~/Library/Application Support/DrawOverIt/Exports inside the app sandbox.&lt;/p&gt;&lt;head rend="h3"&gt;Does the app remember my annotations?&lt;/head&gt;&lt;p&gt;Yes. Sessions persist between launches, and you can save or restore snapshots manually. Enable Ephemeral Canvas if you prefer a fresh slate every time.&lt;/p&gt;&lt;head rend="h3"&gt;Which macOS versions are supported?&lt;/head&gt;&lt;p&gt;Draw Over It targets macOS 13.5 or later with universal binaries for Apple Silicon and Intel.&lt;/p&gt;&lt;head rend="h2"&gt;Ready to draw over anything?&lt;/head&gt;&lt;p&gt;Try it yourself and see how easy visualising your ideas becomes!&lt;/p&gt;Mac App Store&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://draw.wrobele.com/"/><published>2025-10-29T14:36:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747605</id><title>Oracle has adopted BOOLEAN in 23ai and PostgreSQL had it forever</title><updated>2025-10-29T17:10:55.399277+00:00</updated><content>&lt;doc fingerprint="731da2abf15693f9"&gt;
  &lt;main&gt;
    &lt;p&gt;Oracle has finally introduced support for the Boolean data type in the release 23ai. Many thanks to the Engineers at Oracle for implementing this data type for an optimal performance. PL/SQL had BOOLEAN for decades, but developers were not able to declare native boolean type for columns of tables. For this reason, developers returned VARCHAR2/NUMBER from functions instead of BOOLEAN. Interestingly, PostgreSQL, an open-source relational database that has been widely used for many years and a migration target for Oracle, has had support for Boolean data for more than the past two decades. In this article, we will discuss about the workarounds used by developers before Oracle adopted boolean, and how it works in PostgreSQL.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Hidden Cost of Simulating True and False&lt;/head&gt;
    &lt;p&gt;For database engineers and architects designing schemas, it required workarounds to represent "true" or "false" values as ‘Y’/’N’, ‘T’/’F’, or 1/0. While these may function adequately on the surface, they may hinder performance. The lack of a native Boolean data type can be a limitation in database design and impact storage efficiency.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Workarounds We Lived With&lt;/head&gt;
    &lt;p&gt;Before Oracle’s recent version 23ai introduced BOOLEAN as the supported datatypes, following were some of the options.&lt;/p&gt;
    &lt;p&gt;Each of them may add redundant conversions and conditions in application code and PL/SQL functions, leading to more CPU work and larger indexes.&lt;/p&gt;
    &lt;p&gt;While Oracle took 2 decades to bring this up, PostgreSQL had it forever. Postgres stores Boolean values efficiently (internally as a single byte) and allows direct logical operations as follows.&lt;/p&gt;
    &lt;code&gt;SELECT * FROM employees WHERE is_active;&lt;/code&gt;
    &lt;code&gt;UPDATE orders SET is_verified = TRUE WHERE id = 1001;&lt;/code&gt;
    &lt;p&gt;What this clearly means is that it requires – No conversions, no string comparisons, just clean, logical semantics. This approach results in simpler queries, smaller indexes, and faster filtering, especially in analytical workloads with millions of rows.&lt;/p&gt;
    &lt;p&gt;Following chart demonstrates us the table structure in both Oracle and PostgreSQL, where we can avoid the additional checks in the case of PostgreSQL but not in Oracle.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Oracle Table Structure before 23ai&lt;/cell&gt;
        &lt;cell role="head"&gt;Equivalent PostgreSQL Table Structure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;CREATE TABLE user_flags ( user_id NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, is_active CHAR(1) CHECK (is_active IN ('Y', 'N')), is_verified NUMBER(1) CHECK (is_verified IN (0, 1)) );&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;CREATE TABLE user_flags ( user_id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY, is_active BOOLEAN, is_verified BOOLEAN );&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;lb/&gt; Insert Statements&lt;/p&gt;
    &lt;p&gt;You can directly insert TRUE and FALSE if boolean is supported natively, as seen in the case of PostgreSQL.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Oracle Compatible Insert before 23ai&lt;/cell&gt;
        &lt;cell role="head"&gt;PostgreSQL Compatible Insert&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;INSERT INTO user_flags (is_active, is_verified) VALUES ('Y', 1); INSERT INTO user_flags (is_active, is_verified) VALUES ('N', 0);&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;INSERT INTO user_flags (is_active, is_verified) VALUES (TRUE, TRUE); INSERT INTO user_flags (is_active, is_verified) VALUES (FALSE, FALSE);&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;lb/&gt; Select Statements&lt;/p&gt;
    &lt;p&gt;You can see simplified and performant select statements as seen in the table below.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Oracle&lt;/cell&gt;
        &lt;cell role="head"&gt;PostgreSQL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;SQL&amp;gt; SELECT * FROM genericmini_cdc.user_flags WHERE is_active = 'Y' AND is_verified = 1; USER_ID | IS_ACTIVE | IS_VERIFIED --------+-----------+------------ 1 | Y | 1 2 | Y | 1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;postgres=# SELECT * FROM user_flags WHERE is_active AND is_verified; user_id | is_active | is_verified --------+-----------+------------ 1 | t | t (1 row)&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In a nutshell, we can see boolean as the optimal data type over other relevant alternatives. Some of such benefits are listed in the table below.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Aspect&lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;BOOLEAN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;&lt;code&gt;CHAR(1)&lt;/code&gt; / &lt;code&gt;CHAR(5)&lt;/code&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;&lt;code&gt;SMALLINT&lt;/code&gt; / &lt;code&gt;NUMBER(1)&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Meaning&lt;/cell&gt;
        &lt;cell&gt;Explicit logical type with &lt;code&gt;TRUE&lt;/code&gt;, &lt;code&gt;FALSE&lt;/code&gt;, and &lt;code&gt;NULL&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Textual convention (e.g., &lt;code&gt;'Y'&lt;/code&gt;, &lt;code&gt;'N'&lt;/code&gt;, &lt;code&gt;'YES'&lt;/code&gt;, &lt;code&gt;'NO'&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Numeric convention (&lt;code&gt;1&lt;/code&gt; = true, &lt;code&gt;0&lt;/code&gt; = false)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;Typically 1 byte or bit&lt;/cell&gt;
        &lt;cell&gt;1–5 bytes depending on length and encoding&lt;/cell&gt;
        &lt;cell&gt;1–2 bytes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Type Safety&lt;/cell&gt;
        &lt;cell&gt;Accepts only logical truth values&lt;/cell&gt;
        &lt;cell&gt;May store invalid text (e.g., &lt;code&gt;'A'&lt;/code&gt;, &lt;code&gt;'?'&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;May store non-logical numbers (e.g., &lt;code&gt;2&lt;/code&gt;, &lt;code&gt;-1&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Query Simplicity&lt;/cell&gt;
        &lt;cell&gt;Supports direct logical operations (&lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;NOT&lt;/code&gt;, &lt;code&gt;IS TRUE&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Requires explicit comparison (&lt;code&gt;='Y'&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Requires explicit comparison (&lt;code&gt;=1&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Value Interpretation&lt;/cell&gt;
        &lt;cell&gt;Automatically interprets &lt;code&gt;TRUE&lt;/code&gt;, &lt;code&gt;FALSE&lt;/code&gt;, &lt;code&gt;T&lt;/code&gt;, &lt;code&gt;F&lt;/code&gt;, &lt;code&gt;YES&lt;/code&gt;, &lt;code&gt;NO&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;0&lt;/code&gt; (case-insensitive)&lt;/cell&gt;
        &lt;cell&gt;Interpretation depends on convention and case sensitivity&lt;/cell&gt;
        &lt;cell&gt;Limited to numeric values; cannot represent textual forms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Readability&lt;/cell&gt;
        &lt;cell&gt;Self-explanatory and standardized&lt;/cell&gt;
        &lt;cell&gt;Convention-dependent and less portable&lt;/cell&gt;
        &lt;cell&gt;Convention-dependent and less intuitive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Language Integration&lt;/cell&gt;
        &lt;cell&gt;Maps directly to native Boolean types&lt;/cell&gt;
        &lt;cell&gt;Requires conversion logic&lt;/cell&gt;
        &lt;cell&gt;Requires conversion logic&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To simplify end-to-end database migrations from Oracle to PostgreSQL, we have announced our tool called Hexarocket. One of the advantages of using this tool is that – during data migration from Oracle to PostgreSQL, it can automatically map CHAR(1) and NUMBER(1) of Oracle to BOOLEAN in PostgreSQL.&lt;/p&gt;
    &lt;p&gt;Are you looking to migrate from Oracle to PostgreSQL or SQL Server to PostgreSQL? We are here to support with a simple and seamless migration experience within few clicks using HexaRocket. Request us for a demo on HexaRocket today: Schedule a demo.&lt;/p&gt;
    &lt;p&gt;Contact Us Today!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hexacluster.ai/blog/postgresql/oracles-adoption-of-native-boolean-data-type-vs-postgresql/"/><published>2025-10-29T14:51:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747804</id><title>Collins Aerospace: Sending text messages to the cockpit with test:test</title><updated>2025-10-29T17:10:54.802284+00:00</updated><content>&lt;doc fingerprint="ef98aafde223bee7"&gt;
  &lt;main&gt;
    &lt;p&gt;Informed parties: RTX Corporation and Department of Defense Cyber Crime Center (on September 21, 2025)&lt;/p&gt;
    &lt;p&gt;Using the credentials test:test, it was possible to log in at the ARINC OpCenter Message Browser (Screenshot) as U.S. Navy Fleet Logistics Support Wing.&lt;/p&gt;
    &lt;p&gt;Via this web service, text messages can be sent to the cockpit. For obvious reasons, we did not try that. Sent messages could be viewed.&lt;/p&gt;
    &lt;p&gt;Unfortunately, RTX did not respond to our vulnerability report. The account was disabled.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ccc.de/en/disclosure/collins-aerospace-mit-test-test-textnachrichten-bis-ins-cockpit-senden"/><published>2025-10-29T15:07:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748186</id><title>Hosting SQLite Databases on GitHub Pages (2021)</title><updated>2025-10-29T17:10:54.672105+00:00</updated><content>&lt;doc fingerprint="3d0f8f5e6fa86b83"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Hosting SQLite databases on Github Pages&lt;/head&gt;&lt;p&gt;(or IPFS or any static file hoster)&lt;/p&gt;• Last Update&lt;p&gt;I was writing a tiny website to display statistics of how much sponsored content a Youtube creator has over time when I noticed that I often write a small tool as a website that queries some data from a database and then displays it in a graph, a table, or similar. But if you want to use a database, you either need to write a backend (which you then need to host and maintain forever) or download the whole dataset into the browser (which is not so great when the dataset is more than 10MB).&lt;/p&gt;&lt;p&gt;In the past when I’ve used a backend server for these small side projects at some point some external API goes down or a key expires or I forget about the backend and stop paying for whatever VPS it was on. Then when I revisit it years later, I’m annoyed that it’s gone and curse myself for relying on an external service - or on myself caring over a longer period of time.&lt;/p&gt;&lt;p&gt;Hosting a static website is much easier than a "real" server - there’s many free and reliable options (like GitHub, GitLab Pages, Netlify, etc), and it scales to basically infinity without any effort.&lt;/p&gt;&lt;p&gt;So I wrote a tool to be able to use a real SQL database in a statically hosted website!&lt;/p&gt;&lt;p&gt;Here’s a demo using the World Development Indicators dataset - a dataset with 6 tables and over 8 million rows (670 MiByte total).&lt;/p&gt;&lt;code&gt;select country_code, long_name from wdi_country limit 3;&lt;/code&gt;&lt;p&gt;As you can see, we can query the &lt;code&gt;wdi_country&lt;/code&gt; table while fetching only 1kB of data!&lt;/p&gt;&lt;p&gt;This is a full SQLite query engine. As such, we can use e.g. the SQLite JSON functions:&lt;/p&gt;&lt;code&gt;select json_extract(arr.value, '$.foo.bar') as bar
  from json_each('[{"foo": {"bar": 123}}, {"foo": {"bar": "baz"}}]') as arr&lt;/code&gt;&lt;p&gt;We can also register JS functions so they can be called from within a query. Here’s an example with a &lt;code&gt;getFlag&lt;/code&gt; function that gets the flag emoji for a country:&lt;/p&gt;&lt;code&gt;function getFlag(country_code) {
  // just some unicode magic
  return String.fromCodePoint(...Array.from(country_code||"")
    .map(c =&amp;gt; 127397 + c.codePointAt()));
}

await db.create_function("get_flag", getFlag)
return await db.query(`
  select long_name, get_flag("2-alpha_code") as flag from wdi_country
    where region is not null and currency_unit = 'Euro';
`)&lt;/code&gt;&lt;p&gt;Press the Run button to run the following demos. You can change the code in any way you like, though if you make a query too broad it will fetch large amounts of data ;)&lt;/p&gt;&lt;p&gt;Note that this website is 100% hosted on a static file hoster (GitHub Pages).&lt;/p&gt;&lt;p&gt;So how do you use a database on a static file hoster? Firstly, SQLite (written in C) is compiled to WebAssembly. SQLite can be compiled with emscripten without any modifications, and the sql.js library is a thin JS wrapper around the wasm code.&lt;/p&gt;&lt;p&gt;sql.js only allows you to create and read from databases that are fully in memory though - so I implemented a virtual file system that fetches chunks of the database with HTTP Range requests when SQLite tries to read from the filesystem: sql.js-httpvfs. From SQLite’s perspective, it just looks like it’s living on a normal computer with an empty filesystem except for a file called &lt;code&gt;/wdi.sqlite3&lt;/code&gt; that it can read from. Of course it can’t write to this file, but a read-only database is still very useful.&lt;/p&gt;&lt;p&gt;Since fetching data via HTTP has a pretty large overhead, we need to fetch data in chunks and find some balance between the number of requests and the used bandwidth. Thankfully, SQLite already organizes its database in "pages" with a user-defined page size (4 KiB by default). I’ve set the page size to 1 KiB for this database.&lt;/p&gt;&lt;p&gt;Here’s an example of a simple index lookup query:&lt;/p&gt;&lt;code&gt;select indicator_code, long_definition from wdi_series where indicator_name
    = 'Literacy rate, youth total (% of people ages 15-24)'&lt;/code&gt;&lt;p&gt;Run the above query and look at the page read log. SQLite does 7 page reads for that query.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Three page reads are just some to get some schema information (these are already cached)&lt;/item&gt;&lt;item&gt;Two page reads are the index lookup in the index &lt;code&gt;on wdi_series (indicator_name)&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Two page reads are on the &lt;code&gt;wdi_series&lt;/code&gt;table data (the first to find the row value by primary key, the second to get the text data from an overflow page)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Both the index as well as the table reads are B-Tree lookups.&lt;/p&gt;&lt;p&gt;A more complex query: What are the countries with the lowest youth literacy rate, based on the newest data from after 2010?&lt;/p&gt;&lt;code&gt;with newest_datapoints as (
  select country_code, indicator_code, max(year) as year from wdi_data
  join wdi_series using (indicator_code)
  where
    indicator_name = 'Literacy rate, youth total (% of people ages 15-24)'
    and year &amp;gt; 2010
  group by country_code
)
select c.short_name as country, printf('%.1f %%', value) as "Youth Literacy Rate"
from wdi_data
  join wdi_country c using (country_code)
  join newest_datapoints using (indicator_code, country_code, year)
order by value asc limit 10&lt;/code&gt;&lt;p&gt;The above query should do 10-20 GET requests, fetching a total of 130 - 270KiB, depending on if you ran the above demos as well. Note that it only has to do 20 requests and not 270 (as would be expected when fetching 270 KiB with 1 KiB at a time). That’s because I implemented a pre-fetching system that tries to detect access patterns through three separate virtual read heads and exponentially increases the request size for sequential reads. This means that index scans or table scans reading more than a few KiB of data will only cause a number of requests that is logarithmic in the total byte length of the scan. You can see the effect of this by looking at the "Access pattern" column in the page read log above.&lt;/p&gt;&lt;p&gt;All of this only works well when we have indices in the database that match the queries well. For example, the index used in the above query is a &lt;code&gt;INDEX ON wdi_data (indicator_code, country_code, year, value)&lt;/code&gt;. If that index did not include the value column, the SQLite engine would have to do another random access (unpredictable) read and thus HTTP request to retrieve the actual value for every data point. If the index was ordered &lt;code&gt;country_code, indicator_code, ...&lt;/code&gt;, then we would be able to quickly get all indicators for a single country, but not all country values of a single indicator.&lt;/p&gt;&lt;p&gt;We can also make use of the SQLite FTS module so we can do a full-text search on the more text-heavy information in the database - in this case there are over 1000 human development indicators in the database with longer descriptions.&lt;/p&gt;&lt;code&gt;select * from indicator_search
where indicator_search match 'educatio* femal*'
order by rank limit 10&lt;/code&gt;&lt;p&gt;The total amount of data in the &lt;code&gt;indicator_search&lt;/code&gt; FTS table is around 8 MByte. The above query should only fetch around 70 KiB. You can see how it is constructed here.&lt;/p&gt;&lt;p&gt;And finally, here’s a more complete demonstration of the usefulness of this system - here’s an interactive graph showing the development of a few countries over time, for any countries you want using any indicator from the dataset:&lt;/p&gt;&lt;head&gt;Extra information about this indicator&lt;/head&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;Indicator Code&lt;/item&gt;&lt;item rend="dd-1"&gt;IT.NET.USER.ZS&lt;/item&gt;&lt;item rend="dt-2"&gt;Long definition&lt;/item&gt;&lt;item rend="dd-2"&gt;Internet users are individuals who have used the Internet (from any location) in the last 3 months. The Internet can be used via a computer, mobile phone, personal digital assistant, games machine, digital TV etc.&lt;/item&gt;&lt;item rend="dt-3"&gt;Statistical concept and methodology&lt;/item&gt;&lt;item rend="dd-3"&gt;The Internet is a world-wide public computer network. It provides access to a number of communication services including the World Wide Web and carries email, news, entertainment and data files, irrespective of the device used (not assumed to be only via a computer - it may also be by mobile phone, PDA, games machine, digital TV etc.). Access can be via a fixed or mobile network. For additional/latest information on sources and country notes, please also refer to: https://www.itu.int/en/ITU-D/Statistics/Pages/stat/default.aspx&lt;/item&gt;&lt;item rend="dt-4"&gt;Development relevance&lt;/item&gt;&lt;item rend="dd-4"&gt;The digital and information revolution has changed the way the world learns, communicates, does business, and treats illnesses. New information and communications technologies (ICT) offer vast opportunities for progress in all walks of life in all countries - opportunities for economic growth, improved health, better service delivery, learning through distance education, and social and cultural advances. Today's smartphones and tablets have computer power equivalent to that of yesterday's computers and provide a similar range of functions. Device convergence is thus rendering the conventional definition obsolete. Comparable statistics on access, use, quality, and affordability of ICT are needed to formulate growth-enabling policies for the sector and to monitor and evaluate the sector's impact on development. Although basic access data are available for many countries, in most developing countries little is known about who uses ICT; what they are used for (school, work, business, research, government); and how they affect people and businesses. The global Partnership on Measuring ICT for Development is helping to set standards, harmonize information and communications technology statistics, and build statistical capacity in developing countries. However, despite significant improvements in the developing world, the gap between the ICT haves and have-nots remains.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Note that many indicators are only available for some countries, for example the indicator "Women who believe a husband is justified in beating his wife when she burns the food" is based on surveys only conducted in lower-developed countries.&lt;/p&gt;&lt;head rend="h2"&gt;Bonus: DOM as a database&lt;/head&gt;&lt;p&gt;Since we’re already running a database in our browser, why not use our browser as a database using a virtual table called &lt;code&gt;dom&lt;/code&gt;?&lt;/p&gt;&lt;code&gt;select count(*) as number_of_demos from dom
  where selector match '.content div.sqlite-httpvfs-demo';
select count(*) as sqlite_mentions from dom
  where selector match '.content p' and textContent like '%SQLite%';&lt;/code&gt;&lt;p&gt;We can even insert elements directly into the DOM:&lt;/p&gt;&lt;code&gt;insert into dom (parent, tagName, textContent)
    select 'ul#outtable1', 'li', short_name
    from wdi_country where currency_unit = 'Euro'&lt;/code&gt;&lt;p&gt;Output:&lt;/p&gt;&lt;p&gt;And update elements in the DOM:&lt;/p&gt;&lt;code&gt;update dom set textContent =
  get_flag("2-alpha_code") || ' ' || textContent
from wdi_country
where selector match 'ul#outtable1 &amp;gt; li'
  and textContent = wdi_country.short_name&lt;/code&gt;&lt;p&gt;Of course, everything here is open source. The main implementation of the sqlite wrapper is in sql.js-httpvfs. The source code of this blog post is a pandoc markdown file, with the demos being a custom "fenced code block" React component.&lt;/p&gt;&lt;head rend="h2"&gt;Update 2023: Future Work&lt;/head&gt;&lt;p&gt;Since I wrote this article in 2021, a lot of interesting things have happened in this space, many inspired by this proof of concept! Here’s some highlights:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;libgen-ipfs: implements a search engine for the Library Genesis based on IPFS and sql.js-httpvfs.&lt;/item&gt;&lt;item&gt;absurd-sql: implements a backend for sql.js (sqlite3 compiled for the web) that treats IndexedDB like a disk and stores data in blocks there. Inspired by this article.&lt;/item&gt;&lt;item&gt;sqlite itself has now added official support for wasm, citing absurd-sql as related work!&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://phiresky.github.io/blog/2021/hosting-sqlite-databases-on-github-pages/"/><published>2025-10-29T15:32:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748195</id><title>The end of the rip-off economy: consumers use LLMs against information asymmetry</title><updated>2025-10-29T17:10:54.550584+00:00</updated><content/><link href="https://www.economist.com/finance-and-economics/2025/10/27/the-end-of-the-rip-off-economy"/><published>2025-10-29T15:32:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748570</id><title>Tell HN: Twilio support replies with hallucinated features</title><updated>2025-10-29T17:10:54.280189+00:00</updated><content>&lt;doc fingerprint="fe03d1565042ad40"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I was investigating some bug with our voice system and asked support where I can find some debugging information and event logs.&lt;/p&gt;
      &lt;p&gt;They told me where I should go in the interface to see that and reassured that they checked logs and this event exist.&lt;/p&gt;
      &lt;p&gt;It turned out these features and information doesn't exist anywhere in the interface and impossible to retrieve in any way. The support message with hallucinated features is mostly AI written.&lt;/p&gt;
      &lt;p&gt;CEOs tell us AGI is around the corner but in reality it just unreliable information and AI can't even restock the vending machine.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45748570"/><published>2025-10-29T15:54:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748608</id><title>AirTips – Alternative to Bento.me/Linktree</title><updated>2025-10-29T17:10:53.615256+00:00</updated><content>&lt;doc fingerprint="7b1933fb1ea4aa9d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;acoffee&lt;/head&gt;
    &lt;p&gt;NO.0002Pro&lt;/p&gt;
    &lt;p&gt;Links and projects by acoffee — compact, restrained, and a bit playful.&lt;/p&gt;
    &lt;p&gt;TIPS&lt;/p&gt;
    &lt;p&gt;12&lt;/p&gt;
    &lt;p&gt;tips received&lt;/p&gt;
    &lt;p&gt; Compact Customizable API‑ready &lt;/p&gt;
    &lt;p&gt; Invite‑only &lt;/p&gt;
    &lt;head rend="h2"&gt;Modules&lt;/head&gt;
    &lt;p&gt;TEXT&lt;/p&gt;
    &lt;head rend="h3"&gt;New note&lt;/head&gt;
    &lt;p&gt;This is a text module.&lt;/p&gt;
    &lt;p&gt;Number&lt;/p&gt;
    &lt;p&gt;1024&lt;/p&gt;
    &lt;head rend="h3"&gt;Custom HTML&lt;/head&gt;
    &lt;p&gt;Build playful, precise UI with pure HTML.&lt;/p&gt;
    &lt;p&gt; ⚡ Compact 🎛️ Customizable 🔗 API‑ready &lt;/p&gt;
    &lt;p&gt;LAYOUT&lt;/p&gt;
    &lt;p&gt;Responsive grids, badges, cards.&lt;/p&gt;
    &lt;p&gt;CONTENT&lt;/p&gt;
    &lt;p&gt;Headings, lists, emojis, inline SVG.&lt;/p&gt;
    &lt;p&gt;SAFE&lt;/p&gt;
    &lt;p&gt;No scripts. Pure, secure markup.&lt;/p&gt;
    &lt;head class="cursor-pointer text-sm font-medium text-slate-800"&gt;See a tiny JSON payload&lt;/head&gt;
    &lt;code&gt;{
    "items": [
      { "kind": "module", "type": "text", "id": "mod-1" },
      { "kind": "module", "type": "image", "id": "mod-2" }
    ]
  }&lt;/code&gt;
    &lt;p&gt;PROJECTS&lt;/p&gt;
    &lt;p&gt;IMAGE&lt;/p&gt;
    &lt;p&gt;Metric1&lt;/p&gt;
    &lt;p&gt;9383&lt;/p&gt;
    &lt;p&gt;Metric2&lt;/p&gt;
    &lt;p&gt;2.6k&lt;/p&gt;
    &lt;p&gt;Metri3&lt;/p&gt;
    &lt;p&gt;1234.5&lt;/p&gt;
    &lt;p&gt;This is a link&lt;/p&gt;
    &lt;p&gt;x.com/becool_me&lt;/p&gt;
    &lt;p&gt;LINKS&lt;/p&gt;
    &lt;p&gt;Custom HTML with Tailwind classes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://a.coffee/"/><published>2025-10-29T15:57:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748725</id><title>Cursor Composer: Building a fast frontier model with RL</title><updated>2025-10-29T17:10:53.004251+00:00</updated><content>&lt;doc fingerprint="3d5aedd9e03a0a1a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Composer: Building a fast frontier model with RL&lt;/head&gt;
    &lt;p&gt;Composer is our new agent model designed for software engineering intelligence and speed. On our benchmarks, the model achieves frontier coding results with generation speed four times faster than similar models.&lt;/p&gt;
    &lt;p&gt;We achieve these results by training the model to complete real-world software engineering challenges in large codebases. During training, Composer is given access to a set of production search and editing tools and tasked with efficiently solving a diverse range of difficult problems. The final result is a large-scale model optimized for high-speed use as an agent in Cursor.&lt;/p&gt;
    &lt;p&gt;Our motivation comes from our experience developing Cursor Tab, our custom completion model. We found that often developers want the smartest model that can support interactive use, keeping them in the flow of coding. In our development process, we experimented with a prototype agent model, codenamed Cheetah, to better understand the impact of faster agent models. Composer is a smarter version of this model that keeps coding delightful by being fast enough for an interactive experience.&lt;/p&gt;
    &lt;p&gt;Composer is a mixture-of-experts (MoE) language model supporting long-context generation and understanding. It is specialized for software engineering through reinforcement learning (RL) in a diverse range of development environments. At each iteration of training, the model is given a problem description and instructed to produce the best response, be it a code edit, a plan, or an informative answer. The model has access to simple tools, like reading and editing files, and also more powerful ones like terminal commands and codebase-wide semantic search.&lt;/p&gt;
    &lt;p&gt;To measure progress, we constructed an evaluation that measures a model's usefulness to a software developer as faithfully as possible. Our benchmark, Cursor Bench, consists of real agent requests from engineers and researchers at Cursor, along with hand-curated optimal solutions to these requests. The resulting evaluation measures not just the agent’s correctness, but also its adherence to a codebase's existing abstractions and software engineering practices.&lt;/p&gt;
    &lt;p&gt;Reinforcement learning allows us to actively specialize the model for effective software engineering. Since response speed is a critical component for interactive development, we incentivize the model to make efficient choices in tool use and to maximize parallelism whenever possible. In addition, we train the model to be a helpful assistant by minimizing unnecessary responses and claims made without evidence. We also find that during RL, the model learns useful behaviors on its own like performing complex searches, fixing linter errors, and writing and executing unit tests.&lt;/p&gt;
    &lt;p&gt;Efficient training of large MoE models requires significant investment into building infrastructure and systems research. We built custom training infrastructure leveraging PyTorch and Ray to power asynchronous reinforcement learning at scale. We natively train our models at low precision by combining our MXFP8 MoE kernels with expert parallelism and hybrid sharded data parallelism, allowing us to scale training to thousands of NVIDIA GPUs with minimal communication cost. Additionally, training with MXFP8 allows us to deliver faster inference speeds without requiring post-training quantization.&lt;/p&gt;
    &lt;p&gt;During RL, we want our model to be able to call any tool in the Cursor Agent harness. These tools allow editing code, using semantic search, grepping strings, and running terminal commands. At our scale, teaching the model to effectively call these tools requires running hundreds of thousands of concurrent sandboxed coding environments in the cloud. To support this workload, we adapted existing infrastructure we built for Background Agents, rewriting our virtual machine scheduler to support the bursty nature and scale of training runs. This enabled seamless unification of RL environments with production environments.&lt;/p&gt;
    &lt;p&gt;Cursor builds tools for software engineering, and we make heavy use of the tools we develop. A motivation of Composer development has been developing an agent we would reach for in our own work. In recent weeks, we have found that many of our colleagues were using Composer for their day-to-day software development. With this release, we hope that you also find it to be a valuable tool.&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;¹ Benchmarked on an internal benchmark in the Cursor tool harness. We group models into classes based on score and report the best model in each class. "Fast Frontier" includes models designed for efficient inference such as Haiku 4.5 and Gemini Flash 2.5. "Best Open" includes recent open weight model releases such as Qwen Coder and GLM 4.6. "Frontier 7/2025" is the best model available in July of this year. "Best Frontier" includes GPT-5 and Sonnet 4.5, which both outperform Composer. For the Tokens per Second calculation, tokens are standardized across models to the latest Anthropic tokenizer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cursor.com/blog/composer"/><published>2025-10-29T16:04:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748756</id><title>Azure major outage: Portal, Front Door and global regions down</title><updated>2025-10-29T17:10:52.720874+00:00</updated><content>&lt;doc fingerprint="be2fbf68eb38ab73"&gt;
  &lt;main&gt;
    &lt;p&gt;Just started ~5 minutes ago: multiple global regions report that Azure is offline — portal won’t load (even cached), Front Door services unreachable, CDN failing — US East/Central, West US, Netherlands, UK, Italy, NZ all reporting issues. Status pages still show green. Heads up to anyone on Azure. Last week AWS, now this.&lt;/p&gt;
    &lt;p&gt;They added a message at the same time as your comment:&lt;/p&gt;
    &lt;p&gt;"We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly."&lt;/p&gt;
    &lt;p&gt;High availability is touted as a reason for their high prices, but I swear I read about major cloud outages far more than I experience any outages at Hetzner.&lt;/p&gt;
    &lt;p&gt;For us, it looks like most services are still working (eastus and eastus2). Our AKS cluster is still running and taking requests. Failures seem limited to management portal.&lt;/p&gt;
    &lt;p&gt;On our end, our VMs are still working, so our gitlab instance is still up. Our services using Azure App Services are available through their provided url. However, Front Door is failing to resolve any domains that it was responsible for.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45748756"/><published>2025-10-29T16:05:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748799</id><title>Azure Outage</title><updated>2025-10-29T17:10:52.103909+00:00</updated><content>&lt;doc fingerprint="7438911025e2a5dd"&gt;
  &lt;main&gt;
    &lt;p&gt;The Internet is supposed to be decentralized. The big three seem to have all the power now (Amazon, Microsoft, and Google) plus Cloudflare/Oracle.&lt;/p&gt;
    &lt;p&gt;How did we get here? Is it because of scale? Going to market in minutes by using someone else's computers instead of building out your own, like co-location or dedicated servers, like back in the day.&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. In addition. customers may experience issues accessing the Azure Portal. Customers can attempt to use programmatic methods (PowerShell, CLI, etc.) to access/utilize resources if they are unable to access the portal directly. We have failed the portal away from Azure Front Door (AFD) to attempt to mitigate the portal access issues and are continuing to assess the situation.&lt;/p&gt;
    &lt;p&gt;We are actively assessing failover options of internal services from our AFD infrastructure. Our investigation into the contributing factors and additional recovery workstreams continues. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:57 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Update: 16:35 UTC:&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:18 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;It's pretty unlikely. AWS published a public 'RCA' https://aws.amazon.com/message/101925/. A race condition in a DNS 'record allocator' causing all DNS records for DDB to be wiped out.&lt;/p&gt;
    &lt;p&gt;I'm simplifying a bit, but I don't think it's likely that Azure has a similar race condition wiping out DNS records on _one_ system than then propagates to all others. The similarity might just end at "it was DNS".&lt;/p&gt;
    &lt;p&gt;That RCA was fun. A distributed system with members that don't know about each other, don't bother with leader elections, and basically all stomp all over each other updating the records. It "worked fine" until one of the members had slightly increased latency and everything cascade-failed down from there. I'm sure there was missing (internal) context but it did not sound like a well-architected system at all.&lt;/p&gt;
    &lt;p&gt;yea I saw that, but im not sure on how accurate that is. a few large apps/companies I know to be 100% on AWS in us-east-1 are cranking along just fine.&lt;/p&gt;
    &lt;p&gt;Yeah, I am guessing it's just a placeholder till they get more info. I thought I saw somewhere that internally within Microsoft it's seen as a "Sev 1" with "all hands on deck" - Annoyingly I can't remember where I saw it, so if someone spots it before I do, please credit that person :D&lt;/p&gt;
    &lt;p&gt;I noticed that Starbucks mobile ordering was down and thought “welp, I guess I’ll order a bagel and coffee on Grubhub”, then GrubHub was down. My next stop was HN to find the common denominator, and y’all did not disappoint.&lt;/p&gt;
    &lt;p&gt;IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door. But it meant that when the system came back online, if the customers card was denied, the customer got free groceries.&lt;/p&gt;
    &lt;p&gt;There's a Family Dollar by my house that is down at least 2 full days per month because of bad inet connectivity. I live close enough that with a small tower on my roof i can get line of sight to theirs. I've thought about offering them a backup link off my home inet if they give me 50% of sales whenever its in use. It would be a pretty good deal for them, better some sales when their inet is down vs none.&lt;/p&gt;
    &lt;p&gt;Does (should, could) DownDetector also say what customer-facing services are down, when some infrastructure is unworking? Or is that the info that the malefactors are seeking?&lt;/p&gt;
    &lt;p&gt;We are very dependent on Azure and Microsoft Authentication and Microsoft 365 and haven’t had weekly or even monthly issues. I can think of maybe three issues this year.&lt;/p&gt;
    &lt;p&gt;That said, I don't hear about GCP outages all that often. I do think AWS might be leading in outages, but that's a gut feeling, I didn't look up numbers.&lt;/p&gt;
    &lt;p&gt;I've been doing it since 1998 in my bedroom with a dual T1 (and on to real DCs later). While I've had some outages for sure it makes me feel better I am not that divergent in uptime in the long run vs big clouds.&lt;/p&gt;
    &lt;p&gt;Pretty much all Azure services seem to be down. Their status page says it's only the portal since 16:00. It would be nice if these mega-companies could update their status page when they take down a large fraction of the Internet and thousands of services that use them.&lt;/p&gt;
    &lt;p&gt;I don't think it's meant to be serious. It's a comment on Microsoft laying off their staff and stuffing their Azure and Dotnet teams with AI product managers.&lt;/p&gt;
    &lt;p&gt;Azure goes down all the time. On Friday we had an entire regional service down all day. Two weeks ago same thing different region. You only hear about it when it's something everyone uses like the portal, because in general nobody uses Azure unless they're held hostage.&lt;/p&gt;
    &lt;p&gt;It is much more than azure. One of my kids needs a key for their laptop and can't reach that either. Great excuse though, 'Azure ate my homework'. What a ridiculous world we are building. Fuck MS and their account requirements for windows.&lt;/p&gt;
    &lt;p&gt;Yeah, I have non prod environments that don't use FD that are functioning. Routing through FD does not work. And a different app, nonprod doesn't use FD (and is working) but loads assets from the CDN (which is not working).&lt;/p&gt;
    &lt;p&gt;FD and CDN are global resources and are experiencing issues. Probably some other global resources as well.&lt;/p&gt;
    &lt;p&gt;Hate to say it, but DNS is looking like it's still the undisputed champ.&lt;/p&gt;
    &lt;p&gt;downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?&lt;/p&gt;
    &lt;p&gt;My best guess at the moment is something global like the CDN is having problems affecting things everywhere. I'm able to use a legacy application we have that goes directly to resources in uswest3, but I'm not able to use our more modern application which uses APIM/CDN networks at all.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45748799"/><published>2025-10-29T16:08:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749017</id><title>Tailscale Peer Relays</title><updated>2025-10-29T17:10:51.949288+00:00</updated><content>&lt;doc fingerprint="66f1d873751076c5"&gt;
  &lt;main&gt;
    &lt;p&gt;Tailscale Peer Relays provides a customer-deployed and managed traffic relaying mechanism. By advertising itself as a peer relay, a Tailscale node can relay traffic for any peer nodes on the tailnet, even for traffic bound to itself. Tailscale Peer Relays can only relay traffic for nodes on your tailnet, and only for nodes that have access to the peer relay. Because they’re managed entirely by the customer, peer relays are less throughput-constrained than Tailscale’s managed DERP relays, and can provide higher throughput connections for traffic to and from locked-down cloud infrastructure, or behind strict network firewalls.&lt;/p&gt;
    &lt;p&gt;In testing with early design partners, we’ve seen throughputs nearing that of a direct connection; often multiple orders of magnitude higher than Tailscale’s managed DERP fleet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moving past hard NAT&lt;/head&gt;
    &lt;p&gt;Over the past few weeks, you’ve heard us talk about improvements we’ve made to Network Address Translation (NAT) traversal techniques, so that Tailscale can establish direct connections wherever possible (hint: it’s over 90% of the time). However, we’ve also outlined places where this isn’t possible or desirable today for a variety of reasons, especially in cloud environments. And, we’ve postulated a bit about where we think the industry is going.&lt;/p&gt;
    &lt;p&gt;While we’ve been keeping your network reliably connected for years with DERP, we’ve heard from customers that the throughput and performance aspects of a QoS-aware managed relay fleet makes deployments in certain environments difficult or untenable. Furthermore, customers have noted that it’s non-trivial to deploy and manage custom DERP fleets (which run as a separate service and binary).&lt;/p&gt;
    &lt;p&gt;DERP provides an incredibly valuable service, setting up reliable connections between Tailscale clients anywhere in the world (including negotiating connections through peer relays). But often, DERP’s focus as a reliability and NAT traversal tool results in performance tradeoffs.&lt;/p&gt;
    &lt;p&gt;By contrast, Tailscale Peer Relays is designed as a performant connectivity tool, and can perform at a level rivaling direct connections. Peer relays can be placed right next to the resources they serve, and peer relays also run on top of UDP, both characteristics beneficial to lower latency and resource overhead. And, they are built into the Tailscale client itself for ease of deployment.&lt;/p&gt;
    &lt;p&gt;We want to move past even more hard NATs, and put Tailscale’s relaying technology in our customers’ hands, so they can use Tailscale at scale, anywhere, with ease. We believe our new Tailscale Peer Relays connectivity option—unique to Tailscale—gives customers the best performance and flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;Peer relays are configured with a single UDP port that must be available to both sides of a connection. Tailscale Peer Relays is built right into the Tailscale client, and can be enabled with a simple command, using the &lt;code&gt;tailscale set --relay-server-port&lt;/code&gt; flag from the Tailscale CLI. Once enabled via the steps in our documentation, clients can connect to infrastructure in hard NAT environments over the peer relay.&lt;/p&gt;
    &lt;p&gt;And don’t worry, we still prefer to fly direct. Tailscale prefers direct connections wherever possible. Clients can then fall back to available peer relays, and finally leverage Tailscale’s managed DERP fleet, or any customer-deployed custom DERPs, to ensure you have connectivity wherever you need it. All of this traffic, over any connection, is still end-to-end encrypted via WireGuard®.&lt;/p&gt;
    &lt;p&gt;Tailscale Peer Relays is designed for the real world, based on the feedback we’ve received from customers and our own hard-earned networking expertise. It allows customers to make just one firewall exception for connections only coming from their tailnet. Peer relays scale across regions, are resilient to real-world network conditions, and graciously fall back to DERP (Tailscale’s or custom). Your network maintains its shape, but gains all kinds of flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;Connectivity, everywhere, at warp speed&lt;/head&gt;
    &lt;p&gt;Customers can now maintain performance benchmarks even where direct connections aren’t possible, by enabling Tailscale Peer Relays to build a deterministic and high-throughput relay topology.&lt;/p&gt;
    &lt;p&gt;We’ve had customers use peer relays to provide access into unmanaged networks, allowing their partners or customers to provide a controllable and auditable connectivity path without sacrificing performance.&lt;/p&gt;
    &lt;p&gt;In strict private networks, customers can build predictable access paths. Tailscale Peer Relays can be placed in public subnets with VPC peering to private subnets, allowing security teams to efficiently segment their private network infrastructure, while enabling networking teams to roll Tailscale out in full-mesh mode across the subnet.&lt;/p&gt;
    &lt;p&gt;Today, customers are using peer relays to establish relayed connections at near-direct speeds across a variety of environments and settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable high-throughput traffic through cloud NATs, like AWS Managed NAT Gateways: Applications and services behind a Managed NAT Gateway can leverage peer relays to relay traffic that can’t establish direct connections.&lt;/item&gt;
      &lt;item&gt;Relay through network firewalls: Workloads running in strictly firewalled environments can predictably expose a single UDP port, limiting the Tailscale surface area and fast-tracking the approval process for firewall exceptions.&lt;/item&gt;
      &lt;item&gt;Offload from Custom and Managed DERP: Minimize data-in-transit through Tailscale‘s managed DERP network, and remove the need for customer-maintained DERP servers.&lt;/item&gt;
      &lt;item&gt;Provide access to locked down customer networks: Data plane traffic can be relayed through predictable endpoints in customer networks, so that they only need to open minimal numbers of ports to facilitate cross network connections.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;It’s not perfect, but we’re getting there&lt;/head&gt;
    &lt;p&gt;Tailscale Peer Relays is available today as a public beta. We’ve yet to establish all the connectivity paths we want to, and there’s still visibility and debugging improvements to work through. However, we’ve reliably seen our early design partners move to peer relay deployments with relative ease, and we’re ready for you to give it a try on your tailnet.&lt;/p&gt;
    &lt;p&gt;Tailscale Peer Relays can be enabled on all plans, including free (it’s our little way of working through the kinks of the modern internet with our customers). All customers can use two peer relays, for free, forever. As your needs scale, so will the number of available peer relays. To add even more peer relays to your tailnet, come have a chat with us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tailscale.com/blog/peer-relays-beta"/><published>2025-10-29T16:21:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749019</id><title>Floss Before Brushing</title><updated>2025-10-29T17:10:51.641138+00:00</updated><content>&lt;doc fingerprint="a689e3519fa44c96"&gt;
  &lt;main&gt;
    &lt;p&gt;A dental hygienist we met recently told us we should floss before brushing instead of after.&lt;/p&gt;
    &lt;p&gt;“No way” was my first reaction. I thought I had my dental hygiene routine down.&lt;/p&gt;
    &lt;p&gt;It turns out she was right. Flossing first loosens and removes food particles and plaque between your teeth that your toothbrush can’t reach.&lt;/p&gt;
    &lt;p&gt;Once the spaces between teeth are clean, toothpaste’s fluoride can reach more surfaces and protect them better.&lt;/p&gt;
    &lt;p&gt;This logic made sense. And it has shown up in studies too (like this one).&lt;/p&gt;
    &lt;p&gt;3 takeaways –&lt;/p&gt;
    &lt;p&gt;(1) It is amazing how you can do something every day for so many years and realize there is a better way to do it.&lt;/p&gt;
    &lt;p&gt;(2) It is more important to floss than worry about the order. So that’s the first habit to nail.&lt;/p&gt;
    &lt;p&gt;(3) And assuming you do, floss before brushing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alearningaday.blog/2025/10/29/floss-before-brushing/"/><published>2025-10-29T16:21:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749161</id><title>AOL to be sold to Bending Spoons for roughly $1.5B</title><updated>2025-10-29T17:10:51.437518+00:00</updated><content/><link href="https://www.axios.com/2025/10/29/aol-bending-spoons-deal"/><published>2025-10-29T16:28:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749690</id><title>Does brand advertising work? Upwave (YC S12) is hiring engineers to answer that</title><updated>2025-10-29T17:10:51.201018+00:00</updated><content>&lt;doc fingerprint="355c16590c952eaa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Senior Software Engineer&lt;/head&gt;
    &lt;p&gt;Upwave: The Brand Outcomes Measurement Platform&lt;/p&gt;
    &lt;p&gt;Upwave is a leading measurement company entirely focused on measuring and optimizing upper funnel campaigns.. The world’s leading advertisers, agencies, and media partners trust Upwave’s robust, AI-driven platform to bring science to the top of the funnel.&lt;/p&gt;
    &lt;p&gt;With Upwave, marketers maximize the effectiveness of brand spend. Upwave measures Brand Lift, validates Brand Reach, and surfaces Brand Optimization opportunities in one, dynamic platform with cross-channel brand measurement for CTV, Digital, Social, Linear, Addressable, Retail Media, Streaming Audio and more.&lt;/p&gt;
    &lt;p&gt;We’re a profitable, growth-stage company backed by leading venture investors (Y Combinator, Uncork Capital, Bloomberg Beta, Initialized Capital, PivotNorth, Ridge Ventures, Industry Ventures, Conductive Ventures,) and leading AdTechfounders &amp;amp; CEOs.&lt;/p&gt;
    &lt;p&gt;We’re a humble but ambitious team that takes its work seriously but never ourselves. Come join us.&lt;/p&gt;
    &lt;p&gt;As a Senior Software Engineer at Upwave, you’ll be a full-stack problem solver with a backend focus—building the APIs, data pipelines, and systems that power our brand measurement platform. Your work will process billions of ad impressions, manage complex data workflows, and deliver insights that inform marketing decisions for the world’s biggest brands.&lt;/p&gt;
    &lt;p&gt;You’ll collaborate across engineering, product, and data science to ship high-impact features end-to-end, scale our platform for the next phase of growth, and help define the next generation of brand measurement.&lt;/p&gt;
    &lt;p&gt;What you will do:&lt;/p&gt;
    &lt;p&gt;Build AI-powered customer experiences — integrate LLMs and advanced causal inference techniques into production workflows that automatically generate data visualizations, synthesize campaign performance into natural language insights, and help enterprise customers understand and optimize their advertising through our AI analyst "Bayes."&lt;/p&gt;
    &lt;p&gt;Design and build scalable backend systems —develop microservices and RESTful APIs that power the analytics platform behind the world’s top brand campaigns.&lt;/p&gt;
    &lt;p&gt;Contribute across the stack — work from backend APIs to Python analytics services to React frontends, delivering complete features that combine sophisticated data analysis with intuitive user experiences.&lt;/p&gt;
    &lt;p&gt;Engineer data pipelines at scale — design and operate systems that process massive volumes of ad and survey data with MySQL, DynamoDB, and AWS (S3, Lambda, EMR, Kinesis Firehose).&lt;/p&gt;
    &lt;p&gt;Improve reliability and performance — deploy services on Kubernetes and AWS, automate deployments via CI/CD, monitor with DataDog and Sentry, and continuously raise the bar for operational excellence&lt;/p&gt;
    &lt;p&gt;Collaborate deeply — work closely with Product and Data Science to productionize statistical models, integrate advanced analytics into customer-facing tools, and bring cutting-edge AI capabilities to enterprise customers.&lt;/p&gt;
    &lt;p&gt;Deliver insights that move millions — enable brand lift analytics and real-time campaign insights by building reliable, high-throughput systems. Multi-million dollar advertising decisions hinge on our recommendations.&lt;/p&gt;
    &lt;p&gt;About you:&lt;/p&gt;
    &lt;p&gt;You’re an experienced engineer (5+ years) who thrives on solving complex problems across APIs, data systems, and distributed infrastructure. You care about clean architecture, reliable systems, and measurable customer impact.&lt;/p&gt;
    &lt;p&gt;You’ve built powerful, intuitive, API-driven products for professional users.. You’re comfortable across the stack, with experience in RDBMS-backed backends using Spring Boot, Django, Rails, or Express, and single-page frontends built in React, Vue, or Angular.&lt;/p&gt;
    &lt;p&gt;You understand and enjoy programming. You’re fluent in the modern landscape of UI frameworks, API and microservice architectures, databases, and cloud platforms—and know when to use the right tool for the job.&lt;/p&gt;
    &lt;p&gt;You embrace modern AI-powered development tools to move faster and code smarter. You use technologies like Claude Code, Cursor, and GitHub CoPilot to automate routine work, explore ideas quickly, and focus your time on higher-value system design and innovation.&lt;/p&gt;
    &lt;p&gt;You value structured software development practices—testing, documentation, CI/CD, and code review—and care about building maintainable systems that scale.&lt;/p&gt;
    &lt;p&gt;You believe developers should operate what they build. You think about observability, cost, and reliability from day one, and design systems that are easy to deploy and maintain. You’ve built in the cloud and know both its power and pitfalls.&lt;/p&gt;
    &lt;p&gt;You like turning ideas into tools that make real customers more effective. You collaborate closely with Product to design features that solve real-world problems and delight users.&lt;/p&gt;
    &lt;p&gt;You mentor others, share knowledge freely, and understand that healthy human systems are the foundation of healthy technical systems. Teammates look to you for guidance.&lt;/p&gt;
    &lt;p&gt;You want to understand how things work and why. You care more about the best idea winning than whose idea it is.&lt;/p&gt;
    &lt;p&gt;You take responsibility, move quickly to fix problems, and take pride in establishing areas of deep expertise in a fast-changing environment.&lt;/p&gt;
    &lt;p&gt;You believe high-trust, inclusive teams outperform individuals. You communicate clearly and compassionately, and contribute to a culture where people enjoy working together.&lt;/p&gt;
    &lt;p&gt;Bonus points:&lt;/p&gt;
    &lt;p&gt;Have worked with modern backend ecosystems like Java/Kotlin/Groovy (Spring Boot or Grails) and know how to design APIs that scale elegantly.&lt;/p&gt;
    &lt;p&gt;Are fluent with data systems such as MySQL, DynamoDB, and Presto, and understand the tradeoffs between relational and NoSQL storage.&lt;/p&gt;
    &lt;p&gt;Have built cloud-native applications on AWS, especially using Kubernetes and Terraform for automation and scalability.&lt;/p&gt;
    &lt;p&gt;Know your way around modern front-end frameworks like React/Redux and enjoy collaborating up and down the stack.&lt;/p&gt;
    &lt;p&gt;Have startup DNA—you’re comfortable with ambiguity, iterate fast, and make pragmatic technical decisions.&lt;/p&gt;
    &lt;p&gt;Bring experience from AdTech, MarTech, or measurement platforms, or are excited to learn how AI and large-scale data intersect in this space.&lt;/p&gt;
    &lt;p&gt;Why You’ll Like Working Here:&lt;/p&gt;
    &lt;p&gt;Engineering-first company: Upwave’s success depends on high-velocity innovation, and we believe high velocity comes from high efficiency, not high effort. We set priorities rather than deadlines, we don’t crunch, we work reasonable hours, and engineers actually take vacations.&lt;/p&gt;
    &lt;p&gt;Modern tech stack: Python analytics, Kotlin/Java APIs, event streaming (100k+ RPM), DynamoDB, Kubernetes, AWS, Terraform, LLM orchestration.&lt;/p&gt;
    &lt;p&gt;Impact at scale: Your code processes billions of advertising events and directly influences multi-million dollar decisions by Fortune 500 brands.&lt;/p&gt;
    &lt;p&gt;Autonomy and ownership: Our engineers lead projects from design through deployment and monitoring.&lt;/p&gt;
    &lt;p&gt;Ambitious but humble culture: We take our work seriously but never ourselves. Upwavers collaborate hard and support each other generously. We screen for people who are both exceptionally talented and genuinely kind.&lt;/p&gt;
    &lt;p&gt;Remote-first team: Our diverse team spans half the globe (but only one half, to ensure everyone can talk live when we need to). We balance synchronous core hours with flexibility to create a work environment that enables both deep collaboration and deep work.&lt;/p&gt;
    &lt;p&gt;Additional Information:&lt;/p&gt;
    &lt;p&gt;The annual base salary range for this role is $150,000 - $175,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for the new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits.&lt;/p&gt;
    &lt;p&gt;Upwave is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.upwave.com/job/8228849002/"/><published>2025-10-29T17:00:15+00:00</published></entry></feed>