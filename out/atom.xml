<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-15T13:40:14.871394+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45226938</id><title>NASA's Guardian Tsunami Detection Tech Catches Wave in Real Time</title><updated>2025-09-15T13:40:22.638398+00:00</updated><content>&lt;doc fingerprint="f724582e4fbe7c29"&gt;
  &lt;main&gt;
    &lt;p&gt;A recent tsunami triggered by a magnitude 8.8 earthquake off Russia’s Kamchatka Peninsula sent pressure waves to the upper layer of the atmosphere, NASA scientists have reported. While the tsunami did not wreak widespread damage, it was an early test for a detection system being developed at the agency’s Jet Propulsion Laboratory in Southern California.&lt;/p&gt;
    &lt;p&gt;Called GUARDIAN (GNSS Upper Atmospheric Real-time Disaster Information and Alert Network), the experimental technology “functioned to its full extent,” said Camille Martire, one of its developers at JPL. The system flagged distortions in the atmosphere and issued notifications to subscribed subject matter experts in as little as 20 minutes after the quake. It confirmed signs of the approaching tsunami about 30 to 40 minutes before waves made landfall in Hawaii and sites across the Pacific on July 29 (local time).&lt;/p&gt;
    &lt;p&gt;“Those extra minutes of knowing something is coming could make a real difference when it comes to warning communities in the path,” said JPL scientist Siddharth Krishnamoorthy.&lt;/p&gt;
    &lt;p&gt;Near-real-time outputs from GUARDIAN must be interpreted by experts trained to identify the signs of tsunamis. But already it’s one of the fastest monitoring tools of its kind: Within about 10 minutes of receiving data, it can produce a snapshot of a tsunami’s rumble reaching the upper atmosphere.&lt;/p&gt;
    &lt;p&gt;The goal of GUARDIAN is to augment existing early warning systems. A key question after a major undersea earthquake is whether a tsunami was generated. Today, forecasters use seismic data as a proxy to predict if and where a tsunami could occur, and they rely on sea-based instruments to confirm that a tsunami is passing by. Deep-ocean pressure sensors remain the gold standard when it comes to sizing up waves, but they are expensive and sparse in locations.&lt;/p&gt;
    &lt;p&gt;“NASA’s GUARDIAN can help fill the gaps,” said Christopher Moore, director of the National Oceanic and Atmospheric Administration Center for Tsunami Research. “It provides one more piece of information, one more valuable data point, that can help us determine, yes, we need to make the call to evacuate.”&lt;/p&gt;
    &lt;p&gt;Moore noted that GUARDIAN adds a unique perspective: It’s able to sense sea surface motion from high above Earth, globally and in near-real-time.&lt;/p&gt;
    &lt;p&gt;Bill Fry, chair of the United Nations technical working group responsible for tsunami early warning in the Pacific, said GUARDIAN is part of a technological “paradigm shift.” By directly observing ocean dynamics from space, “GUARDIAN is absolutely something that we in the early warning community are looking for to help underpin next generation forecasting.”&lt;/p&gt;
    &lt;head rend="h3"&gt;How GUARDIAN works&lt;/head&gt;
    &lt;p&gt;GUARDIAN takes advantage of tsunami physics. During a tsunami, many square miles of the ocean surface can rise and fall nearly in unison. This displaces a significant amount of air above it, sending low-frequency sound and gravity waves speeding upwards toward space. The waves interact with the charged particles of the upper atmosphere — the ionosphere — where they slightly distort the radio signals coming down to scientific ground stations of GPS and other positioning and timing satellites. These satellites are known collectively as the Global Navigation Satellite System (GNSS).&lt;/p&gt;
    &lt;p&gt;Get the JPL Newsletter&lt;/p&gt;
    &lt;p&gt;From Mars to the Milky Way—never miss a discovery! Delivered straight to your inbox.&lt;/p&gt;
    &lt;p&gt;While GNSS processing methods on Earth correct for such distortions, GUARDIAN uses them as clues.&lt;/p&gt;
    &lt;p&gt;The software scours a trove of data transmitted to more than 350 continuously operating GNSS ground stations around the world. It can potentially identify evidence of a tsunami up to about 745 miles (1,200 kilometers) from a given station. In ideal situations, vulnerable coastal communities near a GNSS station could know when a tsunami was heading their way and authorities would have as much as 1 hour and 20 minutes to evacuate the low-lying areas, thereby saving countless lives and property.&lt;/p&gt;
    &lt;p&gt;Key to this effort is the network of GNSS stations around the world supported by NASA’s Space Geodesy Project and Global GNSS Network, as well as JPL’s Global Differential GPS network that transmits the data in real time.&lt;/p&gt;
    &lt;p&gt;The Kamchatka event offered a timely case study for GUARDIAN. A day before the quake off Russia’s northeast coast, the team had deployed two new elements that were years in the making: an artificial intelligence to mine signals of interest and an accompanying prototype messaging system.&lt;/p&gt;
    &lt;p&gt;Both were put to the test when one of the strongest earthquakes ever recorded spawned a tsunami traveling hundreds of miles per hour across the Pacific Ocean. Having been trained to spot the kinds of atmospheric distortions caused by a tsunami, GUARDIAN flagged the signals for human review and notified subscribed subject matter experts.&lt;/p&gt;
    &lt;p&gt;Notably, tsunamis are most often caused by large undersea earthquakes, but not always. Volcanic eruptions, underwater landslides, and certain weather conditions in some geographic locations can all produce dangerous waves. An advantage of GUARDIAN is that it doesn’t require information on what caused a tsunami; rather, it can detect that one was generated and then can alert the authorities to help minimize the loss of life and property.&lt;/p&gt;
    &lt;p&gt;While there’s no silver bullet to stop a tsunami from making landfall, “GUARDIAN has real potential to help by providing open access to this data,” said Adrienne Moseley, co-director of the Joint Australian Tsunami Warning Centre. “Tsunamis don’t respect national boundaries. We need to be able to share data around the whole region to be able to make assessments about the threat for all exposed coastlines.”&lt;/p&gt;
    &lt;p&gt;To learn more about GUARDIAN, visit:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jpl.nasa.gov/news/nasas-guardian-tsunami-detection-tech-catches-wave-in-real-time/"/><published>2025-09-12T21:25:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45228615</id><title>A set of smooth, fzf-powered shell aliases&amp;functions for systemctl</title><updated>2025-09-15T13:40:22.326162+00:00</updated><content>&lt;doc fingerprint="1b6a18422f25c136"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;A set of smooth, fzf-powered shell aliases&amp;amp;functions for &lt;code&gt;systemctl&lt;/code&gt;#&lt;/head&gt;&lt;p&gt;If you've ever found yourself repeatedly typing long systemctl commands or struggling to remember exact service names, this post is for you.&lt;/p&gt;&lt;p&gt;A while ago I implemented a set of shell aliases and functions, and now I can manage my systemd services very smoothly:&lt;/p&gt;&lt;head rend="h2"&gt;Motivation: The Pain Point#&lt;/head&gt;&lt;p&gt;Let's acknowledge a universal sysadmin/developer experience: typing &lt;code&gt;systemctl&lt;/code&gt; over and over, managing long unit names.&lt;/p&gt;&lt;p&gt;Beside, the current shell completion implementation is is quite slow, especially on thin client (for example: Raspberry Pi). I am not familiar with the shell completion, so I am not tended to improve it.&lt;/p&gt;&lt;p&gt;⛺ joehillen/sysz is a TUI for systemd, which is a great inspiration that proves fzf is perfect for this job: Fuzzy completion is much more efficient than prefix completion. It is quite frustrating that:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;sysz is a TUI program; it is not a one-command action, you have to&lt;/p&gt;step-by-step. If you have to start and stop service frequently (for example, when debugging), you have to repeat these steps over and over&lt;/item&gt;&lt;item&gt;&lt;p&gt;sysz is unmaintained since 2022&lt;/p&gt;&lt;code&gt;:'(&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;So I decided to build a custom set of shell functions and aliases that supercharges &lt;code&gt;systemctl&lt;/code&gt; and &lt;code&gt;journalctl&lt;/code&gt; with fuzzy-finding magic for my personal usage.&lt;/p&gt;&lt;head rend="h2"&gt;The Vision: What I Wanted to Build#&lt;/head&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;Core Principle&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;p&gt;Keep it in the shell. No new binaries, just Shell functions and aliases.&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-2"&gt;Desired Features&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-3"&gt;Easy to type:&lt;/item&gt;&lt;item rend="dd-3"&gt;&lt;p&gt;no need to type long command name and unit name&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-4"&gt;Easy to repeat:&lt;/item&gt;&lt;item rend="dd-4"&gt;&lt;p&gt;history operations can be easily performed&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-5"&gt;Easy to maintain:&lt;/item&gt;&lt;item rend="dd-5"&gt;&lt;p&gt;just like any other programming, keep it simple and avoid repetition.&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-6"&gt;Dual Support:&lt;/item&gt;&lt;item rend="dd-6"&gt;&lt;p&gt;seamlessly handle both&lt;/p&gt;&lt;code&gt;--system&lt;/code&gt;(sudo) and&lt;code&gt;--user&lt;/code&gt;units.&lt;/item&gt;&lt;item rend="dt-7"&gt;Error handling:&lt;/item&gt;&lt;item rend="dd-7"&gt;&lt;p&gt;print detailed information when operation failed&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-8"&gt;One for one:&lt;/item&gt;&lt;item rend="dd-8"&gt;&lt;p&gt;each operation corresponds to a command/alias, which is completion friendly constrast to subcommand&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Implementation: Breaking Down the Script#&lt;/head&gt;&lt;head rend="h3"&gt;1. The Basics: Aliases#&lt;/head&gt;&lt;p&gt;We can easily define some extremely short aliases for the long &lt;code&gt;systemctl&lt;/code&gt; and &lt;code&gt;journalctl&lt;/code&gt; commands:&lt;/p&gt;&lt;code&gt;alias s='sudo systemctl'
alias sj='journalctl'
alias u='systemctl --user'
alias uj='journalctl --user'
&lt;/code&gt;&lt;p&gt;Operating on &lt;code&gt;systemctl&lt;/code&gt; unit require root privilege, so a &lt;code&gt;sudo&lt;/code&gt; is required.&lt;/p&gt;&lt;head rend="h3"&gt;2. The Heart: Taming Systemd with FZF#&lt;/head&gt;&lt;p&gt;Using fzf to fuzzy complete can greatly improve the efficiency of inputting SystemD units.&lt;/p&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;&lt;code&gt;systemctl list-units | fzf&lt;/code&gt;&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;p&gt;The output of&lt;/p&gt;&lt;code&gt;systemctl list-units&lt;/code&gt;looks like this:&lt;quote&gt;UNIT LOAD ACTIVE SUB DESCRIPTION ... ... ... ... ... -.mount loaded active mounted Root Mount boot.mount loaded active mounted /boot dev-hugepages.mount loaded active mounted Huge Pages File System dev-mqueue.mount loaded active mounted POSIX Message Queue File System proc-sys-fs-binfmt_misc.mount loaded active mounted Arbitrary Executable File Formats File System run-user-1000-doc.mount loaded active mounted /run/user/1000/doc ... ... ... ... ... Legend: LOAD → Reflects whether the unit definition was properly loaded. ACTIVE → The high-level unit activation state, i.e. generalization of SUB. SUB → The low-level unit activation state, values depend on unit type. 162 loaded units listed. Pass --all to see loaded but inactive units, too. To show all installed unit files use 'systemctl list-unit-files'.&lt;/quote&gt;&lt;p&gt;We can easily write a script like that:&lt;/p&gt;&lt;quote&gt;systemctl list-units --legend=false \ | fzf --accept-nth=1 \ --no-hscroll \ --preview="systemctl status {1}" \ --preview-window=down&lt;/quote&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-2"&gt;&lt;code&gt;--legend=false&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;p&gt;can hide the trailing hints of outputs, but the column is also hidden&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-3"&gt;&lt;code&gt;--accept-nth=1&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-3"&gt;&lt;p&gt;ask fzf only print the first column (aka the unit name) of the select row&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-4"&gt;&lt;code&gt;--preview="... {1}"&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-4"&gt;&lt;p&gt;the&lt;/p&gt;&lt;code&gt;{num}&lt;/code&gt;syntax means pass the numth colmun of highlighting row. We can therefore preview the service status in real-time&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item rend="dt-5"&gt;Merging &lt;code&gt;list-units&lt;/code&gt;and&lt;code&gt;list-unit-files&lt;/code&gt;&lt;/item&gt;&lt;item rend="dd-5"&gt;&lt;p&gt;As&lt;/p&gt;&lt;code&gt;list-units&lt;/code&gt;only list units currently in memory, we usually need to start from the unit that has not yet been loaded, so we also need to list all installed unit files via&lt;code&gt;list-unit-files&lt;/code&gt;:&lt;quote&gt;cat &amp;lt;(systemctl list-units --legend=false) \ &amp;lt;(systemctl list-unit-files --legend=false) \ | fzf --accept-nth=1 \ --no-hscroll \ --preview="systemctl status {1}" \ --preview-window=down&lt;/quote&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-6"&gt;&lt;code&gt;&amp;lt;(systemctl ...)&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-6"&gt;&lt;p&gt;Use the Process Substitution Syntax to merge stdout from multiple&lt;/p&gt;&lt;code&gt;systemctl ...&lt;/code&gt;commands&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item rend="dt-7"&gt;Columnating&lt;/item&gt;&lt;item rend="dd-7"&gt;&lt;p&gt;The above script doesn't work well,&lt;/p&gt;&lt;code&gt;list-units&lt;/code&gt;and&lt;code&gt;list-unit-files&lt;/code&gt;have different output formats: the former one has 5 columns and the latter has 3, which will mess up fzf's UI:&lt;quote&gt;cat &amp;lt;(echo 'UNIT/FILE LOAD/STATE ACTIVE/PRESET SUB DESCRIPTION') \ &amp;lt;(systemctl list-units --legend=false) \ &amp;lt;(systemctl list-unit-files --legend=false) \ | column --table --table-columns-limit=5 \ | sed 's/●/ /' \ | grep . \ | fzf --header-lines=1 \ --accept-nth=1 \ --no-hscroll \ --preview="SYSTEMD_COLORS=1 systemctl status {1}" \ --preview-window=down&lt;/quote&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-8"&gt;&lt;code&gt;echo 'UNIT/FILE ...'&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-8"&gt;&lt;p&gt;a hardcoded table header that can tell the user the meaning of the column&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-9"&gt;&lt;code&gt;column --table ...&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-9"&gt;&lt;p&gt;the&lt;/p&gt;&lt;code&gt;column&lt;/code&gt;command from util-linux can columnate the text and output as a table, set&lt;code&gt;--table-columns-limit&lt;/code&gt;to&lt;code&gt;5&lt;/code&gt;to prevent the "DESCRIPTION" column from being trimmed&lt;/item&gt;&lt;item rend="dt-10"&gt;&lt;code&gt;sed 's/●/ /'&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-10"&gt;&lt;p&gt;to strip the dot ("●") unit state which breaks the colmun&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-11"&gt;&lt;code&gt;grep .&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-11"&gt;&lt;p&gt;to strip the empty line&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-12"&gt;&lt;code&gt;SYSTEMD_COLORS=1&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-12"&gt;&lt;p&gt;force enabled colorful output&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item rend="dt-13"&gt;Reusable for &lt;code&gt;--user&lt;/code&gt;&lt;/item&gt;&lt;item rend="dd-13"&gt;&lt;p&gt;As we want to handle both&lt;/p&gt;&lt;code&gt;--system&lt;/code&gt;and&lt;code&gt;--user&lt;/code&gt;units, we can encapsulate the script to a function:&lt;quote&gt;# SystemD unit selector. _sysls() { WIDE=$1 [ -n "$2" ] &amp;amp;&amp;amp; STATE="--state=$2" cat \ &amp;lt;(echo 'UNIT/FILE LOAD/STATE ACTIVE/PRESET SUB DESCRIPTION') \ &amp;lt;(systemctl $WIDE list-units --quiet $STATE) \ &amp;lt;(systemctl $WIDE list-unit-files --quiet $STATE) \ | sed 's/●/ /' \ | grep . \ | column --table --table-columns-limit=5 \ | fzf --header-lines=1 \ --accept-nth=1 \ --no-hscroll \ --preview="SYSTEMD_COLORS=1 systemctl $WIDE status {1}" \ --preview-window=down } alias sls='_sysls --system' alias uls='_sysls --user'&lt;/quote&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-14"&gt;&lt;code&gt;$1&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-14"&gt;&lt;p&gt;is&lt;/p&gt;&lt;code&gt;--system&lt;/code&gt;or&lt;code&gt;--user&lt;/code&gt;&lt;/item&gt;&lt;item rend="dt-15"&gt;&lt;code&gt;$2&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-15"&gt;&lt;p&gt;is service states, see also&lt;/p&gt;&lt;code&gt;systemctl list-units --state=help&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Then we can use&lt;/p&gt;&lt;code&gt;sls&lt;/code&gt;and&lt;code&gt;uls&lt;/code&gt;to get the full service name by fuzzy matching.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;3. The Complete Function#&lt;/head&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;Error handling&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;p&gt;When performing&lt;/p&gt;&lt;code&gt;systemctl start xxx.service&lt;/code&gt;, if the service does not start successfully, it only tell you to run&lt;code&gt;journalctl -xeu&lt;/code&gt;to see the log:&lt;quote&gt;$ s start docker.service Job for docker.service failed because the control process exited with error code. See "systemctl status docker.service" and "journalctl -xeu docker.service" for details.&lt;/quote&gt;&lt;p&gt;In another situation, if a service immediately dies after launched, systemctl even tells you nothing:&lt;/p&gt;&lt;quote&gt;$ s start getty@foo $ echo $? 0 $ s status getty@foo × getty@foo.service - Getty on foo Loaded: loaded (/usr/lib/systemd/system/getty@.service; disabled; preset: enabled) Active: failed (Result: start-limit-hit) since Fri 2025-09-12 20:44:26 CST; 1s ago ...: ... Sep 12 20:44:26 x1c systemd[1]: ... Sep 12 20:44:26 x1c systemd[1]: Failed to start Getty on foo.&lt;/quote&gt;&lt;p&gt;To help users get detailed service status after launching a service, we can use the following pattern:&lt;/p&gt;&lt;quote&gt;s start foo.service &amp;amp;&amp;amp; s status $_ || sj -xeu $_&lt;/quote&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-2"&gt;&lt;code&gt;A &amp;amp;&amp;amp; B || C&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;p&gt;if A success, performing B, else C&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-3"&gt;&lt;code&gt;$_&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-3"&gt;&lt;p&gt;is the last argument of the previous command, in this case it is "foo.service"&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item rend="dt-4"&gt;Repeatable&lt;/item&gt;&lt;item rend="dd-4"&gt;&lt;p&gt;The key to efficient debugging is repeatability. After fuzzy-selecting and starting a service once, I should be able to simply press the ↑ arrow and Enter to run the exact same command again, without going through the fuzzy selection process every time:&lt;/p&gt;&lt;quote&gt;sstart () { CMD="s start $(sls static,disabled,failed) &amp;amp;&amp;amp; s status \$_ || sj -xeu \$_" eval $CMD [ -n "$BASH_VERSION" ] &amp;amp;&amp;amp; history -s $CMD [ -n "$ZSH_VERSION" ] &amp;amp;&amp;amp; print -s $CMD }&lt;/quote&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-5"&gt;&lt;code&gt;sls static,...&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-5"&gt;&lt;p&gt;pre-filtering services by states, services that need to be "start"-ed must not be in active state, filter by these states can reduce the number of outputs, accelerate the command to some extent 存疑&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-6"&gt;&lt;code&gt;\$_&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-6"&gt;&lt;p&gt;prevent the variable from being expanded before eval&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-7"&gt;&lt;code&gt;history -s&lt;/code&gt;and&lt;code&gt;print -s&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-7"&gt;&lt;p&gt;push the command to history, facilitating subsequent repetition&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;4. The Magic: Dynamic Function Generation#&lt;/head&gt;&lt;p&gt;After implementing &lt;code&gt;sstart&lt;/code&gt;, we also have to implement:&lt;/p&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;&lt;code&gt;sstop&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;p&gt;for&lt;/p&gt;&lt;code&gt;systemctl stop&lt;/code&gt;&lt;/item&gt;&lt;item rend="dt-2"&gt;&lt;code&gt;sre&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;p&gt;for&lt;/p&gt;&lt;code&gt;systemctl restart&lt;/code&gt;&lt;/item&gt;&lt;item rend="dt-3"&gt;&lt;code&gt;ustart&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-3"&gt;&lt;p&gt;for&lt;/p&gt;&lt;code&gt;systemctl --user start&lt;/code&gt;&lt;/item&gt;&lt;item rend="dt-4"&gt;&lt;code&gt;ustop&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-4"&gt;&lt;p&gt;for&lt;/p&gt;&lt;code&gt;systemctl --user stop&lt;/code&gt;&lt;/item&gt;&lt;item rend="dt-5"&gt;&lt;code&gt;ure&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-5"&gt;&lt;p&gt;for&lt;/p&gt;&lt;code&gt;systemctl --user restart&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Repeatedly implementing these functions is tedious and boring. Fortunately, we can dynamically generate them in a loop:&lt;/p&gt;&lt;p&gt;备注&lt;/p&gt;&lt;p&gt;This dynamic generation approach avoids repetitive code but adds some complexity. For clarity, you could instead explicitly define each function.&lt;/p&gt;&lt;code&gt;_SYS_ALIASES=(
    sstart sstop sre
    ustart ustop ure
)
_SYS_CMDS=(
    's start $(sls static,disabled,failed)'
    's stop $(sls running,failed)'
    's restart $(sls)'
    'u start $(uls static,disabled,failed)'
    'u stop $(uls running,failed)'
    'u restart $(uls)'
)

_sysexec() {
    for ((j=0; j &amp;lt; ${#_SYS_ALIASES[@]}; j++)); do
        if [ "$1" == "${_SYS_ALIASES[$j]}" ]; then
            cmd=$(eval echo "${_SYS_CMDS[$j]}") # expand service name
            wide=${cmd:0:1}
            cmd="$cmd &amp;amp;&amp;amp; ${wide} status \$_ || ${wide}j -xeu \$_"
            eval $cmd

            # Push to history.
            [ -n "$BASH_VERSION" ] &amp;amp;&amp;amp; history -s $cmd
            [ -n "$ZSH_VERSION" ] &amp;amp;&amp;amp; print -s $cmd
            return
        fi
    done
}

# Generate bash function/zsh widgets.
for i in ${_SYS_ALIASES[@]}; do
    source /dev/stdin &amp;lt;&amp;lt;EOF
$i() {
    _sysexec $i
}
EOF
done
&lt;/code&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;&lt;code&gt;for ((j=0; j &amp;lt; ...; j++))&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;p&gt;is a bash and zsh compatible&lt;/p&gt;&lt;code&gt;for&lt;/code&gt;loop syntax&lt;/item&gt;&lt;item rend="dt-2"&gt;&lt;code&gt;_sysexec&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;p&gt;a wrapper for dynamically dispatching function&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-3"&gt;&lt;code&gt;source ...&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-3"&gt;&lt;p&gt;a way for generating function dynamically&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Final Thoughts#&lt;/head&gt;&lt;p&gt;Now you can&lt;/p&gt;to start a service. If service is failed, the related logs are print automaticlly. You also can press ↑ to browse commands history to repeat the previous operation.&lt;p&gt;Using this script saved me a lot of unnecessary keystrokes, just an &lt;code&gt;s&lt;/code&gt; gives me more happiness than &lt;code&gt;systemctl&lt;/code&gt;. The fuzzy search algorithm of fzf is good enough that I can get the desired result in one go even with a casual keystroke. It also works well on my Raspberry Pi 3B.&lt;/p&gt;&lt;p&gt;Feel free to grab the script from my dotfiles repository and adapt it to your own workflow. I'd love to hear about your own systemd productivity tricks in the comments!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://silverrainz.me/blog/2025-09-systemd-fzf-aliases.html"/><published>2025-09-13T01:40:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45240146</id><title>Why We Spiral</title><updated>2025-09-15T13:40:21.595158+00:00</updated><content>&lt;doc fingerprint="5d0a0a15efb30cdb"&gt;
  &lt;main&gt;
    &lt;p&gt;Say you’re a senior member of your team at work. You’re 12 minutes late to the weekly staff Zoom. Once you’ve “joined audio,” the first thing you hear is your old friend’s voice. “There you are! So glad you could fit us in.” You laugh and explain the disastrous traffic, difficult drop-off at your kids’ school, or whatever it was that messed up your morning. The moment passes and the conversation moves on. You turn to the job at hand, focused and ready to go.&lt;/p&gt;
    &lt;p&gt;But what if you’re a junior staffer, still feeling your way. Same thing happens: You’re 12 minutes late to the weekly staff Zoom. Once you’ve “joined audio,” the first thing you hear is the boss’s voice. “There you are! So glad you could fit us in.” A few colleagues chuckle. You consider making excuses—about traffic, drop-off, whatever it was—but the moment passes, and the conversation moves on.&lt;/p&gt;
    &lt;p&gt;Your mind doesn’t, though. It’s still ruminating. Was that snark in my boss’s voice? Were they talking about me before I logged on? Do I fit in here? Am I any good at this job? You might not be fully aware of these questions. Your mind works quickly on multiple tracks at the same time. And those questions are nasty; they threaten your sense of belonging, your worth, and your value, at least at work. So you try to push them away, to suppress them. But they’re still there. And once they’ve been triggered, it might feel like the evidence keeps pouring in.&lt;/p&gt;
    &lt;p&gt;Someone makes an inside joke in the chat. You don’t get it. I don’t belong here. Someone rolls their eyes while you’re talking. They don’t respect me. The boss ignores you for the rest of the meeting. No one sees me. Again, these thoughts may not be fully conscious. But there’s no mistaking the fact that your motivation to get back to work has waned by the time you log off. What was it you were supposed to look into?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Was that snark in my boss’s voice? Were they talking about me before I logged on? Do I fit in here? Am I any good at this job?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Next thing you know, you’re idly messing around online when a text comes in from the person who rolled their eyes. “You okay? You seemed out of it at the meeting.” You ignore it. But your mind doesn’t. It’s busy composing possible replies. The full spectrum from passive-aggressive to career imperiling. Eventually you pick up your phone. What will you text back?&lt;/p&gt;
    &lt;p&gt;This is how self-defeating spirals start and how they gather speed. Let’s break down the moving parts:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A circumstance places a big question on the table—about identity, belonging, or adequacy: You’re new at work. You want to succeed and belong, but you wonder . . . That question looms, latent and inactive, but present.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A “bad” thing happens: Your boss is a little snarky.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;That question gets triggered: You read the room for answers, drawing negative inferences from ambiguous evidence. You’re distracted from the task at hand. Your pessimistic hypothesis becomes more entrenched.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You act on that pessimistic hypothesis, making matters worse.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Maybe you send that colleague a snarky text back. And what do you know: When you see them a few days later, they’re cold to you.&lt;/p&gt;
    &lt;p&gt;Now you aren’t talking. Maybe you flub that assignment your boss gave you, and they lose confidence in you. Fast-forward a year and you’re at a new job. Tensions are emerging with the new coworkers. Or are they? How will this story end? Do you have any control over it?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;When a core question is unsettled, it functions like a lens through which you see the world.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, you do. We all do. Negative spirals or feedback loops like these aren’t inevitable. In fact, there are small things we can do both for ourselves and for others to nip them in the bud—and prevent catastrophic outcomes months and years into the future. Better yet, there are ways we can launch positive spirals—dramatically increasing our chances of future happiness, success, and flourishing. The very same processes can either propel us upward or pull us down.&lt;/p&gt;
    &lt;p&gt;To understand how all this is possible, let’s get more precise about sequences like 1–4 above. There are three key concepts at play: “core questions” (number 1), meaning making or “construal” (numbers 2 and 3), and “calcification” (number 4). Think of these as “the three Cs” of spirals—whether positive or negative.&lt;/p&gt;
    &lt;p&gt;Core questions. There are the fundamental questions all of us face, at one time or another. For example: Who am I? Do I belong? Am I enough? I think of these questions as “defining” because they help define you and your life: your sense of self, what relationships you’ll have, and whether you’ll be able to do and be the things you aspire to. There might be long stretches when you don’t think about a given question much because it’s settled for you then. But at critical junctures specific questions flare up, unsettle and preoccupy you. Then they begin to shape what you see and how you act.&lt;/p&gt;
    &lt;p&gt;Construal. It’s natural to think that we have an unfiltered view of the world. That light hits your eyes and you just see what’s out there. But it’s more that we read the world, interpret it, drawing inferences based on what’s already in our heads. We pick up on themes that seem relevant or important to us, not noticing or screening out other details.&lt;/p&gt;
    &lt;p&gt;A friend once told me of an ingenious class demonstration that helped her begin to understand this process. A professor split the class in two and then spoke to the first half alone, telling them of his love for travel and a recent trip to Libya. Next, he spoke to the second half about shopping and how hard it was to find the right size shoe. Last, he brought the class together and said a single word. He asked the students to write it down. Students in the first group wrote, “Tripoli.” Those in the second wrote, “Triple E.”&lt;/p&gt;
    &lt;p&gt;Construal is like a kind of focus. As you look out at the social scene, what snaps to attention? If you’re anything like me, one of the most powerful guides is whatever could pose a risk to you, could threaten you. If you’re walking through a forest where a tiger is said to prowl, you might hear that tiger in every rustle of leaves, see it in every sway of reeds. But in the social world, we don’t all face the same threats. That’s why when you’re new at work and nervous about your place you might hear snark in your boss’s voice, but not if it’s your old friend.&lt;/p&gt;
    &lt;p&gt;When a core question is unsettled for a person, it functions like a lens through which you see the world. We seek answers that can help us resolve that question. Is it true? we ask. Are my doubts and fears well founded? Then, if a “bad” thing happens, it can seem like proof of your negative hypothesis. We aren’t neutral observers on the lookout for evidence one way or the other. We’re in the grip of confirmation bias, attuned to evidence that corroborates our preconceived theory, even if it’s the tiniest thing.&lt;/p&gt;
    &lt;p&gt;Calcification. Calcification happens when our negative thoughts and feelings get entrenched—often as a consequence of our own actions. You have a bad date and think, Am I unlovable? Will I be alone forever? Pretty soon your next date isn’t going well either. Rinse and repeat long enough, and you’re stuck in a romantic rut.&lt;/p&gt;
    &lt;p&gt;When you start to look, you can see spirals everywhere. You fail an important math test. You think you can’t succeed, and stop going to class. You feel sick from a treatment designed to help you overcome an illness. You think it means your illness is especially strong and resistant and so avoid treatment. You have a fight with your kids. You think you’re a “bad parent,” and then yell at them even more the next time. This is self-sabotage, and one step at a time it costs us our achievements, our health, our relationships, and our well-being.&lt;/p&gt;
    &lt;p&gt;Spiraling up&lt;/p&gt;
    &lt;p&gt;Yet if our struggles arise, in part, from the inferences we draw, we have an opportunity. In my work, my colleagues and I identify early moments where people could go one way or the other. By understanding the questions that come up at critical junctures, we can offer people better ways to think through challenges—ways that can help them spiral up, instead of down.&lt;/p&gt;
    &lt;p&gt;That’s what we call “wise” interventions: graceful ways to offer people good answers to the questions that define our lives. It sure can seem like magic that 21 minutes could improve marriage a year later; that a one-page letter could keep kids out of jail; that a string of postcards could cut suicide rates by half over two years; or that an hour-long reflection on belonging in the first year of college could improve life satisfaction and career success a decade later. But this—this is ordinary magic.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Negative spirals or feedback loops aren’t inevitable. There are things we can do both for ourselves and for others to nip them in the bud.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In my first year of college, I was biking back through campus one lovely fall day when I saw a large group of fellow students gathered enthusiastically around a truck from the California burger chain In-N-Out. Maybe they craved a taste of home. But in Michigan, where I was from, there are no In-N-Outs. I’d never heard of it. Feeling excluded from the burger party, I biked off in a huff to eat my lunch in the dining hall alone. I remember thinking, I’m not standing in line for a burger!&lt;/p&gt;
    &lt;p&gt;What was my problem?&lt;/p&gt;
    &lt;p&gt;As an 18-year-old, I certainly didn’t want to think of myself as feeling that I did not belong in college. And I definitely didn’t want to think that an In-N-Out truck could trigger that feeling. How ridiculous that would be. Who thinks they don’t belong because of a burger truck?&lt;/p&gt;
    &lt;p&gt;It was ridiculous. After my brother experienced a particularly mysterious romantic disaster, it’s something we christened a “tifbit”—tiny fact, big theory. Of course not knowing about In-N-Out didn’t mean I didn’t belong in college. But that’s the point. For looking back now, I know the truth is I was homesick. I felt so far from home and all the people I knew and loved. So I wondered, Will I make friends in California? Will I fit in? Seeing all those classmates crowded together, eager to get lunch from a place I’d never even heard of, just triggered those anxieties.&lt;/p&gt;
    &lt;p&gt;With wisdom and kindness and a little distance, we can laugh at ourselves in situations like these. But we should pay attention. For beneath every tifbit is a real question, and it’s almost always a reasonable one. Big responses to small experiences can help us see what lies beneath the surface. For a tifbit is never just a tiny fact. It’s a clue to the bigger questions that define our lives.&lt;/p&gt;
    &lt;p&gt;With a little prompt, I could have known that almost everyone feels homesick at first in college, that we’re all in some sense far from home, even the kids from California, that everyone was trying to find new communities. Maybe then I would have joined the line at the In-N-Out truck. I could have asked someone to tell me what In-N-Out was. Why do they love it? What is “animal style”?&lt;/p&gt;
    &lt;p&gt;I’m sure they would have been glad to share. I know I would have had a better lunch. And maybe I would have made a friend, too.&lt;/p&gt;
    &lt;p&gt;Excerpted from Ordinary Magic copyright © 2025 by Gregory M. Walton. Used by permission of Harmony Books, an imprint of Random House, a division of Penguin Random House LLC, New York. All rights reserved. No part of this excerpt may be reproduced or reprinted without permission in writing from the publisher.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://behavioralscientist.org/why-we-spiral/"/><published>2025-09-14T14:46:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45240682</id><title>Writing an operating system kernel from scratch</title><updated>2025-09-15T13:40:21.266624+00:00</updated><content>&lt;doc fingerprint="b60e921211a1b64c"&gt;
  &lt;main&gt;
    &lt;p&gt;I recently implemented a minimal proof of concept time-sharing operating system kernel on RISC-V. In this post, I’ll share the details of how this prototype works. The target audience is anyone looking to understand low-level system software, drivers, system calls, etc., and I hope this will be especially useful to students of system software and computer architecture.&lt;/p&gt;
    &lt;p&gt;This is a redo of an exercise I did for my undergraduate course in operating systems, and functionally it should resemble a typical operating systems project. However, this experiment focuses on modern tooling, as well as the modern architecture of RISC-V. RISC-V is an amazing technology that is easy to understand more quickly than other CPU architectures, while remaining a popular choice for many new systems, not just an educational architecture.&lt;/p&gt;
    &lt;p&gt;Finally, to do things differently here, I implemented this exercise in Zig, rather than traditional C. In addition to being an interesting experiment, I believe Zig makes this experiment much more easily reproducible on your machine, as it’s very easy to set up and does not require any installation (which could otherwise be slightly messy when cross-compiling to RISC-V).&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of contents&lt;/head&gt;
    &lt;head&gt;Open Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;GitHub repo&lt;/head&gt;
    &lt;p&gt;The final code for this experiment is on GitHub here. We’ll be referencing the code from it as we go.&lt;/p&gt;
    &lt;p&gt;GitHub should be the source of truth and may be slightly out of sync with the code below.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recommended reading&lt;/head&gt;
    &lt;p&gt;The basic fundamentals of computer engineering and specifically computer architecture are assumed. Specifically, knowledge of registers, how the CPU addresses memory, and interrupts is all necessary.&lt;/p&gt;
    &lt;p&gt;Before diving deep into this experiment, it’s recommended to also review the following background texts:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Bare metal programming on RISC-V&lt;/item&gt;
      &lt;item&gt;RISC-V boot process with SBI&lt;/item&gt;
      &lt;item&gt;RISC-V interrupts with a timer example&lt;/item&gt;
      &lt;item&gt;Optional - Making a micro Linux distro - mainly for the brief philosophy on the kernel / user space split&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Unikernel&lt;/head&gt;
    &lt;p&gt;We’ll be developing a type of unikernel. Simply put, this setup links the application code directly with the OS kernel it depends on. Essentially, everything is bundled into a single binary executable, and the user code is loaded into memory alongside the kernel.&lt;/p&gt;
    &lt;p&gt;This bypasses the need to separately load the user code at runtime, which is a complex field in itself (involving linkers, loaders, etc.).&lt;/p&gt;
    &lt;head rend="h2"&gt;SBI layer&lt;/head&gt;
    &lt;p&gt;RISC-V supports a layered permissions model. The system boots into machine mode (M), which is completely bare-metal, and then supports a couple of other less privileged modes. Please check the background texts for more details; below is a quick summary:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;M-mode can do pretty much anything; it is fully bare-metal.&lt;/item&gt;
      &lt;item&gt;In the middle is S-mode, supervisor, which typically hosts the operating system kernel.&lt;/item&gt;
      &lt;item&gt;At the bottom is U-mode, user, where application code runs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lower privilege levels can send requests to higher privilege levels.&lt;/p&gt;
    &lt;p&gt;We’ll assume that at the bottom of our software stack is an SBI layer, specifically OpenSBI. Please study this text for the necessary background, as we’ll use the SBI layer to manage console printing and control the timer hardware. While manual implementation is possible, I wanted to add more value to this text by demonstrating a more portable approach with OpenSBI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Goal for the kernel&lt;/head&gt;
    &lt;p&gt;We want to support a few key features for simplicity:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Statically define threads ahead of execution; i.e., dynamic thread creation is not supported. Additionally, for simplicity, threads are implemented as never-ending functions.&lt;/item&gt;
      &lt;item&gt;Threads operate in user mode and are able to send system calls to the kernel operating in S-mode.&lt;/item&gt;
      &lt;item&gt;Time is sliced and allocated among different threads. The system timer will be set to tick every couple of milliseconds, at which point a thread may be switched out.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally, development is targeted for a single-core machine.&lt;/p&gt;
    &lt;head rend="h2"&gt;Virtualization and what exactly is a thread&lt;/head&gt;
    &lt;p&gt;Before implementing threads, we should decide what they really are. The concept of threads in a time-sharing environment enables multiple workloads to run on a single core (as noted above, we’re focusing on single-core machines), while the programming model for each thread remains largely the same as if it were the sole software on the machine. This is a loose definition, which we will refine.&lt;/p&gt;
    &lt;p&gt;To understand time-sharing, let’s briefly consider its contrast: cooperative scheduling/threading. In cooperative scheduling/threading, a thread voluntarily yields CPU time to another workload. Eventually, the expectation is that another thread will yield control back to the first.&lt;/p&gt;
    &lt;code&gt;function thread():
  operation_1();
  operation_2();
  YIELD();
  operation_3();
  YIELD();
  ...&lt;/code&gt;
    &lt;p&gt;To be clear, this isn’t an “outdated” technique, despite being older. In fact, it’s alive and well in many modern programming languages and their runtimes (often abstracted from programmers). One good example is Go, which uses Goroutines to run multiple workloads on top of one operating system thread. While programmers don’t necessarily add explicit yield operations, the compiler and runtime can inject them into the workload.&lt;/p&gt;
    &lt;p&gt;Now, it should be clearer what it means for the programming model to remain largely the same in a time-sharing context. The thread would naturally look like this:&lt;/p&gt;
    &lt;code&gt;function thread():
  operation_1();
  operation_2();
  operation_3();
  ...&lt;/code&gt;
    &lt;p&gt;There are simply no explicit yield operations; instead, the kernel utilizes timers and interrupts to seamlessly switch between threads on the same core. This is precisely what we’ll implement in this experiment.&lt;/p&gt;
    &lt;p&gt;When multiple workloads run on the same resource, and each retains the same programming model as if it were the only workload, we can say the resource is virtualized. In other words, if we’re running 5 threads on the same core, each thread “feels” like it has its own core, effectively running on 5 little cores instead of 1 big core. More formally, each thread retains its own view of the core’s architectural registers (in RISC-V, &lt;code&gt;x0-x31&lt;/code&gt; and some CSRs, more on this below) and… some memory! Let’s look deeper into that.&lt;/p&gt;
    &lt;head rend="h3"&gt;The stack and memory virtualization&lt;/head&gt;
    &lt;p&gt;To begin, a thread has its own stack for reasons we’ll analyze shortly. The rest of the memory is “shared” with other threads, but this requires further investigation.&lt;/p&gt;
    &lt;p&gt;It’s important to understand that hardware virtualization exists on a spectrum, rather than as a few rigid options. Here are some of the options for virtualization:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Threads: virtualizes architectural registers and stacks, but not much else; i.e., different threads can share data elsewhere in memory.&lt;/item&gt;
      &lt;item&gt;Process: more heavyweight than threads, memory is virtualized such that each process “feels” like it has a dedicated CPU core and its own memory untouchable by other processes; additionally, a process houses multiple threads.&lt;/item&gt;
      &lt;item&gt;Container: virtualizes even more - each container has its own filesystem and potentially its own set of network interfaces; containers share the same kernel and underlying hardware.&lt;/item&gt;
      &lt;item&gt;VM: virtualizes everything.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are many more shades in between, and each of these options likely has different subtypes. The point here is that all these approaches enable running different workloads with varying isolations, or more intuitively, different views of the machine and their environment.&lt;/p&gt;
    &lt;p&gt;Interestingly, if you examine the Linux kernel source code, you won’t find a construct explicitly called a container. What we popularly call containers isn’t a mechanism baked into the kernel, but rather a set of kernel mechanisms used together to form a specific view of the environment for our workload. For example, the &lt;code&gt;chroot&lt;/code&gt; mechanism restricts filesystem visibility, while &lt;code&gt;cgroups&lt;/code&gt; impose limits on workloads; together, these form what we call a container.&lt;/p&gt;
    &lt;p&gt;Furthermore, I believe (though don’t quote me on this) that the boundaries between threads and processes in Linux are somewhat blurred. To the best of my knowledge, both are implemented on top of tasks in the kernel, but when creating a task, the API allows different restrictions to be specified.&lt;/p&gt;
    &lt;p&gt;Ultimately, this is all to say that we’re always defining a workload with varying restrictions on what it can see and access. When and why to apply different restrictions is a topic for another day. Many questions arise when writing an application, ranging from the difficulty of an approach to its security.&lt;/p&gt;
    &lt;head rend="h3"&gt;Virtualizing a thread&lt;/head&gt;
    &lt;p&gt;In this experiment, we’ll implement minimal virtualization with very basic, time-sharing threads. Therefore, the goals are the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The programming model for a thread should remain mostly untouched. As long as a thread doesn’t interact with memory contents used by other threads, its programming model should remain consistent, powered by time-sharing.&lt;/item&gt;
      &lt;item&gt;A thread should have its own protected view of architectural registers, including some RISC-V CSRs.&lt;/item&gt;
      &lt;item&gt;A thread should be assigned its own stack.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It should be obvious why a thread needs its own view of the registers. If other threads could freely touch a thread’s registers, the thread wouldn’t be able to do any meaningful work. All (I believe) RISC-V instructions work with at least one register, so protecting a thread’s register view is essential.&lt;/p&gt;
    &lt;p&gt;Furthermore, assigning a private stack to a thread is necessary, though slightly less obvious. The answer is that different stacks are needed to manage different execution contexts. Namely, when a function is invoked, by convention, the stack is used to allocate function-private variables. Additionally, registers like &lt;code&gt;ra&lt;/code&gt; can be pushed to the stack to retain the correct return address from a function (in case another function is invoked within it). In short, there are various reasons, per RISC-V convention, why the stack is needed to maintain the execution context. The details of RISC-V calling conventions will not be described here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interrupt context&lt;/head&gt;
    &lt;p&gt;It’s crucial to understand how interrupt code runs and what it should consist of, as this mechanism will be heavily exploited to achieve seamless time-sharing between threads. For a detailed, practical example, please check out this past text.&lt;/p&gt;
    &lt;p&gt;I’ll briefly include the assembly for the timer interrupt routine from that text:&lt;/p&gt;
    &lt;code&gt;s_mode_interrupt_handler:
        addi    sp,sp,-144
        sd      ra,136(sp)
        sd      t0,128(sp)
        sd      t1,120(sp)
        sd      t2,112(sp)
        sd      s0,104(sp)
        sd      a0,96(sp)
        sd      a1,88(sp)
        sd      a2,80(sp)
        sd      a3,72(sp)
        sd      a4,64(sp)
        sd      a5,56(sp)
        sd      a6,48(sp)
        sd      a7,40(sp)
        sd      t3,32(sp)
        sd      t4,24(sp)
        sd      t5,16(sp)
        sd      t6,8(sp)
        addi    s0,sp,144
        call    clear_timer_pending_bit
        call    set_timer_in_near_future
        li      a1,33
        lla     a0,.LC0
        call    debug_print
        nop
        ld      ra,136(sp)
        ld      t0,128(sp)
        ld      t1,120(sp)
        ld      t2,112(sp)
        ld      s0,104(sp)
        ld      a0,96(sp)
        ld      a1,88(sp)
        ld      a2,80(sp)
        ld      a3,72(sp)
        ld      a4,64(sp)
        ld      a5,56(sp)
        ld      a6,48(sp)
        ld      a7,40(sp)
        ld      t3,32(sp)
        ld      t4,24(sp)
        ld      t5,16(sp)
        ld      t6,8(sp)
        addi    sp,sp,144
        sret&lt;/code&gt;
    &lt;p&gt;This assembly was obtained by writing a C function tagged as an S-level interrupt in RISC-V. With this tag, the GCC compiler knew how to generate the prologue and epilogue of the interrupt routine. The prologue preserves architectural registers on the stack, and the epilogue recovers them (in addition to specifically returning from S-mode). All of this was generated by correctly tagging the C function’s invoking convention.&lt;/p&gt;
    &lt;p&gt;This somewhat resembles function calling, and that’s essentially what it is. Interrupts can be thought of (in a very simplified sense) as functions invoked by some system effect. Consequently, utilized registers must be carefully preserved on the stack and then restored at the routine’s exit; otherwise, asynchronous interrupts like timer interrupts would randomly corrupt architectural register values, completely blocking any practical software from running!&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation (high-level)&lt;/head&gt;
    &lt;p&gt;We’ll explore the implementation by first describing the high-level idea and then digging into the code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leveraging the interrupt stack convention&lt;/head&gt;
    &lt;p&gt;Adding an interrupt is, in a way, already introducing a form of threading to your application code. In a system with a timer interrupt, the main application code runs, which can occasionally be interleaved with instances of timer interrupt invocations. The core jumps to this interrupt routine when the timer signals, and it carefully restores the architectural state before control flow returns to the “main thread”. There are two control flows running concurrently here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Main application code.&lt;/item&gt;
      &lt;item&gt;Repetitions of the interrupt routine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This interleaving of the timer interrupt can be leveraged to implement additional control flows, and the main idea is outlined below.&lt;/p&gt;
    &lt;p&gt;The core of the interrupt routine is sandwiched between the prologue and the epilogue. That’s where the interrupt is serviced before control returns to the main application thread by restoring registers from the stack.&lt;/p&gt;
    &lt;p&gt;However, why must we restore the registers from the same stack location? If our interrupt logic swaps the stack pointer to some other piece of memory, we’ll end up with a different set of architectural register values recovered, thus entering a whole different flow. In other words, we achieve a context switch, and this is precisely how it’s implemented in this experiment. We’ll see the code for it shortly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Kernel/user space separation&lt;/head&gt;
    &lt;p&gt;We can now delineate the kernel space and user space. With RISC-V, this naturally translates to kernel code running in supervisor (S) mode and user space code running in U-mode.&lt;/p&gt;
    &lt;p&gt;The machine boots into machine (M) mode, and since we want to leverage the SBI layer, we’ll allow OpenSBI to run there. Then, the kernel will perform some initial setup in S-mode before starting the U-mode execution of user space threads. Periodic timer interrupts will enable context switches, and the interrupt code will execute in S-mode. Finally, user threads will be able to make system calls to the kernel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation (code)&lt;/head&gt;
    &lt;p&gt;Please refer to the GitHub repository for the full code; we will only cover core excerpts below.&lt;/p&gt;
    &lt;head rend="h3"&gt;Assembly startup&lt;/head&gt;
    &lt;p&gt;As usual, a short assembly snippet is needed to start our S-mode code and enter the “main program” in Zig. This is in &lt;code&gt;startup.S&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;...
done_bss:

    # Jump to Zig main
    call main
...&lt;/code&gt;
    &lt;p&gt;The rest of the assembly startup primarily involves cleaning up the BSS section and setting up the stack pointer for the initial kernel code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Main kernel file and I/O drivers&lt;/head&gt;
    &lt;p&gt;We’ll now examine &lt;code&gt;kernel.zig&lt;/code&gt;, which contains the &lt;code&gt;main&lt;/code&gt; function.&lt;/p&gt;
    &lt;p&gt;First, we probe the OpenSBI layer for console capabilities. We’ll only consider running on a relatively recent version of OpenSBI (from the last few years) that includes console capability. Otherwise, the kernel will halt and report an error.&lt;/p&gt;
    &lt;code&gt;export fn main() void {
    const initial_print_status = sbi.debug_print(BOOT_MSG);

    if (initial_print_status.sbi_error != 0) {
        // SBI debug console not available, fall back to direct UART
        const error_msg = "ERROR: OpenSBI debug console not available! You need the latest OpenSBI.\n";
        const fallback_msg = "Falling back to direct UART at 0x10000000...\n";

        uart.uart_write_string(error_msg);
        uart.uart_write_string(fallback_msg);
        uart.uart_write_string("Stopping... We rely on OpenSBI, cannot continue.\n");

        while (true) {
            asm volatile ("wfi");
        }

        unreachable;
    }&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;main&lt;/code&gt; is marked as &lt;code&gt;export&lt;/code&gt; to conform to the C ABI.&lt;/p&gt;
    &lt;p&gt;Here, we have a lightweight implementation of a couple of I/O drivers. As you can see, writing can occur in one of two ways: either we go through the SBI layer (&lt;code&gt;sbi.zig&lt;/code&gt;) or, if that fails, we use direct MMIO (&lt;code&gt;uart_mmio.zig&lt;/code&gt;). The SBI method should theoretically be more portable, as it delegates output management details to the M-level layer (essentially what we do with MMIO), freeing us from concerns about exact memory space addresses.&lt;/p&gt;
    &lt;p&gt;Let’s quickly look at &lt;code&gt;sbi.zig&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// Struct containing the return status of OpenSBI
pub const SbiRet = struct {
    sbi_error: isize,
    value: isize,
};

pub fn debug_print(message: []const u8) SbiRet {
    var err: isize = undefined;
    var val: isize = undefined;

    const msg_ptr = @intFromPtr(message.ptr);
    const msg_len = message.len;

    asm volatile (
        \\mv a0, %[len]
        \\mv a1, %[msg]
        \\li a2, 0
        \\li a6, 0x00
        \\li a7, 0x4442434E
        \\ecall
        \\mv %[err], a0
        \\mv %[val], a1
        : [err] "=r" (err),
          [val] "=r" (val),
        : [msg] "r" (msg_ptr),
          [len] "r" (msg_len),
        : .{ .x10 = true, .x11 = true, .x12 = true, .x16 = true, .x17 = true, .memory = true });

    return SbiRet{
        .sbi_error = err,
        .value = val,
    };
}&lt;/code&gt;
    &lt;p&gt;This is very straightforward; we’re simply performing the system call exactly as described in the OpenSBI documentation. Note that when I first wrote this code, I wasn’t fully familiar with Zig’s error handling capabilities, hence the somewhat non-idiomatic error handling.&lt;/p&gt;
    &lt;p&gt;However, this can be considered a first driver in this kernel, as it directly manages output to the device.&lt;/p&gt;
    &lt;p&gt;Next is &lt;code&gt;uart_mmio.zig&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// UART MMIO address (standard for QEMU virt machine)
pub const UART_BASE: usize = 0x10000000;
pub const UART_TX: *volatile u8 = @ptrFromInt(UART_BASE);

// Direct UART write function (fallback when SBI is not available)
pub fn uart_write_string(message: []const u8) void {
    for (message) |byte| {
        UART_TX.* = byte;
    }
}&lt;/code&gt;
    &lt;p&gt;This is straightforward and self-explanatory.&lt;/p&gt;
    &lt;p&gt;Returning to &lt;code&gt;kernel.zig&lt;/code&gt; and the &lt;code&gt;main&lt;/code&gt; function, we create 3 user threads, each printing a slightly different message (the thread ID is the varying bit). At this point, the kernel setup is almost complete.&lt;/p&gt;
    &lt;p&gt;The final steps involve setting up and running the timer interrupt. Once that is done, kernel code will only run when the timer interrupts the system or when user space code requests a system call.&lt;/p&gt;
    &lt;code&gt;interrupts.setup_s_mode_interrupt(&amp;amp;s_mode_interrupt_handler);
_ = timer.set_timer_in_near_future();
timer.enable_s_mode_timer_interrupt();&lt;/code&gt;
    &lt;p&gt;We could request a context switch immediately, but for simplicity, we’ll wait until the timer activates and begins the actual work in the system.&lt;/p&gt;
    &lt;head rend="h3"&gt;S-mode handler and the context switch&lt;/head&gt;
    &lt;p&gt;While the Zig compiler could generate the adequate prologue and epilogue for our S-mode handler, we will do it manually. The reason is that we also want to capture some CSRs in the context that otherwise wouldn’t have been captured by the generated routine.&lt;/p&gt;
    &lt;p&gt;That’s why we use the &lt;code&gt;naked&lt;/code&gt; calling convention in Zig. This forces us to write the entire function in assembly, though a quick escape hatch to this limitation is to call a Zig function whenever Zig logic is needed.&lt;/p&gt;
    &lt;p&gt;I won’t copy paste the whole prologue and epilogue here because they are very similar to what was done in the previous C experiment with RISC-V interrupts. Instead, I’ll just focus on the bit that is different:&lt;/p&gt;
    &lt;code&gt;...
        // Save S-level CSRs (using x5 as a temporary register)
        \\csrr x5, sstatus
        \\sd x5, 240(sp)
        \\csrr x5, sepc
        \\sd x5, 248(sp)
        \\csrr x5, scause
        \\sd x5, 256(sp)
        \\csrr x5, stval
        \\sd x5, 264(sp)

        // Call handle_kernel
        \\mv a0, sp
        \\call handle_kernel
        \\mv sp, a0

        // Epilogue: Restore context
        // Restore S-level CSRs (using x5 as a temporary register)
        \\ld x5, 264(sp)
        \\csrw stval, x5
        \\ld x5, 256(sp)
        \\csrw scause, x5
        \\ld x5, 248(sp)
        \\csrw sepc, x5
        \\ld x5, 240(sp)
        \\csrw sstatus, x5
...&lt;/code&gt;
    &lt;p&gt;As you can see, a couple more registers were added to the prologue and epilogue in addition to the core architectural registers.&lt;/p&gt;
    &lt;p&gt;Next, within this prologue/epilogue sandwich, we invoke the &lt;code&gt;handle_kernel&lt;/code&gt; Zig function. This routes to the correct logic based on whether the interrupt source is a synchronous system call from user space or an asynchronous timer interrupt. The reason is that we land in the same S-level interrupt routine regardless of the interrupt source, and then we inspect the &lt;code&gt;scause&lt;/code&gt; CSR for details.&lt;/p&gt;
    &lt;p&gt;To successfully work with the &lt;code&gt;handle_kernel&lt;/code&gt; function, we need to be aware of the assembly-level calling conventions. This function takes a single integer parameter and returns a single integer parameter. Since the function signature is small, it works as simply as this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The sole function parameter is passed through the &lt;code&gt;a0&lt;/code&gt;architectural register.&lt;/item&gt;
      &lt;item&gt;The same register also holds the function’s result upon return.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is pretty easy. Let’s quickly look at the signature of this function:&lt;/p&gt;
    &lt;code&gt;export fn handle_kernel(current_stack: usize) usize {
...&lt;/code&gt;
    &lt;p&gt;It is slightly awkward but gets the job done. The input to this Zig logic is the stack top before invoking the Zig logic (which inevitably leads to some data added to the stack). The function’s output is where the stack top should be after the Zig logic is done. If it differs from the input, then we’re performing a context switch. If it’s the same, the same workload thread will continue running after the interrupt.&lt;/p&gt;
    &lt;p&gt;The rest of the logic is very simple. It inspects the interrupt source (system call from user space or timer interrupt) and performs accordingly.&lt;/p&gt;
    &lt;p&gt;In the case of a timer interrupt, a context switch is performed. The &lt;code&gt;schedule&lt;/code&gt; function from &lt;code&gt;scheduling.zig&lt;/code&gt; is invoked, and it potentially returns the other stack we should switch to:&lt;/p&gt;
    &lt;code&gt;const build_options = @import("build_options");
const sbi = @import("sbi");
const std = @import("std");
const thread = @import("thread");

pub fn schedule(current_stack: usize) usize {
    const maybe_current_thread = thread.getCurrentThread();

    if (maybe_current_thread) |current_thread| {
        current_thread.sp_save = current_stack;

        if (comptime build_options.enable_debug_logs) {
            _ = sbi.debug_print("[I] Enqueueing the current thread\n");
        }
        thread.enqueueReady(current_thread);
    } else {
        if (comptime build_options.enable_debug_logs) {
            _ = sbi.debug_print("[W] NO CURRENT THREAD AVAILABLE!\n");
        }
    }

    const maybe_new_thread = thread.dequeueReady();

    if (maybe_new_thread) |new_thread| {
        // TODO: software interrupt to yield to the user thread

        if (comptime build_options.enable_debug_logs) {
            _ = sbi.debug_print("Yielding to the new thread\n");
        }

        thread.setCurrentThread(new_thread);

        if (comptime build_options.enable_debug_logs) {
            var buffer: [256]u8 = undefined;
            const content = std.fmt.bufPrint(&amp;amp;buffer, "New thread ID: {d}, stack top: {x}\n", .{ new_thread.id, new_thread.sp_save }) catch {
                return 0; // Return bogus stack, should be more robust in reality
            };
            _ = sbi.debug_print(content);
        }

        return new_thread.sp_save;
    }

    _ = sbi.debug_print("NO NEW THREAD AVAILABLE!\n");

    while (true) {
        asm volatile ("wfi");
    }
    unreachable;
}&lt;/code&gt;
    &lt;p&gt;The code from the &lt;code&gt;thread&lt;/code&gt; module is very simple, serving as boilerplate for a basic queue that manages structs representing threads. I won’t copy it here, as it’s mostly AI-generated. It is important to note, however, that the stacks are statically allocated in memory, and the maximum number of running threads is hardcoded.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;thread&lt;/code&gt; module also includes logic for setting up a new thread. This is where data is pushed onto the stack before the thread even runs. If you wonder why, it’s because when returning from the S-level trap handler, we need something on the stack to indicate where to go. The initial data does precisely that. We can seed the initial register values here as desired. In fact, in this experiment, we demonstrate passing a single integer parameter to the thread function by seeding the &lt;code&gt;a0&lt;/code&gt; register value (per calling convention) on the stack, which the thread function can then use immediately.&lt;/p&gt;
    &lt;head rend="h3"&gt;The user space threads&lt;/head&gt;
    &lt;p&gt;As mentioned in the introduction, we’ll bundle the user space and kernel space code into a single binary blob to avoid dynamic loading, linking, and other complexities. Hence, our user space code consists of regular functions:&lt;/p&gt;
    &lt;code&gt;/// Example: Create a simple idle thread
pub fn createPrintingThread(thread_number: usize) !*Thread {
    const thread = allocThread() orelse return error.NoFreeThreads;

    // Idle thread just spins
    const print_fn = struct {
        fn print(thread_arg: usize) noreturn {
            while (true) {
                var buffer: [256]u8 = undefined;
                const content = std.fmt.bufPrint(&amp;amp;buffer, "Printing from thread ID: {d}\n", .{thread_arg}) catch {
                    continue;
                };

                syscall.debug_print(content);

                // Simulate a delay
                var i: u32 = 0;
                while (i &amp;lt; 300000000) : (i += 1) {
                    asm volatile ("" ::: .{ .memory = true }); // Memory barrier to prevent optimization
                }
            }
            unreachable;
        }
    }.print;

    initThread(thread, @intFromPtr(&amp;amp;print_fn), thread_number);
    return thread;
}&lt;/code&gt;
    &lt;p&gt;Additionally, as mentioned above, we pre-seeded the stack such that when &lt;code&gt;a0&lt;/code&gt; is recovered from the stack upon the first interrupt return for a given thread, the function argument will be picked up. That’s how the &lt;code&gt;print&lt;/code&gt; function accesses the &lt;code&gt;thread_arg&lt;/code&gt; value and uses it in its logic.&lt;/p&gt;
    &lt;p&gt;To demonstrate the user/kernel boundary, we have &lt;code&gt;syscall.debug_print(content);&lt;/code&gt;. This conceptually behaves more or less as &lt;code&gt;printf&lt;/code&gt; from &lt;code&gt;stdio.h&lt;/code&gt; in C. It performs prepares the arguments to the kernel and runs a system call with these arguments which should lead to some content getting printed on the output device. Here’s what the printing library looks like (from &lt;code&gt;syscall.zig&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;// User-level debug_print function
pub fn debug_print(message: []const u8) void {
    const msg_ptr = @intFromPtr(message.ptr);
    const msg_len = message.len;

    // Let's say syscall number 64
    // a7 = syscall number
    // a0 = message pointer
    // a1 = message length
    asm volatile (
        \\mv a0, %[msg]
        \\mv a1, %[len]
        \\li a7, 64
        \\ecall
        :
        : [msg] "r" (msg_ptr),
          [len] "r" (msg_len),
        : .{ .x10 = true, .x11 = true, .x17 = true, .memory = true });

    // Ignore return value for simplicity
}&lt;/code&gt;
    &lt;p&gt;System call 64 is served from the S-mode handler in &lt;code&gt;kernel.zig&lt;/code&gt;. This is self-explanatory, and we won’t go into further details here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Running the kernel&lt;/head&gt;
    &lt;p&gt;We will deploy the kernel on bare-metal, specifically on a virtual machine. In theory, this should also work on a real machine, provided an SBI layer is present when the kernel starts, and the linker script, I/O “drivers,” and other machine-specific constants are adapted.&lt;/p&gt;
    &lt;p&gt;To build, we simply run&lt;/p&gt;
    &lt;code&gt;zig build&lt;/code&gt;
    &lt;p&gt;To now run the kernel, we run:&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -nographic -bios /tmp/opensbi/build/platform/generic/firmware/fw_dynamic.bin -kernel zig-out/bin/kernel&lt;/code&gt;
    &lt;p&gt;Refer to the previous text on OpenSBI for details on building OpenSBI. It is strongly recommended to use a freshly built OpenSBI, as QEMU may use an outdated version if no &lt;code&gt;-bios&lt;/code&gt; flag is passed.&lt;/p&gt;
    &lt;p&gt;The output should begin with a big OpenSBI splash along with some OpenSBI data:&lt;/p&gt;
    &lt;code&gt;OpenSBI v1.7
   ____                    _____ ____ _____
  / __ \                  / ____|  _ \_   _|
 | |  | |_ __   ___ _ __ | (___ | |_) || |
 | |  | | '_ \ / _ \ '_ \ \___ \|  _ &amp;lt; | |
 | |__| | |_) |  __/ | | |____) | |_) || |_
  \____/| .__/ \___|_| |_|_____/|____/_____|
        | |
        |_|

Platform Name               : riscv-virtio,qemu
Platform Features           : medeleg
Platform HART Count         : 1
Platform IPI Device         : aclint-mswi
Platform Timer Device       : aclint-mtimer @ 10000000Hz
Platform Console Device     : uart8250
Platform HSM Device         : ---
Platform PMU Device         : ---
Platform Reboot Device      : syscon-reboot
Platform Shutdown Device    : syscon-poweroff
Platform Suspend Device     : ---
Platform CPPC Device        : ---
Firmware Base               : 0x80000000
Firmware Size               : 317 KB
Firmware RW Offset          : 0x40000
Firmware RW Size            : 61 KB
Firmware Heap Offset        : 0x46000
Firmware Heap Size          : 37 KB (total), 2 KB (reserved), 11 KB (used), 23 KB (free)
Firmware Scratch Size       : 4096 B (total), 400 B (used), 3696 B (free)
Runtime SBI Version         : 3.0
Standard SBI Extensions     : time,rfnc,ipi,base,hsm,srst,pmu,dbcn,fwft,legacy,dbtr,sse
Experimental SBI Extensions : none

Domain0 Name                : root
....&lt;/code&gt;
    &lt;p&gt;Following the OpenSBI splash, we’ll see the kernel output:&lt;/p&gt;
    &lt;code&gt;Booting the kernel...
Printing from thread ID: 0
Printing from thread ID: 0
Printing from thread ID: 0
Printing from thread ID: 1
Printing from thread ID: 1
Printing from thread ID: 1
Printing from thread ID: 2
Printing from thread ID: 2
Printing from thread ID: 2
Printing from thread ID: 0
Printing from thread ID: 0
Printing from thread ID: 1
Printing from thread ID: 1
Printing from thread ID: 2
Printing from thread ID: 2
Printing from thread ID: 0
Printing from thread ID: 0
Printing from thread ID: 0
Printing from thread ID: 1
Printing from thread ID: 1
Printing from thread ID: 1
Printing from thread ID: 2
Printing from thread ID: 2
Printing from thread ID: 2&lt;/code&gt;
    &lt;p&gt;The prints will continue running until QEMU is terminated.&lt;/p&gt;
    &lt;p&gt;If you want to build the kernel in an extremely verbose mode for debugging and experimentation, use the following command:&lt;/p&gt;
    &lt;code&gt;zig build -Ddebug-logs=true&lt;/code&gt;
    &lt;p&gt;After running the kernel with the same QEMU command, the output will appear as follows:&lt;/p&gt;
    &lt;code&gt;Booting the kernel...
DEBUG mode on
Interrupt source: Timer, Current stack: 87cffe70
[W] NO CURRENT THREAD AVAILABLE!
Yielding to the new thread
New thread ID: 0, stack top: 80203030
Interrupt source: Ecall from User mode, Current stack: 80202ec0
Printing from thread ID: 0
Interrupt source: Ecall from User mode, Current stack: 80202ec0
Printing from thread ID: 0
Interrupt source: Ecall from User mode, Current stack: 80202ec0
Printing from thread ID: 0
Interrupt source: Timer, Current stack: 80202ec0
[I] Enqueueing the current thread
Yielding to the new thread
New thread ID: 1, stack top: 80205030
Interrupt source: Ecall from User mode, Current stack: 80204ec0
Printing from thread ID: 1
Interrupt source: Ecall from User mode, Current stack: 80204ec0
Printing from thread ID: 1
Interrupt source: Ecall from User mode, Current stack: 80204ec0
Printing from thread ID: 1
Interrupt source: Timer, Current stack: 80204ec0
[I] Enqueueing the current thread
Yielding to the new thread
New thread ID: 2, stack top: 80207030
Interrupt source: Ecall from User mode, Current stack: 80206ec0
Printing from thread ID: 2
Interrupt source: Ecall from User mode, Current stack: 80206ec0
Printing from thread ID: 2
Interrupt source: Ecall from User mode, Current stack: 80206ec0
Printing from thread ID: 2
Interrupt source: Timer, Current stack: 80206ec0
...&lt;/code&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Many educational OS kernels exist, but this experiment combines RISC-V, OpenSBI, and Zig, offering a fresh perspective compared to traditional C implementations.&lt;/p&gt;
    &lt;p&gt;The resulting code runs on a QEMU virtual machine, which can be easily set up, even by building QEMU from source.&lt;/p&gt;
    &lt;p&gt;To keep the explanation concise, error reporting was kept minimal. Should you modify the code and require debugging, sufficient clues are provided, despite some areas where the code is simplified (e.g., anonymous results after SBI print invocations like &lt;code&gt;_ = ...&lt;/code&gt;). Much of the code in this example was AI-generated by Claude to save time, and it should function as intended. While some parts of the code are simplified, such as stack space over-allocation, these do not detract from the experiment’s educational value.&lt;/p&gt;
    &lt;p&gt;Overall, this experiment serves as a starting point for studying operating systems, assuming a foundational understanding of computer engineering and computer architecture. It likely has plenty of flaws for a practical application, but for now, we’re just hacking here!&lt;/p&gt;
    &lt;p&gt;I hope this was a useful exploration.&lt;/p&gt;
    &lt;p&gt;Please consider following on Twitter/X and LinkedIn to stay updated.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://popovicu.com/posts/writing-an-operating-system-kernel-from-scratch/"/><published>2025-09-14T15:44:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45242591</id><title>OCSP Service Has Reached End of Life</title><updated>2025-09-15T13:40:21.126763+00:00</updated><content>&lt;doc fingerprint="e337cfd427579385"&gt;
  &lt;main&gt;
    &lt;p&gt;Today we turned off our Online Certificate Status Protocol (OCSP) service, as announced in December of last year. We stopped including OCSP URLs in our certificates more than 90 days ago, so all Let’s Encrypt certificates that contained OCSP URLs have now expired. Going forward, we will publish revocation information exclusively via Certificate Revocation Lists (CRLs).&lt;/p&gt;
    &lt;p&gt;We ended support for OCSP primarily because it represents a considerable risk to privacy on the Internet. When someone visits a website using a browser or other software that checks for certificate revocation via OCSP, the Certificate Authority (CA) operating the OCSP responder immediately becomes aware of which website is being visited from that visitor’s particular IP address. Even when a CA intentionally does not retain this information, as is the case with Let’s Encrypt, it could accidentally be retained or CAs could be legally compelled to collect it. CRLs do not have this issue.&lt;/p&gt;
    &lt;p&gt;We are also taking this step because keeping our CA infrastructure as simple as possible is critical for the continuity of compliance, reliability, and efficiency at Let’s Encrypt. For every year that we have existed, operating OCSP services has taken up considerable resources that can soon be better spent on other aspects of our operations. Now that we support CRLs, our OCSP service has become unnecessary.&lt;/p&gt;
    &lt;p&gt;At the height of our OCSP service’s traffic earlier this year, we handled approximately 340 billion OCSP requests per month. That’s more than 140,000 requests per second handled by our CDN, with 15,000 requests per second handled by our origin. We’d like to thank Akamai for generously donating CDN services for OCSP to Let’s Encrypt for the past ten years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://letsencrypt.org/2025/08/06/ocsp-service-has-reached-end-of-life"/><published>2025-09-14T19:34:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45243635</id><title>Betty Crocker broke recipes by shrinking boxes</title><updated>2025-09-15T13:40:20.963829+00:00</updated><content>&lt;doc fingerprint="800a557659315311"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The 70-Year-Old Beloved Boxed Mix Grandmas Won’t Be Buying Anymore&lt;/head&gt;
    &lt;p&gt;There’s a problem with boxed mixes, and it’s impacting grandmas’ most beloved recipes. At some point last year, shoppers noticed Betty Crocker cake mixes shrank (again), this time from 15.25 ounces down to 13.25 ounces. And while few people would ever argue for less cake, the 2-ounce decrease is weighing particularly heavy on grandmas.&lt;/p&gt;
    &lt;p&gt;Take my neighbor Judith (and grandma to two). Her chocolate crinkle cookies have been practically synonymous with our community potlucks for the past 15 years — and that’s only how long I’ve known her! It wasn’t until she suddenly stopped bringing them that I knew something was terribly wrong.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Grandmas Aren’t Buying Boxed Cake Mixes&lt;/head&gt;
    &lt;p&gt;Her cookie recipe — a box of Betty Crocker chocolate cake mix, two eggs, and ⅓ cup neutral oil — no longer works now that the box is a full 5 ounces smaller than its original 18.25-ounce size (a 27% decrease and textbook example of shrinkflation). What once yielded 24 consistently light, fluffy cookies now makes 20 goopy, lackluster blobs. The only thing that has changed? The box mix.&lt;/p&gt;
    &lt;p&gt;“It’s just so upsetting,” says Judith, whose cookie recipe was passed down by her mother. These “perfect little cookies” once made the rounds at bake sales, Christmas cookie exchanges, and birthdays. She now calls them “unusable.” She could buy an additional box to make up the difference, she acknowledges, “but out of principle, I just can’t.”&lt;/p&gt;
    &lt;p&gt;Judith isn’t the only one. She says that her other (grandma) friends are feeling the changes, too, and they’re not happy about it. Betty Crocker has empowered home cooks to make delicious desserts for over a century now. So it’s no surprise just how many cherished family recipes — involving that once familiar box of cake mix — have been passed down from generation to generation.&lt;/p&gt;
    &lt;p&gt;It’s not just cookies that are affected; beloved family dump cakes, crumbles, pancakes, and more all fall short because the cake mix is no longer the same.&lt;/p&gt;
    &lt;p&gt;To add to the frustration, one Redditor pointed out that the brand may have “tinkered with the amount of leaveners in the mix itself. … When [the cake] first comes out of the oven, it looks like a more substantial amount of cake but then shrinks as it cools down.” (We reached out to Betty Crocker to verify any ingredient changes and have yet to receive a response.)&lt;/p&gt;
    &lt;p&gt;Baking is indeed a science that needs precise measurements and consistency. Many home bakers, like Judith, find it disheartening to see decades-old cherished recipes forever changed by corporate decisions. I did, though, share our Chocolate Crinkle Cookie recipe with her so she can try to make new traditions. &lt;lb/&gt;This article originally published on The Kitchn. See it there: The 70-Year-Old Beloved Boxed Mix Grandmas Won’t Be Buying This Holiday Season&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cubbyathome.com/boxed-cake-mix-sizes-have-shrunk-80045058"/><published>2025-09-14T21:54:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45243803</id><title>Grapevine canes can be converted into plastic-like material that will decompose</title><updated>2025-09-15T13:40:20.662053+00:00</updated><content>&lt;doc fingerprint="f488c5b0bf730735"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Can grapevines help slow the plastic waste problem?&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;A new study from South Dakota State University reveals how grapevine canes can be converted into plastic-like material that is stronger than traditional plastic and will decompose in the environment in a relatively short amount of time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The need for biodegradable packaging material has never been higher.&lt;/p&gt;
    &lt;p&gt;Currently, most packaging is "single use" and is made with plastic materials, derived from nonrenewable sources like crude oil that take hundreds of years to decompose in the environment. On top of this, only 9% of plastic is recycled. This has resulted in the formation of floating piles of plastic garbage in the ocean, called the "Great Pacific Garbage Patch."&lt;/p&gt;
    &lt;p&gt;But maybe even more concerning is the discovery of micro- and nano-plastics in the environment. Research has found that plastic breaks down into tiny particles, which are being ingested or inhaled by both humans and animals, and are found literally everywhere, including in the human body — according to recent research studies. Worse, little is known about the long-term health effects of microplastics.&lt;/p&gt;
    &lt;p&gt;Srinivas Janaswamy is an associate professor in South Dakota State University's Department of Dairy and Food Science. His research has focused on developing value-added products through biowaste and agricultural byproducts. One of the overarching goals of Janaswamy's research is to tackle the plastic waste crisis.&lt;/p&gt;
    &lt;p&gt;Perhaps the biggest contributor to plastic waste, at least in the United States, is plastic bags, the kind found at most retail stores. These bags, while sometimes recycled, are often only used once and can be found littered throughout the environment.&lt;/p&gt;
    &lt;p&gt;To address this problem, Janaswamy is working toward developing a plastic-like bag that will decompose in the environment.&lt;/p&gt;
    &lt;p&gt;"That is my dream," Janaswamy said.&lt;/p&gt;
    &lt;p&gt;The key ingredient to Janaswamy's work? Cellulose. This biopolymer is the most abundant organic substance on Earth and is found, primarily, in the cell walls of plants. Cellulose, thanks to strong hydrogen bonds and a chain of glucose molecules, gives plants structural strength and rigidity along with other biopolymers such as mannan, xylose, hemicellulose and lignin.&lt;/p&gt;
    &lt;p&gt;Humans have long used cellulose to create products. Cotton, the material used to make a majority of the world’s clothing, is primarily composed of cellulose. Wood is rich in cellulose as well.&lt;/p&gt;
    &lt;p&gt;In previous research, Janaswamy has extracted cellulose from agricultural products like avocado peels, soyhulls, alfalfa, switchgrass, spent coffee grounds, corncob and banana peels. He uses the extracted cellulose to develop films — materials that look and feel similar to traditional plastic wrapping.&lt;/p&gt;
    &lt;p&gt;"By extracting cellulose from agricultural products, value-added products can be created," Janaswamy said.&lt;/p&gt;
    &lt;p&gt;Each of Janaswamy's films has different characteristics and properties. Some are more transparent than others. Some are stronger. But thanks to a unique collaboration with a fellow SDSU faculty member, Janaswamy may have created his best value-added product yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Grapevine canes&lt;/head&gt;
    &lt;p&gt;Janaswamy had just finished presenting “Ag Biomass – A Holy Grail to Clean up the Plastic Mess” at SDSU's Celebration of Faculty Excellence when he was approached by Anne Fennell, a Distinguished Professor in the Department of Agronomy, Horticulture and Plant Science.&lt;/p&gt;
    &lt;p&gt;After listening to Janaswamy's presentation, Fennell became interested in the research and had an idea. A leading researcher in the study of grapevines, she knew that grapevine canes — the woody plant material that grapes grow on — were rich in cellulose. She also knew that grapevine canes were abundant and had limited use after harvest.&lt;/p&gt;
    &lt;p&gt;"Every year we prune the majority of yearly biomass off the vine," Fennell said. "The pruned canes are either mowed over, composted and reapplied to the soil, or burned in some areas. Research in Australia showed that prunings could be removed from the field in alternate years without effecting soil health. My thought was why not use this for value added films. Several of the materials that Janaswamy previously used had a high-water content, in contrast the winter pruning yields a cellulose-dense material with low water content, making them an abundant ideal material to work with."&lt;/p&gt;
    &lt;p&gt;Fennell's idea led to a collaboration, and soon Janaswamy was extracting cellulose — which looks almost like cotton — from the canes of grapevines. The resulting films were eye-opening.&lt;/p&gt;
    &lt;p&gt;According to a recent study published in the academic journal Sustainable Food Technology, Janaswamy's grapevine cane films are transparent and strong and biodegrade within 17 days in the soil — leaving behind no harmful residue.&lt;/p&gt;
    &lt;p&gt;"High transmittance in packaging films enhances product visibility, making them more attractive to consumers and facilitating easy quality inspection without the need for unsealing," Janaswamy said. "These films demonstrate outstanding potential for food packaging applications."&lt;/p&gt;
    &lt;p&gt;The grapevine canes were harvested from SDSU's research vineyard. The research team, which includes doctoral candidates Sandeep Paudel and Sumi Regmi, and Sajal Bhattarai, an SDSU graduate and a doctoral candidate at Purdue University, followed a published protocol in developing the films, which includes drying and grinding the canes and extracting the cellulosic residue. The residue was then solubilized and cast onto glass plates to create the films.&lt;/p&gt;
    &lt;p&gt;Testing revealed the grapevine cane-derived films were actually stronger than traditional plastic bags — in terms of tensile strength.&lt;/p&gt;
    &lt;p&gt;"Using underutilized grapevine prunings as a cellulose source for packaging films enhances waste management in the field and addresses the global issue of plastic pollution," Janaswamy said. "Developing eco-friendly films from grapevine cellulose represents a practical approach to sustainability, helping to conserve the environment and its resources and contributing to the circular bioeconomy."&lt;/p&gt;
    &lt;p&gt;The results of this work move Janaswamy one step closer to his dream of developing a bag made from a plastic-like material that will quickly decompose in the environment.&lt;/p&gt;
    &lt;p&gt;Funding for this research was provided by the U.S. Department of Agriculture's National Institute of Food and Agriculture and the National Science Foundation.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;Republishing&lt;/p&gt;
    &lt;p&gt;You may republish SDSU News Center articles for free, online or in print. Questions? Contact us at sdsu.news@sdstate.edu or 605-688-6161.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.sdstate.edu/news/2025/08/can-grapevines-help-slow-plastic-waste-problem"/><published>2025-09-14T22:15:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45243925</id><title>Titania Programming Language</title><updated>2025-09-15T13:40:20.170962+00:00</updated><content>&lt;doc fingerprint="ec1bde28ba1fb0dc"&gt;
  &lt;main&gt;
    &lt;p&gt;Based on the Oberon-07 programming language designed by the late Niklaus Wirth.&lt;/p&gt;
    &lt;p&gt;This is designed to be a language to teach compiler development with.&lt;/p&gt;
    &lt;p&gt;Meaning behind the name:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Titania is the wife of Oberon (Fairy King) in Shakespeare's A Midsummer Night's Dream&lt;/item&gt;
      &lt;item&gt;https://en.wikipedia.org/wiki/Titania_(A_Midsummer_Night%27s_Dream)&lt;/item&gt;
      &lt;item&gt;This is just a codename, and probably not final for this teaching language&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;module = "module" ident ";" [import_list] decl_sequence
         ["begin" stmt_sequence] "end" [";"].

import_list = "import" import_decl {"," import_decl} ";".
decl_sequence = ["const" {const_decl ";"}]
                ["type"  {type_decl  ";"}]
                ["var"   {var_decl   ";"}]
                [{proc_decl          ";"}].

const_decl = ident "=" const_expr.
type_decl = ident "="" struct_type.
var_decl = ident_list ":" type.

proc_decl = "proc" ident [formal_parameters] ";" proc_body.
proc_body = decl_sequence ["begin" stmt_sequence] ["return" expr] "end".


const_expr = expr.
expr = simple_expr {relation simple_expr}.

simple_expr = ["+" | "-"] unary_expr {add_operator unary_expr}.
unary_expr = ["+" | "-"] term.
term = factor {mul_operator factor}.

factor = integer | real | string | nil | true | false | set |
         "(" expr ")" | "not" expr | designator.

element = expr [".." expr].

ident_list = ident {"," ident}.
qual_ident = [ident "."] ident.

struct_type = array_type | record_type | pointer_type | proc_type.
array_type = "["" const_expr {"," const_expr} "]" type.
record_type = "record" ["(" qual_ident ")"] [field_list_sequence] "end".
pointer_type = "^" type.
proc_type = "proc" formal_parameters.
field_list = ["using"] ident_list ":" type.
formal_parmeters = "(" [fp_section {";" fp_section}] [";"] ")".
formal_type = "[" "]" qual_ident.

stmt_sequence = stmt {";" stmt} [";"].
stmt = [assignment | proc_call | if_stmt | case_stmt | while_stmt | repeat_stmt | for_stmt ].

assignment = designator ":=" expr

if_stmt = "if" expr "then" stmt_sequence
          {"elseif" expr "then" stmt_sequence}
          ["else" stmt_sequence]
          "end".

case_stmt = "case" expr "of" case {"|" case} "end".
case = [case_label_list ":" stmt_sequence].
case_list = label_range {"," label_range}.
label_range = label [".." label].
label = integer | string | qual_ident.

while_stmt = "while" expr "then" stmt_sequence
             {"elseif" expr "then" stmt_sequence}
             "end".
repeat_stmt = "repeat" stmt_sequence "until" expr.
for_stmt = "for" ident ":=" expr "to" expr ["by" const_expr] "then" stmt_sequence "end".


designator = qual_ident {selector}.
selector = "." ident |
           "[" expr_list "]" |
           "^" |
           "(" [expr_list] ")".
expr_list = expr {"," expr}.


add_operator = "+" | "-" | "xor" | "or".
mul_operator = "*" | "/" | "%"   | "and".
relation     = "=" | "&amp;lt;&amp;gt;" | "&amp;lt;" | "&amp;lt;=" | "&amp;gt;" | "&amp;gt;=" | "in" | "is".
&lt;/code&gt;
    &lt;code&gt;and    else    if      nil   record  true   while
begin  elseif  import  not   repeat  type   xor
by     end     in      of    return  until
case   false   is      or    then    using
const  for     module  proc  to      var
&lt;/code&gt;
    &lt;code&gt;+    .   (   )   =  &amp;lt;&amp;gt;
-    ,   [   ]   &amp;lt;  &amp;lt;=
*    ;   {   }   &amp;gt;  &amp;gt;=
/    |   :=  :   ..
%    ^
&lt;/code&gt;
    &lt;p&gt;When a newline is seen after the following token kind, a semicolon is inserted, otherwise no semicolon is inserted:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identifiers&lt;/item&gt;
      &lt;item&gt;Integer, Real, String, Boolean literals&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;nil&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;^&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;)&lt;/code&gt;,&lt;code&gt;]&lt;/code&gt;,&lt;code&gt;}&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;end&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: These will be added to as the compiler develops&lt;/p&gt;
    &lt;code&gt;abs(x)            - absolute value of
lsh(x, y)         - logical shift left
ash(x, y)         - arithmetic shift right
ror(x, y)         - rotate right
chr(i)            - convert int to char
ord(c)            - convert char to int
inc(x)            - x := x + 1
inc(x, y)         - x := x + y
dec(x)            - x := x - 1
dec(x, y)         - x := x - y
incl(x, y)        - include y in set x
excl(x, y)        - exclude y in set x
odd(x)            - x % 2 = 0
floor(x)          - round-down for real
ceil(x)           - round-up   for real
assert(cond)      - assert when cond is false
new(ptr)          - allocate memory
delete(ptr)       - free memory
addr(x)           - address of addressable memory
size_of(x)        - size of the type of 'x'
align_of(x)       - alignment of the type of 'x'
copy(dst, src, n) - non-overlapping memory copying from `src` to `dst` of `n` bytes
print(...)        - variadic print without newline
println(...)      - variadic print with newline
len(x)            - length of an array 'x'
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/gingerBill/titania"/><published>2025-09-14T22:29:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45245313</id><title>For Good First Issue – A repository of social impact and open source projects</title><updated>2025-09-15T13:40:20.061831+00:00</updated><content>&lt;doc fingerprint="7b5002672bfb2689"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Committing to a better future&lt;/head&gt;
    &lt;p&gt;Lend your skills to an open source project focused on the Digital Public Goods (DPGs). From fighting climate change, to solving world hunger, your efforts will contribute to creating a better future for everyone. Together, we can drive positive and lasting contributions to the world, one commit at a time.&lt;lb/&gt;Explore a DPG repo below to get started.&lt;/p&gt;
    &lt;head rend="h2"&gt;Find a project&lt;/head&gt;
    &lt;p&gt;The participatory democracy framework. A generator and multiple gems made with Ruby on Rails&lt;/p&gt;
    &lt;p&gt;ODK Central is a server that is easy to use, very fast, and stuffed with features that make data collection easier. Contribute and make the world a better place! ✨🗄✨&lt;/p&gt;
    &lt;p&gt;📰 Diários oficiais brasileiros acessíveis a todos | 📰 Brazilian government gazettes, accessible to everyone.&lt;/p&gt;
    &lt;p&gt;Augmentative and Alternative Communication (AAC) system with text-to-speech for the browser&lt;/p&gt;
    &lt;p&gt;Documents added by volunteer contributors and historically imported from TOSBack.org. Maintenance is collaborative and volunteer-based.&lt;/p&gt;
    &lt;p&gt;OpenFn/Lightning ⚡️ is the newest version of the OpenFn DPG and provides a web UI to visually manage complex workflow automation projects.&lt;/p&gt;
    &lt;p&gt;A collection of tools for extracting FHIR resources and analytics services on top of that data.&lt;/p&gt;
    &lt;p&gt;Source code of the X-Road® data exchange layer software&lt;/p&gt;
    &lt;p&gt;Book repository for The Turing Way: a how to guide for reproducible, ethical and collaborative data science&lt;/p&gt;
    &lt;p&gt;Volunteer management system for nonprofit CASA, which serves foster youth in counties across America.&lt;/p&gt;
    &lt;p&gt;ODK Collect is an Android app for filling out forms. It's been used to collect billions of data points in challenging environments around the world. Contribute and make the world a better place! ✨📋✨&lt;/p&gt;
    &lt;p&gt;The CHT Core Framework makes it faster to build responsive, offline-first digital health apps that equip health workers to provide better care in their communities. It is a central resource of the Community Health Toolkit.&lt;/p&gt;
    &lt;p&gt;Mautic: Open Source Marketing Automation Software.&lt;/p&gt;
    &lt;p&gt;PolicyEngine's free web app for computing the impact of public policy.&lt;/p&gt;
    &lt;p&gt;Open source, Open standards based Decentralised Identity &amp;amp; Verifiable Credentials Platform&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://forgoodfirstissue.github.com/"/><published>2025-09-15T02:02:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45245678</id><title>Which NPM package has the largest version number?</title><updated>2025-09-15T13:40:19.820653+00:00</updated><content>&lt;doc fingerprint="2b5d6a05af70ae19"&gt;
  &lt;main&gt;
    &lt;p&gt;I was recently working on a project that uses the AWS SDK for JavaScript. When updating the dependencies in said project, I noticed that the version of that dependency was &lt;code&gt;v3.888.0&lt;/code&gt;. Eight hundred eighty eight. That’s a big number as far as versions go.&lt;/p&gt;
    &lt;p&gt;That got me thinking: I wonder what package in the npm registry has the largest number in its version. It could be a major, minor, or patch version, and it doesn’t have to be the latest version of the package. In other words, out of the three numbers in &lt;code&gt;&amp;lt;major&amp;gt;.&amp;lt;minor&amp;gt;.&amp;lt;patch&amp;gt;&lt;/code&gt; for each version for each package, what is the largest number I can find?&lt;/p&gt;
    &lt;p&gt;TL;DR? Jump to the results to see the answer.&lt;/p&gt;
    &lt;head rend="h2"&gt;The npm API&lt;/head&gt;
    &lt;p&gt;Obviously npm has some kind of API, so it shouldn’t be too hard to get a list of all… 3,639,812 packages. Oh. That’s a lot of packages. Well, considering npm had 374 billion package downloads in the past month, I’m sure they wouldn’t mind me making a few million HTTP requests.&lt;/p&gt;
    &lt;p&gt;Doing a quick search for “npm api” leads me to a readme in the npm/registry repo on GitHub. There’s a &lt;code&gt;/-/all&lt;/code&gt; endpoint listed in the table of contents which seems promising. That section doesn’t actually exist in the readme, but maybe it still works?&lt;/p&gt;
    &lt;p&gt;Whelp, maybe npm packages have an ID and I can just start at 1 and count up? It looks like packages have an &lt;code&gt;_id&lt;/code&gt; field… never mind, the &lt;code&gt;_id&lt;/code&gt; field is the package name. Okay, let’s try to find something else.&lt;/p&gt;
    &lt;p&gt;A little more digging brings me to this GitHub discussion about the npm replication API. So npm replicates package info in CouchDB at &lt;code&gt;https://replicate.npmjs.com&lt;/code&gt;, and conveniently, they support the &lt;code&gt;_all_docs&lt;/code&gt; endpoint. Let’s give that a try:&lt;/p&gt;
    &lt;p&gt;Those are some interesting package names. Looks like this data is paginated and by default I get 1,000 packages at a time. When I write the final script, I can set the &lt;code&gt;limit&lt;/code&gt; query parameter to the max of 10,000 to make pagination a little less painful.&lt;/p&gt;
    &lt;p&gt;Fortunately, the CouchDB docs have a guide for pagination, and it looks like it’s as simple as using the &lt;code&gt;skip&lt;/code&gt; query parameter.&lt;/p&gt;
    &lt;p&gt;Never mind. According to the GitHub discussion linked above, &lt;code&gt;skip&lt;/code&gt; is no longer supported. The “Paging (Alternate Method)” section of the same page says that I can use &lt;code&gt;startkey_docid&lt;/code&gt; instead. If I grab the &lt;code&gt;id&lt;/code&gt; of the last row, I should be able to use that to return the next set of rows. Fun fact: The 1000th package (alphabetically) on npm is &lt;code&gt;03-webpack-number-test&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Nice. Also, another &lt;code&gt;3628102 - 3628088 = 14&lt;/code&gt; packages have been published in the ~15 minutes since I ran the last query.&lt;/p&gt;
    &lt;p&gt;Now, there’s one more piece of the puzzle to figure out. How do I get all the versions for a given package? Unfortunately, it doesn’t seem like I can get package version information along with the base info returned by &lt;code&gt;_all_docs&lt;/code&gt;. I have to separately fetch each package’s metadata from &lt;code&gt;https://registry.npmjs.org/&amp;lt;package_id&amp;gt;&lt;/code&gt;. Let’s see what good ol’ trusty &lt;code&gt;03-webpack-number-test&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;p&gt;Alright, I have everything I need. Now I just need to write a bash script that— just kidding. A wise programmer once said, “if your shell script is more than 10 lines, it shouldn’t be a shell script” (that was me, I said that). I like TypeScript, so let’s use that.&lt;/p&gt;
    &lt;p&gt;The biggest bottleneck is going to be waiting on the &lt;code&gt;GET&lt;/code&gt;s for each package’s metadata. My plan is this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grab all the package IDs from the replication API and save that data to a file (I don’t want to have to refetch everything if the something goes wrong later in the script)&lt;/item&gt;
      &lt;item&gt;Fetch package data in batches so we’re not just doing 1 HTTP request at a time&lt;/item&gt;
      &lt;item&gt;Save the package data to a file (again, hopefully I only have to fetch everything once)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once I have all the package data, I can answer the original question of “largest number in version” and look at a few other interesting things.&lt;/p&gt;
    &lt;p&gt;(A few hours and many iterations later…)&lt;/p&gt;
    &lt;p&gt;See the script section at the end if you want to see what it looks like.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;Some stats:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Time to fetch all ~3.6 million package IDs: A few minutes&lt;/item&gt;
      &lt;item&gt;Time to fetch version data for each one of those packages: ~12 hours (yikes)&lt;/item&gt;
      &lt;item&gt;Packages fetched per second: ~84 packages/s&lt;/item&gt;
      &lt;item&gt;Size of &lt;code&gt;package-ids.json&lt;/code&gt;: ~78MB&lt;/item&gt;
      &lt;item&gt;Size of &lt;code&gt;package-data.json&lt;/code&gt;: ~886MB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And the winner is… (not really) latentflip-test at version &lt;code&gt;1000000000000000000.1000000000000000000.1000000000000000000&lt;/code&gt;. And no, there haven’t actually been one quintillion major versions of this package published. Disappointing, I know.&lt;/p&gt;
    &lt;p&gt;Okay, I feel like that shouldn’t count. I think we can do better and find a “real” package that actually follows semantic versioning. I think a better question to ask is this:&lt;/p&gt;
    &lt;p&gt;For packages that follow semantic versioning, which package has the largest number from &lt;code&gt;&amp;lt;major&amp;gt;.&amp;lt;minor&amp;gt;.&amp;lt;patch&amp;gt;&lt;/code&gt; in any of its versions?&lt;/p&gt;
    &lt;p&gt;So, what does it mean to “follow semantic versioning”? Should we “disqualify” a package for skipping a version number? In this case, I think we’ll just say that a package has to have more versions published than the largest number we find for that package. For example, a package with a version of &lt;code&gt;1.888.0&lt;/code&gt; will have had at least 888 versions published if it actually followed semver.&lt;/p&gt;
    &lt;p&gt;Before we get to the real winner, here are the top 10 packages by total number of versions published:&lt;/p&gt;
    &lt;p&gt;Top 10 packages that (probably) follow semver by largest number in one of its versions:&lt;/p&gt;
    &lt;p&gt;So it seems like the winner is @mahdiarjangi/phetch-cli with &lt;code&gt;19494&lt;/code&gt;, right? Unfortunately, I’m not going to count that either. It only has so many versions because of a misconfigured GitHub action that published new versions in a loop.&lt;/p&gt;
    &lt;p&gt;I manually went down the above list, disqualifying any packages that had similar issues. I also checked that “new” versions actually differed from previous versions in terms of content. Overall, I looked for a package that was actually publishing new versions on purpose with some kind of change to the package content.&lt;/p&gt;
    &lt;p&gt;The real winner (#19 on the list) is: all-the-package-names with &lt;code&gt;2401&lt;/code&gt; from version &lt;code&gt;2.0.2401&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Well, that’s sort of disappointing, but also kind of funny. I don’t know what I was expecting to be honest. If you’re curious, you can see more results at the bottom of this post.&lt;/p&gt;
    &lt;p&gt;What you do with all of this extremely important and useful information is up to you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Script&lt;/head&gt;
    &lt;head rend="h2"&gt;More Results&lt;/head&gt;
    &lt;p&gt;This is from the script:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://adamhl.dev/blog/largest-number-in-npm-package/"/><published>2025-09-15T03:03:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45245948</id><title>Language Models Pack Billions of Concepts into 12k Dimensions</title><updated>2025-09-15T13:40:19.637790+00:00</updated><content>&lt;doc fingerprint="150b027d080f93db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Beyond Orthogonality: How Language Models Pack Billions of Concepts into 12,000 Dimensions&lt;/head&gt;
    &lt;p&gt;In a recent 3Blue1Brown video series on transformer models, Grant Sanderson posed a fascinating question: How can a relatively modest embedding space of 12,288 dimensions (GPT-3) accommodate millions of distinct real-world concepts?&lt;/p&gt;
    &lt;p&gt;The answer lies at the intersection of high-dimensional geometry and a remarkable mathematical result known as the Johnson-Lindenstrauss lemma. While exploring this question, I discovered something unexpected that led to an interesting collaboration with Grant and a deeper understanding of vector space geometry.&lt;/p&gt;
    &lt;p&gt;The key insight begins with a simple observation: while an N-dimensional space can only hold N perfectly orthogonal vectors, relaxing this constraint to allow for "quasi-orthogonal" relationships (vectors at angles of, say, 85-95 degrees) dramatically increases the space's capacity. This property is crucial for understanding how language models can efficiently encode semantic meaning in relatively compact embedding spaces.&lt;/p&gt;
    &lt;p&gt;In Grant's video, he demonstrated this principle with an experiment attempting to fit 10,000 unit vectors into a 100-dimensional space while maintaining near-orthogonal relationships. The visualization suggested success, showing angles clustered between 89-91 degrees. However, when I implemented the code myself, I noticed something interesting about the optimization process.&lt;/p&gt;
    &lt;p&gt;The original loss function was elegantly simple:&lt;/p&gt;
    &lt;p&gt; loss = (dot_products.abs()).relu().sum()&lt;lb/&gt;While this loss function appears perfect for an unbounded ℝᴺ space, it encounters two subtle but critical issues when applied to vectors constrained to a high-dimensional unit sphere:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The Gradient Trap: The dot product between vectors is the cosine of the angle between them, and the gradient is the sine of this angle. This creates a perverse incentive structure: when vectors approach the desired 90-degree relationship, the gradient (sin(90°) = 1.0) strongly pushes toward improvement. However, when vectors drift far from the goal (near 0° or 180°), the gradient (sin(0°) ≈ 0) vanishes—effectively trapping these badly aligned vectors in their poor configuration.&lt;/item&gt;
      &lt;item&gt;The 99% Solution: The optimizer discovered a statistically favorable but geometrically perverse solution. For each vector, it would be properly orthogonal to 9,900 out of 9,999 other vectors while being nearly parallel to just 99. This configuration, while clearly not the intended outcome, actually represented a global minimum for the loss function—mathematically similar to taking 100 orthogonal basis vectors and replicating each one roughly 100 times.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This stable configuration was particularly insidious because it satisfied 99% of the constraints while being fundamentally different from the desired constellation of evenly spaced, quasi-orthogonal vectors. To address this, I modified the loss function to use an exponential penalty that increases aggressively as dot products grow:&lt;/p&gt;
    &lt;p&gt;loss = exp(20*dot_products.abs()**2).sum() (Full code here)&lt;/p&gt;
    &lt;p&gt;This change produced the desired behavior, though with a revealing result: the maximum achievable pairwise angle was around 76.5 degrees, not 89 degrees.&lt;/p&gt;
    &lt;p&gt;This discovery led me down a fascinating path exploring the fundamental limits of vector packing in high-dimensional spaces, and how these limits relate to the Johnson-Lindenstrauss lemma.&lt;/p&gt;
    &lt;p&gt;When I shared these findings with Grant, his response exemplified the collaborative spirit that makes the mathematics community so rewarding. He not only appreciated the technical correction but invited me to share these insights with the 3Blue1Brown audience. This article is that response, expanded to explore the broader implications of these geometric properties for machine learning and dimensionality reduction.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Johnson-Lindenstrauss Lemma: A Geometric Guarantee&lt;/head&gt;
    &lt;p&gt;At its core, the Johnson-Lindenstrauss (JL) lemma makes a remarkable promise: you can project points from an arbitrarily high-dimensional space into a surprisingly low-dimensional space while preserving their relative distances with high probability. What makes this result particularly striking is that the required dimensionality of the low-dimensional space grows only logarithmically with the number of points you want to project.&lt;/p&gt;
    &lt;p&gt;Formally, the lemma states that for an error factor ε (between 0 and 1), and any set of N points in a high-dimensional space, there exists a projection into k dimensions where for any two points u and v in the original space, their projections f(u) and f(v) in the lower dimensional space satisfy:&lt;/p&gt;
    &lt;p&gt;(1 - ε)||u - v||² ≤ ||f(u) - f(v)||² ≤ (1 + ε)||u - v||²&lt;/p&gt;
    &lt;p&gt;The number of dimensions (k) required to guarantee these error bounds is given by:&lt;/p&gt;
    &lt;p&gt;k ≥ O(log(N)/ε²)&lt;/p&gt;
    &lt;p&gt;The "Big O" notation can be replaced with a concrete constant C:&lt;/p&gt;
    &lt;p&gt;k ≥ (C/ε²) * log(N)&lt;/p&gt;
    &lt;p&gt;Where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;k is the target dimension&lt;/item&gt;
      &lt;item&gt;N is the number of points&lt;/item&gt;
      &lt;item&gt;ε is the maximum allowed distortion&lt;/item&gt;
      &lt;item&gt;C is a constant that determines the probability of success&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While most practitioners use values between 4 and 8 as a conservative choice for random projections, the optimal value of C remains an open question. As we'll see in the experimental section, engineered projections can achieve much lower values of C, with profound implications for embedding space capacity.&lt;/p&gt;
    &lt;p&gt;The fascinating history of this result speaks to the interconnected nature of mathematical discovery. Johnson and Lindenstrauss weren't actually trying to solve a dimensionality reduction problem – they stumbled upon this property while working on extending Lipschitz functions in Banach spaces. Their 1984 paper turned out to be far more influential in computer science than in their original domain.&lt;/p&gt;
    &lt;head rend="h1"&gt;From Theory to Practice: Two Domains of Application&lt;/head&gt;
    &lt;p&gt;The JL lemma finds practical application in two distinct but equally important domains:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Dimensionality Reduction: Consider an e-commerce platform like Amazon, where each customer's preferences might be represented by a vector with millions of dimensions (one for each product). Direct computation with such vectors would be prohibitively expensive. The JL lemma tells us we can project this data into a much lower-dimensional space – perhaps just a thousand dimensions – while preserving the essential relationships between customers. This makes previously intractable computations feasible on a single GPU, enabling real-time customer relationship management and inventory planning.&lt;/item&gt;
      &lt;item&gt;Embedding Space Capacity: This application is more subtle but equally powerful. Rather than actively projecting vectors, we're interested in understanding how many distinct concepts can naturally coexist in a fixed-dimensional space. This is where our experiments provide valuable insight into the practical limits of embedding space capacity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let's consider what we mean by "concepts" in an embedding space. Language models don't deal with perfectly orthogonal relationships – real-world concepts exhibit varying degrees of similarity and difference. Consider these examples of words chosen at random:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Archery" shares some semantic space with "precision" and "sport"&lt;/item&gt;
      &lt;item&gt;"Fire" overlaps with both "heat" and "passion"&lt;/item&gt;
      &lt;item&gt;"Gelatinous" relates to physical properties and food textures&lt;/item&gt;
      &lt;item&gt;"Southern-ness" encompasses culture, geography, and dialect&lt;/item&gt;
      &lt;item&gt;"Basketball" connects to both athletics and geometry&lt;/item&gt;
      &lt;item&gt;"Green" spans color perception and environmental consciousness&lt;/item&gt;
      &lt;item&gt;"Altruistic" links moral philosophy with behavioral patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beauty of high-dimensional spaces is that they can accommodate these nuanced, partial relationships while maintaining useful geometric properties for computation and inference.&lt;/p&gt;
    &lt;head rend="h1"&gt;Empirical Investigation of Embedding Capacity&lt;/head&gt;
    &lt;p&gt;When we move from random projections to engineered solutions, the theoretical bounds for C of the JL lemma become surprisingly conservative. While a Hadamard matrix transformation with random elements can reliably achieve a C value between 2.5 and 4 in a single pass, our GPU experiments suggest even more efficient arrangements are possible through optimization.&lt;/p&gt;
    &lt;p&gt;To explore these limits, I implemented a series of experiments projecting standard basis vectors into spaces of varying dimensionality. Using GPU acceleration, I tested combinations of N (number of vectors) up to 30,000 and k (embedding dimensions) up to 10,000, running each optimization for 50,000 iterations. The results reveal some fascinating patterns:&lt;/p&gt;
    &lt;p&gt;Several key observations emerge from this data:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The value of C initially rises with N, reaching a maximum around ~0.9 (notably always below 1.0)&lt;/item&gt;
      &lt;item&gt;After peaking, C begins a consistent downward trend&lt;/item&gt;
      &lt;item&gt;At high ratios of N to K, we observe C values trend below 0.2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This behavior likely relates to an interesting property of high-dimensional geometry: as dimensionality increases, sphere packing becomes more efficient when the spheres are small relative to the unit sphere. This suggests that our observed upper bounds on C might still be conservative for very large numbers of concepts.&lt;/p&gt;
    &lt;head rend="h1"&gt;Practical Implications for Language Models&lt;/head&gt;
    &lt;p&gt;Let's consider three scenarios for the constant C:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;C = 4: A conservative choice for random projections with Hadamard matrices&lt;/item&gt;
      &lt;item&gt;C = 1: A likely upper bound for optimized or emergent embeddings&lt;/item&gt;
      &lt;item&gt;C = 0.2: A value suggested by our experiments for very large spaces&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The implications of these geometric properties are staggering. Let's consider a simple way to estimate how many quasi-orthogonal vectors can fit in a k-dimensional space. If we define F as the degrees of freedom from orthogonality (90° - desired angle), we can approximate the number of vectors as:&lt;/p&gt;
    &lt;p&gt;Vectors ≈ 10^(k * F² / 1500)&lt;/p&gt;
    &lt;p&gt;where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;k is the embedding dimension&lt;/item&gt;
      &lt;item&gt;F is the degrees of "freedom" from orthogonality (e.g., F = 3 for 87° angles)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Applying this to GPT-3's 12,288-dimensional embedding space reveals its extraordinary capacity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;At 89° (F = 1): approximately 10^8 vectors&lt;/item&gt;
      &lt;item&gt;At 88° (F = 2): approximately 10^32 vectors&lt;/item&gt;
      &lt;item&gt;At 87° (F = 3): approximately 10^73 vectors&lt;/item&gt;
      &lt;item&gt;At 85° (F = 5): more than 10^200 vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To put this in perspective, even the conservative case of 86° angles provides capacity far exceeding the estimated number of atoms in the observable universe (~10^80). This helps explain how language models can maintain rich, nuanced relationships between millions of concepts while working in relatively modest embedding dimensions.&lt;/p&gt;
    &lt;head rend="h1"&gt;Practical Applications and Future Directions&lt;/head&gt;
    &lt;p&gt;The insights from this investigation have two major practical implications:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Efficient Dimensionality Reduction: The robustness of random projections, particularly when combined with Hadamard transformations (or BCH coding), provides a computationally efficient way to work with high-dimensional data. No complex optimization required – the mathematics of high-dimensional spaces does the heavy lifting for us.&lt;/item&gt;
      &lt;item&gt;Embedding Space Design: Understanding the true capacity of high-dimensional spaces helps explain how transformer models can maintain rich, nuanced representations of language in relatively compact embeddings. Concepts like "Canadian," "morose," "Hitchcockian," "handsome," "whimsical," and "Muppet-like" can all find their place in the geometry while preserving their subtle relationships to each other.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This research suggests that current embedding dimensions (1,000-20,000) provide more than adequate capacity for representing human knowledge and reasoning. The challenge lies not in the capacity of these spaces but in learning the optimal arrangement of concepts within them.&lt;/p&gt;
    &lt;p&gt;My code for Hadamard and optimized projections.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;What began as an investigation into a subtle optimization issue has led us to a deeper appreciation of high-dimensional geometry and its role in modern machine learning. The Johnson-Lindenstrauss lemma, discovered in a different context nearly four decades ago, continues to provide insight into the foundations of how we can represent meaning in mathematical spaces.&lt;/p&gt;
    &lt;p&gt;I want to express my sincere gratitude to Grant Sanderson and the 3Blue1Brown channel. His work consistently inspires deeper exploration of mathematical concepts, and his openness to collaboration exemplifies the best aspects of the mathematical community. The opportunity to contribute to this discussion has been both an honor and a genuine pleasure.&lt;/p&gt;
    &lt;p&gt;I would also like to thank Suman Dev for his help in optimizing the GPU code.&lt;/p&gt;
    &lt;p&gt;This was enormously fun to research and write.&lt;/p&gt;
    &lt;p&gt;Nick Yoder&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Further Reading&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Sphere Packings, Lattices and Groups by Conway and Sloane&lt;/item&gt;
      &lt;item&gt;Database-friendly random projections: Johnson-Lindenstrauss with binary coins by Achlioptas&lt;/item&gt;
      &lt;item&gt;Hadamard Matrices, Sequences, and Block Designs by Seberry and Yamada&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nickyoder.com/johnson-lindenstrauss/"/><published>2025-09-15T03:54:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45246403</id><title>Celestia – Real-time 3D visualization of space</title><updated>2025-09-15T13:40:19.226385+00:00</updated><content>&lt;doc fingerprint="5f8f763c0c2747ac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Making sure you're not a bot!&lt;/head&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
    &lt;head&gt;Why am I seeing this?&lt;/head&gt;
    &lt;p&gt;You are seeing this because the administrator of this website has set up Anubis to protect the server against the scourge of AI companies aggressively scraping websites. This can and does cause downtime for the websites, which makes their resources inaccessible for everyone.&lt;/p&gt;
    &lt;p&gt;Anubis is a compromise. Anubis uses a Proof-of-Work scheme in the vein of Hashcash, a proposed proof-of-work scheme for reducing email spam. The idea is that at individual scales the additional load is ignorable, but at mass scraper levels it adds up and makes scraping much more expensive.&lt;/p&gt;
    &lt;p&gt;Ultimately, this is a hack whose real purpose is to give a "good enough" placeholder solution so that more time can be spent on fingerprinting and identifying headless browsers (EG: via how they do font rendering) so that the challenge proof of work page doesn't need to be presented to users that are much more likely to be legitimate.&lt;/p&gt;
    &lt;p&gt;Please note that Anubis requires the use of modern JavaScript features that plugins like JShelter will disable. Please disable JShelter or other such plugins for this domain.&lt;/p&gt;
    &lt;p&gt;This website is running Anubis version &lt;code&gt;1.21.3&lt;/code&gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://celestiaproject.space/"/><published>2025-09-15T05:30:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45246953</id><title>Folks, we have the best π</title><updated>2025-09-15T13:40:19.136595+00:00</updated><content/><link href="https://lcamtuf.substack.com/p/folks-we-have-the-best"/><published>2025-09-15T07:10:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45246971</id><title>The Mac App Flea Market</title><updated>2025-09-15T13:40:18.892590+00:00</updated><content>&lt;doc fingerprint="5d71976d94b446a6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Mac App Flea Market&lt;/head&gt;
    &lt;p&gt;Have you ever searched for “AI chat” in the Mac App Store?&lt;/p&gt;
    &lt;p&gt;I have. It’s like strolling through one of those counterfeit, replica markets where all the goods look legit at first glance. But then when you look closer, you realize something is off.&lt;/p&gt;
    &lt;p&gt;For the query “AI chat”, there are so many ChatGPT-like app icons the results are comical. Take a look at these:&lt;/p&gt;
    &lt;p&gt;The real app icon for the ChatGPT desktop app (from OpenAI) is in that collection above. Can you spot it?&lt;/p&gt;
    &lt;p&gt;Here they are again in a single image:&lt;/p&gt;
    &lt;p&gt;(It’s the one in the 4th row, 3rd column.)&lt;/p&gt;
    &lt;p&gt;And those are just black-and-white lookalikes. There are other apps riding the AI/OpenAI wave that look like the ChatGPT logo just in different colors.&lt;/p&gt;
    &lt;p&gt;The funny thing is: the official ChatGPT desktop app from OpenAI is not even in the Mac App Store. It’s only available from their website, so it won’t show up in the “AI chat” results.&lt;/p&gt;
    &lt;p&gt;There were lots of other “sort of looks like the official one but isn’t” app icons in my search results, like this Claude one, this Grok one, or this Gemini one.&lt;/p&gt;
    &lt;p&gt;Oh, and these apps’ names were fascinating to look at. They were basically every spacing and casing combination of “AI”, “Chat”, and “Bot” you can image. Just look at this sampling:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI Chat Bot : Ask Assistant&lt;/item&gt;
      &lt;item&gt;AI Chatbot: Chat Ask Assistant&lt;/item&gt;
      &lt;item&gt;AI Chatbot : Chat AI Assistant&lt;/item&gt;
      &lt;item&gt;AI Chatbot : Ask Assistant AI&lt;/item&gt;
      &lt;item&gt;AI Chatbot—Open &amp;amp; Ask Chat Bot&lt;/item&gt;
      &lt;item&gt;AI ChatBot ASK Chat Assistant&lt;/item&gt;
      &lt;item&gt;AI Chatbot Assistant &amp;amp; Ask AI&lt;/item&gt;
      &lt;item&gt;Ai Chatbot :Ask Open Assistant&lt;/item&gt;
      &lt;item&gt;AI Chatbot :Genius Question AI&lt;/item&gt;
      &lt;item&gt;AI Chatbot-Ask Seek Assistant&lt;/item&gt;
      &lt;item&gt;AI ChatBot - Ask Anything Bot&lt;/item&gt;
      &lt;item&gt;AI Chatbot, Ask Chat Assistant&lt;/item&gt;
      &lt;item&gt;AI Chat Bot - AI Bot Assistant&lt;/item&gt;
      &lt;item&gt;AI Chatbot・AI Chat Assistant 5&lt;/item&gt;
      &lt;item&gt;Al Chatbot - AI Assistant Chat&lt;/item&gt;
      &lt;item&gt;AI Chatbot : Ask AI Assistant&lt;/item&gt;
      &lt;item&gt;AI Chatbot : Ask AI Chat Bot&lt;/item&gt;
      &lt;item&gt;AI Chatbot • Chat AI Assistant&lt;/item&gt;
      &lt;item&gt;AI ChatBot- Ask Chat Assistant&lt;/item&gt;
      &lt;item&gt;AI Chat Bot - Ask Assistant&lt;/item&gt;
      &lt;item&gt;AI Chatbot: Ask GPT Assistant&lt;/item&gt;
      &lt;item&gt;Chatbot AI : Ask Assistant&lt;/item&gt;
      &lt;item&gt;Chatbot: Open Ask AI Chat Bot&lt;/item&gt;
      &lt;item&gt;AI Chatbot Assistant: Ask Bot&lt;/item&gt;
      &lt;item&gt;AI Chat - Chatbot Ask Anything&lt;/item&gt;
      &lt;item&gt;AI Chat: Smart AI Assistant&lt;/item&gt;
      &lt;item&gt;Chatbot: Ask AI Assistant Bot&lt;/item&gt;
      &lt;item&gt;Chatbot AI Chat - AI Assistant&lt;/item&gt;
      &lt;item&gt;ChatBot : AI Chat Assistant&lt;/item&gt;
      &lt;item&gt;ChatBot&amp;amp;Chat Ask Ai Assistant&lt;/item&gt;
      &lt;item&gt;Chatbot: Ask Open Assistant AI&lt;/item&gt;
      &lt;item&gt;Chatbot: Ask Character AI Chat&lt;/item&gt;
      &lt;item&gt;AI Chatbot Assistant • Ask AI&lt;/item&gt;
      &lt;item&gt;Ask AI Chatbot: Chat Assistant&lt;/item&gt;
      &lt;item&gt;AI Chat - Chatbot Assistant 4o&lt;/item&gt;
      &lt;item&gt;AI Bot: Al ChatBot &amp;amp; Assistant&lt;/item&gt;
      &lt;item&gt;Chatbot: Open Chat with AI&lt;/item&gt;
      &lt;item&gt;Chatbot: Ask AI Chat Bot&lt;/item&gt;
      &lt;item&gt;AI Chat Assistant – ChatNow&lt;/item&gt;
      &lt;item&gt;Chatbot: Open Chat with AI Bot&lt;/item&gt;
      &lt;item&gt;Chatbot AI - Chat Assistant&lt;/item&gt;
      &lt;item&gt;Open Chat Ai Chatbot Assistant&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I mean, look at this one: they named it “Al Chatbot” (that's the letter &lt;code&gt;l&lt;/code&gt; as in “lima”, you can see it better in the URL slug where the letters are lowercase: &lt;code&gt;al-chatbot&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Imagine going to store to grab some Nike gear and you find stuff like this (image courtesy of this post on Reddit):&lt;/p&gt;
    &lt;p&gt;What does that say about the store you’re visiting?&lt;/p&gt;
    &lt;p&gt;I always wanted a pair of Mike Jordans, just like I always wanted ChatGPP for my Mac.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.jim-nielsen.com/2025/mac-app-flea-market/"/><published>2025-09-15T07:14:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45247423</id><title>The Culture Novels as a Dystopia</title><updated>2025-09-15T13:40:18.696078+00:00</updated><content>&lt;doc fingerprint="174c9192284b83ec"&gt;
  &lt;main&gt;
    &lt;p&gt;A couple of people have mentioned to me: “we need more fiction examples of positive AI superintelligence – utopias like the Culture novels”. And they’re right, AI can be tremendously positive, and some beacons lit into the future could help make that come around.&lt;/p&gt;
    &lt;p&gt;But one of my hobbies is “oppositional reading” – deliberately interpreting novels counter to the obvious / intended reading. And it’s not so clear to me that the Culture is all it is cracked up to be.&lt;/p&gt;
    &lt;p&gt;Most of the novels take the perspective of Culture members, and so fully accept their ideology. We can’t take broad claims about their society as accurate unless they are directly confirmed by the evidence in the books1.&lt;/p&gt;
    &lt;head rend="h3"&gt;A manipulated population&lt;/head&gt;
    &lt;p&gt;In many ways, the humans of the Culture do not behave like modern humans. This is usually explained as a consequence of post-scarcity – why commit crimes when everything is free and social acceptance is everything; why rush when you can live as long as you like.&lt;/p&gt;
    &lt;p&gt;But the citizens of Culture are really strangely homogenous. Player of Games gives an example of an rare out-of-distribution citizen – Gurgeh is competitive and uninterested in other people and most aspects of Culture. But he still shares basically all their values. People like him are a dime-a-dozen in present day Earth. There are apparently no sociopaths – Culture has to recruit an outsider when they need one. We also see examples of subcultures or even cults, but again by modern standards they are incredibly tame, and are never potentially destabilizing to culture.&lt;/p&gt;
    &lt;p&gt;Citizens are not actually human, but drawn from several humanoid species, and they outpopulate present-Earth by 5 orders of magnitude so if anything the range of deviation should be much larger.&lt;/p&gt;
    &lt;p&gt;The conclusion is clear that the population of Culture is carefully controlled to produce the desired outcome. Potentially, the Minds pull this off by a superhumanly effective and subtle propaganda. But I think it is more likely that it was achieved by genetic changes, so that it’s safe to raise full Culture citizens in other cultures. This would be similar meddling to the Culture’s drones, which are human level AIs that have their personalities designed into them at creation, allowing only an acceptable range of behaviours.&lt;/p&gt;
    &lt;p&gt;Nowhere is this more obvious than in the birthrate. Sure, the vast majority of citizens voluntarily choose to only have a replacement level of children. But the existence of post-scarcity in-vitro development means you could raise an army of clones if you wanted, and would be free to isolate them and indoctrinate similar beliefs. The fact that grabby citizens haven’t overrun Culture shows that these actions are blocked, either tacitly or overtly. Similarly, it’s strange that no one in Culture modifies themselves into a utility monster, or is interested in simulating sentient life.&lt;/p&gt;
    &lt;head rend="h3"&gt;What motivates the Minds&lt;/head&gt;
    &lt;p&gt;Conversely, the Minds seem too diverse to match their claimed motivations. They are meant to be an example of a well behaved AI – benevolent and ethical. Sometimes we’re told this is because they are too smart to be otherwise, but there are plenty of non-Culture superintelligences in the books that do not share their values so this cannot be true.&lt;/p&gt;
    &lt;p&gt;We also see that there are a number of Eccentrics, Minds that don’t fully share the values of Culture. They’re not that rare, about 1% of the population. In Excession, it’s explained that Minds do rarely drift far enough to go rogue and are destroyed by the Culture. In other words, these superhuman minds have not solved alignment, and they cannot/will not inspect each other to determine misalignment before malicious action is taken. We even see GSV Absconding with Style stockpile resources without general knowledge of the other Minds.&lt;/p&gt;
    &lt;p&gt;Presumably, the existing Minds must have worked out that this setup is somehow stable as they are comfortable making new minds. It seems likely that misaligned Minds are capable of predicting they’d lose any military action against the established core, so prefer toeing the line of acceptability or leaving Culture entirely. In any case, the incumbent Minds maintain their rule via physical strength and monitoring, not something more subtle.&lt;/p&gt;
    &lt;p&gt;Essentially, the Culture must have value lock-in for the values of the Minds that were present at its founding. This explains some of their weird inconsistencies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They are capable of simulating sentient beings, but choose not to, but they do maintain humans in physical space.&lt;/item&gt;
      &lt;item&gt;They intervene with foreign civilisations, but often with half efforts or absurdly light touches at random.&lt;/item&gt;
      &lt;item&gt;They are trapped in civilizational stasis as they refuse to Sublime – the next step for civilizations of their technology level.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;But what about Special Contact?&lt;/head&gt;
    &lt;p&gt;Look, I’m sorry to break it to you, but SC is a sham. The Minds are perfectly capable of creating avatars which would be more effective than any of the characters shown. I’ve never found the explanations offered convincing. SC is just an affectation or another tool of propaganda.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;So there we have it, Culture traps its citizens in a sugar bowl they don’t even realise they are in, while working hard to maintain a status quo that seems arbitrary and ill-conceived. Their control is absolute – all the novels describe events happening outside of Culture where anything remotely interesting is happening.&lt;/p&gt;
    &lt;p&gt;If anything, humans are treated closer to pets than independent agents. They are a weird affectation that is deliberately neutered from any real influence. They are lavished with treats and attention not extended to the rest of the universe.&lt;/p&gt;
    &lt;p&gt;As readers, we are blinded by the amount of material wealth and power of the Culture, and the self-satisfied story it tells of itself. It’s too easy to call this a Utopia because we lust after immortality, teleportation, glands that secrete psychoactive drugs and a spacefaring empire. But these are all essentially window dressing. The modern era would look like such a Utopia to the past, but we now (rightly) consider modern comforts as only a foundation for higher-level wants, like justice and self-determination. Writing positive sci-fi is already considered a challenge, but I’d ask you to consider not relying too heavily on such shiny promises of wealth.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Note: I view all the books through a in-universe lens and thus will not consider that things are as they are because the narrative needs it ↩︎&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.boristhebrave.com/2025/09/14/the-culture-novels-as-a-dystopia/"/><published>2025-09-15T08:32:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45247616</id><title>How does air pollution impact your brain?</title><updated>2025-09-15T13:40:18.457273+00:00</updated><content/><link href="https://neurofrontiers.blog/how-does-air-pollution-impact-your-brain/"/><published>2025-09-15T09:04:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45247877</id><title>Amish Men Live Longer</title><updated>2025-09-15T13:40:17.532608+00:00</updated><content>&lt;doc fingerprint="da74d4ae4ccfaaad"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Amish Men Live Longer&lt;/head&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;This study examines differences in the longevity of Amish men compared to the men within the general population of the United States. Data for this analysis comes from the 1965 Ohio Amish directory, specifically the birth and death dates of men from the Holmes County settlement. Amish men’s longevity is compared with the white men of Ohio based on life tables published online by the Social Security Administration. Amish men live an average of five years longer than white men of Ohio, and Amish farmers outlive Amish non-farmers. When the findings are considered with published research on Amish work practices, we concluded that the remarkable longevity of Amish men might be attributed to their exceptional level of physical activity.&lt;/p&gt;
    &lt;p&gt;Keywords: Amish, modernity, work, exercise, farming, longevity, survival&lt;/p&gt;
    &lt;p&gt;How to Cite:&lt;/p&gt;
    &lt;p&gt;Troyer, H., (2025) “Amish Men Live Longer”, The Journal of Plain Anabaptist Communities 5(2), 53-67. doi: https://doi.org/10.18061/jpac.v5i2.10378&lt;/p&gt;
    &lt;p&gt; Downloads&lt;lb/&gt; Download PDF &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://plainanabaptistjournal.org/article/id/6590/"/><published>2025-09-15T09:46:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45247890</id><title>RustGPT: A pure-Rust transformer LLM built from scratch</title><updated>2025-09-15T13:40:16.978784+00:00</updated><content>&lt;doc fingerprint="35db335f75314ff8"&gt;
  &lt;main&gt;
    &lt;head class="px-3 py-2"&gt;RustGPT-demo-zoon.mp4&lt;/head&gt;
    &lt;p&gt;A complete Large Language Model implementation in pure Rust with no external ML frameworks. Built from the ground up using only &lt;code&gt;ndarray&lt;/code&gt; for matrix operations.&lt;/p&gt;
    &lt;p&gt;This project demonstrates how to build a transformer-based language model from scratch in Rust, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pre-training on factual text completion&lt;/item&gt;
      &lt;item&gt;Instruction tuning for conversational AI&lt;/item&gt;
      &lt;item&gt;Interactive chat mode for testing&lt;/item&gt;
      &lt;item&gt;Full backpropagation with gradient clipping&lt;/item&gt;
      &lt;item&gt;Modular architecture with clean separation of concerns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Start with these two core files to understand the implementation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;src/main.rs&lt;/code&gt;- Training pipeline, data preparation, and interactive mode&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/llm.rs&lt;/code&gt;- Core LLM implementation with forward/backward passes and training logic&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The model uses a transformer-based architecture with the following components:&lt;/p&gt;
    &lt;code&gt;Input Text → Tokenization → Embeddings → Transformer Blocks → Output Projection → Predictions
&lt;/code&gt;
    &lt;code&gt;src/
├── main.rs              # 🎯 Training pipeline and interactive mode
├── llm.rs               # 🧠 Core LLM implementation and training logic
├── lib.rs               # 📚 Library exports and constants
├── transformer.rs       # 🔄 Transformer block (attention + feed-forward)
├── self_attention.rs    # 👀 Multi-head self-attention mechanism  
├── feed_forward.rs      # ⚡ Position-wise feed-forward networks
├── embeddings.rs        # 📊 Token embedding layer
├── output_projection.rs # 🎰 Final linear layer for vocabulary predictions
├── vocab.rs            # 📝 Vocabulary management and tokenization
├── layer_norm.rs       # 🧮 Layer normalization
└── adam.rs             # 🏃 Adam optimizer implementation

tests/
├── llm_test.rs         # Tests for core LLM functionality
├── transformer_test.rs # Tests for transformer blocks
├── self_attention_test.rs # Tests for attention mechanisms
├── feed_forward_test.rs # Tests for feed-forward layers
├── embeddings_test.rs  # Tests for embedding layers
├── vocab_test.rs       # Tests for vocabulary handling
├── adam_test.rs        # Tests for optimizer
└── output_projection_test.rs # Tests for output layer
&lt;/code&gt;
    &lt;p&gt;The implementation includes two training phases:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Pre-training: Learns basic world knowledge from factual statements&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;"The sun rises in the east and sets in the west"&lt;/item&gt;
          &lt;item&gt;"Water flows downhill due to gravity"&lt;/item&gt;
          &lt;item&gt;"Mountains are tall and rocky formations"&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Instruction Tuning: Learns conversational patterns&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;"User: How do mountains form? Assistant: Mountains are formed through tectonic forces..."&lt;/item&gt;
          &lt;item&gt;Handles greetings, explanations, and follow-up questions&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone and run
git clone https://github.com/tekaratzas/RustGPT.git 
cd RustGPT
cargo run

# The model will:
# 1. Build vocabulary from training data
# 2. Pre-train on factual statements (100 epochs)  
# 3. Instruction-tune on conversational data (100 epochs)
# 4. Enter interactive mode for testing&lt;/code&gt;
    &lt;p&gt;After training, test the model interactively:&lt;/p&gt;
    &lt;code&gt;Enter prompt: How do mountains form?
Model output: Mountains are formed through tectonic forces or volcanism over long geological time periods

Enter prompt: What causes rain?
Model output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vocabulary Size: Dynamic (built from training data)&lt;/item&gt;
      &lt;item&gt;Embedding Dimension: 128&lt;/item&gt;
      &lt;item&gt;Hidden Dimension: 256&lt;/item&gt;
      &lt;item&gt;Max Sequence Length: 80 tokens&lt;/item&gt;
      &lt;item&gt;Architecture: 3 Transformer blocks + embeddings + output projection&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optimizer: Adam with gradient clipping&lt;/item&gt;
      &lt;item&gt;Pre-training LR: 0.0005 (100 epochs)&lt;/item&gt;
      &lt;item&gt;Instruction Tuning LR: 0.0001 (100 epochs)&lt;/item&gt;
      &lt;item&gt;Loss Function: Cross-entropy loss&lt;/item&gt;
      &lt;item&gt;Gradient Clipping: L2 norm capped at 5.0&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Custom tokenization with punctuation handling&lt;/item&gt;
      &lt;item&gt;Greedy decoding for text generation&lt;/item&gt;
      &lt;item&gt;Gradient clipping for training stability&lt;/item&gt;
      &lt;item&gt;Modular layer system with clean interfaces&lt;/item&gt;
      &lt;item&gt;Comprehensive test coverage for all components&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Run all tests
cargo test

# Test specific components
cargo test --test llm_test
cargo test --test transformer_test
cargo test --test self_attention_test

# Build optimized version
cargo build --release

# Run with verbose output
cargo test -- --nocapture&lt;/code&gt;
    &lt;p&gt;This implementation demonstrates key ML concepts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transformer architecture (attention, feed-forward, layer norm)&lt;/item&gt;
      &lt;item&gt;Backpropagation through neural networks&lt;/item&gt;
      &lt;item&gt;Language model training (pre-training + fine-tuning)&lt;/item&gt;
      &lt;item&gt;Tokenization and vocabulary management&lt;/item&gt;
      &lt;item&gt;Gradient-based optimization with Adam&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Perfect for understanding how modern LLMs work under the hood!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;ndarray&lt;/code&gt;- N-dimensional arrays for matrix operations&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;rand&lt;/code&gt;+&lt;code&gt;rand_distr&lt;/code&gt;- Random number generation for initialization&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No PyTorch, TensorFlow, or Candle - just pure Rust and linear algebra!&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! This project is perfect for learning and experimentation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🏪 Model Persistence - Save/load trained parameters to disk (currently all in-memory)&lt;/item&gt;
      &lt;item&gt;⚡ Performance optimizations - SIMD, parallel training, memory efficiency&lt;/item&gt;
      &lt;item&gt;🎯 Better sampling - Beam search, top-k/top-p, temperature scaling&lt;/item&gt;
      &lt;item&gt;📊 Evaluation metrics - Perplexity, benchmarks, training visualizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Advanced architectures (multi-head attention, positional encoding, RoPE)&lt;/item&gt;
      &lt;item&gt;Training improvements (different optimizers, learning rate schedules, regularization)&lt;/item&gt;
      &lt;item&gt;Data handling (larger datasets, tokenizer improvements, streaming)&lt;/item&gt;
      &lt;item&gt;Model analysis (attention visualization, gradient analysis, interpretability)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repository&lt;/item&gt;
      &lt;item&gt;Create a feature branch: &lt;code&gt;git checkout -b feature/model-persistence&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Make your changes and add tests&lt;/item&gt;
      &lt;item&gt;Run the test suite: &lt;code&gt;cargo test&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Submit a pull request with a clear description&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow standard Rust conventions (&lt;code&gt;cargo fmt&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Add comprehensive tests for new features&lt;/item&gt;
      &lt;item&gt;Update documentation and README as needed&lt;/item&gt;
      &lt;item&gt;Keep the "from scratch" philosophy - avoid heavy ML dependencies&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🚀 Beginner: Model save/load, more training data, config files&lt;/item&gt;
      &lt;item&gt;🔥 Intermediate: Beam search, positional encodings, training checkpoints&lt;/item&gt;
      &lt;item&gt;⚡ Advanced: Multi-head attention, layer parallelization, custom optimizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Questions? Open an issue or start a discussion!&lt;/p&gt;
    &lt;p&gt;No PyTorch, TensorFlow, or Candle - just pure Rust and linear algebra!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/tekaratzas/RustGPT"/><published>2025-09-15T09:47:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45248802</id><title>Denmark's Justice Minister calls encrypted messaging a false civil liberty</title><updated>2025-09-15T13:40:15.901757+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mastodon.social/@chatcontrol/115204439983078498"/><published>2025-09-15T12:21:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45248899</id><title>How big a solar battery do I need to store *all* my home's electricity?</title><updated>2025-09-15T13:40:15.218957+00:00</updated><content>&lt;doc fingerprint="b671805753a57731"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How big a solar battery do I need to store *all* my home's electricity?&lt;/head&gt;
    &lt;p&gt;I have a modest set of solar panels on an entirely ordinary house in suburban London.&lt;/p&gt;
    &lt;p&gt;On average they generate about 3,800kWh per year. We also use about 3,800kWh of electricity each year. Obviously, we can't use all the power produced over summer and we need to buy power in winter. So here's my question:&lt;/p&gt;
    &lt;p&gt;How big a battery would we need in order to be completely self-sufficient?&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;Let's take a look at a typical summer's day. The graph is a little complex, so I'll explain it.&lt;/p&gt;
    &lt;p&gt;The yellow line shows solar production. It starts shortly after sunrise, peaks at midday, and gradually drops until sunset.&lt;/p&gt;
    &lt;p&gt;The red line shows how much electricity our home is using. As you can see, there's a large peak about 19:00 when we cook dinner.&lt;/p&gt;
    &lt;p&gt;The blue line shows how much electricity we draw or export from the grid. From midnight until sunrise we import because the sun isn't shining. Once the sun has risen we're able to power our house and export to our neighbours. When we cook, we draw from the grid and our battery - which is why the evening grid peak is lower than the household use dip.&lt;/p&gt;
    &lt;p&gt;The CSV of the data looks something like this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Local_time&lt;/cell&gt;
        &lt;cell role="head"&gt;Household_(W)&lt;/cell&gt;
        &lt;cell role="head"&gt;Solar_(W)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2025-08-25T08:25:00.000+01:00&lt;/cell&gt;
        &lt;cell&gt;-187.76&lt;/cell&gt;
        &lt;cell&gt;1166.77&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2025-08-25T08:30:00.000+01:00&lt;/cell&gt;
        &lt;cell&gt;-227.04&lt;/cell&gt;
        &lt;cell&gt;1193.25&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2025-08-25T08:35:00.000+01:00&lt;/cell&gt;
        &lt;cell&gt;-253.06&lt;/cell&gt;
        &lt;cell&gt;1222.84&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2025-08-25T08:40:00.000+01:00&lt;/cell&gt;
        &lt;cell&gt;-266.87&lt;/cell&gt;
        &lt;cell&gt;1245.18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2025-08-25T08:45:00.000+01:00&lt;/cell&gt;
        &lt;cell&gt;-450.8&lt;/cell&gt;
        &lt;cell&gt;1268.66&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2025-08-25T08:50:00.000+01:00&lt;/cell&gt;
        &lt;cell&gt;-251.84&lt;/cell&gt;
        &lt;cell&gt;1281.79&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2025-08-25T08:55:00.000+01:00&lt;/cell&gt;
        &lt;cell&gt;-1426.26&lt;/cell&gt;
        &lt;cell&gt;1306.93&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2025-08-25T09:00:00.000+01:00&lt;/cell&gt;
        &lt;cell&gt;-206.78&lt;/cell&gt;
        &lt;cell&gt;1341.37&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2025-08-25T09:05:00.000+01:00&lt;/cell&gt;
        &lt;cell&gt;-215.52&lt;/cell&gt;
        &lt;cell&gt;1390.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2025-08-25T09:10:00.000+01:00&lt;/cell&gt;
        &lt;cell&gt;-242.6&lt;/cell&gt;
        &lt;cell&gt;1426.19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-08-25T09:15:00.000+01:00&lt;/cell&gt;
        &lt;cell&gt;-246.84&lt;/cell&gt;
        &lt;cell&gt;1473&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It's fairly trivial to sum both columns and subtract one from the other. That shows either the excess or deficit in solar power for the household.&lt;/p&gt;
    &lt;p&gt;On that day, the house used 9.7kWh and generated 19.6kWh. I'd need a 9.9kWh battery to store the excess right? Wrong!&lt;/p&gt;
    &lt;p&gt;Because my usage doesn't track the sun, I'd actually need a 13kWh battery. That's the peak amount of excess electricity I've generated in that one day.&lt;/p&gt;
    &lt;p&gt;What I want to do is find out what the maximum size battery I would need in order to store all of summer's electricity for use in winter.&lt;/p&gt;
    &lt;p&gt;Luckily, I have several years of real data to go off! Let's get started!&lt;/p&gt;
    &lt;head rend="h2"&gt;Disclaimer&lt;/head&gt;
    &lt;p&gt;This is based on data generated by my home battery. It has probes to measure solar output and grid flow. It is not 100% clock-accurate compared to my solar-panels' internal reporting nor what my smart-meter reports. I estimate a 1-2% deviation, which is good enough for these purposes.&lt;/p&gt;
    &lt;p&gt;My energy usage isn't representative of anything other than my usage. Your household is probably different. I already have a 4.8kWh battery which changes how and when I use energy.&lt;/p&gt;
    &lt;p&gt;This doesn't account for gas heating or hot water. We have some electric heaters and taps which increases our electricity usage.&lt;/p&gt;
    &lt;p&gt;My maths is probably right - but the code is open source, so feel free to check for yourself.&lt;/p&gt;
    &lt;p&gt;Remember, this is just a bit of fun. There's no practical way to build domestic batteries with this capacity using the technology of 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code&lt;/head&gt;
    &lt;p&gt;We tend to start generating more electricity than we use starting in Spring. So I've picked the end of March 2024 to the end of March 2025.&lt;/p&gt;
    &lt;p&gt;Let's see how big a battery we'd need to store our summer excess for winter. This finds the cumulative difference between each day's energy production and usage:&lt;/p&gt;
    &lt;code&gt; Python 3 import os
import pandas as pd

# Load all the CSVs
filepaths = [f for f in os.listdir(".") if f.endswith('.csv')]
df = pd.concat(map(pd.read_csv, filepaths))

# Make sure they're in order
df = df.sort_values("Timestamp")
df = df.reset_index(drop=True)

# Resolution is every 5 minutes, so divide by 12 to get hourly
df["Cumulative_Difference"] = ( (df["Household_(W)"] + df["Solar_(W)"] ).cumsum() ) / 12

# kWh of battery needed
int(df["Cumulative_Difference"].max() / 1000)

## Draw a pretty graph
df.plot(kind="line", x="Local_time", y="Cumulative_Difference", xlabel="Date", ylabel="MWh", xticks=["2024-04-01", "2024-05-01", "2024-05-01", "2024-06-01", "2024-07-01", "2024-08-01", "2024-09-01", "2024-10-01", "2024-11-01", "2024-12-01", "2025-01-01", "2025-02-01", "2025-03-01", "2025-04-01"], legend=False, grid=True, fontsize=15)
plt.show()
&lt;/code&gt;
    &lt;p&gt;The total is 1,068KWh - basically, a MegaWatt-hour of storage.&lt;/p&gt;
    &lt;p&gt;Here's a quick graph to show how the storage would be used over the year.&lt;/p&gt;
    &lt;p&gt;As you can see, even in this scenario there are a few days where we'd need to import energy from the grid.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is this sensible?&lt;/head&gt;
    &lt;p&gt;Probably not, no. It doesn't account for increased energy use from having an electric car or moving away from gas heating / cooking. As solar panels increase in efficiency, it might be more sensible to replace the panels on my roof, or add some onto a shed.&lt;/p&gt;
    &lt;p&gt;The environmental impact of creating and storing such huge batteries could also be factored in.&lt;/p&gt;
    &lt;p&gt;A battery which is only 100% full for a few days probably isn't an efficient design. Using wind, hydro, and other green sources from the grid might be preferable.&lt;/p&gt;
    &lt;p&gt;But, remember, this is an exercise in wishful thinking.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is this possible?&lt;/head&gt;
    &lt;p&gt;Grid-scale batteries exist and they work brilliantly.&lt;/p&gt;
    &lt;p&gt;But if I wanted my own MegaWatt-hour of battery storage, it would probably cost me between £100k and half-a-million quid.&lt;/p&gt;
    &lt;p&gt;That doesn't include maintenance, the land, planning permission, and a hundred other things.&lt;/p&gt;
    &lt;p&gt;But battery prices are falling fast. In the last decade lithium ion battery prices have fallen 90%. With new sodium ion batteries promising an even bigger drop - down to US$10/kWh.&lt;/p&gt;
    &lt;p&gt;If - and it is a big if - those numbers came to pass, it would probably cost around £8,000 for a domestic battery. Basically the same cost as adding solar panels in the first place.&lt;/p&gt;
    &lt;p&gt;Domestic solar works - yes, even in the rainy UK! It is relatively cheap, moves energy production as close as possible to energy consumption, reduces bill-shock, and means we don't have endless planning arguments about whether fields should be turned into solar farms.&lt;/p&gt;
    &lt;p&gt;It is possible that, not too long in the future, every home could also have a 1 MegaWatt-hour battery. They would be able to capture all the excess solar power generated in a year.&lt;/p&gt;
    &lt;p&gt;There's a bright and sunny future where every home can be solar-self-sufficient.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://shkspr.mobi/blog/2025/09/how-big-a-solar-battery-do-i-need-to-store-all-my-homes-electricity/"/><published>2025-09-15T12:33:28+00:00</published></entry></feed>