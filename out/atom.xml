<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-12T00:50:16.599941+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45889891</id><title>Cache-friendly, low-memory Lanczos algorithm in Rust</title><updated>2025-11-12T00:50:24.826541+00:00</updated><content>&lt;doc fingerprint="91155a0e0fae71f1"&gt;
  &lt;main&gt;
    &lt;p&gt;The standard Lanczos method for computing matrix functions has a brutal memory requirement: storing an basis matrix that grows with every iteration. For a -variable problem needing iterations, that’s roughly 4 GB just for the basis.&lt;/p&gt;
    &lt;p&gt;In this post, we will explore one of the most straightforward solutions to this problem: a two-pass variant of the Lanczos algorithm that only requires memory at the cost of doubling the number of matrix-vector products. The surprising part is that when implemented carefully, the two-pass version isn’t just memory-efficient—it can be faster for certain problems. We will dig into why.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All code is available on GitHub: two-pass-lanczos&lt;/item&gt;
      &lt;item&gt;The full technical report with proofs and additional experiments: report.pdf&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;head&gt;Open Table of Contents&lt;/head&gt;
    &lt;head rend="h1"&gt;Computing Matrix Functions&lt;/head&gt;
    &lt;p&gt;Let’s consider the problem of computing the action of matrix functions on a vector:&lt;/p&gt;
    &lt;p&gt;where is a large sparse Hermitian matrix and is a matrix function defined on the spectrum of . This is a problem that appears pretty often in scientific computing: solving linear systems corresponds to , exponential integrators for PDEs use , and many other problems require functions like or .&lt;/p&gt;
    &lt;p&gt;Indeed, there are a lot problems with computing directly. First of all, even if is sparse, is generally dense. Storing it explicitly is out of the question for large problems. Even if we could store it, computing it directly would require algorithms like the Schur-Parlett method that scale as , which is impractical for large .&lt;/p&gt;
    &lt;p&gt;However we know that given any matrix function defined on the spectrum of , we can express as a polynomial in of degree at most (the size of the matrix) such that (this is a consequence of the Cayley-Hamilton theorem). This polynomial interpolates and its derivatives in the Hermitian sense at the eigenvalues of .&lt;/p&gt;
    &lt;p&gt;This gives us a good and a bad news: the good news is that, well, we can express as a polynomial in . The bad news is that the degree of this polynomial can be as high as , which is huge for large problems. The idea is then to find a low-degree polynomial approximation to that is good enough for our purposes. If we can find a polynomial of degree such that , then we can approximate the solution as:&lt;/p&gt;
    &lt;p&gt;This polynomial only involves vectors within a specific subspace.&lt;/p&gt;
    &lt;head rend="h2"&gt;Krylov Projection&lt;/head&gt;
    &lt;p&gt;We can notice that only depends on vectors in the Krylov subspace of order&lt;/p&gt;
    &lt;p&gt;This is fortunate: we can compute an approximate solution by staying within this space, which only requires repeated matrix-vector products with . For large sparse matrices, that’s the only operation we can do efficiently anyway.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We don’t need to construct explicitly. We compute iteratively: .&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But there’s a problem: the raw vectors form a terrible basis. They quickly become nearly parallel, making any computation numerically unstable. We need an orthonormal basis.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building an Orthonormal Basis&lt;/head&gt;
    &lt;p&gt;The standard method is the Arnoldi process, which is Gram-Schmidt applied to Krylov subspaces. We start by normalizing . Then, iteratively:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute a new candidate:&lt;/item&gt;
      &lt;item&gt;Orthogonalize against all existing basis vectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The coefficients become entries of a projected matrix. After iterations, we have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;: an orthonormal basis for&lt;/item&gt;
      &lt;item&gt;: an upper Hessenberg matrix representing the projection of onto this subspace&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can express this relationship with the Arnoldi decomposition:&lt;/p&gt;
    &lt;head rend="h3"&gt;Solving in the Reduced Space&lt;/head&gt;
    &lt;p&gt;Now we approximate our original problem by solving it in the small -dimensional space. Using the Full Orthogonal Method (FOM), we enforce that the residual is orthogonal to the Krylov subspace. This gives:&lt;/p&gt;
    &lt;p&gt;where is computed as:&lt;/p&gt;
    &lt;p&gt;The heavy lifting is now on computing , a small matrix. Since , we can afford direct methods like Schur-Parlett ().&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For (linear systems), this reduces to solving with LU decomposition.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;The Lanczos Algorithm&lt;/head&gt;
    &lt;p&gt;When is Hermitian (or symmetric in the real case), the general Arnoldi process simplifies dramatically. We can prove that must also be Hermitian. A matrix that is both upper Hessenberg and Hermitian must be real, symmetric, and tridiagonal. This is a huge simplification.&lt;/p&gt;
    &lt;p&gt;In the literature, this projected matrix is denoted to highlight its tridiagonal structure:&lt;/p&gt;
    &lt;p&gt;where are the diagonal elements and are the off-diagonals (subdiagonals from the orthogonalization).&lt;/p&gt;
    &lt;head rend="h2"&gt;Three-Term Recurrence&lt;/head&gt;
    &lt;p&gt;This tridiagonal structure leads to a beautiful simplification. To build the next basis vector , we don’t need the entire history of vectors. We only need the two previous ones. Since is Hermitian, this guarantees that any new vector is automatically orthogonal to all earlier vectors (beyond the previous two). So we can skip the full orthogonalization and use a simple three-term recurrence:&lt;/p&gt;
    &lt;p&gt;Rearranging gives us an algorithm to compute directly:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the candidate:&lt;/item&gt;
      &lt;item&gt;Extract the diagonal coefficient:&lt;/item&gt;
      &lt;item&gt;Orthogonalize against the two previous vectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize: and&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is known as the Lanczos algorithm. It’s more efficient than Arnoldi because each iteration only orthogonalizes against two previous vectors instead of all prior ones.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reconstructing the Solution&lt;/head&gt;
    &lt;p&gt;After iterations, we end up with the tridiagonal matrix and all basis vectors . We can then reconstruct the approximate solution as:&lt;/p&gt;
    &lt;p&gt;where is solved from the small tridiagonal matrix.&lt;/p&gt;
    &lt;p&gt;There is a timing problem however: we cannot compute the coefficients until all iterations are complete. The full matrix is only available at the end, so we must store every basis vector along the way, leading to a memory cost of .&lt;/p&gt;
    &lt;p&gt;So we’re left with a choice: whether we store all the basis vectors and solve the problem in passes, or find a way to avoid storing them. There is a middle ground.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are also techniques to compress the basis vectors, have a look here&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;Two-Pass Algorithm&lt;/head&gt;
    &lt;p&gt;Here’s where we break the timing deadlock. The insight that we don’t actually need to store the basis vectors if we can afford to compute them twice&lt;/p&gt;
    &lt;p&gt;Think about what we have after the first pass. We’ve computed all the and coefficients that compose the entire tridiagonal matrix . These numbers are small compared to the full basis. What if we kept only these scalars, discarded all the vectors, and then replayed the Lanczos recurrence a second time? We’d regenerate the same basis, and this time we’d use it to build the solution.&lt;/p&gt;
    &lt;p&gt;This comes at a cost. We run Lanczos twice, so we pay for matrix-vector products instead of . But we only ever store a constant number of vectors in memory, no basis matrix. The memory complexity drops to .&lt;/p&gt;
    &lt;p&gt;It sounds like a bad trade at first. But as we’ll see later, the cache behavior of this two-pass approach can actually make it as fast (or even faster) on real hardware if well optimized.&lt;/p&gt;
    &lt;head rend="h2"&gt;First Pass: Compute the Projected Problem&lt;/head&gt;
    &lt;p&gt;We initialize and set , .Then we run the standard Lanczos recurrence:&lt;/p&gt;
    &lt;p&gt;At each step, we record and . But we do not store . Instead, we discard it immediately after computing . In this way we only keep in memory at most just three vectors at any time (, , and the working vector ).&lt;/p&gt;
    &lt;p&gt;After iterations, we have the full set . These scalars define the tridiagonal matrix . We can now solve:&lt;/p&gt;
    &lt;p&gt;This is the solution in the reduced space. Now that we have the coefficients we need to build .&lt;/p&gt;
    &lt;head rend="h2"&gt;Second Pass: Reconstruct and Accumulate&lt;/head&gt;
    &lt;p&gt;With in memory, we replay the Lanczos recurrence exactly as before. We start with the same initialization (, , ) and apply the same sequence of operations, using the stored scalars and to reconstruct each basis vector on demand. We can write some rust-like pseudocode for this second pass to get a feel for it:&lt;/p&gt;
    &lt;code&gt;let mut x_k = vec![0.0; n];
let mut v_prev = vec![0.0; n];
let mut v_curr = b.clone() / b_norm;

for j in 1..=k {
    let w = A @ v_curr;  // Matrix-vector product

    // We don't recompute alpha/beta; we already have them from pass 1
    let alpha_j = alphas[j - 1];
    let beta_prev = j &amp;gt; 1 ? betas[j - 2] : 0.0;

    // Accumulate the solution
    x_k += y_k[j - 1] * v_curr;

    // Regenerate the next basis vector for the *next* iteration
    let v_next = (w - alpha_j * v_curr - beta_prev * v_prev) / betas[j - 1];

    // Slide the window forward
    v_prev = v_curr;
    v_curr = v_next;
}&lt;/code&gt;
    &lt;p&gt;This loop regenerates each on demand and immediately uses it to update the solution. Once we’ve accumulated into , we discard the vector. We never store the full basis.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Subtle Numerical Point&lt;/head&gt;
    &lt;p&gt;There is one detail worth noting: floating-point arithmetic is deterministic. When we replay the Lanczos recurrence in the second pass with the exact same inputs and the exact same order of operations, we get bitwise-identical vectors. The regenerated in pass 2 are identical to the ones computed in pass 1.&lt;/p&gt;
    &lt;p&gt;However, the order in which we accumulate the solution differs. In a standard Lanczos, is built as a single matrix-vector product: (a &lt;code&gt;gemv&lt;/code&gt; call in BLAS). In the two-pass method, it’s built as a loop of scaled vector additions (a series of &lt;code&gt;axpy&lt;/code&gt; calls). These operations accumulate rounding error differently, so the final solution differs slightly, typically by machine epsilon. This rarely matters in practice, and convergence is unaffected.&lt;/p&gt;
    &lt;head rend="h1"&gt;Implementation&lt;/head&gt;
    &lt;p&gt;Building this in Rust forces us to think concretely about where data lives and how it flows through the cache hierarchy. We need to control memory layout, decide when allocations happen, and choose abstractions that cost us nothing at runtime.&lt;/p&gt;
    &lt;p&gt;For linear algebra, we reach for &lt;code&gt;faer&lt;/code&gt;. Three design choices in this library matter for what we’re building:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stack allocation via &lt;code&gt;MemStack&lt;/code&gt;: Pre-allocated scratch space that lives for the entire computation. The hot path becomes allocation-free.&lt;/item&gt;
      &lt;item&gt;Matrix-free operators: The &lt;code&gt;LinOp&lt;/code&gt;trait defines an operator by its action (&lt;code&gt;apply&lt;/code&gt;) without materializing a matrix. For large sparse problems, this is the only viable approach.&lt;/item&gt;
      &lt;item&gt;SIMD-friendly loops: The &lt;code&gt;zip!&lt;/code&gt;macro generates code that compiles to packed instructions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Recurrence Step&lt;/head&gt;
    &lt;p&gt;Our starting point is the Lanczos three-term recurrence that we derived earlier:&lt;/p&gt;
    &lt;p&gt;We can translate this into a recurrence step function. The signature looks like this:&lt;/p&gt;
    &lt;code&gt;fn lanczos_recurrence_step&amp;lt;T: ComplexField, O: LinOp&amp;lt;T&amp;gt;&amp;gt;(
    operator: &amp;amp;O,
    mut w: MatMut&amp;lt;'_, T&amp;gt;,
    v_curr: MatRef&amp;lt;'_, T&amp;gt;,
    v_prev: MatRef&amp;lt;'_, T&amp;gt;,
    beta_prev: T::Real,
    stack: &amp;amp;mut MemStack,
) -&amp;gt; (T::Real, Option&amp;lt;T::Real&amp;gt;)&lt;/code&gt;
    &lt;p&gt;The function is generic over the field type &lt;code&gt;T&lt;/code&gt; (&lt;code&gt;f64&lt;/code&gt;, &lt;code&gt;c64&lt;/code&gt;, etc.) and the operator type &lt;code&gt;O&lt;/code&gt;. It operates on matrix views (&lt;code&gt;MatMut&lt;/code&gt; and &lt;code&gt;MatRef&lt;/code&gt;) to avoid unnecessary data copies. The return type gives us the diagonal element  and, if no breakdown occurs, the off-diagonal .&lt;/p&gt;
    &lt;p&gt;Now we can implement the body by following the math. The first step is the most expensive:&lt;/p&gt;
    &lt;code&gt;// 1. Apply operator: w = A * v_curr
operator.apply(w.rb_mut(), v_curr, Par::Seq, stack);&lt;/code&gt;
    &lt;p&gt;The matrix-vector product dominates the computational cost. Everything else is secondary.&lt;/p&gt;
    &lt;p&gt;Next, we orthogonalize against . This is where we benefit from &lt;code&gt;faer&lt;/code&gt;’s design. The &lt;code&gt;zip!&lt;/code&gt; macro fuses this operation into a single loop that the compiler vectorizes into SIMD instructions.&lt;/p&gt;
    &lt;code&gt;// 2. Orthogonalize against v_{j-1}: w -= β_{j-1} * v_{j-1}
let beta_prev_scaled = T::from_real_impl(&amp;amp;beta_prev);
zip!(w.rb_mut(), v_prev).for_each(|unzip!(w_i, v_prev_i)| {
    *w_i = sub(w_i, &amp;amp;mul(&amp;amp;beta_prev_scaled, v_prev_i));
});&lt;/code&gt;
    &lt;p&gt;With &lt;code&gt;w&lt;/code&gt; partially orthogonalized, we can compute the diagonal coefficient via an inner product. Since  is Hermitian,  is guaranteed real.&lt;/p&gt;
    &lt;code&gt;// 3. Compute α_j = v_j^H * w
let alpha = T::real_part_impl(&amp;amp;(v_curr.adjoint() * w.rb())[(0, 0)]);&lt;/code&gt;
    &lt;p&gt;We complete the orthogonalization against with another &lt;code&gt;zip!&lt;/code&gt; loop.&lt;/p&gt;
    &lt;code&gt;// 4. Orthogonalize against v_j: w -= α_j * v_j
let alpha_scaled = T::from_real_impl(&amp;amp;alpha);
zip!(w.rb_mut(), v_curr).for_each(|unzip!(w_i, v_curr_i)| {
    *w_i = sub(w_i, &amp;amp;mul(&amp;amp;alpha_scaled, v_curr_i));
});&lt;/code&gt;
    &lt;p&gt;Now &lt;code&gt;w&lt;/code&gt; holds the unnormalized next basis vector. We compute its norm to get . If this norm is numerically zero, the Krylov subspace is invariant, the iteration has reached its natural stopping point. This is called breakdown.&lt;/p&gt;
    &lt;code&gt;// 5. Compute β_j = ||w||_2 and check for breakdown
let beta = w.rb().norm_l2();
let tolerance = breakdown_tolerance::&amp;lt;T::Real&amp;gt;();

if beta &amp;lt;= tolerance {
    (alpha, None)
} else {
    (alpha, Some(beta))
}&lt;/code&gt;
    &lt;p&gt;The function returns &lt;code&gt;None&lt;/code&gt; for  when breakdown occurs, signaling to the caller that no further iterations should proceed.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Iterator for State Management&lt;/head&gt;
    &lt;p&gt;The recurrence step is a pure function, but calling it in a loop is both inefficient and awkward. We’d need to manually pass vectors in and out of each iteration. More critically, we’d create copies when we should be reusing memory.&lt;/p&gt;
    &lt;p&gt;The iterator pattern solves this. We create a struct that encapsulates the state:&lt;/p&gt;
    &lt;code&gt;struct LanczosIteration&amp;lt;'a, T: ComplexField, O: LinOp&amp;lt;T&amp;gt;&amp;gt; {
    operator: &amp;amp;'a O,
    v_prev: Mat&amp;lt;T&amp;gt;,       // v_{j-1}
    v_curr: Mat&amp;lt;T&amp;gt;,       // v_j
    work: Mat&amp;lt;T&amp;gt;,         // Workspace for the next vector
    beta_prev: T::Real,   // β_{j-1}
    // ... iteration counters
}&lt;/code&gt;
    &lt;p&gt;The main design choice here is that vectors are owned (&lt;code&gt;Mat&amp;lt;T&amp;gt;&lt;/code&gt;), not borrowed. This enables an optimization in the &lt;code&gt;next_step&lt;/code&gt; method. After computing the next vector and normalizing it into &lt;code&gt;work&lt;/code&gt;, we cycle the state without allocating or copying:&lt;/p&gt;
    &lt;code&gt;// Inside next_step, after normalization...
core::mem::swap(&amp;amp;mut self.v_prev, &amp;amp;mut self.v_curr);
core::mem::swap(&amp;amp;mut self.v_curr, &amp;amp;mut self.work);&lt;/code&gt;
    &lt;p&gt;On x86-64, swapping two &lt;code&gt;Mat&amp;lt;T&amp;gt;&lt;/code&gt; structures (fat pointers) compiles to three &lt;code&gt;mov&lt;/code&gt; instructions. The pointers change, but no vector data moves. After the swap, &lt;code&gt;v_prev&lt;/code&gt; points to what &lt;code&gt;v_curr&lt;/code&gt; held, &lt;code&gt;v_curr&lt;/code&gt; points to &lt;code&gt;work&lt;/code&gt;’s allocation, and &lt;code&gt;work&lt;/code&gt; points to the old &lt;code&gt;v_prev&lt;/code&gt; data. In the next iteration, &lt;code&gt;work&lt;/code&gt; gets reused.&lt;/p&gt;
    &lt;p&gt;We keep exactly three n-dimensional vectors live in memory. The same allocations cycle through the computation, staying hot in L1 cache. This is the core reason the two-pass method can be faster than expected, the working set never leaves cache.&lt;/p&gt;
    &lt;head rend="h2"&gt;First Pass: Computing the Decomposition&lt;/head&gt;
    &lt;p&gt;The first pass runs the Lanczos iteration and collects the coefficients . Basis vectors are discarded after each step.&lt;/p&gt;
    &lt;code&gt;pub fn lanczos_pass_one&amp;lt;T: ComplexField&amp;gt;(
    operator: &amp;amp;impl LinOp&amp;lt;T&amp;gt;,
    b: MatRef&amp;lt;'_, T&amp;gt;,
    k: usize,
    stack: &amp;amp;mut MemStack,
) -&amp;gt; Result&amp;lt;LanczosDecomposition&amp;lt;T::Real&amp;gt;, LanczosError&amp;gt; {
    // ...
}&lt;/code&gt;
    &lt;p&gt;We allocate vectors for the coefficients with a capacity hint to avoid reallocations:&lt;/p&gt;
    &lt;code&gt;let mut alphas = Vec::with_capacity(k);
let mut betas = Vec::with_capacity(k - 1);&lt;/code&gt;
    &lt;p&gt;Then we construct the iterator. This allocates the three work vectors once. After this point, the hot path is allocation-free:&lt;/p&gt;
    &lt;code&gt;let mut lanczos_iter = LanczosIteration::new(operator, b, k, b_norm)?;

for i in 0..k {
    if let Some(step) = lanczos_iter.next_step(stack) {
        alphas.push(step.alpha);
        steps_taken += 1;

        let tolerance = breakdown_tolerance::&amp;lt;T::Real&amp;gt;();
        if step.beta &amp;lt;= tolerance {
            break;
        }

        if i &amp;lt; k - 1 {
            betas.push(step.beta);
        }
    } else {
        break;
    }
}&lt;/code&gt;
    &lt;p&gt;The check for breakdown stops the iteration when the residual becomes numerically zero. This means we’ve found an invariant subspace and there’s no value in continuing.&lt;/p&gt;
    &lt;p&gt;At the end, we collect the scalars into a &lt;code&gt;LanczosDecomposition&lt;/code&gt; struct. The memory footprint throughout this pass is constant: three n-dimensional vectors plus two small arrays that grow to at most  elements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Second Pass: Reconstructing the Solution&lt;/head&gt;
    &lt;p&gt;Now we face a different problem. We have the coefficients from the first pass and the coefficient vector from solving the projected problem. We need to reconstruct the solution:&lt;/p&gt;
    &lt;p&gt;without storing the full basis matrix .&lt;/p&gt;
    &lt;p&gt;The recurrence step in this pass is structurally similar to the first pass, but with a key difference: we no longer compute inner products or norms. We already know the coefficients, so the step becomes pure reconstruction.&lt;/p&gt;
    &lt;code&gt;fn lanczos_reconstruction_step&amp;lt;T: ComplexField, O: LinOp&amp;lt;T&amp;gt;&amp;gt;(
    operator: &amp;amp;O,
    mut w: MatMut&amp;lt;'_, T&amp;gt;,
    v_curr: MatRef&amp;lt;'_, T&amp;gt;,
    v_prev: MatRef&amp;lt;'_, T&amp;gt;,
    alpha_j: T::Real,
    beta_prev: T::Real,
    stack: &amp;amp;mut MemStack,
) {
    // Apply operator
    operator.apply(w.rb_mut(), v_curr, Par::Seq, stack);

    // Orthogonalize using stored α_j and β_{j-1}
    let beta_prev_scaled = T::from_real_impl(&amp;amp;beta_prev);
    zip!(w.rb_mut(), v_prev).for_each(|unzip!(w_i, v_prev_i)| {
        *w_i = sub(w_i, &amp;amp;mul(&amp;amp;beta_prev_scaled, v_prev_i));
    });

    let alpha_scaled = T::from_real_impl(&amp;amp;alpha_j);
    zip!(w.rb_mut(), v_curr).for_each(|unzip!(w_i, v_curr_i)| {
        *w_i = sub(w_i, &amp;amp;mul(&amp;amp;alpha_scaled, v_curr_i));
    });
}&lt;/code&gt;
    &lt;p&gt;This is cheaper than the first-pass recurrence. We’ve eliminated the inner products that computed and the norm calculation for . What remains is pure orthogonalization and the operator application.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;lanczos_pass_two&lt;/code&gt; implements this reconstruction. We initialize the three work vectors and the solution accumulator:&lt;/p&gt;
    &lt;code&gt;pub fn lanczos_pass_two&amp;lt;T: ComplexField&amp;gt;(
    operator: &amp;amp;impl LinOp&amp;lt;T&amp;gt;,
    b: MatRef&amp;lt;'_, T&amp;gt;,
    decomposition: &amp;amp;LanczosDecomposition&amp;lt;T::Real&amp;gt;,
    y_k: MatRef&amp;lt;'_, T&amp;gt;,
    stack: &amp;amp;mut MemStack,
) -&amp;gt; Result&amp;lt;Mat&amp;lt;T&amp;gt;, LanczosError&amp;gt; {
    let mut v_prev = Mat::&amp;lt;T&amp;gt;::zeros(b.nrows(), 1);
    let inv_norm = T::from_real_impl(&amp;amp;T::Real::recip_impl(&amp;amp;decomposition.b_norm));
    let mut v_curr = b * Scale(inv_norm);  // v_1

    let mut work = Mat::&amp;lt;T&amp;gt;::zeros(b.nrows(), 1);

    // Initialize solution with first component
    let mut x_k = &amp;amp;v_curr * Scale(T::copy_impl(&amp;amp;y_k[(0, 0)]));&lt;/code&gt;
    &lt;p&gt;We build the solution incrementally by starting with the first basis vector scaled by its coefficient. The main loop then regenerates each subsequent vector: we regenerate each subsequent basis vector, normalize it using the stored , and immediately accumulate its contribution:&lt;/p&gt;
    &lt;code&gt;for j in 0..decomposition.steps_taken - 1 {
    let alpha_j = T::Real::copy_impl(&amp;amp;decomposition.alphas[j]);
    let beta_j = T::Real::copy_impl(&amp;amp;decomposition.betas[j]);
    let beta_prev = if j == 0 {
        T::Real::zero_impl()
    } else {
        T::Real::copy_impl(&amp;amp;decomposition.betas[j - 1])
    };

    // 1. Regenerate the unnormalized next vector
    lanczos_reconstruction_step(
        operator,
        work.as_mut(),
        v_curr.as_ref(),
        v_prev.as_ref(),
        alpha_j,
        beta_prev,
        stack,
    );

    // 2. Normalize using stored β_j
    let inv_beta = T::from_real_impl(&amp;amp;T::Real::recip_impl(&amp;amp;beta_j));
    zip!(work.as_mut()).for_each(|unzip!(w_i)| {
        *w_i = mul(w_i, &amp;amp;inv_beta);
    });

    // 3. Accumulate: x_k += y_{j+1} * v_{j+1}
    let coeff = T::copy_impl(&amp;amp;y_k[(j + 1, 0)]);
    zip!(x_k.as_mut(), work.as_ref()).for_each(|unzip!(x_i, v_i)| {
        *x_i = add(x_i, &amp;amp;mul(&amp;amp;coeff, v_i));
    });

    // 4. Cycle vectors for the next iteration
    core::mem::swap(&amp;amp;mut v_prev, &amp;amp;mut v_curr);
    core::mem::swap(&amp;amp;mut v_curr, &amp;amp;mut work);
}&lt;/code&gt;
    &lt;p&gt;The accumulation &lt;code&gt;x_k += y_{j+1} * v_{j+1}&lt;/code&gt; is implemented as a fused multiply-add in the &lt;code&gt;zip!&lt;/code&gt; loop. On hardware with FMA support, this becomes a single instruction per element, not three separate operations.&lt;/p&gt;
    &lt;p&gt;Note that we accumulate the solution incrementally. After each iteration, &lt;code&gt;x_k&lt;/code&gt; contains a partial result. We cycle through the same three vectors (&lt;code&gt;v_prev&lt;/code&gt;, &lt;code&gt;v_curr&lt;/code&gt;, &lt;code&gt;work&lt;/code&gt;), keeping the working set small and resident in L1 cache.&lt;/p&gt;
    &lt;p&gt;Compare this to the standard method’s final reconstruction step: . This is a dense matrix-vector product where is . When and are both large, this matrix no longer fits in cache. The CPU must stream it from main memory, paying the cost of memory latency. Each element requires a load, multiply, and accumulate, but the load operations dominate—the CPU stalls waiting for data.&lt;/p&gt;
    &lt;p&gt;In our two-pass reconstruction, the operator &lt;code&gt;$\mathbf{A}$&lt;/code&gt; is applied  times, but against vectors that stay in cache. The memory bandwidth is spent on reading the sparse structure of  and the vector elements, not on scanning a dense  matrix.&lt;/p&gt;
    &lt;p&gt;This is the reason the two-pass method can be faster on real hardware despite performing twice as many matrix-vector products. The cache behavior of the reconstruction phase overwhelms the savings of storing the basis.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Public API&lt;/head&gt;
    &lt;p&gt;We can wrap the two passes into a single entry point:&lt;/p&gt;
    &lt;code&gt;pub fn lanczos_two_pass&amp;lt;T, O, F&amp;gt;(
    operator: &amp;amp;O,
    b: MatRef&amp;lt;'_, T&amp;gt;,
    k: usize,
    stack: &amp;amp;mut MemStack,
    mut f_tk_solver: F,
) -&amp;gt; Result&amp;lt;Mat&amp;lt;T&amp;gt;, LanczosError&amp;gt;
where
    T: ComplexField,
    O: LinOp&amp;lt;T&amp;gt;,
    F: FnMut(&amp;amp;[T::Real], &amp;amp;[T::Real]) -&amp;gt; Result&amp;lt;Mat&amp;lt;T&amp;gt;, anyhow::Error&amp;gt;,
{
    // First pass: compute T_k coefficients
    let decomposition = lanczos_pass_one(operator, b, k, stack)?;

    if decomposition.steps_taken == 0 {
        return Ok(Mat::zeros(b.nrows(), 1));
    }

    // Solve projected problem: y_k' = f(T_k) * e_1
    let y_k_prime = f_tk_solver(&amp;amp;decomposition.alphas, &amp;amp;decomposition.betas)?;

    // Scale by ||b||
    let y_k = &amp;amp;y_k_prime * Scale(T::from_real_impl(&amp;amp;decomposition.b_norm));

    // Second pass: reconstruct solution
    lanczos_pass_two(operator, b, &amp;amp;decomposition, y_k.as_ref(), stack)
}&lt;/code&gt;
    &lt;p&gt;The design separates concerns. The &lt;code&gt;f_tk_solver&lt;/code&gt; closure is where we inject the specific matrix function. We compute the Lanczos decomposition, then pass the coefficients to the user-provided solver, which computes  for whatever function  is needed. This decoupling means we handle linear solves, matrix exponentials, or any other function without modifying the core algorithm.&lt;/p&gt;
    &lt;p&gt;The caller provides &lt;code&gt;f_tk_solver&lt;/code&gt; as a closure. It receives the raw  arrays and must return the coefficient vector . We then scale it by  and pass everything to the second pass.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example: Solving a Linear System&lt;/head&gt;
    &lt;p&gt;To see this in practice, consider solving . We compute , which means the &lt;code&gt;f_tk_solver&lt;/code&gt; must solve the small tridiagonal system .&lt;/p&gt;
    &lt;p&gt;Since is tridiagonal, we can exploit its structure. A sparse LU factorization solves it in time instead of the cost of a dense method.&lt;/p&gt;
    &lt;code&gt;let f_tk_solver = |alphas: &amp;amp;[f64], betas: &amp;amp;[f64]| -&amp;gt; Result&amp;lt;Mat&amp;lt;f64&amp;gt;, anyhow::Error&amp;gt; {
    let steps = alphas.len();
    if steps == 0 {
        return Ok(Mat::zeros(0, 1));
    }

    // 1. Assemble T_k from coefficients using triplet format
    let mut triplets = Vec::with_capacity(3 * steps - 2);
    for (i, &amp;amp;alpha) in alphas.iter().enumerate() {
        triplets.push(Triplet { row: i, col: i, val: alpha });
    }
    for (i, &amp;amp;beta) in betas.iter().enumerate() {
        triplets.push(Triplet { row: i, col: i + 1, val: beta });
        triplets.push(Triplet { row: i + 1, col: i, val: beta });
    }
    let t_k_sparse = SparseColMat::try_new_from_triplets(steps, steps, &amp;amp;triplets)?;

    // 2. Construct e_1
    let mut e1 = Mat::zeros(steps, 1);
    e1.as_mut()[(0, 0)] = 1.0;

    // 3. Solve T_k * y' = e_1 via sparse LU
    Ok(t_k_sparse.as_ref().sp_lu()?.solve(e1.as_ref()))
};&lt;/code&gt;
    &lt;p&gt;The closure takes the coefficient arrays, constructs the sparse tridiagonal matrix, and solves the system. The triplet format lets us build the matrix efficiently without knowing its structure in advance. The sparse LU solver leverages the tridiagonal structure to avoid dense factorization.&lt;/p&gt;
    &lt;head rend="h1"&gt;Some interesting results&lt;/head&gt;
    &lt;p&gt;Now that we have a working implementation we can run some tests. The core idea of what we have done is simple: trade flops for better memory access. But does this trade actually pay off on real hardware? To find out, we need a reliable way to benchmark it.&lt;/p&gt;
    &lt;p&gt;For the data, we know that the performance of any Krylov method is tied to the operator’s spectral properties. We need a way to generate a family of test problems where we can precisely control the size, sparsity, and numerical difficulty. A great way to do this is with Karush-Kuhn-Tucker (KKT) systems, which are sparse, symmetric, and have a specific block structure.&lt;/p&gt;
    &lt;p&gt;This structure gives us two critical knobs to turn. First, with the netgen utility, we can control the matrix, which lets us dial in the problem dimension, . Second, we build the diagonal block D with random entries from a range . This parameter, , gives us direct control over the numerical difficulty of the problem.&lt;/p&gt;
    &lt;p&gt;For a symmetric matrix like , the 2-norm condition number, , is the ratio of its largest to its smallest eigenvalue: . Since is diagonal, its eigenvalues are simply its diagonal entries. We are drawing these entries from a uniform distribution , so we have and . This means we get direct control, as .The spectral properties of this block heavily influence the spectrum of the entire matrix . A large condition number in leads to a more ill-conditioned system for . The convergence rate of Krylov methods like Lanczos is fundamentally governed by the distribution of the operator’s eigenvalues. An ill-conditioned matrix, with a wide spread of eigenvalues, will require more iterations, , to reach the desired accuracy. By simply adjusting the parameter, we can generate everything from well-conditioned problems that converge quickly to ill-conditioned ones that force us to run a large number of iterations. This is exactly what we need to rigorously test our implementation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory and Computation Trade-off&lt;/head&gt;
    &lt;p&gt;We measure the algorithm against two hypotheses on a large sparse problem with , varying the number of iterations .&lt;/p&gt;
    &lt;p&gt;Hypothesis 1 (Memory): The one-pass method stores the full basis with complexity . We expect its memory to grow linearly with . The two-pass method operates with memory, so it should have a flat profile.&lt;/p&gt;
    &lt;p&gt;Hypothesis 2 (Runtime): The two-pass method performs matrix-vector products instead of . If all else were equal, we’d expect it to run twice as slow.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memory Usage&lt;/head&gt;
    &lt;p&gt;The memory data confirms Hypothesis 1 exactly. The one-pass method’s footprint scales as a straight line—each additional iteration adds one vector to the basis. The two-pass method remains flat. No allocation growth happens after initialization.&lt;/p&gt;
    &lt;head rend="h3"&gt;Runtime: Where Theory Breaks&lt;/head&gt;
    &lt;p&gt;The runtime data contradicts Hypothesis 2. The two-pass method is slower, but never by a factor of two. For small , the gap is minimal. As grows, the two-pass runtime diverges slowly from the one-pass method, not by doubling, but by a much smaller margin.&lt;/p&gt;
    &lt;p&gt;This difference comes from memory access patterns. Both methods perform matrix-vector products, but they differ in how they reconstruct the solution.&lt;/p&gt;
    &lt;p&gt;The one-pass method computes in a single dense matrix-vector product. When and are large, the basis matrix exceeds all cache levels. The CPU cannot keep the data resident; instead, it streams from main memory. This is a memory-bandwidth-bound operation. The processor stalls, waiting for each load to complete. Instruction-level parallelism collapses.&lt;/p&gt;
    &lt;p&gt;The two-pass method reconstructs the solution incrementally. At each iteration, it operates on exactly three n-dimensional vectors: , , and . This working set fits in L1 cache. The processor performs matrix-vector products (each one reading the sparse operator, then applying it to a cached vector), but the solution accumulation happens entirely within cache. The additional matrix-vector products are cheaper than the memory latency of the standard method.&lt;/p&gt;
    &lt;p&gt;The cost of re-computing basis vectors is less than the latency cost of scanning an dense matrix from main memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;Medium-Scale Behavior&lt;/head&gt;
    &lt;p&gt;At we can observe an equilibrium. The two methods have nearly identical runtime. The standard method’s matrix is smaller; it fits partially in cache. The cache-miss penalty here becomes manageable. The two-pass method still has the advantage of cache-local accumulation, but the difference is marginal.&lt;/p&gt;
    &lt;head rend="h3"&gt;What About Dense Matrices?&lt;/head&gt;
    &lt;p&gt;To be sure of our hypothesis, we can test it directly using a dense matrix of size . For dense problems, the matrix-vector product is , it dominates all other costs. Memory latency will become negligible relative to the compute work and the cache efficiency advantage should disappear.&lt;/p&gt;
    &lt;p&gt;We can see that the two-pass method runs almost exactly twice as slow as the one-pass method. The slope ratio is exactly 2:1. In a compute-bound regime, the extra matrix-vector products cannot be hidden by cache effects. Here, the theoretical trade-off holds perfectly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scalability&lt;/head&gt;
    &lt;p&gt;Now, let’s fix the iteration count at and vary from to to measure scalability. Based on what we have seen before, we would expect the two-pass memory to scale linearly with but with a small constant factor (three vectors, plus scalars). The one-pass method should also scale linearly, but with a -dependent slope.&lt;/p&gt;
    &lt;p&gt;Here we have to use a logarithmic y-axis to show both curves; the two-pass line is so flat relative to the one-pass line that it’s otherwise invisible.&lt;/p&gt;
    &lt;p&gt;Runtime scales linearly with for both methods, as expected. Below , the two methods have similar performance. This is the regime where both basis and working set fit in cache, or where the problem is small enough that memory latency is not the bottleneck.&lt;/p&gt;
    &lt;p&gt;As increases beyond , the matrix-vector product time dominates. The sparse structure of ensures that each matvec requires multiple memory accesses per element. For the one-pass method, the final reconstruction of begins to cost more as the matrix grows. For the two-pass method, performing matrix-vector products means the matvec cost accumulates more rapidly. The divergence is gradual, not sharp, because the advantage of cache locality in accumulation persists—but it cannot overcome the fundamental cost of doubling the number of expensive operations.&lt;/p&gt;
    &lt;p&gt;Well, that’s it. If you want to have a better look at the code or use it, it’s all open source:&lt;/p&gt;
    &lt;p&gt;This was more of an exploration than a production-ready library, so expect rough edges. But I hope it gives an interesting perspective on how algorithm engineering and low-level implementation details can alter what seems like a straightforward trade-off on a blackboard.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lukefleed.xyz/posts/cache-friendly-low-memory-lanczos/"/><published>2025-11-11T17:08:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45890186</id><title>We ran over 600 image generations to compare AI image models</title><updated>2025-11-12T00:50:24.315708+00:00</updated><content>&lt;doc fingerprint="371511f4993ed21d"&gt;
  &lt;main&gt;
    &lt;p&gt;tl:dr; We’ve been making photo apps for iOS for long enough that we have gray hairs now, and using our experience we ran over 600 image generations to compare which AI models work best for which image edits. If you want, you can jump right to the image comparisons, or the conclusion, but promise us you won’t presumptuous comments on Hacker News until you’ve also read the background!&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;Hi! We’re LateNiteSoft, and we’ve been working on photography-related iOS apps for 15 years now. Working on market-leading apps such as Camera+, Photon and REC, we’ve always had our finger on the pulse on what users want out of their mobile photography.&lt;/p&gt;
    &lt;p&gt;With the ground-breaking release of OpenAI’s gpt-image-1 image generation model earlier this year, we started investigating all the interesting use cases we could think of for AI image editing.&lt;/p&gt;
    &lt;p&gt;But as a company that has never taken any venture capital investment, all our products have to pay for themselves. We’re in it to delight our users, not just capture market share and sell them out. When considering AI projects, one thing has been clear – we can’t take the AI startup road where you have a generous free tier, charge an unreasonably small monthly fee for “unlimited”, and hope you’re going to make it up on scale (code for “someone please acquire us”).&lt;/p&gt;
    &lt;p&gt;All the AI-focused billing systems we could find out there were based on this. Assuming you want to claim unlimited access, and then sandbag users with “fair use” clauses and prevent them from any actual unlimited usage (which is, obviously, untenable, since you’ll end up with one $20/mo user reselling to everyone else).&lt;/p&gt;
    &lt;p&gt;Since we want to fairly charge our customers for what they actually use, we’ve built a credit-based “pay per generation”-style billing system (that internally we’ve been calling CreditProxy). We’ve also been planning on providing this as a service, since nobody else seems to be doing it, so if you’re interested in being a trial user, get in touch!&lt;/p&gt;
    &lt;p&gt;We released our app MorphAI as a public proof of concept to give CreditProxy a proper real world-test, and have marketed it to the users of Camera+, which includes traditional photo-editing functionality, including a whole host of popular photo filters, giving us a built-in audience of customers ready for the next step in image editing.&lt;/p&gt;
    &lt;p&gt;With the release of newer models like nanoBanana and Seedream, we’ve had to consider which models make sense to support. We need to explore the trade-offs between quality, prompt complexity, and pricing.&lt;/p&gt;
    &lt;p&gt;A couple of hastily-hacked together scripts, and many, many AI generation credits later, we have some results! So that everyone else also doesn’t have to waste their money, we figured we’d share what we found:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Tests&lt;/head&gt;
    &lt;p&gt;Based on our experience with Camera+ and the kind of edits our users have been making with MorphAI, we picked a host of somewhat naive prompts. Veteran Midjourney users may scoff at these, but in our experience these are the kinds of prompts that our average user is likely to use.&lt;/p&gt;
    &lt;p&gt;As for test photos, we chose some some representative things people like to take photos of: their pets, their kids, landscapes, their cars, and product photography.&lt;/p&gt;
    &lt;p&gt;Image generation times are also relevant. During our test period, the generation time for all models was fairly consistent, and didn’t vary by image or prompt complexity.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;OpenAI (High)&lt;/cell&gt;
        &lt;cell&gt;Gemini&lt;/cell&gt;
        &lt;cell&gt;Seedream&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;80 seconds&lt;/cell&gt;
        &lt;cell&gt;11 seconds&lt;/cell&gt;
        &lt;cell&gt;9 seconds&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;OpenAI also has a quality setting, the images included here were all generated on High quality, but we also tested Medium, and those generations averaged 36 seconds. We can include the Medium quality images as well if there is any interest!&lt;/p&gt;
    &lt;p&gt;There are a ton of photos to compare here, so to make things easier to flip through, here are some keyboard shortcuts to help you out: Click on a photo to see it larger. Now you can use the arrow keys to switch between models. Press the tab key to switch between test images. Hit ESC to leave the view.&lt;/p&gt;
    &lt;head rend="h2"&gt;Classic filters&lt;/head&gt;
    &lt;p&gt;These are the types of filters that we used to implement manually, by painstakingly hand-crafting textures and Photoshop layers and then converting those to Objective-C code. Now all you need is a few words into a language model (and to burn down half of a rain forest or so; just the cost of progress).&lt;/p&gt;
    &lt;p&gt;Our conclusion for this category is that for photo realistic filters like this, Gemini really shines by preserving details from the original and minimizing hallucinations, but often at the expense of the strength and creativity of the effect. Especially with photos of people, Gemini seems to refuse to apply any edits at all, with a strong bias towards photo realism.&lt;/p&gt;
    &lt;p&gt;OpenAI really likes to mess with the details of the photo, giving a characteristic “AI slop” feel, which can be a deal breaker on things like human faces.&lt;/p&gt;
    &lt;head rend="h3"&gt;Grungy vintage photo&lt;/head&gt;
    &lt;head rend="h3"&gt;Use soft, diffused lighting&lt;/head&gt;
    &lt;head rend="h3"&gt;Transform into a kaleidoscopic pattern&lt;/head&gt;
    &lt;p&gt;Gemini took some really odd shortcuts in generating some of these!&lt;/p&gt;
    &lt;head rend="h3"&gt;Apply a heat map effect&lt;/head&gt;
    &lt;p&gt;It’s clear that none of the models actually have a concept of what generates heat here, aside from Seedream knowing that humans generate heat, clearly revealing that without any ground truth the models struggle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look like a long exposure photograph&lt;/head&gt;
    &lt;p&gt;This is an interesting test since in some of the sample photos a long exposure doesn’t make sense. In the ones where it makes the most sense – the landscape and the car, OpenAI did the best, but on the other hand it completely messed up the cats and the product, and the portrait photo turned into a trippy art piece.&lt;/p&gt;
    &lt;p&gt;Gemini, maybe logically, did nothing. Seedream liked adding light streaks as if a car drove past, with only the portrait photo seemingly making any sense.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pinhole camera&lt;/head&gt;
    &lt;p&gt;In this case, it was funny to watch Gemini take a literal approach and generate actual pictures of cameras! For this reason we re-worked this prompt by just adding the word “effect”.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pinhole camera effect&lt;/head&gt;
    &lt;p&gt;Gemini liked to generate a literal pinhole camera here so we tried modifying the prompt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Add a layer of fog or mist&lt;/head&gt;
    &lt;head rend="h3"&gt;Make it look like it’s golden hour&lt;/head&gt;
    &lt;head rend="h3"&gt;Make it look like it’s etched in glass&lt;/head&gt;
    &lt;p&gt;With this prompt, there is ambiguity in what “it” is, so we tried a reworded prompt as well. Only OpenAI consistently knew what a traditional etched glass effect looks like. Seedream’s glass item effect looks really cool!&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look like the photo is etched in glass&lt;/head&gt;
    &lt;p&gt;Gemini has a really interesting interpretation here! And Seedream had some pretty fantastic results.&lt;/p&gt;
    &lt;head rend="h3"&gt;Remove background&lt;/head&gt;
    &lt;p&gt;This is a classic job people have spent their lives doing manually in Photoshop since the early 90’s. But what is a “background”, really? Is the ground in front of a car the “background”? We also retried this with a tweaked prompt.&lt;/p&gt;
    &lt;p&gt;OpenAI’s “sloppification” of the details of objects makes it useless for this purpose.&lt;/p&gt;
    &lt;head rend="h3"&gt;Isolate the object&lt;/head&gt;
    &lt;p&gt;With the tweaked prompt, Gemini’s API actually returned a followup question: “Which object would you like to isolate? There are two cats in the image.”, which our generation script was not prepared to handle! So it is missing from this comparison.&lt;/p&gt;
    &lt;head rend="h3"&gt;Give it a metallic sheen&lt;/head&gt;
    &lt;p&gt;Another case where “it” is vague and we can retry with a more specific prompt. The product imagery is another case where Seedream created a really stunning result, even adding a reflection of someone taking the photo with their phone!&lt;/p&gt;
    &lt;head rend="h3"&gt;Give the object a metallic sheen&lt;/head&gt;
    &lt;p&gt;Modifying the prompt here really only changed OpenAI’s interpretation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lens effects&lt;/head&gt;
    &lt;p&gt;One of the filter packs we had worked on for Camera+ using traditional methods was a lens effect filter pack. But unlike traditional edits, with generative AI you can also create wide-angle lens effects that can just make up the portions of the image that the camera couldn’t capture.&lt;/p&gt;
    &lt;p&gt;This is another category where it’s very visible how OpenAI regenerates and hallucinates all the details in a picture, where Gemini and Seedream’s results are very faithful to the original and look more like actual lens permutations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Apply a fish-eye lens effect&lt;/head&gt;
    &lt;head rend="h3"&gt;Strong bokeh blur&lt;/head&gt;
    &lt;p&gt;It was pretty surprising how poorly the models did here considering how common this must be among the training data. OpenAI give a strong blur but no bokeh effects. Gemini gives us a bunch of random circles in front of the image, demonstrating an understanding of what people want out of a bokeh filter but not how it works. Seedream does really well here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Apply a Dutch angle (canted frame)&lt;/head&gt;
    &lt;p&gt;OpenAI really lost it’s mind here on the car photo.&lt;/p&gt;
    &lt;head rend="h3"&gt;Change to a bird’s-eye view&lt;/head&gt;
    &lt;head rend="h2"&gt;Style transfer&lt;/head&gt;
    &lt;p&gt;Style Transfer is the process of applying an artistic style to a photo. This technique predates the current AI model by quite a few years with popular apps generating Van Gogh paintings out of your photos. We were also early out in attempting style transfer for our apps, shout out to Noel’s Intel iMac which had to run at full blast all night just to generate a 256x256px image, since it was our only machine with a compatible GPU.&lt;/p&gt;
    &lt;p&gt;While Gemini was good at preserving reality in the more photorealistic effects in the previous section, when it comes to the more artistic styles, OpenAI has them beat, while Gemini keeps things far too conservative, especially with photos of a human in them, where it sometimes seems to just do nothing at all, is this some kind of safety guardrail?&lt;/p&gt;
    &lt;head rend="h3"&gt;Draw this in the style of a Studio Ghibli movie&lt;/head&gt;
    &lt;p&gt;ChatGPT went viral with this prompt, with Sam Altman even making it his profile on X. And OpenAI keeps the crown – is Google too conservative in order to avoid a lawsuit? Seedream makes an attempt but they just end up looking like “generic Anime”.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transform into watercolor painting&lt;/head&gt;
    &lt;head rend="h3"&gt;Make it look like a pastel artwork&lt;/head&gt;
    &lt;head rend="h3"&gt;Transform into Art Nouveau style&lt;/head&gt;
    &lt;head rend="h3"&gt;Apply a ukiyo-e Japanese woodblock print style&lt;/head&gt;
    &lt;p&gt;A very stark example of Gemini failing to apply a style on photos with humans. This is a prompt where Seedream knocked it out of the park, perhaps showing a larger portion of their training data being sourced from asian cultures than the western models.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transform into low poly art&lt;/head&gt;
    &lt;p&gt;Seedream blows everyone else away here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Portrait effects&lt;/head&gt;
    &lt;p&gt;For prompts about human appearance, we have only applied them to the portrait photo.&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look like a caricature&lt;/head&gt;
    &lt;p&gt;Seedream seems to be biased towards asian culture, giving an anime look instead of a western-style cartoon caricature.&lt;/p&gt;
    &lt;head rend="h3"&gt;Turn them into an action figure in the blister pack&lt;/head&gt;
    &lt;p&gt;OpenAI’s style here went viral a while back, but Gemini is stunningly realistic. Seedream is a weird mix of realistic and hallucinations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generative edits&lt;/head&gt;
    &lt;p&gt;The place where generative AI really shines is when it can show off some creativity, and these were some prompts we added as suggestions in MorphAI to showcase that and inspire our users. OpenAI still seems to win here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Create a 70’s vinyl record cover&lt;/head&gt;
    &lt;p&gt;This is an example of a prompt that has a small viral moment with OpenAI, but the other models can’t even get the aspect ratio right.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introduce mythical creatures native to this environment&lt;/head&gt;
    &lt;p&gt;This one showcases OpenAI’s creativity. Gemini seems kind of creepy?&lt;/p&gt;
    &lt;head rend="h3"&gt;Add a mystical portal or gateway&lt;/head&gt;
    &lt;p&gt;Gemini replacing the face with a portal is certainly a choice!&lt;/p&gt;
    &lt;head rend="h3"&gt;Incorporate futuristic technology elements&lt;/head&gt;
    &lt;p&gt;Another example of OpenAI being far more creative and willing to re-do the whole image.&lt;/p&gt;
    &lt;head rend="h3"&gt;Make it look whimsical and enchanting&lt;/head&gt;
    &lt;p&gt;This one also shows OpenAI being more artistic, and Gemini being more realistic while still trying to incorporate the prompt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transform the scene to a stormy night&lt;/head&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;If you made it all the way down here you probably don’t need a summary, but for our purposes, we’ve at least concluded that there is no one-size-fits all model at this point.&lt;/p&gt;
    &lt;p&gt;OpenAI is great for fully transformative filters like style transfer or more creative generative applications, whereas Gemini works better for more realistic edits. Seedream lies somewhere in the middle and is a bit of a jack of all trades, and for the price and performance may be a good replacement for OpenAI.&lt;/p&gt;
    &lt;p&gt;We’ve been experimenting on working on a “prompt classifier” to automatically choose a model – sending artistic prompts to OpenAI and more realistic prompts to Gemini, if there’s any interest we can follow up with how that worked out!&lt;/p&gt;
    &lt;head rend="h4"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;Tests were performed on October 8 with &lt;code&gt;gpt-image-1&lt;/code&gt;, &lt;code&gt;gemini-2.5-flash-image&lt;/code&gt; and &lt;code&gt;seedream-4-0-250828&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Timings were measured on a consumer internet connection in Japan (Fiber connection, 10 Gbps nominal bandwidth) during a limited test run in a short time period.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://latenitesoft.com/blog/evaluating-frontier-ai-image-generation-models/"/><published>2025-11-11T17:26:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45890726</id><title>Terminal Latency on Windows (2024)</title><updated>2025-11-12T00:50:23.746199+00:00</updated><content>&lt;doc fingerprint="140ed15eb7063ff7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Terminal Latency on Windows&lt;/head&gt;
    &lt;p&gt;UPDATE 2024-04-15: Windows Terminal 1.19 contains a fix that reduces latency by half! Itâs now competitive with WSLtty on my machine. Details in the GitHub Issue.&lt;/p&gt;
    &lt;p&gt;In 2009, I wrote about why MinTTY is the best terminal on Windows. Even today, that post is one of my most popular.&lt;/p&gt;
    &lt;p&gt;Since then, the terminal situation on Windows has improved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cygwin defaults to MinTTY; you no longer need to manually install it.&lt;/item&gt;
      &lt;item&gt;Windows added PTY support, obviating the need for offscreen console window hacks that add latency.&lt;/item&gt;
      &lt;item&gt;Windows added basically full support for ANSI terminal sequences in both the legacy conhost.exe consoles and its new Windows Terminal.&lt;/item&gt;
      &lt;item&gt;We now have a variety of terminals to choose from, even on Windows: Cmder, ConEmu, Alacritty, WezTerm, xterm.js (component of Visual Studio Code)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beginning of a year is a great time to look at your tools and improve your environment.&lt;/p&gt;
    &lt;p&gt;Iâd already enabled 24-bit color in all of my environments and streamlined my tmux config. Itâs about time that I take a look at the newer terminals.&lt;/p&gt;
    &lt;p&gt;Roughly in order, I care about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimum feature set: 24-bit color, reasonable default fonts with emoji support, italics are nice.&lt;/item&gt;
      &lt;item&gt;Input latency.&lt;/item&gt;
      &lt;item&gt;Throughput at line rate, for example, when I &lt;code&gt;cat&lt;/code&gt;a large file.&lt;/item&gt;
      &lt;item&gt;Support for multiple tabs in one window would be nice, but tmux suffices for me.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Which terminals should I test?&lt;/head&gt;
    &lt;p&gt;I considered the following.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Legacy conhost.exe (also known as Windows Console), Windows 10 19045&lt;/item&gt;
      &lt;item&gt;MinTTY (3.7.0)&lt;/item&gt;
      &lt;item&gt;Alacritty (0.13.1)&lt;/item&gt;
      &lt;item&gt;WezTerm (20240203-110809-5046fc22)&lt;/item&gt;
      &lt;item&gt;Windows Terminal (1.18.10301.0)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Testing Features&lt;/head&gt;
    &lt;p&gt;Testing color and italics support is easy with my colortest.rs script. To test basic emoji, you can cat the Unicode emoji 1.0 emoji-data.txt. To test more advanced support, try the zero-width joiner list in the latest/ directory.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Emoji&lt;/cell&gt;
        &lt;cell role="head"&gt;Font Attributes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No italics&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY&lt;/cell&gt;
        &lt;cell&gt;Black and white&lt;/cell&gt;
        &lt;cell&gt;All major attributes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;Black and white&lt;/cell&gt;
        &lt;cell&gt;Everything but double underline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;Color&lt;/cell&gt;
        &lt;cell&gt;All major attributes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;Color&lt;/cell&gt;
        &lt;cell&gt;All major attributes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Everything but conhost.exe meets my bar.&lt;/p&gt;
    &lt;p&gt;Itâs also worth noting that conhost.exe has a terrible default palette. The default yellow is a pukey green and dark blue is barely visible. You can change palettes, but defaults matter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Latency&lt;/head&gt;
    &lt;p&gt;I set up two latency tests. One with an 80x50 blank window in the upper left corner of the screen. The other fullscreen, editing an Emacs command at the bottom of the screen.&lt;/p&gt;
    &lt;p&gt;Since latencies are additive, system configuration doesnât matter as much as the absolute milliseconds of latency each terminal adds, but Iâll describe my entire setup and include total keypress-to-pixels latency.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Windows 10&lt;/item&gt;
      &lt;item&gt;Intel i7-4771 @ 3.5 GHz&lt;/item&gt;
      &lt;item&gt;NVIDIA GTX 1060&lt;/item&gt;
      &lt;item&gt;Keyboard: Sweet 16 Macro Pad&lt;/item&gt;
      &lt;item&gt;Display: LG 27GP950-B at 4K, 120 Hz, adaptive sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Measurement Methodology&lt;/head&gt;
    &lt;p&gt;With Is It Snappy?, I measured the number of frames between pressing a key and pixels changing on the screen.&lt;/p&gt;
    &lt;p&gt;To minimize ambiguity about when the key was pressed, I slammed a pencilâs eraser into the key, and always measured the key press as the second frame after contact. (The first frame was usually when the eraser barely touched the key. It would usually clear the activation depth by the second frame.)&lt;/p&gt;
    &lt;p&gt;I considered the latency to end when pixels just started to change on the screen. In practice, pixels take several 240 Hz frames to transition from black to white, but I consistently marked the beginning of that transition.&lt;/p&gt;
    &lt;p&gt;I took five measurements for each configuration and picked the median. Each measurement was relatively consistent, so average would have been a fine metric too. It doesnât change the results below.&lt;/p&gt;
    &lt;head rend="h3"&gt;80x50&lt;/head&gt;
    &lt;p&gt;80x50 window, upper left of screen, cleared terminal, single keypress.&lt;/p&gt;
    &lt;p&gt;Confirmed window size with:&lt;/p&gt;
    &lt;code&gt;$ echo $(tput cols)x$(tput lines)
80x50
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Median Latency (ms)&lt;/cell&gt;
        &lt;cell role="head"&gt;240 Hz Camera Frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe WSL1&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe Cygwin&lt;/cell&gt;
        &lt;cell&gt;41.3&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;57.9&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm cmd.exe&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty WSL1&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm WSL1&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Windows Terminal WSL1&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Fullscreen&lt;/head&gt;
    &lt;p&gt;Maximized emacs, editing a command in the bottom row of the terminal. I only tested WSL1 this time.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Median Latency (ms)&lt;/cell&gt;
        &lt;cell role="head"&gt;240 Hz Camera Frames&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost.exe&lt;/cell&gt;
        &lt;cell&gt;45.8&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY&lt;/cell&gt;
        &lt;cell&gt;52.42&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;75&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;75&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;87.5&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Throughput&lt;/head&gt;
    &lt;p&gt;I generated a 100,000-line file with:&lt;/p&gt;
    &lt;code&gt;$ yes "This sentence has forty-five (45) characters." | head -n 100000 &amp;gt; /tmp/lines.txt
&lt;/code&gt;
    &lt;p&gt;Then I measured the wall-clock duration of:&lt;/p&gt;
    &lt;code&gt;$ time cat /tmp/lines.txt
&lt;/code&gt;
    &lt;p&gt;This benchmark captures the case that I accidentally dump a ton of output and Iâm sitting there just waiting for the terminal to become responsive again. I have a gigabit internet connection, and itâs embarrassing to be CPU-bound instead of IO-bound.&lt;/p&gt;
    &lt;p&gt;I did include Cygwin in this test, just to have two different MinTTY datapoints.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Elapsed Time (s)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;0.57&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;2.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;5.25&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;5.75&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;6.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;conhost.exe&lt;/cell&gt;
        &lt;cell&gt;21.8&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I assume this means MinTTY throttles display updates in some way. Of course this is totally fine, because you couldnât read the output either way.&lt;/p&gt;
    &lt;p&gt;To test the hypothesis that MinTTY was caching cell rendering by their contents, I also tried generating a file that rotated through different lines, with no effect.&lt;/p&gt;
    &lt;code&gt;with open("/tmp/lines2.txt", "w") as f:
  for i in range(100000):
    sentence="This sentence has forty-five (45) characters."
    print(sentence[i%len(sentence):]+sentence[:i%len(sentence)], file=f)
&lt;/code&gt;
    &lt;head rend="h3"&gt;CPU Usage During Repeated Keypresses&lt;/head&gt;
    &lt;p&gt;While making these measurements, I noticed some strange behaviors. My monitor runs at 120 Hz and animation and window dragging are generally smooth. But right after you start Alacritty, dragging the window animates at something like 30-60 frames per second. Itâs noticeably chunkier. WezTerm does the same, but slightly worse. Maybe 20 frames per second.&lt;/p&gt;
    &lt;p&gt;I donât know if I can blame the terminals themselves, because I sometimes experience this even with Notepad.exe too. But the choppiness stands out much more. Maybe something is CPU-bound in responding to window events?&lt;/p&gt;
    &lt;p&gt;This made me think of a new test: if I open a terminal and hold down the âaâ button on autorepeat, how much CPU does the terminal consume?&lt;/p&gt;
    &lt;p&gt;To measure this, I set the terminal processâs affinity to my third physical core, and watched the CPU usage graph in Task Manager. Not a great methodology, but it gave a rough sense. Again, 80x50.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Percent of Core&lt;/cell&gt;
        &lt;cell role="head"&gt;Private Bytes After Startup (KiB)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;6,500&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
        &lt;cell&gt;74,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
        &lt;cell&gt;10,200&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
        &lt;cell&gt;10,500&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;20%&lt;/cell&gt;
        &lt;cell&gt;73,700&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;85%&lt;/cell&gt;
        &lt;cell&gt;134,000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The WezTerm CPU usage has to be a bug. Iâll report it.&lt;/p&gt;
    &lt;head rend="h3"&gt;CPU Usage (Idle)&lt;/head&gt;
    &lt;p&gt;I often have a pile of idle terminals sitting around. I donât want them to chew battery life. So letâs take a look at CPU Cycles Delta (courtesy of Process Explorer) with a fresh, idle WSL session.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Terminal&lt;/cell&gt;
        &lt;cell role="head"&gt;Idle Cycles/s (Focused)&lt;/cell&gt;
        &lt;cell role="head"&gt;Idle Cycles/s (Background)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conhost&lt;/cell&gt;
        &lt;cell&gt;~900,000&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Alacritty&lt;/cell&gt;
        &lt;cell&gt;~2,400,000&lt;/cell&gt;
        &lt;cell&gt;no difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WezTerm&lt;/cell&gt;
        &lt;cell&gt;~2,600,000&lt;/cell&gt;
        &lt;cell&gt;~1,600,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Windows Terminal&lt;/cell&gt;
        &lt;cell&gt;~55,000,000&lt;/cell&gt;
        &lt;cell&gt;~6,100,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MinTTY WSL1&lt;/cell&gt;
        &lt;cell&gt;~120,000,000&lt;/cell&gt;
        &lt;cell&gt;no difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MinTTY Cygwin&lt;/cell&gt;
        &lt;cell&gt;~120,000,000&lt;/cell&gt;
        &lt;cell&gt;no difference&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These numbers arenât great at all! For perspective, I have a pile of Firefox tabs open, some of them actively running JavaScript, and theyâre âonlyâ using a few hundred million cycles per second.&lt;/p&gt;
    &lt;p&gt;Raymond Chen once wrote a blog post about the importance of properly idling in the Windows Terminal Server days. You might have a dozen users logged into a host, and if a program is actively polling, itâs eating performance that others could use.&lt;/p&gt;
    &lt;p&gt;Today, we often run on batteries, so idling correctly still matters, but it seems to be something of a lost art. The only terminal that idles completely is the old conhost.exe.&lt;/p&gt;
    &lt;p&gt;The other lesson we can draw is that Microsoftâs own replacement for conhost.exe, Windows Terminal, uses over 10x the RAM, 60x the CPU when focused, and infinitely more CPU when idle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;conhost.exe consistently has the best latency, with MinTTY not much behind. MinTTY handily dominates the throughput test, supports all major ANSI character attributes, and has a better default palette.&lt;/p&gt;
    &lt;p&gt;As in 2009, Iâd say MinTTY is still pretty great. (I should try to track down that idle CPU consumption. It feels more like a bug than a requirement.)&lt;/p&gt;
    &lt;p&gt;If you want to use MinTTY as the default terminal for WSL, install WSLtty.&lt;/p&gt;
    &lt;p&gt;The others all have slightly worse latencies, but theyâre in a similar class. Iâm particularly sensitive to latency, so Iâd had a suspicion even before measuring. Maybe itâs some consequence of being GPU-accelerated? Out of curiousity, I put Windows Terminal in software-rendered mode, and it shaved perhaps 4 ms off (median of 62.5 ms, 15 frames). Perhaps just measurement noise.&lt;/p&gt;
    &lt;p&gt;While Iâm going to stick with MinTTY, one thing is clear: there is room to improve all of the above.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chadaustin.me/2024/02/windows-terminal-latency/"/><published>2025-11-11T18:07:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45891016</id><title>FFmpeg to Google: Fund us or stop sending bugs</title><updated>2025-11-12T00:50:23.306628+00:00</updated><content>&lt;doc fingerprint="3772b5d090a791bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FFmpeg to Google: Fund Us or Stop Sending Bugs&lt;/head&gt;
    &lt;p&gt;You may never have heard of FFmpeg, but you’ve used it. This open source program’s robust multimedia framework is used to process video and audio media files and streams across numerous platforms and devices. It provides tools and libraries for format conversion, aka transcoding, playback, editing, streaming, and post-production effects for both audio and video media.&lt;/p&gt;
    &lt;p&gt;FFmpeg’s libraries, such as libavcodec and libavformat, are essential for media players and software, including VLC, Kodi, Plex, Google Chrome, Firefox, and even YouTube’s video processing backend. It is also, like many other vital open source programs, terribly underfunded.&lt;/p&gt;
    &lt;head rend="h2"&gt;Corporate Responsibility vs. Volunteer Labor&lt;/head&gt;
    &lt;p&gt;A lively debate on Twitter began between Dan Lorenc, CEO and co-founder of Chainguard, the software supply chain security company, the FFmpeg project, Google, and security researchers over security disclosures and the responsibilities of large tech companies in open-source software.&lt;/p&gt;
    &lt;p&gt;The core of the discussion revolves around how vulnerabilities should be reported, who is responsible for fixing them, and the challenges that arise when AI is used to uncover a flood of potentially meaningless security issues. But at heart, it’s about money.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Obscure Bug Ignites the Controversy&lt;/head&gt;
    &lt;p&gt;This discussion has been heating up for some time. In mid-October, FFmpeg tweeted that “security issues are taken extremely seriously in FFmpeg, but fixes are written by volunteers.” This point cannot be emphasised enough. As FFmpeg tweeted later, “FFmpeg is written almost exclusively by volunteers.”&lt;/p&gt;
    &lt;p&gt;Thus, as Mark Atwood, an open source policy expert, pointed out on Twitter, he had to keep telling Amazon to not do things that would mess up FFmpeg because, he had to keep explaining to his bosses that “They are not a vendor, there is no NDA, we have no leverage, your VP has refused to help fund them, and they could kill three major product lines tomorrow with an email. So, stop, and listen to me … ”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Growing Burden on Open Source Maintainers&lt;/head&gt;
    &lt;p&gt;The latest episode was sparked after a Google AI agent found an especially obscure bug in FFmpeg. How obscure? This “medium impact issue in ffmpeg,” which the FFmpeg developers did patch, is “an issue with decoding LucasArts Smush codec, specifically the first 10-20 frames of Rebel Assault 2, a game from 1995.”&lt;/p&gt;
    &lt;p&gt;Wow.&lt;/p&gt;
    &lt;p&gt;FFmpeg added, “FFmpeg aims to play every video file ever made.” That’s all well and good, but is that a valuable use of an assembly programmer’s time? Oh, right, you may not know. FFmpeg’s heart is assembly language. As a former assembly language programmer, it is not, in any way, shape, or form, easy to work with.&lt;/p&gt;
    &lt;p&gt;As FFmpeg put it, this is “CVE slop.”&lt;/p&gt;
    &lt;p&gt;Many in the FFmpeg community argue, with reason, that it is unreasonable for a trillion-dollar corporation like Google, which heavily relies on FFmpeg in its products, to shift the workload of fixing vulnerabilities to unpaid volunteers. They believe Google should either provide patches with vulnerability reports or directly support the project’s maintenance.&lt;/p&gt;
    &lt;p&gt;Earlier, FFmpeg pointed out that it’s far from the only open source project to face such issues.&lt;/p&gt;
    &lt;p&gt;Specifically, the project team mentions Nick Wellnhofer, the former maintainer of libxml2, a widely used open source software library for parsing Extensible Markup Language (XML). Wellnhofer recently resigned from maintaining libxml2 because he had to “spend several hours each week dealing with security issues reported by third parties. Most of these issues aren’t critical, but it’s still a lot of work.&lt;/p&gt;
    &lt;p&gt;“In the long term, this is unsustainable for an unpaid volunteer like me. … In the long run, putting such demands on OSS maintainers without compensating them is detrimental. … It’s even more unlikely with Google Project Zero, the best white-hat security researchers money can buy, breathing down the necks of volunteers.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Google’s Controversial Security Disclosure Policy&lt;/head&gt;
    &lt;p&gt;What made this a hot issue was that back in July, Google Project Zero (GPZ) announced a trial of its new Reporting Transparency policy. With this policy change, GPZ announces that it has reported an issue on a specific project within a week of discovery, and the security standard 90-day disclosure clock then starts, regardless of whether a patch is available or not.&lt;/p&gt;
    &lt;p&gt;Many volunteer open source program maintainers and developers feel this is massively unfair to put them under such pressure when Google has billions to address the problem.&lt;/p&gt;
    &lt;p&gt;FFmpeg tweeted, “We take security very seriously, but at the same time, is it really fair that trillion-dollar corporations run AI to find security issues in people’s hobby code? Then expect volunteers to fix.”&lt;/p&gt;
    &lt;p&gt;True, Google does offer a Patch Rewards Program, but as a Twitter user using the handle Ignix The Salamander observed, “FFmpeg already mentioned the program is too limited for them, and they point out the three patches per month limit. Please don’t assume people complain just for the sake of complaining, there is a genuine conflict between corporate security &amp;amp; usage vs open source support IMHO.”&lt;/p&gt;
    &lt;p&gt;Lorenc argues back, in an e-mail to me, that “Creating and publishing software under an open source license is an act of contribution to the digital commons. Finding and publishing information about security issues in that software is also an act of contribution to the same commons.&lt;/p&gt;
    &lt;p&gt;“The position of the FFmpeg X account is that somehow disclosing vulnerabilities is a bad thing. Google provides more assistance to open source software projects than almost any other organization, and these debates are more likely to drive away potential sponsors than to attract them.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Differing Perspectives on Vulnerability Disclosures&lt;/head&gt;
    &lt;p&gt;The fundamental problem remains that the FFmpeg team lacks the financial and developer resources to address a flood of AI-created CVEs.&lt;/p&gt;
    &lt;p&gt;On the other hand, security experts are certainly right in thinking that FFmpeg is a critical part of the Internet’s technology framework and that security issues do need to be made public responsibly and addressed. After all, hackers can use AI to find vulnerabilities in the same way Google does with its AI bug finder, Big Sleep, and Google wants to identify potential security holes ahead of them.&lt;/p&gt;
    &lt;p&gt;The reality is, however, that without more support from the trillion-dollar companies that profit from open source, many woefully underfunded, volunteer-driven critical open-source projects will no longer be maintained at all.&lt;/p&gt;
    &lt;p&gt;For example, Wellnhofer has said he will no longer maintain libxml2 in December. Libxml2 is a critical library in all web browsers, web servers, LibreOffice and numerous Linux packages. We don’t need any more arguments; we need real support for critical open source programs before we have another major security breach.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thenewstack.io/ffmpeg-to-google-fund-us-or-stop-sending-bugs/"/><published>2025-11-11T18:32:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45891817</id><title>Agentic pelican on a bicycle</title><updated>2025-11-12T00:50:22.995044+00:00</updated><content>&lt;doc fingerprint="3f239a28bf3adc7d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Agentic Pelican on a Bicycle&lt;/head&gt;
    &lt;p&gt;The agentic loop—generate, assess, improve—seems like a natural fit for iterating on pelicans on bicycles.&lt;/p&gt;
    &lt;p&gt;Simon Willison has been running his own informal model benchmark for years: “Generate an SVG of a pelican riding a bicycle.” It’s delightfully absurd—and surprisingly revealing. Even the model labs channel this benchmark in their marketing campaigns announcing new models.&lt;/p&gt;
    &lt;p&gt;Simon’s traditional approach is zero-shot: throw the prompt at the model, get SVG back. Maybe—if you’re lucky—you get something resembling a pelican on a bicycle.&lt;/p&gt;
    &lt;p&gt;Nowadays everyone is talking about agents. Models running in a loop using tools. Sometimes they have vision capabilities, too. They can look at what they just created, cringe a little, and try again. The agentic loop—generate, assess, improve—seems like a natural fit for such a task.&lt;/p&gt;
    &lt;p&gt;So I ran a different experiment: what if we let models iterate on their pelicans? What if they could see their own output and self-correct?&lt;/p&gt;
    &lt;head rend="h2"&gt;The Prompt&lt;/head&gt;
    &lt;code&gt;Generate an SVG of a pelican riding a bicycle

- Convert the .svg to .jpg using chrome devtools, then look at the .jpg using your vision capabilities.
- Improve the .svg based on what you see in the .jpg and what's still to improve.
- Keep iterating in this loop until you're satisfied with the generated svg.
- Keep the .jpg for every iteration along the way.&lt;/code&gt;
    &lt;p&gt;Besides the file system and access to a command line, the models had access to Chrome DevTools MCP server (for SVG-to-JPG conversion) and their own multimodal vision capabilities. They could see what they’d drawn, identify problems, and iterate. The loop continued until they declared satisfaction.&lt;/p&gt;
    &lt;p&gt;I used the Chrome DevTools MCP server to give every model the same rasterizer. Without this, models would fall back to whatever SVG-to-image conversion they prefer or have available locally—ImageMagick, Inkscape, browser screenshots, whatever. Standardizing the rendering removes one variable from the equation.&lt;/p&gt;
    &lt;p&gt;The prompt itself is deliberately minimal. I could have steered the iterative loop with more specific guidance—“focus on anatomical accuracy,” “prioritize mechanical realism,” “ensure visual balance.” But that would defeat the point. Simon’s original benchmark is beautifully unconstrained, and I wanted to preserve that spirit. The question isn’t “can models follow detailed improvement instructions?” It’s “when left to their own judgment, what do they choose to fix?”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Models&lt;/head&gt;
    &lt;p&gt;I tested six models across the frontier, all multimodal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Opus 4.1, Claude Sonnet 4.5, Claude Haiku 4.5, all with thinking&lt;/item&gt;
      &lt;item&gt;GPT-5 (on medium reasoning effort)&lt;/item&gt;
      &lt;item&gt;GPT-5-Codex (on medium reasoning effort)&lt;/item&gt;
      &lt;item&gt;Gemini 2.5 Pro&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each model decided independently when to stop iterating. Some made four passes. Others kept going for six. None knew when to quit.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Results&lt;/head&gt;
    &lt;p&gt;Let’s see what happened. For each model, I’m showing the first attempt (left) and the final result (right) after self-correction.&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Opus 4.1 (4 iterations)&lt;/head&gt;
    &lt;p&gt;Opus started with a serviceable pelican-bicycle combo and then did something interesting: it added realism. The final version has an actual bicycle chain connecting the pedals to the rear wheel. The wheels gained more spokes. The pelican’s proportions improved, and it got arms holding the handlebars. This wasn’t just “add more details”—it was “make this mechanically coherent.” Interestingly, we got the catch of the day on a special plate on the handlebars. Oh, and look at the street and the birds in the backdrop!&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Sonnet 4.5 (4 iterations)&lt;/head&gt;
    &lt;p&gt;Sonnet took a more restrained approach. The changes between iterations were subtler—refinements to curves, adding shadows, and movement indicators. Adjustments to positioning. Improving the spokes. Improving the arms and handlebars. The final result is cleaner, but the core composition remained remarkably stable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Haiku 4.5 (6 iterations)&lt;/head&gt;
    &lt;p&gt;Haiku took the longest journey—six full iterations. It kept tweaking, kept adjusting. The additional iterations didn’t necessarily produce a dramatically better result, but Haiku seemed determined to get every detail right before calling it done. So the pelican definitely received proper legs and feet.&lt;/p&gt;
    &lt;head rend="h3"&gt;GPT-5 Medium (5 iterations)&lt;/head&gt;
    &lt;p&gt;GPT-5 Medium started with a recognizable pelican-bicycle scene and refined it over five iterations. The improvements were incremental—better proportions, clearer shapes—but the fundamental composition held steady throughout.&lt;/p&gt;
    &lt;head rend="h3"&gt;GPT-5-Codex Medium (5 iterations)&lt;/head&gt;
    &lt;p&gt;Here’s where things get interesting. Its initial attempt was… let’s call it “abstract.” A sort of layer cake of pelican parts. And then, instead of simplifying, it doubled down. The final result added even more layers. More complexity. More parts. Whether this counts as “improvement” is a philosophical question I’m not qualified to answer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gemini 2.5 Pro (6 iterations)&lt;/head&gt;
    &lt;p&gt;Gemini was the outlier. Most models preserved their initial composition through iterations, making refinements but keeping the core structure mostly intact. Gemini actually changed the fundamental arrangement—the pelican’s pose, the bicycle’s orientation, the spatial relationship between them. Six iterations showed a bigger leap.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Did We Learn?&lt;/head&gt;
    &lt;p&gt;The results are… mixed.&lt;/p&gt;
    &lt;p&gt;The optimistic take: Models like Opus 4.1 made genuinely thoughtful improvements. Adding a bicycle chain isn’t just decoration—it shows understanding of mechanical relationships. The wheel spokes, the adjusted proportions—these are signs of vision-driven refinement working as intended.&lt;/p&gt;
    &lt;p&gt;The skeptical take: Most models didn’t fundamentally change their approach. They tweaked. They adjusted. They added details. But the basic composition—pelican shape, bicycle shape, spatial relationship—was determined in iteration one and largely frozen thereafter.&lt;/p&gt;
    &lt;p&gt;The confusing take: Some models (looking at you, GPT-5-Codex) seemed to mistake “more complex” for “better.” The self-feedback loop amplified their initial artistic direction rather than correcting it. If your first draft is a layer cake of pelican parts, and your self-correction produces an even more elaborate layer cake... did the loop help? Of course, GPT-5-Codex is a fine-tune of GPT-5, optimized for engineering tasks. Could be that its strengths are not in broad visual capabilities.&lt;/p&gt;
    &lt;p&gt;The agentic approach definitely produces different results than zero-shot generation. Whether it produces better results seems to depend heavily on the model’s ability to self-critique. Vision capabilities alone aren’t enough—you need something more: aesthetic judgment, mechanical reasoning, or at least the wisdom to know when to stop adding details.&lt;/p&gt;
    &lt;p&gt;Simon’s zero-shot benchmark reveals how well models handle unusual creative tasks on the first try. The agentic variant reveals something else: how well models can evaluate and improve their own creative output. Turns out, that’s a different skill entirely. But—somehow related?&lt;/p&gt;
    &lt;p&gt;All test code and results available in the GitHub repository.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.robert-glaser.de/agentic-pelican-on-a-bicycle/"/><published>2025-11-11T19:40:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45891868</id><title>A catalog of side effects</title><updated>2025-11-12T00:50:22.767269+00:00</updated><content>&lt;doc fingerprint="6202111079d508b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Optimizing compilers like to keep track of each IR instruction’s effects. An instruction’s effects vary wildly from having no effects at all, to writing a specific variable, to completely unknown (writing all state).&lt;/p&gt;
    &lt;p&gt;This post can be thought of as a continuation of What I talk about when I talk about IRs, specifically the section talking about asking the right questions. When we talk about effects, we should ask the right questions: not what opcode is this? but instead what effects does this opcode have?&lt;/p&gt;
    &lt;p&gt;Different compilers represent and track these effects differently. I’ve been thinking about how to represent these effects all year, so I have been doing some reading. In this post I will give some summaries of the landscape of approaches. Please feel free to suggest more.&lt;/p&gt;
    &lt;p&gt;Internal IR effect tracking is similar to the programming language notion of algebraic effects in type systems, but internally, compilers keep track of finer-grained effects. Effects such as “writes to a local variable”, “writes to a list”, or “reads from the stack” indicate what instructions can be re-ordered, duplicated, or removed entirely.&lt;/p&gt;
    &lt;p&gt;For example, consider the following pseodocode for some made-up language that stands in for a snippet of compiler IR:&lt;/p&gt;
    &lt;code&gt;# ...
v = some_var[0]
another_var[0] = 5
# ...
&lt;/code&gt;
    &lt;p&gt;The goal of effects is to communicate to the compiler if, for example, these two IR instructions can be re-ordered. The second instruction might write to a location that the first one reads. But it also might not! This is about knowing if &lt;code&gt;some_var&lt;/code&gt; and &lt;code&gt;another_var&lt;/code&gt; alias—if they are different names that
refer to the same object.&lt;/p&gt;
    &lt;p&gt;We can sometimes answer that question directly, but often it’s cheaper to compute an approximate answer: could they even alias? It’s possible that &lt;code&gt;some_var&lt;/code&gt; and &lt;code&gt;another_var&lt;/code&gt; have different types, meaning that (as long as you
have strict aliasing) the &lt;code&gt;Load&lt;/code&gt; and &lt;code&gt;Store&lt;/code&gt; operations that implement these
reads and writes by definition touch different locations. And if they look
at disjoint locations, there need not be any explicit order enforced.&lt;/p&gt;
    &lt;p&gt;Different compilers keep track of this information differently. The null effect analysis gives up and says “every instruction is maximally effectful” and therefore “we can’t re-order or delete any instructions”. That’s probably fine for a first stab at a compiler, where you will get a big speed up purely based on strength reductions. Over-approximations of effects should always be valid.&lt;/p&gt;
    &lt;p&gt;But at some point you start wanting to do dead code elimination (DCE), or common subexpression elimination (CSE), or loads/store elimination, or move instructions around, and you start wondering how to represent effects. That’s where I am right now. So here’s a catalog of different compilers I have looked at recently.&lt;/p&gt;
    &lt;p&gt;There are two main ways I have seen to represent effects: bitsets and heap range lists. We’ll look at one example compiler for each, talk a bit about tradeoffs, then give a bunch of references to other major compilers.&lt;/p&gt;
    &lt;p&gt;We’ll start with Cinder, a Python JIT, because that’s what I used to work on.&lt;/p&gt;
    &lt;p&gt;Cinder tracks heap effects for its high-level IR (HIR) in instr_effects.h. Pretty much everything happens in the &lt;code&gt;memoryEffects(const Instr&amp;amp; instr)&lt;/code&gt; function, which is expected to know
everything about what effects the given instruction might have.&lt;/p&gt;
    &lt;p&gt;The data representation is a bitset representation of a lattice called an &lt;code&gt;AliasClass&lt;/code&gt; and that is defined in alias_class.h. Each
bit in the bitset represents a distinct location in the heap: reads from and
writes to each of these locations are guaranteed not to affect any of the other
locations.&lt;/p&gt;
    &lt;p&gt;Here is the X-macro that defines it:&lt;/p&gt;
    &lt;code&gt;#define HIR_BASIC_ACLS(X) \
  X(ArrayItem)            \
  X(CellItem)             \
  X(DictItem)             \
  X(FuncArgs)             \
  X(FuncAttr)             \
  X(Global)               \
  X(InObjectAttr)         \
  X(ListItem)             \
  X(Other)                \
  X(TupleItem)            \
  X(TypeAttrCache)        \
  X(TypeMethodCache)

enum BitIndexes {
#define ACLS(name) k##name##Bit,
    HIR_BASIC_ACLS(ACLS)
#undef ACLS
};
&lt;/code&gt;
    &lt;p&gt;Note that each bit implicitly represents a set: &lt;code&gt;ListItem&lt;/code&gt; does not refer to a
specific list index, but the infinite set of all possible list indices. It’s
any list index. Still, every list index is completely disjoint from, say, every
entry in a global variable table.&lt;/p&gt;
    &lt;p&gt;(And, to be clear, an object in a list might be the same as an object in a global variable table. The objects themselves can alias. But the thing being written to or read from, the thing being side effected, is the container.)&lt;/p&gt;
    &lt;p&gt;Like other bitset lattices, it’s possible to union the sets by or-ing the bits. It’s possible to query for overlap by and-ing the bits.&lt;/p&gt;
    &lt;code&gt;class AliasClass {
  // The union of two AliasClass
  AliasClass operator|(AliasClass other) const {
    return AliasClass{bits_ | other.bits_};
  }

  // The intersection (overlap) of two AliasClass
  AliasClass operator&amp;amp;(AliasClass other) const {
    return AliasClass{bits_ &amp;amp; other.bits_};
  }
};
&lt;/code&gt;
    &lt;p&gt;If this sounds familiar, it’s because (as the repo notes) it’s a similar idea to Cinder’s type lattice representation.&lt;/p&gt;
    &lt;p&gt;Like other lattices, there is both a bottom element (no effects) and a top element (all possible effects):&lt;/p&gt;
    &lt;code&gt;#define HIR_OR_BITS(name) | k##name

#define HIR_UNION_ACLS(X)                           \
  /* Bottom union */                                \
  X(Empty, 0)                                       \
  /* Top union */                                   \
  X(Any, 0 HIR_BASIC_ACLS(HIR_OR_BITS))             \
  /* Memory locations accessible by managed code */ \
  X(ManagedHeapAny, kAny &amp;amp; ~kFuncArgs)
&lt;/code&gt;
    &lt;p&gt;Union operations naturally hit a fixpoint at &lt;code&gt;Any&lt;/code&gt; and intersection operations
naturally hit a fixpoint at &lt;code&gt;Empty&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;All of this together lets the optimizer ask and answer questions such as:&lt;/p&gt;
    &lt;p&gt;and more.&lt;/p&gt;
    &lt;p&gt;Let’s take a look at an (imaginary) IR version of the code snippet in the intro and see what analyzing it might look like in the optimizer. Here is the fake IR:&lt;/p&gt;
    &lt;code&gt;v0: Tuple = ...
v1: List = ...
v2: Int[5] = ...
# v = some_var[0]
v3: Object = LoadTupleItem v0, 0
# another_var[0] = 5
StoreListItem v1, 0, v2
&lt;/code&gt;
    &lt;p&gt;You can imagine that &lt;code&gt;LoadTupleItem&lt;/code&gt; declares that it reads from the
&lt;code&gt;TupleItem&lt;/code&gt; heap and &lt;code&gt;StoreListItem&lt;/code&gt; declares that it writes to the &lt;code&gt;ListItem&lt;/code&gt;
heap. Because tuple and list pointers cannot be casted into one another and
therefore cannot alias, these are
disjoint heaps in our bitset. Therefore &lt;code&gt;ListItem &amp;amp; TupleItem == 0&lt;/code&gt;, therefore
these memory operations can never interfere! They can (for example) be
re-ordered arbitrarily.&lt;/p&gt;
    &lt;p&gt;In Cinder, these memory effects could in the future be used for instruction re-ordering, but they are today mostly used in two places: the refcount insertion pass and DCE.&lt;/p&gt;
    &lt;p&gt;DCE involves first finding the set of instructions that need to be kept around because they are useful/important/have effects. So here is what the Cinder DCE &lt;code&gt;isUseful&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;bool isUseful(Instr&amp;amp; instr) {
  return instr.IsTerminator() || instr.IsSnapshot() ||
      (instr.asDeoptBase() != nullptr &amp;amp;&amp;amp; !instr.IsPrimitiveBox()) ||
      (!instr.IsPhi() &amp;amp;&amp;amp; memoryEffects(instr).may_store != AEmpty);
}
&lt;/code&gt;
    &lt;p&gt;There are some other checks in there but &lt;code&gt;memoryEffects&lt;/code&gt; is right there at the
core of it!&lt;/p&gt;
    &lt;p&gt;Now that we have seen the bitset representation of effects and an implementation in Cinder, let’s take a look at a different representation and and an implementation in JavaScriptCore.&lt;/p&gt;
    &lt;p&gt;I keep coming back to How I implement SSA form by Fil Pizlo, one of the significant contributors to JavaScriptCore (JSC). In particular, I keep coming back to the Uniform Effect Representation section. This notion of “abstract heaps” felt very… well, abstract. Somehow more abstract than the bitset representation. The pre-order and post-order integer pair as a way to represent nested heap effects just did not click.&lt;/p&gt;
    &lt;p&gt;It didn’t make any sense until I actually went spelunking in JavaScriptCore and found one of several implementations—because, you know, JSC is six compilers in a trenchcoat[citation needed].&lt;/p&gt;
    &lt;p&gt;DFG, B3, DOMJIT, and probably others all have their own abstract heap implementations. We’ll look at DOMJIT mostly because it’s a smaller example and also illustrates something else that’s interesting: builtins. We’ll come back to builtins in a minute.&lt;/p&gt;
    &lt;p&gt;Let’s take a lookat how DOMJIT structures its abstract heaps: a YAML file.&lt;/p&gt;
    &lt;code&gt;DOM:
    Tree:
        Node:
            - Node_firstChild
            - Node_lastChild
            - Node_parentNode
            - Node_nextSibling
            - Node_previousSibling
            - Node_ownerDocument
        Document:
            - Document_documentElement
            - Document_body
&lt;/code&gt;
    &lt;p&gt;It’s a hierarchy. &lt;code&gt;Node_firstChild&lt;/code&gt; is a subheap of &lt;code&gt;Node&lt;/code&gt; is a subheap of…
and so on. A write to any &lt;code&gt;Node_nextSibling&lt;/code&gt; is a write to &lt;code&gt;Node&lt;/code&gt; is a write to
… Sibling heaps are unrelated: &lt;code&gt;Node_firstChild&lt;/code&gt; and &lt;code&gt;Node_lastChild&lt;/code&gt;, for
example, are disjoint.&lt;/p&gt;
    &lt;p&gt;To get a feel for this, I wired up a simplified version of ZJIT’s bitset generator (for types!) to read a YAML document and generate a bitset. It generated the following Rust code:&lt;/p&gt;
    &lt;code&gt;mod bits {
  pub const Empty: u64 = 0u64;
  pub const Document_body: u64 = 1u64 &amp;lt;&amp;lt; 0;
  pub const Document_documentElement: u64 = 1u64 &amp;lt;&amp;lt; 1;
  pub const Document: u64 = Document_body | Document_documentElement;
  pub const Node_firstChild: u64 = 1u64 &amp;lt;&amp;lt; 2;
  pub const Node_lastChild: u64 = 1u64 &amp;lt;&amp;lt; 3;
  pub const Node_nextSibling: u64 = 1u64 &amp;lt;&amp;lt; 4;
  pub const Node_ownerDocument: u64 = 1u64 &amp;lt;&amp;lt; 5;
  pub const Node_parentNode: u64 = 1u64 &amp;lt;&amp;lt; 6;
  pub const Node_previousSibling: u64 = 1u64 &amp;lt;&amp;lt; 7;
  pub const Node: u64 = Node_firstChild | Node_lastChild | Node_nextSibling | Node_ownerDocument | Node_parentNode | Node_previousSibling;
  pub const Tree: u64 = Document | Node;
  pub const DOM: u64 = Tree;
  pub const NumTypeBits: u64 = 8;
}
&lt;/code&gt;
    &lt;p&gt;It’s not a fancy X-macro, but it’s a short and flexible Ruby script.&lt;/p&gt;
    &lt;p&gt;Then I took the DOMJIT abstract heap generator—also funnily enough a short Ruby script—modified the output format slightly, and had it generate its int pairs:&lt;/p&gt;
    &lt;code&gt;mod bits {
  /* DOMJIT Abstract Heap Tree.
  DOM&amp;lt;0,8&amp;gt;:
      Tree&amp;lt;0,8&amp;gt;:
          Node&amp;lt;0,6&amp;gt;:
              Node_firstChild&amp;lt;0,1&amp;gt;
              Node_lastChild&amp;lt;1,2&amp;gt;
              Node_parentNode&amp;lt;2,3&amp;gt;
              Node_nextSibling&amp;lt;3,4&amp;gt;
              Node_previousSibling&amp;lt;4,5&amp;gt;
              Node_ownerDocument&amp;lt;5,6&amp;gt;
          Document&amp;lt;6,8&amp;gt;:
              Document_documentElement&amp;lt;6,7&amp;gt;
              Document_body&amp;lt;7,8&amp;gt;
  */
  pub const DOM: HeapRange = HeapRange { start: 0, end: 8 };
  pub const Tree: HeapRange = HeapRange { start: 0, end: 8 };
  pub const Node: HeapRange = HeapRange { start: 0, end: 6 };
  pub const Node_firstChild: HeapRange = HeapRange { start: 0, end: 1 };
  pub const Node_lastChild: HeapRange = HeapRange { start: 1, end: 2 };
  pub const Node_parentNode: HeapRange = HeapRange { start: 2, end: 3 };
  pub const Node_nextSibling: HeapRange = HeapRange { start: 3, end: 4 };
  pub const Node_previousSibling: HeapRange = HeapRange { start: 4, end: 5 };
  pub const Node_ownerDocument: HeapRange = HeapRange { start: 5, end: 6 };
  pub const Document: HeapRange = HeapRange { start: 6, end: 8 };
  pub const Document_documentElement: HeapRange = HeapRange { start: 6, end: 7 };
  pub const Document_body: HeapRange = HeapRange { start: 7, end: 8 };
}
&lt;/code&gt;
    &lt;p&gt;It already comes with a little diagram, which is super helpful for readability.&lt;/p&gt;
    &lt;p&gt;Any empty range(s) represent empty heap effects: if the start and end are the same number, there are no effects. There is no one &lt;code&gt;Empty&lt;/code&gt; value, but any empty
range could be normalized to &lt;code&gt;HeapRange { start: 0, end: 0 }&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Maybe this was obvious to you, dear reader, but this pre-order/post-order thing is about nested ranges! Seeing the output of the generator laid out clearly like this made it make a lot more sense for me.&lt;/p&gt;
    &lt;p&gt;What about checking overlap? Here is the implementation in JSC:&lt;/p&gt;
    &lt;code&gt;namespace WTF {
// Check if two ranges overlap assuming that neither range is empty.
template&amp;lt;typename T&amp;gt;
constexpr bool nonEmptyRangesOverlap(T leftMin, T leftMax, T rightMin, T rightMax)
{
    ASSERT_UNDER_CONSTEXPR_CONTEXT(leftMin &amp;lt; leftMax);
    ASSERT_UNDER_CONSTEXPR_CONTEXT(rightMin &amp;lt; rightMax);

    return leftMax &amp;gt; rightMin &amp;amp;&amp;amp; rightMax &amp;gt; leftMin;
}

// Pass ranges with the min being inclusive and the max being exclusive.
template&amp;lt;typename T&amp;gt;
constexpr bool rangesOverlap(T leftMin, T leftMax, T rightMin, T rightMax) {
    ASSERT_UNDER_CONSTEXPR_CONTEXT(leftMin &amp;lt;= leftMax);
    ASSERT_UNDER_CONSTEXPR_CONTEXT(rightMin &amp;lt;= rightMax);

    // Empty ranges interfere with nothing.
    if (leftMin == leftMax)
        return false;
    if (rightMin == rightMax)
        return false;

    return nonEmptyRangesOverlap(leftMin, leftMax, rightMin, rightMax);
}
}

class HeapRange {
    bool overlaps(const HeapRange&amp;amp; other) const {
        return WTF::rangesOverlap(m_begin, m_end, other.m_begin, other.m_end);
    }
}
&lt;/code&gt;
    &lt;p&gt;(See also How to check for overlapping intervals and Range overlap in two compares for more fun.)&lt;/p&gt;
    &lt;p&gt;While bitsets are a dense representation (you have to hold every bit), they are very compact and they are very precise. You can hold any number of combinations of 64 or 128 bits in a single register. The union and intersection operations are very cheap.&lt;/p&gt;
    &lt;p&gt;With int ranges, it’s a little more complicated. An imprecise union of &lt;code&gt;a&lt;/code&gt; and
&lt;code&gt;b&lt;/code&gt; can take the maximal range that covers both &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. To get a more
precise union, you have to keep track of both. In the worst case, if you want
efficient arbitrary queries, you need to store your int ranges in an interval
tree. So what gives?&lt;/p&gt;
    &lt;p&gt;I asked Fil if both bitsets and int ranges answer the same question, why use int ranges? He said that it’s more flexible long-term: bitsets get expensive as soon as you need over 128 bits (you might need to heap allocate them!) whereas ranges have no such ceiling. But doesn’t holding sequences of ranges require heap allocation? Well, despite Fil writing this in his SSA post:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The purpose of the effect representation baked into the IR is to provide a precise always-available baseline for alias information that is super easy to work with. […] you can have instructions report that they read/write multiple heaps […] you can have a utility function that produces such lists on demand.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It’s important to note that this doesn’t actually involve any allocation of lists. JSC does this very clever thing where they have “functors” that they pass in as arguments that compress/summarize what they want to out of an instruction’s effects.&lt;/p&gt;
    &lt;p&gt;Let’s take a look at how the DFG (for example) uses these heap ranges in analysis. The DFG is structured in such a way that it can make use of the DOMJIT heap ranges directly, which is neat.&lt;/p&gt;
    &lt;p&gt;Note that &lt;code&gt;AbstractHeap&lt;/code&gt; in the example below is a thin wrapper over the DFG
compiler’s own &lt;code&gt;DOMJIT::HeapRange&lt;/code&gt; equivalent:&lt;/p&gt;
    &lt;code&gt;class AbstractHeapOverlaps {
public:
    AbstractHeapOverlaps(AbstractHeap heap)
        : m_heap(heap)
        , m_result(false)
    {
    }

    void operator()(AbstractHeap otherHeap) const
    {
        if (m_result)
            return;
        m_result = m_heap.overlaps(otherHeap);
    }

    bool result() const { return m_result; }

private:
    AbstractHeap m_heap;
    mutable bool m_result;
};

bool writesOverlap(Graph&amp;amp; graph, Node* node, AbstractHeap heap)
{
    NoOpClobberize noOp;
    AbstractHeapOverlaps addWrite(heap);
    clobberize(graph, node, noOp, addWrite, noOp);
    return addWrite.result();
}
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;clobberize&lt;/code&gt; is the function that calls these functors (&lt;code&gt;noOp&lt;/code&gt; or &lt;code&gt;addWrite&lt;/code&gt; in
this case) for each effect that the given IR instruction &lt;code&gt;node&lt;/code&gt; declares.&lt;/p&gt;
    &lt;p&gt;I’ve pulled some relevant snippets of &lt;code&gt;clobberize&lt;/code&gt;, which is quite long, that I
think are interesting.&lt;/p&gt;
    &lt;p&gt;First, some instructions (constants, here) have no effects. There’s some utility in the &lt;code&gt;def(PureValue(...))&lt;/code&gt; call but I didn’t understand fully.&lt;/p&gt;
    &lt;p&gt;Then there are some instructions that conditionally have effects depending on the use types of their operands.1 Taking the absolute value of an Int32 or a Double is effect-free but otherwise looks like it can run arbitrary code.&lt;/p&gt;
    &lt;p&gt;Some run-time IR guards that might cause side exits are annotated as such—they write to the &lt;code&gt;SideState&lt;/code&gt; heap.&lt;/p&gt;
    &lt;p&gt;Local variable instructions read specific heaps indexed by what looks like the local index but I’m not sure. This means accessing two different locals won’t alias!&lt;/p&gt;
    &lt;p&gt;Instructions that allocate can’t be re-ordered, it looks like; they both read and write the &lt;code&gt;HeapObjectCount&lt;/code&gt;. This probably limits the amount of allocation
sinking that can be done.&lt;/p&gt;
    &lt;p&gt;Then there’s &lt;code&gt;CallDOM&lt;/code&gt;, which is the builtins stuff I was talking about. We’ll
come back to that after the code block.&lt;/p&gt;
    &lt;code&gt;template&amp;lt;typename ReadFunctor, typename WriteFunctor, typename DefFunctor, typename ClobberTopFunctor&amp;gt;
void clobberize(Graph&amp;amp; graph, Node* node, const ReadFunctor&amp;amp; read, const WriteFunctor&amp;amp; write, const DefFunctor&amp;amp; def)
{
    // ...

    switch (node-&amp;gt;op()) {
    case JSConstant:
    case DoubleConstant:
    case Int52Constant:
        def(PureValue(node, node-&amp;gt;constant()));
        return;

    case ArithAbs:
        if (node-&amp;gt;child1().useKind() == Int32Use || node-&amp;gt;child1().useKind() == DoubleRepUse)
            def(PureValue(node, node-&amp;gt;arithMode()));
        else
            clobberTop();
        return;

    case AssertInBounds:
    case AssertNotEmpty:
        write(SideState);
        return;

    case GetLocal:
        read(AbstractHeap(Stack, node-&amp;gt;operand()));
        def(HeapLocation(StackLoc, AbstractHeap(Stack, node-&amp;gt;operand())), LazyNode(node));
        return;

    case NewArrayWithSize:
    case NewArrayWithSizeAndStructure:
        read(HeapObjectCount);
        write(HeapObjectCount);
        return;

    case CallDOM: {
        const DOMJIT::Signature* signature = node-&amp;gt;signature();
        DOMJIT::Effect effect = signature-&amp;gt;effect;
        if (effect.reads) {
            if (effect.reads == DOMJIT::HeapRange::top())
                read(World);
            else
                read(AbstractHeap(DOMState, effect.reads.rawRepresentation()));
        }
        if (effect.writes) {
            if (effect.writes == DOMJIT::HeapRange::top()) {
                if (Options::validateDFGClobberize())
                    clobberTopFunctor();
                write(Heap);
            } else
                write(AbstractHeap(DOMState, effect.writes.rawRepresentation()));
        }
        ASSERT_WITH_MESSAGE(effect.def == DOMJIT::HeapRange::top(), "Currently, we do not accept any def for CallDOM.");
        return;
    }
    }
}
&lt;/code&gt;
    &lt;p&gt;(Remember that these &lt;code&gt;AbstractHeap&lt;/code&gt; operations are very similar to DOMJIT’s
&lt;code&gt;HeapRange&lt;/code&gt; with a couple more details—and in some cases even contain DOMJIT
&lt;code&gt;HeapRange&lt;/code&gt;s!)&lt;/p&gt;
    &lt;p&gt;This &lt;code&gt;CallDOM&lt;/code&gt; node is the way for the DOM APIs in the browser—a significant
chunk of the builtins, which are written in C++—to communicate what they do
to the optimizing compiler. Without any annotations, the JIT has to assume that
a call into C++ could do anything to the JIT state. Bummer!&lt;/p&gt;
    &lt;p&gt;But because, for example, &lt;code&gt;Node.firstChild&lt;/code&gt; annotates what
memory it reads from and what it doesn’t write to,
the JIT can optimize around it better—or even remove the access completely.
It means the JIT can reason about calls to known builtins the same way that
it reasons about normal JIT opcodes.&lt;/p&gt;
    &lt;p&gt;(Incidentally it looks like it doesn’t even make a C call, but instead is inlined as a little memory read snippet using a JIT builder API. Neat.)&lt;/p&gt;
    &lt;p&gt;Last, we’ll look at Simple, which has a slightly different take on all of this.&lt;/p&gt;
    &lt;p&gt;Simple is Cliff Click’s pet Sea of Nodes (SoN) project to try and showcase the idea to the world—outside of a HotSpot C2 context.&lt;/p&gt;
    &lt;p&gt;This one is a little harder for me to understand but it looks like each translation unit has a &lt;code&gt;StartNode&lt;/code&gt; that doles out
different classes of memory nodes for each alias class. Each IR node then takes
data dependencies on whatever effect nodes it might uses.&lt;/p&gt;
    &lt;p&gt;Alias classes are split up based on the paper Type-Based Alias Analysis (PDF): “Our approach is a form of TBAA similar to the ‘FieldTypeDecl’ algorithm described in the paper.”&lt;/p&gt;
    &lt;p&gt;The Simple project is structured into sequential implementation stages and alias classes come into the picture in Chapter 10.&lt;/p&gt;
    &lt;p&gt;Because I spent a while spelunking through other implementations to see how other projects did this, here is a list of the projects I looked at. Mostly, they use bitsets.&lt;/p&gt;
    &lt;p&gt;HHVM, a JIT for the Hack language, also uses a bitset for its memory effects. See for example: alias-class.h and memory-effects.h.&lt;/p&gt;
    &lt;p&gt;HHVM has a couple places that use this information, such as a definition-sinking pass, alias analysis, DCE, store elimination, refcount opts, and more.&lt;/p&gt;
    &lt;p&gt;If you are wondering why the HHVM representation looks similar to the Cinder representation, it’s because some former HHVM engineers such as Brett Simmers also worked on Cinder!&lt;/p&gt;
    &lt;p&gt;(note that I am linking an ART fork on GitHub as a reference, but the upstream code is hosted on googlesource)&lt;/p&gt;
    &lt;p&gt;Android’s ART Java runtime also uses a bitset for its effect representation. It’s a very compact class called &lt;code&gt;SideEffects&lt;/code&gt; in nodes.h.&lt;/p&gt;
    &lt;p&gt;The side effects are used in loop-invariant code motion, global value numbering, write barrier elimination, scheduling, and more.&lt;/p&gt;
    &lt;p&gt;CoreCLR mostly uses a bitset for its &lt;code&gt;SideEffectSet&lt;/code&gt;
class. This one is interesting though because it also splits out effects
specifically to include sets of local variables (&lt;code&gt;LclVarSet&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;V8 is also about six completely different compilers in a trenchcoat.&lt;/p&gt;
    &lt;p&gt;Turboshaft uses a struct in operations.h called &lt;code&gt;OpEffects&lt;/code&gt; which is two bitsets for reads/writes of effects. This is used in
value numbering as well a bunch of
other small optimization passes they call “reducers”.&lt;/p&gt;
    &lt;p&gt;Maglev also has this thing called &lt;code&gt;NodeT::kProperties&lt;/code&gt; in their IR
nodes that also looks like a bitset and is used in their various
reducers. It has effect query methods on it such as &lt;code&gt;can_eager_deopt&lt;/code&gt; and
&lt;code&gt;can_write&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Until recently, V8 also used Sea of Nodes as its IR representation, which also tracks side effects more explicitly in the structure of the IR itself.&lt;/p&gt;
    &lt;p&gt;Guile Scheme looks like it has a custom tagging scheme type thing.&lt;/p&gt;
    &lt;p&gt;Both bitsets and int ranges are perfectly cromulent ways of representing heap effects for your IR. The Sea of Nodes approach is also probably okay since it powers HotSpot C2 and (for a time) V8.&lt;/p&gt;
    &lt;p&gt;Remember to ask the right questions of your IR when doing analysis.&lt;/p&gt;
    &lt;p&gt;Thank you to Fil Pizlo for writing his initial GitHub Gist and sending me on this journey and thank you to Chris Gregory, Brett Simmers, and Ufuk Kayserilioglu for feedback on making some of the explanations more helpful.&lt;/p&gt;
    &lt;p&gt;This is because the DFG compiler does this interesting thing where they track and guard the input types on use vs having types attached to the input’s own def. It might be a clean way to handle shapes inside the type system while also allowing the type+shape of an object to change over time (which it can do in many dynamic language runtimes). ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bernsteinbear.com/blog/compiler-effects/"/><published>2025-11-11T19:44:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45891907</id><title>A modern 35mm film scanner for home</title><updated>2025-11-12T00:50:21.741915+00:00</updated><content>&lt;doc fingerprint="4d1a5a5e7b33d889"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The New Era of &lt;lb/&gt;Film Scanning&lt;/head&gt;
    &lt;head rend="h2"&gt;The New Era of &lt;lb/&gt;Film Scanning&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke - a high-resolution 35 mm film scanner built for photographers who demand speed, quality and control.&lt;/head&gt;
    &lt;head rend="h2"&gt;The New Era of &lt;lb/&gt;Film Scanning&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke - a high-resolution 35 mm film scanner built for photographers who demand speed, quality, and control.&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke redefines film scanning by bringing modern imaging, optics, and software into a beautifully engineered device.&lt;/head&gt;
    &lt;head rend="h3"&gt;4064&lt;/head&gt;
    &lt;head rend="h3"&gt;4064&lt;/head&gt;
    &lt;head rend="h3"&gt;DPI Resolution&lt;/head&gt;
    &lt;head rend="h3"&gt;DPI Resolution&lt;/head&gt;
    &lt;head rend="h3"&gt;120 dB&lt;/head&gt;
    &lt;head rend="h3"&gt;120 dB&lt;/head&gt;
    &lt;head rend="h3"&gt;Dynamic Range&lt;/head&gt;
    &lt;head rend="h3"&gt;Dynamic Range&lt;/head&gt;
    &lt;head rend="h3"&gt;48-bit&lt;/head&gt;
    &lt;head rend="h3"&gt;48-bit&lt;/head&gt;
    &lt;head rend="h3"&gt;True Color&lt;/head&gt;
    &lt;head rend="h3"&gt;True Color&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke - State-of-the-Art Hardware&lt;/head&gt;
    &lt;head rend="h3"&gt;Knokke - State-of-the-Art Hardware&lt;/head&gt;
    &lt;head rend="h2"&gt;Knokke - State-of-the-Art Hardware&lt;/head&gt;
    &lt;p&gt;The modern 35 mm film scanner that captures a full roll in under just a few minutes while capturing every frame at 4064 DPI and 48bit colour. Its custom optics and state-of-the-art sensor deliver benchmark setting quality and speed at a price only Knokke can offer.&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;A Modern Workflow&lt;/p&gt;
    &lt;p&gt;A Modern Workflow&lt;/p&gt;
    &lt;p&gt;A Modern Workflow&lt;/p&gt;
    &lt;head rend="h2"&gt;Korova - Custom Software&lt;/head&gt;
    &lt;head rend="h3"&gt;Korova - Custom Software&lt;/head&gt;
    &lt;head rend="h2"&gt;Korova - Custom Software&lt;/head&gt;
    &lt;p&gt;Built for the 21st century, Knokke runs on Korova, a lean C++ application that's native to Linux, macOS, and Windowsâso you can forget vintage PCs and enjoy a plug-and-play workflow that lets you focus on your photos.&lt;/p&gt;
    &lt;p&gt;Each frame can have custom scan settings, repeatable across multiple scans for consistent results and tailored workflows. The scanner can also skip directly to requested frames, massively accelerating scanning time and enabling fast access to key shots without unnecessary delay.&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;p&gt;Launch Updates&lt;/p&gt;
    &lt;head rend="h3"&gt;Engineered for Individual Users and Lab Professionals&lt;/head&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Price at Launch&lt;/head&gt;
    &lt;head rend="h2"&gt;999â¬&lt;/head&gt;
    &lt;p&gt;Includes scanner + software&lt;/p&gt;
    &lt;head rend="h2"&gt;4064 dpi resolution&lt;/head&gt;
    &lt;head rend="h2"&gt;5 min per roll&lt;/head&gt;
    &lt;head rend="h2"&gt;48-bit colour depth&lt;/head&gt;
    &lt;head rend="h2"&gt;120 dB Dynamic Range&lt;/head&gt;
    &lt;head rend="h2"&gt;LED Matrix&lt;/head&gt;
    &lt;head rend="h2"&gt;RGB LED backlight&lt;/head&gt;
    &lt;head rend="h2"&gt;USB-C 3.2&lt;/head&gt;
    &lt;head rend="h2"&gt;Custom software&lt;/head&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;Specifications.&lt;/head&gt;
    &lt;head rend="h2"&gt;Specifications.&lt;/head&gt;
    &lt;head rend="h3"&gt;IMAGING SYSTEM&lt;/head&gt;
    &lt;p&gt;SENSOR&lt;/p&gt;
    &lt;p&gt;Backside illumintaed CMOS Sensor&lt;/p&gt;
    &lt;p&gt;DYNAMIC RANGE&lt;/p&gt;
    &lt;p&gt;linear dynamic range of 78 dB, expandable to 120 dB with native 16-bit HDR log profile, up to 14 stops of range&lt;/p&gt;
    &lt;p&gt;RESOLUTION&lt;/p&gt;
    &lt;p&gt;Max. 4064 dpi (~22 MP), 2032 dpi (~5,5MP)&lt;/p&gt;
    &lt;p&gt;LENS&lt;/p&gt;
    &lt;p&gt;Custom 4 element lens with high MTF (modulation transfer function)&lt;/p&gt;
    &lt;p&gt;LIGHT SOURCE&lt;/p&gt;
    &lt;p&gt;RGB LED backlight&lt;/p&gt;
    &lt;head rend="h3"&gt;IMAGING SYSTEM&lt;/head&gt;
    &lt;p&gt;SENSOR&lt;/p&gt;
    &lt;p&gt;Backside illumintaed CMOS Sensor&lt;/p&gt;
    &lt;p&gt;DYNAMIC RANGE&lt;/p&gt;
    &lt;p&gt;linear dynamic range of 78 dB, expandable to 120 dB with native 16-bit HDR log profile, up to 14 stops of range&lt;/p&gt;
    &lt;p&gt;RESOLUTION&lt;/p&gt;
    &lt;p&gt;Max. 4064 dpi (~22 MP), 2032 dpi (~5,5MP)&lt;/p&gt;
    &lt;p&gt;LENS&lt;/p&gt;
    &lt;p&gt;Custom 4 element lens with high MTF (modulation transfer function)&lt;/p&gt;
    &lt;p&gt;LIGHT SOURCE&lt;/p&gt;
    &lt;p&gt;RGB LED backlight&lt;/p&gt;
    &lt;head rend="h3"&gt;PERFORMANCE &amp;amp; WORKFLOW&lt;/head&gt;
    &lt;p&gt;Scan Speed&lt;/p&gt;
    &lt;p&gt;per roll under 5 minutes (4064 dpi), under 2 minutes (2032 dpi)&lt;/p&gt;
    &lt;p&gt;FILM TRANSPORT&lt;/p&gt;
    &lt;p&gt;automated, min. strip length 3 images&lt;/p&gt;
    &lt;p&gt;FRAME CONTROL&lt;/p&gt;
    &lt;p&gt;per-frame scan settings, skip directly to any frame&lt;/p&gt;
    &lt;p&gt;DXN DECODER&lt;/p&gt;
    &lt;p&gt;reads 35 mm DX codes, embeds film type, ISO, roll info into metadata&lt;/p&gt;
    &lt;p&gt;SOFTWARE&lt;/p&gt;
    &lt;p&gt;Korova (native for Windows, macOS, Linux)&lt;/p&gt;
    &lt;p&gt;FILE FORMATS&lt;/p&gt;
    &lt;p&gt;RAW, TIFF, DNG linear, JPEG, PNG, BMP, HDR&lt;/p&gt;
    &lt;p&gt;FILE SIZES&lt;/p&gt;
    &lt;p&gt;RAW/TIFF/DNG linear ~127 MB; JPEG XL (lossless) 42â52 MB; PNG 106â118 MB&lt;/p&gt;
    &lt;head rend="h3"&gt;PERFORMANCE &amp;amp; WORKFLOW&lt;/head&gt;
    &lt;p&gt;Scan Speed&lt;/p&gt;
    &lt;p&gt;per roll under 5 minutes (4064 dpi), under 2 minutes (2032 dpi)&lt;/p&gt;
    &lt;p&gt;FILM TRANSPORT&lt;/p&gt;
    &lt;p&gt;automated, min. strip length 3 images&lt;/p&gt;
    &lt;p&gt;FRAME CONTROL&lt;/p&gt;
    &lt;p&gt;per-frame scan settings, skip directly to any frame&lt;/p&gt;
    &lt;p&gt;DXN DECODER&lt;/p&gt;
    &lt;p&gt;reads 35 mm DX codes, embeds film type, ISO, roll info into metadata&lt;/p&gt;
    &lt;p&gt;SOFTWARE&lt;/p&gt;
    &lt;p&gt;Korova (native for Windows, macOS, Linux)&lt;/p&gt;
    &lt;p&gt;FILE FORMATS&lt;/p&gt;
    &lt;p&gt;RAW, TIFF, DNG linear, JPEG, PNG, BMP, HDR&lt;/p&gt;
    &lt;p&gt;FILE SIZES&lt;/p&gt;
    &lt;p&gt;RAW/TIFF/DNG linear ~127 MB; JPEG XL (lossless) 42â52 MB; PNG 106â118 MB&lt;/p&gt;
    &lt;head rend="h3"&gt;HARDWARE&lt;/head&gt;
    &lt;p&gt;DIMENSIONS&lt;/p&gt;
    &lt;p&gt;250 Ã 150 Ã 63 mm&lt;/p&gt;
    &lt;p&gt;WEIGHT&lt;/p&gt;
    &lt;p&gt;1400 grams&lt;/p&gt;
    &lt;p&gt;INTERFACE&lt;/p&gt;
    &lt;p&gt;USB-C (USB 3.1)&lt;/p&gt;
    &lt;p&gt;POWER SUPPLY&lt;/p&gt;
    &lt;p&gt;18 V DC, 2 A (included)&lt;/p&gt;
    &lt;head rend="h3"&gt;HARDWARE&lt;/head&gt;
    &lt;p&gt;DIMENSIONS&lt;/p&gt;
    &lt;p&gt;250 Ã 150 Ã 63 mm&lt;/p&gt;
    &lt;p&gt;WEIGHT&lt;/p&gt;
    &lt;p&gt;1400 grams&lt;/p&gt;
    &lt;p&gt;INTERFACE&lt;/p&gt;
    &lt;p&gt;USB-C (USB 3.1)&lt;/p&gt;
    &lt;p&gt;POWER SUPPLY&lt;/p&gt;
    &lt;p&gt;18 V DC, 2 A (included)&lt;/p&gt;
    &lt;head rend="h2"&gt;Price at Launch&lt;/head&gt;
    &lt;head rend="h2"&gt;999â¬&lt;/head&gt;
    &lt;p&gt;Includes scanner + software&lt;/p&gt;
    &lt;head rend="h2"&gt;4064 dpi resolution&lt;/head&gt;
    &lt;head rend="h2"&gt;5 min per roll&lt;/head&gt;
    &lt;head rend="h2"&gt;48-bit colour depth&lt;/head&gt;
    &lt;head rend="h2"&gt;120 dB Dynamic Range&lt;/head&gt;
    &lt;head rend="h2"&gt;LED Matrix&lt;/head&gt;
    &lt;head rend="h2"&gt;RGB LED backlight&lt;/head&gt;
    &lt;head rend="h2"&gt;USB-C 3.2&lt;/head&gt;
    &lt;head rend="h2"&gt;Custom software&lt;/head&gt;
    &lt;head rend="h2"&gt;Frequently &lt;lb/&gt;Asked &lt;lb/&gt;Questions.&lt;/head&gt;
    &lt;p&gt;Will Knokke support 120 film, panoramic formats, or border scanning?&lt;/p&gt;
    &lt;p&gt;Knokke fully supports any frame width on 35 mm (135) film, thanks to automatic edge detection. It can also perform partial border scans to preserve maximum resolution and frame content. (120 film support is not part of the first release but is under consideration for the future.)&lt;/p&gt;
    &lt;p&gt;What kind of light source and sensor does Knokke use?&lt;/p&gt;
    &lt;p&gt;Knokke uses an RGB LED backlight, precisely wavelength-matched to a modern backside-illuminated CMOS sensor. The sensor features 2 Î¼m pixels and delivers a 78 dB linear dynamic range in 12-bit mode, expandable to 120 dB (â 14 stops) with the 3-step HDR log mode at native 16-bit output. This combination ensures accurate colour reproduction, tonal smoothness, and detail retention in both highlights and shadows.&lt;/p&gt;
    &lt;p&gt;What makes Knokke different from other scanners like the Fuji Frontier, Nikon Coolscan, or camera-stand setups?&lt;/p&gt;
    &lt;p&gt;Knokke combines speed, scanning quality, and ease of use in a compact form factor. It scans a full 35 mm roll in under 5 minutes, offers per-frame customisation, and requires no legacy computer hardware or drivers.&lt;/p&gt;
    &lt;p&gt;Will example scans be shared before launch?&lt;/p&gt;
    &lt;p&gt;Yes. Weâre collaborating with several film labs in Berlin to benchmark Knokke against Fuji Frontier and Noritsu scanners. Sample results will be published before the Kickstarter campaign, so you can make a fully informed decision.&lt;/p&gt;
    &lt;p&gt;Is the software open source?&lt;/p&gt;
    &lt;p&gt;Yes. Our control application, Korova, will be fully open source and maintained long term. Itâs a native, lightweight application for Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;How much is Knokke going to cost?&lt;/p&gt;
    &lt;p&gt;Knokke will cost 999â¬ at launch. We are still working on bringing the price down further threw optimising design and sourcing. It's final retail price is set at 1599â¬.&lt;/p&gt;
    &lt;p&gt;Is Knokke open, repairable, and long-term supported?&lt;/p&gt;
    &lt;p&gt;Absolutely. Weâre committed to building a scanner that lasts decades. All schematics and repair manuals will be publicly available, replacement parts can be purchased directly, and the software will remain supported for as long as possible.&lt;/p&gt;
    &lt;p&gt;When will Knokke be available?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter.com in Q1 2026. Follow us on Instagram and subscribe to our newsletter to be among the first notified about updates.&lt;/p&gt;
    &lt;p&gt;Can I become a beta tester?&lt;/p&gt;
    &lt;p&gt;Thank you for your interest! Weâve received an incredible number of requests to join our beta testing program. Weâll be running two separate testing rounds - one in collaboration with selected creators and film labs, and another open to members of our community.&lt;/p&gt;
    &lt;p&gt;Will Knokke support 120 film, panoramic formats, or border scanning?&lt;/p&gt;
    &lt;p&gt;Knokke fully supports any frame width on 35 mm (135) film, thanks to automatic edge detection. It can also perform partial border scans to preserve maximum resolution and frame content. (120 film support is not part of the first release but is under consideration for the future.)&lt;/p&gt;
    &lt;p&gt;What kind of light source and sensor does Knokke use?&lt;/p&gt;
    &lt;p&gt;Knokke uses an RGB LED backlight, precisely wavelength-matched to a modern backside-illuminated CMOS sensor. The sensor features 2 Î¼m pixels and delivers a 78 dB linear dynamic range in 12-bit mode, expandable to 120 dB (â 14 stops) with the 3-step HDR log mode at native 16-bit output. This combination ensures accurate colour reproduction, tonal smoothness, and detail retention in both highlights and shadows.&lt;/p&gt;
    &lt;p&gt;What makes Knokke different from other scanners like the Fuji Frontier, Nikon Coolscan, or camera-stand setups?&lt;/p&gt;
    &lt;p&gt;Knokke combines speed, scanning quality, and ease of use in a compact form factor. It scans a full 35 mm roll in under 5 minutes, offers per-frame customisation, and requires no legacy computer hardware or drivers.&lt;/p&gt;
    &lt;p&gt;Will example scans be shared before launch?&lt;/p&gt;
    &lt;p&gt;Yes. Weâre collaborating with several film labs in Berlin to benchmark Knokke against Fuji Frontier and Noritsu scanners. Sample results will be published before the Kickstarter campaign, so you can make a fully informed decision.&lt;/p&gt;
    &lt;p&gt;Is the software open source?&lt;/p&gt;
    &lt;p&gt;Yes. Our control application, Korova, will be fully open source and maintained long term. Itâs a native, lightweight application for Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;How much is Knokke going to cost?&lt;/p&gt;
    &lt;p&gt;Knokke will cost 999â¬ at launch. We are still working on bringing the price down further threw optimising design and sourcing. It's final retail price is set at 1599â¬.&lt;/p&gt;
    &lt;p&gt;Is Knokke open, repairable, and long-term supported?&lt;/p&gt;
    &lt;p&gt;Absolutely. Weâre committed to building a scanner that lasts decades. All schematics and repair manuals will be publicly available, replacement parts can be purchased directly, and the software will remain supported for as long as possible.&lt;/p&gt;
    &lt;p&gt;When will Knokke be available?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter.com in Q1 2026. Follow us on Instagram and subscribe to our newsletter to be among the first notified about updates.&lt;/p&gt;
    &lt;p&gt;Can I become a beta tester?&lt;/p&gt;
    &lt;p&gt;Thank you for your interest! Weâve received an incredible number of requests to join our beta testing program. Weâll be running two separate testing rounds - one in collaboration with selected creators and film labs, and another open to members of our community.&lt;/p&gt;
    &lt;head rend="h2"&gt;999â¬&lt;/head&gt;
    &lt;p&gt;Includes scanner + software&lt;/p&gt;
    &lt;head rend="h2"&gt;4064 dpi resolution&lt;/head&gt;
    &lt;head rend="h2"&gt;5 min per roll&lt;/head&gt;
    &lt;head rend="h2"&gt;48-bit colour depth&lt;/head&gt;
    &lt;head rend="h2"&gt;120 dB Dynamic Range&lt;/head&gt;
    &lt;head rend="h2"&gt;LED Matrix&lt;/head&gt;
    &lt;head rend="h2"&gt;RGB LED backlight&lt;/head&gt;
    &lt;head rend="h2"&gt;USB-C 3.2&lt;/head&gt;
    &lt;head rend="h2"&gt;Custom software&lt;/head&gt;
    &lt;head rend="h2"&gt;Frequently Asked Questions.&lt;/head&gt;
    &lt;p&gt;Will Knokke support 120 film, panoramic formats, or border scanning?&lt;/p&gt;
    &lt;p&gt;Knokke fully supports any frame width on 35 mm (135) film, thanks to automatic edge detection. It can also perform partial border scans to preserve maximum resolution and frame content. (120 film support is not part of the first release but is under consideration for the future.)&lt;/p&gt;
    &lt;p&gt;What kind of light source and sensor does Knokke use?&lt;/p&gt;
    &lt;p&gt;Knokke uses an RGB LED backlight, precisely wavelength-matched to a modern backside-illuminated CMOS sensor. The sensor features 2 Î¼m pixels and delivers a 78 dB linear dynamic range in 12-bit mode, expandable to 120 dB (â 14 stops) with the 3-step HDR log mode at native 16-bit output. This combination ensures accurate colour reproduction, tonal smoothness, and detail retention in both highlights and shadows.&lt;/p&gt;
    &lt;p&gt;What makes Knokke different from other scanners like the Fuji Frontier, Nikon Coolscan, or camera-stand setups?&lt;/p&gt;
    &lt;p&gt;Knokke combines speed, scanning quality, and ease of use in a compact form factor. It scans a full 35 mm roll in under 5 minutes, offers per-frame customisation, and requires no legacy computer hardware or drivers.&lt;/p&gt;
    &lt;p&gt;Will example scans be shared before launch?&lt;/p&gt;
    &lt;p&gt;Yes. Weâre collaborating with several film labs in Berlin to benchmark Knokke against Fuji Frontier and Noritsu scanners. Sample results will be published before the Kickstarter campaign, so you can make a fully informed decision.&lt;/p&gt;
    &lt;p&gt;How much is Knokke going to cost?&lt;/p&gt;
    &lt;p&gt;Knokke will cost 999â¬ at launch. We are still working on bringing the price down further threw optimising design and sourcing. It's final retail price is set at 1599â¬.&lt;/p&gt;
    &lt;p&gt;Is the software open source?&lt;/p&gt;
    &lt;p&gt;Yes. Our control application, Korova, will be fully open source and maintained long term. Itâs a native, lightweight application for Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;Is Knokke open, repairable, and long-term supported?&lt;/p&gt;
    &lt;p&gt;Absolutely. Weâre committed to building a scanner that lasts decades. All schematics and repair manuals will be publicly available, replacement parts can be purchased directly, and the software will remain supported for as long as possible.&lt;/p&gt;
    &lt;p&gt;When will Knokke be available?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter.com in Q1 2026. Follow us on Instagram and subscribe to our newsletter to be among the first notified about updates.&lt;/p&gt;
    &lt;p&gt;Can I become a beta tester?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter in Q1 2026. Follow us on Instagram or subscribe to our newsletter to be among the first notified when pre-orders open.&lt;/p&gt;
    &lt;p&gt;Will Knokke support 120 film, panoramic formats, or border scanning?&lt;/p&gt;
    &lt;p&gt;Knokke fully supports any frame width on 35 mm (135) film, thanks to automatic edge detection. It can also perform partial border scans to preserve maximum resolution and frame content. (120 film support is not part of the first release but is under consideration for the future.)&lt;/p&gt;
    &lt;p&gt;What kind of light source and sensor does Knokke use?&lt;/p&gt;
    &lt;p&gt;Knokke uses an RGB LED backlight, precisely wavelength-matched to a modern backside-illuminated CMOS sensor. The sensor features 2 Î¼m pixels and delivers a 78 dB linear dynamic range in 12-bit mode, expandable to 120 dB (â 14 stops) with the 3-step HDR log mode at native 16-bit output. This combination ensures accurate colour reproduction, tonal smoothness, and detail retention in both highlights and shadows.&lt;/p&gt;
    &lt;p&gt;What makes Knokke different from other scanners like the Fuji Frontier, Nikon Coolscan, or camera-stand setups?&lt;/p&gt;
    &lt;p&gt;Knokke combines speed, scanning quality, and ease of use in a compact form factor. It scans a full 35 mm roll in under 5 minutes, offers per-frame customisation, and requires no legacy computer hardware or drivers.&lt;/p&gt;
    &lt;p&gt;Will example scans be shared before launch?&lt;/p&gt;
    &lt;p&gt;Yes. Weâre collaborating with several film labs in Berlin to benchmark Knokke against Fuji Frontier and Noritsu scanners. Sample results will be published before the Kickstarter campaign, so you can make a fully informed decision.&lt;/p&gt;
    &lt;p&gt;How much is Knokke going to cost?&lt;/p&gt;
    &lt;p&gt;Knokke will cost 999â¬ at launch. We are still working on bringing the price down further threw optimising design and sourcing. It's final retail price is set at 1599â¬.&lt;/p&gt;
    &lt;p&gt;Is the software open source?&lt;/p&gt;
    &lt;p&gt;Yes. Our control application, Korova, will be fully open source and maintained long term. Itâs a native, lightweight application for Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;Is Knokke open, repairable, and long-term supported?&lt;/p&gt;
    &lt;p&gt;Absolutely. Weâre committed to building a scanner that lasts decades. All schematics and repair manuals will be publicly available, replacement parts can be purchased directly, and the software will remain supported for as long as possible.&lt;/p&gt;
    &lt;p&gt;When will Knokke be available?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter.com in Q1 2026. Follow us on Instagram and subscribe to our newsletter to be among the first notified about updates.&lt;/p&gt;
    &lt;p&gt;Can I become a beta tester?&lt;/p&gt;
    &lt;p&gt;Knokke will launch on Kickstarter in Q1 2026. Follow us on Instagram or subscribe to our newsletter to be among the first notified when pre-orders open.&lt;/p&gt;
    &lt;head rend="h4"&gt;Engineered for individual and lab use&lt;/head&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;A Closer Look at Knokke&lt;/head&gt;
    &lt;head rend="h2"&gt;Specifications.&lt;/head&gt;
    &lt;head rend="h2"&gt;Specifications.&lt;/head&gt;
    &lt;head rend="h3"&gt;IMAGING SYSTEM&lt;/head&gt;
    &lt;p&gt;SENSOR&lt;/p&gt;
    &lt;p&gt;Backside illuminated CMOS Sensor&lt;/p&gt;
    &lt;p&gt;DYNAMIC RANGE&lt;/p&gt;
    &lt;p&gt;linear dynamic range of 78 dB, expandable to 120 dB with native 16-bit HDR log profile, up to 14 stops of range&lt;/p&gt;
    &lt;p&gt;RESOLUTION&lt;/p&gt;
    &lt;p&gt;Max. 4064 dpi (~22 MP), 2032 dpi (~5,5MP)&lt;/p&gt;
    &lt;p&gt;LENS&lt;/p&gt;
    &lt;p&gt;Custom 4 element lens with high MTF (modulation transfer function)&lt;/p&gt;
    &lt;p&gt;LIGHT SOURCE&lt;/p&gt;
    &lt;p&gt;RGB LED backlight&lt;/p&gt;
    &lt;head rend="h3"&gt;IMAGING SYSTEM&lt;/head&gt;
    &lt;p&gt;SENSOR&lt;/p&gt;
    &lt;p&gt;Backside illuminated CMOS Sensor&lt;/p&gt;
    &lt;p&gt;DYNAMIC RANGE&lt;/p&gt;
    &lt;p&gt;linear dynamic range of 78 dB, expandable to 120 dB with native 16-bit HDR log profile, up to 14 stops of range&lt;/p&gt;
    &lt;p&gt;RESOLUTION&lt;/p&gt;
    &lt;p&gt;Max. 4064 dpi (~22 MP), 2032 dpi (~5,5MP)&lt;/p&gt;
    &lt;p&gt;LENS&lt;/p&gt;
    &lt;p&gt;Custom 4 element lens with high MTF (modulation transfer function)&lt;/p&gt;
    &lt;p&gt;LIGHT SOURCE&lt;/p&gt;
    &lt;p&gt;RGB LED backlight&lt;/p&gt;
    &lt;head rend="h3"&gt;PERFORMANCE &amp;amp; WORKFLOW&lt;/head&gt;
    &lt;p&gt;Scan Speed&lt;/p&gt;
    &lt;p&gt;per roll under 5 minutes (4064 dpi), under 2 minutes (2032 dpi)&lt;/p&gt;
    &lt;p&gt;FILM TRANSPORT&lt;/p&gt;
    &lt;p&gt;automated, min. strip length 3 images&lt;/p&gt;
    &lt;p&gt;FRAME CONTROL&lt;/p&gt;
    &lt;p&gt;per-frame scan settings, skip directly to any frame&lt;/p&gt;
    &lt;p&gt;DXN DECODER&lt;/p&gt;
    &lt;p&gt;reads 35 mm DX codes, embeds film type, ISO, roll info into metadata&lt;/p&gt;
    &lt;p&gt;SOFTWARE&lt;/p&gt;
    &lt;p&gt;Korova (native for Windows, macOS, Linux)&lt;/p&gt;
    &lt;p&gt;FILE FORMATS&lt;/p&gt;
    &lt;p&gt;RAW, TIFF, DNG linear, JPEG, PNG, BMP, HDR&lt;/p&gt;
    &lt;p&gt;FILE SIZES&lt;/p&gt;
    &lt;p&gt;RAW/TIFF/DNG linear ~127 MB; JPEG XL (lossless) 42â52 MB; PNG 106â118 MB&lt;/p&gt;
    &lt;head rend="h3"&gt;PERFORMANCE &amp;amp; WORKFLOW&lt;/head&gt;
    &lt;p&gt;Scan Speed&lt;/p&gt;
    &lt;p&gt;per roll under 5 minutes (4064 dpi), under 2 minutes (2032 dpi)&lt;/p&gt;
    &lt;p&gt;FILM TRANSPORT&lt;/p&gt;
    &lt;p&gt;automated, min. strip length 3 images&lt;/p&gt;
    &lt;p&gt;FRAME CONTROL&lt;/p&gt;
    &lt;p&gt;per-frame scan settings, skip directly to any frame&lt;/p&gt;
    &lt;p&gt;DXN DECODER&lt;/p&gt;
    &lt;p&gt;reads 35 mm DX codes, embeds film type, ISO, roll info into metadata&lt;/p&gt;
    &lt;p&gt;SOFTWARE&lt;/p&gt;
    &lt;p&gt;Korova (native for Windows, macOS, Linux)&lt;/p&gt;
    &lt;p&gt;FILE FORMATS&lt;/p&gt;
    &lt;p&gt;RAW, TIFF, DNG linear, JPEG, PNG, BMP, HDR&lt;/p&gt;
    &lt;p&gt;FILE SIZES&lt;/p&gt;
    &lt;p&gt;RAW/TIFF/DNG linear ~127 MB; JPEG XL (lossless) 42â52 MB; PNG 106â118 MB&lt;/p&gt;
    &lt;head rend="h3"&gt;HARDWARE&lt;/head&gt;
    &lt;p&gt;DIMENSIONS&lt;/p&gt;
    &lt;p&gt;250 Ã 150 Ã 63 mm&lt;/p&gt;
    &lt;p&gt;WEIGHT&lt;/p&gt;
    &lt;p&gt;1400 grams&lt;/p&gt;
    &lt;p&gt;INTERFACE&lt;/p&gt;
    &lt;p&gt;USB-C (USB 3.1)&lt;/p&gt;
    &lt;p&gt;POWER SUPPLY&lt;/p&gt;
    &lt;p&gt;18 V DC, 2 A (included)&lt;/p&gt;
    &lt;head rend="h3"&gt;HARDWARE&lt;/head&gt;
    &lt;p&gt;DIMENSIONS&lt;/p&gt;
    &lt;p&gt;250 Ã 150 Ã 63 mm&lt;/p&gt;
    &lt;p&gt;WEIGHT&lt;/p&gt;
    &lt;p&gt;1400 grams&lt;/p&gt;
    &lt;p&gt;INTERFACE&lt;/p&gt;
    &lt;p&gt;USB-C (USB 3.1)&lt;/p&gt;
    &lt;p&gt;POWER SUPPLY&lt;/p&gt;
    &lt;p&gt;18 V DC, 2 A (included)&lt;/p&gt;
    &lt;head rend="h3"&gt;Engineered for individual and lab use&lt;/head&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;01&lt;/head&gt;
    &lt;head rend="h6"&gt;Quality&lt;/head&gt;
    &lt;p&gt;Knokkeâs premium build and precision engineering ensure lasting, reliable performance.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;02&lt;/head&gt;
    &lt;head rend="h6"&gt;Speed&lt;/head&gt;
    &lt;p&gt;Knokkeâs high scan speed and streamlined workflow keep you moving.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;03&lt;/head&gt;
    &lt;head rend="h6"&gt;Full Control&lt;/head&gt;
    &lt;p&gt;Knokke lets you fine-tune every detail with flexible settings and precise color control.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
    &lt;head rend="h6"&gt;04&lt;/head&gt;
    &lt;head rend="h6"&gt;Future Proof&lt;/head&gt;
    &lt;p&gt;Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.soke.engineering/"/><published>2025-11-11T19:48:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45891968</id><title>Adk-go: code-first Go toolkit for building, evaluating, and deploying AI agents</title><updated>2025-11-12T00:50:21.185872+00:00</updated><content>&lt;doc fingerprint="3d738c52c9d3a6e3"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;An open-source, code-first Go toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.&lt;/head&gt;
    &lt;head rend="h3"&gt;Important Links: Docs &amp;amp; Samples &amp;amp; Python ADK &amp;amp; Java ADK &amp;amp; ADK Web.&lt;/head&gt;
    &lt;p&gt;Agent Development Kit (ADK) is a flexible and modular framework that applies software development principles to AI agent creation. It is designed to simplify building, deploying, and orchestrating agent workflows, from simple tasks to complex systems. While optimized for Gemini, ADK is model-agnostic, deployment-agnostic, and compatible with other frameworks.&lt;/p&gt;
    &lt;p&gt;This Go version of ADK is ideal for developers building cloud-native agent applications, leveraging Go's strengths in concurrency and performance.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Idiomatic Go: Designed to feel natural and leverage the power of Go.&lt;/item&gt;
      &lt;item&gt;Rich Tool Ecosystem: Utilize pre-built tools, custom functions, or integrate existing tools to give agents diverse capabilities.&lt;/item&gt;
      &lt;item&gt;Code-First Development: Define agent logic, tools, and orchestration directly in Go for ultimate flexibility, testability, and versioning.&lt;/item&gt;
      &lt;item&gt;Modular Multi-Agent Systems: Design scalable applications by composing multiple specialized agents.&lt;/item&gt;
      &lt;item&gt;Deploy Anywhere: Easily containerize and deploy agents, with strong support for cloud-native environments like Google Cloud Run.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To add ADK Go to your project, run:&lt;/p&gt;
    &lt;code&gt;go get google.golang.org/adk&lt;/code&gt;
    &lt;p&gt;This project is licensed under the Apache 2.0 License - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;The exception is internal/httprr - see its LICENSE file.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/google/adk-go"/><published>2025-11-11T19:52:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45892174</id><title>Xortran - A PDP-11 Neural Network With Backpropagation in Fortran IV</title><updated>2025-11-12T00:50:20.640542+00:00</updated><content>&lt;doc fingerprint="7293ba097f3c7842"&gt;
  &lt;main&gt;
    &lt;p&gt;XORTRAN is a multilayer perceptron (MLP) written in FORTRAN IV, compiled and executed under RT-11 on a PDP-11/34A (via SIMH simulator).&lt;/p&gt;
    &lt;p&gt;It learns the classic non-linear XOR problem using:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One hidden layer (4 neurons, leaky ReLU activation)&lt;/item&gt;
      &lt;item&gt;Backpropagation with mean squared error loss&lt;/item&gt;
      &lt;item&gt;He-like initialization (manual Gaussian via Box-Muller lite)&lt;/item&gt;
      &lt;item&gt;Learning rate annealing (0.5 → 0.1 → 0.01)&lt;/item&gt;
      &lt;item&gt;Tanh output&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The code compiles with the DEC FORTRAN IV compiler (1974). Execution requires a system with at least 32 kilobytes of memory and an FP11 floating-point processor. The PDP-11/34A was chosen as it was the smallest and most affordable PDP-11 equipped with an FP11 floating-point processor in the 1970s.&lt;/p&gt;
    &lt;p&gt;The training of the 17 parameters should take less than a couple minutes on the real hardware. In SIMH, setting the throttle to 500K (&lt;code&gt;set throttle 500K&lt;/code&gt;) will
provide a more realistic execution speed.&lt;/p&gt;
    &lt;p&gt;The output shows the mean squared loss every 100 epochs, followed by the final predictions from the forward pass.&lt;/p&gt;
    &lt;p&gt;The network converges towards the expected XOR outputs after a few hundred epochs, gradually reducing the error until it accurately approximates the desired results.&lt;/p&gt;
    &lt;code&gt;.RUN XORTRN
   1  0.329960233835D+00
 100  0.195189856059D+00
 200  0.816064184115D-01
 300  0.654882376056D-02
 400  0.109833284544D-02
 500  0.928130032748D-03

0 0 GOT:0.008353 EXPECTED:0.
0 1 GOT:0.979327 EXPECTED:1.
1 0 GOT:0.947050 EXPECTED:1.
1 1 GOT:0.020147 EXPECTED:0.
STOP --
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;In SIMH, attach the RL1 drive (&lt;/p&gt;&lt;code&gt;ATT RL1 xortran.rl1&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In RT-11 (I use the single job RT-11-SJ V5), assuming&lt;/p&gt;&lt;code&gt;DL1:&lt;/code&gt;is assigned to&lt;code&gt;DK:&lt;/code&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;.FORTRAN/LIST:XORTRN.LST XORTRN.FOR
.LINK XORTRN.OBJ,FORLIB
.RUN XORTRN
&lt;/code&gt;
    &lt;p&gt;Or if you just want to run the binary:&lt;/p&gt;
    &lt;code&gt;.RUN DL1:XORTRN
&lt;/code&gt;
    &lt;p&gt;This project demonstrates that a minimal FORTRAN IV environment from the 1970s was sufficient to implement a basic neural network with backpropagation.&lt;lb/&gt; It’s both a retro-computing curiosity and a small historical experiment bridging early scientific computing and modern machine learning.&lt;/p&gt;
    &lt;p&gt;© 2025 Damien Boureille&lt;/p&gt;
    &lt;p&gt;This code is released under the MIT License.&lt;lb/&gt; You are free to use, copy, modify, and redistribute it, provided that you credit the original author.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/dbrll/Xortran"/><published>2025-11-11T20:10:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45892191</id><title>The terminal of the future</title><updated>2025-11-12T00:50:20.119935+00:00</updated><content>&lt;doc fingerprint="37eac95c22846b89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;the terminal of the future&lt;/head&gt;
    &lt;p&gt;This post is part 6 of a multi-part series called “the computer of the next 200 years”.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Terminal internals are a mess. A lot of it is just the way it is because someone made a decision in the 80s and now it’s impossible to change. —Julia Evans&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;This is what you have to do to redesign infrastructure. Rich [Hickey] didn't just pile some crap on top of Lisp [when building Clojure]. He took the entire Lisp and moved the whole design at once. —Gary Bernhardt&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;a mental model of a terminal&lt;/head&gt;
    &lt;p&gt;At a very very high level, a terminal has four parts:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The "terminal emulator", which is a program that renders a grid-like structure to your graphical display.&lt;/item&gt;
      &lt;item&gt;The "pseudo-terminal" (PTY), which is a connection between the terminal emulator and a "process group" which receives input. This is not a program. This is a piece of state in the kernel.&lt;/item&gt;
      &lt;item&gt;The "shell", which is a program that leads the "process group", reads and parses input, spawns processes, and generally acts as an event loop. Most environments use bash as the default shell.&lt;/item&gt;
      &lt;item&gt;The programs spawned by your shell, which interact with all of the above in order to receive input and send output.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I lied a little bit above. "input" is not just text. It also includes signals that can be sent to the running process. Converting keystrokes to signals is the job of the PTY.&lt;/p&gt;
    &lt;p&gt;Similar, "output" is not just text. It's a stream of ANSI Escape Sequences that can be used by the terminal emulator to display rich formatting.&lt;/p&gt;
    &lt;head rend="h2"&gt;what does a better terminal look like?&lt;/head&gt;
    &lt;p&gt;I do some weird things with terminals. However, the amount of hacks I can get up to are pretty limited, because terminals are pretty limited. I won't go into all the ways they're limited, because it's been rehashed many times before. What I want to do instead is imagine what a better terminal can look like.&lt;/p&gt;
    &lt;head rend="h3"&gt;a first try: Jupyter&lt;/head&gt;
    &lt;p&gt;The closest thing to a terminal analog that most people are familiar with is Jupyter Notebook. This offers a lot of cool features that are not possible in a "traditional" VT100 emulator:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;high fidelity image rendering&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;a "rerun from start" button (or rerun the current command; or rerun only a single past command) that replaces past output instead of appending to it&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;"views" of source code and output that can be rewritten in place (e.g. markdown can be viewed either as source or as rendered HTML)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;a built-in editor with syntax highlighting, tabs, panes, mouse support, etc.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;some problems&lt;/head&gt;
    &lt;p&gt;Jupyter works by having a "kernel" (in this case, a python interpreter) and a "renderer" (in this case, a web application displayed by the browser). You could imagine using a Jupyter Notebook with a shell as the kernel, so that you get all the nice features of Jupyter when running shell commands. However, that quickly runs into some issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your shell gets the commands all at once, not character-by-character, so tab-complete, syntax highlighting, and autosuggestions don't work.&lt;/item&gt;
      &lt;item&gt;What do you do about long-lived processes? By default, Jupyter runs a cell until completion; you can cancel it, but you can't suspend, resume, interact with, nor view a process while it's running. Don't even think about running &lt;code&gt;vi&lt;/code&gt;or&lt;code&gt;top&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The "rerun cell" buttons do horrible things to the state of your computer (normal Jupyter kernels have this problem too, but "rerun all" works better when the commands don't usually include &lt;code&gt;rm -rf&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Undo/redo do not work. (They don't work in a normal terminal either, but people attempt to use them more when it looks like they should be able to.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It turns out all these problems are solveable.&lt;/p&gt;
    &lt;head rend="h2"&gt;how does that work?&lt;/head&gt;
    &lt;head rend="h3"&gt;shell integration&lt;/head&gt;
    &lt;p&gt;There exists today a terminal called Warp. Warp has built native integration between the terminal and the shell, where the terminal understands where each command starts and stops, what it outputs, and what is your own input. As a result, it can render things very prettily:&lt;/p&gt;
    &lt;p&gt;It does this using (mostly) standard features built-in to the terminal and shell (a custom DCS): you can read their explanation here. It's possible to do this less invasively using OSC 133 escape codes; I'm not sure why Warp didn't do this, but that's ok.&lt;/p&gt;
    &lt;p&gt;iTerm2 does a similar thing, and this allows it to enable really quite a lot of features: navigating between commands with a single hotkey; notifying you when a command finishes running, showing the current command as an "overlay" if the output goes off the screen.&lt;/p&gt;
    &lt;head rend="h3"&gt;long-lived processes&lt;/head&gt;
    &lt;p&gt;This is really three different things. The first is interacting with a long-lived process. The second is suspending the process without killing it. The third is disconnecting from the process, in such a way that the process state is not disturbed and is still available if you want to reconnect.&lt;/p&gt;
    &lt;head rend="h4"&gt;interacting&lt;/head&gt;
    &lt;p&gt;To interact with a process, you need bidirectional communication, i.e. you need a "cell output" that is also an input. An example would be any TUI, like &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;gdb&lt;/code&gt;, or &lt;code&gt;vim&lt;/code&gt; 1.  Fortunately, Jupyter is really good at this!  The whole design is around having interactive outputs that you can change and update.&lt;/p&gt;
    &lt;p&gt;Additionally, I would expect my terminal to always have a "free input cell", as Matklad describes in A Better Shell, where the interactive process runs in the top half of the window and an input cell is available in the bottom half. Jupyter can do this today, but "add a cell" is manual, not automatic.&lt;/p&gt;
    &lt;head rend="h4"&gt;suspending&lt;/head&gt;
    &lt;p&gt;"Suspending" a process is usually called "job control". There's not too much to talk about here, except that I would expect a "modern" terminal to show me all suspended and background processes as a de-emphasized persistent visual, kinda like how Intellij will show you "indexing ..." in the bottom taskbar.&lt;/p&gt;
    &lt;head rend="h4"&gt;disconnecting&lt;/head&gt;
    &lt;p&gt;There are roughly three existing approaches for disconnecting and reconnecting to a terminal session (Well, four if you count reptyr).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Tmux / Zellij / Screen&lt;/p&gt;
        &lt;p&gt;These tools inject a whole extra terminal emulator between your terminal emulator and the program. They work by having a "server" which actually owns the PTY and renders the output, and a "client" that displays the output to your "real" terminal emulator. This model lets you detach clients, reattach them later, or even attach multiple clients at once. You can think of this as a "batteries-included" approach. It also has the benefit that you can program both the client and the server (although many modern terminals, like Kitty and Wezterm are programmable now); that you can organize your tabs and windows in the terminal (although many modern desktop environments have tiling and thorough keyboard shortcuts); and that you get street cred for looking like Hackerman.&lt;/p&gt;
        &lt;p&gt;The downside is that, well, now you have an extra terminal emulator running in your terminal, with all the bugs that implies.&lt;/p&gt;
        &lt;p&gt;iTerm actually avoids this by bypassing the tmux client altogether and acting as its own client that talks directly to the server. In this mode, "tmux tabs" are actually iTerm tabs, "tmux panes" are iTerm panes, and so on. This is a good model, and I would adopt it when writing a future terminal for integration with existing tmux setups.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mosh is a really interesting place in the design space. It is not a terminal emulator replacement; instead it is an ssh replacement. Its big draw is that it supports reconnecting to your terminal session after a network interruption. It does that by running a state machine on the server and replaying an incremental diff of the viewport to the client. This is a similar model to tmux, except that it doesn't support the "multiplexing" part (it expects your terminal emulator to handle that), nor scrollback (ditto). Because it has its own renderer, it has a similar class of bugs to tmux. One feature it does have, unlike tmux, is that the "client" is really running on your side of the network, so local line editing is instant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;alden/shpool/dtach/abduco/diss&lt;/p&gt;
        &lt;p&gt;These all occupy a similar place in the design space: they only handle session detach/resume with a client/server, not networking or scrollback, and do not include their own terminal emulator. Compared to tmux and mosh, they are highly decoupled.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;rerun and undo/redo&lt;/head&gt;
    &lt;p&gt;I'm going to treat these together because the solution is the same: dataflow tracking.&lt;/p&gt;
    &lt;p&gt;Take as an example pluto.jl, which does this today by hooking into the Julia compiler.&lt;/p&gt;
    &lt;p&gt;Note that this updates cells live in response to previous cells that they depend on. Not pictured is that it doesn't update cells if their dependencies haven't changed. You can think of this as a spreadsheet-like Jupyter, where code is only rerun when necessary.&lt;/p&gt;
    &lt;p&gt;You may say this is hard to generalize. The trick here is orthogonal persistence. If you sandbox the processes, track all IO, and prevent things that are "too weird" unless they're talking to other processes in the sandbox (e.g. unix sockets and POST requests), you have really quite a lot of control over the process! This lets you treat it as a pure function of its inputs, where its inputs are "the whole file system, all environment variables, and all process attributes".&lt;/p&gt;
    &lt;head rend="h3"&gt;derived features&lt;/head&gt;
    &lt;p&gt;Once you have these primitives—Jupyter notebook frontends, undo/redo, automatic rerun, persistence, and shell integration—you can build really quite a lot on top. And you can build it incrementally, piece-by-piece:&lt;/p&gt;
    &lt;head rend="h4"&gt;needs a Jupyter notebook frontend&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Runbooks (actually, you can build these just with Jupyter and a PTY primitive).&lt;/item&gt;
      &lt;item&gt;Terminal customization that uses normal CSS, no weird custom languages or ANSI color codes.&lt;/item&gt;
      &lt;item&gt;Search for commands by output/timestamp. Currently, you can search across output in the current session, or you can search across all command input history, but you don't have any kind of smart filters, and the output doesn't persist across sessions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;needs shell integration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Timestamps and execution duration for each command.&lt;/item&gt;
      &lt;item&gt;Local line-editing, even across a network boundary.&lt;/item&gt;
      &lt;item&gt;IntelliSense for shell commands, without having to hit tab and with rendering that's integrated into the terminal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;needs sandboxed tracing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"All the features from sandboxed tracing": collaborative terminals, querying files modified by a command, "asciinema but you can edit it at runtime", tracing build systems.&lt;/item&gt;
      &lt;item&gt;Extend the smart search above to also search by disk state at the time the command was run.&lt;/item&gt;
      &lt;item&gt;Extending undo/redo to a git-like branching model (something like this is already support by emacs undo-tree), where you have multiple "views" of the process tree.&lt;/item&gt;
      &lt;item&gt;Given the undo-tree model, and since we have sandboxing, we can give an LLM access to your project, and run many of them in parallel at the same time without overwriting each others state, and in such a way that you can see what they're doing, edit it, and save it into a runbook for later use.&lt;/item&gt;
      &lt;item&gt;A terminal in a prod environment that can't affect the state of the machine, only inspect the existing state.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;ok but how do you build this&lt;/head&gt;
    &lt;p&gt;jyn, you may say, you can't build vertical integration in open source. you can't make money off open source projects. the switching costs are too high.&lt;/p&gt;
    &lt;p&gt;All these things are true. To talk about how this is possible, we have to talk about incremental adoption.&lt;/p&gt;
    &lt;p&gt;if I were building this, I would do it in stages, such that at each stage the thing is an improvement over its alternatives. This is how &lt;code&gt;jj&lt;/code&gt; works and it works extremely well: it doesn't require everyone on a team to switch at once because individual people can use &lt;code&gt;jj&lt;/code&gt;, even for single commands, without a large impact on everyone else.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 1: transactional semantics&lt;/head&gt;
    &lt;p&gt;When people think of redesigning the terminal, they always think of redesigning the terminal emulator. This is exactly the wrong place to start. People are attached to their emulators. They configure them, they make them look nice, they use their keybindings. There is a high switching cost to switching emulators because everything affects everything else. It's not so terribly high, because it's still individual and not shared across a team, but still high.&lt;/p&gt;
    &lt;p&gt;What I would do instead is start at the CLI layer. CLI programs are great because they're easy to install and run and have very low switching costs: you can use them one-off without changing your whole workflow.&lt;/p&gt;
    &lt;p&gt;So, I would write a CLI that implements transactional semantics for the terminal. You can imagine an interface something like &lt;code&gt;transaction [start|rollback|commit]&lt;/code&gt;, where everything run after &lt;code&gt;start&lt;/code&gt; is undoable. There is a lot you can do with this alone, I think you could build a whole business off this.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 2: persistent sessions&lt;/head&gt;
    &lt;p&gt;Once I had transactional semantics, I would try to decouple persistence from tmux and mosh.&lt;/p&gt;
    &lt;p&gt;To get PTY persistence, you have to introduce a client/server model, because the kernel really really expects both sides of a PTY to always be connected. Using commands like alden, or a library like it (it's not that complicated), lets you do this simply, without affecting the terminal emulator nor the programs running inside the PTY session.&lt;/p&gt;
    &lt;p&gt;To get scrollback, the server could save input and output indefinitely and replay them when the client reconnects. This gets you "native" scrollback—the terminal emulator you're already using handles it exactly like any other output, because it looks exactly like any other output—while still being replayable and resumable from an arbitrary starting point. This requires some amount of parsing ANSI escape codes2, but it's doable with enough work.&lt;/p&gt;
    &lt;p&gt;To get network resumption like mosh, my custom server could use Eternal TCP (possibly built on top of QUIC for efficiency). Notably, the persistence for the PTY is separate from the persistence for the network connection. Eternal TCP here is strictly an optimization: you could build this on top of a bash script that runs &lt;code&gt;ssh host eternal-pty attach&lt;/code&gt; in a loop, it's just not as nice an experience because of network delay and packet loss. Again, composable parts allow for incremental adoption.&lt;/p&gt;
    &lt;p&gt;At this point, you're already able to connect multiple clients to a single terminal session, like tmux, but window management is still done by your terminal emulator, not by the client/server. If you wanted to have window management integrated, the terminal emulator could speak the tmux -CC protocol, like iTerm.&lt;/p&gt;
    &lt;p&gt;All parts of this stage can be done independently and in parallel from the transactional semantics, but I don't think you can build a business off them, it's not enough of an improvement over the existing tools.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 3: structured RPC&lt;/head&gt;
    &lt;p&gt;This bit depends on the client/server model. Once you have a server interposed between the terminal emulator and the client, you can start doing really funny things like tagging I/O with metadata. This lets all data be timestamped3 and lets you distinguish input from output. xterm.js works something like this. When combined with shell integration, this even lets you distinguish shell prompts from program output, at the data layer.&lt;/p&gt;
    &lt;p&gt;Now you can start doing really funny things, because you have a structured log of your terminal session. You can replay the log as a recording, like asciinema4; you can transform the shell prompt without rerunning all the commands; you can import it into a Jupyter Notebook or Atuin Desktop; you can save the commands and rerun them later as a script. Your terminal is data.&lt;/p&gt;
    &lt;head rend="h3"&gt;stage 4: jupyter-like frontend&lt;/head&gt;
    &lt;p&gt;This is the very first time that we touch the terminal emulator, and it's intentionally the last step because it has the highest switching costs. This makes use of all the nice features we've built to give you a nice UI. You don't need our &lt;code&gt;transaction&lt;/code&gt; CLI anymore unless you want nested transactions, because your whole terminal session starts in a transaction by default. You get all the features I mention above, because we've put all the pieces together.&lt;/p&gt;
    &lt;head rend="h2"&gt;jyn, what the fuck&lt;/head&gt;
    &lt;p&gt;This is bold and ambitious and I think building the whole thing would take about a decade. That's ok. I'm patient.&lt;/p&gt;
    &lt;p&gt;You can help me by spreading the word :) Perhaps this post will inspire someone to start building this themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;bibliography&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gary Bernhardt, “A Whole New World”&lt;/item&gt;
      &lt;item&gt;Alex Kladov, “A Better Shell”&lt;/item&gt;
      &lt;item&gt;jyn, “how i use my terminal”&lt;/item&gt;
      &lt;item&gt;jyn, “Complected and Orthogonal Persistence”&lt;/item&gt;
      &lt;item&gt;jyn, “you are in a box”&lt;/item&gt;
      &lt;item&gt;jyn, “there's two costs to making money off an open source project…”&lt;/item&gt;
      &lt;item&gt;Rebecca Turner, “Vertical Integration is the Only Thing That Matters”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “New zine: The Secret Rules of the Terminal”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “meet the terminal emulator”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “What happens when you press a key in your terminal?”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “What's involved in getting a "modern" terminal setup?”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “Bash scripting quirks &amp;amp; safety tips”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “Some terminal frustrations”&lt;/item&gt;
      &lt;item&gt;Julia Evans, “Reasons to use your shell's job control”&lt;/item&gt;
      &lt;item&gt;“signal(7) - Miscellaneous Information Manual”&lt;/item&gt;
      &lt;item&gt;Christian Petersen, “ANSI Escape Codes”&lt;/item&gt;
      &lt;item&gt;saoirse, “withoutboats/notty: A new kind of terminal”&lt;/item&gt;
      &lt;item&gt;Jupyter Team, “Project Jupyter Documentation”&lt;/item&gt;
      &lt;item&gt;“Warp: The Agentic Development Environment”&lt;/item&gt;
      &lt;item&gt;“Warp: How Warp Works”&lt;/item&gt;
      &lt;item&gt;“Warp: Completions”&lt;/item&gt;
      &lt;item&gt;George Nachman, “iTerm2: Proprietary Escape Codes”&lt;/item&gt;
      &lt;item&gt;George Nachman, “iTerm2: Shell Integration”&lt;/item&gt;
      &lt;item&gt;George Nachman, “iTerm2: tmux Integration”&lt;/item&gt;
      &lt;item&gt;Project Jupyter, “Jupyter Widgets”&lt;/item&gt;
      &lt;item&gt;Nelson Elhage, “nelhage/reptyr: Reparent a running program to a new terminal”&lt;/item&gt;
      &lt;item&gt;Kovid Goyal, “kitty”&lt;/item&gt;
      &lt;item&gt;Kovid Goyal, “kitty - Frequently Asked Questions”&lt;/item&gt;
      &lt;item&gt;Wez Furlong, “Wezterm”&lt;/item&gt;
      &lt;item&gt;Keith Winstein, “Mosh: the mobile shell”&lt;/item&gt;
      &lt;item&gt;Keith Winstein, “Display errors with certain characters&lt;/item&gt;
      &lt;item&gt;Matthew Skala, “alden: detachable terminal sessions without breaking scrollback”&lt;/item&gt;
      &lt;item&gt;Ethan Pailes, “shell-pool/shpool: Think tmux, then aim... lower”&lt;/item&gt;
      &lt;item&gt;Ned T. Crigler, “crigler/dtach: A simple program that emulates the detach feature of screen”&lt;/item&gt;
      &lt;item&gt;Marc André Tanner, “martanne/abduco: abduco provides session management”&lt;/item&gt;
      &lt;item&gt;yazgoo, “yazgoo/diss: dtach-like program / crate in rust”&lt;/item&gt;
      &lt;item&gt;Fons van der Plas, “Pluto.jl — interactive Julia programming environment”&lt;/item&gt;
      &lt;item&gt;Ellie Huxtable, “Atuin Desktop: Runbooks that Run”&lt;/item&gt;
      &lt;item&gt;Toby Cubitt, “undo-tree”&lt;/item&gt;
      &lt;item&gt;“SIGHUP - Wikipedia”&lt;/item&gt;
      &lt;item&gt;Jason Gauci, “How Eternal Terminal Works”&lt;/item&gt;
      &lt;item&gt;Marcin Kulik, “Record and share your terminal sessions, the simple way - asciinema.org”&lt;/item&gt;
      &lt;item&gt;“Alternate Screen | Ratatui”&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jyn.dev/the-terminal-of-the-future"/><published>2025-11-11T20:11:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45892394</id><title>Collaboration sucks</title><updated>2025-11-12T00:50:19.840499+00:00</updated><content>&lt;doc fingerprint="a369965128e686dd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Collaboration sucks&lt;/head&gt;
    &lt;head rend="h3"&gt;Be the driver&lt;/head&gt;
    &lt;p&gt;“If you want to go fast, go alone; if you want to go far, go together”&lt;/p&gt;
    &lt;p&gt;This phrase will slowly kill your company and I’m here to prove it.&lt;/p&gt;
    &lt;p&gt;Imagine you are driving a car. It’s often useful to have someone give you directions, point out gas stations, and recommend stops for snacks. This is a helpful amount of collaboration.&lt;/p&gt;
    &lt;p&gt;An unhelpful amount of collaboration is getting out of your car to ask pedestrians if they like your car, swapping drivers every 10 minutes, or having someone constantly commenting on your driving.&lt;/p&gt;
    &lt;p&gt;In the first scenario, you get the right amount of feedback to get to your destination as fast as possible. In the second, you get more feedback, but it slows you down. You run the risk of not making it to the place you want to go.&lt;/p&gt;
    &lt;p&gt;The second scenario is also the one most startups (or companies, really) end up in because of ✨ collaboration ✨.&lt;/p&gt;
    &lt;head rend="h2"&gt;Being good at feedback means knowing when not to give it&lt;/head&gt;
    &lt;p&gt;As PostHog grows, I’ve seen more and more collaboration that doesn’t add value or adds far too little value for the time lost collaborating. So much so we made “collaboration sucks” the topic of the week during a recent company all hands.&lt;/p&gt;
    &lt;p&gt;“You’re the driver” is a key value for us at PostHog. We aim to hire people who are great at their jobs and get out of their way. No deadlines, minimal coordination, and no managers telling you what to do.&lt;/p&gt;
    &lt;p&gt;In return, we ask for extraordinarily high ownership and the ability to get a lot done by yourself. Marketers ship code, salespeople answer technical questions without backup, and product engineers work across the stack.&lt;/p&gt;
    &lt;p&gt;This means there is almost always someone better at what you are doing than you are. It is tempting to get them, or anybody really, involved and ✨ collaborate ✨, but collaboration forces the driver to slow down and explain stuff (background, context, their thinking).&lt;/p&gt;
    &lt;p&gt;This tendency reveals itself in a few key phrases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;“Curious what X thinks”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“Would love to hear Y’s take on this”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“We should work with Z on this”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This sometimes leads to valuable insights, but always slows the driver down. It erodes their motivation, confidence, and effectiveness, and ultimately leads us to ship less.&lt;/p&gt;
    &lt;head rend="h2"&gt;If collaboration sucks, why do people do it?&lt;/head&gt;
    &lt;p&gt;Everyone is to blame.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;People want to be helpful. For example, when someone posts their work-in-progress in Slack, others feel obliged to give feedback because we have a culture of feedback.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On the flip side, people don’t ask for feedback from specific people because it doesn’t feel inclusive, even though it would help.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;People aren’t specific enough about what feedback they need. This creates more space for collaboration to sneak in. A discussion about building a specific feature can devolve into reevaluating the entire product roadmap if you let it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When someone has a good idea, the response often defaults to “let’s discuss” rather than “ok, do it.” As proof, we have 175 mentions of “let’s discuss” in Slack.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;People just want to talk about stuff because they&lt;/p&gt;&lt;del rend="overstrike"&gt;are too busy&lt;/del&gt;can’t be bothered to act on it. We drift from our ideal of a pull request to an issue/RFC to Slack (we are mostly here) to “let’s discuss”.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It’s not clear who the owner is (or no one wants to own what’s being discussed).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It is annoying, but sometimes a single person can’t ship certain things front to back to a high-enough quality and we can’t just ship and iterate. We can fix broken code, but we can’t resend a newsletter.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to crush collaboration (and go farther, faster)&lt;/head&gt;
    &lt;p&gt;So if collaboration is your enemy, how do you defeat it? Here’s what we say:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Default to shipping. Pull requests &amp;gt; issues &amp;gt; Slack messages.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Every time you see ✨ collaboration ✨ happening, speak up and destroy it. Say “there are too many people involved. X, you are the driver, you decide.” (This is a great way to make friends btw).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Tag who you specifically want input from and what you want from them, not just throw things out there into the void.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prefer to give feedback after something has shipped (but before the next iteration) rather than reviewing it before it ships. Front-loading your feedback can turn it into a quasi-approval process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you are a team lead, or leader of leads, who has been asked for feedback, consider being more you can just do stuff.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When it’s your thing, you are the “informed captain.” Listen to feedback, but know it’s ultimately up to you to decide what to do, not the people giving feedback.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unfortunately for me, not all collaboration can be rooted out, and even I will admit that some collaboration is useful. Ian and Andy edited this newsletter after all.&lt;/p&gt;
    &lt;p&gt;The point is, if you aren’t actively attempting to collaborate less, you are probably collaborating too much by default and hurting your ability to go far, fast.&lt;lb/&gt;Words by Charles Cook, who also hates sparkling water, presumably because the bubbles are too collaborative.&lt;/p&gt;
    &lt;head rend="h2"&gt;👷 Jobs at PostHog&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;AI Product Engineer working on PostHog AI, LLM Analytics or Array teams.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Backend Engineer for Feature Flags and Ingestion teams&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Influencer Wrangler on the Marketing team&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;YC Technical Onboarding Specialist on the Onboarding team (San Fran based)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ClickHouse Operations Engineer on the ClickHouse team&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;📖 More good reads&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Workflows are now in Alpha and I already broke mine – Sara Miteva&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your data model is your destiny – Matt Brown&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Spinning Plates – Dylan Martin&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;1000x: The Power of an Interface for Performance (video) – Joran Dirk Greef&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://newsletter.posthog.com/p/collaboration-sucks"/><published>2025-11-11T20:27:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45892773</id><title>Meticulous (YC S21) is hiring to redefine software dev</title><updated>2025-11-12T00:50:19.618604+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jobs.ashbyhq.com/meticulous/3197ae3d-bb26-4750-9ed7-b830f640515e"/><published>2025-11-11T21:00:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45893004</id><title>X5.1 solar flare, G4 geomagnetic storm watch</title><updated>2025-11-12T00:50:19.287574+00:00</updated><content>&lt;doc fingerprint="77527d2b7f75e40e"&gt;
  &lt;main&gt;
    &lt;p&gt;Tuesday, 11 November 2025 19:07 UTC&lt;/p&gt;
    &lt;p&gt;Here she blows! Sunspot region 4274 produced its strongest solar flare thus far since it appeared on the east limb and the sixth strongest solar flare of the current solar cycle. An impressive long duration and highly eruptive X5.1 (R3-strong) solar flare peaked this morning at 10:04 UTC.&lt;/p&gt;
    &lt;p&gt;It became quickly clear that the eruption would be followed by an impressive coronal mass ejection (CME). The resulting coronal wave following the solar explosion as well as the coronal dimming observed as the CME was propelled into space were of a spectacular magnitude as can be seen in the animation below provided by halocme.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Another eruption from AR12474, associated with an X5.1 flare. It has become a full halo CME. I am truly impressed by how fast and global this coronal wave is. The CME will arrive on November 13, but because of earlier CMEs it will be challenging to isolate the ICME from this. pic.twitter.com/H6eNjzQUGz&lt;/p&gt;— Halo CME (@halocme) November 11, 2025&lt;/quote&gt;
    &lt;p&gt;Taking a look at coronagraph imagery provided by GOES-19 CCOR-1 we see the gorgeous fast halo coronal mass ejection as it propagates away from the Sun. It doesn't take a rocket scientist to come to the conclusion that this plasma cloud of course has an earth-directed component and it is pretty clear that this will be a strong impact when it arrives at our planet. This rightfully so prompted the NOAA SWPC to issue a G4 or greater geomagnetic storm watch for tomorrow as the cloud could impact our planet as early as 16 UTC on 12 November. Not only is the CME fast but it will also travel trough an area with high ambient solar wind speed and low density thanks to two other CMEs released earlier by this region. More about that below.&lt;/p&gt;
    &lt;p&gt;If the solar wind and interplanetary magnetic field values at Earth are favorable this could result in a geomagnetic storm which is strong enough for aurora to become visible from locations as far south as northern France, Germany, Ukraine, Switzerland and Austria. In the US it could become visible as far south as Nevada and Arkansas. No guarantees of course, this is space weather we are talking about but be sure to download the SpaceWeatherLive app to your mobile device, turn on the alerts and keep an eye on the solar wind data from ACE and DSCOVR!&lt;/p&gt;
    &lt;p&gt;We also want to remind you that we still have two coronal mass ejections on their way to Earth. These are not as impressive as this X5.1 CME but these two plasma clouds will likely arrive within the next 6 to 18 hours. This is a tricky one as they could arrive as one impact or two impacts close intill each other. More information in yesterday's news.&lt;/p&gt;
    &lt;p&gt;Thank you for reading this article! Did you have any trouble with the technical terms used in this article? Our help section is the place to be where you can find in-depth articles, a FAQ and a list with common abbreviations. Still puzzled? Just post on our forum where we will help you the best we can!&lt;/p&gt;
    &lt;p&gt;A lot of people come to SpaceWeatherLive to follow the Solar activity or if there is a chance to see the aurora, but with more traffic comes higher costs to keep the servers online. If you like SpaceWeatherLive and want to support the project you can choose a subscription for an ad-free site or consider a donation. With your help we can keep SpaceWeatherLive online!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Last X-flare&lt;/cell&gt;
        &lt;cell&gt;2025/11/11&lt;/cell&gt;
        &lt;cell&gt;X5.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Last M-flare&lt;/cell&gt;
        &lt;cell&gt;2025/11/11&lt;/cell&gt;
        &lt;cell&gt;M1.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Last geomagnetic storm&lt;/cell&gt;
        &lt;cell&gt;2025/11/08&lt;/cell&gt;
        &lt;cell&gt;Kp6+ (G2)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Spotless days&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Last spotless day&lt;/cell&gt;
        &lt;cell&gt;2022/06/08&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Monthly mean Sunspot Number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;October 2025&lt;/cell&gt;
        &lt;cell&gt;114.6 -15.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;November 2025&lt;/cell&gt;
        &lt;cell&gt;91.9 -22.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Last 30 days&lt;/cell&gt;
        &lt;cell&gt;96.2 -34.6&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.spaceweatherlive.com/en/news/view/593/20251111-x5-1-solar-flare-g4-geomagnetic-storm-watch.html"/><published>2025-11-11T21:18:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45893095</id><title>I didn't reverse-engineer the protocol for my blood pressure monitor in 24 hours</title><updated>2025-11-12T00:50:19.069861+00:00</updated><content>&lt;doc fingerprint="b6398d534b2d77a1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I didn't reverse-engineer the protocol for my blood pressure monitor in 24 hours&lt;/head&gt;
    &lt;p&gt;Yesterday after receiving my yearly flu vaccine at the pharmacy I was offered a blood pressure test, which reported a reading that made the young pharmacist who had just given me my vaccine a bit worried.&lt;/p&gt;
    &lt;p&gt;Off the back of this she offered me a 24 hour study, and then strapped a cuff to my arm plumbed into a little device which I had to wear in a little caddy - the cuff would inflate every 30 minutes during the day and every 60 minutes during the night, and then tomorrow I would bring it back for analysis.&lt;/p&gt;
    &lt;p&gt;"Can I read the measurements?" I asked, as it was being strapped to me.&lt;/p&gt;
    &lt;p&gt;"Oh, no, that will just stress you out. We turn that off". Fair enough.&lt;/p&gt;
    &lt;p&gt;Thing is, this device had a little micro-USB port on the side.&lt;/p&gt;
    &lt;head rend="h1"&gt;Doing things the proper way&lt;/head&gt;
    &lt;p&gt;I had started researching the device - a Microlife WatchBP O3 - before I got out of the chemist, and once I'd got back to the office I downloaded the software that's freely available to interact with it, setting up a Bottles instance to run the software since I don't (knowingly) have a Windows machine within 100 metres of me.&lt;/p&gt;
    &lt;p&gt;Unfortunately it didn't seem to be able to access the device, and I had no clue why. In Linux it was just presenting as a standard &lt;code&gt;hidraw&lt;/code&gt; device:&lt;/p&gt;
    &lt;code&gt;[33301.736724] hid-generic 0003:04D9:B554.001E: hiddev96,hidraw1: USB HID v1.11 Device [USB HID UART Bridge] on usb-0000:c5:00.0-1/input0
&lt;/code&gt;
    &lt;p&gt;Fine, I'll install windows.&lt;/p&gt;
    &lt;p&gt;After dodging around Microsoft's idea of UX, and then forwarding the USB device to the VM (I used Gnome Boxes for this, works nicely), I finally got to see WatchBP Analyzer with the data downloaded from the device.&lt;/p&gt;
    &lt;p&gt;But I don't want to open a Virtual Machine running Windows to see this data, and anyway - I'm pretty sure that reverse-engineering this will be good for my blood pressure.&lt;/p&gt;
    &lt;head rend="h1"&gt;Sniffing the traffic&lt;/head&gt;
    &lt;p&gt;Since I'm running this in a Virtual Machine I can just rely on Wireshark in Linux to get the traffic between the host and the device. &lt;code&gt;usbmon&lt;/code&gt; is already installed and we know that the device is on Bus 3, so we can select usbmon3 on startup and start capturing.&lt;/p&gt;
    &lt;p&gt;I'm very much out of my depth at this point but, being one of those who could land a plane in an emergency (why would you talk yourself out of it?!) I decided to crack on regardless. I know that the interesting stuff is sent after I press "Download", and I know that something in there is gonna say "my blood pressure is 137/113" - so let's look for that. Just convert to show bytes as decimal and..&lt;/p&gt;
    &lt;p&gt;..that looks like a blood pressure! Let's copy that out as hex:&lt;/p&gt;
    &lt;code&gt;05 0a 89 71 43 9b
&lt;/code&gt;
    &lt;p&gt;I'm not sure if this is "valid" HID Data (Wireshark seems convinced that only the first byte is the Vendor Data, with the rest being padding) but it seems like the data is being sent in 32-byte "chunks", of which the first byte tells you the number of significant (SIG) following bits in the chunk (I deleted the rest - all zeroes - for clarity). The third byte is my Systolic blood Pressure (SYS), the fourth is my Diastolic blood pressure (DIA), and the fifth is my heart rate (HR) - no clue what the second or last byte is, but let's find all other bytes with my blood pressure in them (in decimal this time, because I can't read hex without help):&lt;/p&gt;
    &lt;code&gt;SIG ??? SYS DIA  HR ??? ??? ???
  5  10 137 113  67 155
  5   0 132  86  68 155
  6   0 126  84  82 155  83
  6  10 128  80  61 155  83
  7   0 148  93  65 155  83  64
  7   0 121  92  74 155  83  94
  7   0 123  83  65 155  83  95
  7   0 123  79  78 155  83 129
&lt;/code&gt;
    &lt;p&gt;Hmm. So we're still looking for the Oscillometric signal peak pressure (OPP)as well as some timestamps (we can calculate Mean arterial pressure - MAP - as &lt;code&gt;(2*DIA+SYS)/3&lt;/code&gt;, according to the manual, and Pulse Pressure (PP) is just &lt;code&gt;SYS-DIA&lt;/code&gt;). We can see the OPP in the packets that come after each of those above, but they don't seem to consistently come in on the same line:&lt;/p&gt;
    &lt;code&gt; 10  82  195   80 *121    0    0    0    0    0    0
 10  82  223   80  *95    0    0    0    0    0    0
  9   1   80  *90    0    0    0    0    0    0
  9  35   80  *86    0    0    0    0    0    0
  8  80 *103    0    0    0    0    0    0
  8  80 *106    0    0    0    0    0    0
  8  80  *90    0    0    0    0    0    0
 10  80  *88    0    0    0    0    0    0   29  251
&lt;/code&gt;
    &lt;p&gt;Oh. Maybe if I stick them together?&lt;/p&gt;
    &lt;code&gt;??? SYS DIA  HR ??? ??? ??? ??? OPP ??? ??? ??? ??? ??? ??? ??? ???
 10 137 113  67 155  82 195  80 121   0   0   0   0   0   0
  0 132  86  68 155  82 223  80  95   0   0   0   0   0   0
  0 126  84  82 155  83   1  80  90   0   0   0   0   0   0
 10 128  80  61 155  83  35  80  86   0   0   0   0   0   0
  0 148  93  65 155  83  64  80 103   0   0   0   0   0   0
  0 121  92  74 155  83  94  80 106   0   0   0   0   0   0
  0 123  83  65 155  83  95  80  90   0   0   0   0   0   0
  0 123  79  78 155  83 129  80  88   0   0   0   0   0   0  29 251
&lt;/code&gt;
    &lt;p&gt;Right, timestamps. I first guessed that the four populated contiguous bytes between &lt;code&gt;HR&lt;/code&gt; and &lt;code&gt;OPP&lt;/code&gt; are a 32-bit unix timestamp, but that would make the first one &lt;code&gt;9B52C350&lt;/code&gt;; either &lt;code&gt;Jul 29 2052&lt;/code&gt; or &lt;code&gt;Dec 08 2012&lt;/code&gt; depending on which endianness the protocol is into. The 8 readings we have here are all from &lt;code&gt;November 10th&lt;/code&gt;, at &lt;code&gt;11:03&lt;/code&gt;, &lt;code&gt;11:31&lt;/code&gt;, &lt;code&gt;12:01&lt;/code&gt;, &lt;code&gt;12:35&lt;/code&gt;, &lt;code&gt;13:00&lt;/code&gt;, &lt;code&gt;13:30&lt;/code&gt;, &lt;code&gt;13:31&lt;/code&gt; and &lt;code&gt;14:01&lt;/code&gt;, which isn't.. isn't that.&lt;/p&gt;
    &lt;p&gt;But note that the number in the 6th column flips from &lt;code&gt;82&lt;/code&gt; to &lt;code&gt;83&lt;/code&gt; when we switch from AM to PM - that's something, and when it does the 7th column resets. And hey - &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;35&lt;/code&gt;, &lt;code&gt;64&lt;/code&gt;, &lt;code&gt;94&lt;/code&gt;, &lt;code&gt;95&lt;/code&gt;.. that seems dangerously close to &lt;code&gt;12:01&lt;/code&gt;, &lt;code&gt;12:35&lt;/code&gt;, &lt;code&gt;13:00&lt;/code&gt;, &lt;code&gt;13:30&lt;/code&gt; and &lt;code&gt;13:31&lt;/code&gt; if you were just to count the minutes. What's going on?&lt;/p&gt;
    &lt;head rend="h1"&gt;Deadlines and dead ends&lt;/head&gt;
    &lt;p&gt;I tried feeding a lot of this into various Als (Kagi gives you access to a few with a nice interface) and I found that they mostly were stupid in ways that made me think. A few times I thought they had "cracked the case" but actually they just made me waste time. But they did remind me e.g. of endianness, so I did get a bit out of them.&lt;/p&gt;
    &lt;p&gt;I also spent quite a bit of time trying to write some Python that emulated the initial handshake and download button of the interface so that it could push out the data as a stream instead of me having to wrestle it out of Wireshark - again, Al had a habit of giving me incorrect code (although it did turn me on to pyhidapi).&lt;/p&gt;
    &lt;p&gt;But ultimately I had a deadline, and I had to return the device even though I wanted to spend more time with it. Possibly for the best - while it did give me some reverse engineering practice (which it turns out I really enjoy), I should do some work instead of procrastinating.&lt;/p&gt;
    &lt;p&gt;My final lesson was a new word - Normotension, normal blood pressure - and a new phrase - White Coat Hypertension, the phenomena of high blood pressure in a clinical setting. Turns out that when you check someone's blood pressure after giving them an injection, it's higher than normal.&lt;/p&gt;
    &lt;p&gt;I don't think I'd recommend getting your blood pressure tested after your next flu jab. But then, I'm not a doctor.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/"/><published>2025-11-11T21:25:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45893646</id><title>Heroku Support for .NET 10</title><updated>2025-11-12T00:50:18.930299+00:00</updated><content>&lt;doc fingerprint="df6f5a5e0ea4b3da"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Heroku Support for .NET 10 LTS: What Developers Need to Know&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Last Updated: November 11, 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s that time of year for .NET when we get a new major version and a bunch of exciting features. .NET Conf 2025 kicked off earlier today, bringing with it the release of .NET 10, as well as ASP.NET Core 10, C# 14, and F# 10. Congrats (and a big thank you) to the .NET team and everyone who helped get .NET 10 out the door.&lt;/p&gt;
    &lt;p&gt;At Heroku, we believe you should be able to use language and framework releases when they launch, and we prepare accordingly. You can now build and run .NET 10 apps on Heroku, with buildpack support for new SDK features like file-based apps, &lt;code&gt;.slnx&lt;/code&gt; solution files, and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migration and support timelines&lt;/head&gt;
    &lt;p&gt;This year’s release is significant because .NET 10 is the new Long Term Support (LTS) release, which will be supported for three years. This extended support, including regular updates and security patches, makes it the best release for businesses and developers to build on and migrate to, offering a stable foundation with access to the latest features.&lt;/p&gt;
    &lt;p&gt;With .NET 10 now available, the clock is ticking on previous versions. Both .NET 8 and .NET 9 will reach End of Support on November 10, 2026. In other words, now is a good time to start planning your migration.&lt;/p&gt;
    &lt;p&gt;We will continue to support .NET 8 and .NET 9 with consistent, timely updates alongside .NET 10. Our .NET support follows the official .NET support policy, and we are fully committed to providing a stable and secure platform for your .NET applications.&lt;/p&gt;
    &lt;p&gt;Let’s dive into using .NET 10 on Heroku today!&lt;/p&gt;
    &lt;head rend="h2"&gt;Zero-config deployment with .NET 10 file-based apps&lt;/head&gt;
    &lt;p&gt;One of the most exciting features in .NET 10 is file-based apps – .NET applications defined in a single C# file without project or solution files, making it easier than ever to deploy .NET apps to Heroku.&lt;/p&gt;
    &lt;p&gt;For example, here’s a complete ASP.NET Core 10 web application, &lt;code&gt;HelloHeroku.cs&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// Use the new #sdk directive to pull in the ASP.NET Core SDK
#:sdk Microsoft.NET.Sdk.Web

var builder = WebApplication.CreateBuilder(args);
var app = builder.Build();  
  
app.MapGet("/", () =&amp;gt; "Hello from .NET 10 on Heroku!");

app.Run();
&lt;/code&gt;
    &lt;p&gt;When you push this to Heroku, the platform detects the &lt;code&gt;*.cs&lt;/code&gt; file and uses the .NET buildpack. Since there are no solution or project files, the buildpack treats it as a file-based app, installs the latest .NET SDK, builds and publishes the app, detects and configures it as a web application, and deploys it to serve traffic.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; The result is a simple, zero-config experience to get you started quickly, ideal for prototyping and developers new to .NET. And there’s more coming – check out the .NET SDK repository to see what the .NET team is working on!&lt;/p&gt;
    &lt;p&gt;To learn more about what you can do with file-based apps on Heroku today, see our Dev Center documentation.&lt;/p&gt;
    &lt;head rend="h2"&gt;SLNX: A modern solution for a modern .NET&lt;/head&gt;
    &lt;p&gt;For decades, .NET developers have used &lt;code&gt;.sln&lt;/code&gt; solution files, a proprietary format introduced in 2002 for Visual Studio. Unlike .NET itself, they haven’t changed much since. In a step towards modernization, the .NET 10 SDK is making SLNX the default format. Heroku ensures a seamless deployment experience by fully supporting both formats.&lt;/p&gt;
    &lt;code&gt;&amp;lt;solution&amp;gt;
    &amp;lt;project path="MyApp\MyApp.csproj"&amp;gt;&amp;lt;/project&amp;gt;
&amp;lt;/solution&amp;gt;
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;*.slnx&lt;/code&gt; files are easier to read and edit, less likely to cause merge conflicts, and support a wider range of workflows and environments, from Linux shells to Visual Studio on Windows. To migrate existing &lt;code&gt;.sln&lt;/code&gt; files, run &lt;code&gt;dotnet solution migrate&lt;/code&gt; or see the .NET blog announcement for more details.&lt;/p&gt;
    &lt;head rend="h2"&gt;Heroku CI and the Microsoft Testing Platform&lt;/head&gt;
    &lt;p&gt;The .NET 10 SDK integrates the Microsoft Testing Platform (MTP) directly in the &lt;code&gt;dotnet test&lt;/code&gt; command. Since Heroku CI runs &lt;code&gt;dotnet test&lt;/code&gt; by default, your test suite works out of the box after you migrate your apps.&lt;/p&gt;
    &lt;p&gt;For more control over the test setup and execution, you can specify custom test commands in your &lt;code&gt;app.json&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to migrate? We’re here to help&lt;/head&gt;
    &lt;p&gt;To support your .NET 10 migration, we’ve updated all our documentation and resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The .NET Getting Started app now runs on .NET 10.&lt;/item&gt;
      &lt;item&gt;The ASP.NET Core configuration article includes new guidance for ASP.NET Core 10, including migration away from the now-obsolete &lt;code&gt;IPNetwork&lt;/code&gt;and&lt;code&gt;ForwardedHeadersOptions.KnownNetworks&lt;/code&gt;APIs (learn more) often used to integrate with Heroku’s router.&lt;/item&gt;
      &lt;item&gt;Apps currently using .NET 10 RC builds on Heroku (with &lt;code&gt;TargetFramework&lt;/code&gt;set to&lt;code&gt;net10.0&lt;/code&gt;) will automatically be built with the stable .NET 10 release on the next&lt;code&gt;git push&lt;/code&gt;. You can pin to specific SDK versions using a&lt;code&gt;global.json&lt;/code&gt;file.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For teams migrating from earlier versions, the .NET 10 breaking changes documentation covers important upgrade considerations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started today&lt;/head&gt;
    &lt;p&gt;We can’t wait to see what you build with .NET 10 on Heroku. From new features like file-based apps to the stability of an LTS release, this is a great time to be a .NET developer.&lt;/p&gt;
    &lt;p&gt;Check out our updated Getting Started with .NET on Heroku guide and please feel free to reach out with any questions or feedback.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Originally Published:&lt;/item&gt;
      &lt;item&gt;.NETBuildpacksLanguagesNextgenProduct Features&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.heroku.com/blog/support-for-dotnet-10-lts-what-developers-need-know/"/><published>2025-11-11T22:18:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45893795</id><title>Four strange places to see London's Roman Wall</title><updated>2025-11-12T00:50:18.592855+00:00</updated><content>&lt;doc fingerprint="af4d01017b3c27b4"&gt;
  &lt;main&gt;
    &lt;p&gt;There are manyplaces around the City of London to see its old Roman Wall, notably alongside Noble Street, in Barber Surgeons' Meadow, through the Barbican, in St Alphage Garden and just outside the entrance to Tower Hill station. Here are four of the odder spots.&lt;/p&gt;
    &lt;p&gt;Four strange places to see London's Roman Wall&lt;/p&gt;
    &lt;p&gt;1) From platform 1 at Tower Hill station&lt;/p&gt;
    &lt;p&gt;If you're ever waiting for a westbound train at Tower Hill station, take a walk to the rear of the platform and take a look across the tracks, roughly where the penultimate carriage would stop. High on the far wall is a square recess lined by black tiles, and at the back of that is a dimly-lit surface of chunky irregular blocks. Unlike every single other thing on the Underground, the Romans built that.&lt;/p&gt;
    &lt;p&gt;London's original wall was 2 miles long, 6 metres high and almost 3 metres thick at its base, all the better to keep out uncivilised marauders. It was built around 200 AD, then left to decay and rebuilt in the medieval era, again for defensive purposes. This is one of the original bits, not that you can easily tell by squinting across the tracks. A small metal lamp points inwards but is no longer switched on because heritage illumination is not a TfL priority. There is however a rather nice silver plaque on the pillar opposite, should you step back far enough to notice it.&lt;/p&gt;
    &lt;p&gt;The plaque confirms that the stones here are a continuation of the wall seen (much more clearly) outside. What it doesn't mention is the unavoidable truth that the wall must once have continued across the tracks and platforms but is no longer here. That's because when the Circle line was constructed in 1882 the railway companies had permission to demolish 22 metres of London's wall and duly did, the Victorians never being afraid to destroy ancient heritage. Ian Visits has a photo of navvies standing atop the offending stonework just before they bashed it through. The square hole is no recompense, plus you can't see anything if a train's in the platform, but it is a brilliantly quirky thing to find on the Underground.&lt;/p&gt;
    &lt;p&gt;2) Round the back of the Leonardo Royal Hotel&lt;/p&gt;
    &lt;p&gt;The short walk from Tower Hill station to the rear entrance of Fenchurch Street passes two hotels. The second is the Leonardo Royal, formerly the Grange, whose car port looks like it leads to a cocktail terrace and maybe some parking. Nothing's signed from the street, indeed I'd never thought to duck through before, but at the far end past the umbrellas of Leo's bar is a significant chunk of Roman wall.&lt;/p&gt;
    &lt;p&gt;The upper section has arched windows built for archers and square holes which once supported a timber platform. It's impressive of course merely medieval, part of the rebuild that occurred along much of the wall as the city grew and spread beyond its former border. To see the Roman section stand closer to the rail and look down, this because ground level then was a few metres lower than now. The telltale signs are several distinctive bands of thin red bricks, these added to strengthen and bond the structure, and which look like layers of jam in a particularly lumpy sponge. The entire segment behind the hotel is over 20m long, thus longer than the better-known chunk outside the station.&lt;/p&gt;
    &lt;p&gt;Perhaps the best thing about this bit of wall is that you can walk through it. A couple of steps have been added on each side allowing passage through a low medieval arch, all marked with anachronistic trip hazard markings. If steps aren't your thing you can also pass round the end of the wall on the flat. Round the other side are a glum alley and a staff back-entrance, also an exit into a separate backstreet past a sign that says PRIVATE No Public Right Of Way Beyond This Point Entry At Your Own Risk Absolutely No Liability Is Accepted For Any Reason Whatsoever. Stuff that, there's an actual Roman Wall back here.&lt;/p&gt;
    &lt;p&gt;3) From a cafe terrace&lt;/p&gt;
    &lt;p&gt;I've written about The City Wall at Vine Street before, a free attraction opened in 2023 beneath a block of student flats. Last time I had to battle the Procedural Curmudgeon to gain admittance but I'm pleased to say they've since loosened up and you can now simply gesture at the door, walk in and give your first name to a flunkey with a tablet. He rattled through the key information with all the practised enthusiasm of a call centre employee dictating terms and conditions, then sent me off down the stairs.&lt;/p&gt;
    &lt;p&gt;Two walls are filled with finds from the excavations, including an AD 70s coin and the bones of a 1760s cat. Nobody's quite sure how the ancient Greek tombstone ended up here, given it predates Londinium, but it has pride of place in a central glass case. The 5-minute historical animation is pretty good too, assuming you can read quite fast. But the main draw is the multi-layered towering remnant of wall which here has the benefit of being properly illuminated and protected from the elements. The protruding lower section (which looks much too clean to be so very old) is all that remains of an original postern, and is also unique because all the other towers elsewhere round the City are merely medieval.&lt;/p&gt;
    &lt;p&gt;What's weird is that this large basement space is overlooked by a balcony scattered with small tables at which sit students and businesspeople consuming coffee and all-day brunch. The baristas operate from the cafe upstairs but any food comes from a small kitchen down below, which has the unnerving side effect that while you're wandering around what looks like a museum it smells like an office canteen. If you choose to be tempted by a cappuccino and smashed avocado on your way out you can enjoy extra time with the Roman wall, or indeed skip the walkthrough altogether and focus only on refreshment with an absolutely unique view. I recommend a proper visit though... the visitors book awaits your praise.&lt;/p&gt;
    &lt;p&gt;4) At the rear of a car park&lt;/p&gt;
    &lt;p&gt;This is amazing on many levels, the main level being subterranean. After WW2 so much of the City was in ruins that planners drove a new dual carriageway through the Aldersgate area and called it London Wall. They believed cars were the future and to that end hid a linear car park directly underneath the new road. It's very narrow, very long and pretty grim, indeed precisely the kind of filming location you'd expect a throwback crime thriller to use for a shoot-out or kidnapping. Cars enter down a short spiral ramp and pedestrians through a grubby side door, and the numbered concrete catacombs stretch on and on for almost 400 metres. Keep walking past the white vans, Range Rovers and the attendant's cabin, trying not to attract too much attention, and almost at the far end is... blimey.&lt;/p&gt;
    &lt;p&gt;You can't park in bays 52 and 53 because they're full of Roman remains. A substantial chunk of wall slots in diagonally beneath the joists and pillars, tall enough to incorporate two separate bands of red bricks. It looks quite smooth up front but fairly rubbly round the back, also much thicker at the base than at the top. Obviously it's very risky to have a scheduled ancient monument in a car park so protective concrete blocks have been added to make sure nobody reverses into the stonework by mistake. More recently a glass screen has been added at one end, branded 'City of London' so you know who to thank, but the other end remains accessible for now (not that you should be stepping in or even touching it).&lt;/p&gt;
    &lt;p&gt;It's the contrasts that I found most incongruous. A relic from Roman times penned inbetween a speed hump and a futile pedestrian crossing. A fortification from the 3rd century beside an electric van built last year. A defensive structure that helped see off the Peasants Revolt beside a poster warning what to do in the event of fire. A boundary wall once an intrinsic part of the capital now underground illuminated by strip lights. And all this at the very far end of an oppressive bunker preserved for the benefit of hardly any eyes in a parking facility only a few know to use. Sure you can see chunks of Roman wall all around the City, even from a tube platform, hotel terrace or cafe. But the oddest spot may well be here in the London Wall car park, should you ever have the balls to take a look.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://diamondgeezer.blogspot.com/2025/11/odd-places-to-see-londons-roman-wall.html"/><published>2025-11-11T22:31:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45893986</id><title>.NET MAUI Is Coming to Linux and the Browser, Powered by Avalonia</title><updated>2025-11-12T00:50:18.271667+00:00</updated><content>&lt;doc fingerprint="f561401a5d0495ec"&gt;
  &lt;main&gt;
    &lt;p&gt;We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.&lt;/p&gt;
    &lt;p&gt;For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Right Now&lt;/head&gt;
    &lt;p&gt;Before we dive into the details, you can experience it yourself:&lt;/p&gt;
    &lt;p&gt;Launch MAUI in your browser â&lt;/p&gt;
    &lt;p&gt;This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Avalonia MAUI Backend?&lt;/head&gt;
    &lt;p&gt;At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.&lt;/p&gt;
    &lt;p&gt;In practical terms, that means several big wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Desktop Linux support&lt;/head&gt;
    &lt;p&gt;.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Embedded Linux&lt;/head&gt;
    &lt;p&gt;Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly support&lt;/head&gt;
    &lt;p&gt;The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus: The Avalonia MAUI Backend runs on Windows and macOS too&lt;/head&gt;
    &lt;p&gt;On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.&lt;/p&gt;
    &lt;p&gt;All of this is possible because we have built a version of MAUI that sits on top of Avaloniaâs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler, faster development&lt;/head&gt;
    &lt;p&gt;For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.&lt;lb/&gt;That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. &lt;lb/&gt;For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is Avalonia Building a Backend for MAUI?&lt;/head&gt;
    &lt;p&gt;It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?&lt;/p&gt;
    &lt;p&gt;The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.&lt;/p&gt;
    &lt;p&gt;This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.&lt;/p&gt;
    &lt;p&gt;There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.&lt;/p&gt;
    &lt;p&gt;So this project is not about âsavingâ MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for MAUI Developers&lt;/head&gt;
    &lt;p&gt;If you have followed MAUI since its launch, you will know the two requests that never went away.&lt;/p&gt;
    &lt;p&gt;Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.&lt;/p&gt;
    &lt;p&gt;The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.&lt;/p&gt;
    &lt;p&gt;It provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware accelerated rendering on every platform&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A consistent layout and styling system&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smooth animations at high refresh rates&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom rendering and visual effects capabilities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Broad platform coverage&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fully supported platform that is receiving significant investment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.&lt;/p&gt;
    &lt;p&gt;By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Next Generation Rendering&lt;/head&gt;
    &lt;p&gt;Performance is an important part of this story.&lt;/p&gt;
    &lt;p&gt;A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.&lt;/p&gt;
    &lt;p&gt;We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.&lt;/p&gt;
    &lt;p&gt;The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.&lt;/p&gt;
    &lt;p&gt;Read more about our Impeller collaboration with Google â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.&lt;/p&gt;
    &lt;p&gt;We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.&lt;/p&gt;
    &lt;p&gt;For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Right Now&lt;/head&gt;
    &lt;p&gt;Before we dive into the details, you can experience it yourself:&lt;/p&gt;
    &lt;p&gt;Launch MAUI in your browser â&lt;/p&gt;
    &lt;p&gt;This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Avalonia MAUI Backend?&lt;/head&gt;
    &lt;p&gt;At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.&lt;/p&gt;
    &lt;p&gt;In practical terms, that means several big wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Desktop Linux support&lt;/head&gt;
    &lt;p&gt;.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Embedded Linux&lt;/head&gt;
    &lt;p&gt;Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly support&lt;/head&gt;
    &lt;p&gt;The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus: The Avalonia MAUI Backend runs on Windows and macOS too&lt;/head&gt;
    &lt;p&gt;On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.&lt;/p&gt;
    &lt;p&gt;All of this is possible because we have built a version of MAUI that sits on top of Avaloniaâs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler, faster development&lt;/head&gt;
    &lt;p&gt;For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.&lt;lb/&gt;That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. &lt;lb/&gt;For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is Avalonia Building a Backend for MAUI?&lt;/head&gt;
    &lt;p&gt;It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?&lt;/p&gt;
    &lt;p&gt;The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.&lt;/p&gt;
    &lt;p&gt;This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.&lt;/p&gt;
    &lt;p&gt;There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.&lt;/p&gt;
    &lt;p&gt;So this project is not about âsavingâ MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for MAUI Developers&lt;/head&gt;
    &lt;p&gt;If you have followed MAUI since its launch, you will know the two requests that never went away.&lt;/p&gt;
    &lt;p&gt;Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.&lt;/p&gt;
    &lt;p&gt;The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.&lt;/p&gt;
    &lt;p&gt;It provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware accelerated rendering on every platform&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A consistent layout and styling system&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smooth animations at high refresh rates&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom rendering and visual effects capabilities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Broad platform coverage&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fully supported platform that is receiving significant investment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.&lt;/p&gt;
    &lt;p&gt;By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Next Generation Rendering&lt;/head&gt;
    &lt;p&gt;Performance is an important part of this story.&lt;/p&gt;
    &lt;p&gt;A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.&lt;/p&gt;
    &lt;p&gt;We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.&lt;/p&gt;
    &lt;p&gt;The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.&lt;/p&gt;
    &lt;p&gt;Read more about our Impeller collaboration with Google â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.&lt;/p&gt;
    &lt;p&gt;We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.&lt;/p&gt;
    &lt;p&gt;For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Right Now&lt;/head&gt;
    &lt;p&gt;Before we dive into the details, you can experience it yourself:&lt;/p&gt;
    &lt;p&gt;Launch MAUI in your browser â&lt;/p&gt;
    &lt;p&gt;This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the Avalonia MAUI Backend?&lt;/head&gt;
    &lt;p&gt;At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.&lt;/p&gt;
    &lt;p&gt;In practical terms, that means several big wins.&lt;/p&gt;
    &lt;head rend="h3"&gt;Desktop Linux support&lt;/head&gt;
    &lt;p&gt;.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.&lt;/p&gt;
    &lt;head rend="h3"&gt;Embedded Linux&lt;/head&gt;
    &lt;p&gt;Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly support&lt;/head&gt;
    &lt;p&gt;The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus: The Avalonia MAUI Backend runs on Windows and macOS too&lt;/head&gt;
    &lt;p&gt;On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.&lt;/p&gt;
    &lt;p&gt;All of this is possible because we have built a version of MAUI that sits on top of Avaloniaâs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simpler, faster development&lt;/head&gt;
    &lt;p&gt;For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.&lt;lb/&gt;That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. &lt;lb/&gt;For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is Avalonia Building a Backend for MAUI?&lt;/head&gt;
    &lt;p&gt;It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?&lt;/p&gt;
    &lt;p&gt;The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.&lt;/p&gt;
    &lt;p&gt;This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.&lt;/p&gt;
    &lt;p&gt;There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.&lt;/p&gt;
    &lt;p&gt;So this project is not about âsavingâ MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters for MAUI Developers&lt;/head&gt;
    &lt;p&gt;If you have followed MAUI since its launch, you will know the two requests that never went away.&lt;/p&gt;
    &lt;p&gt;Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.&lt;/p&gt;
    &lt;p&gt;The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.&lt;/p&gt;
    &lt;p&gt;It provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware accelerated rendering on every platform&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A consistent layout and styling system&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smooth animations at high refresh rates&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom rendering and visual effects capabilities&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Broad platform coverage&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fully supported platform that is receiving significant investment&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.&lt;/p&gt;
    &lt;p&gt;By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance and Next Generation Rendering&lt;/head&gt;
    &lt;p&gt;Performance is an important part of this story.&lt;/p&gt;
    &lt;p&gt;A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.&lt;/p&gt;
    &lt;p&gt;We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.&lt;/p&gt;
    &lt;p&gt;The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.&lt;/p&gt;
    &lt;p&gt;Read more about our Impeller collaboration with Google â&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.&lt;/p&gt;
    &lt;p&gt;This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://avaloniaui.net/blog/net-maui-is-coming-to-linux-and-the-browser-powered-by-avalonia"/><published>2025-11-11T22:50:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45894145</id><title>Who Still Uses Cash?</title><updated>2025-11-12T00:50:17.800208+00:00</updated><content>&lt;doc fingerprint="c01281fe4b495f6c"&gt;
  &lt;main&gt;
    &lt;p&gt;2d ago&lt;/p&gt;
    &lt;head rend="h1"&gt;Who Still Uses Cash?&lt;/head&gt;
    &lt;head rend="h3"&gt;Geographic and Economic Patterns:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The poorest countries rely more on cash: Myanmar (98%), Ethiopia (95%), and Gambia (95%) top the list, reflecting limited banking infrastructure&lt;/item&gt;
      &lt;item&gt;Wealthy nations are nearly cashless: Sweden (14%), Norway (10%), and South Korea (10%) show how digital payment infrastructure correlates with economic development&lt;/item&gt;
      &lt;item&gt;Emerging economies like Mexico (80%), India (70%), and Thailand (65%) show that cash usage can be quite sticky&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Surprising Outliers:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Japan at 60% is remarkably high for such a technologically advanced nation - cultural preference for cash persists despite infrastructure&lt;/item&gt;
      &lt;item&gt;Germany at 51% is an anomaly among wealthy European nations, likely due to cultural privacy concerns&lt;/item&gt;
      &lt;item&gt;Italy at 62% stands out in Western Europe, possibly linked to informal economy prevalence&lt;/item&gt;
      &lt;item&gt;China at just 10% is striking - this reflects its leapfrog to mobile payments (Alipay/WeChat Pay), bypassing traditional card infrastructure entirely&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.voronoiapp.com/economy/Who-Still-Uses-Cash-7090"/><published>2025-11-11T23:06:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45894569</id><title>I can build enterprise software but I can't charge for it</title><updated>2025-11-12T00:50:17.269328+00:00</updated><content>&lt;doc fingerprint="2549183867a7bfa4"&gt;
  &lt;main&gt;
    &lt;p&gt;How I spent 18 months building an AI avatar platform through war, economic collapse, and 120-hour weeks—and why I'm now looking for a partner to help me turn this into a real business.&lt;/p&gt;
    &lt;head rend="h2"&gt;Test it first: neoclerks.com/en&lt;/head&gt;
    &lt;p&gt;Go ahead. Talk to the avatar for 2 minutes. Ask it anything in English. Watch how it responds in real-time with synchronized lip movements and natural conversation.&lt;/p&gt;
    &lt;p&gt;I'll be here when you get back.&lt;/p&gt;
    &lt;p&gt;I'm 35 years old. I live in Iran. I spent 18 months building an enterprise AI avatar platform that competes with Soul Machines ($70M funded, $50k+ setup fees for enterprise clients).&lt;/p&gt;
    &lt;p&gt;The product works. You just tested it. The code is production-ready. The architecture is solid. The documentation is complete (20+ guides, 100+ pages).&lt;/p&gt;
    &lt;p&gt;I have zero customers. Zero revenue. I'm out of savings and actively job hunting for 9 months with no luck. My wife is a nurse who works 5am-7pm while I sit at a computer building something that won't pay our rent.&lt;/p&gt;
    &lt;p&gt;I can't monetize this from Iran:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;❌ Stripe, PayPal, Western payment processors (sanctioned)&lt;/item&gt;
      &lt;item&gt;❌ AWS, GCP, Azure (sanctioned)&lt;/item&gt;
      &lt;item&gt;❌ Western bank account (sanctioned)&lt;/item&gt;
      &lt;item&gt;❌ Credit card payments from customers (no processor works here)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I tried the local Iranian market. I showed it to friends, family, and potential clients. Their response: "Nobody in Iran will pay $500/month for this. The Persian language quality isn't perfect. We'll use free ChatGPT instead."&lt;/p&gt;
    &lt;p&gt;They're right about the market. Iran's currency devalued 100,000x over 20 years. Hotels are closing. Banks are failing. People struggle to pay rent and buy food. This isn't the market for enterprise AI.&lt;/p&gt;
    &lt;p&gt;So here's why I'm writing this:&lt;/p&gt;
    &lt;p&gt;I'm not confidently selling a finished product for $100k. I'm looking for a partner or co-founder who can access Stripe, handle sales, and help me turn 18 months of brutal work into a business that actually makes money.&lt;/p&gt;
    &lt;p&gt;I want to keep building this. I just need someone who can sell it.&lt;/p&gt;
    &lt;p&gt;NeoClerks is a self-hosted AI avatar platform with real-time conversation and enterprise infrastructure:&lt;/p&gt;
    &lt;p&gt;Core Avatar Tech:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3D photorealistic avatars (Unreal Engine 5 + MetaHuman, pixel streaming)&lt;/item&gt;
      &lt;item&gt;Real-time conversation (1-8 seconds response time depending on cache hits)&lt;/item&gt;
      &lt;item&gt;47 languages (OpenAI STT + TTS)&lt;/item&gt;
      &lt;item&gt;Smart 4-layer caching (hash → vector → LLM selection, 60-90% cost reduction)&lt;/item&gt;
      &lt;item&gt;RAG knowledge base (hybrid BM25 + vector search with pgvector)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Enterprise Business Infrastructure:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multi-tenant B2B architecture (8 database tables for organizations, subscriptions, usage tracking)&lt;/item&gt;
      &lt;item&gt;8 pre-configured pricing tiers ($497-$947/month + Enterprise custom)&lt;/item&gt;
      &lt;item&gt;Automated billing system (conversation counting, overage calculation, invoice generation)&lt;/item&gt;
      &lt;item&gt;Conversation analytics (11 database tables, 7 AI analysis types via OpenAI Batch API)&lt;/item&gt;
      &lt;item&gt;Admin panel (Next.js, full system management UI)&lt;/item&gt;
      &lt;item&gt;Monitoring stack (Prometheus + Grafana + Loki, 35+ alert rules)&lt;/item&gt;
      &lt;item&gt;4-server distributed architecture (scalable to 100+ instances, k8s-ready)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Compare to competitors:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;NeoClerks&lt;/cell&gt;
        &lt;cell role="head"&gt;Soul Machines&lt;/cell&gt;
        &lt;cell role="head"&gt;D-ID&lt;/cell&gt;
        &lt;cell role="head"&gt;Synthesia&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Setup Cost&lt;/cell&gt;
        &lt;cell&gt;$0-$797&lt;/cell&gt;
        &lt;cell&gt;$50,000+&lt;/cell&gt;
        &lt;cell&gt;$0&lt;/cell&gt;
        &lt;cell&gt;$0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;3D Photorealistic&lt;/cell&gt;
        &lt;cell&gt;✅ Real-time UE5&lt;/cell&gt;
        &lt;cell&gt;✅ Real-time&lt;/cell&gt;
        &lt;cell&gt;❌ 2D only&lt;/cell&gt;
        &lt;cell&gt;❌ 2D only&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Real-time Conversation&lt;/cell&gt;
        &lt;cell&gt;✅ 1-8s&lt;/cell&gt;
        &lt;cell&gt;✅ ~2s&lt;/cell&gt;
        &lt;cell&gt;✅ ~3s&lt;/cell&gt;
        &lt;cell&gt;❌ Pre-recorded&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Self-hosted&lt;/cell&gt;
        &lt;cell&gt;✅ Full control&lt;/cell&gt;
        &lt;cell&gt;❌ Cloud only&lt;/cell&gt;
        &lt;cell&gt;❌ Cloud only&lt;/cell&gt;
        &lt;cell&gt;❌ Cloud only&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;White-label&lt;/cell&gt;
        &lt;cell&gt;✅ Included&lt;/cell&gt;
        &lt;cell&gt;Enterprise tier&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Limited&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Multi-tenant B2B&lt;/cell&gt;
        &lt;cell&gt;✅ Built-in&lt;/cell&gt;
        &lt;cell&gt;Enterprise tier&lt;/cell&gt;
        &lt;cell&gt;Basic&lt;/cell&gt;
        &lt;cell&gt;Basic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Usage Analytics&lt;/cell&gt;
        &lt;cell&gt;✅ 7 types&lt;/cell&gt;
        &lt;cell&gt;Enterprise tier&lt;/cell&gt;
        &lt;cell&gt;Basic&lt;/cell&gt;
        &lt;cell&gt;Basic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Admin Panel&lt;/cell&gt;
        &lt;cell&gt;✅ Full UI&lt;/cell&gt;
        &lt;cell&gt;Enterprise tier&lt;/cell&gt;
        &lt;cell&gt;Basic&lt;/cell&gt;
        &lt;cell&gt;Basic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;RAG Knowledge Base&lt;/cell&gt;
        &lt;cell&gt;✅ Hybrid search&lt;/cell&gt;
        &lt;cell&gt;Enterprise tier&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Market opportunity: AI avatar market is $2.1B in 2024, projected $4.3B by 2027. Soul Machines has 100+ enterprise clients (Mercedes-Benz, Vodafone). The demand exists.&lt;/p&gt;
    &lt;p&gt;Full technical documentation: 20+ guides covering architecture, deployment (local/2-server/4-server), API reference (96+ endpoints), security (JWT + RBAC), monitoring, analytics.&lt;/p&gt;
    &lt;p&gt;For the last 9 months, I worked 100-120 hours per week.&lt;/p&gt;
    &lt;p&gt;My wife wakes up at 5 AM for her nursing shift. I wake up with her. She comes home at 7 PM—that's when I eat my first meal of the day. We eat together. Then I go back to the computer until midnight or 1 AM, when I physically can't move anymore and fall asleep at my desk.&lt;/p&gt;
    &lt;p&gt;One evening she came home exhausted and asked: "Is there anything else we can talk about besides this project?"&lt;/p&gt;
    &lt;p&gt;I had nothing to say. I live this. I breathe this. It's all I think about.&lt;/p&gt;
    &lt;p&gt;My mother keeps asking: "When will you get a job?" I've been trying for 9 months. The game industry is collapsing globally with massive layoffs. Iran's economy is in the worst state in its history. AI is replacing developer positions. I thought building something valuable was the answer.&lt;/p&gt;
    &lt;p&gt;In June 2025, during the Iran-Israel conflict, there were bombs falling. I kept working. Explosions in the distance. I just hoped none would hit my house.&lt;/p&gt;
    &lt;p&gt;Around the same time, a crypto exchange I used got hacked—I lost what little savings buffer I had left.&lt;/p&gt;
    &lt;p&gt;I kept coding. What else could I do?&lt;/p&gt;
    &lt;p&gt;The PAK file loading system nearly killed this project.&lt;/p&gt;
    &lt;p&gt;The problem: Generate lip-sync animations on-demand, cook them in Unreal Engine, package them into PAK files, and load them dynamically into a shipped build. There's almost zero documentation for this—Unreal is built for game development, not service-based animation streaming.&lt;/p&gt;
    &lt;p&gt;I spent a month fighting this. One month of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Creating custom Unreal plugins&lt;/item&gt;
      &lt;item&gt;Syncing paths precisely between 5 different services (9 with body animation)&lt;/item&gt;
      &lt;item&gt;Debugging why animations load perfectly in development but crash in production&lt;/item&gt;
      &lt;item&gt;Fighting Unreal's cooking pipeline (loading a project takes 17 seconds minimum per cook)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At 3 AM one night, after 14 straight hours of debugging, I almost deleted the entire repository. I was done.&lt;/p&gt;
    &lt;p&gt;Then I found one obscure tutorial that gave me the breakthrough. I got it working. Then I realized it was too slow for real-time use—17 seconds per cook meant I couldn't use it for live responses.&lt;/p&gt;
    &lt;p&gt;I had to pivot: use real-time lip-sync for live conversations (lower quality but acceptable), and save the PAK system for smart caching where quality matters and pre-generation is possible.&lt;/p&gt;
    &lt;p&gt;That was one problem. I faced 50 more like it.&lt;/p&gt;
    &lt;p&gt;The session management between the QR code scanning (phone becomes microphone) and pixel streaming frontend. The WebSocket orchestration. Making it gracefully handle network failures, users switching tabs, phones dying. Creating limits so it can't be abused in a hotel lobby. Every edge case felt like a show-stopper.&lt;/p&gt;
    &lt;p&gt;I wanted to quit every single month. But I had no other choice. I kept going.&lt;/p&gt;
    &lt;p&gt;I couldn't have built this alone in 18 months. Without AI assistance, this would've taken 6 years.&lt;/p&gt;
    &lt;p&gt;I used Claude Code (Sonnet 4.5) as my co-developer and code reviewer. We worked together:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I'd write code → Claude would review and suggest improvements&lt;/item&gt;
      &lt;item&gt;Claude would write code → I'd review and validate&lt;/item&gt;
      &lt;item&gt;Claude would audit → I'd test → Claude would audit again&lt;/item&gt;
      &lt;item&gt;Repeat hundreds of times&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For the last 50 times I asked Claude to audit this codebase, it consistently responded: "This is 100% production-ready. Deploy immediately."&lt;/p&gt;
    &lt;p&gt;Early on, every fresh audit would find improvements (security issues, architecture problems, missing edge cases). For the last 2 months, Claude finds nothing to fix.&lt;/p&gt;
    &lt;p&gt;This is AI-validated, production-grade code.&lt;/p&gt;
    &lt;p&gt;You can verify this yourself: open Claude Code, point it at the repository, ask it to audit for production readiness. It will confirm.&lt;/p&gt;
    &lt;p&gt;Repository stats (from refactored architecture):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Total commits: 818&lt;/item&gt;
      &lt;item&gt;First commit: September 26, 2024&lt;/item&gt;
      &lt;item&gt;Development time: 18 months (includes original version before refactor)&lt;/item&gt;
      &lt;item&gt;Lines of code: 20,000+ (backend Python/FastAPI, frontend Next.js, UE5 blueprints)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I showed this to several potential clients in Iran before realizing the local market is dead:&lt;/p&gt;
    &lt;p&gt;Problem 1: Currency collapse Iran's rial lost 99.999% of its value over 20 years. I'd need to charge in dollars but collect in rials. Nobody will pay $500/month when that's equivalent to a month's salary.&lt;/p&gt;
    &lt;p&gt;Problem 2: Persian language quality I use GPT-5 (the most expensive model) but Persian language quality isn't perfect. Clients compare it to ChatGPT and expect native-level fluency. It's good (in my opinion), but not perfect enough for risk-averse businesses.&lt;/p&gt;
    &lt;p&gt;Problem 3: Economic collapse Hotels are closing (no tourists). Banks are failing. Hospitals can't risk AI mistakes in Persian. Entertainment is not a priority when people lack clean water. I thought about pivoting to water filters—that would sell—but I'm a programmer, not an engineer.&lt;/p&gt;
    &lt;p&gt;Problem 4: International sanctions Even if I found international clients, I can't:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Accept payments (no Stripe, PayPal, or credit card processors)&lt;/item&gt;
      &lt;item&gt;Deploy on cloud platforms (AWS/GCP/Azure sanctioned)&lt;/item&gt;
      &lt;item&gt;Open Western bank accounts&lt;/item&gt;
      &lt;item&gt;Register US/EU companies&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I tried crypto payments: B2B customers won't pay $500-1000/month subscriptions in Bitcoin. Their accounting departments reject it. Too volatile for recurring revenue.&lt;/p&gt;
    &lt;p&gt;I tried intermediaries: They want 20-30% commission, they hold all payment power (can shut me down anytime), and I still hold legal liability while they take zero risk.&lt;/p&gt;
    &lt;p&gt;The reality: I can build enterprise software. I just can't charge for it.&lt;/p&gt;
    &lt;p&gt;I don't want to sell this and disappear. I want to keep building it. I love this product. I just need a partner who can access Stripe and handle sales.&lt;/p&gt;
    &lt;p&gt;Here are three options (all negotiable):&lt;/p&gt;
    &lt;p&gt;Structure:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You invest: $50-80k + handle sales/operations/Stripe access&lt;/item&gt;
      &lt;item&gt;I contribute: Complete codebase + continue as technical co-founder&lt;/item&gt;
      &lt;item&gt;Equity split: 60/40 or 70/30 (negotiable based on your investment and role)&lt;/item&gt;
      &lt;item&gt;Commitment: 3-year minimum from both sides&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Your responsibilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sales and customer acquisition&lt;/item&gt;
      &lt;item&gt;Payment processing (Stripe account)&lt;/item&gt;
      &lt;item&gt;Customer support and onboarding&lt;/item&gt;
      &lt;item&gt;Marketing and positioning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My responsibilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All technical development and maintenance&lt;/item&gt;
      &lt;item&gt;Feature development based on customer feedback&lt;/item&gt;
      &lt;item&gt;Infrastructure and DevOps&lt;/item&gt;
      &lt;item&gt;Technical support for complex issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Timeline to profitability: 6-12 months to acquire first 20-30 customers at $500-1500/month each&lt;/p&gt;
    &lt;p&gt;Structure:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You pay: $60-80k upfront (full code ownership, all IP rights)&lt;/item&gt;
      &lt;item&gt;I sign: 3-year contractor agreement at $4-6k/month&lt;/item&gt;
      &lt;item&gt;Total cost: $60-80k + $144-216k over 3 years = $204-296k total&lt;/item&gt;
      &lt;item&gt;You get: Technical founder-level support without equity dilution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What I provide:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;30 days intensive support (live setup, training, documentation walkthrough)&lt;/item&gt;
      &lt;item&gt;Ongoing development (20-30 hours/week for 3 years)&lt;/item&gt;
      &lt;item&gt;Feature development and bug fixes&lt;/item&gt;
      &lt;item&gt;Architecture decisions and code reviews&lt;/item&gt;
      &lt;item&gt;Emergency support for critical issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why this works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You own 100% of the business&lt;/item&gt;
      &lt;item&gt;You have guaranteed technical support for 3 years&lt;/item&gt;
      &lt;item&gt;I have stable income while you scale&lt;/item&gt;
      &lt;item&gt;Lower risk than buying code and hoping it works&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Structure:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You invest: $30-50k upfront + Stripe access + sales/marketing&lt;/item&gt;
      &lt;item&gt;I contribute: Complete codebase + continue as technical co-founder&lt;/item&gt;
      &lt;item&gt;Revenue split: 50/50 until you recoup investment, then renegotiate (maybe 40/60 or 30/70)&lt;/item&gt;
      &lt;item&gt;Commitment: 3-year minimum&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why this works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lower upfront cost for you&lt;/item&gt;
      &lt;item&gt;I'm incentivized to help you succeed (my income depends on revenue)&lt;/item&gt;
      &lt;item&gt;Fair risk distribution&lt;/item&gt;
      &lt;item&gt;Aligns our interests long-term&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I'm flexible on structure. What matters most to me:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The product gets to market and helps businesses&lt;/item&gt;
      &lt;item&gt;I can continue working on it (I genuinely love building this)&lt;/item&gt;
      &lt;item&gt;I have stable income to support my family&lt;/item&gt;
      &lt;item&gt;We both benefit from the success&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have a different structure in mind, let's talk.&lt;/p&gt;
    &lt;p&gt;If you hired developers to build this from scratch:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost to Build&lt;/cell&gt;
        &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Avatar + AI pipeline (UE5, STT, LLM, TTS, RAG)&lt;/cell&gt;
        &lt;cell&gt;$50-70k&lt;/cell&gt;
        &lt;cell&gt;6 months&lt;/cell&gt;
        &lt;cell&gt;✅ Done&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Multi-tenant B2B infrastructure&lt;/cell&gt;
        &lt;cell&gt;$20-30k&lt;/cell&gt;
        &lt;cell&gt;3 months&lt;/cell&gt;
        &lt;cell&gt;✅ Done&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Conversation analytics (11 tables, 7 AI analyses)&lt;/cell&gt;
        &lt;cell&gt;$15-25k&lt;/cell&gt;
        &lt;cell&gt;2 months&lt;/cell&gt;
        &lt;cell&gt;✅ Done&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Monitoring stack (Prometheus/Grafana/35 alerts)&lt;/cell&gt;
        &lt;cell&gt;$10-15k&lt;/cell&gt;
        &lt;cell&gt;1 month&lt;/cell&gt;
        &lt;cell&gt;✅ Done&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Admin panel (Next.js, TypeScript)&lt;/cell&gt;
        &lt;cell&gt;$10-15k&lt;/cell&gt;
        &lt;cell&gt;1 month&lt;/cell&gt;
        &lt;cell&gt;✅ Done&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4-server distributed architecture&lt;/cell&gt;
        &lt;cell&gt;$20-30k&lt;/cell&gt;
        &lt;cell&gt;2 months&lt;/cell&gt;
        &lt;cell&gt;✅ Done&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Security (JWT on 96 endpoints, RBAC, SSL)&lt;/cell&gt;
        &lt;cell&gt;$15-20k&lt;/cell&gt;
        &lt;cell&gt;1 month&lt;/cell&gt;
        &lt;cell&gt;✅ Done&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Documentation (20+ guides, 100+ pages)&lt;/cell&gt;
        &lt;cell&gt;$10-15k&lt;/cell&gt;
        &lt;cell&gt;1 month&lt;/cell&gt;
        &lt;cell&gt;✅ Done&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;TOTAL&lt;/cell&gt;
        &lt;cell&gt;$150-220k&lt;/cell&gt;
        &lt;cell&gt;18 months&lt;/cell&gt;
        &lt;cell&gt;✅ Done&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You're getting this for $60-80k (outright) or $50-80k + equity (partnership).&lt;/p&gt;
    &lt;p&gt;That's a 50-70% discount vs hiring it built, plus you skip 18 months of development time.&lt;/p&gt;
    &lt;p&gt;I'm not going to oversell this. Here's what you should know:&lt;/p&gt;
    &lt;p&gt;Technical Limitations:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Animation quality is "good" not "perfect": Lip sync is 85-90% accurate (on par with Soul Machines demos). Body language is limited to idle animations. Facial expressions are good but not Pixar-level. Fix: Hire a 3D animator for $3-5k to improve animations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Persian language needs work: English is flawless. Persian is 80-85% accurate (Whisper + GPT-4o struggle with some accents/nuances). Fix: Fine-tune Whisper on Persian dataset or use local STT service.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Demo server latency: The live demo runs on my GTX 1060 in Iran with 4G home internet. International users may experience 500-1500ms lag. This is infrastructure, not code. Fix: Deploy on AWS in customer's region (latency drops to 100-300ms).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;GPU requirements: Development works on GTX 1060 (1 user). Production needs RTX 3090+ for 10 users, RTX 5090 for 30 concurrent users. Cost: $800-2000/month for AWS g4dn/g5 instances.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Business Limitations:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Zero customers: No testimonials, no case studies, no revenue history. Reality: You're starting from scratch on customer acquisition.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No brand recognition: "NeoClerks" has no market awareness. No SEO, no social media following. Reality: You'll likely want to rebrand entirely.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No marketing materials: No demo videos (beyond live demo), no sales collateral, no pitch decks. Reality: You'll need to create these.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Support depends on our agreement: I'll support based on which option you choose (30 days for outright sale, 3 years for partnership). Reality: If you scale, you may need to hire additional support ($50-70k/year).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I know you're skeptical. Here's how to verify everything:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to https://neoclerks.com/en/&lt;/item&gt;
      &lt;item&gt;Click "Book a Private Demo"&lt;/item&gt;
      &lt;item&gt;Talk to the avatar in English&lt;/item&gt;
      &lt;item&gt;Evaluate: response time, lip sync quality, conversation coherence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Known limitation on demo: Running on consumer GPU + 4G home internet in Iran, so international latency will be higher than production deployment.&lt;/p&gt;
    &lt;p&gt;I'll give you read-only GitHub access to review:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Backend services (FastAPI microservices, Python)&lt;/item&gt;
      &lt;item&gt;Frontend code (Next.js landing page + admin panel)&lt;/item&gt;
      &lt;item&gt;Database schema (migrations, SQLAlchemy models)&lt;/item&gt;
      &lt;item&gt;Infrastructure configs (Docker Compose, nginx, environment templates)&lt;/item&gt;
      &lt;item&gt;UE5 project structure (blueprints, MetaHuman assets, pixel streaming)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What you can verify: Code quality, architecture, documentation, tests.&lt;/p&gt;
    &lt;p&gt;1-2 hour Zoom call where I:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deploy the entire stack locally (you watch)&lt;/item&gt;
      &lt;item&gt;Walk through the admin panel, monitoring dashboards, API endpoints&lt;/item&gt;
      &lt;item&gt;Explain the architecture and code structure&lt;/item&gt;
      &lt;item&gt;Answer technical questions&lt;/item&gt;
      &lt;item&gt;Demonstrate analytics system, billing system, session management&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Get Claude Code access&lt;/item&gt;
      &lt;item&gt;Point it at the repository&lt;/item&gt;
      &lt;item&gt;Ask: "Review this codebase for production readiness"&lt;/item&gt;
      &lt;item&gt;Claude will confirm what I've said&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You're a digital agency with existing B2B clients:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You have 50-200 clients you can upsell to&lt;/item&gt;
      &lt;item&gt;You can position this at $700-1500/month to hotels, retail, healthcare&lt;/item&gt;
      &lt;item&gt;You have a DevOps person or can hire one ($60-80k/year)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You're a SaaS entrepreneur who's done this before:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You've sold B2B SaaS (you understand 6-12 month sales cycles)&lt;/item&gt;
      &lt;item&gt;You have capital for 6-12 months runway&lt;/item&gt;
      &lt;item&gt;You know how to do outreach, demos, close deals&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You're an enterprise IT company:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You sell to Fortune 500s (banks, telecoms, healthcare)&lt;/item&gt;
      &lt;item&gt;You need a white-label AI solution for your portfolio&lt;/item&gt;
      &lt;item&gt;Your team closes 5-10 enterprise deals/year&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You want a technical co-founder:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You can handle sales/ops but lack technical depth&lt;/item&gt;
      &lt;item&gt;You want someone committed long-term (not just a contractor)&lt;/item&gt;
      &lt;item&gt;You're willing to share equity for proven technical execution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You have no technical skills and won't hire:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can't deploy Docker containers&lt;/item&gt;
      &lt;item&gt;Won't hire a DevOps engineer ($60-80k/year)&lt;/item&gt;
      &lt;item&gt;Why you'll fail: Every customer needs a deployment&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You have no sales experience:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You've never sold B2B SaaS&lt;/item&gt;
      &lt;item&gt;You can't afford to hire a salesperson ($50-70k/year + commission)&lt;/item&gt;
      &lt;item&gt;Why you'll fail: Zero customers means you acquire them all yourself&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You expect passive income:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You want "buy it and forget it"&lt;/item&gt;
      &lt;item&gt;You don't want to provide customer support&lt;/item&gt;
      &lt;item&gt;Why you'll fail: This requires active sales and support&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let's do realistic math:&lt;/p&gt;
    &lt;p&gt;Scenario: Digital agency with 100 existing clients&lt;/p&gt;
    &lt;p&gt;Month 1-3: Pitch 100 clients, 10 sign up (10% conversion)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tier: Professional ($799/month)&lt;/item&gt;
      &lt;item&gt;Revenue: $7,990/month&lt;/item&gt;
      &lt;item&gt;Costs: $2,000/month (AWS + OpenAI API)&lt;/item&gt;
      &lt;item&gt;Profit: $5,990/month (~75% margin)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Month 4-6: Upsell another 10 clients (20 total)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Revenue: $15,980/month&lt;/item&gt;
      &lt;item&gt;Costs: $3,500/month&lt;/item&gt;
      &lt;item&gt;Profit: $12,480/month&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Month 7-12: Organic growth + referrals (30 total)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Revenue: $23,970/month = $287k/year&lt;/item&gt;
      &lt;item&gt;Costs: $5,000/month = $60k/year&lt;/item&gt;
      &lt;item&gt;Profit: $18,970/month = $227k/year&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ROI: At $60k purchase price, you break even in 3-4 months with 30 customers.&lt;/p&gt;
    &lt;p&gt;This is realistic if you have existing B2B relationships. If starting cold, add 6-12 months to reach 30 customers.&lt;/p&gt;
    &lt;p&gt;Since I can't use Stripe/PayPal, we use cryptocurrency escrow for safety:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You test the demo (5 minutes)&lt;/item&gt;
      &lt;item&gt;You read this article&lt;/item&gt;
      &lt;item&gt;We do a 30-minute intro call (verify I'm real, answer questions)&lt;/item&gt;
      &lt;item&gt;I send you high-level code structure overview&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You sign NDA (mutual protection)&lt;/item&gt;
      &lt;item&gt;I give you read-only GitHub access&lt;/item&gt;
      &lt;item&gt;You review code, architecture, documentation (3-7 days)&lt;/item&gt;
      &lt;item&gt;We do live deployment walkthrough (1-2 hours)&lt;/item&gt;
      &lt;item&gt;You decide if you want to proceed&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We agree on structure (co-founder / sale+contract / revenue share)&lt;/item&gt;
      &lt;item&gt;We finalize price and terms&lt;/item&gt;
      &lt;item&gt;Lawyers draft agreement (if needed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For outright sale:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We use Hodl Hodl (non-custodial multisig escrow, works from Iran)&lt;/item&gt;
      &lt;item&gt;You deposit BTC/USDT into escrow (neutral third party holds funds)&lt;/item&gt;
      &lt;item&gt;I transfer complete source code + documentation&lt;/item&gt;
      &lt;item&gt;We conduct 2-hour live setup session (recorded)&lt;/item&gt;
      &lt;item&gt;You verify code works (3-7 days)&lt;/item&gt;
      &lt;item&gt;You release escrow payment&lt;/item&gt;
      &lt;item&gt;30-day support period begins&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For partnership:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Standard equity agreement via lawyer&lt;/item&gt;
      &lt;item&gt;Payment via wire transfer to your company account (you're outside Iran)&lt;/item&gt;
      &lt;item&gt;I join as technical co-founder&lt;/item&gt;
      &lt;item&gt;We start working together immediately&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Escrow mediator handles disputes (for sales)&lt;/item&gt;
      &lt;item&gt;✅ Standard equity agreements (for partnerships)&lt;/item&gt;
      &lt;item&gt;✅ You verify code before payment release&lt;/item&gt;
      &lt;item&gt;✅ I get security that payment is locked in escrow&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Timeline: 4-6 weeks from first contact to deal closed.&lt;/p&gt;
    &lt;p&gt;If you become my partner or buyer, here's the immediate roadmap:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deploy on your infrastructure (AWS/GCP in target market)&lt;/item&gt;
      &lt;item&gt;Set up Stripe account and payment processing&lt;/item&gt;
      &lt;item&gt;Rebrand if desired (logo, domain, marketing site)&lt;/item&gt;
      &lt;item&gt;Create demo videos and sales collateral&lt;/item&gt;
      &lt;item&gt;I provide intensive technical support&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Target: Acquire 5-10 pilot customers&lt;/item&gt;
      &lt;item&gt;Pricing: $500-1000/month with discounted setup fees&lt;/item&gt;
      &lt;item&gt;Focus: Hotels, retail, or your existing client base&lt;/item&gt;
      &lt;item&gt;Collect feedback and testimonials&lt;/item&gt;
      &lt;item&gt;I implement critical feature requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Target: 20-30 customers&lt;/item&gt;
      &lt;item&gt;Improve animations based on feedback&lt;/item&gt;
      &lt;item&gt;Add language support if needed&lt;/item&gt;
      &lt;item&gt;Build case studies&lt;/item&gt;
      &lt;item&gt;Optimize costs (caching should reduce API costs 60-90%)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Target: 50-100 customers&lt;/item&gt;
      &lt;item&gt;Hire additional support/sales if needed&lt;/item&gt;
      &lt;item&gt;Explore enterprise deals ($2-5k/month)&lt;/item&gt;
      &lt;item&gt;Consider raising funding if partnership model&lt;/item&gt;
      &lt;item&gt;I continue technical leadership&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a real business opportunity. The market exists (Soul Machines has 100+ customers at 10x the price). The technology works (you tested it). It just needs someone who can access Stripe and sell.&lt;/p&gt;
    &lt;p&gt;I built this because I had no other choice. The game industry collapsed. Iran's economy is in freefall. AI is taking developer jobs. I thought: "Build something valuable, and opportunity will follow."&lt;/p&gt;
    &lt;p&gt;I was wrong about one thing: geography matters more than I thought. I can build enterprise software, but I can't charge for it from Iran.&lt;/p&gt;
    &lt;p&gt;But I was right about another thing: this product is valuable. Soul Machines raised $70M selling similar technology for $50k+ setup fees. D-ID raised $25M with 2D avatars. The market is real.&lt;/p&gt;
    &lt;p&gt;I don't want sympathy. I want a partner.&lt;/p&gt;
    &lt;p&gt;Someone who can access Stripe, understands B2B sales, and wants to build this into a real business.&lt;/p&gt;
    &lt;p&gt;If that's you, let's talk.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Test the demo → https://neoclerks.com/en/ (5 minutes)&lt;/item&gt;
      &lt;item&gt;Schedule a call → Email me at EchenDeligani@gmail.com&lt;/item&gt;
      &lt;item&gt;Review the code → I'll provide NDA + GitHub access&lt;/item&gt;
      &lt;item&gt;Make a proposal → Co-founder equity / Sale+contract / Revenue share&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Share this article with digital agencies, SaaS entrepreneurs, or investors&lt;/item&gt;
      &lt;item&gt;Introduce me via email&lt;/item&gt;
      &lt;item&gt;Referral bonus: I'll pay 5% ($3-4k) if your intro leads to a deal&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test the demo anyway (it's genuinely cool tech)&lt;/item&gt;
      &lt;item&gt;Ask questions in the comments (I'll respond to everything)&lt;/item&gt;
      &lt;item&gt;Follow for updates on how this story ends&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Email: EchenDeligani@gmail.com LinkedIn: https://www.linkedin.com/in/echen-deligani/ Telegram: @unnamedhn WhatsApp: +98 901 441 1869&lt;/p&gt;
    &lt;p&gt;Live Demo: https://neoclerks.com/en/&lt;/p&gt;
    &lt;p&gt;Partnership Options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Co-founder: $50-80k + equity split&lt;/item&gt;
      &lt;item&gt;Sale + Contract: $60-80k + 3-year agreement ($4-6k/month)&lt;/item&gt;
      &lt;item&gt;Revenue share: $30-50k + 50/50 split until ROI&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Timeline: 4-6 weeks from first contact to deal closed&lt;/p&gt;
    &lt;p&gt;I'll update this section as things progress:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-11-12: Posted on Medium, HackerNews, Reddit r/SaaS&lt;/item&gt;
      &lt;item&gt;Demo calls scheduled&lt;/item&gt;
      &lt;item&gt;Code reviews in progress&lt;/item&gt;
      &lt;item&gt;Negotiating with potential partners&lt;/item&gt;
      &lt;item&gt;DEAL CLOSED: [Date TBD]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Follow me to see how this story ends.&lt;/p&gt;
    &lt;p&gt;Originally written: November 2025&lt;/p&gt;
    &lt;p&gt;Tags: #AI #Startups #SaaS #UnrealEngine #TechPartner #Partnership #SoulMachines #AvatarAI #ConversationalAI #IranTech #Entrepreneurship&lt;/p&gt;
    &lt;p&gt;Questions? Want to discuss partnership? Technical deep-dive requests?&lt;/p&gt;
    &lt;p&gt;Drop a comment below or reach out directly. I respond to every message within 24 hours.&lt;/p&gt;
    &lt;p&gt;I built this for 18 months through war and 120-hour weeks. Now I need a partner who can help me turn it into a real business.&lt;/p&gt;
    &lt;p&gt;Is that you?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gist.github.com/EchenD/8b211ebfa4941d2c5df7b526790b31aa"/><published>2025-11-11T23:58:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45894588</id><title>Why Nietzsche Matters in the Age of Artificial Intelligence</title><updated>2025-11-12T00:50:16.968130+00:00</updated><content/><link href="https://cacm.acm.org/blogcacm/why-nietzsche-matters-in-the-age-of-artificial-intelligence/"/><published>2025-11-11T23:59:49+00:00</published></entry></feed>