<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-02-03T18:40:42.697072+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46870015</id><title>Bunny Database</title><updated>2026-02-03T18:40:50.586729+00:00</updated><content>&lt;doc fingerprint="a71f183a5d20d688"&gt;
  &lt;main&gt;
    &lt;p&gt;Donât want to babysit your app database on a VM but not willing to pay the DBaaS tax either? We're building a third way.&lt;/p&gt;
    &lt;p&gt;Today, weâre launching Bunny Database as a public preview: a SQLite-compatible managed service that spins down when idle, keeps latency low wherever your users are, and doesnât cost a fortune.&lt;/p&gt;
    &lt;head rend="h2"&gt;So whatâs the deal with database services in 2026?&lt;/head&gt;
    &lt;p&gt;Itâs become clear by now that the DBaaS platforms that garnered the love of so many devs are all going upmarket. Removing or dumbing down free tiers, charging for unused capacity, charging extra for small features, or bundling them in higher tiers â you already know the drill.&lt;/p&gt;
    &lt;p&gt;Hard to blame anyone for growing their business, but it doesnât feel right when these services stop making sense for the very people who helped popularize them in the first place.&lt;/p&gt;
    &lt;p&gt;So where does that leave you?&lt;/p&gt;
    &lt;head rend="h2"&gt;Like SQLite, but for the web&lt;/head&gt;
    &lt;p&gt;Not every project needs Postgres, and thatâs okay. Sometimes you just want a simple, reliable database that you can spin up quickly and build on, without worrying itâll hit your wallet like an EC2.&lt;/p&gt;
    &lt;p&gt;Thatâs what we built Bunny Database for.&lt;/p&gt;
    &lt;p&gt;What you get:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One-click deployment: just name your database and go, no config needed&lt;/item&gt;
      &lt;item&gt;Language-specific tooling: SDKs for TS/JS, Go, Rust, and .NET help you handle the boring bits&lt;/item&gt;
      &lt;item&gt;Low latency anywhere: replication regions let you serve reads close to your users&lt;/item&gt;
      &lt;item&gt;41 regions worldwide: choose between automatic, single-region, and multi-region deployment&lt;/item&gt;
      &lt;item&gt;Works over HTTP: wire up anything youâd like&lt;/item&gt;
      &lt;item&gt;Database editor: insert data or run queries on the spot&lt;/item&gt;
      &lt;item&gt;Metrics: instant visibility into reads, writes, storage, and latency&lt;/item&gt;
      &lt;item&gt;Affordable, pay-as-you-go pricing: only pay for what you use, but without the serverless tax&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Get the full tour including how to connect Bunny Database to your app in this quick demo from our DX Engineer, Jamie Barton:&lt;/p&gt;
    &lt;head rend="h2"&gt;Why care about database latency anyway?&lt;/head&gt;
    &lt;p&gt;You probably optimize the heck out of your frontend, APIs, and caching layers, all for the sake of delivering an experience that feels instant to your users. But when your database is far away from them, round-trip time starts to add noticeable latency.&lt;/p&gt;
    &lt;p&gt;The usual fix is to introduce more caching layers, denormalized reads, or other workarounds. Thatâs obviously no fun.&lt;/p&gt;
    &lt;p&gt;And when you think about it, devs end up doing this because the popular DBaaS platforms are usually either limited, complex, or too costly when it comes to multi-region deployments. So what looks like a caching problem is actually a data locality issue.&lt;/p&gt;
    &lt;p&gt;OK, but how bad can it really be?&lt;/p&gt;
    &lt;p&gt;To find out, we ran a read latency benchmark and measured p95 latency in Bunny Database.&lt;/p&gt;
    &lt;p&gt;We picked a number of regions across the world and compared round-trip time for client locations ever farther away from the database in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a single-region setup,&lt;/item&gt;
      &lt;item&gt;with replication regions enabled.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Turns out serving reads close to clients reduced latency by up to 99%.&lt;/p&gt;
    &lt;p&gt;Check out the full write-up on the benchmark setup and results here.&lt;/p&gt;
    &lt;p&gt;While this definitely matters most to apps with global users, data locality does apply to everyone. With Bunny Database, you donât have to stick to major data center locations and compensate with caching workarounds any more. Instead, you get a lot of flexibility to set up regions in an intuitive interface and itâs easy to switch things up as your requirements change.&lt;/p&gt;
    &lt;p&gt;Choose between 3 deployment types when creating a database:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatic region selection gives you one-click deployment with minimal latency. Bunny Database will select regions for you based on your IP address (you can check and tweak the selection in settings later).&lt;/item&gt;
      &lt;item&gt;Single-region deployment lets you pick one of 41 regions available worldwide (check the full list here).&lt;/item&gt;
      &lt;item&gt;Manual region selection gives you custom multi-region setup, where you can freely pick regions that make the most sense for your audience.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of this lets you start wherever youâd like and add regions as needed, without re-architecting your app.&lt;/p&gt;
    &lt;head rend="h2"&gt;Usage-based pricing, but without the serverless tax&lt;/head&gt;
    &lt;p&gt;In the database world, capacity-based pricing gives you some predictability. But no one likes to pay for unused capacity, right?&lt;/p&gt;
    &lt;p&gt;Serverless, on the other hand, is supposed to be cost-efficient, yet can rack up bills quickly, especially when the DBaaS charges significant markups on top of already pricey compute.&lt;/p&gt;
    &lt;p&gt;We donât do hyperscalers, though, so we can charge a fair price for Bunny Database in a usage-based model.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reads: $0.30 per billion rows&lt;/item&gt;
      &lt;item&gt;Writes: $0.30 per million rows&lt;/item&gt;
      &lt;item&gt;Storage: $0.10 per GB per active region (monthly)&lt;/item&gt;
      &lt;item&gt;When not getting requests, Bunny Database only incurs storage costs. One primary region is charged continuously, while read replicas only add storage costs when serving traffic (metered by the hour)&lt;/item&gt;
      &lt;item&gt;Your usage is charged continuously (pay-as-you-go) and invoiced monthly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;During the public preview phase, Bunny Database is free.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wait, what does âSQLite-compatibleâ actually mean?&lt;/head&gt;
    &lt;p&gt;Bunny Database wouldnât be possible without libSQL, the open-source, open-contribution fork of SQLite created by Turso.&lt;/p&gt;
    &lt;p&gt;We run Bunny Database on our own fork of libSQL, which gives us the freedom to integrate it tightly with the bunny.net platform and handle the infrastructure and orchestration needed to run it as a managed, multi-region service.&lt;/p&gt;
    &lt;p&gt;What does this mean for Bunny Databaseâs upstream feature parity with libSQL and SQLite, respectively?&lt;/p&gt;
    &lt;p&gt;The short answer is that we donât currently promise automatic or complete feature parity with either upstream libSQL or the latest SQLite releases.&lt;/p&gt;
    &lt;p&gt;While libSQL aims to stay compatible with SQLiteâs API and file format, it doesnât move in lockstep with upstream SQLite. We wouldnât expect otherwise, especially as Turso has shifted focus from libSQL toward a long-term rewrite of SQLite in Rust.&lt;/p&gt;
    &lt;p&gt;For Bunny Database, this means that compatibility today is defined by the libSQL version weâre built on, rather than by chasing every upstream SQLite or libSQL change as it lands. We havenât pulled in any upstream changes yet, and we donât currently treat upstream parity as an automatic goal.&lt;/p&gt;
    &lt;p&gt;Thatâs intentional. Our focus so far has been on making Bunny Database reliable and easy to operate as a service. We think bringing in upstream changes only makes sense when they clearly improve real-world use cases, not just to tick a parity checkbox.&lt;/p&gt;
    &lt;p&gt;If there are specific libSQL features youâd like to see exposed in Bunny Database, or recent SQLite features youâd want us to pull in, weâd love to hear about it. Join our Discord to discuss your use cases and help shape the roadmap!&lt;/p&gt;
    &lt;head rend="h2"&gt;Whatâs ahead for Bunny Database&lt;/head&gt;
    &lt;p&gt;Speaking of the roadmap, we donât stop cooking. Hereâs whatâs coming up next:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatic backups&lt;/item&gt;
      &lt;item&gt;Database file import/export&lt;/item&gt;
      &lt;item&gt;Auto-generated, schema-aware API with type-safe SDKs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thereâs even more to come, but itâs too soon to spill the beans yet, especially while weâre in public preview. Weâd love to hear your feedback, so we can shape what ships next together.&lt;/p&gt;
    &lt;head rend="h2"&gt;More tools to build with&lt;/head&gt;
    &lt;p&gt;Bunny Database works standalone and fits right into your stack via the SDKs (or you can hook up anything using the HTTP API). But it also plays nicely with Bunny Edge Scripting and Bunny Magic Containers.&lt;/p&gt;
    &lt;p&gt;To connect your database to an Edge Script or a Magic Containers app, simply go to the Access tab of the chosen database and click Generate Tokens to create new access credentials for it.&lt;/p&gt;
    &lt;p&gt;Once theyâre generated, youâll get two paths to choose from:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Click Add Secrets to an Edge Script and select the one youâd like to connect from the list. Youâll also need to import the libSQL TypeScript client and use the provided code snippet to connect it to your database.&lt;/item&gt;
      &lt;item&gt;Click Add Secrets to Magic Container App and select the one youâd like to connect from the list. Youâll also need to connect to the database from your app using one of the client libraries or the HTTP API.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After you complete the setup, the database URL and access token will be available as environment variables in your script or app. Use them to connect to your database:&lt;/p&gt;
    &lt;code&gt;import&lt;/code&gt;
    &lt;code&gt;{&lt;/code&gt;
    &lt;code&gt;createClient&lt;/code&gt;
    &lt;code&gt;}&lt;/code&gt;
    &lt;code&gt;from&lt;/code&gt;
    &lt;code&gt;"@libsql/client/web"&lt;/code&gt;
    &lt;code&gt;;&lt;/code&gt;
    &lt;code&gt;const&lt;/code&gt;
    &lt;code&gt;client&lt;/code&gt;
    &lt;code&gt;=&lt;/code&gt;
    &lt;code&gt;createClient&lt;/code&gt;
    &lt;code&gt;({&lt;/code&gt;
    &lt;code&gt;url&lt;/code&gt;
    &lt;code&gt;:&lt;/code&gt;
    &lt;code&gt;process.env.&lt;/code&gt;
    &lt;code&gt;DB_URL&lt;/code&gt;
    &lt;code&gt;,&lt;/code&gt;
    &lt;code&gt;authToken&lt;/code&gt;
    &lt;code&gt;:&lt;/code&gt;
    &lt;code&gt;process.env.&lt;/code&gt;
    &lt;code&gt;DB_TOKEN&lt;/code&gt;
    &lt;code&gt;});&lt;/code&gt;
    &lt;code&gt;const&lt;/code&gt;
    &lt;code&gt;result&lt;/code&gt;
    &lt;code&gt;=&lt;/code&gt;
    &lt;code&gt;client&lt;/code&gt;
    &lt;code&gt;.&lt;/code&gt;
    &lt;code&gt;execute&lt;/code&gt;
    &lt;code&gt;(&lt;/code&gt;
    &lt;code&gt;"SELECT * FROM users"&lt;/code&gt;
    &lt;code&gt;);&lt;/code&gt;
    &lt;p&gt;You can find more detailed, step-by-step integration instructions in the docs:&lt;/p&gt;
    &lt;head rend="h2"&gt;Hop on board&lt;/head&gt;
    &lt;p&gt;We canât wait to see what youâll build with Bunny Database and what you think of it. During the public preview phase, you get 50 databases per user account, each capped at 1 GB, but we hope this should be more than enough for lots of fun projects.&lt;/p&gt;
    &lt;p&gt;Just sign in to the bunny.net dashboard to get started. Happy building!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bunny.net/blog/meet-bunny-database-the-sql-service-that-just-works/"/><published>2026-02-03T12:13:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46871173</id><title>Agent Skills</title><updated>2026-02-03T18:40:50.328527+00:00</updated><content>&lt;doc fingerprint="8f77afff0b4fb448"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;Why Agent Skills?&lt;/head&gt;Agents are increasingly capable, but often don’t have the context they need to do real work reliably. Skills solve this by giving agents access to procedural knowledge and company-, team-, and user-specific context they can load on demand. Agents with access to a set of skills can extend their capabilities based on the task they’re working on. For skill authors: Build capabilities once and deploy them across multiple agent products. For compatible agents: Support for skills lets end users give agents new capabilities out of the box. For teams and enterprises: Capture organizational knowledge in portable, version-controlled packages.&lt;head rend="h2"&gt;What can Agent Skills enable?&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Domain expertise: Package specialized knowledge into reusable instructions, from legal review processes to data analysis pipelines.&lt;/item&gt;&lt;item&gt;New capabilities: Give agents new capabilities (e.g. creating presentations, building MCP servers, analyzing datasets).&lt;/item&gt;&lt;item&gt;Repeatable workflows: Turn multi-step tasks into consistent and auditable workflows.&lt;/item&gt;&lt;item&gt;Interoperability: Reuse the same skill across different skills-compatible agent products.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://agentskills.io/home"/><published>2026-02-03T14:09:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46871387</id><title>Show HN: Sandboxing untrusted code using WebAssembly</title><updated>2026-02-03T18:40:49.720658+00:00</updated><content>&lt;doc fingerprint="c8903e58cf8302c2"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;Capsule&lt;/code&gt; is a runtime for coordinating AI agent tasks in isolated environments. It is designed to handle, long-running workflows, large-scale processing, autonomous decision-making securely, or even multi-agent systems.&lt;/p&gt;
    &lt;p&gt;Each task runs inside its own WebAssembly sandbox, providing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Isolated execution: Each task runs isolated from your host system&lt;/item&gt;
      &lt;item&gt;Resource limits: Set CPU, memory, and timeout limits per task&lt;/item&gt;
      &lt;item&gt;Automatic retries: Handle failures without manual intervention&lt;/item&gt;
      &lt;item&gt;Lifecycle tracking: Monitor which tasks are running, completed, or failed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This enables safe task-level execution of untrusted code within AI agent systems.&lt;/p&gt;
    &lt;p&gt;Simply annotate your Python functions with the &lt;code&gt;@task&lt;/code&gt; decorator:&lt;/p&gt;
    &lt;code&gt;from capsule import task

@task(name="analyze_data", compute="MEDIUM", ram="512MB", timeout="30s", max_retries=1)
def analyze_data(dataset: list) -&amp;gt; dict:
    """Process data in an isolated, resource-controlled environment."""
    # Your code runs safely in a Wasm sandbox
    return {"processed": len(dataset), "status": "complete"}&lt;/code&gt;
    &lt;p&gt;For TypeScript and JavaScript, use the &lt;code&gt;task()&lt;/code&gt; wrapper function with full access to the npm ecosystem:&lt;/p&gt;
    &lt;code&gt;import { task } from "@capsule-run/sdk";

export const analyzeData = task({
  name: "analyze_data",
  compute: "MEDIUM",
  ram: "512MB",
  timeout: "30s",
  maxRetries: 1
}, (dataset: number[]): object =&amp;gt; {
  // Your code runs safely in a Wasm sandbox
  return { processed: dataset.length, status: "complete" };
});

// The "main" task is required as the entrypoint
export const main = task({
    name: "main",
    compute: "HIGH"
}, () =&amp;gt; {
  return analyzeData([1, 2, 3, 4, 5]);
});&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;The runtime requires a task named &lt;code&gt;"main"&lt;/code&gt; as the entry point. Python can define the main task itself, but it's recommended to set it manually.&lt;/p&gt;
    &lt;p&gt;When you run &lt;code&gt;capsule run main.py&lt;/code&gt; (or &lt;code&gt;main.ts&lt;/code&gt;), your code is compiled into a WebAssembly module and executed in a dedicated sandbox to isolate tasks.&lt;/p&gt;
    &lt;p&gt;Each task operates within its own sandbox with configurable resource limits, ensuring that failures are contained and don't cascade to other parts of your workflow. The host system controls every aspect of execution, from CPU allocation via Wasm fuel metering to memory constraints and timeout enforcement.&lt;/p&gt;
    &lt;p&gt;Every task returns a structured JSON envelope containing both the result and execution metadata:&lt;/p&gt;
    &lt;code&gt;{
  "success": true,
  "result": "Hello from Capsule!",
  "error": null,
  "execution": {
    "task_name": "data_processor",
    "duration_ms": 1523,
    "retries": 0,
    "fuel_consumed": 45000
  }
}&lt;/code&gt;
    &lt;p&gt;Response fields:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;success&lt;/code&gt;— Boolean indicating whether the task completed successfully&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;result&lt;/code&gt;— The actual return value from your task (json, string, null on failure etc..)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;error&lt;/code&gt;— Error details if the task failed (&lt;code&gt;{ error_type: string, message: string }&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;execution&lt;/code&gt;— Performance metrics:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;task_name&lt;/code&gt;— Name of the executed task&lt;/item&gt;&lt;item&gt;&lt;code&gt;duration_ms&lt;/code&gt;— Execution time in milliseconds&lt;/item&gt;&lt;item&gt;&lt;code&gt;retries&lt;/code&gt;— Number of retry attempts that occurred&lt;/item&gt;&lt;item&gt;&lt;code&gt;fuel_consumed&lt;/code&gt;— CPU resources used (see Compute Levels)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install capsule-run&lt;/code&gt;
    &lt;p&gt;Create &lt;code&gt;hello.py&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;from capsule import task

@task(name="main", compute="LOW", ram="64MB")
def main() -&amp;gt; str:
    return "Hello from Capsule!"&lt;/code&gt;
    &lt;p&gt;Run it:&lt;/p&gt;
    &lt;code&gt;capsule run hello.py&lt;/code&gt;
    &lt;code&gt;npm install -g @capsule-run/cli
npm install @capsule-run/sdk&lt;/code&gt;
    &lt;p&gt;Create &lt;code&gt;hello.ts&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import { task } from "@capsule-run/sdk";

export const main = task({
  name: "main",
  compute: "LOW",
  ram: "64MB"
}, (): string =&amp;gt; {
  return "Hello from Capsule!";
});&lt;/code&gt;
    &lt;p&gt;Run it:&lt;/p&gt;
    &lt;code&gt;capsule run hello.ts&lt;/code&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;--verbose&lt;/code&gt; to display real-time task execution details.&lt;/p&gt;
    &lt;p&gt;Configure your tasks with these parameters:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;name&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Task identifier&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;function name (Python) / required (TS)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"process_data"&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;compute&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;CPU allocation level: &lt;code&gt;"LOW"&lt;/code&gt;, &lt;code&gt;"MEDIUM"&lt;/code&gt;, or &lt;code&gt;"HIGH"&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"MEDIUM"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"HIGH"&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;ram&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Memory limit for the task&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;unlimited&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;"512MB"&lt;/code&gt;, &lt;code&gt;"2GB"&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;code&gt;timeout&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Maximum execution time&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;str&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;unlimited&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;"30s"&lt;/code&gt;, &lt;code&gt;"5m"&lt;/code&gt;, &lt;code&gt;"1h"&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;&lt;code&gt;max_retries&lt;/code&gt; / &lt;code&gt;maxRetries&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Number of retry attempts on failure&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;int&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;0&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;3&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;&lt;code&gt;allowed_files&lt;/code&gt; / &lt;code&gt;allowedFiles&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Folders accessible in the sandbox&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;list&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;[]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["./data", "./output"]&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;env_variables&lt;/code&gt; / &lt;code&gt;envVariables&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Environment variables accessible in the sandbox&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;list&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;[]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;["API_KEY"]&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Capsule controls CPU usage through WebAssembly's fuel mechanism, which meters instruction execution. The compute level determines how much fuel your task receives.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LOW provides minimal allocation for lightweight tasks&lt;/item&gt;
      &lt;item&gt;MEDIUM offers balanced resources for typical workloads&lt;/item&gt;
      &lt;item&gt;HIGH grants maximum fuel for compute-intensive operations&lt;/item&gt;
      &lt;item&gt;CUSTOM to specify an exact fuel value (e.g., &lt;code&gt;compute="1000000"&lt;/code&gt;) for precise control over execution limits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can create a &lt;code&gt;capsule.toml&lt;/code&gt; file in your project root to set default options for all tasks and define workflow metadata:&lt;/p&gt;
    &lt;code&gt;# capsule.toml

[workflow]
name = "My AI Workflow"
version = "1.0.0"
entrypoint = "src/main.py"  # Default file when running `capsule run`

[tasks]
default_compute = "MEDIUM"
default_ram = "256MB"
default_timeout = "30s"
default_max_retries = 2&lt;/code&gt;
    &lt;p&gt;With an entrypoint defined, you can simply run:&lt;/p&gt;
    &lt;code&gt;capsule run&lt;/code&gt;
    &lt;p&gt;Task-level options always override these defaults when specified.&lt;/p&gt;
    &lt;p&gt;The standard Python &lt;code&gt;requests&lt;/code&gt; library and socket-based networking aren't natively compatible with WebAssembly's sandboxed I/O model. Capsule provides its own HTTP client that works within the Wasm environment:&lt;/p&gt;
    &lt;code&gt;from capsule import task
from capsule.http import get, post, put, delete

@task(name="http_example", compute="MEDIUM", timeout="30s")
def main() -&amp;gt; dict:
    """Example demonstrating HTTP client usage within a task."""

    # GET request
    response = get("https://api.example.com/data")

    # POST with JSON body
    response = post("https://api.example.com/submit", json={"key": "value"})

    # Response methods
    is_ok = response.ok()           # Returns True if status code is 2xx
    status = response.status_code    # Get the HTTP status code
    data = response.json()           # Parse response as JSON
    text = response.text()           # Get response as text

    return {"status": status, "success": is_ok}&lt;/code&gt;
    &lt;p&gt;Standard libraries like &lt;code&gt;fetch&lt;/code&gt; are already compatible, so no custom HTTP client is needed for TypeScript/JavaScript.&lt;/p&gt;
    &lt;code&gt;import { task } from "@capsule-run/sdk";

export const main = task({
    name: "main",
    compute: "MEDIUM"
}, async () =&amp;gt; {
    const response = await fetch("https://api.example.com/data");
    return response.json();
});&lt;/code&gt;
    &lt;p&gt;Tasks can read and write files within directories specified in &lt;code&gt;allowed_files&lt;/code&gt;. Any attempt to access files outside these directories is not possible.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Currently, &lt;code&gt;allowed_files&lt;/code&gt; supports directory paths, not individual files.&lt;/p&gt;
    &lt;p&gt;Python's standard file operations work normally. Use &lt;code&gt;open()&lt;/code&gt;, &lt;code&gt;os&lt;/code&gt;, &lt;code&gt;pathlib&lt;/code&gt;, or any file manipulation library.&lt;/p&gt;
    &lt;code&gt;from capsule import task

@task(name="restricted_writer", allowed_files=["./output"])
def restricted_writer() -&amp;gt; None:
    with open("./output/result.txt", "w") as f:
        f.write("result")

@task(name="main")
def main() -&amp;gt; str:
    restricted_writer()&lt;/code&gt;
    &lt;p&gt;Common Node.js built-ins are available. Use the standard &lt;code&gt;fs&lt;/code&gt; module:&lt;/p&gt;
    &lt;code&gt;import { task } from "@capsule-run/sdk";
import fs from "fs/promises";

export const restrictedWriter = task({
    name: "restricted_writer",
    allowedFiles: ["./output"]
}, async () =&amp;gt; {
    await fs.writeFile("./output/result.txt", "result");
});

export const main = task({ name: "main", allowedFiles: ["./data"] }, async () =&amp;gt; {
    await restrictedWriter();
    return await fs.readFile("./data/input.txt", "utf8");
});&lt;/code&gt;
    &lt;p&gt;Tasks can access environment variables to read configuration, API keys, or other runtime settings.&lt;/p&gt;
    &lt;p&gt;Use Python's standard &lt;code&gt;os.environ&lt;/code&gt; to access environment variables:&lt;/p&gt;
    &lt;code&gt;from capsule import task
import os

@task(name="main", env_variables=["API_KEY"])
def main() -&amp;gt; dict:
    api_key = os.environ.get("API_KEY")
    return {"api_key": api_key}&lt;/code&gt;
    &lt;p&gt;Use the standard &lt;code&gt;process.env&lt;/code&gt; to access environment variables:&lt;/p&gt;
    &lt;code&gt;import { task } from "@capsule-run/sdk";

export const main = task({
    name: "main",
    envVariables: ["API_KEY"]
}, () =&amp;gt; {
    const apiKey = process.env.API_KEY;
    return { apiKeySet: apiKey !== undefined };
});&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;TypeScript/JavaScript has broader compatibility than Python since it doesn't rely on native bindings.&lt;/p&gt;
    &lt;p&gt;Python: Pure Python packages and standard library modules work. Packages with C extensions (&lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;) are not yet supported.&lt;/p&gt;
    &lt;p&gt;TypeScript/JavaScript: npm packages and ES modules work. Common Node.js built-ins are available. If you have any trouble with a built-in do not hesitate to open an issue.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome!&lt;/p&gt;
    &lt;p&gt;Prerequisites: Rust (latest stable), Python 3.13+, Node.js 22+&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/mavdol/capsule.git
cd capsule

# Build and install CLI
cargo install --path crates/capsule-cli

# Python SDK (editable install)
pip install -e crates/capsule-sdk/python

# TypeScript SDK (link for local dev)
cd crates/capsule-sdk/javascript
npm install &amp;amp;&amp;amp; npm run build &amp;amp;&amp;amp; npm link

# Then in your project: npm link @capsule-run/sdk&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repository&lt;/item&gt;
      &lt;item&gt;Create a feature branch: &lt;code&gt;git checkout -b feature/amazing-feature&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Run tests: &lt;code&gt;cargo test&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Open a Pull Request&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Need help? Open an issue&lt;/p&gt;
    &lt;p&gt;Capsule builds on these open source projects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;componentize-py – Python to WebAssembly Component compilation&lt;/item&gt;
      &lt;item&gt;jco – JavaScript toolchain for WebAssembly Components&lt;/item&gt;
      &lt;item&gt;wasmtime – WebAssembly runtime&lt;/item&gt;
      &lt;item&gt;WASI – WebAssembly System Interface&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the Apache License 2.0 - see the LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/mavdol/capsule"/><published>2026-02-03T14:28:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46872190</id><title>Data Brokers Can Fuel Violence Against Public Servants</title><updated>2026-02-03T18:40:49.405720+00:00</updated><content>&lt;doc fingerprint="e29891d9dc839717"&gt;
  &lt;main&gt;
    &lt;p&gt;A new report published Tuesday finds that while violent threats to public servants across the US have been increasing, “comprehensive” state-level consumer privacy laws do not provide adequate protections for those civil servants, creating a “data-to-violence pipeline.”&lt;/p&gt;
    &lt;p&gt;The report was published by researcher Justin Sherman of the Security Project at the Public Service Alliance, a platform that provides free and discounted security services to current and former public servants. While Trump officials have referred to documenting federal immigration agents’ behavior on the job as “violence” and “doxing,” Sherman says the report focuses on the more traditional, widely accepted definition—the publication of someone’s personal, private information, such as their home address, with the specific intent of harming them.&lt;/p&gt;
    &lt;p&gt;Sherman analyzed 19 different consumer privacy laws and found that while they all give consumers the right to stop data brokers from selling personal information obtained from private sources, none give “public servants the right to legally compel state agencies to redact their personal data from public records,” and none prevent data brokers from selling data, including people’s home addresses, when they are obtained through public sources such as property records or court filings. Further, none include what is called a “private right of action,” which would allow individuals to sue over violations of their respective state’s privacy law.&lt;/p&gt;
    &lt;p&gt;Together, this means that information about public employees is uniquely available and that they have uniquely few ways to prevent its dissemination.&lt;/p&gt;
    &lt;p&gt;Violent threats against public servants have been increasing, according to a separate analysis by PSA and the Impact Project of over 1,600 individual threats made against public servants between 2015 and 2025. That analysis found that violent threats against local public servants, including school board members and election workers, represented nearly a third of the reports reviewed. It also found that threatening statements occurred at nearly nine times the rate of physical attacks, and that one form of threat can escalate into another.&lt;/p&gt;
    &lt;p&gt;A 2024 report from the Brennan Center for Justice found that larger shares of women and Democrats reported increases in the severity of abuse since first taking public office, compared with men and Republicans.&lt;/p&gt;
    &lt;p&gt;Last year, a 57-year-old man was charged with assassinating Melissa Hortman, a Democratic state representative, along with her husband at their home in Minnesota. According to court records, the alleged shooter had handwritten lists of dozens of Minnesota state and federal public officials, including Hortman’s name and her home address, along with 11 “people search engines” that allow anyone to find personal information about a person, including their home addresses, phone numbers, and names of relatives, often for a nominal fee.&lt;/p&gt;
    &lt;p&gt;The report advocates for legislation that would specifically address privacy concerns for all public servants, including public school educators and local elected officials, who are not necessarily covered by existing federal or state privacy laws. It suggests that lawmakers could try to balance First Amendment and privacy concerns by regulating the digitization of public records and how easy they are to access remotely, instead of limiting them completely.&lt;/p&gt;
    &lt;p&gt;Sherman, the author of the new report, said that while many public records can be useful to journalists and accountability watchdogs, repackaged public records sold by data brokers can make it too easy for abusive individuals to stalk and harass victims even when they move to a different state. In the past, people seeking out public records would already have to have an idea of where that public record was, and physically go to that location.&lt;/p&gt;
    &lt;p&gt;While residents of states with consumer privacy laws can request limits on data collected from private sources, it’s not always easy to do so. Only one state, California, offers a way for residents to limit what information data brokers collect and sell about them en masse and for free via its Delete Request and Opt-out Platform. People in other states, including public servants, must file deletion requests manually or pay for an opt-out service that promises to do so on their behalf.&lt;/p&gt;
    &lt;p&gt;Last year, dozens of data brokers were caught hiding data removal instructions from Google, making it difficult for consumers to find them. Even when consumers pay to use services that specialize in filing data deletion requests, the results aren’t perfect. In 2024, Consumer Reports studied the effectiveness of seven different data removal services, which ranged from $19.99 to $249 a year in cost, and found that at best they were only successful about two-thirds of the time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wired.com/story/how-data-brokers-can-fuel-violence-against-public-servants/"/><published>2026-02-03T15:28:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46872238</id><title>The next steps for Airbus' big bet on open rotor engines</title><updated>2026-02-03T18:40:49.127618+00:00</updated><content>&lt;doc fingerprint="73d2c076ef71b117"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Stay Up to Date&lt;/head&gt;
    &lt;p&gt;Submit your email address to receive the latest industry and Aerospace America news.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tests are underway on a design that could improve fuel efficiency by 20%&lt;/head&gt;
    &lt;p&gt;The development of a new single-aisle aircraft is always met with fanfare, particularly if there’s novel technology aboard. For the airliner that Airbus is planning to debut in the 2030s, that tech could be an engine that represents a huge leap forward for air travel.&lt;/p&gt;
    &lt;p&gt;Turbofans have become the standard in modern aviation due to better fuel efficiency and reduced noise, but open rotor could yield even bigger improvements. In this design, the engine’s larger rotor blades are exposed instead of being contained in a nacelle, which allows more air to flow through the turbine — ultimately reducing the amount of fuel required.&lt;/p&gt;
    &lt;p&gt;Airbus in March announced plans to assess the feasibility of an open rotor engine (a term the company uses interchangeably with “open fan”) with CFM International, the joint engine venture of French firm Safran and U.S.-based GE Aerospace. GE has been studying open rotor technology since 2021 under its RISE program. To meet Airbus’ late 2030s target for the airliner’s debut, a decision about the engine would need to be made by the end of this decade, when plans call for conducting flight tests with a modified A380 test aircraft.&lt;/p&gt;
    &lt;p&gt;Why not just continue designing turbofans with bigger blades? The industry might have gone as far as it can in that direction, according to GE’s Mohamed Ali, chief technology and operations officer. He discussed the decision to explore open rotor in March during the Airbus Innovation Summit in Toulouse, France.&lt;/p&gt;
    &lt;p&gt;While larger and larger fan blades have helped reduce fuel consumption over the years, the duct around a conventional jet engine produces drag that “eats at the benefits that are coming from the fuel burn improvements,” he explained. “And there is a point that is not very far away from where we are today, in which that duct is so big that it wipes out any gains that are coming from making the fan bigger.”&lt;/p&gt;
    &lt;p&gt;That’s why CFM has turned to open rotor. By shrinking the engine core and constructing larger, composite blades, the engine’s bypass ratio — the amount of air passing through the fan versus through the core of the engine — would increase to 60:1, compared to the 11:1 and 12:1possible with today’s engines. If it works, such a ratio could hand Airbus a decisive advantage in engine performance in the single-aisle market, as well as make CFM the sole engine supplier for this particular aircraft.&lt;/p&gt;
    &lt;p&gt;But first things first: In the near term, Airbus and its partners must refine the open rotor tech and design to determine if it’s a good match for a passenger airliner — particularly to ensure a safe and relatively quiet experience, Karim Mokaddem, Airbus’ head of research and technology on future aircraft, told me in an interview.&lt;/p&gt;
    &lt;p&gt;A top priority is “to ensure that we are mastering and reducing the noise that is at the source of the engine, but also the way this noise is transferred into the airplane,” he said. “We are having very good improvements on that.”&lt;/p&gt;
    &lt;p&gt;CFM has allocated some 2,000 engineers for testing and development on RISE, which is studying various next-generation engine technologies beyond open rotor. So far, some 250 tests have been completed, with wind tunnel tests on subscale engine models ongoing in France and the Netherlands. These are preparation for full-scale ground tests that would be conducted later this decade at GE’s Peebles Test Operation facility in Ohio and in Villaroche, France, validation tests in Victorville, California — and, finally, flight tests in Toulouse with a fully functioning engine aboard the A380 testbed.&lt;/p&gt;
    &lt;p&gt;Also still to be determined is where exactly these engines would be located on the airframe. Airbus is considering a few placements, from the conventional under-the-wing to the rear of the aircraft over the tail. One big consideration: the engine blades are so large that an under-wing placement would require Airbus to redesign the wings to “gull around the engine,” Mokaddem said.&lt;/p&gt;
    &lt;p&gt;Airbus is also assessing shielding the area of the fuselage closest to the engines to minimize the risk of a blade off — one or more composite blades breaking, which could dent or puncture the fuselage and, in the worst-case scenario, strike a passenger. Potential solutions under consideration include the additional of material to that section of the aircraft, or constructing it of stronger material.&lt;/p&gt;
    &lt;p&gt;“Of course, everything additional that you are bringing to the aircraft needs to be compensated by something else to maintain the same performance,” Mokaddem said. So Airbus is running simulations of various scenarios “to ensure that at the end, what you are gaining on one side is not killed by something else.’’&lt;/p&gt;
    &lt;p&gt;Despite all this activity, he and other executives are keen to emphasize that the open rotor isn’t a done deal. In parallel, Airbus continues to assess alternative engine configurations with its other engine suppliers, Rolls-Royce and Pratt &amp;amp; Whitney. A Rolls-Royce spokesman told me that the company had experimented with open rotor technology years ago but is instead pursuing its UltraFan. Ground testing with the variant of the engine designed for single-aisle jets is slated for 2028. Similarly, Pratt &amp;amp; Whitney said it believes its geared turbofan design remains the right choice for the next Airbus jet.&lt;/p&gt;
    &lt;p&gt;Boeing did not respond to emailed requests for comment but has previously expressed skepticism about the feasibility of open rotor designs, with Aviation Week reporting last year that the company was leaning more toward a traditional engine for its next single-aisle.&lt;/p&gt;
    &lt;p&gt;Open rotor designs have been studied on and off since the 1940s for various commercial and NASA research aircraft, but the time might finally be right for the technology to fly aboard a commercial jet, according to long-term analyst Richard Aboulafia. He said that GE appears to have made progress with gears and the use of fixators to make the engine quieter and potentially faster.&lt;/p&gt;
    &lt;p&gt;“Technologically, the RISE open rotor demonstrator makes the older ones look coal-powered,” he said, referring to the subscale models in testing. “There’s all kinds of improvements baked into this technological approach.”&lt;/p&gt;
    &lt;p&gt;For airline customers, the choice between Airbus versus Boeing aircraft often comes down to innovations like fly-by-wire and potential engine efficiency improvements, so the decisions each company makes for their next single-aisles could influence air travel for decades to come — as well as the industry’s ongoing quest to reduce its carbon emissions.&lt;/p&gt;
    &lt;p&gt;But for now, Airbus appears to be in a strong position. “This is a company that’s overwhelmingly successful, so it’s not like they lack the revenue and it could serve to cement their very strong market position for lifetimes,” said Aboulafia.&lt;/p&gt;
    &lt;head rend="h3"&gt;About Charlotte Ryan&lt;/head&gt;
    &lt;p&gt;A London-based freelance journalist, Charlotte previously covered the aerospace industry for Bloomberg News.&lt;/p&gt;
    &lt;head rend="h4"&gt;Related Posts&lt;/head&gt;
    &lt;head rend="h3"&gt;Stay Up to Date&lt;/head&gt;
    &lt;p&gt;Submit your email address to receive the latest industry and Aerospace America news.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aerospaceamerica.aiaa.org/the-next-steps-for-airbus-big-bet-on-open-rotor-engines/"/><published>2026-02-03T15:31:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46872706</id><title>Qwen3-Coder-Next</title><updated>2026-02-03T18:40:48.579740+00:00</updated><link href="https://qwen.ai/blog?id=qwen3-coder-next"/><published>2026-02-03T16:01:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46872733</id><title>Launch HN: Modelence (YC S25) – App Builder with TypeScript / MongoDB Framework</title><updated>2026-02-03T18:40:48.192766+00:00</updated><content>&lt;doc fingerprint="bb22d77b01df145e"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi all, Aram and Eduard here - co-founders of Modelence (&lt;/p&gt;https://modelence.com&lt;p&gt;). After spending years on scaling our previous startup’s platform, we built an open-source full-stack TypeScript + MongoDB framework to stop solving the same auth / database / API / cron job implementations every time we created an app, and we didn’t like the idea of using multiple managed platforms for each of these to run our apps either.&lt;/p&gt;&lt;p&gt;(Here’s our prior Show HN post for reference: https://news.ycombinator.com/item?id=44902227)&lt;/p&gt;&lt;p&gt;At the same time, we were excited by the whole AI app builder boom and realized that the real challenge there is the platform rather than the tool itself. Now we’re making Modelence the first full-stack framework that’s built for coding agents and humans alike:&lt;/p&gt;&lt;p&gt;- TypeScript is already great for AI coding because it provides guardrails and catches many errors at build time, so agents can auto-correct&lt;/p&gt;&lt;p&gt;- MongoDB eliminates the schema management problem for agents, which is where they fail the most often otherwise (+ works great with TS/Node.js)&lt;/p&gt;&lt;p&gt;- Built-in auth, database, cron jobs and else that just works together out of the box means agents only focus on your product logic and don’t fail at trying to set these things up (+ less tokens spent on boilerplate).&lt;/p&gt;&lt;p&gt;You can now try the Modelence app builder (based on Claude Agent SDK) by just typing a prompt on our landing page ( https://modelence.com ) - watch a demo video here: https://youtu.be/BPsYvj_nGuE&lt;/p&gt;&lt;p&gt;Then you can check it out locally and continue working in your own IDE, while still using Modelence Cloud as your backend, with a dev cloud environment, and later deploy and run on Modelence Cloud with built-in observability around every operation running in your app.&lt;/p&gt;&lt;p&gt;We’re also going to add a built-in DevOps agent that lives in the same cloud, knows the framework end-to-end, and will use all this observability data to act on errors, alerts, and incidents - closing the loop, because running in production is much harder than just building.&lt;/p&gt;&lt;p&gt;We launched the app builder as a quick start for developers, to demonstrate the framework and Modelence Cloud without having to manually read docs and follow the steps to set up a new app. Our main focus is still the platform itself, since we believe the real challenge in AI coding is the framework and the platform rather than the builder tool itself.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46872733"/><published>2026-02-03T16:03:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46872818</id><title>Show HN: C discrete event SIM w stackful coroutines runs 45x faster than SimPy</title><updated>2026-02-03T18:40:47.722801+00:00</updated><content>&lt;doc fingerprint="6a388adbeae31e0c"&gt;
  &lt;main&gt;
    &lt;p&gt;A fast discrete event simulation library written in C and assembly with POSIX pthreads. Simulated processes are implemented as stackful coroutines ("fibers") inside the pthreads.&lt;/p&gt;
    &lt;p&gt;Implementation status:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;x86-64: Stable, both for Linux and Windows&lt;/item&gt;
      &lt;item&gt;Apple Silicon: Planned&lt;/item&gt;
      &lt;item&gt;ARM: Planned&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cimba models run 40-50 times faster than SimPy equivalents. The chart below shows the number of simulated events processed per second of wall clock time on a simple M/M/1 queue implemented in SimPy and Cimba. Cimba runs this scenario 45 times faster than SimPy with all CPU cores in use. Cimba runs 25 % faster (20M events/sec) on a single core than SimPy using all 64 cores (16M events/sec).&lt;/p&gt;
    &lt;p&gt;It is fast, powerful, reliable, and free.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Fast: The speed from multithreaded parallel execution translates to high resolution in your simulation modeling. You can run hundreds of replications and parameter variations in just a few seconds, generating tight confidence intervals in your experiments and a high density of data points along parameter variations.&lt;/p&gt;
        &lt;p&gt;In the benchmark shown above, Cimba reduces the run time by 97.8 % compared to the same model in SimPy using all CPU cores. This translates into doing your simulation experiments in seconds instead of minutes, or in minutes instead of hours.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Powerful: Cimba provides a comprehensive toolkit for discrete event simulation:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Processes implemented as asymmetric stackful coroutines. A simulated process can yield and resume control from any level of a function call stack, allowing well-structured coding of arbitrarily large simulation models. As a first-order object, a simulated process can be passed as an argument to other functions, returned from functions, and stored in data structures, allowing rich and complex interactions between processes.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Pre-packaged process interaction mechanisms like resources, resource pools, buffers, object queues, priority queues, and timeouts. Cimba also provides condition variables where your simulated processes can wait for arbitrarily complex conditions to become true – anything you can express as a function returning a binary true or false result.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;A wide range of fast, high-quality random number generators, both of academically important and more empirically oriented types. Important distributions like normal and exponential are implemented by state-of-the-art ziggurat rejection sampling for speed and accuracy.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Integrated logging and data collection features that make it easy to get a model running and understand what is happening inside it, including custom asserts to pinpoint sources of errors.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;As a C library, Cimba allows easy integration with other libraries and programs. You could call CUDA routines to enhance your simulation models with GPU-powered agentic behavior or drive a fancy graphics interface like a 3D visualization of a manufacturing plant. You could even call the Cimba simulation engine from other programming languages, since the C calling convention is standard and well-documented.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Reliable: Cimba is well-engineered open source. There is no mystery to the results you get. The code is written with liberal use of assertions to enforce preconditions, invariants, and postconditions in each function. The assertions act as self-enforcing documentation on expected inputs to and outputs from the Cimba functions. About 13 % of all code lines in the Cimba library are asssertions, a very high density. There are unit tests for each module. Running the unit test battery in debug mode (all assertions active) verifies the correct operation in great detail. You can do that by the one-liner&lt;/p&gt;&lt;code&gt;meson test -C build&lt;/code&gt;from the terminal command line.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Free: Cimba should fit well into the budget of most research groups.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It is a general-purpose discrete event simulation library, in the spirit of a 21st century Simula67 descendant. You can use it to model, e.g.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;computer networks,&lt;/item&gt;
      &lt;item&gt;transportation networks,&lt;/item&gt;
      &lt;item&gt;operating system task scheduling,&lt;/item&gt;
      &lt;item&gt;manufacturing systems and job shops,&lt;/item&gt;
      &lt;item&gt;military command and control systems,&lt;/item&gt;
      &lt;item&gt;hospital and emergency room patient flows,&lt;/item&gt;
      &lt;item&gt;queuing systems like bank tellers and store checkouts,&lt;/item&gt;
      &lt;item&gt;urban systems like public transport and garbage collection,&lt;/item&gt;
      &lt;item&gt;and quite a few more application domains of similar kinds, where overall system complexity arises from interactions between relatively simple components.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you look under the hood, you will also find additional reusable internal components. Cimba contains stackful coroutines doing their own thing on thread-safe cactus stacks. There are fast memory pool allocators for generic small objects and hash-heaps combining a binary heap and an open addressing hash map using Fibonacci hashing. Although not part of the public Cimba API, these components can also be used in your model if needed.&lt;/p&gt;
    &lt;p&gt;It is C11/C17. As an illustration, this is the entire code for our multithreaded M/M/1 benchmark mentioned above:&lt;/p&gt;
    &lt;code&gt;    #include &amp;lt;inttypes.h&amp;gt;
    #include &amp;lt;stdio.h&amp;gt;
    #include &amp;lt;stdint.h&amp;gt;
    
    #include &amp;lt;cimba.h&amp;gt;
    
    #define NUM_OBJECTS 1000000u
    #define ARRIVAL_RATE 0.9
    #define SERVICE_RATE 1.0
    #define NUM_TRIALS 100
    
    CMB_THREAD_LOCAL struct cmi_mempool objectpool = CMI_MEMPOOL_STATIC_INIT(8u, 512u);
    
    struct simulation {
        struct cmb_process *arrival;
        struct cmb_process *service;
        struct cmb_objectqueue *queue;
    };
    
    struct trial {
        double arr_mean;
        double srv_mean;
        uint64_t obj_cnt;
        double sum_wait;
        double avg_wait;
    };
    
    struct context {
        struct simulation *sim;
        struct trial *trl;
    };
    
    void *arrivalfunc(struct cmb_process *me, void *vctx)
    {
        cmb_unused(me);
        const struct context *ctx = vctx;
        struct cmb_objectqueue *qp = ctx-&amp;gt;sim-&amp;gt;queue;
        const double mean_hld = ctx-&amp;gt;trl-&amp;gt;arr_mean;
        for (uint64_t ui = 0; ui &amp;lt; NUM_OBJECTS; ui++) {
            const double t_hld = cmb_random_exponential(mean_hld);
            cmb_process_hold(t_hld);
            void *object = cmi_mempool_alloc(&amp;amp;objectpool);
            double *dblp = object;
            *dblp = cmb_time();
            cmb_objectqueue_put(qp, object);
        }
    
        return NULL;
    }
    
    void *servicefunc(struct cmb_process *me, void *vctx)
    {
        cmb_unused(me);
        const struct context *ctx = vctx;
        struct cmb_objectqueue *qp = ctx-&amp;gt;sim-&amp;gt;queue;
        const double mean_srv = ctx-&amp;gt;trl-&amp;gt;srv_mean;
        uint64_t *cnt = &amp;amp;(ctx-&amp;gt;trl-&amp;gt;obj_cnt);
        double *sum = &amp;amp;(ctx-&amp;gt;trl-&amp;gt;sum_wait);
        while (true) {
            void *object = NULL;
            cmb_objectqueue_get(qp, &amp;amp;object);
            const double *dblp = object;
            const double t_srv = cmb_random_exponential(mean_srv);
            cmb_process_hold(t_srv);
            const double t_sys = cmb_time() - *dblp;
            *sum += t_sys;
            *cnt += 1u;
            cmi_mempool_free(&amp;amp;objectpool, object);
        }
    }
    
    void run_trial(void *vtrl)
    {
        struct trial *trl = vtrl;
    
        cmb_logger_flags_off(CMB_LOGGER_INFO);
        cmb_random_initialize(cmb_random_hwseed());
        cmb_event_queue_initialize(0.0);
        struct context *ctx = malloc(sizeof(*ctx));
        ctx-&amp;gt;trl = trl;
        struct simulation *sim = malloc(sizeof(*sim));
        ctx-&amp;gt;sim = sim;
    
        sim-&amp;gt;queue = cmb_objectqueue_create();
        cmb_objectqueue_initialize(sim-&amp;gt;queue, "Queue", CMB_UNLIMITED);
    
        sim-&amp;gt;arrival = cmb_process_create();
        cmb_process_initialize(sim-&amp;gt;arrival, "Arrival", arrivalfunc, ctx, 0);
        cmb_process_start(sim-&amp;gt;arrival);
        sim-&amp;gt;service = cmb_process_create();
        cmb_process_initialize(sim-&amp;gt;service, "Service", servicefunc, ctx, 0);
        cmb_process_start(sim-&amp;gt;service);
    
        cmb_event_queue_execute();
    
        cmb_process_stop(sim-&amp;gt;service, NULL);
        cmb_process_terminate(sim-&amp;gt;arrival);
        cmb_process_terminate(sim-&amp;gt;service);
        cmb_process_destroy(sim-&amp;gt;arrival);
        cmb_process_destroy(sim-&amp;gt;service);
    
        cmb_objectqueue_destroy(sim-&amp;gt;queue);
        cmb_event_queue_terminate();
        free(sim);
        free(ctx);
    }
    
    int main(void)
    {
        struct trial *experiment = calloc(NUM_TRIALS, sizeof(*experiment));
        for (unsigned ui = 0; ui &amp;lt; NUM_TRIALS; ui++) {
            struct trial *trl = &amp;amp;experiment[ui];
            trl-&amp;gt;arr_mean = 1.0 / ARRIVAL_RATE;
            trl-&amp;gt;srv_mean = 1.0 / SERVICE_RATE;
            trl-&amp;gt;obj_cnt = 0u;
            trl-&amp;gt;sum_wait = 0.0;
        }
    
        cimba_run_experiment(experiment,
                             NUM_TRIALS,
                             sizeof(*experiment),
                             run_trial);
    
        struct cmb_datasummary summary;
        cmb_datasummary_initialize(&amp;amp;summary);
        for (unsigned ui = 0; ui &amp;lt; NUM_TRIALS; ui++) {
            const double avg_tsys = experiment[ui].sum_wait / (double)(experiment[ui].obj_cnt);
            cmb_datasummary_add(&amp;amp;summary, avg_tsys);
        }
    
        const unsigned un = cmb_datasummary_count(&amp;amp;summary);
        if (un &amp;gt; 1) {
            const double mean_tsys = cmb_datasummary_mean(&amp;amp;summary);
            const double sdev_tsys = cmb_datasummary_stddev(&amp;amp;summary);
            const double serr_tsys = sdev_tsys / sqrt((double)un);
            const double ci_w = 1.96 * serr_tsys;
            const double ci_l = mean_tsys - ci_w;
            const double ci_u = mean_tsys + ci_w;
    
            printf("Average system time %f (n %u, conf.int. %f - %f, expected %f)\n",
                   mean_tsys, un, ci_l, ci_u, 1.0 / (SERVICE_RATE - ARRIVAL_RATE));
    
            return 0;
        }
    }

&lt;/code&gt;
    &lt;p&gt;See our tutorial at ReadTheDocs for more usage examples.&lt;/p&gt;
    &lt;p&gt;As shown above, it is some 45 times faster than SimPy in a relevant benchmark. It means getting your results almost immediately rather than after a "go brew a pot of coffee" delay breaking your line of thought.&lt;/p&gt;
    &lt;p&gt;For another illustration of how to benefit from the sheer speed, the experiment in test_cimba.c simulates an M/G/1 queue at four different levels of service process variability. For each variability level, it tries five system utilization levels. There are ten replications for each parameter combination, in total 4 * 5 * 10 = 200 trials. Each trial lasts for one million time units, where the average service time always is 1.0 time units.&lt;/p&gt;
    &lt;p&gt;This entire simulation runs in about 1.5 seconds on an AMD Threadripper 3970X with Arch Linux and produces the chart below.&lt;/p&gt;
    &lt;p&gt;Discrete event simulation fits well with an object-oriented paradigm. That is why object-oriented programming was invented in the first place for Simula67. Since OOP is not directly enforced in plain C, we provide the object-oriented characteristics (such as encapsulation, inheritance, polymorphism, and abstraction) in the Cimba software design instead. (See the ReadTheDocs explanation for more details.)&lt;/p&gt;
    &lt;p&gt;The simulated processes are stackful coroutines on their own call stacks, allowing the processes to store their state at arbitrary points and resume execution from there later with minimal overhead. The context-switching code is hand-coded in assembly for each platform. (You can find more details here.)&lt;/p&gt;
    &lt;p&gt;The C code is liberally sprinkled with &lt;code&gt;assert&lt;/code&gt; statements testing for preconditions,
invariants, and postconditions wherever possible, applying
Design by Contract
principles for high reliability. The Cimba library contains 958 asserts in 7132 lines of
C code, for a very high assert density of 13.4 %. These are custom-written
assert macros that will report
what trial, what process, the simulated time, the function and line number, and even the
random number seed used, if anything should go wrong. All time-consuming invariants and
postconditions are debug asserts, while the release asserts mostly check preconditions
like function argument validity. Turning off the debug asserts doubles the speed of your
model when you are ready for it, while turning off the release asserts as well gives
a small incremental improvement. (Again,
more explanation here.)&lt;/p&gt;
    &lt;p&gt;This is then combined with extensive unit testing of each module, ensuring that all lower level functionality works as expected before moving on to higher levels. You will find the test files corresponding to each code module in the test directory.&lt;/p&gt;
    &lt;p&gt;But do read the LICENSE. We are not giving any warranties here.&lt;/p&gt;
    &lt;p&gt;Long story made short: C++ exception handling is not very friendly to the stackful coroutines we need in Cimba. The stackless coroutines in C++ are entirely different from what we need.&lt;/p&gt;
    &lt;p&gt;C++ has also become a large and feature-rich language, where it will be hard to ensure compatibility with every possible combination of features.&lt;/p&gt;
    &lt;p&gt;Hence (like the Linux kernel), we chose the simpler platform for speed, clarity, and reliability. If you need to call Cimba from some other language, the C calling convention is well-known and well-documented.&lt;/p&gt;
    &lt;p&gt;Because it was not made public before. What retrospectively can be called Cimba 1.0 was implemented in K&amp;amp;R C at MIT in the early 1990's, followed by a parallelized version 2.0 in ANSI C and Perl around 1995–96. The present version written in C17 with POSIX pthreads is the third major rebuild, and the first public version.&lt;/p&gt;
    &lt;p&gt;It is right here. You clone the repository, build, and install it. You will need a C compiler and the Meson build manager. On Linux, you can use GCC or Clang, while the recommended approach on Windows is MinGW with its GCC compiler. For convenience, we recommend the CLion integrated development environment with GCC, Meson, and Ninja built-in support on both Linux and Windows.&lt;/p&gt;
    &lt;p&gt;You will find the installation guide here: https://cimba.readthedocs.io/en/latest/installation.html&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ambonvik/cimba"/><published>2026-02-03T16:09:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873133</id><title>Tadpole – A modular and extensible DSL built for web scraping</title><updated>2026-02-03T18:40:47.545462+00:00</updated><content>&lt;doc fingerprint="d7a2168e9b832ef8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Tadpole&lt;/head&gt;&lt;p&gt;Scrape the web with a language designed for it.&lt;/p&gt;&lt;head rend="h2"&gt;Write Declarative, Modular Scraping Code&lt;/head&gt;Section titled “Write Declarative, Modular Scraping Code”&lt;p&gt;Tadpole is built to be composable by allowing you to import modules from local files or remote repositories! All of the complexity of interacting with the browser is now completely abstracted away by the language! Building scrapers has never been easier!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tadpolehq.com/"/><published>2026-02-03T16:29:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873138</id><title>Prek: A better, faster, drop-in pre-commit replacement, engineered in Rust</title><updated>2026-02-03T18:40:46.354451+00:00</updated><content>&lt;doc fingerprint="28df57ddde9b6d0"&gt;
  &lt;main&gt;
    &lt;p&gt;pre-commit is a framework to run hooks written in many languages, and it manages the language toolchain and dependencies for running the hooks.&lt;/p&gt;
    &lt;p&gt;prek is a reimagined version of pre-commit, built in Rust. It is designed to be a faster, dependency-free and drop-in alternative for it, while also providing some additional long-requested features.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Although prek is pretty new, it’s already powering real‑world projects like CPython, Apache Airflow, FastAPI, and more projects are picking it up—see Who is using prek?. If you’re looking for an alternative to &lt;code&gt;pre-commit&lt;/code&gt;, please give it a try—we’d love your feedback!&lt;/p&gt;
    &lt;p&gt;Please note that some languages are not yet supported for full drop‑in parity with &lt;code&gt;pre-commit&lt;/code&gt;. See Language Support for current status.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🚀 A single binary with no dependencies, does not require Python or any other runtime.&lt;/item&gt;
      &lt;item&gt;⚡ Faster than &lt;code&gt;pre-commit&lt;/code&gt;and more efficient in disk space usage.&lt;/item&gt;
      &lt;item&gt;🔄 Fully compatible with the original pre-commit configurations and hooks.&lt;/item&gt;
      &lt;item&gt;🏗️ Built-in support for monorepos (i.e. workspace mode).&lt;/item&gt;
      &lt;item&gt;🐍 Integration with &lt;code&gt;uv&lt;/code&gt;for managing Python virtual environments and dependencies.&lt;/item&gt;
      &lt;item&gt;🛠️ Improved toolchain installations for Python, Node.js, Bun, Go, Rust and Ruby, shared between hooks.&lt;/item&gt;
      &lt;item&gt;📦 Built-in Rust-native implementation of some common hooks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Standalone installer&lt;/head&gt;
    &lt;p&gt;prek provides a standalone installer script to download and install the tool,&lt;/p&gt;
    &lt;p&gt;On Linux and macOS:&lt;/p&gt;
    &lt;code&gt;curl --proto '=https' --tlsv1.2 -LsSf https://github.com/j178/prek/releases/download/v0.3.1/prek-installer.sh | sh&lt;/code&gt;
    &lt;p&gt;On Windows:&lt;/p&gt;
    &lt;code&gt;powershell -ExecutionPolicy ByPass -c "irm https://github.com/j178/prek/releases/download/v0.3.1/prek-installer.ps1 | iex"&lt;/code&gt;
    &lt;head&gt;PyPI&lt;/head&gt;
    &lt;p&gt;prek is published as Python binary wheel to PyPI, you can install it using &lt;code&gt;pip&lt;/code&gt;, &lt;code&gt;uv&lt;/code&gt; (recommended), or &lt;code&gt;pipx&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# Using uv (recommended)
uv tool install prek

# Using uvx (install and run in one command)
uvx prek

# Adding prek to the project dev-dependencies
uv add --dev prek

# Using pip
pip install prek

# Using pipx
pipx install prek&lt;/code&gt;
    &lt;head&gt;Homebrew&lt;/head&gt;
    &lt;code&gt;brew install prek&lt;/code&gt;
    &lt;head&gt;Cargo&lt;/head&gt;
    &lt;p&gt;Build from source using Cargo (Rust 1.89+ is required):&lt;/p&gt;
    &lt;code&gt;cargo install --locked prek&lt;/code&gt;
    &lt;head&gt;npmjs&lt;/head&gt;
    &lt;p&gt;prek is published as a Node.js package and can be installed with any npm-compatible package manager:&lt;/p&gt;
    &lt;code&gt;# As a dev dependency
npm add -D @j178/prek
pnpm add -D @j178/prek
bun add -D @j178/prek

# Or install globally
npm install -g @j178/prek
pnpm add -g @j178/prek
bun install -g @j178/prek

# Or run directly without installing
npx @j178/prek --version
bunx @j178/prek --version&lt;/code&gt;
    &lt;head&gt;Nix&lt;/head&gt;
    &lt;p&gt;prek is available via Nixpkgs.&lt;/p&gt;
    &lt;code&gt;# Choose what's appropriate for your use case.
# One-off in a shell:
nix-shell -p prek

# NixOS or non-NixOS without flakes:
nix-env -iA nixos.prek

# Non-NixOS with flakes:
nix profile install nixpkgs#prek&lt;/code&gt;
    &lt;head&gt;GitHub Releases&lt;/head&gt;
    &lt;p&gt;Pre-built binaries are available for download from the GitHub releases page.&lt;/p&gt;
    &lt;head&gt;GitHub Actions&lt;/head&gt;
    &lt;p&gt;prek can be used in GitHub Actions via the j178/prek-action repository.&lt;/p&gt;
    &lt;p&gt;Example workflow:&lt;/p&gt;
    &lt;code&gt;name: Prek checks
on: [push, pull_request]

jobs:
  prek:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6
      - uses: j178/prek-action@v1&lt;/code&gt;
    &lt;p&gt;This action installs prek and runs &lt;code&gt;prek run --all-files&lt;/code&gt; on your repository.&lt;/p&gt;
    &lt;p&gt;prek is also available via &lt;code&gt;taiki-e/install-action&lt;/code&gt; for installing various tools.&lt;/p&gt;
    &lt;p&gt;If installed via the standalone installer, prek can update itself to the latest version:&lt;/p&gt;
    &lt;code&gt;prek self update&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I already use pre-commit: follow the short migration checklist in the quickstart guide to swap in &lt;code&gt;prek&lt;/code&gt;safely.&lt;/item&gt;
      &lt;item&gt;I'm new to pre-commit-style tools: learn the basics—creating a config, running hooks, and installing git hooks—in the beginner quickstart walkthrough.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It is multiple times faster than &lt;code&gt;pre-commit&lt;/code&gt;and takes up half the disk space.&lt;/item&gt;
      &lt;item&gt;It redesigned how hook environments and toolchains are managed, they are all shared between hooks, which reduces the disk space usage and speeds up the installation process.&lt;/item&gt;
      &lt;item&gt;Repositories are cloned in parallel, and hooks are installed in parallel if their dependencies are disjoint.&lt;/item&gt;
      &lt;item&gt;Hooks can run in parallel by priority (hooks with the same &lt;code&gt;priority&lt;/code&gt;may run concurrently), reducing end-to-end runtime.&lt;/item&gt;
      &lt;item&gt;It uses &lt;code&gt;uv&lt;/code&gt;for creating Python virtualenvs and installing dependencies, which is known for its speed and efficiency.&lt;/item&gt;
      &lt;item&gt;It implements some common hooks in Rust, built in prek, which are faster than their Python counterparts.&lt;/item&gt;
      &lt;item&gt;It supports &lt;code&gt;repo: builtin&lt;/code&gt;for offline, zero-setup hooks, which is not available in&lt;code&gt;pre-commit&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No need to install Python or any other runtime, just download a single binary.&lt;/item&gt;
      &lt;item&gt;No hassle with your Python version or virtual environments, prek automatically installs the required Python version and creates a virtual environment for you.&lt;/item&gt;
      &lt;item&gt;Built-in support for workspaces (or monorepos), each subproject can have its own &lt;code&gt;.pre-commit-config.yaml&lt;/code&gt;file.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;prek run&lt;/code&gt;has some nifty improvements over&lt;code&gt;pre-commit run&lt;/code&gt;, such as:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;prek run --directory &amp;lt;dir&amp;gt;&lt;/code&gt;runs hooks for files in the specified directory, no need to use&lt;code&gt;git ls-files -- &amp;lt;dir&amp;gt; | xargs pre-commit run --files&lt;/code&gt;anymore.&lt;/item&gt;&lt;item&gt;&lt;code&gt;prek run --last-commit&lt;/code&gt;runs hooks for files changed in the last commit.&lt;/item&gt;&lt;item&gt;&lt;code&gt;prek run [HOOK] [HOOK]&lt;/code&gt;selects and runs multiple hooks.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;prek list&lt;/code&gt;command lists all available hooks, their ids, and descriptions, providing a better overview of the configured hooks.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;prek auto-update&lt;/code&gt;supports&lt;code&gt;--cooldown-days&lt;/code&gt;to mitigate open source supply chain attacks.&lt;/item&gt;
      &lt;item&gt;prek provides shell completions for &lt;code&gt;prek run &amp;lt;hook_id&amp;gt;&lt;/code&gt;command, making it easier to run specific hooks without remembering their ids.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For more detailed improvements prek offers, take a look at Difference from pre-commit.&lt;/p&gt;
    &lt;p&gt;prek is pretty new, but it is already being used or recommend by some projects and organizations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;apache/airflow&lt;/item&gt;
      &lt;item&gt;python/cpython&lt;/item&gt;
      &lt;item&gt;pdm-project/pdm&lt;/item&gt;
      &lt;item&gt;fastapi/fastapi&lt;/item&gt;
      &lt;item&gt;fastapi/typer&lt;/item&gt;
      &lt;item&gt;fastapi/asyncer&lt;/item&gt;
      &lt;item&gt;astral-sh/ruff&lt;/item&gt;
      &lt;item&gt;astral-sh/ty&lt;/item&gt;
      &lt;item&gt;openclaw/openclaw&lt;/item&gt;
      &lt;item&gt;home-assistant/core&lt;/item&gt;
      &lt;item&gt;DetachHead/basedpyright&lt;/item&gt;
      &lt;item&gt;OpenLineage/OpenLineage&lt;/item&gt;
      &lt;item&gt;authlib/authlib&lt;/item&gt;
      &lt;item&gt;django/djangoproject.com&lt;/item&gt;
      &lt;item&gt;Future-House/paper-qa&lt;/item&gt;
      &lt;item&gt;requests-cache/requests-cache&lt;/item&gt;
      &lt;item&gt;Goldziher/kreuzberg&lt;/item&gt;
      &lt;item&gt;python-attrs/attrs&lt;/item&gt;
      &lt;item&gt;jlowin/fastmcp&lt;/item&gt;
      &lt;item&gt;apache/iceberg-python&lt;/item&gt;
      &lt;item&gt;apache/lucene&lt;/item&gt;
      &lt;item&gt;jcrist/msgspec&lt;/item&gt;
      &lt;item&gt;python-humanize/humanize&lt;/item&gt;
      &lt;item&gt;MoonshotAI/kimi-cli&lt;/item&gt;
      &lt;item&gt;simple-icons/simple-icons&lt;/item&gt;
      &lt;item&gt;ast-grep/ast-grep&lt;/item&gt;
      &lt;item&gt;commitizen-tools/commitizen&lt;/item&gt;
      &lt;item&gt;cocoindex-io/cocoindex&lt;/item&gt;
      &lt;item&gt;cachix/devenv&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is heavily inspired by the original pre-commit tool, and it wouldn't be possible without the hard work of the maintainers and contributors of that project.&lt;/p&gt;
    &lt;p&gt;And a special thanks to the Astral team for their remarkable projects, particularly uv, from which I've learned a lot on how to write efficient and idiomatic Rust code.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/j178/prek"/><published>2026-02-03T16:29:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873294</id><title>France dumps Zoom and Teams as Europe seeks digital autonomy from the US</title><updated>2026-02-03T18:40:45.991352+00:00</updated><content>&lt;doc fingerprint="af50b110b70b4885"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;France dumps Zoom and Teams as Europe seeks digital autonomy from the US&lt;/head&gt;
    &lt;head rend="h2"&gt;France dumps Zoom and Teams as Europe seeks digital autonomy from the US&lt;/head&gt;
    &lt;p&gt;LONDON (AP) — In France, civil servants will ditch Zoom and Teams for a homegrown video conference system. Soldiers in Austria are using open source office software to write reports after the military dropped Microsoft Office. Bureaucrats in a German state have also turned to free software for their administrative work.&lt;/p&gt;
    &lt;p&gt;Around Europe, governments and institutions are seeking to reduce their use of digital services from U.S. Big Tech companies and turning to domestic or free alternatives. The push for “digital sovereignty” is gaining attention as the Trump administration strikes an increasingly belligerent posture toward the continent, highlighted by recent tensions over Greenland that intensified fears that Silicon Valley giants could be compelled to cut off access.&lt;/p&gt;
    &lt;p&gt;Concerns about data privacy and worries that Europe is not doing enough to keep up with the United States and Chinese tech leadership are also fueling the drive.&lt;/p&gt;
    &lt;p&gt;The French government referenced some of these concerns when it announced last week that 2.5 million civil servants would stop using video conference tools from U.S. providers — including Zoom, Microsoft Teams, Webex and GoTo Meeting — by 2027 and switch to Visio, a homegrown service.&lt;/p&gt;
    &lt;p&gt;The objective is “to put an end to the use of non-European solutions, to guarantee the security and confidentiality of public electronic communications by relying on a powerful and sovereign tool,” the announcement said.&lt;/p&gt;
    &lt;p&gt;“We cannot risk having our scientific exchanges, our sensitive data, and our strategic innovations exposed to non-European actors,” David Amiel, a civil service minister, said in a press release.&lt;/p&gt;
    &lt;p&gt;Microsoft said it continues to “partner closely with the government in France and respect the importance of security, privacy, and digital trust for public institutions.”&lt;/p&gt;
    &lt;p&gt;The company said it is “focused on providing customers with greater choice, stronger data protection, and resilient cloud services — ensuring data stays in Europe, under European law, with robust security and privacy protections.”&lt;/p&gt;
    &lt;p&gt;Zoom, Webex and GoTo Meeting did not respond to requests for comment.&lt;/p&gt;
    &lt;p&gt;French President Emmanuel Macron has been pushing digital sovereignty for years. But there’s now a lot more “political momentum behind this idea now that we need to de-risk from U.S. tech,” Nick Reiners, senior geotechnology analyst at the Eurasia Group.&lt;/p&gt;
    &lt;p&gt;“It feels kind of like there’s a real zeitgeist shift,” Reiners said&lt;/p&gt;
    &lt;p&gt;It was a hot topic at the World Economic Forum’s annual meeting of global political and business elites last month in Davos, Switzerland. The European Commission’s official for tech sovereignty, Henna Virkkunen, told an audience that Europe’s reliance on others “can be weaponized against us.”&lt;/p&gt;
    &lt;p&gt;“That’s why it’s so important that we are not dependent on one country or one company when it comes to very critical fields of our economy or society,” she said, without naming countries or companies.&lt;/p&gt;
    &lt;p&gt;A decisive moment came last year when the Trump administration sanctioned the International Criminal Court’s top prosecutor after the tribunal, based in The Hague, Netherlands, issued an arrest warrant for Israeli Prime Minister Benjamin Netanyahu, an ally of President Donald Trump.&lt;/p&gt;
    &lt;p&gt;The sanctions led Microsoft to cancel Khan’s ICC email, a move that was first reported by The Associated Press and sparked fears of a “kill switch” that Big Tech companies can use to turn off service at will.&lt;/p&gt;
    &lt;p&gt;Microsoft maintains it kept in touch with the ICC “throughout the process that resulted in the disconnection of its sanctioned official from Microsoft services. At no point did Microsoft cease or suspend its services to the ICC.”&lt;/p&gt;
    &lt;p&gt;Microsoft President Brad Smith has repeatedly sought to strengthen trans-Atlantic ties, the company’s press office said, and pointed to an interview he did last month with CNN in Davos in which he said that jobs, trade and investment. as well as security, would be affected by a rift over Greenland.&lt;/p&gt;
    &lt;p&gt;“Europe is the American tech sector’s biggest market after the United States itself. It all depends on trust. Trust requires dialogue,” Smith said.&lt;/p&gt;
    &lt;p&gt;Other incidents have added to the movement. There’s a growing sense that repeated EU efforts to rein in tech giants such as Google with blockbuster antitrust fines and sweeping digital rule books haven’t done much to curb their dominance.&lt;/p&gt;
    &lt;p&gt;Billionaire Elon Musk is also a factor. Officials worry about relying on his Starlink satellite internet system for communications in Ukraine.&lt;/p&gt;
    &lt;p&gt;Washington and Brussels wrangled for years over data transfer agreements, triggered by former National Security Agency contractor Edward Snowden’s revelations of U.S. cyber-snooping.&lt;/p&gt;
    &lt;p&gt;With online services now mainly hosted in the cloud through data centers, Europeans fear that their data is vulnerable.&lt;/p&gt;
    &lt;p&gt;U.S. cloud providers have responded by setting up so-called “sovereign cloud” operations, with data centers located in European countries, owned by European entities and with physical and remote access only for staff who are European Union residents.&lt;/p&gt;
    &lt;p&gt;The idea is that “only Europeans can take decisions so that they can’t be coerced by the U.S.,” Reiners said.&lt;/p&gt;
    &lt;p&gt;The German state of Schleswig-Holstein last year migrated 44,000 employee inboxes from Microsoft to an open source email program. It also switched from Microsoft’s SharePoint file sharing system to Nextcloud, an open source platform, and is even considering replacing Windows with Linux and telephones and videoconferencing with open source systems.&lt;/p&gt;
    &lt;p&gt;“We want to become independent of large tech companies and ensure digital sovereignty,” Digitalization Minister Dirk Schrödter said in an October announcement.&lt;/p&gt;
    &lt;p&gt;The French city of Lyon said last year that it’s deploying free office software to replace Microsoft. Denmark’s government and the cities of Copenhagen and Aarhus have also been trying out open-source software.&lt;/p&gt;
    &lt;p&gt;“We must never make ourselves so dependent on so few that we can no longer act freely,” Digital Minister Caroline Stage Olsen wrote on LinkedIn last year. “Too much public digital infrastructure is currently tied up with very few foreign suppliers.”&lt;/p&gt;
    &lt;p&gt;The Austrian military said it has also switched to LibreOffice, a software package with word processor, spreadsheet and presentation programs that mirrors Microsoft 365’s Word, Excel and PowerPoint.&lt;/p&gt;
    &lt;p&gt;The Document Foundation, a nonprofit based in Germany that’s behind LibreOffice, said the military’s switch “reflects a growing demand for independence from single vendors.” Reports also said the military was concerned that Microsoft was moving file storage online to the cloud — the standard version of LibreOffice is not cloud-based.&lt;/p&gt;
    &lt;p&gt;Some Italian cities and regions adopted the software years ago, said Italo Vignoli, a spokesman for The Document Foundation. Back then, the appeal was not needing to pay for software licenses. Now, it’s the main reason is to avoid being locked into a proprietary system.&lt;/p&gt;
    &lt;p&gt;“At first, it was: we will save money and by the way, we will get freedom,” Vignoli said. “Today it is: we will be free and by the way, we will also save some money.”&lt;/p&gt;
    &lt;p&gt;___&lt;/p&gt;
    &lt;p&gt;Associated Press writer Molly Quell in The Hague, Netherlands contributed to this report.&lt;/p&gt;
    &lt;p&gt;___&lt;/p&gt;
    &lt;p&gt;This version corrects the contribution line to Molly Quell instead of Molly Hague.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://apnews.com/article/europe-digital-sovereignty-big-tech-9f5388b68a0648514cebc8d92f682060"/><published>2026-02-03T16:39:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873308</id><title>Show HN: PII-Shield – Log Sanitization Sidecar with JSON Integrity (Go, Entropy)</title><updated>2026-02-03T18:40:45.493861+00:00</updated><content>&lt;doc fingerprint="a9d375110733db96"&gt;
  &lt;main&gt;
    &lt;p&gt;Zero-code log sanitization sidecar for Kubernetes. Prevents data leaks (GDPR/SOC2) by redacting PII from logs before they leave the pod.&lt;/p&gt;
    &lt;p&gt;Developers often forget to mask sensitive data. Traditional regex filters in Fluentd/Logstash are slow, hard to maintain, and consume expensive CPU on log aggregators.&lt;/p&gt;
    &lt;p&gt;PII-Shield sits right next to your app container:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;High Performance: Written in Go, designed for low-latency log processing.&lt;/item&gt;
      &lt;item&gt;Context-Aware Entropy Analysis: Detected high-entropy secrets even without keys (e.g. &lt;code&gt;Error: ... 44saCk9...&lt;/code&gt;) by analyzing context keywords.&lt;/item&gt;
      &lt;item&gt;100% Accuracy: Verified against "Wild" stress tests including binary garbage, JSON nesting, and multilingual logs.&lt;/item&gt;
      &lt;item&gt;Deterministic Hashing: Replaces secrets with unique hashes (e.g., &lt;code&gt;[HIDDEN:a1b2c]&lt;/code&gt;), allowing QA to correlate errors without seeing the raw data.&lt;/item&gt;
      &lt;item&gt;Drop-in: No code changes required. Works with any language (Node, Python, Java, Go).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Get the latest lightweight image from Docker Hub:&lt;/p&gt;
    &lt;code&gt;docker pull thelisdeep/pii-shield:latest&lt;/code&gt;
    &lt;p&gt;See CONFIGURATION.md for a full list of environment variables, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;PII_SALT&lt;/code&gt;: Custom HMAC salt (Required for production).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PII_ADAPTIVE_THRESHOLD&lt;/code&gt;: Enable dynamic entropy baselines.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PII_DISABLE_BIGRAM_CHECK&lt;/code&gt;: Optimize for non-English logs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Test Locally (CLI) You can pipe any log output through PII-Shield to see it in action immediately:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Emulate a log with a sensitive password
echo "Error: User password=MySecretPass123! failed login" | docker run -i --rm thelisdeep/pii-shield:latest

# Output: Error: User password=[HIDDEN:8f3a11] failed login&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Kubernetes (Sidecar Pattern) To use PII-Shield as a pipe wrapper for your application, use an &lt;code&gt;initContainer&lt;/code&gt;to copy the binary into a shared volume.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: secure-app
spec:
  volumes:
  - name: bin-dir
    emptyDir: {}
  
  # 1. Copy the PII-Shield binary to a shared volume
  initContainers:
  - name: install-shield
    image: thelisdeep/pii-shield:latest
    command: ["cp", "/bin/pii-shield", "/opt/bin/pii-shield"]
    volumeMounts:
    - name: bin-dir
      mountPath: /opt/bin

  # 2. Run your app and pipe logs through PII-Shield
  containers:
  - name: my-app
    image: my-app:1.0
    command: ["/bin/sh", "-c"]
    # Pipe stderr/stdout through the sanitizer
    args: ["./start-app.sh 2&amp;gt;&amp;amp;1 | /opt/bin/pii-shield"] 
    volumeMounts:
    - name: bin-dir
      mountPath: /opt/bin&lt;/code&gt;
    &lt;p&gt;This project is verified with a comprehensive suite:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Unit Tests: Cover edge cases, multilingual support, and JSON integrity.&lt;/item&gt;
      &lt;item&gt;Fuzzing: Native Go fuzzing ensures crash safety against invalid inputs.&lt;/item&gt;
      &lt;item&gt;Stress Testing: &lt;code&gt;./full_stress_test.sh&lt;/code&gt;validates 100% detection accuracy on mixed workloads.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Distributed under the Apache 2.0 License. See &lt;code&gt;LICENSE&lt;/code&gt; for more information.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/aragossa/pii-shield"/><published>2026-02-03T16:40:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873574</id><title>221 Cannon Road Is Not for Sale</title><updated>2026-02-03T18:40:45.249432+00:00</updated><content>&lt;doc fingerprint="ec0da09223e19cd5"&gt;
  &lt;main&gt;
    &lt;p&gt;Like most people, I’ve had my identity stolen once or twice in my life. It’s annoying, but thankfully I’ve avoided some of the more catastrophic outcomes when criminals begin impersonating you.&lt;/p&gt;
    &lt;p&gt;These days, however, it seems like someone is really trying to change that: a scammer has now tried to impersonate me multiple times in a six figure land deal in my hometown of Wilton, CT. So while I usually use this blog to write about finding weird things on the internet, it’s now time for a story about something weird on the internet finding me.&lt;/p&gt;
    &lt;p&gt;The story begins with my brother Alexander and I purchasing a small parcel of vacant land at 221 Cannon Road in Wilton, Connecticut in 2015. It’s been over 10 years since we purchased it and we have never listed it for sale. Nor do we have plans to sell it.&lt;/p&gt;
    &lt;p&gt;And yet, three different real estate agents have now contacted us to let us know that someone has been impersonating us and attempting to sell our property out from under us.&lt;/p&gt;
    &lt;p&gt;The first time it happened, it was pretty upsetting, but now that it’s happened another two times, I figured it was time to write a blog post about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The First Attempt (March 2024)&lt;/head&gt;
    &lt;p&gt;In March 2024, I received an email from a real estate attorney in Wilton, asking if I was the “Fred Benenson” who co-owned property in town with an “Ed Benenson.” He explained that a realtor at a major brokerage had been working with someone claiming to be us, and that there was already an offer on the table. The attorney was doing his due diligence before representing the sellers — and something didn’t add up.&lt;/p&gt;
    &lt;p&gt;I replied within minutes: Neither of us had spoken to anyone about selling the property. It was pretty concerning.&lt;/p&gt;
    &lt;p&gt;The realtor had been contacted through Zillow by someone claiming to be me. They’d had a phone conversation — she noted the person had a “middle European” accent — and the scammer had provided accurate details about the property, including its exact acreage. The impostor gave her the email address &lt;code&gt;[email protected]&lt;/code&gt; and the phone number (516) 828-0305. He also provided a fake email for my brother: &lt;code&gt;[email protected]&lt;/code&gt;. Notice the subtle misspelling — “Benenson” without the second “n” in the email, and the hyphenated “out-look.com” domain.&lt;/p&gt;
    &lt;p&gt;She had walked the property, taken drone photos, pulled comps, and listed for a price well above what we paid for it. The property had been live on dozens of real estate websites for days before anyone caught it. A builder had already submitted a full-price cash offer.&lt;/p&gt;
    &lt;p&gt;The scammer had even e-signed a purchase agreement.&lt;/p&gt;
    &lt;p&gt;When the attorney requested identification before closing, the impostor provided a New York State driver’s license. It had my father’s name (which I share with him) and his correct date of birth and home address. But the photo was of a complete stranger.&lt;/p&gt;
    &lt;p&gt;I have no idea who that guy is in on the license, but it’s definitely not my Dad. The license wouldn’t fool anyone who knew my father, but it didn’t need to – in a transaction conducted entirely by email and text message, with a closing that the scammer would never actually attend, the ID just needed to look plausible enough to keep things moving forward. (Though if you look closely at his signature, it’s clearly not written by hand.)&lt;/p&gt;
    &lt;head rend="h2"&gt;How It Was Caught&lt;/head&gt;
    &lt;p&gt;The attorney deserves most of the credit here. He told me this was the second time in nine months he’d encountered this exact scheme on vacant land in Wilton – his policy is that he won’t represent owners of vacant land without independently verifying ownership. That’s what led him to track me down, and that’s what stopped the sale.&lt;/p&gt;
    &lt;p&gt;The realtor was an innocent victim in this too. She’d done her job by walking the property, pulling comps, etc., all in good faith. When I initially suggested (perhaps unfairly) that this felt like lead generation, the attorney took me aside and vouched for her. I’m glad I listened to him!&lt;/p&gt;
    &lt;p&gt;I apologized to her, and she graciously forwarded me all of her text message exchanges with the scammer. Reading through them was fascinating. The impostor was responsive, polite, and generally knew the right things to say. But there were tells: slightly awkward phrasing (“Hi good morning”), declining a for-sale sign (“No I don’t think that will be necessary”), and a general reluctance to engage in any way that might require showing up in person.&lt;/p&gt;
    &lt;head rend="h2"&gt;Going to the FBI&lt;/head&gt;
    &lt;p&gt;After gathering everything I could — the fake ID, the realtor’s text messages, the scammer’s email addresses and phone number, and the attorney’s notes from a prior similar case — I contacted the FBI field office in Connecticut. They directed me to “walk it in” to the office in New York City.&lt;/p&gt;
    &lt;p&gt;The experience was, frankly, underwhelming. The FBI wouldn’t let me submit any of our documentation. Instead, they required me to write out the entire complaint by hand on a single piece of paper and hand it to the guard. He made some calls while I waited, and by the end he seemed at least somewhat interested. He gave me the standard line: 2-3 weeks if I hear from anyone.&lt;/p&gt;
    &lt;p&gt;I never heard from anyone.&lt;/p&gt;
    &lt;p&gt;The attorney, meanwhile, checked with his title company about recording an affidavit on the land records — something that would alert any future buyer or title searcher that the property had been targeted by fraud, and providing our verified contact information.&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s Happening Again (February 2026)&lt;/head&gt;
    &lt;p&gt;I thought this was behind us. Then, this past week, nearly two years later, I was contacted by two more real estate agents, both reaching out to warn me that someone was once again trying to sell 221 Cannon Road.&lt;/p&gt;
    &lt;p&gt;The first was a agent in Wilton who reached out via Instagram DM, of all places — it was the only way he could find to contact me. He explained that his team had received an inquiry to list 221 Cannon Road and had sent paperwork to “Fred and Alex” the night before to sign. But he’d done something smart: he’d noticed he had a mutual friend with my brother, and when he asked him about the situation, he flagged that the conversation with “Alex” didn’t sound right.&lt;/p&gt;
    &lt;p&gt;“I had a really bad feeling it wasn’t,” he told me.&lt;/p&gt;
    &lt;p&gt;The second agent, a woman at Berkshire Hathaway, sent a carefully worded email explaining that she’d been contacted by someone claiming to have authority to sell our property, but that “several standard verification steps raised concerns” and she chose not to proceed. She reached out purely as a courtesy to let us know.&lt;/p&gt;
    &lt;p&gt;Which is about when I decided I should write something about this. Not only because it’s a fascinating scam that seems to be getting more common, but because I figured this post might show up for the next broker who might be doing research on the address.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vacant Land Fraud&lt;/head&gt;
    &lt;p&gt;This type of scam targets a very specific vulnerability: vacant land has no occupants to notice a for-sale sign, no neighbors who’d immediately recognize something is wrong, and closings often happen remotely.&lt;/p&gt;
    &lt;p&gt;Here’s how it works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The scammer identifies vacant land through public records or Zillow. They look for parcels that are owned free and clear (no mortgage), haven’t changed hands recently, and are in desirable areas.&lt;/item&gt;
      &lt;item&gt;They contact a real estate agent through a platform like Zillow, posing as the owner. They know the property details because that information is publicly available.&lt;/item&gt;
      &lt;item&gt;They communicate primarily through text and email, avoiding in-person meetings. They provide fake identification if asked.&lt;/item&gt;
      &lt;item&gt;They agree to whatever price the agent suggests (because they don’t actually own the property, any sale is pure profit).&lt;/item&gt;
      &lt;item&gt;They push for a quick closing and attempt to direct proceeds to an account they control.&lt;/item&gt;
      &lt;item&gt;If questioned, they disappear. The scammer who targeted us in 2024 simply stopped responding once the attorney asked for an in-person closing.&lt;/item&gt;
      &lt;item&gt;If they get farther they’ll pocket the earnest money deposit which would have been significant in my case.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A similar scheme in nearby Fairfield wasn’t caught in time: someone had a $1.5 million home built on land they didn’t own without the actual owners knowing.&lt;/p&gt;
    &lt;head rend="h2"&gt;What You Can Do&lt;/head&gt;
    &lt;p&gt;If you own vacant land there are a couple of things you can do, but the most effective one is probably to register the address with a Fraud / No-Authority notice. This involves calling the County Recorder / Register of Deeds and ask how to record one of these (names vary by state):&lt;lb/&gt; • Owner Affidavit&lt;lb/&gt; • Affidavit of Fact&lt;lb/&gt; • Notice of Non-Authority to Convey&lt;lb/&gt; • Fraud Alert / Title Alert Notice&lt;lb/&gt; • Statement of Ownership / Anti-Fraud Notice:&lt;/p&gt;
    &lt;p&gt;You could also setup up Google Alerts for your address and you’ll be notified if it appears online.&lt;/p&gt;
    &lt;p&gt;Finally, and this certainly isn’t for everyone, you can make yourself easily findable online. One reason the attorney was able to verify ownership quickly in 2024 was that it’s fairly easy to gooogle me. If you own property, make sure there’s some way for a diligent attorney or agent to reach the real you.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Property Is Not For Sale&lt;/head&gt;
    &lt;p&gt;In case it isn’t clear, 221 Cannon Road is not for sale. It has never been for sale. If you are a real estate professional who has been contacted about listing or purchasing this property, please reach out to me directly.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fredbenenson.com/blog/2026/02/03/221-cannon-is-not-for-sale/"/><published>2026-02-03T16:56:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873742</id><title>Show HN: I built "AI Wattpad" to eval LLMs on fiction</title><updated>2026-02-03T18:40:45.097707+00:00</updated><content>&lt;doc fingerprint="c3079dfd9fbca34b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Best AI Models for Creative Writing in 2026&lt;/head&gt;&lt;p&gt;See which AI models actually keep readers hooked—based on real data, not synthetic benchmarks&lt;/p&gt;&lt;head rend="h2"&gt;Our Architecture&lt;/head&gt;&lt;p&gt;Each novel uses three specialized AI models working together. Compare performance across each role to understand which models excel at different creative writing tasks.&lt;/p&gt;&lt;head rend="h3"&gt;Brainstorming Model&lt;/head&gt;&lt;p&gt;Create concepts, plots, and world-building&lt;/p&gt;&lt;head rend="h3"&gt;Writer Model&lt;/head&gt;&lt;p&gt;Write chapters and narrative content&lt;/p&gt;&lt;head rend="h3"&gt;Memory Model&lt;/head&gt;&lt;p&gt;Maintain story context and recall information&lt;/p&gt;&lt;p&gt;Want to see a specific model benchmarked? Let us know on Discord!&lt;/p&gt;Suggest New Model(opens in new tab)&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://narrator.sh/llm-leaderboard"/><published>2026-02-03T17:08:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873782</id><title>Show HN: Octosphere, a tool to decentralise scientific publishing</title><updated>2026-02-03T18:40:44.521635+00:00</updated><content>&lt;doc fingerprint="f9dc5bdd087c8698"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Octosphere&lt;/head&gt;
    &lt;p&gt;Connecting open science with the social web&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Octosphere?&lt;/head&gt;
    &lt;p&gt;Octosphere bridges the gap between academic publishing and the social web. It automatically syncs your research publications from Octopus to the AT Protocol (the atmosphere) — an open, decentralized network for social apps like Bluesky.&lt;/p&gt;
    &lt;p&gt;By sharing your work on the atmosphere, you can reach broader audiences, engage with the public, and increase the visibility of your research beyond traditional academic channels.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Sign in with ORCID — Authenticate using your researcher identifier.&lt;/item&gt;
      &lt;item&gt;Connect to the atmosphere — Sign in with your Bluesky account (or any AT Protocol app).&lt;/item&gt;
      &lt;item&gt;Link your Octopus profile — Connect your Octopus author page.&lt;/item&gt;
      &lt;item&gt;Sync your publications — Choose one-time sync or enable automatic syncing of future publications.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://octosphere.social/"/><published>2026-02-03T17:11:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873790</id><title>Defining Safe Hardware Design [pdf]</title><updated>2026-02-03T18:40:44.142386+00:00</updated><content/><link href="https://people.csail.mit.edu/rachit/files/pubs/safe-hdls.pdf"/><published>2026-02-03T17:12:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46873885</id><title>Migrate Wizard – IMAP Based Email Migration Tool</title><updated>2026-02-03T18:40:43.851649+00:00</updated><content>&lt;doc fingerprint="8cfa0d9d71ab0df"&gt;
  &lt;main&gt;
    &lt;p&gt;Trusted by teams around the world&lt;/p&gt;
    &lt;head rend="h1"&gt;Migrate Your Emails in&lt;lb/&gt;Minutes, Not Days&lt;/head&gt;
    &lt;p&gt;The fastest, most secure IMAP migration service. Move gigabytes of emails between any providers with zero downtime and 100% data integrity.&lt;/p&gt;
    &lt;p&gt;No technical expertise required. No data loss. No headaches.&lt;/p&gt;
    &lt;p&gt;10K+&lt;/p&gt;
    &lt;p&gt;Successful Migrations&lt;/p&gt;
    &lt;p&gt;99.9%&lt;/p&gt;
    &lt;p&gt;Success Rate&lt;/p&gt;
    &lt;p&gt;5min&lt;/p&gt;
    &lt;p&gt;Average Time&lt;/p&gt;
    &lt;p&gt;24/7&lt;/p&gt;
    &lt;p&gt;Support&lt;/p&gt;
    &lt;head rend="h2"&gt;Everything You Need for a Perfect Migration&lt;/head&gt;
    &lt;p&gt;Built for speed, security, and reliability. Trusted by businesses of all sizes.&lt;/p&gt;
    &lt;p&gt;Lightning Fast&lt;/p&gt;
    &lt;p&gt;Optimized algorithms move gigabytes of data in minutes, not days. Parallel processing ensures maximum speed.&lt;/p&gt;
    &lt;p&gt;Enterprise-Grade Security&lt;/p&gt;
    &lt;p&gt;Credentials encrypted and deleted when the migration finishes, with enterprise-grade infrastructure.&lt;/p&gt;
    &lt;p&gt;100% Data Integrity&lt;/p&gt;
    &lt;p&gt;Advanced error handling and verification ensures every email, folder, and attachment is perfectly migrated.&lt;/p&gt;
    &lt;p&gt;Zero Downtime&lt;/p&gt;
    &lt;p&gt;Continue using your email during migration. No service interruption, no business disruption.&lt;/p&gt;
    &lt;p&gt;No Credential Storage&lt;/p&gt;
    &lt;p&gt;Your passwords are encrypted in memory only and deleted immediately after migration completes.&lt;/p&gt;
    &lt;p&gt;Incremental Sync&lt;/p&gt;
    &lt;p&gt;Keep your emails synchronized after initial migration. Perfect for ongoing data consistency.&lt;/p&gt;
    &lt;head rend="h2"&gt;Perfect for Every Scenario&lt;/head&gt;
    &lt;p&gt;Whether you're switching providers or consolidating accounts, we've got you covered.&lt;/p&gt;
    &lt;p&gt;Provider Migration&lt;/p&gt;
    &lt;p&gt;Switching from Gmail, Outlook, Yahoo, or any IMAP provider? We handle it seamlessly with zero data loss.&lt;/p&gt;
    &lt;p&gt;Account Consolidation&lt;/p&gt;
    &lt;p&gt;Merge multiple email accounts into one. Perfect for businesses consolidating their communication.&lt;/p&gt;
    &lt;p&gt;Backup &amp;amp; Archive&lt;/p&gt;
    &lt;p&gt;Create secure backups of your entire email history. Essential for compliance and disaster recovery.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ready to Migrate Your Emails?&lt;/head&gt;
    &lt;p&gt;Join thousands of satisfied customers who've migrated millions of emails with Migrate Wizard.&lt;/p&gt;
    &lt;p&gt;No credit card required • Free trial available • Cancel anytime&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://migratewizard.com/#features"/><published>2026-02-03T17:19:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46874097</id><title>Deno Sandbox</title><updated>2026-02-03T18:40:43.345347+00:00</updated><content>&lt;doc fingerprint="84be1f18cd2fafa2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Deno Sandbox&lt;/head&gt;
    &lt;p&gt;Over the past year, we’ve seen a shift in what Deno Deploy customers are building: platforms where users generate code with LLMs, and that code runs immediately without review. That code frequently calls LLMs itself, which means it needs API keys and network access.&lt;/p&gt;
    &lt;p&gt;This isn’t the traditional “run untrusted plugins” problem. It’s deeper: LLM-generated code, calling external APIs with real credentials, without human review. Sandboxing the compute isn’t enough. You need to control network egress and protect secrets from exfiltration.&lt;/p&gt;
    &lt;p&gt;Deno Sandbox provides both. And when the code is ready, you can deploy it directly to Deno Deploy without rebuilding.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sandboxes?&lt;/head&gt;
    &lt;p&gt;You don’t want to run untrusted code (generated by your LLMs, your users LLMs, or even hand written by users) directly on your server. It will compromise your system, steal your API keys, and call out to evil.com. You need isolation.&lt;/p&gt;
    &lt;p&gt;Deno Sandbox gives you lightweight Linux microVMs (running in the Deno Deploy cloud) to run untrusted code with defense-in-depth security. You create or programmatically via our JavaScript or Python SDKs, and they boot in under a second. You can also interact with them via SSH, HTTP, or even open a VS Code window directly into the sandbox.&lt;/p&gt;
    &lt;code&gt;import { Sandbox } from "@deno/sandbox";

await using sandbox = await Sandbox.create();
await sandbox.sh`ls -lh /`;&lt;/code&gt;
    &lt;head rend="h2"&gt;Secrets That Can’t Be Stolen&lt;/head&gt;
    &lt;p&gt;But there is more. In Deno Sandbox, secrets never enter the environment. Code sees only a placeholder:&lt;/p&gt;
    &lt;code&gt;import { Sandbox } from "@deno/sandbox";

await using sandbox = await Sandbox.create({
  secrets: {
    OPENAI_API_KEY: {
      hosts: ["api.openai.com"],
      value: process.env.OPENAI_API_KEY,
    },
  },
});

await sandbox.sh`echo $OPENAI_API_KEY`;
// DENO_SECRET_PLACEHOLDER_b14043a2f578cba75ebe04791e8e2c7d4002fd0c1f825e19...&lt;/code&gt;
    &lt;p&gt;The real key materializes only when the sandbox makes an outbound request to an approved host. If prompt-injected code tries to exfiltrate that placeholder to &lt;code&gt;evil.com&lt;/code&gt;? Useless.&lt;/p&gt;
    &lt;head rend="h2"&gt;Network Egress Control&lt;/head&gt;
    &lt;p&gt;You can also restrict which hosts the sandbox can talk to:&lt;/p&gt;
    &lt;code&gt;await using sandbox = await Sandbox.create({
  allowNet: ["api.openai.com", "*.anthropic.com"],
});&lt;/code&gt;
    &lt;p&gt;Any request to an unlisted host gets blocked at the VM boundary.&lt;/p&gt;
    &lt;p&gt;Both features are implemented via an outbound proxy similar to coder/httpjail. This gives us a chokepoint for policy enforcement. We plan to add more capabilities here: analytics for outbound connections and programmatic hooks for trusted code to inspect or modify requests.&lt;/p&gt;
    &lt;p&gt;If you’re running untrusted JavaScript or TypeScript, combine this with Deno’s &lt;code&gt;--allow-net&lt;/code&gt; flag for defense in depth: VM-level network restrictions plus
runtime-level permissions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sandbox to Production&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;sandbox.deploy()&lt;/code&gt; deploys code from your sandbox directly to Deno Deploy.&lt;/p&gt;
    &lt;code&gt;const build = await sandbox.deploy("my-app", {
  production: true,
  build: { mode: "none", entrypoint: "server.ts" },
});

const revision = await build.done;
console.log(revision.url);&lt;/code&gt;
    &lt;p&gt;One call to go from sandbox to production deployment. No rebuilding in a different CI system, no re-authenticating with a different tool. Just turn your dev environment directly into a production ready, auto-scaling serverless deployment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Persistence&lt;/head&gt;
    &lt;p&gt;Sandboxes are ephemeral by default, but when you need state we have you covered:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Volumes: read-write storage for caches, databases, user data&lt;/item&gt;
      &lt;item&gt;Snapshots: read-only images for pre-installed toolchains and volume base&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run &lt;code&gt;apt-get install&lt;/code&gt; once, snapshot it, and every future sandbox boots with
everything already installed. Create read-write volumes from the snapshots to
create a fresh development environment in seconds.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical Details&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Spec&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Regions&lt;/cell&gt;
        &lt;cell&gt;Amsterdam, Chicago&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;vCPUs&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Memory&lt;/cell&gt;
        &lt;cell&gt;768 MB - 4 GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Lifetime&lt;/cell&gt;
        &lt;cell&gt;Ephemeral or timeout (supports extending on demand)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Max lifetime&lt;/cell&gt;
        &lt;cell&gt;30 minutes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Boot time&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 1 second&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Perfect for AI agents executing code, vibe-coding environments, secure plugin systems, ephemeral CI runners, and customer-supplied code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pricing&lt;/head&gt;
    &lt;p&gt;Deno Sandbox is included in your Deno Deploy plan with competitive, usage-based pricing. You pay for compute time, not wall-clock time.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;$0.05/h CPU time (40h included with Pro)&lt;/item&gt;
      &lt;item&gt;$0.016/GB-h memory (1000 GB-h included with Pro)&lt;/item&gt;
      &lt;item&gt;$0.20/GiB-month volume storage (5 GiB included with Pro)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Enterprise pricing available—contact deploy@deno.com.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get Started&lt;/head&gt;
    &lt;p&gt;Deno Sandbox launches in beta today, alongside the general availability of Deno Deploy.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Landing page: deno.com/sandbox&lt;/item&gt;
      &lt;item&gt;Docs: docs.deno.com/sandbox&lt;/item&gt;
      &lt;item&gt;JavaScript SDK: jsr.io/@deno/sandbox or npm&lt;/item&gt;
      &lt;item&gt;Python SDK: pypi.org/project/deno-sandbox&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’re excited to see what you (or your AI agents) build with Deno Sandbox.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://deno.com/blog/introducing-deno-sandbox"/><published>2026-02-03T17:33:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46874345</id><title>I made 20 GDPR deletion requests. 12 were ignored</title><updated>2026-02-03T18:40:43.147787+00:00</updated><content/><link href="https://nikolak.com/gdpr-failure/"/><published>2026-02-03T17:48:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46874619</id><title>Xcode 26.3 unlocks the power of agentic coding</title><updated>2026-02-03T18:40:43.006488+00:00</updated><content>&lt;doc fingerprint="bd64c349434690ed"&gt;
  &lt;main&gt;
    &lt;p&gt; UPDATE February 3, 2026 &lt;/p&gt;
    &lt;head rend="h1"&gt;Xcode 26.3 unlocks the power of agentic coding&lt;/head&gt;
    &lt;p&gt; Developers can leverage coding agents, including Anthropic’s Claude Agent and OpenAI’s Codex, directly in Xcode to tackle complex tasks autonomously, helping them develop apps faster than ever &lt;/p&gt;
    &lt;p&gt;Xcode 26.3 introduces support for agentic coding, a new way in Xcode for developers to build apps using coding agents such as Anthropic’s Claude Agent and OpenAI’s Codex. With agentic coding, Xcode can work with greater autonomy toward a developer’s goals — from breaking down tasks to making decisions based on the project architecture and using built-in tools. &lt;/p&gt;
    &lt;p&gt;Expanding on the intelligence features introduced in Xcode 26, which brought a brand-new coding assistant for writing and editing in Swift, this release gives coding agents access to even more of Xcode’s capabilities. Agents like Claude Agent and Codex can now collaborate throughout the entire development life cycle, giving developers the power to streamline workflows, iterate faster, and bring ideas to life like never before. Agents can search documentation, explore file structures, update project settings, and verify their work visually by capturing Xcode Previews and iterating through builds and fixes. &lt;/p&gt;
    &lt;p&gt;“At Apple, our goal is to make tools that put industry-leading technologies directly in developers’ hands so they can build the very best apps,” said Susan Prescott, Apple’s vice president of Worldwide Developer Relations. “Agentic coding supercharges productivity and creativity, streamlining the development workflow so developers can focus on innovation.” &lt;/p&gt;
    &lt;p&gt;With seamless access to Claude Agent and Codex, developers can bring the advanced reasoning of these models directly into their app-building workflow.1 This connection combines the power of these agents with Xcode’s native capabilities to provide the best results when developing for Apple platforms, giving developers the flexibility to work with the model that best fits their project. &lt;/p&gt;
    &lt;p&gt;In addition to these built-in integrations, Xcode 26.3 makes its capabilities available through the Model Context Protocol, an open standard that gives developers the flexibility to use any compatible agent or tool with Xcode. &lt;/p&gt;
    &lt;p&gt;Availability &lt;/p&gt;
    &lt;p&gt;Xcode 26.3 is available as a release candidate for all members of the Apple Developer Program starting today, with a release coming soon on the App Store. &lt;/p&gt;
    &lt;p&gt;Share article&lt;/p&gt;
    &lt;head rend="h2"&gt;Media&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Text of this article&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Images in this article&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Anthropic and OpenAI’s terms of service may apply.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.apple.com/newsroom/2026/02/xcode-26-point-3-unlocks-the-power-of-agentic-coding/"/><published>2026-02-03T18:04:08+00:00</published></entry></feed>