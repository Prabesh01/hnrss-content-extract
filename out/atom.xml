<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-19T19:07:50.857797+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45271857</id><title>With Strings Attached</title><updated>2025-09-19T19:07:55.569413+00:00</updated><content>&lt;doc fingerprint="3ab0127539a903ba"&gt;
  &lt;main&gt;
    &lt;p&gt;In March 2025, an anonymous buyer purchased the 1715 “Baron Knoop” Stradivarius for $23 million (U.S.), making it the most expensive violin ever sold. (The seller, the American stringed-instrument collector David L. Fulton, had purchased it for a more modest $2.75 million in 1992.) Previous record setters have included the 1721 “Lady Blunt,” which fetched $15.9 million in 2011, and the “Joachim‑Ma,” which went for $11.25 million in February 2025.&lt;/p&gt;
    &lt;p&gt;All three of these models were made by Antonio Stradivari, a Cremonese luthier whose output in the seventeenth and eighteenth centuries is said to epitomize perfection in violin manufacturing. Depending on your point of view, they may indeed be examples of flawless human handiwork. Or they might be, as the fiction writer and journalist Ambrose Bierce once put it, objects that “tickle human ears by friction of a horse’s tail on the entrails of a cat.” Either way, where do these exorbitant value judgments come from?&lt;/p&gt;
    &lt;p&gt;Tom Wilder looks for the answers through the wider cultural world that brought the violin to prominence after its development in the sixteenth century and laid the stage for it to become the most iconic instrument of Western music: a physical manifestation of “taste, refinement, and wealth.” The guitar may exist on a similarly high level of symbolism, but the appraisal of an individual six-string turns more on its provenance and on any alterations by famous owners than on the maker. As two examples, an acoustic Gibson owned by John Lennon sold for $2.4 million in 2015, while the 1959 Martin D‑18E that Kurt Cobain used on Nirvana’s MTV Unplugged in New York album went for just over $6 million in 2020 (he had picked it up for just $5,000). Pricey, but not near the numbers a Stradivarius commands.&lt;/p&gt;
    &lt;p&gt;As one of the world’s busiest ports, a centre of global finance, and the largest city in Europe, nineteenth-century London was the destination for musicians looking to join a growing orchestral landscape, teach affluent students, and make a name for themselves. Into this urban hullabaloo they brought their Continental instruments, including those crafted in Cremona, Italy.&lt;/p&gt;
    &lt;p&gt;Music had to that point been shaded with raucousness in so‑called free and easies: publike entertainment rooms with shows supplied by amateurs. Spurred by the influx of higher-calibre musicians, Wilder explains, “the civilizing of industrial society — meaning its lower classes — was to be achieved through the suppression of traditionally popular (though barbarous) pastimes, and their replacement by ‘endless sources of rational amusement.’ ” There emerged two conjoined ideas: that music was a “respectablizing activity” and that it ought, therefore, to be morally uplifting.&lt;/p&gt;
    &lt;p&gt;And so the lowly fiddle received a reputation overhaul, transmogrified through the wiles of teachers, dealers, and performers from a symbol of sin and avarice (the “devil’s own instrument”) to an emblem of skill and high culture, the model for civilized recreation. In the Royal Albert and other purpose-built concert halls, great symphonies by European composers such as Haydn and Mozart were performed. Across the upper social strata, salon concerts and music lessons became a decorous pastime, with well-to-dos (including Anne Blunt, Baroness Wentworth, who was not the original owner of the Stradivarius that bears her name, though she had it for three decades) relying upon the appraisals of luthiers, auctioneers, and firms like W. E. Hill &amp;amp; Sons to provide the best instruments they could afford.&lt;/p&gt;
    &lt;p&gt;Around the same time, arts and crafts, once considered interchangeable concepts, were diverging. Crafts — with which violin making had largely been allied — became “associated with the body and lowly physical labor, while the fine arts were linked with a higher, contemplative pleasure.” Societal opinion had shifted not just musical tastes but the creation of the instrument into the realm of art.&lt;/p&gt;
    &lt;p&gt;As with all art, the public imagination is an important factor when ascribing quality. Beginning in the nineteenth century, Wilder argues, it was the “communal judgment of a largely middle-class public that now defined cultural borders and values that conformed to a new concept of the artistic masterwork — an absolute, sacrosanct, musical text that contained within itself the life force of its master creator.” Then followed the rise and rise of the indisputable master — in the high-concept compositions of Beethoven, the rediscovery of Bach, and the establishment of the “Old Master” in the sphere of painting.&lt;/p&gt;
    &lt;p&gt;Concurrently with these ideas came various forms of institutional gatekeeping. In classrooms, general repertoire moved away from contemporary works and took on a historical dimension. Already by 1870, concert halls had become museums of sound, with most performed repertoire written by dead composers. “Contemporary composers, like contemporary violin makers,” Wilder writes, “were expected to bow down before their more distinguished predecessors.”&lt;/p&gt;
    &lt;p&gt;Those antecedents had been cemented by W. E. Hill &amp;amp; Sons as much as by the Cremonese families, notably the Amati, Guarneri, and Stradivari households. Profiting from the great man theory of history, these luthiers were perceived to have natural abilities and characteristics that led their creations to become definitive in their field. Indeed, modern violins are largely “artistically derivative” copies of those predecessors: “the product of a protoindustrial process based on an internal mold conceived to ensure consistency.” As Wilder notes, a violin’s worth became less associated with its physical or tonal qualities than with the abstract notions of creativity, genius, and authenticity. Any with a Cremonese pedigree became what the art critic John Berger called a “spiritualized possession.” This is not solely because the public is gullible, or the art world crazy; the urge to own possessions and exhibit ourselves through them is deep-seated in all of us.&lt;/p&gt;
    &lt;p&gt;To have or play a violin created by a recognized master confirmed one’s reputation and gilded the music brought forth from horsehair and catgut. As the Canadian violinist James Ehnes writes in the foreword to A Cultural History of the Violin in Nineteenth-Century London, “The objective beauty of a finely crafted and useful tool becomes enhanced by qualities that are not inherent to the object, but rather to the art that it is capable of producing. Violins become ‘bold,’ ‘colorful,’ or ‘sweet.’ ”&lt;/p&gt;
    &lt;p&gt;Ehnes plays a Stradivarius, the 1715 “Marsick.” While speaking with me recently, ahead of a recital in Birmingham, England, he said that his violin is never out of his sight, nor is it ever played by anyone else. The paradox of the instruments’ overvaluation is not lost on him: “How did we get to this point, where a seventeenth- or eighteenth-century violin, manufactured to be a simple musician’s tool, can now sometimes sell for millions of dollars, often to be locked away in a safe or bank vault, its voice — the reason for its existence — silenced?”&lt;/p&gt;
    &lt;p&gt;Wilder, a seasoned luthier with Wilder &amp;amp; Davis Luthiers Inc., which has workshops in Montreal, Toronto, Banff, and Vancouver, writes with refreshing cynicism about the current scene, not sparing his colleagues and clients: “When today’s musicians demonstrate a continuing partiality for Cremonese masterworks, they are validating the verdicts of their mentors.” He notes that the Western canon “provided the bourgeoisie with the opportunity to wrap itself in history while simultaneously laying claim to artistic authority.” And he smacks down the artistic profession as being “largely about transmuting transcendence into pounds and pence.”&lt;/p&gt;
    &lt;p&gt;Although Wilder rightly rebukes the bloated language of modern auction houses as “clotted twaddle,” his own prose is something of an academic gravy, thickened with sentences like “The alleged autonomous ‘thingness’ of works of canonical music assumes that true value is a function of the created thing itself.” Elsewhere he writes, “Consecration is about the power to impose legitimizing categories of perception and appreciation and to determine what has meaning and what has value — the very core of the production of culture.”&lt;/p&gt;
    &lt;p&gt;The name Stradivarius remains a part of the common lexicon, associated with concepts of excellence, craftsmanship, and wealth. The violins may be rare, excellently made, and, to some, worth the money. But none of that is actually worth a fig if their price outshines their purpose: to provide a little ear tickling and make it pleasurable to be inside your own head.&lt;/p&gt;
    &lt;p&gt;J. R. Patterson was born on a farm in Manitoba. His writing appears widely, including in The Atlantic and National Geographic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://reviewcanada.ca/magazine/2025/10/with-strings-attached-review-a-cultural-history-of-the-violin-in-nineteenth-century-london/"/><published>2025-09-17T04:59:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45294058</id><title>U.S. already has the critical minerals it needs, according to new analysis</title><updated>2025-09-19T19:07:55.472304+00:00</updated><content>&lt;doc fingerprint="e9780551b293659e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;U.S. already has the critical minerals it needs – but they're being thrown away, new analysis shows&lt;/head&gt;
    &lt;p&gt;In new Science article, Colorado School of Mines researchers call for more research, development and policy to increase critical mineral recovery&lt;/p&gt;
    &lt;p&gt;All the critical minerals the U.S. needs annually for energy, defense and technology applications are already being mined at existing U.S. facilities, according to a new analysis published today in the journal Science.&lt;/p&gt;
    &lt;p&gt;The catch? These minerals, such as cobalt, lithium, gallium and rare earth elements like neodymium and yttrium, are currently being discarded as tailings of other mineral streams like gold and zinc, said Elizabeth Holley, associate professor of mining engineering at Colorado School of Mines and lead author of the new paper.&lt;/p&gt;
    &lt;p&gt;"The challenge lies in recovery," Holley said. "It's like getting salt out of bread dough – we need to do a lot more research, development and policy to make the recovery of these critical minerals economically feasible."&lt;/p&gt;
    &lt;p&gt;To conduct the analysis, Holley and her team built a database of annual production from federally permitted metal mines in the U.S. They used a statistical resampling technique to pair these data with the geochemical concentrations of critical minerals in ores, recently compiled by the U.S. Geological Survey, Geoscience Australia and the Geologic Survey of Canada.&lt;/p&gt;
    &lt;p&gt;Using this approach, Holley’s team was able to estimate the quantities of critical minerals being mined and processed every year at U.S. metal mines but not being recovered. Instead, these valuable minerals are ending up as discarded tailings that must be stored and monitored to prevent environmental contamination.&lt;/p&gt;
    &lt;p&gt;“This is a brand-new view of ‘low hanging fruit’ – we show where each critical mineral exists and the sites at which even 1 percent recovery of a particular critical mineral could make a huge difference, in many cases dramatically reducing or even eliminating the need to import that mineral,” Holley said.&lt;/p&gt;
    &lt;p&gt;The analysis in Science looks at a total of 70 elements used in applications ranging from consumer electronics like cell phones to medical devices to satellites to renewable energy to fighter jets and shows that unrecovered byproducts from other U.S. mines could meet the demand for all but two – platinum and palladium.&lt;/p&gt;
    &lt;p&gt;Among the elements included in the analysis are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cobalt (Co): The lustrous bluish-gray metal, a key component in electric car batteries, is a byproduct of nickel and copper mining. Recovering less than 10 percent of the cobalt currently being mined and processed but not recovered would be more than enough to fuel the entire U.S. battery market.&lt;/item&gt;
      &lt;item&gt;Germanium (Ge): The brittle silvery-white semi-metal used for electronics and infrared optics, including sensors on missiles and defense satellites, is present in zinc and molybdenum mines. If the U.S. recovered less than 1 percent of the germanium currently mined and processed but not recovered from U.S. mines, it would not have to import any germanium to meet industry needs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The benefits of enhanced recovery are not only economic and geopolitical but also environmental, Holley said – recovering these critical minerals instead of sending them to tailings piles would reduce the environmental impact of mine waste and open more opportunities for reuse in construction and other industries.&lt;/p&gt;
    &lt;p&gt;“Now that we know which sites are low-hanging fruit, we need to conduct detailed analyses of the minerals in which these chemical elements reside and then test the technologies suitable for recovery of those elements from those specific minerals,” Holley said. “We also need policies that incentivize mine operators to incorporate additional processing infrastructure. Although these elements are needed, their market value may not be sufficient to motivate operators to invest in new equipment and processes without the right policies in place.”&lt;/p&gt;
    &lt;p&gt;Co-authors on the paper are Karlie Hadden, PhD candidate in geology; Dorit Hammerling, associate professor of applied mathematics and statistics; Rod Eggert, research professor of economics and business; Erik Spiller, research professor of mining engineering; and Priscilla Nelson, professor of mining engineering.&lt;/p&gt;
    &lt;p&gt;Read the full paper, "Byproduct recovery from US metal mines could reduce import reliance for critical minerals," on the Science website. To access the data and figures before the paper appears in print, contact Mines Media Relations Specialist Erich Kirshner at erich.kirshner@mines.edu.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.minesnewsroom.com/news/us-already-has-critical-minerals-it-needs-theyre-being-thrown-away-new-analysis-shows"/><published>2025-09-18T19:41:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45294390</id><title>Shipping 100 hardware units in under eight weeks</title><updated>2025-09-19T19:07:55.398897+00:00</updated><content/><link href="https://farhanhossain.substack.com/p/how-we-shipped-100-hardware-units"/><published>2025-09-18T20:11:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45294440</id><title>Apple: SSH and FileVault</title><updated>2025-09-19T19:07:55.334083+00:00</updated><content>&lt;doc fingerprint="201543195f03b8c1"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;apple_ssh_and_filevault(7)&lt;/cell&gt;
        &lt;cell&gt;Miscellaneous Information Manual&lt;/cell&gt;
        &lt;cell&gt;apple_ssh_and_filevault(7)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;code&gt;apple_ssh_and_filevault&lt;/code&gt; —
    SSH and FileVault&lt;/p&gt;
    &lt;p&gt;When FileVault is enabled, the data volume is locked and unavailable during and after booting, until an account has been authenticated using a password. The macOS version of OpenSSH stores all of its configuration files, both system-wide and per-account, in the data volume. Therefore, the usually configured authentication methods and shell access are not available during this time. However, when Remote Login is enabled, it is possible to perform password authentication using SSH even in this situation. This can be used to unlock the data volume remotely over the network. However, it does not immediately permit an SSH session. Instead, once the data volume has been unlocked using this method, macOS will disconnect SSH briefly while it completes mounting the data volume and starting the remaining services dependent on it. Thereafter, SSH (and other enabled services) are fully available.&lt;/p&gt;
    &lt;p&gt;The capability to unlock the data volume over SSH appeared in macOS 26 Tahoe.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;1 July, 2025&lt;/cell&gt;
        &lt;cell&gt;Darwin&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://keith.github.io/xcode-man-pages/apple_ssh_and_filevault.7.html"/><published>2025-09-18T20:15:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45295898</id><title>Want to piss off your IT department? Are the links not malicious looking enough?</title><updated>2025-09-19T19:07:55.177344+00:00</updated><content>&lt;doc fingerprint="1ee9dc214d0432b1"&gt;
  &lt;main&gt;
    &lt;p&gt;This tool is guaranteed to help with that!&lt;/p&gt;
    &lt;p&gt;This is a tool that takes any link and makes it look malicious. It works on the idea of a redirect. Much like https://tinyurl.com/ for example. Where tinyurl makes an url shorter, this site makes it look malicious.&lt;/p&gt;
    &lt;p&gt;Place any link in the below input, press the button and get back a fishy(phishy, heh...get, it?) looking link. The fishy link doesn't actually do anything, it will just redirect you to the original link you provided.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://phishyurl.com/"/><published>2025-09-18T22:40:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45296638</id><title>David Lynch LA House</title><updated>2025-09-19T19:07:54.978689+00:00</updated><content>&lt;doc fingerprint="2aa02ad177176fe2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tour David Lynch's house as it hits the market&lt;/head&gt;
    &lt;p&gt;David Lynch's LA estate is for sale at $15m, and the listing pictures offer a glimpse into the late filmmaker's aesthetic and creative universe&lt;/p&gt;
    &lt;p&gt;David Lynch, the visionary American filmmaker behind Twin Peaks, Blue Velvet and Mulholland Dr, passed away this January, yet his creative universe endures in objects, spaces and ideas.&lt;/p&gt;
    &lt;p&gt;Among the most striking of these relics is his larger-than-life, meticulously designed Hollywood Hills home; a cinematic setting in its own right. Perched on a sweeping 2.3-acre hillside, David Lynch’s private compound, which is now listed for $15 million by Marc Silver of The Agency, unfolds like one of his own intricately plotted storylines. A showcase of Mid-Century modern architecture, the estate was conceived with the same care and cinematic precision that defined his work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inside David Lynch's Los Angeles estate&lt;/head&gt;
    &lt;p&gt;The property, set across five contiguous parcels, reads like a storyboard in relief: three main residences and several ancillary structures stepping down the hillside, each capturing a different note in Lynch’s creative oeuvre.&lt;/p&gt;
    &lt;p&gt;The story behind this compound started in 1987, when he acquired the pink-hued Beverly Johnson House designed in the early 1960s by Lloyd Wright, son of Frank Lloyd Wright. The home, in fact, was recognised by Historic Places LA as an exemplary work of Mid-Century Modern residential design. Then in 1991, he commissioned Eric Lloyd Wright (Lloyd Wright’s son) to add a pool and pool house, extending the Wright imprint on his property with a new generation.&lt;/p&gt;
    &lt;p&gt;Across the years, Lynch kept expanding the plotline: in 1989, he purchased an adjoining two-bedroom Brutalist house; in 1995, a studio building; and later, more pieces of land, ultimately shaping a seven-structure sanctuary with 10 bedrooms and 11 bathrooms spread over roughly 11,000 square feet. The result was a creative campus perched above the city.&lt;/p&gt;
    &lt;p&gt;At the heart of the compound lies the architectural crescendo – the approximately 2,000 square feet home where light pours through generous windows and skylights to rake across organic textures and bold geometries. The facade’s cement chevrons catch the sun; inside, simple metalwork and natural woods are drenched in material honesty that often surfaced in Lynch’s films.&lt;/p&gt;
    &lt;p&gt;Two neighbouring addresses deepen the lore: 7029 Senalda served as the home of Asymmetrical Productions, while 7035 Senalda attained near-mythic status as both the Madison residence in the movie Lost Highway and Lynch’s own studio, complete with a library, screening room and editing suite – spaces where he refined major works, including Mulholland Drive.&lt;/p&gt;
    &lt;p&gt;Receive our daily digest of inspiration, escapism and design stories from around the world direct to your inbox.&lt;/p&gt;
    &lt;p&gt;Beyond the exemplary structures, Lynch left a personal handprint, collaborating on additional buildings: a sculptural two-storey guest house and a one-bedroom retreat finished in his favoured smooth grey plaster. Outdoors the terraces, courtyards and planted walkways offer a counterpoint to the intensity of production and everyday life.&lt;/p&gt;
    &lt;p&gt;As a listing note from The Agency suggests, this is a 'creative sanctuary and architectural landmark,' with provenance unlike any other in Los Angeles. For admirers of Lynch, it reads as both home and archive: a lived-in factory of ideas, meticulously composed and, at last, ready for its next act.&lt;/p&gt;
    &lt;p&gt;Aditi Sharma is a content specialist with 14 years of experience in the design and lifestyle space. She specialises in producing content that resonates with diverse audiences, bridging global trends with local stories, and translating complex ideas into engaging, accessible narratives.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Setchu unveils minimalist fragrances that smell like river fish and tatami mats&lt;p&gt;The brand led by celebrated young designer Satoshi Kuwata unveils a range of six fragrances that combine Japanese and Western influences&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Diamond Suite Eleven is a Monegasque escape of epic proportions&lt;p&gt;Experience 195 sq m of seaside calm at this new suite atop the Monte-Carlo Bay Hotel &amp;amp; Resort&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; London Fashion Week S/S 2026: live updates from the Wallpaper* team&lt;p&gt;From 18-22 September, London Fashion Week arrives in the British capital. Follow along for a first look at the shows, presentations and other style happenings, as seen by the Wallpaper* editors&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wallpaper.com/design-interiors/david-lynch-house-los-angeles-for-sale"/><published>2025-09-19T00:30:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45297066</id><title>Help Us Raise $200k to Free JavaScript from Oracle</title><updated>2025-09-19T19:07:54.786216+00:00</updated><content>&lt;doc fingerprint="e7214019db839d69"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Help Us Raise $200k to Free JavaScript from Oracle&lt;/head&gt;
    &lt;p&gt;After more than 27,000 people signed our open letter to Oracle about the “JavaScript” trademark, we filed a formal Cancellation Petition with the US Patent and Trademark Office. Ten months in, we’re finally reaching the crucial discovery phase.&lt;/p&gt;
    &lt;p&gt;Deno initiated this petition since we have legal standing as a JavaScript runtime, but it’s really on behalf of all developers. If we win, “JavaScript” becomes public domain – free for all developers, conferences, book authors, and companies to use without fear of trademark threats.&lt;/p&gt;
    &lt;p&gt;We’re asking for your support through our GoFundMe campaign so we can put forward the strongest case possible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why $200k?&lt;/head&gt;
    &lt;p&gt;Because federal litigation is expensive. Discovery is the most resource-intensive stage of litigation, where evidence is collected and arguments are built.&lt;/p&gt;
    &lt;p&gt;We don’t want to cut corners – we want to make the best case possible by funding:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Professional public surveys that carry legal weight in front of the USPTO, proving that “JavaScript” is universally recognized as the name of a language, not Oracle’s brand.&lt;/item&gt;
      &lt;item&gt;Expert witnesses from academia and industry to testify on JavaScript’s history, usage, and meaning.&lt;/item&gt;
      &lt;item&gt;Depositions and records from standards bodies, browser vendors, and industry leaders showing Oracle has no role in the language’s development.&lt;/item&gt;
      &lt;item&gt;Legal filings and responses to counter Oracle’s claims at every step.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If there are leftover funds, we’ll donate them to the OpenJS to continue defending civil liberties in the digital space. None of the funds will go to Deno.&lt;/p&gt;
    &lt;head rend="h3"&gt;Oracle officially denies “JavaScript” is generic&lt;/head&gt;
    &lt;p&gt;On August 6th, 2025, Oracle for the first time addressed the validity of the trademark. Their response to our petition denies that “JavaScript” is a generic term.&lt;/p&gt;
    &lt;p&gt;If you’re a web developer, it’s self-evident that Oracle has nothing to do with JavaScript. The trademark system was never meant to let companies squat on commonly-used names and rent-seek – it was designed to protect active brands in commerce. US law makes this distinction explicit.&lt;/p&gt;
    &lt;p&gt;We urge you to read our petition and open letter to understand our arguments.&lt;/p&gt;
    &lt;p&gt;If we don’t win discovery, Oracle locks in ownership of the word “JavaScript.” This is the decisive moment.&lt;/p&gt;
    &lt;p&gt;But this case is bigger than JavaScript. It’s about whether trademark law works as written, or whether billion-dollar corporations can ignore the rule that trademarks cannot be generic or abandoned. “JavaScript” is obviously both. If Oracle wins anyway, it undermines the integrity of the whole system.&lt;/p&gt;
    &lt;p&gt;Let’s make sure the law holds. Please donate. Please share and upvote this.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://deno.com/blog/javascript-tm-gofundme"/><published>2025-09-19T01:40:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45298034</id><title>The health benefits of sunlight may outweigh the risk of skin cancer</title><updated>2025-09-19T19:07:54.694331+00:00</updated><content/><link href="https://www.economist.com/science-and-technology/2025/09/17/the-health-benefits-of-sunlight-may-outweigh-the-risk-of-skin-cancer"/><published>2025-09-19T04:46:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45298336</id><title>Nostr</title><updated>2025-09-19T19:07:54.419116+00:00</updated><content>&lt;doc fingerprint="4738d95838432210"&gt;
  &lt;main&gt;&lt;p&gt;Nostr is an apolitical communication commons. A simple standard that defines a scalable architecture of clients and servers that can be used to spread information freely. Not controlled by any corporation or government, anyone can build on Nostr and anyone can use it.&lt;/p&gt;&lt;p&gt;Nostr embraces the chaos of the early internet—multiple kinds of data, diverse forms of user interaction and different clients providing their own perspectives over the same underlying information.&lt;/p&gt;&lt;p&gt;The client is the app that is running on your computer or phone, the server is whatever is running on a cloud somewhere with a domain name. In centralized platforms and other protocols, one client talks to a single server. In Nostr clients connect to many.&lt;/p&gt;&lt;p&gt;Nostr's single unit of information is a cryptographically signed note, these are created by users in their client software and published to one or more relays.&lt;/p&gt;&lt;p&gt;Clients are smart and act as agents for the users who install them. They decide which relays to connect to and when and what data to request according to the circumstance and user preferences.&lt;/p&gt;&lt;p&gt;These are the servers that notes are published to and read from. They cannot change the contents of notes (that would invalidate the signature), but they can decide what to store and for how long.&lt;/p&gt;&lt;p&gt;In Nostr, every user is represented by a secret number called a "key" and every message carries a digital "signature" that proves its authorship authorship and authenticity without the need for any authority to say so. This foundation of trust enables the decentralized broadcasting of information.&lt;/p&gt;&lt;p&gt;Nostr doesn't subscribe to political ideals of "free speech" — it simply recognizes that different people have different morals and preferences and each server, being privately owned, can follow their own criteria for rejecting content as they please and users are free to choose what to read and from where.&lt;/p&gt;&lt;p&gt;When the network effect is not tied to a single organization a group of users cannot harm others.&lt;/p&gt;Watch ›&lt;p&gt;If you are a programmer or know how to run servers it is trivial to run your own relay with your own rules.&lt;/p&gt;Write code ›&lt;p&gt;Besides being a natural medium for a Twitter-like microblogging social network, Nostr can also be used for other purposes. And not only similar things like sharing videos, longform articles, pictures or voice notes. There are initiatives on Nostr for the development of sub-protocols that power closed groups, decentralized wikipedia, couchsurfing, marketplaces or web annotations; as well as protocols that don't use Nostr for the core data but as a coordination and discovery mechanism, such as decentralized code collaboration using git, file hosting, torrent sharing and video livestreaming.&lt;/p&gt;Browse the NIPs&lt;p&gt;Nostr is an idea with a lot of open-source software around it and a large userbase, but not a finished, polished product that you can buy without stress. We're still pretty much in the phase where new programmers and early adopters are needed to help us refine the protocol flows and the user experience.&lt;/p&gt;&lt;p&gt;The so-called "outbox model" is the canonical way of implementing a censorship-resistant client, but its parameters are fluid.&lt;/p&gt;Learn about it ›&lt;p&gt;NIP-29 describes a way to do closed groups for forums or chat that can be very efficient by relying on a relay but are still censorship-resistant.&lt;/p&gt;Read the guide ›&lt;p&gt;Nostr enables true freedom by allowing users to stay connected to their audience even in adverse scenarios.&lt;/p&gt;&lt;p&gt;Patricia publishes a special event announcing which servers she will use as her "outbox relay" for publishing notes. She then starts sending all her posts to these chosen relays, creating a predictable place where others can find her content.&lt;/p&gt;&lt;p&gt;When Florian wants to read Patricia's posts, his client looks up her announcement and connects directly to her announced outbox relays.&lt;/p&gt;&lt;p&gt;This targeted approach is simple and efficient.&lt;/p&gt;&lt;p&gt;If Patricia's current relay bans her or shuts down, she can easily switch to different relays — perhaps some run by friends, a paid service, or her own server.&lt;/p&gt;&lt;p&gt;She simply publishes a new announcement and continues to post without losing her audience.&lt;/p&gt;&lt;p&gt;Florian's client continuously monitors for updates to Patricia's relay list.&lt;/p&gt;&lt;p&gt;When she switches relays, his client eventually starts querying her new locations, ensuring he never misses her posts even as the network topology changes.&lt;/p&gt;&lt;p&gt;From the publisher side to the follower side clients behave smartly, keeping a local state and reacting to new information in order to ensure that the flow of information continues.&lt;/p&gt;&lt;p&gt;After all, these notes are important, it would be sad to miss too many of them.&lt;/p&gt;&lt;p&gt;It may sound like Nostr is very good, but what about these hard issues?&lt;/p&gt;&lt;p&gt;A protocol is like a common language that multiple different software can use to talk to each other, it's like e-mail, HTML or HTTP.&lt;/p&gt;&lt;p&gt;When we say "protocol" we mean that there is no need to use a specific app in order to be in Nostr: there are many apps that talk the same language and can be used (mostly) interchangeably — and each has its own take on how to do and display things.&lt;/p&gt;&lt;p&gt;In the default feed you never see any spam, because clients will only fetch information from people that you follow. In that sense no one can "push" spam into you.&lt;/p&gt;&lt;p&gt;It's trickier when you want to see, for example, replies to your posts, in that case a client might be programmed to fetch anything that claims to be a reply from anyone, which might include spam.&lt;/p&gt;&lt;p&gt;The way we can deal with it on Nostr is by restricting our area of contact with the spam: for example, some clients may easily decide to only display replies that come from people followed by people you follow. More refined strategies involve announcing and then only reading notes from relays known to be "safe" according to your criteria (could be relays that require payment, relays that do screening for humans, relays that only accept members of certain communities or political affiliations etc).&lt;/p&gt;&lt;p&gt;There are no perfect solutions. But these do not exist anywhere, centralized platforms are also full of spam. Nostr at least isn't naïve and tries to build resiliency from the start.&lt;/p&gt;&lt;p&gt;Yes, Nostr is just a basic client-server architecture. And the fact that users can naturally spread among hundreds of different relays while clients can query dozens of relays that they're interested in at the same time means the network has a natural load balancer (which doesn't prevent a single relay from having its own internal load balancer either).&lt;/p&gt;&lt;p&gt;Another (almost the opposite) concern that may be raised is with problems arising from clients having to connect to too many relays if the profiles being followed for whatever reason decide to spread way too much, but this shouldn't be a problem either because people tend to follow many accounts with similar content and these will tend to share relays. Still, if it happens, it's cheap for native apps to open many hundreds of WebSocket connections simultaneously (as they will be getting very few data in each of those). For web apps that isn't so hard, but we can still go up to a few hundreds without big problems. Regardless of any of that, in any complete enough app that wants to display a "following feed" it's already necessary to store events in a local database, and that will make all these issues easy to deal with as you can do the event requests in batches instead of all at once.&lt;/p&gt;&lt;p&gt;Harassment is similar to spam in the sense that anyone can still create the undesired content and publish to the relays that accept them. All the techniques mentioned in avoiding spam can also be applied in this case, but if we're talking about specific individuals with a permanent identity and not only an army of bots in this case the problem becomes easier, as those individuals can just be blocked by their target and their content will vanish. Presumably friends of such target will also block, and creative solutions involving shared blocklists can be created such that some people don't even have to click the block button directly.&lt;/p&gt;&lt;p&gt;Other approaches involving, for example, relays with restricted read (that can emulate "protected account"/"only friends" features seen in centralized platforms) can further improve this.&lt;/p&gt;&lt;p&gt;There are many problems with Mastodon, mostly due to the fact that it doesn't rely on any cryptography. Because it cannot do the multi-master approach of Nostr due to lack of cryptography, identities are assumed to be "owned" by the server, which is fully trusted by its tenants. Mastodon server owners can do all the harm centralized platforms can do to their underlings, which are completely helpless in case of misbehavior or even in the normal case where a server owner loses their server or decides to shut down for whatever reason.&lt;/p&gt;&lt;p&gt;Worse than that, for many of its purported features, such as blocking or direct messages, users have to also trust owners of the other servers.&lt;/p&gt;&lt;p&gt;There are also problems with reliance on the DNS system, but we don't have to talk about those.&lt;/p&gt;&lt;p&gt;The most interesting feature of Mastodon is that by its nature it creates communities with shared values that grow in each of its servers. Or, should I say, that should be a feature if it actually worked like that. In fact these are not really communities, but a mashup of users that may share some interests among each other, but also have other interests and those other interests end up polluting the supposed "community" with things that do not interest the other users.&lt;/p&gt;&lt;p&gt;Nostr, on the other hand, can create real communities around relays, specifically because users don't have to fully belong to those relays, but can go to them only for some of their needs and go to other relays for other needs.&lt;/p&gt;&lt;p&gt;Bluesky has many problems, the two most pronounced are:&lt;/p&gt;&lt;code&gt;Relay-AppView-Client&lt;/code&gt; flow assumes only
                one canonical source of data at each step (unlike Nostr multi-master architecture)
                that source is always a server that has power to censor, shadowban, reorder data and
                so on.
              &lt;p&gt;&lt;code&gt;Clients&lt;/code&gt; are assumed to be dumb and trust the &lt;code&gt;AppView&lt;/code&gt;, and
              here you have room for all sorts of undesired shenanigans. Then AppViews also assume
              to source their data from a single &lt;code&gt;Relay&lt;/code&gt;, and here you have room for the
              same effect.
            &lt;/p&gt;&lt;p&gt; You could argue that Bluesky &lt;code&gt;Clients&lt;/code&gt; could become smart and start
              sourcing data from multiple &lt;code&gt;AppViews&lt;/code&gt;, or from multiple Relays, or that
              the &lt;code&gt;AppViews&lt;/code&gt; could rely on multiple &lt;code&gt;Relays&lt;/code&gt;, or that the
              &lt;code&gt;Clients&lt;/code&gt; could talk directly to the &lt;code&gt;PDSes&lt;/code&gt; — and all of that
              is possible and would indeed bring solutions, but notice that if those things started
              happening Bluesky would end up becoming Nostr, except with more steps.
            &lt;/p&gt;&lt;p&gt;Yes, this clip answers it well.&lt;/p&gt;&lt;p&gt;But basically the answer is the same as the question about scale: if users can go to whatever relay they want we'll see relays ran by all sorts of people and entities. Running servers is very cheap, and a relay can run on a $5/mo server and house at least a few thousand users. It's not hard to imagine relays ran by communities, individuals who just want to be useful to others, big organizations wanting to gain good will with some parts of the public, but also companies, client makers, and, of course, dedicated entities who sell relay hosting for very cheap.&lt;/p&gt;&lt;p&gt;It's not a feature of the world at large to be able to see or hear everything that is happening everywhere at all times. Nostr inherits that property from the world, making it so that you can only see what you focus your attention on (and you're allowed to see by the relay that hosts that information).&lt;/p&gt;&lt;p&gt;It's only possible to search on what you have seen, so search engines will always have to crawl some parts of the network they chose to and index those to enable public search. The word "chose" is employed because, as we know, there can't be a "global" view of the network (and no one would want such a thing anyway as it would be full of spam), so indexers have to choose. This is not different from Google deciding what websites to index.&lt;/p&gt;&lt;p&gt;On the other hand, it's surprisingly doable for clients to store all the posts from people you follow, or all the posts you have seen or interacted with over time (since it's just text, a huge amount of notes can fit in the same space that would otherwise be required to store a single photo, for example) then provide local search over that. That kind of search will be sufficient for most of the cases you would reach out for a search bar in a centralized platform (which is to search for things that you have seen before), and perhaps even more useful since it would naturally filter out all the unrelated garbage.&lt;/p&gt;&lt;p&gt;Last, niche or community-oriented relays can also provide very useful search capabilities by just indexing the notes they have stored locally, already filtered and scoped to that relay's topic or cohort (imagine searching over a Discord, Slack or Telegram group, for example).&lt;/p&gt;&lt;p&gt;The most basic way to do that is by following the natural habits used by most centralized social platforms users since a long time ago: by looking at the people you follow and whom they're interacting with.&lt;/p&gt;&lt;p&gt;But also it's not true that Nostr doesn't have algorithms. Nostr can have algorithms of all kinds: manual, automatic, AI-powered or rule-based. Some of these algorithms can be run entirely locally on clients (for example, surfacing posts from the times when you were not online, or from people that make fewer posts), while other algorithms can be provided by all sorts of relays, either by naturally surfacing posts from a community of people you don't follow or by dedicated relays that have the stated purpose of curating content desirable for a target audience or even by targeting specific users.&lt;/p&gt;&lt;p&gt;Nostr uses the same cryptographic principles of Bitcoin and was kickstarted mostly by a community of Bitcoiners, so it has disproportionately attracted the attention of Bitcoiners at the start, but aside from that it doesn't have any relationship with Bitcoin. It doesn't depend on Bitcoin for anything and you don't have to know or have or care about any Bitcoin in order to use Nostr.&lt;/p&gt;&lt;p&gt;What about "zaps"? Zaps are a standard for tipping Nostr content using Bitcoin that is implemented by some Nostr clients, but it's fully and completely optional and if you don't care about Bitcoin you don't have to bother about it.&lt;/p&gt;&lt;p&gt;Quotes from those who know better and decided to like Nostr.&lt;/p&gt;&lt;quote&gt;No one owns Nostr, no one can own Nostr, it's an open-source protocol. No one can build a fence to stop the flow of information.— Uncle Bob&lt;/quote&gt;&lt;quote&gt;Nostr is an open protocol.— Edward SnowdenIf a platform is a silo, a protocol is a river: no one owns it, and everyone is free to swim.&lt;/quote&gt;&lt;quote&gt;I want to claim that Nostr has discovered a new fundamental architecture for distributed protocols. Not federated, not P2P.— Gordon Brander&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nostr.com/"/><published>2025-09-19T05:49:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45299170</id><title>Ruby Central's Attack on RubyGems [pdf]</title><updated>2025-09-19T19:07:54.211409+00:00</updated><content/><link href="https://pup-e.com/goodbye-rubygems.pdf"/><published>2025-09-19T08:09:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45299625</id><title>Statistical Physics with R: Ising Model with Monte Carlo</title><updated>2025-09-19T19:07:53.781710+00:00</updated><content>&lt;doc fingerprint="d9f7ce022b3b170f"&gt;
  &lt;main&gt;
    &lt;p&gt;Classical Ising Model is a land mark system in statistical physics. The model explains the physics of spin glasses and magnetic materials, and cooperative phenomenon in general, for example phase transitions and neural networks. This package provides utilities to simulate one dimensional Ising Model with Metropolis and Glauber Monte Carlo with single flip dynamics in periodic boundary conditions. Utility functions for exact solutions are provided.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Effective ergodicity in single-spin-flip dynamics&lt;lb/&gt;Mehmet Suezen, Phys. Rev. E 90, 032141&lt;lb/&gt;Dataset&lt;/item&gt;
      &lt;item&gt;Anomalous diffusion in convergence to effective ergodicity, Suezen, Mehmet, arXiv:1606.08693&lt;lb/&gt;Dataset&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/msuzen/isingLenzMC"/><published>2025-09-19T09:19:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45300615</id><title>Dynamo AI (YC W22) Is Hiring a Senior Kubernetes Engineer</title><updated>2025-09-19T19:07:53.098408+00:00</updated><content>&lt;doc fingerprint="a770c0f34e35ef6a"&gt;
  &lt;main&gt;
    &lt;p&gt;Compliant-Ready AI for the Enterprise&lt;/p&gt;
    &lt;p&gt;Dynamo AI is building the future of secure, scalable AI systems. Our platform helps enterprises safely deploy powerful AI models in production, with reliability, control, and trust at the core. We’re a team of builders working at the intersection of machine learning, infrastructure, and security.&lt;/p&gt;
    &lt;p&gt;As a Senior Kubernetes Engineer, you’ll lead the full onboarding journey for our enterprise customers — from first engagement to successful production rollout. You will own the deployment of Dynamo AI clusters (Kubernetes-based) into customer environments and serve as the technical bridge between our product and the customer’s infrastructure.&lt;/p&gt;
    &lt;p&gt;This is a deeply hands-on and customer-facing role. You’ll work with Kubernetes, Helm, and cloud-native tools to deliver secure, scalable deployments of cutting-edge AI systems. You’ll partner with engineering, product, and leadership to bring customer feedback directly into our roadmap and shape how AI is adopted across industries. Serving this role, you will grow into an expert in building the most cutting-edge enterprise-level AI systems.&lt;/p&gt;
    &lt;p&gt;This role will work with the U.S. government clients, so it requires U.S. government security clearance or US citizenship. Our company policy also requires in-office presence in San Francisco or New York office for 2-3 days per week.&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;The enterprise platform for enabling private, secure, and regulation-compliant Gen AI models.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/dynamo-ai/jobs/fU1oC9q-senior-kubernetes-engineer"/><published>2025-09-19T12:00:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45300865</id><title>Ants that seem to defy biology – they lay eggs that hatch into another species</title><updated>2025-09-19T19:07:52.961046+00:00</updated><content>&lt;doc fingerprint="86d0118d0e3c41d7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;These Ant Queens Seem to Defy Biology: They Lay Eggs That Hatch Into Another Species&lt;/head&gt;
    &lt;head rend="h2"&gt;Iberian harvester ant queens produce offspring of their own species and of the builder harvester ant, seemingly by cloning males&lt;/head&gt;
    &lt;p&gt;Iberian harvester ant queens have a unique superpower: They can lay eggs that hatch into an entirely different species.&lt;/p&gt;
    &lt;p&gt;This discovery, described in a new paper published September 3 in the journal Nature, defies a fundamental principle of biology and may cause scientists to reconsider how they define a species.&lt;/p&gt;
    &lt;p&gt;“The classic concept says that [a species] is a group of organisms with similar physical and genetic characteristics that can reproduce with each other in nature and produce fertile offspring,” says Xim Cerdá, an ecologist at Doñana Biological Station in Spain who was not involved with the research, to Miguel Ángel Criado at El País. “But it turns out that’s not the case; two species are needed here. We’re going to have to rethink the concept.”&lt;/p&gt;
    &lt;head rend="h4"&gt;Did you know? How many ants are on the planet?&lt;/head&gt;
    &lt;p&gt;Scientists estimate that 20 quadrillion ants are crawling around the Earth, according to a 2022 study.&lt;/p&gt;
    &lt;p&gt;Scientists recently discovered that Iberian harvester ant queens (Messor ibericus) mate with males of another species, the builder harvester ant (Messor structor). When they do, the M. ibericus queens store the M. structor male’s sperm, then use it to fertilize some of the eggs they lay. Researchers think the M. ibericus queens remove their own genetic material from the eggs’ nuclei, so that when those eggs hatch, they effectively turn out to be M. structor male clones.&lt;/p&gt;
    &lt;p&gt;The queens produce males of both M. ibericus and M. structor, and all the worker ants in M. ibericus colonies are female hybrids of the two species.&lt;/p&gt;
    &lt;p&gt;“It’s an absolutely fantastic, bizarre story of a system that allows things to happen that seem almost unimaginable,” says Jacobus Boomsma, an evolutionary biologist at the University of Copenhagen who was not involved with the research, to Nature’s Max Kozlov.&lt;/p&gt;
    &lt;p&gt;Even more perplexing is the fact that M. ibericus and M. structor are not closely related, evolutionarily speaking. The two species diverged more than five million years ago, according to the paper. For comparison, scientists think humans and chimpanzees split from a common ancestor that lived between six million and eight million years ago.&lt;/p&gt;
    &lt;p&gt;Proving the relationship between M. ibericus and M. structor was challenging. The scientists dug up various M. ibericus colonies they found along the sides of farm roads near Lyon, France, looking for male ants. But among a colony of 10,000 ants, there might be only a few males, writes Science’s Erik Stokstad.&lt;/p&gt;
    &lt;p&gt;In the end, they found 132 males from 26 M. ibericus colonies. Of those, about half were nearly hairless—a hallmark of M. structor—while the others were covered in dense hair, a trait typically found in M. ibericus. DNA testing confirmed their hunch: The hairy males were M. ibericus, and the bald ones were M. structor.&lt;/p&gt;
    &lt;p&gt;Even more intriguing, the males of both species shared M. ibericus mitochondrial DNA, which is inherited from the mother, suggesting they had all been born from M. ibericus queens.&lt;/p&gt;
    &lt;p&gt;This discovery is so novel and so unusual that the researchers had to come up with a new term to describe the behavior exhibited by M. ibericus queens: “xenoparity,” which essentially means “foreign birth.”&lt;/p&gt;
    &lt;p&gt;The team also wanted to go beyond genetic evidence: They hoped to observe births of M. structor ants from an M. ibericus queen. So, they reared colonies in their laboratory. Then, they waited.&lt;/p&gt;
    &lt;p&gt;“It was very difficult, because in lab conditions, it’s nearly impossible to have males,” says co-author Jonathan Romiguier, an ecologist at the University of Montpellier in France, to New Scientist’s Tim Vernimmen. “We had something like 50 colonies and monitored them for two years without a single male being born. Then we got lucky.” Observing the births of M. structor males was another key piece of evidence in describing the ants’ strange biology.&lt;/p&gt;
    &lt;p&gt;As for the M. ibericus males in the colony, the queens mate with them to produce the next generation of M. ibericus queens.&lt;/p&gt;
    &lt;p&gt;But why do M. ibericus queens clone M. structor males? Scientists aren’t totally sure, but they say the partnership must be beneficial to both species.&lt;/p&gt;
    &lt;p&gt;For M. ibericus, this adaptation ensures they have plenty of workers, which are responsible for many important tasks in a colony, including building the nest, gathering food and raising the larvae. The arrangement also keeps M. structor males around for future M. ibericus queens to mate with, even in places without M. structor colonies. Shockingly, M. structor colonies are only found in mountainous areas across a small range. But by transporting the M. structor male clones around, M. ibericus has allowed that species to spread to new places.&lt;/p&gt;
    &lt;p&gt;However, the unique setup might not last forever. Because the M. structor males are clones and do not appear to be mating with members of their own species, they are probably accumulating harmful genetic mutations, which makes them more vulnerable in the long run, reports New Scientist. But, for now, the relationship seems to be working.&lt;/p&gt;
    &lt;p&gt;“Every step in this coevolutionary game makes perfect sense and uses the entire toolbox of reproductive tricks that we know ants are capable of employing,” says Sara Helms Cahan, an evolutionary ecologist at the University of Vermont who was not involved with the research, to Science. “The end result is fantastical but incredibly successful, with one species carrying another in its pocket, as it were, all over southern Europe.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.smithsonianmag.com/smart-news/these-ant-queens-seem-to-defy-biology-they-lay-eggs-that-hatch-into-another-species-180987292/"/><published>2025-09-19T12:25:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45301845</id><title>As Android developer verification gets ready to go, a new reason to be worried</title><updated>2025-09-19T19:07:52.748928+00:00</updated><content>&lt;doc fingerprint="3f2eb1338bbe2cdb"&gt;
  &lt;main&gt;
    &lt;p&gt;Affiliate links on Android Authority may earn us a commission. Learn more.&lt;/p&gt;
    &lt;head rend="h1"&gt;As Android developer verification gets ready to go, here's a new reason to be worried&lt;/head&gt;
    &lt;p&gt;September 18, 2025&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Google is putting together its framework for Android developer verification, connecting dev names to even sideloaded apps.&lt;/item&gt;
      &lt;item&gt;Recent additions to the Android SDK offer a little insight into how the system may ultimately operate.&lt;/item&gt;
      &lt;item&gt;One variable suggests that users may not be able to sideload even verified apps without an active network connection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Android’s approach to software openness is changing in some fundamental ways right now, and the shift has not been happening without a fair amount of controversy. While Google has always let you install Android apps from outside its managed app ecosystem, sideloading their APK files, the company will now start mandating that developers register their identity, and block the installation of apps from unverified sources.&lt;/p&gt;
    &lt;p&gt;For fans of open platforms, that’s resulted in some spicy takes (my own included), but in the weeks since the news first broke we’ve learned a little more about Google’s plans for implementing this program — and hearing about some critical workarounds, like maintaining the ability to sideload unverified apps over a connection to another device running ADB (the Android Debug Bridge).&lt;/p&gt;
    &lt;p&gt;Don’t want to miss the best from Android Authority?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set us as a favorite source in Google Discover to support us and make sure you never miss our latest exclusive reports, expert analysis, and much more. You can also set us as a preferred source in Google Search — find out more here.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While we’re breathing a little easier now that we know about that option, there are still plenty of headaches that this move could cause, and today we’re thinking about one that’s been brought up by Android fans on Reddit like user WesternImpression394. There, they’ve spotted some of the groundwork Google’s been laying in the Android SDK (not the AOSP, as claimed in that thread) to support developer verification.&lt;/p&gt;
    &lt;p&gt;One of the variables defined there is labeled in a way that immediately gets our attention: &lt;code&gt;DEVELOPER_VERIFICATION_FAILED_REASON_NETWORK_UNAVAILABLE&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When you stop and think about it for a moment, that makes all the sense in the world — Google isn’t just interested in attaching someone’s name to all those anonymous APKs floating around out there, but presumably doing so in a way that allows the company to take action based on the name, like blacklisting devs who spread malware. And indeed, there’s a similar &lt;code&gt;DEVELOPER_VERIFICATION_FAILED_REASON_DEVELOPER_BLOCKED&lt;/code&gt; variable. While it’s easy enough to verify something like a cryptographic signature locally, Android might want to prevent you from installing an app if it can’t get online and check if the name is on just such a no-no list.&lt;/p&gt;
    &lt;p&gt;Admittedly, this probably won’t cause a problem for most users ever, and we are looking at some kind of extreme corner case situation where you’ve already downloaded an APK, but no longer have network connectivity, nor access to a device running ADB (or an app already installed to run ADB commands locally). There could even be a cached copy of the ban list that would let you install offline up to a point. That said, Android has literally billions of users, and even rare situations will probably happen for someone eventually.&lt;/p&gt;
    &lt;p&gt;We’ve still got a year to go before developer verification starts actually impacting any Android end-users, although devs will start signing up in the months to come. That leaves plenty of time for us to learn more about the all-too-important details behind how the system will ultimately work — and hopefully, plan ahead for how to work around it for users who genuinely need to.&lt;/p&gt;
    &lt;p&gt;Thank you for being part of our community. Read our Comment Policy before posting.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.androidauthority.com/android-sideload-offline-3598988/"/><published>2025-09-19T14:07:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45302065</id><title>I regret building this $3000 Pi AI cluster</title><updated>2025-09-19T19:07:52.585398+00:00</updated><content>&lt;doc fingerprint="27f255577da246bd"&gt;
  &lt;main&gt;
    &lt;p&gt;I ordered a set of 10 Compute Blades in April 2023 (two years ago), and they just arrived a few weeks ago. In that time Raspberry Pi upgraded the CM4 to a CM5, so I ordered a set of 10 16GB CM5 Lite modules for my blade cluster. That should give me 160 GB of total RAM to play with.&lt;/p&gt;
    &lt;p&gt;This was the biggest Pi cluster I've built, and it set me back around $3,000, shipping included:&lt;/p&gt;
    &lt;p&gt;There's another Pi-powered blade computer, the Xerxes Pi. It's smaller and cheaper, but it just wrapped up its own Kickstarter. Will it ship in less than two years? Who knows, but I'm a sucker for crowdfunded blade computers, so of course I backed it!&lt;/p&gt;
    &lt;p&gt;But my main question, after sinking in a substantial amount of money: are Pi clusters even worth it anymore? I There's no way this cluster could beat the $8,000, 4-node Framework Desktop cluster in performance. But what about in price per gigaflop, or in efficiency or compute density?&lt;/p&gt;
    &lt;p&gt;There's only one way to find out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Compute Blade Cluster Build&lt;/head&gt;
    &lt;p&gt;I made a video going over everything in this blog post—and the entire cluster build (and rebuild, and rebuild again) process. You can watch it here, or on YouTube:&lt;/p&gt;
    &lt;p&gt;But if you're on the blog, you're probably not the type to sit through a video anyway. So moving on...&lt;/p&gt;
    &lt;head rend="h2"&gt;Clustering means doing everything over n times&lt;/head&gt;
    &lt;p&gt;In the course of going from 'everything's in the box' to 'running AI and HPC benchmarks reliably', I rebuilt the cluster basically three times:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;First, my hodgepodge of random NVMe SSDs laying around the office was unreliable. Some drives wouldn't work with the Pi 5's PCIe bus, it seems, other ones were a little flaky (there's a reason these were spares sitting around the place, and not in use!)&lt;/item&gt;
      &lt;item&gt;After replacing all the SSDs with Patriot P300s, they were more reliable, but the CM5s would throttle under load&lt;/item&gt;
      &lt;item&gt;I put these CM heatsinks on without screwing them in... then realized they would pop off sometimes, so I took all the blades out again and screwed them into the CM5s/Blades so they were more secure for the long term.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Compute Blade Cluster HPL Top500 Test&lt;/head&gt;
    &lt;p&gt;The first benchmark I ran was my top500 High Performance Linpack cluster benchmark. This is my favorite cluster benchmark, because it's the traditional benchmark they'd run on massive supercomputers to get on the top500 supercomputer list.&lt;/p&gt;
    &lt;p&gt;Before I installed heatsinks, the cluster got 275 Gflops, which is an 8.5x speedup over a single 8 GB CM5. Not bad, but I noticed the cluster was only using 105 Watts of power during the run. Definitely more headroom available.&lt;/p&gt;
    &lt;p&gt;After fixing the thermals, the cluster did not throttle, and used around 130W. At full power, I got 325 Gflops, which is a 10x performance improvement (for 10x 16GB CM5s) over a single 8 GB CM5.&lt;/p&gt;
    &lt;p&gt;Compared to the $8,000 Framework Cluster I benchmarked last month, this cluster is about 4 times slower:&lt;/p&gt;
    &lt;p&gt;But the Pi cluster is slightly more energy efficient, on a Gflops/W basis:&lt;/p&gt;
    &lt;p&gt;But what about price?&lt;/p&gt;
    &lt;p&gt;The Pi is a little less cost-effective for HPC applications than a Framework Desktop running a AMD's fastest APU. So discounting the fact we're only talking CPUs, I don't think any hyperscalers are looking to swap out a few thousand AMD EPYC systems for 10,000+ Raspberry Pis :)&lt;/p&gt;
    &lt;p&gt;But what about AI use cases?&lt;/p&gt;
    &lt;head rend="h2"&gt;Compute Blade Cluster AI Test&lt;/head&gt;
    &lt;p&gt;With 160 GB of total RAM, shared by the CPU and iGPU, this could be a small, efficient AI Cluster, right? Well, you'd think.&lt;/p&gt;
    &lt;p&gt;But no: currently llama.cpp can't speed up AI using Vulkan on the Pi 5 iGPU. That means we have 160 GB of RAM, but only CPU-powered inference. On pokey Arm Cortex A76 CPU cores with 10 GB/sec or so of memory bandwidth.&lt;/p&gt;
    &lt;p&gt;A small model (Llama 3.2:3B), running on a single Pi, isn't horrible; you get about 6 tokens per second. But that is pretty weak compared to even an Intel N100 (much less a single Framework Desktop):&lt;/p&gt;
    &lt;p&gt;You could have 10 nodes running 10 models, and that might be a very niche use case, but the real test would be running a larger AI model across all nodes. So I switched tracks to Llama 3.3:70B, which is a 40 GB model. It has to run across multiple Pis, since no single Pi has more than 16 GB of RAM.&lt;/p&gt;
    &lt;p&gt;Just as with the Framework cluster, llama.cpp RPC was very slow, since it splits up the model layers on all the cluster members, then goes round-robin style asking each node to perform its prompt processing, then token generation.&lt;/p&gt;
    &lt;p&gt;The Pi cluster couldn't even make it to token generation (tg) on my default settings, so I had to dial things back and only generate 16 tokens at a time to allow it to complete.&lt;/p&gt;
    &lt;p&gt;And after all that? Only 0.28 tokens per second, which is 25x slower than the Framework Cluster, running the same model (except on AI Max iGPUs with Vulkan).&lt;/p&gt;
    &lt;p&gt;I also tried Exo and distributed-llama. Exo was having trouble even running a small 3B model on even a 2 or 3 node Pi cluster configuration, so I stopped trying to get that working.&lt;/p&gt;
    &lt;p&gt;Distributed llama worked, but only with up to 8 nodes for the 70B model. Doing that, I got a more useful 0.85 tokens/s, but that's still 5x slower than the Framework cluster (and it was a bit more fragile than llama.cpp RPC—the tokens were sometimes gibberish):&lt;/p&gt;
    &lt;p&gt;You can find all my AI cluster benchmarking results in the issue Test various AI clustering setups on 10 node Pi 5 cluster over on GitHub.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gatesworks and Conclusion&lt;/head&gt;
    &lt;p&gt;Bottom line: this cluster's not a powerhouse. And dollar for dollar, if you're spending over $3k on a compute cluster, it's not the best value.&lt;/p&gt;
    &lt;p&gt;It is efficient, quiet, and compact. So if density is important, and if you need lots of small, physically separate nodes, this could actually make sense.&lt;/p&gt;
    &lt;p&gt;Like the only real world use case besides learning is for CI jobs or high security edge deployments, where you're not allowed to run multiple things on one server.&lt;/p&gt;
    &lt;p&gt;That's what Unredacted Labs is building Pi clusters for: they're building Tor exit relays on blades, after they found the Pi was the most efficient way to run massive amounts of nodes. If your goal is efficiency and node density, this does win, ever so slightly.&lt;/p&gt;
    &lt;p&gt;But for 99% of you reading this: this is not the cluster you're looking for.&lt;/p&gt;
    &lt;p&gt;Two years ago, when I originally ordered the Blades, Gateworks reached out. They were selling a souped up version of the Compute Blade, made to an industrial spec. The GBlade is around Pi 4 levels of performance, but with 10 gig networking, along with a 1 gig management interface.&lt;/p&gt;
    &lt;p&gt;But... it's discontinued. It doesn't look like any type of compute blade really lit the world on fire, and like the Blade movie series, the Compute Blade is more of a cult classic than a mainstream hit.&lt;/p&gt;
    &lt;p&gt;This is a bad cluster. Except for maybe blade 9, which dies every time I run a benchmark. But I will keep it going, knowing it's definitely easier to maintain than the 1,050 node Pi cluster at UC Santa Barbera, which to my knowledge is still the world's largest!&lt;/p&gt;
    &lt;p&gt;Before I go, I just wanted to give a special thanks to everyone who supports my on Patreon, GitHub, YouTube Memberships, and Floatplane. It really helps when I take on these months- (or years!) long projects.&lt;/p&gt;
    &lt;head rend="h2"&gt;Parts Used&lt;/head&gt;
    &lt;p&gt;You might not want to replicate my cluster setup — but I always get asked what parts I used (especially the slim Ethernet cables... everyone asks about those!), so here's the parts list:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compute Blade DEV&lt;/item&gt;
      &lt;item&gt;Compute Blade Standard Fan Unit&lt;/item&gt;
      &lt;item&gt;Compute Blade 10" 3D Print Rackmount&lt;/item&gt;
      &lt;item&gt;Raspberry Pi CM5 16GB (CM5016000)&lt;/item&gt;
      &lt;item&gt;GLOTRENDS Aluminum CM5 Heatsink&lt;/item&gt;
      &lt;item&gt;Patriot P300 256GB NVMe SSD 10-pack&lt;/item&gt;
      &lt;item&gt;GigaPlus 2.5 Gbps 10 port PoE+ switch&lt;/item&gt;
      &lt;item&gt;GigaPlus 10" Rack Mount 3D Print ears&lt;/item&gt;
      &lt;item&gt;Monoprice Cat6A SlimRun 6" Cat6 patch cables (10 pack)&lt;/item&gt;
      &lt;item&gt;ioplex SFP+ Twinax DAC patch cable&lt;/item&gt;
      &lt;item&gt;DeskPi RackMate TT&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;Oh come on Jeff, you forgot to buy GPUs for your AI cluster. Such a beginner mistake.&lt;/p&gt;
    &lt;p&gt;All you needed to do is buy 4x xtx 7900 used on ebay and build a four node raspberry pi cluster using the external GPU setup you've come up with in one of your previous blog posts [0].&lt;/p&gt;
    &lt;p&gt;[0] https://www.jeffgeerling.com/blog/2024/use-external-gpu-on-r...&lt;/p&gt;
    &lt;p&gt;Heh, more to come on that soon... Nvidia is back on the table!&lt;/p&gt;
    &lt;p&gt;I wonder what the 8x limitation was. Those numbers line up really well with a post from b4rtaz about "Qwen3 30B A3B Q40 hits 13.04 tok/s on 4× Raspberry Pi 5"&lt;/p&gt;
    &lt;p&gt;Working backwards from 70b to 3b active tokens getting 13.04 tok/s on 4x pi5s... with a very rough cut from 70b to 3b i would expect 23x slower so 0.5 tok/s for 4x pi5s, so with 8x it should be just around 1 tok/s but i guess with additional overhead from transfers it would be a bit under&lt;/p&gt;
    &lt;p&gt;Maybe the Pi cluster doesn’t have great performance, but there’s something attractive about having a small cluster of computers. I know it myself, because I’ve been thinking about using a small cluster to serve my websites, even if performance isn’t the best.&lt;/p&gt;
    &lt;p&gt;Web traffic is probably one area where something like this would be reasonable; it's not generally a huge load on the system, and having a few low power nodes would be adequate for good performance. I've run this website off a Pi cluster before, and outside of someone DDoSing it, it ran like a champ!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jeffgeerling.com/blog/2025/i-regret-building-3000-pi-ai-cluster"/><published>2025-09-19T14:28:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45302220</id><title>Trevor Milton's Nikola case dropped by SEC following Trump pardon</title><updated>2025-09-19T19:07:52.455850+00:00</updated><content>&lt;doc fingerprint="2be08319d910f1c5"&gt;
  &lt;main&gt;
    &lt;p&gt;The US Securities and Exchange Comission (SEC) has dropped its fraud case against Nikola‘s founder and former CEO Trevor Milton, after Trump’s pardon six months ago.&lt;/p&gt;
    &lt;p&gt;Trevor Milton, who had been convicted of securities fraud, received a pardon from the US President Donald Trump in March, a month after the Phoenix-based electric and hydrogen truck maker Nikola filed for bankrupcy.&lt;/p&gt;
    &lt;p&gt;The full and unconditional presidential pardon overturned a four-year prison sentence, imposed in December 2023, for deceiving investors about the company’s progress and products.&lt;/p&gt;
    &lt;p&gt;In an X post this Tuesday, the company’s founder said that “the SEC has dropped their case against me with prejudice.”&lt;/p&gt;
    &lt;p&gt;“5 years of outright lies by the media, corrupt prosecutors, former Nikola executives and short sellers is finally over,” he wrote.&lt;/p&gt;
    &lt;p&gt;Milton added that he comes “out of this thankful to my God for one more day in this life and for such a wonderful family and wife who never backed down against the evil men behind this.” (Read the full statement below.)&lt;/p&gt;
    &lt;p&gt;As the company deals with the Chapter 11 plan in a Delaware bankrupcy court, Milton asked for a $69 million indemnification in legal fees against Nikola, which the company has pushed back.&lt;/p&gt;
    &lt;p&gt;According to Nikola, Milton’s demand is invalid, as the ex-executive acted in a “grossly negligent, reckless” behaviour, engaging in “bad faith actions” during his time at the company.&lt;/p&gt;
    &lt;p&gt;However, last month, he argued that the bankrupcy plan does not fully reflect the presidential pardon he received.&lt;/p&gt;
    &lt;p&gt;“I don’t know him, but I was… they say it was very unfair. And they say the thing that he did wrong was he was one of the first people that supported a gentleman named Donald Trump for president,” the President said then.&lt;/p&gt;
    &lt;p&gt;The filing last month noted that “President Trump expressly decided here that Milton is factually innocent, the pardon did, contrary to the debtors’ assertions, wipe the slate clean.”&lt;/p&gt;
    &lt;p&gt;In May, Nikola‘s creditors committee asked the bankrupcy court to investigate whether Milton was dissipating personal assets that should be used to satisfy his debt to the company.&lt;/p&gt;
    &lt;p&gt;Milton stepped down from CEO in 2020, after allegations from Hindenburg Research that accused him of repeatedly misrepresenting Nikola‘s technological capabilities.&lt;/p&gt;
    &lt;p&gt;Read Trevor Milton’s X statement below:&lt;/p&gt;
    &lt;p&gt;“The SEC has dropped their case against me with prejudice. 5 years of outright lies by the media, corrupt prosecutors, former Nikola executives and short sellers is finally over.&lt;/p&gt;
    &lt;p&gt;They falsely indicted me, silenced me, deleted my followers including here on X, stole my company, bankrupted my company, debanked me, targeted my friends and family, stole most my wealth and tried to put me in prison.&lt;/p&gt;
    &lt;p&gt;But it’s over now and eventually our creator will make it right.&lt;/p&gt;
    &lt;p&gt;I come out of this thankful to my God for one more day in this life and for such a wonderful family and wife who never backed down against the evil men behind this.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eletric-vehicles.com/nikola/trevor-miltons-nikola-case-dropped-by-sec-following-trump-pardon/"/><published>2025-09-19T14:43:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45302222</id><title>Revamping an Old TV as a Gift (2019)</title><updated>2025-09-19T19:07:52.043770+00:00</updated><content>&lt;doc fingerprint="67cc3e778305c99a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Revamping an old tv as a gift&lt;/head&gt;
    &lt;p&gt;This entry is a summary of what I built for my dad's 50th birthday, in 2017.&lt;/p&gt;
    &lt;p&gt;The plan was to get a vintage TV to play some shows from the 70s-80s, and operation should be seamless.&lt;/p&gt;
    &lt;p&gt;The sacrificial lamb, found in the flea market:&lt;/p&gt;
    &lt;p&gt;With the lid off&lt;/p&gt;
    &lt;p&gt;The tuner&lt;/p&gt;
    &lt;p&gt;The attack plan:&lt;/p&gt;
    &lt;head rend="h1"&gt;Getting the raspberry pi to output video to the TV&lt;/head&gt;
    &lt;p&gt;Because the raspberry only outputs composite video, I needed a 'composite RF modulator' that'd convert the signal to a format that this TV can display.&lt;/p&gt;
    &lt;p&gt;These modulators output different channels at different frequencies.&lt;/p&gt;
    &lt;p&gt;These frequencies are what you 'tune' to by rotating the tuner's knob (learn more about tuners here). I left the tv tuner in a fixed channel, the same as what the modulator outputs.&lt;/p&gt;
    &lt;p&gt;Let there be video!&lt;/p&gt;
    &lt;head rend="h1"&gt;Software-based channels&lt;/head&gt;
    &lt;p&gt;With the Pi's output being displayed on the TV the next step was to get back the functionality of being able to rotate the knob to change channels.&lt;lb/&gt; I did this with software-based channels, controlled by a multi-polar rotary switch.&lt;/p&gt;
    &lt;p&gt;Switch connected to GPIO&lt;/p&gt;
    &lt;head rend="h1"&gt;Powering the pi and modulator inside the TV&lt;/head&gt;
    &lt;p&gt;The raspberry pi needs a 5v power source, and the RF modulator needed 9v.&lt;lb/&gt; I found a 12v rail and mounted an LM7809 and LM7805 to obtain the needed voltages inside the TV.&lt;/p&gt;
    &lt;p&gt;LM7809 and LM7805 placed using part of the TV as a heatsink&lt;/p&gt;
    &lt;head rend="h1"&gt;Software&lt;/head&gt;
    &lt;p&gt;Initially, the idea was to have a large set of shows/chapters (and advertisements) per channel, and pick from them randomly.&lt;/p&gt;
    &lt;p&gt;I had just recently started to get familiar with &lt;code&gt;gstreamer&lt;/code&gt; and could not get
my player to continuously play seamlessly -- either changing pads or containers
or something else would always make it get stuck after a while.&lt;/p&gt;
    &lt;p&gt;I opted to go with a massive hack: each channel is a single 8-hours long video, with the advertisements baked in.&lt;/p&gt;
    &lt;p&gt;On poweroff, the timestamp of the nearest keyframe is saved and on power-on, playback resumes from there.&lt;/p&gt;
    &lt;p&gt;When a video reaches the end of playback, it will start again from the beginning.&lt;/p&gt;
    &lt;p&gt;The code can be found here, but be warned, it is very bad.&lt;/p&gt;
    &lt;p&gt;First time I got it working -- the black spots are an artifact my phone's recording..&lt;/p&gt;
    &lt;p&gt;Final version&lt;/p&gt;
    &lt;head rend="h1"&gt;Extra&lt;/head&gt;
    &lt;p&gt;I also made a fake parcel-tracking website that'd display the status of the package.&lt;/p&gt;
    &lt;head rend="h1"&gt;Inspiration&lt;/head&gt;
    &lt;p&gt;This guy made me think about this in the first place. Our approaches are quite different, as I wanted to have the TV be 'stand alone'.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.davidv.dev/posts/revamping-an-old-tv-as-a-gift/"/><published>2025-09-19T14:43:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45302721</id><title>Kernel: Introduce Multikernel Architecture Support</title><updated>2025-09-19T19:07:51.734046+00:00</updated><content>&lt;doc fingerprint="4690087fdcd96e07"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;[RFC Patch 0/7] kernel: Introduce multikernel architecture support&lt;/head&gt;
    &lt;p&gt; Thread information [Search the all archive] &lt;/p&gt;
    &lt;quote&gt;Cong Wang [this message] ` [RFC Patch 1/7] kexec: Introduce multikernel support via kexec Cong Wang ` [RFC Patch 2/7] x86: Introduce SMP INIT trampoline for multikernel CPU bootstrap Cong Wang ` [RFC Patch 3/7] x86: Introduce MULTIKERNEL_VECTOR for inter-kernel communication Cong Wang ` [RFC Patch 4/7] kernel: Introduce generic multikernel IPI communication framework Cong Wang ` [RFC Patch 5/7] x86: Introduce arch_cpu_physical_id() to obtain physical CPU ID Cong Wang ` [RFC Patch 6/7] kexec: Implement dynamic kimage tracking Cong Wang ` [RFC Patch 7/7] kexec: Add /proc/multikernel interface for " Cong Wang ` [syzbot ci] Re: kernel: Introduce multikernel architecture support syzbot ci ` [RFC Patch 0/7] " Pasha Tatashin&lt;/quote&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;From:&lt;/cell&gt;
        &lt;cell&gt;Cong Wang &amp;lt;xiyou.wangcong-AT-gmail.com&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;To:&lt;/cell&gt;
        &lt;cell&gt;linux-kernel-AT-vger.kernel.org&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Subject:&lt;/cell&gt;
        &lt;cell&gt;[RFC Patch 0/7] kernel: Introduce multikernel architecture support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Date:&lt;/cell&gt;
        &lt;cell&gt;Thu, 18 Sep 2025 15:25:59 -0700&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Message-ID:&lt;/cell&gt;
        &lt;cell&gt;&amp;lt;20250918222607.186488-1-xiyou.wangcong@gmail.com&amp;gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Cc:&lt;/cell&gt;
        &lt;cell&gt;pasha.tatashin-AT-soleen.com, Cong Wang &amp;lt;xiyou.wangcong-AT-gmail.com&amp;gt;, Cong Wang &amp;lt;cwang-AT-multikernel.io&amp;gt;, Andrew Morton &amp;lt;akpm-AT-linux-foundation.org&amp;gt;, Baoquan He &amp;lt;bhe-AT-redhat.com&amp;gt;, Alexander Graf &amp;lt;graf-AT-amazon.com&amp;gt;, Mike Rapoport &amp;lt;rppt-AT-kernel.org&amp;gt;, Changyuan Lyu &amp;lt;changyuanl-AT-google.com&amp;gt;, kexec-AT-lists.infradead.org, linux-mm-AT-kvack.org&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;This patch series introduces multikernel architecture support, enabling multiple independent kernel instances to coexist and communicate on a single physical machine. Each kernel instance can run on dedicated CPU cores while sharing the underlying hardware resources. The multikernel architecture provides several key benefits: - Improved fault isolation between different workloads - Enhanced security through kernel-level separation - Better resource utilization than traditional VM (KVM, Xen etc.) - Potential zero-down kernel update with KHO (Kernel Hand Over) Architecture Overview: The implementation leverages kexec infrastructure to load and manage multiple kernel images, with each kernel instance assigned to specific CPU cores. Inter-kernel communication is facilitated through a dedicated IPI framework that allows kernels to coordinate and share information when necessary. Key Components: 1. Enhanced kexec subsystem with dynamic kimage tracking 2. Generic IPI communication framework for inter-kernel messaging 3. Architecture-specific CPU bootstrap mechanisms (only x86 so far) 4. Proc interface for monitoring loaded kernel instances Patch Summary: Patch 1/7: Introduces basic multikernel support via kexec, allowing multiple kernel images to be loaded simultaneously. Patch 2/7: Adds x86-specific SMP INIT trampoline for bootstrapping CPUs with different kernel instances. Patch 3/7: Introduces dedicated MULTIKERNEL_VECTOR for x86 inter-kernel communication. Patch 4/7: Implements generic multikernel IPI communication framework for cross-kernel messaging and coordination. Patch 5/7: Adds arch_cpu_physical_id() function to obtain physical CPU identifiers for proper CPU management. Patch 6/7: Replaces static kimage globals with dynamic linked list infrastructure to support multiple kernel images. Patch 7/7: Adds /proc/multikernel interface for monitoring and debugging loaded kernel instances. The implementation maintains full backward compatibility with existing kexec functionality while adding the new multikernel capabilities. IMPORTANT NOTES: 1) This is a Request for Comments (RFC) submission. While the core architecture is functional, there are numerous implementation details that need improvement. The primary goal is to gather feedback on the high-level design and overall approach rather than focus on specific coding details at this stage. 2) This patch series represents only the foundational framework for multikernel support. It establishes the basic infrastructure and communication mechanisms. We welcome the community to build upon this foundation and develop their own solutions based on this framework. 3) Testing has been limited to the author's development machine using hard-coded boot parameters and specific hardware configurations. Community testing across different hardware platforms, configurations, and use cases would be greatly appreciated to identify potential issues and improve robustness. Obviously, don't use this code beyond testing. This work enables new use cases such as running real-time kernels alongside general-purpose kernels, isolating security-critical applications, and providing dedicated kernel instances for specific workloads etc.. Signed-off-by: Cong Wang &amp;lt;cwang@multikernel.io&amp;gt; --- Cong Wang (7): kexec: Introduce multikernel support via kexec x86: Introduce SMP INIT trampoline for multikernel CPU bootstrap x86: Introduce MULTIKERNEL_VECTOR for inter-kernel communication kernel: Introduce generic multikernel IPI communication framework x86: Introduce arch_cpu_physical_id() to obtain physical CPU ID kexec: Implement dynamic kimage tracking kexec: Add /proc/multikernel interface for kimage tracking arch/powerpc/kexec/crash.c | 8 +- arch/x86/include/asm/idtentry.h | 1 + arch/x86/include/asm/irq_vectors.h | 1 + arch/x86/include/asm/smp.h | 7 + arch/x86/kernel/Makefile | 1 + arch/x86/kernel/crash.c | 4 +- arch/x86/kernel/head64.c | 5 + arch/x86/kernel/idt.c | 1 + arch/x86/kernel/setup.c | 3 + arch/x86/kernel/smp.c | 15 ++ arch/x86/kernel/smpboot.c | 161 +++++++++++++ arch/x86/kernel/trampoline_64_bsp.S | 288 ++++++++++++++++++++++ arch/x86/kernel/vmlinux.lds.S | 6 + include/linux/kexec.h | 22 +- include/linux/multikernel.h | 81 +++++++ include/uapi/linux/kexec.h | 1 + include/uapi/linux/reboot.h | 2 +- init/main.c | 2 + kernel/Makefile | 2 +- kernel/kexec.c | 103 +++++++- kernel/kexec_core.c | 359 ++++++++++++++++++++++++++++ kernel/kexec_file.c | 33 ++- kernel/multikernel.c | 314 ++++++++++++++++++++++++ kernel/reboot.c | 10 + 24 files changed, 1411 insertions(+), 19 deletions(-) create mode 100644 arch/x86/kernel/trampoline_64_bsp.S create mode 100644 include/linux/multikernel.h create mode 100644 kernel/multikernel.c -- 2.34.1&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/ml/all/20250918222607.186488-1-xiyou.wangcong@gmail.com/"/><published>2025-09-19T15:29:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45304706</id><title>An untidy history of AI across four books</title><updated>2025-09-19T19:07:51.482479+00:00</updated><content>&lt;doc fingerprint="f49b93910b258fdf"&gt;
  &lt;main&gt;
    &lt;p&gt;The history of artificial intelligence (AI) cannot be separated entirely from the general development of technologies that go back to the ancient world. Like the abacus, the machines we today call AI reproduce and automate our formal and cognitive abilities, albeit at higher levels of generality. More officially, AI research began in the postwar era with the “symbolic” paradigm, which sought to program human faculties such as logic, knowledge, ontology, and semantics within software architecture. It was harder than it sounds. Despite the inveterate optimism of the broader field, the symbolic approach encountered major logistical and conceptual limitations, and by the turn of the century had begun to stagnate.&lt;/p&gt;
    &lt;p&gt;A competing approach, machine learning, developed algorithms that, through brute optimization, appeared to replicate some of the mind’s basic effects. At first, the paradigm was constrained by a paucity of data and computing power, but those bottlenecks cracked open in the new millennium when the Internet accumulated galaxies of information and a niche technology (graphic processing units, otherwise known as GPUs, used in PCs and gaming consoles) proved useful for the intense computation required by machine-learning models.&lt;/p&gt;
    &lt;p&gt;In 2011, computer scientists Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton designed a neural network (a model loosely inspired by brain structures) to tackle the legendary ImageNet competition, a shoestring contest in automated image captioning that was ridiculed by many AI researchers at the time. The team’s model described images with 85 percent accuracy, a major improvement from previous attempts. In short order, most resources in AI research were rerouted into this neglected subfield, which ultimately led to the neural networks that today facilitate social media, search engines, and e-commerce, as well as a novel consumer product.&lt;/p&gt;
    &lt;p&gt;In 2015, an obscure nonprofit called OpenAI was founded by Sutskever, Elon Musk, Sam Altman, and a roster of computer scientists and engineers. Seven years later, the organization released ChatGPT, introducing the public to generative AI with “zero fanfare,” as one article described the marketing for the product. OpenAI, blindsided by its reception, had not secured enough computing power for the traffic it received. That was only three years ago. Now generative AI is ubiquitous, and OpenAI is speculatively valued at $300 billion.&lt;/p&gt;
    &lt;p&gt;It should surprise no one to see this brief account of technology exhibit the capriciousness of history: the skips, loops, and halts of progress; the weird contingencies (GPUs); the wrongheadedness of consensus; the arbitrariness of recognition; the maddening unpredictability of success. Yet a popular fantasy offers a tidier narrative that reduces the history of computing to a plottable sequence of triumphs and epiphanies in which progress is trivial and steadily exponential. I am referring to the hype surrounding AI, those industry-driven gusts of hot air blowing through every quarter of society and the cultural mania they are meant to inflame.&lt;/p&gt;
    &lt;p&gt;Princeton University computer scientists Arvind Narayanan and Sayash Kapoor have written AI Snake Oil to help nonexpert citizens identify and resist AI hype by relying on “common-sense ways of assessing whether or not a purported advance is plausible.” While not denying “genuine and remarkable” advances in generative AI, the authors are deeply concerned, even pessimistic, about the social consequences of its widespread adoption and use.&lt;/p&gt;
    &lt;p&gt;A big part of the problem, the authors maintain, is confusion about the meaning of artificial intelligence itself, a confusion that sustains and originates in the present AI commercial boom. Consider Hollywood’s renewed obsession with renegade AI (Mission: Impossible—Dead Reckoning Part One, Atlas, The Creator) or the commercial scramble to slap the AI label on vacuum cleaners, humidifiers, and other basic appliances, or even on the seasoned algorithms of Spotify and YouTube. More recently, the emergence of services that nominally use machine learning (Amazon Fresh) or don’t use it at all (the “AI” scheduler software Live Time) have only amplified the public’s bewilderment about the identity and capabilities of artificial intelligence.&lt;/p&gt;
    &lt;p&gt;Narayanan and Kapoor are particularly worried about the conflation of generative AI, which produces content through probabilistic response to human input, and predictive AI, which is purported to accurately forecast outcomes in the world, whether those be the success of a job candidate or the likelihood of a civil war. While products employing generative AI are “immature, unreliable, and prone to misuse,” Narayanan and Kapoor write, those using predictive AI “not only [do] not work today but will likely never work.” Such critical distinctions have been lost in the maelstrom of hype, allowing grifters, techno-messiahs, and pseudo-intellectuals to further manipulate the public with myths and prophecies.&lt;/p&gt;
    &lt;p&gt;While boosterism is hardly unique in the history of business and technology, the exceptional scale and intensity of this wave of hype is evident in the expanding bookshelf of titles by authors engaging in nothing less than a form of technological augury: The Singularity Is Nearer, by Google’s Ray Kurzweil; Nexus, by Yuval Noah Harari; and Genesis, by former Microsoft executive Craig Mundie, former CEO of Google Eric Schmidt, and the late Henry Kissinger, are just a few of many.&lt;/p&gt;
    &lt;p&gt;A puzzling characteristic of many AI prophets is their unfamiliarity with the technology itself. After the publication, in 2015, of Homo Deus, a book which appeals to pop evolutionary biology and post-humanist fantasies in order to prognosticate about technological innovation, Harari, who trained as a military historian, discovered he had earned “the reputation of an AI expert.” Nexus intends to “provide a more accurate historical perspective on the AI revolution,” but it reads like an undergraduate exercise in misreading, category error, and shoehorning. Explaining the basics of machine learning, Harari compares the pre-training of “baby algorithms” to the childhoods of “organic newborns,” blundering into the single worst explanatory analogy for the technique. What little we know of how humans learn (which allows us to independently generalize from very little data) is that it functions nothing like machine learning (which must be trained on oceans of data). Undeterred, Harari underscores the capacity of models to “teach themselves new things” in an iterative fashion. He offers the example of “present-day chess-playing AI” that are “taught nothing except the basic rules of the game.” Never mind that Stockfish, currently the world’s most successful chess engine, is programmed with several human game strategies. Harari fails to explain that while machine-learning models assemble a template of solutions to a specific problem (e.g., the best possible move in a given chess position), the framework in which those problems and solutions are defined is entirely constructed by engineers. Such models are entrenched in a particular complex of human judgment and knowledge that they functionally cannot transcend.&lt;/p&gt;
    &lt;p&gt;In passage after passage, Harari bungles straightforward issues and ideas concerning artificial intelligence. Philosopher Nick Bostrom’s version of the “alignment problem,” a staple in AI discourse, is a simple thought experiment that illustrates how an artificial intelligence could accomplish human goals through unforeseen means that violate the broader interests of its designers. An AI tasked with maximizing viewers’ time spent on a social-media platform might just accomplish that goal by exposing them to grotesque, false, or politically radical content. But Harari, attempting to argue that the alignment problem is a timeless conundrum, applies it to historical events that did not materially involve artificial intelligence (i.e., the “American invasion of Iraq”) when “short-term military” ambitions diverged from “long-term geopolitical goals.” Yet Bostrom’s warning is not about basic shortsightedness but a longsightedness that is blind to intervening steps taken by nonhuman systems.&lt;/p&gt;
    &lt;p&gt;In some cases, such ignorance seems strategic. Harari discusses the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) system, a machine-learning tool adopted by several state courts to score a defendant’s likelihood of recidivism. Harari rightly portrays the use of COMPAS as a scandal wherein “opaque algorithms” threaten “democratic transparency.” Yet he does not mention the most basic flaw of COMPAS: As Narayanan and Kapoor write, the “tool wasn’t very accurate to begin with; it had a relative accuracy of 64 percent,” marginally better than flipping a coin—a figure they believe is “likely to be an overestimate,” although such assessments are disputed by the tool’s owner and other researchers. But Harari’s elision is perplexing, given his critical stance toward the technology, his citation of a Criminal Justice study outlining the “mixed” performance of these systems, and his reference of the ProPublicainvestigation of COMPAS, which Narayanan and Kapoor cite.&lt;/p&gt;
    &lt;p&gt;The opacity of machine-learning tools is a genuine technical problem, but Harari adopts it as a magician’s silk behind which he shifts from mystifying to mythologizing his subject. In this practice, though, Harari is a bumbling acolyte compared to the high priesthood of Kissinger, Mundie, and Schmidt. The trio’s Genesis succeeds The Age of AI (2021), a tome Narayanan and Kapoor describe as “incessant in its hyperbole” and “littered with AI hype.” Indeed, it’s challenging to assess the claims within Genesis, because its idea of artificial intelligence resides so far afield of this writer’s (admittedly inexpert) understanding of the technology. (Perhaps it is technical illiteracy underlying my conviction that the phrase “interstellar fleets” should never appear in a text hoping to be taken seriously as a technological forecast.) Eloquent for its slapdash genre, Genesis is a sequence of pretentious historical odysseys that bring human endeavors (science, politics, warfare, etc.) to the brink of metamorphosis at the hands of AI:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our minds remain childlike with respect to God, our world, and now our newest creations.…&lt;/p&gt;
      &lt;p&gt;But will AIs be conquerors? Will human leaders become their proxies: sovereigns without sovereignty? Or, perhaps, will godlike AIs resurrect the once-ubiquitous human invocation of divine right, with AIs themselves as anointers of kings?…&lt;/p&gt;
      &lt;p&gt;Might the apparently superior intelligence of machines with structures based on the human brain, combined with our intense reliance on them, lead some to believe that we humans are ourselves becoming, or merging with, the divine?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It seems sufficient to ridicule this as the typical effluent of Silicon Valley’s intellectual culture, until you detect its political inflection. Kissinger, Mundie, and Schmidt habitually ponder the “fatalism,” “passivity,” “submission,” and “faith” with which “individual humans and whole human societies may respond to the advent of powerful AI.” Like Harari, the authors belabor the “opacity” of AI in order to legitimize musings like this: “Will the age of AI not only fail to propel humanity forward but instead catalyze a return to a premodern acceptance of unexplained authority?” These loaded questions might provoke similar queries from the reader. Could the passivity that preoccupies these sages betray some wish to instill that attitude in their readership? Might the plutocrats and tycoons they represent somehow benefit from making fatalism seem respectable and even reasonable to the general public? Does the depiction of AI as omnipotent, omniscient, and unknowable perhaps work to mesmerize the media, cow potential regulators, and, above all else, juice financial markets?&lt;/p&gt;
    &lt;p&gt;Fixated on revolutions and catastrophes, beginnings and endings, Genesis offers an eschatology centered on the “existential” risks posed by “misaligned AI.” The authors compare artificial intelligence to nuclear weapons in order to frame the geopolitical jockeying over AI as an “arms race” that recapitulates the Cold War. While their Kissingerian approach to this grim future curiously resembles the postwar international formation (“Unipolarity may be one pathway that could minimize the risk of extinction”), their equation of nuclear Armageddon (a long-standing, real possibility) with AI’s (ill-defined, hypothetical) global danger is not distinct to them. The strategy is the hobbyhorse of OpenAI’s Sam Altman, who lavished Genesis with advanced praise and apparently enjoys telling audiences that artificial intelligence will “most likely lead to the end of the world.”&lt;/p&gt;
    &lt;p&gt;Narayanan and Kapoor argue that the “bugbear of existential risk” from artificial intelligence serves to “overstate its capabilities and underemphasize its limitations” while distracting elected officials and citizens “from the more immediate harms of AI snake oil.” I would add that it monopolizes our imagination and sustains a frenzied pitch of the discourse around AI, both of which attract investors while affording large companies a means of regulatory capture. When Altman appeared before a Senate committee in 2023 to testify about the dangers of AI, he advocated for a government agency that would conveniently solidify OpenAI’s first-mover advantage by placing the burden of regulation on new competitors while neglecting “many of the transparency requirements that researchers had been arguing for OpenAI to follow.” AI systems that are imprudently embedded within social structures will pose threats, but Narayanan and Kapoor argue that “society already has the tools to address [those] risks calmly” while the specter of rogue AI cultivated by Altman, the authors of Genesis, and the so-called AI safety community is “best left to the realm of science fiction.”&lt;/p&gt;
    &lt;p&gt;Importing ideas from science fiction is the business of Ray Kurzweil; literally so. The titular event of Kurzweil’s The Singularity Is Near (2005) was first popularized by sci-fi legend Vernor Vinge in his 1993 essay that predicted the emergence of “superhuman intelligence” and closing of the “human era” within thirty years. The premise of Kurzweil’s sequel, The Singularity Is Nearer, is that humanity has begun the final preparations for this belated technological rapture, an event guaranteed by his “law of accelerating returns,” which supposedly describes how “positive feedback loops” and declining costs in information technologies make “it easier to design [their] next stage.” Artificial intelligence will orchestrate across numerous domains to bring about progress so precipitous and consistent that, Kurzweil asserts, humans will “merge with AI” around 2045. This is Kurzweil’s “Singularity,” the imaginary event that illustrates the primitive mechanics of his thought, which consist almost entirely in extrapolation.&lt;/p&gt;
    &lt;p&gt;A typical Kurzweil prophecy begins by citing recent improvements in a particular industry or field. Assessing medicine, for instance, he notes that in 2023 a drug designed using machine learning “entered phase-II clinical trials to treat a rare lung disease.” He then pontificates on thinly related philosophic or mathematical subjects, discombobulating the reader with unexplained jargon and Very Large Numbers—“1024 operations per second,” “306,000,000 gigabytes,” “100 trillion human beings,” “a googleplex of zeros,” “1010 123 possible universes,” a “million billion billion billion billion billion billion possibilities”—which are meant somehow to assure us that “exponential” advancement shall blast through any remaining ceilings, roadblocks, or bottlenecks, at least the ones that Kurzweil mentions. The interphase of this performance is like watching a bird struggling beneath a net. Because once Kurzweil escapes the trap of evidence and intellectual humility, he truly flies. As AI revolutionizes medicine, he asserts, applications will surge by the late 2020s, enabling us to combat biological limitations on the human lifespan through the 2030s with AI-controlled nanorobots, ultimately leading to the “definitive” defeat of aging. In the 2040s, cloud-based technologies will allow us to abandon our biological shells altogether by uploading our minds into digital environments.&lt;/p&gt;
    &lt;p&gt;One might wonder why Kurzweil commits himself to such specific time frames, having had to revise them before. Isn’t it advantageous to the soothsayer to remain tentative and vague? But then you remember that Kurzweil is seventy-seven years old and that just maybe (in the spirit of conjecture) he has chosen the next three decades as the window of our transcendence because they are the ones in which he has the best, not to say the last, chance of seeing his prophecy fulfilled. (As a fail-safe, he has paid to have his body “cryogenically frozen and preserved” so he can be resurrected to marvel at his prescience.) For Kurzweil, death is a technical problem we must solve no matter how pathetic or grotesque the solution. The reader’s jaw creaks open as Kurzweil describes the “dad bot” he trained on personal family records as “the first step in bringing my father back.” The conversation he proceeds to have with his simulated “father” is pitiful, but not for the reasons Kurzweil would believe.&lt;/p&gt;
    &lt;p&gt;Why is the essential promise of technology—the alleviation of drudgery—not enough? Maybe, in the case of AI, because it remains unclear what drudgery it can realistically alleviate. I, along with Narayanan and Kapoor, don’t doubt that machine learning will find positive applications in various industries (including medicine) while the underlying computer science will continue its winding amble forward. (AI is not a hopeless deviant technology like cryptocurrency.) But the promise of artificial intelligence does not provide any reason to believe we are living in “the most exciting and momentous years in all of history,” as Kurzweil puts it.&lt;/p&gt;
    &lt;p&gt;After reading these books, I began to question whether “hype” is a sufficient term for describing an uncoordinated yet global campaign of obfuscation and manipulation advanced by many Silicon Valley leaders, researchers, and journalists. The public is vulnerable to this campaign, in part, because of the cumulative nature of technological innovation. Understanding products such as ChatGPT, for example, requires a baseline familiarity with the tools and subjects it builds upon (e.g., transformers; neural networks), which are themselves subject to similar requirements (e.g., backpropagation; linear algebra.) In this way, such technologies levy a compounded cognitive cost. At some critical threshold unique to each technology, that burden becomes too great and ordinary people no longer have the time or energy to resist the sort of deception that is the incubator of hype. Paradoxically, the sure sign that a technology has undergone this transition is not widespread disinterest but superficial fascination and wide-eyed utopianism (nuclear fusion and quantum computing are good case studies). Hype appears, then, as a social mechanism through which technology becomes a kind of magic. When the authors of Genesis invoke Arthur C. Clarke—“Any sufficiently advanced technology is indistinguishable from magic”—they, of course, don’t mention that he was describing a nineteenth-century scientist’s first impressions of twentieth-century technology. For them, Clarke’s adage echoes their only real goal: to artificially prolong our childlike enchantment with newfangled toys and tools in order to buy time for the technicians to make good on unearthly promises.&lt;/p&gt;
    &lt;p&gt;Building or adapting a technology before articulating its function is usually the hallmark of a doomed product (see Google Glass, Apple Vision Pro, or the Metaverse). Over the past three decades, however, many leading tech startups, corporations, and venture-capital firms have operated according to a backward logic that has nevertheless proven remarkably successful for machine learning. This success is due, in part, to personalities like Sam Altman and Elon Musk, who have perfected the art of manufacturing public enthusiasm. In this case, the hype surrounding AI amounts to more than harmless promotion. By shaping expectations of what it can accomplish (such as a future civilization enthralled to godlike machines), Kurzweil, Harari, and their ilk pave the way for broad public acceptance of the comparatively humble promises and predictions of tech CEOs (what are fully self-driving cars before those interstellar fleets?). But it is all the same cartoon divorced from the realities of a powerful but limited technology. If there is any prediction one could make with confidence about AI, it is that its successful applications will be hammered relentlessly into public consciousness. But there will be little accounting for the opportunity costs incurred by an all-or-nothing industry that neglected the unglamorous problems and workaday inefficiencies that machine learning might have actually resolved. The project of making life a bit better for most people is being traded for the unthinkable waste in service of an impossible utopia.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hedgehogreview.com/issues/lessons-of-babel/articles/perplexity"/><published>2025-09-19T18:15:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45305042</id><title>Internal emails reveal Ticketmaster helped scalpers jack up prices, FTC says</title><updated>2025-09-19T19:07:51.220814+00:00</updated><content>&lt;doc fingerprint="783e8bdb83d0c50a"&gt;
  &lt;main&gt;
    &lt;p&gt;The Federal Trade Commission sued Live Nation and Ticketmaster on Thursday, alleging that the companies tacitly worked with scalpers to profit from jacking up ticket prices on the secondary market.&lt;/p&gt;
    &lt;p&gt;As the FTC alleged in a press release, Ticketmaster's years of turning a blind eye to scalpers violated the FTC Act and the Better Online Ticket Sales Act, costing customers "billions in inflated prices and additional fees." Further, artists' efforts to keep event costs low were repeatedly frustrated by executives' greedy bid to drive Ticketmaster revenue by reaping as many additional fees as possible, the FTC alleged.&lt;/p&gt;
    &lt;p&gt;Rather than blocking scalping, Ticketmaster allegedly provided tech support to help so-called brokers exceed "fake" ticket limits that seemingly only applied to genuine customers buying tickets to see events.&lt;/p&gt;
    &lt;p&gt;The FTC's investigation revealed that five of the biggest brokers controlled thousands of fake or purchased Ticketmaster accounts buying hundreds of thousands of event tickets and making it impossible for fans to "have a shot at buying fair-priced tickets," FTC chair Andrew Ferguson said.&lt;/p&gt;
    &lt;p&gt;From 2020 to 2024, just one broker known to Ticketmaster managed more than 13,000 accounts to circumvent ticket limits, the FTC alleged. But Ticketmaster allegedly saw only diminished revenue when it considered intervening, with one senior executive admitting—in an internal email that "copied Live Nation leadership"—turning a "blind eye" to brokers quickly became a "matter of policy." Ticketmaster also allegedly decreased fees for the highest-volume resellers to "incentivize them" to use Ticketmaster's platform for resales.&lt;/p&gt;
    &lt;p&gt;"American live entertainment is the best in the world and should be accessible to all of us," Ferguson said. "It should not cost an arm and a leg to take the family to a baseball game or attend your favorite musician’s show."&lt;/p&gt;
    &lt;head rend="h2"&gt;Ticketmaster’s “triple dip” to collect more fees&lt;/head&gt;
    &lt;p&gt;Ticketmaster controls about 80 percent of "major concert venues’ primary ticketing" and increasingly controls "a growing share of ticket resales in the secondary market," the FTC noted.&lt;/p&gt;
    &lt;p&gt;Seemingly, Ticketmaster uses this dominant position to "triple dip" on fees, maximizing gains from events by charging fees at initial purchase, then again from both sellers and buyers on the secondary market. Ticketmaster generates most of its revenue from these fees, the FTC said, raking in over $11 billion from 2019 through 2024—nearly $4 billion of which came from resale fees.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arstechnica.com/tech-policy/2025/09/ticketmaster-intentionally-screwed-fans-out-of-billions-ftc-lawsuit-says/"/><published>2025-09-19T18:47:14+00:00</published></entry></feed>