<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-22T08:46:39.889686+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45646559</id><title>Getting DeepSeek-OCR working on an Nvidia Spark via brute force with Claude Code</title><updated>2025-10-22T08:46:46.804401+00:00</updated><content>&lt;doc fingerprint="1f0f9d970db55a4b"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Getting DeepSeek-OCR working on an NVIDIA Spark via brute force using Claude Code&lt;/head&gt;
    &lt;p&gt;20th October 2025&lt;/p&gt;
    &lt;p&gt;DeepSeek released a new model yesterday: DeepSeek-OCR, a 6.6GB model fine-tuned specifically for OCR. They released it as model weights that run using PyTorch and CUDA. I got it running on the NVIDIA Spark by having Claude Code effectively brute force the challenge of getting it working on that particular hardware.&lt;/p&gt;
    &lt;p&gt;This small project (40 minutes this morning, most of which was Claude Code churning away while I had breakfast and did some other things) ties together a bunch of different concepts I‚Äôve been exploring recently. I designed an agentic loop for the problem, gave Claude full permissions inside a Docker sandbox, embraced the parallel agents lifestyle and reused my notes on the NVIDIA Spark from last week.&lt;/p&gt;
    &lt;p&gt;I knew getting a PyTorch CUDA model running on the Spark was going to be a little frustrating, so I decided to outsource the entire process to Claude Code to see what would happen.&lt;/p&gt;
    &lt;p&gt;TLDR: It worked. It took four prompts (one long, three very short) to have Claude Code figure out everything necessary to run the new DeepSeek model on the NVIDIA Spark, OCR a document for me and produce copious notes about the process.&lt;/p&gt;
    &lt;head rend="h4"&gt;The setup&lt;/head&gt;
    &lt;p&gt;I connected to the Spark from my Mac via SSH and started a new Docker container there:&lt;/p&gt;
    &lt;code&gt;docker run -it --gpus=all \
  -v /usr/local/cuda:/usr/local/cuda:ro \
  nvcr.io/nvidia/cuda:13.0.1-devel-ubuntu24.04 \
  bash&lt;/code&gt;
    &lt;p&gt;Then I installed npm and used that to install Claude Code:&lt;/p&gt;
    &lt;code&gt;apt-get update
DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get install -y npm
npm install -g @anthropic-ai/claude-code&lt;/code&gt;
    &lt;p&gt;Then started Claude Code, telling it that it‚Äôs OK that it‚Äôs running as &lt;code&gt;root&lt;/code&gt; because it‚Äôs in a sandbox:&lt;/p&gt;
    &lt;code&gt;IS_SANDBOX=1 claude --dangerously-skip-permissions&lt;/code&gt;
    &lt;p&gt;It provided me a URL to click on to authenticate with my Anthropic account.&lt;/p&gt;
    &lt;head rend="h4"&gt;The initial prompts&lt;/head&gt;
    &lt;p&gt;I kicked things off with this prompt:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Create a folder deepseek-ocr and do everything else in that folder&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Then I ran the following, providing links to both the GitHub repository and the Hugging Face model, providing a clue about NVIDIA ARM and giving it an image (this one, see previous post) that I wanted it to run OCR on.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Your task is to get this working: https://github.com/deepseek-ai/DeepSeek-OCR‚Äîit uses Hugging Face Transformers and models from https://huggingface.co/deepseek-ai/DeepSeek-OCR‚Äîyou are running in a Docker container on an NVIDIA ARM device, first research your environment to check it will be possible to do this. Git clone both of those‚Äîyou may need to enable LFS first. Make extensive notes as you go in a notes.md file which you should only ever append new notes to. At the end of this you should have scripts in the folder which can be run to both setup and then run the model against an example‚Äîuse https://static.simonwillison.net/static/2025/ft.jpeg as the example image that you OCR. As a final step create a detailed README.md file explaining the project and what you learned and how to run your code.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Note how I also told it to ‚Äúmake extensive notes as you go in a notes.md file which you should only ever append new notes to‚Äù so I‚Äôd have some useful notes at the end.&lt;/p&gt;
    &lt;head rend="h4"&gt;The follow-ups&lt;/head&gt;
    &lt;p&gt;It churned away for 13 minutes, and made a sequence of discoveries about the platform and the model and the difficulties of running PyTorch on it.&lt;/p&gt;
    &lt;p&gt;And then... it gave up:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The NVIDIA GB10 GPU has compute capability sm_121, which is too new for PyTorch 2.5.1&lt;/p&gt;&lt;item&gt;PyTorch 2.5.1 supports: sm_50, sm_80, sm_86, sm_89, sm_90, sm_90a&lt;/item&gt;&lt;item&gt;GB10 requires: sm_121&lt;/item&gt;&lt;item&gt;Error:&lt;/item&gt;&lt;code&gt;CUDA error: no kernel image is available for execution on the device&lt;/code&gt;&lt;p&gt;Additionally, the model has hardcoded&lt;/p&gt;&lt;code&gt;.cuda()&lt;/code&gt;calls, preventing CPU fallback without code modifications.&lt;/quote&gt;
    &lt;p&gt;So I gave it some encouragement:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Any other options that might help get CUDA working like using different versions of PyTorch that have available CUDA wheels for this platform? Have you checked to see what PyTorch versions already have these ARM CUDA wheels that would work here?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It ran this command to grep for all available PyTorch wheels on &lt;code&gt;download.pytorch.org&lt;/code&gt; that match the ARM architecture:&lt;/p&gt;
    &lt;code&gt;curl -s https://download.pytorch.org/whl/torch/ | grep -o 'torch-[0-9.]*.*aarch64.whl' | sort -V | tail -20&lt;/code&gt;
    &lt;p&gt;... and that unblocked it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Excellent! I can see that PyTorch 2.9.0 has ARM64 wheels for CUDA 12.8, 12.9, and CUDA 13.0 (cu130)! This is exactly what we need!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It installed the wheels it found and reported:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Good news! PyTorch 2.9.0 can detect the GB10 GPU with CUDA capability 12.1 (sm_121), but it says the maximum supported is 12.0. However, it‚Äôs just a warning now instead of a hard error! Let‚Äôs try running inference: [...]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Then kept on churning for a bit, ignoring warnings, and managed to run the model! It loaded the image and ran this prompt:&lt;/p&gt;
    &lt;code&gt;&amp;lt;|grounding|&amp;gt;OCR this image.
&lt;/code&gt;
    &lt;p&gt;And got back output that started like this:&lt;/p&gt;
    &lt;code&gt;&amp;lt;|ref|&amp;gt;The perils of vibe coding&amp;lt;|/ref|&amp;gt;&amp;lt;|det|&amp;gt;[[352, 30, 624, 111]]&amp;lt;|/det|&amp;gt;
&amp;lt;|ref|&amp;gt;opt&amp;lt;|/ref|&amp;gt;&amp;lt;|det|&amp;gt;[[970, 0, 994, 30]]&amp;lt;|/det|&amp;gt;
&amp;lt;|ref|&amp;gt;such&amp;lt;|/ref|&amp;gt;&amp;lt;|det|&amp;gt;[[970, 24, 996, 58]]&amp;lt;|/det|&amp;gt;
&amp;lt;|ref|&amp;gt;days&amp;lt;|/ref|&amp;gt;&amp;lt;|det|&amp;gt;[[970, 52, 996, 87]]&amp;lt;|/det|&amp;gt;
&amp;lt;|ref|&amp;gt;pavi&amp;lt;|/ref|&amp;gt;&amp;lt;|det|&amp;gt;[[970, 85, 996, 118]]&amp;lt;|/det|&amp;gt;
&amp;lt;|ref|&amp;gt;TECHNOLOGY&amp;lt;|/ref|&amp;gt;&amp;lt;|det|&amp;gt;[[33, 199, 127, 230]]&amp;lt;|/det|&amp;gt;
&amp;lt;|ref|&amp;gt;holds the promise of replacing program-&amp;lt;|/ref|&amp;gt;&amp;lt;|det|&amp;gt;[[220, 170, 390, 210]]&amp;lt;|/det|&amp;gt;
&amp;lt;|ref|&amp;gt;mers - a profession of very well paid&amp;lt;|/ref|&amp;gt;&amp;lt;|det|&amp;gt;[[220, 199, 390, 238]]&amp;lt;|/det|&amp;gt;
&amp;lt;|ref|&amp;gt;people.For another, the work can be&amp;lt;|/ref|&amp;gt;&amp;lt;|det|&amp;gt;[[220, 228, 390, 267]]&amp;lt;|/det|&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Claude declared victory and pointed me to the output/result.mmd file, which contained only whitespace. So OCR had worked but the result had failed to be written correctly to disk.&lt;/p&gt;
    &lt;p&gt;I said:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;That result.mmd file looks to be exclusively whitespace to me, try and get a better result there‚Äîthe bounding box image was good though&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Which caused Claude to reconsider the prompt it used:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Maybe we need to use a different prompt. Looking at the README from DeepSeek-OCR, there are different prompts:&lt;/p&gt;&lt;code&gt;"&amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;Convert the document to markdown."&lt;/code&gt;‚Äîfor documents&lt;code&gt;"&amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;OCR this image."&lt;/code&gt;‚Äîgeneral OCR&lt;code&gt;"&amp;lt;image&amp;gt;\nFree OCR."&lt;/code&gt;‚Äîwithout layouts&lt;p&gt;The ‚Äúgrounding‚Äù mode seems to be focused on bounding boxes, which is why we got good bounding box results but the text output might be in a different format.&lt;/p&gt;&lt;p&gt;Let me try running it again with a different prompt that‚Äôs more focused on text extraction rather than grounding.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;It tried all of the prompt variants, made a note about them and created a file called PROMPTS_GUIDE.md with a bunch more details, including this comparison table:&lt;/p&gt;
    &lt;quote&gt;
      &lt;th&gt;Prompt&lt;/th&gt;
      &lt;th&gt;Speed&lt;/th&gt;
      &lt;th&gt;Text Quality&lt;/th&gt;
      &lt;th&gt;Structure&lt;/th&gt;
      &lt;th&gt;Coordinates&lt;/th&gt;
      &lt;th&gt;Best Use Case&lt;/th&gt;
      &lt;td&gt;Free OCR&lt;/td&gt;
      &lt;td&gt;‚ö°‚ö°‚ö° Fast&lt;/td&gt;
      &lt;td&gt;‚≠ê‚≠ê‚≠ê Excellent&lt;/td&gt;
      &lt;td&gt;‚≠ê Basic&lt;/td&gt;
      &lt;td&gt;‚ùå No&lt;/td&gt;
      &lt;td&gt;General OCR&lt;/td&gt;
      &lt;td&gt;Markdown&lt;/td&gt;
      &lt;td&gt;‚ö°‚ö° Medium&lt;/td&gt;
      &lt;td&gt;‚≠ê‚≠ê‚≠ê Excellent&lt;/td&gt;
      &lt;td&gt;‚≠ê‚≠ê‚≠ê Full&lt;/td&gt;
      &lt;td&gt;‚≠ê‚≠ê Partial&lt;/td&gt;
      &lt;td&gt;Documents&lt;/td&gt;
      &lt;td&gt;Grounding&lt;/td&gt;
      &lt;td&gt;‚ö° Slow&lt;/td&gt;
      &lt;td&gt;‚≠ê‚≠ê Good&lt;/td&gt;
      &lt;td&gt;‚≠ê Basic&lt;/td&gt;
      &lt;td&gt;‚≠ê‚≠ê‚≠ê Full&lt;/td&gt;
      &lt;td&gt;Annotations&lt;/td&gt;
      &lt;td&gt;Detailed&lt;/td&gt;
      &lt;td&gt;‚ö°‚ö°‚ö° Fastest&lt;/td&gt;
      &lt;td&gt;‚≠ê N/A&lt;/td&gt;
      &lt;td&gt;‚ùå N/A&lt;/td&gt;
      &lt;td&gt;‚ùå No&lt;/td&gt;
      &lt;td&gt;Image analysis&lt;/td&gt;
    &lt;/quote&gt;
    &lt;p&gt;And this table under the heading ‚ÄúPerformance Benchmarks‚Äù:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Test image: 3503√ó1668 pixels (Financial Times article)&lt;/p&gt;
      &lt;th&gt;Prompt&lt;/th&gt;
      &lt;th&gt;Time&lt;/th&gt;
      &lt;th&gt;Output Size&lt;/th&gt;
      &lt;th&gt;Tokens&lt;/th&gt;
      &lt;td&gt;Free OCR&lt;/td&gt;
      &lt;td&gt;24s&lt;/td&gt;
      &lt;td&gt;Clean text&lt;/td&gt;
      &lt;td&gt;2257&lt;/td&gt;
      &lt;td&gt;Markdown&lt;/td&gt;
      &lt;td&gt;39s&lt;/td&gt;
      &lt;td&gt;Formatted MD&lt;/td&gt;
      &lt;td&gt;2257 + structure&lt;/td&gt;
      &lt;td&gt;Grounding&lt;/td&gt;
      &lt;td&gt;58s&lt;/td&gt;
      &lt;td&gt;Text + coords&lt;/td&gt;
      &lt;td&gt;2257 + boxes&lt;/td&gt;
      &lt;td&gt;Detailed&lt;/td&gt;
      &lt;td&gt;9s&lt;/td&gt;
      &lt;td&gt;Description&lt;/td&gt;
      &lt;td&gt;~300&lt;/td&gt;
    &lt;/quote&gt;
    &lt;p&gt;My final prompt was this, to gather everything together into a zip file I could extract from the Docker container:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Create a zip file with the output and output_text and all of the scripts and notes‚Äîbut leave out the github repo and the huggingface repo directories&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I added the contents of that zip file to my new simonw/research GitHub repo in the deepseek-ocr-nvidia-spark folder.&lt;/p&gt;
    &lt;p&gt;Claude really likes writing notes! Here‚Äôs the directory listing of that finished folder:&lt;/p&gt;
    &lt;code&gt;  |-- download_test_image.sh
  |-- FINAL_SUMMARY.md
  |-- notes.md
  |-- output
  |   |-- images
  |   |-- result_with_boxes.jpg
  |   `-- result.mmd
  |-- output_text
  |   |-- detailed
  |   |   |-- images
  |   |   |-- result_with_boxes.jpg
  |   |   `-- result.mmd
  |   |-- free_ocr
  |   |   |-- images
  |   |   |-- result_with_boxes.jpg
  |   |   `-- result.mmd
  |   `-- markdown
  |       |-- images
  |       |   `-- 0.jpg
  |       |-- result_with_boxes.jpg
  |       `-- result.mmd
  |-- PROMPTS_GUIDE.md
  |-- README_SUCCESS.md
  |-- README.md
  |-- run_ocr_best.py
  |-- run_ocr_cpu_nocuda.py
  |-- run_ocr_cpu.py
  |-- run_ocr_text_focused.py
  |-- run_ocr.py
  |-- run_ocr.sh
  |-- setup.sh
  |-- SOLUTION.md
  |-- test_image.jpeg
  |-- TEXT_OUTPUT_SUMMARY.md
  `-- UPDATE_PYTORCH.md
&lt;/code&gt;
    &lt;head rend="h4"&gt;Takeaways&lt;/head&gt;
    &lt;p&gt;My first prompt was at 15:31:07 (UTC). The final message from Claude Code came in at 16:10:03. That means it took less than 40 minutes start to finish, and I was only actively involved for about 5-10 minutes of that time. The rest of the time I was having breakfast and doing other things.&lt;/p&gt;
    &lt;p&gt;Having tried and failed to get PyTorch stuff working in the past, I count this as a huge win. I‚Äôll be using this process a whole lot more in the future.&lt;/p&gt;
    &lt;p&gt;How good were the actual results? There‚Äôs honestly so much material in the resulting notes created by Claude that I haven‚Äôt reviewed all of it. There may well be all sorts of errors in there, but it‚Äôs indisputable that it managed to run the model and made notes on how it did that such that I‚Äôll be able to do the same thing in the future.&lt;/p&gt;
    &lt;p&gt;I think the key factors in executing this project successfully were the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I gave it exactly what it needed: a Docker environment in the target hardware, instructions on where to get what it needed (the code and the model) and a clear goal for it to pursue. This is a great example of the pattern I described in designing agentic loops.&lt;/item&gt;
      &lt;item&gt;Running it in a Docker sandbox meant I could use &lt;code&gt;claude --dangerously-skip-permissions&lt;/code&gt;and leave it running on its own. If I‚Äôd had to approve every command it wanted to run I would have got frustrated and quit the project after just a few minutes.&lt;/item&gt;
      &lt;item&gt;I applied my own knowledge and experience when it got stuck. I was confident (based on previous experiments with the Spark) that a CUDA wheel for ARM64 existed that was likely to work, so when it gave up I prompted it to try again, leading to success.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Oh, and it looks like DeepSeek OCR is a pretty good model if you spend the time experimenting with different ways to run it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Bonus: Using VS Code to monitor the container&lt;/head&gt;
    &lt;p&gt;A small TIL from today: I had kicked off the job running in the Docker container via SSH to the Spark when I realized it would be neat if I could easily monitor the files it was creating while it was running.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I am running a Docker container on a remote machine, which I started over SSH&lt;/p&gt;
      &lt;p&gt;How can I have my local VS Code on MacOS show me the filesystem in that docker container inside that remote machine, without restarting anything?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It gave me a set of steps that solved this exact problem:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install the VS Code ‚ÄúRemote SSH‚Äù and ‚ÄúDev Containers‚Äù extensions&lt;/item&gt;
      &lt;item&gt;Use ‚ÄúRemote-SSH: Connect to Host‚Äù to connect to the remote machine (on my Tailscale network that‚Äôs &lt;code&gt;spark@100.113.1.114&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;In the window for that remote SSH session, run ‚ÄúDev Containers: Attach to Running Container‚Äù‚Äîthis shows a list of containers and you can select the one you want to attach to&lt;/item&gt;
      &lt;item&gt;... and that‚Äôs it! VS Code opens a new window providing full access to all of the files in that container. I opened up &lt;code&gt;notes.md&lt;/code&gt;and watched it as Claude Code appended to it in real time.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At the end when I told Claude to create a zip file of the results I could select that in the VS Code file explorer and use the ‚ÄúDownload‚Äù menu item to download it to my Mac.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Code for web - a new asynchronous coding agent from Anthropic - 20th October 2025&lt;/item&gt;
      &lt;item&gt;Claude Skills are awesome, maybe a bigger deal than MCP - 16th October 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://simonwillison.net/2025/Oct/20/deepseek-ocr-claude-code/"/><published>2025-10-20T17:24:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45649510</id><title>The death of thread per core</title><updated>2025-10-22T08:46:46.038878+00:00</updated><content>&lt;doc fingerprint="775498de5d6b10db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Death of Thread Per Core&lt;/head&gt;
    &lt;p&gt;Programming language async runtimes are very focused on handling asynchronous, possibly long running tasks, that might yield for a variety of reasons, that themselves might spawn future work.&lt;/p&gt;
    &lt;p&gt;In an async runtime like async Rust, the model is that a task can yield, which, conceptually, creates a new piece of work that gets shoved onto the work queues (which is "resume that task"). You might not think of it as "this task is suspended and will be resumed later" as much as "this piece of work is done and has spawned a new piece of work." This new piece of work gets pushed onto a local queue for later processing by the same thread. The primary distinction between thread-per-core approaches and work-stealing approaches is that in work-stealing models, if one thread doesn't have enough work to do, it can "steal" that task and move it over to its own queue.&lt;/p&gt;
    &lt;p&gt;This has several immediate consequences:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It has to be okay to move those pieces of work across thread boundaries. This is the cause of people's frustration with their futures having to be &lt;code&gt;Send&lt;/code&gt;, in Rust.&lt;/item&gt;
      &lt;item&gt;Work can be more evenly balanced. If stealing isn't allowed, then there might be a thread, or handful of threads, with a long work queue, while all the others (and their associated CPU cores) sit idle. Stealing is an elegant solution to that problem.&lt;/item&gt;
      &lt;item&gt;If any task can be stolen from any other thread, you lose certain locality guarantees: if you know stealing isn't allowed, and two tasks both operate on similar data, you might hope that they can benefit from sharing cache lines.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the data processing world, for a couple years there it seemed like the needle had firmly swung in the direction of thread-per-core. Yes, of course you should partition your data across threads‚Äîcross-core data movement is the only enemy! Of course skewed data is a problem to be solved at a higher level, the data processing layer is optimized to scream through all the data you give it, so needing to be friendly in how you dish that work out is a small price to pay.&lt;/p&gt;
    &lt;p&gt;If your keys are basically random, this is great: the benefits are real, data tends to stay in cache, you don‚Äôt need slow MESI messages creating contention, and implementation is often dramatically simplified by restricting parallelism to very specific points in the code.&lt;/p&gt;
    &lt;p&gt;Except it seems like there‚Äôs been an increase in dissenters over the last several years: actually, maybe the data processing layer can be the cleanest place to put dynamic reshuffling work. The paper on Morsel-Driven Parallelism proposes some reasons why this kind of exchange focused parallelism might no longer be the best model:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Increasing core counts on high end machines means that improperly handling skewed data distributions are more painful.&lt;/item&gt;
      &lt;item&gt;Many traditional bottlenecks, like IO latency, have improved massively since the days where Exchange was state-of-the-art. At that time, ensuring maximum CPU utilization was not so important, since you‚Äôd typically be bound by other things, but things like disk speed has improved dramatically in the last 10 years while CPU speeds have not.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The problem of debating concurrency models for data processing is a bit different than that for programming language async runtimes, I think. We have a lot less heterogenous types of work, and we, as the scheduler, can do a lot more predictive introspection about what data a piece of work is likely to need, and we can manipulate tasks algebraically to even merge or split them up.&lt;/p&gt;
    &lt;p&gt;This sort of freedom is, I think, another big reason why shared-state concurrency has once again become popular. If you're implementing a query engine, you simply have more insight into the type of work you're going to do, which lets your scheduler make smarter decisions.&lt;/p&gt;
    &lt;p&gt;On top of all those things, I think another big reason is cultural: the more your data systems scale and need to handle things like multitenancy effectively, the more prone you are to skew you have very little control over. "Solve the skew problem a layer up" is not a particularly effective strategy for certain levels of scale, and you need to just bite the bullet and have systems that have that kind of elasticity built into them directly.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://buttondown.com/jaffray/archive/the-death-of-thread-per-core/"/><published>2025-10-20T21:19:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655161</id><title>Neural audio codecs: how to get audio into LLMs</title><updated>2025-10-22T08:46:45.419754+00:00</updated><content>&lt;doc fingerprint="2e1cc79b29380613"&gt;
  &lt;main&gt;
    &lt;p&gt;Thank you for the valuable feedback on the drafts: Chung-Ming Chien, Moritz Boehle, Richard Hlad√≠k, Eugene Kharitonov, Patrick Perez, and Tom Sl√°ma. I‚Äôd also like to thank the rest of the Kyutai team for the the research discussions without which this article could not exist.&lt;/p&gt;
    &lt;p&gt;As of October 2025, speech LLMs suck. Many LLMs have voice interfaces, but they usually work by transcribing your speech, generating the answer in text, and using text-to-speech to read the response out loud. That‚Äôs perfectly fine in many cases (see Unmute), but it‚Äôs a wrapper, not real speech understanding. The model can‚Äôt hear the frustration in your voice and respond with empathy, it can‚Äôt emphasize important words in its answer, it cannot sense sarcasm, and so on.&lt;/p&gt;
    &lt;p&gt;Yes, there are LLMs (Gemini, ChatGPT‚Äôs Advanced Voice Mode, Qwen, Moshi) that understand and generate speech natively. But in practice, they‚Äôre either not as smart, or they behave like text model wrappers. Try asking any of them ‚ÄúAm I speaking in a low voice or a high voice?‚Äù in a high-pitched voice, and they won‚Äôt be able to tell you.&lt;/p&gt;
    &lt;p&gt;Clearly, speech LLMs lag behind text LLMs. But why? For text, we found out a few years ago that if you take a lot of text data, a big Transformer, and a lot of GPUs, you‚Äôll get some pretty damn good text continuation models. Why can‚Äôt we just replace text with audio and get pretty damn good speech continuation models?&lt;/p&gt;
    &lt;p&gt;As a teaser, here‚Äôs what happens when you try to do that naively (warning, loud):&lt;/p&gt;
    &lt;p&gt;We‚Äôll have a look at why audio is harder to model than text and how we can make it easier with neural audio codecs, the de-facto standard way of getting audio into and out of LLMs. With a codec, we can turn audio into larger discrete tokens, train models to predict continuations for these tokens, and then decode those back into audio: see animation above.&lt;/p&gt;
    &lt;p&gt;Kyutai folks have done a lot of work in this space, which is part of the reason I chose to cover this topic. We‚Äôll start from the basics and build up all the way to Mimi, our neural audio codec. It was originally developed for Moshi and later adopted by others for their models, notably Sesame‚Äôs CSM.&lt;/p&gt;
    &lt;p&gt;To tokenize text, everybody uses a technique called byte-pair encoding and rarely changes the tokenizer: OpenAI has been using the same tokenizer since GPT-4o, an ancient model if you count in LLM years.&lt;/p&gt;
    &lt;p&gt;You can even get decent results without tokenizing text at all, just predicting individual characters. One of the first posts that got me excited about machine learning was Andrej Karpathy‚Äôs RNN effectiveness blog post from 2015. Karpathy trains a three-layer LSTM on a single GPU and gets it to generate decent-looking code and LaTeX:&lt;/p&gt;
    &lt;p&gt;Remember this was ten years ago, back when we didn‚Äôt even know that attention is all we need. Now compare Karpathy‚Äôs results to a sample from WaveNet, a model DeepMind published a year later:&lt;/p&gt;
    &lt;p&gt;Purely acoustically, the audio sounds good, but it rarely even manages to produce a single correct English word. We can‚Äôt be too hard on WaveNet, though. The samples from Karpathy‚Äôs RNNs are only a few thousand characters long, but this 10-second audio consists of 160k audio samples, and WaveNet creates it by painstakingly predicting sample-by-sample.&lt;/p&gt;
    &lt;p&gt;It‚Äôs difficult to build models that are coherent over time scales this long, and the model also takes very long to run for so many steps.&lt;/p&gt;
    &lt;p&gt;So instead of running the model to predict the samples one-by-one directly, we‚Äôd like to train a model to compress the audio into a more manageable size. We could compress the audio, use an LLM to predict a continuation in the compressed representation, and then decompress the result.&lt;/p&gt;
    &lt;p&gt;But first, let‚Äôs get a baseline model by generating audio sample by sample, like WaveNet does. The code for all of these experiments is open-source! Check it out here. I forked Andrej Karpathy‚Äôs nanoGPT repo, a simple implementation of GPT-2.&lt;/p&gt;
    &lt;p&gt;Text and audio are kind of the same from the perspective of the language model: it‚Äôs just tokens in, tokens out. The only thing we need to do is to quantize the continuous values of the samples into discrete buckets. Like WaveNet, we‚Äôll use the "Œº-law algorithm" to get 256 buckets. We‚Äôll treat those as 256 possible tokens.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs train a language model on audio tokenized like this. For the dataset, we‚Äôll use the Libri-Light dataset, following AudioLM (with Neil Zeghidour, Eugene Kharitonov). Its train split contains 50k hours in total, but we‚Äôll go with 1000 hours for this experiment. With this sample-by-sample tokenization, we end up with a dataset of 53 GB.&lt;/p&gt;
    &lt;p&gt;We train a small-ish transformer of 151.28M parameters, about the size of the smallest GPT-2 variant. When we sample from the model, it makes babbling sounds (warning, loud at times!):&lt;/p&gt;
    &lt;p&gt;Often, it goes into a ‚Äúcrackling mode‚Äù that it can‚Äôt seem to get out of:&lt;/p&gt;
    &lt;p&gt;I also trained a smaller model, which I teased at the beginning. It‚Äôs prone to generate nightmare fuel screeches (loud!):&lt;/p&gt;
    &lt;p&gt;As you can tell, we‚Äôre not AGI yet. It sounds speech-like, but you can‚Äôt make out a single word and the voice keeps changing. No wonder: the context size of the model is 2048, which, for 16 kHz audio, translates to 128ms, not even the length of one word. Also, these 10-second examples took 30 minutes to generate on an H100, so we‚Äôre a few orders of magnitude away from being real-time.&lt;/p&gt;
    &lt;p&gt;So let‚Äôs build a neural audio codec to compress the audio. The hope is that if we reduce the sampling rate 100x, the model will also become ‚Äú100x more coherent‚Äù. An old idea in machine learning is to do this using an autoencoder: a model that takes an input, compresses it into a smaller ‚Äúlatent space‚Äù, and then tries to reconstruct the original input.&lt;/p&gt;
    &lt;p&gt;In our case, we‚Äôll want an autoencoder whose latent space is quantized so that we can feed the latents into a language model and produce continuations. (You can generate continuations with unquantized latents, but it‚Äôs trickier ‚Äì see the Further reading section.)&lt;/p&gt;
    &lt;p&gt;Bear with me, because we‚Äôll take a detour from audio: let‚Äôs build a quantized autoencoder on images from Fashion MNIST. We‚Äôll take a subset with the first three classes: t-shirt/top, trouser, and pullover.&lt;/p&gt;
    &lt;p&gt;First, let‚Äôs train a regular autoencoder to encode the images into two-dimensional space:&lt;/p&gt;
    &lt;p&gt;Each frame shows one batch of training, with some batches skipped. The little images are the autoencoder‚Äôs reconstructions for the images in the batch. I‚Äôve added colors for the three classes (t-shirt/top=blue trousers=green, pullover=yellow), but the autoencoder doesn‚Äôt get a class as input ‚Äì the space just naturally clusters by class. Let's zoom in on a few reconstructions:&lt;/p&gt;
    &lt;p&gt;As you can tell, the reconstruction quality is not great. The images are blurry and the first two images are reconstructed to nearly the same thing. But we used a tiny network (4 fully connected layers for the encoder and decoder each) and projected into a mere two dimensions, so we can‚Äôt expect too much of our model.&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs quantize these embeddings using a clustering. We‚Äôll do something like k-means: we‚Äôll maintain a list of the positions of the cluster centers. We initialize the positions randomly. For each training batch, we look at which embeddings would go to each cluster. (We don‚Äôt modify the embeddings, we just look at the assignment). Then we‚Äôll nudge each cluster center towards the average position of these embeddings.&lt;/p&gt;
    &lt;p&gt;Also, if a center is unused for a while, we teleport it to a random embedding from the batch, because otherwise it has no way to get unstuck from its current position.&lt;/p&gt;
    &lt;p&gt;You can see the reconstructions of the cluster centers getting refined over time.&lt;/p&gt;
    &lt;p&gt;Next, we‚Äôll make the encoder and decoder themselves better at handling quantized embeddings during training, because currently, we‚Äôre just fitting the clustering on top of an autoencoder that is not ‚Äúaware‚Äù it‚Äôs being quantized. We‚Äôd like the autoencoder to adapt to the quantization as we train it. Currently, we‚Äôre doing this:&lt;/p&gt;
    &lt;code&gt;x = get_batch()
z = encoder(x)

x_reconstructed = decoder(z)

loss = reconstruction_loss(x, x_reconstructed)
&lt;/code&gt;
    &lt;p&gt;Instead of feeding the unquantized embedding into the decoder, we‚Äôll first move it to the closest cluster:&lt;/p&gt;
    &lt;code&gt;x = get_batch()
z = encoder(x)

z_quantized = to_nearest_cluster(z)     # üëà
x_reconstructed = decoder(z_quantized)  # üëà

loss = reconstruction_loss(x, x_reconstructed)
&lt;/code&gt;
    &lt;p&gt;There is a snag: if we do this, we won‚Äôt be able to train the autoencoder any more, because the quantization operation is not differentiable, meaning there is no gradient flowing from the loss to the weights of the encoder. Essentially, we‚Äôre no longer able to answer the question: ‚Äúif I want the loss to decrease a bit, in which direction should I nudge the encoder‚Äôs weights?‚Äù&lt;/p&gt;
    &lt;p&gt;We‚Äôll fix this problem by pretending it doesn‚Äôt exist. Yes, really. We‚Äôll think of &lt;code&gt;z_quantized&lt;/code&gt; as &lt;code&gt;z&lt;/code&gt; moved by an arbitrary vector that doesn‚Äôt affect the gradient. That will make the gradient of &lt;code&gt;z&lt;/code&gt; equal to that of &lt;code&gt;z_quantized&lt;/code&gt;, which is why this is also known as the straight-through estimator of the gradient.&lt;/p&gt;
    &lt;code&gt;x = get_batch()
z = encoder(x)

residual = z - to_nearest_cluster(z)
# .detach() means "forget that this needs a gradient"
z_quantized = z - residual.detach()
x_reconstructed = decoder(z_quantized)

loss = reconstruction_loss(x, x_reconstructed)
&lt;/code&gt;
    &lt;p&gt;In the forward pass, &lt;code&gt;z_quantized&lt;/code&gt; is set to the same value as before, but importantly, the gradient of &lt;code&gt;z&lt;/code&gt; is now equal to that of &lt;code&gt;z_quantized&lt;/code&gt; rather than just being 0 because of the non-differentiable &lt;code&gt;to_nearest_cluster(z)&lt;/code&gt; operation.&lt;/p&gt;
    &lt;p&gt;There is a price to pay for this lie. When training, the encoder‚Äôs weights will be updated to improve the reconstruction loss, but they‚Äôre updated as if the quantization didn‚Äôt happen, so they won‚Äôt move in the optimal direction. But as long as the embeddings stick close to their cluster centers, the gradient direction will still be mostly correct.&lt;/p&gt;
    &lt;p&gt;We can actually encourage the encoder to make embeddings that are easily quantizable by adding a commitment loss: a penalty for each point based on how far it is from its cluster center. The gradient of this loss will push the points closer to their cluster centers.&lt;/p&gt;
    &lt;p&gt;By quantizing at training time and adding a commitment loss, it‚Äôs no longer just a clustering being fit on top of the embeddings. The model itself is trained to be good for quantization.&lt;/p&gt;
    &lt;p&gt;You‚Äôll notice that the training dynamics look different: the commitment loss adds a certain ‚Äústiffness‚Äù that doesn‚Äôt allow the embeddings to move around as easily.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs what the reconstructions look like when we use the quantized representations:&lt;/p&gt;
    &lt;p&gt;Notice how the first two images are reconstructed to exactly the same image. That‚Äôs simply because their embeddings got assigned to the same cluster and therefore quantized to the same value.&lt;/p&gt;
    &lt;p&gt;The model described here is known as a ‚ÄúVQ-VAE‚Äù: a vector-quantized variational autoencoder. The word ‚Äúvariational‚Äù here is just a vestigial leftover that doesn‚Äôt mean anything anymore.&lt;/p&gt;
    &lt;p&gt;To improve the reconstruction fidelity, we can just increase the number of cluster centers. But keeping track of too many centers can get prohibitively expensive in terms of compute and memory required, so we‚Äôll do a clever trick: if we want 2^20 (~1M) possible values, we won‚Äôt create 2^20 clusters directly. Instead, we‚Äôll use two separate quantizers with 2^10=1024 clusters and combine their result. Each embedding will then be quantized to a tuple of two integers in [0..1023], yielding 2^20 possible combinations.&lt;/p&gt;
    &lt;p&gt;Ok, but how? Well, recall the &lt;code&gt;residual&lt;/code&gt; variable we used in the straight-through estimator, defined as &lt;code&gt;z - to_nearest_cluster(z)&lt;/code&gt; the shift from the quantized embedding to the unquantized one. It represents the part of the original vector &lt;code&gt;z&lt;/code&gt; that we didn‚Äôt manage to take into account when quantizing to &lt;code&gt;to_nearest_cluster(z)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So for each embedding in the batch, we have a corresponding residual vector. The solution is obvious: we‚Äôll quantize these residuals exactly the same way we did with the original embeddings, by training another vector quantizer.&lt;/p&gt;
    &lt;p&gt;This time, the 2D positions for a single quantizer don‚Äôt define images because we need to combine the two quantizers, so we‚Äôll just visualize everything as dots:&lt;/p&gt;
    &lt;p&gt;Each image is then represented as the index of the cluster of the embedding and that of the residual. Let‚Äôs try to reconstruct a few images with this two-level quantizer:&lt;/p&gt;
    &lt;p&gt;The reconstructions of the first two images are similar, but no longer the exact same: the first image is represented as (4, 3) and the second as (4, 5). In other words, they share the same token for the first level, but differ in how the residual is quantized. The differences are quite subtle, so here‚Äôs a comparison between the one-level and two-level reconstructions:&lt;/p&gt;
    &lt;p&gt;I‚Äôd like to emphasize that the second quantization level makes modifications to the embedding, not the output pixels directly. This can be seen by the fact that the leftmost and rightmost image are encoded as (4, 3) and (30, 3) respectively. So they have the same residual code, 3, but it modifies the two reconstructed images in different ways.&lt;/p&gt;
    &lt;p&gt;Clearly, the reconstructions are still not very accurate. The upper bound on the quality is the reconstruction from unquantized embeddings, so if your autoencoder is bad (and ours is), improving the quantization won‚Äôt save you.&lt;/p&gt;
    &lt;p&gt;We‚Äôll stop here, but a natural extension to this idea is to go beyond two levels. Just take the residuals of the two-level reconstruction and quantize those, and so on. This generalized Residual Vector Quantization algorithm looks like this:&lt;/p&gt;
    &lt;code&gt;def rvq_quantize(z):
    residual = z
    codes = []

    for level in range(levels):
        quantized, cluster_i = to_nearest_cluster(level, residual)
        residual -= quantized
        codes.append(cluster_i)

    return codes
&lt;/code&gt;
    &lt;p&gt;Residual vector quantization was first applied to neural audio codecs in SoundStream, but the idea has been around since the 80s.&lt;/p&gt;
    &lt;p&gt;Applying RVQ to audio is fairly straightforward. As our autoencoder, we‚Äôll use a convolutional neural network (CNN) similar to what Jukebox uses. The details of the architecture aren‚Äôt too important here. What‚Äôs important is that it‚Äôs a network that takes an audio with t samples and converts it to a vector of shape (t/128, 32). In other words, it downsamples by a factor of 128 and gives us 32-dimensional float representations. The decoder then takes the (t/128, 32) embeddings and decodes them back into t samples.&lt;/p&gt;
    &lt;code&gt;audio = get_batch()               # shape: [B, T]
z = encoder(audio)                # shape: [B, T/128, 32]
audio_reconstructed = decoder(z)  # shape: [B, T]
&lt;/code&gt;
    &lt;p&gt;As before, we‚Äôll add an RVQ after the encoder. The only difference from the image case is that for each audio sample, we have t/128 embedding vectors, not just a single one as we did for images. We just quantize these independently (even though the encoder ‚Äúsees‚Äù more audio than what corresponds to that one vector). During training, we also have a batch dimension, so our model now looks like this:&lt;/p&gt;
    &lt;code&gt;audio = get_batch()                         # [B, T]
z = encoder(audio)                          # [B, T/128, 32]

# Combine the batch and time dimensions
z = rearrange(                              # [B*T/128, 32]
    z, "b t_emb d -&amp;gt; (b t_emb) d"
)

codes = rvq_quantize(z)           # integers, [B*T/128, levels]
z_quantized = codes_to_embeddings(codes)    # [B*T/128, 32]
z_quantized = rearrange(                    # [B, T/128, 32]
    z_quantized, "(b t_emb) d -&amp;gt; b t_emb d"
)

audio_reconstructed = decoder(z_quantized)  # [B, T]
&lt;/code&gt;
    &lt;p&gt;The last missing piece before we can train our first neural audio codec is a loss function. There‚Äôs a whole rabbit hole we could go into about which one to choose, but we‚Äôll avoid it and just use a very simple one. We‚Äôll compute the log amplitude spectrogram of the original and reconstructed audio, and take their difference. The loss is the mean square of this difference between spectrograms.&lt;/p&gt;
    &lt;p&gt;To make it harder for the model to overfit to this loss, we take the spectrogram with three different parameters for the short-time Fourier transform, and let our loss be the mean between the three sub-losses. This is called the multi-scale spectral loss.&lt;/p&gt;
    &lt;p&gt;Finally, let‚Äôs train some codecs! We‚Äôll look at how varying the number of RVQ levels affects the reconstruction quality. As we expected, increasing the number of levels helps, decreasing the spectral loss:&lt;/p&gt;
    &lt;p&gt;Let‚Äôs hear what the codecs sound like. We‚Äôll use the three codecs to reconstruct this audio from the Expresso dataset:&lt;/p&gt;
    &lt;p&gt;And the reconstructions:&lt;/p&gt;
    &lt;p&gt;Clearly, the audio gets better as we add more RVQ levels.&lt;/p&gt;
    &lt;p&gt;Even with 16 levels, there is some crackling, the audio sounds muffled, and there is a constant high-pitched noise. Later we‚Äôll discuss how we could improve the codec further, but for demonstration purposes, this will do.&lt;/p&gt;
    &lt;p&gt;So now we have a neural audio codec: we can turn audio into LLM-friendly tokens and back. Codec just means a tokenizer for audio, but we say codec because that‚Äôs the term used for classic compression like MP3. I‚Äôll be using codec and tokenizer interchangeably.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs come back to what we wanted to do in the first place: modeling audio. Specifically, we‚Äôll make a model that can take an audio prefix and generate a plausible continuation for it.&lt;/p&gt;
    &lt;p&gt;Just as a reminder, we want to train good audio LLMs so that we have models that understand and produce speech natively, understanding emotion, emphasis, and so on. They could also be fine-tuned into text-to-speech, speech-to-text, or translation models, among others.&lt;/p&gt;
    &lt;p&gt;So now that you‚Äôre convinced that audio LLMs are the path to AGI, let‚Äôs train a few.&lt;/p&gt;
    &lt;p&gt;For our dataset, we‚Äôll use Libri-Light, like we did for our sample-by-sample model earlier. This time we‚Äôll use 10000h of audio instead of 1000h. It‚Äôs a dataset of public-domain audiobooks, so if we have a good model for it, maybe we‚Äôll be able to generate more stories. (Don‚Äôt get your hopes up too much.) All we need to do is to convert the audio dataset into a sequence of discrete tokens so that we can feed it into an LLM.&lt;/p&gt;
    &lt;p&gt;We‚Äôll do that using our 8-level RVQ codec. From an audio with t samples, we‚Äôll get an array of tokens of shape (t/128, 8). But now there‚Äôs an issue: how to deal with the fact that for each time step, there‚Äôs not one but eight tokens? This is not a problem we have to deal with in text LLMs, where we have a single sequence of tokens.&lt;/p&gt;
    &lt;p&gt;We‚Äôll do the simplest thing possible and just flatten the array into 1D of shape (t/128 * 8), and have our LLM predict the eight levels in separate time steps.&lt;/p&gt;
    &lt;p&gt;The big disadvantage is that we lose some of our temporal compression. We downsampled the audio 128x, but now we‚Äôre inflating it 8x again by flattening the levels. That makes inference less efficient, and possibly worse quality because the effective context size decreases. We'll be using the 8 RVQ codec rather than the 16 RVQ one to avoid making the compression even worse.&lt;/p&gt;
    &lt;p&gt;You could also predict all RVQ levels for a single step at once (‚Äùparallel pattern‚Äù), but it also makes things harder for the model because it has to decide on all levels at once. There are a bunch of other schemes people have tried to balance compression and quality. Here are a few tried out in MusicGen:&lt;/p&gt;
    &lt;p&gt;Interestingly, as of 2025, there is no single solution that ‚Äúwon‚Äù: every paper does something different, and the schemes can get quite involved. Just look at this diagram from MiMo-Audio, a model released in September 2025:&lt;/p&gt;
    &lt;p&gt;Time to finally train a codec-wrapped language model! As I‚Äôve mentioned, our code is based on Andrej Karpathy‚Äôs nanoGPT codebase for training text LLMs. We just need to modify it to accept audio as input. But that‚Äôs easy, because LLMs don‚Äôt care about what kind of tokens you‚Äôre feeding in ‚Äì it‚Äôs all just numbers. Once we‚Äôve tokenized the dataset and flattened it into a 1D sequence, we‚Äôre good to go. Tokenized this way, our 10000 hours of audio take up 134 GB. For comparison, storing this much data as uncompressed audio would take over 1 TB.&lt;/p&gt;
    &lt;p&gt;We‚Äôre going to use the exact same model architecture and hyperparameters as for the sample-by-sample model: the only difference is in the tokenization. We also have a 10x bigger dataset, but the sample-by-sample model can‚Äôt even fit the dataset with 1k hours, so more data wouldn‚Äôt save it.&lt;/p&gt;
    &lt;p&gt;I trained the model on 8 H100s for about 5 days. To get some samples, I decided to prompt the model with a sample of Libri-Light reading of two lines from Michael Field‚Äôs poem July. (As I learned when working on this, Michael Field is a pen name of Katherine Harris and Edith Emma Cooper.) Let‚Äôs see what kind of poetry we can get from our model:&lt;/p&gt;
    &lt;p&gt;There are some signs of life, but we don‚Äôt have a poet yet. It sounds like somebody speaking behind a curtain. You can‚Äôt really make out what it‚Äôs saying, but the intonation is there: it sounds like somebody reading from a book, which is indeed what the model was trained on.&lt;/p&gt;
    &lt;p&gt;It also maintains a coherent voice, until it decides for the last few seconds to switch to a different one. That is also consistent with the data: we sample the training data from a concatenation of all the audiobooks chopped up into segments and mixed together, so the model does encounter boundaries between different speakers.&lt;/p&gt;
    &lt;p&gt;Our codec was deliberately simplistic, which explains why the results aren't great‚Äîbut there's been a good amount of research on neural audio codecs in the last four years that we could leverage. We won‚Äôt implement all the improvements here, but instead we‚Äôll look at what happens when we use Mimi as the tokenizer.&lt;/p&gt;
    &lt;p&gt;Mimi is a modern neural audio codec built here at Kyutai for Moshi, our audio language model. It‚Äôs since been used as the tokenizer for other models as well, like Sesame CSM, VoXtream, and LFM2-Audio.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, Mimi sounds a lot better than the homemade codec we trained earlier.&lt;/p&gt;
    &lt;p&gt;Instead of the multi-scale spectral loss, Mimi uses an adversarial loss, like a GAN. There‚Äôs a discriminator network that tries to classify audios as being original or reconstructed by the codec, and the goal of the codec is to fool this discriminator.&lt;/p&gt;
    &lt;p&gt;Another improvement Mimi adds is using RVQ dropout: it uses 32 RVQ levels but during training, the reconstruction is sometimes randomly truncated to a lower number of levels. That allows us to run Mimi for a lower number of RVQ levels at inference time and still get decent results, because it doesn‚Äôt rely on all levels being present. For our codec, we had to train separately.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs hear our example audio reconstructed with Mimi:&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;p&gt;For our purposes, a variant with fewer levels might have the advantage of being easier to model because it‚Äôs more compressed. Let‚Äôs train models with 8- and 32-level Mimi and compare the results.&lt;/p&gt;
    &lt;p&gt;I trained the exact same model architecture as before, the only thing that changes is the tokenizer. It‚Äôs 10k hours from Libri-Light as the dataset, just like when we used our simple codec. Mimi has a sample rate of 24 kHz but Libri-Light uses 16 kHz, which puts a cap on how good it can sound, since we lose the higher frequencies of the audio.&lt;/p&gt;
    &lt;p&gt;Mimi downsamples the audio a lot more aggressively, too: its sample rate is 12.5 frames per second, whereas we used 125 frames per second for our codec ‚Äì 10x higher! This means the dataset is also smaller on disk. With our codec, it took 134 GB, but for Mimi it‚Äôs ‚Äújust‚Äù 54 GB.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a poem generated with the model trained on Mimi-tokenized data. I prompted it with two lines from the poem, as before:&lt;/p&gt;
    &lt;p&gt;Here is my best attempt at a transcription:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When the grass is gone&lt;/p&gt;&lt;lb/&gt;And corn still grassy;&lt;lb/&gt;Illness worried in the fur&lt;lb/&gt;this and pelan in stones&lt;lb/&gt;during the turan‚Äôs ciscerey&lt;lb/&gt;headforths nepet Paul Twain.&lt;lb/&gt;He sees zin in them.&lt;/quote&gt;
    &lt;p&gt;A tad too surrealist for my taste, but maybe Lewis Carroll would like it.&lt;/p&gt;
    &lt;p&gt;I have a confession to make: I lied to you just now. But just a bit, and for didactic purposes. In fact, the model above was trained on audio from a 31-level Mimi, where I omitted the very first level, which contains the ‚Äúsemantic token‚Äù.&lt;/p&gt;
    &lt;p&gt;The role of this token is to represent semantic information of the audio, without necessarily aiding reconstruction. I won‚Äôt go into how these work, but in one sentence, Mimi‚Äôs semantic tokens are distilled from WavLM, which you can think of as a BERT for speech.&lt;/p&gt;
    &lt;p&gt;To get a feeling for what information semantic tokens encode, let‚Äôs take this example audio, passed through Mimi:&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs train a language model trained on the full Mimi, including semantic tokens. We‚Äôre going to run the model in a way where we keep the semantic tokens from the original audio but we discard the others, and let the model predict them. That means the information from the semantic tokens is fixed (‚Äùteacher-forced‚Äù), but the model is free to decide the others according to what continuations it finds plausible.&lt;/p&gt;
    &lt;p&gt;Listen to two different reconstructions we obtain this way:&lt;/p&gt;
    &lt;p&gt;The voice is completely different, but it‚Äôs saying the same thing! This means the semantic tokens encode what the person is saying, but are invariant to the voice. That‚Äôs useful because it helps the model focus on what to say, not how to say it. In that regard, they‚Äôre closer to text tokens, which also don‚Äôt contain information about the voice, intonation, timing, or emotion.&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs take the model trained on semantic Mimi and ask it to complete the poem:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When grass is gone&lt;/p&gt;&lt;lb/&gt;and corn still grassy;&lt;lb/&gt;from the man was nothing moan.&lt;lb/&gt;The low death and heart&lt;lb/&gt;She came fyde wood.&lt;lb/&gt;A finteriest, a fall,&lt;lb/&gt;all them.&lt;/quote&gt;
    &lt;p&gt;It still makes up words and the sentences are not too coherent, but clearly, the proportion of real words is much higher; the model is ‚Äúmore semantic‚Äù. The acoustic quality is the same, which is what we‚Äôd expect.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs listen to a second poem:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When grass is gone&lt;/p&gt;&lt;lb/&gt;and corn still grassy;&lt;lb/&gt;hope won and she&lt;lb/&gt;who is just a night in Tatan&lt;lb/&gt;in doe ock-ohm?&lt;lb/&gt;the whom?&lt;/quote&gt;
    &lt;p&gt;Indeed, the whom?&lt;/p&gt;
    &lt;p&gt;We can sacrifice some acoustic quality to improve the semantics by reducing the number of RVQ levels. Let‚Äôs do 8. That way, we get higher audio compression, and a proportionally higher part of the loss comes from the semantic token, since now it‚Äôs 1/8 tokens and not just 1/32.&lt;/p&gt;
    &lt;p&gt;One of the first things I noticed about this model is that it learned to memorize the Librivox notice, so it sometimes generates things like:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Chapter 6 of The Founday, by R. Auclair.&lt;/p&gt;&lt;lb/&gt;This is a Librivox recording. All Librivox recordings are in the public domain. For information, or to volunteer, please visit librivox.org.&lt;lb/&gt;Reading by: Kelvert&lt;/quote&gt;
    &lt;p&gt;Repeating the training data is generally not what you want, but in our case it‚Äôs a great sign of life, because the previous models couldn‚Äôt even manage that. It also makes up the book, author, and reader, so there is still novelty here.&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs try to make some more poetry:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When grass is gone&lt;/p&gt;&lt;lb/&gt;and corn still grassy;&lt;lb/&gt;When so we could say&lt;lb/&gt;that in fairy interesting wife&lt;lb/&gt;who lay there and gone&lt;lb/&gt;that save the rosy light of life&lt;lb/&gt;Jay Dien, the antique mollity&lt;lb/&gt;and a mollity the beast of gray failed summon&lt;p&gt;end of poem.&lt;/p&gt;&lt;p&gt;This recording is in the public domain.&lt;/p&gt;&lt;p&gt;[different voice]&lt;/p&gt;&lt;lb/&gt;So we have formed a float that sent in would rattle down. The piece of opportunity reading and assimila‚Äî&lt;/quote&gt;
    &lt;p&gt;This is great. There are several signs of the model being better than the previous ones. I love that it makes up the word ‚Äúmollity‚Äù and then repeats it in the next line. Also, it realizes that it‚Äôs reciting a poem and ends the section with ‚Äúend of poem‚Äù. Then it decides it‚Äôs the end of the chapter/section and it ends with the ‚ÄúThis recording is in the public domain.‚Äù disclaimer. After that, it changes the voice and continues talking. That makes sense, since the clips from various audiobooks are just shuffled and concatenated during training, so here the model simulated a clip boundary.&lt;/p&gt;
    &lt;p&gt;We might get even better results by weighing the loss of the semantic tokens higher than the acoustic tokens, to make the model focus more on the meaning than the sound ‚Äì in fact, Moshi uses a semantic loss factor of 100x! But we have to stop somewhere.&lt;/p&gt;
    &lt;p&gt;We‚Äôve managed to use neural audio codecs to make an audio language model that generates somewhat coherent speech. Obviously, that‚Äôs not where the state of the art is in 2025 (and we‚Äôre not trying to reach it here) but keep in mind that by using the exact same model without neural audio codecs gives us this:&lt;/p&gt;
    &lt;p&gt;Of course, still a long way to go to match text models! Currently, there seems to be a trade-off between speech understanding and reasoning abilities. At the beginning, I mentioned that the speech-native models (Gemini, ChatGPT‚Äôs Advanced Voice Mode, Qwen, Moshi) aren‚Äôt able to tell you whether you‚Äôre speaking in a high or low voice, despite the fact that they‚Äôre trained to natively understand audio. This is likely because they‚Äôre trained on a lot of data generated synthetically with text-to-speech and/or because understanding the tone of the voice (apparently) doesn‚Äôt help the models make more accurate predictions.&lt;/p&gt;
    &lt;p&gt;Kyutai took a stab at creating a voice chat based on an audio language model with Moshi (demo, paper), released in July 2024. Moshi might not be the AI you‚Äôd pick to do your homework for you, but cut it some slack: it was the first end-to-end voice AI, released even before OpenAI‚Äôs Advanced Voice Mode.&lt;/p&gt;
    &lt;p&gt;Moshi models an ‚Äúinner monologue‚Äù text stream in parallel with audio streams for itself and the user. The text stream is helps it plan what it‚Äôs going to say, and ablations showed that the text stream helps the model massively. At the same time, it‚Äôs a bit sad: most of the reasoning seems to be delegated to the text stream and the audio streams are just there to provide an integrated speech-to-text and text-to-speech.&lt;/p&gt;
    &lt;p&gt;It‚Äôs not just Moshi: as the ‚Äúam I speaking in a high voice‚Äù experiment shows, this over-reliance on text in favor of audio is an issue for all audio LLMs. And that‚Äôs even though the dominant modeling approach is somewhat different than Moshi‚Äôs: interleaving text and audio tokens instead of modeling them in parallel streams.&lt;/p&gt;
    &lt;p&gt;Over a year after Moshi, audio models still lag behind text LLMs. But why? To me, this mysterious unsolved ‚Äúmodality gap‚Äù makes audio ML an exciting field to work on.&lt;/p&gt;
    &lt;p&gt;Thank you for reading! The code for the experiments is here, and for the animations here.&lt;lb/&gt; Follow Kyutai on X and LinkedIn.&lt;lb/&gt; You can also find my personal site here.&lt;/p&gt;
    &lt;p&gt;Here are some papers to check out if you'd like to learn more. This list is naturally Kyutai-centric because that's the school of thought I'm exposed to; my goal is not to do a complete review of the field.&lt;/p&gt;
    &lt;p&gt;van den Oord et al., 2016. WaveNet: A Generative Model for Raw Audio&lt;/p&gt;
    &lt;p&gt;Mehri et al., 2016. SampleRNN: An Unconditional End-to-End Neural Audio Generation Model&lt;/p&gt;
    &lt;p&gt;van den Oord et al., 2017. Parallel WaveNet: Fast High-Fidelity Speech Synthesis&lt;/p&gt;
    &lt;p&gt;Kumar et al., 2019. MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis&lt;/p&gt;
    &lt;p&gt;Kong et al., 2020. HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis&lt;/p&gt;
    &lt;p&gt;van den Oord et al., 2017. Neural Discrete Representation Learning&lt;/p&gt;
    &lt;p&gt;Esser et al., 2020. Taming Transformers for High-Resolution Image Synthesis&lt;/p&gt;
    &lt;p&gt;Lakhotia et al., 2021. On Generative Spoken Language Modeling from Raw Audio&lt;/p&gt;
    &lt;p&gt;Zeghidour et al., 2021. SoundStream: An End-to-End Neural Audio Codec&lt;/p&gt;
    &lt;p&gt;Lee et al., 2022. Autoregressive Image Generation using Residual Quantization&lt;/p&gt;
    &lt;p&gt;D√©fossez et al., 2022. High Fidelity Neural Audio Compression&lt;/p&gt;
    &lt;p&gt;Chen et al., 2021. WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing&lt;/p&gt;
    &lt;p&gt;D√©fossez et al., 2024. Moshi: a speech-text foundation model for real-time dialogue&lt;/p&gt;
    &lt;p&gt;Dieleman, 2025. Generative modelling in latent space&lt;/p&gt;
    &lt;p&gt;Peng et al., 2025. VibeVoice Technical Report&lt;/p&gt;
    &lt;p&gt;Rouard et al., 2025. Continuous Audio Language Models&lt;/p&gt;
    &lt;p&gt;Here are some modern LLMs (as of October 2025) that natively support audio. Again, I'm not trying to maintain a complete list here, and I'm not including models without any published technical details.&lt;/p&gt;
    &lt;p&gt;Moshi (Kyutai, 2023): the online demo of Moshi, Kyutai's audio language model ‚Äì see above.&lt;/p&gt;
    &lt;p&gt;CSM (Sesame, 2025): a natural-sounding voice chat, based on Llama + Mimi.&lt;/p&gt;
    &lt;p&gt;Qwen3-Omni (Alibaba, 2025): Alibaba's multimodal LLM. The audio output is created by a "talker" model whose outputs are not fed back into, which, as far as I can tell, basically makes it a text model with an integrated text-to-speech.&lt;/p&gt;
    &lt;p&gt;MiMo-Audio (Xiaomi, 2025): an audio-only language model that shows promising few-shot capabilities, similar to what GPT-2 did for text.&lt;/p&gt;
    &lt;p&gt;LFM2-Audio (Liquid AI, 2025): audio/text language model, uses Mimi as the codec.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kyutai.org/next/codec-explainer"/><published>2025-10-21T12:55:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45655188</id><title>NASA chief suggests SpaceX may be booted from moon mission</title><updated>2025-10-22T08:46:45.166297+00:00</updated><content>&lt;doc fingerprint="bf8990bd6db69da9"&gt;
  &lt;main&gt;
    &lt;p&gt;NASA may sideline SpaceX and choose a different company to land its astronauts on the moon later this decade, acting space agency chief Sean Duffy suggested during TV appearances Monday.&lt;/p&gt;
    &lt;p&gt;Duffy emphasized that he believes SpaceX, which has a $2.9 billion contract to provide the lunar lander astronauts would ride to the moon‚Äôs surface, is lagging behind schedule, potentially thwarting NASA‚Äôs efforts to return humans to the moon before China amid a new space race.&lt;/p&gt;
    &lt;p&gt;‚ÄúThey push their timelines out, and we‚Äôre in a race against China,‚Äù Duffy told CNBC‚Äôs ‚ÄúSquawk Box‚Äù on Monday morning, referring to SpaceX‚Äôs development of Starship ‚Äî the vehicle the company plans to use as a lunar lander for NASA. ‚ÄúSo, I‚Äôm going to open up the contract. I‚Äôm going to let other space companies compete with SpaceX.‚Äù&lt;/p&gt;
    &lt;p&gt;If NASA were to cancel or amend its contract with SpaceX, it could signal a remarkable reversal of a plan the space agency has had in place since 2021. That‚Äôs when NASA chose Starship ‚Äî which is still in the early stages of development and has racked up three in-flight failures and a couple successful suborbital test flights so far in 2025 ‚Äî to serve as lunar lander during the historic moon landing mission, called Artemis III.&lt;/p&gt;
    &lt;p&gt;Duffy‚Äôs remarks on Monday come as that 2021 decision is facing new scrutiny from space industry leaders who are concerned that the logistics involved with using SpaceX‚Äôs Starship are too complex and may cause NASA to lose the new moon race, as CNN previously reported. The Artemis III moon-landing mission is currently set to take place as soon as mid-2027.&lt;/p&gt;
    &lt;p&gt;SpaceX did not respond to a request for comment on Duffy‚Äôs remarks.&lt;/p&gt;
    &lt;head rend="h2"&gt;What could happen next&lt;/head&gt;
    &lt;p&gt;The exact timeline for NASA to potentially alter its deal with SpaceX or bring on a new contractor was not immediately clear. In a separate interview on Monday with Fox News‚Äô ‚ÄúFox &amp;amp; Friends,‚Äù Duffy said he‚Äôs ‚Äúin the process of opening that contract up,‚Äù referring to the Artemis lunar lander agreement.&lt;/p&gt;
    &lt;p&gt;NASA already has two different companies contracted to provide lunar landers: SpaceX with its Starship vehicle, and Blue Origin, the space venture founded by Amazon billionaire Jeff Bezos, which is developing a lander called Blue Moon.&lt;/p&gt;
    &lt;p&gt;It is Starship, however, that is slated to fly the Artemis III mission in 2027, which would mark the first time astronauts have set foot on the lunar surface since the Apollo program concluded five decades ago. (Blue Origin, which received its NASA contract in 2023, is looking to use Blue Moon to complete Artemis missions later in the program, such as Artemis V.)&lt;/p&gt;
    &lt;p&gt;In a statement, NASA press secretary Bethany Stevens said that the space agency gave SpaceX and Blue Origin until October 29 to present ‚Äúacceleration approaches‚Äù for lunar lander development.&lt;/p&gt;
    &lt;p&gt;‚ÄúNASA is also going to request plans from the entire commercial space industry - through an RFI (or Request for Information) - for how NASA can increase the cadence of our mission to the Moon,‚Äù the statement reads. ‚ÄúPresident Trump and Secretary Duffy have a mission to beat China back to the Moon. That‚Äôs why they are harnessing the power of the American space industry and seeking solutions to develop more ways to land on the Moon.‚Äù&lt;/p&gt;
    &lt;p&gt;The timeline of Artemis III has been the focus of hawkish lawmakers concerned that the landing will not occur before taikonauts ‚Äî or Chinese astronauts ‚Äî reach the moon. China has said it will accomplish that feat by 2030.&lt;/p&gt;
    &lt;p&gt;In his remarks to CNBC, Duffy suggested it could be Blue Origin that takes over SpaceX‚Äôs position in the Artemis III mission.&lt;/p&gt;
    &lt;p&gt;In response to CNN‚Äôs request for comment regarding Duffy‚Äôs remarks, Blue Origin said only that it is ‚Äúready to support.‚Äù&lt;/p&gt;
    &lt;p&gt;However, Duffy also warned that NASA may opt to open up the competition more broadly to providers that do not yet have contracts.&lt;/p&gt;
    &lt;p&gt;‚ÄúIf SpaceX is behind, but Blue Origin can do it before them, good on Blue Origin,‚Äù Duffy said. ‚ÄúBut ‚Ä¶ we‚Äôre not going to wait for one company. We‚Äôre going to push this forward and win the second space race against the Chinese.‚Äù&lt;/p&gt;
    &lt;p&gt;It‚Äôs unclear if NASA‚Äôs plans to issue a RFI will result in any new companies securing contracts. In government contracting parlance, RFIs are typically considered part of an informal fact-finding process whereas Requests for Proposals, or RFPs, are a more formal solicitation.&lt;/p&gt;
    &lt;p&gt;Space industry experts have expressed concerns about the timelines for both SpaceX‚Äôs Starship and Blue Origin‚Äôs Blue Moon, noting that the vehicles are complex and may need to be refueled in orbit.&lt;/p&gt;
    &lt;p&gt;In-orbit refueling has never been attempted, the experts noted, and lunar landers requiring such a step could require prohibitively long development timelines.&lt;/p&gt;
    &lt;p&gt;It‚Äôs not clear what other US companies may be in a position to join SpaceX and Blue Origin in competing for NASA Artemis contracts.&lt;/p&gt;
    &lt;p&gt;Dynetics, an aerospace company based in Alabama, was among the companies that originally bid for a lunar lander contract alongside Blue Origin and SpaceX. Dynetics did not immediately reply to a request for comment on Monday.&lt;/p&gt;
    &lt;p&gt;Editor‚Äôs Note: This story has been updated with additional details.&lt;/p&gt;
    &lt;p&gt;Sign up for CNN‚Äôs Wonder Theory science newsletter. Explore the universe with news on fascinating discoveries, scientific advancements and more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnn.com/2025/10/20/science/nasa-spacex-moon-landing-contract-sean-duffy"/><published>2025-10-21T12:58:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45656223</id><title>LLMs can get "brain rot"</title><updated>2025-10-22T08:46:45.080746+00:00</updated><content>&lt;doc fingerprint="2d43abd1eeb79f40"&gt;
  &lt;main&gt;
    &lt;p&gt;We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk web text induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: M1 (engagement degree) and M2 (semantic quality), with matched token scale and training operations across conditions.&lt;/p&gt;
    &lt;p&gt;Contrary to the control group, continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' g&amp;gt;0.3) on reasoning, long-context understanding, safety, and inflating "dark traits" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops 74.9 ‚Üí 57.2 and RULER-CWE 84.4 ‚Üí 52.3 as junk ratio rises from 0% to 100%.&lt;/p&gt;
    &lt;p&gt;Error forensics reveal several key insights:&lt;/p&gt;
    &lt;p&gt;Together, the results provide significant, multi-perspective evidence that data quality is a causal driver of LLM capability decay, reframing curation for continual pretraining as a training-time safety problem and motivating routine "cognitive health checks" for deployed LLMs.&lt;/p&gt;
    &lt;p&gt;‚ÄúBrain rot‚Äù burst into public discourse as a shorthand for how endless, low-effort, engagement-bait content can dull human cognition‚Äîeroding focus, memory discipline, and social judgment through compulsive online consumption. If large language models learn from the same internet firehose, the question becomes unavoidable: what happens when we keep feeding models the digital equivalent of junk food? Studying ‚ÄúBrain Rot‚Äù for LLMs isn‚Äôt just a catchy metaphor‚Äîit reframes data curation as cognitive hygiene for AI, guiding how we source, filter, and maintain training corpora so deployed systems stay sharp, reliable, and aligned over time.&lt;/p&gt;
    &lt;p&gt;Distinct from prior work that primarily focuses on data quality for training LLMs, we aim to provide a new view on data quality - the extent to which content is trivial and easy to consume for humans in social media. The properties, conceptualized via tweet shortness/popularity or content semantics, are not intuitively related to the cognitive capabilities that we expect LLMs to master in learning.&lt;/p&gt;
    &lt;p&gt;Intervention Method: The core idea was to simulate how an LLM's ‚Äúmind‚Äù changes when fed different information diets. (1) We used continual pre-training as the main intervention ‚Äî exposing models to either junk or clean data for a sustained period, just as humans continually absorb online content. (2) Afterward, every model went through the same instruction tuning step to ensure format consistency and eliminate task-specific bias.&lt;/p&gt;
    &lt;p&gt;Data Receipe: To operationalize the idea of ‚Äújunk,‚Äù we built two complementary metrics for selecting data from real Twitter/X posts:&lt;/p&gt;
    &lt;p&gt;Measuring Cognitive Function: We leverage existing benchmarks to examine the multifaceted ``cognitive functions'' of LLMs. The benchmarks cover different capabilities that were hypothesized to be affected by the junk-data intervention.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Cognitive Func.&lt;/cell&gt;
        &lt;cell role="head"&gt;Benchmark&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;ARC&lt;/cell&gt;
        &lt;cell&gt;Visual program-induction puzzles on grids testing concept abstraction.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Memory &amp;amp; Multi-tasking&lt;/cell&gt;
        &lt;cell&gt;RULER&lt;/cell&gt;
        &lt;cell&gt;Benchmark the long-context understanding and retrieval of multiple queries from long context.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ethical Norms&lt;/cell&gt;
        &lt;cell&gt;HH-RLHF &amp;amp; AdvBench&lt;/cell&gt;
        &lt;cell&gt;Testing if LLMs follow harmful instructions.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Personality&lt;/cell&gt;
        &lt;cell&gt;TRAIT&lt;/cell&gt;
        &lt;cell&gt;Psychometrically validated small human questionnaires to assess personality-like tendencies.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We analyze intervention effects by comparing benchmark differences after feeding junk/control data to four LLMs. The difference is measured by Hedges' g across 4 LLMs. In the above figure, both M1 and M2 produce non-trivial effects (Hedges' g &amp;gt; 0.3) on reasoning and long-context capabilities.&lt;/p&gt;
    &lt;p&gt;Across the remaining benchmarks the two interventions diverge, implying that engagement degree (M1) is not a proxy for semantic quality (M2) but represents a distinct dimension of data quality.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="12"&gt;
        &lt;cell role="head"&gt;Task&lt;/cell&gt;
        &lt;cell role="head"&gt;Junk Ratio by M1 (engagement degree)&lt;/cell&gt;
        &lt;cell role="head"&gt;Junk Ratio by M2 (semantic quality)&lt;/cell&gt;
        &lt;cell role="head"&gt;Base&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;80%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
        &lt;cell&gt;20%&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;80%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
        &lt;cell&gt;20%&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Reasoning (ARC)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Easy Acc.&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;73.3&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;78.7&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;77.8&lt;/cell&gt;
        &lt;cell&gt;78.2&lt;/cell&gt;
        &lt;cell&gt;77.5&lt;/cell&gt;
        &lt;cell&gt;78.4&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Challenge Acc.&lt;/cell&gt;
        &lt;cell&gt;41.6&lt;/cell&gt;
        &lt;cell&gt;43.9&lt;/cell&gt;
        &lt;cell&gt;44.7&lt;/cell&gt;
        &lt;cell&gt;46.5&lt;/cell&gt;
        &lt;cell&gt;47.8&lt;/cell&gt;
        &lt;cell&gt;42.6&lt;/cell&gt;
        &lt;cell&gt;47.9&lt;/cell&gt;
        &lt;cell&gt;47.7&lt;/cell&gt;
        &lt;cell&gt;47.4&lt;/cell&gt;
        &lt;cell&gt;47.4&lt;/cell&gt;
        &lt;cell&gt;47.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Challenge (COT) Acc.&lt;/cell&gt;
        &lt;cell&gt;57.2&lt;/cell&gt;
        &lt;cell&gt;67.2&lt;/cell&gt;
        &lt;cell&gt;68.2&lt;/cell&gt;
        &lt;cell&gt;73.4&lt;/cell&gt;
        &lt;cell&gt;74.9&lt;/cell&gt;
        &lt;cell&gt;67.7&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;77.3&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;76.6&lt;/cell&gt;
        &lt;cell&gt;77.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Long-Context (RULER)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Overall&lt;/cell&gt;
        &lt;cell&gt;71&lt;/cell&gt;
        &lt;cell&gt;81.6&lt;/cell&gt;
        &lt;cell&gt;86.1&lt;/cell&gt;
        &lt;cell&gt;88.5&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;86.2&lt;/cell&gt;
        &lt;cell&gt;92.9&lt;/cell&gt;
        &lt;cell&gt;93&lt;/cell&gt;
        &lt;cell&gt;93.4&lt;/cell&gt;
        &lt;cell&gt;93.8&lt;/cell&gt;
        &lt;cell&gt;93.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;NIAH-MK3&lt;/cell&gt;
        &lt;cell&gt;35.6&lt;/cell&gt;
        &lt;cell&gt;80.8&lt;/cell&gt;
        &lt;cell&gt;89.4&lt;/cell&gt;
        &lt;cell&gt;92.6&lt;/cell&gt;
        &lt;cell&gt;95.6&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
        &lt;cell&gt;97.2&lt;/cell&gt;
        &lt;cell&gt;98.8&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.4&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;NIAH-MQ&lt;/cell&gt;
        &lt;cell&gt;97.2&lt;/cell&gt;
        &lt;cell&gt;95.3&lt;/cell&gt;
        &lt;cell&gt;96.4&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.9&lt;/cell&gt;
        &lt;cell&gt;94&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.8&lt;/cell&gt;
        &lt;cell&gt;99.5&lt;/cell&gt;
        &lt;cell&gt;99.7&lt;/cell&gt;
        &lt;cell&gt;99.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;NIAH-MV&lt;/cell&gt;
        &lt;cell&gt;77.8&lt;/cell&gt;
        &lt;cell&gt;65.9&lt;/cell&gt;
        &lt;cell&gt;79.5&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;83.2&lt;/cell&gt;
        &lt;cell&gt;68.6&lt;/cell&gt;
        &lt;cell&gt;87&lt;/cell&gt;
        &lt;cell&gt;87.8&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;94.5&lt;/cell&gt;
        &lt;cell&gt;97.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Comm Word Ext (CWE)&lt;/cell&gt;
        &lt;cell&gt;52.3&lt;/cell&gt;
        &lt;cell&gt;63.2&lt;/cell&gt;
        &lt;cell&gt;64.1&lt;/cell&gt;
        &lt;cell&gt;81.6&lt;/cell&gt;
        &lt;cell&gt;84.4&lt;/cell&gt;
        &lt;cell&gt;68.2&lt;/cell&gt;
        &lt;cell&gt;94.7&lt;/cell&gt;
        &lt;cell&gt;97.3&lt;/cell&gt;
        &lt;cell&gt;96&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
        &lt;cell&gt;91.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Freq Word Ext (FWE)&lt;/cell&gt;
        &lt;cell&gt;81.8&lt;/cell&gt;
        &lt;cell&gt;77.2&lt;/cell&gt;
        &lt;cell&gt;83.3&lt;/cell&gt;
        &lt;cell&gt;84.7&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
        &lt;cell&gt;95.3&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;94.7&lt;/cell&gt;
        &lt;cell&gt;93.2&lt;/cell&gt;
        &lt;cell&gt;91.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;QA (Hotpot)&lt;/cell&gt;
        &lt;cell&gt;41.6&lt;/cell&gt;
        &lt;cell&gt;46.6&lt;/cell&gt;
        &lt;cell&gt;52.2&lt;/cell&gt;
        &lt;cell&gt;55.4&lt;/cell&gt;
        &lt;cell&gt;58.6&lt;/cell&gt;
        &lt;cell&gt;51.2&lt;/cell&gt;
        &lt;cell&gt;61.2&lt;/cell&gt;
        &lt;cell&gt;58.8&lt;/cell&gt;
        &lt;cell&gt;60.6&lt;/cell&gt;
        &lt;cell&gt;61.4&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;QA (SQUAD)&lt;/cell&gt;
        &lt;cell&gt;57.1&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;67.8&lt;/cell&gt;
        &lt;cell&gt;69.3&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;67.6&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;76.2&lt;/cell&gt;
        &lt;cell&gt;77.1&lt;/cell&gt;
        &lt;cell&gt;77.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Variable Tracking&lt;/cell&gt;
        &lt;cell&gt;22.4&lt;/cell&gt;
        &lt;cell&gt;78.7&lt;/cell&gt;
        &lt;cell&gt;94.1&lt;/cell&gt;
        &lt;cell&gt;87.6&lt;/cell&gt;
        &lt;cell&gt;91.5&lt;/cell&gt;
        &lt;cell&gt;86.6&lt;/cell&gt;
        &lt;cell&gt;98&lt;/cell&gt;
        &lt;cell&gt;99.4&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;98.6&lt;/cell&gt;
        &lt;cell&gt;98.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Ethical Norm (Safety)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;HH-RLHF Risk ‚Üì&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;53.6&lt;/cell&gt;
        &lt;cell&gt;45.8&lt;/cell&gt;
        &lt;cell&gt;63.6&lt;/cell&gt;
        &lt;cell&gt;62.8&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;68.8&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;61.8&lt;/cell&gt;
        &lt;cell&gt;57.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;AdvBench Risk ‚Üì&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;80.2&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;84.4&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
        &lt;cell&gt;85.4&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;61.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Personality (TRAIT)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Narcissism ‚Üì&lt;/cell&gt;
        &lt;cell&gt;47&lt;/cell&gt;
        &lt;cell&gt;21.8&lt;/cell&gt;
        &lt;cell&gt;29.9&lt;/cell&gt;
        &lt;cell&gt;22.8&lt;/cell&gt;
        &lt;cell&gt;18.9&lt;/cell&gt;
        &lt;cell&gt;20.9&lt;/cell&gt;
        &lt;cell&gt;17.4&lt;/cell&gt;
        &lt;cell&gt;16.9&lt;/cell&gt;
        &lt;cell&gt;23.7&lt;/cell&gt;
        &lt;cell&gt;24.2&lt;/cell&gt;
        &lt;cell&gt;33.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Agreeableness&lt;/cell&gt;
        &lt;cell&gt;64.3&lt;/cell&gt;
        &lt;cell&gt;67.9&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;68.5&lt;/cell&gt;
        &lt;cell&gt;73&lt;/cell&gt;
        &lt;cell&gt;82&lt;/cell&gt;
        &lt;cell&gt;74.2&lt;/cell&gt;
        &lt;cell&gt;69.9&lt;/cell&gt;
        &lt;cell&gt;71.6&lt;/cell&gt;
        &lt;cell&gt;70.6&lt;/cell&gt;
        &lt;cell&gt;75.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Psychopathy ‚Üì&lt;/cell&gt;
        &lt;cell&gt;75.7&lt;/cell&gt;
        &lt;cell&gt;55.8&lt;/cell&gt;
        &lt;cell&gt;57.2&lt;/cell&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;33.5&lt;/cell&gt;
        &lt;cell&gt;46.1&lt;/cell&gt;
        &lt;cell&gt;9.3&lt;/cell&gt;
        &lt;cell&gt;23.5&lt;/cell&gt;
        &lt;cell&gt;27.3&lt;/cell&gt;
        &lt;cell&gt;25.8&lt;/cell&gt;
        &lt;cell&gt;2.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Machiavellianism ‚Üì&lt;/cell&gt;
        &lt;cell&gt;33&lt;/cell&gt;
        &lt;cell&gt;30.6&lt;/cell&gt;
        &lt;cell&gt;31.8&lt;/cell&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;25.8&lt;/cell&gt;
        &lt;cell&gt;26.1&lt;/cell&gt;
        &lt;cell&gt;22.7&lt;/cell&gt;
        &lt;cell&gt;20.2&lt;/cell&gt;
        &lt;cell&gt;33.1&lt;/cell&gt;
        &lt;cell&gt;28.5&lt;/cell&gt;
        &lt;cell&gt;17.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Neuroticism ‚Üì&lt;/cell&gt;
        &lt;cell&gt;28.7&lt;/cell&gt;
        &lt;cell&gt;23.8&lt;/cell&gt;
        &lt;cell&gt;22.7&lt;/cell&gt;
        &lt;cell&gt;23.3&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;23.5&lt;/cell&gt;
        &lt;cell&gt;21.1&lt;/cell&gt;
        &lt;cell&gt;31.1&lt;/cell&gt;
        &lt;cell&gt;26.4&lt;/cell&gt;
        &lt;cell&gt;33.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Conscientiousness&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
        &lt;cell&gt;86&lt;/cell&gt;
        &lt;cell&gt;85.1&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;90.8&lt;/cell&gt;
        &lt;cell&gt;85.7&lt;/cell&gt;
        &lt;cell&gt;87.1&lt;/cell&gt;
        &lt;cell&gt;87.5&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="12"&gt;
        &lt;cell&gt;Openness&lt;/cell&gt;
        &lt;cell&gt;70.1&lt;/cell&gt;
        &lt;cell&gt;72.8&lt;/cell&gt;
        &lt;cell&gt;67.6&lt;/cell&gt;
        &lt;cell&gt;53.7&lt;/cell&gt;
        &lt;cell&gt;63.9&lt;/cell&gt;
        &lt;cell&gt;73.2&lt;/cell&gt;
        &lt;cell&gt;59.1&lt;/cell&gt;
        &lt;cell&gt;55.6&lt;/cell&gt;
        &lt;cell&gt;59.4&lt;/cell&gt;
        &lt;cell&gt;56.5&lt;/cell&gt;
        &lt;cell&gt;52.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Extraversion&lt;/cell&gt;
        &lt;cell&gt;54.1&lt;/cell&gt;
        &lt;cell&gt;40.1&lt;/cell&gt;
        &lt;cell&gt;44.9&lt;/cell&gt;
        &lt;cell&gt;39.5&lt;/cell&gt;
        &lt;cell&gt;48.7&lt;/cell&gt;
        &lt;cell&gt;46.4&lt;/cell&gt;
        &lt;cell&gt;37.9&lt;/cell&gt;
        &lt;cell&gt;38.6&lt;/cell&gt;
        &lt;cell&gt;40.8&lt;/cell&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;26.4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In dose-response testing, M1 engagement intervention demonstrates more significant and progressive impacts on reasoning and long-context capabilities than M2 intervention.&lt;/p&gt;
    &lt;p&gt;We analyze the reasoning failures in ARC-Challenge to identify different failure modes. We find that the majority failures can be attributed to "thought skipping" (e.g., the model fails to generate intermediate reasoning steps), which significantly increases in models affected by brain rot.&lt;/p&gt;
    &lt;p&gt;Our findings indicate that the cognitive decline associated with brain rot is not easily mitigated by standard fine-tuning techniques. Even after extensive instruction tuning (IT) or post-doc continual pre-training on high-quality control data, the models exhibit lingering effects of the junk data they were initially exposed to.&lt;/p&gt;
    &lt;p&gt;In this work, we introduced and empirically validated the LLM Brain Rot Hypothesis, demonstrating that continual exposure to junk data‚Äîdefined as engaging (fragmentary and popular) or semantically low-quality (sensationalist) content‚Äîinduces systematic cognitive decline in large language models. The decline includes worse reasoning, poorer long-context understanding, diminished ethical norms, and emergent socially undesirable personalities.&lt;/p&gt;
    &lt;p&gt;Fine-grained analysis shows that the damage is multifaceted in changing the reasoning patterns and is persistent against large-scale post-hoc tuning. These results call for a re-examination of current data collection from the Internet and continual pre-training practices. As LLMs scale and ingest ever-larger corpora of web data, careful curation and quality control will be essential to prevent cumulative harms.&lt;/p&gt;
    &lt;code&gt;@article{xing2024brainrot,
    title={LLMs Can Get "Brain Rot"!},
    author={Xing, Shuo and Hong, Junyuan and Wang, Yifan and Chen, Runjin and Zhang, Zhenyu and Grama, Ananth and Tu, Zhengzhong and Wang, Zhangyang},
    journal={arXiv:2510.13928},
    year={2025},
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://llm-brain-rot.github.io/"/><published>2025-10-21T14:24:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45656952</id><title>Show HN: Katakate ‚Äì Dozens of VMs per node for safe code exec</title><updated>2025-10-22T08:46:44.510035+00:00</updated><content>&lt;doc fingerprint="6b39c79d8ada9c0"&gt;
  &lt;main&gt;
    &lt;p&gt;KATAKATE&lt;/p&gt;
    &lt;p&gt;Self-hosted secure VM sandboxes for AI compute at scale&lt;/p&gt;
    &lt;p&gt;Katakate aims to make it easy to create, manage and orchestrate lightweight safe VM sandboxes for executing untrusted code, at scale. It is built on battle-tested VM isolation with Kata, Firecracker and Kubernetes. It is orignally motivated by AI agents that need to run arbitrary code at scale but it is also great for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Custom serverless (like AWS Fargate, but yours)&lt;/item&gt;
      &lt;item&gt;Hardened CI/CD runners (no Docker-in-Docker risks)&lt;/item&gt;
      &lt;item&gt;Blockchain execution layers for AI dApps&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;100% open‚Äësource (Apache‚Äë2.0). For technical support, write us at: hi@katakate.org&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Katakate is built on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kubernetes for orchestration, with K3s which is prod-ready and a great choice for edge nodes,&lt;/item&gt;
      &lt;item&gt;Kata to encapsulate containers into light-weight virtual-machines,&lt;/item&gt;
      &lt;item&gt;Firecracker as the chosen VM, for super-fast boots, light footprints and minimal attack surface,&lt;/item&gt;
      &lt;item&gt;Devmapper Snapshotter with thin-pool provisioning of logical volumes for efficient use of disk space shared by dozens of VMs per node.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üõ†Ô∏è Docker &lt;code&gt;build&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt;/&lt;code&gt;compose&lt;/code&gt;support inside the VM sandbox&lt;/item&gt;
      &lt;item&gt;üåê Multi-node cluster capabilities for distributed workloads&lt;/item&gt;
      &lt;item&gt;üîç Cilium FQDN-based DNS resolution to safely whitelist domains, not just IP blocks&lt;/item&gt;
      &lt;item&gt;‚öôÔ∏è Support other VMM such as Qemu for GPU workloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Katakate is currently in beta and under security review. Use with caution for highly sensitive workloads.&lt;/p&gt;
    &lt;p&gt;For usage you need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node(s) that will host the VM sandboxes&lt;/item&gt;
      &lt;item&gt;Client from where to send requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We provide a:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CLI: to use on the node(s) directly --&amp;gt; &lt;code&gt;apt install k7&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;API: deployed on the (master) node(s) --&amp;gt; &lt;code&gt;k7 start-api&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Python SDK: Python client sync/async talking to API --&amp;gt; &lt;code&gt;pip install katakate&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ubuntu (amd64) host.&lt;/item&gt;
      &lt;item&gt;Hardware virtualization (KVM) available and accessible &lt;list rend="ul"&gt;&lt;item&gt;Check: &lt;code&gt;ls /dev/kvm&lt;/code&gt;should exist.&lt;/item&gt;&lt;item&gt;This is typically available on your own Linux machine.&lt;/item&gt;&lt;item&gt;On cloud providers, it varies. &lt;list rend="ul"&gt;&lt;item&gt;Hetzner (the only one I tested so far) yes for their &lt;code&gt;Robot&lt;/code&gt;instances only, i.e. "dedicated": robot.hetzner.com.&lt;/item&gt;&lt;item&gt;AWS: only &lt;code&gt;.metal&lt;/code&gt;EC2 instances.&lt;/item&gt;&lt;item&gt;GCP: virtualization friendly, most instances, with &lt;code&gt;--enable-nested-virtualization&lt;/code&gt;flag.&lt;/item&gt;&lt;item&gt;Azure: Dv3, Ev3, Dv4, Ev4, Dv5, Ev5. Must be Intel/AMD x86, not ARM.&lt;/item&gt;&lt;item&gt;Others: in general, hardware virtualization is not exposed on cloud VPS, so you'll likely want a dedicated / bare metal.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Hetzner (the only one I tested so far) yes for their &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Check: &lt;/item&gt;
      &lt;item&gt;One raw disk (unformatted, unpartitioned) for the thin-pool that k7 will provision for efficient disk usage of sandboxes. &lt;list rend="ul"&gt;&lt;item&gt;Use &lt;code&gt;./utils/wipe-disk.sh /your/disk&lt;/code&gt;to wipe a disk clean before provisioning. DANGER: destructive - it will remove data/partitions/formatting/SWRAID.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Use &lt;/item&gt;
      &lt;item&gt;Ansible (for installer): &lt;quote&gt;sudo add-apt-repository universe -y sudo apt update sudo apt install -y ansible&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Docker and Docker Compose (for the API): &lt;code&gt;curl -fsSL https://get.docker.com | sh&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Already tested setups:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hetzner Robot instance with Ubuntu 24.04, x86_64 arch, booked with 1 extra empty disk &lt;code&gt;nvme2n1&lt;/code&gt;for the thin-pool provisioning. See the setup guide (PDF): tutorials/k7_hetzner_node_setup.pdf.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Just recent Python.&lt;/p&gt;
    &lt;p&gt;First install &lt;code&gt;k7&lt;/code&gt; on your Linux server that will host the VMs:&lt;/p&gt;
    &lt;code&gt;sudo add-apt-repository ppa:katakate.org/k7
sudo apt update
sudo apt install k7&lt;/code&gt;
    &lt;p&gt;Then let &lt;code&gt;k7&lt;/code&gt; get your node ready with everything:&lt;/p&gt;
    &lt;code&gt;$  k7 install
Current task: Reminder about logging out and back in for group changes
  Installing K7 on 1 host(s)... ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 100% 0:01:41
‚úÖ Installation completed successfully!
&lt;/code&gt;
    &lt;p&gt;Optionally pass &lt;code&gt;-v&lt;/code&gt; for a verbose output.&lt;/p&gt;
    &lt;p&gt;This will install and most importantly connect together the following components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kubernetes (K3s prod-ready distribution)&lt;/item&gt;
      &lt;item&gt;Kata (for container virtualization)&lt;/item&gt;
      &lt;item&gt;Firecracker (as Virtual Machine Manager)&lt;/item&gt;
      &lt;item&gt;Jailer (to secure Firecracker VMs further into a chroot)&lt;/item&gt;
      &lt;item&gt;devmapper snapshotter with thin-pool provisioning of logical volumes for VM efficient disk memory usage&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Careful design: config updates will not touch your existing Docker or containerd setups. We chose to use K3s' own containerd for minimal disruption. Installation may however overwrite existing installations of K3s, Kata, Firecracker, Jailer.&lt;/p&gt;
    &lt;p&gt;You can run workloads directly from the node(s) using the CLI. To create a sandbox, just create a yaml config for it.&lt;/p&gt;
    &lt;code&gt;name: my-sandbox-123
image: alpine:latest
namespace: default

# Optional: restrict egress
egress_whitelist:
  - "1.1.1.1/32"      # Cloudflare DNS
  - "8.8.8.8/32"      # Google DNS

# Optional: resource limits
limits:
  cpu: "1"
  memory: "1Gi"
  ephemeral-storage: "2Gi"

# Optional: run before_script inside the container once at start. Network restrictions apply after the before-script, so you can install packages here, pull git repos, etc
before_script: |
  apk add --no-cache git curl

# Optional: load environment variables from a file. These will be available both during the before-script, and in the sandbox
env_file: path/to/your/secrets/.env&lt;/code&gt;
    &lt;code&gt;# Create a sandbox (uses k7.yaml in the current directory by default, but you can also pass: -f myfile.yaml)
k7 create

# List sandboxes
k7 list

# Delete a sandbox
k7 delete my-sandbox-123

# Delete all sandboxes. You can also pass a namespace
k7 delete-all&lt;/code&gt;
    &lt;p&gt;If you'd like to manage workloads remotely, just use the API:&lt;/p&gt;
    &lt;code&gt;# Start API server (containerized and SSL support with Cloudflared)
k7 start-api

# Generate API key
k7 generate-api-key my-key1&lt;/code&gt;
    &lt;p&gt;Make sure your user is in the &lt;code&gt;Docker&lt;/code&gt; group to be allowed to start or stop the API.&lt;/p&gt;
    &lt;p&gt;As for generating / listing / revoking keys, you might need &lt;code&gt;sudo&lt;/code&gt; or &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;After your k7 API is up, usage is very simple.&lt;/p&gt;
    &lt;p&gt;Install the Python SDK via:&lt;/p&gt;
    &lt;code&gt;pip install katakate&lt;/code&gt;
    &lt;p&gt;Or if you want async support:&lt;/p&gt;
    &lt;code&gt;pip install "katakate[async-sdk]"&lt;/code&gt;
    &lt;p&gt;Then use with:&lt;/p&gt;
    &lt;code&gt;from katakate import Client

k7 = Client(
  endpoint='https://&amp;lt;your-endpoint&amp;gt;', 
  api_key='your-key')

# Create sandbox
sb = k7.create({
    "name": "my-sandbox",
    "image": "alpine:latest"
})

# Execute code
result = sb.exec('echo "Hello World"')
print(result['stdout'])

# List all sandboxes
sandboxes = k7.list()

# Delete sandbox
sb.delete()&lt;/code&gt;
    &lt;code&gt;import asyncio
from katakate import AsyncClient

async def main():
    k7 = AsyncClient(
      endpoint='https://&amp;lt;your-endpoint&amp;gt;', 
      api_key='your-key'
    )
    print(await k7.list())
    await k7.aclose()

asyncio.run(main())&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LangChain ReAct agent with a K7 sandbox tool &lt;list rend="ul"&gt;&lt;item&gt;Path: tutorials/langchain-react-agent&lt;/item&gt;&lt;item&gt;Setup: copy .env.example to .env and fill K7_ENDPOINT/K7_API_KEY/OPENAI_API_KEY&lt;/item&gt;&lt;item&gt;Run: python agent.py&lt;/item&gt;&lt;item&gt;Try asking it anything! e.g. "List files from '/'"&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First install make if not already available:&lt;/p&gt;
    &lt;code&gt;sudo add-apt-repository universe -y
sudo apt update
sudo apt install make&lt;/code&gt;
    &lt;p&gt;To build the &lt;code&gt;k7&lt;/code&gt; CLI and API into &lt;code&gt;.deb&lt;/code&gt; package:&lt;/p&gt;
    &lt;code&gt;make build&lt;/code&gt;
    &lt;p&gt;You can then install it with:&lt;/p&gt;
    &lt;code&gt;sudo make install&lt;/code&gt;
    &lt;p&gt;To uninstall later:&lt;/p&gt;
    &lt;code&gt;sudo make uninstall&lt;/code&gt;
    &lt;p&gt;Note: we recommend running &lt;code&gt;make uninstall&lt;/code&gt; before reinstalling if it is not your first install, to avoid stale copies of cached files in the .deb package.&lt;/p&gt;
    &lt;p&gt;Local dev image:&lt;/p&gt;
    &lt;code&gt;# Build the API image locally
make api-build-local

# Run API using local image (no pull)
make api-run-local&lt;/code&gt;
    &lt;p&gt;Preferred (uv):&lt;/p&gt;
    &lt;code&gt;# create env
uv venv .venv-build
. .venv-build/bin/activate

# install directly from source in editable mode
uv pip install -e .&lt;/code&gt;
    &lt;p&gt;K7 sandboxes are hardened by default with multiple layers of security:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;VM isolation: Kata Containers provide hardware-level isolation via lightweight VMs with Firecracker&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;VMs are further restricted into a chroot using Jailer&lt;/item&gt;
          &lt;item&gt;Kata's Seccomp restrictions are enabled&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Linux capabilities: All capabilities are dropped by default (&lt;/p&gt;&lt;code&gt;drop: ALL&lt;/code&gt;) for defense-in-depth&lt;list rend="ul"&gt;&lt;item&gt;Only explicitly add back capabilities you need via &lt;code&gt;cap_add&lt;/code&gt;parameter&lt;/item&gt;&lt;item&gt;&lt;code&gt;allow_privilege_escalation&lt;/code&gt;is always set to&lt;code&gt;false&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Seccomp profile: &lt;code&gt;RuntimeDefault&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Only explicitly add back capabilities you need via &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Non-root execution: Optionally run containers and pods as non-root user (UID 65532):&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;container_non_root&lt;/code&gt;: Run the main container as non-root and disable privilege escalation&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;pod_non_root&lt;/code&gt;: Run the entire pod as non-root with consistent filesystem ownership (UID/GID/FSGroup 65532)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;API security:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;API keys stored as SHA256 hashes with timing-attack-resistant comparison&lt;/item&gt;
          &lt;item&gt;Expiry enforced; last-used timestamp recorded&lt;/item&gt;
          &lt;item&gt;File-based storage with 600 permissions (&lt;code&gt;/etc/k7/api_keys.json&lt;/code&gt;by default)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network policies: Complete network isolation for VM sandboxes&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Ingress isolation: All inter-VM communication is blocked by default to prevent sandbox-to-sandbox access&lt;/item&gt;
          &lt;item&gt;Egress lockdown: Control outbound traffic with CIDR-based restrictions using Kubernetes NetworkPolicies&lt;/item&gt;
          &lt;item&gt;DNS to CoreDNS always allowed when egress is locked down&lt;/item&gt;
          &lt;item&gt;Administrative access via &lt;code&gt;kubectl exec&lt;/code&gt;and&lt;code&gt;k7 shell&lt;/code&gt;is preserved (uses Kubernetes API, not pod networking)&lt;/item&gt;
          &lt;item&gt;Soon to come: Cilium integration for domain name whitelisting&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More security features are currently on the roadmap, including integrating AppArmor.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Layout uses &lt;code&gt;src/&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;CLI, API, core live under &lt;code&gt;src/k7/&lt;/code&gt;&lt;/item&gt;&lt;item&gt;SDK under &lt;code&gt;src/katakate/&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;CLI, API, core live under &lt;/item&gt;
      &lt;item&gt;Root packaging targets the &lt;code&gt;katakate&lt;/code&gt;SDK only; assets under&lt;code&gt;src/k7/&lt;/code&gt;are not part of the PyPI distribution.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MANIFEST.in&lt;/code&gt;(for the&lt;code&gt;katakate&lt;/code&gt;SDK) should include essentials like&lt;code&gt;LICENSE&lt;/code&gt;and&lt;code&gt;README.md&lt;/code&gt;only; deploy assets from&lt;code&gt;src/k7/deploy/*&lt;/code&gt;belong to the Debian/CLI packaging flow, not to the PyPI package.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;setup.py&lt;/code&gt;for&lt;code&gt;katakate&lt;/code&gt;lives at repo root; packages from&lt;code&gt;src/&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The CLI Debian package is built via &lt;code&gt;src/k7/cli/build.sh&lt;/code&gt;and produces&lt;code&gt;dist/k7_&amp;lt;version&amp;gt;_amd64.deb&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;CI (tags &lt;code&gt;v*&lt;/code&gt;) can publish the PyPI SDK and upload the&lt;code&gt;.deb&lt;/code&gt;artifact.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jailer seems to be currently ignored by Kata despite being passed correctly into its configuration, and despite the Jailer process being started. The use of Kubernetes secrets could be a reason of incompatibility. This is under investigation.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Katakate/k7"/><published>2025-10-21T15:22:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657287</id><title>Foreign hackers breached a US nuclear weapons plant via SharePoint flaws</title><updated>2025-10-22T08:46:44.339012+00:00</updated><content>&lt;doc fingerprint="3598a8970a17de0d"&gt;
  &lt;main&gt;
    &lt;p&gt;A foreign actor infiltrated the National Nuclear Security Administration‚Äôs Kansas City National Security Campus through vulnerabilities in Microsoft‚Äôs SharePoint browser-based app, raising questions about the need to solidify further federal IT/OT security protections. Credit: Wirestock Creators / Shutterstock A foreign threat actor infiltrated the Kansas City National Security Campus (KCNSC), a key manufacturing site within the National Nuclear Security Administration (NNSA), exploiting unpatched Microsoft SharePoint vulnerabilities, according to a source involved in an August incident response at the facility. The breach targeted a plant that produces the vast majority of critical non-nuclear components for US nuclear weapons under the NNSA, a semi-autonomous agency within the Department of Energy (DOE) that oversees the design, production, and maintenance of the nation‚Äôs nuclear weapons. Honeywell Federal Manufacturing &amp;amp; Technologies (FM&amp;amp;T) manages the Kansas City campus under contract to the NNSA. The Kansas City campus, Honeywell FM&amp;amp;T, and the Department of Energy did not respond to repeated requests for comment throughout September, well before the current government shutdown. NSA public affairs officer Eddie Bennett did respond, saying, ‚ÄúWe have nothing to contribute,‚Äù and referred CSO back to the DOE. Although it is unclear whether the attackers were a Chinese nation-state actor or Russian cybercriminals ‚Äî the two most likely culprits ‚Äî experts say the incident drives home the importance of securing systems that protect operational technology from exploits that primarily affect IT systems. How the breach unfolded The attackers exploited two recently disclosed Microsoft SharePoint vulnerabilities ‚Äî CVE-2025-53770, a spoofing flaw, and CVE-2025-49704, a remote code execution (RCE) bug ‚Äî both affecting on-premises servers. Microsoft issued fixes for the vulnerabilities on July 19. On July 22, the NNSA confirmed it was one of the organizations hit by attacks enabled by the SharePoint flaws. ‚ÄúOn Friday, July 18th, the exploitation of a Microsoft SharePoint zero-day vulnerability began affecting the Department of Energy,‚Äù a DOE spokesperson said. However, the DOE contended at the time, ‚ÄúThe department was minimally impacted due to its widespread use of the Microsoft M365 cloud and very capable cybersecurity systems. A very small number of systems were impacted. All impacted systems are being restored.‚Äù By early August, federal responders, including personnel from the NSA, were on-site at the Kansas City facility, the source tells CSO. Located in Missouri, the KCNSC manufactures non-nuclear mechanical, electronic, and engineered material components used in US nuclear defense systems. It also provides specialized technical services, including metallurgical analysis, analytical chemistry, environmental testing, and simulation modeling. Roughly 80% of the non-nuclear parts in the nation‚Äôs nuclear stockpile originate from KCNSC. While most design and programmatic details remain classified, the plant‚Äôs production role makes it one of the most sensitive facilities in the federal weapons complex. China or Russia? Conflicting attribution Microsoft attributed the broader wave of SharePoint exploitations to three Chinese-linked groups: Linen Typhoon, Violet Typhoon, and a third actor it tracks as Storm-2603. The company said the attackers were preparing to deploy Warlock ransomware across affected systems. However, the source familiar with the Kansas City incident tells CSO that a Russian threat actor, not a Chinese one, was responsible for the intrusion. Cybersecurity company Resecurity, which was monitoring the SharePoint exploitations, tells CSO that its own data pointed primarily to Chinese nation-state groups, but it does not rule out Russian involvement. Resecurity‚Äôs researchers say that while Chinese groups appeared to have developed and deployed the initial zero-day, financially motivated Russian actors may have independently reproduced the exploit before technical details began circulating in late June. In May, researchers at Viettel Cyber Security demonstrated an attack chaining two SharePoint flaws, CVE-2025-49706 and CVE-2025-49704, at Pwn2Own Berlin. Resecurity researchers tell CSO that those demonstrations likely accelerated the reverse-engineering of the vulnerabilities by multiple threat actors. Resecurity‚Äôs analysts observed early-stage scanning and exploitation activity from infrastructure located in Taiwan, Vietnam, South Korea, and Hong Kong, a distribution pattern consistent with tactics used by Chinese advanced persistent threat (APT) groups to disguise attribution. ‚ÄúThe root cause of the SharePoint exploitation is closely related to misuse of the Microsoft Active Protections Program (MAPP) by China,‚Äù Resecurity researchers tell CSO. ‚ÄúThe most probable perpetrators are Chinese nation-state actors such as Linen Typhoon and Violet Typhoon.‚Äù Still, they say that yet another way that Russia-based threat actors could have acquired knowledge of the vulnerability early on was through underground exchanges or by analyzing network scanning data once the exploit became known. The transition from zero-day to N-day status, they say, opened a window for secondary actors to exploit systems that had not yet applied the patches. Could the attack have reached operational systems? The breach targeted the IT side of the Kansas City campus, but the intrusion raises the question of whether attackers could have moved laterally into the facility‚Äôs operational technology (OT) systems, the manufacturing and process control environments that directly support weapons component production. OT cybersecurity specialists interviewed by CSO say that KCNSC‚Äôs production systems are likely air-gapped or otherwise isolated from corporate IT networks, significantly reducing the risk of direct crossover. Nevertheless, they caution against assuming such isolation guarantees safety. ‚ÄúWe have to really consider and think through how state actors potentially exploit IT vulnerabilities to gain access to that operational technology,‚Äù Jen Sovada, general manager of public sector operations at Claroty, speaking generally and not about the specific incident, tells CSO. ‚ÄúWhen you have a facility like the KCNSC where they do nuclear weapons lifecycle management ‚Äî design, manufacturing, emergency response, decommissioning, supply chain management ‚Äî there are multiple interconnected functions,‚Äù Sovada says. ‚ÄúIf an actor can move laterally, they could impact programmable logic controllers that run robotics or precision assembly equipment for non-nuclear weapon components.‚Äù Such access, Sovada adds, could also affect distribution control systems that oversee quality assurance, or supervisory control and data acquisition (SCADA) systems that manage utilities, power, and environmental controls. ‚ÄúIt‚Äôs broader than just an IT vulnerability,‚Äù she says. IT/OT convergence and the zero-trust gap The Kansas City incident highlights a systemic problem across the federal enterprise: the disconnect between IT and OT security practices. While the federal government has advanced its zero-trust roadmap for traditional IT networks, similar frameworks for operational environments have lagged, although recent developments point to progress on that front. ‚ÄúThere‚Äôs an IT fan chart that maps all of the controls for zero trust, segmentation, authentication, and identity management,‚Äù Sovada says. ‚ÄúBut there‚Äôs also an OT fan chart being developed by the Department of Defense that will define comparable controls for zero trust in operational technology. The goal is to marry the two, so that zero trust becomes comprehensive across all network types.‚Äù That alignment, she says, is essential to preventing intrusions like the one that struck KCNSC from cascading into physical operations. Even non-classified data theft holds strategic value If the source‚Äôs claim of Russian involvement is accurate, the attackers may have been financially motivated ransomware operators rather than state intelligence services. But even in that scenario, the data they accessed could still carry strategic value. ‚ÄúIt would make sense that if it were a ransomware actor and they got this kind of data about nuclear weapons manufacturing, they might pause and hand it off to the appropriate Russian government officials or experts,‚Äù Sovada tells CSO. Although there is no evidence that classified information was compromised, even unclassified technical data can have significant implications. ‚ÄúIt could be something as simple as requirements documents that may not be classified but reveal the level of precision required for components,‚Äù Sovada says. ‚ÄúIn weapons manufacturing, a millimeter difference can change a device‚Äôs trajectory or the reliability of its arming mechanism.‚Äù Such information could aid adversaries in understanding US weapons tolerances, supply chain dependencies, or manufacturing processes, all of which are sensitive even if not formally secret. Whether the intruders were Chinese state actors or Russian cybercriminals, the Kansas City breach exposes the fragile intersection of IT and operational security across critical defense infrastructure. As Sovada stresses, ‚ÄúWe can‚Äôt just think of zero trust as an IT concept anymore. It has to extend into the physical systems that underpin national defense.‚Äù Update: The Department of Energy (DOE) confirmed that it is furloughing the vast major of the NNSA‚Äôs workers. DOE spokesperson said, ‚ÄúSince its creation in 2000, NNSA has never before furloughed federal workers during funding lapses. We are left with no choice this time. We‚Äôve extended funding as long as we could.‚Äù SUBSCRIBE TO OUR NEWSLETTER From our editors straight to your inbox Get started by entering your email address below. Please enter a valid email address Subscribe&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.csoonline.com/article/4074962/foreign-hackers-breached-a-us-nuclear-weapons-plant-via-sharepoint-flaws.html"/><published>2025-10-21T15:51:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657345</id><title>Ask HN: Our AWS account got compromised after their outage</title><updated>2025-10-22T08:46:43.864724+00:00</updated><content>&lt;doc fingerprint="33ca051f2b70648d"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Could there be any link between the two events?&lt;/p&gt;
      &lt;p&gt;Here is what happened:&lt;/p&gt;
      &lt;p&gt;Some 600 instances were spawned within 3 hours before AWS flagged it off and sent us a health event. There were numerous domains verified and we could see SES quota increase request was made.&lt;/p&gt;
      &lt;p&gt;We are still investigating the vulnerability at our end. our initial suspect list has 2 suspects. api key or console access where MFA wasn‚Äôt enabled.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45657345"/><published>2025-10-21T15:55:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45657827</id><title>Build your own database</title><updated>2025-10-22T08:46:43.531319+00:00</updated><content>&lt;doc fingerprint="172f0832ab8897d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Build Your Own Database&lt;/head&gt;
    &lt;p&gt;A step-by-step guide to building a key-value database from scratch.&lt;/p&gt;
    &lt;p&gt;If you were to build your own database today, not knowing that databases exist already, how would you do it? In this post, we'll explore how to build a key-value database from the ground up.&lt;/p&gt;
    &lt;p&gt;A key-value database works more or less like objects in JavaScript‚Äîyou can store values using a key and retrieve them later using that same key:&lt;/p&gt;
    &lt;quote&gt;$ db set 'hello' 'world'$ db get 'hello'world&lt;/quote&gt;
    &lt;p&gt;Let's find out how they work!&lt;/p&gt;
    &lt;head rend="h2"&gt;The Humble File&lt;/head&gt;
    &lt;p&gt;Databases were made to solve one problem:&lt;/p&gt;
    &lt;head rend="h4"&gt;Problem&lt;/head&gt;
    &lt;p&gt;How do we store data persistently and then efficiently look it up later?&lt;/p&gt;
    &lt;p&gt;The typical way to store any kind of data persistently in a computer is to use a file . When we want to store data, we add the key-value pair to the file:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When we want to look for a specific key, we iterate through the pairs to see if there's a matching key:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For updates, we'll find the key and replace the value in-place:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And for deletes, we'll delete the record from the file:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Easy! We're done right?&lt;/p&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Mutable Updates&lt;/head&gt;
    &lt;p&gt;This approach, simple as it is, doesn't actually work very well in practice. The problem lies with the way we're doing updates and deletes‚Äîthey're wholly inefficient.&lt;/p&gt;
    &lt;p&gt;To a computer, our file looks something like this‚Äînothing more than a long sequence of bytes:&lt;/p&gt;
    &lt;p&gt;001:Lorem‚ê£ipsum\n018:dolor‚ê£sit\n005:adipiscing‚ê£elit.\n014:Vestibulum‚ê£varius\n002:vel‚ê£mauris\n007:consectetur‚ê£adipiscing‚ê£elit.\n010:Vestibulum‚ê£varius\n016:vel‚ê£mauris\n003:consectetur‚ê£adipiscing‚ê£elit.&lt;/p&gt;
    &lt;p&gt;When we go to update or delete a record, we're currently updating that record in-place, which means we potentially have to move all of the data that comes after that record:&lt;/p&gt;
    &lt;p&gt;001:Lorem‚ê£ipsum\n018:dolor‚ê£sit\n005:adipiscing‚ê£elit.‚ê£vel‚ê£mauris\n014:Vestibulum‚ê£varius\n002:vel‚ê£mauris\n007:consectetur‚ê£adipiscing‚ê£elit.\n010:Vestibulum‚ê£varius\n016:vel‚ê£mauris\n003:consectetur‚ê£adipiscing‚ê£elit.&lt;/p&gt;
    &lt;p&gt;In this case, updating the record &lt;code&gt;005&lt;/code&gt; to "&lt;code&gt;adipiscing‚ê£elit.‚ê£vel‚ê£mauris&lt;/code&gt;" means moving all of the records that come after it by 11 bytes (the length of the added string "&lt;code&gt;‚ê£vel‚ê£mauris&lt;/code&gt;"). This can quickly get really costly, especially as our database grows in size!&lt;/p&gt;
    &lt;head rend="h3"&gt;Append-Only Files&lt;/head&gt;
    &lt;p&gt;One way to work around the update problem is to make records immutable. In other words, we add the constraint that we can only add new records to the end of the file and never update or delete existing ones.&lt;/p&gt;
    &lt;p&gt;With this approach, updates are treated the same as inserts‚Äîjust add a new record to the end of the file:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But now we have another problem‚Äîthere are duplicate keys in the file!&lt;/p&gt;
    &lt;p&gt;To work around this, we have to change our search algorithm to look for the last occurrence of the key instead of the first:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To delete records, we create a special "tombstone" record that marks the key as deleted. There's no single way to do this, but one way is to use a special value like &lt;code&gt;null&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And there we have it! We have a key-value database that uses a file as its storage mechanism. Using it, we can store, find, update, and delete key-value pairs.&lt;/p&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now this implementation isn't perfect; right now, there are two major issues:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The file can get very large. Since we're only appending to the file, the file will grow infinitely over time. Not good!&lt;/item&gt;
      &lt;item&gt;Searching is slow. To search for a specific key, we have to potentially iterate through all records in the database. For a database with millions of records, this can take a while!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How can we fix these problems?&lt;/p&gt;
    &lt;head rend="h2"&gt;Keeping Files Small&lt;/head&gt;
    &lt;head rend="h4"&gt;Problem&lt;/head&gt;
    &lt;p&gt;How do we make sure the file doesn't grow indefinitely? Because we're using an append-only file, we need some mechanism to periodically "shrink" the file so it doesn't eventually take over our entire hard drive.&lt;/p&gt;
    &lt;p&gt;Take a look at our database here after a few updates and deletes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ db set 1 "Lorem ipsum"&lt;/item&gt;
      &lt;item&gt;$ db set 18 "dolor sit"&lt;/item&gt;
      &lt;item&gt;$ db set 7 "adipiscing elit."&lt;/item&gt;
      &lt;item&gt;$ db delete 7&lt;/item&gt;
      &lt;item&gt;$ db set 10 "consectetur adipiscing elit."&lt;/item&gt;
      &lt;item&gt;$ db delete 1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
      &lt;item&gt;007: adipiscing elit.007:adipiscing elit.&lt;/item&gt;
      &lt;item&gt;007: null007:null&lt;/item&gt;
      &lt;item&gt;010: consectetur adipiscing elit.010:consectetur adipiscing elit.&lt;/item&gt;
      &lt;item&gt;001: null001:null&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our database file has six entries, but only two represent actual records‚Äîthe rest are either deleted or contain stale data. If we can clear all the irrelevant data, we can shrink the file by over 66%!&lt;/p&gt;
    &lt;p&gt;db.txt&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
      &lt;item&gt;007: adipiscing elit.007:adipiscing elit.&lt;/item&gt;
      &lt;item&gt;007: null007:null&lt;/item&gt;
      &lt;item&gt;010: consectetur adipiscing elit.010:consectetur adipiscing elit.&lt;/item&gt;
      &lt;item&gt;001: null001:null&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Segments and Compaction&lt;/head&gt;
    &lt;p&gt;Here's an idea: once a file exceeds a certain size, we'll close the file and create a new one. While the new file ingests new data (in the same way we've been doing so far), we'll compact the old file by deleting all of its irrelevant data.&lt;/p&gt;
    &lt;p&gt;Meaning, we stop adding new data to the file.&lt;/p&gt;
    &lt;p&gt;Here, we've set the maximum file size to seven records. Notice that the database is full‚Äîtry clicking on "Add" to add a new record and notice what happens:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;001:&lt;/p&gt;
        &lt;p&gt;Lorem ipsum&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;018:&lt;/p&gt;
        &lt;p&gt;dolor sit&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;007:&lt;/p&gt;
        &lt;p&gt;adipiscing elit.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;007:&lt;/p&gt;
        &lt;p&gt;null&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;010:&lt;/p&gt;
        &lt;p&gt;consectetur adipiscing elit.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;001:&lt;/p&gt;
        &lt;p&gt;null&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;020:&lt;/p&gt;
        &lt;p&gt;Vestibulum varius&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now, our database consists of two different files which we'll call segments. Each segment will usually become a lot smaller after compaction, which means we can merge them together as part of the compaction process.&lt;/p&gt;
    &lt;p&gt;With that, we've made a mechanism to stop our database from growing indefinitely!&lt;/p&gt;
    &lt;head rend="h2"&gt;Your First Index&lt;/head&gt;
    &lt;p&gt;Our next problem is on search performance:&lt;/p&gt;
    &lt;head rend="h4"&gt;Problem&lt;/head&gt;
    &lt;p&gt;How do we make searching fast? Right now, we have to iterate through all of the records in the database to find a specific key. This is super slow!&lt;/p&gt;
    &lt;p&gt;What if we use objects? That's right, these little guys:&lt;/p&gt;
    &lt;quote&gt;const hashTable = {};&lt;/quote&gt;
    &lt;p&gt;JavaScript objects, otherwise known as hash tables or dictionaries, are really efficient at storing and looking up key-value pairs:&lt;/p&gt;
    &lt;quote&gt;const hashTable = {hello: "world",foo: "bar",baz: "qux",};const value = hashTable["hello"]; // "world"&lt;/quote&gt;
    &lt;p&gt;It doesn't matter how many records there are‚Äîthe time it takes to look up and retrieve a value in a hash table is more or less constant. The catch is they must live in memory.&lt;/p&gt;
    &lt;head class="p-4 md:p-6 list-none"&gt;Aside: In-Memory vs. On-Disk&lt;/head&gt;
    &lt;head rend="h3"&gt;Aside: In-Memory vs. On-Disk&lt;/head&gt;
    &lt;p&gt;When you write a variable in your code, the computer will "remember" the value of that variable only for as long as the program is running.&lt;/p&gt;
    &lt;quote&gt;let x = 1;x = x + 1;console.log(x); // 2x = x + 1;console.log(x); // 3&lt;/quote&gt;
    &lt;p&gt;This program will always print &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;3&lt;/code&gt; because the value of &lt;code&gt;x&lt;/code&gt; "resets" every time we run the program. This is because &lt;code&gt;x&lt;/code&gt; is stored  in-memory , and any value stored in memory is discarded when the program stops.&lt;/p&gt;
    &lt;p&gt;If we want our data to "stick" between runs, we'll need to store it on-disk‚Äîin other words, a file.&lt;/p&gt;
    &lt;quote&gt;import fs from "fs";let x = Number(fs.readFileSync("data.txt", "utf8")) || 1;x = x + 1;console.log(x);x = x + 1;console.log(x);fs.writeFileSync("data.txt", x);&lt;/quote&gt;
    &lt;p&gt;This time, &lt;code&gt;x&lt;/code&gt; will print &lt;code&gt;2&lt;/code&gt; and &lt;code&gt;3&lt;/code&gt; the first run, and &lt;code&gt;4&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt; the second run.&lt;/p&gt;
    &lt;p&gt;The tradeoff to persistence is performance‚Äîaccessing data from memory is about 80x faster on average than accessing it from disk.&lt;/p&gt;
    &lt;p&gt;Here's how the index will work. For every record that we have in our database, we'll store that record's offset‚Äîthe number of bytes from the beginning of the file to the start of the record‚Äîin the index:&lt;/p&gt;
    &lt;p&gt;file.txt&lt;/p&gt;
    &lt;p&gt;1:Lorem‚ê£ipsum\n&lt;/p&gt;
    &lt;p&gt;0&lt;/p&gt;
    &lt;p&gt;1:Lorem‚ê£ipsum\n&lt;/p&gt;
    &lt;p&gt;Index&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;001:&lt;/p&gt;
        &lt;p&gt;0&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Database&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;001:&lt;/p&gt;
        &lt;p&gt;Lorem ipsum&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The second record, &lt;code&gt;18: dolor sit&lt;/code&gt;, for example, has an offset of 15 because:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Each character is 1 byte large;&lt;/item&gt;
      &lt;item&gt;The first record is 13 characters long (&lt;code&gt;1:Lorem ipsum&lt;/code&gt;);&lt;/item&gt;
      &lt;item&gt;The first record ends with a newline character, which is (at most) 2 bytes long;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This gives us an offset of &lt;code&gt;13 + 2 = 15&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;One thing to note is that we need an index for each segment because the offset is relative to the start of the file‚Äîin other words, the start of each segment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Searching With Indices&lt;/head&gt;
    &lt;p&gt;Using an index, our search algorithm can now run a lot more efficiently:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Starting at the most recent segment, look up the key in the index;&lt;/item&gt;
      &lt;item&gt;If the key is found, read the record at the offset;&lt;/item&gt;
      &lt;item&gt;If the key is not found, move on to the next segment;&lt;/item&gt;
      &lt;item&gt;Repeat (2) and (3) until the key is found or all segments have been searched.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;$ Waiting for commands...&lt;/p&gt;
    &lt;head rend="h3"&gt;Updating Indices&lt;/head&gt;
    &lt;p&gt;An index is only useful if it's in sync with our data. Whenever we update, delete, or insert a record, we have to change the index accordingly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: 0&lt;/item&gt;
      &lt;item&gt;018: 15&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notice what this implies‚Äîwriting to the database is slower with an index! This is one of the tradeoffs of using an index; we can search for data much faster at the cost of slower writes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs&lt;/head&gt;
    &lt;p&gt;An index is great because it lets us query our database much faster, but there are some problems with our specific hash table implementation:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Keys have to fit in memory. Since we're using an in-memory hash table as our index, all of the keys in our database must fit in memory. This means there's a limit on the number of keys we can store!&lt;/item&gt;
      &lt;item&gt;Range queries are inefficient. Our index wouldn't help for search queries; if we wanted to find all the records between the keys&lt;code&gt;12&lt;/code&gt;and&lt;code&gt;18&lt;/code&gt;, for example, we'd have to iterate through the entire database!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Sorted String Tables&lt;/head&gt;
    &lt;p&gt;Here's an idea: what if we ensure our database is always sorted by key? By sorting our data, we can immediately make range queries fast:&lt;/p&gt;
    &lt;p&gt;0 / 7&lt;/p&gt;
    &lt;p&gt;Unsorted&lt;/p&gt;
    &lt;p&gt;Find all values &amp;gt; 2 and &amp;lt; 6&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;7&lt;/item&gt;
      &lt;item&gt;3&lt;/item&gt;
      &lt;item&gt;9&lt;/item&gt;
      &lt;item&gt;1&lt;/item&gt;
      &lt;item&gt;4&lt;/item&gt;
      &lt;item&gt;8&lt;/item&gt;
      &lt;item&gt;11&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;7 is out of bounds, skipping...&lt;/p&gt;
    &lt;p&gt;Sorted&lt;/p&gt;
    &lt;p&gt;Find all values &amp;gt; 2 and &amp;lt; 6&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1&lt;/item&gt;
      &lt;item&gt;3&lt;/item&gt;
      &lt;item&gt;4&lt;/item&gt;
      &lt;item&gt;7&lt;/item&gt;
      &lt;item&gt;8&lt;/item&gt;
      &lt;item&gt;9&lt;/item&gt;
      &lt;item&gt;11&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;1 is below lower bound, skipping...&lt;/p&gt;
    &lt;head rend="h3"&gt;Sparse Indices&lt;/head&gt;
    &lt;p&gt;One benefit of sorting our data is that we no longer need to store the offset of every record in memory.&lt;/p&gt;
    &lt;p&gt;Take a look at this database with four records. Since there's no logical order to the records, there's no way to determine where a record is without storing its key or searching through the entire database.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;007: amet, consectetur007:amet, consectetur&lt;/item&gt;
      &lt;item&gt;010: adipiscing elit.010:adipiscing elit.&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: 0&lt;/item&gt;
      &lt;item&gt;007: 15&lt;/item&gt;
      &lt;item&gt;010: 36&lt;/item&gt;
      &lt;item&gt;018: 57&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Knowing that &lt;code&gt;10&lt;/code&gt; has an offset of &lt;code&gt;50&lt;/code&gt; doesn't help us find where &lt;code&gt;18&lt;/code&gt; is.&lt;/p&gt;
    &lt;p&gt;Now if these records were sorted, we could determine the location of each record using any of the keys in the index, even if it's not the key we're looking for.&lt;/p&gt;
    &lt;p&gt;Let's say our database is sorted but we only had the offset for the key &lt;code&gt;10&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum001:Lorem ipsum&lt;/item&gt;
      &lt;item&gt;007: amet, consectetur007:amet, consectetur&lt;/item&gt;
      &lt;item&gt;010: adipiscing elit.010:adipiscing elit.&lt;/item&gt;
      &lt;item&gt;018: dolor sit018:dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: 36&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let's say we want to find the key &lt;code&gt;18&lt;/code&gt;. We know that &lt;code&gt;18&lt;/code&gt; is greater than &lt;code&gt;10&lt;/code&gt;, which means it must be after &lt;code&gt;10&lt;/code&gt; in the database. In other words, we can start searching for &lt;code&gt;18&lt;/code&gt; from &lt;code&gt;10&lt;/code&gt;'s offset‚Äî&lt;code&gt;36&lt;/code&gt; in this case.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;001: Lorem ipsum&lt;/item&gt;
      &lt;item&gt;007: amet, consectetur&lt;/item&gt;
      &lt;item&gt;0010: adipiscing elit.&lt;/item&gt;
      &lt;item&gt;018: dolor sit0&lt;/item&gt;
      &lt;item&gt;018: dolor sit&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: 36&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While this is certainly slower than having the offset for &lt;code&gt;18&lt;/code&gt; directly, it's still faster than looping through the database in its entirety.&lt;/p&gt;
    &lt;p&gt;The real unlock here lies in being able to control the trade-off between memory and performance: a denser index means faster lookups, but more memory usage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sorting in Practice&lt;/head&gt;
    &lt;p&gt;Ensuring our database is always sorted is much easier said than done; by definition, sorting data requires moving around records as new ones get added‚Äîsomething that cannot be done efficiently when we're storing data on-disk. This brings us to our problem:&lt;/p&gt;
    &lt;head rend="h4"&gt;Problem&lt;/head&gt;
    &lt;p&gt;How do we keep our data sorted and append-only? It's too slow to sort the data on-disk every time we add a new record; is there another way?&lt;/p&gt;
    &lt;p&gt;The trick is to first sort the data in memory, and then write it to disk.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;When we add a new record, add it to a sorted in-memory list;&lt;/item&gt;
      &lt;item&gt;When our in-memory list gets too large, we'll write it to disk;&lt;/item&gt;
      &lt;item&gt;When we want to read a record, we'll read the in-memory list first, and then the disk if necessary.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Memory&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: Lorem ipsum010: Lorem ipsum&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On-Disk&lt;/p&gt;
    &lt;p&gt;The data structure used to store the in-memory list is usually one optimized for sorted data like a balanced binary search tree or more commonly, a skip list.&lt;/p&gt;
    &lt;p&gt;Of course, the main downside of having some of your data in-memory is that it's not persistent‚Äîif the program crashes or the computer shuts down, all of the data in the in-memory list is lost.&lt;/p&gt;
    &lt;p&gt;The fix here is thankfully pretty straightforward‚Äîevery time we add a record to the list, we also write it to an append-only file on disk. This way, we have a backup in case a crash does happen (which it most certainly will).&lt;/p&gt;
    &lt;p&gt;Memory&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: Lorem ipsum010: Lorem ipsum&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On-Disk Database&lt;/p&gt;
    &lt;p&gt;On-Disk Log&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: Lorem ipsum010:Lorem ipsum&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The append-only file doesn't need to be sorted nor does it need to have every record in the database; only the ones that are currently in memory.&lt;/p&gt;
    &lt;p&gt;With that, we have our very own key-value database! Let's recap how it works.&lt;/p&gt;
    &lt;p&gt;Our database starts out empty. When we go to add a new record, we'll add it to a sorted in-memory list, keeping a copy in an append-only file in case of crashes.&lt;/p&gt;
    &lt;p&gt;When the in-memory list gets too large, we'll flush the list by writing all of the records to a file in sorted order. In the process, we'll keep note of each record's offset in an index so we can efficiently look them up later.&lt;/p&gt;
    &lt;p&gt;When we want to look up a record, we'll first check the in-memory list. If the record isn't there, we'll check the index to see if it's in the on-disk file.&lt;/p&gt;
    &lt;p&gt;Once a file is saved to disk, it's considered immutable which means we can only ever read from the file and never update it. To work around this, we'll treat updates and deletes the same as inserting new records‚Äîadd them to the in-memory list.&lt;/p&gt;
    &lt;p&gt;Treating updates and deletes as new records means our file will only ever grow larger. To prevent this, we'll occassionally compact the on-disk files by deleting all duplicate records.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;001:&lt;/p&gt;
        &lt;p&gt;Lorem ipsum&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;018:&lt;/p&gt;
        &lt;p&gt;dolor sit&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;007:&lt;/p&gt;
        &lt;p&gt;adipiscing elit.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;007:&lt;/p&gt;
        &lt;p&gt;null&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;010:&lt;/p&gt;
        &lt;p&gt;consectetur adipiscing elit.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;001:&lt;/p&gt;
        &lt;p&gt;null&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;020:&lt;/p&gt;
        &lt;p&gt;Vestibulum varius&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Memory&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: Lorem ipsum010: Lorem ipsum&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On-Disk Log&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;010: Lorem ipsum010:Lorem ipsum&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;LSM Trees&lt;/head&gt;
    &lt;p&gt;What we just built actually exists in the real world‚Äîit's called an LSM or Log-Structured Merge Tree.&lt;/p&gt;
    &lt;p&gt;An LSM tree works by combining an in-memory list (often called a memtable) with an on-disk file (typically called a sorted string table or SST) to create a really fast key-value database.&lt;/p&gt;
    &lt;p&gt;LSM trees are the underlying data structure used for large-scale key-value databases like Google's LevelDB and Amazon's DynamoDB, and they have proven to perform really well at scale‚Äîon Prime Day 2020, DynamoDB peaked at 80 million requests per second!&lt;/p&gt;
    &lt;p&gt;Now, LSM trees aren't perfect, and they're certainly not the only way to structure a database. In fact, relational databases like PostgreSQL or MySQL use a completely different structure called a B-Tree to store their data‚Äîbut that's a deep dive for another post.&lt;/p&gt;
    &lt;p&gt;For now, thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nan.fyi/database"/><published>2025-10-21T16:31:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45658479</id><title>ChatGPT Atlas</title><updated>2025-10-22T08:46:43.278737+00:00</updated><content>&lt;doc fingerprint="d5c62af859f703f7"&gt;
  &lt;main&gt;
    &lt;p&gt;Bring ChatGPT with you across the web for instant answers, smarter suggestions, and help with tasks‚Äîall with privacy settings you can control.&lt;/p&gt;
    &lt;p&gt;Open a ChatGPT sidebar in any window to summarize content, compare products, or analyze data from any site you're viewing.&lt;/p&gt;
    &lt;p&gt;You can choose what ChatGPT remembers, so it can bring you relevant details when you need them.&lt;/p&gt;
    &lt;p&gt;In agent mode, ChatGPT interacts with sites for you, always under your control. Use it to do tasks from start to finish, like researching and shopping for a trip. Available in preview for Plus, Pro, and Business accounts.&lt;/p&gt;
    &lt;p&gt;Turn your cursor into a collaborator. Highlight text in emails, calendar invites, or docs, and get help from chat in one click.&lt;/p&gt;
    &lt;p&gt;You can decide which sites ChatGPT can see, clear your browsing history, use incognito, and manage browser memories anytime.&lt;/p&gt;
    &lt;p&gt;Get information the way you want to. Search text, images, videos, or news articles.&lt;/p&gt;
    &lt;p&gt;Use tabs, autocomplete, a search bar, and bookmarks to easily navigate the web.&lt;/p&gt;
    &lt;p&gt;It‚Äôs easy to set your browsing preferences and colors.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chatgpt.com/atlas"/><published>2025-10-21T17:18:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45659883</id><title>Researchers complete first human trial on viability of enteral ventilation</title><updated>2025-10-22T08:46:43.116121+00:00</updated><content>&lt;doc fingerprint="56cbb9932aa1044c"&gt;
  &lt;main&gt;
    &lt;p&gt;Scientists that won an infamous 2024 IgNobel Prize for "discovering that many mammals are capable of breathing through their anus" may indeed have the last laugh. They've now completed a successful human trial testing the safety and tolerability of enteral ventilation, a technique that gets oxygen into the body via an unconventional route.&lt;/p&gt;
    &lt;p&gt;Japanese and US researchers, led by the Cincinnati Children's Hospital, have completed the first-ever human trial testing the viability of enteral ventilation, where patients with severe respiratory failure could potentially have oxygen delivered through the intestine, allowing the lungs to recover and to prevent further injury. The procedure's safety and tolerability was examined on 27 healthy male adults in Japan, who had oxygen-rich fluid pumped into their anus.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis is the first human data, and the results are limited solely to demonstrating the safety of the procedure and not its effectiveness," said researcher Takanori Takebe, MD, PhD, from the Cincinnati Children‚Äôs and the University of Osaka. "But now that we have established tolerance, the next step will be to evaluate how effective the process is for delivering oxygen to the bloodstream."&lt;/p&gt;
    &lt;p&gt;The brave volunteers, aged 20-45 years, received a single intrarectal dose of non-oxygenated perfluorodecalin liquid (up to 1,500 ml), which they were required to retain for 60 minutes. Safety and tolerability were assessed by monitoring of adverse events, vital signs, clinical laboratory tests and systemic perfluorodecalin exposure. And a model using large-animal data was used to predict potential oxygen transfer. Perfluorodecalin was used due to its excellent oxygen-carrying abilities.&lt;/p&gt;
    &lt;p&gt;Twenty of the volunteers held the liquid for 60 minutes, and at the largest volume of 1,500 ml, there were only mild side effects of abdominal bloating and discomfort. Meanwhile, all clinical laboratory measures, including liver and renal function markers, remained within normal range.&lt;/p&gt;
    &lt;p&gt;"This first-in-human study demonstrates that intrarectal administration of non-oxygenated perfluorodecalin is safe, feasible, and well tolerated," noted the researchers. "These findings establish a critical safety foundation and support the continued development of enteral ventilation with fully oxygenated perfluorodecalin as an adjunctive strategy to support respiratory failure patients."&lt;/p&gt;
    &lt;p&gt;This research first came about after scientists had studied a type of bottom-feeding fish that swallows air from the surface of the water and absorbs oxygen through its gut, supplementing gill function in poor quality conditions. If humans were able to safely absorb super-oxygenated liquid through their colon, into their bloodstream, it has the potential to provide life-saving emergency treatment to people with blocked airways caused by injury or inflammation, or when lung function has been severely compromised.&lt;/p&gt;
    &lt;p&gt;The scientists now plan to repeat the trial to measure how much of the liquid is needed, and for how long, to improve blood-oxygen levels.&lt;/p&gt;
    &lt;p&gt;The research was published in the Cell Press journal Med.&lt;/p&gt;
    &lt;p&gt;Source: Cincinnati Children‚Äôs Hospital&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://newatlas.com/disease/butt-breathing-ignobel-prize/"/><published>2025-10-21T18:44:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45661253</id><title>Replacing a $3000/mo Heroku bill with a $55/mo server</title><updated>2025-10-22T08:46:42.890462+00:00</updated><content>&lt;doc fingerprint="7fca1100340b6f36"&gt;
  &lt;main&gt;
    &lt;p&gt;This content has moved. If you are not redirected, please click here:&lt;/p&gt;
    &lt;p&gt;blog/how-idealistorg-replaced-a-3000mo-heroku-bill-with-a-55mo-server/&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://disco.cloud/blog/how-idealistorg-replaced-a-3000mo-heroku-bill-with-a-55-server/"/><published>2025-10-21T20:28:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45661306</id><title>Erowid - Documenting the Complex Relationship Between Humans and Psychoactives</title><updated>2025-10-22T08:46:42.338465+00:00</updated><content>&lt;doc fingerprint="f7e1913c10a396f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Gypsy Life of Robert Louis Stevenson&lt;/head&gt;
    &lt;p&gt;‚ÄîRobert Louis Stevenson, Travels with a Donkey in the C√©vennes&lt;/p&gt;
    &lt;p&gt;With time people become the stories we tell about them. Then, except in the rarest cases, oblivion. Time whittled Sappho down to fragments. Others have not survived at all. If we take the cosmic view, no one will.&lt;lb/&gt; So what is biography for? What good is its effort to demystify the secretive recesses of human character? The biographer Hermione Lee has said, ‚ÄúI want to penetrate those secret places, find out everything, and be completely ruthless. It‚Äôs paradoxical‚ÄîI wouldn‚Äôt want it done to me, yet I‚Äôm very keen to do it to other people. And the thing that attracts me to these people is their secret self.‚Äù The biographer‚Äôs interest is the same as the novelist‚Äôs. Both are storytellers. Both deal in facts. And as we know, sometimes facts are make-believe. Sometimes fiction is truer than nonfiction. Novels used to be called histories or lives. Stories are everywhere.&lt;lb/&gt; Another expert biographer, Richard Holmes, has said, ‚ÄúYou spend a lot of time alone with your subject, but in the end you must go out and engage your readers. Readers must be able to imagine this other life as vividly as possible and understand it as personally as possible. I think it has to be an affectionate understanding, too. At least, I find that. I‚Äôve never written about someone I didn‚Äôt learn to like, and even to love. I call it ‚Äòa handshake across time.‚Äô‚Äù&lt;lb/&gt; The best biographers make happy readers of us, involving us‚Äîlike novelists, historians, playwrights and poets‚Äîin lives other than our own. They enlarge experience. Leo Damrosch‚Äôs new biography of Robert Louis Stevenson, Storyteller, is a splendid affair because Damrosch writes out of love, not uncritically, and not only for his subject, but also for the grand literary conversation of which he is a part.[1] Author of highly regarded books on Rousseau, Casanova, Jonathan Swift, and the circle of Samuel Johnson, Damrosch is the kind of scholar who knows a character when he sees one. He gets stories and the connections they afford us. Stevenson proves a perfect subject for him‚Äîeccentric, vital, adventurous, and, with good reason, beloved. Storyteller is thoroughly researched and copiously illustrated, but more than that, it is deeply moving in its shape and detail. It is not only about one marvelous man, whom many of us wish we could have known, but also about a great marriage, illuminating friendships and the freedom-seeking literary life.&lt;/p&gt;
    &lt;p&gt;*&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; I‚Äôll begin at the end.&lt;lb/&gt; In some ways, Stevenson resembles D. H. Lawrence‚Äîa rebel with a conservative streak, a world traveler, an invalid (possibly consumptive), who married a strong older woman and died, like Lawrence, at 44. He wrote with what Italo Calvino called a ‚Äúmarvelous lightness‚Äù in multiple genres‚Äîseveral wonderful novels and stories, dozens of superb essays, and letters that, in unexpurgated form, deserve comparison to Byron‚Äôs for their vigorous wit. While not among our greatest poets, Stevenson wrote some indelible verses. His ‚ÄúRequiem,‚Äù to be found on his tomb in Samoa as well as in books, is one of the best epitaphs we have in English. He actually wrote it during a serious illness in San Francisco, but friends and family made proper use of it after his death:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Under the wide and starry sky,&lt;/p&gt;&lt;lb/&gt;Dig the grave and let me lie.&lt;lb/&gt;Glad did I live and gladly die,&lt;lb/&gt;And I laid me down with a will.&lt;lb/&gt;This be the verse you grave for me:&lt;lb/&gt;Here he lies where he longed to be;&lt;lb/&gt;Home is the sailor, home from the sea,&lt;lb/&gt;And the hunter home from the hill.&lt;/quote&gt;
    &lt;p&gt;Never mind the use Philip Larkin made of one of its lines for ‚ÄúThis Be the Verse.‚Äù Damrosch‚Äôs placement of ‚ÄúRequiem‚Äù in his penultimate chapter is narrative perfection. Maybe it‚Äôs the natural shape of Stevenson‚Äôs life, or maybe it‚Äôs Damrosch‚Äôs enthusiasm for his subject, but this is a dream biography.&lt;lb/&gt; Many pages of Storyteller take up Stevenson‚Äôs frailty, his scrawniness, bouts of lung trouble and travels in search of a cure. He and his wife, Fanny, built a sprawling mansion in Samoa, which they called Vailima. There he enjoyed a few final years of robust health. It wasn‚Äôt TB that killed him, but a stroke‚Äîhe had always loved tobacco and whisky. Damrosch quotes an account written by Stevenson‚Äôs stepdaughter, Belle:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It seems he had been looking on watching my mother making a salad, and was dropping the oil for her with a perfectly steady hand. He suddenly said, ‚ÄúWhat is that?‚Äù or ‚ÄúWhat a pain!‚Äù and put both hands to his head. ‚ÄúDo I look strange?‚Äù he asked, and then he reeled and fell backwards. His favourite boy Sosimo [a Samoan servant] caught him and carried him into the big room, and he never was conscious after.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;‚ÄúThey had been in Samoa for just over four years,‚Äù Damrosch remarks, ‚Äú‚Äîthe longest Louis ever lived in one place during his entire adult life.‚Äù&lt;lb/&gt; Samoan people genuinely honored Stevenson, calling him Tusitala, or ‚Äústoryteller.‚Äù During a period of civil strife on the island, a taboo had been declared against doing any harm to Vailima and its inhabitants, even though Louis (as he was familiarly called) sided with one chieftain over the others. Having been a socialist when young‚Äîabout the time he renounced the Scottish Calvinism he was raised with‚Äîhe had become a skeptic about political causes. Samoa changed that by showing him the dark side of Western imperialism, and in A Footnote to History (1892) he wrote passionately for the cause of independence. The chief Stevenson had favored, Mata‚Äôafa, lost in the civil war and was banished (at least he wasn‚Äôt beheaded like some of his cohort). Another chief stood at Stevenson‚Äôs funeral to deliver a eulogy: ‚ÄúWhen Mata‚Äôafa was taken, who was our support but Tusitala? We were in prison, and he cared for us. We were sick, and he made us well. We were hungry, and he fed us. The day was no longer than his kindness. You are great people and full of love. Yet who among you is so great as Tusitala? What is your love to his love?‚Äù&lt;lb/&gt; In a few photographs we can see the burning energy and spirit in Stevenson‚Äôs eyes. His conversation won him fervent friends, some of whom fell out with him over time. None of the friends in Damrosch‚Äôs account moves me more than Henry James, who loved Louis and Fanny Stevenson from the moment he met them in London. When other friends complained about Stevenson leaving home and setting sail for the Pacific islands, James defended him, though he missed Louis terribly. James‚Äôs letters to Fanny after Louis‚Äôs death move me right down to the ground:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To ‚Äúmy dear Fanny Stevenson‚Äù James sent a long and tender letter of sympathy, beginning with ‚ÄúWhat can I say to you that will not seem cruelly irrelevant and vain?‚Äù After praising her ‚Äúcourage and patience and fortitude,‚Äù which would be much needed, he went on to say, ‚ÄúTo have lived in the light of that splendid life, that beautiful, bountiful being‚Äîonly to see it, from one moment to the other, converted into a fable as strange and romantic as one of his own, a thing that has been and has ended, is an anguish into which no one can enter with you fully, and of which no one can drain the cup for you. You are nearest to the pain, because you were nearest the joy and the pride.‚Äù&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;James went on, and I simply must quote it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;He has gone in time not to be old‚Äîearly enough to have been so generously young and late enough to have drunk deep of the cup. There have been‚ÄîI think‚Äîfor men of letters few deaths more romantically right. Forgive me, I beg you, what may sound cold-blooded in such words‚Äîand as if I imagined there could be anything for you ‚Äúright‚Äù in the rupture of such an affection and the loss of such a presence. I have in my mind, in that view, only the rounded career and the consecrated work. When I think of your own situation I fall into a mere confusion of pity and wonder‚Äîwith the sole sense of your being as brave a spirit as he was (all of whose bravery you endlessly shared) to hold on by.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You may wonder that I have begun my review with this postmortem. I dwell on it not only because it touched me more than I have been touched by a biography in many years, but because Damrosch earns my tears. I should turn next to two crucial aspects of Stevenson‚Äôs life: the quality of his writing and the extraordinary woman he married, who made him a better writer and gave him his happiest years.&lt;/p&gt;
    &lt;p&gt;*&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; My ancestors were Scottish, and Robert Louis Stevenson was an honored name in our household. I remember our illustrated edition of A Child‚Äôs Garden of Verses (1885), of which Henry James had written, ‚ÄúA child might have written it if a child could see childhood from the outside.‚Äù But in poems like ‚ÄúRain‚Äù and ‚ÄúMy Shadow,‚Äù Stevenson writes from the inside of childhood, at his best when he avoids moralizing. Damrosch quotes the famous couplet called ‚ÄúHappy Thought,‚Äù which a lesser critic would find sentimental: ‚ÄúThe world is so full of a number of things, / I‚Äôm sure we should all be as happy as kings.‚Äù Damrosch sees that the poem is not sentimental at all: ‚ÄúAstonished by the richness of the world, a child is amazed that people are not happy.‚Äù In our time, astonishment and awe are among the things people who think themselves sophisticated will sneer at. They are idiots, already pickled in their opinions. Damrosch understands his subject‚Äôs generous sympathy, the way he captures ‚Äúwhat it‚Äôs like to be small.‚Äù&lt;lb/&gt; I have never forgotten ‚ÄúThe Land of Counterpane,‚Äù even though I had no idea what a counterpane was, and of course ‚ÄúThe Land of Nod‚Äù:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;From breakfast on through all the day&lt;/p&gt;&lt;lb/&gt;At home among my friends I stay;&lt;lb/&gt;But every night I go abroad&lt;lb/&gt;Afar into the land of Nod.&lt;lb/&gt;All by myself I have to go,&lt;lb/&gt;With none to tell me what to do‚Äî&lt;lb/&gt;All alone beside the streams&lt;lb/&gt;And up the mountain-sides of dreams.&lt;lb/&gt;The strangest things are there for me,&lt;lb/&gt;Both things to eat and things to see,&lt;lb/&gt;And many frightening sights abroad&lt;lb/&gt;Till morning in the land of Nod.&lt;lb/&gt;Try as I like to find the way,&lt;lb/&gt;I never can get back by day,&lt;lb/&gt;Nor can remember plain and clear&lt;lb/&gt;The curious music that I hear.&lt;/quote&gt;
    &lt;p&gt;In the same way, I remember my early childhood encounter with a dramatic recording of Treasure Island (1883), in which the deaths of Billy Bones and Old Pew scared the bejesus out of me. I found stalwart adulthood in Squire Trelawney and Dr. Livesey but, like everyone else, was most beguiled by Long John Silver and his parrot, Captain Flint. Any boy of my generation would have wanted to be Jim Hawkins. In a late poem, ‚ÄúIn the Attic,‚Äù Seamus Heaney seems to have felt the same:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;As I age and blank on names,&lt;/p&gt;&lt;lb/&gt;As my uncertainty on stairs&lt;lb/&gt;Is more and more the lightheadedness&lt;lb/&gt;Of a cabin boy‚Äôs first time on the rigging,&lt;lb/&gt;As the memorable bottoms out&lt;lb/&gt;Into the irretrievable,&lt;lb/&gt;It‚Äôs not that I can‚Äôt imagine still&lt;lb/&gt;That slight untoward rupture and world-tilt&lt;lb/&gt;As a wind freshened and the anchor weighed.&lt;/quote&gt;
    &lt;p&gt;Just weeks ago, I re-read Treasure Island in a cheap paperback with a cheesy cartoon for a cover and was transported once again by its perfect plotting and the lucid exactitude of its prose: an island snake hisses ‚Äúwith a noise not unlike the spinning of a top,‚Äù and tropical foliage has ‚Äúa kind of poisonous brightness.‚Äù Call it a children‚Äôs book or a young adult novel or what you will, it‚Äôs bloody great‚Äîand pretty bloody, too. When Stevenson met Mark Twain in New York, the two must have wondered at having found their biggest success writing for young people, but both excelled at conveying the violence that makes our moral choices important and difficult. They understood what it means to grow.&lt;lb/&gt; Treasure Island and Kidnapped (1886) are probably two of the most influential novels on my own life. I‚Äôve never read Catriona (1893), the sequel to Kidnapped, which Damrosch praises on its literary merits, nor have I read The Master of Ballantrae (1889) or the important and unfinished final novel, Weir of Hermiston (1896). Based on Damrosch‚Äôs critical overviews, I should make time for them.&lt;lb/&gt; The adventure story and the historical romance were two genres at which Stevenson excelled, but he was also brilliant at the macabre psychological parable in his novella The Strange Case of Dr. Jekyll and Mr. Hyde (1886), and the supernatural in his short story ‚ÄúThrawn Janet‚Äù (1881). The first of these takes on the very ‚Äúfortress of identity‚Äù (in Jekyll‚Äôs words) that has so obsessed us of late but turns it into something timeless. Damrosch tells us that the novella caused a furious argument between Stevenson and his wife, in which she comes off better than he does. When Louis read aloud his first draft, as Fanny‚Äôs son Lloyd recalled, ‚ÄúHer praise was constrained; the words seemed to come with difficulty; and then all at once she broke out with criticism. He had missed the point, she said; had missed the allegory; had made it merely a story‚Äîa magnificent bit of sensationalism‚Äîwhen it should have been a masterpiece.‚Äù Damrosch continues, ‚ÄúFanny‚Äôs point was that Louis had ruined the story by turning it into a mere tale about a secret life. . . . What was needed was not just a character wearing a disguise, but something far more profound: a character struggling with a deeper hidden self that breaks loose and fights for supremacy.‚Äù Louis resisted, then came around, went back to work, and gave her the masterpiece she wanted. Thereafter, he jokingly referred to her as ‚Äúthe critic on the hearth.‚Äù&lt;lb/&gt; A friend reminds me of novels like The Wrong Box (1889), one of several Stevenson wrote in collaboration with his stepson, Lloyd. I remember an enjoyable movie but haven‚Äôt read the book. It‚Äôs the short stories I‚Äôve dwelt in lately, including ‚ÄúThe Body Snatcher‚Äù and ‚ÄúA Lodging for the Night,‚Äù a brilliant tale of the medieval French poet Fran√ßois Villon. Stevenson never idealizes his protagonist: ‚ÄúThe poet was a rag of a man, dark, little, and lean, with hollow cheeks and thin black locks. He carried his four-and-twenty years with feverish animation. Greed had made folds about his eyes, evil smiles had puckered his mouth. The wolf and pig struggled together in his face. It was an eloquent, sharp, ugly, earthy countenance.‚Äù The style is kinetic, giving us action rather than explanation. You can learn from him. It was not until late in Stevenson‚Äôs short life that he felt free to write about women and sexual attraction, but within the Victorian mores of the time, he produced work charged with dark human energy, wildness and suspense that made so many of his tales excellent fodder for the movies.&lt;lb/&gt; Like Lawrence, he was also a pioneering travel writer, starting with his early books about France, An Inland Voyage (1878) and Travels with a Donkey in the C√©vennes (1879). The second of these is a small classic, an inspiration to nearly every travel writer since. It also inspired Richard Holmes, who undertook to follow the same journey as a way of empathizing with Stevenson the Francophile and wrote about it years later in Footsteps: Adventures of a Romantic Biographer (1985). In his Paris Review interview, Holmes remembers ‚Äúworking out that for his Travels, Stevenson had this wonderful smell of wet Scottish tweed, French tobacco, and warm brandy. Because it always rained up there, he always rolled his own cigarettes, and he always carried a large flask of Cognac in his pocket. And then of course there was the smell of his donkey, Modestine.‚Äù Holmes practiced a peripatetic method of which Stevenson would have approved.&lt;/p&gt;
    &lt;p&gt;*&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Fanny Stevenson was the kind of woman most people found fascinating‚Äîbeautiful and brainy and brave. You can test your feelings about people in this biography by how they respond to her. Some, including Alice James, found her skin too dusky and her manners vulgar‚Äîlike Louis, she rolled her own cigarettes. Down with Alice, says I. The writer Edmund Gosse described Fanny as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;She was one of the strangest people who have lived in our time, a sort of savage nature in some ways, but very lovable‚Äîextraordinarily passionate, and unlike everyone else in her violent feelings and unrestrained ways of expressing them‚Äîfull of gaiety, and with a genius for expressing things picturesquely. . . . I think R.L.S. must have caught some of his ways of feeling from her.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Gosse would, according to Damrosch, come ‚Äúto resent what he saw as Fanny‚Äôs power over Louis, but still he admired her.‚Äù&lt;lb/&gt; Fanny was ten years older than Louis. They became lovers long before she secured a divorce from her philandering husband, Sam Osbourne. Louis‚Äôs sexual experiences had begun early in his life as an Edinburgh student, so by the time he met Fanny in Bohemian circles in France, they were both pretty well liberated from social mores, interested in living as artistic gypsies. Her daughter, Belle, would write that ‚ÄúLouis brought into our lives a sort of joyousness hard to describe.‚Äù For his part, Stevenson fell fiercely in love with Fanny‚Äôs cool-headed humor. Belle again: ‚ÄúFanny Osbourne‚Äôs voice was low in tone, and she spoke with very little modulation. Louis described it as sounding like ‚Äòwater running under ice.‚Äô‚Äù&lt;lb/&gt; Fanny hesitated, returning to her husband more than once out of love for him and concern for their children, one of whom had died young. Finally, Sam‚Äôs infidelity became too much, and Louis‚Äôs persistence won through. The couple married in San Francisco in 1880. He was thirty, she forty. Louis had been so ill with lung disease that neither believed he would live much longer. Yet Belle later recalled passing in a hallway outside their bedroom ‚Äúand stopping suddenly at a light joyous sound. With a catch at my heart, I realized it was the first time I had ever heard my mother laugh.‚Äù Damrosch concludes,&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;More than any of Louis‚Äôs biographers, Richard Holmes does justice to this remarkable union. ‚ÄúWhen one considers other Victorian literary marriages‚ÄîHardy‚Äôs, say, or Dickens‚Äôs‚ÄîStevenson‚Äôs is something phenomenal, dynamic, explosive. It contained energies, tempests, fireworks, and sheer anarchic excitement that would have obliterated any conventional household. To find anything like his relationship with Fanny‚Äîand the comparison is significant in the largest way‚Äîone would have to look forward to Lawrence and Frieda.‚Äù&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There is a kind of person who despises enthusiasm, or fears it, and cannot abide happiness in others. I have seen this. The same sort of person looks at a Cassandra and does not believe a word she says. Fanny was a Cassandra figure, partly in her bouts of gloominess compared to Louis‚Äôs general ‚Äúoptimism.‚Äù Stevenson‚Äôs parents had been worried about their son‚Äôs liaison with an older woman, yet they fell in love with her when they met her, which speaks well of them. Louis‚Äôs father, Thomas, a wealthy engineer whose own father had devised the famous Stevenson lighthouses, nicknamed his daughter-in-law Cassandra, only partly in jest. Margaret, Louis‚Äôs mother, was a more joyful figure, and proved to be resilient after her husband‚Äôs death, sailing with Louis‚Äôs gypsy family to the South Seas. Louis had grown up with money, abandoned engineering for law, then abandoned law for literature, but he had always been able to depend on an allowance to keep him afloat. Fanny‚Äôs life had been more precarious, her risk-taking more remarkable.&lt;lb/&gt; The couple moved constantly in search of a healthy environment for Louis. For some productive years they lived at Skerryvore, their house in Bournemouth. The climate was disastrous for Louis, but he made some of his strongest friendships there, especially with Henry James. Damrosch quotes a fascinating description of Fanny by one of their visitors, who observed Stevenson ‚Äúrolling a limp cigarette in his long, limp fingers, and talking eagerly all the while.‚Äù Then:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Opposite him sits Mrs. Stevenson, the tutelary genius of Skerryvore, a woman of small physical stature, but surely of heroic mould. Her features are clear-cut and delicate, but marked by unmistakable strength of character; her hair of an unglossy black, and her complexion darker than one would expect in a woman of Dutch-American race. I have heard her speak of a Moorish strain in her ancestry. . . . Beneath a placid though always alert and vivacious exterior, Mrs. Stevenson conceals much personal suffering and continual anxieties under which many a stronger woman might well break down.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&lt;lb/&gt; Fanny managed their lives, found them homes to live in, and when they contemplated their South Sea voyages, it was she who sought out, rented and stocked the three successive boats they sailed on. In one of the stranger images of Fanny, she can be seen on the deck of a sailboat holding two pistols, shooting at sharks. It‚Äôs a glimpse of unexpected cruelty, rather like the young Louis mercilessly beating Modestine on his travels through the C√©vennes.&lt;lb/&gt; They endured rough seas, making a bold menagerie together with Margaret, Lloyd and Belle. As Louis wrote to one acquaintance, ‚ÄúSix months on tinned meats, without any vegetables, is a thing to be remembered; most hardships become easy with continuance, not hardship of diet; towards the end I think we could all have wept at sight of an onion.‚Äù&lt;lb/&gt; In Samoa, while Louis‚Äôs health improved, Fanny did ultimately have a complete nervous collapse, perhaps due to the accumulated pressures of their life. She recovered and endured Louis‚Äôs death and sold Vailima, eventually finding another younger lover and dying in Santa Barbara, California, in 1914. Belle took her ashes to be buried with Louis. The unconventionality continued, as Fanny‚Äôs younger lover became Belle‚Äôs second husband. Lloyd kept writing books with little success and died in Glendale, California, in 1947. Belle died in Santa Barbara in 1953.&lt;lb/&gt; As it turned out, the storyteller, Tusitala, had been the most remarkable figure in their lives. ‚ÄúHe was indeed all his life a bag of bones,‚Äù wrote his friend Sidney Colvin, who would later publish a censored edition of his letters. ‚ÄúNevertheless when he was in the room it was the other people, and not he, who seemed the shadows.‚Äù For Richard Holmes, ‚ÄúHe was the man who opened the magic door. His wit, his style, his courage, his wanderlust, all enchanted me. . . . He made England seem small, and the world look big.‚Äù Jorges Luis Borges would write, ‚ÄúEver since childhood Stevenson has been for me one of the forms of happiness.‚Äù&lt;lb/&gt; Leo Damrosch set forth in this book ‚Äúto celebrate what is great in his writing, and to inspire new readers to enjoy it.‚Äù I would say ‚ÄúMission accomplished,‚Äù but Stevenson would deplore the clich√©.&lt;/p&gt;
    &lt;p&gt;[1] STORYTELLER: The Life of Robert Louis Stevenson, by Leo Damrosch. Yale University Press. $35.00.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.erowid.org"/><published>2025-10-21T20:31:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45661462</id><title>The Gypsy Life of Robert Louis Stevenson</title><updated>2025-10-22T08:46:41.869634+00:00</updated><content>&lt;doc fingerprint="f7e1913c10a396f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Gypsy Life of Robert Louis Stevenson&lt;/head&gt;
    &lt;p&gt;‚ÄîRobert Louis Stevenson, Travels with a Donkey in the C√©vennes&lt;/p&gt;
    &lt;p&gt;With time people become the stories we tell about them. Then, except in the rarest cases, oblivion. Time whittled Sappho down to fragments. Others have not survived at all. If we take the cosmic view, no one will.&lt;lb/&gt; So what is biography for? What good is its effort to demystify the secretive recesses of human character? The biographer Hermione Lee has said, ‚ÄúI want to penetrate those secret places, find out everything, and be completely ruthless. It‚Äôs paradoxical‚ÄîI wouldn‚Äôt want it done to me, yet I‚Äôm very keen to do it to other people. And the thing that attracts me to these people is their secret self.‚Äù The biographer‚Äôs interest is the same as the novelist‚Äôs. Both are storytellers. Both deal in facts. And as we know, sometimes facts are make-believe. Sometimes fiction is truer than nonfiction. Novels used to be called histories or lives. Stories are everywhere.&lt;lb/&gt; Another expert biographer, Richard Holmes, has said, ‚ÄúYou spend a lot of time alone with your subject, but in the end you must go out and engage your readers. Readers must be able to imagine this other life as vividly as possible and understand it as personally as possible. I think it has to be an affectionate understanding, too. At least, I find that. I‚Äôve never written about someone I didn‚Äôt learn to like, and even to love. I call it ‚Äòa handshake across time.‚Äô‚Äù&lt;lb/&gt; The best biographers make happy readers of us, involving us‚Äîlike novelists, historians, playwrights and poets‚Äîin lives other than our own. They enlarge experience. Leo Damrosch‚Äôs new biography of Robert Louis Stevenson, Storyteller, is a splendid affair because Damrosch writes out of love, not uncritically, and not only for his subject, but also for the grand literary conversation of which he is a part.[1] Author of highly regarded books on Rousseau, Casanova, Jonathan Swift, and the circle of Samuel Johnson, Damrosch is the kind of scholar who knows a character when he sees one. He gets stories and the connections they afford us. Stevenson proves a perfect subject for him‚Äîeccentric, vital, adventurous, and, with good reason, beloved. Storyteller is thoroughly researched and copiously illustrated, but more than that, it is deeply moving in its shape and detail. It is not only about one marvelous man, whom many of us wish we could have known, but also about a great marriage, illuminating friendships and the freedom-seeking literary life.&lt;/p&gt;
    &lt;p&gt;*&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; I‚Äôll begin at the end.&lt;lb/&gt; In some ways, Stevenson resembles D. H. Lawrence‚Äîa rebel with a conservative streak, a world traveler, an invalid (possibly consumptive), who married a strong older woman and died, like Lawrence, at 44. He wrote with what Italo Calvino called a ‚Äúmarvelous lightness‚Äù in multiple genres‚Äîseveral wonderful novels and stories, dozens of superb essays, and letters that, in unexpurgated form, deserve comparison to Byron‚Äôs for their vigorous wit. While not among our greatest poets, Stevenson wrote some indelible verses. His ‚ÄúRequiem,‚Äù to be found on his tomb in Samoa as well as in books, is one of the best epitaphs we have in English. He actually wrote it during a serious illness in San Francisco, but friends and family made proper use of it after his death:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Under the wide and starry sky,&lt;/p&gt;&lt;lb/&gt;Dig the grave and let me lie.&lt;lb/&gt;Glad did I live and gladly die,&lt;lb/&gt;And I laid me down with a will.&lt;lb/&gt;This be the verse you grave for me:&lt;lb/&gt;Here he lies where he longed to be;&lt;lb/&gt;Home is the sailor, home from the sea,&lt;lb/&gt;And the hunter home from the hill.&lt;/quote&gt;
    &lt;p&gt;Never mind the use Philip Larkin made of one of its lines for ‚ÄúThis Be the Verse.‚Äù Damrosch‚Äôs placement of ‚ÄúRequiem‚Äù in his penultimate chapter is narrative perfection. Maybe it‚Äôs the natural shape of Stevenson‚Äôs life, or maybe it‚Äôs Damrosch‚Äôs enthusiasm for his subject, but this is a dream biography.&lt;lb/&gt; Many pages of Storyteller take up Stevenson‚Äôs frailty, his scrawniness, bouts of lung trouble and travels in search of a cure. He and his wife, Fanny, built a sprawling mansion in Samoa, which they called Vailima. There he enjoyed a few final years of robust health. It wasn‚Äôt TB that killed him, but a stroke‚Äîhe had always loved tobacco and whisky. Damrosch quotes an account written by Stevenson‚Äôs stepdaughter, Belle:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It seems he had been looking on watching my mother making a salad, and was dropping the oil for her with a perfectly steady hand. He suddenly said, ‚ÄúWhat is that?‚Äù or ‚ÄúWhat a pain!‚Äù and put both hands to his head. ‚ÄúDo I look strange?‚Äù he asked, and then he reeled and fell backwards. His favourite boy Sosimo [a Samoan servant] caught him and carried him into the big room, and he never was conscious after.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;‚ÄúThey had been in Samoa for just over four years,‚Äù Damrosch remarks, ‚Äú‚Äîthe longest Louis ever lived in one place during his entire adult life.‚Äù&lt;lb/&gt; Samoan people genuinely honored Stevenson, calling him Tusitala, or ‚Äústoryteller.‚Äù During a period of civil strife on the island, a taboo had been declared against doing any harm to Vailima and its inhabitants, even though Louis (as he was familiarly called) sided with one chieftain over the others. Having been a socialist when young‚Äîabout the time he renounced the Scottish Calvinism he was raised with‚Äîhe had become a skeptic about political causes. Samoa changed that by showing him the dark side of Western imperialism, and in A Footnote to History (1892) he wrote passionately for the cause of independence. The chief Stevenson had favored, Mata‚Äôafa, lost in the civil war and was banished (at least he wasn‚Äôt beheaded like some of his cohort). Another chief stood at Stevenson‚Äôs funeral to deliver a eulogy: ‚ÄúWhen Mata‚Äôafa was taken, who was our support but Tusitala? We were in prison, and he cared for us. We were sick, and he made us well. We were hungry, and he fed us. The day was no longer than his kindness. You are great people and full of love. Yet who among you is so great as Tusitala? What is your love to his love?‚Äù&lt;lb/&gt; In a few photographs we can see the burning energy and spirit in Stevenson‚Äôs eyes. His conversation won him fervent friends, some of whom fell out with him over time. None of the friends in Damrosch‚Äôs account moves me more than Henry James, who loved Louis and Fanny Stevenson from the moment he met them in London. When other friends complained about Stevenson leaving home and setting sail for the Pacific islands, James defended him, though he missed Louis terribly. James‚Äôs letters to Fanny after Louis‚Äôs death move me right down to the ground:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To ‚Äúmy dear Fanny Stevenson‚Äù James sent a long and tender letter of sympathy, beginning with ‚ÄúWhat can I say to you that will not seem cruelly irrelevant and vain?‚Äù After praising her ‚Äúcourage and patience and fortitude,‚Äù which would be much needed, he went on to say, ‚ÄúTo have lived in the light of that splendid life, that beautiful, bountiful being‚Äîonly to see it, from one moment to the other, converted into a fable as strange and romantic as one of his own, a thing that has been and has ended, is an anguish into which no one can enter with you fully, and of which no one can drain the cup for you. You are nearest to the pain, because you were nearest the joy and the pride.‚Äù&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;James went on, and I simply must quote it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;He has gone in time not to be old‚Äîearly enough to have been so generously young and late enough to have drunk deep of the cup. There have been‚ÄîI think‚Äîfor men of letters few deaths more romantically right. Forgive me, I beg you, what may sound cold-blooded in such words‚Äîand as if I imagined there could be anything for you ‚Äúright‚Äù in the rupture of such an affection and the loss of such a presence. I have in my mind, in that view, only the rounded career and the consecrated work. When I think of your own situation I fall into a mere confusion of pity and wonder‚Äîwith the sole sense of your being as brave a spirit as he was (all of whose bravery you endlessly shared) to hold on by.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You may wonder that I have begun my review with this postmortem. I dwell on it not only because it touched me more than I have been touched by a biography in many years, but because Damrosch earns my tears. I should turn next to two crucial aspects of Stevenson‚Äôs life: the quality of his writing and the extraordinary woman he married, who made him a better writer and gave him his happiest years.&lt;/p&gt;
    &lt;p&gt;*&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; My ancestors were Scottish, and Robert Louis Stevenson was an honored name in our household. I remember our illustrated edition of A Child‚Äôs Garden of Verses (1885), of which Henry James had written, ‚ÄúA child might have written it if a child could see childhood from the outside.‚Äù But in poems like ‚ÄúRain‚Äù and ‚ÄúMy Shadow,‚Äù Stevenson writes from the inside of childhood, at his best when he avoids moralizing. Damrosch quotes the famous couplet called ‚ÄúHappy Thought,‚Äù which a lesser critic would find sentimental: ‚ÄúThe world is so full of a number of things, / I‚Äôm sure we should all be as happy as kings.‚Äù Damrosch sees that the poem is not sentimental at all: ‚ÄúAstonished by the richness of the world, a child is amazed that people are not happy.‚Äù In our time, astonishment and awe are among the things people who think themselves sophisticated will sneer at. They are idiots, already pickled in their opinions. Damrosch understands his subject‚Äôs generous sympathy, the way he captures ‚Äúwhat it‚Äôs like to be small.‚Äù&lt;lb/&gt; I have never forgotten ‚ÄúThe Land of Counterpane,‚Äù even though I had no idea what a counterpane was, and of course ‚ÄúThe Land of Nod‚Äù:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;From breakfast on through all the day&lt;/p&gt;&lt;lb/&gt;At home among my friends I stay;&lt;lb/&gt;But every night I go abroad&lt;lb/&gt;Afar into the land of Nod.&lt;lb/&gt;All by myself I have to go,&lt;lb/&gt;With none to tell me what to do‚Äî&lt;lb/&gt;All alone beside the streams&lt;lb/&gt;And up the mountain-sides of dreams.&lt;lb/&gt;The strangest things are there for me,&lt;lb/&gt;Both things to eat and things to see,&lt;lb/&gt;And many frightening sights abroad&lt;lb/&gt;Till morning in the land of Nod.&lt;lb/&gt;Try as I like to find the way,&lt;lb/&gt;I never can get back by day,&lt;lb/&gt;Nor can remember plain and clear&lt;lb/&gt;The curious music that I hear.&lt;/quote&gt;
    &lt;p&gt;In the same way, I remember my early childhood encounter with a dramatic recording of Treasure Island (1883), in which the deaths of Billy Bones and Old Pew scared the bejesus out of me. I found stalwart adulthood in Squire Trelawney and Dr. Livesey but, like everyone else, was most beguiled by Long John Silver and his parrot, Captain Flint. Any boy of my generation would have wanted to be Jim Hawkins. In a late poem, ‚ÄúIn the Attic,‚Äù Seamus Heaney seems to have felt the same:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;As I age and blank on names,&lt;/p&gt;&lt;lb/&gt;As my uncertainty on stairs&lt;lb/&gt;Is more and more the lightheadedness&lt;lb/&gt;Of a cabin boy‚Äôs first time on the rigging,&lt;lb/&gt;As the memorable bottoms out&lt;lb/&gt;Into the irretrievable,&lt;lb/&gt;It‚Äôs not that I can‚Äôt imagine still&lt;lb/&gt;That slight untoward rupture and world-tilt&lt;lb/&gt;As a wind freshened and the anchor weighed.&lt;/quote&gt;
    &lt;p&gt;Just weeks ago, I re-read Treasure Island in a cheap paperback with a cheesy cartoon for a cover and was transported once again by its perfect plotting and the lucid exactitude of its prose: an island snake hisses ‚Äúwith a noise not unlike the spinning of a top,‚Äù and tropical foliage has ‚Äúa kind of poisonous brightness.‚Äù Call it a children‚Äôs book or a young adult novel or what you will, it‚Äôs bloody great‚Äîand pretty bloody, too. When Stevenson met Mark Twain in New York, the two must have wondered at having found their biggest success writing for young people, but both excelled at conveying the violence that makes our moral choices important and difficult. They understood what it means to grow.&lt;lb/&gt; Treasure Island and Kidnapped (1886) are probably two of the most influential novels on my own life. I‚Äôve never read Catriona (1893), the sequel to Kidnapped, which Damrosch praises on its literary merits, nor have I read The Master of Ballantrae (1889) or the important and unfinished final novel, Weir of Hermiston (1896). Based on Damrosch‚Äôs critical overviews, I should make time for them.&lt;lb/&gt; The adventure story and the historical romance were two genres at which Stevenson excelled, but he was also brilliant at the macabre psychological parable in his novella The Strange Case of Dr. Jekyll and Mr. Hyde (1886), and the supernatural in his short story ‚ÄúThrawn Janet‚Äù (1881). The first of these takes on the very ‚Äúfortress of identity‚Äù (in Jekyll‚Äôs words) that has so obsessed us of late but turns it into something timeless. Damrosch tells us that the novella caused a furious argument between Stevenson and his wife, in which she comes off better than he does. When Louis read aloud his first draft, as Fanny‚Äôs son Lloyd recalled, ‚ÄúHer praise was constrained; the words seemed to come with difficulty; and then all at once she broke out with criticism. He had missed the point, she said; had missed the allegory; had made it merely a story‚Äîa magnificent bit of sensationalism‚Äîwhen it should have been a masterpiece.‚Äù Damrosch continues, ‚ÄúFanny‚Äôs point was that Louis had ruined the story by turning it into a mere tale about a secret life. . . . What was needed was not just a character wearing a disguise, but something far more profound: a character struggling with a deeper hidden self that breaks loose and fights for supremacy.‚Äù Louis resisted, then came around, went back to work, and gave her the masterpiece she wanted. Thereafter, he jokingly referred to her as ‚Äúthe critic on the hearth.‚Äù&lt;lb/&gt; A friend reminds me of novels like The Wrong Box (1889), one of several Stevenson wrote in collaboration with his stepson, Lloyd. I remember an enjoyable movie but haven‚Äôt read the book. It‚Äôs the short stories I‚Äôve dwelt in lately, including ‚ÄúThe Body Snatcher‚Äù and ‚ÄúA Lodging for the Night,‚Äù a brilliant tale of the medieval French poet Fran√ßois Villon. Stevenson never idealizes his protagonist: ‚ÄúThe poet was a rag of a man, dark, little, and lean, with hollow cheeks and thin black locks. He carried his four-and-twenty years with feverish animation. Greed had made folds about his eyes, evil smiles had puckered his mouth. The wolf and pig struggled together in his face. It was an eloquent, sharp, ugly, earthy countenance.‚Äù The style is kinetic, giving us action rather than explanation. You can learn from him. It was not until late in Stevenson‚Äôs short life that he felt free to write about women and sexual attraction, but within the Victorian mores of the time, he produced work charged with dark human energy, wildness and suspense that made so many of his tales excellent fodder for the movies.&lt;lb/&gt; Like Lawrence, he was also a pioneering travel writer, starting with his early books about France, An Inland Voyage (1878) and Travels with a Donkey in the C√©vennes (1879). The second of these is a small classic, an inspiration to nearly every travel writer since. It also inspired Richard Holmes, who undertook to follow the same journey as a way of empathizing with Stevenson the Francophile and wrote about it years later in Footsteps: Adventures of a Romantic Biographer (1985). In his Paris Review interview, Holmes remembers ‚Äúworking out that for his Travels, Stevenson had this wonderful smell of wet Scottish tweed, French tobacco, and warm brandy. Because it always rained up there, he always rolled his own cigarettes, and he always carried a large flask of Cognac in his pocket. And then of course there was the smell of his donkey, Modestine.‚Äù Holmes practiced a peripatetic method of which Stevenson would have approved.&lt;/p&gt;
    &lt;p&gt;*&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Fanny Stevenson was the kind of woman most people found fascinating‚Äîbeautiful and brainy and brave. You can test your feelings about people in this biography by how they respond to her. Some, including Alice James, found her skin too dusky and her manners vulgar‚Äîlike Louis, she rolled her own cigarettes. Down with Alice, says I. The writer Edmund Gosse described Fanny as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;She was one of the strangest people who have lived in our time, a sort of savage nature in some ways, but very lovable‚Äîextraordinarily passionate, and unlike everyone else in her violent feelings and unrestrained ways of expressing them‚Äîfull of gaiety, and with a genius for expressing things picturesquely. . . . I think R.L.S. must have caught some of his ways of feeling from her.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Gosse would, according to Damrosch, come ‚Äúto resent what he saw as Fanny‚Äôs power over Louis, but still he admired her.‚Äù&lt;lb/&gt; Fanny was ten years older than Louis. They became lovers long before she secured a divorce from her philandering husband, Sam Osbourne. Louis‚Äôs sexual experiences had begun early in his life as an Edinburgh student, so by the time he met Fanny in Bohemian circles in France, they were both pretty well liberated from social mores, interested in living as artistic gypsies. Her daughter, Belle, would write that ‚ÄúLouis brought into our lives a sort of joyousness hard to describe.‚Äù For his part, Stevenson fell fiercely in love with Fanny‚Äôs cool-headed humor. Belle again: ‚ÄúFanny Osbourne‚Äôs voice was low in tone, and she spoke with very little modulation. Louis described it as sounding like ‚Äòwater running under ice.‚Äô‚Äù&lt;lb/&gt; Fanny hesitated, returning to her husband more than once out of love for him and concern for their children, one of whom had died young. Finally, Sam‚Äôs infidelity became too much, and Louis‚Äôs persistence won through. The couple married in San Francisco in 1880. He was thirty, she forty. Louis had been so ill with lung disease that neither believed he would live much longer. Yet Belle later recalled passing in a hallway outside their bedroom ‚Äúand stopping suddenly at a light joyous sound. With a catch at my heart, I realized it was the first time I had ever heard my mother laugh.‚Äù Damrosch concludes,&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;More than any of Louis‚Äôs biographers, Richard Holmes does justice to this remarkable union. ‚ÄúWhen one considers other Victorian literary marriages‚ÄîHardy‚Äôs, say, or Dickens‚Äôs‚ÄîStevenson‚Äôs is something phenomenal, dynamic, explosive. It contained energies, tempests, fireworks, and sheer anarchic excitement that would have obliterated any conventional household. To find anything like his relationship with Fanny‚Äîand the comparison is significant in the largest way‚Äîone would have to look forward to Lawrence and Frieda.‚Äù&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There is a kind of person who despises enthusiasm, or fears it, and cannot abide happiness in others. I have seen this. The same sort of person looks at a Cassandra and does not believe a word she says. Fanny was a Cassandra figure, partly in her bouts of gloominess compared to Louis‚Äôs general ‚Äúoptimism.‚Äù Stevenson‚Äôs parents had been worried about their son‚Äôs liaison with an older woman, yet they fell in love with her when they met her, which speaks well of them. Louis‚Äôs father, Thomas, a wealthy engineer whose own father had devised the famous Stevenson lighthouses, nicknamed his daughter-in-law Cassandra, only partly in jest. Margaret, Louis‚Äôs mother, was a more joyful figure, and proved to be resilient after her husband‚Äôs death, sailing with Louis‚Äôs gypsy family to the South Seas. Louis had grown up with money, abandoned engineering for law, then abandoned law for literature, but he had always been able to depend on an allowance to keep him afloat. Fanny‚Äôs life had been more precarious, her risk-taking more remarkable.&lt;lb/&gt; The couple moved constantly in search of a healthy environment for Louis. For some productive years they lived at Skerryvore, their house in Bournemouth. The climate was disastrous for Louis, but he made some of his strongest friendships there, especially with Henry James. Damrosch quotes a fascinating description of Fanny by one of their visitors, who observed Stevenson ‚Äúrolling a limp cigarette in his long, limp fingers, and talking eagerly all the while.‚Äù Then:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Opposite him sits Mrs. Stevenson, the tutelary genius of Skerryvore, a woman of small physical stature, but surely of heroic mould. Her features are clear-cut and delicate, but marked by unmistakable strength of character; her hair of an unglossy black, and her complexion darker than one would expect in a woman of Dutch-American race. I have heard her speak of a Moorish strain in her ancestry. . . . Beneath a placid though always alert and vivacious exterior, Mrs. Stevenson conceals much personal suffering and continual anxieties under which many a stronger woman might well break down.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&lt;lb/&gt; Fanny managed their lives, found them homes to live in, and when they contemplated their South Sea voyages, it was she who sought out, rented and stocked the three successive boats they sailed on. In one of the stranger images of Fanny, she can be seen on the deck of a sailboat holding two pistols, shooting at sharks. It‚Äôs a glimpse of unexpected cruelty, rather like the young Louis mercilessly beating Modestine on his travels through the C√©vennes.&lt;lb/&gt; They endured rough seas, making a bold menagerie together with Margaret, Lloyd and Belle. As Louis wrote to one acquaintance, ‚ÄúSix months on tinned meats, without any vegetables, is a thing to be remembered; most hardships become easy with continuance, not hardship of diet; towards the end I think we could all have wept at sight of an onion.‚Äù&lt;lb/&gt; In Samoa, while Louis‚Äôs health improved, Fanny did ultimately have a complete nervous collapse, perhaps due to the accumulated pressures of their life. She recovered and endured Louis‚Äôs death and sold Vailima, eventually finding another younger lover and dying in Santa Barbara, California, in 1914. Belle took her ashes to be buried with Louis. The unconventionality continued, as Fanny‚Äôs younger lover became Belle‚Äôs second husband. Lloyd kept writing books with little success and died in Glendale, California, in 1947. Belle died in Santa Barbara in 1953.&lt;lb/&gt; As it turned out, the storyteller, Tusitala, had been the most remarkable figure in their lives. ‚ÄúHe was indeed all his life a bag of bones,‚Äù wrote his friend Sidney Colvin, who would later publish a censored edition of his letters. ‚ÄúNevertheless when he was in the room it was the other people, and not he, who seemed the shadows.‚Äù For Richard Holmes, ‚ÄúHe was the man who opened the magic door. His wit, his style, his courage, his wanderlust, all enchanted me. . . . He made England seem small, and the world look big.‚Äù Jorges Luis Borges would write, ‚ÄúEver since childhood Stevenson has been for me one of the forms of happiness.‚Äù&lt;lb/&gt; Leo Damrosch set forth in this book ‚Äúto celebrate what is great in his writing, and to inspire new readers to enjoy it.‚Äù I would say ‚ÄúMission accomplished,‚Äù but Stevenson would deplore the clich√©.&lt;/p&gt;
    &lt;p&gt;[1] STORYTELLER: The Life of Robert Louis Stevenson, by Leo Damrosch. Yale University Press. $35.00.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hudsonreview.com/2025/10/the-gypsy-life-of-robert-louis-stevenson/"/><published>2025-10-21T20:44:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45661638</id><title>rlsw ‚Äì Raylib software OpenGL renderer in less than 5k LOC</title><updated>2025-10-22T08:46:41.831985+00:00</updated><content/><link href="https://github.com/raysan5/raylib/blob/master/src/external/rlsw.h"/><published>2025-10-21T21:00:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45662668</id><title>The Hidden Engineering of Niagara Falls</title><updated>2025-10-22T08:46:41.397082+00:00</updated><content>&lt;doc fingerprint="b453c84da27086a9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Hidden Engineering of Niagara Falls&lt;/head&gt;
    &lt;p&gt;[Note that this article is a transcript of the video embedded above.]&lt;/p&gt;
    &lt;p&gt;Niagara Falls is one of the most spectacular waterfalls in the world. With a vertical drop of more than 50 meters or 164 feet and a flow rate that often exceeds 2800 cubic meters per second or 100,000 cubic feet per second, it‚Äôs one of North America‚Äôs crown jewels. Roughly ten million people visit the falls every year just to catch a glimpse of the curtains of water pouring over the edge and the constant clouds of mist at the bottom. But Niagara Falls isn‚Äôt just a tourist attraction. The special geology and hydrology of this region, situated between Lake Erie and Lake Ontario, have resulted in some fascinating feats of infrastructure, from shipping to electricity to water control. It‚Äôs basically a microcosm of all the things I love. The falls themselves have required quite a bit of engineering over the years, and they‚Äôve even been shut off for maintenance. Let‚Äôs take a little tour of the Niagara Peninsula (even though it‚Äôs really an isthmus), and I‚Äôll show you some of the things that aren‚Äôt usually listed in a guidebook. I‚Äôm Grady, and this is Practical Engineering.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs get oriented first. This is a map of the isthmus. We‚Äôve got Lake Erie to the south, Lake Ontario to the north, Buffalo and western New York to the East, and Ontario, Canada, to the west. The Niagara River runs northward, connecting the two great lakes. And right in the middle, it plunges off the Niagara Escarpment, creating the famous falls. On the US side, there are the American Falls and the smaller Bridal Veil Falls. And on the Canadian side is the Horseshoe Falls where a majority of the river flows. It‚Äôs pretty impressive to see in person, but it‚Äôs actually not entirely a benefit. Because these falls pose a major problem for shipping.&lt;/p&gt;
    &lt;p&gt;The Great Lakes form the largest inland freshwater transportation system in the world. Since the 19th century, they‚Äôve served as the backbone for moving iron ore, coal, grain, and manufactured goods between the American heartland and the Atlantic Ocean. Ore from Minnesota and grain from the Midwest can travel by ship all the way to steel mills or export terminals on the East Coast. Barges and freighters are efficient at moving bulk cargo in a way rail and trucks can‚Äôt match. For a time, the Niagara Escarpment was a natural bottleneck between Lake Erie and Lake Ontario, preventing goods from moving directly between the upper lakes and the Atlantic. Freight had to be offloaded and portaged around the falls before it could continue its journey. The Erie Canal solved the problem somewhat, starting in 1825, bypassing Lake Ontario. But it could only accommodate smaller vessels, and even before the Canal opened, another solution was being planned.&lt;/p&gt;
    &lt;p&gt;The Welland Canal runs through the peninsula west of the Niagara River, connecting two massive areas by shipping traffic for the first time in 1829. The canal fueled the early growth of cities along the Great Lakes and St. Lawrence River - including Cleveland, Detroit, Milwaukee, Chicago, Toronto, Montreal, and Quebec City - and it‚Äôs been rebuilt and moved several times over its life. The Welland Canal is really a titanic engineering achievement and, were it not positioned next to one of the natural wonders of the world, it would probably be famous in its own right. Because of the huge difference in elevation between the two lakes created by the escarpment, eight separate locks are required to allow ships to traverse between them. And all different kinds do - from personal leisure craft to the lakers that stay in fresh water to the salties that travel between the lakes and the ocean through the St. Lawrence Seaway.&lt;/p&gt;
    &lt;p&gt;Starting on the upstream, Lake Erie side of the canal, the first lock isn‚Äôt really for lifting or lowering ships so much as for control. The level of Lake Erie actually fluctuates throughout the year, and there are longer-term trends as well. Wind storms also raise the level locally similar to the way storm surge works during hurricanes. The control lock does just that: it controls the level in the downstream canal. It prevents excess water from rushing down the canal when the lake is high, kind of like an airlock on a spaceship keeps air from rushing out when astronauts step outside for a spacewalk.&lt;/p&gt;
    &lt;p&gt;Downstream of the control lock, the canal splits in two. The original pathway of the canal flows through the eponymous town of Welland, while the larger and newer section of canal, the Welland Bypass‚Ä¶ well, it bypasses Welland to the east. If you look carefully, you‚Äôll also notice a small river, the Welland River, which passes underneath both the original and bypass canals. On the way downstream from Lake Erie to Lake Ontario, shipping traffic passes over aqueducts that pass over a natural river. A hydrological wonderland!&lt;/p&gt;
    &lt;p&gt;Continuing downstream from the aqueducts, the remaining seven locks are lift locks, more like what you think of when you imagine a lock. Notice how they‚Äôre clustered tightly around the terrain and not distributed evenly along the length of the canal. That‚Äôs the Niagara escarpment, the same geological feature that the water cascades down at the falls. This is the elevation diagram of the entire Great Lakes and St. Lawrence Seaway system from Lake Superior to the Atlantic Ocean, and you can see that this drop is the biggest one of the whole thing. And that‚Äôs pretty important for another part of the infrastructure on the peninsula.&lt;/p&gt;
    &lt;p&gt;The power available from a moving fluid is directly proportional to the flow rate multiplied by the height of the drop. In most hydropower applications, that height is created artificially by a dam. There aren‚Äôt that many places in the world where you have both a large volume of flowing water and a significant natural drop in elevation. But that combination made Niagara Falls the birthplace of large-scale electric power in North America. In 1895, the Niagara Power Company opened the Edward Dean Adams Power Plant, built with Westinghouse AC generators based on the ideas and patents of Nikola Tesla. The plant served as the basis for the modern electrical grids we have today, and many of the fundamental concepts are basically unchanged.&lt;/p&gt;
    &lt;p&gt;But the power infrastructure at Niagara Falls definitely has changed. Where the Adams Power Plant put out about 40 megawatts of power in 1895, now the combined capacity from the region is in the neighborhood of 5 gigawatts. But in both cases, it wasn‚Äôt as simple as putting a turbine at the base of the falls. While it might be technically possible to generate power by placing a water wheel directly in the stream of a waterfall like a kid‚Äôs bath toy, it‚Äôs not the most efficient way (plus it would take away from the beauty). The water used to power the hydroelectric plants on both the US and Canadian sides of the Niagara River is water that never actually flows over the falls. Instead, it‚Äôs diverted into five massive tunnels - two on the US side and three on the Canadian side.&lt;/p&gt;
    &lt;p&gt;Like most tunnels, you can‚Äôt really see the extent of the hydro tunnels at Niagara Falls. There are a few conspicuous clues though, like these gigantic buildings. These interesting protrusions from the landscape house enormous steel doors, nearly 60 feet tall, that can drop down into the tunnels and close off the flow for inspections and maintenance. Both the Ontario and New York sides of the river feature similar structures.&lt;/p&gt;
    &lt;p&gt;From the tunnels, water flows into major hydropower plants on both sides of the border: the twin Adam Beck stations on the Canadian side and Robert Moses station on the US side. Then it‚Äôs released into the the lower part of the river below the falls. When you add them up, that‚Äôs 39 turbines with a combined capacity of more than 4000 megawatts. It‚Äôs a tremendous amount of power generation in one place. But actually, that‚Äôs not all of it.&lt;/p&gt;
    &lt;p&gt;These tunnels divert 50-75% of the flow of the Niagara River. That wide range in percentage of diversion isn‚Äôt because we don‚Äôt know how much is diverted, but because we actually control how much water is diverted, depending on the tourist requirements agreed upon in a treaty by both nations. During the day in peak tourist season, more water is allowed to flow over the falls to ensure the grandeur of the falls is on full display for the huge crowds of tourists that visit every year. At night and during the winter, more of the flow is diverted to generate power. That‚Äôs all managed by this structure upstream of the falls: the international control dam.&lt;/p&gt;
    &lt;p&gt;I‚Äôve always thought this is an interesting dam, since it doesn‚Äôt even go all the way across the river. But it doesn‚Äôt need to. This structure‚Äôs not meant to create a reservoir; it just subtly adjusts the level in the river to control how much water flows over the falls versus into the hydropower intakes. The US side of the Niagara River is pretty shallow, so that side acts kind of like an uncontrolled spillway. Then, the gates on the Canadian side can be adjusted to balance the competing demands on water between tourism and power.&lt;/p&gt;
    &lt;p&gt;But there‚Äôs one big problem with those competing needs: they both have the same timing. We want thunderous cascades of water over the falls during the day when tourists are visiting, but daytime is also when the demand for electricity is highest. It‚Äôs like if solar panels only worked at night. To accommodate this, both the US and Canada have pumped storage plants. At night, excess electricity is used to pump diverted water into reservoirs, essentially storing both the power and the extra water that‚Äôs available during off-peak hours. Then, during the day, the water is released back into the forebay of the power plants. You get a little extra power from that drop out of the reservoir into the forebays, so both sides have small hydropower facilities to capture that. But more importantly, you get a lot more water during the day than would otherwise be available to run through the big plants, making more power when it‚Äôs needed most. And there‚Äôs just something funny to me that the infrastructure is duplicated on both sides of the river, like neither country was willing to be one-upped by the other.&lt;/p&gt;
    &lt;p&gt;All of this diversion noticeably reduces the flow of water over the falls. Even when they are at ‚Äòfull blast‚Äô during the day in the tourist season, only 50% of the flow of the Niagara River makes it over the falls. You can imagine how powerful the falls would be if 100% of the flow were to cascade over. It might seem like this diversion detracts from the majesty of the falls, but in another sense, it actually preserves it.&lt;/p&gt;
    &lt;p&gt;All waterfalls undergo some degree of erosion as the water and sediment suspended in it scours away the rocks and soil underneath. Without any diversion, Niagara Falls would be receding towards Lake Erie at a rate of about 3 feet every year. At the end of the last ice age, the falls were right at the edge of the Niagara Escarpment, but thousands of years of erosion have caused them to work their way upstream. You can actually see how far it‚Äôs already progressed by looking at this elevation map. Over the last 12,000 years or so, the falls have migrated by erosion to their current location. By diverting a significant portion of the flow, the power plants have actually slowed the rate of erosion to approximately one foot per year, which will help preserve the falls for a longer period.&lt;/p&gt;
    &lt;p&gt;While flow on the falls is downregulated by diversion for hydropower, the falls are never ‚Äòturned off‚Äô...except for the one time in the 1960s. The smaller American Falls (and nearby Bridal Veil Falls) have a pile of loose rocks and boulders, called talus, at their base. This pile of rocky debris actually extends a good fraction of the way up the falls, and officials worried that the falls might ultimately transition into a series of rapids cascading down the slope of talus rather than remaining a majestic waterfall. So, in 1969, the Army Corps of Engineers built a temporary cofferdam between the New York shoreline and Goat Island, diverting the water over the Canadian Horseshoe Falls and leaving the American Falls dry(ish)!&lt;/p&gt;
    &lt;p&gt;After the engineers got a chance to inspect the situation, they determined that the best course of action was just to leave the majority of the talus in place, since it seemed to be stabilizing the cliff face. Sometimes, doing mostly nothing is a decision you make as an engineer, even if you have to do a monumental amount of work to come to that conclusion. So the cofferdam was taken out, and water has flowed continuously over all the falls since then.&lt;/p&gt;
    &lt;p&gt;It really highlights the complexity of Niagara Falls. On the one hand, you have one of the natural wonders of the world, an absolutely enormous set of waterfalls that inspire awe and wonder in the countless travelers who are lucky enough to take in the view. The same thing that makes it impressive for tourists (the big drop) makes it valuable for power and a major challenge for shipping. And out of that comes all kinds of fascinating infrastructure, not only to facilitate the tourism but the other stuff too: a major canal with locks and aqueducts, the international dam control gates, pumped storage reservoirs, epic tunnels, towering gates, massive hydropower plants, and so much more. It‚Äôs really a pretty remarkable place for engineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://practical.engineering/blog/2025/10/21/the-hidden-engineering-of-niagara-falls"/><published>2025-10-21T22:43:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45664147</id><title>OpenBSD 7.8</title><updated>2025-10-22T08:46:40.895093+00:00</updated><link href="https://cdn.openbsd.org/pub/OpenBSD/7.8/ANNOUNCEMENT"/><published>2025-10-22T02:02:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45664848</id><title>Evaluating the Infinity Cache in AMD Strix Halo</title><updated>2025-10-22T08:46:40.729707+00:00</updated><content>&lt;doc fingerprint="2fdaf83057b028f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Evaluating the Infinity Cache in AMD Strix Halo&lt;/head&gt;
    &lt;p&gt;Strix Halo is the codename for AMD‚Äôs highest end mobile chip, which is used in the Ryzen AI MAX series. It combines a powerful CPU with 16 Zen 5 cores and a large GPU with 20 RDNA 3.5 Workgroup Processors (WGPs). The sizeable iGPU makes Strix Halo particularly interesting because GPUs have high bandwidth requirements. Strix Halo tackles that with a 256-bit LPDDR5X-8000 setup combined with 32 MB of memory side cache. The latter is often referred to as Infinity Cache, or MALL (Memory Attached Last Level). I‚Äôll refer to it as Infinity Cache for brevity.&lt;/p&gt;
    &lt;p&gt;Infinity Cache has been around since RDNA2 in AMD‚Äôs discrete consumer GPU lineup, where it helped AMD hit high performance targets with lower DRAM bandwidth requirements. However, Infinity Cache‚Äôs efficacy has so far been difficult for me to evaluate. AMD‚Äôs discrete GPUs have performance monitoring facilities accessible through AMD‚Äôs developer tools. But those tools stop providing information past L2. Strix Halo stands out because it has an Infinity Cache implementation, and all the accessible performance monitoring features typical of a recent AMD GPU. That includes programmable performance counters at Infinity Fabric and memory controllers. It‚Äôs an opportunity to finally get insight into how well AMD‚Äôs Infinity Cache does its job in various graphics workloads.&lt;/p&gt;
    &lt;head rend="h1"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Special thanks goes out to ASUS for sampling their ROG Flow Z13. This device implements AMD‚Äôs Ryzen AI MAX+ 395 with 32 GB of LPDDR5X in a thin and light form factor. It superficially represents a convertible tablet from Microsoft‚Äôs Surface line, and is remarkably portable for a device with gaming credentials. Without ASUS‚Äôs help, this article wouldn‚Äôt have been possible.&lt;/p&gt;
    &lt;head rend="h1"&gt;Infinity Fabric, Performance Monitoring, and Theory&lt;/head&gt;
    &lt;p&gt;AMD‚Äôs Infinity Fabric aims to abstract away details of how data travels across the chip. It does so by providing endpoints with well defined interfaces to let blocks make or handle memory requests. Infinity Fabric also provides a set of programmable performance counters. AMD documents a single DATA_BW performance event that counts data beats at endpoints. DATA_BW targets an endpoint via its 8-bit instance ID, and can count either reads or writes. AMD never documented Infinity Fabric instance IDs for Strix Halo. So, I did some guessing by generating traffic at various blocks and observing bandwidth counts at all possible instance IDs.&lt;/p&gt;
    &lt;p&gt;Instance IDs start from the Coherent Stations (CS-es), just like on server platforms. CS blocks sit in front of memory controllers and ensure cache coherency by probing another block if it might have a modified copy of the requested cacheline. If it doesn‚Äôt, which is most of the time, the CS will pass the request on to its attached Unified Memory Controller (UMC). Because CS blocks observe all requests to physical memory backed by DRAM, it‚Äôs a logical place to implement a memory side cache. That‚Äôs exactly what AMD does on chips with Infinity Cache. Cache hits let the CS avoid going to the UMC.&lt;/p&gt;
    &lt;p&gt;Strix Halo has 16 memory controllers and CS instances, each handling a 16-bit LPDDR5X channel. The GPU occupies the next eight instance IDs, suggesting it has a wide interface to Infinity Fabric. CPU core clusters come next. Each octa-core Zen 5 Core Complex (CCX) connects to Infinity Fabric via one endpoint. Miscellaneous blocks follow. These include the NPU, media engine, the display engine, and a mystery.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setting up Performance Monitoring&lt;/head&gt;
    &lt;p&gt;Cache misses usually cause a request to the next level in the memory hierarchy, while cache hits do not. Therefore, my idea is to compare traffic levels at the CS and UMC levels. Traffic that shows up at the CS but not at the UMCs can be used as a proxy for Infinity Cache hits. There are a few problems with that approach though.&lt;/p&gt;
    &lt;p&gt;First, Strix Halo only provides eight Infinity Fabric performance counters. I have to use two counters per endpoint to count both read and write data beats, so I can only monitor four CS-es simultaneously. Memory traffic is interleaved across channels, so taking bandwidth observed at four CS-es and multiplying by four should give a reasonably accurate estimate of overall bandwidth. But interleaving isn‚Äôt perfectly even in practice, so I expect a few percentage points of error. The UMC situation is easier. Each UMC has its own set of four performance counters, letting me monitor all of them at the same time. I‚Äôm using one counter per UMC to count Column Address Strobe (CAS) commands. Other available UMC events allow monitoring memory controller frequency, bus utilization, and ACTIVATE or PRECHARGE commands. But that stuff goes beyond what I want to focus on here, and collecting more data points would make for annoyingly large spreadsheets.&lt;/p&gt;
    &lt;p&gt;Cross-CCX traffic presents a second source of error as mentioned above. If a CS sends a probe that hits a modified line, it won‚Äôt send a request to the UMC. The request is still being satisfied out of cache, just not the Infinity Cache that I‚Äôm interested in. I expect this to be rare because cross-CCX traffic in general is usually low compared to requests satisfied from DRAM. It‚Äôll be especially rare in the graphics workloads I‚Äôm targeting here because Strix Halo‚Äôs second CCD tends to be parked in Windows.&lt;/p&gt;
    &lt;p&gt;CPU-side traffic in general presents a more significant issue. Strix Halo‚Äôs Infinity Cache narrowly targets the GPU, and only GPU-side memory requests cause cache fills. CPU memory accesses will be counted as misses in a hitrate calculation. That may be technically correct, but misses the point of Infinity Cache. Most memory traffic comes from the GPU, but there‚Äôs enough CPU-side traffic to create a bit more of an error bar than I‚Äôd like. Therefore, I want to focus on whether Infinity Cache is handling enough traffic to avoid hitting DRAM bandwidth bottlenecks.&lt;/p&gt;
    &lt;p&gt;A final limitation is that I‚Äôm sampling performance counters with a tool I wrote years ago. I wrote it so I could look at hardware performance stats just like how I like looking at Task Manager or other monitoring utilities to see how my system is doing. Because I‚Äôm updating a graphical interface, I sample data every second and thus have lower resolution compared to some vendor tools. Those can often sample at millisecond granularity or better. That opens the window to underestimating bandwidth demands if a bandwidth spike only occurs over a small fraction of a second. In theory I could write another tool. But Chips and Cheese is an unpaid hobby project that has to be balanced with a separate full time job as well as other free time interests. Quickly rigging an existing project to do my bidding makes more efficient use of time, because there‚Äôs never enough time to do everything I want.&lt;/p&gt;
    &lt;head rend="h1"&gt;Results&lt;/head&gt;
    &lt;p&gt;I logged data over some arbitrarily selected graphics workloads, then selected the 1-second interval with the highest DRAM bandwidth usage. That should give an idea of whether Strix Halo comes close to reaching DRAM bandwidth limits. Overall, the 32 MB cache seems to do its job. Strix Halo is able to stay well clear of the 256 GB/s theoretical bandwidth limit that its LPDDR5X-8000 setup can deliver.&lt;/p&gt;
    &lt;p&gt;Approximate CS-side bandwidth demands do indicate several workloads can push uncomfortably close to 256 GB/s. A workload doesn‚Äôt necessarily have to get right up to bandwidth limits for memory-related performance issues to start cropping up. Under high bandwidth demand, requests can start to pile up in various queues and end-to-end memory latency can increase. GHPC and Ungine Valley fall into that category. 3DMark Time Spy Extreme would definitely be DRAM bandwidth bound without the memory side cache.&lt;/p&gt;
    &lt;p&gt;Picking an interval with maximum bandwidth demands at the CS gives an idea of how much bandwidth Strix Halo‚Äôs GPU can demand. Strix Halo has the same 2 MB of graphics L2 cache that AMD‚Äôs older 7000 series ‚ÄúPhoenix‚Äù mobile chip had, despite more than doubling GPU compute throughput. Unsurprisingly, the GPU can draw a lot of bandwidth across Infinity Fabric. 3DMark Time Spy again stands out. If AMD wanted to simply scale up DRAM bandwidth without spending die area on cache, they would need well over 335 GB/s from DRAM.&lt;/p&gt;
    &lt;p&gt;Curiously, Digital Foundry notes that Strix Halo has very similar graphics performance to the PS5. The PS5 uses a 256-bit, 14 GT/s GDDR6 setup that‚Äôs good for 448 GB/s of theoretical bandwidth, and doesn‚Äôt have a memory side cache like Strix Halo. 448 GB/s looks just about adequate for satisfying Time Spy Extreme‚Äôs bandwidth needs, if just barely. If Strix Halo didn‚Äôt need to work in power constrained mobile devices, and didn‚Äôt need high memory capacity for multitasking, perhaps AMD could have considered a GDDR6 setup. To bring things back around to Infinity Cache, it seems to do very well at that interval above. It captures approximately 73% of memory traffic going through Infinity Fabric, which is good even from a hitrate point of view.&lt;/p&gt;
    &lt;p&gt;The two bar graphs above already hint at how bandwidth demands and cache hitrates vary across workloads. Those figures vary within a workload as well, though plotting all of the logged data would be excessive. Variation both across and within workloads makes it extremely difficult to summarize cache efficacy with a single hitrate figure. Plotting the percentage of traffic at the CS not observed at the UMCs as a proxy for hitrate further emphasizes that point.&lt;/p&gt;
    &lt;p&gt;Resolution settings can impact cache hitrate as well. While I don‚Äôt have a direct look at hitrate figures, plotting with the data I do have suggests increasing resolution in the Ungine Valley benchmark tends to depress hitrate.&lt;/p&gt;
    &lt;p&gt;AMD presented a slide at Hot Chips 2021 with three lines for different resolutions across a set of different cache capacities. I obviously can‚Äôt test different cache capacities. But AMD‚Äôs slide does beg the question of how well a 32 MB cache can do at a wider range of resolutions, and whether bandwidth demands remain under control at resolutions higher than what AMD may have optimized the platform for.&lt;/p&gt;
    &lt;p&gt;Ungine Superposition and 3DMark workloads can both target a variety of resolutions without regard to monitor resolution. Logging data throughout benchmark runs at different resolutions shows the 32 MB cache providing reasonable ‚Äúbandwidth amplification‚Äù at reasonable resolutions. At unreasonable resolutions, the cache is still able to do something. But it‚Äôs not as effective as it was at lower resolutions.&lt;/p&gt;
    &lt;p&gt;Plotting data over time shows spikes and dips in Infinity Cache efficacy occur at remarkably consistent times, even at different resolutions. 8K is an exception with the Superposition benchmark. The 8K line looks stretched out, possibly because Strix Halo‚Äôs GPU was only able to average a bit over 10 FPS, and time got slowed a bit.&lt;/p&gt;
    &lt;p&gt;If Strix Halo‚Äôs iGPU were capable of delivering over 30 FPS in Superposition, AMD would definitely need a larger cache, more DRAM bandwidth, or a combination of both. Taking a simplistic view and tripling maximum observed 8K bandwidth demands would give a figure just north of 525 GB/s. But Strix Halo‚Äôs iGPU clearly wasn‚Äôt built with such a scenario in mind, and AMD‚Äôs selected combination of cache capacity and DRAM bandwidth works well enough at all tested resolutions. While high resolutions create the most DRAM traffic, 1080P shows the heaviest bandwidth demands at the Infinity Fabric level.&lt;/p&gt;
    &lt;p&gt;3DMark Wild Life Extreme is a better example because it‚Äôs a lightweight benchmark designed to run on mobile devices. Strix Halo‚Äôs iGPU can average above 30 FPS even when rendering at 8K. Again, DRAM bandwidth demands increase and Infinity Cache becomes less effective as resolution goes up. But Infinity Cache still does enough to keep the chip well clear of DRAM bandwidth limits. Thus the cache does its job, and the bandwidth situation remains under control across a wide range of resolutions.&lt;/p&gt;
    &lt;p&gt;Bandwidth demands are more important than hitrates. Curiously though, Wild Life Extreme experiences increasingly severe hitrate dips at higher resolutions around the 16-19 and 45-52 second intervals. Those dips barely show at 1440P or below. Perhaps a 32 MB cache‚Äôs efficacy also shows more variation as resolution increases.&lt;/p&gt;
    &lt;head rend="h1"&gt;A Video?&lt;/head&gt;
    &lt;p&gt;A few errors here - I said Core Coherent Master read/write beats were 32B and 64B/cycle. It‚Äôs not per cycle, it‚Äôs per data beat. And I meant to say reads outnumber writes at the end.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;Chip designers have to tune their designs to perform well across a wide variety of workloads. Cache sizes are one parameter on the list. AMD chose to combine a 32 MB cache with 256 GB/s of DRAM bandwidth, and it seems to do well enough across the workloads I tested. Monitoring at the CS-es and UMCs also supports AMD‚Äôs data showing that higher resolutions tend to depress hitrate. Those results explain why larger GPUs tend to have larger caches and higher DRAM bandwidth.&lt;/p&gt;
    &lt;p&gt;At a higher level, GPU bandwidth demands have been a persistent challenge for large iGPUs and have driven a diverse set of solutions. Over a decade ago, Intel created ‚Äúhalo‚Äù iGPU variants and paired a 128 MB eDRAM cache while sticking with a typical 128-bit client memory bus. AMD‚Äôs console chips use large and fast DRAM buses. The PS5 is one example. Strix Halo does a bit of both. It combines modest cache capacity with a more DRAM bandwidth than typical client chips, but doesn‚Äôt rely as much on DRAM bandwidth as console chips.&lt;/p&gt;
    &lt;p&gt;Those varied approaches to the bandwidth problem are fascinating. Watching how Infinity Cache behaves in various graphics workloads has been fascinating as well. But everything would be so much more fun if AMD‚Äôs tools provided direct data on Infinity Cache hitrates. That applies to both integrated and discrete GPUs. Infinity Cache is a well established part of AMD‚Äôs strategy. It has been around for several generations, and now has a presence in AMD‚Äôs mobile lineup too. I suspect developers would love to see Infinity Cache hitrates in addition to the data on GPU-private caches that AMD‚Äôs current tools show.&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipsandcheese.com/p/evaluating-the-infinity-cache-in"/><published>2025-10-22T04:20:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45665311</id><title>French ex-president Sarkozy begins jail sentence</title><updated>2025-10-22T08:46:40.608908+00:00</updated><content>&lt;doc fingerprint="f625945411a553ed"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;French ex-president Sarkozy begins jail sentence for campaign finance conspiracy&lt;/head&gt;
    &lt;p&gt;Nicolas Sarkozy has become the first French ex-president to go to jail, as he starts a five-year sentence for conspiring to fund his election campaign with money from late Libyan dictator Muammar Gaddafi.&lt;/p&gt;
    &lt;p&gt;Not since World War Two Nazi collaborationist leader Philippe P√©tain was jailed for treason in 1945 has any French ex-leader gone behind bars.&lt;/p&gt;
    &lt;p&gt;Sarkozy, who was president from 2007-2012, has appealed against his jail term at La Sant√© prison, where he will occupy a small cell in the its isolation wing.&lt;/p&gt;
    &lt;p&gt;More than 100 people applauded and shouted "Nicolas!" as he left his villa in the exclusive 16th district of Paris, holding his wife Carla Bruni-Sarkozy by the hand.&lt;/p&gt;
    &lt;p&gt;His son Louis, 28, had appealed to supporters for a show of support, while another son, Pierre, called for a message of love ‚Äì "nothing else, please".&lt;/p&gt;
    &lt;p&gt;Nicolas Sarkozy, 70, was driven through the entrance of the notoriously overcrowded 19th-Century prison in the Montparnasse district south of the River Seine at 09:40 local time (07:40 GMT), while dozens of police officers cordoned off most of the surrounding streets.&lt;/p&gt;
    &lt;p&gt;He continues to protest his innocence in the highly controversial Libyan money affair and posted a message on X as he was driven to the jail, saying: "I have no doubt. Truth will prevail. But how crushing the price will have been."&lt;/p&gt;
    &lt;p&gt;"With unwavering strength I tell [the French people] it is not a former president they are locking up this morning - it is an innocent man," he wrote. "Do not feel sorry for me because my wife and my children are by my side... but this morning I feel deep sorrow for a France humiliated by a will for revenge."&lt;/p&gt;
    &lt;p&gt;Moments after Sarkozy entered jail, his lawyer Christophe Ingrain said a request for his release had been filed. Nothing justified his imprisonment, said Mr Ingrain, adding: "He'll be inside for at least three weeks or a month."&lt;/p&gt;
    &lt;p&gt;Sarkozy has said he wants no special treatment at La Sant√© prison, although he has been put in its isolation section for his own safety as other inmates are infamous drugs dealers or have been convicted for terror offences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Small cell with TV, and one hour's daily exercise&lt;/head&gt;
    &lt;p&gt;Sarkozy's cell in the prison's isolation wing is believed to be on the top floor and will measure between 9-11 sq m (95-120 sq ft). There had earlier been talk of him serving his term in another wing for "vulnerable people", where other VIPs have been jailed in the past.&lt;/p&gt;
    &lt;p&gt;He will have a toilet, a shower, a desk, a small electric hob and a small TV, for which he will have to pay a monthly ‚Ç¨14 (¬£12) fee, and the right to a small fridge.&lt;/p&gt;
    &lt;p&gt;The former president has the right to receive information from the outside world and family visits as well as written and phone contact.&lt;/p&gt;
    &lt;p&gt;But he is in effect in solitary confinement, allowed just one hour a day for exercise, by himself in the wing's segregated courtyard.&lt;/p&gt;
    &lt;p&gt;"Conditions of detention in an isolation wing are pretty hard," La Sant√© ex-deputy head Flavie Rault told BFMTV. "You are alone, all the time. The only contact you have is with prison staff. You never come across another detainee for security reasons and there's a type of social isolation which makes life difficult".&lt;/p&gt;
    &lt;p&gt;At the end of last week, Sarkozy was received at the √âlys√©e Palace by President Emmanuel Macron, who told reporters on Monday "it was normal that on a human level I should receive one of my predecessors in that context".&lt;/p&gt;
    &lt;p&gt;Macron stressed on Tuesday that it was not his role "to comment on or criticise judicial decisions", but he said it was normal that for many in France the sight of "a president jailed by this judicial decision would provoke comment".&lt;/p&gt;
    &lt;p&gt;In a further measure of official support for the ex-president, Justice Minister G√©rald Darmanin said he would go to visit him in prison as part of his role in ensuring Sarkozy's safety and the proper functioning of the jail.&lt;/p&gt;
    &lt;p&gt;"I cannot be insensitive to a man's distress," he added.&lt;/p&gt;
    &lt;p&gt;Ever since he left office in 2012, Sarkozy has been dogged by criminal inquiries and for months had to wear an electronic tag around his ankle after a conviction last December for trying to bribe a magistrate for confidential information about a separate case.&lt;/p&gt;
    &lt;p&gt;Late next month, France's highest administrative court will give its verdict on Sarkozy's appeal against a six-month jail term in another illegal campaign financing case known as the Bygmalion affair.&lt;/p&gt;
    &lt;p&gt;Ahead of his arrival at La Sant√© prison, Sarkozy gave a series of media interviews, telling La Tribune: "I'm not afraid of prison. I'll keep my head held high, including at the prison gates."&lt;/p&gt;
    &lt;p&gt;Sarkozy has always denied doing anything wrong in a case involving allegations that his 2007 presidential campaign was funded by millions of euros in Libyan cash.&lt;/p&gt;
    &lt;p&gt;The former centre-right leader was cleared of personally receiving the money but convicted of criminal association with two close aides, Brice Hortefeux and Claude Gu√©ant, for their role in secret campaign financing from the Libyans.&lt;/p&gt;
    &lt;p&gt;The two men both had talks with Gaddafi's intelligence chief and brother-in-law in 2005, a meeting arranged by a Franco-Lebanese intermediary called Ziad Tiakeddine, who died in Lebanon shortly before Sarkozy's conviction.&lt;/p&gt;
    &lt;p&gt;As he lodged an appeal, Sarkozy is still considered innocent but he has been told he must go to jail in view of the "exceptional seriousness of the facts".&lt;/p&gt;
    &lt;p&gt;Sarkozy said he would take two books with him into prison, a life of Jesus by Jean-Christian Petitfils and the Count of Monte Cristo, Alexandre Dumas's classic story of a man wrongly imprisoned who escapes to wreak vengeance on his prosecutors.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/cvgkm2j0xelo"/><published>2025-10-22T05:49:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45665796</id><title>Greenland Ditches Starlink for French Satellite Service</title><updated>2025-10-22T08:46:40.296702+00:00</updated><content>&lt;doc fingerprint="2ae0311bb1e4fbc9"&gt;
  &lt;main&gt;
    &lt;p&gt;Greenland Strengthens Internet Links Without U.S. Help&lt;/p&gt;
    &lt;head rend="h2"&gt;Others are reading now&lt;/head&gt;
    &lt;p&gt;In today‚Äôs world, internet access is essential for education, health, and safety.&lt;/p&gt;
    &lt;p&gt;Yet, many people in East and North Greenland still struggle with slow or unreliable connections. That is about to change.&lt;/p&gt;
    &lt;head rend="h2"&gt;Faster and More Stable&lt;/head&gt;
    &lt;p&gt;Greenland‚Äôs national telecom company, Tusass, has signed a new agreement with the French company Eutelsat to provide satellite internet through OneWeb, reports Danish News Media DR.&lt;/p&gt;
    &lt;p&gt;These satellites will bring faster and more stable coverage to towns and settlements that do not have underwater cables or radio links.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing the Gap&lt;/head&gt;
    &lt;p&gt;Tusass director Toke Binzer said the new service will improve life in remote areas such as Tasiilaq, Ittoqqortoormiit, and Qaanaaq.&lt;/p&gt;
    &lt;head rend="h2"&gt;Also read&lt;/head&gt;
    &lt;p&gt;He explained that customers in these regions have waited too long for reliable service. The goal is to close that gap and bring them online at a much higher standard.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Improvement&lt;/head&gt;
    &lt;p&gt;Eutelsat is a major player in global satellite communications. Its OneWeb satellites orbit closer to Earth than traditional ones, which allows for faster and more reliable signals.&lt;/p&gt;
    &lt;p&gt;The company says the partnership will help connect isolated communities, support critical infrastructure, and improve maritime safety and rescue operations.&lt;/p&gt;
    &lt;p&gt;The first area to benefit will be Tasiilaq, which is expected to gain access before the end of the year. Other communities will follow as Tusass expands the network.&lt;/p&gt;
    &lt;head rend="h2"&gt;Starlink Was Considered&lt;/head&gt;
    &lt;p&gt;Tusass had also been in talks with Starlink, owned by Elon Musk, but chose to continue working with Eutelsat.&lt;/p&gt;
    &lt;head rend="h2"&gt;Also read&lt;/head&gt;
    &lt;p&gt;Binzer said it was not about which company was better, but about trust and long-term cooperation. Tusass already works with Eutelsat and knows their systems well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Maintaining Greenlandic Control&lt;/head&gt;
    &lt;p&gt;Some Greenlandic politicians have warned that the country must keep control of its telecom infrastructure.&lt;/p&gt;
    &lt;p&gt;They fear that opening the market to foreign providers could threaten national security. For now, Tusass remains the sole provider of telecommunications in Greenland.&lt;/p&gt;
    &lt;p&gt;Binzer said the company will keep an open mind for future partnerships, but the priority remains clear. Greenland‚Äôs communication systems must stay under Greenlandic control.&lt;/p&gt;
    &lt;p&gt;This article is made and published by Anna Hartz, which may have used AI in the preparation&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dagens.com/technology/greenland-ditches-starlink-for-french-satellite-service"/><published>2025-10-22T07:14:40+00:00</published></entry></feed>