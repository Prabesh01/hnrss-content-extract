<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-26T15:39:43.466075+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46357814</id><title>The Algebra of Loans in Rust</title><updated>2025-12-26T15:39:52.892295+00:00</updated><content>&lt;doc fingerprint="37017cfe48196f59"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Algebra of Loans in Rust&lt;/head&gt;
    &lt;p&gt;The heart of Rust borrow-checking is this: when a borrow is taken, and until it expires, access to the borrowed place is restricted. For example you may not read from a place while it is mutably borrowed.&lt;/p&gt;
    &lt;p&gt;Over the recent months, lively discussions have been happening around teaching the borrow-checker to understand more things than just “shared borrow”/”mutable borrow”. We’re starting to have a few ideas floating around, so I thought I put them all down in a table so we can see how they interact. I’ll start with the tables and explain the new reference types after.&lt;/p&gt;
    &lt;p&gt;To clarify the vocabulary I’ll be using:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A “place” is a memory location, represented by an expression like &lt;code&gt;x&lt;/code&gt;,&lt;code&gt;*x&lt;/code&gt;,&lt;code&gt;x.field&lt;/code&gt;etc1;&lt;/item&gt;
      &lt;item&gt;“Taking a borrow” is the action of evaluating &lt;code&gt;&amp;amp;place&lt;/code&gt;,&lt;code&gt;&amp;amp;mut place&lt;/code&gt;,&lt;code&gt;&amp;amp;own place&lt;/code&gt;etc to a value;&lt;/item&gt;
      &lt;item&gt;A “loan” happens when we take a borrow of a place. The loan remembers the place that was borrowed and the kind of reference used to borrow it. When we take that borrow, the resulting reference type gets a fresh lifetime; as long as that lifetime is present in the type of a value somewhere, the loan is considered “live”;&lt;/item&gt;
      &lt;item&gt;While a loan is live, further operations on the borrowed place are restricted;&lt;/item&gt;
      &lt;item&gt;After the loan expires, operations on the borrowed place may be restricted too.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: These tables are not enough to understand everything about these new references. E.g. after you write to a &lt;code&gt;&amp;amp;uninit&lt;/code&gt; you can reborrow it as &lt;code&gt;&amp;amp;own&lt;/code&gt;, and vice-versa; this is not reflected in the
tables. Another example: dropping the contents of a place removes any pinning restriction placed on
it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Table 1: Given a reference, what actions are possible with it&lt;/head&gt;
    &lt;p&gt;How to read this table: the reference I have gives me the column, the action I want to do with it gives me the row (the reference types mean that the action is a reborrow with that type).&lt;/p&gt;
    &lt;p&gt;For example, if I have a &lt;code&gt;&amp;amp;own T&lt;/code&gt; I can reborrow it into a &lt;code&gt;&amp;amp;mut T&lt;/code&gt; but not a &lt;code&gt;&amp;amp;pin own T&lt;/code&gt;. If
I have a &lt;code&gt;&amp;amp;mut T&lt;/code&gt; I may write a new value to it but not drop its contents.&lt;/p&gt;
    &lt;p&gt;“Move out” means “move the value out without putting a new one in”. “Drop” means “run the drop code in-place”.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell role="head"&gt;&amp;amp;&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;mut&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;own&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;pin&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;pin mut&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;pin own&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;uninit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;mut&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;own&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;pin&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;pin mut&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;pin own&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;uninit&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Read&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Write&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Move out&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Drop&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Table 2: If a loan was taken and is live, what can I still do to the place&lt;/head&gt;
    &lt;p&gt;How to read this table: a borrow of place &lt;code&gt;p&lt;/code&gt; was taken and is still live; the kind of borrow gives
me the column. The operations I may still do on the place (without going through that borrow) give
me the row.&lt;/p&gt;
    &lt;p&gt;For example, if a &lt;code&gt;&amp;amp;&lt;/code&gt;-borrow was taken and is live, I may still read the place.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell role="head"&gt;&amp;amp;&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;mut&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;own&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;pin&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;pin mut&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;pin own&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;uninit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;mut&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;own&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;pin&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;pin mut&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;pin own&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;uninit&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Read&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Write&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Move out&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Drop&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Unsurprisingly, most of the time we can’t do anything else to the place, because the borrow is exclusive.&lt;/p&gt;
    &lt;head rend="h4"&gt;Table 3: If a loan was taken and expired, what can I now do to the place&lt;/head&gt;
    &lt;p&gt;How to read this table: a borrow of place &lt;code&gt;p&lt;/code&gt; was taken and has expired; the kind of borrow gives me
the column. The operations I may now do on the place give me the row.&lt;/p&gt;
    &lt;p&gt;For example, if a &lt;code&gt;&amp;amp;&lt;/code&gt;-borrow was taken and expired, I may no longer read the place.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell role="head"&gt;&amp;amp;&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;mut&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;own&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;pin&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;pin mut&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;pin own&lt;/cell&gt;
        &lt;cell role="head"&gt;&amp;amp;uninit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;mut&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;own&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;pin&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;pin mut&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;pin own&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;&amp;amp;uninit&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Read&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Write&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Move out&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Drop&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;code&gt;&amp;amp;own&lt;/code&gt; and &lt;code&gt;&amp;amp;uninit&lt;/code&gt; both have the property that when they expire the place is considered to have
been uninitialized. So we the only actions available are those that work on uninitialized places:
writing and &lt;code&gt;&amp;amp;uninit&lt;/code&gt; borrows.&lt;/p&gt;
    &lt;p&gt;The other point to note are the pinning loans: after a pinning loan expires, non-pinning mutable loans can’t be taken of that same place until the value is dropped.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s with all these new reference types?&lt;/head&gt;
    &lt;p&gt;All of these are speculative ideas, but at this point they’ve been circulating a bunch so should be pretty robust.&lt;/p&gt;
    &lt;head rend="h3"&gt;The owning reference &lt;code&gt;&amp;amp;own T&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;&amp;amp;own T&lt;/code&gt;2 (which has also been called &lt;code&gt;&amp;amp;move T&lt;/code&gt;) is a reference that says “I have full ownership over
this value, in particular I am responsible for dropping it”. It feels like &lt;code&gt;Box&lt;/code&gt; except that we
don’t control the allocation the value resides in. In particular, we can move the &lt;code&gt;T&lt;/code&gt; out of a &lt;code&gt;&amp;amp;own
T&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Like &lt;code&gt;Box&lt;/code&gt;, &lt;code&gt;&amp;amp;own T&lt;/code&gt; drops the contained value when it is dropped.&lt;/p&gt;
    &lt;p&gt;When I owning-borrow &lt;code&gt;&amp;amp;own x&lt;/code&gt; a place, I have given up full ownership of the value, and thus must
assume that the place is uninitialized when the borrow expires.&lt;/p&gt;
    &lt;p&gt;It makes for some interesting APIs:&lt;/p&gt;
    &lt;code&gt;impl&amp;lt;T&amp;gt; Vec&amp;lt;T&amp;gt; {
    // Pop the last value and return a reference to it (instead of moving it out directly).
    fn pop_own(&amp;amp;mut self) -&amp;gt; Option&amp;lt;&amp;amp;own T&amp;gt; { .. }
    // Iterate over the contained values, emptying the Vec as we go.
    fn drain_own(&amp;amp;mut self) -&amp;gt; impl Iterator&amp;lt;Item = &amp;amp;own T&amp;gt; { .. }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;The uninitialized reference &lt;code&gt;&amp;amp;uninit T&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;&amp;amp;uninit T&lt;/code&gt;3 (which has also been called &lt;code&gt;&amp;amp;out T&lt;/code&gt;) is a reference to an allocated but
not-yet-initialized location. Much like when we do &lt;code&gt;let x;&lt;/code&gt;, the only thing one can do with that
reference is write to it. Once written to, it can be reborrowed into everything we want.&lt;/p&gt;
    &lt;code&gt;// Typical usage is to initialize a value:
impl MyType {
    fn init(&amp;amp;uninit self) -&amp;gt; &amp;amp;own Self {
        *self = new_value();
        &amp;amp;own *self
    }
}
let x: MyType;
let ptr: &amp;amp;own MyType = MyType::init(&amp;amp;uninit x);
// `ptr` can be used much like a `Box` would. It cannot be returned from the
// current function though.
&lt;/code&gt;
    &lt;p&gt;It has a nice synergy with &lt;code&gt;&amp;amp;own T&lt;/code&gt;: we can get from &lt;code&gt;&amp;amp;uninit T&lt;/code&gt; to &lt;code&gt;&amp;amp;own T&lt;/code&gt; by writing a value into
the reference, and get from &lt;code&gt;&amp;amp;own T&lt;/code&gt; to &lt;code&gt;&amp;amp;uninit T&lt;/code&gt; by moving the value out of the reference.&lt;/p&gt;
    &lt;p&gt;Both have the property that when they expire, the original place is considered uninitialized4.&lt;/p&gt;
    &lt;head rend="h3"&gt;The pinning references &lt;code&gt;&amp;amp;pin T&lt;/code&gt;/&lt;code&gt;&amp;amp;pin mut T&lt;/code&gt;/&lt;code&gt;&amp;amp;pin own T&lt;/code&gt;&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Pinning is a notoriously subtle notion, if you’re not familiar with it I recommand the std docs on the topic. If you are familiar with it you may instead enjoy this blog post of mine that shines an original light on the notion.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Pinning references are variants of the existing reference types that also add a “pinning requirement” to the borrowed place. This requirement forbids moving the value out or deallocating the place without running &lt;code&gt;Drop&lt;/code&gt; on the value first. This applies even after the pinning borrow has
expired.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;&amp;amp;pin mut T&lt;/code&gt;5 is the most common of these, it exists in today’s Rust as &lt;code&gt;Pin&amp;lt;&amp;amp;mut T&amp;gt;&lt;/code&gt; and is crucial
to our async story. &lt;code&gt;&amp;amp;pin T&lt;/code&gt; would be &lt;code&gt;Pin&amp;lt;&amp;amp;T&amp;gt;&lt;/code&gt;; it’s less clear how useful it is but I can imagine
usecases.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;&amp;amp;pin own T&lt;/code&gt; finally would be the owning variant. This one has the tricky requirement that it must
not be passed to &lt;code&gt;mem::forget&lt;/code&gt; as that would break the drop invariant. This isn’t possible in
today’s Rust, but there have been proposals over the years as such non-forgettable types are needed
for other things.&lt;/p&gt;
    &lt;p&gt;One funky aspect of &lt;code&gt;&amp;amp;pin own T&lt;/code&gt; is that I think it’s ok to reborrow a &lt;code&gt;&amp;amp;own T&lt;/code&gt; into a &lt;code&gt;&amp;amp;pin own T&lt;/code&gt;:
since the only way for the borrow to expire entails dropping the pointed-to value, there’s no way to
break the pin guarantee so it doesn’t matter how we got that reference.&lt;/p&gt;
    &lt;p&gt;You’ll notice I didn’t list &lt;code&gt;&amp;amp;pin uninit T&lt;/code&gt;. That’s because pinning is a property of a value, and
&lt;code&gt;&amp;amp;pin uninit T&lt;/code&gt; doesn’t point to a value. To pin-initialize a value, one can just write the value to
a &lt;code&gt;&amp;amp;uninit T&lt;/code&gt; then reborrow &lt;code&gt;&amp;amp;uninit T -&amp;gt; &amp;amp;own T -&amp;gt; &amp;amp;pin own T&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You can find more details in my blog post on the topic. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It’s been proposed a number of times; my go-to writeup on the topic is this one by Daniel Henry-Mantilla. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;That’s also been proposed a few times. I present a rather tame take on it, proposals are typically more featureful. The one I’m most aware of is this one by the in-place-init working group. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;There are proposals that would allow in the code above to use the&lt;/p&gt;&lt;code&gt;&amp;amp;own MyType&lt;/code&gt;to prove to the borrowck that&lt;code&gt;x&lt;/code&gt;is now initialized, but we won’t go into that. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;While pinning has existed in Rust for a couple years now, there’s now a push to integrate it into the language properly, and this is where I take that syntax from. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nadrieril.github.io/blog/2025/12/21/the-algebra-of-loans-in-rust.html"/><published>2025-12-22T19:25:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46360080</id><title>Overlooked No More: Inge Lehmann, Who Discovered the Earth's Inner Core</title><updated>2025-12-26T15:39:52.465956+00:00</updated><content/><link href="https://www.nytimes.com/2025/12/20/obituaries/inge-lehmann-overlooked.html"/><published>2025-12-22T22:44:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46363870</id><title>Show HN: GeneGuessr – a daily biology web puzzle</title><updated>2025-12-26T15:39:52.222450+00:00</updated><content>&lt;doc fingerprint="bb7da7a56e64d3c2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GeneGuessr - Daily Protein Guessing Game&lt;/head&gt;
    &lt;head rend="h2"&gt;How to Play GeneGuessr&lt;/head&gt;
    &lt;head rend="h3"&gt;Welcome to GeneGuessr!&lt;/head&gt;
    &lt;p&gt;This is the protein of the day. Can you figure out which gene made it?&lt;/p&gt;
    &lt;p&gt;You will see spoiler bars that cover valuable hints. Tap the spoiler bar to reveal a hint underneath.&lt;/p&gt;
    &lt;p&gt;Look up your favorite gene with the search bar. Submit it as your first guess.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feedback cards&lt;/head&gt;
    &lt;p&gt;Each of your guesses will appear as a feedback card.&lt;/p&gt;
    &lt;p&gt;The feedback bar percentage shows how close you got.&lt;/p&gt;
    &lt;p&gt;Look for highlighted properties. They match your target.&lt;/p&gt;
    &lt;head rend="h3"&gt;Revealing hints&lt;/head&gt;
    &lt;p&gt;It costs 1 hint to remove a spoiler bar. You get +1 hint for each guess.&lt;/p&gt;
    &lt;p&gt;When the hint is too obvious, the bar stays locked. Just try unlocking another one.&lt;/p&gt;
    &lt;p&gt;You get to make 10 guesses before the game ends. Feel free to experiment!&lt;/p&gt;
    &lt;p&gt; Protein data: UniProt &lt;/p&gt;
    &lt;p&gt; Gene nomenclature: HGNC &lt;/p&gt;
    &lt;p&gt; Gene summaries: NCBI Gene &lt;/p&gt;
    &lt;p&gt; Gene Ontology: GO Consortium &lt;/p&gt;
    &lt;p&gt; Architecture: CATH &lt;/p&gt;
    &lt;p&gt; Pathway data: Reactome &lt;/p&gt;
    &lt;p&gt; Tissue specificity: Human Protein Atlas RNA expression (tau metric) &lt;/p&gt;
    &lt;p&gt; 3D structures: RCSB PDB, AlphaFold DB, SWISS-MODEL &lt;/p&gt;
    &lt;p&gt; Structure viewer: Mol* (via PDBe integration) &lt;/p&gt;
    &lt;p&gt; Origin age: Litman T &amp;amp; Stein WD (2019). Obtaining estimates for the ages of all the protein-coding genes and most of the ontology-identified noncoding genes of the human genome, assigned to 19 phylostrata Semin Oncol 46(1):3-9 &lt;/p&gt;
    &lt;p&gt; First publication year: Zwick ME, Kraemer SA &amp;amp; Carter GW (2019). Dataset of frequency patterns of publications for human protein-coding genes. Data Brief 28:104770 &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://geneguessr.brinedew.bio/"/><published>2025-12-23T09:40:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46379173</id><title>Keystone (YC S25) is hiring engineer #1 to automate coding</title><updated>2025-12-26T15:39:51.599771+00:00</updated><content>&lt;doc fingerprint="233a92f07e6fe468"&gt;
  &lt;main&gt;
    &lt;p&gt;Your team's on-call AI engineer&lt;/p&gt;
    &lt;p&gt;About Keystone&lt;/p&gt;
    &lt;p&gt;We're building AI-native error monitoring that automatically investigates production issues and generates code fixes. Think Sentry, but built from the ground up for a world where AI can actually understand your codebase, trace through logs, and tell you exactly what broke and how to fix it.&lt;/p&gt;
    &lt;p&gt;We're starting here and expanding until we're the default tool for building product, period. Our mission is to free engineers from the drudgery of digging through logs, setting up systems, and debugging- so they can focus on understanding users and designing great products.&lt;/p&gt;
    &lt;p&gt;We're in-person in SoMa, San Francisco. We raised a $5.2M seed from True Ventures, Twenty Two Ventures, Pear VC, and Ritual Capital- plus the founders of YC, Dropbox, Supabase, Eight Sleep, Graphite, Resend, RocketMoney, and more. Early design partners include teams at Perplexity and Lovable.&lt;/p&gt;
    &lt;p&gt;About the Role&lt;/p&gt;
    &lt;p&gt;You'd be the first engineering hire, working directly with me (the founder) to build the core product. You'll have more ownership and influence over the product, culture, and technical direction than you'd get almost anywhere else.&lt;/p&gt;
    &lt;p&gt;Example projects:&lt;/p&gt;
    &lt;p&gt;You might be a great fit if you:&lt;/p&gt;
    &lt;p&gt;Stack: TypeScript, React (Next.js), Python, Postgres, Redis, AWS&lt;/p&gt;
    &lt;p&gt;Comp &amp;amp; benefits: Top-of-market salary + significant equity, full health/dental/vision, all meals covered, equipment budget&lt;/p&gt;
    &lt;p&gt;To apply: submit an application here on workatastartup.com or email founders [at] withkeystone [dot] com with “Keystone Founding Engineer” in the subject line. I will reply to all such emails.&lt;/p&gt;
    &lt;p&gt;If there appears to be a fit, we'll reach to schedule 2-3 short technicals. After, we'll schedule an onsite in our office, where you'll work on a small project, discuss ideas, and get a sense for our in-person culture.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/keystone/jobs/J3t9XeM-founding-engineer"/><published>2025-12-24T21:01:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46384167</id><title>Python 3.15’s interpreter for Windows x86-64 should hopefully be 15% faster</title><updated>2025-12-26T15:39:51.509536+00:00</updated><content>&lt;doc fingerprint="2e649111fe3785d0"&gt;
  &lt;main&gt;
    &lt;p&gt;24 December 2025&lt;/p&gt;
    &lt;p&gt;Some time ago I posted an apology piece for Python’s tail calling results. I apologized for communicating performance results without noticing a compiler bug had occured.&lt;/p&gt;
    &lt;p&gt;I can proudly say today that I am partially retracting that apology, but only for two platforms—macOS AArch64 (XCode Clang) and Windows x86-64 (MSVC).&lt;/p&gt;
    &lt;p&gt;In our own experiments, the tail calling interpreter for CPython was found to beat the computed goto interpreter by 5% on pyperformance on AArch64 macOS using XCode Clang, and roughly 15% on pyperformance on Windows on an experimental internal version of MSVC. The Windows build is against a switch-case interpreter, but this in theory shouldn’t matter too much, more on that in the next section.&lt;/p&gt;
    &lt;p&gt;This is of course, a hopefully accurate result. I tried to be more diligent here, but I am of course not infallible. However, I have found that sharing early and making a fool of myself often works well, as it has led to people catching bugs in my code, so I shall continue doing so :).&lt;/p&gt;
    &lt;p&gt;Also this assumes the change doesn’t get reverted later in Python 3.15’s development cycle.&lt;/p&gt;
    &lt;p&gt;Just a recap. There are two popular current ways of writing C-based interpreters.&lt;/p&gt;
    &lt;p&gt;Switch-cases:&lt;/p&gt;
    &lt;code&gt;switch (opcode) {
    case INST_1: ...
    case INST_2: ...
}
&lt;/code&gt;
    &lt;p&gt;Where we just switch-case to the correct instruction handler.&lt;/p&gt;
    &lt;p&gt;And the other popular way is a GCC/Clang extension called labels-as-values/computed gotos.&lt;/p&gt;
    &lt;code&gt;goto *dispatch_table[opcode];
INST_1: ...
INST_2: ...
&lt;/code&gt;
    &lt;p&gt;Which is basically the same idea, but to instead jump to the address of the next label. Traditionally, the key optimization here is that it needs only one jump to go to the next instruction, while in the switch-case interpreter, a naiive compiler would need two jumps.&lt;/p&gt;
    &lt;p&gt;With modern compilers however, the benefits of the computed gotos is a lot less, mainly because modern compilers have gotten better and modern hardware has also gotten better. In Nelson Elhage’s excellent investigation on the next kind of interpreter, the speedup of computed gotos over switch case on modern Clang was only in the low single digits on pyperformance.&lt;/p&gt;
    &lt;p&gt;A 3rd way that was suggested decades ago, but not really entirely feasible is call/tail-call threaded interpreters. In this scheme, each bytecode handler is its own function, and we tail-call from one handler to the next in the instruction stream:&lt;/p&gt;
    &lt;code&gt;return dispatch_table[opcode];

PyObject *INST_1(...) {

}

PyObject *INST_2(...) {
}
&lt;/code&gt;
    &lt;p&gt;This wasn’t too feasible in C for one main reason—tail call optimization was merely an optimization. It’s something the C compiler might do, or might not do. This means if you’re unlucky and the C compiler chooses not to perform the tail call, your interpreter might stack overflow!&lt;/p&gt;
    &lt;p&gt;Some time ago, Clang introduced &lt;code&gt;__attribute__((musttail))&lt;/code&gt;, which allowed
for mandating that a call must be tail-called. Otherwise, the compilation
will fail. To my knowledge, the first time this was popularized for use
in a mainstream interpreter was in
Josh Haberman’s Protobuf blog post.&lt;/p&gt;
    &lt;p&gt;Later on, Haoran Xu noticed that the GHC calling convention combined with tail calls produced efficient code. They used this for their baseline JIT in a paper and termed the technique Copy-and-Patch.&lt;/p&gt;
    &lt;p&gt;After using a fixed XCode Clang, our performance numbers on CPython 3.14/3.15 suggest that the tail calling interpreter does provide a modest speedup over computed gotos. Around the 5% geomean range on pyperformance.&lt;/p&gt;
    &lt;p&gt;To my understanding, &lt;code&gt;uv&lt;/code&gt; already ships Python 3.14 on macOS with tail calling,
which might be responsible for some of the speedups you see on there.
We’re planning to ship the official 3.15 macOS binaries on &lt;code&gt;python.org&lt;/code&gt; with
tail calling as well.&lt;/p&gt;
    &lt;p&gt;However, you’re not here for that. The title of this blog post is clearly about MSVC Windows x86-64. So what about that?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[!CAUTION] The features for MSVC discussed below are to my knowledge, experimental. They are not guaranteed to always be around unless the MSVC team decide to keep them. Use at your own risk!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;These are the preliminary pyperformance results for CPython on MSVC with tail-calling vs switch-case. Any number above 1.00x is a speedup (e.g. &lt;code&gt;1.01x == 1% speedup&lt;/code&gt;), anything below 1.00x is a slowdown.
The speedup is a geomtric mean of around 15-16%, with a
range of ~60% slowdown (one or two outliers) to 78% speedup.
However, the key thing is that the vast majority of benchmaarks sped up!&lt;/p&gt;
    &lt;p&gt;Chart credits to Michael Droettboom&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[!WARNING] These results are on an experimental internal MSVC compiler, public results below.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;To verify this and make sure I wasn’t wrong yet again, I checked the results on my machine with Visual Studio 2026. These are the results from this issue.&lt;/p&gt;
    &lt;code&gt;Mean +- std dev: [spectralnorm_tc_no] 146 ms +- 1 ms -&amp;gt; [spectralnorm_tc] 98.3 ms +- 1.1 ms: 1.48x faster
Mean +- std dev: [nbody_tc_no] 145 ms +- 2 ms -&amp;gt; [nbody_tc] 107 ms +- 2 ms: 1.35x faster
Mean +- std dev: [bm_django_template_tc_no] 26.9 ms +- 0.5 ms -&amp;gt; [bm_django_template_tc] 22.8 ms +- 0.4 ms: 1.18x faster
Mean +- std dev: [xdsl_tc_no] 64.2 ms +- 1.6 ms -&amp;gt; [xdsl_tc] 56.1 ms +- 1.5 ms: 1.14x faster
&lt;/code&gt;
    &lt;p&gt;So yeah, the speedups are real! For a large-ish library like xDSL, we see a 14% speedup, while for smaller microbenchmarks like nbody and spectralnorm, the speedups are greater.&lt;/p&gt;
    &lt;p&gt;Thanks to Chris Eibl and Brandt Bucher, we managed to get the PR for this on MSVC over the finish line. I also want to sincerely thank the MSVC team. I can’t say this enough: they have been a joy to work with and I’m very impressed by what they’ve done, and I want to congratulate them on releasing Visual Studio 2026.&lt;/p&gt;
    &lt;p&gt;This is now listed in the What’s New for 3.15 notes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Builds using Visual Studio 2026 (MSVC 18) may now use the new tail-calling interpreter. Results on an early experimental MSVC compiler reported roughly 15% speedup on the geometric mean of pyperformance on Windows x86-64 over the switch-case interpreter. We have observed speedups ranging from 15% for large pure-Python libraries to 40% for long-running small pure-Python scripts on Windows. (Contributed by Chris Eibl, Ken Jin, and Brandt Bucher in gh-143068. Special thanks to the MSVC team including Hulon Jenkins.)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is the documentation for [[msvc::musttail]].&lt;/p&gt;
    &lt;p&gt;I used to believe the the tail calling interpreters get their speedup from better register use. While I still believe that now, I suspect that is not the main reason for speedups in CPython.&lt;/p&gt;
    &lt;p&gt;My main guess now is that tail calling resets compiler heuristics to sane levels, so that compilers can do their jobs.&lt;/p&gt;
    &lt;p&gt;Let me show an example, at the time of writing, CPython 3.15’s interpreter loop is around 12k lines of C code. That’s 12k lines in a single function for the switch-case and computed goto interpreter.&lt;/p&gt;
    &lt;p&gt;This has caused many issues for compilers in the past, too many to list in fact. I have a EuroPython 2025 talk about this. In short, this overly large function breaks a lot of compiler heuristics.&lt;/p&gt;
    &lt;p&gt;One of the most beneficial optimisations is inlining. In the past, we’ve found that compilers sometimes straight up refuse to inline even the simplest of functions in that 12k loc eval loop. I want to stress that this is not the fault of the compiler. It’s actually doing the correct thing—you usually don’t want to increase the code size of something already super large. Unfortunately, this does’t bode well for our interpreter.&lt;/p&gt;
    &lt;p&gt;You might say just write the interpreter in assembly! However, the whole point of this exercise is to not do that.&lt;/p&gt;
    &lt;p&gt;Ok enough talk, let’s take a look at the code now. Taking a real example, we examine &lt;code&gt;BINARY_OP_ADD_INT&lt;/code&gt; which adds two Python integers.
Cleaning up the code so it’s readable, things look like this:&lt;/p&gt;
    &lt;code&gt;TARGET(BINARY_OP_ADD_INT) {
    // Increment the instruction pointer.
    _Py_CODEUNIT* const this_instr = next_instr;
    frame-&amp;gt;instr_ptr = next_instr;
    next_instr += 6;
    _PyStackRef right = stack_pointer[-1];

    // Check that LHS is an int.
    PyObject *value_o = PyStackRef_AsPyObjectBorrow(left);
    if (!_PyLong_CheckExactAndCompact(value_o)) {
        JUMP_TO_PREDICTED(BINARY_OP);
    }

    // Check that RHS is an int.
    // ... (same code as above for LHS)

    // Add them together.
    PyObject *left_o = PyStackRef_AsPyObjectBorrow(left);
    PyObject *right_o = PyStackRef_AsPyObjectBorrow(right);
    res = _PyCompactLong_Add((PyLongObject *)left_o, (PyLongObject *)right_o);

    // If the addition fails, fall back to the generic instruction.
    if (PyStackRef_IsNull(res)) {
        JUMP_TO_PREDICTED(BINARY_OP);
    }

    // Close the references.
    PyStackRef_CLOSE_SPECIALIZED(left, _PyLong_ExactDealloc);
    PyStackRef_CLOSE_SPECIALIZED(right, _PyLong_ExactDealloc);

    // Write to the stack, and dispatch.
    stack_pointer[-2] = res;
    stack_pointer += -1;
    DISPATCH();
}
&lt;/code&gt;
    &lt;p&gt;Seems simple enough, let’s take a look at the assembly for switch-case on VS 2026. Note again, this is a non-PGO build for easy source information, PGO generally makes some of these problems go away, but not all of them:&lt;/p&gt;
    &lt;code&gt;                if (!_PyLong_CheckExactAndCompact(value_o)) {
00007FFC4DE24DCE  mov         rcx,rbx  
00007FFC4DE24DD1  mov         qword ptr [rsp+58h],rax  
00007FFC4DE24DD6  call        _PyLong_CheckExactAndCompact (07FFC4DE227F0h)  
00007FFC4DE24DDB  test        eax,eax  
00007FFC4DE24DDD  je          _PyEval_EvalFrameDefault+10EFh (07FFC4DE258FFh)
...
                res = _PyCompactLong_Add((PyLongObject *)left_o, (PyLongObject *)right_o);
00007FFC4DE24DFF  mov         rdx,rbx  
00007FFC4DE24E02  mov         rcx,r15  
00007FFC4DE24E05  call        _PyCompactLong_Add (07FFC4DD34150h)  
00007FFC4DE24E0A  mov         rbx,rax  
...
                PyStackRef_CLOSE_SPECIALIZED(value, _PyLong_ExactDealloc);
00007FFC4DE24E17  lea         rdx,[_PyLong_ExactDealloc (07FFC4DD33BD0h)]  
00007FFC4DE24E1E  mov         rcx,rsi  
00007FFC4DE24E21  call        PyStackRef_CLOSE_SPECIALIZED (07FFC4DE222A0h) 
&lt;/code&gt;
    &lt;p&gt;Huh… all our functions were not inlined. Surely that must’ve mean they were too big or something right? Let’s look at &lt;code&gt;PyStackReF_CLOSE_SPECIALIZED&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;static inline void
PyStackRef_CLOSE_SPECIALIZED(_PyStackRef ref, destructor destruct)
{
    assert(!PyStackRef_IsNull(ref));
    if (PyStackRef_RefcountOnObject(ref)) {
        Py_DECREF_MORTAL_SPECIALIZED(BITS_TO_PTR(ref), destruct);
    }
}
&lt;/code&gt;
    &lt;p&gt;That looks … inlineable?&lt;/p&gt;
    &lt;p&gt;Here’s how &lt;code&gt;BINARY_OP_ADD_INT&lt;/code&gt; looks with tail calling on VS 2026 (again,
no PGO):&lt;/p&gt;
    &lt;code&gt;                if (!_PyLong_CheckExactAndCompact(left_o)) {
00007FFC67164785  cmp         qword ptr [rax+8],rdx  
00007FFC67164789  jne         _TAIL_CALL_BINARY_OP_ADD_INT@@_A+149h (07FFC67164879h)  
00007FFC6716478F  mov         r9,qword ptr [rax+10h]  
00007FFC67164793  cmp         r9,10h  
00007FFC67164797  jae         _TAIL_CALL_BINARY_OP_ADD_INT@@_A+149h (07FFC67164879h) 
...
                res = _PyCompactLong_Add((PyLongObject *)left_o, (PyLongObject *)right_o);
00007FFC6716479D  mov         eax,dword ptr [rax+18h]  
00007FFC671647A0  and         r9d,3  
00007FFC671647A4  and         r8d,3  
00007FFC671647A8  mov         edx,1  
00007FFC671647AD  sub         rdx,r9  
00007FFC671647B0  mov         ecx,1  
00007FFC671647B5  imul        rdx,rax  
00007FFC671647B9  mov         eax,dword ptr [rbx+18h]  
00007FFC671647BC  sub         rcx,r8  
00007FFC671647BF  imul        rcx,rax  
00007FFC671647C3  add         rcx,rdx  
00007FFC671647C6  call        medium_from_stwodigits (07FFC6706E9E0h)  
00007FFC671647CB  mov         rbx,rax  
...
                PyStackRef_CLOSE_SPECIALIZED(value, _PyLong_ExactDealloc);
00007FFC671647EB  test        bpl,1  
00007FFC671647EF  jne         _TAIL_CALL_BINARY_OP_ADD_INT@@_A+0ECh (07FFC6716481Ch)  
00007FFC671647F1  add         dword ptr [rbp],0FFFFFFFFh  
00007FFC671647F5  jne         _TAIL_CALL_BINARY_OP_ADD_INT@@_A+0ECh (07FFC6716481Ch)  
00007FFC671647F7  mov         rax,qword ptr [_PyRuntime+25F8h (07FFC675C45F8h)]  
00007FFC671647FE  test        rax,rax  
00007FFC67164801  je          _TAIL_CALL_BINARY_OP_ADD_INT@@_A+0E4h (07FFC67164814h)  
00007FFC67164803  mov         r8,qword ptr [_PyRuntime+2600h (07FFC675C4600h)]  
00007FFC6716480A  mov         edx,1  
00007FFC6716480F  mov         rcx,rbp  
00007FFC67164812  call        rax  
00007FFC67164814  mov         rcx,rbp  
00007FFC67164817  call        _PyLong_ExactDealloc (07FFC67073DA0h) 
&lt;/code&gt;
    &lt;p&gt;Would you look at that, suddenly our trivial functions get inlined :).&lt;/p&gt;
    &lt;p&gt;You might also say, surely this does not happen on PGO builds? Well the issue I linked above actually says it does! So yeah happy days.&lt;/p&gt;
    &lt;p&gt;Once again I want to stress, this is not the compiler’s fault! It’s just that the CPython interpreter loop is not the best thing to optimize.&lt;/p&gt;
    &lt;p&gt;Unfortunately, for now, you will have to build from source.&lt;/p&gt;
    &lt;p&gt;With VS 2026, after cloning CPython, for a release build with PGO:&lt;/p&gt;
    &lt;code&gt;$env:PlatformToolset = "v145"
./PCbuild/build.bat --tail-call-interp -c Release -p x64 --pgo
&lt;/code&gt;
    &lt;p&gt;Hopefully, we can distribute this in an easier binary form in the future once Python 3.15’s development matures!&lt;/p&gt;
    &lt;p&gt;I was asked for a cross-compiler test. So here’s a quick and dirty toy benchmark of pystones. The last row is the tail call enabled build. All configurations have PGO. On this toy benchmark, we get roughly a 30% uplift. Note that this is unscientific as it was only a sample size of 1 and I cannot disable Turbo Boost on my laptop on Windows for some reason.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Compiler&lt;/cell&gt;
        &lt;cell role="head"&gt;PlatformToolSet&lt;/cell&gt;
        &lt;cell role="head"&gt;Pystones/second (higher is better)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;VS2019&lt;/cell&gt;
        &lt;cell&gt;142&lt;/cell&gt;
        &lt;cell&gt;677544&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;VS2022&lt;/cell&gt;
        &lt;cell&gt;143&lt;/cell&gt;
        &lt;cell&gt;710773&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;VS2026&lt;/cell&gt;
        &lt;cell&gt;145&lt;/cell&gt;
        &lt;cell&gt;682089&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;VS2026+TC&lt;/cell&gt;
        &lt;cell&gt;145&lt;/cell&gt;
        &lt;cell&gt;970306&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fidget-spinner.github.io/posts/no-longer-sorry.html"/><published>2025-12-25T13:02:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46385600</id><title>Clearspace (YC W23) Is Hiring a Founding Network Engineer (VPN and Proxy)</title><updated>2025-12-26T15:39:51.016096+00:00</updated><content>&lt;doc fingerprint="b792f12e5d21eddd"&gt;
  &lt;main&gt;
    &lt;p&gt;Eliminate compulsive phone usage&lt;/p&gt;
    &lt;p&gt;About Clearspace&lt;/p&gt;
    &lt;p&gt;Clearspace is building the intentionality layer of the internet. Our mission is to build technology as effective at protecting human attention as social media is at exploiting it (infinite scrolling, short-form feeds, manipulative notifications, etc). Our category defining mobile app has been featured on Huberman Lab, New York Times Wirecutter, NPR Marketplace, Forbes, TBPN.&lt;/p&gt;
    &lt;p&gt;People that want a better relationship with their devices have nowhere to turn except for willpower. We are building a system allows users to control what their devices see by processing and filtering network traffic against natural language rules.&lt;/p&gt;
    &lt;p&gt;About The Role&lt;/p&gt;
    &lt;p&gt;We’re looking for a networking-obsessed engineer to own the VPN + first-hop policy proxy that powers our AI agent. You don’t need to be an IKEv2 expert, we care more about deep networking intuition, debugging skill, and the desire to go all the way down the stack until the truth reveals itself.&lt;/p&gt;
    &lt;p&gt;You might be a great fit if you’ve built:&lt;/p&gt;
    &lt;p&gt;What You’ll Build&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;Nice to Have&lt;/p&gt;
    &lt;p&gt;Compensation&lt;/p&gt;
    &lt;p&gt;At Clearspace we help people reduce compulsive phone usage.&lt;/p&gt;
    &lt;p&gt;We exist to protect people's attention from the exploits of modern technology platforms and make space for the things that matter to them most.&lt;/p&gt;
    &lt;p&gt;We believe the technology to protect someones attention should be just as sophisticated and effective as the tech that is exploiting it and are building a world-class engineering team to arm the world with a comprehensive attention protection stack.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/clearspace/jobs/5LtM86I-founding-network-engineer-at-clearspace"/><published>2025-12-25T17:01:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46386211</id><title>Fahrplan – 39C3</title><updated>2025-12-26T15:39:50.030382+00:00</updated><content>&lt;doc fingerprint="ad728d1d22b2a7ef"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Sat - Day 1 - December 27&lt;/head&gt;
    &lt;head rend="h5"&gt;Opening Ceremony&lt;/head&gt;
    &lt;p&gt;pajowu, Stella&lt;/p&gt;
    &lt;head rend="h5"&gt;All Sorted by Machines of Loving Grace? "AI", Cybernetics, and Fascism and how to Intervene&lt;/head&gt;
    &lt;p&gt;Katika Kühnreich&lt;/p&gt;
    &lt;head rend="h5"&gt;The art of text (rendering)&lt;/head&gt;
    &lt;p&gt;Nicolas Rougier&lt;/p&gt;
    &lt;head rend="h5"&gt;A Tale of Two Leaks: How Hackers Breached the Great Firewall of China&lt;/head&gt;
    &lt;p&gt;Jade Sheffey&lt;/p&gt;
    &lt;head rend="h5"&gt;OpenAutoLab: photographic film processing machine. Fully automatic and DIY-friendly.&lt;/head&gt;
    &lt;p&gt;Kauz&lt;/p&gt;
    &lt;head rend="h5"&gt;Zentrum für Politische Schönheit: Ein Jahr Adenauer SRP+ und der Walter Lübke Memorial Park&lt;/head&gt;
    &lt;p&gt;Stefan Pelzer, Philipp Ruch&lt;/p&gt;
    &lt;head rend="h5"&gt;Demystifying Fuzzer Behaviour&lt;/head&gt;
    &lt;p&gt;Addison&lt;/p&gt;
    &lt;head rend="h5"&gt;ISDN + POTS Telephony at Congress and Camp&lt;/head&gt;
    &lt;p&gt;Harald "LaF0rge" Welte&lt;/p&gt;
    &lt;head rend="h5"&gt;Brennende Wälder und Kommentarspalten - Klimaupdate mit dem FragDenStaat Climate Helpdesk&lt;/head&gt;
    &lt;p&gt;Joschi Wolf&lt;/p&gt;
    &lt;head rend="h5"&gt;Building hardware - easier than ever - harder than it should be&lt;/head&gt;
    &lt;p&gt;Kliment&lt;/p&gt;
    &lt;head rend="h5"&gt;Neuroexploitation by Design: Wie Algorithmen in Glücksspielprodukten sich Wirkweisen des Reinforcement Learning und dopaminergen Belohnungssystems zunutze machen&lt;/head&gt;
    &lt;p&gt;Elke Smith&lt;/p&gt;
    &lt;head rend="h5"&gt;FeTAp 611 unplugged: Taking a rotary dial phone to the mobile age&lt;/head&gt;
    &lt;p&gt;Michael Weiner&lt;/p&gt;
    &lt;head rend="h5"&gt;Who cares about the Baltic Jammer? – Terrestrial Navigation in the Baltic Sea Region&lt;/head&gt;
    &lt;p&gt;Lars, Niklas Hehenkamp, Markus&lt;/p&gt;
    &lt;head rend="h5"&gt;Liberating Bluetooth on the ESP32&lt;/head&gt;
    &lt;p&gt;Antonio Vázquez Blanco (Antón)&lt;/p&gt;
    &lt;head rend="h5"&gt;Chaos macht Küche&lt;/head&gt;
    &lt;p&gt;Ingwer Andersen&lt;/p&gt;
    &lt;head rend="h5"&gt;Developing New Medicines in the Age of AI and Personalized Medicine&lt;/head&gt;
    &lt;p&gt;Dennis Özcelik&lt;/p&gt;
    &lt;head rend="h5"&gt;Endlich maschinenlesbare Urteile! Open access für Juristen&lt;/head&gt;
    &lt;p&gt;Beata Hubrig, Nuri Khadem-Al-Charieh&lt;/p&gt;
    &lt;head rend="h5"&gt;Opening pAMDora's box and unleashing a thousand paths on the journey to play Beatsaber custom songs&lt;/head&gt;
    &lt;p&gt;tihmstar&lt;/p&gt;
    &lt;head rend="h5"&gt;Not an Impasse: Child Safety, Privacy, and Healing Together&lt;/head&gt;
    &lt;p&gt;Kate Sim&lt;/p&gt;
    &lt;head rend="h5"&gt;KIM 1.5: Noch mehr Kaos In der Medizinischen Telematikinfrastruktur (TI)&lt;/head&gt;
    &lt;p&gt;Christoph Saatjohann&lt;/p&gt;
    &lt;head rend="h5"&gt;RedScout42 – Zur digitalen Wohnungsfrage&lt;/head&gt;
    &lt;p&gt;Sandra, Leonard&lt;/p&gt;
    &lt;head rend="h5"&gt;All my Deutschlandtickets gone: Fraud at an industrial scale&lt;/head&gt;
    &lt;p&gt;Q Misell, 551724 / maya boeckh&lt;/p&gt;
    &lt;head rend="h5"&gt;Of Boot Vectors and Double Glitches: Bypassing RP2350's Secure Boot&lt;/head&gt;
    &lt;p&gt;stacksmashing, nsr&lt;/p&gt;
    &lt;head rend="h5"&gt;„KI“, Digitalisierung und Longevity als Fix für ein kaputtes Gesundheitssystem?&lt;/head&gt;
    &lt;p&gt;Manuel Hofmann&lt;/p&gt;
    &lt;head rend="h5"&gt;Chaos all year round&lt;/head&gt;
    &lt;p&gt;Deanna&lt;/p&gt;
    &lt;head rend="h5"&gt;To sign or not to sign: Practical vulnerabilities in GPG &amp;amp; friends&lt;/head&gt;
    &lt;p&gt;49016, Liam&lt;/p&gt;
    &lt;head rend="h5"&gt;Handy weg bis zur Ausreise – Wie Cellebrite ins Ausländeramt kam&lt;/head&gt;
    &lt;p&gt;Chris Köver&lt;/p&gt;
    &lt;head rend="h5"&gt;Pwn2Roll: Who Needs a 595€ Remote When You Have wheelchair.py?&lt;/head&gt;
    &lt;p&gt;elfy&lt;/p&gt;
    &lt;head rend="h5"&gt;Escaping Containment: A Security Analysis of FreeBSD Jails&lt;/head&gt;
    &lt;p&gt;ilja, Michael Smith&lt;/p&gt;
    &lt;head rend="h5"&gt;Die Känguru-Rebellion: Digital Independence Day&lt;/head&gt;
    &lt;p&gt;Marc-Uwe Kling, Linus Neumann&lt;/p&gt;
    &lt;head rend="h5"&gt;And so it begins - Wie unser Rechtsstaat auf dem Highway Richtung Trumpismus rast – und warum afghanische Kläger*innen für uns die Notbremse ziehen&lt;/head&gt;
    &lt;p&gt;Eva, Elaha&lt;/p&gt;
    &lt;head rend="h5"&gt;1965 + 60 Years of Algorithmic Art with Computers&lt;/head&gt;
    &lt;p&gt;Enna Gerhard, Frieder Nake&lt;/p&gt;
    &lt;head rend="h5"&gt;Life on Hold: What Does True Solidarity Look Like Beyond Duldung, Camps, Deportation, and Payment Cards?&lt;/head&gt;
    &lt;p&gt;Hafid Shaaib, Eric Noel Mbiakeu&lt;/p&gt;
    &lt;head rend="h5"&gt;Chatkontrolle - Ctrl+Alt+Delete&lt;/head&gt;
    &lt;p&gt;khaleesi, Markus Reuter&lt;/p&gt;
    &lt;head rend="h5"&gt;Excuse me, what precise time is It?&lt;/head&gt;
    &lt;p&gt;Oliver Ettlin&lt;/p&gt;
    &lt;head rend="h5"&gt;BitUnlocker: Leveraging Windows Recovery to Extract BitLocker Secrets&lt;/head&gt;
    &lt;p&gt;Alon Leviev&lt;/p&gt;
    &lt;head rend="h5"&gt;Not To Be Trusted - A Fiasco in Android TEEs&lt;/head&gt;
    &lt;p&gt;0ddc0de, gannimo, Philipp&lt;/p&gt;
    &lt;head rend="h5"&gt;Hacking washing machines&lt;/head&gt;
    &lt;p&gt;Severin von Wnuck-Lipinski, Hajo Noerenberg&lt;/p&gt;
    &lt;head rend="h5"&gt;Doomsday-Porn, Schäferhunde und die „niedliche Abschiebung“ von nebenan: Wie autoritäre Akteure KI-generierte Inhalte für Social Media nutzen&lt;/head&gt;
    &lt;p&gt;Katharina Nocun&lt;/p&gt;
    &lt;head rend="h5"&gt;Throwing your rights under the Omnibus - how the EU's reform agenda threatens to erase a decade of digital rights&lt;/head&gt;
    &lt;p&gt;Thomas Lohninger, Ralf Bendrath&lt;/p&gt;
    &lt;head rend="h5"&gt;DNGerousLINK: A Deep Dive into WhatsApp 0-Click Exploits on iOS and Samsung Devices&lt;/head&gt;
    &lt;p&gt;Zhongrui Li, Yizhe Zhuang, Kira Chen&lt;/p&gt;
    &lt;head rend="h5"&gt;Bluetooth Headphone Jacking: A Key to Your Phone&lt;/head&gt;
    &lt;p&gt;Dennis Heinze, Frieder Steinmetz&lt;/p&gt;
    &lt;head rend="h5"&gt;Breaking architecture barriers: Running x86 games and apps on ARM&lt;/head&gt;
    &lt;p&gt;neobrain&lt;/p&gt;
    &lt;head rend="h5"&gt;The Eyes of Photon Science: Imaging, Simulation and the Quest to Make the Invisible Visible&lt;/head&gt;
    &lt;p&gt;MarKuster&lt;/p&gt;
    &lt;head rend="h5"&gt;Coding Dissent: Art, Technology, and Tactical Media&lt;/head&gt;
    &lt;p&gt;Helena Nikonole&lt;/p&gt;
    &lt;head rend="h5"&gt;AI-generated content in Wikipedia - a tale of caution&lt;/head&gt;
    &lt;p&gt;Mathias Schindler&lt;/p&gt;
    &lt;head rend="h5"&gt;Building a NOC from scratch&lt;/head&gt;
    &lt;p&gt;lilly, Scientress&lt;/p&gt;
    &lt;head rend="h5"&gt;From Silicon to Darude Sand-storm: breaking famous synthesizer DSPs&lt;/head&gt;
    &lt;p&gt;giulioz&lt;/p&gt;
    &lt;head rend="h5"&gt;Unnecessarily Complicated Kitchen – Die Wissenschaft des guten Geschmacks&lt;/head&gt;
    &lt;p&gt;LukasQ&lt;/p&gt;
    &lt;head rend="h2"&gt;Sun - Day 2 - December 28&lt;/head&gt;
    &lt;head rend="h5"&gt;Junghacker:innentag Einführung&lt;/head&gt;
    &lt;head rend="h5"&gt;Protecting the network data of one billion people: Breaking network crypto in popular Chinese mobile apps&lt;/head&gt;
    &lt;p&gt;Mona&lt;/p&gt;
    &lt;head rend="h5"&gt;Hatupangwingwi: The story how Kenyans fought back against intrusive digital identity systems&lt;/head&gt;
    &lt;p&gt;Mustafa Mahmoud Yousif&lt;/p&gt;
    &lt;head rend="h5"&gt;Lightning Talks - Tag 2&lt;/head&gt;
    &lt;p&gt;bonnie, Gilbert, Andi Bräu&lt;/p&gt;
    &lt;head rend="h5"&gt;Digitale Inklusion: Wie wir digitale Barrierefreiheit für alle erreichen können&lt;/head&gt;
    &lt;p&gt;Jakob Sponholz, Kathrin Klapper, Lena Christina Müller&lt;/p&gt;
    &lt;head rend="h5"&gt;Skynet Starter Kit: From Embodied AI Jailbreak to Remote Takeover of Humanoid Robots&lt;/head&gt;
    &lt;p&gt;Shipei Qu, Zikai Xu, Xuangan Xiao&lt;/p&gt;
    &lt;head rend="h5"&gt;Suing spyware in Europe: news from the front!&lt;/head&gt;
    &lt;p&gt;Lori Roussey, Celia/Irídia&lt;/p&gt;
    &lt;head rend="h5"&gt;Neue Chaos Events - InselChaos und Håck ma’s Castle plaudern aus dem Nähkästchen&lt;/head&gt;
    &lt;p&gt;Erwin Ernst "eest9" Steinhammer, lasii, Daniel, Niklas&lt;/p&gt;
    &lt;head rend="h5"&gt;A post-American, enshittification-resistant internet&lt;/head&gt;
    &lt;p&gt;Cory Doctorow&lt;/p&gt;
    &lt;head rend="h5"&gt;A space odyssey #2: How to study moon rocks from the Soviet sample return mission Luna 24&lt;/head&gt;
    &lt;p&gt;Paul Koetter, Christopher Hamann&lt;/p&gt;
    &lt;head rend="h5"&gt;Agentic ProbLLMs: Exploiting AI Computer-Use and Coding Agents&lt;/head&gt;
    &lt;p&gt;Johann Rehberger&lt;/p&gt;
    &lt;head rend="h5"&gt;selbstverständlich antifaschistisch! Aktuelle Informationen zu den Verfahren im Budapest-Komplex - von family &amp;amp; friends Hamburg&lt;/head&gt;
    &lt;p&gt;Andreas family &amp;amp; friends Hamburg, Birgit family &amp;amp; friends Hamburg&lt;/p&gt;
    &lt;head rend="h5"&gt;Chaospager - How to construct an Open Pager System for c3&lt;/head&gt;
    &lt;p&gt;Max, Julian&lt;/p&gt;
    &lt;head rend="h5"&gt;Chaos Communication Chemistry: DNA information technology and security based on molecular entropy&lt;/head&gt;
    &lt;p&gt;Anne Lüscher&lt;/p&gt;
    &lt;head rend="h5"&gt;Live, Die, Repeat: The fight against data retention and boundless access to data&lt;/head&gt;
    &lt;p&gt;Klaus Landefeld&lt;/p&gt;
    &lt;head rend="h5"&gt;Power Cycle B7 oder Warum kauft man eine Zeche?&lt;/head&gt;
    &lt;p&gt;Kohlenpod, kater, Stephan&lt;/p&gt;
    &lt;head rend="h5"&gt;Cracking open what makes Apple's Low-Latency WiFi so fast&lt;/head&gt;
    &lt;p&gt;Henri Jäger&lt;/p&gt;
    &lt;head rend="h5"&gt;Awful interception: misadventures of the russian surveillance machinery&lt;/head&gt;
    &lt;p&gt;Xeniax&lt;/p&gt;
    &lt;head rend="h5"&gt;Amateurfunk im All – Kontakt mit Fram2&lt;/head&gt;
    &lt;p&gt;akira25, flx, Gato&lt;/p&gt;
    &lt;head rend="h5"&gt;Über europäische Grenzen hinweg auf klinischen Daten rechnen - aber sicher!&lt;/head&gt;
    &lt;p&gt;Hendrik Ballhausen&lt;/p&gt;
    &lt;head rend="h5"&gt;CCC-Jahresrückblick&lt;/head&gt;
    &lt;p&gt;Constanze Kurz, khaleesi, Matthias Marx, Linus Neumann, erdgeist&lt;/p&gt;
    &lt;head rend="h5"&gt;Persist, resist, stitch&lt;/head&gt;
    &lt;p&gt;Philo&lt;/p&gt;
    &lt;head rend="h5"&gt;Lessons from Building an Open-Architecture Secure Element&lt;/head&gt;
    &lt;p&gt;Jan Pleskac&lt;/p&gt;
    &lt;head rend="h5"&gt;Auf die Dauer hilft nur Power: Herausforderungen für dezentrale Netzwerke aus Sicht der Soziologie&lt;/head&gt;
    &lt;p&gt;Marco Wähner&lt;/p&gt;
    &lt;head rend="h5"&gt;Current Drone Wars&lt;/head&gt;
    &lt;p&gt;Leonard&lt;/p&gt;
    &lt;head rend="h5"&gt;Variable Fonts — It Was Never About File Size&lt;/head&gt;
    &lt;p&gt;Bernd&lt;/p&gt;
    &lt;head rend="h5"&gt;A Quick Stop at the HostileShop&lt;/head&gt;
    &lt;p&gt;Mike Perry&lt;/p&gt;
    &lt;head rend="h5"&gt;In-house electronics manufacturing from scratch: How hard can it be?&lt;/head&gt;
    &lt;p&gt;Augustin Bielefeld, Alexander Willer&lt;/p&gt;
    &lt;head rend="h5"&gt;CPU Entwicklung in Factorio: Vom D-Flip-Flop bis zum eigenen Betriebssystem&lt;/head&gt;
    &lt;p&gt;PhD (Philipp)&lt;/p&gt;
    &lt;head rend="h5"&gt;Amtsgeheimnis raus, Datenhalde rein: was die Informationsfreiheit in Österreich bringt&lt;/head&gt;
    &lt;p&gt;Markus (fin) Hametner, Erwin Ernst "eest9" Steinhammer&lt;/p&gt;
    &lt;head rend="h5"&gt;How to render cloud FPGAs useless&lt;/head&gt;
    &lt;p&gt;Dirk&lt;/p&gt;
    &lt;head rend="h5"&gt;freiheit.exe - Utopien als Malware&lt;/head&gt;
    &lt;p&gt;Christiane Mudra&lt;/p&gt;
    &lt;head rend="h5"&gt;Recharge your batteries with us - an empowering journey through the energy transition&lt;/head&gt;
    &lt;p&gt;Salacidre, JulianeB&lt;/p&gt;
    &lt;head rend="h5"&gt;Prometheus: Reverse-Engineering Overwatch&lt;/head&gt;
    &lt;p&gt;breakingbread&lt;/p&gt;
    &lt;head rend="h5"&gt;Trump government demands access to European police databases and biometrics&lt;/head&gt;
    &lt;p&gt;Matthias Monroy&lt;/p&gt;
    &lt;head rend="h5"&gt;Verlorene Domains, offene Türen - Was alte Behördendomains verraten&lt;/head&gt;
    &lt;p&gt;Tim Philipp Schäfers (TPS)&lt;/p&gt;
    &lt;head rend="h5"&gt;CSS Clicker Training: Making games in a "styling" language&lt;/head&gt;
    &lt;p&gt;Lyra Rebane&lt;/p&gt;
    &lt;head rend="h5"&gt;Wie wir alte Flipperautomaten am Leben erhalten&lt;/head&gt;
    &lt;p&gt;Axel Böttcher&lt;/p&gt;
    &lt;head rend="h5"&gt;Power Cycles statt Burnout – Wie Einflussnahme nicht verpufft&lt;/head&gt;
    &lt;p&gt;Rahel Becker, Anna Kassautzki&lt;/p&gt;
    &lt;head rend="h5"&gt;Don’t look up: There are sensitive internal links in the clear on GEO satellites&lt;/head&gt;
    &lt;p&gt;Nadia Heninger, Annie Dai&lt;/p&gt;
    &lt;head rend="h5"&gt;Textiles 101: Fast Fiber Transform&lt;/head&gt;
    &lt;p&gt;octoprog&lt;/p&gt;
    &lt;head rend="h5"&gt;How To Minimize Bugs in Cryptography Code&lt;/head&gt;
    &lt;p&gt;Jade&lt;/p&gt;
    &lt;head rend="h5"&gt;Machine Vision – Vom Algorithmus zum Baumpilz im digitalen Metabolismus&lt;/head&gt;
    &lt;p&gt;Thomas Knüsel&lt;/p&gt;
    &lt;head rend="h5"&gt;Xous: A Pure-Rust Rethink of the Embedded Operating System&lt;/head&gt;
    &lt;p&gt;bunnie, Sean "xobs" Cross&lt;/p&gt;
    &lt;head rend="h5"&gt;51 Ways to Spell the Image Giraffe: The Hidden Politics of Token Languages in Generative AI&lt;/head&gt;
    &lt;p&gt;Ting-Chun Liu, Leon-Etienne Kühr&lt;/p&gt;
    &lt;head rend="h5"&gt;When Vibe Scammers Met Vibe Hackers: Pwning PhaaS with Their Own Weapons&lt;/head&gt;
    &lt;p&gt;Chiao-Lin Yu (Steven Meow)&lt;/p&gt;
    &lt;head rend="h5"&gt;The Maybe Talent Show&lt;/head&gt;
    &lt;p&gt;Norman Müller-Schmitz, lukas-schmukas, James Bonne d'age&lt;/p&gt;
    &lt;head rend="h5"&gt;Code to Craft: Procedural Generation for the Physical World&lt;/head&gt;
    &lt;p&gt;bleeptrack&lt;/p&gt;
    &lt;head rend="h5"&gt;Reverse engineering the Pixel TitanM2 firmware&lt;/head&gt;
    &lt;p&gt;willem&lt;/p&gt;
    &lt;head rend="h5"&gt;The Small Packet of Bits That Can Save (or Destabilize) a City&lt;/head&gt;
    &lt;p&gt;Manuel Rábade&lt;/p&gt;
    &lt;head rend="h5"&gt;GPTDash – Der Reverse-Turing-Test&lt;/head&gt;
    &lt;p&gt;Benny, Kilian, BratscherBen&lt;/p&gt;
    &lt;head rend="h2"&gt;Mon - Day 3 - December 29&lt;/head&gt;
    &lt;head rend="h5"&gt;Azubi-Tag Einführung&lt;/head&gt;
    &lt;head rend="h5"&gt;Greenhouse Gas Emission Data: Public, difficult to access, and not always correct&lt;/head&gt;
    &lt;p&gt;Hanno Böck&lt;/p&gt;
    &lt;head rend="h5"&gt;Design for 3D-Printing&lt;/head&gt;
    &lt;p&gt;rahix&lt;/p&gt;
    &lt;head rend="h5"&gt;Lightning Talks - Tag 3&lt;/head&gt;
    &lt;p&gt;bonnie, Gilbert, Andi Bräu&lt;/p&gt;
    &lt;head rend="h5"&gt;The Museum of Care: Open-Source Survival Kit Collection&lt;/head&gt;
    &lt;p&gt;Nika Dubrovsky&lt;/p&gt;
    &lt;head rend="h5"&gt;Celestial navigation with very little math&lt;/head&gt;
    &lt;p&gt;Trammell Hudson&lt;/p&gt;
    &lt;head rend="h5"&gt;a media-almost-archaeology on data that is too dirty for "AI"&lt;/head&gt;
    &lt;p&gt;jiawen uffline&lt;/p&gt;
    &lt;head rend="h5"&gt;Hacking Karlsruhe - 10 years later&lt;/head&gt;
    &lt;p&gt;Jürgen Bering&lt;/p&gt;
    &lt;head rend="h5"&gt;What Makes Bike-Sharing Work? Insights from 43 Million Kilometers of European Cycling Data&lt;/head&gt;
    &lt;p&gt;Martin Lellep, Georg Balke, FelixW&lt;/p&gt;
    &lt;head rend="h5"&gt;Teckids – eine verstehbare (digitale) Welt&lt;/head&gt;
    &lt;p&gt;Keno, Darius Auding&lt;/p&gt;
    &lt;head rend="h5"&gt;BE Modded: Exploring and hacking the Vital Bracelet ecosystem&lt;/head&gt;
    &lt;p&gt;cyanic&lt;/p&gt;
    &lt;head rend="h5"&gt;Wer hat Angst vor dem Neutralitätsgebot?&lt;/head&gt;
    &lt;p&gt;Hannah Vos, Vivian Kube&lt;/p&gt;
    &lt;head rend="h5"&gt;Shit for Future: turning human shit into a climate solution&lt;/head&gt;
    &lt;p&gt;Elena&lt;/p&gt;
    &lt;head rend="h5"&gt;Watch Your Kids: Inside a Children's Smartwatch&lt;/head&gt;
    &lt;p&gt;Nils Rollshausen&lt;/p&gt;
    &lt;head rend="h5"&gt;When 8 Bits is Overkill: Making Blinkenlights with a 1-bit CPU&lt;/head&gt;
    &lt;p&gt;girst (Tobi)&lt;/p&gt;
    &lt;head rend="h5"&gt;Supplements und Social Media – wenn der Online-Hype zur realen Gesundheitsgefahr wird&lt;/head&gt;
    &lt;p&gt;Christoph Wiedmer&lt;/p&gt;
    &lt;head rend="h5"&gt;Programmierte Kriegsverbrechen? Über KI-Systeme im Kriegseinsatz in Gaza und warum IT-Fachleute sich dazu äußern müssen&lt;/head&gt;
    &lt;p&gt;Rainer Rehak&lt;/p&gt;
    &lt;head rend="h5"&gt;Making the Magic Leap past NVIDIA's secure bootchain and breaking some Tesla Autopilots along the way&lt;/head&gt;
    &lt;p&gt;EliseZeroTwo&lt;/p&gt;
    &lt;head rend="h5"&gt;Learning from South Korean Telco Breaches&lt;/head&gt;
    &lt;p&gt;Shinjo "peremen" Park, Yonghyu "perillamint" Ban&lt;/p&gt;
    &lt;head rend="h5"&gt;Gegenmacht - Best of Informationsfreiheit&lt;/head&gt;
    &lt;p&gt;Arne Semsrott&lt;/p&gt;
    &lt;head rend="h5"&gt;There is NO WAY we ended up getting arrested for this (Malta edition)&lt;/head&gt;
    &lt;p&gt;mixy1, Luke Bjorn Scerri, girogio&lt;/p&gt;
    &lt;head rend="h5"&gt;APT Down and the mystery of the burning data centers&lt;/head&gt;
    &lt;p&gt;Christopher Kunz, Sylvester&lt;/p&gt;
    &lt;head rend="h5"&gt;Von wegen Eisblumen! Wie man mit Code, Satelliten und Schiffsexpeditionen die bunte Welt des arktischen Phytoplanktons sichtbar macht&lt;/head&gt;
    &lt;p&gt;Moritz Zeising (er/he)&lt;/p&gt;
    &lt;head rend="h5"&gt;Schlechte Karten - IT-Sicherheit im Jahr null der ePA für alle&lt;/head&gt;
    &lt;p&gt;Bianca Kastl&lt;/p&gt;
    &lt;head rend="h5"&gt;Set-top box Hacking: freeing the 'Freebox'&lt;/head&gt;
    &lt;p&gt;Frédéric Hoguin&lt;/p&gt;
    &lt;head rend="h5"&gt;Wer liegt hier wem auf der Tasche? Genug mit dem Bürgergeld-Fetisch. Stürmt die Paläste!&lt;/head&gt;
    &lt;p&gt;Helena Steinhaus&lt;/p&gt;
    &lt;head rend="h5"&gt;The Last of Us - Fighting the EU Surveillance Law Apocalypse&lt;/head&gt;
    &lt;p&gt;Svea Windwehr, Chloé Berthélémy&lt;/p&gt;
    &lt;head rend="h5"&gt;AI Agent, AI Spy&lt;/head&gt;
    &lt;p&gt;Udbhav Tiwari, Meredith Whittaker&lt;/p&gt;
    &lt;head rend="h5"&gt;Build a Fake Phone, Find Real Bugs: Qualcomm GPU Emulation and Fuzzing with LibAFL QEMU&lt;/head&gt;
    &lt;p&gt;Romain Malmain, Scott Bauer&lt;/p&gt;
    &lt;head rend="h5"&gt;Transkultureller Hack auf die klassische Musikszene – Vortrag und Konzert&lt;/head&gt;
    &lt;p&gt;Johanna-Leonore Dahlhoff, Neina Doroshenko, Peter Klohmann, Alireza Meghrazi Solouklou, Mirweis Neda, Maria Carolina Pardo Reyes, Eduardo Sabella, Sarah Luisa Wurmer&lt;/p&gt;
    &lt;head rend="h5"&gt;Netzpolitik in der Schweiz: Zwischen Bodensee und Matterhorn&lt;/head&gt;
    &lt;p&gt;Kire, Rahel&lt;/p&gt;
    &lt;head rend="h5"&gt;The Angry Path to Zen: AMD Zen Microcode Tools and Insights&lt;/head&gt;
    &lt;p&gt;Benjamin Kollenda&lt;/p&gt;
    &lt;head rend="h5"&gt;Blackbox Palantir&lt;/head&gt;
    &lt;p&gt;Constanze Kurz, Franziska Görlitz&lt;/p&gt;
    &lt;head rend="h5"&gt;Aber hier Leben? Nein danke! …oder doch? Wie wir der autoritären Zuspitzung begegnen können.&lt;/head&gt;
    &lt;p&gt;Jaša Hiergeblieben, Lisa Zugezogen&lt;/p&gt;
    &lt;head rend="h5"&gt;Race conditions, transactions and free parking&lt;/head&gt;
    &lt;p&gt;Benjamin W. Broersma&lt;/p&gt;
    &lt;head rend="h5"&gt;Hegemony Eroding: Excavating Diversity in Latent Space&lt;/head&gt;
    &lt;p&gt;Karim Hamdi&lt;/p&gt;
    &lt;head rend="h5"&gt;10 years of Dieselgate&lt;/head&gt;
    &lt;p&gt;Felix Domke, Karsten Burger&lt;/p&gt;
    &lt;head rend="h5"&gt;The Heartbreak Machine: Nazis in the Echo Chamber&lt;/head&gt;
    &lt;p&gt;Martha Root, Eva Hoffmann, Christian Fuchs&lt;/p&gt;
    &lt;head rend="h5"&gt;Light in the Dark(net)&lt;/head&gt;
    &lt;p&gt;Tobias Höller&lt;/p&gt;
    &lt;head rend="h5"&gt;The Spectrum - Hackspace Beyond Hacking&lt;/head&gt;
    &lt;p&gt;sjaelv, MultisampledNight&lt;/p&gt;
    &lt;head rend="h5"&gt;Rowhammer in the Wild: Large-Scale Insights from FlippyR.AM&lt;/head&gt;
    &lt;p&gt;Martin Heckel, Florian Adamsky, Daniel Gruss&lt;/p&gt;
    &lt;head rend="h5"&gt;Peep-Show für die Polizei. Staatliche Überwachung von Queers in Hamburger Toiletten bis 1980&lt;/head&gt;
    &lt;p&gt;Simon Schultz&lt;/p&gt;
    &lt;head rend="h5"&gt;Human microservices at the Dutch Railways: modern architecture, ancient hardware?&lt;/head&gt;
    &lt;p&gt;Maarten W&lt;/p&gt;
    &lt;head rend="h5"&gt;Von Fuzzern zu Agenten: Entwicklung eines Cyber Reasoning Systems für die AIxCC&lt;/head&gt;
    &lt;p&gt;Mischa Meier (mmisc), Annika Kuntze&lt;/p&gt;
    &lt;head rend="h5"&gt;PRÜF&lt;/head&gt;
    &lt;p&gt;Nico Semsrott&lt;/p&gt;
    &lt;head rend="h5"&gt;Verschlüsselung brechen durch physischen Zugriff - Smartphone Beschlagnahme durch Polizei&lt;/head&gt;
    &lt;p&gt;Davy Wang, Viktor Schlüter&lt;/p&gt;
    &lt;head rend="h5"&gt;Spectre in the real world: Leaking your private data from the cloud with CPU vulnerabilities&lt;/head&gt;
    &lt;p&gt;Thijs Raymakers&lt;/p&gt;
    &lt;head rend="h5"&gt;Die große Datenschutz-, Datenpannen- und DS-GVO-Show&lt;/head&gt;
    &lt;p&gt;Alvar C.H. Freude&lt;/p&gt;
    &lt;head rend="h2"&gt;Tue - Day 4 - December 30&lt;/head&gt;
    &lt;head rend="h5"&gt;Asahi Linux - Porting Linux to Apple Silicon&lt;/head&gt;
    &lt;p&gt;sven&lt;/p&gt;
    &lt;head rend="h5"&gt;Atoms in Space&lt;/head&gt;
    &lt;p&gt;manuel&lt;/p&gt;
    &lt;head rend="h5"&gt;I Hated All The Cross-Stitch Software So I Made My Own: My Deranged Outsider Software Suite For Making Deranged Outsider Art&lt;/head&gt;
    &lt;p&gt;yomimono&lt;/p&gt;
    &lt;head rend="h5"&gt;How to keep Open Source open without leaving our communities open to threats&lt;/head&gt;
    &lt;p&gt;Quintessence&lt;/p&gt;
    &lt;head rend="h5"&gt;CCC&amp;amp;T - Cosmic ray, the Climate Catastrophe and Trains.&lt;/head&gt;
    &lt;p&gt;FantasticMisterFux, louiT&lt;/p&gt;
    &lt;head rend="h5"&gt;CUII: Wie Konzerne heimlich Webseiten in Deutschland sperren&lt;/head&gt;
    &lt;p&gt;Lina Lastname, Elias Zeidler (Northernside)&lt;/p&gt;
    &lt;head rend="h5"&gt;“End Of 10”: How the FOSS Community is Combatting Software-Driven Resource and Energy Consumption&lt;/head&gt;
    &lt;p&gt;Joseph P. De Veaugh-Geiss, Carolina Silva Rode, belobe&lt;/p&gt;
    &lt;head rend="h5"&gt;What You Hack Is What You Mean: 35 Years of Wiring Sense into Text&lt;/head&gt;
    &lt;p&gt;Torsten Roeder&lt;/p&gt;
    &lt;head rend="h5"&gt;Security of Cardiac Implantable Electronic Devices&lt;/head&gt;
    &lt;p&gt;dilucide&lt;/p&gt;
    &lt;head rend="h5"&gt;Who runs the www? WSIS+20 and the future of Internet governance&lt;/head&gt;
    &lt;p&gt;Sophia Longwe&lt;/p&gt;
    &lt;head rend="h5"&gt;Fossile Industrie liebt KI!&lt;/head&gt;
    &lt;p&gt;Stefan, Yannik &amp;amp; Rike, Moritz&lt;/p&gt;
    &lt;head rend="h5"&gt;Laser Beams &amp;amp; Light Streams: Letting Hackers Go Pew Pew, Building Affordable Light-Based Hardware Security Tooling&lt;/head&gt;
    &lt;p&gt;Patch, Sam. Beaumont (PANTH13R)&lt;/p&gt;
    &lt;head rend="h5"&gt;Breaking BOTS: Cheating at Blue Team CTFs with AI Speed-Runs&lt;/head&gt;
    &lt;p&gt;Leo Meyerovich, Sindre Breda&lt;/p&gt;
    &lt;head rend="h5"&gt;Von Groschen und SpurLos - GNU Taler auch auf eurem Event!&lt;/head&gt;
    &lt;p&gt;Mikolai Gütschow, signum&lt;/p&gt;
    &lt;head rend="h5"&gt;We, the EU, and 1064 Danes decided to look into YouTube: A story about how the EU gave us a law, 1064 Danes gave us their YouTube histories, and reality gave us a headache&lt;/head&gt;
    &lt;p&gt;David, LK Seiling&lt;/p&gt;
    &lt;head rend="h5"&gt;Battling Obsolescence – Keeping an 80s laser tag system alive&lt;/head&gt;
    &lt;p&gt;Trikkitt&lt;/p&gt;
    &lt;head rend="h5"&gt;Security Nightmares&lt;/head&gt;
    &lt;p&gt;Constanze Kurz, Ron&lt;/p&gt;
    &lt;head rend="h5"&gt;Infrastructure Review&lt;/head&gt;
    &lt;p&gt;nicoduck&lt;/p&gt;
    &lt;head rend="h5"&gt;Closing Ceremony&lt;/head&gt;
    &lt;p&gt;Stella, pajowu&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/"/><published>2025-12-25T18:40:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46387657</id><title>Maybe the default settings are too high</title><updated>2025-12-26T15:39:49.832297+00:00</updated><content>&lt;doc fingerprint="e65b52868faaab9c"&gt;
  &lt;main&gt;
    &lt;p&gt;I’ve been reading Lord of the Rings for two months and I’m just at the end of the first part. It’s not because I’m not enjoying it. It’s one of the most enjoyable reading experiences I can remember.&lt;/p&gt;
    &lt;p&gt;From the beginning, I’ve read the whole thing aloud. I’ve found reading aloud helpful for staying engaged — limiting myself to mouth-speed rather than eye-speed means I won’t rush, miss important details, and then lose interest, which has always been a problem for me.&lt;/p&gt;
    &lt;p&gt;At first I was anxious to read a 1,500-page book this way, because it would take so long. But, as someone pointed out to me, if I’m enjoying it, why would I want to be done with it sooner?&lt;/p&gt;
    &lt;p&gt;So I tried slowing down even more, and discovered something. I slowed to a pace that felt almost absurd, treating each sentence as though it might be a particularly important one. I gave each one maybe triple the usual time and attention, ignoring the fact that there are hundreds of pages to go.&lt;/p&gt;
    &lt;p&gt;This leisurely pace made Middle-Earth blossom before my eyes. When I paused after each comma, and let each sentence ring for a small moment after the period, the events of the story reached me with more weight and strength. That extra time gave space for Tolkien’s images and moods to propagate in my mind, which they did automatically.&lt;/p&gt;
    &lt;p&gt;Some part of me still wanted to rush and get on with it, to make good time, to gloss over the songs and lore to get to Moria and Mount Doom and the other marquee moments of the story. But the more I ignored that impulse, the better the experience got.&lt;/p&gt;
    &lt;p&gt;By offering the book about triple the usual amount of attentiveness, I was getting about triple the storyness (i.e. meaning, engagement, literary pleasure). Whatever the thing is that I’m seeking when I pick up a novel in the first place, there’s much more of it available at this pace.&lt;/p&gt;
    &lt;head rend="h3"&gt;Eating Comprehension&lt;/head&gt;
    &lt;p&gt;This effect reminded me of a paradox around eating I recognized long ago. When you slow down your eating speed, say to half or a third your default speed, you get much more enjoyment out of a smaller amount of food. The extra attention given to each bite allows more of the “good stuff,” whatever that is exactly, to reach you.&lt;/p&gt;
    &lt;p&gt;What’s paradoxical is that it’s precisely the seeking of that “good stuff” that normally drives me to eat so quickly, and miss most of what I’m seeking. When you try to barrel ahead to access the good stuff quicker, you get less of it in the end. Slow down and much more of it is released.&lt;/p&gt;
    &lt;p&gt;And it’s released automatically, in both reading and eating. You don’t have to search it out. The good stuff (the meaning in the text, the pleasure in the eating) just rises up to meet you in that extra time you give it. Slowing down, and offering more time to the act of consumption, immediately increases reading comprehension (and eating comprehension).&lt;/p&gt;
    &lt;p&gt;Both are analogous to slowing down while you vacuum a carpet. If you pass the vacuum head too quickly, you miss half the dirt. Slow down, and you can hear how much more grit is sent skittering up the tube. The suction and bristles are working, but they need more time to do their work fully, to draw up the deeper-lying stuff.&lt;/p&gt;
    &lt;head rend="h3"&gt;Question the default settings&lt;/head&gt;
    &lt;p&gt;It seems that my default consumption speeds for reading and eating (and maybe everything else) reduce the rewards of those things significantly, undermining the point of doing either.&lt;/p&gt;
    &lt;p&gt;Part of it is my own impatience. But I also suspect that modern living, with its infinite supply of consumables, tends to push our rate-of-intake dials too high. I’m not going to run out of books, or snacks, or opportunities to learn something. There’s always more, so not every crust of bread or printed page needs to be appreciated fully.&lt;/p&gt;
    &lt;p&gt;Internally though, the mind is juggling like Lucy and Ethel on the conveyor belt at the chocolate factory. Our receptors for meaning and appreciation, like the vacuum head, need more time to do their full work, to make all the connections they’re designed to make.&lt;/p&gt;
    &lt;p&gt;It might sound like I’m just offering clichés – less is more, stop and smell the roses, take your time – and I guess I am. But clichés suffer the same issue: they are often profound insights, consumed and passed on too rapidly for their real meaning to register anymore. You really should stop and smell roses, as you know if you’re in the habit of doing that.&lt;/p&gt;
    &lt;p&gt;At least see what happens when you reduce your consumption speed – of anything, but especially books, information, and food – by a half, or two thirds. Notice that (1) something in you really wants to plow through at the highest viable setting, and (2) how much more of the reward is released when you slow down anyway.&lt;/p&gt;
    &lt;p&gt;As far as I can tell, almost everything becomes more satisfying when you give it more time and intention, even things like checking the mailbox or writing a shopping list.&lt;/p&gt;
    &lt;head rend="h3"&gt;Speed alters taste&lt;/head&gt;
    &lt;p&gt;Slowing down your rate of consumption will inevitably change what you want to consume. Reading throwaway news articles or AI slop with great care and attention is only going to show you how empty of value it is. Reading dense writing in inky old books, crafted for your mind by great masters, becomes easier without the rushed pace, and the meaning just blooms out of it.&lt;/p&gt;
    &lt;p&gt;Same with food. Try to savor a cheap, waxy “chocolate” bar, or a bag of store-brand cheese puffs, and you discover a harsh taste that you don’t want to look at too closely. Enjoy a homemade pastry with great attention, and discover there’s even more in it than you realized.&lt;/p&gt;
    &lt;p&gt;Mass production is good in so many ways, but the faster we tend to consume its fruits, the more we end up seeking things for their glossy, candied surfaces. The more we go for these surface-level rewards, the more the culture focuses on offering only that part – such as TikTok videos, processed food, CGI-forward movies, and public discourse in the form of unexamined talking points.&lt;/p&gt;
    &lt;p&gt;Who knows how far we’ve drifted from the best modes of consuming the things we value. Once something becomes a norm, it seems like an appropriate standard, no matter how much has been lost. Apparently, reading silently and alone was unusual until as late as the 18th century. Certainly sit-down meals and cooking at home were.&lt;/p&gt;
    &lt;p&gt;I don’t mean to sound like a scold. Let’s say none of this is morally good or bad. It’s just that in so much of what we do, we could be getting much more of the part of it that we really seek — but it’s only available at slower speeds.&lt;/p&gt;
    &lt;p&gt;If you’re curious, try consuming things more slowly, so slowly it seems silly to others — say a third your habitual speed — and see what rises up to meet you.&lt;/p&gt;
    &lt;p&gt;***&lt;/p&gt;
    &lt;head rend="h2"&gt;Want to quit something in January?&lt;/head&gt;
    &lt;p&gt;Recently I opened a discussion forum for Raptitude readers who want to give something up for the month of December (alcohol, social media, snacks, etc).&lt;/p&gt;
    &lt;p&gt;It’s been a real success, and many people want to do something similar in January. If you want to quit something, or just give it up for a month, you’re invited to join.&lt;/p&gt;
    &lt;p&gt;Follow this link at the end of this post to get an invite.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.raptitude.com/2025/12/maybe-the-default-settings-are-too-high/"/><published>2025-12-25T23:13:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46388213</id><title>MiniMax M2.1: Built for Real-World Complex Tasks, Multi-Language Programming</title><updated>2025-12-26T15:39:47.409273+00:00</updated><content>&lt;doc fingerprint="7a878369f356732f"&gt;
  &lt;main&gt;
    &lt;p&gt;在10月底的M2中，我们主要解决模型成本和模型开放性的问题。在M2.1中，我们致力于提升真实世界复杂任务中的表现：重点聚焦于更多编程语言和办公场景的可用性，并在这个领域做到最好的水平。&lt;/p&gt;
    &lt;p&gt;MiniMax M2.1 具体模型亮点如下:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;卓越多编程语言能力&lt;/p&gt;
        &lt;p&gt;过去很多模型主要围绕 Python 优化, 但真实世界的系统往往是多语言协作的结果。&lt;/p&gt;
        &lt;p&gt;在 M2.1 中, 我们系统性提升了 Rust / Java / Golang / C++ / Kotlin / Objective-C / TypeScript / JavaScript 等语言的能力, 多语言任务整体表现达到业内领先水平, 覆盖从底层系统到应用层开发的完整链路。&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WebDev 与 AppDev：能力与美学的整体跃迁&lt;/p&gt;
        &lt;p&gt;针对业界普遍存在的移动端开发短板, M2.1 显著加强了原生 Android / iOS 开发能力。&lt;/p&gt;
        &lt;p&gt;同时, 我们系统性提升了模型在 Web 与 App 场景中的设计理解与美学表达能力, 能够出色地构建复杂交互、3D科学场景模拟与高质量可视化表达, 推动 vibe coding 成为可持续、可交付的生产实践。&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;复合指令约束提升，办公场景变为可能&lt;/p&gt;
        &lt;p&gt;作为开源模型中率先系统性引入 Interleaved Thinking 的模型系列, M2.1 systematic problem-solving 能力再次升级。&lt;/p&gt;
        &lt;p&gt;模型不仅关注代码执行是否正确, 同时关注模型对“复合指令约束”的整合执行能力, 在真实办公场景具备更高的可用性。&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;更简洁高效的回复&lt;/p&gt;
        &lt;p&gt;相比 M2, MiniMax-M2.1 的模型回复以及思维链更加简洁, 在实际编程与交互体验中, 响应速度显著提升, Token 消耗明显下降, 在 AI Coding与Agent驱动的连续工作流中更加流畅和高效。&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;出色的 Agent / 工具脚手架泛化能力&lt;/p&gt;
        &lt;p&gt;M2.1 在各类编程工具与 Agent 框架中均有出色表现。在 Claude Code、Droid (Factory AI)、Cline、Kilo Code、Roo Code、BlackBox 等工具中展现一致且稳定的效果, 并对 Skill.md、Claude.md / agent.md / cursorrule、Slash Command 等 Context Management机制提供可靠支持。&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;高质量对话和写作&lt;/p&gt;
        &lt;p&gt;M2.1 不再只是“代码能力更强”, 在日常对话、技术说明与写作场景中, 也能提供更具细节与结构性的回答。&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;基准测试概览&lt;/head&gt;
    &lt;p&gt;MiniMax-M2.1 在 VIBE 综合榜单中表现卓越，以平均 88.6 分的成绩展现了接近Claude Opus 4.5的全栈构建能力，并在几乎所有子集上都显著优于Claude Sonnet 4.5。&lt;/p&gt;
    &lt;head rend="h3"&gt;使用者评价&lt;/head&gt;
    &lt;p&gt;我们非常期待像 M2.1 这样强大的开源模型，它在各类软件开发任务中都能带来前沿水准的表现，甚至还能在部分场景下比头部闭源模型更好。开发者应当拥有选择权，而 M2.1 正是大家急需的那个优质选项！&lt;/p&gt;
    &lt;p&gt;Eno Reyes&lt;/p&gt;
    &lt;p&gt;Co-Founder, CTO of Factory&lt;/p&gt;
    &lt;p&gt;MiniMax M2.1 在可读性与惯用结构方面与生产级工程要求高度契合，在 Go、Rust、C++ 等多语言场景下均表现稳定。精炼的交错推理机制显著压缩逻辑路径，减少冗余步骤，让多文件重构与缺陷修复等复杂任务得以更高精度完成。更可贵的是，M2.1 在激活参数量受限的前提下仍能提供可靠性能，为大规模智能体编码流程提供了兼顾效能与资源利用的均衡方案。我们期待与 MiniMax 团队展开持续、紧密的合作，在 Fireworks 平台同步支持其最新创新成果！&lt;/p&gt;
    &lt;p&gt;Benny Chen&lt;/p&gt;
    &lt;p&gt;Co-Founder of Fireworks&lt;/p&gt;
    &lt;p&gt;MiniMax M2 系列在代码生成能力上表现突出，过去几个月已迅速跻身 Cline 平台最受欢迎的模型之列。M2.1 再次实现能力层面的显著跃升，我们期待与 MiniMax 团队继续深化合作，共同推进 AI 编码技术的演进。&lt;/p&gt;
    &lt;p&gt;Saoud Rizwan&lt;/p&gt;
    &lt;p&gt;Founder, CEO of Cline&lt;/p&gt;
    &lt;p&gt;我们对M2.1的发布而兴奋！我们的用户已经离不开MiniMax提供的最优秀的编程辅助能力和高性价比，内测显示，M2.1在架构设计、服务编排、代码评审直至部署上线的全链路环节中均表现优异，速度与资源效率均处于领先水平。&lt;/p&gt;
    &lt;p&gt;Scott Breitenother&lt;/p&gt;
    &lt;p&gt;Co-Founder, CEO of Kilo&lt;/p&gt;
    &lt;p&gt;我们的用户非常喜欢 MiniMax M2 在编码能力与效率方面的表现。最新发布的 M2.1 在此基础上实现了速度与可靠性的实质性提升，并在更多语言及框架中保持稳定输出。对于强调高吞吐、Agentic Coding且对速度与成本敏感的研发流程，M2.1 是稳妥且具性价比的选择。&lt;/p&gt;
    &lt;p&gt;Matt Rubens&lt;/p&gt;
    &lt;p&gt;Co-Founder, CEO of RooCode&lt;/p&gt;
    &lt;head rend="h2"&gt;Showcases&lt;/head&gt;
    &lt;head rend="h2"&gt;物理世界Agent&lt;/head&gt;
    &lt;head rend="h2"&gt;多语言 Coding&lt;/head&gt;
    &lt;head rend="h2"&gt;Agentic Tool Use&lt;/head&gt;
    &lt;head rend="h2"&gt;数字员工&lt;/head&gt;
    &lt;p&gt;以下效果演示是 M2.1 在 AgentCompany Benchmark 中的行为轨迹记录。&lt;/p&gt;
    &lt;head rend="h2"&gt;全链路办公自动化&lt;/head&gt;
    &lt;head rend="h2"&gt;如何使用&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MiniMax-M2.1 API 已在 MiniMax开放平台 开放使用：https://platform.minimaxi.com/docs/guides/text-generation&lt;/item&gt;
      &lt;item&gt;基于 MiniMax-M2.1 的通用 Agent 产品 MiniMax Agent 现已全面开放使用：https://agent.minimaxi.com/&lt;/item&gt;
      &lt;item&gt; 开源以及本地部署使用： https://huggingface.co/MiniMaxAI/MiniMax-M2.1 &lt;lb/&gt;https://github.com/MiniMax-AI/MiniMax-M2.1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;为了方便用户使用, 我们提供了两个版本的 API, M2.1 和 M2.1-lightning。这两个 API 结果完全一样, 但是后者速度更快, 方便对 TPS 有需求的用户来使用。同时, 在 M2 手动 Cache 的基础上, M2.1 全面支持自动 Cache, 无需设置, 自动生效, 为开发者带来更流畅的体验、更低的成本与更优的延时表现。&lt;/p&gt;
    &lt;p&gt;我们在 Coding Plan 里面会根据资源负载给用户提供大比例的 M2.1-lightning, 并保持 Coding Plan 的价格不变。也就是说, Coding Plan 用户免费获得了大部分时间更快的推理速度。欢迎大家点击下单~&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.minimaxi.com/news/minimax-m21"/><published>2025-12-26T01:02:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46388907</id><title>TurboDiffusion: 100–200× Acceleration for Video Diffusion Models</title><updated>2025-12-26T15:39:46.921545+00:00</updated><content>&lt;doc fingerprint="f4d4ba193592fac2"&gt;
  &lt;main&gt;
    &lt;p&gt;This repository provides the official implementation of TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by &lt;lb/&gt; TurboDiffusion primarily uses SageAttention, SLA (Sparse-Linear Attention) for attention acceleration, and rCM for timestep distillation.&lt;/p&gt;
    &lt;p&gt;Paper: TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times&lt;/p&gt;
    &lt;p&gt;Note: the checkpoints and paper are not finalized, and will be updated later to improve quality.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Checkpoint Link&lt;/cell&gt;
        &lt;cell role="head"&gt;Best Resolution&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;TurboWan2.2-I2V-A14B-720P&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Huggingface Model&lt;/cell&gt;
        &lt;cell&gt;720p&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;TurboWan2.1-T2V-1.3B-480P&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Huggingface Model&lt;/cell&gt;
        &lt;cell&gt;480p&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;TurboWan2.1-T2V-14B-480P&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Huggingface Model&lt;/cell&gt;
        &lt;cell&gt;480p&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;TurboWan2.1-T2V-14B-720P&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Huggingface Model&lt;/cell&gt;
        &lt;cell&gt;720p&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: All checkpoints support generating videos at 480p or 720p. The "Best Resolution" column indicates the resolution at which the model provides the best video quality.&lt;/p&gt;
    &lt;p&gt;Base environment: &lt;code&gt;python&amp;gt;=3.9&lt;/code&gt;, &lt;code&gt;torch&amp;gt;=2.7.0&lt;/code&gt;. &lt;code&gt;torch==2.8.0&lt;/code&gt; is recommended, as higher versions may cause OOM.&lt;/p&gt;
    &lt;p&gt;Install TurboDiffusion by pip:&lt;/p&gt;
    &lt;code&gt;conda create -n turbodiffusion python=3.12
conda activate turbodiffusion

pip install turbodiffusion --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Or compile from source:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/thu-ml/TurboDiffusion.git
cd TurboDiffusion
git submodule update --init --recursive
pip install -e . --no-build-isolation&lt;/code&gt;
    &lt;p&gt;To enable SageSLA, a fast SLA forward pass based on SageAttention, install SpargeAttn first:&lt;/p&gt;
    &lt;code&gt;pip install git+https://github.com/thu-ml/SpargeAttn.git --no-build-isolation&lt;/code&gt;
    &lt;p&gt;For GPUs with more than 40GB of GPU memory, e.g., H100, please use the unquantized checkpoints (without &lt;code&gt;-quant&lt;/code&gt;) and remove &lt;code&gt;--quant_linear&lt;/code&gt; from the command. For RTX 5090, RTX 4090, or similar GPUs, please use the quantized checkpoints (with &lt;code&gt;-quant&lt;/code&gt;) and add &lt;code&gt;--quant_linear&lt;/code&gt; in the command.)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Download the VAE (applicable for both Wan2.1 and Wan2.2) and umT5 text encoder checkpoints:&lt;/p&gt;
        &lt;code&gt;mkdir checkpoints cd checkpoints wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/Wan2.1_VAE.pth wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/models_t5_umt5-xxl-enc-bf16.pth&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Download our quantized model checkpoints (For RTX 5090 or similar GPUs):&lt;/p&gt;
        &lt;quote&gt;# For Wan2.1-T2V-1.3B wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P-quant.pth # For Wan2.2-I2V-14B wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P-quant.pth wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P-quant.pth&lt;/quote&gt;
        &lt;p&gt;Or download our unquantized model checkpoints (For H100 or similar GPUs):&lt;/p&gt;
        &lt;quote&gt;# For Wan2.1-T2V-1.3B wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P.pth # For Wan2.2-I2V-14B wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P.pth wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P.pth&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use the inference script for the T2V models:&lt;/p&gt;
        &lt;quote&gt;export PYTHONPATH=turbodiffusion # Arguments: # --dit_path Path to the finetuned TurboDiffusion checkpoint # --model Model to use: Wan2.1-1.3B or Wan2.1-14B (default: Wan2.1-1.3B) # --num_samples Number of videos to generate (default: 1) # --num_steps Sampling steps, 1–4 (default: 4) # --sigma_max Initial sigma for rCM (default: 80); larger choices (e.g., 1600) reduce diversity but may enhance quality # --vae_path Path to Wan2.1 VAE (default: checkpoints/Wan2.1_VAE.pth) # --text_encoder_path Path to umT5 text encoder (default: checkpoints/models_t5_umt5-xxl-enc-bf16.pth) # --num_frames Number of frames to generate (default: 81) # --prompt Text prompt for video generation # --resolution Output resolution: "480p" or "720p" (default: 480p) # --aspect_ratio Aspect ratio in W:H format (default: 16:9) # --seed Random seed for reproducibility (default: 0) # --save_path Output file path including extension (default: output/generated_video.mp4) # --attention_type Attention module to use: original, sla or sagesla (default: sagesla) # --sla_topk Top-k ratio for SLA/SageSLA attention (default: 0.1), we recommend using 0.15 for better video quality # --quant_linear Enable quantization for linear layers, pass this if using a quantized checkpoint # --default_norm Use the original LayerNorm and RMSNorm of Wan models python turbodiffusion/inference/wan2.1_t2v_infer.py \ --model Wan2.1-1.3B \ --dit_path checkpoints/TurboWan2.1-T2V-1.3B-480P-quant.pth \ --resolution 480p \ --prompt "A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about." \ --num_samples 1 \ --num_steps 4 \ --quant_linear \ --attention_type sagesla \ --sla_topk 0.1&lt;/quote&gt;
        &lt;p&gt;Or the script for the I2V model:&lt;/p&gt;
        &lt;quote&gt;export PYTHONPATH=turbodiffusion # --image_path Path to the input image # --high_noise_model_path Path to the high noise TurboDiffusion checkpoint # --low_noise_model_path Path to the high noise TurboDiffusion checkpoint # --boundary Timestep boundary for switching from high to low noise model (default: 0.9) # --model Model to use: Wan2.2-A14B (default: Wan2.2-A14B) # --num_samples Number of videos to generate (default: 1) # --num_steps Sampling steps, 1–4 (default: 4) # --sigma_max Initial sigma for rCM (default: 200); larger choices (e.g., 1600) reduce diversity but may enhance quality # --vae_path Path to Wan2.2 VAE (default: checkpoints/Wan2.2_VAE.pth) # --text_encoder_path Path to umT5 text encoder (default: checkpoints/models_t5_umt5-xxl-enc-bf16.pth) # --num_frames Number of frames to generate (default: 81) # --prompt Text prompt for video generation # --resolution Output resolution: "480p" or "720p" (default: 720p) # --aspect_ratio Aspect ratio in W:H format (default: 16:9) # --adaptive_resolution Enable adaptive resolution based on input image size # --ode Use ODE for sampling (sharper but less robust than SDE) # --seed Random seed for reproducibility (default: 0) # --save_path Output file path including extension (default: output/generated_video.mp4) # --attention_type Attention module to use: original, sla or sagesla (default: sagesla) # --sla_topk Top-k ratio for SLA/SageSLA attention (default: 0.1), we recommend using 0.15 for better video quality # --quant_linear Enable quantization for linear layers, pass this if using a quantized checkpoint # --default_norm Use the original LayerNorm and RMSNorm of Wan models python turbodiffusion/inference/wan2.2_i2v_infer.py \ --model Wan2.2-A14B \ --low_noise_model_path checkpoints/TurboWan2.2-I2V-A14B-low-720P-quant.pth \ --high_noise_model_path checkpoints/TurboWan2.2-I2V-A14B-high-720P-quant.pth \ --resolution 720p \ --adaptive_resolution \ --image_path assets/i2v_inputs/i2v_input_0.jpg \ --prompt "POV selfie video, ultra-messy and extremely fast. A white cat in sunglasses stands on a surfboard with a neutral look when the board suddenly whips sideways, throwing cat and camera into the water; the frame dives sharply downward, swallowed by violent bursts of bubbles, spinning turbulence, and smeared water streaks as the camera sinks. Shadows thicken, pressure ripples distort the edges, and loose bubbles rush upward past the lens, showing the camera is still sinking. Then the cat kicks upward with explosive speed, dragging the view through churning bubbles and rapidly brightening water as sunlight floods back in; the camera races upward, water streaming off the lens, and finally breaks the surface in a sudden blast of light and spray, snapping back into a crooked, frantic selfie as the cat resurfaces." \ --num_samples 1 \ --num_steps 4 \ --quant_linear \ --attention_type sagesla \ --sla_topk 0.1 \ --ode&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Interactive inference via the terminal is available at &lt;code&gt;turbodiffusion/serve/&lt;/code&gt;. This allows multi-turn video generation without reloading the model.&lt;/p&gt;
    &lt;p&gt;We evaluate video generation on a single RTX 5090 GPU. The E2E Time refers to the end-to-end diffusion generation latency, excluding text encoding and VAE decoding.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4549s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 38s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 184s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 5.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 1.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4767s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 72.6s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 24s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4767s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 72.6s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 24s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 4767s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 72.6s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 24s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 1676s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 26.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 9.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 1676s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 26.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 9.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 1676s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 26.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 9.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Original, E2E Time: 1676s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;FastVideo, E2E Time: 26.3s&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;TurboDiffusion, E2E Time: 9.9s&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In this repo, we provide training code based on Wan2.1 and its synthetic data. The training builds on the rCM codebase (https://github.com/NVlabs/rcm), with infrastructure support including FSDP2, Ulysses CP, and selective activation checkpointing (SAC). For rCM training instructions, please refer to the original rCM repository; SLA (Sparse-Linear Attention) training guidance is provided here.&lt;/p&gt;
    &lt;p&gt;For rCM/SLA training, additionally run:&lt;/p&gt;
    &lt;code&gt;pip install megatron-core hydra-core wandb webdataset
pip install --no-build-isolation transformer_engine[pytorch]&lt;/code&gt;
    &lt;p&gt;Download the Wan2.1 pretrained checkpoints in &lt;code&gt;.pth&lt;/code&gt; format and VAE/text encoder to &lt;code&gt;assets/checkpoints&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# make sure git lfs is installed
git clone https://huggingface.co/worstcoder/Wan assets/checkpoints&lt;/code&gt;
    &lt;p&gt;FSDP2 relies on Distributed Checkpoint (DCP) for loading and saving checkpoints. Before training, convert &lt;code&gt;.pth&lt;/code&gt; teacher checkpoints to &lt;code&gt;.dcp&lt;/code&gt; first:&lt;/p&gt;
    &lt;code&gt;python -m torch.distributed.checkpoint.format_utils torch_to_dcp assets/checkpoints/Wan2.1-T2V-1.3B.pth assets/checkpoints/Wan2.1-T2V-1.3B.dcp&lt;/code&gt;
    &lt;p&gt;After training, the saved &lt;code&gt;.dcp&lt;/code&gt; checkpoints can be converted to &lt;code&gt;.pth&lt;/code&gt; using the script &lt;code&gt;scripts/dcp_to_pth.py&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We provide Wan2.1-14B-synthesized datasets. Download to &lt;code&gt;assets/datasets&lt;/code&gt; using:&lt;/p&gt;
    &lt;code&gt;# make sure git lfs is installed
git clone https://huggingface.co/datasets/worstcoder/Wan_datasets assets/datasets&lt;/code&gt;
    &lt;p&gt;We implement white-box SLA training by aligning the predictions of the SLA-enabled model with those of the full-attention pretrained model. Unlike black-box training in the original paper, which tunes the pretrained model using diffusion loss, white-box training mitigates distribution shift and is less sensitive to the training data.&lt;/p&gt;
    &lt;p&gt;Single-node training example:&lt;/p&gt;
    &lt;code&gt;WORKDIR="/your/path/to/turbodiffusion"
cd $WORKDIR
export PYTHONPATH=turbodiffusion

# the "IMAGINAIRE_OUTPUT_ROOT" environment variable is the path to save experiment output files
export IMAGINAIRE_OUTPUT_ROOT=${WORKDIR}/outputs
CHECKPOINT_ROOT=${WORKDIR}/assets/checkpoints
DATASET_ROOT=${WORKDIR}/assets/datasets/Wan2.1_14B_480p_16:9_Euler-step100_shift-3.0_cfg-5.0_seed-0_250K

# your Wandb information
export WANDB_API_KEY=xxx
export WANDB_ENTITY=xxx

registry=registry_sla
experiment=wan2pt1_1pt3B_res480p_t2v_SLA

torchrun --nproc_per_node=8 \
    -m scripts.train --config=rcm/configs/${registry}.py -- experiment=${experiment} \
        model.config.teacher_ckpt=${CHECKPOINT_ROOT}/Wan2.1-T2V-1.3B.dcp \
        model.config.tokenizer.vae_pth=${CHECKPOINT_ROOT}/Wan2.1_VAE.pth \
        model.config.text_encoder_path=${CHECKPOINT_ROOT}/models_t5_umt5-xxl-enc-bf16.pth \
        model.config.neg_embed_path=${CHECKPOINT_ROOT}/umT5_wan_negative_emb.pt \
        dataloader_train.tar_path_pattern=${DATASET_ROOT}/shard*.tar&lt;/code&gt;
    &lt;p&gt;Please refer to &lt;code&gt;turbodiffusion/rcm/configs/experiments/sla/wan2pt1_t2v.py&lt;/code&gt; for the 14B config or perform modifications as needed.&lt;/p&gt;
    &lt;p&gt;The parameter updates from SLA training can be merged into rCM checkpoints using &lt;code&gt;turbodiffusion/scripts/merge_models.py&lt;/code&gt;, enabling rCM to perform sparse attention inference. Specify &lt;code&gt;--base&lt;/code&gt; as the rCM model, &lt;code&gt;--diff_base&lt;/code&gt; as the pretrained model, and &lt;code&gt;--diff_target&lt;/code&gt; as the SLA-tuned model.&lt;/p&gt;
    &lt;p&gt;We thank the community effort Comfyui_turbodiffusion for integrating TurboDiffusion into ComfyUI.&lt;/p&gt;
    &lt;p&gt;We're actively working on the following features and improvements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Organize and release training code&lt;/item&gt;
      &lt;item&gt;Optimize infrastructure for better parallel&lt;/item&gt;
      &lt;item&gt;vLLM-Omni integration&lt;/item&gt;
      &lt;item&gt;Support for more video generation models&lt;/item&gt;
      &lt;item&gt;Support for autoregressive video generation models&lt;/item&gt;
      &lt;item&gt;More hardware-level operator optimizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome community members to help maintain and extend TurboDiffusion. Welcome to join the TurboDiffusion Team and contribute together!&lt;/p&gt;
    &lt;p&gt;If you use this code or find our work valuable, please cite:&lt;/p&gt;
    &lt;code&gt;@article{zhang2025turbodiffusion,
  title={TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times},
  author={Zhang, Jintao and Zheng, Kaiwen and Jiang, Kai and Wang, Haoxu and Stoica, Ion and Gonzalez, Joseph E and Chen, Jianfei and Zhu, Jun},
  journal={arXiv preprint arXiv:2512.16093},
  year={2025}
}

@software{turbodiffusion2025,
  title={TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times},
  author={The TurboDiffusion Team},
  url={https://github.com/thu-ml/TurboDiffusion},
  year={2025}
}

@inproceedings{zhang2025sageattention,
  title={SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration}, 
  author={Zhang, Jintao and Wei, Jia and Zhang, Pengle and Zhu, Jun and Chen, Jianfei},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025}
}

@article{zhang2025sla,
  title={SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention},
  author={Zhang, Jintao and Wang, Haoxu and Jiang, Kai and Yang, Shuo and Zheng, Kaiwen and Xi, Haocheng and Wang, Ziteng and Zhu, Hongzhou and Zhao, Min and Stoica, Ion and others},
  journal={arXiv preprint arXiv:2509.24006},
  year={2025}
}

@article{zheng2025rcm,
  title={Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency},
  author={Zheng, Kaiwen and Wang, Yuji and Ma, Qianli and Chen, Huayu and Zhang, Jintao and Balaji, Yogesh and Chen, Jianfei and Liu, Ming-Yu and Zhu, Jun and Zhang, Qinsheng},
  journal={arXiv preprint arXiv:2510.08431},
  year={2025}
}

@inproceedings{zhang2024sageattention2,
  title={Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization},
  author={Zhang, Jintao and Huang, Haofeng and Zhang, Pengle and Wei, Jia and Zhu, Jun and Chen, Jianfei},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2025}
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/thu-ml/TurboDiffusion"/><published>2025-12-26T03:19:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46390055</id><title>Building an AI agent inside a 7-year-old Rails monolith</title><updated>2025-12-26T15:39:46.759017+00:00</updated><content>&lt;doc fingerprint="6c1bd1132f25fa62"&gt;
  &lt;main&gt;
    &lt;p&gt;I (incorrectly) convinced myself over the last few months that there’s no low-hanging fruit that would work for our product and business. This is a story of just how wrong I was.&lt;/p&gt;
    &lt;p&gt;I was at SF Ruby, in San Francisco, a few weeks ago. Most of the tracks were, of course, heavily focused on AI. Lots of stories from people building AIs into all sorts of products using Ruby and Rails,&lt;/p&gt;
    &lt;p&gt;They were good talks. But most of them assumed a kind of software I don’t work on — systems without strong boundaries, without multi-tenant concerns, without deeply embedded authorization rules.&lt;/p&gt;
    &lt;p&gt;I kept thinking: this is interesting, but it doesn’t map cleanly to my world. At Mon Ami, we can’t just release a pilot unless it passes strict data access checks.&lt;/p&gt;
    &lt;p&gt;Then I saw a talk about using the RubyLLM gem to build a RAG-like system. The conversation (LLM calls) context was augmented using function calls (tools). This is when it clicked. I could encode my complicated access logic into a specific function call and ensure the LLM gets access to some of our data without having to give it unrestricted access.&lt;/p&gt;
    &lt;p&gt;RubyLLM is a neat gem that abstracts away the interaction with many LLM providers with a clean API.&lt;/p&gt;
    &lt;code&gt;gem "ruby_llm"&lt;/code&gt;
    &lt;p&gt;It is configured in an initializer with the API keys for the providers you want to use.&lt;/p&gt;
    &lt;code&gt;RubyLLM.configure do |config|
  config.openai_api_key = Rails.application.credentials.dig(:openai_api_key)
  config.anthropic_api_key = Rails.application.credentials.dig(:anthropic_api_key)
  # config.default_model = "gpt-4.1-nano"

  # Use the new association-based acts_as API (recommended)
  config.use_new_acts_as = true

  # Increase timeout for slow API responses
  config.request_timeout = 600  # 10 minutes (default is 300)
  config.max_retries = 3        # Retry failed requests
end

# Load LLM tools from main app
Dir[Rails.root.join('app/tools/**/*.rb')].each { |f| require f }&lt;/code&gt;
    &lt;p&gt;It provides a Conversation model as an abstraction for an LLM thread. The Conversation contains a set of Messages. It also provides a way of defining structured responses and function calls available.&lt;/p&gt;
    &lt;code&gt;AVAILABLE_TOOLS = [
  Tools::Client::SearchTool
].freeze

conversation = Conversation.find(conversation_id)
chat = conversation.with_tools(*AVAILABLE_TOOLS)

chat.ask 'What is the phone number for John Snow?'&lt;/code&gt;
    &lt;p&gt;A Conversation is initialized by passing a model (gpt-5, claude-sonnet-4.5, etc) and has a method for chatting to it.&lt;/p&gt;
    &lt;code&gt;conversation = Conversation.new(model: RubyLLM::Model.find_by(model_id: 'gpt-4o-mini'))&lt;/code&gt;
    &lt;p&gt;RubyLLM comes with a neat DSL for defining accepted parameters (the descriptions are passed to the LLM as context since it needs to decide if the tool should be used based on the conversation). The tool implements an execute method returning a hash. The hash is then presented to the LLM. This is all the magic needed.&lt;/p&gt;
    &lt;code&gt;class SearchTool &amp;lt; BaseTool
  description 'Search for clients by name, ID, or email address. Returns matching clients.'

  param :query,
    desc: 'Search query - can be client name, ID, or email address',
    type: :string

  def execute(query:)
  end
end&lt;/code&gt;
    &lt;p&gt;We’ll now build a modest function call and a messaging interface. The function call allows searching a client using Algolia and ensuring the resulting set is visible to the user (by merging in the pundit policy).&lt;/p&gt;
    &lt;code&gt;def execute(query:)
  response = Algolia::SearchClient
    .create(app_id, search_key)
    .search_single_index(Client.index_name, {
      query: query.truncate(250)
    })

  ids = response.hits.map { |hit| hit[:id] }.compact

  base_scope = Client.where(id: ids)
  client = Admin::Org::ClientPolicy::Scope.new(base_scope).resolve.first or return {}

  {
    id: client.id,
    ami_id: client.slug,
    slug: client.slug,
    name: client.full_name,
    email: client.email
  }
end&lt;/code&gt;
    &lt;p&gt;The LLM acts as the magic glue between the natural language input submitted by the user, decides which (if any) tool to use to augment the context, and then responds to the user. No model should ever know Jon Snow’s phone number from a SaaS service, but this approach allows this sort of retrieval.&lt;/p&gt;
    &lt;p&gt;The UI is built with a remote form that enqueues an Active Job.&lt;/p&gt;
    &lt;code&gt;= turbo_stream_from @conversation, :messages

.container-fluid.h-100.d-flex.flex-column
  .sticky-top
    %h2.mb-0
      Conversation ##{@conversation.id}

  .flex-grow-1
    = render @messages

  .p-3.border-top.bg-white.sticky-bottom#message-form
  = form_with url: path, method: :post, local: false, data: { turbo_stream: true } do |f|
    = f.text_area :content
    = f.submit 'Send'&lt;/code&gt;
    &lt;p&gt;The job will process the Message.&lt;/p&gt;
    &lt;code&gt;class ProcessMessageJob &amp;lt; ApplicationJob
  queue_as :default

  def perform(conversation_id, message)
    conversation = Conversation.find(conversation_id)
    conversation.ask message
  end
end&lt;/code&gt;
    &lt;p&gt;The conversation has broadcast refresh enabled to update the UI when the response is received.&lt;/p&gt;
    &lt;code&gt;class Conversation &amp;lt; RubyLLM::Conversation
  broadcasts_refreshes
end&lt;/code&gt;
    &lt;p&gt;The form has a stimulus controller that checks for new messages being appended in order to scroll to the end of the conversation.&lt;/p&gt;
    &lt;p&gt;I checked a few OpenAI models for this implementation: gpt-5, gpt-4o, gpt4. GPT-5 has a big context, meaning we could have long-running conversations, but because there are a number of round-trips, the delay to queries requiring 3+ consecutive tools made the Agent feel sluggish.&lt;/p&gt;
    &lt;p&gt;GPT-4, on the other hand, is interestingly very prone to hallucinations - rushing to respond to queries with made-up data instead of calling the necessary tools. GPT-4o strikes, so far, the best balance between speed and correctness.&lt;/p&gt;
    &lt;p&gt;Building this tool took probably about 2-3 days of Claude-powered development (AIs building AIs). The difficulty and the complexity of building such a tool were the things that surprised me the most. The tool service object is essentially an API controller action - pass inputs and get a JSON back. Interestingly.&lt;/p&gt;
    &lt;p&gt;Before building this Agent, I looked at the other gems in this space. ActiveAgent (a somewhat similar gem for interacting with LLMs) is a decent contender that moves the prompts to a view file. It didn’t fit my needs since it had no built-in support for defining tools or having long-running conversations.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://catalinionescu.dev/ai-agent/building-ai-agent-part-1/"/><published>2025-12-26T07:35:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46390667</id><title>Geometric Algorithms for Translucency Sorting in Minecraft [pdf]</title><updated>2025-12-26T15:39:46.639773+00:00</updated><content/><link href="https://douira.dev/assets/document/douira-master-thesis.pdf"/><published>2025-12-26T09:43:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46391077</id><title>The First Web Server</title><updated>2025-12-26T15:39:46.408697+00:00</updated><content>&lt;doc fingerprint="fb0f55d54bf40ea7"&gt;
  &lt;main&gt;
    &lt;p&gt;Late December 1990 was a pivotal time, although none of us realized it for a few years. Tim Berners-Lee, A British computer scientist working in Switzerland, was working on what became the World Wide Web. Over the course of a few months, he invented HTML, the web browser, and the web server, to make it easier to share information. Sometime in late December, the first web server reached a usable state. By some accounts it was December 20, 1990. By at least one account I found, it was December 25.&lt;/p&gt;
    &lt;head rend="h2"&gt;The first web server’s address&lt;/head&gt;
    &lt;p&gt;The early work on the World Wide Web took place on NeXT workstations. Berners-Lee’s workstation lived at info.cern.ch.CERN is the European Organization for Nuclear Research, an intergovernmental organization that operates the largest particle physics laboratory in the world. It might be the most momentous shadow IT project in history.&lt;/p&gt;
    &lt;p&gt;No screenshots exist of the web page in its earliest form, unfortunately, although I did find an approximation of how the page appeared in 1992. Not surprisingly, the first web page was technical information about the web, including how HTML, web servers, and web browsers worked.&lt;/p&gt;
    &lt;p&gt;The earliest copy of the page I could find on archive.org, from 2000, stated the web page and the computer that hosted it no longer exist. In August 2006, CERN memorialized the first web page and first web server with a page about it.&lt;/p&gt;
    &lt;p&gt;Berners-Lee’s original goal was making information more accessible. Valuable data resided in various formats on computers throughout the organization. Berners-Lee’s goal was to unlock the data so it could link together and be readable from any machine.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happened next&lt;/head&gt;
    &lt;p&gt;It took a few years for the World Wide Web to go worldwide. By January 1993, NSCA Mosaic, a cross-platform web browser, was available, which gave rise to Netscape. The web caught on quickly on college campuses with browsers that ran on all of the major platforms of the time. Efforts to commercialize the web led to the dotcom boom, and, eventually, to the online world we know today.&lt;/p&gt;
    &lt;p&gt;David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dfarq.homeip.net/the-first-web-server/"/><published>2025-12-26T11:12:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46391410</id><title>I'm a laptop weirdo and that's why I like my new Framework 13</title><updated>2025-12-26T15:39:45.866198+00:00</updated><content>&lt;doc fingerprint="560efd35c9737c9d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I'm a laptop weirdo and that's why I like my new Framework 13&lt;/head&gt;
    &lt;p&gt;This month I sold my 2021 M1 Max Macbook Pro and bought a Framework 13 DIY Edition laptop. After I got everything setup I sat down to write about the experience. Some ~4500 words later I realized I needed to break my thoughts into multiple posts.&lt;/p&gt;
    &lt;p&gt;See also:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Framework 13 DIY Edition Hardware Thoughts&lt;/item&gt;
      &lt;item&gt;Setting up my new Framework Laptop 13 DIY Edition with NixOS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My new Framework 13 laptop just arrived. After I finally set everything up I started writing a post about the experience. I thought I'd write a little bit about my previous laptops, but a lot of fond memories I had forgotten about came flooding back. The tinkerings and many openings of laptops past. If you will indulge me, I've been feeling nostalgic. This is for the other laptop weirdos out there that that feel the same.&lt;/p&gt;
    &lt;head rend="h3"&gt;I have a history of doing terrible acts to laptops&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; The only image I could find of my NC10 was this blurry, 2021 flip phone photo of me removing the windows sticker.&lt;/p&gt;
    &lt;p&gt;In 2008, I managed to get my hands on a Samsung NC10 Netbook in a fancy metallic blue color. [^ Back when netbooks where a thing circle 2007-2013] Prior to this I only had desktops. The specs were pretty humble (from wikipedia):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A single core 1.6 GHz Intel Atom N270&lt;/item&gt;
      &lt;item&gt;Integrated Intel GMA 950 graphics&lt;/item&gt;
      &lt;item&gt;1 GB DDR2 RAM&lt;/item&gt;
      &lt;item&gt;10.2 inch 1024x600 screen and a VGA connector of all things.&lt;/item&gt;
      &lt;item&gt;83-key keyboard rather than the usual 87 or 88 keys on a laptop.&lt;/item&gt;
      &lt;item&gt;A 160 GB HDD&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Something could be done about that though! You could upgrade the RAM to a powerful 2GB. You could replace the slow HDD with an SSD. You could add a touch screen. You could make a Hackintosh out of it if you replaced the wifi card. If you wanted to, you could do those things and I was a weirdo, so I did!&lt;/p&gt;
    &lt;p&gt;I found a lot of fun in trying to get as much as I could out of that hardware. In fact I'd say the act of doing all that was far more enjoyable than actually using the laptop once the tinkering was done. After the novelty and slowness of a Hackintosh wore off I put Linux on the Netbook. I still sought the thrill of the hunt.&lt;/p&gt;
    &lt;p&gt;I installed a lite weight distro CrunchBang [^ or just #!] and messed around. I read more about different minimalist distros and came across two others I could hop to: Arch and Gentoo. This feels like an inflection point in my life, I choose to try Arch since I wouldn't have to compile everything on a single core. [^ Who know what would have happened if I picked Gentoo. I might have a beard now.] The screen was small and I wanted to maximize its usefulness so I started trying tiling WMs. Why not XMonad?&lt;/p&gt;
    &lt;p&gt;It turns out the GMA950 was undervolted on the NC10. Someone made a shareware tool called the GMABooster that could restore the max clock rate. The original website http://www.gmabooster.com/home.htm is long toast and not on wayback. This Arch forum thread has details though:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It allows a user, not a manufacturer to choose the desired GMA speed. It combines a sophisticated assembler-level technology and the user-friendly graphic user interface, offering You to near double the GMA core perfomance without even a need to restart a computer..&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The package was on AUR so I could squeeze out a little more performance. I could finally watch 480 YouTube videos instead of 360. At some point, long after I had stopped using the netbook, the AUR package became abandoned. I adopted it as maintainer and mirrored the binary in GitHub. This was the first time I ever was a package maintainer. [^ I am on a couple random packages in nixpkgs now.] Nowadays the package is memorialized in the the AUR archive.&lt;/p&gt;
    &lt;p&gt;I had a device that I could repeatedly break and remake. Did I do anything productive or meaningful with it? Absolutely not. Did I learn a lot in the process? I'd say so!&lt;/p&gt;
    &lt;head rend="h3"&gt;In the past you could do terrible things to Macbooks too&lt;/head&gt;
    &lt;p&gt;When I went to College I got a 2011 Macbook Pro. The kind that would overheat and desolder the GPU. [^ Some clever people have found hardware hacks to repair the problem https://www.jeffgeerling.com/blog/2017/fixing-2011-macbook-pro-booting-grey-screen-amd-radeon-video-glitch] Mine managed to last a long time and didn't need replacing until 2019. The RAM was not built-in yet on Macbooks. Apple said the model could only support up to 8GB total RAM, but you could actually get 16GB to work. Also, this was back when Macbooks had CD drives. I replaced the my drive with an Other World Computing DIY Optical Drive to HDD Upgrade Kit. [^ And you could put the drive into an "OWC SuperSlim" enclosure to turn it into a USB CD drive.] and installed SSDs in both slots. With two drives I was able to install rEFInd as a boot manager and triple boot:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OSX as a stable install for my course work&lt;/item&gt;
      &lt;item&gt;Windows for games&lt;/item&gt;
      &lt;item&gt;Linux so I could break my install repeatedly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I iterated on my Arch install so many times that I started to keep a checklist about my setup process to help me remember everything. Certain stylistic choices were set and still used to this day. [^ This is when I started using Inconsolata for a monospace font and Zenburn for a color scheme.] I couldn't change quite as many things about this laptop, but I still made an effort to change what I could.&lt;/p&gt;
    &lt;head rend="h3"&gt;As laptops grew thinner they grew more boring&lt;/head&gt;
    &lt;p&gt;When it came time for a new laptop I was not looking at Macbook Pros anymore. Apple had made changes, like the touch bar and removing magsafe, that felt like they were targeting a different audience. So instead I had been eyeing a ThinkPad.&lt;lb/&gt; [^ It's almost cliche to buy one and install Linux.] The prices on the Lenovo store are mostly made up and constantly discounted. My housemate had access to a corpo portal for Lenovo that let me get one at a heavily reduced price. The cost of 3 year service coverage was also discounted so I got some figuring it could help to cover cost of parts if if something failed.&lt;/p&gt;
    &lt;p&gt;So I bought a Gen 7 X1 Carbon and... I just used it. No mods were possible on this laptop. When I had an SSD failure I asked Lenovo if they could mail me the drive so I could do the install. They said they had to send someone to confirm the issue. So a technician came out and replaced the drive.&lt;/p&gt;
    &lt;head rend="h3"&gt;The gift and curse of a free Macbook Pro&lt;/head&gt;
    &lt;p&gt;Finally in 2023 I was laid off by HubSpot. Part of severance was the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Laptops &amp;amp; WFH Set-Up: Impacted employees may keep their HubSpot laptops (it will be cleaned of any company data remotely), as well as any work from home gear like monitors and keyboards.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Thus a pretty high spec 2021 M1 Max Macbook Pro fell into my lap. I gave my X1 Carbon to a friend to avoid creating yet more ewaste that sits in my closet.&lt;/p&gt;
    &lt;p&gt;The 2021 version was a bit of return to form: touch bar was gone, magsafe was back, etc. However even the iFixit review said the "design represents a major move in the right direction" but still only rated the laptop a 4/10 for repairability. [^ The score was eventually updated to a 5/10 when Apple later released a service manual and access to parts.]&lt;/p&gt;
    &lt;p&gt;I felt some dissonance though. If I was looking to buy a laptop, I wouldn't have picked this one. macOS was getting less enjoyable to use with each update. Likewise the Linux Desktop experience was really coming into its own. [^ By 2023 essentially all my games were playable!] However I felt bad about buying a new laptop when I now had a perfectly good one. So I held onto it and once again, no mods were done or could be done with this laptop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Finally buying a Framework 13&lt;/head&gt;
    &lt;p&gt;I had waited on getting a Framework laptop because I wanted to see them go through a couple iterations. I wanted to see if the promise of repairing, replacing and upgrading actually came true. From what I read it mostly has! [^ People with Framework 15 do seem to be waiting though.]&lt;/p&gt;
    &lt;p&gt;What changed the decision for me was the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lugging around a powerful 16 inch laptop was a drag. Having a laptop when traveling is nice if I need to hurriedly rebook something. Mobile sites and apps tend to restrict you in weird ways.&lt;/item&gt;
      &lt;item&gt;Despite being a couple years old, the laptop was still worth a lot. People probably want Macbooks for local LLM inference. So I felt pretty good a buyer will actually use the laptop.&lt;/item&gt;
      &lt;item&gt;The Framework release a refresh of the 13 with the new AMD chips.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then I had a friend get Laptop 13 and attest to liking it. That was the last push I needed to finally buy one. Now I can be a laptop weirdo again.&lt;/p&gt;
    &lt;p&gt;You can't change the RAM on laptops now.&lt;lb/&gt; You can't change the SSD on laptops now.&lt;lb/&gt; You can't easily repair the screen on laptops now.&lt;/p&gt;
    &lt;p&gt;You can do all that and more with a Framework laptop.&lt;lb/&gt; You can be a laptop weirdo with a Framework laptop.&lt;/p&gt;
    &lt;p&gt;Weirdo typically has two interpretations:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;A possibly dangerous person.&lt;/p&gt;&lt;lb/&gt;A strange, odd, eccentric person.&lt;/quote&gt;
    &lt;p&gt;To both of those I say: all us laptop weirdos can now put a snack drawer in our laptops.&lt;lb/&gt; You cannot stop us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.matthewbrunelle.com/im-a-laptop-weirdo-and-thats-why-i-like-my-new-framework-13/"/><published>2025-12-26T12:27:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46391448</id><title>Unix "find" expressions compiled to bytecode</title><updated>2025-12-26T15:39:45.700481+00:00</updated><content>&lt;doc fingerprint="1fe85974ea331f40"&gt;
  &lt;main&gt;
    &lt;p&gt; nullprogram.com/blog/2025/12/23/ &lt;/p&gt;
    &lt;p&gt;In preparation for a future project, I was thinking about at the unix &lt;code&gt;find&lt;/code&gt; utility. It operates a file system hierarchies, with basic
operations selected and filtered using a specialized expression language.
Users compose operations using unary and binary operators, grouping with
parentheses for precedence. &lt;code&gt;find&lt;/code&gt; may apply the expression to a great
many files, so compiling it into a bytecode, resolving as much as possible
ahead of time, and minimizing the per-element work, seems like a prudent
implementation strategy. With some thought, I worked out a technique to do
so, which was simpler than I expected, and I’m pleased with the results. I
was later surprised all the real world &lt;code&gt;find&lt;/code&gt; implementations I examined
use tree-walk interpreters instead. This article describes how my
compiler works, with a runnable example, and lists ideas for improvements.&lt;/p&gt;
    &lt;p&gt;For a quick overview, the syntax looks like this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find [-H|-L] path... [expression...]
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Technically at least one path is required, but most implementations imply &lt;code&gt;.&lt;/code&gt; when none are provided. If no expression is supplied, the default is
&lt;code&gt;-print&lt;/code&gt;, e.g. print everything under each listed path. This prints the
whole tree, including directories, under the current directory:&lt;/p&gt;
    &lt;p&gt;To only print files, we could use &lt;code&gt;-type f&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find . -type f -a -print
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Where &lt;code&gt;-a&lt;/code&gt; is the logical AND binary operator. &lt;code&gt;-print&lt;/code&gt; always evaluates
to true. It’s never necessary to write &lt;code&gt;-a&lt;/code&gt;, and adjacent operations are
implicitly joined with &lt;code&gt;-a&lt;/code&gt;. We can keep chaining them, such as finding
all executable files:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find . -type f -executable -print
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;If no &lt;code&gt;-exec&lt;/code&gt;, &lt;code&gt;-ok&lt;/code&gt;, or &lt;code&gt;-print&lt;/code&gt; (or similar side-effect extensions like
&lt;code&gt;-print0&lt;/code&gt; or &lt;code&gt;-delete&lt;/code&gt;) are present, the whole expression is wrapped in an
implicit &lt;code&gt;( expr ) -print&lt;/code&gt;. So we could also write this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find . -type f -executable
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Use &lt;code&gt;-o&lt;/code&gt; for logical OR. To print all files with the executable bit or
with a &lt;code&gt;.exe&lt;/code&gt; extension:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find . -type f \( -executable -o -name '*.exe' \)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;I needed parentheses because &lt;code&gt;-o&lt;/code&gt; has lower precedence than &lt;code&gt;-a&lt;/code&gt;, and
because parentheses are shell metacharacters I also needed to escape them
for the shell. It’s a shame &lt;code&gt;find&lt;/code&gt; didn’t use &lt;code&gt;[&lt;/code&gt; and &lt;code&gt;]&lt;/code&gt; instead! There’s
also a unary logical NOT operator, &lt;code&gt;!&lt;/code&gt;. To print all non-executable files:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find . -type f ! -executable
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Binary operators are short-circuiting, so this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find -type d -a -exec du -sh {} +
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Only lists the sizes of directories, as the &lt;code&gt;-type d&lt;/code&gt; fails causing the
whole expression to evaluate to false without evaluating &lt;code&gt;-exec&lt;/code&gt;. Or
equivalently with &lt;code&gt;-o&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find ! -type d -o -exec du -sh {} +
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;If it’s not a directory then the left-hand side evaluates to true, and the right-hand side is not evaluated. All three implementations I examined (GNU, BSD, BusyBox) have a &lt;code&gt;-regex&lt;/code&gt; extension, and eagerly compile the
regular expression even if the operation is never evaluated:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ find . -print -o -regex [
find: bad regex '[': Invalid regular expression
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;I was surprised by this because it doesn’t seem to be in the spirit of the original utility (“The second expression shall not be evaluated if the first expression is true.”), and I’m used to the idea of short-circuit validation for the right-hand side of a logical expression. Recompiling for each evaluation would be unwise, but it could happen lazily such that an invalid regular expression only causes an error if it’s actually used. No big deal, just a curiosity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bytecode design&lt;/head&gt;
    &lt;p&gt;A bytecode interpreter needs to track just one result at a time, making it a single register machine, with a 1-bit register at that. I came up with these five opcodes:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;halt
not
braf   LABEL
brat   LABEL
action NAME [ARGS...]
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Obviously &lt;code&gt;halt&lt;/code&gt; stops the program. While I could just let it “run off the
end” it’s useful to have an actual instruction so that I can attach a
label and jump to it. The &lt;code&gt;not&lt;/code&gt; opcode negates the register. &lt;code&gt;braf&lt;/code&gt; is
“branch if false”, jumping (via relative immediate) to the labeled (in
printed form) instruction if the register is false. &lt;code&gt;brat&lt;/code&gt; is “branch if
true”. Together they implement the &lt;code&gt;-a&lt;/code&gt; and &lt;code&gt;-o&lt;/code&gt; operators. In practice
there are no loops and jumps are always forward: &lt;code&gt;find&lt;/code&gt; is not Turing
complete.&lt;/p&gt;
    &lt;p&gt;In a real implementation each possible action (&lt;code&gt;-name&lt;/code&gt;, &lt;code&gt;-ok&lt;/code&gt;, &lt;code&gt;-print&lt;/code&gt;,
&lt;code&gt;-type&lt;/code&gt;, etc.) would get a dedicated opcode. This requires implementing
each operator, at least in part, in order to correctly parse the whole
&lt;code&gt;find&lt;/code&gt; expression. For now I’m just focused on the bytecode compiler, so
this opcode is a stand-in, and it kind of pretends based on looks. Each
action sets the register, and actions like &lt;code&gt;-print&lt;/code&gt; always set it to true.
My compiler is called &lt;code&gt;findc&lt;/code&gt; (“find compiler”).&lt;/p&gt;
    &lt;p&gt;Update: Or try the online demo via Wasm! This version includes a peephole optimizer I wrote after publishing this article.&lt;/p&gt;
    &lt;p&gt;I assume readers of this program are familiar with &lt;code&gt;push&lt;/code&gt; macro
and &lt;code&gt;Slice&lt;/code&gt; macro. Because of the latter it requires a very
recent C compiler, like GCC 15 (e.g. via w64devkit) or Clang 22. Try
out some &lt;code&gt;find&lt;/code&gt; commands and see how they appear as bytecode. The simplest
case is also optimal:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ findc
// path: .
        action  -print
        halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Print the path then halt. Simple. Stepping it up:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ findc -type f -executable
// path: .
        action  -type f
        braf    L1
        action  -executable
L1:     braf    L2
        action  -print
L2:     halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;If the path is not a file, it skips over the rest of the program by way of the second branch instruction. It’s correct, but already we can see room for improvement. This would be better:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;        action  -type f
        braf    L1
        action  -executable
        braf    L1
        action  -print
L1:     halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;More complex still:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ findc -type f \( -executable -o -name '*.exe' \)
// path: .
        action  -type f
        braf    L1
        action  -executable
        brat    L1
        action  -name *.exe
L1:     braf    L2
        action  -print
L2:     halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Inside the parentheses, if &lt;code&gt;-executable&lt;/code&gt; succeeds, the right-hand side is
skipped. Though the &lt;code&gt;brat&lt;/code&gt; jumps straight to a &lt;code&gt;braf&lt;/code&gt;. It would be better
to jump ahead one more instruction:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;        action  -type f
        braf    L2
        action  -executable
        brat    L1
        action  -name *.exe
        braf    L2
L1      action  -print
L2:     halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Silly things aren’t optimized either:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ findc ! ! -executable
// path: .
        action  -executable
        not
        not
        braf    L1
        action  -print
L1:     halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Two &lt;code&gt;not&lt;/code&gt; in a row cancel out, and so these instructions could be
eliminated. Overall this compiler could benefit from a peephole
optimizer, scanning over the program repeatedly, making small
improvements until no more can be made:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Delete &lt;code&gt;not&lt;/code&gt;-&lt;code&gt;not&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;A &lt;code&gt;brat&lt;/code&gt; to a &lt;code&gt;braf&lt;/code&gt; re-targets ahead one instruction, and vice versa.&lt;/item&gt;
      &lt;item&gt;Jumping onto an identical jump adopts its target for itself.&lt;/item&gt;
      &lt;item&gt;A &lt;code&gt;not&lt;/code&gt;-&lt;code&gt;braf&lt;/code&gt; might convert to a &lt;code&gt;brat&lt;/code&gt;, and vice versa.&lt;/item&gt;
      &lt;item&gt;Delete side-effect-free instructions before &lt;code&gt;halt&lt;/code&gt; (e.g. &lt;code&gt;not&lt;/code&gt;-&lt;code&gt;halt&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Exploit always-true actions, e.g. &lt;code&gt;-print&lt;/code&gt;-&lt;code&gt;braf&lt;/code&gt; can drop the branch.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Writing a bunch of peephole pattern matchers sounds kind of fun. Though my compiler would first need a slightly richer representation in order to detect and fix up changes to branches. One more for the road:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ findc -type f ! \( -executable -o -name '*.exe' \)
// path: .
        action  -type f
        braf    L1
        action  -executable
        brat    L2
        action  -name *.exe
L2:     not
L1:     braf    L3
        action  -print
L3:     halt
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;The unoptimal jumps hint at my compiler’s structure. If you’re feeling up for a challenge, pause here to consider how you’d build this compiler, and how it might produce these particular artifacts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Parsing and compiling&lt;/head&gt;
    &lt;p&gt;Before I even considered the shape of the bytecode I knew I needed to convert &lt;code&gt;find&lt;/code&gt; infix into a compiler-friendly postfix. That is, this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;-type f -a ! ( -executable -o -name *.exe )
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Becomes:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;-type f -executable -name *.exe -o ! -a
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Which, importantly, erases the parentheses. This comes in as an &lt;code&gt;argv&lt;/code&gt;
array, so it’s already tokenized for us by the shell or runtime. The
classic shunting-yard algorithm solves this problem easily enough.
We have an output queue that goes into the compiler, and a token stack for
tracking &lt;code&gt;-a&lt;/code&gt;, &lt;code&gt;-o&lt;/code&gt;, &lt;code&gt;!&lt;/code&gt;, and &lt;code&gt;(&lt;/code&gt;. Then we walk &lt;code&gt;argv&lt;/code&gt; in order:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Actions go straight into the output queue.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If we see one of the special stack tokens we push it onto the stack, first popping operators with greater precedence into the queue, stopping at &lt;code&gt;(&lt;/code&gt;.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If we see &lt;code&gt;)&lt;/code&gt; we pop the stack into the output queue until we see &lt;code&gt;(&lt;/code&gt;.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When we’re out of tokens, pop the remaining stack into the queue. My parser synthesizes &lt;code&gt;-a&lt;/code&gt; where it’s implied, so the compiler always sees
logical AND. If the expression contains no &lt;code&gt;-exec&lt;/code&gt;, &lt;code&gt;-ok&lt;/code&gt;, or &lt;code&gt;-print&lt;/code&gt;,
after processing is complete the parser puts &lt;code&gt;-print&lt;/code&gt; then &lt;code&gt;-a&lt;/code&gt; into the
queue, which effectively wraps the whole expression in &lt;code&gt;( expr ) -print&lt;/code&gt;.
By clearing the stack first, the real expression is effectively wrapped in
parentheses, so no parenthesis tokens need to be synthesized.&lt;/p&gt;
    &lt;p&gt;I’ve used the shunting-yard algorithm many times before, so this part was easy. The new part was coming up with an algorithm to convert a series of postfix tokens into bytecode. My solution is the compiler maintains a stack of bytecode fragments. That is, each stack element is a sequence of one or more bytecode instructions. Branches use relative addresses, so they’re position-independent, and I can concatenate code fragments without any branch fix-ups. It takes the following actions from queue tokens:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;For an action token, create an &lt;code&gt;action&lt;/code&gt; instruction, and push it onto
the fragment stack as a new fragment.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For a &lt;code&gt;!&lt;/code&gt; token, pop the top fragment, append a &lt;code&gt;not&lt;/code&gt; instruction, and
push it back onto the stack.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For a &lt;code&gt;-a&lt;/code&gt; token, pop the top two fragments, join then with a &lt;code&gt;braf&lt;/code&gt; in
the middle which jumps just beyond the second fragment. That is, if the
first fragment evaluates to false, skip over the second fragment into
whatever follows.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For a &lt;code&gt;-o&lt;/code&gt; token, just like &lt;code&gt;-a&lt;/code&gt; but use &lt;code&gt;brat&lt;/code&gt;. If the first fragment
is true, we skip over the second fragment.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the expression is valid, at the end of this process the stack contains exactly one fragment. Append a &lt;code&gt;halt&lt;/code&gt; instruction to this fragment, and
that’s our program! If the final fragment contained a branch just beyond
its end, this &lt;code&gt;halt&lt;/code&gt; is that branch target. A few peephole optimizations
and could probably be an optimal program for this instruction set.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nullprogram.com/blog/2025/12/23/"/><published>2025-12-26T12:35:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46391472</id><title>ChatGPT conversations still lack timestamps after years of requests</title><updated>2025-12-26T15:39:45.506662+00:00</updated><content/><link href="https://community.openai.com/t/timestamps-for-chats-in-chatgpt/440107?page=3"/><published>2025-12-26T12:39:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46391514</id><title>Package managers keep using Git as a database, it never works out</title><updated>2025-12-26T15:39:45.381140+00:00</updated><content>&lt;doc fingerprint="b324261b0df047d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Using git as a database is a seductive idea. You get version history for free. Pull requests give you a review workflow. It’s distributed by design. GitHub will host it for free. Everyone already knows how to use it.&lt;/p&gt;
    &lt;p&gt;Package managers keep falling for this. And it keeps not working out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cargo&lt;/head&gt;
    &lt;p&gt;The crates.io index started as a git repository. Every Cargo client cloned it. This worked fine when the registry was small, but the index kept growing. Users would see progress bars like “Resolving deltas: 74.01%, (64415/95919)” hanging for ages, the visible symptom of Cargo’s libgit2 library grinding through delta resolution on a repository with thousands of historic commits.&lt;/p&gt;
    &lt;p&gt;The problem was worst in CI. Stateless environments would download the full index, use a tiny fraction of it, and throw it away. Every build, every time.&lt;/p&gt;
    &lt;p&gt;RFC 2789 introduced a sparse HTTP protocol. Instead of cloning the whole index, Cargo now fetches files directly over HTTPS, downloading only the metadata for dependencies your project actually uses. (This is the “full index replication vs on-demand queries” tradeoff in action.) By April 2025, 99% of crates.io requests came from Cargo versions where sparse is the default. The git index still exists, still growing by thousands of commits per day, but most users never touch it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Homebrew&lt;/head&gt;
    &lt;p&gt;GitHub explicitly asked Homebrew to stop using shallow clones. Updating them was “an extremely expensive operation” due to the tree layout and traffic of homebrew-core and homebrew-cask.&lt;/p&gt;
    &lt;p&gt;Users were downloading 331MB just to unshallow homebrew-core. The .git folder approached 1GB on some machines. Every &lt;code&gt;brew update&lt;/code&gt; meant waiting for git to grind through delta resolution.&lt;/p&gt;
    &lt;p&gt;Homebrew 4.0.0 in February 2023 switched to JSON downloads for tap updates. The reasoning was blunt: “they are expensive to git fetch and git clone and GitHub would rather we didn’t do that… they are slow to git fetch and git clone and this provides a bad experience to end users.”&lt;/p&gt;
    &lt;p&gt;Auto-updates now run every 24 hours instead of every 5 minutes, and they’re much faster because there’s no git fetch involved.&lt;/p&gt;
    &lt;head rend="h2"&gt;CocoaPods&lt;/head&gt;
    &lt;p&gt;CocoaPods is the package manager for iOS and macOS development. It hit the limits hard. The Specs repo grew to hundreds of thousands of podspecs across a deeply nested directory structure. Cloning took minutes. Updating took minutes. CI time vanished into git operations.&lt;/p&gt;
    &lt;p&gt;GitHub imposed CPU rate limits. The culprit was shallow clones, which force GitHub’s servers to compute which objects the client already has. The team tried various band-aids: stopping auto-fetch on &lt;code&gt;pod install&lt;/code&gt;, converting shallow clones to full clones, sharding the repository.&lt;/p&gt;
    &lt;p&gt;The CocoaPods blog captured it well: “Git was invented at a time when ‘slow network’ and ‘no backups’ were legitimate design concerns. Running endless builds as part of continuous integration wasn’t commonplace.”&lt;/p&gt;
    &lt;p&gt;CocoaPods 1.8 gave up on git entirely for most users. A CDN became the default, serving podspec files directly over HTTP. The migration saved users about a gigabyte of disk space and made &lt;code&gt;pod install&lt;/code&gt; nearly instant for new setups.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nixpkgs&lt;/head&gt;
    &lt;p&gt;Nixpkgs is currently stress-testing GitHub’s infrastructure. In November 2025, GitHub contacted the NixOS team about periodic maintenance jobs failing and causing “issues achieving consensus between replicas.” If unresolved, the repository could have become read-only.&lt;/p&gt;
    &lt;p&gt;The repository totals 83GB with half a million tree objects and 20,000 forks. A local clone is only 2.5GB — the rest is GitHub’s fork network storing every pull request branch and merge commit. The CI queries mergeability daily, creating new merge commits each time.&lt;/p&gt;
    &lt;p&gt;Unlike CocoaPods, Nixpkgs can’t easily move to a CDN. The Nix expressions are the package definitions, not metadata pointing elsewhere. Binary caches already serve built packages over HTTP, but nixpkgs itself remains a git repository — and it’s still growing.&lt;/p&gt;
    &lt;head rend="h2"&gt;vcpkg&lt;/head&gt;
    &lt;p&gt;vcpkg is Microsoft’s C++ package manager. It uses git tree hashes to version its ports, with the curated registry at github.com/Microsoft/vcpkg containing over 2,000 libraries.&lt;/p&gt;
    &lt;p&gt;The problem is that vcpkg needs to retrieve specific versions of ports by their git tree hash. When you specify a &lt;code&gt;builtin-baseline&lt;/code&gt; in your vcpkg.json (functioning like a lockfile for reproducible builds), vcpkg looks up historical commits to find the exact port versions you need. This only works if you have the full commit history.&lt;/p&gt;
    &lt;p&gt;Shallow clones break everything. GitHub Actions uses shallow clones by default. DevContainers shallow-clone vcpkg to save space. CI systems optimize for fast checkouts. All of these result in the same error: “vcpkg was cloned as a shallow repository… Try again with a full vcpkg clone.”&lt;/p&gt;
    &lt;p&gt;The workarounds are ugly. One proposed solution involves parsing vcpkg.json to extract the baseline hash, deriving the commit date, then fetching with &lt;code&gt;--shallow-since=&amp;lt;date&amp;gt;&lt;/code&gt;. Another suggests including twelve months of history, hoping projects upgrade before their baseline falls off the cliff. For GitHub Actions, you need &lt;code&gt;fetch-depth: 0&lt;/code&gt; in your checkout step, downloading the entire repository history just to resolve dependencies.&lt;/p&gt;
    &lt;p&gt;A vcpkg team member explained the fundamental constraint: “Port versions don’t use commit hashes, we use the git tree hash of the port directory. As far as I know, there is no way to deduce the commit that added a specific tree hash.” An in-product fix is infeasible. The architecture baked in git deeply enough that there’s no escape hatch.&lt;/p&gt;
    &lt;p&gt;Unlike Cargo, Homebrew, and CocoaPods, vcpkg hasn’t announced plans to move away from git registries. Custom registries must still be git repositories. The documentation describes filesystem registries as an alternative, but these require local or mounted paths rather than HTTP access. There’s no CDN, no sparse protocol, no HTTP-based solution on the horizon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Go modules&lt;/head&gt;
    &lt;p&gt;Grab’s engineering team went from 18 minutes for &lt;code&gt;go get&lt;/code&gt; to 12 seconds after deploying a module proxy. That’s not a typo. Eighteen minutes down to twelve seconds.&lt;/p&gt;
    &lt;p&gt;The problem was that &lt;code&gt;go get&lt;/code&gt; needed to fetch each dependency’s source code just to read its go.mod file and resolve transitive dependencies. Cloning entire repositories to get a single file.&lt;/p&gt;
    &lt;p&gt;Go had security concerns too. The original design wanted to remove version control tools entirely because “these fragment the ecosystem: packages developed using Bazaar or Fossil, for example, are effectively unavailable to users who cannot or choose not to install these tools.” Beyond fragmentation, the Go team worried about security bugs in version control systems becoming security bugs in &lt;code&gt;go get&lt;/code&gt;. You’re not just importing code; you’re importing the attack surface of every VCS tool on the developer’s machine.&lt;/p&gt;
    &lt;p&gt;GOPROXY became the default in Go 1.13. The proxy serves source archives and go.mod files independently over HTTP. Go also introduced a checksum database (sumdb) that records cryptographic hashes of module contents. This protects against force pushes silently changing tagged releases, and ensures modules remain available even if the original repository is deleted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond package managers&lt;/head&gt;
    &lt;p&gt;The same pattern shows up wherever developers try to use git as a database.&lt;/p&gt;
    &lt;p&gt;Git-based wikis like Gollum (used by GitHub and GitLab) become “somewhat too slow to be usable” at scale. Browsing directory structure takes seconds per click. Loading pages takes longer. GitLab plans to move away from Gollum entirely.&lt;/p&gt;
    &lt;p&gt;Git-based CMS platforms like Decap hit GitHub’s API rate limits. A Decap project on GitHub scales to about 10,000 entries if you have a lot of collection relations. A new user with an empty cache makes a request per entry to populate it, burning through the 5,000 request limit quickly. If your site has lots of content or updates frequently, use a database instead.&lt;/p&gt;
    &lt;p&gt;Even GitOps tools that embrace git as a source of truth have to work around its limitations. ArgoCD’s repo server can run out of disk space cloning repositories. A single commit invalidates the cache for all applications in that repo. Large monorepos need special scaling considerations.&lt;/p&gt;
    &lt;head rend="h2"&gt;The pattern&lt;/head&gt;
    &lt;p&gt;The hosting problems are symptoms. The underlying issue is that git inherits filesystem limitations, and filesystems make terrible databases.&lt;/p&gt;
    &lt;p&gt;Directory limits. Directories with too many files become slow. CocoaPods had 16,000 pod directories in a single Specs folder, requiring huge tree objects and expensive computation. Their fix was hash-based sharding: split directories by the first few characters of a hashed name, so no single directory has too many entries. Git itself does this internally with its objects folder, splitting into 256 subdirectories. You’re reinventing B-trees, badly.&lt;/p&gt;
    &lt;p&gt;Case sensitivity. Git is case-sensitive, but macOS and Windows filesystems typically aren’t. Check out a repo containing both &lt;code&gt;File.txt&lt;/code&gt; and &lt;code&gt;file.txt&lt;/code&gt; on Windows, and the second overwrites the first. Azure DevOps had to add server-side enforcement to block pushes with case-conflicting paths.&lt;/p&gt;
    &lt;p&gt;Path length limits. Windows restricts paths to 260 characters, a constraint dating back to DOS. Git supports longer paths, but Git for Windows inherits the OS limitation. This is painful with deeply nested node_modules directories, where &lt;code&gt;git status&lt;/code&gt; fails with “Filename too long” errors.&lt;/p&gt;
    &lt;p&gt;Missing database features. Databases have CHECK constraints and UNIQUE constraints; git has nothing, so every package manager builds its own validation layer. Databases have locking; git doesn’t. Databases have indexes for queries like “all packages depending on X”; with git you either traverse every file or build your own index. Databases have migrations for schema changes; git has “rewrite history and force everyone to re-clone.”&lt;/p&gt;
    &lt;p&gt;The progression is predictable. Start with a flat directory of files. Hit filesystem limits. Implement sharding. Hit cross-platform issues. Build server-side enforcement. Build custom indexes. Eventually give up and use HTTP or an actual database. You’ve built a worse version of what databases already provide, spread across git hooks, CI pipelines, and bespoke tooling.&lt;/p&gt;
    &lt;p&gt;None of this means git is bad. Git excels at what it was designed for: distributed collaboration on source code, with branching, merging, and offline work. The problem is using it for something else entirely. Package registries need fast point queries for metadata. Git gives you a full-document sync protocol when you need a key-value lookup.&lt;/p&gt;
    &lt;p&gt;If you’re building a package manager and git-as-index seems appealing, look at Cargo, Homebrew, CocoaPods, vcpkg, Go. They all had to build workarounds as they grew, causing pain for users and maintainers. The pull request workflow is nice. The version history is nice. You will hit the same walls they did.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nesbitt.io/2025/12/24/package-managers-keep-using-git-as-a-database.html"/><published>2025-12-26T12:46:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46391599</id><title>LearnixOS</title><updated>2025-12-26T15:39:45.217576+00:00</updated><content>&lt;doc fingerprint="7fe43b9d48af5e9f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Learnix Operating System&lt;/head&gt;
    &lt;p&gt;"If you can't explain it simply, you don't understand it well enough." - Albert Einstein&lt;/p&gt;
    &lt;p&gt;Hello there!1&lt;/p&gt;
    &lt;p&gt;In this book we are going to write and learn about operating systems together!&lt;/p&gt;
    &lt;p&gt;We are going to implement an entire POSIX compliant OS in Rust and not use ANY2 external libraries. All of the thought process, code and implementations will be explained and documented here as well as in this repo which all the code snippets are from.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: ALL the syntax highlighting of the Rust code is custom and create by me! If you see and bug, please write in the comments or submit an issue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Base Knowledge&lt;/head&gt;
    &lt;p&gt;This book will be technical, and will assume a little bit of a programming knowledge background, but not necessarily in rust&lt;/p&gt;
    &lt;p&gt;If you are not coming from a low level programming knowledge that's fine!&lt;/p&gt;
    &lt;p&gt;Just make sure you know this stuff or learn it as you read. Also if in any place on this book I take some things for granted, please, open an issue here and let me know so I could explain it better.&lt;/p&gt;
    &lt;p&gt;Some of the base knowledge that you would need to have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Some assembly knowledge. (just understand simple movs, and arithmetic operations, at a very basic level3)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Some knowledge on memory. (what's a pointer, what's an address)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A knowledge in rust is not that important, but knowing at least one programming language is. I myself have some more learning in Rust, and in this book I will also explain some great features that it has!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A lot of motivation to learn and understand because it is a complex subject.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Roadmap of this book&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compiling a stand alone binary&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Boot loading, Debugging, stages and some legacy stuff&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Important cpu modes and instructions&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Paging, writing out own malloc&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Utilizing the Interrupt Descriptor Table&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;File systems and Disk Drivers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Thinking in terms of processes&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Writing a shell&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Running our first program! (Which off course will be Doom)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To be continued (Hopefully virtualization section and loading a vm of other OS)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.learnix-os.com"/><published>2025-12-26T12:59:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46392115</id><title>Rob Pike Goes Nuclear over GenAI</title><updated>2025-12-26T15:39:44.255642+00:00</updated><link href="https://skyview.social/?url=https%3A%2F%2Fbsky.app%2Fprofile%2Frobpike.io%2Fpost%2F3matwg6w3ic2s&amp;viewtype=tree"/><published>2025-12-26T14:08:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46392815</id><title>High School Student Discovers 1.5M Potential New Astronomical Objects</title><updated>2025-12-26T15:39:44.001859+00:00</updated><content>&lt;doc fingerprint="ad5111b84a980758"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;High School Student Discovers 1.5 Million Potential New Astronomical Objects by Developing an A.I. Algorithm&lt;/head&gt;
    &lt;head rend="h2"&gt;The 18-year-old won $250,000 for training a machine learning model to analyze understudied data from NASA’s retired NEOWISE telescope&lt;/head&gt;
    &lt;p&gt;In a leap forward for astronomy, a researcher has developed an artificial intelligence algorithm and discovered more than one million objects in space by parsing through understudied data from a NASA telescope.&lt;/p&gt;
    &lt;p&gt;The breakthrough is detailed in a study published in November in The Astronomical Journal. What the study doesn’t detail, however, is that the paper’s sole author is 18 years old.&lt;/p&gt;
    &lt;p&gt;Matteo Paz from Pasadena, California, recently won the first place prize of $250,000 in the 2025 Regeneron Science Talent Search for combining machine learning with astronomy. Self-described as the nation’s “oldest and most prestigious science and math competition for high school seniors,” the contest recognized Paz for developing his A.I. algorithm. The young scientist’s tool processed 200 billion data entries from NASA’s now-retired Near-Earth Object Wide-field Infrared Survey Explorer (NEOWISE) telescope. His model revealed 1.5 million previously unknown potential celestial bodies.&lt;/p&gt;
    &lt;p&gt;“I was just happy to have had the privilege. Not only placing in the top ten, but winning first place, came as a visceral surprise,” the teenager tells Forbes’ Kevin Anderton. “It still hasn’t fully sunk in.”&lt;/p&gt;
    &lt;p&gt;Paz’s interest in astronomy turned into real research when he participated in the Planet Finder Academy at the California Institute of Technology (Caltech) in summer 2022. There, he studied astronomy and computer science under the guidance of his mentor, Davy Kirkpatrick, an astronomer and senior scientist at the university’s Infrared Processing and Analysis Center (IPAC).&lt;/p&gt;
    &lt;p&gt;Kirkpatrick had been working with data from the NEOWISE infrared telescope, which NASA launched in 2009 with the aim of searching for near-Earth asteroids and comets. The telescope’s survey, however, also collected data on the shifting heat of variable objects: rare phenomena that emit flashing, changing or otherwise dynamic light, such as exploding stars. It was Kirkpatrick’s idea to look for these elusive objects in NEOWISE’s understudied data.&lt;/p&gt;
    &lt;p&gt;“At that point, we were creeping up towards 200 billion rows in the table of every single [NEOWISE] detection that we had made over the course of over a decade,” Kirkpatrick explains in a Caltech statement. “So, my idea for the summer was to take a little piece of the sky and see if we could find some variable stars. Then we could highlight those to the astronomic community, saying, ‘Here’s some new stuff we discovered by hand; just imagine what the potential is in the dataset.’”&lt;/p&gt;
    &lt;p&gt;Paz, however, had no intention of doing it by hand. Instead, he worked on an A.I. model that sorted through the raw data in search of tiny changes in infrared radiation, which could indicate the presence of variable objects. Paz and Kirkpatrick continued working together after the summer to perfect the model, which ultimately flagged 1.5 million potential new objects, including supernovas and black holes.&lt;/p&gt;
    &lt;p&gt;“Prior to Matteo’s work, no one had tried to use the entire (200-billion-row) table to identify and classify all of the significant variability that was there,” Kirkpatrick tells Business Insider’s Morgan McFall-Johnsen in an email. He adds that Caltech researchers are already making use of Paz’s catalog of potential variable objects, called VarWISE, to study binary star systems.&lt;/p&gt;
    &lt;p&gt;“The variable candidates that he’s uncovered will be widely studied,” says Amy Mainzer, NEOWISE’s principal investigator for NASA, to Business Insider.&lt;/p&gt;
    &lt;p&gt;As for the A.I. model, Paz explains that it might be applicable to “anything else that comes in a temporal format,” such as stock market chart analysis and atmospheric effects like pollution, according to the statement. It’s no surprise the teenager is interested in the climate—as fires burned in L.A. earlier this year, the Eaton Fire forced him and his family to evacuate their home, Forbes reports.&lt;/p&gt;
    &lt;p&gt;Other teenage scientists recognized by the contest studied mosquito control, drug-resistant fungus, the human genome and mathematics.&lt;/p&gt;
    &lt;p&gt;“The remarkable creativity and dedication of these students bring renewed hope for our future,” Maya Ajmera, president of the Society for Science, which oversees the award, says in a statement. “Driven by their ingenuity, these young scientists are developing groundbreaking solutions that have the potential to transform our world and propel society forward.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.smithsonianmag.com/smart-news/high-school-student-discovers-1-5-million-potential-new-astronomical-objects-by-developing-an-ai-algorithm-180986429/"/><published>2025-12-26T15:13:21+00:00</published></entry></feed>