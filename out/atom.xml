<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-19T13:46:49.380007+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45963780</id><title>Cloudflare Global Network experiencing issues</title><updated>2025-11-19T13:46:54.609692+00:00</updated><content>&lt;doc fingerprint="40d4ee565e054cd0"&gt;
  &lt;main&gt;
    &lt;p&gt;Cloudflare services are currently operating normally. We are no longer observing elevated errors or latency across the network.&lt;/p&gt;
    &lt;p&gt;Our engineering teams continue to closely monitor the platform and perform a deeper investigation into the earlier disruption, but no configuration changes are being made at this time.&lt;/p&gt;
    &lt;p&gt;At this point, it is considered safe to re-enable any Cloudflare services that were temporarily disabled during the incident. We will provide a final update once our investigation is complete.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 17:44 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We continue to monitor the system through recovery and we are seeing errors and latency return to normal levels. A full post-incident investigation and details about the incident will be made available asap.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 17:14 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We continue to see errors drop as we work through services globally and clearing remaining errors and latency.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 16:46 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We continue to see errors and latency improve but still have reports of intermittent errors. The team continues to monitor the situation as it improves, and looking for ways to accelerate full recovery.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 16:27 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Bot scores will be impacted intermittently while we undergo global recovery. We will update once we believe bot scores are fully recovered.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 16:04 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;The team is continuing to focus on restoring service post-fix. We are mitigating several issues that remain post-deployment.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 15:40 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to monitor for any further issues.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 15:23 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Some customers may be still experiencing issues logging into or using the Cloudflare dashboard. We are working on a fix to resolve this, and continuing to monitor for any further issues.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 14:57 UTC&lt;/p&gt;
    &lt;p&gt;Monitoring&lt;/p&gt;
    &lt;p&gt;A fix has been implemented and we believe the incident is now resolved. We are continuing to monitor for errors to ensure all services are back to normal.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 14:42 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We've deployed a change which has restored dashboard services. We are still working to remediate broad application services impact&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 14:34 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to work on a fix for this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 14:22 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing working on restoring service for application services customers.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:58 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing working on restoring service for application services customers.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:35 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We have made changes that have allowed Cloudflare Access and WARP to recover. Error levels for Access and WARP users have returned to pre-incident rates. We have re-enabled WARP access in London.&lt;/p&gt;
    &lt;p&gt;We are continuing to work towards restoring other services.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:13 UTC&lt;/p&gt;
    &lt;p&gt;Identified&lt;/p&gt;
    &lt;p&gt;The issue has been identified and a fix is being implemented.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:09 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;During our attempts to remediate, we have disabled WARP access in London. Users in London trying to access the Internet via WARP will see a failure to connect.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 13:04 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to investigate this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 12:53 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to investigate this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 12:37 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are seeing services recover, but customers may continue to observe higher-than-normal error rates as we continue remediation efforts.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 12:21 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are continuing to investigate this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 12:03 UTC&lt;/p&gt;
    &lt;p&gt;Investigating&lt;/p&gt;
    &lt;p&gt;Cloudflare is experiencing an internal service degradation. Some services may be intermittently impacted. We are focused on restoring service. We will update as we are able to remediate. More updates to follow shortly.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 11:48 UTC&lt;/p&gt;
    &lt;p&gt;This incident affected: Cloudflare Sites and Services (Access, Bot Management, CDN/Cache, Dashboard, Firewall, Network, WARP, Workers).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cloudflarestatus.com/incidents/8gmgl950y3h7"/><published>2025-11-18T11:35:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45967079</id><title>Show HN: Browser-based interactive 3D Three-Body problem simulator</title><updated>2025-11-19T13:46:54.499355+00:00</updated><content>&lt;doc fingerprint="1b8cc1fe919783ba"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;About the N-Body Simulator&lt;/head&gt;
    &lt;head rend="h3"&gt;What is the Three-Body Problem?&lt;/head&gt;
    &lt;p&gt;The three-body problem is one of the most famous challenges in classical physics and celestial mechanics. It asks: given the initial positions, masses, and velocities of three bodies in space, can we predict their future motion under mutual gravitational attraction?&lt;/p&gt;
    &lt;p&gt;Unlike the two-body problem (which has an exact analytical solution), the three-body problem has no general closed-form solution. This makes numerical simulation the primary tool for studying these complex gravitational systems.&lt;/p&gt;
    &lt;head rend="h3"&gt;N-Body Gravitational Simulation&lt;/head&gt;
    &lt;p&gt;This simulator uses Newton's law of universal gravitation to model the gravitational forces between every pair of bodies:&lt;/p&gt;
    &lt;p&gt;F = G × m₁ × m₂ / (r² + ε²)&lt;/p&gt;
    &lt;p&gt;Each body experiences the sum of all pairwise gravitational forces from every other body. For N bodies, this requires calculating N(N-1)/2 force pairs each timestep. The ε² term is a softening parameter that prevents numerical singularities when bodies pass very close together.&lt;/p&gt;
    &lt;p&gt;The simulation supports multiple integration methods. By default, it uses the Velocity Verlet integration method, a symplectic integrator that provides superior energy conservation compared to simpler methods like Euler integration. This makes it ideal for long-term orbital mechanics simulations.&lt;/p&gt;
    &lt;p&gt;Users can switch to the 4th-order Runge-Kutta (RK4) method in the Advanced Settings, which offers higher accuracy per timestep and typically shows lower energy drift in short simulations. However, RK4 is not symplectic and accumulates systematic phase errors over long simulation times, causing orbits to gradually decay or expand. This makes RK4 better suited for short to medium duration simulations where minimizing instantaneous error is the priority, while Verlet excels at maintaining correct orbital shapes over extended periods.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preset Configurations&lt;/head&gt;
    &lt;p&gt;The simulator includes several famous periodic three-body orbits discovered through numerical searches:&lt;/p&gt;
    &lt;head rend="h4"&gt;2D Orbits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Figure-8 choreography: Discovered by Cris Moore in 1993, where three equal masses chase each other along a figure-eight shaped path&lt;/item&gt;
      &lt;item&gt;Lagrange triangular configuration: Equilateral triangle configuration with circular orbits.&lt;/item&gt;
      &lt;item&gt;Butterfly, Broucke, Hénon, and Yarn: Periodic orbits from the Šuvakov-Dmitrašinović database of three-body choreographies, discovered through systematic numerical exploration of initial conditions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;3D Orbits&lt;/head&gt;
    &lt;p&gt;Three-dimensional periodic orbits from Li and Liao (2025), which discovered 10,059 new periodic solutions including 21 choreographic orbits and 273 "piano-trio" orbits (where two equal-mass bodies share one orbit while a third body follows another). Paper | GitHub&lt;/p&gt;
    &lt;head rend="h3"&gt;Features &amp;amp; Applications&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time Physics: Experience gravitational dynamics in 3D with interactive controls&lt;/item&gt;
      &lt;item&gt;Multiple Integration Methods: Choose between Velocity Verlet (energy-conserving) and RK4 (high accuracy).&lt;/item&gt;
      &lt;item&gt;Exploration Platform: Experiment with different initial conditions and masses&lt;/item&gt;
      &lt;item&gt;Timeline Playback: Scrub through simulation history to analyze orbital behavior&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;How to Use&lt;/head&gt;
    &lt;p&gt;Getting Started: Use the preset configurations (Figure-8 or Lagrange) to see stable three-body orbits, or generate random initial conditions to explore chaotic dynamics.&lt;/p&gt;
    &lt;p&gt;Controls: Adjust body masses, simulation speed, and physics parameters. Use the timeline to review and analyze orbital patterns. Drag bodies while paused to create custom configurations.&lt;/p&gt;
    &lt;p&gt;Sharing: Click "Share Configuration" to generate a URL that preserves your exact simulation initial state.&lt;/p&gt;
    &lt;head rend="h3"&gt;Energy Conservation &amp;amp; Simulation Accuracy&lt;/head&gt;
    &lt;p&gt;The simulator displays two important energy metrics in the Advanced Settings panel:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Total Energy: The sum of kinetic energy (½mv²) and gravitational potential energy (-Gm₁m₂/r) of all bodies. In an ideal gravitational system, this value should remain constant over time.&lt;/item&gt;
      &lt;item&gt;Energy Drift: The percentage change in total energy from the initial state. This measures the numerical accuracy of the simulation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In real physics, energy is conserved in isolated systems. However, numerical integration methods introduce small errors at each timestep. The energy drift indicator helps you evaluate simulation quality:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Green (&amp;lt;1%): Excellent energy conservation - the simulation is highly accurate&lt;/item&gt;
      &lt;item&gt;Yellow (1-5%): Moderate drift - acceptable for most purposes but consider reducing timestep&lt;/item&gt;
      &lt;item&gt;Red (&amp;gt;5%): Significant drift - simulation may be unreliable, reduce timestep or try other integration methods&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Velocity Verlet integration method is "symplectic," meaning it preserves the phase-space structure of Hamiltonian systems. While RK4 typically shows lower energy drift in short-term simulations (better local accuracy), Verlet prevents systematic phase errors that accumulate over extended simulations. This makes Verlet ideal for long-term orbital mechanics where maintaining orbital stability over thousands of periods is more important than minimizing instantaneous error.&lt;/p&gt;
    &lt;p&gt;Why is Total Energy Negative? In gravitational systems, total energy is often negative, and this is perfectly normal! Gravitational potential energy is defined as zero at infinite separation and becomes increasingly negative as bodies move closer together (PE = -Gm₁m₂/r). When total energy is negative, it means the system is gravitationally bound - the bodies don't have enough kinetic energy to escape to infinity, so they remain in orbit. This is exactly what you see in stable orbital systems like planets around stars or the choreographed orbits in this simulator. A negative total energy that remains constant indicates a stable, bound orbital system.&lt;/p&gt;
    &lt;head rend="h3"&gt;Technical Details&lt;/head&gt;
    &lt;p&gt;Built with Three.js for WebGL-accelerated 3D graphics and modern JavaScript. The physics engine implements N-body gravitational calculations with a configurable softening parameter to prevent numerical singularities during close encounters.&lt;/p&gt;
    &lt;p&gt;The simulator tracks up to 10,000 frames of history, allowing you to review and analyze the evolution of complex orbital systems. All simulations are deterministic and reproducible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feedback&lt;/head&gt;
    &lt;p&gt;Have suggestions, found a bug, or want to share your thoughts? Give feedback and help improve this simulator!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://trisolarchaos.com/?pr=O_8(0.6)&amp;n=3&amp;s=5.0&amp;so=0.00&amp;im=rk4&amp;dt=1.00e-4&amp;rt=1.0e-6&amp;at=1.0e-8&amp;bs=0.15&amp;sf=0&amp;sv=0&amp;cm=free&amp;kt=1&amp;st=1&amp;tl=1500&amp;cp=2.5208,1.5125,2.5208&amp;ct=0.0000,0.0000,0.1670"/><published>2025-11-18T15:00:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45967211</id><title>Gemini 3</title><updated>2025-11-19T13:46:54.231093+00:00</updated><content>&lt;doc fingerprint="f9d7a1cf9b3f9a95"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A new era of intelligence with Gemini 3&lt;/head&gt;
    &lt;head rend="h3"&gt;A note from Google and Alphabet CEO Sundar Pichai:&lt;/head&gt;
    &lt;p&gt;Nearly two years ago we kicked off the Gemini era, one of our biggest scientific and product endeavors ever undertaken as a company. Since then, it’s been incredible to see how much people love it. AI Overviews now have 2 billion users every month. The Gemini app surpasses 650 million users per month, more than 70% of our Cloud customers use our AI, 13 million developers have built with our generative models, and that is just a snippet of the impact we’re seeing.&lt;/p&gt;
    &lt;p&gt;And we’re able to get advanced capabilities to the world faster than ever, thanks to our differentiated full stack approach to AI innovation — from our leading infrastructure to our world-class research and models and tooling, to products that reach billions of people around the world.&lt;/p&gt;
    &lt;p&gt;Every generation of Gemini has built on the last, enabling you to do more. Gemini 1’s breakthroughs in native multimodality and long context window expanded the kinds of information that could be processed — and how much of it. Gemini 2 laid the foundation for agentic capabilities and pushed the frontiers on reasoning and thinking, helping with more complex tasks and ideas, leading to Gemini 2.5 Pro topping LMArena for over six months.&lt;/p&gt;
    &lt;p&gt;And now we’re introducing Gemini 3, our most intelligent model, that combines all of Gemini’s capabilities together so you can bring any idea to life.&lt;/p&gt;
    &lt;p&gt;It’s state-of-the-art in reasoning, built to grasp depth and nuance — whether it’s perceiving the subtle clues in a creative idea, or peeling apart the overlapping layers of a difficult problem. Gemini 3 is also much better at figuring out the context and intent behind your request, so you get what you need with less prompting. It’s amazing to think that in just two years, AI has evolved from simply reading text and images to reading the room.&lt;/p&gt;
    &lt;p&gt;And starting today, we’re shipping Gemini at the scale of Google. That includes Gemini 3 in AI Mode in Search with more complex reasoning and new dynamic experiences. This is the first time we are shipping Gemini in Search on day one. Gemini 3 is also coming today to the Gemini app, to developers in AI Studio and Vertex AI, and in our new agentic development platform, Google Antigravity — more below.&lt;/p&gt;
    &lt;p&gt;Like the generations before it, Gemini 3 is once again advancing the state of the art. In this new chapter, we’ll continue to push the frontiers of intelligence, agents, and personalization to make AI truly helpful for everyone.&lt;/p&gt;
    &lt;p&gt;We hope you like Gemini 3, we'll keep improving it, and look forward to seeing what you build with it. Much more to come!&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing Gemini 3: our most intelligent model that helps you bring any idea to life&lt;/head&gt;
    &lt;p&gt;Demis Hassabis, CEO of Google DeepMind and Koray Kavukcuoglu, CTO of Google DeepMind and Chief AI Architect, Google, on behalf of the Gemini team&lt;/p&gt;
    &lt;p&gt;Today we’re taking another big step on the path toward AGI and releasing Gemini 3.&lt;/p&gt;
    &lt;p&gt;It’s the best model in the world for multimodal understanding and our most powerful agentic and vibe coding model yet, delivering richer visualizations and deeper interactivity — all built on a foundation of state-of-the-art reasoning.&lt;/p&gt;
    &lt;p&gt;We’re beginning the Gemini 3 era by releasing Gemini 3 Pro in preview and making it available today across a suite of Google products so you can use it in your daily life to learn, build and plan anything. We’re also introducing Gemini 3 Deep Think — our enhanced reasoning mode that pushes Gemini 3 performance even further — and giving access to safety testers before making it available to Google AI Ultra subscribers.&lt;/p&gt;
    &lt;head rend="h2"&gt;State-of-the-art reasoning with unprecedented depth and nuance&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro can bring any idea to life with its state-of-the-art reasoning and multimodal capabilities. It significantly outperforms 2.5 Pro on every major AI benchmark.&lt;/p&gt;
    &lt;p&gt;It tops the LMArena Leaderboard with a breakthrough score of 1501 Elo. It demonstrates PhD-level reasoning with top scores on Humanity’s Last Exam (37.5% without the usage of any tools) and GPQA Diamond (91.9%). It also sets a new standard for frontier models in mathematics, achieving a new state-of-the-art of 23.4% on MathArena Apex.&lt;/p&gt;
    &lt;p&gt;Beyond text, Gemini 3 Pro redefines multimodal reasoning with 81% on MMMU-Pro and 87.6% on Video-MMMU. It also scores a state-of-the-art 72.1% on SimpleQA Verified, showing great progress on factual accuracy. This means Gemini 3 Pro is highly capable at solving complex problems across a vast array of topics like science and mathematics with a high degree of reliability.&lt;/p&gt;
    &lt;p&gt;Gemini 3 is state-of-the-art across a range of key AI benchmarks. See details on our evaluation methodology.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Pro also brings a new level of depth and nuance to every interaction. Its responses are smart, concise and direct, trading cliché and flattery for genuine insight — telling you what you need to hear, not just what you want to hear. It acts as a true thought partner that gives you new ways to understand information and express yourself, from translating dense scientific concepts by generating code for high-fidelity visualizations to creative brainstorming.&lt;/p&gt;
    &lt;p&gt;Gemini 3 can code a visualization of plasma flow in a tokamak and write a poem capturing the physics of fusion.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gemini 3 Deep Think&lt;/head&gt;
    &lt;p&gt;Gemini 3 Deep Think mode pushes the boundaries of intelligence even further, delivering a step-change in Gemini 3’s reasoning and multimodal understanding capabilities to help you solve even more complex problems.&lt;/p&gt;
    &lt;p&gt;In testing, Gemini 3 Deep Think outperforms Gemini 3 Pro’s already impressive performance on Humanity’s Last Exam (41.0% without the use of tools) and GPQA Diamond (93.8%). It also achieves an unprecedented 45.1% on ARC-AGI-2 (with code execution, ARC Prize Verified), demonstrating its ability to solve novel challenges.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Deep Think mode excels on some of the most challenging AI benchmarks. See details on our evaluation methodology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gemini 3 helps you learn, build and plan anything&lt;/head&gt;
    &lt;head rend="h3"&gt;Learn anything&lt;/head&gt;
    &lt;p&gt;Gemini was built from the start to seamlessly synthesize information about any topic across multiple modalities, including text, images, video, audio and code. Gemini 3 pushes the frontier of multimodal reasoning to help you learn in ways that make sense for you by combining its state-of-the-art reasoning, vision and spatial understanding, leading multilingual performance, and 1 million-token context window.&lt;/p&gt;
    &lt;p&gt;For example, if you want to learn how to cook in your family tradition, Gemini 3 can decipher and translate handwritten recipes in different languages into a shareable family cookbook. Or if you want to learn about a new topic, you can give it academic papers, long video lectures or tutorials and it can generate code for interactive flashcards, visualizations or other formats that will help you master the material. It can even analyze videos of your pickleball match, identify areas where you can improve and generate a training plan for overall form improvements.&lt;/p&gt;
    &lt;p&gt;Gemini 3 can help you learn and preserve family cooking traditions. Try it in Gemini Canvas.&lt;/p&gt;
    &lt;p&gt;Gemini 3 can help you analyze complex information like research papers and can generate code for an interactive guide.&lt;/p&gt;
    &lt;p&gt;Get expert-level sports analysis on your pickleball match to help improve your game.&lt;/p&gt;
    &lt;p&gt;To help you make better sense of information on the web, AI Mode in Search now uses Gemini 3 to enable new generative UI experiences like immersive visual layouts and interactive tools and simulations, all generated completely on the fly based on your query.&lt;/p&gt;
    &lt;p&gt;Learn a complex topic like how RNA polymerase works with generative UI in AI Mode in Search.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build anything&lt;/head&gt;
    &lt;p&gt;Building on the success of 2.5 Pro, Gemini 3 delivers on the promise of bringing any idea to life for developers. It’s exceptional at zero-shot generation and handles complex prompts and instructions to render richer, more interactive web UI.&lt;/p&gt;
    &lt;p&gt;Gemini 3 is the best vibe coding and agentic coding model we’ve ever built – making our products more autonomous and boosting developer productivity. It tops the WebDev Arena leaderboard by scoring an impressive 1487 Elo. It also scores 54.2% on Terminal-Bench 2.0, which tests a model’s tool use ability to operate a computer via terminal and it greatly outperforms 2.5 Pro on SWE-bench Verified (76.2%), a benchmark that measures coding agents.&lt;/p&gt;
    &lt;p&gt;You can now build with Gemini 3 in Google AI Studio, Vertex AI, Gemini CLI and our new agentic development platform, Google Antigravity. It’s also available in third-party platforms like Cursor, GitHub, JetBrains, Manus, Replit and more.&lt;/p&gt;
    &lt;p&gt;Code a retro 3D spaceship game with richer visualizations and improved interactivity. Try it in AI Studio.&lt;/p&gt;
    &lt;p&gt;Bring your imagination to life by building, deconstructing and remixing detailed 3D voxel art using code. Try it in AI Studio.&lt;/p&gt;
    &lt;p&gt;Build a playable sci-fi world with shaders using Gemini 3. Try it in AI Studio.&lt;/p&gt;
    &lt;p&gt;You can vibe code richer, more interactive web UI and apps with Gemini 3.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introducing a new agent-first development experience&lt;/head&gt;
    &lt;p&gt;As model intelligence accelerates with Gemini 3, we have the opportunity to reimagine the entire developer experience. Today we’re releasing Google Antigravity, our new agentic development platform that enables developers to operate at a higher, task-oriented level.&lt;/p&gt;
    &lt;p&gt;Using Gemini 3’s advanced reasoning, tool use and agentic coding capabilities, Google Antigravity transforms AI assistance from a tool in a developer’s toolkit into an active partner. While the core of Google Antigravity is a familiar AI IDE experience, its agents have been elevated to a dedicated surface and given direct access to the editor, terminal and browser. Now, agents can autonomously plan and execute complex, end-to-end software tasks simultaneously on your behalf while validating their own code.&lt;/p&gt;
    &lt;p&gt;In addition to Gemini 3 Pro, Google Antigravity also comes tightly coupled with our latest Gemini 2.5 Computer Use model for browser control and our top-rated image editing model Nano Banana (Gemini 2.5 Image).&lt;/p&gt;
    &lt;p&gt;Google Antigravity uses Gemini 3 to drive an end-to-end agentic workflow for a flight tracker app. The agent independently plans, codes the application and validates its execution through browser-based computer use.&lt;/p&gt;
    &lt;head rend="h3"&gt;Plan anything&lt;/head&gt;
    &lt;p&gt;Since introducing the agentic era with Gemini 2, we’ve made a lot of progress, not only advancing Gemini’s coding agent abilities, but also improving its ability to reliably plan ahead over longer horizons. Gemini 3 demonstrates this by topping the leaderboard on Vending-Bench 2, which tests longer horizon planning by managing a simulated vending machine business. Gemini 3 Pro maintains consistent tool usage and decision-making for a full simulated year of operation, driving higher returns without drifting off task.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Pro demonstrates better long-horizon planning to generate significantly higher returns compared to other frontier models.&lt;/p&gt;
    &lt;p&gt;This means Gemini 3 can better help you get things done in everyday life. By combining deeper reasoning with improved, more consistent tool use, Gemini 3 can take action on your behalf by navigating more complex, multi-step workflows from start to finish — like booking local services or organizing your inbox — all while under your control and guidance.&lt;/p&gt;
    &lt;p&gt;Google AI Ultra subscribers can try these agentic capabilities in the Gemini app with Gemini Agent today. We’ve learned a lot improving Gemini’s agentic capabilities, and we’re excited to see how you use it as we expand to more Google products soon.&lt;/p&gt;
    &lt;p&gt;Gemini Agent can help you organize your Gmail inbox. Try it now in the Gemini app for Google AI Ultra subscribers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building Gemini 3 responsibly&lt;/head&gt;
    &lt;p&gt;Gemini 3 is our most secure model yet, and has undergone the most comprehensive set of safety evaluations of any Google AI model to date. The model shows reduced sycophancy, increased resistance to prompt injections and improved protection against misuse via cyberattacks.&lt;/p&gt;
    &lt;p&gt;In addition to our in-house testing for the critical domains in our Frontier Safety Framework, we've also partnered on evaluations with world-leading subject matter experts, provided early access to bodies like the UK AISI, and obtained independent assessments from industry experts like Apollo, Vaultis, Dreadnode and more. For more information, see the Gemini 3 model card.&lt;/p&gt;
    &lt;head rend="h2"&gt;The next era of Gemini&lt;/head&gt;
    &lt;p&gt;This is just the start of the Gemini 3 era. As of today, Gemini 3 starts rolling out:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For everyone in the Gemini app and for Google AI Pro and Ultra subscribers in AI Mode in Search&lt;/item&gt;
      &lt;item&gt;For developers in the Gemini API in AI Studio, our new agentic development platform, Google Antigravity; and Gemini CLI&lt;/item&gt;
      &lt;item&gt;For enterprises in Vertex AI and Gemini Enterprise&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For Gemini 3 Deep Think mode, we’re taking extra time for safety evaluations and input from safety testers before making it available to Google AI Ultra subscribers in the coming weeks.&lt;/p&gt;
    &lt;p&gt;We plan to release additional models to the Gemini 3 series soon so you can do more with AI. We look forward to getting your feedback and seeing what you learn, build and plan with Gemini.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.google/products/gemini/gemini-3/"/><published>2025-11-18T15:09:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45967814</id><title>Google Antigravity</title><updated>2025-11-19T13:46:54.060849+00:00</updated><link href="https://antigravity.google/"/><published>2025-11-18T15:47:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45968121</id><title>The code and open-source tools I used to produce a science fiction anthology</title><updated>2025-11-19T13:46:53.845718+00:00</updated><content>&lt;doc fingerprint="2fc09a362ef7ad11"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;&lt;p&gt; Last month I published &lt;/p&gt;Think Weirder: The Year's Best Science Fiction Ideas&lt;p&gt;, a 16-story anthology featuring Greg Egan, Isabel J. Kim, Ray Nayler, Caroline M. Yoachim, and twelve other wonderful authors. The book ended up being the #1 New Release in the Short Stories Anthologies category for a short time on Amazon, outselling many other newly released short story anthologies published by the big NYC publishers with large marketing departments. &lt;/p&gt;&lt;/div&gt;
      &lt;p&gt; I'm not a professional publisher. I have a full-time job and two small kids, so all of this work happened after my kids went to sleep. I had to use my time judiciously, which meant creating an efficient process. Fortunately I'm a programmer, and it turns out that programming skills translate surprisingly well to book publishing. This post is about how I built a complete publishing pipeline using Python, YAML files, and LaTeX â and why you might want to do something similar if you're considering publishing a book. I know that by writing this I'll have my choices questioned by professional designers, but hopefully the software concepts will be helpful. &lt;/p&gt;
      &lt;p&gt; My initial thought: can I really do ALL of this? &lt;/p&gt;
      &lt;p&gt; When I started this project, I had some worries. Professional publishers have entire departments of specialists. How could I possibly handle all of that myself? &lt;/p&gt;
      &lt;p&gt; The answer turned out to be: build tools that automate the repetitive parts, and use simple file formats that make everything transparent and debuggable. &lt;/p&gt;
      &lt;p&gt; Step 1: Tracking stories with plain text files &lt;/p&gt;
      &lt;p&gt; The first challenge was tracking hundreds of candidate stories from different magazines. I read 391 stories published in 2024 before selecting the final 16. That's a lot of stories to keep organized. &lt;/p&gt;
      &lt;p&gt; I could have used a spreadsheet, but I went with plain YAML files instead. Here's why this worked well for me: &lt;/p&gt;
      &lt;div&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Git-friendly: Every decision I made was tracked in version control&lt;/item&gt;
          &lt;item&gt;Human-readable: I could open any file in a text editor and understand what I was looking at&lt;/item&gt;
          &lt;item&gt;Easy to build scripts around: I wrote several Python functions to do different kinds of metadata introspection that I'll go through&lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
      &lt;p&gt; The structure looks like this: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;data/
  story-progress.yaml       # Central tracking file
  markets.yaml              # Magazine metadata
  themes.yaml               # Theme occurrence tracking
  subgenres.yaml            # Subgenre tallies
stories/
  clarkesworld-magazine/
    nelson_11_24.yaml       # Individual story files
    pak_06_24.yaml
  reactor-magazine/
    larson_breathing.yaml
  ...&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Each story file is pure YAML containing the full story text plus metadata: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;title: "Twenty-Four Hours"
author: H.H. Pak
market: clarkesworld-magazine
url: https://clarkesworldmagazine.com/pak_06_24/
word_count: 4540
year: 2024
slug: pak_06_24
summary: ...&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Not all stories have public URLs available, but that's OK because all of the fields are optional. The central &lt;code&gt;story-progress.yaml&lt;/code&gt; tracks editorial state:
&lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;clarkesworld-magazine-nelson_11_24:
  title: "LuvHomeâ¢"
  author: Resa Nelson
  market: clarkesworld-magazine
  status: accepted  # or: not_started/relevant/rejected
  date_added: '2024-09-08T08:22:47.033192'&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Step 2: A simple command-line tool &lt;/p&gt;
      &lt;p&gt; I built a small Python CLI tool (&lt;code&gt;se.py&lt;/code&gt;) to help me navigate all this data. Since I do all this work at night after my kids go to sleep, I wanted something fast that mirrored a lot of the other work I do on the command line. The tool is simple:
&lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;python se.py âhelp
usage: se.py [-h] {markets,stories,relevant,decide,accepted,compile} ...

Story Evaluator CLI

positional arguments:
  {markets,stories,relevant,decide,accepted,compile}
                        Available commands
    markets             List markets
    stories             Manage stories
    relevant            List URLs for stories marked as relevant
    decide              Make accept/reject decisions on relevant stories
    accepted            Manage accepted stories
    compile             Show anthology compilation statistics

optional arguments:
  -h, âhelp            show this help message and exit&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; The &lt;code&gt;compile&lt;/code&gt; command ended up being really useful â it gave me instant feedback on anthology size and composition:
&lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;ANTHOLOGY COMPILATION STATISTICS
============================================================
Total Stories: 16
Total Word Count: 115,093 words
Average Word Count: 7,193 words
Unique Authors: 16
Markets Represented: 4

STORIES BY MARKET:
  analog-magazine: 2 stories (12.5%)
  asimovs-magazine: 2 stories (12.5%)
  clarkesworld-magazine: 10 stories (62.5%)
  reactor-magazine: 2 stories (12.5%)&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; This was really helpful during the selection process. I could quickly check how far along I was toward my ~120k word goal, and make sure I hadn't accidentally included multiple stories by the same author. &lt;/p&gt;
      &lt;p&gt; Step 3: Typesetting the print book &lt;/p&gt;
      &lt;p&gt; This part surprised me the most. I initially thought I'd have to learn Adobe InDesign or pay someone to do the typesetting. But I decided to use LaTeX instead, since I had some previous experience with it (another publishing friend sent me some of his example files, and I had some academic experience). The process worked out better than expected. &lt;/p&gt;
      &lt;p&gt; I used XeLaTeX with the &lt;code&gt;memoir&lt;/code&gt; document class. Here's what I liked about this approach:
&lt;/p&gt;
      &lt;div&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Reproducible: I can rebuild the entire book from source in a few seconds, and I can use the same templates next year&lt;/item&gt;
          &lt;item&gt;Professional typography: LaTeX handles ligatures, kerning, and line breaking better than I could manually&lt;/item&gt;
          &lt;item&gt;Custom fonts: I used Crimson Pro for body text and Rajdhani for titles&lt;/item&gt;
          &lt;item&gt;Again, version control that I'm used to: The entire book is just text files in Git&lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
      &lt;p&gt; The main parts of the master file for the book are really simple: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;\documentclass[final,11pt,twoside]{memoir}
\usepackage{compelling}

\begin{document}
\begin{frontmatter}
  \include{title}
  \tableofcontents
\end{frontmatter}

\begin{mainmatter}
  \include{introduction}
  \include{death-and-the-gorgon}
  \include{the-best-version-of-yourself}
  % ... 14 more stories
  \include{acknowledgements}
\end{mainmatter}
\end{document}&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;div&gt;&lt;p&gt; All the formatting rules live in &lt;/p&gt;&lt;code&gt;compelling.sty&lt;/code&gt;&lt;p&gt;, a custom style package. &lt;/p&gt;Here's a link to the full, messy file&lt;p&gt;. Some highlights: &lt;/p&gt;&lt;/div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;% 6x9 inch trade paperback size
\setstocksize{9in}{6in}
\settrimmedsize{9in}{6in}{*}

% Margins
\setlrmarginsandblock{1.00in}{0.75in}{*}
\setulmarginsandblock{0.75in}{0.75in}{*}

% Typography nerding
\usepackage[final,protrusion=true,factor=1125,
            stretch=70,shrink=70]{microtype}

% Custom fonts loaded from local files
\setromanfont[
  Ligatures=TeX,
  Path=./Crimson_Pro/static/,
  UprightFont=CrimsonPro-Regular,
  BoldFont=CrimsonPro-Bold,
  ItalicFont=CrimsonPro-Italic,
  BoldItalicFont=CrimsonPro-BoldItalic
]{Crimson Pro}


\setsansfont[
  Path=./Rajdhani/,
  UprightFont=Rajdhani-Bold,
  BoldFont=Rajdhani-Bold,
  ItalicFont=Rajdhani-Bold,
  BoldItalicFont=Rajdhani-Bold
]{Rajdhani}

% Chinese font family for CJK characters
\newfontfamily\chinesefont{PingFang SC}&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; The &lt;code&gt;microtype&lt;/code&gt; package does a lot of subtle work with character spacing and line breaking that makes the text look professionally typeset.
&lt;/p&gt;
      &lt;p&gt; I wanted story titles in bold sans-serif with author names underneath in a lighter gray. Here's how I set that up: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;\renewcommand{\chapter}[2]{
    \pagestyle{DefaultStyle}
    \stdchapter*{
        \sffamily
        \LARGE 
        \textbf{\MakeUppercase{#1}}
        \\ 
        \large 
        \color{dark-gray} 
        {\MakeUppercase{#2}}
    }
    \addcontentsline{toc}{chapter}{
        \protect\parbox[t]{\dimexpr\textwidth-3em}{
            \sffamily#1
            \\ 
            \protect\small
            \protect\color{gray}
            \protect\textit{#2}
        }
    }
    \def\leftmark{#1}
    \def\rightmark{#2}
}&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; This redefines the &lt;code&gt;chapter&lt;/code&gt; command to take two arguments, the title and byline, and sets up both the chapter formatting, TOC formatting, and makes sure that the title and byline are printed in the headers on alternating pages.
&lt;/p&gt;
      &lt;p&gt; Now every story file just says: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;\chapter{Death and the Gorgon}{by Greg Egan}
[story content]&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Most authors send me stories as HTML, PDF, or word, so I needed a way to convert them to LaTeX. I wrote a simple Python script to do this, which saved me a huge amount of manual formatting work. &lt;/p&gt;
      &lt;p&gt; Step 4: Creating the ebook &lt;/p&gt;
      &lt;p&gt; Print was one thing, but I also needed an ebook. This turned out to be easier than I expected because I could reuse all the LaTeX source I'd already created. &lt;/p&gt;
      &lt;p&gt; I used Pandoc to convert from LaTeX to EPUB: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;# Convert LaTeX to EPUB
pandoc 2025.tex -o Think_Weirder_2025.epub \
  âtoc \
  âepub-cover-image=cover_optimized.jpg \
  âcss=epub-style.css \
  âmetadata title="Think Weirder" \
  âmetadata author="Edited by Joe Stech"&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Pandoc's default table of contents only showed story titles. But I wanted author names too, like you see in print anthologies. EPUBs are just zipped collections of XHTML files, so I wrote a small post-processing script: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;def modify_toc(nav_content, authors):
    """Add author bylines to TOC entries."""
    pattern = r'&amp;lt;a href="([^"]+)"&amp;gt;([^&amp;lt;]+)&amp;lt;/a&amp;gt;'

    def add_author(match):
        href, title = match.group(1), match.group(2)
        chapter_id = extract_id_from_href(href)

        if chapter_id in authors:
            author = authors[chapter_id]
            return f'&amp;lt;a href="{href}"&amp;gt;{title}&amp;lt;br /&amp;gt;\n' \
                   f'&amp;lt;em&amp;gt;{author}&amp;lt;/em&amp;gt;&amp;lt;/a&amp;gt;'
        return match.group(0)

    return re.sub(pattern, add_author, nav_content)&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; The script unzips the EPUB, finds the navigation file, adds author bylines, and rezips everything. Now the ebook table of contents matches the print version. &lt;/p&gt;
      &lt;p&gt; What I learned &lt;/p&gt;
      &lt;p&gt; The whole process took longer than I expected â many months of night work. The simple software I wrote really made it a feasible one-person project though, and motivates me to go through the whole process again next year. &lt;/p&gt;
      &lt;p&gt; Staying organized is crucial. When hundreds of stories are involved, it's easy to forget details, so using &lt;code&gt;se.py&lt;/code&gt; to save metadata in the moment that could be sliced and diced later was so important.
&lt;/p&gt;
      &lt;p&gt; Reproducible builds were a lifesaver. I made changes to the book layout right up until the week before publication. Because I could rebuild the entire book in seconds, and everything was backed up in git, I could experiment freely without worrying about breaking things. &lt;/p&gt;
      &lt;p&gt; Simple file formats made me comfortable. When something went wrong, I could always open a YAML file or look at the LaTeX source and understand what was happening. I never hit a point where the tools were a black box. &lt;/p&gt;
      &lt;p&gt; I didn't need to understand everything up front. I learned LaTeX details as I went (arguably I still don't really understand LaTeX). Same with Pandoc. I got something basic working first, then incrementally improved it. &lt;/p&gt;
      &lt;p&gt; Can you do this too? &lt;/p&gt;
      &lt;p&gt; If you're thinking about publishing a book â whether it's an anthology, a novel, or a collection of technical writing â I think this approach is worth considering. There's something motivating about having a detailed understanding of every step in the production process. If you have questions feel free to reach out, I love talking about this hobby! You can email me at joe@thinkweirder.com. &lt;/p&gt;
      &lt;div&gt;&lt;p&gt; And if you enjoy concept-driven science fiction that is heavy on novel ideas, check out &lt;/p&gt;Think Weirder! &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://compellingsciencefiction.com/posts/the-code-and-open-source-tools-i-used-to-produce-a-science-fiction-anthology.html"/><published>2025-11-18T16:10:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45968362</id><title>Solving a million-step LLM task with zero errors</title><updated>2025-11-19T13:46:53.663640+00:00</updated><content>&lt;doc fingerprint="189cb409d4d7b6cf"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 12 Nov 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Solving a Million-Step LLM Task with Zero Errors&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AI&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2511.09030"/><published>2025-11-18T16:26:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45969250</id><title>Pebble, Rebble, and a path forward</title><updated>2025-11-19T13:46:53.379634+00:00</updated><content>&lt;doc fingerprint="fc4b101b5b1408f0"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I believe the Pebble community, Core Devices, Rebble and I all want the same thing. We love our Pebbles and want them to keep working long into the future. We love the community that has sprung up around Pebble, and how it’s persevered - next year will be the 14th anniversary of the original Kickstarter campaign!&lt;/p&gt;
      &lt;p&gt;But I have to respond to claims made by Rebble posted on their blog yesterday. I will link to their post so you can read their side of the story, and I’ve asked them to link back to this blog post from theirs.&lt;/p&gt;
      &lt;p&gt;Look - I’m the first person to call myself out when I fail. I wrote a detailed blog post about Success and Failure at Pebble and often write in detail about learning from my mistakes. But in this specific case, you’ll find that I’ve done my utmost to respect the Pebble legacy and community. Rebble is misleading the community with false accusations.&lt;/p&gt;
      &lt;p&gt;For those just passing through, here’s the TLDR: &lt;/p&gt;
      &lt;p&gt;Core Devices is a small company I started in 2025 to relaunch Pebble and build new Pebble smartwatches. Rebble is a non-profit organization that has supported the Pebble community since 2017. Rebble has done a ton of great work over the years and deserves recognition and support for that.&lt;/p&gt;
      &lt;p&gt;Core Devices and Rebble negotiated an agreement where Core would pay $0.20/user/month to support Rebble services. But the agreement broke down after over the following disagreement. &lt;/p&gt;
      &lt;p&gt;Rebble believes that they ‘100%’ own the data of the Pebble Appstore. They’re attempting to create a walled garden around 13,000 apps and faces that individual Pebble developers created and uploaded to the Pebble Appstore between 2012 and 2016. Rebble later scraped this data in 2017. &lt;/p&gt;
      &lt;p&gt;I disagree. I’m working hard to keep the Pebble ecosystem open source. I believe the contents of the Pebble Appstore should be freely available and not controlled by one organization. &lt;/p&gt;
      &lt;p&gt;Rebble posted a blog post yesterday with a bunch of false accusations, and in this post I speak to each of them.&lt;/p&gt;
      &lt;p&gt;Sections&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Dec 2016 - Pebble shut down. Some IP was sold to Fitbit. I blogged about why I think we failed. Fitbit continued to run the Pebble Appstore and web services for 1.5 years. I really appreciated that.&lt;list rend="ul"&gt;&lt;item&gt;Rebble organization grew out of the official Pebble Developers Discord.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;July 2018, Fitbit shut down the Pebble appstore.&lt;list rend="ul"&gt;&lt;item&gt;Before it shut down, Rebble (and others) scraped all 13,000 apps and metadata from the Pebble Appstore. Rebble began hosting a copy of the appstore. They created a new Dev Portal where developers could upload new apps, roughly 500 have been uploaded since July 2018.&lt;/item&gt;&lt;item&gt;Rebble also reverse engineered many Pebble web services (weather, timeline and voice transcription) and provided them as a paid service for the Pebble community.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Jan 2025 - Google open sourced PebbleOS, breathing new life into the community.&lt;/item&gt;
        &lt;item&gt;March 2025 - I announced a new company (Core Devices) and 2 new watches - store.rePebble.com&lt;/item&gt;
        &lt;item&gt;November 2025 - we finished shipping out 5,000 Pebble 2 Duos. We’re working hard on Pebble Time 2. We’re aiming to start shipping in January.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Accusation 1: ‘Rebble paid for the work that [Eric] took as a base for his commercial watches’&lt;/p&gt;
      &lt;p&gt;Facts:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;I think they’re accusing me of ‘stealing’ open source contributions to PebbleOS that Rebble paid for. This is entirely false.&lt;/item&gt;
        &lt;item&gt;We did not take any PebbleOS work Rebble paid for ‘as a base for [our] commercial watches’. &lt;del rend="overstrike"&gt;To my best of my knowledge&lt;/del&gt;&lt;del rend="overstrike"&gt;,&lt;/del&gt;&lt;del rend="overstrike"&gt;Rebble never paid the&lt;/del&gt;&lt;del rend="overstrike"&gt;developer who ported NimBLE into PebbleOS.&lt;/del&gt;&lt;del rend="overstrike"&gt;My best guess is that they are referring to Rebble having paid CodeCoup, the company behind&lt;/del&gt;&lt;del rend="overstrike"&gt;NimBLE&lt;/del&gt;&lt;del rend="overstrike"&gt;, to fix some bugs that affected older non-Core Devices watches. Any Rebble-sponsored CodeCoup commits are not present in our repo. In fact, the opposite is true - we paid Codecoup $10,000 to fix multiple BLE stack issues, some of them on the host side that benefit all devices, including old Pebbles.&lt;/del&gt; Update: I’m told Rebble did pay him, months later. My point is valid - when we shifted development to our repo, Rebble had not paid anything. More broadly, I reject the premise that using open source software under the terms of the license, regardless of who funds development, is ‘stealing’.&lt;/item&gt;
        &lt;item&gt;We started using our own repo for PebbleOS development because PRs on the Rebble repo reviews were taking too long. We only had one firmware engineer at the time (now we have a whopping 2!) and he felt like he was being slowed down too much. All of our contributions to PebbleOS have been 100% open source.&lt;/item&gt;
        &lt;item&gt;Overall, the feedback that PebbleOS could benefit from open governance is well taken. Long term, PebbleOS would be a good fit for open source organization with experience in open governance, like Apache or Linux Foundation. I wrote about this last week.&lt;/item&gt;
        &lt;item&gt;With our small team and fairly quick development schedule, it's true that we haven't PRed our changes into Rebble’s repo. It’s tough to prioritize this while we are busy fixing bugs and getting ready for Pebble Time 2.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Accusation 2: ‘Core took Rebble’s work’ on &lt;code&gt;libpebblecommon&lt;/code&gt; to create &lt;code&gt;libpebble3&lt;/code&gt;&lt;/p&gt;
      &lt;p&gt;Facts:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;The majority (&amp;gt;90%) of our new open source&lt;code&gt;libpebble3&lt;/code&gt; library was written by Core Devices employees.  The remainder comes from &lt;code&gt;libpebblecommon&lt;/code&gt;, another open source library written by two people.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;In April 2025, Core purchased the copyright to the &lt;code&gt;libpebblecommon&lt;/code&gt; code from the two maintainers and incorporated it into &lt;code&gt;libpebble3&lt;/code&gt;**, which is also open source**.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;All our contributions to &lt;code&gt;libpebble3&lt;/code&gt; are GPL-3.0 licensed. Here’s the motivation behind that our licensing strategy for this repo. We use the same CLA agreement as Matrix, QT and MySQL. Our CLA explicitly includes a clause that requires to Core Devices to distribute all contributions under an OSI-compatible FOSS license (e.g. GPLv3).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Note that neither Rebble &lt;code&gt;libpebblecommon&lt;/code&gt; maintainer signed the Rebble blog post.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Side note regarding Cobble, I don’t think Rebble even knows this but in 2024, I personally spent over $30,000 to support its development, way before PebbleOS was open source. It was my own way to support the community.&lt;/p&gt;
      &lt;p&gt;Accusation 3: ‘Core promised that they would let Rebble maintain and own the developer site’&lt;/p&gt;
      &lt;p&gt;Facts:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Nothing of the sort was agreed upon. See the full written agreement that Core Devices has with Rebble towards the bottom. Rebble agreed that Core would host the developer site.&lt;/item&gt;
        &lt;item&gt;I have been maintaining and updating the developer site personally - all open source. Having two sources of truth would be confusing for the community.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Accusation 4: ‘[Eric] scraped our app store, in violation of the agreement that we reached with him previously’&lt;/p&gt;
      &lt;p&gt;Note: ‘scraping’ usually means to automated extraction of data from a website.&lt;/p&gt;
      &lt;p&gt;Facts: &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Here’s what happened. I wanted to highlight some of my favourite watchfaces on the Pebble Appstore. Last Monday Nov 10, after I put my kids to sleep and between long calls with factories in Asia, I started building a webapp to help me quickly go through Pebble Appstore and decide which were my top picks.&lt;/item&gt;
        &lt;item&gt;Let me be crystal clear - my little webapp did not download apps or ‘scrape’ anything from Rebble. The webapp displayed the name of each watchface and screenshots and let me click on my favs. I used it to manually look through 6000 watchfaces with my own eyes. I still have 7,000 to go. Post your server logs, they will match up identically to the app I (well…Claude) wrote (source code here)&lt;/item&gt;
        &lt;item&gt;I integrated these picks into the Pebble Appstore on Saturday and posted about it on Sunday.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;All of four of these accusations could have been clarified simply by asking me. Instead, Rebble decided to post them on their blog and threaten a lawsuit. &lt;/p&gt;
      &lt;p&gt;How did we get here?&lt;/p&gt;
      &lt;p&gt;Why are there dueling blog posts in the Pebbleverse? &lt;/p&gt;
      &lt;p&gt;I think most of the people are behind Rebble are great and the community overall is awesome. I know they truly mean well, but there are many aspects of the org that are severely troubling. I am very close with one of the Rebble board members, who I consider a personal friend. Over the years, I learned a lot about the organization and helped coach him through some major disputes between board members. &lt;/p&gt;
      &lt;p&gt;I exchanged literally thousands of messages with my friend on this topic over the span of 3 years. I refrained from getting too involved, despite being asked several times to join Rebble as a board member or lead the organization. I demurred - I saw how painful it was for him and I had no interest in being part of that. &lt;/p&gt;
      &lt;p&gt;Core Devices + Rebble: 2025&lt;/p&gt;
      &lt;p&gt;PebbleOS is now open source! Yay. This is thanks to the work of many Googlers, ex-Pebblers and others - I called out (hopefully) all of them in my blog post in March. I really wanted Rebble to be a part of the Pebble revival going forward. I hired 3 people from Rebble to join Core Devices. I regularly brought up Rebble’s efforts over the years.&lt;/p&gt;
      &lt;p&gt;I engaged with Rebble folks in discussions in the spring on how we could formally work together, and then made some concrete proposals in the summer. One difficulty was that Core Devices is a business with customers and schedules. This didn’t always sync up with the timeframes of a non-profit. Things became very drawn out. It was very hard to pin people down, even on simple stuff like what the goals of Rebble as an organization were. &lt;/p&gt;
      &lt;p&gt;Regardless, I continued pushing to make Rebble a key part of the Pebble relaunch.&lt;/p&gt;
      &lt;p&gt;By August, we finally got close to an agreement.&lt;/p&gt;
      &lt;p&gt;On September 30 2025, we agreed to the following document and published respective blog posts (ours, theres). Core Devices would pay Rebble $0.20/user/month. I considered it a donation to a group that has done so much to support the community. But I purposely pushed for openness - no single group (Core Devices or Rebble) should be in control. &lt;/p&gt;
      &lt;p&gt;Notice the final bullet in the App store section: &lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;All binary/metadata (including historical apps) will be published as archive file (no scraping Rebble services) &lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;Looking back, we should have had more clear wording in this agreement. But this was after months of chat discussions and hours of Zoom calls. I honestly thought that we had reached an agreement to make the archive open, like in this message I received from a Rebble board member.&lt;/p&gt;
      &lt;p&gt;By the end of October, Rebble has changed their mind about providing an archive file.&lt;/p&gt;
      &lt;p&gt;Not withstanding their false accusations of theft, the crux of our disagreement is the archive of 13,000 Pebble apps and watchfaces that were uploaded to the Pebble Appstore in July 2018 before it was shut down. &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;I believe that these apps and watchfaces should be archived publicly and freely accessible by anyone. They should not held behind a walled garden by one organization. I repeatedly advocated for hosting this data on a neutral 3rd party like Archive.org.&lt;/item&gt;
        &lt;item&gt;Rebble believes ‘the data behind the Pebble App Store is 100% Rebble’ (this is a direct quote from their blog post). They repeatedly refer to all watchfaces and watchapps as ‘our data’.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;This is just plainly false. The apps and watchfaces were originally uploaded by individual developers to an appstore run by a company that no longer exists. These folks created beautiful work and shared them freely with the Pebble community. I’ve spoken with numerous Pebble app developers about this. After the fall of Pebble Tech Corp, none of them envisioned one single organization claiming ownership of their work and restricting access, or charging money for access.&lt;/p&gt;
      &lt;p&gt;Let’s do the right thing - honour the original developers and create a free publicly available archive of their beautiful watchfaces and watchapps. &lt;/p&gt;
      &lt;p&gt;It's easy to assume the worst in situations like this. But our plan for the appstore is pretty straightforward. We’re working on rewriting the appstore frontend to be native in the mobile app rather than a web view. Rebble’s appstore backend API will be the data source. Rebble’s dev portal is where developers upload apps. No subscription or Rebble account will not be required to download apps. We intend to curate how the appstore is displayed Pebble app.&lt;/p&gt;
      &lt;p&gt;We’re excited to see other Pebble-supporting mobile apps pop up - like MicroPebble and GadgetBridge, offering different features and experiences. We’d love to support these efforts with open source code or financially.&lt;/p&gt;
      &lt;p&gt;Reading things like ‘We’re happy to let them build whatever they want as long as it doesn’t hurt Rebble’ in their blog post worries me. Take our voice-to-text and weather features. Rebble currently offers these as part of their paid subscription. Our new Pebble mobile app includes a on-device speech-to-text feature. We’re planning to include weather for free in our app and make the data available to all watchfaces so you don’t need to configure each one separately. These features are better for users but would they ‘hurt’ Rebble? Will I need to ask permission from Rebble before building these features? It’s clear that the goals of a non-profit and device manufacturer will not always be in alignment.&lt;/p&gt;
      &lt;p&gt;Now consider the appstore. It’s a fundamental part of the Pebble experience. Even before yesterday’s accusations, I felt wary about relying too heavily on a 3rd party like Rebble to provide such a critical service. When people buy a watch from Core Devices, they expect to be able to download apps and watchfaces. If Rebble leadership changes their mind, how can I be certain I can deliver a good experience for our customers? This is one of the primary reasons I think it’s important for an archive of the Pebble Appstore to be freely available.&lt;/p&gt;
      &lt;p&gt;Rebble - prove that you believe in an open, unrestricted Pebble community. Tear down the walled garden you are trying to create. Publish your copy of the Pebble Appstore archive. Stop saying that you ‘100%’ own other developers data. Let’s move on from this ridiculous sideshow and focus on making Pebble awesome!&lt;/p&gt;
      &lt;p&gt;I’ve worked hard to structure everything that we’re doing to be sustainable for the long term, and to do right by the Pebble community. I think Rebble should do the same. &lt;/p&gt;
      &lt;p&gt;I earned almost nothing from Pebble Tech Corp. I paid myself a $65,000 salary each year. I did not get any payout through the asset sale. I fought to make sure that all Pebble employees were taken care of as best as possible, and that the Pebble community would live on. I believe that at every turn, I’ve done right by the community.&lt;/p&gt;
      &lt;p&gt;I didn’t relaunch Pebble to make a lot of money. My goal this time round is to make it sustainable. I want to continue making more watches and cool gadgets. There are no investors. I am taking huge risks doing this. I relaunched it because I love Pebble and want it to live on long into the future. Generally, I am excited and positive for the future, despite everything.&lt;/p&gt;
      &lt;p&gt;For everyone else, again, I apologize for the extreme amounts of inside baseball and the better things you could be doing with your time. I’ll leave the comments open here. Please refrain from any personal attacks or vicious comments (at myself or other people) - follow the HN guidelines.&lt;/p&gt;
      &lt;p&gt;Eric Migicovsky&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ericmigi.com/blog/pebble-rebble-and-a-path-forward/"/><published>2025-11-18T17:24:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45969909</id><title>I am stepping down as the CEO of Mastodon</title><updated>2025-11-19T13:46:53.255317+00:00</updated><content>&lt;doc fingerprint="6c92901d0c810af1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;My next chapter with Mastodon&lt;/head&gt;
    &lt;p&gt;Eugen Rochko&lt;/p&gt;
    &lt;p&gt;Strategy &amp;amp; Product Advisor, Founder&lt;/p&gt;
    &lt;p&gt;After nearly 10 years, I am stepping down as the CEO of Mastodon and transferring my ownership of the trademark and other assets to the Mastodon non-profit. Over the course of my time at Mastodon, I have centered myself less and less in our outward communications, and to some degree, this is the culmination of that trend. Mastodon is bigger than me, and though the technology we develop on is itself decentralized—with heaps of alternative fediverse projects demonstrating that participation in this ecosystem is possible without our involvement—it benefits our community to ensure that the project itself which so many people have come to love and depend on remains true to its values. There are too many examples of founder egos sabotaging thriving communities, and while I’d like to think myself an exception, I understand why people would prefer better guardrails.&lt;/p&gt;
    &lt;p&gt;But it would be uncouth for me to pretend that there isn’t some self-interest involved. Being in charge of a social media project is, turns out, quite the stressful endeavour, and I don’t have the right personality for it. I think I need not elaborate that the passion so many feel for social media does not always manifest in healthy ways. You are to be compared with tech billionaires, with their immense wealth and layered support systems, but with none of the money or resources. It manifests in what people expect of you, and how people talk about you. I remember somebody jokingly suggesting that I challenge Elon Musk to a fight (this was during his and Mark Zuckerberg’s martial arts feud), and quietly thinking to myself, I am literally not paid enough for that. I remember also, some Spanish newspaper article that for some reason, concluded that I don’t dress as fashionably as Jeff Bezos, based on the extremely sparse number of pictures of myself I have shared on the web. Over an entire decade, these tiny things chip away at you slowly. Some things chip faster. I steer clear of showing vulnerability online, but there was a particularly bad interaction with a user last summer that made me realise that I need to take a step back and find a healthier relationship with the project, ultimately serving as the impetus to begin this restructuring process.&lt;/p&gt;
    &lt;p&gt;As for what the legacy of my run will be, I find hard to answer. For one, I think it is not up for me to judge. On the other hand, it is as much about what didn’t happen as it is about what did. I’ve always thought that one of the most important responsibilities I had was to say “no”. It is not a popular thing to do, nor is it a fun thing to do, but being pulled into too many different directions at once can spell disaster for any project. I’d like to think I avoided some trouble by being careful. But I’m also aware that my aversion to public appearances cost Mastodon some opportunities in publicity. Ultimately, while I cannot take sole credit for it, I am nevertheless most proud of how far we’ve made it over these last 10 years. From the most barebones project written out of my childhood bedroom, to one of the last remaining and thriving pieces of the original, community-centred internet.&lt;/p&gt;
    &lt;p&gt;I have so much passion for Mastodon and the fediverse. The fediverse is an island within an increasingly dystopian capitalist hellscape. And from my perspective, Mastodon is our best shot at bringing this vision of a better future to the masses. This is why I’m sticking around, albeit in a more advisory, and less public, role.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.joinmastodon.org/2025/11/my-next-chapter-with-mastodon/"/><published>2025-11-18T18:13:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45970391</id><title>OrthoRoute – GPU-accelerated autorouting for KiCad</title><updated>2025-11-19T13:46:53.109183+00:00</updated><content>&lt;doc fingerprint="f2d15d492c15bffa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;OrthoRoute — GPU-Accelerated Autorouting for KiCad&lt;/head&gt;
    &lt;p&gt;OrthoRoute is a GPU-accelerated PCB autorouter that uses a Manhattan lattice and the PathFinder algorithm to route high-density boards. Built as a KiCad plugin using the IPC API, it handles complex designs with thousands of nets that make traditional push-and-shove routers give up.&lt;/p&gt;
    &lt;p&gt;Never trust the autorouter, but at least this one is fast.&lt;/p&gt;
    &lt;head rend="h4"&gt;This document is a complement to the README in the Github repository. The README provides information about performance, capabilities, and tests. This document reflects more on the why and how OrthoRoute was developed.&lt;/head&gt;
    &lt;head rend="h1"&gt;Why I Built This&lt;/head&gt;
    &lt;p&gt;This is a project born out of necessity. Another thing I was working on needed an enormous backplane. A PCB with sixteen connectors, with 1,100 pins on each connector. That’s 17,600 individual pads, and 8,192 airwires that need to be routed. Here, just take a look:&lt;/p&gt;
    &lt;p&gt;Look at that shit. Hand routing this would take months. For a laugh, I tried FreeRouting, the KiCad autorouter plugin, and it routed 4% of the traces in seven hours. If that trend held, which it wouldn’t, that would be a month of autorouting. And it probably wouldn’t work in the end. I had a few options, all of which would take far too long&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I could route the board by hand. This would be painful and take months, but I would get a good-looking board at the end.&lt;/item&gt;
      &lt;item&gt;I could YOLO everything and just let the FreeRouting autorouter handle it. It would take weeks, because the first traces are easy, the last traces take the longest. This would result in an ugly board.&lt;/item&gt;
      &lt;item&gt;I could spend a month or two building my own autorouter plugin for KiCad. I have a fairly powerful GPU and I thought routing a PCB is a very parallel problem. I could also implement my own routing algorithms to make the finished product look good.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When confronted with a task that will take months, always choose the more interesting path.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New KiCad API, and a ‘Traditional’ Autorouter&lt;/head&gt;
    &lt;p&gt;KiCad, Pre-version 9.0, had a SWIG-based plugin system. There are serious deficits with this system compared to the new IPC plugin system released with KiCad 9. The SWIG-based system was locked to the Python environment bundled with KiCad. Process isolation, threading, and performance constraints were a problem. Doing GPU programming with CuPy or PyTorch, while not impossible, is difficult.&lt;/p&gt;
    &lt;p&gt;The new IPC plugin system for KiCad is a godsend. The basic structure of the OrthoRoute plugin looks something like this:&lt;/p&gt;
    &lt;p&gt;The OrthoRoute plugin communicates with KiCad via the IPC API over a UNIX-ey socket. This API is basically a bunch of C++ classes that gives me access to board data – nets, pads, copper pour geometry, airwires, and everything else. This allows me to build a second model of a PCB inside a Python script and model it however I want. With a second model of a board inside my plugin, all I have to do is draw the rest of the owl.&lt;/p&gt;
    &lt;head rend="h2"&gt;Development of the Manhattan Routing Engine&lt;/head&gt;
    &lt;p&gt;After wrapping my head around the the ability to read and write board information to and from KiCad, I had to figure out a way to route this stupidly complex backplane. A non-orthogonal autorouter is a good starting point, but I simply used that as an exercise to wrap my head around the KiCad IPC API. The real build is a ‘Manhattan Orthogonal Routing Engine’, the tool needed to route my mess of a backplane.&lt;/p&gt;
    &lt;head rend="h3"&gt;Project PathFinder&lt;/head&gt;
    &lt;p&gt;The algorithm used for this autorouter is PathFinder: a negotiation-based performance-driven router for FPGAs. My implementation of PathFinder treats the PCB as a graph: nodes are intersections on an x–y grid where vias can go, and edges are the segments between intersections where copper traces can run. Each edge and node is treated as a shared resource.&lt;/p&gt;
    &lt;p&gt;PathFinder is iterative. In the first iteration, all nets (airwires) are routed greedily, without accounting for overuse of nodes or edges. Subsequent iterations account for congestion, increasing the “cost” of overused edges and ripping up the worst offenders to re-route them. Over time, the algorithm converges to a PCB layout where no edge or node is over-subscribed by multiple nets.&lt;/p&gt;
    &lt;p&gt;With this architecture – the PathFinder algorithm on a very large graph, within the same order of magnitude of the largest FPGAs – it makes sense to run the algorithm with GPU acceleration. There are a few factors that went into this decision:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Everyone who’s routing giant backplanes probably has a gaming PC. Or you can rent a GPU from whatever company is advertising on MUNI bus stops this month.&lt;/item&gt;
      &lt;item&gt;The PathFinder algorithm requires hundreds of billions of calculations for every iteration, making single-core CPU computation glacially slow.&lt;/item&gt;
      &lt;item&gt;With CUDA, I can implement a SSSP (parallel Dijkstra) to find a path through a weighted graph very fast.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Adapting FPGA Algorithms to PCBs&lt;/head&gt;
    &lt;p&gt;The original PathFinder paper was, “A Negotiation-Based Performance-Driven Router for FPGAs” and from 1995, this meant early FPGAs like the Xilinx 3000 series and others manufactured by Tryptych. These devices were simple, and to get a good idea of how they worked, check out Ken Shirriff’s blog. Here’s what the inside of a Xilinx XC2064 looks like:&lt;/p&gt;
    &lt;p&gt;That looks complicated, but it’s really exceptionally simple. All the LUTs, or logic elements, are connected to each other with wires. Where the wires cross over, there are fuzes. Burn the fuzes and you’ve connected the wires together. It’s a simple graph and all the complexity of the actual paths inside the chip are abstracted away. For a circuit board, I don’t have this luxury. I have to figure out how to get the signal from the pads on the top layer of the PCB and ‘drill down’ with vias into the grid. I need to come up with some way to account for both the edges of the graph and nodes of the graph, something that’s untread territory with the PathFinder algorithm.&lt;/p&gt;
    &lt;p&gt;The first step of that is the pad escape planner that pre-computes the escape routing of all the pads. Because the entire Manhattan Routing Engine is designed for a backplane, we can make some assumptions: All of the components are going to be SMD, because THT parts would kill the efficiency of a routing lattice. The components are going to be arranged on a grid, and just to be nice I’d like some ‘randomization’ in where it puts the vias punching down into the grid. Here’s what the escape planning looks like:&lt;/p&gt;
    &lt;head rend="h3"&gt;How PathFinder Almost Killed Me, and How I made PathFinder not suck&lt;/head&gt;
    &lt;p&gt;I found every bug imaginable while developing OrthoRoute. For one, congestion of nets would grow each iterations. The router would start fine with 9,495 edges with congestion in iteration 1. Then iteration 2: 18,636 edges. Iteration 3: 36,998 edges. The overuse was growing by 3× per iteration instead of converging. Something was fundamentally broken. The culprit? History costs were decaying instead of accumulating. The algorithm needs to remember which edges were problematic in past iterations, but my implementation had &lt;code&gt;history_decay=0.995&lt;/code&gt;, so it was forgetting 0.5% of the problem every iteration. By iteration 10, it had forgotten everything. No memory = no learning = explosion.&lt;/p&gt;
    &lt;p&gt;With the history fixed, I ran another test. I got oscillation. The algorithm would improve for 12 iterations (9,495 → 5,527, a 42% improvement!), then spike back to 11,817, then drop to 7,252, then spike to 14,000. The pattern repeated forever. The problem was “adaptive hotset sizing”—when progress slowed, the algorithm would enlarge the set of nets being rerouted from 150 to 225, causing massive disruption. Fixing the hotset at 100 nets eliminated the oscillation.&lt;/p&gt;
    &lt;p&gt;Even with fixed hotsets, late-stage oscillation returned after iteration 15. Why? The present cost factor escalates exponentially: &lt;code&gt;pres_fac = 1.15^iteration&lt;/code&gt;. By iteration 19, present cost was 12.4× stronger than iteration 1, completely overwhelming history (which grows linearly). The solution: cap &lt;code&gt;pres_fac_max=8.0&lt;/code&gt; to keep history competitive throughout convergence.&lt;/p&gt;
    &lt;p&gt;PathFinder is designed for FPGAs, and each and every Xilinx XC3000 chip is the same as every other XC3000 chip. Configuring the parameters for an old Xilinx chip means every routing problem will probably converge on that particular chip. PCBs are different; every single PCB is different from every other PCB. There is no single set of history, pressure, and decay parameters that will work on every single PCB.&lt;/p&gt;
    &lt;p&gt;What I had to do was figure out these paramaters on the fly. So that’s what I did. Right now I’m using Board-adaptive parameters for the Manhattan router. Before beginning the PathFinder algorithm it analyzes the board in KiCad for the number of signal layers, how many nets will be routed, and how dense the set of nets are. It’s clunky, but it kinda works.&lt;/p&gt;
    &lt;p&gt;Where PathFinder was tuned once for each family of FPGAs, I’m auto-tuning it for the entire class of circuit boards. A huge backplane gets careful routing and an Arduino clone gets fast, aggressive routing. The hope is that both will converge – produce a valid routing solution – and maybe that works. Maybe it doesn’t. There’s still more work to do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Routing The Monster Board&lt;/head&gt;
    &lt;p&gt;After significant testing with “small” boards (actually 500+ net subsets of my large backplane, with 18 layers), I started work on the entire purpose of this project, the 8000+ net, 17000 pad monster board. There was one significant problem: it wouldn’t fit on my GPU. Admittedly, I only have a 16GB Nvidia 5080, but even this was far too small for the big backplane.&lt;/p&gt;
    &lt;p&gt;This led me to develop a ‘cloud routing solution’. It boils down to extracting a “OrthoRoute PCB file” from the OrthoRoute plugin. From there, I rent a Linux box with a GPU and run the autorouting algorithm with a headless mode. This produces an “OrthoRoute Solution file”. I import this back into KiCad by running the OrthoRoute plugin on my local machine, and importing the solution file, then pushing that to KiCad.&lt;/p&gt;
    &lt;p&gt;Here’s the result:&lt;/p&gt;
    &lt;p&gt;That’s it, that’s the finished board. A few specs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;44,233 blind and buried vias. 68,975 track segments.&lt;/item&gt;
      &lt;item&gt;Routed on an 80GB A100 GPU, rented on vast.io. The total VRAM required to route this board was 33.5 GB, so close to being under 32GB and allowing me to rent a cheaper GPU&lt;/item&gt;
      &lt;item&gt;Total time to route this board to completion was 41 hours. This is far better than the months it would have taken FreeRouting to route this board, but it’s still not fast.&lt;/item&gt;
      &lt;item&gt;The routing result is good but not great. A big problem is the DRC-awareness of the escape pad planning. There are traces that don’t quite overlap, but because of the geometry generated by the escape route planner they don’t pass a strict DRC. This could be fixed in future versions. There are also some overlapping traces in what PathFinder generated. Not many, but a few.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While the output from my autorouter isn’t perfect, no one would expect an autorouter to produce a perfect result, ready for production. It’s an autorouter, something you shouldn’t trust. Turning the result for OrthoRoute into a DRC-compliant board took a few days, but it was far easier than the intractable problem of eight thousand airwires I had at the beginning.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Future of OrthoRoute&lt;/head&gt;
    &lt;p&gt;I built this for one reason: to route my pathologically large backplane. Mission accomplished. And along the way, I accidentally built something more useful than I expected.&lt;/p&gt;
    &lt;p&gt;OrthoRoute proves that GPU-accelerated routing isn’t just theoretical, and that algorithms designed for routing FPGAs can be adapted to the more general class of circuit boards. It’s fast, too. The Manhattan lattice approach handles high-density designs that make traditional autorouters choke. And the PathFinder implementation converges in minutes on boards that would take hours or days with CPU-based approaches.&lt;/p&gt;
    &lt;p&gt;More importantly, the architecture is modular. The hard parts—KiCad IPC integration, GPU acceleration framework, DRC-aware routing space generation are done. Adding new routing strategies on top of this foundation is straightforward. Someone could implement different algorithms, optimize for specific board types, or extend it to handle flex PCBs.&lt;/p&gt;
    &lt;p&gt;The code is up on GitHub. I’m genuinely curious what other people will do with it. Want to add different routing strategies? Optimize for RF boards? Extend it to flex PCBs? PRs welcome, contributors welcome.&lt;/p&gt;
    &lt;p&gt;And yes, you should still manually route critical signals. But for dense digital boards with hundreds of mundane power and data nets? Let the GPU handle it while you grab coffee. That’s what autorouters are for.&lt;/p&gt;
    &lt;p&gt;Never trust the autorouter. But at least this one is fast.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bbenchoff.github.io/pages/OrthoRoute.html"/><published>2025-11-18T18:54:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45972519</id><title>Blender 5.0</title><updated>2025-11-19T13:46:52.919993+00:00</updated><content/><link href="https://www.blender.org/download/releases/5-0/"/><published>2025-11-18T21:39:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45973709</id><title>Cloudflare outage on November 18, 2025 post mortem</title><updated>2025-11-19T13:46:52.613321+00:00</updated><content>&lt;doc fingerprint="bc1b3b5b0cb0a8cb"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;On 18 November 2025 at 11:20 UTC (all times in this blog are UTC), Cloudflare's network began experiencing significant failures to deliver core network traffic. This showed up to Internet users trying to access our customers' sites as an error page indicating a failure within Cloudflare's network. &lt;/p&gt;
      &lt;p&gt;The issue was not caused, directly or indirectly, by a cyber attack or malicious activity of any kind. Instead, it was triggered by a change to one of our database systems' permissions which caused the database to output multiple entries into a âfeature fileâ used by our Bot Management system. That feature file, in turn, doubled in size. The larger-than-expected feature file was then propagated to all the machines that make up our network.&lt;/p&gt;
      &lt;p&gt;The software running on these machines to route traffic across our network reads this feature file to keep our Bot Management system up to date with ever changing threats. The software had a limit on the size of the feature file that was below its doubled size. That caused the software to fail.&lt;/p&gt;
      &lt;p&gt;After we initially wrongly suspected the symptoms we were seeing were caused by a hyper-scale DDoS attack, we correctly identified the core issue and were able to stop the propagation of the larger-than-expected feature file and replace it with an earlier version of the file. Core traffic was largely flowing as normal by 14:30. We worked over the next few hours to mitigate increased load on various parts of our network as traffic rushed back online. As of 17:06 all systems at Cloudflare were functioning as normal.&lt;/p&gt;
      &lt;p&gt;We are sorry for the impact to our customers and to the Internet in general. Given Cloudflare's importance in the Internet ecosystem any outage of any of our systems is unacceptable. That there was a period of time where our network was not able to route traffic is deeply painful to every member of our team. We know we let you down today.&lt;/p&gt;
      &lt;p&gt;This post is an in-depth recount of exactly what happened and what systems and processes failed. It is also the beginning, though not the end, of what we plan to do in order to make sure an outage like this will not happen again.&lt;/p&gt;
      &lt;p&gt;The chart below shows the volume of 5xx error HTTP status codes served by the Cloudflare network. Normally this should be very low, and it was right up until the start of the outage. &lt;/p&gt;
      &lt;p&gt;The volume prior to 11:20 is the expected baseline of 5xx errors observed across our network. The spike, and subsequent fluctuations, show our system failing due to loading the incorrect feature file. Whatâs notable is that our system would then recover for a period. This was very unusual behavior for an internal error.&lt;/p&gt;
      &lt;p&gt;The explanation was that the file was being generated every five minutes by a query running on a ClickHouse database cluster, which was being gradually updated to improve permissions management. Bad data was only generated if the query ran on a part of the cluster which had been updated. As a result, every five minutes there was a chance of either a good or a bad set of configuration files being generated and rapidly propagated across the network.&lt;/p&gt;
      &lt;p&gt;This fluctuation made it unclear what was happening as the entire system would recover and then fail again as sometimes good, sometimes bad configuration files were distributed to our network. Initially, this led us to believe this might be caused by an attack. Eventually, every ClickHouse node was generating the bad configuration file and the fluctuation stabilized in the failing state.&lt;/p&gt;
      &lt;p&gt;Errors continued until the underlying issue was identified and resolved starting at 14:30. We solved the problem by stopping the generation and propagation of the bad feature file and manually inserting a known good file into the feature file distribution queue. And then forcing a restart of our core proxy.&lt;/p&gt;
      &lt;p&gt;The remaining long tail in the chart above is our team restarting remaining services that had entered a bad state, with 5xx error code volume returning to normal at 17:06.&lt;/p&gt;
      &lt;p&gt;The following services were impacted:&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;
            &lt;p&gt;Service / Product&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell role="head"&gt;
            &lt;p&gt;Impact description&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Core CDN and security services&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;HTTP 5xx status codes. The screenshot at the top of this post shows a typical error page delivered to end users.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Turnstile&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Turnstile failed to load.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Workers KV&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Workers KV returned a significantly elevated level of HTTP 5xx errors as requests to KVâs âfront endâ gateway failed due to the core proxy failing.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Dashboard&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;While the dashboard was mostly operational, most users were unable to log in due to Turnstile being unavailable on the login page.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Email Security&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;While email processing and delivery were unaffected, we observed a temporary loss of access to an IP reputation source which reduced spam-detection accuracy and prevented some new-domain-age detections from triggering, with no critical customer impact observed. We also saw failures in some Auto Move actions; all affected messages have been reviewed and remediated.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Access&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Authentication failures were widespread for most users, beginning at the start of the incident and continuing until the rollback was initiated at 13:05. Any existing Access sessions were unaffected.&lt;/p&gt;
            &lt;p&gt;All failed authentication attempts resulted in an error page, meaning none of these users ever reached the target application while authentication was failing. Successful logins during this period were correctly logged during this incident.Â &lt;/p&gt;
            &lt;p&gt;Any Access configuration updates attempted at that time would have either failed outright or propagated very slowly. All configuration updates are now recovered.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt;As well as returning HTTP 5xx errors, we observed significant increases in latency of responses from our CDN during the impact period. This was due to large amounts of CPU being consumed by our debugging and observability systems, which automatically enhance uncaught errors with additional debugging information.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;How Cloudflare processes requests, and how this went wrong today&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Every request to Cloudflare takes a well-defined path through our network. It could be from a browser loading a webpage, a mobile app calling an API, or automated traffic from another service. These requests first terminate at our HTTP and TLS layer, then flow into our core proxy system (which we call FL for âFrontlineâ), and finally through Pingora, which performs cache lookups or fetches data from the origin if needed.&lt;/p&gt;
      &lt;p&gt;We previously shared more detail about how the core proxy works here.Â &lt;/p&gt;
      &lt;p&gt;As a request transits the core proxy, we run the various security and performance products available in our network. The proxy applies each customerâs unique configuration and settings, from enforcing WAF rules and DDoS protection to routing traffic to the Developer Platform and R2. It accomplishes this through a set of domain-specific modules that apply the configuration and policy rules to traffic transiting our proxy.&lt;/p&gt;
      &lt;p&gt;One of those modules, Bot Management, was the source of todayâs outage.Â &lt;/p&gt;
      &lt;p&gt;Cloudflareâs Bot Management includes, among other systems, a machine learning model that we use to generate bot scores for every request traversing our network. Our customers use bot scores to control which bots are allowed to access their sites â or not.&lt;/p&gt;
      &lt;p&gt;The model takes as input a âfeatureâ configuration file. A feature, in this context, is an individual trait used by the machine learning model to make a prediction about whether the request was automated or not. The feature configuration file is a collection of individual features.&lt;/p&gt;
      &lt;p&gt;This feature file is refreshed every few minutes and published to our entire network and allows us to react to variations in traffic flows across the Internet. It allows us to react to new types of bots and new bot attacks. So itâs critical that it is rolled out frequently and rapidly as bad actors change their tactics quickly.&lt;/p&gt;
      &lt;p&gt;A change in our underlying ClickHouse query behaviour (explained below) that generates this file caused it to have a large number of duplicate âfeatureâ rows. This changed the size of the previously fixed-size feature configuration file, causing the bots module to trigger an error.&lt;/p&gt;
      &lt;p&gt;As a result, HTTP 5xx error codes were returned by the core proxy system that handles traffic processing for our customers, for any traffic that depended on the bots module. This also affected Workers KV and Access, which rely on the core proxy.&lt;/p&gt;
      &lt;p&gt;Unrelated to this incident, we were and are currently migrating our customer traffic to a new version of our proxy service, internally known as FL2. Both versions were affected by the issue, although the impact observed was different.&lt;/p&gt;
      &lt;p&gt;Customers deployed on the new FL2 proxy engine, observed HTTP 5xx errors. Customers on our old proxy engine, known as FL, did not see errors, but bot scores were not generated correctly, resulting in all traffic receiving a bot score of zero. Customers that had rules deployed to block bots would have seen large numbers of false positives. Customers who were not using our bot score in their rules did not see any impact.&lt;/p&gt;
      &lt;p&gt;Throwing us off and making us believe this might have been an attack was another apparent symptom we observed: Cloudflareâs status page went down. The status page is hosted completely off Cloudflareâs infrastructure with no dependencies on Cloudflare. While it turned out to be a coincidence, it led some of the team diagnosing the issue to believe that an attacker may be targeting both our systems as well as our status page. Visitors to the status page at that time were greeted by an error message:&lt;/p&gt;
      &lt;p&gt;In the internal incident chat room, we were concerned that this might be the continuation of the recent spate of high volume Aisuru DDoS attacks:&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;The query behaviour change&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;I mentioned above that a change in the underlying query behaviour resulted in the feature file containing a large number of duplicate rows. The database system in question uses ClickHouseâs software.&lt;/p&gt;
      &lt;p&gt;For context, itâs helpful to know how ClickHouse distributed queries work. A ClickHouse cluster consists of many shards. To query data from all shards, we have so-called distributed tables (powered by the table engine &lt;code&gt;Distributed&lt;/code&gt;) in a database called &lt;code&gt;default&lt;/code&gt;. The Distributed engine queries underlying tables in a database &lt;code&gt;r0&lt;/code&gt;. The underlying tables are where data is stored on each shard of a ClickHouse cluster.&lt;/p&gt;
      &lt;p&gt;Queries to the distributed tables run through a shared system account. As part of efforts to improve our distributed queries security and reliability, thereâs work being done to make them run under the initial user accounts instead.&lt;/p&gt;
      &lt;p&gt;Before today, ClickHouse users would only see the tables in the &lt;code&gt;default&lt;/code&gt; database when querying table metadata from ClickHouse system tables such as &lt;code&gt;system.tables&lt;/code&gt; or &lt;code&gt;system.columns&lt;/code&gt;.&lt;/p&gt;
      &lt;p&gt;Since users already have implicit access to underlying tables in &lt;code&gt;r0&lt;/code&gt;, we made a change at 11:05 to make this access explicit, so that users can see the metadata of these tables as well. By making sure that all distributed subqueries can run under the initial user, query limits and access grants can be evaluated in a more fine-grained manner, avoiding one bad subquery from a user affecting others.&lt;/p&gt;
      &lt;p&gt;The change explained above resulted in all users accessing accurate metadata about tables they have access to. Unfortunately, there were assumptions made in the past, that the list of columns returned by a query like this would only include the â&lt;code&gt;default&lt;/code&gt;â database:&lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;SELECT
  name,
  type
FROM system.columns
WHERE
  table = 'http_requests_features'
order by name;&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;Note how the query does not filter for the database name. With us gradually rolling out the explicit grants to users of a given ClickHouse cluster, after the change at 11:05 the query above started returning âduplicatesâ of columns because those were for underlying tables stored in the r0 database.&lt;/p&gt;
      &lt;p&gt;This, unfortunately, was the type of query that was performed by the Bot Management feature file generation logic to construct each input âfeatureâ for the file mentioned at the beginning of this section.Â &lt;/p&gt;
      &lt;p&gt;The query above would return a table of columns like the one displayed (simplified example):&lt;/p&gt;
      &lt;p&gt;However, as part of the additional permissions that were granted to the user, the response now contained all the metadata of the &lt;code&gt;r0&lt;/code&gt; schema effectively more than doubling the rows in the response ultimately affecting the number of rows (i.e. features) in the final file output.Â &lt;/p&gt;
      &lt;p&gt;Each module running on our proxy service has a number of limits in place to avoid unbounded memory consumption and to preallocate memory as a performance optimization. In this specific instance, the Bot Management system has a limit on the number of machine learning features that can be used at runtime. Currently that limit is set to 200, well above our current use of ~60 features. Again, the limit exists because for performance reasons we preallocate memory for the features.&lt;/p&gt;
      &lt;p&gt;When the bad file with more than 200 features was propagated to our servers, this limit was hit â resulting in the system panicking. The FL2 Rust code that makes the check and was the source of the unhandled error is shown below:&lt;/p&gt;
      &lt;p&gt;This resulted in the following panic which in turn resulted in a 5xx error:&lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;thread fl2_worker_thread panicked: called Result::unwrap() on an Err value&lt;/code&gt;
      &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Other impact during the incident&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Other systems that rely on our core proxy were impacted during the incident. This included Workers KV and Cloudflare Access. The team was able to reduce the impact to these systems at 13:04, when a patch was made to Workers KV to bypass the core proxy. Subsequently, all downstream systems that rely on Workers KV (such as Access itself) observed a reduced error rate.Â &lt;/p&gt;
      &lt;p&gt;The Cloudflare Dashboard was also impacted due to both Workers KV being used internally and Cloudflare Turnstile being deployed as part of our login flow.&lt;/p&gt;
      &lt;p&gt;Turnstile was impacted by this outage, resulting in customers who did not have an active dashboard session being unable to log in. This showed up as reduced availability during two time periods: from 11:30 to 13:10, and between 14:40 and 15:30, as seen in the graph below.&lt;/p&gt;
      &lt;p&gt;The first period, from 11:30 to 13:10, was due to the impact to Workers KV, which some control plane and dashboard functions rely upon. This was restored at 13:10, when Workers KV bypassed the core proxy system. The second period of impact to the dashboard occurred after restoring the feature configuration data. A backlog of login attempts began to overwhelm the dashboard. This backlog, in combination with retry attempts, resulted in elevated latency, reducing dashboard availability. Scaling control plane concurrency restored availability at approximately 15:30.&lt;/p&gt;
      &lt;p&gt;Now that our systems are back online and functioning normally, work has already begun on how we will harden them against failures like this in the future. In particular we are:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Hardening ingestion of Cloudflare-generated configuration files in the same way we would for user-generated input&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Enabling more global kill switches for features&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Eliminating the ability for core dumps or other error reports to overwhelm system resources&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Reviewing failure modes for error conditions across all core proxy modules&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Today was Cloudflare's worst outage since 2019. We've had outages that have made our dashboard unavailable. Some that have caused newer features to not be available for a period of time. But in the last 6+ years we've not had another outage that has caused the majority of core traffic to stop flowing through our network.&lt;/p&gt;
      &lt;p&gt;An outage like today is unacceptable. We've architected our systems to be highly resilient to failure to ensure traffic will always continue to flow. When we've had outages in the past it's always led to us building new, more resilient systems.&lt;/p&gt;
      &lt;p&gt;On behalf of the entire team at Cloudflare, I would like to apologize for the pain we caused the Internet today. &lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;
            &lt;p&gt;Time (UTC)&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell role="head"&gt;
            &lt;p&gt;Status&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell role="head"&gt;
            &lt;p&gt;Description&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;11:05&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Normal.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Database access control change deployed.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;11:28&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Impact starts.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Deployment reaches customer environments, first errors observed on customer HTTP traffic.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;11:32-13:05&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;The team investigated elevated traffic levels and errors to Workers KV service.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;The initial symptom appeared to be degraded Workers KV response rate causing downstream impact on other Cloudflare services.&lt;/p&gt;
            &lt;p&gt;Mitigations such as traffic manipulation and account limiting were attempted to bring the Workers KV service back to normal operating levels.&lt;/p&gt;
            &lt;p&gt;The first automated test detected the issue at 11:31 and manual investigation started at 11:32. The incident call was created at 11:35.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;13:05&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Workers KV and Cloudflare Access bypass implemented â impact reduced.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;During investigation, we used internal system bypasses for Workers KV and Cloudflare Access so they fell back to a prior version of our core proxy. Although the issue was also present in prior versions of our proxy, the impact was smaller as described below.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;13:37&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Work focused on rollback of the Bot Management configuration file to a last-known-good version.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;We were confident that the Bot Management configuration file was the trigger for the incident. Teams worked on ways to repair the service in multiple workstreams, with the fastest workstream a restore of a previous version of the file.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;14:24&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Stopped creation and propagation of new Bot Management configuration files.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;We identified that the Bot Management module was the source of the 500 errors and that this was caused by a bad configuration file. We stopped automatic deployment of new Bot Management configuration files.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;14:24&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Test of new file complete.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;We observed successful recovery using the old version of the configuration file and then focused on accelerating the fix globally.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;14:30&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Main impact resolved. Downstream impacted services started observing reduced errors.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;A correct Bot Management configuration file was deployed globally and most services started operating correctly.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;17:06&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;All services resolved. Impact ends.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;All downstream services restarted and all operations fully restored.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/18-november-2025-outage/"/><published>2025-11-18T23:31:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45974012</id><title>I made a down detector for down detector</title><updated>2025-11-19T13:46:52.066528+00:00</updated><content>&lt;doc fingerprint="7e59a9a7de17484b"&gt;
  &lt;main&gt;
    &lt;p&gt;A tiny independent status checker.&lt;/p&gt;
    &lt;p&gt;Waiting for the latest checks from all regions.&lt;/p&gt;
    &lt;p&gt;Checks by region&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;p&gt;Target: downdetector.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://downdetectorsdowndetector.com"/><published>2025-11-19T00:05:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45974681</id><title>Strace-macOS: A clone of the strace command for macOS</title><updated>2025-11-19T13:46:51.650243+00:00</updated><content>&lt;doc fingerprint="3a291a15e86658d0"&gt;
  &lt;main&gt;
    &lt;p&gt;A system call tracer for macOS using the LLDB debugger API.&lt;/p&gt;
    &lt;p&gt;Status: Beta - Core functionality works, but some features are still in development.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Works with SIP enabled - Unlike &lt;code&gt;dtruss&lt;/code&gt;, doesn't require disabling System Integrity Protection&lt;/item&gt;
      &lt;item&gt;Pure Python implementation - No kernel extensions or compiled components&lt;/item&gt;
      &lt;item&gt;Multiple output formats - JSON Lines and strace-compatible text output&lt;/item&gt;
      &lt;item&gt;Syscall filtering - Filter by syscall name or category (&lt;code&gt;-e trace=file&lt;/code&gt;,&lt;code&gt;-e trace=network&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Symbolic decoding - Automatically decodes flags, error codes, and struct fields&lt;/item&gt;
      &lt;item&gt;Color output - Syntax highlighting when output is a TTY&lt;/item&gt;
      &lt;item&gt;Summary statistics - Time/call/error counts with &lt;code&gt;-c&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Run directly
nix run github:Mic92/strace-macos -- ls

# Install to profile
nix profile install github:Mic92/strace-macos&lt;/code&gt;
    &lt;p&gt;strace-macos requires macOS system Python (has LLDB bindings):&lt;/p&gt;
    &lt;code&gt;# Install directly from GitHub
/usr/bin/python3 -m pip install --user git+https://github.com/Mic92/strace-macos

# Then run (if ~/Library/Python/3.x/bin is in PATH)
strace /usr/local/bin/git status  # or any homebrew-installed binary

# Or run directly from repository without installing
git clone https://github.com/Mic92/strace-macos
cd strace-macos
/usr/bin/python3 -m strace_macos /usr/local/bin/git status&lt;/code&gt;
    &lt;code&gt;# Basic usage (use non-system binaries like homebrew or nix-installed)
strace /usr/local/bin/git status

# Output to file
strace -o trace.txt /usr/local/bin/git status

# JSON output
strace --json /usr/local/bin/git status &amp;gt; trace.jsonl

# Filter syscalls by name
strace -e trace=open,close /usr/local/bin/git status

# Filter by category*
strace -e trace=file /usr/local/bin/git status    # All file operations
strace -e trace=network /usr/local/bin/curl https://example.com   # Network syscalls only
strace -e trace=process /usr/local/bin/git status # Process lifecycle syscalls&lt;/code&gt;
    &lt;p&gt;* See Syscall Filtering for all supported categories.&lt;/p&gt;
    &lt;code&gt;strace -p 1234&lt;/code&gt;
    &lt;code&gt;strace -c /usr/local/bin/git status
# % time     seconds  usecs/call     calls    errors syscall
# ------ ----------- ----------- --------- --------- ----------------
#  45.23    0.001234          12       103           read
#  32.10    0.000876           8       110           write
#  ...&lt;/code&gt;
    &lt;p&gt;strace-macos supports filtering syscalls by name or category using the &lt;code&gt;-e trace=&lt;/code&gt; option.&lt;/p&gt;
    &lt;p&gt;Specify one or more syscall names separated by commas:&lt;/p&gt;
    &lt;code&gt;strace -e trace=open,close,read,write /usr/local/bin/git status&lt;/code&gt;
    &lt;p&gt;Use predefined categories to trace groups of related syscalls:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example Syscalls&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;file&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;File operations&lt;/cell&gt;
        &lt;cell&gt;open, close, read, write, stat, unlink&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;network&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Network operations&lt;/cell&gt;
        &lt;cell&gt;socket, connect, send, recv, bind&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;process&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Process lifecycle&lt;/cell&gt;
        &lt;cell&gt;fork, exec, wait, exit, kill&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;memory&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Memory management&lt;/cell&gt;
        &lt;cell&gt;mmap, munmap, brk, mprotect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;signal&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Signal handling&lt;/cell&gt;
        &lt;cell&gt;signal, sigaction, sigprocmask, kill&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ipc&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Inter-process communication&lt;/cell&gt;
        &lt;cell&gt;pipe, shm_open, msgget, semop&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;thread&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Thread operations&lt;/cell&gt;
        &lt;cell&gt;pthread_create, bsdthread_register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;time&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Time and timers&lt;/cell&gt;
        &lt;cell&gt;gettimeofday, setitimer, utimes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;sysinfo&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;System information&lt;/cell&gt;
        &lt;cell&gt;sysctl, getpid, getuid, uname&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;security&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Security/MAC operations&lt;/cell&gt;
        &lt;cell&gt;__mac_*, csops, csrctl&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;debug&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Debugging and tracing&lt;/cell&gt;
        &lt;cell&gt;ptrace, kdebug_trace, panic_with_data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;misc&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Miscellaneous syscalls&lt;/cell&gt;
        &lt;cell&gt;ioctl, fcntl, kqueue, connectx&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# Trace only file operations
strace -e trace=file /usr/local/bin/git status

# Trace only network syscalls
strace -e trace=network /usr/local/bin/curl https://example.com

# Trace process management syscalls
strace -e trace=process /usr/local/bin/git status&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;Linux strace&lt;/cell&gt;
        &lt;cell role="head"&gt;strace-macos&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Filter by syscall name&lt;/cell&gt;
        &lt;cell&gt;✅ &lt;code&gt;-e trace=open,close&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;✅ &lt;code&gt;-e trace=open,close&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Filter by category&lt;/cell&gt;
        &lt;cell&gt;✅ &lt;code&gt;-e trace=file&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;✅ &lt;code&gt;-e trace=file&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Negation (&lt;code&gt;!&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;✅ &lt;code&gt;-e trace=!open&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;❌ Not yet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Regex filtering&lt;/cell&gt;
        &lt;cell&gt;✅ &lt;code&gt;-e trace=/^open/&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;❌ Not yet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Path filtering&lt;/cell&gt;
        &lt;cell&gt;✅ &lt;code&gt;-P /etc/passwd&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;❌ Not yet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;FD filtering&lt;/cell&gt;
        &lt;cell&gt;✅ &lt;code&gt;-e trace-fd=3&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;❌ Not yet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;%desc&lt;/code&gt; category&lt;/cell&gt;
        &lt;cell&gt;✅ FD-related syscalls&lt;/cell&gt;
        &lt;cell&gt;❌ Not yet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Percent prefix&lt;/cell&gt;
        &lt;cell&gt;✅ &lt;code&gt;%file&lt;/code&gt; or &lt;code&gt;file&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;file&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 12+ (Monterey or later)&lt;/item&gt;
      &lt;item&gt;Apple Silicon (ARM64) - primary platform&lt;/item&gt;
      &lt;item&gt;Intel (x86_64) - work in progress&lt;/item&gt;
      &lt;item&gt;Xcode Command Line Tools (for LLDB)&lt;/item&gt;
      &lt;item&gt;System Python (&lt;code&gt;/usr/bin/python3&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Important: Must use macOS system Python - LLDB bindings don't work with Homebrew/pyenv/Nix Python.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! See CONTRIBUTING.md for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Development environment setup&lt;/item&gt;
      &lt;item&gt;Code style guidelines&lt;/item&gt;
      &lt;item&gt;Testing instructions&lt;/item&gt;
      &lt;item&gt;How to add new syscalls&lt;/item&gt;
      &lt;item&gt;Pull request process&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Current Status: 3/13 tests passing (spawn functionality working)&lt;/p&gt;
    &lt;code&gt;strace-macos (Python CLI)
    ↓
LLDB Python API
    ↓
debugserver (macOS debugging APIs)
    ↓
Target Process
&lt;/code&gt;
    &lt;p&gt;The tracer uses LLDB's Python bindings to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set breakpoints at syscall entry/exit points&lt;/item&gt;
      &lt;item&gt;Read CPU registers to extract syscall arguments&lt;/item&gt;
      &lt;item&gt;Decode arguments symbolically (flags, errno, structs)&lt;/item&gt;
      &lt;item&gt;Format output in strace-compatible or JSON format&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Working:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spawn and trace new processes ✅&lt;/item&gt;
      &lt;item&gt;Attach to running processes ✅&lt;/item&gt;
      &lt;item&gt;Basic syscall capture (entry/exit) ✅&lt;/item&gt;
      &lt;item&gt;Argument decoding (integers, strings, pointers, buffers, iovecs) ✅&lt;/item&gt;
      &lt;item&gt;Symbolic flag decoding (O_RDONLY, etc.) ✅&lt;/item&gt;
      &lt;item&gt;Error code decoding (ENOENT, etc.) ✅&lt;/item&gt;
      &lt;item&gt;Struct decoding (stat, sockaddr, msghdr, etc.) ✅&lt;/item&gt;
      &lt;item&gt;Syscall filtering by name and category ✅&lt;/item&gt;
      &lt;item&gt;Summary statistics (&lt;code&gt;-c&lt;/code&gt;) ✅&lt;/item&gt;
      &lt;item&gt;JSON and text output formats ✅&lt;/item&gt;
      &lt;item&gt;Color output with syntax highlighting ✅&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Planned:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multi-threaded process support&lt;/item&gt;
      &lt;item&gt;Follow forks (&lt;code&gt;-f&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Negation filtering (&lt;code&gt;-e trace=!open&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Regex filtering (&lt;code&gt;-e trace=/^open/&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Path-based filtering (&lt;code&gt;-P /path&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;FD-based filtering (&lt;code&gt;-e trace-fd=3&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;String truncation control (&lt;code&gt;-s&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Relative/absolute timestamps (&lt;code&gt;-t&lt;/code&gt;,&lt;code&gt;-tt&lt;/code&gt;,&lt;code&gt;-ttt&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;macOS ships with &lt;code&gt;dtruss&lt;/code&gt;, a DTrace-based syscall tracer. However:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires disabling System Integrity Protection (SIP)&lt;/item&gt;
      &lt;item&gt;Doesn't work on modern macOS versions without workarounds&lt;/item&gt;
      &lt;item&gt;Limited filtering capabilities&lt;/item&gt;
      &lt;item&gt;No symbolic decoding of arguments&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;strace-macos works with SIP enabled and provides richer output.&lt;/p&gt;
    &lt;p&gt;strace-macos aims for compatibility with Linux strace where possible:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;Linux strace&lt;/cell&gt;
        &lt;cell role="head"&gt;strace-macos&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Basic tracing&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Attach to PID&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Syscall filtering*&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Summary stats&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Follow forks&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;⏳&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Symbolic decoding&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;JSON output&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Color output&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;* See Syscall Filtering for detailed feature comparison.&lt;/p&gt;
    &lt;p&gt;MIT License - see LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;Jörg Thalheim joerg@thalheim.io&lt;/p&gt;
    &lt;p&gt;For commercial support, please contact Mic92 at joerg@thalheim.io or reach out to Numtide.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CONTRIBUTING.md - Development and contribution guide&lt;/item&gt;
      &lt;item&gt;tests/README.md - Test suite documentation&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Mic92/strace-macos"/><published>2025-11-19T01:18:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45977457</id><title>Ultra-processed food linked to harm in every major human organ, study finds</title><updated>2025-11-19T13:46:51.567488+00:00</updated><content>&lt;doc fingerprint="ab8fd07c80188d2e"&gt;
  &lt;main&gt;
    &lt;p&gt;Ultra-processed food (UPF) is linked to harm in every major organ system of the human body and poses a seismic threat to global health, according to the world’s largest review.&lt;/p&gt;
    &lt;p&gt;UPF is also rapidly displacing fresh food in the diets of children and adults on every continent, and is associated with an increased risk of a dozen health conditions, including obesity, type 2 diabetes, heart disease and depression.&lt;/p&gt;
    &lt;p&gt;The sharp rise in UPF intake worldwide is being spurred by profit-driven corporations using a range of aggressive tactics to drive consumption, skewer scientific debate and prevent regulation, the review of evidence suggests.&lt;/p&gt;
    &lt;p&gt;The findings, from a series of three papers published in the Lancet, come as millions of people increasingly consume UPF such as ready meals, cereals, protein bars, fizzy drinks and fast food.&lt;/p&gt;
    &lt;p&gt;In the UK and US, more than half the average diet now consists of UPF. For some, especially people who are younger, poorer or from disadvantaged areas, a diet comprising as much as 80% UPF is typical.&lt;/p&gt;
    &lt;p&gt;Evidence reviewed by 43 of the world’s leading experts suggests that diets high in UPF are linked to overeating, poor nutritional quality and higher exposure to harmful chemicals and additives.&lt;/p&gt;
    &lt;p&gt;A systematic review of 104 long-term studies conducted for the series found 92 reported greater associated risks of one or more chronic diseases, and early death from all causes.&lt;/p&gt;
    &lt;p&gt;One of the Lancet series authors, Prof Carlos Monteiro, professor of public health nutrition at the University of São Paulo, said the findings underlined why urgent action is needed to tackle UPF.&lt;/p&gt;
    &lt;p&gt;“The first paper in this Lancet series indicates that ultra-processed foods harm every major organ system in the human body. The evidence strongly suggests that humans are not biologically adapted to consume them.”&lt;/p&gt;
    &lt;p&gt;He and his colleagues in Brazil came up with the Nova classification system for foods. It groups them by level of processing, ranging from one – unprocessed or minimally processed foods, such as whole fruits and vegetables – to four: ultra-processed.&lt;/p&gt;
    &lt;p&gt;This category is made up of products that have been industrially manufactured, often using artificial flavours, emulsifiers and colouring. They include soft drinks and packaged snacks, and tend to be extremely palatable and high in calories but low in nutrients.&lt;/p&gt;
    &lt;p&gt;They are also designed and marketed to displace fresh food and traditional meals, while maximising corporate profits, Monteiro said.&lt;/p&gt;
    &lt;p&gt;Critics argue UPF is an ill-defined category and existing health policies, such as those aimed at reducing sugar and salt consumption, are sufficient to deal with the threat.&lt;/p&gt;
    &lt;p&gt;Monteiro and his co-authors acknowledged valid scientific critiques of Nova and UPF – such as lack of long-term clinical and community trials, an emerging understanding of mechanisms, and the existence of subgroups with different nutritional values.&lt;/p&gt;
    &lt;p&gt;However, they argued future research must not delay immediate action to tackle the scourge of UPF, which they say is justified by the current evidence.&lt;/p&gt;
    &lt;p&gt;“The growing consumption of ultra-processed foods is reshaping diets worldwide, displacing fresh and minimally processed foods and meals,” Monteiro warned.&lt;/p&gt;
    &lt;p&gt;“This change in what people eat is fuelled by powerful global corporations who generate huge profits by prioritising ultra-processed products, supported by extensive marketing and political lobbying to stop effective public health policies to support healthy eating.”&lt;/p&gt;
    &lt;p&gt;The second paper in the series proposes policies to regulate and reduce UPF production, marketing and consumption. Although some countries have brought in rules to reformulate foods and control UPF, “the global public health response is still nascent, akin to where the tobacco control movement was decades ago”, it said.&lt;/p&gt;
    &lt;p&gt;The third paper says that global corporations, not individual choices, are driving the rise of UPF. UPF is a leading cause of the “chronic disease pandemic” linked to diet, with food companies putting profit above all else, the authors said.&lt;/p&gt;
    &lt;p&gt;The main barrier to protecting health is “corporate political activities, coordinated transnationally through a global network of front groups, multi-stakeholder initiatives, and research partners, to counter opposition and block regulation”.&lt;/p&gt;
    &lt;p&gt;Series co-author Prof Barry Popkin, from the University of North Carolina, said: “We call for including ingredients that are markers of UPFs in front-of-package labels, alongside excessive saturated fat, sugar, and salt, to prevent unhealthy ingredient substitutions, and enable more effective regulation.”&lt;/p&gt;
    &lt;p&gt;The authors also proposed stronger marketing restrictions, especially for adverts aimed at children, as well as banning UPF in public places such as schools and hospitals and putting limits on UPF sales and shelf space in supermarkets.&lt;/p&gt;
    &lt;p&gt;One success story is Brazil’s national school food programme, which has eliminated most UPF and will require 90% of food to be fresh or minimally processed by 2026.&lt;/p&gt;
    &lt;p&gt;Scientists not involved in the series broadly welcomed the review of evidence but also called for more research into UPF, cautioning that association with health harm may not mean causation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/society/2025/nov/18/ultra-processed-food-linked-to-harm-in-every-major-human-organ-study-finds"/><published>2025-11-19T09:15:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45977542</id><title>Multimodal Diffusion Language Models for Thinking-Aware Editing and Generation</title><updated>2025-11-19T13:46:51.473097+00:00</updated><content>&lt;doc fingerprint="732fca743eb04b92"&gt;
  &lt;main&gt;
    &lt;p&gt;While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. This model, MMaDA-Parallel, is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our approach significantly improves cross-modal alignment and semantic consistency, achieving a 6.9% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis.&lt;/p&gt;
    &lt;p&gt;Architecture of MMaDA-Parallel. During Training, image and text responses are masked and predicted in parallel with a uniform mask predictor. During Sampling, the model performs parallel decoding to generate both image and text responses jointly, enabling continuous cross-modal interaction.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[2025-11-11] We release our codes and models for MMaDA-Parallel, with two released 8B models MMaDA-Parallel-A and MMaDA-Parallel-M.&lt;/item&gt;
      &lt;item&gt;[2025-11-10] We release our research paper for Parallel Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Our model has been successfully validated on synthetic datasets focusing on environments, still life, architecture, and natural landscapes. Its performance on out-of-distribution inputs—such as human faces or real-world photographic imagery—has not yet been fully explored. We are actively expanding our training corpus to include more diverse datasets.&lt;/p&gt;
    &lt;p&gt;First, start with a torch environment with torch 2.3.1 or higher version, then install the following dependencies:&lt;/p&gt;
    &lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;
    &lt;p&gt;We provide two varients of MMaDA-Parallel with different tokenizers. MMaDA-Parallel-A is trained with tokenizer Amused-VQ, and MMaDA-Parallel-M is trained with tokenizer Magvitv2.&lt;/p&gt;
    &lt;p&gt;You can directly use the local gradio app to experience the parallel generation with MMaDA-Parallel-A:&lt;/p&gt;
    &lt;code&gt;python app.py&lt;/code&gt;
    &lt;p&gt;Or you can use the inference script to generate the parallel generation results:&lt;/p&gt;
    &lt;code&gt;cd MMaDA-Parallel-A
python inference.py \
    --checkpoint tyfeld/MMaDA-Parallel-A \
    --vae_ckpt tyfeld/MMaDA-Parallel-A \
    --prompt "Replace the laptops with futuristic transparent tablets displaying holographic screens, and change the drink to a cup of glowing blue energy drink." \
    --image_path examples/image.png \
    --height 512 \
    --width 512 \
    --timesteps 64 \
    --text_steps 128 \
    --text_gen_length 256 \
    --text_block_length 32 \
    --cfg_scale 0 \
    --cfg_img 4.0 \
    --temperature 1.0 \
    --text_temperature 0 \
    --seed 42 \
    --output_dir output/results_interleave&lt;/code&gt;
    &lt;code&gt;cd MMaDA-Parallel-M
python inference.py interleave_root=./interleave_validation  &lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Release the MMaDA-Parallel code and paper.&lt;/item&gt;
      &lt;item&gt;Evaluation on ParaBench code.&lt;/item&gt;
      &lt;item&gt;Refine MMaDA-Parallel-M and update the corresponding checkpoint.&lt;/item&gt;
      &lt;item&gt;Training code for SFT and ParaRL.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;@article{tian2025mmadaparallel,
  title={MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation},
  author={Tian, Ye and Yang, Ling and Yang, Jiongfan and Wang, Anran and Tian, Yu and Zheng, Jiani and Wang, Haochen and Teng, Zhiyang and Wang, Zhuochen and Wang, Yinjie and Tong, Yunhai and Wang, Mengdi and Li, Xiangtai},
  journal={arXiv preprint arXiv:2511.09611},
  year={2025}
}
&lt;/code&gt;
    &lt;p&gt;This work is heavily based on MMaDA and Lumina-DiMOO. Thanks to all the authors for their great work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/tyfeld/MMaDA-Parallel"/><published>2025-11-19T09:27:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45978541</id><title>The Cities Skylines Paradox: how the sequel stumbled</title><updated>2025-11-19T13:46:51.273038+00:00</updated><content>&lt;doc fingerprint="36a3f39136075ff8"&gt;
  &lt;main&gt;
    &lt;code&gt;@selix&lt;/code&gt;
    &lt;head rend="h1"&gt;The Cities Skyline Paradox&lt;/head&gt;
    &lt;p&gt;Why the sequel stumbled&lt;lb/&gt;and how a new studio might save it &lt;/p&gt;
    &lt;p&gt;Announcement: “An Update on Cities: Skylines II” (Paradox Interactive).&lt;/p&gt;
    &lt;p&gt;In mid-November 2025 Paradox Interactive and its long-time partner Colossal Order announced a quiet but monumental shift. After more than fifteen years together, the companies would “pursue independent paths”. The Cities: Skylines franchise – Paradox’s flagship city-building series – would be handed to Iceflake Studios, an internal Finnish team. Colossal Order (CO) would finish one last “Bike Patch” and an asset-editing beta, then move on to other projects. The announcement formalised a split that players and critics had anticipated for months. Cities: Skylines II (CS2) had launched in October 2023 to technical issues, design missteps and a conspicuous lack of mod support. A year later, many of those problems persisted, and Paradox’s patience wore thin.&lt;/p&gt;
    &lt;p&gt;In this article I attempt to disentangle the facts of that breakup, to understand why CO floundered, why Iceflake has been given the keys, and whether the sequel’s underlying issues can realistically be fixed.&lt;/p&gt;
    &lt;head rend="h2"&gt;A brief history of the series&lt;/head&gt;
    &lt;p&gt;Cities: Skylines (2015) emerged from the rubble of Maxis’ SimCity reboot, combining approachable city-planning mechanics with modding openness. Developed by the Helsinki-based Colossal Order and published by Paradox Interactive, CS1 quickly became the dominant city builder. Its success spawned dozens of expansions and thousands of user-made mods via Steam Workshop. CO – a studio of around thirty people – became a darling of the simulation genre.&lt;/p&gt;
    &lt;p&gt;Technical sources: Launch performance warning (GameSpot); CS2 performance analysis (Paavo Huhtala).&lt;/p&gt;
    &lt;p&gt;In 2023 CO attempted to leap ahead with a sequel. Built in Unity’s High Definition Render Pipeline (HDRP) and promising per-citizen simulation, a dynamic economy and cross-platform modding, CS2 launched on PC in October 2023. Even before release, Paradox warned that performance might not meet players’ expectations. The warning was prescient: the game shipped with heavy GPU bottlenecks, slow simulation speeds and a bare-bones economy. An autopsy by developer Paavo Huhtala found that every pedestrian model had 6,000 vertices (complete with fully modelled teeth) and that props such as pallet stacks were rendered in full detail even when invisible. The engine lacked occlusion culling and relied on high-resolution shadow maps, causing “an innumerable number of draw calls”. The result was a city builder that taxed even high-end GPUs while leaving CPU cores idle.&lt;/p&gt;
    &lt;p&gt;Player critique: “One Year Later – Cities: Skylines II Is Still a Broken, Lifeless Mess” (Paradox Plaza forums).&lt;/p&gt;
    &lt;p&gt;Alongside the rendering problems were deeper simulation issues. A year after release one forum thread titled “One Year Later – Cities: Skylines II Is Still a Broken, Lifeless Mess” complained of mindless citizens, dead public spaces and traffic AI that took nonsensical routes. The poster wrote that the sequel’s touted dynamic economy was “nonexistent”. Such criticisms weren’t isolated; they reflected a broader perception that CS2 had shipped as an unfinished Early Access game. CO acknowledged the problems and postponed the console release and paid DLC to focus on patches. Despite multiple updates, players still reported simulation slow-downs and path-finding issues in 2024 and 2025.&lt;/p&gt;
    &lt;p&gt;Modding coverage: Paradox Mods FAQ (Shacknews); Hallikainen on missing mod support (Game Rant).&lt;/p&gt;
    &lt;p&gt;Modding – a pillar of the first game – was largely absent. Paradox and CO announced that, unlike CS1’s open Steam Workshop, CS2 would use Paradox Mods, a centralised platform to ensure cross-platform compatibility. In October 2023 Shacknews quoted an official FAQ explaining that mods would be “confined in official capacity to the Paradox Mods platform” because the publisher wanted a single hub accessible on both PC and console. The FAQ went further: “We won’t support other platforms such as Steam Workshop”. This business decision frustrated PC modders and delayed many of the quality-of-life fixes that CS1 had enjoyed through community mods. In February 2024, CO CEO Mariina Hallikainen admitted that the team’s “biggest regret” was launching without mod support; Gamerant summarised her comments, noting that she acknowledged community frustration over the missing Editor and inadequate mod tools.&lt;/p&gt;
    &lt;head rend="h2"&gt;The facts of the change&lt;/head&gt;
    &lt;p&gt;Paradox’s November 17 2025 update sets out the formal arrangements. The post states that Paradox and Colossal Order “mutually decided to pursue independent paths” and that the decision was taken “thoughtfully and in the interest of both teams”. The Cities: Skylines franchise will move to Iceflake Studios, one of Paradox’s internal management-game teams based in Tampere, Finland. Iceflake will take over “all existing and future development” of CS2, including free updates, ongoing work on the in-game Editor and console versions, and future expansions. CO will deliver one final update, colloquially called the Bike Patch, adding bicycle infrastructure, Old Town buildings and bug fixes. A beta of the asset-editing tools will be released before year-end, after which Iceflake will assume full development duties from the start of 2026.&lt;/p&gt;
    &lt;p&gt;Statements from the principals frame the split as amicable. Hallikainen thanked Paradox for fifteen years of collaboration and said CO was “excited to channel our experience, creativity, and passion into new projects”. Paradox deputy CEO Mattias Lilja expressed gratitude for CO’s achievements and emphasised Paradox’s commitment to “provide [Cities players] with more content and new experiences”. Iceflake studio manager Lasse Liljedahl called taking the reins “an immense honor and a great responsibility” and said the team sees “a strong foundation and so much potential waiting to be unleashed”. Together, the statements project optimism: the old guard departs gracefully, the publisher pledges continued support, and a new studio vows to unlock the game’s latent promise.&lt;/p&gt;
    &lt;p&gt;On launching early: Paradox on Cities: Skylines II and iteration regrets (Kotaku).&lt;/p&gt;
    &lt;p&gt;Yet hidden between the lines is a tacit admission of failure. In an October 2024 interview discussed in Kotaku, Lilja conceded that launching CS2 in October 2023 was a mistake, saying that Paradox and CO were “actually in agreement that iterating this live was probably the right way to go” but that, in hindsight, they “should probably not launch that early”. In other words, the game was knowingly released unfinished with the hope that post-launch patches would complete it; the strategy backfired. By late 2025 the sequel remained tarnished, and shifting development to an internal studio gave Paradox a way to reframe the narrative without cancelling the project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Colossal Order faltered&lt;/head&gt;
    &lt;p&gt;Several interlocking factors contributed to CO’s struggles with CS2.&lt;/p&gt;
    &lt;head rend="h3"&gt;Technical overreach&lt;/head&gt;
    &lt;p&gt;The team aimed high: a next-generation city builder with per-citizen simulation, realistic economies and cinematic visuals. But CO was still a 30-person studio – tiny by AAA standards – and Unity HDRP proved unforgiving. The engine’s GPU bottlenecks weren’t the result of exotic path-tracing but of ordinary models being rendered at absurd detail. Buildings and props lacked lower-detail meshes and proper occlusion culling, so millions of polygons were drawn even when off-screen. Shadows were computed at high resolution for every object. These problems could theoretically be solved through asset rework and rendering optimisations, but doing so required months of drudge work and careful pipeline changes – hard tasks for a small team already firefighting bugs.&lt;/p&gt;
    &lt;p&gt;On the simulation side, CO promised a dynamic economy and deep agent-based behaviours, but the implementation lagged behind the ambition. Players complained that citizens moved like drones, parks were empty and emergency services were purely decorative. Traffic AI took nonsensical routes, and public transport usage barely affected congestion. Economic interactions between industries were shallow, and the employment model produced bizarre labour shortages or surpluses. Fixing such systemic issues often requires redesign rather than quick patches; CO did release an Economy 2.0 update in mid-2024, but by the time of the split the simulation still felt off.&lt;/p&gt;
    &lt;head rend="h3"&gt;Management and business constraints&lt;/head&gt;
    &lt;p&gt;CO was simultaneously developing the PC release, console ports and multiple DLCs while also building an entirely new modding platform. Paradox’s decision to use Paradox Mods for cross-platform compatibility meant that CO had to engineer modding tools that worked on PC, Xbox and PlayStation while meeting console platform security requirements. As the Shacknews article notes, Paradox and CO confirmed that mods would be “confined in official capacity to the Paradox Mods platform” and that there would be no official support for Steam Workshop or Nexus Mods. The rationale was to provide a “centralized, cross-platform hub”, but it removed the de-facto modding infrastructure that had empowered CS1. Building a secure, cross-platform modding system is a multi-year effort; CO underestimated the work and ended up shipping the game without modding tools at all. Hallikainen later called this omission their “biggest regret”.&lt;/p&gt;
    &lt;p&gt;At the same time, Paradox wanted a steady flow of revenue from DLC and console versions. Lilja’s comments reveal that the publisher deliberately chose to release early and iterate publicly. That strategy might work for small indie games, but CS2’s player base expected a polished sequel, and paying customers became unwilling beta testers. Patches that fixed one issue often introduced new bugs, and repeated delays of the console release eroded trust.&lt;/p&gt;
    &lt;head rend="h3"&gt;Human factors&lt;/head&gt;
    &lt;p&gt;CO’s team had been working on city-builders for over a decade. Burnout and fatigue likely played a role. The company’s history is entwined with the Cities series; moving on allows them to avoid being perpetually defined by “the team that broke Cities” and to experiment with new projects. Their public statements emphasise gratitude and optimism, suggesting that leaving the franchise was as much a relief as a dismissal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Iceflake might succeed&lt;/head&gt;
    &lt;p&gt;Past work: Surviving the Aftermath review (Screen Rant).&lt;/p&gt;
    &lt;p&gt;Iceflake Studios isn’t a household name, but it has relevant experience. Founded in 2007 and acquired by Paradox in 2020, Iceflake developed Surviving the Aftermath, a post-apocalyptic colony-builder that entered early access in 2019 and reached full release in November 2021. Screen Rant’s review described it as an “entertaining city-building game” and praised its blend of survival mechanics and management. The game launched rough in early access but steadily improved; by 1.0 it was viewed as “mixed or average” by Metacritic (around 69/100) and maintained a consistent player base. Unlike CS2, its challenges stemmed more from content depth and pacing than from catastrophic performance problems. Iceflake therefore has experience iterating a complex simulation into a stable product.&lt;/p&gt;
    &lt;p&gt;As an internal studio, Iceflake is directly accountable to Paradox. The publisher can allocate more resources, embed technical specialists and control the roadmap more closely than with an external partner. Iceflake also inherits CS2’s source code, toolchain and documentation. Without the emotional investment that CO had, Iceflake may be more willing to prune systems, simplify mechanics and cut features that don’t work. Liljedahl emphasised that Iceflake sees “a strong foundation and so much potential waiting to be unleashed”. The foundation isn’t nothing: CS2 has larger maps, improved road tools, realistic topography and flexible zoning. If Iceflake can optimise assets, implement proper level-of-detail and occlusion culling and iteratively rework the simulation, the game could reach a state where it’s enjoyable for mainstream players.&lt;/p&gt;
    &lt;p&gt;However, expectations must be managed. Iceflake cannot rewrite the engine from scratch. The Unity/HDRP foundation, the cross-platform modding constraints and many of the simulation patterns are baked in. The studio will likely focus on performance optimisation, bug fixing and incremental economy/traffic improvements rather than grand redesigns. The Paradox Mods platform will remain the only officially supported mod hub, so deep code mods akin to CS1’s may never return. That’s a business decision that Iceflake cannot overturn.&lt;/p&gt;
    &lt;head rend="h2"&gt;Paradox’s course correction&lt;/head&gt;
    &lt;p&gt;The publisher’s response to CS2’s troubled launch reveals a broader shift within Paradox. Kotaku’s October 2024 piece notes that Paradox executives have been on an “apology tour” addressing missteps across several projects, including Bloodlines 2, Prison Architect 2 and the cancelled Life By You. Lilja admitted to PC Gamer that they misjudged hardware compatibility and that releasing early was a misstep. By moving CS2 to an internal studio, Paradox signals a desire to control timelines, budgets and quality more tightly. It mirrors similar decisions: Paradox previously shifted development of Bloodlines 2 to a new studio and delayed Prison Architect 2 indefinitely due to technical problems. The company appears to be prioritising quality over rushing sequels out the door.&lt;/p&gt;
    &lt;p&gt;Paradox has also been transparent about what the short-term roadmap entails: the Bike Patch, asset-mod beta and ongoing console work. After Iceflake takes over, the studio will share its own plans. The messaging emphasises continuity rather than abandonment. There’s no talk of a Cities: Skylines III, and Paradox continues to encourage players to connect their Paradox accounts for cosmetic rewards. Whether this rebuilds trust depends on execution.&lt;/p&gt;
    &lt;p&gt;Ultimately, Cities: Skylines II is a cautionary tale of ambition outrunning capacity. Colossal Order set out to deliver the most realistic, detailed city-builder ever made but underestimated the technical and design challenges. A small team built an engine that rendered thousands of hidden vertices, shipped without proper mod support and relied on patches to finish the simulation. Paradox, eager to capitalise on the success of CS1, allowed an unfinished game to launch, hoping to “iterate live”. Players rightly rebelled. A year later the sequel still feels unfinished, and the publisher has handed the project to an internal studio while letting the original creators bow out gracefully.&lt;/p&gt;
    &lt;p&gt;Does this mean CS2 is doomed? Not necessarily. Iceflake inherits a game with a solid core and a passionate community. The studio’s history with Surviving the Aftermath shows it can shepherd a complex management game from rough early access to a polished release. Paradox’s decision to move development in-house suggests a willingness to allocate resources and accept delays. Significant performance fixes – better LODs, occlusion culling, asset optimisation – are engineering tasks that can be accomplished over time. Simulation adjustments to traffic and economy are harder but not impossible. What CS2 will never become is CS1 with all the modding freedom; the Paradox Mods platform and console parity goals make that clear. For players willing to accept that constraint, there is still hope that Iceflake can turn CS2 into a stable, satisfying city builder. The road will be long, but at least the car is now being driven by a team that isn’t running on fumes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://selix.net/notes/the-cities-skyline-paradox"/><published>2025-11-19T12:02:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45978545</id><title>Pimped Amiga 500</title><updated>2025-11-19T13:46:50.339776+00:00</updated><content>&lt;doc fingerprint="2c406c29212e33b1"&gt;
  &lt;main&gt;
    &lt;p&gt;Back in the early ’90s, I had an Amiga 2000 with just one expansion card: a SCSI controller paired with a massive 290 MB hard drive. Getting software and games to run from the hard drive—with only 1 MB of chip RAM—required a lot of tricks. But it was fun, and it taught me a lot about computers.&lt;/p&gt;
    &lt;p&gt;A few months ago, I stumbled upon a cheap Amiga 500, and I couldn’t resist. I decided to restore it from the ground up and add a GottaGoFast RAM + IDE controller to finally build what would have been my dream machine in 1990: an Amiga running OS 1.3 with fast RAM!&lt;/p&gt;
    &lt;p&gt;This is the story of my pimped Amiga 500: 1 MB chip RAM, 8 MB fast RAM, and 512 MB of storage. Quite a beast for its time! 🙂&lt;/p&gt;
    &lt;head rend="h2"&gt;Used Materials&lt;/head&gt;
    &lt;p&gt;Here is the hardwares pieces I used:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Amiga 500 I bought with a “512K memory expansions”&lt;/item&gt;
      &lt;item&gt;IDE68K + GottaGo FastRAM 8MB from AmigaStore.eu&lt;/item&gt;
      &lt;item&gt;A 512M CompaqFlash card (LIMEI, “professional grade”)&lt;/item&gt;
      &lt;item&gt;A 40 pin 3.5in IDE ribbon cable&lt;/item&gt;
      &lt;item&gt;A dremel to creare a compaqflash slot&lt;/item&gt;
      &lt;item&gt;Some (dupond) wires and solder&lt;/item&gt;
      &lt;item&gt;Some pin headers&lt;/item&gt;
      &lt;item&gt;A multimeter&lt;/item&gt;
      &lt;item&gt;Isopropyl alchool&lt;/item&gt;
      &lt;item&gt;Q-tips&lt;/item&gt;
      &lt;item&gt;Facom “Contact Spay”&lt;/item&gt;
      &lt;item&gt;Ambro-sol galvanized zinc spray paint&lt;/item&gt;
      &lt;item&gt;A driller and a dremel&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;First boot&lt;/head&gt;
    &lt;p&gt;At first, I had a hard time getting a Workbench disk to boot properly — even though the game disks I tested worked just fine. (They probably have better error correction routines.)&lt;/p&gt;
    &lt;p&gt;Each time I tried to start Workbench from different floppies, I ran into either “Read/Write Error” or “Please insert disk in drive 0!” messages. After several attempts and a few frustrating retries, I finally managed to reach a command prompt.&lt;/p&gt;
    &lt;p&gt;That’s when I noticed something strange: the system was reporting 1 MB of chip RAM. Wait a second — this is an Amiga 500, not a 500+! Even with a memory expansion, it should normally show 512 KB chip RAM and 512 KB slow RAM. This means my A500 must have been modified to convert the slow RAM into chip RAM. (For reference: “slow RAM” sits on the same bus as chip memory, but it’s not directly addressable by the custom chips.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Inside&lt;/head&gt;
    &lt;p&gt;Of course, I found a memory expansion installed: a SupraRam 500 Rev.2 (details here), identifiable by the four DIP switches. It’s a very neat card — the battery uses a standard coin cell, which is much less prone to leaking than typical NiMH batteries.&lt;lb/&gt;Here’s a look at the expansion card inside the machine:&lt;/p&gt;
    &lt;p&gt;The motherboard is a Rev 6A, which is internally ready for 1 MB of chip RAM but only has 512 KB installed. Judging by the setup, this Amiga seems to have been modified to provide 1 MB chip RAM: the JP7A jumper is fully open, and JP2 has pins 1 and 2 shorted!&lt;lb/&gt;As you can see in this photo, the jumpers reveal the modification:&lt;/p&gt;
    &lt;p&gt;Inside, there’s a fat Agnus 8372A (capable of addressing 1 MB chip RAM) paired with a Denise R8 (OCS) rather than a SuperDenise (ECS). While it’s not an ECS setup, this combination at least allows Extra Half-Brite (EHB) mode.&lt;lb/&gt;The Agnus and Denise chips are shown here, highlighting the OCS configuration:&lt;/p&gt;
    &lt;head rend="h2"&gt;Hardware restoration&lt;/head&gt;
    &lt;head rend="h3"&gt;Plastics&lt;/head&gt;
    &lt;p&gt;The plastics on this Amiga were just a bit yellowed — nothing too severe. I was able to recover them easily using the same Retrobright box I used for my pimped Amiga 600.&lt;/p&gt;
    &lt;p&gt;The power supply, however, had a noticeably stronger yellow tint compared to the other parts. I applied Retrobright to all components, and for the power supply, I gave it a longer exposure. It hasn’t fully returned to its original color, but it’s much improved.&lt;/p&gt;
    &lt;p&gt;On the left: before cleaning and Retrobright; on the right: after treatment:&lt;/p&gt;
    &lt;head rend="h3"&gt;Metallic shield&lt;/head&gt;
    &lt;p&gt;Both the upper and lower shield parts were in poor condition, showing some corrosion. While these shields aren’t strictly necessary for the Amiga to function, I wanted to keep my A500 as authentic as possible.&lt;/p&gt;
    &lt;p&gt;I treated the metal with Ambro-Sol spray (98% zinc) — a kind of metallic paint that also protects against corrosion. Before painting, I lightly sanded all corroded areas to ensure a smooth finish. The paint has a matte finish, which I actually prefer over the original look.&lt;/p&gt;
    &lt;p&gt;On the left: before painting; on the right: after treatment:&lt;/p&gt;
    &lt;head rend="h3"&gt;Keyboard&lt;/head&gt;
    &lt;p&gt;The keyboard was covered in dust and had a noticeable yellow tint. I removed all the keys to thoroughly clean each part and also subjected them to the Retrobright process.&lt;/p&gt;
    &lt;p&gt;Unfortunately, I didn’t take any photos of the cleaned keyboard on its own, but the results should be visible in the overall photos of the restored A500.&lt;/p&gt;
    &lt;head rend="h3"&gt;The mouse&lt;/head&gt;
    &lt;p&gt;The mouse wasn’t working properly and showed several issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The classic problem: dirty internal wheels.&lt;/item&gt;
      &lt;item&gt;The spring on the white wheel, which ensures the ball touches the encoder wheels, was too loose, so the ball didn’t make proper contact.&lt;/item&gt;
      &lt;item&gt;The right mouse button was dead or broken.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I replaced the right button with a new one — slightly more “clicky,” but it didn’t require any extra pressure to use.&lt;/p&gt;
    &lt;p&gt;Next, I cleaned all internal parts using alcohol and some Q-Tips, and I retensioned the spring by gently pulling both sides at the same time.&lt;/p&gt;
    &lt;p&gt;The final result: a cleaner interior and a fully functional, “like new” mouse.&lt;lb/&gt;Here’s the after look inside the mouse:&lt;/p&gt;
    &lt;head rend="h3"&gt;Floppy drive&lt;/head&gt;
    &lt;p&gt;The floppy drive in my Amiga 500 is a Matsushita JU-253-031P, recognizable by its plain black top cover over the mechanism. While it gives a clean look, it also makes dust removal more challenging compared to other drives.&lt;/p&gt;
    &lt;p&gt;I carefully used Q-Tips to remove as much dust as possible, paying special attention to the read/write heads, which are still easily accessible and crucial for reliable disk reading.&lt;/p&gt;
    &lt;p&gt;Additionally, I had to resolder the wires on the small floppy detector button, which had been causing the “Please insert disk in drive” errors during reading.&lt;/p&gt;
    &lt;p&gt;Here’s a look at the drive during cleaning and after reassembly:&lt;/p&gt;
    &lt;head rend="h3"&gt;Motherboard &amp;amp; Memory card&lt;/head&gt;
    &lt;p&gt;The motherboard was in pretty good condition. I simply applied some FACOM Contact Spray, which helps remove dust, humidity, and oxidation. It’s said to also provide some protection for the circuits — well, it certainly can’t hurt!&lt;/p&gt;
    &lt;p&gt;I did the same for the memory expansion card. Additionally, I replaced the soldered battery with a battery holder, making the setup cleaner and allowing the battery to be easily swapped in the future.&lt;/p&gt;
    &lt;p&gt;Here’s a look at the motherboard and memory card after cleaning and the battery upgrade:&lt;/p&gt;
    &lt;head rend="h2"&gt;Extentions&lt;/head&gt;
    &lt;p&gt;I installed the IDE68k + GottaGoFastram combo along with the patched Kickstart ROM that allows booting directly from an IDE device. I also picked up a 512 MB CompactFlash card, which provides more than enough space — considering that back in the mid-80s, even 20 or 40 MB felt enormous.&lt;lb/&gt;The patched Kickstart 1.3 includes an scsi.device , making it possible to boot from the emulated hard drive (the CF card). Without it, you would need to boot from a floppy — just like some disk controllers required back in the day.&lt;/p&gt;
    &lt;p&gt;Booting from the IDE interface requires two signals: /INT2 and /OVR.&lt;lb/&gt;The kit comes with Dupont wires and small clip-style “pliers” to grab these signals respectively from pin 21 of CIA A and pin 29 of Gary.&lt;lb/&gt;I wasn’t a fan of this approach — the clips are fragile and can easily detach when moving the Amiga.&lt;/p&gt;
    &lt;p&gt;Both signals are actually available on the Zorro II 86-pin connector next to the 68000 CPU (see: mklboards.fi).&lt;lb/&gt;So I decided to solder both wires directly to the Zorro II connector. It’s cleaner, safer, and mechanically rock-solid.&lt;/p&gt;
    &lt;p&gt;Here are the tests I ran before finalizing the modification:&lt;/p&gt;
    &lt;p&gt;At first, the CF wasn’t powering up. Pin 20 of the IDE connector should provide +5 V for powering CF cards, but I measured 0 V.&lt;lb/&gt;I ended up taking +5 V from the keyed pin on the adapter and wiring it directly to the CF’s 5 V pin.&lt;lb/&gt;It seems something is missing from the Amigashop.eu hardware or in the documentation, because the kit is supposed to include everything required.&lt;/p&gt;
    &lt;p&gt;To simplify things, I modified the CF adapter, removing the bottom power connector and adding only the single required +5 V pin on top.&lt;lb/&gt;This reduces the height of the board — which turned out to be necessary for the next step.&lt;/p&gt;
    &lt;p&gt;I slightly modified the A500 case to fit the CF card reader under the floppy drive, making card swaps extremely convenient without reopening the machine each time.&lt;lb/&gt;I began by drilling two holes to mount the reader from the underside of the chassis:&lt;/p&gt;
    &lt;p&gt;Then I placed the Cf card reader to calibrate the hole needed for the compaq flash to be inserted. It first made some small holes with a drill and I finished the job with a dremel.&lt;/p&gt;
    &lt;p&gt;Because of the new placement, I needed a longer ribbon cable between the CF adapter and the IDE controller.&lt;lb/&gt;I eventually took the required +5 V for the adapter from the floppy drive connector — cleaner and more reliable.&lt;/p&gt;
    &lt;p&gt;Finaly I added a red led to monitor IDE drive activity in addition to the floppy drive. In fact I used two 3mm leds glued between the two original ones of the Amiga 500. the mod is fully reversible. I used some aluminium adhesive to both isolate power led from the red light and better diffuse the red light on the original drive led. As you can see, there is one resistor for both leds.&lt;/p&gt;
    &lt;p&gt;Finally, I added a red LED to monitor IDE activity, complementing the original floppy LED.&lt;lb/&gt;I used two 3 mm LEDs glued between the Amiga’s two original indicators.&lt;lb/&gt;The mod is fully reversible.&lt;lb/&gt;I used aluminum adhesive tape to prevent the power LED from bleeding into the IDE LED, and to better diffuse the red light through the original light pipe.&lt;lb/&gt;A single resistor drives both LEDs.&lt;/p&gt;
    &lt;p&gt;The result looks great and gives clear feedback: IDE activity, floppy activity, or both at once.&lt;/p&gt;
    &lt;p&gt;On the left: no IDE or floppy activity — on the right: IDE activity.&lt;/p&gt;
    &lt;p&gt;Now: left = floppy only — right = both IDE and floppy working simultaneously:&lt;/p&gt;
    &lt;p&gt;With the hardware restored and the extensions fully installed, it was finally time to move on to the next step: preparing the operating system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Preparing OS Install&lt;/head&gt;
    &lt;head rend="h3"&gt;Amiberry configuration&lt;/head&gt;
    &lt;p&gt;To make the installation process easier, I prepared the system using Amiberry first. I used a Kickstart 1.3 ROM patched with IDE controller support, similar to the physical ROM I purchased from Amigastore.eu. The version I used can be found here: https://www.uprough.net/releases/Amiga_Roots_Music_Tools_Beta_2/&lt;/p&gt;
    &lt;p&gt;Below are the Amiberry settings I used to replicate my Amiga 500 hardware as closely as possible:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU: 68000, 24-bit addressing, A500 cycle-exact (×2)&lt;/item&gt;
      &lt;item&gt;Chipset: ECS Agnus with A600 selected under “Chipset Extra” — this is important, otherwise the IDE controller will be disabled&lt;/item&gt;
      &lt;item&gt;RAM: Same as my real A500 — 1 MB Chip, 8 MB Z2 Fast&lt;/item&gt;
      &lt;item&gt;Expansion: Enabled the A600 IDE controller&lt;/item&gt;
      &lt;item&gt;Hard Drive: Mapped the Linux device corresponding to my USB CF card reader, selected the Commodore A600 controller, and set the mode to ATA-1 — this is essential, or the CF card won’t be detected correctly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These settings allow Amiberry to behave almost exactly like the upgraded A500 hardware, making the OS installation straightforward and 100% compatible with the real machine.&lt;/p&gt;
    &lt;head rend="h3"&gt;HDToolsBox&lt;/head&gt;
    &lt;p&gt;Nothing particularly unusual here, except that I first had to free some space on the “IDE Setup” floppy (I honestly don’t remember where I originally got it). Without doing so, HDToolBox refused to save the new drive-type definition.&lt;lb/&gt;To make room, I simply removed the Shell program from that floppy, since it’s already available on the Workbench disk anyway.&lt;/p&gt;
    &lt;p&gt;Once that was sorted out, here’s what I essentially did:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edited the ToolTypes of HDToolBox to point to scsi.device&lt;/item&gt;
      &lt;item&gt;Launched HDToolBox.&lt;/item&gt;
      &lt;item&gt;Selected the CF drive and clicked “Change Drive Type”.&lt;/item&gt;
      &lt;item&gt;Created a new drive type definition.&lt;/item&gt;
      &lt;item&gt;Set the Manufacturer, Drive Name, and Revision fields.&lt;/item&gt;
      &lt;item&gt;Saved and selected this newly created drive type.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These steps allow HDToolBox to correctly recognize and handle the CF card as a proper fixed drive under Workbench.&lt;/p&gt;
    &lt;head rend="h3"&gt;Partitions&lt;/head&gt;
    &lt;p&gt;Below is the partitioning scheme I chose. I generally prefer to separate the operating system, its accompanying utilities, applications, games, and user data — essentially the Amiga equivalent of a “/home” directory.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DH0 – Workbench: 24 MB, 100 buffers&lt;/item&gt;
      &lt;item&gt;DH1 – Apps: 85 MB, 100 buffers&lt;/item&gt;
      &lt;item&gt;DH2 – Games: 140 MB, 100 buffers&lt;/item&gt;
      &lt;item&gt;DH3 – Data: 212 MB, 150 buffers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For all partitions, I used FFS with a maxtransfer value of 0x1FE00 .&lt;lb/&gt;I formatted each partition using a command like:&lt;/p&gt;
    &lt;p&gt;format DRIVE DH0 name Workbench FFS QUICK&lt;/p&gt;
    &lt;head rend="h3"&gt;Workbench 1.3 install&lt;/head&gt;
    &lt;p&gt;Installing Workbench 1.3 is fairly straightforward: it simply involves copying the contents of the Workbench and Extra disks onto the bootable partition, then editing the startup-sequence.&lt;lb/&gt;I later discovered that the A590 Install disk actually includes a dedicated tool for installing Workbench — but here’s the manual method I followed:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
          &lt;p&gt;3&lt;/p&gt;
          &lt;p&gt;4&lt;/p&gt;
          &lt;p&gt;5&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;copy workbench1.3: to dh0: all clone&lt;/p&gt;
          &lt;p&gt;copy "extra 1.3:" to dh0: all done&lt;/p&gt;
          &lt;p&gt;rename DH0:s/startup-sequence DH0:s/startup-sequence.FD&lt;/p&gt;
          &lt;p&gt;rename DH0:s/startup-sequence.HD DH0:s/startup-sequence&lt;/p&gt;
          &lt;p&gt;edit DH0:s/startup-sequence ; replace the call "Execute s:Startup-sequence" by "Execute s:Startup-sequence.FD"&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I also copied HDToolBox from the “IDE Setup” disk into &lt;code&gt;DH0:/Tools&lt;/code&gt; for convenience.&lt;/p&gt;
    &lt;p&gt;After removing all floppy disks and resetting the virtual machine, the Amiga immediately booted from the hard drive.&lt;lb/&gt;Before applying any customisations, I confirmed that everything worked properly on the real hardware.&lt;/p&gt;
    &lt;p&gt;Here’s the Workbench 1.3 booting directly from the CF card:&lt;/p&gt;
    &lt;head rend="h2"&gt;Installed Software&lt;/head&gt;
    &lt;p&gt;In this chapter, I’m going to give an overview of all the software I installed on the A500, along with their sources — and no, it’s not always from Aminet.net!&lt;/p&gt;
    &lt;head rend="h3"&gt;Sources&lt;/head&gt;
    &lt;head rend="h4"&gt;Where I got it&lt;/head&gt;
    &lt;p&gt;Before diving into the software itself, here’s a quick overview of the main sources I used to gather everything described in this chapter&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;List of software compatible with OS 1.3 https://eab.abime.net/showthread.php?t=52283&lt;/item&gt;
      &lt;item&gt;https://demozoo.org/&lt;/item&gt;
      &lt;item&gt;https://ftp.funet.fi/pub/amiga/&lt;/item&gt;
      &lt;item&gt;Workbench 13. Extras&lt;/item&gt;
      &lt;item&gt;https://amr.abime.net/&lt;/item&gt;
      &lt;item&gt;https://archive.org/download/CommodoreAmigaApplicationsADF&lt;/item&gt;
      &lt;item&gt;https://archive.org/download/commodore-amiga-compilations-applications&lt;/item&gt;
      &lt;item&gt;https://aminet.net/&lt;/item&gt;
      &lt;item&gt;Ramdisk icon from https://github.com/LessNick/Amiga-WorkBench-Re-Design/tree/master&lt;/item&gt;
      &lt;item&gt;Hard disk partition icon from A2091 install disk https://amigamuseum.emu-france.info/Fichiers/ADF/Utilitaires-Applications/&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Installed Tools&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Software&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
        &lt;cell role="head"&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DiskMaster 2&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Archive.org – compilation&lt;p&gt;Scanned Manual&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CygnusED 2.12&lt;/cell&gt;
        &lt;cell&gt;Full features tet editor&lt;/cell&gt;
        &lt;cell&gt;Archive.org – Neck utilities&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PowerPacker&lt;/cell&gt;
        &lt;cell&gt;Compression and tool to read compressed content&lt;/cell&gt;
        &lt;cell&gt;Aminet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DMS 1.11&lt;/cell&gt;
        &lt;cell&gt;Disk imager&lt;/cell&gt;
        &lt;cell&gt;Aminet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TSGui&lt;/cell&gt;
        &lt;cell&gt;Graphical interface for ADF and DMS&lt;/cell&gt;
        &lt;cell&gt;Aminet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LHA 1.38&lt;/cell&gt;
        &lt;cell&gt;Amiga’s default archiving tool&lt;/cell&gt;
        &lt;cell&gt;Aminet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mostra 1.06&lt;/cell&gt;
        &lt;cell&gt;Image viewer&lt;/cell&gt;
        &lt;cell&gt;ftp.funet.fi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Sysinfo&lt;/cell&gt;
        &lt;cell&gt;Hardware and system informations&lt;/cell&gt;
        &lt;cell&gt;Aminet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;X-Copy Pro 3.31&lt;/cell&gt;
        &lt;cell&gt;Disk copier&lt;/cell&gt;
        &lt;cell&gt;Archive.org – Neck utilities&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SuperDuper&lt;/cell&gt;
        &lt;cell&gt;Disk copier&lt;/cell&gt;
        &lt;cell&gt;Aminet / fish-0488&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Bootx 4.5&lt;/cell&gt;
        &lt;cell&gt;Antivirus&lt;/cell&gt;
        &lt;cell&gt;Aminet / fish-0641&lt;p&gt;Latest virus database&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Workbench enhancements&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Software&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
        &lt;cell role="head"&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ARP 1.3&lt;/cell&gt;
        &lt;cell&gt;Better AmigaDos commands&lt;/cell&gt;
        &lt;cell&gt;Aminet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WShell 2.0&lt;/cell&gt;
        &lt;cell&gt;Better shell&lt;/cell&gt;
        &lt;cell&gt;Archive.org – original software disk&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MyMenus&lt;/cell&gt;
        &lt;cell&gt;Allow to make custom menu entries&lt;/cell&gt;
        &lt;cell&gt;Aminet / fish-0225&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Fkeys&lt;/cell&gt;
        &lt;cell&gt;Key shortcuts for windows and screen switcher&lt;/cell&gt;
        &lt;cell&gt;Aminet / fish-0532&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dmouse 1.20&lt;/cell&gt;
        &lt;cell&gt;Screen and mouse blanker + windows management&lt;/cell&gt;
        &lt;cell&gt;Archive.org – Neck utilities&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MSClock&lt;/cell&gt;
        &lt;cell&gt;Clock on title bar&lt;/cell&gt;
        &lt;cell&gt;Aminet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Setclock v34.3&lt;/cell&gt;
        &lt;cell&gt;Y2k patch for setclock&lt;/cell&gt;
        &lt;cell&gt;Obligement&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Applications and games&lt;/head&gt;
    &lt;p&gt;For applications, I simply installed a few classic programs from the era, mostly for fun. By today’s standards, these tools aren’t particularly productive, but they give a great sense of how software worked back then. All of them were sourced from archives.org and ftp.funet.fi (see the “Sources / Where I Got It” section for links):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deluxe Paint IV&lt;/item&gt;
      &lt;item&gt;Pro-Tracker 3.10 – music editor (https://ftp.funet.fi/pub/amiga/audio/apps/compose/)&lt;/item&gt;
      &lt;item&gt;ANIMagic&lt;/item&gt;
      &lt;item&gt;Brillance 2 : contains commorodre installer for OS 1.3 =&amp;gt; copy to C&lt;/item&gt;
      &lt;item&gt;Disney Animation Studio&lt;/item&gt;
      &lt;item&gt;PageSetter 2&lt;/item&gt;
      &lt;item&gt;Wordworth 1.1&lt;/item&gt;
      &lt;item&gt;Scala MM 200&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As for games, I only included those that are natively installable on the A500. I didn’t see the point of using JST, since I can rely on WHDLoad on my other Amigas. The games I chose come from my personal list of best Amiga titles, curated over time:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Turbo Trax&lt;/item&gt;
      &lt;item&gt;Fiendish freddy&lt;/item&gt;
      &lt;item&gt;Lionheart&lt;/item&gt;
      &lt;item&gt;MetalKombat&lt;/item&gt;
      &lt;item&gt;Ducktales&lt;/item&gt;
      &lt;item&gt;Flashback&lt;/item&gt;
      &lt;item&gt;Hare Raising Havoc&lt;/item&gt;
      &lt;item&gt;Base Jump&lt;/item&gt;
      &lt;item&gt;KidChaos&lt;/item&gt;
      &lt;item&gt;Conan the Cimmerian&lt;/item&gt;
      &lt;item&gt;Dragon Heart&lt;/item&gt;
      &lt;item&gt;BosCar&lt;/item&gt;
      &lt;item&gt;BlackViper&lt;/item&gt;
      &lt;item&gt;MegaTyphoon&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Configuration &amp;amp; customizations&lt;/head&gt;
    &lt;p&gt;This section describes the steps I followed to customize my A500, presented roughly in the order I tackled them. Some steps are explained in more detail than others, depending on the level of customization involved.&lt;lb/&gt;Basically, I followed an order that allowed me to set up a fully usable environment before diving into more advanced tweaks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bare minimum for a usable OS&lt;/head&gt;
    &lt;head rend="h4"&gt;A file manager with OS 1.3 feeling&lt;/head&gt;
    &lt;p&gt;First, I installed DiskMaster 2 — a must-have if you want a proper file manager on base Workbench 1.3, which can’t even display files and directories that have no associated icons.&lt;/p&gt;
    &lt;p&gt;Here’s what I did to set it up:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Copied the executable to SYS:c/dm .&lt;/item&gt;
      &lt;item&gt;Created a setup file named dm.conf in SYS:s with the following customizations: &lt;list rend="ul"&gt;&lt;item&gt;SetFormat "NS T DMY A" to remove unnecessary comments from the file list&lt;/item&gt;&lt;item&gt;Barformat "DiskMaster Chip:%C Fast:%F %T %D.%M"&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Optimized window dimensions&lt;/item&gt;
      &lt;item&gt;Added a Version command: AddCmd Version, 20, extern c:version %s; Wait 2&lt;/item&gt;
      &lt;item&gt;Added a PlayMod command&lt;/item&gt;
      &lt;item&gt;Customized the Editors menu&lt;/item&gt;
      &lt;item&gt;Simplified the Archives menu to only LHA + DMS&lt;/item&gt;
      &lt;item&gt;Simplified the Tools menu and added Execute script&lt;/item&gt;
      &lt;item&gt;Simplified the Project menu&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To launch DiskMaster, I run: dm s:dm.conf either from the shell or via a custom menu, as explained later.&lt;/p&gt;
    &lt;p&gt;Below is the full configuration file for reference:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
          &lt;p&gt;3&lt;/p&gt;
          &lt;p&gt;4&lt;/p&gt;
          &lt;p&gt;5&lt;/p&gt;
          &lt;p&gt;6&lt;/p&gt;
          &lt;p&gt;7&lt;/p&gt;
          &lt;p&gt;8&lt;/p&gt;
          &lt;p&gt;9&lt;/p&gt;
          &lt;p&gt;10&lt;/p&gt;
          &lt;p&gt;11&lt;/p&gt;
          &lt;p&gt;12&lt;/p&gt;
          &lt;p&gt;13&lt;/p&gt;
          &lt;p&gt;14&lt;/p&gt;
          &lt;p&gt;15&lt;/p&gt;
          &lt;p&gt;16&lt;/p&gt;
          &lt;p&gt;17&lt;/p&gt;
          &lt;p&gt;18&lt;/p&gt;
          &lt;p&gt;19&lt;/p&gt;
          &lt;p&gt;20&lt;/p&gt;
          &lt;p&gt;21&lt;/p&gt;
          &lt;p&gt;22&lt;/p&gt;
          &lt;p&gt;23&lt;/p&gt;
          &lt;p&gt;24&lt;/p&gt;
          &lt;p&gt;25&lt;/p&gt;
          &lt;p&gt;26&lt;/p&gt;
          &lt;p&gt;27&lt;/p&gt;
          &lt;p&gt;28&lt;/p&gt;
          &lt;p&gt;29&lt;/p&gt;
          &lt;p&gt;30&lt;/p&gt;
          &lt;p&gt;31&lt;/p&gt;
          &lt;p&gt;32&lt;/p&gt;
          &lt;p&gt;33&lt;/p&gt;
          &lt;p&gt;34&lt;/p&gt;
          &lt;p&gt;35&lt;/p&gt;
          &lt;p&gt;36&lt;/p&gt;
          &lt;p&gt;37&lt;/p&gt;
          &lt;p&gt;38&lt;/p&gt;
          &lt;p&gt;39&lt;/p&gt;
          &lt;p&gt;40&lt;/p&gt;
          &lt;p&gt;41&lt;/p&gt;
          &lt;p&gt;42&lt;/p&gt;
          &lt;p&gt;43&lt;/p&gt;
          &lt;p&gt;44&lt;/p&gt;
          &lt;p&gt;45&lt;/p&gt;
          &lt;p&gt;46&lt;/p&gt;
          &lt;p&gt;47&lt;/p&gt;
          &lt;p&gt;48&lt;/p&gt;
          &lt;p&gt;49&lt;/p&gt;
          &lt;p&gt;50&lt;/p&gt;
          &lt;p&gt;51&lt;/p&gt;
          &lt;p&gt;52&lt;/p&gt;
          &lt;p&gt;53&lt;/p&gt;
          &lt;p&gt;54&lt;/p&gt;
          &lt;p&gt;55&lt;/p&gt;
          &lt;p&gt;56&lt;/p&gt;
          &lt;p&gt;57&lt;/p&gt;
          &lt;p&gt;58&lt;/p&gt;
          &lt;p&gt;59&lt;/p&gt;
          &lt;p&gt;60&lt;/p&gt;
          &lt;p&gt;61&lt;/p&gt;
          &lt;p&gt;62&lt;/p&gt;
          &lt;p&gt;63&lt;/p&gt;
          &lt;p&gt;64&lt;/p&gt;
          &lt;p&gt;65&lt;/p&gt;
          &lt;p&gt;66&lt;/p&gt;
          &lt;p&gt;67&lt;/p&gt;
          &lt;p&gt;68&lt;/p&gt;
          &lt;p&gt;69&lt;/p&gt;
          &lt;p&gt;70&lt;/p&gt;
          &lt;p&gt;71&lt;/p&gt;
          &lt;p&gt;72&lt;/p&gt;
          &lt;p&gt;73&lt;/p&gt;
          &lt;p&gt;74&lt;/p&gt;
          &lt;p&gt;75&lt;/p&gt;
          &lt;p&gt;76&lt;/p&gt;
          &lt;p&gt;77&lt;/p&gt;
          &lt;p&gt;78&lt;/p&gt;
          &lt;p&gt;79&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Reset&lt;/p&gt;
          &lt;p&gt;AddMenu Project, Printer Setup, SetPrinter&lt;/p&gt;
          &lt;p&gt;AddMenu Project, Change Command, ChgCmd&lt;/p&gt;
          &lt;p&gt;AddMenu Project, Save Config, S, SaveConfig&lt;/p&gt;
          &lt;p&gt;AddMenu Project, About, About&lt;/p&gt;
          &lt;p&gt;AddMenu Project, Quit, Q, Confirm "Are you sure you want to quit?" Yes No;Quit&lt;/p&gt;
          &lt;p&gt;AddMenu Tools, Run Selected, Single;External run %s&lt;/p&gt;
          &lt;p&gt;AddMenu Tools, Execute Script, Single;External run Execute %s&lt;/p&gt;
          &lt;p&gt;AddMenu Tools, Swap S&amp;lt;-&amp;gt;D, Swap&lt;/p&gt;
          &lt;p&gt;AddMenu Tools, Run DM Script, Single;Batch %s&lt;/p&gt;
          &lt;p&gt;AddMenu Archives, Lha Add, StdIO "CON:0/12/640/100/Add Window";Extern "Lha &amp;lt;* -r a";StdIO CLOSE&lt;/p&gt;
          &lt;p&gt;AddMenu Archives, Lha Extract, StdIO "CON:0/12/640/100/Extract Window";Extern Lha &amp;lt;* x %s;StdIO CLOSE&lt;/p&gt;
          &lt;p&gt;AddMenu Archives, Lha List, StdIO "CON:0/12/640/160/List Window";Extern Lha v %s;Wait;StdIO CLOSE&lt;/p&gt;
          &lt;p&gt;AddMenu Archives, DMS Write, StdIO "CON:0/12/640/160/List Window";Extern DMS write %s TO DF0:;Wait;StdIO CLOSE&lt;/p&gt;
          &lt;p&gt;AddMenu Disk, Format, Format&lt;/p&gt;
          &lt;p&gt;AddMenu Disk, DiskCopy, DiskCopy&lt;/p&gt;
          &lt;p&gt;AddMenu Disk, Format DF0:, Confirm "Are you sure?";Format DF0:&lt;/p&gt;
          &lt;p&gt;AddMenu Disk, Format DF1:, Format DF1: VERIFY "WorkDisk"&lt;/p&gt;
          &lt;p&gt;AddMenu Disk, Clear DF0:, Format DF0: QUICK INSTALL VERIFY&lt;/p&gt;
          &lt;p&gt;AddMenu Disk, Copy DF0: DF0:, DiskCopy DF0: DF0:&lt;/p&gt;
          &lt;p&gt;AddMenu Disk, Copy DF0: DF1:, DiskCopy DF0: DF1:&lt;/p&gt;
          &lt;p&gt;AddMenu Control, Lock as Source, Lock S&lt;/p&gt;
          &lt;p&gt;AddMenu Control, Lock as Dest, Lock D&lt;/p&gt;
          &lt;p&gt;AddMenu Control, UnLock, UnLock&lt;/p&gt;
          &lt;p&gt;AddMenu Control, UnLock all, UnLock All&lt;/p&gt;
          &lt;p&gt;AddMenu Control, Toggle Expand, Expand&lt;/p&gt;
          &lt;p&gt;AddMenu Editors, Textra, T, Extern run Textra %s&lt;/p&gt;
          &lt;p&gt;AddMenu Editors, CygnusED, T, Extern run Sys:Utilities/CygnusED %s&lt;/p&gt;
          &lt;p&gt;AddMenu Editors, EditPad, T, Extern run Sys:Utilities/Notepad %s&lt;/p&gt;
          &lt;p&gt;Button "Parent"&lt;/p&gt;
          &lt;p&gt;SetFormat "NS T DMY A"&lt;/p&gt;
          &lt;p&gt;BarFormat "DiskMaster Chip:%C Fast:%F %T %D.%M"&lt;/p&gt;
          &lt;p&gt;TitleFormat "%B/%F %I/%C"&lt;/p&gt;
          &lt;p&gt;OpenScreen 2&lt;/p&gt;
          &lt;p&gt;Color 05A FFF 002 F80&lt;/p&gt;
          &lt;p&gt;Font topaz/8&lt;/p&gt;
          &lt;p&gt;OpenWindow 278 11 84 245 CMD&lt;/p&gt;
          &lt;p&gt;AddCmd Root, 10, Root&lt;/p&gt;
          &lt;p&gt;AddCmd Parent, 10, Parent&lt;/p&gt;
          &lt;p&gt;AddCmd All, 30, Select *&lt;/p&gt;
          &lt;p&gt;AddCmd Clear, 30, Deselect *&lt;/p&gt;
          &lt;p&gt;AddCmd Select, 30, Select&lt;/p&gt;
          &lt;p&gt;AddCmd Exclude, 30, DeSelect&lt;/p&gt;
          &lt;p&gt;AddCmd Copy, 20, ReqPattern;Copy %s %d&lt;/p&gt;
          &lt;p&gt;AddCmd Cp New, 20, Copy %s %d NEWER&lt;/p&gt;
          &lt;p&gt;AddCmd Move, 20, ReqPattern;Move %s %d&lt;/p&gt;
          &lt;p&gt;AddCmd Delete, 30, ReqPattern;Confirm "All selected files will be lost.";Delete %s&lt;/p&gt;
          &lt;p&gt;AddCmd Rename, 20, Recurse OFF;Rename %s&lt;/p&gt;
          &lt;p&gt;AddCmd Protect, 20, Recurse OFF;Protect %s&lt;/p&gt;
          &lt;p&gt;AddCmd Comment, 20, Recurse OFF;Comment %s&lt;/p&gt;
          &lt;p&gt;AddCmd Find, 20, ReqPattern "Please enter search pattern";Find %s&lt;/p&gt;
          &lt;p&gt;AddCmd Read, 20, Read %s&lt;/p&gt;
          &lt;p&gt;AddCmd HexRead, 20, Read %s HEX&lt;/p&gt;
          &lt;p&gt;AddCmd ShowPic, 20, ShowPic %s&lt;/p&gt;
          &lt;p&gt;AddCmd MakeDir, 20, MakeDir&lt;/p&gt;
          &lt;p&gt;AddCmd Size, 20, UnMark OFF;Check %s&lt;/p&gt;
          &lt;p&gt;AddCmd Version, 20, extern c:version %s; Wait 2&lt;/p&gt;
          &lt;p&gt;AddCmd Playmod, 20, extern run APPS:Protracker/backplay %s&lt;/p&gt;
          &lt;p&gt;OpenWindow 362 11 278 245&lt;/p&gt;
          &lt;p&gt;OpenWindow 0 11 278 245&lt;/p&gt;
          &lt;p&gt;AddAutoCmd FORM????ILBM,ShowPic %s&lt;/p&gt;
          &lt;p&gt;AddAutoCmd FORM????ACBM,ShowPic %s&lt;/p&gt;
          &lt;p&gt;AddAutoCmd FORM????8SVX,ShowPic %s&lt;/p&gt;
          &lt;p&gt;AddAutoCmd FORM????ANIM,Extern View %s&lt;/p&gt;
          &lt;p&gt;AddAutoCmd ??-lh,StdIO "CON:0/12/640/100/Extract Window";Extern Lha &amp;lt;* x %s;StdIO CLOSE&lt;/p&gt;
          &lt;p&gt;AddAutoCmd TEXT,Read %s&lt;/p&gt;
          &lt;p&gt;AddAutoCmd DEFAULT,Read %s HEX&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Text editors&lt;/head&gt;
    &lt;p&gt;Once you have a proper file manager, the next thing you’ll do most often while configuring and customizing Workbench 1.3 is editing configuration files. For this reason, I installed two excellent text editors — far superior to the default NotePad or ED.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Textra: Installed to SYS:c as a lightweight but powerful editor for quick edits and rapid file changes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;CygnusED: Installed to SYS:Utilities , with req.library placed in SYS:libs , providing a full-featured, professional editor for more complex tasks (albeit heavier).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both editors complement each other: Textra for speed, CygnusED for advanced editing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Better shell&lt;/head&gt;
    &lt;p&gt;If, like me, you’re used to Bash or Zsh, the original Amiga Shell — even in the 3.x releases — feels quite limited, missing some “basic” features we take for granted. Fortunately, two tools make the CLI interface far more user-friendly: ARP 1.3 and WShell.&lt;/p&gt;
    &lt;p&gt;For ARP, I simply followed the installer and opted not to install the ARP shell, keeping the setup minimal.&lt;/p&gt;
    &lt;p&gt;WShell, on the other hand, comes with an installer that can be run directly from the CLI: Wshell-install&lt;/p&gt;
    &lt;p&gt;It doesn’t create an icon, so it’s invisible from Workbench by default. I made several customizations to integrate it better:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Copied NewWSH to the Workbench partition, allowing WShell to be started via an icon.&lt;/item&gt;
      &lt;item&gt;Set the ToolTypes as follows:&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
          &lt;p&gt;3&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;CONSOLE=CON:20/15/580/150/WShell/CLOSE&lt;/p&gt;
          &lt;p&gt;FROM=S:WShell-Startup&lt;/p&gt;
          &lt;p&gt;NAME=WShell&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Configured the default shell window in S:ENV/shellwindow : CON:20/15/580/150/WShell/CLOSE&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I also tweaked the FComp configuration (&lt;code&gt;
			SYS:s/Config-Fcomp&lt;/code&gt;) to get more familiar key usage:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TAB for autocomplete instead of ESC&lt;/item&gt;
      &lt;item&gt;Arrow keys Up/Down for line navigation&lt;/item&gt;
      &lt;item&gt;HOME / END for session top/bottom&lt;/item&gt;
      &lt;item&gt;PAGE UP / DOWN for session page up/down&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here is the full configuration file for reference:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
          &lt;p&gt;3&lt;/p&gt;
          &lt;p&gt;4&lt;/p&gt;
          &lt;p&gt;5&lt;/p&gt;
          &lt;p&gt;6&lt;/p&gt;
          &lt;p&gt;7&lt;/p&gt;
          &lt;p&gt;8&lt;/p&gt;
          &lt;p&gt;9&lt;/p&gt;
          &lt;p&gt;10&lt;/p&gt;
          &lt;p&gt;11&lt;/p&gt;
          &lt;p&gt;12&lt;/p&gt;
          &lt;p&gt;13&lt;/p&gt;
          &lt;p&gt;14&lt;/p&gt;
          &lt;p&gt;15&lt;/p&gt;
          &lt;p&gt;16&lt;/p&gt;
          &lt;p&gt;17&lt;/p&gt;
          &lt;p&gt;18&lt;/p&gt;
          &lt;p&gt;19&lt;/p&gt;
          &lt;p&gt;20&lt;/p&gt;
          &lt;p&gt;21&lt;/p&gt;
          &lt;p&gt;22&lt;/p&gt;
          &lt;p&gt;23&lt;/p&gt;
          &lt;p&gt;24&lt;/p&gt;
          &lt;p&gt;25&lt;/p&gt;
          &lt;p&gt;26&lt;/p&gt;
          &lt;p&gt;27&lt;/p&gt;
          &lt;p&gt;28&lt;/p&gt;
          &lt;p&gt;29&lt;/p&gt;
          &lt;p&gt;30&lt;/p&gt;
          &lt;p&gt;31&lt;/p&gt;
          &lt;p&gt;32&lt;/p&gt;
          &lt;p&gt;33&lt;/p&gt;
          &lt;p&gt;34&lt;/p&gt;
          &lt;p&gt;35&lt;/p&gt;
          &lt;p&gt;36&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;; Options record: SORT/S,GROUP/S,NOPATH/S,NOTOOLTYPES/S&lt;/p&gt;
          &lt;p&gt;OPTIONS nopath ; notooltypes&lt;/p&gt;
          &lt;p&gt;; FILETYPE records: `FILETYPE/K,FMT/K,REMOVE/S'&lt;/p&gt;
          &lt;p&gt;FILETYPE TEXT FMT "E %a"&lt;/p&gt;
          &lt;p&gt;FILETYPE ILBM FMT "sys:utilities/display %a*N"&lt;/p&gt;
          &lt;p&gt;FILETYPE DOC FMT "sys:utilities/more %a*N"&lt;/p&gt;
          &lt;p&gt;; Command records: `COMMAND/K,PATH/K,PAT/K,FMT/K,REMOVE/S'&lt;/p&gt;
          &lt;p&gt;COMMAND EXecute PATH S: ; an argument PATH&lt;/p&gt;
          &lt;p&gt;COMMAND DELete FMT "%f%0 %1 %2 %3%l" ; multiple files&lt;/p&gt;
          &lt;p&gt;COMMAND REName FMT "REName FROM %0 TO %0" ; command-specific rewrite&lt;/p&gt;
          &lt;p&gt;COMMAND tex PAT "#?.tex"&lt;/p&gt;
          &lt;p&gt;COMMAND DVisw PAT "#?.dvi" FMT "%f%r0%l"&lt;/p&gt;
          &lt;p&gt;COMMAND wait FMT "You're waiting ... %0" ; input context example&lt;/p&gt;
          &lt;p&gt;COMMAND VERsion PATH "libs:,devs:"&lt;/p&gt;
          &lt;p&gt;; Hotkeys: `KEY/K,QUAL/K,PATH/K,PAT/K,FMT/K,AUTO/S,REMOVE/S'&lt;/p&gt;
          &lt;p&gt;KEY 66 QUAL 0 ; TAB key for completion &lt;/p&gt;
          &lt;p&gt;KEY 29 FMT ";Choices: %0 %1 %2 %3 %4 %5 %6 %7 %8 %9"&lt;/p&gt;
          &lt;p&gt;; Input keys: `KEY/K,QUAL/K,NAME/K,PATH/K,PAT/K,FMT/K,AUTO/S,REMOVE/S'&lt;/p&gt;
          &lt;p&gt;KEY 76 QUAL 8 NAME CTRL-UARROW FMT "*E[101]" ; search up&lt;/p&gt;
          &lt;p&gt;KEY 77 QUAL 8 NAME CTRL-RARROW FMT "*E[100]" ; search down&lt;/p&gt;
          &lt;p&gt;KEY 62 QUAL 0 NAME KPUARROW FMT "*E[103]" ; line up&lt;/p&gt;
          &lt;p&gt;KEY 30 QUAL 0 NAME KPDARROW FMT "*E[102]" ; line down&lt;/p&gt;
          &lt;p&gt;KEY 31 QUAL 0 NAME PGUP FMT "*E[113]" ; page up&lt;/p&gt;
          &lt;p&gt;KEY 63 QUAL 0 NAME PGDOWN FMT "*E[112]" ; page down&lt;/p&gt;
          &lt;p&gt;KEY 61 QUAL 0 NAME HOME FMT "*E[99]" ; session top&lt;/p&gt;
          &lt;p&gt;KEY 29 QUAL 0 NAME END FMT "*E[98]" ; session bottom&lt;/p&gt;
          &lt;p&gt;KEY 79 QUAL 16 NAME LALT-LARROW FMT "*E[79]" ; skip left name alt-control-O&lt;/p&gt;
          &lt;p&gt;KEY 78 QUAL 16 NAME LALT-RARROW FMT "*E[73]" ; skip right name alt-control-I&lt;/p&gt;
          &lt;p&gt;KEY 79 QUAL 8 NAME CTRL-LARROW FMT "*E[85]" ; del left name alt-control-U&lt;/p&gt;
          &lt;p&gt;KEY 78 QUAL 8 NAME CTRL-RARROW FMT "*E[89]" ; del right name alt-control-Y&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I also customized the WShell prompt in S:WShell-Startup to make it more informative and visually clear: the time is displayed between brackets in black (color 2), followed by the current path in orange (color 3).&lt;/p&gt;
    &lt;p&gt;Here is the content of SYS/s:WShell-Startup :&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;prompt "%2[%t] %3%c%1&amp;gt;"&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Additionally, I modified SYS:/s:ENV/titlebar to display the shell number, free fast memory, and free chip memory:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;%w %n - %mc chip / %mf fast&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Finally, I set WShell as the default CLI by adding it somewhere in the startup-sequence.&lt;lb/&gt;The extract below for reference:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
          &lt;p&gt;3&lt;/p&gt;
          &lt;p&gt;4&lt;/p&gt;
          &lt;p&gt;5&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;; WShell&lt;/p&gt;
          &lt;p&gt;assign remove CON: ; is replaced by the next line&lt;/p&gt;
          &lt;p&gt;C:DHOpts CON: PIP: ; set the new display handler&lt;/p&gt;
          &lt;p&gt;C:FComp ; enable completion and history navigation&lt;/p&gt;
          &lt;p&gt;C:SetExecute ; use wshell for Execute command&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Custom menu for quick access to most usefull tools&lt;/head&gt;
    &lt;p&gt;The final touch for a more usable Workbench 1.3 is customizing the system menu to include shortcuts to the most frequently used tools, such as DiskMaster, Textra, and NewShell.&lt;/p&gt;
    &lt;p&gt;To achieve this, I installed MyMenu following the official documentation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Copied the main program to SYS:C .&lt;/item&gt;
      &lt;item&gt;Copied MyMenu.conf to SYS:S and configured it according to my preferences.&lt;/item&gt;
      &lt;item&gt;Copied MyMenu-Handler to SYS:L&lt;/item&gt;
      &lt;item&gt;Called MyMenu in the startup-sequence, right after LoadWB.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The full configuration file is as follows:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
          &lt;p&gt;3&lt;/p&gt;
          &lt;p&gt;4&lt;/p&gt;
          &lt;p&gt;5&lt;/p&gt;
          &lt;p&gt;6&lt;/p&gt;
          &lt;p&gt;7&lt;/p&gt;
          &lt;p&gt;8&lt;/p&gt;
          &lt;p&gt;9&lt;/p&gt;
          &lt;p&gt;10&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;color 2&lt;/p&gt;
          &lt;p&gt;menu &amp;lt;D&amp;gt; Tools DiskMaster | CLI SYS:c/dm SYS:s/dm.conf&lt;/p&gt;
          &lt;p&gt;menu &amp;lt;S&amp;gt; Tools NewShell | WB SYS:NewWSH&lt;/p&gt;
          &lt;p&gt;menu &amp;lt;B&amp;gt; Tools BootX | WB SYS:System/bootx&lt;/p&gt;
          &lt;p&gt;menu &amp;lt;T&amp;gt; Tools Textra | CLI SYS:c/Textra&lt;/p&gt;
          &lt;p&gt;menu &amp;lt;S&amp;gt; Tools CygnusED | WB SYS:Utilities/CygnusED&lt;/p&gt;
          &lt;p&gt;menu &amp;lt;A&amp;gt; Floppy Dms-Adf | WB SYS:tools/tsgui&lt;/p&gt;
          &lt;p&gt;menu &amp;lt;D&amp;gt; Floppy SuperDuper | WB SYS:tools/SD&lt;/p&gt;
          &lt;p&gt;menu &amp;lt;X&amp;gt; Floppy X-Copy | CLI SYS:c/xCopy&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Workbench enhancements &amp;amp; tools&lt;/head&gt;
    &lt;p&gt;The following software is not strictly necessary, but each clearly enhances the Workbench 1.3 experience. They are easy to install, require little to no configuration, and bring useful improvements to everyday use. I’ll go quickly through them:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Software&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
        &lt;cell role="head"&gt;Comment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;FKeys&lt;/cell&gt;
        &lt;cell&gt;keyboard shortcuts to switch between Windows and screen&lt;/cell&gt;
        &lt;cell&gt;Copied to a new Commodities drawer on SYS: and run from the startup-sequence.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dmouse&lt;/cell&gt;
        &lt;cell&gt;Fine-tuned mouse accelerator and screen blanker&lt;/cell&gt;
        &lt;cell&gt;Executable to SYS:C, handler to SYS:L launched via startup-sequence: dmouse -a1 -t0 -A0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Msclock&lt;/cell&gt;
        &lt;cell&gt;Displays the time on the menu bar&lt;/cell&gt;
        &lt;cell&gt;Same installation logic as DMouse: executable to SYS:C , handler to SYS:L, then run from startup-sequence: msclock -d -m -o .&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PPnew&lt;/cell&gt;
        &lt;cell&gt;Powerpacker tools &amp;amp; libraries (required for some packed programs and mods)&lt;/cell&gt;
        &lt;cell&gt;Copied PPMore/powerpacker.library to SYS:libs , pp and PPMore to SYS:C , PPMore.doc to SYS:docs , same for ppShow and ppAnim&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LHA&lt;/cell&gt;
        &lt;cell&gt;Default file archiver on AmigaOS&lt;/cell&gt;
        &lt;cell&gt;Ran LHA_e138.run to extract files, then copied lha, splitlzh , and joinlzh to SYS:C&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;BootX&lt;/cell&gt;
        &lt;cell&gt;Up-to-date antivirus for OS 1.3&lt;/cell&gt;
        &lt;cell&gt;On my setup it crashes often, but it can detect viruses in memory, bootblocks, floppies, and files. Installation: libs/reqtools.library.13 to SYS:libs, all BootX files to SYS:system , BootX.doc to SYS:docs, latest recognition file to SYS:system. Adjusted colors for a Workbench 1.3 look: color1=blue 05A, color2=white FFF, color3=black 002, color4=orange F80.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Setclock v34.3&lt;/cell&gt;
        &lt;cell&gt;Y2K-compatible clock for OS 1.3&lt;/cell&gt;
        &lt;cell&gt;Prevents year misinterpretation (e.g., 2000=1979).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Mostra 1.08&lt;/cell&gt;
        &lt;cell&gt;Image viewer&lt;/cell&gt;
        &lt;cell&gt;Copied to SYS:Utilities&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These tools improve daily usability, add visual polish, and ensure compatibility with classic file formats and archives.&lt;/p&gt;
    &lt;head rend="h3"&gt;Floppy disk Tools&lt;/head&gt;
    &lt;p&gt;Even though I can manipulate Amiga floppies on my other machines, sometimes it’s quicker to work directly on the A500 when it’s connected. The following software makes floppy management much easier:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Software&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
        &lt;cell role="head"&gt;Comment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;X-Copy&lt;/cell&gt;
        &lt;cell&gt;Well-known floppy disk copier&lt;/cell&gt;
        &lt;cell&gt;Copied to SYS:C&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DMS amd TSGui&lt;/cell&gt;
        &lt;cell&gt;Floppy disk (un)archiver and associated GUI&lt;/cell&gt;
        &lt;cell&gt;Ran dms1111.run to extract DMS, and unlha for the TSGui archive. Then copied: dms to SYS:C, DMS.doc to SYS:docs, tsgui to SYS:Tools&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SuperDuper&lt;/cell&gt;
        &lt;cell&gt;Another floppy disk copier&lt;/cell&gt;
        &lt;cell&gt;Copied sd to SYS:Tools and documentation to SYS:Docs.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Tested but removed&lt;/head&gt;
    &lt;p&gt;I also tried installing and using some other interesting tools and hacks, but ultimately removed them because they caused crashes or unexpected behavior on my setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ZoomDaemon: Adds a “new window” widget to minimize windows. However, it also displayed this for invisible Workbench windows, which looked awkward — and it caused frequent crashes. At least my system is stable again without it.&lt;/item&gt;
      &lt;item&gt;NoClick2: Ran fine in Amiberry/UAE, but crashed on the real Amiga 500.&lt;/item&gt;
      &lt;item&gt;SimGen + RunBack: Fun for adding backdrop pictures, but it led to unexpected and frequent Guru Meditation errors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sometimes, stability wins over flashy features, especially when working with a vintage machine like the A500.&lt;/p&gt;
    &lt;head rend="h3"&gt;Startup-sequence&lt;/head&gt;
    &lt;p&gt;It’s now time to share my startup-sequence. Of course, everyone has their own rules and preferences, so I’m simply presenting mine as an example.&lt;/p&gt;
    &lt;p&gt;My approach was guided by three main goals:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simplify the default OS 1.3 naming conventions: instead of juggling startup-sequence, startup-sequence.FD, and startupII.&lt;/item&gt;
      &lt;item&gt;Consolidate everything related to my base but customized Workbench into a single file for easier maintenance.&lt;/item&gt;
      &lt;item&gt;Create a user-startup, similar to OS 2.0+, mainly to handle application-specific assigns and personal tweaks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The full startup-sequence file is provided below for reference:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1&lt;/p&gt;
          &lt;p&gt;2&lt;/p&gt;
          &lt;p&gt;3&lt;/p&gt;
          &lt;p&gt;4&lt;/p&gt;
          &lt;p&gt;5&lt;/p&gt;
          &lt;p&gt;6&lt;/p&gt;
          &lt;p&gt;7&lt;/p&gt;
          &lt;p&gt;8&lt;/p&gt;
          &lt;p&gt;9&lt;/p&gt;
          &lt;p&gt;10&lt;/p&gt;
          &lt;p&gt;11&lt;/p&gt;
          &lt;p&gt;12&lt;/p&gt;
          &lt;p&gt;13&lt;/p&gt;
          &lt;p&gt;14&lt;/p&gt;
          &lt;p&gt;15&lt;/p&gt;
          &lt;p&gt;16&lt;/p&gt;
          &lt;p&gt;17&lt;/p&gt;
          &lt;p&gt;18&lt;/p&gt;
          &lt;p&gt;19&lt;/p&gt;
          &lt;p&gt;20&lt;/p&gt;
          &lt;p&gt;21&lt;/p&gt;
          &lt;p&gt;22&lt;/p&gt;
          &lt;p&gt;23&lt;/p&gt;
          &lt;p&gt;24&lt;/p&gt;
          &lt;p&gt;25&lt;/p&gt;
          &lt;p&gt;26&lt;/p&gt;
          &lt;p&gt;27&lt;/p&gt;
          &lt;p&gt;28&lt;/p&gt;
          &lt;p&gt;29&lt;/p&gt;
          &lt;p&gt;30&lt;/p&gt;
          &lt;p&gt;31&lt;/p&gt;
          &lt;p&gt;32&lt;/p&gt;
          &lt;p&gt;33&lt;/p&gt;
          &lt;p&gt;34&lt;/p&gt;
          &lt;p&gt;35&lt;/p&gt;
          &lt;p&gt;36&lt;/p&gt;
          &lt;p&gt;37&lt;/p&gt;
          &lt;p&gt;38&lt;/p&gt;
          &lt;p&gt;39&lt;/p&gt;
          &lt;p&gt;40&lt;/p&gt;
          &lt;p&gt;41&lt;/p&gt;
          &lt;p&gt;42&lt;/p&gt;
          &lt;p&gt;43&lt;/p&gt;
          &lt;p&gt;44&lt;/p&gt;
          &lt;p&gt;45&lt;/p&gt;
          &lt;p&gt;46&lt;/p&gt;
          &lt;p&gt;47&lt;/p&gt;
          &lt;p&gt;48&lt;/p&gt;
          &lt;p&gt;49&lt;/p&gt;
          &lt;p&gt;50&lt;/p&gt;
          &lt;p&gt;51&lt;/p&gt;
          &lt;p&gt;52&lt;/p&gt;
          &lt;p&gt;53&lt;/p&gt;
          &lt;p&gt;54&lt;/p&gt;
          &lt;p&gt;55&lt;/p&gt;
          &lt;p&gt;56&lt;/p&gt;
          &lt;p&gt;57&lt;/p&gt;
          &lt;p&gt;58&lt;/p&gt;
          &lt;p&gt;59&lt;/p&gt;
          &lt;p&gt;60&lt;/p&gt;
          &lt;p&gt;61&lt;/p&gt;
          &lt;p&gt;62&lt;/p&gt;
          &lt;p&gt;63&lt;/p&gt;
          &lt;p&gt;64&lt;/p&gt;
          &lt;p&gt;65&lt;/p&gt;
          &lt;p&gt;66&lt;/p&gt;
          &lt;p&gt;67&lt;/p&gt;
          &lt;p&gt;68&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;SetPatch &amp;gt;NIL:&lt;/p&gt;
          &lt;p&gt;SetPatchMrgCop &amp;gt;NIL:&lt;/p&gt;
          &lt;p&gt;SYS:System/FastMemFirst&lt;/p&gt;
          &lt;p&gt;SetClock load &lt;/p&gt;
          &lt;p&gt;Addbuffers df0: 30&lt;/p&gt;
          &lt;p&gt;; faster text rendition&lt;/p&gt;
          &lt;p&gt;FF &amp;gt;NIL: -0&lt;/p&gt;
          &lt;p&gt;; preload most used commands&lt;/p&gt;
          &lt;p&gt;resident c:Resident pure&lt;/p&gt;
          &lt;p&gt;resident c:List pure&lt;/p&gt;
          &lt;p&gt;resident c:CD pure&lt;/p&gt;
          &lt;p&gt;resident c:Mount pure&lt;/p&gt;
          &lt;p&gt;resident c:Assign pure&lt;/p&gt;
          &lt;p&gt;resident c:Makedir pure&lt;/p&gt;
          &lt;p&gt;resident c:dir pure&lt;/p&gt;
          &lt;p&gt;resident CLI L:Shell-Seg SYSTEM pure add; activate Shell&lt;/p&gt;
          &lt;p&gt;; assign&lt;/p&gt;
          &lt;p&gt;assign sys: dh0:&lt;/p&gt;
          &lt;p&gt;assign c: SYS:c&lt;/p&gt;
          &lt;p&gt;assign L: SYS:l&lt;/p&gt;
          &lt;p&gt;assign FONTS: SYS:fonts&lt;/p&gt;
          &lt;p&gt;assign CGFONTS: SYS:CGFonts&lt;/p&gt;
          &lt;p&gt;assign CGCACHE: SYS:CGFonts/CGCache&lt;/p&gt;
          &lt;p&gt;assign S: SYS:s&lt;/p&gt;
          &lt;p&gt;assign DEVS: SYS:devs&lt;/p&gt;
          &lt;p&gt;assign LIBS: SYS:libs&lt;/p&gt;
          &lt;p&gt;; Ramdisk config&lt;/p&gt;
          &lt;p&gt;makedir ram:t&lt;/p&gt;
          &lt;p&gt;makedir ram:env&lt;/p&gt;
          &lt;p&gt;makedir ram:clipboards &lt;/p&gt;
          &lt;p&gt;assign t: ram:t&lt;/p&gt;
          &lt;p&gt;assign ENV: ram:env&lt;/p&gt;
          &lt;p&gt;assign CLIPS: ram:clipboards&lt;/p&gt;
          &lt;p&gt;copy S:env/ ENV: QUIET&lt;/p&gt;
          &lt;p&gt;copy S:Ramdisk.info ram:Disk.info&lt;/p&gt;
          &lt;p&gt;copy S:ram.info ram:.info&lt;/p&gt;
          &lt;p&gt;; Mounts&lt;/p&gt;
          &lt;p&gt;mount speak:&lt;/p&gt;
          &lt;p&gt;mount aux:&lt;/p&gt;
          &lt;p&gt;mount pipe:&lt;/p&gt;
          &lt;p&gt;; WShell&lt;/p&gt;
          &lt;p&gt;assign remove CON: ; is replaced by the next line&lt;/p&gt;
          &lt;p&gt;C:DHOpts CON: PIP: ; set the new displaz handler&lt;/p&gt;
          &lt;p&gt;C:FComp ; enable completion and history navigation&lt;/p&gt;
          &lt;p&gt;C:SetExecute ; use wshell for Execute command&lt;/p&gt;
          &lt;p&gt;; set keymap&lt;/p&gt;
          &lt;p&gt;SYS:System/SetMap F&lt;/p&gt;
          &lt;p&gt;;set path for Workbench&lt;/p&gt;
          &lt;p&gt;path ram: c: sys:utilities sys:system s: sys:prefs add&lt;/p&gt;
          &lt;p&gt;C:dmouse &amp;gt;NIL: -a1 -t0 -A0&lt;/p&gt;
          &lt;p&gt;C:msclock &amp;gt;NIL: -d -m -o&lt;/p&gt;
          &lt;p&gt;SYS:commodities/Fkeys &amp;gt;NIL:&lt;/p&gt;
          &lt;p&gt;execute s:user-startup&lt;/p&gt;
          &lt;p&gt;; load workbench&lt;/p&gt;
          &lt;p&gt;LoadWB delay&lt;/p&gt;
          &lt;p&gt;C:MyMenu&lt;/p&gt;
          &lt;p&gt;endcli&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;ScreenShots&lt;/head&gt;
    &lt;p&gt;To produce cleaner and more polished screenshots, I captured them using Amiberry / UAE rather than the real hardware.&lt;/p&gt;
    &lt;p&gt;This allows for crisp images that clearly show the Workbench, tools, and customizations without the glare or color inconsistencies that sometimes appear on a CRT display.&lt;/p&gt;
    &lt;p&gt;Below are several examples illustrating my setup and configurations:&lt;/p&gt;
    &lt;head rend="h2"&gt;See it live on real hardware&lt;/head&gt;
    &lt;p&gt;If you want to see the fully restored and customized Amiga 500 in action, here’s a video showing it running on the real hardware. It demonstrates the Workbench, tools, and all the tweaks described in this article.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.pimyretro.org/pimped-amiga-500/"/><published>2025-11-19T12:02:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45978813</id><title>How do the pros get someone to leave a cult?</title><updated>2025-11-19T13:46:50.191874+00:00</updated><content>&lt;doc fingerprint="f724919d2ae3c799"&gt;
  &lt;main&gt;
    &lt;p&gt;When the phone rings at Patrick Ryan and Joseph Kelly’s home in Philadelphia, chances are the caller is desperate. One couple rang because their son was about to abandon his medical practice to follow a new-age guru in Spain. Another call came from a husband whose wife was emptying their life savings for a self-proclaimed prophet in Australia. Yet another family phoned about their niece, who was in a relationship with a man stealing from her, maybe drugging her, probably sexually assaulting her.&lt;/p&gt;
    &lt;p&gt;These families had tried everything else. When nothing worked, they heard there were two men in Philadelphia who might still be able to bring their loved one home.&lt;/p&gt;
    &lt;p&gt;What Ryan and Kelly do is unusual: they help people leave cults. Over the past 40 years, they have handled hundreds of cases – some simple and local, others stretching across borders and decades. They have been hired by families of both modest and considerable means. They say they have even been hired by government agencies, and that some cults they have investigated have left them genuinely afraid for their lives.&lt;/p&gt;
    &lt;p&gt;Although many people are involved in cultic studies and education, fewer than 10 people in the US do anything like what Ryan and Kelly do. And among those, only Kelly and Ryan practice their strange and unique method: embedding themselves in families’ lives, pulling on threads like marionettists, sometimes for years.&lt;/p&gt;
    &lt;p&gt;Their method goes something like this. A family reaches out about their daughter, husband, nephew or grandchild. Ryan and Kelly conduct an assessment that can take anywhere from a day to a week (they would not say exactly). They charge $2,500 for the assessment, then $250 an hour after that, interviewing the family until they understand the dynamics well enough to devise a strategy. Then, over months or sometimes years, they work to create the conditions in which a person might begin to question the beliefs their life has been built on.&lt;/p&gt;
    &lt;p&gt;Normally, Kelly and Ryan work by strengthening the existing relationships in a person’s life. It can be a long game. They will educate the family about the cultic group, and give advice about what to say (or not to say). They will bring in experts: psychiatrists, lawyers, priests that can provide perspective and counsel. The goal is to untangle the family dynamics that might have made someone vulnerable to a cult in the first place.&lt;/p&gt;
    &lt;p&gt;Very occasionally, they meet face to face with the person involved in a cult. But these encounters look nothing like a drug intervention, with friends gathered in a circle and the reason for the meeting laid bare. Instead, Ryan and Kelly will act covertly. In one case, a son (the cult member) came home for a few days. His parents told him that Ryan and Kelly were friends of theirs, “family mediators” who happened to be “in town for a few days, to meet with some colleagues” – both technically true. The pair made sure to “forget” a book at the family home, and return the next day to collect it, as they began to build rapport.&lt;/p&gt;
    &lt;p&gt;I met Kelly and Ryan at their place in south Philadelphia, a three-story house they share with a big dog named Kenny and a bright green parrot named Greta.&lt;/p&gt;
    &lt;p&gt;Greta was a consolation prize Ryan bought for himself after a failed intervention, the second he ever attempted. It was the 1980s and his client, a woman who had recently finished her master’s at a prestigious university, had been drawn into a scam job. It was essentially a pyramid scheme built around a health regimen. Before you could sell it, you had to try it, so you knew what you were selling.&lt;/p&gt;
    &lt;p&gt;The regimen? Multiple enemas a day. “It escalated to 40 to 60 enemas a day,” Ryan said. “And when you do that many enemas, it upsets the electrolyte balance in your body and you begin hallucinating.”&lt;/p&gt;
    &lt;p&gt;He spent three days trying to reason with her, but she would not budge. Ryan asked himself: what value do I have if I can’t even talk someone out of an enema cult? Frustrated, he went for a walk, saw a bird in a pet shop window who said: “Hello, hello.” He put her in his coat, fashioned a small cage, took her on an airplane and brought her home.&lt;/p&gt;
    &lt;p&gt;Their approach has changed a lot since those early interventions.&lt;/p&gt;
    &lt;p&gt;First, they are careful with language. They don’t love the word cult. They say it’s a cudgel: too blunt an instrument to get at the heart of the problem. Also, even if a client leaves a group and returns home, Ryan and Kelly wouldn’t say they “got them out”. They describe themselves as mediators who build bridges through which families can reach their loved ones. Sometimes, the person crosses that bridge. Sometimes, the outcome is more complicated.&lt;/p&gt;
    &lt;p&gt;Second, they have worked hard to distance themselves from “deprogramming” – the practice most people associate with cult interventions. In the 1970s and 80s, deprogramming could involve kidnappings, involuntary confinement and even violence. In one case Kelly mentioned, a cult member was held at gunpoint. It was controversial, and its effectiveness was questionable. “That,” Ryan said more than once, “is not what we do.”&lt;/p&gt;
    &lt;p&gt;Nowadays, they focus more on helping someone reach their own informed conclusion about the group they are part of, trying to soften the obstacles that might cloud their judgment.&lt;/p&gt;
    &lt;p&gt;For instance: one of the tricky parts, they explained, is communicating with a person who has been given tools to block out other people’s perspectives. This set of tools or ideas is what Ryan and Kelly call a group’s “gatekeeper”.&lt;/p&gt;
    &lt;p&gt;Ryan gave me an example. One client came from an extremely rigid, orthodox Catholic family. The family had a plan for life: retire early, save well, put the kids through college. But against these goals, the wife had joined an eastern religious group and was donating thousands of dollars to it. She had quit her job, and the marriage was collapsing.&lt;/p&gt;
    &lt;p&gt;The gatekeeper, Ryan and Kelly decided, was that the woman perceived her spouse “as dogmatic, fundamentalist – but not spiritual”. They needed to change her mind about her husband.&lt;/p&gt;
    &lt;p&gt;So Ryan called an old friend of Kelly’s, a Jesuit priest who lived in a parish near the family’s home. Ryan asked the priest to meet the husband. The two men became friends and agreed to meet regularly – all according to Ryan and Kelly’s plan. Every so often, the husband would text his wife: “I’m coming home late tonight, meeting my priest friend.”&lt;/p&gt;
    &lt;p&gt;“She’s like, ‘What priest friend?’” Ryan said.&lt;/p&gt;
    &lt;p&gt;After a few months, the wife became curious enough to want to meet her husband’s new friend. The priest, who was genuinely thrilled, nearly veered off plan by offering to speak with her directly. He believed she was ready to hear his views on spirituality. But Ryan stopped him: “I told him, look, they hired us to be strategists. I have a strategy for this.”&lt;/p&gt;
    &lt;p&gt;Ryan mapped out the parish and planned a tour. He made sure the route passed through the library specifically, the section with many eastern religious books. “You’re gonna go through there,” Ryan told the priest.&lt;/p&gt;
    &lt;p&gt;On a Friday, the husband brought his wife along to visit. The priest greeted them warmly and showed her the grounds. They walked through the library. She saw the books.&lt;/p&gt;
    &lt;p&gt;Soon, the priest was coming over for barbecues. They all became friends. And she began openly talking with her husband about the group she was involved in: the good and the bad. They had passed the group’s gatekeeper. But the work was not finished.&lt;/p&gt;
    &lt;p&gt;All groups have a rhythm, like a pulse across the calendar year. We have holidays, and we have tax season. There are highs and lows. If you want to talk to someone about how dangerous their group is, you probably do not want to do it right after they have taken ayahuasca or gone on retreat. But the lows come just as reliably.&lt;/p&gt;
    &lt;p&gt;When the wife finally started to complain about the group, the husband called Ryan: “She’s going to leave!” But Ryan told him firmly: “No, she’s not. Don’t push it.”&lt;/p&gt;
    &lt;p&gt;By the third cycle, the third low point, when she was sleep deprived, working long hours and truly miserable, Ryan gave the husband a single line. “Just say to her this: ‘You gave it a good shot.’ And nothing more.”&lt;/p&gt;
    &lt;p&gt;“She said: ‘Yeah, I have. Will you help me get my stuff?’ And he said: ‘OK.’”&lt;/p&gt;
    &lt;p&gt;The whole time, the wife knew her husband had consulted Ryan and Kelly, though she did not know they had orchestrated his friendship with the priest. During the five years they worked on the case, she assumed they were anti-religious bad actors. A few months after she left the group, she met Ryan and Kelly for the first time.&lt;/p&gt;
    &lt;p&gt;In Ryan’s telling, she loved chatting with Kelly and himself because they so clearly understood what she appreciated about the group. But they also saw that she was being made to sleep only a few hours a night, drink toilet water, and work hundreds of hours recruiting members for a guru accused of sexual misconduct and labor law violations.&lt;/p&gt;
    &lt;p&gt;Ryan and Kelly started doing this work because when they were younger, they themselves had been in what would be described as cults. They were Transcendental Meditation (TM) instructors in the 70s and 80s. After about a decade with TM, they felt disturbed by their relationship to the organization, and they sued – Kelly in 1986, and Ryan in 1989 – for negligence and fraud. Kelly joined a suit as a Doe along with six others, claiming the organization had “fraudulently promised that the practice … would confer certain personal and societal benefits”, which never materialized. Ryan says that during the course of his TM training he was constantly surveilled and led to believe that he would be able to levitate and save humanity.&lt;/p&gt;
    &lt;p&gt;The case Kelly joined, which dragged on for several years, included expert testimony from clinical psychologist Margaret Singer, a brainwashing specialist who had previously assessed Charles Manson. Neither case won, but their lawsuits eventually settled, and through the course of the litigation, Ryan and Kelly left the organization. (TM did not respond to a request for comment; however, Bob Roth, CEO of the TM-associated David Lynch Foundation, did let me know the American Heart Association recently named Transcendental Meditation an official stress reducer for treating high blood pressure.)&lt;/p&gt;
    &lt;p&gt;Kelly joined another group after leaving TM. He followed his new guru for five more years. Meanwhile, Ryan told me he got busy investigating and trying to expose cults, including the group Kelly had joined. In those early days, Ryan considered himself a sort of “cult fighter”, with a much more black and white view of what cults were and what it meant to be a part of one. They finally started working together when Kelly had a falling out with his second group, whose guru was eventually convicted for child sexual abuse.&lt;/p&gt;
    &lt;p&gt;They have had a close relationship ever since, working and living together with their dog and bird in a big house they told me was once used as a base of operations by the Philly mafia, which seems oddly fitting. They mostly prefer to keep details about their personal lives off-record. Often, the families they work with need to hear very hard things, and being a sort of blank slate makes it easier for them to be whoever their clients need them to be.&lt;/p&gt;
    &lt;p&gt;Throughout reporting this piece, privacy was an issue. Ryan and Kelly told me many more details about their cases off the record. All these cases are anonymized, with some crucial details changed, to protect the identities of their clients and their families. Furthermore, Kelly and Ryan urge their clients not to speak with the media. The firmest “no” I ever got was when I asked Ryan if I could speak to a former client. The second was when I asked if they could show me emails or letters to prove they had worked with government agencies. This made it difficult to verify all the details of their stories, though I found the situations they described were consistent with other accounts of ex-members from cults they say their clients were a part of. When cases did make it to court, the details Ryan and Kelly provided me matched the legal testimony I found.&lt;/p&gt;
    &lt;p&gt;But without being able to speak to their former clients, some of the stories told here remain just that: stories in the telling of Ryan and Kelly. I was, however, able to speak with many of their collaborators, who confirmed that they had seen Ryan and Kelly’s method work close up. One of the people I spoke with, Dr Janja Lalich, is a professor emerita of sociology at California University State, Chico and author of multiple books on cults including Bounded Choice: True Believers and Charismatic Cults. Lalich lectures and consults on cultic studies, and regularly testifies as a cult expert in court cases internationally. She started studying them because she, too, joined and left a cult when she was younger. It was a radical Marxist-Leninist cult that eventually “imploded”; a process she details in her book. The members collectively overthrew the leadership and all left at the same time, she explained, “which was great”.&lt;/p&gt;
    &lt;p&gt;Lalich worked on a couple of cases with Kelly and Ryan in the 90s, when they were starting out. She did not like the work. She found it stressful and difficult, and felt some reservations about the way the process interfered with people’s lives. But the three of them have remained close over the years and still collaborate in the broader cult-awareness space, attending conferences and teaching workshops. She confirmed for me a lot of the claims Kelly and Ryan made about the cults they have dealt with, including the idea that most people who join cultic groups leave on their own.&lt;/p&gt;
    &lt;p&gt;Ryan concedes that their work can look a lot like meddling in someone’s life. But he is also firm in that they are not “hired hitmen”. They work with psychologists, psychiatrists and social workers to provide oversight, several of whom I spoke with for this piece. “You can’t just interfere with someone’s life because you don’t like what they’re doing,” Ryan told me. When Kelly and Ryan take on a case, it’s because there is some dynamic in the family system that they think their expertise can help untangle. In every case, the group in question is offering something to the person involved that the family might not be able to understand or appreciate. But to Ryan and Kelly, this appreciation is exactly the point.&lt;/p&gt;
    &lt;p&gt;One of their cases in the 90s involved a cult leader who was systematically sexually assaulting the group’s members. “I can’t get into all the details,” Ryan said. “He was horrible, a horrible man.” Ryan and Kelly had been flying regularly to Australia to work on the case. The client’s niece, a girl in the group, was beginning to fall out with the cult. The leader had been arrested and was on trial for crimes related to the cult’s activities.&lt;/p&gt;
    &lt;p&gt;In their process, Ryan and Kelly require what they call 50 things: “You have to find 50 things that you could agree with the person on.” Ryan gestured to a painting on the wall in their living room. It was a strange, surrealist-looking canvas with a big Tesla coil in the center and lightning shooting out at some pigeons. Ryan said, “If you look at this piece of art and say, ‘That’s really ugly,’ then we’re going to start off … not on the right page, right?”&lt;/p&gt;
    &lt;p&gt;But if I could appreciate what he found appealing, then, he said: “I think you have the right to criticize it.” The number may seem arbitrary, but their goal is to find 50 things a family can appreciate about a cult before discussing what they do not agree with.&lt;/p&gt;
    &lt;p&gt;I put this number to Lalich and she said the notion of having to find 50 things seemed a bit extreme. “ I certainly could never find 50 things about my cult that I thought were good.” The spirit of it seemed right to her though, at least: that the family needs to tone down their rhetoric, or they will just push the cult-involved member away.&lt;/p&gt;
    &lt;p&gt;In Kelly and Ryan’s case, the girl’s uncle, their client, had a very difficult time finding anything positive about the group or the leader who had allegedly raped his niece. When the trial came, the uncle wanted to testify against the leader, and Ryan and Kelly told him not to. “We said, if you testify, your niece … will cut you off.”&lt;/p&gt;
    &lt;p&gt;The uncle went to court anyway. Just as Ryan had predicted, the niece fell off the map entirely. She was scared they would kidnap her – try to deprogram or threaten her. Ryan and Kelly pulled some strings to find out that she had done some traveling, but otherwise, for “20 years”, Ryan said, “they didn’t know if she was alive or dead.”&lt;/p&gt;
    &lt;p&gt;On Ryan and Kelly’s counsel, the family made a social media account in the 2010s to post information about the family: weddings, births, etc. After nearly 30 years, the girl, now in middle age, finally reached out. The family had posted about how the grandfather was getting old, and she called to say she wanted to see him before he died.&lt;/p&gt;
    &lt;p&gt;Much has been written about the psychology of cults, the archetypes of cult leaders and the way they can create tragic, abusive conditions for their members. In just the past few years there have been Christian sects convicted of manslaughter of children, doomsday groups killing police officers, and starvation cults with bodies piled in mass graves. While Lalich says that to her, it is pretty clear what is or is not a cult, she also concedes that groups exist on a broad continuum ranging from extremely dangerous to “more or less” benign. She does not think that there is such a thing as a “harmless” cult – since all these groups exert some measure of coercion and manipulation. But for Ryan and Kelly, defining precisely what is or is not a cult is actually counterproductive, since so much of what they do is appeal to the person inside the cult who they are trying to reason with.&lt;/p&gt;
    &lt;p&gt;So, rather than labeling a group as a cult, Ryan and Kelly focus on “cultic relationships” that exist between a member and an organization. “Ten million people have learned Transcendental Meditation,” Ryan clarified. “Ten million people are not in a cult.” His voice rose and he shrugged. “I mean, they’ve been lied to. As a teacher, we lied to them. We told them things that were just absolutely not true.”&lt;/p&gt;
    &lt;p&gt;“Bonkers,” Kelly added from his rocking chair.&lt;/p&gt;
    &lt;p&gt;“Bonkers,” Ryan confirmed.&lt;/p&gt;
    &lt;p&gt;Over the course of their careers Ryan and Kelly have found that in order to mediate people’s relationships with these groups, they have to gain a better understanding of how they are drawn in to begin with. How is it that a cult leader can make a person seriously believe that they can levitate, or that drinking toilet water is acceptable? They have to understand how exactly a group manages to shake people’s fundamental assumptions about reality.&lt;/p&gt;
    &lt;p&gt;For example, Kelly described a case in which a leader would command people to have sex with one another: “‘You, woman, sleep with that woman.’ ‘You, sleep with that man.’” Even if participants were straight, the leader would ask them: “What is your limitation?” This is an archetype of cult leader that Kelly calls a “crazy adept”: “the disruptor, who comes in and destroys the norms in order to build up a better, purer reality.”&lt;/p&gt;
    &lt;p&gt;One of their close collaborators, Ashlen Hilliard, told me about a harrowing case whose details she preferred to keep tightly under wraps. She said they were referred to the case by a US government agency investigating the group, and it had proved extremely dangerous. If they were publicly known to be helping members leave, the group could retaliate. “I care about this,” Ryan said of this interview, “but I care more about not dying.”&lt;/p&gt;
    &lt;p&gt;Hilliard explained that in this group, words like “victim” were twisted out of shape. “Instead of assigning a negative meaning to a word like ‘victim’, they say: this is a word that indicates a badge of honor.” Then, when a member was subject to sexual violence or other abuse by the group, being a “victim” was reframed as something positive. Often, people in these groups have experienced past trauma, and this destabilization of the concept of victimhood can feel freeing – at least initially.&lt;/p&gt;
    &lt;p&gt;What Kelly and Ryan mean when they say these groups are “offering something” to people, it is exactly that. There is a hole a group fills: alienation from community, family, sexuality; pressure to follow a certain life plan, addiction, unrealized spirituality, economic catastrophe – all reasons to join a group. We all have deep pains that make us hope that maybe, if the world were different, we wouldn’t feel the way we do.&lt;/p&gt;
    &lt;p&gt;Part of why their work is so necessarily confidential is that there is always the possibility a person will go back to their group. These are people trying to make sense of a reality whose fundamental rules have been turned on its head. When is anyone ever “done” making sense of things, anyway?&lt;/p&gt;
    &lt;p&gt;Kelly still thinks about a moment with the guru he followed after leaving Transcendental Meditation, back in 1985. He had been meditating at the feet of the guru, Prakashanand Saraswati (who they called Swami-ji, or “guru”), for several days. When he looked up, he saw the Swami surrounded by “a golden light.” He was not seeing an illusion. It was a real experience, built on ideas and promises laid out by the guru: a supreme, divine, transcendent love. “The wave merging into the ocean,” Kelly said.&lt;/p&gt;
    &lt;p&gt;After that experience, Kelly felt Swami-ji could do no wrong. For the next three years, even when he saw the women visiting Swami-ji’s bedroom, the demands for thousands of dollars, the outbursts of rage; it all felt insignificant, or easily dismissed.&lt;/p&gt;
    &lt;p&gt;For that reason, Kelly and Ryan are not looking to convince people of any particular version of reality or truth. They do not seem to be interested in truth at all, really. When you use your experience to test whether or not something is true (the holiness of a guru, the righteousness of a cause) then, Ryan told me: “The person who gives you that experience will own you.” Their work is to usher people into a state of skepticism about the conclusion they have drawn from their experiences; beginning to open them up to the idea that individual experience is not the same as truth or reality.&lt;/p&gt;
    &lt;p&gt;This lighter touch approach is controversial. While interviewing people in the broader cult-awareness network, I found that Ryan and Kelly had drawn some criticism for affiliating with a certain group of academics that some people in their sphere disparage as “cult apologists”. This group belongs to a branch of cultic study that, like Ryan and Kelly, avoid the term “cult”, preferring the term “New Religious Movement”. Kelly and Ryan have consulted these academics over the years and have kept some as trusted contacts. Lalich and others say these apologists undermine survivors’ efforts to hold cults accountable for their abuses, by brushing over the harms such as child neglect and sexual abuse committed by groups like the Children of God (The Family International) or the Unification Church, even testifying in court on a cult’s behalf. It’s a bitter, complicated split in the field of cultic study, but these academics say, among other things, that they are speaking out for freedom of religion. When Ryan and Kelly mentioned these apologists, they said they understood Lalich’s criticism, but that there was a way in which they could see things “through their lens”.&lt;/p&gt;
    &lt;p&gt;Ryan and Kelly are not cult apologists, but in order to do their work they have had to keep an open mind. They neither fully endorse cults’ rights to exist, nor consider groups as bad per se. They arrive from as ideologically empty a place as they can, a skeptical place that is neither here, nor there. Doing work like this, the big question of epistemology, of what we can know and what to believe, become everyday practical quandaries.&lt;/p&gt;
    &lt;p&gt;“I just know what is not real,” Ryan told me once. Take even the broadest existential question: what are we doing here?&lt;/p&gt;
    &lt;p&gt;“The only way that can be answered, in my mind, is by a feeling,” he said. “And, that feeling is so easily manipulated.”&lt;/p&gt;
    &lt;p&gt;You have to be a certain kind of person to do this work. Though Lalich does not do interventions any more, she is glad there are people who do it in the “legitimate way”. When I asked her who she thought did it in the “legitimate way”, she only named four people. Of them, only three, including Ryan and Kelly, were still actively taking cases.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/science/2025/nov/19/how-to-leave-a-cult-experts-intervention"/><published>2025-11-19T12:31:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45978880</id><title>Geothermal's Time Has Come</title><updated>2025-11-19T13:46:50.028604+00:00</updated><content/><link href="https://www.economist.com/interactive/science-and-technology/2025/11/18/geothermal-time-has-finally-come"/><published>2025-11-19T12:38:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45979297</id><title>Your Smartphone, Their Rules: App Stores Enable Corporate-Government Censorship</title><updated>2025-11-19T13:46:49.839622+00:00</updated><content>&lt;doc fingerprint="b584999a93b71151"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Your Smartphone, Their Rules: How App Stores Enable Corporate-Government Censorship&lt;/head&gt;
    &lt;p&gt;Subscribe to the Free Future Newsletter&lt;lb/&gt; Free Future home&lt;/p&gt;
    &lt;p&gt;Who controls what you can do on your mobile phone? What happens when your device can only run what the government decides is OK? We are dangerously close to this kind of totalitarian control, thanks to a combination of government overreach and technocratic infrastructure choices.&lt;/p&gt;
    &lt;p&gt;Most Americans have a smartphone, and the average American spends over 5 hours a day on their phone. While these devices are critical to most people’s daily lives, what they can actually do is shaped by what apps are readily available. A slim majority of American smartphone users use an iPhone, which means they can only install apps available from Apple’s AppStore. Nearly all the rest of US smartphone users use some variant of Android, and by default they get their apps from Google’s Play Store.&lt;/p&gt;
    &lt;p&gt;Collectively, these two app stores shape the universe of what is available to most people as they use the Internet and make their way through their daily lives. When those app stores block or limit apps based on government requests, they are shaping what people can do, say, communicate, and experience.&lt;/p&gt;
    &lt;p&gt;Recently, Apple pulled an app called ICEBlock from the AppStore, making it unavailable in one fell swoop. This app was designed to let people anonymously report public sightings of ICE agents. In the United States people absolutely have a First Amendment right to inform others about what they have seen government officials doing and where — very much including immigration agents whose tactics have been controversial and violent. Apple pulled the ICEBlock app at the demand of the US Department of Justice. The following day, Google pulled a similar app called Red Dot from the Google Play Store.&lt;/p&gt;
    &lt;p&gt;The DOJ’s pressuring of Apple is an unacceptable, censorious overreach. And Google’s subsequent removal of Red Dot looks like troubling premature capitulation. While some experts and activists have expressed concerns over ICEBlock’s design and development practices, those concerns are no reason for the government to meddle in software distribution. The administration’s ostensible free speech warriors are trying to shape how Americans can communicate with each other about matters of pressing political concern.&lt;/p&gt;
    &lt;p&gt;Infrastructure choices&lt;lb/&gt; But the government’s overreach isn’t the whole story here. The current structure of the mobile phone ecosystem enables this kind of abuse and control.&lt;/p&gt;
    &lt;p&gt;Apple’s iOS (the operating system for any iPhone) is designed to only be able to run apps from the AppStore. If Apple hasn’t signed off on it, the app won’t run. This centralized control is ripe for abuse:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple has handed the Chinese government control over what apps are available to iPhone users in China, including banning gay dating apps.&lt;/item&gt;
      &lt;item&gt;The corporation has used its authority over the AppStore to block a game that critiqued its labor practices.&lt;/item&gt;
      &lt;item&gt;Apple’s guidelines say that “‘Enemies’ within the context of a game cannot solely target a specific … government, corporation, or any other real entity.” That represents a potential for sweeping censorship of anyone who wants to use the art of games to criticize companies or otherwise advance political messages.&lt;/item&gt;
      &lt;item&gt;It banned the popular game Fortnite from the App Store as it was battling the gamemaker to get a bigger cut of money from user transactions.&lt;/item&gt;
      &lt;item&gt;In 2012 Apple rejected an app that compiled reports of highly controversial overseas drone strikes by the U.S. government during the “War on Terror.”&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unlike Apple, Google’s Android operating system has traditionally allowed relatively easy access to “sideloading”, which just means installing apps through means other than Google’s Play Store. Although most installations default to getting apps from the Play Store, the availability of sideloading means that even if Google censors apps in the Play Store, people can still install them. Even apps critical of Google can make it onto an Android device. It’s also possible to run a variant of Android without the Play Store at all, such as GrapheneOS.&lt;/p&gt;
    &lt;p&gt;Unfortunately that is all set to change with a recent Google announcement that it will block apps from “certified Android” devices (which is nearly all Android phones) unless they come from what Google calls a “verified developer.” This means that the common Android user trying to install an app will have to get Google’s blessing: does this app come from someone that Google has “verified”? How Google will decide who is allowed to be verified and who is not is still unclear. Can a developer become “unverified”?&lt;/p&gt;
    &lt;p&gt;This upcoming change is framed by Google as a security measure, but merely knowing the identity of the developer of an app doesn’t provide any security. So the only way that the “verified developer” requirement can offer security is if Google withholds “verified developer” status from people it deems bad actors. But Google’s ability to withhold that status can be abused in the same way that Apple’s AppStore lock-in is being abused. A government will simply make a demand: “treat this developer as a bad actor” and effectively cut off any app by targeting its developer.&lt;/p&gt;
    &lt;p&gt;When a lever of control is available, the would-be censors will try to use it. It has never been true that someone who buys a Lenovo or Dell laptop, for example, has to let Lenovo or Dell tell them what programs they can and cannot install on their computer. Yet that will soon be the situation with regards to nearly all cell phones used in the United States.&lt;/p&gt;
    &lt;p&gt;Note that American iPhones are limited to only apps from the AppStore, but European Union (EU) iPhones don’t have that restriction. The EU’s Digital Markets Act (DMA) required Apple to permit alternate app stores and sideloading (which Apple calls “web distribution”). As a result, marketplaces like AltStore are starting to become available — but Apple only lets EU customers use them. The European regime is not perfect, however; while sideloaded apps and alternative app stores aren’t subject to the app store’s constraints, they are still obliged to follow Apple’s “Notarization” requirements, which requires Apple to review all iOS apps – even from these alternate sources – on the basis of several vaguely worded rationales. For example, if the DoJ were to claim that ICEBlock “promoted physical harm” (even though it clearly does not), Apple could use this as an excuse to justify revoking their notarization of the app, which would prevent it from being installed even from these alternate channels.&lt;/p&gt;
    &lt;p&gt;App store security and surveillance&lt;lb/&gt; Both Apple and Google make claims that their app distribution mechanisms improve security for their users. And clearly, these tech giants do block some abusive apps by exercising the control they have.&lt;/p&gt;
    &lt;p&gt;But both of them also regularly allow apps that contain common malicious patterns, including many apps built with surveillance tooling that sell their users’ data to data brokers. If either tech giant were serious about user security, they could ban these practices, but they do not. Google’s security claims are also undermined by the fact that the cellphone hacking company Cellebrite tells law enforcement that Google’s Pixel phones can be hacked, while those running GrapheneOS, created by a small non-profit, cannot. (Asked by a reporter why that was so, Google did not respond.)&lt;/p&gt;
    &lt;p&gt;Making matters worse, organizations like Google are unclear about their policies, and some of their policy statements can put developers and users at risk. Discussing blocking Red Dot, for example, Google told 404Media that “apps that have user generated content must also conduct content moderation.” This implies that Google could become unwilling to distribute fully end-to-end encrypted apps, like Signal Private Messenger or Delta Chat, since those app vendors by design are incapable of reviewing user-generated content. End-to-end encrypted apps are the gold standard for secure communications, and no app store that signals a willingness to remove them can claim to put security first.&lt;/p&gt;
    &lt;p&gt;In addition, even if you’ve carefully curated the apps you have installed from these dominant app stores to avoid spyware and use strongly secure apps, the stores themselves monitor the devices, keeping dossiers of what apps are installed on each device, and maybe more. Being a user of these app stores means being under heavy, regular surveillance.&lt;/p&gt;
    &lt;p&gt;Other options exist&lt;lb/&gt; These centralized, surveilled, censorship-enabling app stores are not the only way to distribute software. Consider alternative app stores for Android, like Accrescent, which prioritizes privacy and security requirements in its apps, and F-Droid, which enables installation of free and open source apps. In addition to offering quality tools and auditing, F-Droid’s policies incentivize the apps distributed on the platform to trim out overwhelming amounts of corporate spyware that infest both Google and Apple’s app stores. Neither F-Droid nor Accrescent do any surveillance of their users at all.&lt;/p&gt;
    &lt;p&gt;The F-Droid developers recently wrote about the impact that Google’s upcoming developer registration requirements are likely to have on the broader ecosystem of privacy-preserving Android apps. The outcome doesn’t look good: the ability to install free and open source software on a common device might be going away. Those few people left using unusual devices (“uncertified” Android deployments like GrapheneOS, or even more obscure non-Android operating systems like phosh) will still have the freedom to install tools that they want, but the overwhelming majority of people will be stuck with what can quickly devolve into a government-controlled cop-in-your-pocket.&lt;/p&gt;
    &lt;p&gt;How we can push back&lt;lb/&gt; In an increasingly centralized world, it will take very little for an abusive government to cause an effective organizing tool to disappear, to block an app that belongs to a critical dissenting media outlet, or to force invasive malware into a software update used by everyone. We need a shared infrastructure that doesn’t permit this kind of centralized control. We can disrupt oligopolistic control over software through user choice (e.g., preferring and installing free software), building good protocol frameworks (e.g., demanding tools that use open standards for interoperability), and through regulatory intervention (e.g., breaking up monopolistic actors, or mandating that an OS must allow sideloading, as the EU did with the DMA).&lt;/p&gt;
    &lt;p&gt;The device you carry with you that is privy to much of your life should be under your control, not under the control of an abusive government or corporations that do its bidding.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.aclu.org/news/free-speech/app-store-oligopoly"/><published>2025-11-19T13:28:40+00:00</published></entry></feed>