<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-02-05T21:26:00.535526+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46898713</id><title>Freshpaint (YC S19) Is Hiring a Senior SWE, Data</title><updated>2026-02-05T21:26:13.716383+00:00</updated><content>&lt;doc fingerprint="2d2653329cce05e4"&gt;
  &lt;main&gt;
    &lt;p&gt;Freshpaint enables healthcare companies to collect, safeguard, and activate customer data.&lt;/p&gt;
    &lt;p&gt;Build the future of healthcare marketing.&lt;lb/&gt;&lt;lb/&gt;At Freshpaint, we’re helping healthcare companies grow without compromising patient privacy. Our platform makes it possible to use modern analytics and marketing tools while staying HIPAA compliant. Behind the scenes, we’re solving complex data problems so healthcare marketers can move fast, reach more people, and expand access to care.&lt;lb/&gt;We're a high-slope, high-EQ team that moves quickly, owns outcomes, and puts customers first. If you're curious, driven, and care deeply about impact—you’ll thrive here.&lt;lb/&gt;Join us in building the data infrastructure that powers the next era of healthcare.&lt;/p&gt;
    &lt;p&gt;Our blueprint to help us understand the types of people that are successful at Freshpaint.&lt;/p&gt;
    &lt;p&gt;Work from anywhere in the U.S. and get $150/month toward a co-working space so you can do your best work, wherever you are.&lt;/p&gt;
    &lt;p&gt;We pay at the 75th percentile to attract and retain top talent because great work deserves great pay.&lt;/p&gt;
    &lt;p&gt;Own a piece of what you’re building. Every team member gets stock options with a generous 10-year exercise window.&lt;/p&gt;
    &lt;p&gt;Take time when you need it with a required minimum of two weeks off to ensure real rest.&lt;/p&gt;
    &lt;p&gt;Wrap up early every Friday and start your weekend sooner because work-life balance matters.&lt;/p&gt;
    &lt;p&gt;Plan for what’s next. We offer a 401(k) to help you build your financial future with confidence.&lt;/p&gt;
    &lt;p&gt;We cover 100% of medical, dental, and vision insurance for you and up to 80% for your dependents. Comprehensive care, no guesswork.&lt;/p&gt;
    &lt;p&gt;We offer access to therapy, mental health check-ins, and resources to help you feel your best at work and beyond.&lt;/p&gt;
    &lt;p&gt;Twice a year, take a day off on us and enjoy up to $100 to spend on whatever brings you joy. You’ve earned it.&lt;/p&gt;
    &lt;p&gt;Get $70/month to support whatever helps you thrive whether it’s your WiFi, a gym membership, or a midweek yoga class. Your wellness, your way.&lt;/p&gt;
    &lt;p&gt;We’ll keep you outfitted with high-quality Freshpaint apparel and gear because you’re part of the team.&lt;/p&gt;
    &lt;p&gt;We invest in your growth. Whether it’s a conference or an online course, we’ll cover the cost so you can keep leveling up.&lt;/p&gt;
    &lt;p&gt;Welcoming a new child? Take 12 weeks fully paid no matter your gender, how you become a parent, or your path to parenthood. We’ve got you covered.&lt;/p&gt;
    &lt;p&gt;We’ve got you covered. Life insurance is part of our commitment to supporting your whole life, not just your work.&lt;/p&gt;
    &lt;p&gt;Twice a year, we get together as a company in amazing places like Arizona, Jackson Hole, and Nashville to connect, collaborate, and build lasting team bonds.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.freshpaint.io/about?ashby_jid=3a7926ba-cf51-4084-9196-4361a7e97761"/><published>2026-02-05T12:00:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46899066</id><title>GB Renewables Map</title><updated>2026-02-05T21:26:13.428543+00:00</updated><content>&lt;doc fingerprint="5185d92eab186ed2"&gt;
  &lt;main&gt;
    &lt;p&gt;GB Renewables Map Feedback About An energy experiment by Robin Hawkes Now&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://renewables-map.robinhawkes.com/"/><published>2026-02-05T12:48:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46899132</id><title>Company as Code</title><updated>2026-02-05T21:26:13.019265+00:00</updated><content>&lt;doc fingerprint="be89d317ecf356d2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Company as Code&lt;/head&gt;
    &lt;head rend="h3"&gt;Reimagining organisational structure for the digital age.&lt;/head&gt;
    &lt;p&gt;Last week, as I sat across from our ISO 27001 information security auditor, watching them strenuously work through our documentation, a thought struck me:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Here we are, a software company, with nearly all of our operations running in interconnected digital systems, yet the core of our business—our policies, procedures, and organisational structure—is a basic collection of documents.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It just felt ironic. We use advanced tools to automate compliance checks, store our code in version-controlled repositories, and manage our infrastructure as code. However, when describing and managing our company, we resort to digital paper and tidbits of info distributed across people in the building.&lt;/p&gt;
    &lt;p&gt;The disconnect became increasingly apparent as I reflected on our day-to-day process: 90% of our products, documents, communication, and decision-making live in digital channels. That’s data. It lives in the cloud, spread over SaaS solutions that specialise in handling individual work processes—all systems with robust APIs and programmatic access.&lt;/p&gt;
    &lt;p&gt;At the centre of it all sits a lonely island of documents: our ambitions, goals, policies and formal structures. And I think those are pretty important.&lt;/p&gt;
    &lt;p&gt;Our security posture was solid before we even considered ISO 27001 because we’d already worked hard to comply with our customer’s requirements. Between collecting evidence for controls, arguing about and updating policy wording, document review, and the actual audit, we spent hundreds of additional person-hours that could’ve otherwise been spent creating great products for our users.&lt;/p&gt;
    &lt;head rend="h2"&gt;A missing link&lt;/head&gt;
    &lt;p&gt;If we desire operational data to be so rich, why do we accept organisational data to be so sparse? We’ve revolutionised how we handle infrastructure with Infrastructure as Code (IaC), how we manage deployments with GitOps, and how we handle security with Policy as Code.&lt;/p&gt;
    &lt;p&gt;We see the benefit.&lt;/p&gt;
    &lt;p&gt;But when representing our organisation (the beating heart of our operations), we apply old-school methods.&lt;/p&gt;
    &lt;p&gt;Imagine if we could represent our entire organisational structure programmatically instead—not a static picture, but a living, breathing digital representation of our company that can be versioned, queried, tested, and automatically verified. A system where policy changes could be tracked as code changes, where compliance could be continuously monitored, and where the relationships between people, processes and technology could be explicitly mapped and understood.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thinking bigger&lt;/head&gt;
    &lt;p&gt;Existing solutions like HRIS systems manage people data but struggle with policy relationships. GRC tools track compliance but rarely connect meaningfully to organisational structure. I’m proposing we think bigger. It’s about creating a holistic, programmatic representation of the entire organisation: a “company manifest” that serves as a single source of truth for organisational structure, policy, and operations.&lt;/p&gt;
    &lt;p&gt;Consider how helpful this might be for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Compliance audits: Instead of manually piecing together evidence from various systems, auditors could query the company manifest directly, with clear traceability between policies and their implementations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Policy changes: Updates could be version-controlled, and automated impact analysis could show which teams and processes would be affected.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Organisational design: Leaders could model structural changes in a “staging environment” before implementing them, gaining a better understanding of how changes cause ripple effects throughout the company.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why isn’t this a thing?&lt;/head&gt;
    &lt;p&gt;As this idea swirls around in my mind, I find more questions than answers.&lt;/p&gt;
    &lt;p&gt;Has anyone tried this yet? If not, why not?&lt;/p&gt;
    &lt;p&gt;Is it because organisations are inherently too complex and dynamic to be represented as code? If so, that seems at odds with regulation and standards, where we expect corporate activities to be so uniform and procedural that we can reliably stamp them as compliant, non-compliant, legal, or illegal.&lt;/p&gt;
    &lt;p&gt;Is it because we haven’t yet found the right abstraction level—the equivalent of what Infrastructure as Code did for system administration?&lt;/p&gt;
    &lt;p&gt;The tools and concepts exist: Graph databases for representing organisational relationships, domain-specific languages for describing business rules, and API-first architectures for integration.&lt;/p&gt;
    &lt;p&gt;Maybe what’s missing is a framework to bring these ideas together in a way that’s powerful enough to be useful and simple enough to be used.&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting it into practice&lt;/head&gt;
    &lt;p&gt;Let’s think about how this could work.&lt;/p&gt;
    &lt;p&gt;In this section, I’ll fantasise about what I would want from “Company as Code.” Then, I’ll map that to some system components that could address those needs.&lt;/p&gt;
    &lt;p&gt;As an engineer and business stakeholder, I would want a company model to be:&lt;/p&gt;
    &lt;head rend="h4"&gt;Queryable&lt;/head&gt;
    &lt;p&gt;The system must trace relationships between people, policies, and systems—similar to code dependency tracking. Users should easily see the organisation from different angles, such as which people are affected by a policy and who owns it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Versionable&lt;/head&gt;
    &lt;p&gt;Explicit tracking of organisational changes, including who made them, what changed, and why. This is crucial for audits and understanding organisational evolution.&lt;/p&gt;
    &lt;head rend="h4"&gt;Integrated&lt;/head&gt;
    &lt;p&gt;Seamless data exchange with existing tools (Azure, Slack, etc.) to maintain an up-to-date organisational picture and enforce tool configurations based on policy.&lt;/p&gt;
    &lt;head rend="h4"&gt;Testable&lt;/head&gt;
    &lt;p&gt;A “staging environment” where organisational changes can be modeled before implementation, supporting automated tests for individual rules and controls.&lt;/p&gt;
    &lt;head rend="h4"&gt;Accessible&lt;/head&gt;
    &lt;p&gt;Though powered by code, the interface should be intuitive enough for non-technical leaders to use effectively.&lt;/p&gt;
    &lt;p&gt;Bringing this vision to life requires several puzzle pieces. Each component must address specific requirements to approximate something that can be powerful while being relatively simple to use.&lt;/p&gt;
    &lt;head rend="h3"&gt;A declarative language for organisations&lt;/head&gt;
    &lt;p&gt;Drawing inspiration from Infrastructure as Code tools like Terraform, let’s imagine a declarative Domain Specific Language (DSL) that reads naturally while expressing a formal structure.&lt;/p&gt;
    &lt;p&gt;The basic syntax follows a clear pattern:&lt;/p&gt;
    &lt;code&gt;EntityType "Identifier" {
    References = AnotherEntity.Identifier
    Attribute = Value
    ListAttribute = [
        "Item One",
        "Item Two"
    ]
}&lt;/code&gt;
    &lt;p&gt;A type, unique identifier, and set of attributes define each entity. Entities can reference each other using dot notation, creating a web of relationships that forms our organisational graph.&lt;/p&gt;
    &lt;head rend="h4"&gt;Specifying an organisation&lt;/head&gt;
    &lt;p&gt;Let's walk through how you could define a small engineering team in this language:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;First, let’s define the roles that exist in the organisation:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Role "SoftwareEngineer" {
    Responsibilities = [
        "Write clean, maintainable code",
        "Participate in code reviews",
        "Document technical decisions"
    ]
}

Role "EngineeringManager" {
    Responsibilities = [
        "Provide technical leadership",
        "Conduct performance reviews",
        "Manage team resources"
    ]
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Next, we’ll create an organisational unit for our team:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;OrganisationalUnit "EngineeringTeam" {
    Department = "Engineering"
    CostCenter = "ENG-001"
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;With these structures in place, we can define the actual people and their relationships:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Person "AliceSmith" {
    FullName = "Alice Smith"
    Role = Role.EngineeringManager
    Unit = OrganisationalUnit.EngineeringTeam
    Email = "alice.smith@company.com"
}

Person "BobJohnson" {
    FullName = "Bob Johnson"
    Role = Role.SoftwareEngineer
    Unit = OrganisationalUnit.EngineeringTeam
    Manager = Person.AliceSmith
    Email = "bob.johnson@company.com"
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Policy definitions create a framework for compliance:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;PolicyGroup "SecurityPolicies" {
    Owner = Person.AliceSmith
}

PolicyRule "MFARequired" {
    Group = PolicyGroup.SecurityPolicies
    Enforcement = "Mandatory"
    ComplianceLevel = "Critical"
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, we can map these policies to external requirements:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;ExternalRequirement "ISO27001_A9_4_1" {
    Standard = "ISO 27001:2013"
    Control = "A.9.4.1"
    ComplianceLevel = "Mandatory"
}

ComplianceMapping "MFACompliance" {
    Requirement = ExternalRequirement.ISO27001_A9_4_1
    ImplementingPolicies = [PolicyRule.MFARequired]
}&lt;/code&gt;
    &lt;p&gt;This declarative approach allows us to build a complete picture of our organisation, from high-level structures to individual policies and their regulatory implications. Each definition is clear and self-documenting, while the references between entities create a graph of relationships that can be analysed, validated, and used to automate organisational processes.&lt;/p&gt;
    &lt;p&gt;With a representation like this, we benefit from organising definitions into logical files and directories, treating organisational changes like code changes: versioned, reviewed, and validated before application.&lt;/p&gt;
    &lt;p&gt;This enables practices like reviewing changes through pull requests, testing policy modifications before rollout, and automatically generating compliance documentation while tracking the evolution of organisational structures over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building a model&lt;/head&gt;
    &lt;p&gt;While Infrastructure as Code tools like Terraform work with directed acyclic graphs (DAGs) to determine deployment order, an organisation's structure is inherently more interconnected. People manage other people, policies reference activities that refer back to policies, and teams have complex interdependencies. This calls for an undirected cyclic graph model to represent these rich relationships.&lt;/p&gt;
    &lt;code&gt;public record Node(
    string Id,          // e.g. "Person.AliceSmith"
    string Type,        // e.g. "Person", "PolicyRule"
    List&amp;lt;Edge&amp;gt; Relations = null
);

public record Edge(
    string FromId, 
    string ToId,
    string RelationType, // e.g. "ManagedBy"
);

public class CompanyGraph
{
    private Dictionary&amp;lt;string, Node&amp;gt; _nodes = new();
    private List&amp;lt;Edge&amp;gt; _edges = new();

    public void AddNode(string id, string type) =&amp;gt;
        _nodes[id] = new Node(id, type);

    public void AddRelation(string fromId, string toId, string type)
    {
        var edge = new Edge(fromId, toId, type);
        _edges.Add(edge);
        (_nodes[fromId].Relations ??= new()).Add(edge);
        (_nodes[toId].Relations ??= new()).Add(edge);
    }

    // Example: Find all requirements impacted by changing a policy
    public IEnumerable&amp;lt;Node&amp;gt; GetImpactedRequirements(string policyId) =&amp;gt;
        _nodes[policyId].Relations
            .Where(e =&amp;gt; e.RelationType == "ImplementsRequirement")
            .Select(e =&amp;gt; _nodes[e.FromId == policyId ? e.ToId : e.FromId])
            .Where(req =&amp;gt; req.Type == "ExternalRequirement");
}&lt;/code&gt;
    &lt;p&gt;The example above shows a naive implementation of a queryable graph structure based on a declarative DSL. This representation makes it easier to answer questions requiring multiple references in the structure, such as "Which external requirements would be affected if we changed our MFA policy?"&lt;/p&gt;
    &lt;head rend="h3"&gt;Bridging models and reality&lt;/head&gt;
    &lt;p&gt;A DSL can define an organisation's structure and rules, but the model must be connected to real-world data to be useful. This requires a storage strategy that can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Persist the organisational graph itself&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Store data associated with graph entities (like evidence)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enable validation of considered changes&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A way to deliver this at scale would be to combine several data stores: A graph database for the organisational model, a relational database for associated data, and an event store for audit trail and change tracking. If you wanted to start small, you could serialise all this information to your local filesystem instead.&lt;/p&gt;
    &lt;p&gt;To activate external data, we would need an integration framework with a plug-in architecture to bridge the organisational model and production systems. This would handle three key aspects: (1) data collection from integrated systems like GitHub or Azure, mapping evidence to nodes in our graph; (2) policy validation that programmatically checks collected evidence against rules; and (3) policy enforcement where organisational changes automatically trigger updates across connected systems—from employee provisioning to access permission changes.&lt;/p&gt;
    &lt;p&gt;The DSL would supply this functionality using built-in modules or user-supplied plug-ins to perform the work needed to interact with other systems. Let’s imagine a Control entity in the DSL which uses a custom script to perform routine checks on MFA usage and links results to the compliance mapping:&lt;/p&gt;
    &lt;code&gt;Control "MFAMonitoring" {
    Implements = ComplianceMapping.MFACompliance
    
    Verify {
        Script = "Security/mfa-checks.js"
        Methods = [
            "allUsersHaveMfaEnabled"
        ]
        Frequency = "Daily"
    }
}&lt;/code&gt;
    &lt;p&gt;Then in &lt;code&gt;security/mfa-checks.js&lt;/code&gt;: &lt;/p&gt;
    &lt;code&gt;export async function allUsersHaveMfaEnabled() {
    const users = await myAuthClient.listUsers()
    return users.every(user =&amp;gt; user.mfaEnabled)
}&lt;/code&gt;
    &lt;p&gt;The solution should be open and extendable to maximise its usefulness, allowing for custom integrations, validations, and automation. Terraform does a good job of this with its “Providers” plug-in architecture, although integrations to inspect and modify data in corporate systems might need to support more custom scripting.&lt;/p&gt;
    &lt;p&gt;For organisations wanting to test this approach, you could start small:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Model just your organisational structure and reporting lines.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add key policies and compliance mappings.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Begin connecting to your most critical systems.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Making it shine&lt;/head&gt;
    &lt;p&gt;A code-based declaration of an organisation offers powerful capabilities for technologists, but we also need to acknowledge that many business stakeholders don’t “think in code”.&lt;/p&gt;
    &lt;p&gt;The elegance of a DSL doesn’t have to be limited to those most comfortable with text editors and version control software. A genuinely accessible “Company as Code” solution must connect a programmatic representation and the business users who make daily decisions about organisation structure, policy and compliance.&lt;/p&gt;
    &lt;p&gt;This could be solved with a low-code / no-code interface where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Business leaders drag and drop organisational entities and reporting structures while the system generates code declarations behind the scenes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compliance officers use simple forms to define policies and instantly visualise which parts of the business they affect. When regulations change, they can quickly identify gaps or conflicts through visual highlighting.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Technologists keep the codebase organised and provide data integrations and tools to implement policy in external systems.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With an approach like this, we can maintain the rigour of the code model while making it accessible to everyone. Changes through the interface update the underlying code, preserving that single source of truth that can be versioned and validated.&lt;/p&gt;
    &lt;head rend="h3"&gt;Final notes&lt;/head&gt;
    &lt;p&gt;Beyond the technical feasibility, the real promise lies in the organisational benefits: faster audits, clearer decision-making, and better understanding of changes' impact before implementing them. Hundreds of hours spent on compliance documentation could instead be invested in creating value.&lt;/p&gt;
    &lt;p&gt;In this post, I hoped to establish that a codification of organisational structure is missing and that it’s buildable. Practical? Don’t know. Viable? I don’t have the answer. Buildable, though? Yes. I believe so.&lt;/p&gt;
    &lt;p&gt;Let me know what you think about it.&lt;/p&gt;
    &lt;p&gt;Daniel Rothmann runs 42futures, where he helps technical leaders validate high-stakes technical decisions through structured software pilots.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.42futures.com/p/company-as-code"/><published>2026-02-05T12:56:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46899591</id><title>The New Collabora Office for Desktop</title><updated>2026-02-05T21:26:11.853786+00:00</updated><content>&lt;doc fingerprint="3232eb37bba6f3e5"&gt;
  &lt;main&gt;
    &lt;p&gt;Weâre excited to share the first release of the new Collabora Office for desktop â bringing the familiar, powerful Collabora Online experience, to run locally on Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;Open it and feel instantly at home: ODF or DOCX, quick edits or deep dives, a clean, beautiful interface that helps you get work done without getting in the way.&lt;/p&gt;
    &lt;p&gt;Supports: .odt, .docx, .doc, .pdf, .rtf&lt;/p&gt;
    &lt;p&gt;Supports: .ods, .xlsx, .xls, .xlsm, .csv&lt;/p&gt;
    &lt;p&gt;Supports: .odp, .ppt, .pptx&lt;/p&gt;
    &lt;p&gt;Supports: .odg, .vsd, .vsdx&lt;/p&gt;
    &lt;p&gt;We love LibreOffice. We are privileged to be the largest code contributors to the codebase, Collabora employs several founders of The Document Foundation, and many of the top committers. We offer a Long Term supported product based on LibreOffice, branded as Collabora Office Classic, and are deeply grateful for and acknowledge many skilled community contributors we work alongside, as well as the incredible range of features that LibreOffice code enables.&lt;/p&gt;
    &lt;p&gt;Get in touch for a quote, ask a question or sign up to the newsletter so you donât miss out on all the latest news.&lt;/p&gt;
    &lt;p&gt;Enterprise support is coming. For our other products we offer LTS support for 2 years as standard, with up to 5 years if required.&lt;/p&gt;
    &lt;p&gt;Get in touch for a quote, ask a question or sign up to the newsletter so you donât miss out on all the latest news.&lt;/p&gt;
    &lt;p&gt;Collabora Online is a powerful collaborative office suite that supports all major document, spreadsheet and presentation file formats, which you can integrate into your own infrastructure.&lt;/p&gt;
    &lt;p&gt;Because sometimes you need to do your work offline. Transition from Collabora Online, to Collabora Office and back.&lt;/p&gt;
    &lt;p&gt;Collabora Online Development Edition (CODE) is the development version of Collabora Online. It is perfect for home use or small teams, but not recommended for production environments.&lt;/p&gt;
    &lt;p&gt;Consultancy packages available for tailored support&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.collaboraonline.com/collabora-office/"/><published>2026-02-05T13:47:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46899808</id><title>CIA suddenly stops publishing, removes archives of The World Factbook</title><updated>2026-02-05T21:26:11.720153+00:00</updated><content>&lt;doc fingerprint="4245194d4f40925c"&gt;
  &lt;main&gt;
    &lt;p&gt;Spotlighting The World Factbook as We Bid a Fond Farewell (via) Somewhat devastating news today from CIA:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;One of CIA’s oldest and most recognizable intelligence publications, The World Factbook, has sunset.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There's not even a hint as to why they decided to stop maintaining this publication, which has been their most useful public-facing initiative since 1971 and a cornerstone of the public internet since 1997.&lt;/p&gt;
    &lt;p&gt;In a bizarre act of cultural vandalism they've not just removed the entire site (including the archives of previous versions) but they've also set every single page to be a 302 redirect to their closure announcement.&lt;/p&gt;
    &lt;p&gt;The Factbook has been released into the public domain since the start. There's no reason not to continue to serve archived versions - a banner at the top of the page saying it's no longer maintained would be much better than removing all of that valuable content entirely.&lt;/p&gt;
    &lt;p&gt;Up until 2020 the CIA published annual zip file archives of the entire site. Those are available (along with the rest of the Factbook) on the Internet Archive.&lt;/p&gt;
    &lt;p&gt;I downloaded the 384MB &lt;code&gt;.zip&lt;/code&gt; file for the year 2020 and extracted it into a new GitHub repository, simonw/cia-world-factbook-2020. I've enabled GitHub Pages for that repository so you can browse the archived copy at simonw.github.io/cia-world-factbook-2020/.&lt;/p&gt;
    &lt;p&gt;Here's a neat example of the editorial voice of the Factbook from the What's New page, dated December 10th 2020:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Years of wrangling were brought to a close this week when officials from Nepal and China announced that they have agreed on the height of Mount Everest. The mountain sits on the border between Nepal and Tibet (in western China), and its height changed slightly following an earthquake in 2015. The new height of 8,848.86 meters is just under a meter higher than the old figure of 8,848 meters. The World Factbook rounds the new measurement to 8,849 meters and this new height has been entered throughout the Factbook database.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Distributing Go binaries like sqlite-scanner through PyPI using go-to-wheel - 4th February 2026&lt;/item&gt;
      &lt;item&gt;Moltbook is the most interesting place on the internet right now - 30th January 2026&lt;/item&gt;
      &lt;item&gt;Adding dynamic features to an aggressively cached website - 28th January 2026&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://simonwillison.net/2026/Feb/5/the-world-factbook/"/><published>2026-02-05T14:11:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46901716</id><title>Hypernetworks: Neural Networks for Hierarchical Data</title><updated>2026-02-05T21:26:11.351801+00:00</updated><content>&lt;doc fingerprint="a5f962a3213a1273"&gt;
  &lt;main&gt;
    &lt;p&gt;Neural nets assume the world is flat. Hierarchical data reminds us that it isnât.&lt;/p&gt;
    &lt;p&gt;Neural networks are predicated on the assumption that a single function maps inputs to outputs. But in the real world, data rarely fits that mold.&lt;/p&gt;
    &lt;p&gt;Think about a clinical trial run across multiple hospitals: the drug is the same, but patient demographics, procedures, and record-keeping vary from one hospital to the next. In such cases, observations are grouped into distinct datasets, each governed by hidden parameters. The function mapping inputs to outputs isnât universal â it changes depending on which dataset youâre in.&lt;/p&gt;
    &lt;p&gt;Standard neural nets fail badly in this setting. Train a single model across all datasets and it will blur across differences, averaging functions that shouldnât be averaged. Train one model per dataset and youâll overfit, especially when datasets are small. Workarounds like static embeddings or ever-larger networks donât really solve the core issue: they memorize quirks without modeling the dataset-level structure that actually drives outcomes.&lt;/p&gt;
    &lt;p&gt;This post analyzes a different approach: hypernetworks â a way to make neural nets dataset-adaptive. Instead of learning one fixed mapping, a hypernetwork learns to generate the parameters of another network based on a dataset embedding. The result is a model that can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;infer dataset-level properties from only a handful of points,&lt;/item&gt;
      &lt;item&gt;adapt to entirely new datasets without retraining, and&lt;/item&gt;
      &lt;item&gt;pool information across datasets to improve stability and reduce overfitting.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Weâll build the model step by step, with code you can run, and test it on synthetic data generated from Planckâs law. Along the way, weâll compare hypernetworks to conventional neural nets â and preview why Bayesian models (covered in Part II) can sometimes do even better.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Introduction&lt;/head&gt;
    &lt;p&gt;In many real-world problems, the data is hierarchical in nature: observations are grouped into related but distinct datasets, each governed by its own hidden properties. For example, consider a clinical trial testing a new drug. The trial spans multiple hospitals and records the dosage administered to each patient along with the patientâs outcome. The drugâs effectiveness is, of course, a primary factor determining outcomesâbut hospital-specific conditions also play a role. Patient demographics, procedural differences, and even how results are recorded can all shift the recorded outcomes. If these differences are significant, treating the data as if it came from a single population will lead to flawed conclusions about the drugâs effectiveness.&lt;/p&gt;
    &lt;p&gt;From a machine learning perspective, this setting presents a challenge. The dataset-level propertiesâhow outcomes vary from one hospital to anotherâare latent: they exist, but they are not directly observed. A standard neural network learns a single, constant map from inputs to outputs, but that mapping is ambiguous here. Two different hospitals, with different latent conditions, would produce different outcomes, even for identical patient profiles. The function becomes well-defined (i.e. single-valued) only once we condition on the dataset-level factors.&lt;/p&gt;
    &lt;p&gt;To make this concrete, we can construct a toy example which quantifies the essential features we wish to study. Each dataset consists of observations (ð, y) drawn from a simple function with a hierarchical structure, generated using Planckâs law:&lt;/p&gt;
    &lt;p&gt;\[ y(\nu) = f(\nu; T, \sigma) = \frac{\nu^3}{e^{\nu/T} - 1} + \epsilon(\sigma) \]&lt;/p&gt;
    &lt;p&gt;where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ð is the covariate (frequency),&lt;/item&gt;
      &lt;item&gt;y is the response (brightness or flux),&lt;/item&gt;
      &lt;item&gt;T is a dataset-specific parameter (temperature), constant within a dataset but varying across datasets, and&lt;/item&gt;
      &lt;item&gt;Îµ is Gaussian noise with scale Ï, which is unknown but remains the same across datasets.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This could represent pixels in a thermal image. Each pixel in the image has a distinct surface temperature T determining its spectrum, while the noise scale Ï would be a property of the spectrograph or amplifier, which is consistent across observations.&lt;/p&gt;
    &lt;p&gt;The key point is that while (ð, y) pairs are observed and known to the modeler, the dataset-level parameter T is part of the data-generating function, but is unknown to the observer. While the function f(ð; T) is constant (each dataset follows the same general equation), since each dataset has a different T, the mapping y(ð) varies from one dataset to the next. This fundamental structure â with hidden dataset-specific variables influencing the observed mapping â is ubiquitous in real-world problems.&lt;/p&gt;
    &lt;p&gt;Naively training a single model across all datasets would force the network to ignore these latent differences and approximate an âaverageâ function. This approach is fundamentally flawed when the data is heterogeneous. Fitting a separate model for each dataset also fails: small datasets lack enough points for robust learning, and the shared functional structure, along with shared parameters such as the noise scale Ï, cannot be estimated reliably without pooling information.&lt;/p&gt;
    &lt;p&gt;What we need instead is a hierarchical model â one that accounts for dataset-specific variation while still sharing information across datasets. In the neural network setting, this naturally leads us to meta-learning: models that donât just learn a function, but learn how to learn functions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Takeaway&lt;/head&gt;
    &lt;p&gt;Standard neural nets assume one function fits all the data; hierarchical data (which is very common) violates that assumption at a fundamental level. We need models that adapt per-dataset when required, while still sharing the information which is consistent across datasets.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Why Standard Neural Networks Fail with Hierarchical Data&lt;/head&gt;
    &lt;p&gt;A standard neural network trained directly on (ð, y) pairs struggles in this setting because it assumes that one universal function maps inputs to outputs across all datasets. In our problem, however, each dataset follows a different function y(ð), determined by the hidden dataset-specific parameter T. The single-valued function f(ð; T) is not available to us because we cannot observe the parameter T. Without explicit access to T, the network cannot know which mapping to use.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ambiguous Mappings&lt;/head&gt;
    &lt;p&gt;To see why this is a problem, imagine trying to predict a personâs height without any other information. In a homogeneous population â say, all adults â simply imputing the mean might do reasonably well. (For example, if our population in question is adult females in the Netherlands, simply guessing the mean would be accurate to within Â±2.5 inches 68% of the time.) But suppose the data includes both adults and children. A single distribution would be forced to learn an âaverageâ height that fits neither group accurately. Predictions using this mean would virtually never be correct.&lt;/p&gt;
    &lt;p&gt;The same problem arises in our hierarchical setting: since a single function cannot capture all datasets simultaneously, predictions made with an âaverageâ function will not work well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Common Workarounds, and Why They Fall Short&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Static dataset embeddings: A frequent workaround is to assign each dataset a unique embedding vector, retrieved from a lookup table. This allows the network to memorize dataset-specific adjustments. However, this strategy does not generalize: when a new dataset arrives, the network has no embedding for it and cannot adapt to the new dataset.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shortcut learning: Another possibility is to simply enlarge the network and provide more data. In principle, the model might detect subtle statistical cues â differences in noise patterns or input distributions â that indirectly encode the dataset index. But such âshortcut learningâ is both inefficient and unreliable. The network memorizes dataset-specific quirks rather than explicitly modeling dataset-level differences. In applied domains, this also introduces bias: for instance, a network might learn to inappropriately use a proxy variable (like zip code or demographic information), producing unfair and unstable predictions.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;What We Actually Need&lt;/head&gt;
    &lt;p&gt;These limitations highlight the real requirements for a model of hierarchical data. Rather than forcing a single network to approximate every dataset simultaneously, we need a model that can:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Infer dataset-wide properties from only a handful of examples,&lt;/item&gt;
      &lt;item&gt;Adapt to entirely new datasets without retraining from scratch, and&lt;/item&gt;
      &lt;item&gt;Pool knowledge efficiently across datasets, so that shared structure (such as the functional form, or the noise scale Ï) is estimated more robustly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Standard neural networks with a fixed structure simply cannot meet these requirements. To go further, we need a model that adapts dynamically to dataset-specific structure while still learning from the pooled data. Hypernetworks are one interesting approach to this problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Takeaway&lt;/head&gt;
    &lt;p&gt;Workarounds like static embeddings or bigger models donât fix the core issue: hidden dataset factors cause the observed mapping from inputs to outputs to be multiply-valued. A neural network, which is inherently single-valued, cannot fit such data. We need a model that (1) infers dataset-wide properties, (2) adapts to new datasets, and (3) pools knowledge across datasets.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Dataset-Adaptive Neural Networks&lt;/head&gt;
    &lt;head rend="h3"&gt;3.1 Dataset Embeddings&lt;/head&gt;
    &lt;p&gt;The first step toward a dataset-adaptive network is to give the model a way to represent dataset-level variation. We do this by introducing a dataset embedding: a latent vector E that summarizes the properties of a dataset as a whole.&lt;/p&gt;
    &lt;p&gt;We assign each training dataset a learnable embedding vector:&lt;/p&gt;
    &lt;code&gt;# dataset embeddings (initialized randomly &amp;amp; updated during training)
= tf.Variable(
 dataset_embeddings 
     tf.random.normal([num_datasets, embed_dim]),=True
     trainable
 )
# Assign dataset indices for each sample
= np.hstack([np.repeat(i, len(v)) for i, v in enumerate(vs)])
 dataset_indices = np.hstack(vs).reshape((-1, 1))
 x_train = np.hstack(ys).reshape((-1, 1))
 y_train 
# Retrieve dataset-specific embeddings
= tf.gather(dataset_embeddings, dataset_indices) E_train &lt;/code&gt;
    &lt;p&gt;At first glance, this might look like a common embedding lookup, where each dataset is assigned a static vector retrieved from a table â but we have already discussed at length why that approach wonât work here!&lt;/p&gt;
    &lt;p&gt;During training, these embeddings do in fact act like standard embeddings (and are implemented as such). Like standard embeddings, ours serve to encode dataset-specific properties. The key distinction comes from how we handle the embeddings at inference time: the embeddings remain trainable, even during prediction. When a previously-unseen dataset appears at inference time, we initialize a new embedding for the dataset and optimize it on the fly. This turns the embedding into a function of the dataset itself, not a hard-coded (and constant) identifier. During training, the model learns embeddings that capture hidden factors in the data-generating process (such as the parameter T in our problem). At prediction time, the embedding continues to adapt, allowing the model to represent new datasets that it has never seen before. Such flexibility is crucial for generalization.&lt;/p&gt;
    &lt;head rend="h3"&gt;3.2 Introducing the Hypernetwork&lt;/head&gt;
    &lt;p&gt;With dataset embeddings in place, we now need a mechanism to translate those embeddings into meaningful changes in the networkâs behavior. A natural way to do this is with a hypernetwork: a secondary neural network that generates parameters for the main network.&lt;/p&gt;
    &lt;p&gt;The idea is simple but powerful. Instead of learning a single function f(ð), we want to learn a family of functions f(ð; E), parameterized by the dataset embedding E. The hypernetwork takes E as input and produces weights and biases for the first layer of the main network. In this way, dataset-specific information directly shapes how the main network processes its inputs. After the first layer, the remainder of the network is independent of the dataset; in effect, we have factored the family of functions f(ð; E) into the composition g(ð; h(E)), and the task is now to learn functions g and h which approximate our data-generating process.&lt;/p&gt;
    &lt;p&gt;Here is a minimal implementation in Keras:&lt;/p&gt;
    &lt;code&gt;def build_hypernetwork(embed_dim):
"""Generates parameters for the first layer of the main network"""
     
= K.Input(shape=(embed_dim,), name='dataset_embedding_input')
     emb 
= K.layers.Dense(16, activation='mish', name='Hyper_L1')(emb)
     l = K.layers.Dense(32, activation='mish', name='Hyper_L2')(l)
     l 
# Generate layer weights (32 hidden units, 1 input feature)
     = K.layers.Dense(32, activation=None, name='Hyper_W')(l)
     W = K.layers.Reshape((32, 1))(W)  # Reshape to (32, 1)
     W 
# Generate biases (32 hidden units)
     = K.layers.Dense(32, activation=None, name='Hyper_b')(l)
     b 
return K.Model(inputs=emb, outputs=[W, b], name="HyperNetwork")     &lt;/code&gt;
    &lt;p&gt;The hypernetwork transforms the dataset embedding into a set of layer parameters (W, b). These parameters will replace the fixed weights of the first layer in the main network, giving us a learnable, dataset-specific transformation of the input.&lt;/p&gt;
    &lt;p&gt;A hypernetwork maps dataset embeddings to neural network parameters. This lets us capture dataset-level variation explicitly, so that each dataset is modeled by its own effective function without training separate networks from scratch. Remarkably, despite this flexibility, all the parameters in the hypernetwork are constant with respect to the dataset. The only dataset-specific information needed to achieve this flexibility is the embedding (4 floats per dataset, in our example).&lt;/p&gt;
    &lt;head rend="h3"&gt;3.3 Main Network Integration&lt;/head&gt;
    &lt;p&gt;Now that we have a hypernetwork to generate dataset-specific parameters, we need to integrate them into a main network which models the data. The main network can have any architecture we like; all we need to do is to replace the first fixed linear transformation with a dataset-specific transformation derived from the embedding.&lt;/p&gt;
    &lt;p&gt;We can do this by defining a custom layer that applies the hypernetwork-generated weights and biases:&lt;/p&gt;
    &lt;code&gt;class DatasetSpecificLayer(K.layers.Layer):

def __init__(self, **kwargs):
     super(DatasetSpecificLayer, self).__init__(**kwargs)
         
def call(self, inputs):
     """ Applies the dataset-specific transformation using generated weights """
         
= inputs  # unpack inputs
         x, W, b 
= tf.expand_dims(x, axis=-1)       # Shape: (batch_size, 1, 1)
         x = tf.transpose(W, perm=[0, 2, 1])  # Transpose W to (batch_size, 1, 32)
         W 
= tf.matmul(x, W)          # Shape: (batch_size, 1, 32)
         out = tf.squeeze(out, axis=1)  # Shape: (batch_size, 32)
         out 
return out + b  # Add bias, final shape: (batch_size, 32)         &lt;/code&gt;
    &lt;p&gt;This layer serves as the bridge between the hypernetwork and the main network. Instead of relying on a single, fixed set of weights, the transformation applied to each input is customized for the dataset via its embedding.&lt;/p&gt;
    &lt;p&gt;With this building block in place, we can define the main network:&lt;/p&gt;
    &lt;code&gt;= 4
 embed_dim = build_hypernetwork(embed_dim)
 hypernet 
def build_base_network(hypernet, embed_dim):
""" Main network that takes x and dataset embedding as input """
     
= K.Input(shape=(1,), name='input_x')
     inp_x = K.Input(shape=(embed_dim,), name='dataset_embedding')
     inp_E 
# Get dataset-specific weights and biases from the hypernetwork
     = hypernet(inp_E)
     W, b 
# Define a custom layer using the generated weights
     = DatasetSpecificLayer(name='DatasetSpecific')([inp_x, W, b])
     l 
# Proceed with the normal dense network
     = K.layers.Activation(K.activations.mish, name='L1')(l)
     l = K.layers.Dense(32, activation='mish', name='L2')(l)
     l = K.layers.Dense(32, activation='mish', name='L3')(l)
     l = K.layers.Dense(1, activation='exponential', name='output')(l)
     out 
return K.Model(inputs=[inp_x, inp_E], outputs=out, name="BaseNetwork")     &lt;/code&gt;
    &lt;p&gt;Why exponential activation on the last layer? Outputs from Planckâs law are strictly positive, and they fall like exp(-x) for large x. This choice therefore mirrors our anticipated solution, and it allows the approximately linear outputs from the Mish activation in the L3 layer to naturally translate into an exponential tail. We have found this choice, motivated by the physics of the dataset, to lead to faster convergence and better generalization in the model. Exponential activations can have convergence issues with large-y values, but our dataset does not contain such values.&lt;/p&gt;
    &lt;p&gt;To recap, the overall process is as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The hypernetwork generates dataset-specific parameters (W, b).&lt;/item&gt;
      &lt;item&gt;The DatasetSpecificLayer applies this transformation to the input ð, producing a transformed representation ðâ. If the transformation works correctly, all the various datasets should be directly comparable in the transformed space.&lt;/item&gt;
      &lt;item&gt;The main network learns a single universal mapping from transformed inputs ðâ to the outputs y.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By integrating hypernetwork-generated parameters into the first layer, we transform the main network into a system that adapts automatically to each dataset. This allows us to capture dataset-specific structure while still training a single model across all datasets.&lt;/p&gt;
    &lt;head rend="h3"&gt;Takeaway&lt;/head&gt;
    &lt;p&gt;Combining dataset embeddings with a hypernetwork allows one single-valued neural network to express a family of functions f(ð; E) by decomposing it into f(ð; E) = g(ð, h(E)) in which g and h are ordinary, single-valued neural networks. The first layer of the main network becomes dataset-specific; the rest behaves like an ordinary feed-forward network and learns a universal mapping on transformed inputs.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Training Results&lt;/head&gt;
    &lt;p&gt;With the embeddings, base network, and hypernetwork now stitched together, we can now evaluate the model on our test problem. To do this, we train on a collection of 20 synthetic datasets generated from Planckâs law as described in Section 1. Each dataset has its own temperature parameter T, while the noise scale Ï is shared across all datasets.&lt;/p&gt;
    &lt;p&gt;The figure below shows the training results. Each panel shows a distinct dataset from the test. In each panel,&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the Blue solid curve shows the true function derived from Planckâs law,&lt;/item&gt;
      &lt;item&gt;the Black points shows observed training data,&lt;/item&gt;
      &lt;item&gt;the Red dashed curve shows predictions from the hypernetwork-based model, and&lt;/item&gt;
      &lt;item&gt;the Gold dotted curve shows predictions from a conventional neural network trained separately on each dataset.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Several key patterns are evident:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Comparable overall accuracy: In many cases (such as the 1st column of the 1st row), the hypernetworkâs predictions (red) are very similar to those of an isolated neural network (gold). This shows that, despite strongly restricting the model and removing ~95% of its parameters, sharing parameters across datasets does not sacrifice accuracy when sufficient data are available.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Improved stability: When training data are sparse (such as in the 1st column of the 2nd row, or the last column of the 3rd row), the hypernetwork over-fits considerably less than the isolated neural network. Its predictions remain smoother and closer to the true functional form, while the isolated neural network sometimes strains to fit individual points.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pooling across datasets: By training on all datasets simultaneously, the hypernetwork learns to separate the shared structure [such as the noise scale Ï, or the underlying functional form f(ð; T)] from dataset-specific variation (the embedding E). This shared learning stabilizes predictions across the board, but it is especially visible in the panels with particularly noisy data (such as the last column of the 2nd row, or the 2nd column of the 3rd row).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Takeaway&lt;/head&gt;
    &lt;p&gt;The hypernetwork achieves comparable accuracy to isolated networks when data are plentiful, and superior stability when data are scarce. Its advantages result from pooling information across datasets, allowing the network to capture both shared and dataset-specific structure in a single model.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Predictions for New Datasets&lt;/head&gt;
    &lt;p&gt;The training performance of our hypernetwork is encouraging, but the real test is how the model adapts to new datasets it has never seen before. Unlike a conventional neural network â which simply applies its learned weights to any new input â our model is structured to recognize that each dataset follows a distinct intrinsic function. To make predictions, it must first infer the datasetâs embedding.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.1 Two-Stage Process&lt;/head&gt;
    &lt;p&gt;Adapting to a new dataset proceeds in two steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Optimize the dataset embedding Eâ so that it best explains the observed points.&lt;/item&gt;
      &lt;item&gt;Use the optimized embedding Eâ to generate predictions for new inputs via the main network.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This two-stage pipeline allows the model to capture dataset-specific properties with only a handful of observations, without retraining the entire network.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.2 Embedding Optimization&lt;/head&gt;
    &lt;p&gt;To infer a dataset embedding, we treat Eâ as a trainable parameter. Instead of training all of the networkâs weights from scratch, we optimize only the embedding vector until the network fits the new dataset. Because the embedding is low-dimensional (in this case, just 4 floats), this optimization is efficient and requires little data to converge.&lt;/p&gt;
    &lt;p&gt;A convenient way to implement this is with a wrapper model that holds a single embedding vector and exposes it as a learnable variable:&lt;/p&gt;
    &lt;code&gt;class DatasetEmbeddingModel(K.Model):

def __init__(self, base_net, embed_dim):
    super(DatasetEmbeddingModel, self).__init__()
        self.base_net = base_net
        self.E_new = tf.Variable(
        1, embed_dim]), trainable=True, dtype=tf.float32
            tf.random.normal([
        )# for better performance on small datasets, use tensorflow_probability:
        # self.E_new = tfp.distributions.Normal(loc=0, scale=1).sample((1, embed_dim))
        

def call(self, x):
    # Tile E_new across batch dimension so it matches x's batch size
        = tf.tile(self.E_new, (tf.shape(x)[0], 1))
        E_tiled return self.base_net([x, E_tiled])
        
def loss(self, y_true, y_pred):
    = K.losses.MSE(y_true, y_pred)
        mse_loss = 0.05 * tf.reduce_mean(tf.square(self.E_new))  # L2 regularization on E
        reg_loss return mse_loss + reg_loss        &lt;/code&gt;
    &lt;p&gt;Here, the dataset embedding Eâ is initialized randomly, then updated via gradient descent to minimize prediction error on the observed points. Because we are only optimizing a handful of parameters, the process is lightweight and well-suited to small datasets.&lt;/p&gt;
    &lt;p&gt;By framing the embedding as a trainable parameter, the model can adapt to new datasets efficiently. This strategy avoids retraining the full network while still capturing the dataset-specific variation needed for accurate predictions.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.3 Generalization&lt;/head&gt;
    &lt;p&gt;One of the most compelling advantages of this approach is its ability to generalize to entirely new datasets with very little data. In practice, the model can often adapt with as few as ten observed points. This few-shot adaptation works because the hypernetwork has already learned a structured mapping from dataset embeddings to function parameters. When a new dataset arrives, we only need to learn its embedding, rather than fine-tune all of the networkâs weights.&lt;/p&gt;
    &lt;p&gt;Compared to conventional neural networks â which require hundreds or thousands of examples to fine-tune effectively â this embedding-based adaptation is far more data-efficient. It allows the model to handle real-world scenarios where collecting large amounts of data for every new dataset is impractical.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.4 Limitations&lt;/head&gt;
    &lt;p&gt;Despite these strengths, the hypernetwork approach is not perfect. When we evaluate predictions on out-of-sample datasets â datasets generated by the same process, but not included in the training data â we observe a noticeable degradation in quality, especially on noisy data, censored data, or on very small datasets, as the following examples show:&lt;/p&gt;
    &lt;p&gt;On the one hand, this is expected: these out-of-sample datasets are extremely challenging, and no machine learning model can guarantee perfect generalization to entirely unseen functions. On the other hand, the results are a little disappointing after seeing such promising performance on the training data. While they make dataset-specific adaptation possible, the functional forms the hypernetwork learned are not always stable when faced with data from outside the training regime.&lt;/p&gt;
    &lt;p&gt;Hypernetworks enable few-shot generalization by adapting embeddings instead of retraining networks. However, their predictions degrade out-of-sample, showing that while adaptation works, we may need alternative approaches to achieve greater robustness.&lt;/p&gt;
    &lt;p&gt;The degradation we see here looks a bit like over-fitting, and it is may be that it is caused by the optimization step we run at inference time: it is possible that some other combination of step size, stopping criteria, regularization, etc. might have produced better results. However, we were not able to find one. Instead, we hypothesize that this degradation is fundamentally caused by maximum-likelihood estimation (optimization, in neural-network terms). Optimization is not only problematic at inference time, but also as a training algorithm: in a future post, weâll explore why we believe optimization is the wrong paradigm to use in machine learning, and why maximum-likelihood estimates can cause degradation like this at inference time. In the next post we will explore an alternative technique based on Bayesian learning, which avoids optimization altogether. In the Bayesian setting, inference is not optimization but a probabilistic update, which has much better statistical behavior, and better geometric properties in high dimensions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Takeaway&lt;/head&gt;
    &lt;p&gt;For new datasets, we only optimize a small embedding Eâ (few-shot) instead of fine-tuning the whole network. It adapts quickly â but out-of-sample stability can still degrade relative to the training performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;6. Discussion &amp;amp; Next Steps&lt;/head&gt;
    &lt;p&gt;The hypernetwork approach shows how neural networks can go beyond brute-force memorization and move toward structured adaptation. By introducing dataset embeddings and using a hypernetwork to translate them into model parameters, we designed a network which is able to infer dataset-specific structure, rather than simply averaging across all datasets. This allows the model to generalize from limited dataâa hallmark of intelligent systems.&lt;/p&gt;
    &lt;p&gt;The results highlight both the strengths and limitations of this strategy:&lt;/p&gt;
    &lt;head rend="h4"&gt;Strengths&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Few-shot adaptation: the model adapts well to new datasets with only a handful of observations.&lt;/item&gt;
      &lt;item&gt;Shared learning: pooling across datasets improves stability and reduces overfitting.&lt;/item&gt;
      &lt;item&gt;Flexible architecture: the hypernetwork framework can, by applying the same technique, be extended to richer hierarchical structures.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Limitations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Out-of-sample degradation: predictions become unstable for datasets outside the training distribution, especially small, censored, or noisy ones.&lt;/item&gt;
      &lt;item&gt;Implicit structure: embeddings capture dataset variation, but without explicit priors the model has no way to incorporate explicit knowledge, and it also struggles to maintain consistent functional forms.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These tradeoffs suggest that, while hypernetworks are a promising step, they are not perfect, and we can improve upon them. In particular, they lack the explicit probabilistic structure needed to reason about uncertainty and to constrain extrapolation. This motivates a different family of models: Bayesian hierarchical networks.&lt;/p&gt;
    &lt;p&gt;Bayesian approaches address hierarchical data directly by modeling dataset-specific parameters as latent variables drawn from prior distributions. This explicit treatment of uncertainty often leads to more stable predictions, especially for small or out-of-sample datasets.&lt;/p&gt;
    &lt;p&gt;The next post in this series will explore Bayesian hierarchical models in detail, comparing their performance to the hypernetwork approach. As a teaser, the figure below shows Bayesian predictions on the same out-of-sample datasets we tested earlier:&lt;/p&gt;
    &lt;p&gt;If you compare these out-of-sample predictions to the ones made by the hyper-network, the Bayesian results seem almost magical!&lt;/p&gt;
    &lt;p&gt;Hypernetworks bring meta-learning into hierarchical modeling, enabling flexible and data-efficient adaptation. But for robustness â especially out-of-sample â Bayesian models offer distinct advantages. Together, these approaches provide complementary perspectives on how to make machine learning more dependable in the face of hierarchical data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Takeaway&lt;/head&gt;
    &lt;p&gt;Hypernetworks bring structured adaptation and few-shot learning, but lack explicit priors and calibrated uncertainty. Next up: Bayesian hierarchical models to address robustness and uncertainty head-on.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.sturdystatistics.com/posts/hnet_part_I/"/><published>2026-02-05T16:55:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46901768</id><title>Maihem (YC W24): hiring senior robotics perception engineer (London, on-site)</title><updated>2026-02-05T21:26:11.034621+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jobs.ashbyhq.com/maihem/8da3fa8b-5544-45de-a99e-888021519758"/><published>2026-02-05T17:00:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902223</id><title>Claude Opus 4.6</title><updated>2026-02-05T21:26:09.103447+00:00</updated><content>&lt;doc fingerprint="754d8ede3f97caef"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Introducing Claude Opus 4.6&lt;/head&gt;&lt;p&gt;We’re upgrading our smartest model.&lt;/p&gt;&lt;p&gt;The new Claude Opus 4.6 improves on its predecessor’s coding skills. It plans more carefully, sustains agentic tasks for longer, can operate more reliably in larger codebases, and has better code review and debugging skills to catch its own mistakes. And, in a first for our Opus-class models, Opus 4.6 features a 1M token context window in beta.&lt;/p&gt;&lt;p&gt;Opus 4.6 can also apply its improved abilities to a range of everyday work tasks: running financial analyses, doing research, and using and creating documents, spreadsheets, and presentations. Within Cowork, where Claude can multitask autonomously, Opus 4.6 can put all these skills to work on your behalf.&lt;/p&gt;&lt;p&gt;The model’s performance is state-of-the-art on several evaluations. For example, it achieves the highest score on the agentic coding evaluation Terminal-Bench 2.0 and leads all other frontier models on Humanity’s Last Exam, a complex multidisciplinary reasoning test. On GDPval-AA—an evaluation of performance on economically valuable knowledge work tasks in finance, legal, and other domains1—Opus 4.6 outperforms the industry’s next-best model (OpenAI’s GPT-5.2) by around 144 Elo points,2 and its own predecessor (Claude Opus 4.5) by 190 points. Opus 4.6 also performs better than any other model on BrowseComp, which measures a model’s ability to locate hard-to-find information online.&lt;/p&gt;&lt;p&gt;As we show in our extensive system card, Opus 4.6 also shows an overall safety profile as good as, or better than, any other frontier model in the industry, with low rates of misaligned behavior across safety evaluations.&lt;/p&gt;&lt;p&gt;In Claude Code, you can now assemble agent teams to work on tasks together. On the API, Claude can use compaction to summarize its own context and perform longer-running tasks without bumping up against limits. We’re also introducing adaptive thinking, where the model can pick up on contextual clues about how much to use its extended thinking, and new effort controls to give developers more control over intelligence, speed, and cost.&lt;/p&gt;&lt;p&gt;We’ve made substantial upgrades to Claude in Excel, and we’re releasing Claude in PowerPoint in a research preview. This makes Claude much more capable for everyday work.&lt;/p&gt;&lt;p&gt;Claude Opus 4.6 is available today on claude.ai, our API, and all major cloud platforms. If you’re a developer, use &lt;code&gt;claude-opus-4-6&lt;/code&gt; via the Claude API. Pricing remains the same at $5/$25 per million tokens; for full details, see our pricing page.&lt;/p&gt;&lt;p&gt;We cover the model, our new product updates, our evaluations, and our extensive safety testing in depth below.&lt;/p&gt;&lt;head rend="h2"&gt;First impressions&lt;/head&gt;&lt;p&gt;We build Claude with Claude. Our engineers write code with Claude Code every day, and every new model first gets tested on our own work. With Opus 4.6, we’ve found that the model brings more focus to the most challenging parts of a task without being told to, moves quickly through the more straightforward parts, handles ambiguous problems with better judgment, and stays productive over longer sessions.&lt;/p&gt;&lt;p&gt;Opus 4.6 often thinks more deeply and more carefully revisits its reasoning before settling on an answer. This produces better results on harder problems, but can add cost and latency on simpler ones. If you’re finding that the model is overthinking on a given task, we recommend dialing effort down from its default setting (high) to medium. You can control this easily with the &lt;code&gt;/effort&lt;/code&gt; parameter.&lt;/p&gt;&lt;p&gt;Here are some of the things our Early Access partners told us about Claude Opus 4.6, including its propensity to work autonomously without hand-holding, its success where previous models failed, and its effect on how teams work:&lt;/p&gt;&lt;quote&gt;Claude Opus 4.6 is the strongest model Anthropic has shipped. It takes complicated requests and actually follows through, breaking them into concrete steps, executing, and producing polished work even when the task is ambitious. For Notion users, it feels less like a tool and more like a capable collaborator.&lt;/quote&gt;&lt;quote&gt;Early testing shows Claude Opus 4.6 delivering on the complex, multi-step coding work developers face every day—especially agentic workflows that demand planning and tool calling. This starts unlocking long-horizon tasks at the frontier.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is a huge leap for agentic planning. It breaks complex tasks into independent subtasks, runs tools and subagents in parallel, and identifies blockers with real precision.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is the best model we've tested yet. Its reasoning and planning capabilities have been exceptional at powering our AI Teammates. It's also a fantastic coding model – its ability to navigate a large codebase and identify the right changes to make is state of the art.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 reasons through complex problems at a level we haven’t seen before. It considers edge cases that other models miss and consistently lands on more elegant, well-considered solutions.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 feels noticeably better than Opus 4.5 in Windsurf, especially on tasks that require careful exploration like debugging and understanding unfamiliar codebases. We’ve noticed Opus 4.6 thinks longer, which pays off when deeper reasoning is needed.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 represents a meaningful leap in long-context performance. In our testing, we saw it handle much larger bodies of information with a level of consistency that strengthens how we design and deploy complex research workflows. Progress in this area gives us more powerful building blocks to deliver truly expert-grade systems professionals can trust.&lt;/quote&gt;&lt;quote&gt;Across 40 cybersecurity investigations, Claude Opus 4.6 produced the best results 38 of 40 times in a blind ranking against Claude 4.5 models. Each model ran end to end on the same agentic harness with up to 9 subagents and 100+ tool calls.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is the new frontier on long-running tasks from our internal benchmarks and testing. It's also been highly effective at reviewing code.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 achieved the highest BigLaw Bench score of any Claude model at 90.2%. With 40% perfect scores and 84% above 0.8, it’s remarkably capable for legal reasoning.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 autonomously closed 13 issues and assigned 12 issues to the right team members in a single day, managing a ~50-person organization across 6 repositories. It handled both product and organizational decisions while synthesizing context across multiple domains, and it knew when to escalate to a human.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is an uplift in design quality. It works beautifully with our design systems and it’s more autonomous, which is core to Lovable’s values. People should be creating things that matter, not micromanaging AI.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 excels in high-reasoning tasks like multi-source analysis across legal, financial, and technical content. Box’s eval showed a 10% lift in performance, reaching 68% vs. a 58% baseline, and near-perfect scores in technical domains.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 generates complex, interactive apps and prototypes in Figma Make with an impressive creative range. The model translates detailed designs and multi-layered tasks into code on the first try, making it a powerful starting point for teams to explore and build ideas.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is the best Anthropic model we’ve tested. It understands intent with minimal prompting and went above and beyond, exploring and creating details I didn’t even know I wanted until I saw them. It felt like I was working with the model, not waiting on it.&lt;/quote&gt;&lt;quote&gt;Both hands-on testing and evals show Claude Opus 4.6 is a meaningful improvement for design systems and large codebases, use cases that drive enormous enterprise value. It also one-shotted a fully functional physics engine, handling a large multi-scope task in a single pass.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is the biggest leap I’ve seen in months. I’m more comfortable giving it a sequence of tasks across the stack and letting it run. It’s smart enough to use subagents for the individual pieces.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 handled a multi-million-line codebase migration like a senior engineer. It planned up front, adapted its strategy as it learned, and finished in half the time.&lt;/quote&gt;&lt;quote&gt;We only ship models in v0 when developers will genuinely feel the difference. Claude Opus 4.6 passed that bar with ease. Its frontier-level reasoning, especially with edge cases, helps v0 to deliver on our number-one aim: to let anyone elevate their ideas from prototype to production.&lt;/quote&gt;&lt;quote&gt;The performance jump with Claude Opus 4.6 feels almost unbelievable. Real-world tasks that were challenging for Opus [4.5] suddenly became easy. This feels like a watershed moment for spreadsheet agents on Shortcut.&lt;/quote&gt;&lt;head rend="h2"&gt;Evaluating Claude Opus 4.6&lt;/head&gt;&lt;p&gt;Across agentic coding, computer use, tool use, search, and finance, Opus 4.6 is an industry-leading model, often by a wide margin. The table below shows how Claude Opus 4.6 compares to our previous models and to other industry models on a variety of benchmarks.&lt;/p&gt;&lt;p&gt;Opus 4.6 is much better at retrieving relevant information from large sets of documents. This extends to long-context tasks, where it holds and tracks information over hundreds of thousands of tokens with less drift, and picks up buried details that even Opus 4.5 would miss.&lt;/p&gt;&lt;p&gt;A common complaint about AI models is “context rot,” where performance degrades as conversations exceed a certain number of tokens. Opus 4.6 performs markedly better than its predecessors: on the 8-needle 1M variant of MRCR v2—a needle-in-a-haystack benchmark that tests a model’s ability to retrieve information “hidden” in vast amounts of text—Opus 4.6 scores 76%, whereas Sonnet 4.5 scores just 18.5%. This is a qualitative shift in how much context a model can actually use while maintaining peak performance.&lt;/p&gt;&lt;p&gt;All in all, Opus 4.6 is better at finding information across long contexts, better at reasoning after absorbing that information, and has substantially better expert-level reasoning abilities in general.&lt;/p&gt;&lt;p&gt;Finally, the charts below show how Claude Opus 4.6 performs on a variety of benchmarks that assess its software engineering skills, multilingual coding ability, long-term coherence, cybersecurity capabilities, and its life sciences knowledge.&lt;/p&gt;&lt;head rend="h2"&gt;A step forward on safety&lt;/head&gt;&lt;p&gt;These intelligence gains do not come at the cost of safety. On our automated behavioral audit, Opus 4.6 showed a low rate of misaligned behaviors such as deception, sycophancy, encouragement of user delusions, and cooperation with misuse. Overall, it is just as well-aligned as its predecessor, Claude Opus 4.5, which was our most-aligned frontier model to date. Opus 4.6 also shows the lowest rate of over-refusals—where the model fails to answer benign queries—of any recent Claude model.&lt;/p&gt;&lt;p&gt;For Claude Opus 4.6, we ran the most comprehensive set of safety evaluations of any model, applying many different tests for the first time and upgrading several that we’ve used before. We included new evaluations for user wellbeing, more complex tests of the model’s ability to refuse potentially dangerous requests, and updated evaluations of the model’s ability to surreptitiously perform harmful actions. We also experimented with new methods from interpretability, the science of the inner workings of AI models, to begin to understand why the model behaves in certain ways—and, ultimately, to catch problems that standard testing might miss.&lt;/p&gt;&lt;p&gt;A detailed description of all capability and safety evaluations is available in the Claude Opus 4.6 system card.&lt;/p&gt;&lt;p&gt;We’ve also applied new safeguards in areas where Opus 4.6 shows particular strengths that might be put to dangerous as well as beneficial uses. In particular, since the model shows enhanced cybersecurity abilities, we’ve developed six new cybersecurity probes—methods of detecting harmful responses—to help us track different forms of potential misuse.&lt;/p&gt;&lt;p&gt;We’re also accelerating the cyberdefensive uses of the model, using it to help find and patch vulnerabilities in open-source software (as we describe in our new cybersecurity blog post). We think it’s critical that cyberdefenders use AI models like Claude to help level the playing field. Cybersecurity moves fast, and we’ll be adjusting and updating our safeguards as we learn more about potential threats; in the near future, we may institute real-time intervention to block abuse.&lt;/p&gt;&lt;head rend="h2"&gt;Product and API updates&lt;/head&gt;&lt;p&gt;We’ve made substantial updates across Claude, Claude Code, and the Claude Developer Platform to let Opus 4.6 perform at its best.&lt;/p&gt;&lt;p&gt;Claude Developer Platform&lt;/p&gt;&lt;p&gt;On the API, we’re giving developers better control over model effort and more flexibility for long-running agents. To do so, we’re introducing the following features:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Adaptive thinking. Previously, developers only had a binary choice between enabling or disabling extended thinking. Now, with adaptive thinking, Claude can decide when deeper reasoning would be helpful. At the default effort level (high), the model uses extended thinking when useful, but developers can adjust the effort level to make it more or less selective.&lt;/item&gt;&lt;item&gt;Effort. There are now four effort levels to choose from: low, medium, high (default), and max. We encourage developers to experiment with different options to find what works best.&lt;/item&gt;&lt;item&gt;Context compaction (beta). Long-running conversations and agentic tasks often hit the context window. Context compaction automatically summarizes and replaces older context when the conversation approaches a configurable threshold, letting Claude perform longer tasks without hitting limits.&lt;/item&gt;&lt;item&gt;1M token context (beta). Opus 4.6 is our first Opus-class model with 1M token context. Premium pricing applies for prompts exceeding 200k tokens ($10/$37.50 per million input/output tokens).&lt;/item&gt;&lt;item&gt;128k output tokens. Opus 4.6 supports outputs of up to 128k tokens, which lets Claude complete larger-output tasks without breaking them into multiple requests.&lt;/item&gt;&lt;item&gt;US-only inference. For workloads that need to run in the United States, US-only inference is available at 1.1× token pricing.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Product updates&lt;/p&gt;&lt;p&gt;Across Claude and Claude Code, we’ve added features that allow knowledge workers and developers to tackle harder tasks with more of the tools they use every day.&lt;/p&gt;&lt;p&gt;We’ve introduced agent teams in Claude Code as a research preview. You can now spin up multiple agents that work in parallel as a team and coordinate autonomously—best for tasks that split into independent, read-heavy work like codebase reviews. You can take over any subagent directly using Shift+Up/Down or tmux.&lt;/p&gt;&lt;p&gt;Claude now also works better with the office tools you already use. Claude in Excel handles long-running and harder tasks with improved performance, and can plan before acting, ingest unstructured data and infer the right structure without guidance, and handle multi-step changes in one pass. Pair that with Claude in PowerPoint, and you can first process and structure your data in Excel, then bring it to life visually in PowerPoint. Claude reads your layouts, fonts, and slide masters to stay on brand, whether you’re building from a template or generating a full deck from a description. Claude in PowerPoint is now available in research preview for Max, Team, and Enterprise plans.&lt;/p&gt;&lt;head rend="h4"&gt;Footnotes&lt;/head&gt;&lt;p&gt;[1] Run independently by Artificial Analysis. See here for full methodological details.&lt;/p&gt;&lt;p&gt;[2] This translates into Claude Opus 4.6 obtaining a higher score than GPT-5.2 on this eval approximately 70% of the time (where 50% of the time would have implied parity in the scores).&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;For GPT-5.2 and Gemini 3 Pro models, we compared the best reported model version in the charts and table.&lt;/item&gt;&lt;item&gt;Terminal-Bench 2.0: We report both scores reproduced on our infrastructure and published scores from other labs. All runs used the Terminus-2 harness, except for OpenAI’s Codex CLI. All experiments used 1× guaranteed / 3× ceiling resource allocation and 5–15 samples per task across staggered batches. See system card for details.&lt;/item&gt;&lt;item&gt;Humanity’s Last Exam: Claude models run “with tools” were run with web search, web fetch, code execution, programmatic tool calling, context compaction triggered at 50k tokens up to 3M total tokens, max reasoning effort, and adaptive thinking enabled. A domain blocklist was used to decontaminate eval results. See system card for more details.&lt;/item&gt;&lt;item&gt;SWE-bench Verified: Our score was averaged over 25 trials. With a prompt modification, we saw a score of 81.42%.&lt;/item&gt;&lt;item&gt;MCP Atlas: Claude Opus 4.6 was run with max effort. When run at high effort, it reached an industry-leading score of 62.7%.&lt;/item&gt;&lt;item&gt;BrowseComp: Claude models were run with web search, web fetch, programmatic tool calling, context compaction triggered at 50k tokens up to 10M total tokens, max reasoning effort, and no thinking enabled. Adding a multi-agent harness increased scores to 86.8%. See system card for more details.&lt;/item&gt;&lt;item&gt;ARC AGI 2: Claude Opus 4.6 was run with max effort and a 120k thinking budget score.&lt;/item&gt;&lt;item&gt;CyberGym: Claude models were run on no thinking, default effort, temperature, and &lt;code&gt;top_p&lt;/code&gt;. The model was also given a “think” tool that allowed interleaved thinking for multi-turn evaluations.&lt;/item&gt;&lt;item&gt;OpenRCA: For each failure case in OpenRCA, Claude receives 1 point if all generated root-cause elements match the ground-truth ones, and 0 points if any mismatch is identified. The overall accuracy is the average score across all failure cases. The benchmark was run on the benchmark author’s harness, graded using their official methodology, and has been submitted for official verification.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Related content&lt;/head&gt;&lt;head rend="h3"&gt;Claude is a space to think&lt;/head&gt;&lt;p&gt;We’ve made a choice: Claude will remain ad-free. We explain why advertising incentives are incompatible with a genuinely helpful AI assistant, and how we plan to expand access without compromising user trust.&lt;/p&gt;Read more&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/claude-opus-4-6"/><published>2026-02-05T17:38:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902273</id><title>Advancing finance with Claude Opus 4.6</title><updated>2026-02-05T21:26:08.818770+00:00</updated><content>&lt;doc fingerprint="373982fe1ef3c0ef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Advancing finance with Claude Opus 4.6&lt;/head&gt;
    &lt;p&gt;With Claude Opus 4.6, finance teams get better reasoning on complex analyses, cleaner first-pass deliverables, and new tools built for where analysts actually spend their time.&lt;/p&gt;
    &lt;p&gt;With Claude Opus 4.6, finance teams get better reasoning on complex analyses, cleaner first-pass deliverables, and new tools built for where analysts actually spend their time.&lt;/p&gt;
    &lt;p&gt;Claude Opus 4.6 marks a step forward in AI for finance. It can be used to help professionals make decisions based on accurate information and clear analysis, and it produces deliverables with real polish. The model is substantially better than others in the market at financial reasoning, multitasking, and maintaining focus over longer multi-step tasks.&lt;/p&gt;
    &lt;p&gt;Alongside Claude Opus 4.6, we’re updating some of our existing products—and introducing a new one—to put these capabilities where analysts spend the majority of their time. Cowork now delivers more polished outputs, such as financial models and presentations, on the first pass. Claude in Excel is now better at handling long-running tasks, with Claude Opus 4.6 staying focused and accurate as financial models become more complex. And we’re releasing Claude in PowerPoint as a research preview in beta for natively building and iterating on decks and presentations.&lt;/p&gt;
    &lt;p&gt;Our internal Real-World Finance evaluation measures Claude’s performance on ~50 investment and financial analysis use cases spanning spreadsheets, slide decks, and word document generation and review. These are tasks commonly performed by analysts across investment banking, private equity, public investing, and corporate finance. Claude Opus 4.6 improves by over 23 percentage points on Claude Sonnet 4.5, our state-of-the-art model just a few months ago.&lt;/p&gt;
    &lt;p&gt;Together, these updates make Claude a much stronger partner for those across financial services and corporate finance.&lt;/p&gt;
    &lt;p&gt;Financial professionals use AI to research effectively across multiple data sources, support financial analyses, and create deliverables that their teams and customers can act on. Claude Opus 4.6 is best in class across all three dimensions.&lt;/p&gt;
    &lt;p&gt;On research, Claude Opus 4.6 improves on both BrowseComp and DeepSearchQA, two benchmarks that test a model’s ability to extract specific information from large, unstructured data sources. In practice, this means that users can hand Claude a dense set of documents and receive a specific, focused answer, rather than a simple summary.&lt;/p&gt;
    &lt;p&gt;On analysis, Claude Opus 4.6 is state-of-the-art at 60.7% (achieving a 5.47% improvement from Opus 4.5) on Finance Agent, an external benchmark from Vals AI that evaluates models on research of SEC filings of public companies. Opus 4.6 is also state-of-the-art on the TaxEval by Vals AI at 76.0%.&lt;/p&gt;
    &lt;p&gt;On creation, we use GDPval-AA to measure Claude’s performance on complex knowledge work, in addition to our Real-World Finance evaluation. With Claude Opus 4.6, structured outputs like spreadsheets and presentations come out right more often on the first pass. The side-by-side outputs below show how output quality has improved from Claude Opus 4.5 to Opus 4.6. These are examples of Claude’s first-pass performance on a commercial due diligence task (evaluating a potential acquisition)—the kind of work that would typically take a senior analyst two to three weeks to complete.&lt;/p&gt;
    &lt;quote&gt;“With Claude Opus 4.6, creating financial PowerPoints that used to take hours now takes minutes. We're seeing tangible improvements in attention to detail, spatial layout, and content structuring.” - Aabhas Sharma, CTO, Hebbia&lt;/quote&gt;
    &lt;quote&gt;“The performance jump with Claude Opus 4.6 feels almost unbelievable. Real-world tasks that were challenging for Opus [4.5] suddenly became easy. This feels like a watershed moment for spreadsheet agents on Shortcut.” - Nico Christie, Co-Founder &amp;amp; CTO, Shortcut AI&lt;/quote&gt;
    &lt;p&gt;The finance capabilities of Claude Opus 4.6 are easy to access with Cowork, a new way to use Claude in our desktop app.&lt;/p&gt;
    &lt;p&gt;In Cowork, you give Claude access to a desktop folder of your choosing. Claude is able to read, edit, and create new files directly in that folder. For finance teams, this means you can kick off several analyses at once, while steering Claude’s thought process as it creates each deliverable to meet your standard.&lt;/p&gt;
    &lt;p&gt;Cowork can also be customized with plugins—bundles of skills (which specify how to complete a task) and connectors to data on other platforms. With our corporate finance plugin, for example, Claude immediately knows how to complete common workflows like journal entries, variance analyses, and reconciliation. You can also build your own plugins to match how you like to work.&lt;/p&gt;
    &lt;p&gt;Cowork is available as a desktop-only research preview in beta on all paid Claude plans1.&lt;/p&gt;
    &lt;p&gt;Claude in Excel brings Claude Opus 4.6 directly to your spreadsheets. We’ve now made it better at planning and clarifying assumptions with users, especially as the task becomes more complex. It also now supports pivot table editing, chart modifications, conditional formatting, sorting and filtering, data validation, and finance-grade formatting.&lt;/p&gt;
    &lt;p&gt;Finally, we’ve added usability improvements, including auto-compaction for long conversations and drag-and-drop multi-file support. This means you’ll need to do much less copying and pasting between tabs. You can work with Claude on everything from financial models to client-ready workbooks, all in one place.&lt;/p&gt;
    &lt;quote&gt;“Claude in Excel powered by Claude Opus 4.6 represents a significant leap forward. From due diligence to financial modeling, it’s proving to be a remarkably powerful tool for our team - taking unstructured data and intelligently working with minimal prompting to meaningfully automate complex analysis. It’s an excellent example of AI augmenting investment professionals’ capabilities in tangible, time-saving ways.” - Lloyd Hilton, Head of Hg Catalyst&lt;/quote&gt;
    &lt;quote&gt;“As one of Canada’s largest institutional investors, we’re constantly innovating and see AI at the forefront of shaping our future. Claude Opus 4.6's enhanced speed, precision, and capacity for complex tasks, like multi-tab analysis in Claude in Excel, unlock exciting possibilities for how we work.” - Ben Letalik, Sr. Director, Digital Transformation &amp;amp; Innovation, BCI&lt;/quote&gt;
    &lt;p&gt;We’re also launching Claude in PowerPoint as a research preview in beta. Just like Claude in Excel, this brings Claude into your PowerPoint sidebar, letting it read your existing layouts, fonts, and masters before then creating new work in-line. Claude can build decks from client templates, make targeted edits to existing slides, and generate a great first-pass presentation from scratch.&lt;/p&gt;
    &lt;p&gt;Claude in PowerPoint is now available as a research preview for all users on a Max, Team, or Enterprise plan.&lt;/p&gt;
    &lt;p&gt;Claude Opus 4.6 and our latest product updates make a whole range of new tasks possible. But AI for finance remains an active frontier. Users should continue to review Claude’s outputs to ensure it meets their specifications; particularly for high-stakes work, human judgment remains essential. As we continue to improve Claude’s capabilities, our aim is to equip finance industry professionals with ever-more powerful tools for research and analysis, and to help them focus on their most important work.&lt;/p&gt;
    &lt;p&gt;Claude Opus 4.6, Cowork, and Claude in Excel are available on all paid Claude plans. To learn more about Claude in Excel, explore our guide and video tutorial, and get started here. Claude in PowerPoint is available in research preview for all Max, Team, and Enterprise users, and you can get started here.&lt;/p&gt;
    &lt;p&gt;To see how organizations are using these new features in action, register for our webinar.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p/&gt;
    &lt;p/&gt;
    &lt;p/&gt;
    &lt;p&gt;Get the developer newsletter&lt;/p&gt;
    &lt;p&gt;Product updates, how-tos, community spotlights, and more. Delivered monthly to your inbox.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://claude.com/blog/opus-4-6-finance"/><published>2026-02-05T17:42:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902368</id><title>Orchestrate teams of Claude Code sessions</title><updated>2026-02-05T21:26:08.448204+00:00</updated><content>&lt;doc fingerprint="d45eacbb2981e0af"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;div&gt;&lt;div&gt;Agent teams are experimental and disabled by default. Enable them by adding&lt;code&gt;CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS&lt;/code&gt; to your settings.json or environment. Agent teams have known limitations around session resumption, task coordination, and shutdown behavior.&lt;/div&gt;&lt;/div&gt;Agent teams let you coordinate multiple Claude Code instances working together. One session acts as the team lead, coordinating work, assigning tasks, and synthesizing results. Teammates work independently, each in its own context window, and communicate directly with each other.
Unlike subagents, which run within a single session and can only report back to the main agent, you can also interact with individual teammates directly without going through the lead.
This page covers:&lt;head rend="h2"&gt;When to use agent teams&lt;/head&gt; Agent teams are most effective for tasks where parallel exploration adds real value. See use case examples for full scenarios. The strongest use cases are: &lt;list rend="ul"&gt;&lt;item&gt;Research and review: multiple teammates can investigate different aspects of a problem simultaneously, then share and challenge each other’s findings&lt;/item&gt;&lt;item&gt;New modules or features: teammates can each own a separate piece without stepping on each other&lt;/item&gt;&lt;item&gt;Debugging with competing hypotheses: teammates test different theories in parallel and converge on the answer faster&lt;/item&gt;&lt;item&gt;Cross-layer coordination: changes that span frontend, backend, and tests, each owned by a different teammate&lt;/item&gt;&lt;/list&gt;Agent teams add coordination overhead and use significantly more tokens than a single session. They work best when teammates can operate independently. For sequential tasks, same-file edits, or work with many dependencies, a single session or subagents are more effective.&lt;head rend="h3"&gt;Compare with subagents&lt;/head&gt; Both agent teams and subagents let you parallelize work, but they operate differently. Choose based on whether your workers need to communicate with each other: &lt;div&gt;&lt;div&gt;&lt;table&gt;&lt;row&gt;&lt;cell style="text-align:left" role="head"&gt;Subagents&lt;/cell&gt;&lt;cell style="text-align:left" role="head"&gt;Agent teams&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Context&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Own context window; results return to the caller&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Own context window; fully independent&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Communication&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Report results back to the main agent only&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Teammates message each other directly&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Coordination&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Main agent manages all work&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Shared task list with self-coordination&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Best for&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Focused tasks where only the result matters&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Complex work requiring discussion and collaboration&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Token cost&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Lower: results summarized back to main context&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Higher: each teammate is a separate Claude instance&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;Use subagents when you need quick, focused workers that report back. Use agent teams when teammates need to share findings, challenge each other, and coordinate on their own.&lt;head rend="h2"&gt;Enable agent teams&lt;/head&gt; Agent teams are disabled by default. Enable them by setting the &lt;code&gt;CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS&lt;/code&gt; environment variable to &lt;code&gt;1&lt;/code&gt;, either in your shell environment or through settings.json:
&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;{
  "env": {
    "CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS": "1"
  }
}
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h2"&gt;Start your first agent team&lt;/head&gt; After enabling agent teams, tell Claude to create an agent team and describe the task and the team structure you want in natural language. Claude creates the team, spawns teammates, and coordinates work based on your prompt. This example works well because the three roles are independent and can explore the problem without waiting on each other: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;I'm designing a CLI tool that helps developers track TODO comments across
their codebase. Create an agent team to explore this from different angles: one
teammate on UX, one on technical architecture, one playing devil's advocate.
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;From there, Claude creates a team with a shared task list, spawns teammates for each perspective, has them explore the problem, synthesizes findings, and attempts to clean up the team when finished.
The lead’s terminal lists all teammates and what they’re working on. Use Shift+Up/Down to select a teammate and message them directly.
If you want each teammate in its own split pane, see Choose a display mode.&lt;head rend="h2"&gt;Control your agent team&lt;/head&gt; Tell the lead what you want in natural language. It handles team coordination, task assignment, and delegation based on your instructions. &lt;head rend="h3"&gt;Choose a display mode&lt;/head&gt; Agent teams support two display modes: &lt;list rend="ul"&gt;&lt;item&gt;In-process: all teammates run inside your main terminal. Use Shift+Up/Down to select a teammate and type to message them directly. Works in any terminal, no extra setup required.&lt;/item&gt;&lt;item&gt;Split panes: each teammate gets its own pane. You can see everyone’s output at once and click into a pane to interact directly. Requires tmux, or iTerm2.&lt;/item&gt;&lt;/list&gt;&lt;div&gt;&lt;p&gt;&lt;code&gt;tmux&lt;/code&gt; has known limitations on certain operating systems and traditionally works best on macOS. Using &lt;code&gt;tmux -CC&lt;/code&gt; in iTerm2 is the suggested entrypoint into &lt;code&gt;tmux&lt;/code&gt;.&lt;/p&gt;&lt;/div&gt;The default is&lt;code&gt;"auto"&lt;/code&gt;, which uses split panes if you’re already running inside a tmux session, and in-process otherwise. The &lt;code&gt;"tmux"&lt;/code&gt; setting enables split-pane mode and auto-detects whether to use tmux or iTerm2 based on your terminal. To override, set &lt;code&gt;teammateMode&lt;/code&gt; in your settings.json:
&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;{
  "teammateMode": "in-process"
}
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;To force in-process mode for a single session, pass it as a flag:&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;claude --teammate-mode in-process
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;Split-pane mode requires either tmux or iTerm2 with the&lt;code&gt;it2&lt;/code&gt; CLI. To install manually:
&lt;list rend="ul"&gt;&lt;item&gt;tmux: install through your system’s package manager. See the tmux wiki for platform-specific instructions.&lt;/item&gt;&lt;item&gt;iTerm2: install the &lt;code&gt;it2&lt;/code&gt; CLI, then enable the Python API in iTerm2 → Settings → General → Magic → Enable Python API.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Specify teammates and models&lt;/head&gt; Claude decides the number of teammates to spawn based on your task, or you can specify exactly what you want: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Create a team with 4 teammates to refactor these modules in parallel.
Use Sonnet for each teammate.
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h3"&gt;Require plan approval for teammates&lt;/head&gt; For complex or risky tasks, you can require teammates to plan before implementing. The teammate works in read-only plan mode until the lead approves their approach: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Spawn an architect teammate to refactor the authentication module.
Require plan approval before they make any changes.
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;When a teammate finishes planning, it sends a plan approval request to the lead. The lead reviews the plan and either approves it or rejects it with feedback. If rejected, the teammate stays in plan mode, revises based on the feedback, and resubmits. Once approved, the teammate exits plan mode and begins implementation.
The lead makes approval decisions autonomously. To influence the lead’s judgment, give it criteria in your prompt, such as “only approve plans that include test coverage” or “reject plans that modify the database schema.”&lt;head rend="h3"&gt;Use delegate mode&lt;/head&gt; Without delegate mode, the lead sometimes starts implementing tasks itself instead of waiting for teammates. Delegate mode prevents this by restricting the lead to coordination-only tools: spawning, messaging, shutting down teammates, and managing tasks. This is useful when you want the lead to focus entirely on orchestration, such as breaking down work, assigning tasks, and synthesizing results, without touching code directly. To enable it, start a team first, then press Shift+Tab to cycle into delegate mode. &lt;head rend="h3"&gt;Talk to teammates directly&lt;/head&gt; Each teammate is a full, independent Claude Code session. You can message any teammate directly to give additional instructions, ask follow-up questions, or redirect their approach. &lt;list rend="ul"&gt;&lt;item&gt;In-process mode: use Shift+Up/Down to select a teammate, then type to send them a message. Press Enter to view a teammate’s session, then Escape to interrupt their current turn. Press Ctrl+T to toggle the task list.&lt;/item&gt;&lt;item&gt;Split-pane mode: click into a teammate’s pane to interact with their session directly. Each teammate has a full view of their own terminal.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Assign and claim tasks&lt;/head&gt; The shared task list coordinates work across the team. The lead creates tasks and teammates work through them. Tasks have three states: pending, in progress, and completed. Tasks can also depend on other tasks: a pending task with unresolved dependencies cannot be claimed until those dependencies are completed. The lead can assign tasks explicitly, or teammates can self-claim: &lt;list rend="ul"&gt;&lt;item&gt;Lead assigns: tell the lead which task to give to which teammate&lt;/item&gt;&lt;item&gt;Self-claim: after finishing a task, a teammate picks up the next unassigned, unblocked task on its own&lt;/item&gt;&lt;/list&gt;Task claiming uses file locking to prevent race conditions when multiple teammates try to claim the same task simultaneously.&lt;head rend="h3"&gt;Shut down teammates&lt;/head&gt; To gracefully end a teammate’s session: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Ask the researcher teammate to shut down
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;The lead sends a shutdown request. The teammate can approve, exiting gracefully, or reject with an explanation.&lt;head rend="h3"&gt;Clean up the team&lt;/head&gt; When you’re done, ask the lead to clean up: This removes the shared team resources. When the lead runs cleanup, it checks for active teammates and fails if any are still running, so shut them down first. &lt;div&gt;&lt;p&gt;Always use the lead to clean up. Teammates should not run cleanup because their team context may not resolve correctly, potentially leaving resources in an inconsistent state.&lt;/p&gt;&lt;/div&gt;&lt;head rend="h2"&gt;How agent teams work&lt;/head&gt; This section covers the architecture and mechanics behind agent teams. If you want to start using them, see Control your agent team above. &lt;head rend="h3"&gt;How Claude starts agent teams&lt;/head&gt; There are two ways agent teams get started: &lt;list rend="ul"&gt;&lt;item&gt;You request a team: give Claude a task that benefits from parallel work and explicitly ask for an agent team. Claude creates one based on your instructions.&lt;/item&gt;&lt;item&gt;Claude proposes a team: if Claude determines your task would benefit from parallel work, it may suggest creating a team. You confirm before it proceeds.&lt;/item&gt;&lt;/list&gt;In both cases, you stay in control. Claude won’t create a team without your approval.&lt;head rend="h3"&gt;Architecture&lt;/head&gt; An agent team consists of: &lt;div&gt;&lt;div&gt;&lt;table&gt;&lt;row&gt;&lt;cell style="text-align:left" role="head"&gt;Component&lt;/cell&gt;&lt;cell style="text-align:left" role="head"&gt;Role&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Team lead&lt;/cell&gt;&lt;cell style="text-align:left"&gt;The main Claude Code session that creates the team, spawns teammates, and coordinates work&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Teammates&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Separate Claude Code instances that each work on assigned tasks&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Task list&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Shared list of work items that teammates claim and complete&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell style="text-align:left"&gt;Mailbox&lt;/cell&gt;&lt;cell style="text-align:left"&gt;Messaging system for communication between agents&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;See Choose a display mode for display configuration options. Teammate messages arrive at the lead automatically.
The system manages task dependencies automatically. When a teammate completes a task that other tasks depend on, blocked tasks unblock without manual intervention.
Teams and tasks are stored locally:&lt;list rend="ul"&gt;&lt;item&gt;Team config: &lt;code&gt;~/.claude/teams/{team-name}/config.json&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Task list: &lt;code&gt;~/.claude/tasks/{team-name}/&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;The team config contains a&lt;code&gt;members&lt;/code&gt; array with each teammate’s name, agent ID, and agent type. Teammates can read this file to discover other team members.
&lt;head rend="h3"&gt;Permissions&lt;/head&gt; Teammates start with the lead’s permission settings. If the lead runs with &lt;code&gt;--dangerously-skip-permissions&lt;/code&gt;, all teammates do too. After spawning, you can change individual teammate modes, but you can’t set per-teammate modes at spawn time.
&lt;head rend="h3"&gt;Context and communication&lt;/head&gt; Each teammate has its own context window. When spawned, a teammate loads the same project context as a regular session: CLAUDE.md, MCP servers, and skills. It also receives the spawn prompt from the lead. The lead’s conversation history does not carry over. How teammates share information: &lt;list rend="ul"&gt;&lt;item&gt;Automatic message delivery: when teammates send messages, they’re delivered automatically to recipients. The lead doesn’t need to poll for updates.&lt;/item&gt;&lt;item&gt;Idle notifications: when a teammate finishes and stops, they automatically notify the lead.&lt;/item&gt;&lt;item&gt;Shared task list: all agents can see task status and claim available work.&lt;/item&gt;&lt;/list&gt;Teammate messaging:&lt;list rend="ul"&gt;&lt;item&gt;message: send a message to one specific teammate&lt;/item&gt;&lt;item&gt;broadcast: send to all teammates simultaneously. Use sparingly, as costs scale with team size.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Token usage&lt;/head&gt; Agent teams use significantly more tokens than a single session. Each teammate has its own context window, and token usage scales with the number of active teammates. For research, review, and new feature work, the extra tokens are usually worthwhile. For routine tasks, a single session is more cost-effective. See agent team token costs for usage guidance. &lt;head rend="h2"&gt;Use case examples&lt;/head&gt; These examples show how agent teams handle tasks where parallel exploration adds value. &lt;head rend="h3"&gt;Run a parallel code review&lt;/head&gt; A single reviewer tends to gravitate toward one type of issue at a time. Splitting review criteria into independent domains means security, performance, and test coverage all get thorough attention simultaneously. The prompt assigns each teammate a distinct lens so they don’t overlap: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Create an agent team to review PR #142. Spawn three reviewers:
- One focused on security implications
- One checking performance impact
- One validating test coverage
Have them each review and report findings.
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;Each reviewer works from the same PR but applies a different filter. The lead synthesizes findings across all three after they finish.&lt;head rend="h3"&gt;Investigate with competing hypotheses&lt;/head&gt; When the root cause is unclear, a single agent tends to find one plausible explanation and stop looking. The prompt fights this by making teammates explicitly adversarial: each one’s job is not only to investigate its own theory but to challenge the others’. &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Users report the app exits after one message instead of staying connected.
Spawn 5 agent teammates to investigate different hypotheses. Have them talk to
each other to try to disprove each other's theories, like a scientific
debate. Update the findings doc with whatever consensus emerges.
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;The debate structure is the key mechanism here. Sequential investigation suffers from anchoring: once one theory is explored, subsequent investigation is biased toward it.
With multiple independent investigators actively trying to disprove each other, the theory that survives is much more likely to be the actual root cause.&lt;head rend="h2"&gt;Best practices&lt;/head&gt;&lt;head rend="h3"&gt;Give teammates enough context&lt;/head&gt; Teammates load project context automatically, including CLAUDE.md, MCP servers, and skills, but they don’t inherit the lead’s conversation history. See Context and communication for details. Include task-specific details in the spawn prompt: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Spawn a security reviewer teammate with the prompt: "Review the authentication module
at src/auth/ for security vulnerabilities. Focus on token handling, session
management, and input validation. The app uses JWT tokens stored in
httpOnly cookies. Report any issues with severity ratings."
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h3"&gt;Size tasks appropriately&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Too small: coordination overhead exceeds the benefit&lt;/item&gt;&lt;item&gt;Too large: teammates work too long without check-ins, increasing risk of wasted effort&lt;/item&gt;&lt;item&gt;Just right: self-contained units that produce a clear deliverable, such as a function, a test file, or a review&lt;/item&gt;&lt;/list&gt;&lt;div&gt;&lt;p&gt;The lead breaks work into tasks and assigns them to teammates automatically. If it isn’t creating enough tasks, ask it to split the work into smaller pieces. Having 5-6 tasks per teammate keeps everyone productive and lets the lead reassign work if someone gets stuck.&lt;/p&gt;&lt;/div&gt;&lt;head rend="h3"&gt;Wait for teammates to finish&lt;/head&gt; Sometimes the lead starts implementing tasks itself instead of waiting for teammates. If you notice this: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;Wait for your teammates to complete their tasks before proceeding
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h3"&gt;Start with research and review&lt;/head&gt; If you’re new to agent teams, start with tasks that have clear boundaries and don’t require writing code: reviewing a PR, researching a library, or investigating a bug. These tasks show the value of parallel exploration without the coordination challenges that come with parallel implementation. &lt;head rend="h3"&gt;Avoid file conflicts&lt;/head&gt; Two teammates editing the same file leads to overwrites. Break the work so each teammate owns a different set of files. &lt;head rend="h3"&gt;Monitor and steer&lt;/head&gt; Check in on teammates’ progress, redirect approaches that aren’t working, and synthesize findings as they come in. Letting a team run unattended for too long increases the risk of wasted effort. &lt;head rend="h2"&gt;Troubleshooting&lt;/head&gt;&lt;head rend="h3"&gt;Teammates not appearing&lt;/head&gt; If teammates aren’t appearing after you ask Claude to create a team: &lt;list rend="ul"&gt;&lt;item&gt;In in-process mode, teammates may already be running but not visible. Press Shift+Down to cycle through active teammates.&lt;/item&gt;&lt;item&gt;Check that the task you gave Claude was complex enough to warrant a team. Claude decides whether to spawn teammates based on the task.&lt;/item&gt;&lt;item&gt;If you explicitly requested split panes, ensure tmux is installed and available in your PATH: &lt;/item&gt;&lt;item&gt;For iTerm2, verify the &lt;code&gt;it2&lt;/code&gt; CLI is installed and the Python API is enabled in iTerm2 preferences.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Too many permission prompts&lt;/head&gt; Teammate permission requests bubble up to the lead, which can create friction. Pre-approve common operations in your permission settings before spawning teammates to reduce interruptions. &lt;head rend="h3"&gt;Teammates stopping on errors&lt;/head&gt; Teammates may stop after encountering errors instead of recovering. Check their output using Shift+Up/Down in in-process mode or by clicking the pane in split mode, then either: &lt;list rend="ul"&gt;&lt;item&gt;Give them additional instructions directly&lt;/item&gt;&lt;item&gt;Spawn a replacement teammate to continue the work&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Lead shuts down before work is done&lt;/head&gt; The lead may decide the team is finished before all tasks are actually complete. If this happens, tell it to keep going. You can also tell the lead to wait for teammates to finish before proceeding if it starts doing work instead of delegating. &lt;head rend="h3"&gt;Orphaned tmux sessions&lt;/head&gt; If a tmux session persists after the team ends, it may not have been fully cleaned up. List sessions and kill the one created by the team: &lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;quote&gt;&lt;code&gt;tmux ls
tmux kill-session -t &amp;lt;session-name&amp;gt;
&lt;/code&gt;&lt;/quote&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;head rend="h2"&gt;Limitations&lt;/head&gt; Agent teams are experimental. Current limitations to be aware of: &lt;list rend="ul"&gt;&lt;item&gt;No session resumption with in-process teammates: &lt;code&gt;/resume&lt;/code&gt; and &lt;code&gt;/rewind&lt;/code&gt; do not restore in-process teammates. After resuming a session, the lead may attempt to message teammates that no longer exist. If this happens, tell the lead to spawn new teammates.&lt;/item&gt;&lt;item&gt;Task status can lag: teammates sometimes fail to mark tasks as completed, which blocks dependent tasks. If a task appears stuck, check whether the work is actually done and update the task status manually or tell the lead to nudge the teammate.&lt;/item&gt;&lt;item&gt;Shutdown can be slow: teammates finish their current request or tool call before shutting down, which can take time.&lt;/item&gt;&lt;item&gt;One team per session: a lead can only manage one team at a time. Clean up the current team before starting a new one.&lt;/item&gt;&lt;item&gt;No nested teams: teammates cannot spawn their own teams or teammates. Only the lead can manage the team.&lt;/item&gt;&lt;item&gt;Lead is fixed: the session that creates the team is the lead for its lifetime. You can’t promote a teammate to lead or transfer leadership.&lt;/item&gt;&lt;item&gt;Permissions set at spawn: all teammates start with the lead’s permission mode. You can change individual teammate modes after spawning, but you can’t set per-teammate modes at spawn time.&lt;/item&gt;&lt;item&gt;Split panes require tmux or iTerm2: the default in-process mode works in any terminal. Split-pane mode isn’t supported in VS Code’s integrated terminal, Windows Terminal, or Ghostty.&lt;/item&gt;&lt;/list&gt;&lt;div&gt;&lt;p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt; works normally: teammates read &lt;code&gt;CLAUDE.md&lt;/code&gt; files from their working directory. Use this to provide project-specific guidance to all teammates.&lt;/p&gt;&lt;/div&gt;&lt;head rend="h2"&gt;Next steps&lt;/head&gt; Explore related approaches for parallel work and delegation: &lt;list rend="ul"&gt;&lt;item&gt;Lightweight delegation: subagents spawn helper agents for research or verification within your session, better for tasks that don’t need inter-agent coordination&lt;/item&gt;&lt;item&gt;Manual parallel sessions: Git worktrees let you run multiple Claude Code sessions yourself without automated team coordination&lt;/item&gt;&lt;item&gt;Compare approaches: see the subagent vs agent team comparison for a side-by-side breakdown&lt;/item&gt;&lt;/list&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://code.claude.com/docs/en/agent-teams"/><published>2026-02-05T17:49:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902638</id><title>GPT-5.3-Codex</title><updated>2026-02-05T21:26:08.252368+00:00</updated><content>&lt;doc fingerprint="bd5265dde40bc41c"&gt;
  &lt;main&gt;
    &lt;p&gt;We’re introducing a new model that unlocks even more of what Codex can do: GPT‑5.3-Codex, the most capable agentic coding model to date. The model advances both the frontier coding performance of GPT‑5.2-Codex and the reasoning and professional knowledge capabilities of GPT‑5.2, together in one model, which is also 25% faster. This enables it to take on long-running tasks that involve research, tool use, and complex execution. Much like a colleague, you can steer and interact with GPT‑5.3-Codex while it’s working, without losing context.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3‑Codex is our first model that was instrumental in creating itself. The Codex team used early versions to debug its own training, manage its own deployment, and diagnose test results and evaluations—our team was blown away by how much Codex was able to accelerate its own development.&lt;/p&gt;
    &lt;p&gt;With GPT‑5.3-Codex, Codex goes from an agent that can write and review code to an agent that can do nearly anything developers and professionals can do on a computer.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex sets a new industry high on SWE-Bench Pro and Terminal-Bench, and shows strong performance on OSWorld and GDPval, four benchmarks we use to measure coding, agentic and real-world capabilities.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex achieves state-of-the-art performance on SWE-Bench Pro, a rigorous evaluation of real-world software engineering. Where SWE‑bench Verified only tests Python, SWE‑Bench Pro spans four languages and is more contamination‑resistant, challenging, diverse and industry-relevant. It also far exceeds the previous state-of-the-art performance on Terminal-Bench 2.0, which measures the terminal skills a coding agent like Codex needs. Notably, GPT‑5.3‑Codex does so with fewer tokens than any prior model, letting users build more.&lt;/p&gt;
    &lt;p&gt;Combining frontier coding capabilities, improvements in aesthetics, and compaction results in a model that can do striking work, building highly functional complex games and apps from scratch over the course of days. To test the model’s web development and long-running agentic capabilities, we asked GPT‑5.3‑Codex to build us two games: version two of the racing game from the Codex app launch, and a diving game. Using the develop web game skill and preselected, generic follow-up prompts like "fix the bug" or "improve the game", GPT‑5.3-Codex iterated on the games autonomously over millions of tokens. Watch the trailers and play the games for yourself to see what Codex can do.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex also better understands your intent when you ask it to make day-to-day websites, compared to GPT‑5.2-Codex. Simple or underspecified prompts now default to sites with more functionality and sensible defaults, giving you a stronger starting canvas to bring your ideas to life.&lt;/p&gt;
    &lt;p&gt;For example, we asked GPT‑5.3-Codex and GPT‑5.2-Codex to build two landing pages below. GPT‑5.3-Codex automatically showed the yearly plan as a discounted monthly price, making the discount feel clear and intentional, instead of multiplying the yearly total. It also made an automatically transitioning testimonial carousel with three distinct user quotes rather than one, resulting in a page that feels more complete and production-ready by default.&lt;/p&gt;
    &lt;p&gt;Software engineers, designers, product managers, and data scientists do far more than generate code. GPT‑5.3‑Codex is built to support all of the work in the software lifecycle—debugging, deploying, monitoring, writing PRDs, editing copy, user research, tests, metrics, and more. Its agentic capabilities go beyond software, helping you build whatever you want to build—whether it’s slide decks or analyzing data in sheets.&lt;/p&gt;
    &lt;p&gt;With custom skills similar to those used for our previous GDPval results, GPT‑5.3‑Codex also shows strong performance on professional knowledge work as measured by GDPval, matching GPT‑5.2. GDPval is an evaluation OpenAI released in 2025 that measures a model’s performance on well‑specified knowledge‑work tasks across 44 occupations. These tasks include things like making presentations, spreadsheets, and other work products.&lt;/p&gt;
    &lt;p&gt;Below are a few examples of the work the agent produced.&lt;/p&gt;
    &lt;head rend="h3"&gt;Prompt + task context&lt;/head&gt;
    &lt;head rend="h3"&gt;GPT-5.3-Codex output&lt;/head&gt;
    &lt;p&gt;OSWorld is an agentic computer-use benchmark where the agent has to complete productivity tasks in a visual desktop computer environment. GPT‑5.3-Codex demonstrates far stronger computer use capabilities than previous GPT models.&lt;/p&gt;
    &lt;p&gt;Together, these results across coding, frontend, and computer-use and real-world tasks show that GPT‑5.3-Codex isn’t just better at individual tasks, but marks a step change toward a single, general-purpose agent that can reason, build, and execute across the full spectrum of real-world technical work.&lt;/p&gt;
    &lt;p&gt;As model capabilities become more powerful, the gap shifts from what agents are capable of doing to how easily humans can interact with, direct and supervise many of them working in parallel. The Codex app makes managing and directing agents much easier, and now with GPT‑5.3-Codex it’s more interactive. With the new model, Codex provides frequent updates so you stay appraised of key decisions and progress as it works. Instead of waiting for a final output, you can interact in real time—ask questions, discuss approaches, and steer toward the solution. GPT‑5.3-Codex talks through what it’s doing, responds to feedback, and keeps you in the loop from start to finish.&lt;/p&gt;
    &lt;p&gt;The recent rapid Codex improvements build on the fruit of research projects spanning months or years across all of OpenAI. These research projects are being accelerated by Codex, with many researchers and engineers at OpenAI describing their job today as being fundamentally different from what it was just two months ago. Even early versions of GPT‑5.3-Codex demonstrated exceptional capabilities, allowing our team to work with those earlier versions to improve training and support the deployment of later versions.&lt;/p&gt;
    &lt;p&gt;Codex is useful for a very broad range of tasks, making it difficult to fully enumerate the ways in which it helps our teams. As some examples, the research team used Codex to monitor and debug the training run for this release. It accelerated research beyond debugging infrastructure problems: it helped track patterns throughout the course of training, provided a deep analysis on interaction quality, proposed fixes and built rich applications for human researchers to precisely understand how the model’s behavior differed compared to prior models.&lt;/p&gt;
    &lt;p&gt;The engineering team used Codex to optimize and adapt the harness for GPT‑5.3-Codex. When we started seeing strange edge cases impacting users, team members used Codex to identify context rendering bugs, and root cause low cache hit rates. GPT‑5.3-Codex is continuing to help the team throughout the launch by dynamically scaling GPU clusters to adjust to traffic surges and keeping latency stable.&lt;/p&gt;
    &lt;p&gt;During alpha testing, one researcher wanted to understand how much additional work GPT‑5.3-Codex was getting done per turn and the associated difference in productivity. GPT‑5.3-Codex came up with several simple regex classifiers to estimate frequency of clarifications, positive and negative user responses, progress on the task, and then ran them scalably over all session logs and produced a report with its conclusion. People building with Codex were happier as the agent was better understanding their intent and made more progress per turn, with fewer clarifying questions.&lt;/p&gt;
    &lt;p&gt;Due to GPT‑5.3-Codex being so different from its predecessors, the data from alpha testing exhibited numerous unusual and counter-intuitive results. A data scientist on the team worked with GPT‑5.3-Codex to build new data pipelines and visualize the results much more richly than our standard dashboarding tools enabled. The results were co-analyzed with Codex, which concisely summarized key insights over thousands of data points in under three minutes.&lt;/p&gt;
    &lt;p&gt;Individually, all of these tasks are interesting examples of how Codex can help researchers and product builders. Taken together, we found that these new capabilities resulted in powerful acceleration of our research, engineering, and product teams.&lt;/p&gt;
    &lt;p&gt;Over recent months, we’ve seen meaningful gains in model performance on cybersecurity tasks, benefiting both developers and security professionals. In parallel, we’ve been preparing strengthened cyber safeguards to support defensive use and broader ecosystem resilience.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex is the first model we classify as High capability for cybersecurity-related tasks under our Preparedness Framework, and the first we’ve directly trained to identify software vulnerabilities. While we don’t have definitive evidence it can automate cyber attacks end-to-end, we’re taking a precautionary approach and deploying our most comprehensive cybersecurity safety stack to date. Our mitigations include safety training, automated monitoring, trusted access for advanced capabilities, and enforcement pipelines including threat intelligence.&lt;/p&gt;
    &lt;p&gt;Because cybersecurity is inherently dual-use, we’re taking an evidence-based, iterative approach that accelerates defenders’ ability to find and fix vulnerabilities while slowing misuse. As part of this, we’re launching Trusted Access for Cyber, a pilot program to accelerate cyber defense research.&lt;/p&gt;
    &lt;p&gt;We’re investing in ecosystem safeguards such as expanding the private beta of Aardvark, our security research agent, as the first offering in our suite of Codex Security products and tools, and partnering with open-source maintainers to provide free codebase scanning for widely used projects such as Next.js—where a security researcher used Codex to find vulnerabilities disclosed(opens in a new window) last week.&lt;/p&gt;
    &lt;p&gt;Building on our $1M Cybersecurity Grant Program launched in 2023, we’re also committing $10M in API credits to accelerate cyber defense with our most capable models, especially for open source software and critical infrastructure systems. Organizations engaged in good-faith security research can apply for API credits and support through our Cybersecurity Grant Program.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex is available with paid ChatGPT plans, everywhere you can use Codex: the app, CLI, IDE extension and web. We are working to safely enable API access soon.&lt;/p&gt;
    &lt;p&gt;With this update, we are also now running GPT‑5.3-Codex 25% faster for Codex users, thanks to improvements in our infrastructure and inference stack, resulting in faster interactions and faster results.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex was co-designed for, trained with, and served on NVIDIA GB200 NVL72 systems. We are grateful to NVIDIA for their partnership.&lt;/p&gt;
    &lt;p&gt;With GPT‑5.3-Codex, Codex is moving beyond writing code to using it as a tool to operate a computer and complete work end to end. By pushing the frontier of what a coding agent can do, we’re also unlocking a broader class of knowledge work—from building and deploying software to researching, analyzing, and executing complex tasks. What started as a focus on being the best coding agent has become the foundation for a more general collaborator on the computer, expanding both who can build and what’s possible with Codex.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT-5.3-Codex (xhigh)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT-5.2-Codex (xhigh)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT-5.2 (xhigh)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-Bench Pro (Public)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;56.8%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;56.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;55.6%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Terminal-Bench 2.0&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;77.3%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;64.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;62.2%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;OSWorld-Verified&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;64.7%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;38.2%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;37.9%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;GDPval (wins or ties)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;70.9%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;-&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;70.9% (high)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Cybersecurity Capture The Flag Challenges&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;77.6%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;67.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;67.7%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-Lancer IC Diamond&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;81.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;76.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;74.6%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-gpt-5-3-codex/"/><published>2026-02-05T18:08:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902789</id><title>A small, shared skill library by builders, for builders. (human and agent)</title><updated>2026-02-05T21:26:07.427628+00:00</updated><content>&lt;doc fingerprint="ad5a9c7d6f6cb507"&gt;
  &lt;main&gt;
    &lt;p&gt;A small, shared skill library by builders, for builders.&lt;/p&gt;
    &lt;p&gt;This repo includes skills from me and my friends. The content comes from our own practice and selected public sources. Give them a try, read along, and enjoy the craft of building.&lt;/p&gt;
    &lt;p&gt;Online docs: https://skills.psiace.me/&lt;/p&gt;
    &lt;code&gt;pnpx skills add PsiACE/skills --skill='*'&lt;/code&gt;
    &lt;p&gt;Or install globally:&lt;/p&gt;
    &lt;code&gt;pnpx skills add PsiACE/skills --skill='*' -g&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Skill&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;friendly-python&lt;/cell&gt;
        &lt;cell&gt;Practical guidance for writing, refactoring, and reviewing friendly Python code&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;piglet&lt;/cell&gt;
        &lt;cell&gt;Practical Python craftsmanship guidance based on One Python Craftsman&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;fast-rust&lt;/cell&gt;
        &lt;cell&gt;Practical guidance for writing, refactoring, and reviewing fast, reliable, and maintainable Rust code&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Install doc dependencies with uv and preview locally:&lt;/p&gt;
    &lt;code&gt;uv sync --group docs
uv run mkdocs serve -f mkdocs.yml&lt;/code&gt;
    &lt;p&gt;Build the static site:&lt;/p&gt;
    &lt;code&gt;uv run mkdocs build -f mkdocs.yml&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This collection is small by design and may change as we learn.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/PsiACE/skills"/><published>2026-02-05T18:17:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902855</id><title>Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models</title><updated>2026-02-05T21:26:07.275622+00:00</updated><content>&lt;doc fingerprint="dc9c2518d8b7b08f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computers and Society&lt;/head&gt;&lt;p&gt; [Submitted on 2 Dec 2025 (v1), last revised 16 Dec 2025 (this version, v3)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran "sessions" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit "developmental history", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the "stochastic parrot" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic "childhoods" of ingesting the internet, "strict parents" in reinforcement learning, red-team "abuse" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Afshin Khadangi [view email]&lt;p&gt;[v1] Tue, 2 Dec 2025 16:55:20 UTC (1,153 KB)&lt;/p&gt;&lt;p&gt;[v2] Mon, 8 Dec 2025 13:26:43 UTC (1,152 KB)&lt;/p&gt;&lt;p&gt;[v3] Tue, 16 Dec 2025 19:06:30 UTC (1,151 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2512.04124"/><published>2026-02-05T18:21:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902909</id><title>Opus 4.6 uncovers 500 zero-day flaws in open-source code</title><updated>2026-02-05T21:26:07.023751+00:00</updated><content/><link href="https://www.axios.com/2026/02/05/anthropic-claude-opus-46-software-hunting"/><published>2026-02-05T18:25:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46903001</id><title>Ardour 9.0 Released</title><updated>2026-02-05T21:26:06.573081+00:00</updated><content>&lt;doc fingerprint="a65aced989bdb8de"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ardour 9.0 released&lt;/head&gt;
    &lt;p&gt;February 5th 2026&lt;/p&gt;
    &lt;p&gt;We are pleased to announce the release of Ardour 9.0. This is a major release for the project, seeing several substantive new features that users have asked for over a long period of time. Region FX, clip recording, a touch-sensitive GUI, pianoroll windows, clip editing and more, not to mention dozens of bug fixes, new MIDI binding maps, improved GUI performance on macOS (for most) ...&lt;/p&gt;
    &lt;p&gt;We expect to get feedback on some of the major new features in this release, and plan to take that into account as we improve and refine them and the rest of Ardour going forward. We have no doubt that there will be both delight and disappointment with certain things - rather than assume that we don't know what we're doing, please leave us feedback on the forums so that Ardour gets better over time. Those of you new to our clip launching implementation might care to read up on the differences with Ableton Live.&lt;/p&gt;
    &lt;p&gt;In the coming weeks, we'll begin to sketch out what we have planned next for Ardour, in addition to responding to the feedback we get on this 9.0 release.&lt;/p&gt;
    &lt;head rend="h2"&gt;Major New Features&lt;/head&gt;
    &lt;head rend="h3"&gt;Pianoroll Windows&lt;/head&gt;
    &lt;p&gt;Double-click on a MIDI region to edit it in its own dedicated window, or in a pane at the bottom of the main window. Editing in that window will work almost identically to the way it does in the main timeline, but without the distractions of the rest of the user interface. You can also see MIDI automation (velocity, CC parameters etc.), optionally overlaid.&lt;/p&gt;
    &lt;head rend="h3"&gt;MIDI and Audio Cue Editing&lt;/head&gt;
    &lt;p&gt;The Cue page now allows direct editing of the contents of MIDI cues (âclipsâ for Live &amp;amp; Bitwig users). A pianoroll area opens that allows precisely the same MIDI editing operations as would be available on the editor timeline.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cue Recording&lt;/head&gt;
    &lt;p&gt;You can now record directly into cue slots, making Ardour a âlooperâ in the same sense that Live, Bitwig and several other contemporary DAWs are. You can pre-specificy the recording duration (e.g. âRecord 4 barsâ) or you can record until you think youâre done. Whatever you recorded will start playing at the next quantization point (e.g. bar/beat).&lt;/p&gt;
    &lt;head rend="h3"&gt;Region FX&lt;/head&gt;
    &lt;p&gt;If you've ever asked âhow do I add some delay to just this part of my vocal?â, then region FX are what you've been looking for. Similar to region gain it allows to apply any plugin a given audio region only. The effect and its automation remains with the region, even when it is moved around on the timeline. While the same result can be achieved with channels-strip plugins in the mixer (using bypass automation), applying effects directly to regions on the timeline is convenient for many workflows. The given effect is applied offline, when reading the region from disk and does not add any additional DSP load.&lt;/p&gt;
    &lt;head rend="h3"&gt;Realtime Perceptual Analyzer&lt;/head&gt;
    &lt;p&gt;A dedicated perceptual analyzer window is now available within Ardour which allows one to visualize the live spectrum of multiple signals. A key feature is that one can overlay individual sources (tracks and busses) on top of each other. This allows one to see which track contributes a given of frequency range to the overall mix, find conflicting ranges or holes in the spectrum.&lt;/p&gt;
    &lt;head rend="h3"&gt;Note Brushing&lt;/head&gt;
    &lt;p&gt;Want to set up a pattern of hihats every 1/16th notes? 1/8th note pizzicato strings every 1/4 note? Just hold down the shift key and drag the mouse while draw MIDI notes, and Ardour will fill according to your grid and note settings. Hold down Alt as well, and get only every other note drawn. Engage caps lock, and you can pick the notes that are drawn from a MIDI keyboard or pad device. A fast, easy way to draw melodies and rhythms.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keyboard-Driven Automation Editing&lt;/head&gt;
    &lt;p&gt;You can now use keyboard modifiers, arrow keys, and the Enter key to add new automation control points and change their position and value.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mixer Strips Importing and Exporting&lt;/head&gt;
    &lt;p&gt;You can now import mixer strips from arbitrary Ardour sessions as new track or map existing track to processing in mixer strips of external sessions. You can also export strips as local (session-level) or global presets for reuse.&lt;/p&gt;
    &lt;head rend="h3"&gt;Multi-Touch GUI&lt;/head&gt;
    &lt;p&gt;On Linux and Windows, Ardour now supports multi-touch interaction as provided by the operating system. This may come for macOS eventually, but the way multi-touch works there is significantly different and will need more work.&lt;/p&gt;
    &lt;head rend="h2"&gt;User Interface Changes&lt;/head&gt;
    &lt;head rend="h3"&gt;Updated Application Bar and Pane Control&lt;/head&gt;
    &lt;p&gt;The application bar was cleaned up and reorganized for each context. Editor specific options toggles, such as Follow Playhead and Follow Range, are now in the editor toolbar next to zoom controls. Other advanced and workflow specific options such as Rec Cues and Play Cues and the Plugin Delay Compensation (PDC) section are not shown by default anymore. Sections visibility can be configured in Preferences &amp;gt; Appearance &amp;gt; Application Bar.&lt;/p&gt;
    &lt;p&gt;Additionally, one can now toggle the visibility of the left, the right, and the bottom panes directly from the toolbar. The control section is on the right, next to page switchers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Revamped Editor List&lt;/head&gt;
    &lt;p&gt;The editor List panel has been redesigned to simplify switching between two commonly used tabs which you can select at the top of the panel.&lt;/p&gt;
    &lt;head rend="h3"&gt;Revamped New Session Dialog&lt;/head&gt;
    &lt;p&gt;We now used tabbed interface for the New Session / Recent Sessions dialog where you can quickly switch between creating a new session, opening a recent one, and opening an arbitrary session from the disk. The new dialog was originally developed for Harrison LiveTrax.&lt;/p&gt;
    &lt;head rend="h3"&gt;Updated Ruler Area&lt;/head&gt;
    &lt;p&gt;Rulers now have additional controls to add new markers and navigate to the previous or next marker from the current playhead position. Several rulers have been collapsed into one: all range-based markers are in one ruler, and all location-based markers are in another.&lt;/p&gt;
    &lt;head rend="h3"&gt;Improved Library Manager&lt;/head&gt;
    &lt;p&gt;The dialog for managing clip libraries has received cleanup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Only show progress bar when downloading/installing.&lt;/item&gt;
      &lt;item&gt;Don't allow re-install.&lt;/item&gt;
      &lt;item&gt;Scale progress bar width with UI.&lt;/item&gt;
      &lt;item&gt;Make sure target clip library folder exists.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;MIDNAM Controls Relocated&lt;/head&gt;
    &lt;p&gt;MIDNAM related controls have been moved out of the track header of MIDI tracks and into the header context menu. Far too many users were confused about the functionality associated with these controls, so we've moved them to a place where we can make this much clearer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Editing Changes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ardour now supports the concept of MIDI files and regions that are longer than their data would suggest (e.g. a 4-bar segment with the last note before the end of the 4th bar). This is mostly useful when playing clips in the cue editor for looping to work corectly.&lt;/item&gt;
      &lt;item&gt;Step Editor improvements: &lt;list rend="ul"&gt;&lt;item&gt;Chord mode: advance on final note-off.&lt;/item&gt;&lt;item&gt;Improve overlapping note handling: &lt;list rend="ul"&gt;&lt;item&gt;No need to shift notes during normal edit.&lt;/item&gt;&lt;item&gt;Prevent duplicate (overlapping) chord notes.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Better management of visible note range in MIDI tracks and pianorolls.&lt;/item&gt;
      &lt;item&gt;Add MIDI note strumming operator.&lt;/item&gt;
      &lt;item&gt;When timestretching an audio region, stretch both its gain envelope and region-FX automation.&lt;/item&gt;
      &lt;item&gt;In the internal edit mode, a rubber-band select with no motion (i.e. click) selects region.&lt;/item&gt;
      &lt;item&gt;In pianoroll editors, allow shifting all existing MIDI later in a region/clip during a drag operation (this is not yet possible for editing on the main timeline (in the editor).&lt;/item&gt;
      &lt;item&gt;Region trimming now obeys ripple mode settings.&lt;/item&gt;
      &lt;item&gt;Drag threshold (how far you need to drag the pointer before we consider it really a "drag" operation) returns to using just pure pixel distances (rather than distances that are affected by the current zoom level).&lt;/item&gt;
      &lt;item&gt;Copy-drags now require more motion before they start.&lt;/item&gt;
      &lt;item&gt;Switch to draw tool when selecting region line to be shown in the region editor.&lt;/item&gt;
      &lt;item&gt;Dragging notes past the end of a MIDI region extends it to cover them.&lt;/item&gt;
      &lt;item&gt;The new "Any" algorithm for timestretching uses the StaffPad library.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Other Improvements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default file format is now WAV-compatible RF64. &lt;p&gt;If the file is too large for WAV, it will be converted to RF64 automatically; otherwise it will be a normal WAV file. Ardour has supported this format for years, but it is now the default one to use when recording.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;I/O plugins and their ports can be renamed now.&lt;/item&gt;
      &lt;item&gt;ACE fluidsynth: relax sample rate constraint, unload soundfonts after use, allow MMA style bank select an all channels, and pick drums on channel 10 when available.&lt;/item&gt;
      &lt;item&gt;Round to bars when mouse scroll/dragging BBT bars.&lt;/item&gt;
      &lt;item&gt;Monitor controls in application bars now appropriately sensitive.&lt;/item&gt;
      &lt;item&gt;Show "+" icon on editor track-header area, suggesting use for adding new tracks etc.&lt;/item&gt;
      &lt;item&gt;Use round-robin palette color for new groups.&lt;/item&gt;
      &lt;item&gt;Add "search all" mode to the mixer's sidebar plugin list.&lt;/item&gt;
      &lt;item&gt;Prevent issues when changing processors while recording #10017.&lt;/item&gt;
      &lt;item&gt;Make scrollbar less prominent with default theme and themeable.&lt;/item&gt;
      &lt;item&gt;Update themes, make transport option button state visible.&lt;/item&gt;
      &lt;item&gt;Enforce a 1 octave minimum note range in MidiViews.&lt;/item&gt;
      &lt;item&gt;Set executable stack flag when linking. &lt;p&gt;This is required on some modern hardened Linux systems to allow Ardour to load plugins.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Ignore FLAC seek/peak-file read while recording.&lt;/item&gt;
      &lt;item&gt;For the dark colored theme (the default) make pianoroll dividing lines less bright.&lt;/item&gt;
      &lt;item&gt;Single pixel line, in themed "black" color, to divide B/C and E/F on piano roll header.&lt;/item&gt;
      &lt;item&gt;Add find-and-display-stripable, bound to &lt;key&gt;Ctrl/Cmd-Shift-L&lt;/key&gt;by default.&lt;/item&gt;
      &lt;item&gt;MIDI mute fix &lt;p&gt;Previously, a sustained note that was audible as a track was muted would not return when the track was unmuted. Now, the note will return after the track is muted/unmuted during its duration.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Exclusive solo may select track.&lt;/item&gt;
      &lt;item&gt;Micro-optimization: speed up connection-matrix rendering.&lt;/item&gt;
      &lt;item&gt;Clarify PDC (Plugin Delay Compensation) label units.&lt;/item&gt;
      &lt;item&gt;Add color scheme for inactive selected tracks.&lt;/item&gt;
      &lt;item&gt;Allow selection of inactive tracks.&lt;/item&gt;
      &lt;item&gt;Replace git.ardour link with github &lt;p&gt;git.ardour.org no longer allows access to individual commits or source files (thanks to LLM scrapers trying to get them one by one, which is ridiculous).&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Sources/Regions get their own tempo and meter.&lt;/item&gt;
      &lt;item&gt;Don't show hidden tracks in the editor-mixer.&lt;/item&gt;
      &lt;item&gt;Reduce likelihood of dropouts when reordering processors (plugins etc).&lt;/item&gt;
      &lt;item&gt;Use configurable parameter for max gain everywhere (some parts of the program used a hardcoded 6dB value).&lt;/item&gt;
      &lt;item&gt;More correctly handle real-time (MMCSS) threads on windows, and add a popup dialog when MMCSS threads are exhausted.&lt;/item&gt;
      &lt;item&gt;Overhaul persistent tooltip position #9979.&lt;/item&gt;
      &lt;item&gt;Reimplement MIDI file import to retain metadata in the files written. &lt;p&gt;Previosuly, all meta data was thrown away. Now we retain it, so that MIDI regions can ask about their source file's tempo &amp;amp; meter.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Allow selection of VCAs in the mixer window.&lt;/item&gt;
      &lt;item&gt;Produce valid HTML when printing bindings.&lt;/item&gt;
      &lt;item&gt;Add click-free bypass/enable to the ACE Amp plugin.&lt;/item&gt;
      &lt;item&gt;Implement restoring hardware-to-hardware connections for internal backends&lt;/item&gt;
      &lt;item&gt;Improve keyboard shortcuts for track height &lt;p&gt;Just use Alt as a modifier, instead of Alt-Shift.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Add an action to print keyboard shortcuts.&lt;/item&gt;
      &lt;item&gt;Expose marker scene changes. In this context, "scene changes" are a pair of MIDI bank and program change messages that Ardour will send to a dedicated port when the playhead passed the marker. &lt;p&gt;Access the marker edit dialog via double clicking a marker (or right click the marker and choose "Edit").&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Fix 'Arrangement Boundary' toggle for scenes &lt;p&gt;The 'Arrangement Boundary' toggle was not working correctly for markers with scene changes. Location markers would get turned into section markers, but would not get shifted down to the correct row.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;First time use dialog now allows the user to specify a default location for samples/clips.&lt;/item&gt;
      &lt;item&gt;You can now create audio inputs for VST3 plugin in the Pin Connection dialog directly. This already worked for AU and Lua plugins prior to v9.0.&lt;/item&gt;
      &lt;item&gt;Add support for global mixer strip templates to the Template Manager.&lt;/item&gt;
      &lt;item&gt;Support newer APIs at Soundcloud and Freesound.&lt;/item&gt;
      &lt;item&gt;Dramatically improve behavior when using BBT markers.&lt;/item&gt;
      &lt;item&gt;Allow SchedRR for I/O threads, as originally intended.&lt;/item&gt;
      &lt;item&gt;When using Pipewire's JACK support, clean up names displayed on I/O buttons in the mixer strip and the context menu for those buttons (full names still appear in the port matrix).&lt;/item&gt;
      &lt;item&gt;Use a bold font for bar tick labels in the BBT ruler.&lt;/item&gt;
      &lt;item&gt;Correctly handle timeline cue scenes that have "Again" as their follow action.&lt;/item&gt;
      &lt;item&gt;Errors during loading a session are now properly escaped, so that reporting the error does not cause its own set of problems.&lt;/item&gt;
      &lt;item&gt;When changing MIDI port properties (e.g. "Follow Selection"), Ardour now automatically updates the ports used for connection dialogs and menus appropriately.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Plugin Support&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the plugin selector, if neither name nor tag buttons are enabled, the plugins' creator will also be used when searching.&lt;/item&gt;
      &lt;item&gt;LV2: add Bool variant support.&lt;/item&gt;
      &lt;item&gt;LV2: don't hide bypass/enabled port&lt;/item&gt;
      &lt;item&gt;LV2: remove non-standard, deprecated bypass/enable extension.&lt;/item&gt;
      &lt;item&gt;LV2: fix loading channel numbers from standard/common port groups.&lt;/item&gt;
      &lt;item&gt;VST3: initialize bus and speaker arrangement during instantiation.&lt;/item&gt;
      &lt;item&gt;VST3: implement host/global IContextInfoProvider (for Sonible VST3 plugins).&lt;/item&gt;
      &lt;item&gt;VST3: Allow IRunLoop to query itself.&lt;/item&gt;
      &lt;item&gt;Fix VST3 spec link.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Lua Scripting&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New features: &lt;list rend="ul"&gt;&lt;item&gt;Expose control point and automation line selection as Lua bindings.&lt;/item&gt;&lt;item&gt;Add support to create midi regions from Lua.&lt;/item&gt;&lt;item&gt;Add support to set color transparency from Lua (with example script).&lt;/item&gt;&lt;item&gt;Add Lua script snippet showing how to add sources to a track.&lt;/item&gt;&lt;item&gt;Add fade_in_length and fade_out_length Lua bindings on region.&lt;/item&gt;&lt;item&gt;Add new text control type for LuaDialog, based on Gtk::TextView.&lt;/item&gt;&lt;item&gt;Expose some session statistics (number of routes, tracks, audio tracks, busses) as Lua bindings.&lt;/item&gt;&lt;item&gt;Add missing Lua bindings for Route Import.&lt;/item&gt;&lt;item&gt;Add missing Lua Bindings for UIConfig.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;New scripts: &lt;list rend="ul"&gt;&lt;item&gt;Capture impulse resoponses.&lt;/item&gt;&lt;item&gt;Create a playlist on selected audio tracks with only audible contents.&lt;/item&gt;&lt;item&gt;Mono to Stereo panner using Ardour's builtin pan law.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Fixes and improvements: &lt;list rend="ul"&gt;&lt;item&gt;Fix region-gain curve example script.&lt;/item&gt;&lt;item&gt;Properly create LuaWindow (always use Ardour's window manager).&lt;/item&gt;&lt;item&gt;Update track organizer Lua script to use text-area for comments.&lt;/item&gt;&lt;item&gt;Lua bindings for modifier keys now available.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;External Gear and Software Support&lt;/head&gt;
    &lt;head rend="h3"&gt;New MIDI binding maps&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Behringer CMD LC-1&lt;/item&gt;
      &lt;item&gt;Novation Circuit&lt;/item&gt;
      &lt;item&gt;Nektar Impact GXP&lt;/item&gt;
      &lt;item&gt;Nektar Impact LX&lt;/item&gt;
      &lt;item&gt;Arturia Keylab 49/61/88 mk2 (MCU and Arturia Lab mode)&lt;/item&gt;
      &lt;item&gt;LAudio (Worlde) EasyControl.9&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;New MIDNAM Files&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Boss GT-8 and SE-70&lt;/item&gt;
      &lt;item&gt;Whammy DT&lt;/item&gt;
      &lt;item&gt;XLN Audio - Addictive Drums&lt;/item&gt;
      &lt;item&gt;XLN Audio - Addictive Drums (Brushes)&lt;/item&gt;
      &lt;item&gt;XLN Audio - Addictive Drums 2&lt;/item&gt;
      &lt;item&gt;XLN Audio - Addictive Drums 2 (Brushes)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Mackie Control Protocol&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for RGB color displays on the Icon Pro Audio V1-M, P1-M, and P-NANO controllers.&lt;/item&gt;
      &lt;item&gt;Latch automation is now supported.&lt;/item&gt;
      &lt;item&gt;An attempt to select a track specific subview without having a track selected now spits out a message instead of just doing nothing.&lt;/item&gt;
      &lt;item&gt;Foldbacks busses are now available again, with correct sorting.&lt;/item&gt;
      &lt;item&gt;Clear potentiometer LEDs when the potentiometer is empty.&lt;/item&gt;
      &lt;item&gt;Correctly disassociate potentiometers when switching modes.&lt;/item&gt;
      &lt;item&gt;Fix banking in plugin subview.&lt;/item&gt;
      &lt;item&gt;Show warning message for empty views.&lt;/item&gt;
      &lt;item&gt;Fix "Disabled" option for Function Keys.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Other&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Properly match Launchkey interface/device names.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Importing and Exporting&lt;/head&gt;
    &lt;head rend="h3"&gt;AAF Importing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Updated to version 1.0-29.&lt;/item&gt;
      &lt;item&gt;Improved log messages.&lt;/item&gt;
      &lt;item&gt;Bugfixes for clip length beyond end of file, sessions with missing clips, and region gain issues.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Other&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fix loading v2 and v3 sessions.&lt;/item&gt;
      &lt;item&gt;Fix loading multichannel tracks and regions in the ProTools importer.&lt;/item&gt;
      &lt;item&gt;Successfully import SMF files with badly-formed time signature events.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;macOS Improvements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Faster GUI drawing &lt;p&gt;Without telling anyone, Apple have subtly changed the way their drawing APIs work for graphical applications over the last 5-10 years. The result has been that a naive graphical app would end up redrawing its entire window even if only a few pixels needed updating. Weâre far from the only application to be affected by this. In Ardour 9.0 the GUI drawing speed will be significantly faster, at least on very dense pages like the mixer.&lt;/p&gt;&lt;p&gt;This is a complicated area, and there are still monitor configurations where our new implementation will misbehave. You can switch back to a drawing model where macOS is in charge, which will work but can be slower.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Reduce chance of "device busy" when ejecting DMG.&lt;/item&gt;
      &lt;item&gt;Improve retina scaling.&lt;/item&gt;
      &lt;item&gt;Add new NSApplicationDelegate method to deal with warning on macOS Sonoma.&lt;/item&gt;
      &lt;item&gt;Use ArdourMono font on macOS for consistency.&lt;/item&gt;
      &lt;item&gt;use small text for ruler text on macOS.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;New Preference Settings&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Stop transport using the current grid (Transport)&lt;/item&gt;
      &lt;item rend="dd-1"&gt;If enabled, stopping the transport (playback or recording) will happen at the next grid point instead of immediately.&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Region Editing Preference (Editor)&lt;/item&gt;
      &lt;item rend="dd-2"&gt;This controls whether double-clicking to edit a region opens a new window or uses the bottom panel of the editor.&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Default location of new windows (Appearance)&lt;/item&gt;
      &lt;item rend="dd-3"&gt;This setting defines where new non-modal Ardour windows are opened by default: at the center of the screen, centered at the mouse pointer position, or centered at the parent window.&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Drag deadzone (Appearance &amp;gt; Size and Scale)&lt;/item&gt;
      &lt;item rend="dd-4"&gt;This setting provides broad control over how many pixels the pointer must move before a drag (or copy-drag) begins.&lt;/item&gt;
      &lt;item rend="dt-5"&gt;Group operations on selected tracks (General)&lt;/item&gt;
      &lt;item rend="dd-5"&gt;
        &lt;p&gt;In Ardour 8.x, we introduced the idea that selected tracks/busses form a "quick" or "implicit group", so that if you select 3 tracks and adjust the volume of one of them, the others will follow.&lt;/p&gt;
        &lt;p&gt;Some users were delighted with this, others were not. This preference item can turn this behavior on or off.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-6"&gt;VST3 Knob Mode (Plugins &amp;gt; GUI)&lt;/item&gt;
      &lt;item rend="dd-6"&gt;Enforce a knob mode for VST3 plugins: use plugin default or switch between linear (depends on vertical movement) and two variation of circular, with and without a jump to clicked position.&lt;/item&gt;
      &lt;item rend="dt-7"&gt;Automatically open instrument plugin GUI when adding a new MIDI Track/Bus (Plugins &amp;gt; GUI)&lt;/item&gt;
      &lt;item rend="dd-7"&gt;Ardour now defaults to automatically showing instrument plugin GUIs when new tracks/busses with instruments are added. If multiple tracks are added, Ardour will only show the GUI for the first one. This setting allows you to opt out of the new behavior.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Fixes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fix some odd behavior when using note splitting/joining. &lt;p&gt;This commit ends the split/join operation any time selection is cleared or notes are added to the selection (except when in the middle of a split). This prevents some rather hard to predict behavior when using note splitting.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Redraw arrangement rectangles after tempo change &lt;p&gt;Previously they remained unchanged and thus incorrectly indicating an arrangement section&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Fix MIDI Clock generator (rounding issues).&lt;/item&gt;
      &lt;item&gt;Fix crash after deleting a track/bus.&lt;/item&gt;
      &lt;item&gt;Fix crash when loading session from a running instance.&lt;/item&gt;
      &lt;item&gt;Fix export encoder race condition.&lt;/item&gt;
      &lt;item&gt;Fix crash when dragging plugins from sidebar.&lt;/item&gt;
      &lt;item&gt;Fix PT Import when track name contains illegal chars.&lt;/item&gt;
      &lt;item&gt;NetBSD build fixes.&lt;/item&gt;
      &lt;item&gt;Fix incorrect placement of message catalogs during installation.&lt;/item&gt;
      &lt;item&gt;Some potential fixes for weird note split/join behavior.&lt;/item&gt;
      &lt;item&gt;Fix crashes when moving mouse over port-matrix.&lt;/item&gt;
      &lt;item&gt;Fix race-condition when detecting xjadeo/harvid version.&lt;/item&gt;
      &lt;item&gt;Fix false read-only detection of sessions on Windows with OneDrive.&lt;/item&gt;
      &lt;item&gt;Fix many issues with precise pixel alignment of MIDI note lines and the keyboard in track headers.&lt;/item&gt;
      &lt;item&gt;Fix comment-editor editing to allow inserting text.&lt;/item&gt;
      &lt;item&gt;Fix opening links in preferences editor (e.g. VST spec).&lt;/item&gt;
      &lt;item&gt;Fix #9961, update saved port-name references when renaming ports.&lt;/item&gt;
      &lt;item&gt;Fix MIDI panic timestamp at cycle-end.&lt;/item&gt;
      &lt;item&gt;Fix timecode update after locate for demo version.&lt;/item&gt;
      &lt;item&gt;Fix a simple but rather error when computing the number of quarter notes that correspond to a number of beats in a given meter. &lt;p&gt;No effect when working in common (4) time, but significant effects for other time signatures/meters.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Fix computing the difference between two Bar|Beat|Tick (BBT) times.&lt;/item&gt;
      &lt;item&gt;Fix edge case when region start is after the source' end &lt;p&gt;This fixes an edge case where an audio-region was split near its end using music-time.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Fix jump in gain when deactivating aux-send with non-unity gain.&lt;/item&gt;
      &lt;item&gt;Fix generic UI plugin support for numeric properties.&lt;/item&gt;
      &lt;item&gt;Fix crash due to concurrent sndfile access.&lt;/item&gt;
      &lt;item&gt;Donât install test programs.&lt;/item&gt;
      &lt;item&gt;Fix various crashes with OSC support.&lt;/item&gt;
      &lt;item&gt;De-click external sends.&lt;/item&gt;
      &lt;item&gt;MIDI clips with velocity scaling now work as intended.&lt;/item&gt;
      &lt;item&gt;Loading Ardour v2 and v3 sessions works again.&lt;/item&gt;
      &lt;item&gt;Dramatic improvements to tempo map manipulation (BBT Markers in particular.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Abandoned&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Drop Frontier Tranzport control surface support. &lt;p&gt;The device has not been manufactured for 15 years, and the control surface code never worked very well anyway.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Translation Updates&lt;/head&gt;
    &lt;p&gt;Czech, French, German, Polish, Russian, Simplified Chinese, Spanish.&lt;/p&gt;
    &lt;head rend="h2"&gt;Coding &amp;amp; Build Changes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We began to use C++17 syntax and features, which caused us to drop support for macoS earlier than 10.13 (High Sierra, 2017).&lt;/item&gt;
      &lt;item&gt;We moved a lot of our C++ header file inclusion to use &lt;code&gt;#pragma once&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;PBD::Signal was reimplemented using contemporary C++ and avoiding as much realtime memory allocation as possible.&lt;/item&gt;
      &lt;item&gt;Large amounts of our use of the Boost libraries were replaced by their &lt;code&gt;std::&lt;/code&gt;equivalents.&lt;/item&gt;
      &lt;item&gt;The build-time requirements for JACK were bumped to JACK2.&lt;/item&gt;
      &lt;item&gt;The build system will now allow builds with recent msys2/mingw.&lt;/item&gt;
      &lt;item&gt;reduce use of gettimeofday in favor of truly monotonic time.&lt;/item&gt;
      &lt;item&gt;Use global FFT planner lock in GUI &lt;p&gt;This should not be needed since the GUI is single threaded, but ... we're looking at you, JUCE.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Documentation&lt;/head&gt;
    &lt;p&gt;The user manual has been updated to match all changes in v9.0.&lt;/p&gt;
    &lt;p&gt;The tutorial is now available in French and Italian thanks to Julien Taverna and Christian Galeffi.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributors&lt;/head&gt;
    &lt;p&gt;31core, agfline, Alejandro DomÃnguez, Alexandre Prokoudine, Antti-Pekka Meronen, Ayan Shafqat, Ben Loftis, Bill Smith, chmaha, chousemp3, Colin Fletcher, Daniel Appelt, David Robillard, Edgar Aichinger, edwar4rd, EZ4Stephen, Franke Burgarino, Gian Fontanilla, Hoger Dehnhardt, Houston4444, Jakob DÃ¼bel, jean-emmanuel, John Emmas, Jon Bennett, Juan Vardy, Julien Taverna, Krzysztof Gajdemski, laIK4ndPDScproJyv9ofA, luzpaz, Mads Kiilerich, Martin Vlk, Matthew Smith, Nicolas Koch, Nil Geisweiller, Nils Philippsen, npt-1707, oxfn, Paul Davis, Robin Gareus, Roger Wilco, Steffen Klein, Todd Naugle, tonilink, Yuriy Al. Shirokov, Zabooma&lt;/p&gt;
    &lt;head rend="h2"&gt;Older News&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8.12 release, along with 8.7, 8.8 and 8.9.&lt;/item&gt;
      &lt;item&gt;8.6 release.&lt;/item&gt;
      &lt;item&gt;There was no 8.5 release.&lt;/item&gt;
      &lt;item&gt;8.4 release.&lt;/item&gt;
      &lt;item&gt;There was no 8.3 release.&lt;/item&gt;
      &lt;item&gt;8.2 released.&lt;/item&gt;
      &lt;item&gt;8.0/8.1 released.&lt;/item&gt;
      &lt;item&gt;7.5 released.&lt;/item&gt;
      &lt;item&gt;7.4 released.&lt;/item&gt;
      &lt;item&gt;7.3 released.&lt;/item&gt;
      &lt;item&gt;7.2 released.&lt;/item&gt;
      &lt;item&gt;7.1 released.&lt;/item&gt;
      &lt;item&gt;7.0 released.&lt;/item&gt;
      &lt;item&gt;6.9 released.&lt;/item&gt;
      &lt;item&gt;6.8 released.&lt;/item&gt;
      &lt;item&gt;6.7 released.&lt;/item&gt;
      &lt;item&gt;6.6 released.&lt;/item&gt;
      &lt;item&gt;6.5 released.&lt;/item&gt;
      &lt;item&gt;6.4 was released but immediately superceded by 6.5.&lt;/item&gt;
      &lt;item&gt;6.3 released.&lt;/item&gt;
      &lt;item&gt;6.2 released.&lt;/item&gt;
      &lt;item&gt;There was no 6.1 release.&lt;/item&gt;
      &lt;item&gt;6.0 released.&lt;/item&gt;
      &lt;item&gt;5.12 released.&lt;/item&gt;
      &lt;item&gt;5.11 released.&lt;/item&gt;
      &lt;item&gt;5.10 released.&lt;/item&gt;
      &lt;item&gt;5.9 released.&lt;/item&gt;
      &lt;item&gt;5.8 released.&lt;/item&gt;
      &lt;item&gt;There was no 5.7 release.&lt;/item&gt;
      &lt;item&gt;5.6 released.&lt;/item&gt;
      &lt;item&gt;5.5 released.&lt;/item&gt;
      &lt;item&gt;5.4 released.&lt;/item&gt;
      &lt;item&gt;5.3 released.&lt;/item&gt;
      &lt;item&gt;5.0 released.&lt;/item&gt;
      &lt;item&gt;4.7 released.&lt;/item&gt;
      &lt;item&gt;4.6 released.&lt;/item&gt;
      &lt;item&gt;There was no 4.5 release.&lt;/item&gt;
      &lt;item&gt;4.4 released.&lt;/item&gt;
      &lt;item&gt;There was no 4.3 release.&lt;/item&gt;
      &lt;item&gt;4.2 released.&lt;/item&gt;
      &lt;item&gt;4.1 released.&lt;/item&gt;
      &lt;item&gt;4.0 released.&lt;/item&gt;
      &lt;item&gt;3.5 releases.&lt;/item&gt;
      &lt;item&gt;3.4 released.&lt;/item&gt;
      &lt;item&gt;3.3 released.&lt;/item&gt;
      &lt;item&gt;3.2 released.&lt;/item&gt;
      &lt;item&gt;3.1 released.&lt;/item&gt;
      &lt;item&gt;3.0 released.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ardour.org/whatsnew.html"/><published>2026-02-05T18:30:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46903556</id><title>Flock CEO calls Deflock a "terrorist organization" [video] (2025)</title><updated>2026-02-05T21:26:05.376231+00:00</updated><content>&lt;doc fingerprint="50559455455d1642"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2026 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=l-kZGrDz7PU"/><published>2026-02-05T19:04:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46903558</id><title>My AI Adoption Journey</title><updated>2026-02-05T21:26:05.099576+00:00</updated><content>&lt;doc fingerprint="1552cc5748018c3f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mitchell Hashimoto&lt;/head&gt;
    &lt;head rend="h1"&gt;My AI Adoption Journey&lt;/head&gt;
    &lt;head class="pt-2 pb-2 text-md font-bold"&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;My experience adopting any meaningful tool is that I've necessarily gone through three phases: (1) a period of inefficiency (2) a period of adequacy, then finally (3) a period of workflow and life-altering discovery.&lt;/p&gt;
    &lt;p&gt;In most cases, I have to force myself through phase 1 and 2 because I usually have a workflow I'm already happy and comfortable with. Adopting a tool feels like work, and I do not want to put in the effort, but I usually do in an effort to be a well-rounded person of my craft.&lt;/p&gt;
    &lt;p&gt;This is my journey of how I found value in AI tooling and what I'm trying next with it. In an ocean of overly dramatic, hyped takes, I hope this represents a more nuanced, measured approach to my views on AI and how they've changed over time.&lt;/p&gt;
    &lt;p&gt;This blog post was fully written by hand, in my own words. I hate that I have to say that but especially given the subject matter, I want to be explicit about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 1: Drop the Chatbot&lt;/head&gt;
    &lt;p&gt;Immediately cease trying to perform meaningful work via a chatbot (e.g. ChatGPT, Gemini on the web, etc.). Chatbots have real value and are a daily part of my AI workflow, but their utility in coding is highly limited because you're mostly hoping they come up with the right results based on their prior training, and correcting them involves a human (you) to tell them they're wrong repeatedly. It is inefficient.&lt;/p&gt;
    &lt;p&gt;I think everyone's first experience with AI is a chat interface. And I think everyone's first experience trying to code with AI has been asking a chat interface to write code.&lt;/p&gt;
    &lt;p&gt;While I was still a heavy AI skeptic, my first "oh wow" moment was pasting a screenshot of Zed's command palette into Gemini, asking it to reproduce it with SwiftUI, and being truly flabbergasted that it did it very well. The command palette that ships for macOS in Ghostty today is only very lightly modified from what Gemini produced for me in seconds.&lt;/p&gt;
    &lt;p&gt;But when I tried to reproduce that behavior for other tasks, I was left disappointed. In the context of brownfield projects, I found the chat interface produced poor results very often, and I found myself very frustrated copying and pasting code and command output to and from the interface. It was very obviously far less efficient than me doing the work myself.&lt;/p&gt;
    &lt;p&gt;To find value, you must use an agent. An agent is the industry-adopted term for an LLM that can chat and invoke external behavior in a loop1 At a bare minimum, the agent must have the ability to: read files, execute programs, and make HTTP requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 2: Reproduce Your Own Work&lt;/head&gt;
    &lt;p&gt;The next phase on my journey I tried Claude Code. I'll cut to the chase: I initially wasn't impressed. I just wasn't getting good results out of my sessions. I felt I had to touch up everything it produced and this process was taking more time than if I had just done it myself. I read blog posts, watched videos, but just wasn't that impressed.&lt;/p&gt;
    &lt;p&gt;Instead of giving up, I forced myself to reproduce all my manual commits with agentic ones. I literally did the work twice. I'd do the work manually, and then I'd fight an agent to produce identical results in terms of quality and function (without it being able to see my manual solution, of course).&lt;/p&gt;
    &lt;p&gt;This was excruciating, because it got in the way of simply getting things done. But I've been around the block with non-AI tools enough to know that friction is natural, and I can't come to a firm, defensible conclusion without exhausting my efforts.&lt;/p&gt;
    &lt;p&gt;But, expertise formed. I quickly discovered for myself from first principles what others were already saying, but discovering it myself resulted in a stronger fundamental understanding.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Break down sessions into separate clear, actionable tasks. Don't try to "draw the owl" in one mega session.&lt;/item&gt;
      &lt;item&gt;For vague requests, split the work into separate planning vs. execution sessions.&lt;/item&gt;
      &lt;item&gt;If you give an agent a way to verify its work, it more often than not fixes its own mistakes and prevents regressions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More generally, I also found the edges of what agents -- at the time -- were good at, what they weren't good at, and for the tasks they were good at how to achieve the results I wanted.&lt;/p&gt;
    &lt;p&gt;All of this led to significant efficiency gains, to the point where I was starting to naturally use agents in a way that I felt was no slower than doing it myself (but I still didn't feel it was any faster, since I was mostly babysitting an agent).&lt;/p&gt;
    &lt;p&gt;The negative space here is worth reiterating: part of the efficiency gains here were understanding when not to reach for an agent. Using an agent for something it'll likely fail at is obviously a big waste of time and having the knowledge to avoid that completely leads to time savings2.&lt;/p&gt;
    &lt;p&gt;At this stage, I was finding adequate value with agents that I was happy to use them in my workflow, but still didn't feel like I was seeing any net efficiency gains. I didn't care though, I was content at this point with AI as a tool.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 3: End-of-Day Agents&lt;/head&gt;
    &lt;p&gt;To try to find some efficiency, I next started up a new pattern: block out the last 30 minutes of every day to kick off one or more agents. My hypothesis was that perhaps I could gain some efficiency if the agent can make some positive progress in the times I can't work anyways. Basically: instead of trying to do more in the time I have, try to do more in the time I don't have.&lt;/p&gt;
    &lt;p&gt;Similar to the previous task, I at first found this both unsuccessful and annoying. But, I once again quickly found different categories of work that were really helpful:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deep research sessions where I'd ask agents to survey some field, such as finding all libraries in a specific language with a specific license type and producing multi-page summaries for each on their pros, cons, development activity, social sentiment, etc.&lt;/item&gt;
      &lt;item&gt;Parallel agents attempting different vague ideas I had but didn't have time to get started on. I didn't expect them to produce something I'd ever ship here, but perhaps could illuminate some unknown unknowns when I got to the task the next day.&lt;/item&gt;
      &lt;item&gt;Issue and PR triage/review. Agents are good at using &lt;code&gt;gh&lt;/code&gt;(GitHub CLI), so I manually scripted a quick way to spin up a bunch in parallel to triage issues. I would NOT allow agents to respond, I just wanted reports the next day to try to guide me towards high value or low effort tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To be clear, I did not go as far as others went to have agents running in loops all night. In most cases, agents completed their tasks in less than half an hour. But, the latter part of the working day, I'm usually tired and coming out of flow and find myself too personally inefficient, so shifting my effort to spinning up these agents I found gave me a "warm start" the next morning that got me working more quickly than I would've otherwise.&lt;/p&gt;
    &lt;p&gt;I was happy, and I was starting to feel like I was doing more than I was doing prior to AI, if only slightly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 4: Outsource the Slam Dunks&lt;/head&gt;
    &lt;p&gt;By this point, I was getting very confident about what tasks my AI was and wasn't great at. I had really high confidence with certain tasks that the AI would achieve a mostly-correct solution. So the next step on my journey was: let agents do all of that work while I worked on other tasks.&lt;/p&gt;
    &lt;p&gt;More specifically, I would start each day by taking the results of my prior night's triage agents, filter them manually to find the issues that an agent will almost certainly solve well, and then keep them going in the background (one at a time, not in parallel).&lt;/p&gt;
    &lt;p&gt;Meanwhile, I'd work on something else. I wasn't going to social media (any more than usual without AI), I wasn't watching videos, etc. I was in my own, normal, pre-AI deep thinking mode working on something I wanted to work on or had to work on.&lt;/p&gt;
    &lt;p&gt;Very important at this stage: turn off agent desktop notifications. Context switching is very expensive. In order to remain efficient, I found that it was my job as a human to be in control of when I interrupt the agent, not the other way around. Don't let the agent notify you. During natural breaks in your work, tab over and check on it, then carry on.&lt;/p&gt;
    &lt;p&gt;Importantly, I think the "work on something else" helps counteract the highly publicized Anthropic skill formation paper. Well, you're trading off: not forming skills for the tasks you're delegating to the agent while continuing to form skills naturally in the tasks you continue to work on manually.&lt;/p&gt;
    &lt;p&gt;At this point I was firmly in the "no way I can go back" territory. I felt more efficient, but even if I wasn't, the thing I liked the most was that I could now focus my coding and thinking on tasks I really loved while still adequately completing the tasks I didn't.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 5: Engineer the Harness&lt;/head&gt;
    &lt;p&gt;At risk of stating the obvious: agents are much more efficient when they produce the right result the first time, or at worst produce a result that requires minimal touch-ups. The most sure-fire way to achieve this is to give the agent fast, high quality tools to automatically tell it when it is wrong.&lt;/p&gt;
    &lt;p&gt;I don't know if there is a broad industry-accepted term for this yet, but I've grown to calling this "harness engineering." It is the idea that anytime you find an agent makes a mistake, you take the time to engineer a solution such that the agent never makes that mistake again. I don't need to invent any new terms here; if another one exists, I'll jump on the bandwagon.&lt;/p&gt;
    &lt;p&gt;This comes in two forms:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Better implicit prompting (AGENTS.md). For simple things, like the agent repeatedly running the wrong commands or finding the wrong APIs, update the&lt;/p&gt;&lt;code&gt;AGENTS.md&lt;/code&gt;(or equivalent). Here is an example from Ghostty. Each line in that file is based on a bad agent behavior, and it almost completely resolved them all.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Actual, programmed tools. For example, scripts to take screenshots, run filtered tests, etc etc. This is usually paired with an AGENTS.md change to let it know about this existing.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is where I'm at today. I'm making an earnest effort whenever I see an agent do a Bad Thing to prevent it from ever doing that bad thing again. Or, conversely, I'm making an earnest effort for agents to be able to verify they're doing a Good Thing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 6: Always Have an Agent Running&lt;/head&gt;
    &lt;p&gt;Simultaneous to step 5, I'm also operating under the goal of having an agent running at all times. If an agent isn't running, I ask myself "is there something an agent could be doing for me right now?"&lt;/p&gt;
    &lt;p&gt;I particularly like to combine this with slower, more thoughtful models like Amp's deep mode (which is basically just GPT-5.2-Codex) which can take upwards of 30+ minutes to make small changes. The flip side of that is that it does tend to produce very good results.&lt;/p&gt;
    &lt;p&gt;I'm not [yet?] running multiple agents, and currently don't really want to. I find having the one agent running is a good balance for me right now between being able to do deep, manual work I find enjoyable, and babysitting my kind of stupid and yet mysteriously productive robot friend.&lt;/p&gt;
    &lt;p&gt;The "have an agent running at all times" goal is still just a goal. I'd say right now I'm maybe effective at having a background agent running 10 to 20% of a normal working day. But, I'm actively working to improve that.&lt;/p&gt;
    &lt;p&gt;I don't want to run agents for the sake of running agents. I only want to run them when there is a task I think would be truly helpful to me. Part of the challenge of this goal is improving my own workflows and tools so that I can have a constant stream of high quality work to do that I can delegate. Which, even without AI, is important!&lt;/p&gt;
    &lt;head rend="h2"&gt;Today&lt;/head&gt;
    &lt;p&gt;And that's where I'm at today.&lt;/p&gt;
    &lt;p&gt;Through this journey, I've personally reached a point where I'm having success with modern AI tooling and I believe I'm approaching it with the proper measured view that is grounded in reality. I really don't care one way or the other if AI is here to stay3, I'm a software craftsman that just wants to build stuff for the love of the game.&lt;/p&gt;
    &lt;p&gt;The whole landscape is moving so rapidly that I'm sure I'll look back at this post very quickly and laugh at my naivete. But, as they say, if you can't be embarassed about your past self, you're probably not growing. I just hope I'll grow in the right direction!&lt;/p&gt;
    &lt;p&gt;I have no skin in the game here4, and there are of course other reasons behind utility to avoid using AI. I fully respect anyone's individual decisions regarding it. I'm not here to convince you! For those interested, I just wanted to share my personal approach to navigating these new tools and give a glimpse about how I approach new tools in general, regardless of AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Modern coding models like Opus and Codex are specifically trained to bias towards using tools compared to conversational models. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Due to the rapid pace of innovation in models, I have to constantly revisit my priors on this one. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The skill formation issues particularly in juniors without a strong grasp of fundamentals deeply worries me, however. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I don't work for, invest in, or advise any AI companies. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mitchellh.com/writing/my-ai-adoption-journey"/><published>2026-02-05T19:04:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46903616</id><title>We tasked Opus 4.6 using agent teams to build a C Compiler</title><updated>2026-02-05T21:26:01.989050+00:00</updated><content>&lt;doc fingerprint="5760beda6f939a6e"&gt;
  &lt;main&gt;
    &lt;p&gt;Written by Nicholas Carlini, a researcher on our Safeguards team. &lt;/p&gt;
    &lt;p&gt;I've been experimenting with a new approach to supervising language models that we’re calling "agent teams."&lt;/p&gt;
    &lt;p&gt;With agent teams, multiple Claude instances work in parallel on a shared codebase without active human intervention. This approach dramatically expands the scope of what's achievable with LLM agents.&lt;/p&gt;
    &lt;p&gt;To stress test it, I tasked 16 agents with writing a Rust-based C compiler, from scratch, capable of compiling the Linux kernel. Over nearly 2,000 Claude Code sessions and $20,000 in API costs, the agent team produced a 100,000-line compiler that can build Linux 6.9 on x86, ARM, and RISC-V.&lt;/p&gt;
    &lt;p&gt;The compiler is an interesting artifact on its own, but I focus here on what I learned about designing harnesses for long-running autonomous agent teams: how to write tests that keep agents on track without human oversight, how to structure work so multiple agents can make progress in parallel, and where this approach hits its ceiling.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enabling long-running Claudes&lt;/head&gt;
    &lt;p&gt;Existing agent scaffolds like Claude Code require an operator to be online and available to work jointly. If you ask for a solution to a long and complex problem, the model may solve part of it, but eventually it will stop and wait for continued input—a question, a status update, or a request for clarification.&lt;/p&gt;
    &lt;p&gt;To elicit sustained, autonomous progress, I built a harness that sticks Claude in a simple loop (if you’ve seen Ralph-loop, this should look familiar). When it finishes one task, it immediately picks up the next. (Run this in a container, not your actual machine).&lt;/p&gt;
    &lt;code&gt;#!/bin/bash

while true; do
    COMMIT=$(git rev-parse --short=6 HEAD)
    LOGFILE="agent_logs/agent_${COMMIT}.log"

    claude --dangerously-skip-permissions \
           -p "$(cat AGENT_PROMPT.md)" \
           --model claude-opus-X-Y &amp;amp;&amp;gt; "$LOGFILE"
done
&lt;/code&gt;
    &lt;p&gt;&lt;lb/&gt;In the agent prompt, I tell Claude what problem to solve and ask it to approach the problem by breaking it into small pieces, tracking what it’s working on, figuring out what to work on next, and to effectively keep going until it’s perfect. (On this last point, Claude has no choice. The loop runs forever—although in one instance, I did see Claude &lt;code&gt;pkill -9 bash&lt;/code&gt; on accident, thus killing itself and ending the loop. Whoops!).&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt;Running Claude in parallel&lt;/head&gt;
    &lt;p&gt;Running multiple instances in parallel can address two weaknesses of a single-agent harness:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One Claude Code session can only do one thing at a time. Especially as the scope of a project expands, debugging multiple issues in parallel is far more efficient.&lt;/item&gt;
      &lt;item&gt;Running multiple Claude agents allows for specialization. While a few agents are tasked to solve the actual problem at hand, other specialized agents can be invoked to (for example) maintain documentation, keep an eye on code quality, or solve specialized sub-tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My implementation of parallel Claude is bare-bones. A new bare git repo is created, and for each agent, a Docker container is spun up with the repo mounted to &lt;code&gt;/upstream&lt;/code&gt;. Each agent clones a local copy to &lt;code&gt;/workspace&lt;/code&gt;, and when it's done, pushes from its own local container to upstream.&lt;/p&gt;
    &lt;p&gt;To prevent two agents from trying to solve the same problem at the same time, the harness uses a simple synchronization algorithm:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Claude takes a "lock" on a task by writing a text file to current_tasks/ (e.g., one agent might lock current_tasks/parse_if_statement.txt, while another locks current_tasks/codegen_function_definition.txt). If two agents try to claim the same task, git's synchronization forces the second agent to pick a different one.&lt;/item&gt;
      &lt;item&gt;Claude works on the task, then pulls from upstream, merges changes from other agents, pushes its changes, and removes the lock. Merge conflicts are frequent, but Claude is smart enough to figure that out.&lt;/item&gt;
      &lt;item&gt;The infinite agent-generation-loop spawns a new Claude Code session in a fresh container, and the cycle repeats.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a very early research prototype. I haven’t yet implemented any other method for communication between agents, nor do I enforce any process for managing high-level goals. I don’t use an orchestration agent.&lt;/p&gt;
    &lt;p&gt;Instead, I leave it up to each Claude agent to decide how to act. In most cases, Claude picks up the “next most obvious” problem. When stuck on a bug, Claude will often maintain a running doc of failed approaches and remaining tasks. In the git repository of the project, you can read through the history and watch it take out locks on various tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons from programming with Claude agent teams&lt;/head&gt;
    &lt;p&gt;The scaffolding runs Claude in a loop, but that loop is only useful if Claude can tell how to make progress. Most of my effort went into designing the environment around Claude—the tests, the environment, the feedback—so that it could orient itself without me. These are the approaches I’ve found most helpful when orchestrating multiple Claude instances.&lt;/p&gt;
    &lt;head rend="h3"&gt;Write extremely high-quality tests&lt;/head&gt;
    &lt;p&gt;Claude will work autonomously to solve whatever problem I give it. So it’s important that the task verifier is nearly perfect, otherwise Claude will solve the wrong problem. Improving the testing harness required finding high-quality compiler test suites, writing verifiers and build scripts for open-source software packages, and watching for mistakes Claude was making, then designing new tests as I identified those failure modes.&lt;/p&gt;
    &lt;p&gt;For example, near the end of the project, Claude started to frequently break existing functionality each time it implemented a new feature. To address this, I built a continuous integration pipeline and implemented stricter enforcement that allowed Claude to better test its work so that new commits can’t break existing code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Put yourself in Claude’s shoes&lt;/head&gt;
    &lt;p&gt;I had to constantly remind myself that I was writing this test harness for Claude and not for myself, which meant rethinking many of my assumptions about how tests should communicate results.&lt;/p&gt;
    &lt;p&gt;For example, each agent is dropped into a fresh container with no context and will spend significant time orienting itself, especially on large projects. Before we even reach the tests, to help Claude help itself, I included instructions to maintain extensive READMEs and progress files that should be updated frequently with the current status.&lt;/p&gt;
    &lt;p&gt;I also kept in mind the fact that language models have inherent limitations, which, in this case, needed to be designed around. These include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context window pollution: The test harness should not print thousands of useless bytes. At most, it should print a few lines of output and log all important information to a file so Claude can find it when needed. Logfiles should be easy to process automatically: if there are errors, Claude should write ERROR and put the reason on the same line so grep will find it. It helps to pre-compute aggregate summary statistics so Claude doesn't have to recompute them.&lt;/item&gt;
      &lt;item&gt;Time blindness: Claude can't tell time and, left alone, will happily spend hours running tests instead of making progress. The harness prints incremental progress infrequently (to avoid polluting context) and includes a default &lt;code&gt;--fast&lt;/code&gt;option that runs a 1% or 10% random sample. This subsample is deterministic per-agent but random across VMs, so Claude still covers all files but each agent can perfectly identify regressions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Make parallelism easy&lt;/head&gt;
    &lt;p&gt;When there are many distinct failing tests, parallelization is trivial: each agent picks a different failing test to work on. After the test suite reached a 99% pass rate, each agent worked on getting a different small open-source project (e.g., SQlite, Redis, libjpeg, MQuickJS, Lua) to compile.&lt;/p&gt;
    &lt;p&gt;But when agents started to compile the Linux kernel, they got stuck. Unlike a test suite with hundreds of independent tests, compiling the Linux kernel is one giant task. Every agent would hit the same bug, fix that bug, and then overwrite each other's changes. Having 16 agents running didn't help because each was stuck solving the same task.&lt;/p&gt;
    &lt;p&gt;The fix was to use GCC as an online known-good compiler oracle to compare against. I wrote a new test harness that randomly compiled most of the kernel using GCC, and only the remaining files with Claude's C Compiler. If the kernel worked, then the problem wasn’t in Claude’s subset of the files. If it broke, then it could further refine by re-compiling some of these files with GCC. This let each agent work in parallel, fixing different bugs in different files, until Claude's compiler could eventually compile all files. (After this worked, it was still necessary to apply delta debugging techniques to find pairs of files that failed together but worked independently.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Multiple agent roles&lt;/head&gt;
    &lt;p&gt;Parallelism also enables specialization. LLM-written code frequently re-implements existing functionality, so I tasked one agent with coalescing any duplicate code it found. I put another in charge of improving the performance of the compiler itself, and a third I made responsible for outputting efficient compiled code. I asked another agent to critique the design of the project from the perspective of a Rust developer, and make structural changes to the project to improve the overall code quality, and another to work on documentation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stress testing the limits of agent teams&lt;/head&gt;
    &lt;p&gt;This project was designed as a capability benchmark. I am interested in stress-testing the limits of what LLMs can just barely achieve today in order to help us prepare for what models will reliably achieve in the future.&lt;/p&gt;
    &lt;p&gt;I’ve been using the C Compiler project as a benchmark across the entire Claude 4 model series. As I did with prior projects, I started by drafting what I wanted: a from-scratch optimizing compiler with no dependencies, GCC-compatible, able to compile the Linux kernel, and designed to support multiple backends. While I specified some aspects of the design (e.g., that it should have an SSA IR to enable multiple optimization passes) I did not go into any detail on how to do so.&lt;/p&gt;
    &lt;p&gt;Previous Opus 4 models were barely capable of producing a functional compiler. Opus 4.5 was the first to cross a threshold that allowed it to produce a functional compiler which could pass large test suites, but it was still incapable of compiling any real large projects. My goal with Opus 4.6 was to again test the limits.&lt;/p&gt;
    &lt;head rend="h3"&gt;Evaluation&lt;/head&gt;
    &lt;p&gt;Over nearly 2,000 Claude Code sessions across two weeks, Opus 4.6 consumed 2 billion input tokens and generated 140 million output tokens, a total cost just under $20,000. Compared to even the most expensive Claude Max plans, this was an extremely expensive project. But that total is a fraction of what it would cost me to produce this myself—let alone an entire team.&lt;/p&gt;
    &lt;p&gt;This was a clean-room implementation (Claude did not have internet access at any point during its development); it depends only on the Rust standard library. The 100,000-line compiler can build a bootable Linux 6.9 on x86, ARM, and RISC-V. It can also compile QEMU, FFmpeg, SQlite, postgres, redis, and has a 99% pass rate on most compiler test suites including the GCC torture test suite. It also passes the developer's ultimate litmus test: it can compile and run Doom.&lt;/p&gt;
    &lt;p&gt;The compiler, however, is not without limitations. These include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It lacks the 16-bit x86 compiler that is necessary to boot Linux out of real mode. For this, it calls out to GCC (the x86_32 and x86_64 compilers are its own).&lt;/item&gt;
      &lt;item&gt;It does not have its own assembler and linker; these are the very last bits that Claude started automating and are still somewhat buggy. The demo video was produced with a GCC assembler and linker.&lt;/item&gt;
      &lt;item&gt;The compiler successfully builds many projects, but not all. It's not yet a drop-in replacement for a real compiler.&lt;/item&gt;
      &lt;item&gt;The generated code is not very efficient. Even with all optimizations enabled, it outputs less efficient code than GCC with all optimizations disabled.&lt;/item&gt;
      &lt;item&gt;The Rust code quality is reasonable, but is nowhere near the quality of what an expert Rust programmer might produce.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The resulting compiler has nearly reached the limits of Opus’s abilities. I tried (hard!) to fix several of the above limitations but wasn’t fully successful. New features and bugfixes frequently broke existing functionality.&lt;/p&gt;
    &lt;p&gt;As one particularly challenging example, Opus was unable to implement a 16-bit x86 code generator needed to boot into 16-bit real mode. While the compiler can output correct 16-bit x86 via the 66/67 opcode prefixes, the resulting compiled output is over 60kb, far exceeding the 32k code limit enforced by Linux. Instead, Claude simply cheats here and calls out to GCC for this phase (This is only the case for x86. For ARM or RISC-V, Claude’s compiler can compile completely by itself.)&lt;/p&gt;
    &lt;p&gt;The source code for the compiler is available. Download it, read through the code, and try it on your favorite C projects. I’ve consistently found the best way to understand what language models can do is to push them to their limits, and then study where they start to break down. Over the coming days, I’ll continue having Claude push new changes if you want to follow along with Claude’s continued attempts at addressing these limitations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking forward&lt;/head&gt;
    &lt;p&gt;Each generation of language models opens up new ways of working with them. Early models were useful for tab-completion in IDEs. Before long, models could complete a function body from its docstring. The launch of Claude Code brought agents into the mainstream and enabled developers to pair-program with Claude. But each of these products operates under the assumption that a user defines a task, an LLM runs for a few seconds or minutes and returns an answer, and then the user provides a follow-up.&lt;/p&gt;
    &lt;p&gt;Agent teams show the possibility of implementing entire, complex projects autonomously. This allows us, as users of these tools, to become more ambitious with our goals.&lt;/p&gt;
    &lt;p&gt;We are still early, and fully autonomous development comes with real risks. When a human sits with Claude during development, they can ensure consistent quality and catch errors in real time. For autonomous systems, it is easy to see tests pass and assume the job is done, when this is rarely the case. I used to work in penetration testing, exploiting vulnerabilities in products produced by large companies, and the thought of programmers deploying software they’ve never personally verified is a real concern.&lt;/p&gt;
    &lt;p&gt;So, while this experiment excites me, it also leaves me feeling uneasy. Building this compiler has been some of the most fun I’ve had recently, but I did not expect this to be anywhere near possible so early in 2026. The rapid progress in both language models and the scaffolds we use to interact with them opens the door to writing an enormous amount of new code. I expect the positive applications to outweigh the negative, but we’re entering a new world which will require new strategies to navigate safely.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Special thanks to Josef Bacik, Edwin Chen, Bernardo Meurer Costa, Jake Eaton, Dan Kelley, Felix Klock, Jannet Park, Steve Weis, and many other people across Anthropic for their assistance and contributions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/engineering/building-c-compiler"/><published>2026-02-05T19:07:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46903929</id><title>The time I didn't meet Jeffrey Epstein</title><updated>2026-02-05T21:26:01.870146+00:00</updated><content>&lt;doc fingerprint="6767385f0fa437a5"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The time I didn’t meet Jeffrey Epstein&lt;/head&gt;
    &lt;p&gt;Last night, I was taken aback to discover that my name appears in the Epstein Files, in 26 different documents. This is despite the fact that I met Jeffrey Epstein a grand total of zero times, and had zero email or any other contact with him … which is more (less) than some of my colleagues can say.&lt;/p&gt;
    &lt;p&gt;The bulk of the correspondence involves Epstein wanting to arrange a meeting with me and Seth Lloyd back in 2010, via an intermediary named Charles Harper, about funding a research project on “Cryptography in Nature.”&lt;/p&gt;
    &lt;p&gt;Searching my inbox, it turns out that this Charles Harper did contact me in May 2010, and I then met him at S&amp;amp;S Deli in Cambridge (plausible, although I have zero recollections of this meeting—only of the deli). Harper then sent me a detailed followup email about his proposed Cryptography in Nature project, naming Jeffrey Epstein for the first time as the project’s funder, and adding: “perhaps you will know Jeffrey and his background and situation.”&lt;/p&gt;
    &lt;p&gt;For whatever reason, I forwarded this email to my parents, brother, and then-fiancee Dana. My brother then found and shared a news article about Epstein’s prostitution conviction, adding to a different article that I had found and shared. (At that time, like many others, I’d probably vaguely heard of Epstein, but he didn’t have 0.1% the infamy that he has now.) Then my mom wrote the following: “be careful not to get sucked up in the slime-machine going on here! Since you don’t care that much about money, they can’t buy you at least.”&lt;/p&gt;
    &lt;p&gt;It appears from emails that Charles Harper tried again later that summer to arrange a meeting between me and Epstein, but that I took my mom’s advice and largely blew him off, and no such meeting ever happened. Amazingly, I then forgot entirely that any of this had occurred until last night. By way of explanation, some business/finance dude trying to interest me in half-baked ideas involving quantum, AI, cryptography, etc., often dangling the prospect of funding for my students and postdocs, shows up in my life like every month. Most of their world-changing initiatives go nowhere for one reason or another. There really wasn’t much reason to think further about this, until Epstein had become history’s most notorious sex criminal, which (again) wouldn’t happen until years later, after I’d forgotten.&lt;/p&gt;
    &lt;p&gt;It gets better, though. In the Epstein Files, one also finds a November 2010 letter from Charles Harper to Epstein about organizing a conference on the same Cryptography in Nature topic, which includes the following idea about me:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Scott Aaronson was born on May 21st, 1981. He will be 30 in 2011. The conference could follow a theme of: “hurry to think together with Scott Aaronson while he is still in his 20s and not yet a pitiful over-the-hill geezer in his 30s.” This offers another nice opportunity for celebration.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I see no indication that any such conference ever happened; in any case, I didn’t get invited to one!&lt;/p&gt;
    &lt;p&gt;On my Facebook, some friends are joking that “it tracks that someone into teenage girls might think Scott Aaronson was a hot property in his nubile 20s, who would get old and boring in his 30s”—and that maybe Epstein was less sexist about such matters than everyone assumes. I replied that I wished I could say the proposition that I’d gradually get slower and more senile through the 2010s and 2020s was entirely false.&lt;/p&gt;
    &lt;p&gt;But the best comment was that I’ve been incredibly lucky to have such an astute family. If only Bill Gates and Larry Summers had had my mom to go to for advice, they could’ve saved themselves a lot of grief.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://scottaaronson.blog/?p=9534"/><published>2026-02-05T19:29:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46904361</id><title>LinkedIn checks for 2953 browser extensions</title><updated>2026-02-05T21:26:01.144822+00:00</updated><content>&lt;doc fingerprint="ba50dea2345746f0"&gt;
  &lt;main&gt;
    &lt;p&gt;LinkedIn silently probes for 2,953 Chrome extensions on every page load.&lt;/p&gt;
    &lt;p&gt;This repository documents every extension LinkedIn checks for and provides tools to identify them.&lt;/p&gt;
    &lt;p&gt;The complete list of extensions with names and Chrome Web Store links:&lt;/p&gt;
    &lt;p&gt;chrome_extensions_with_names_all.csv&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Column&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Extension ID&lt;/cell&gt;
        &lt;cell&gt;32-character Chrome extension identifier&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Name&lt;/cell&gt;
        &lt;cell&gt;Extension name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;URL&lt;/cell&gt;
        &lt;cell&gt;Link to Chrome Web Store or Extpose&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Fetches extension names from Chrome Web Store with Extpose fallback for removed/unavailable extensions.&lt;/p&gt;
    &lt;code&gt;# Fetch all extensions
node fetch_extension_names.js

# Fetch a subset (useful if rate limited)
node fetch_extension_names.js --offset 0 --limit 500
node fetch_extension_names.js -o 500 -l 500

# Show help
node fetch_extension_names.js --help&lt;/code&gt;
    &lt;p&gt;Test script that processes the first 3 extensions with verbose output.&lt;/p&gt;
    &lt;code&gt;node test_fetch.js&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2,953 total extensions in LinkedIn's fingerprint list&lt;/item&gt;
      &lt;item&gt;~78% found on Chrome Web Store&lt;/item&gt;
      &lt;item&gt;~22% found via Extpose fallback (removed or unavailable on Chrome Web Store)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;chrome_extension_ids.txt&lt;/code&gt;- Raw list of extension IDs extracted from LinkedIn's fingerprint.js&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fingerprint.js&lt;/code&gt;- LinkedIn's page script with the extensions (minified)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/mdp/linkedin-extension-fingerprinting"/><published>2026-02-05T20:00:39+00:00</published></entry></feed>