<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-18T00:59:24.484900+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46648714</id><title>Zep AI (Agent Context Engineering, YC W24) Is Hiring Forward Deployed Engineers</title><updated>2026-01-18T00:59:30.707731+00:00</updated><content>&lt;doc fingerprint="af57259bffd2ec7b"&gt;
  &lt;main&gt;
    &lt;p&gt;Agent Context Is Hard. We Fixed It.&lt;/p&gt;
    &lt;p&gt;Zep assembles the right context from chat history, business data, and user behavior so agents are personalized, accurate, and fast. Our open source project Graphiti hit 20k GitHub stars in under 12 months. Sub-200ms retrieval, SOC 2 Type 2/HIPAA certified, used by teams from startups to Fortune 500s.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/zep-ai/jobs/"/><published>2026-01-16T17:00:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46648916</id><title>East Germany balloon escape</title><updated>2026-01-18T00:59:30.563205+00:00</updated><content>&lt;doc fingerprint="be46c17b47664bf8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;East Germany balloon escape&lt;/head&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Native name&lt;/cell&gt;&lt;cell&gt;Die Ballonflucht&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Date&lt;/cell&gt;&lt;cell&gt;16 September 1979&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Time&lt;/cell&gt;&lt;cell&gt;02:00 am (approximate)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Duration&lt;/cell&gt;&lt;cell&gt;25 minutes&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Location&lt;/cell&gt;&lt;cell&gt;Oberlemnitz, East Germany&lt;p&gt;(takeoff)&lt;/p&gt;&lt;p&gt;Naila, West Germany&lt;/p&gt;&lt;p&gt;(landing)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Coordinates&lt;/cell&gt;&lt;cell&gt;50°28′59″N 11°35′29″E / 50.48306°N 11.59139°E[1]&lt;p&gt;(takeoff)&lt;/p&gt;&lt;p&gt;50°19′52.7″N 11°40′13.1″E / 50.331306°N 11.670306°E[1]&lt;/p&gt;&lt;p&gt;(landing)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Organised by&lt;/cell&gt;&lt;cell&gt;Peter Strelzyk &amp;amp; family&lt;p&gt;Günter Wetzel &amp;amp; family&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Participants&lt;/cell&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Outcome&lt;/cell&gt;&lt;cell&gt;Successful escape to West Germany&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Non-fatal injuries&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;On 16 September 1979, eight people from two families escaped from East Germany by crossing the border into West Germany at night in a homemade hot air balloon. The unique feat was the result of over a year and a half of preparations involving three different balloons, various modifications, and a first, unsuccessful attempt. The failed attempt alerted the East German authorities to the plot, but the police were unable to identify the escapees before their second, successful flight two months later.&lt;/p&gt;&lt;head rend="h2"&gt;Background&lt;/head&gt;[edit]&lt;p&gt;East Germany, then part of the Eastern Bloc, was separated from West Germany in the Western Bloc by the inner German border and the Berlin Wall, which were heavily fortified with watchtowers, land mines, armed soldiers, and various other measures to prevent illegal crossings. East German border troops were instructed to prevent defection to West Germany by all means, including lethal force (Schießbefehl; "order to fire").[2]&lt;/p&gt;&lt;p&gt;Peter Strelzyk (1942–2017), an electrician and former East German Air Force mechanic, and Günter Wetzel (born 1955), a bricklayer by trade,[3] were colleagues at a local plastics factory.[4] Friends for four years, they shared a desire to flee the country and began discussing ways to get across the border. On 7 March 1978, they agreed to plan an escape.[5] They considered building a helicopter but quickly realized they would be unable to acquire an engine capable of powering such a craft. They then decided to explore the idea of constructing a hot air balloon,[6] having been inspired by a television program about ballooning.[3] An alternate account is that a relative shared a magazine article about the International Balloon Festival in Albuquerque, New Mexico.[5]&lt;/p&gt;&lt;head rend="h2"&gt;Construction&lt;/head&gt;[edit]&lt;p&gt;Strelzyk and Wetzel began research into balloons. Their plan was to escape with their wives and a total of four children (aged 2 to 15). They calculated the weight of the eight passengers and the craft itself to be around 750 kilograms (1,650 lb). Subsequent calculations determined a balloon capable of lifting this weight would need to hold 2,000 cubic metres (71,000 cu ft) of air heated to 100 °C (212 °F). The next calculation was the amount of material needed for the balloon, estimated to be 800 square metres (8,600 sq ft).[6]&lt;/p&gt;&lt;p&gt;The pair lived in Pößneck, a small town of about 20,000 where large quantities of cloth could not be obtained without raising attention. They tried neighbouring towns of Rudolstadt, Saalfeld, and Jena without success.[7] They travelled 50 km (31 mi) to Gera, where they purchased 1-metre-wide (3 ft 3 in) rolls of cotton cloth totalling 850 metres (2,790 ft) in length at a department store after telling the astonished clerk that they needed the large quantity of material to use as tent lining for their camping club.[6][7]&lt;/p&gt;&lt;p&gt;Wetzel spent two weeks sewing the cloth into a balloon-shaped bag, 15 metres (49 ft) wide by 20 metres (66 ft) long, on a 40-year-old manually operated sewing machine. Strelzyk spent the time building the gondola and burner assembly. The gondola was made from an iron frame, sheet metal floor, and clothesline run around the perimeter every 150 millimetres (5.9 in) for the sides. The burner was made using two 11-kilogram (24 lb) bottles of liquid propane household gas, hoses, water pipe, a nozzle, and a piece of stove pipe.[6]&lt;/p&gt;&lt;head rend="h2"&gt;First test&lt;/head&gt;[edit]&lt;p&gt;The team was ready to test the craft in April 1978. After days of searching, they found a suitable secluded forest clearing near Ziegenrück, 10 km (6.2 mi) from the border and 30 km (19 mi) from Pößneck. After lighting the burner one night, they failed to inflate the balloon. They thought the problem might stem from the fact that they had laid the balloon on the ground. After weeks of additional searching, they found a 25-metre (82 ft) cliff at a rock quarry where they could suspend the balloon vertically before inflation, but that also proved unsuccessful.[6]&lt;/p&gt;&lt;p&gt;The pair then decided to fill the bag with ambient-temperature air before using the burner to raise the air temperature and provide lift. They constructed a blower with a 14 hp (10 kW) 250 cc (15 cu in) motorcycle engine taken from Wetzel's old MZ, started with a Trabant automobile starter powered by jumper cables from Strelzyk's Moskvitch sedan.[8] This engine, silenced by a Trabant muffler, turned 1-metre-long (3.3 ft) fan blades to inflate the balloon. They also used a home-made flamethrower, similar to the gondola's burner, to pre-heat the air faster. With these modifications in place, they returned to the secluded clearing to try again but still could not inflate the balloon. But using the blower did allow them to discover that the cotton material with which they fashioned the balloon was too porous and leaked excessively.[6]&lt;/p&gt;&lt;p&gt;Their unsuccessful effort had cost them 2,400 DDM (US$360). Strelzyk disposed of the cloth by burning it in his furnace over several weeks.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Second test&lt;/head&gt;[edit]&lt;p&gt;Strelzyk and Wetzel purchased samples of different fabrics in local stores, including umbrella material and various samples of taffeta and nylon. They used an oven to test the material for heat resistance. In addition, they created a test rig from a vacuum cleaner and a water-filled glass tube to determine which material would allow the vacuum to exert the most suction on the water, and consequently which was the most impervious to air. The umbrella covering performed the best but was also the most expensive. They instead selected a synthetic kind of taffeta.[6]&lt;/p&gt;&lt;p&gt;To purchase a large quantity of fabric without arousing too much suspicion, the pair again drove to a distant city. This time they travelled over 160 kilometres (100 mi) to a department store in Leipzig. Their new cover story was that they belonged to a sailing club and needed the material to make sails. The quantity they needed had to be ordered, and although they feared the purchase might be reported to East Germany's State Security Service (Stasi), they returned the next day and picked up the material without incident. They paid 4,800 DDM (US$720) for 800 metres (2,600 ft) of 1-metre-wide (3 ft 3 in) fabric.[6] On the way home, they also purchased an electric motor to speed up the pedal-operated sewing machine they had been using to sew the material into the desired balloon shape.[7]&lt;/p&gt;&lt;p&gt;Wetzel spent the next week sewing the material into another balloon, accomplishing the task faster the second time with the now-electric sewing machine. Soon afterwards, the two men returned to the forest clearing and inflated the bag in about five minutes using the blower and flame thrower. The bag rose and held air, but the burner on the gondola was not powerful enough to create the heat needed for lift. The pair continued experimenting for months, doubling the number of propane tanks and trying different fuel mixtures. Disappointed with the result, Wetzel decided to abandon the project and instead started to pursue the idea of building a small gasoline engine-powered light aeroplane[6] or a glider.[5]&lt;/p&gt;&lt;p&gt;Strelzyk continued trying to improve the burner. In June 1979, he discovered that with the propane tank inverted, additional pressure caused the liquid propane to evaporate, which produced a bigger flame. He modified the gondola to mount the propane tanks upside down, and returned to the test site where he found the new configuration produced a 12-metre (39 ft) long flame. Strelzyk was ready to attempt an escape.[6]&lt;/p&gt;&lt;head rend="h2"&gt;First escape attempt&lt;/head&gt;[edit]&lt;p&gt;On 3 July 1979, the weather and wind conditions were favourable. The entire Strelzyk family lifted from a forest clearing at 1:30 am and climbed at a rate of 4 metres (13 ft) per second. They reached an altitude of 2,000 metres (6,600 ft) according to an altimeter Strelzyk had made by modifying a barometer. A light wind was blowing them towards the border. The balloon then entered clouds, and atmospheric water vapour condensed on the balloon, adding weight which caused it to descend prematurely. The family landed safely approximately 180 metres (590 ft) short of the border, at the edge of the heavily mined border zone. Unsure of where they were, Strelzyk explored until he found a piece of litter – a bread bag from a bakery in Wernigerode, an East German town. The group spent nine hours carefully extricating themselves from the 500-metre (1,600 ft) wide border zone to avoid detection. They also had to travel unnoticed through a 5 km (3.1 mi) restricted zone before hiking back a total of 14 km (8.7 mi) to their car and the launch paraphernalia they had left behind.[6] They made it home just in time to report their absence from work and school was due to sickness.[7]&lt;/p&gt;&lt;p&gt;The abandoned balloon was discovered by the authorities later that morning. Strelzyk destroyed all compromising evidence and sold his car, fearing that it could link him to the escape attempt.[6] On 14 August, the Stasi launched an appeal to find the "perpetrator of a serious offence", listing in detail all the items recovered at the landing site.[9] Strelzyk felt that the Stasi would eventually trace the balloon to him and the Wetzels. He agreed with Wetzel that their best chance was to quickly build another balloon and get out as soon as possible.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Successful escape&lt;/head&gt;[edit]&lt;p&gt;Strelzyk and Wetzel decided to double the balloon's size to 4,000 cubic metres (140,000 cu ft) in volume, 20 metres (66 ft) in diameter, and 25 metres (82 ft) in height. They needed 1,250 square metres (13,500 sq ft) of taffeta, and purchased the material, in various colours and patterns, all over the country in order to escape suspicion. Wetzel sewed a third balloon, using over 6 kilometres (3.7 mi) of thread, and Strelzyk rebuilt everything else as before. In six weeks, they had prepared the 180-kilogram (400 lb) balloon and a payload of 550 kilograms (1,210 lb), including the gondola, equipment, and cargo (the two families). Confident in their calculations, they found the weather conditions right on 15 September, when a violent thunderstorm created the correct winds. The two families set off for the launch site in Strelzyk's replacement car (a Wartburg) and a moped. Arriving at 1:30 am, they needed just ten minutes to inflate the balloon and an additional three minutes to heat the air.[6]&lt;/p&gt;&lt;p&gt;Lifting off just after 2:00 am, the group failed to cut the tethers holding the gondola to the ground at the same time, tilting the balloon and sending the flame towards the fabric, which caught fire. After putting out the fire with an extinguisher brought along for just such an emergency, they climbed to 2,000 metres (6,600 ft) in nine minutes, drifting towards West Germany at 30 kilometres per hour (19 mph). The balloon flew for 28 minutes, with the temperature plummeting to −8 °C (18 °F) in the unsheltered gondola, which consisted solely of clothesline railing.&lt;/p&gt;&lt;p&gt;A design miscalculation resulted in the burner stovepipe being too long, causing the flame to be too high in the balloon, creating excessive pressure which caused the balloon to split. The air rushing out of the split extinguished the burner flame. Wetzel was able to re-light the flame with a match, and had to do so several more times before the group landed. At one point, they increased the flame to the maximum possible extent and rose to 2,500 metres (8,200 ft). They later learned they had been high enough to be detected, but not identified, on radar by West German air traffic controllers.[6] They had also been detected on the East German side by a night watchman at the district culture house in Bad Lobenstein. The report of an unidentified flying object heading toward the border caused guards to activate search lights, but the balloon was too high and out of reach of the lights.[10]&lt;/p&gt;&lt;p&gt;The tear in the balloon meant the group had to use the burner much more often, greatly limiting the distance it could travel. Wetzel later said he thought they could have travelled another 50 kilometres (31 mi) had the balloon remained intact. They made out the border crossing at Rudolphstein on the A9 and saw the search lights. When the propane ran out, they descended quickly, landing near the town of Naila, in the West German state of Bavaria and only 10 km (6 mi) from the border. The only injury was suffered by Wetzel, who broke his leg upon landing.[6] Various clues indicated to the families that the balloon had made it across the border. These included spotting red and yellow coloured lights, not common in East Germany,[3] and small farms, in contrast to the large state-run operations in the east. Another clue was modern farm equipment, unlike the older equipment used in East Germany.[11] Two Bavarian State Police officers saw the balloon's flickering light and headed to where they thought it would land. There they found Strelzyk and Wetzel, who first asked if they had made it to the West, although they noticed the police car was an Audi – another sign they were in West Germany. Upon learning they had, the escapees happily called for their families to join them.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Aftermath&lt;/head&gt;[edit]&lt;p&gt;East Germany immediately increased border security, closed all small airports close to the border, and ordered the planes kept farther inland.[6] Propane gas tanks became registered products, and large quantities of fabric suitable for balloon construction could no longer be purchased. Mail from East Germany to the two escaped families was prohibited.[12]&lt;/p&gt;&lt;p&gt;Erich Strelzyk learned of his brother's escape on the ZDF news and was arrested in his Potsdam apartment three hours after the landing. The arrest of family members was standard procedure to deter others from attempting escape. He was charged with "aiding and abetting escape", as were Strelzyk's sister Maria and her husband, who were sentenced to 2½ years. The three were eventually released with the help of Amnesty International.[12]&lt;/p&gt;&lt;p&gt;The families decided to initially settle in Naila where they had landed. Wetzel worked as an automobile mechanic and Strelzyk opened a TV repair shop in Bad Kissingen. Due to pressure from Stasi spies, the Strelzyks moved to Switzerland in 1985.[10] After German reunification in 1990, they returned to their old home in their hometown of Pößneck.[13] The Wetzels remained in Bavaria.[7]&lt;/p&gt;&lt;p&gt;West German weekly magazine Stern paid Strelzyk and Wetzel for exclusive rights to the story.[3]&lt;/p&gt;&lt;p&gt;The escape has been portrayed in two films: Night Crossing (1982) and Balloon (2018). The former, also called With the Wind to the West – the English translation of the German title – was an English-language film produced by Disney. The latter was a German-language production which "both families welcomed [Director] Herbig’s desire to, as he put it, 'make a German film for an international audience.'" The Strelzyks were reportedly "moved to tears" at the screening of Balloon at Rockefeller Center in New York City.[12] Herbig claimed in 2018 that both the Strelzyk and Wetzel families had been dissatisfied with the Disney film.[14]&lt;/p&gt;&lt;p&gt;Peter Strelzyk died in 2017 at age 74 after a long illness.[13]&lt;/p&gt;&lt;p&gt;In 2017, the balloon was put on permanent display at the Haus der Bayerischen Geschichte: Museum in Regensburg.[10]&lt;/p&gt;&lt;head rend="h2"&gt;Escapees&lt;/head&gt;[edit]&lt;p&gt;The family members included:[3]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Peter Strelzyk, aged 37&lt;/item&gt;&lt;item&gt;Doris Strelzyk&lt;/item&gt;&lt;item&gt;Frank Strelzyk, aged 15&lt;/item&gt;&lt;item&gt;Andreas Strelzyk, aged 11&lt;/item&gt;&lt;item&gt;Günter Wetzel, aged 24&lt;/item&gt;&lt;item&gt;Petra Wetzel&lt;/item&gt;&lt;item&gt;Peter Wetzel, aged 5&lt;/item&gt;&lt;item&gt;Andreas Wetzel, aged 2&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Media&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;The Disney film Night Crossing (1982) is an adaptation of the story[13]&lt;/item&gt;&lt;item&gt;Michael Herbig's film Balloon (2018) is a German-language adaptation of the story[15]&lt;/item&gt;&lt;item&gt;BBC program Outlook, "Fleeing Communism in a Hot Air Balloon"[16]&lt;/item&gt;&lt;item&gt;PBS Nova program, "History's Great Escapes" (2004)[17]&lt;/item&gt;&lt;item&gt;Doris Strelzyk, Peter Strelzyk, Gudrun Giese: Destiny Balloon Escape. Quadriga, Berlin 1999, ISBN 3-88679-330-3&lt;/item&gt;&lt;item&gt;Jürgen Petschull, With the Wind to the West. The Adventurous Flight from Germany to Germany. Goldmann, Munich 1980, ISBN 3-442-11501-9&lt;/item&gt;&lt;item&gt;Kristen Fulton (Author), Torben Kuhlmann (Illustrator), Flight for Freedom: The Wetzel Family’s Daring Escape from East Germany. March 3, 2020, ISBN 978-1452149608&lt;/item&gt;&lt;item&gt;The Netflix series White Rabbit Project, episode 2, "Jailbreak"&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;See also&lt;/head&gt;[edit]&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ a b Wetzel, Günter. "Die Nacht der Flucht". Ballonflucht.de. Archived from the original on 19 September 2020. Retrieved 16 September 2019.&lt;/item&gt;&lt;item&gt;^ Hertle, Hans-Hermann; Nooke, Maria (2009). Die Todesopfer an der Berliner Mauer 1961–1989. Ein biographisches Handbuch. Ch. Links Verlag. ISBN 978-3-86153-517-1.&lt;/item&gt;&lt;item&gt;^ a b c d e Getler, Michael (28 September 1979). "Harrowing Flight From East Germany". The Washington Post. Archived from the original on 26 April 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ Snow, Philipp (16 September 2009). "Balloon escape from the GDR With hot air to freedom". Spiegel Online (in German). Archived from the original on 7 April 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c Simpson, Paul (2013). The Mammoth Book of Prison Breaks. Little, Brown Book Group. p. 216. ISBN 978-1-4721-0024-5. Archived from the original on 16 September 2023. Retrieved 1 April 2018.&lt;/item&gt;&lt;item&gt;^ a b c d e f g h i j k l m n o p q r s Dornberg, John (February 1980). "The Freedom Balloon". Popular Mechanics. pp. 100–103. Retrieved 22 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c d e Overbye, Stine (13 April 2017). "Fathers wanted to escape GDR in a hot air balloon". Historia (in Dutch). Archived from the original on 1 April 2018. Retrieved 30 March 2018.&lt;/item&gt;&lt;item&gt;^ Petschull, Jürgen (27 September 1979). "Das Himmelfahrtskommando" [High-flying mission] (PDF). Stern (in German). No. 40. p. 34. Archived from the original (PDF) on 12 July 2024 – via Museum Naila.&lt;/item&gt;&lt;item&gt;^ Souerbry, Rachel. "How Two Families Escaped East Germany In A Homemade Hot Air Balloon". ranker.com. Archived from the original on 1 April 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c "Wetzel und Peter Strelzyk Ballonhülle der Strelzyks". museum.bayern (in German). Archived from the original on 8 April 2019. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ "East-West: The Great Balloon Escape". Time. 1 October 1979. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c "The Balloon Escape of Peter Strelzyk". goethe-rutheneum.de (in German). Archived from the original on 11 February 2013. Retrieved 30 March 2018.&lt;/item&gt;&lt;item&gt;^ a b c "Man who fled East Germany in a homemade balloon and whose story was made into a film dies". The Express. 15 March 2017. Archived from the original on 1 April 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ Connolly, Kate (17 October 2018). "Film of daring balloon escape from East revives German identity debate". Archived from the original on 8 February 2021. Retrieved 10 May 2019.&lt;/item&gt;&lt;item&gt;^ Ballon at IMDb&lt;/item&gt;&lt;item&gt;^ "Fleeing Communism in a Hot Air Balloon". bbc. Archived from the original on 12 December 2018. Retrieved 29 March 2018.&lt;/item&gt;&lt;item&gt;^ "Great Escapes". pbs.org. Archived from the original on 16 April 2019. Retrieved 16 April 2019.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;External links&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Escape by balloon by Günter Wetzel (participant website)&lt;/item&gt;&lt;item&gt;Video of balloon on museum display&lt;/item&gt;&lt;item&gt;BBC Outlook program&lt;/item&gt;&lt;item&gt;Photograph of Güenter Wetzel, Peter and Doris Strelzyk Archived 1 April 2018 at the Wayback Machine&lt;/item&gt;&lt;item&gt;Photograph of the actual balloon, inflated in 1985 at a festival in Hof, Bavaria&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/East_Germany_balloon_escape"/><published>2026-01-16T17:16:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46655524</id><title>Counterfactual evaluation for recommendation systems</title><updated>2026-01-18T00:59:30.454231+00:00</updated><content>&lt;doc fingerprint="b472dbf60d6b017c"&gt;
  &lt;main&gt;
    &lt;p&gt;When I first started working on recommendation systems, I thought there was something weird about the way we did offline evaluation. First, we split customer interaction data into training and validation sets. Then, we train our recommenders on the training set before evaluating them on the validation set, usually on metrics such as recall, precision, and NDCG. This is similar to how we evaluate supervised machine learning models and doesn’t seem unusual at first glance.&lt;/p&gt;
    &lt;p&gt;But don’t our recommendations change how customers click or purchase? If customers can only interact with items shown to them, why do we perform offline evaluation on static historical data?&lt;/p&gt;
    &lt;p&gt;It took me a while to put a finger on it but I think this is why it felt weird: We’re treating recommendations as an observational problem when it really is an interventional problem.&lt;/p&gt;
    &lt;p&gt;Problems solved via supervised machine learning are usually observational problems. Given an observation such as product title, description, and image, we try to predict the product category. Our model learns P(category=phone|title=“…”, description=“…”, image=image01.jpeg).&lt;/p&gt;
    &lt;p&gt;On the other hand, recommendations are an interventional problem. We want to learn how different interventions (i.e., item recommendations) lead to different outcomes (i.e., clicks, purchases). By using logged customer interaction data as labels, the observational offline evaluation approach ignores the interventional nature of recommendations.&lt;/p&gt;
    &lt;p&gt;As a result, we’re not evaluating if users would click or purchase more due to our new recommendations; we’re evaluating how well the new recommendations fit logged data. Thus, what our model learns is P(view3=iphone|view1=pixel, view2=galaxy) when what we really want is P(click=True|recommend=iphone, view1=pixel, view2=galaxy).&lt;/p&gt;
    &lt;p&gt;The straightforward way to evaluate recommendations as an interventional problem is via A/B testing. Our interventions (i.e., new recommendations) are shown to users, we log their behavior attributed to our new recommendations, and then measure how metrics such as click-thru-rate and conversion change. However, it requires more effort relative to offline evaluation, experiment cycles may be long as we need enough data to make a judgement, and there’s the risk of deploying terrible experiments. Also, we may not have easy access to A/B testing we’re working on the research side of things.&lt;/p&gt;
    &lt;p&gt;The less direct approach is counterfactual evaluation. Counterfactual evaluation tries to answer “what would have happened if we show users our new recommendations instead of the existing recommendations?” This allows us to estimate the outcomes of potential A/B tests without actually running them.&lt;/p&gt;
    &lt;p&gt;The most widely known technique for counterfactual evaluation is Inverse Propensity Scoring (IPS). It’s sometimes also referred to as inverse probability weighting/sampling. The intuition behind it is that we can estimate how customer interactions will change—by reweighting how often each interaction will occur—based on how much more (or less) each item is shown by our new recommendation model. Here’s the IPS equation.&lt;/p&gt;
    &lt;p&gt;Let’s try to understand it by starting from the right. In section 1, &lt;code&gt;r&lt;/code&gt; represents the reward for an observation. This is the number of clicks or purchases or whatever metric is important to you in the logged data.&lt;/p&gt;
    &lt;p&gt;Next is the importance weight. The denominator (section 2a) represents our existing production recommender’s (&lt;code&gt;π0&lt;/code&gt;) probability of making a recommendation (aka action &lt;code&gt;a&lt;/code&gt;) given the context &lt;code&gt;x&lt;/code&gt;; the numerator (section 2b) represents the same probability but for our new recommender (&lt;code&gt;πe&lt;/code&gt;). (&lt;code&gt;π&lt;/code&gt; stands for recommendation policy.) For a user-to-item recommender, &lt;code&gt;x&lt;/code&gt; is the user; for an item-to-item recommender, &lt;code&gt;x&lt;/code&gt; is an item.&lt;/p&gt;
    &lt;p&gt;With the importance weight, we can compute how often a recommendation is made via the new model relative to the existing model. We can then use the ratio to update our logged rewards. For example, we have an old model (&lt;code&gt;π0&lt;/code&gt;) and new model (&lt;code&gt;πe&lt;/code&gt;) that recommend iPhone on the Pixel detail page, but with different probabilities:&lt;/p&gt;
    &lt;p&gt;In this scenario, the new model will recommend iPhone 0.6/0.4 = 1.5x as often as the old model. Thus, assuming a non-zero reward (i.e., the user clicked or purchased), we can reweight the logged reward to be worth 1.5x as much.&lt;/p&gt;
    &lt;p&gt;Finally, we average over our data (section 3) to get the IPS estimate (section 4) for our new recommender. This IPS estimate suggests how much reward (i.e., clicks, purchases) the new recommender would get relative to the production recommender if the new recommender was shown to users.&lt;/p&gt;
    &lt;p&gt;But how do we get the probability of making a recommendation (&lt;code&gt;a&lt;/code&gt;) given the context (&lt;code&gt;x&lt;/code&gt;)? Well, we can normalize the raw scores for each recommendation (via Plackett-Luce) to get each recommendation’s probability. Alternatively, if our recommendations are pre-computed, we can count the frequency of each recommendation in our recommendation store. My preferred approach is to use the impression count for each recommendation—I believe this is the most direct measure of the probability of making a recommendation and best adjusts for the presentation bias.&lt;/p&gt;
    &lt;p&gt;This dependence on recommendation probabilities or impressions likely explains why counterfactual evaluation isn’t more widely adopted in academic papers—most public datasets don’t include them. One exception is the Open Bandit Dataset which includes the recommendation probability (&lt;code&gt;action_prob&lt;/code&gt;) for each recommendation observation.&lt;/p&gt;
    &lt;p&gt;However, IPS has its pitfalls. One challenge is insufficient support. This happens when our new recommender being evaluated (&lt;code&gt;πe&lt;/code&gt;) makes a recommendation (&lt;code&gt;a&lt;/code&gt;) that our existing production recommender (&lt;code&gt;π0&lt;/code&gt;) didn’t make. Thus, &lt;code&gt;π0&lt;/code&gt;’s probability of &lt;code&gt;a&lt;/code&gt; is zero and we can’t compute the importance weight. We can mitigate this by deliberately showing random samples of non-recommended items on a sliver of traffic to log interactions for potential recommendations. (Spoiler: PMs might not like this.) A more palatable approach is ensure that all eligible items have a non-zero recommendation probability and then sample based on that probability. This gives all items a chance to be recommended.&lt;/p&gt;
    &lt;p&gt;IPS can also suffer from high variance when the new model (&lt;code&gt;πe&lt;/code&gt;) recommends very differently from the old model (&lt;code&gt;π0&lt;/code&gt;). Suppose &lt;code&gt;π0&lt;/code&gt; makes a recommendation (&lt;code&gt;a&lt;/code&gt;) with a probability of 0.001 and we logged a single click. If &lt;code&gt;πe&lt;/code&gt; makes the same recommendation (&lt;code&gt;a&lt;/code&gt;) with a probability of 0.1, we would reweight that single click by 100x—this is likely a severe overestimation. One solution is to ensure that the new recommenders being evaluated don’t differ too much from the production recommender, thus preventing the importance weight from exploding.&lt;/p&gt;
    &lt;p&gt;Another solution is Clipped IPS (CIPS). CIPS lets us set a maximum threshold for the importance weight. For example, if our threshold is 10, an importance weight greater than 10 is clipped to it. However, tuning the clipping parameter can be tricky.&lt;/p&gt;
    &lt;p&gt;Another approach is Self-Normalized IPS (SNIPS). SNIPS divides the IPS estimate by the importance weight. This rescaling prevents overinflated IPS estimates. Relative to CIPS, SNIPS is simpler and doesn’t require setting a parameter.&lt;/p&gt;
    &lt;p&gt;Which works better? At a recent RecSys 2021 tutorial, Yuta Saito compared various methods via experiments on synthetic data generated via Open Bandit Pipeline with 10 possible actions. He also assessed the direct method (DM) which we didn’t discuss. In a nutshell, DM trains a model to impute missing rewards. Think of it as similar to building an environment model for reinforcement learning, such as OpenAI gym or Criteo reco-gym, which we can then use to train and evaluate our RL models.&lt;/p&gt;
    &lt;p&gt;He found that IPS outperformed DM as the amount of logged data increases, and that CIPS didn’t perform much better than IPS. Overall, SNIPS performed the best (i.e., had the least error) and without the need for any parameter tuning. The tutorial goes on to discuss other estimators such as Doubly Robust (combining DM and SNIPS) as well as counterfactual learning—highly recommend checking it out.&lt;/p&gt;
    &lt;p&gt;Nonetheless, one downside of SNIPS is that it requires computing the importance weight for all observations; in IPS, we only need the importance weight for observations with non-zero reward. If we consider how most recommendations have zero reward (&amp;lt;10% CTR or conversion), SNIPS increases storage requirements of recommendation probabilities and computation of importance weights by 10x or more. That said, the authors of SNIPS found that the increase in computation is made up for via faster convergence.&lt;/p&gt;
    &lt;p&gt;Let me conclude by clarifying that I’m not suggesting for us to stop training and evaluating recsys models via the observational paradigm. Despite its limitations, it has several benefits. First, it’s an established evaluation framework with many public datasets and standard metrics. This makes it easier to compare various techniques. Second, we can collect training and evaluation data even before deploying our first recommender. Customer interaction data is generated organically when customers use our platforms. Thus, the conventional offline evaluation approach is a good place to start.&lt;/p&gt;
    &lt;p&gt;Nonetheless, if you’re keen to try a new evaluation approach, or find your offline metrics diverging from online A/B testing outcomes, consider counterfactual evaluation via SNIPS. In addition, though I’ve been discussing counterfactual evaluation in the context of recsys, it’s also applicable to other use cases where you want to simulate A/B tests offline.&lt;/p&gt;
    &lt;p&gt;Thanks to Arnab Bhadury, Vicki Boykis, and Yuta Saito for reading drafts of this.&lt;/p&gt;
    &lt;p&gt;If you found this useful, please cite this write-up as:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Yan, Ziyou. (Apr 2022). Counterfactual Evaluation for Recommendation Systems. eugeneyan.com. https://eugeneyan.com/writing/counterfactual-evaluation/.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;@article{yan2022counterfactual,
  title   = {Counterfactual Evaluation for Recommendation Systems},
  author  = {Yan, Ziyou},
  journal = {eugeneyan.com},
  year    = {2022},
  month   = {Apr},
  url     = {https://eugeneyan.com/writing/counterfactual-evaluation/}
}&lt;/code&gt;
    &lt;p&gt;Join 11,800+ readers getting updates on machine learning, RecSys, LLMs, and engineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eugeneyan.com/writing/counterfactual-evaluation/"/><published>2026-01-17T05:20:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656358</id><title>Show HN: Streaming gigabyte medical images from S3 without downloading them</title><updated>2026-01-18T00:59:30.053664+00:00</updated><content>&lt;doc fingerprint="286db0183daf5dc5"&gt;
  &lt;main&gt;
    &lt;p&gt;A modern, cloud-native tile server for Whole Slide Images. One command to start serving tiles directly from S3.&lt;/p&gt;
    &lt;code&gt;# Installation (requires Rust, see alternatives below)
cargo install wsi-streamer

# On your local machine
wsi-streamer s3://my-slides-bucket --s3-region eu-west-3&lt;/code&gt;
    &lt;p&gt;That's it. No configuration files, no local storage, no complex setup. Open &lt;code&gt;http://localhost:3000/view/sample.svs&lt;/code&gt; in your browser to view a slide.&lt;/p&gt;
    &lt;p&gt;Whole Slide Images are large (1-3GB+) and typically live in object storage. Traditional viewers require downloading entire files before serving a single tile. WSIStreamer takes a different approach: it understands slide formats natively, fetches only the bytes needed via HTTP range requests, and returns JPEG tiles immediately.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Range-based streaming — fetches only the bytes needed for each tile, no local files&lt;/item&gt;
      &lt;item&gt;Built-in viewer — OpenSeadragon-based web viewer with pan, zoom, and dark theme&lt;/item&gt;
      &lt;item&gt;Native format support — Rust parsers for Aperio SVS and pyramidal TIFF&lt;/item&gt;
      &lt;item&gt;Production-ready — HMAC-SHA256 signed URL authentication&lt;/item&gt;
      &lt;item&gt;Multi-level caching — slides, blocks, and encoded tiles&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install from crates.io:&lt;/p&gt;
    &lt;code&gt;cargo install wsi-streamer&lt;/code&gt;
    &lt;p&gt;Or build from source:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/PABannier/WSIStreamer.git
cd WSIStreamer
cargo build --release&lt;/code&gt;
    &lt;p&gt;Or run with Docker:&lt;/p&gt;
    &lt;code&gt;# Pull from GitHub Container Registry
docker run -p 3000:3000 -e WSI_S3_BUCKET=my-bucket ghcr.io/pabannier/wsistreamer:latest

# Or use Docker Compose for local development with MinIO
docker compose up --build&lt;/code&gt;
    &lt;code&gt;# Serve slides from S3
wsi-streamer s3://my-slides

# Custom port
wsi-streamer s3://my-slides --port 8080

# S3-compatible storage (MinIO, etc.)
wsi-streamer s3://slides --s3-endpoint http://localhost:9000&lt;/code&gt;
    &lt;code&gt;# List slides
curl http://localhost:3000/slides

# Get slide metadata
curl http://localhost:3000/slides/sample.svs

# Fetch a tile (level 0, position 0,0)
curl http://localhost:3000/tiles/sample.svs/0/0/0.jpg -o tile.jpg

# Get thumbnail
curl "http://localhost:3000/slides/sample.svs/thumbnail?max_size=256" -o thumb.jpg&lt;/code&gt;
    &lt;code&gt;# Enable HMAC-SHA256 authentication
wsi-streamer s3://my-slides --auth-enabled --auth-secret "$SECRET"

# Generate signed URLs
wsi-streamer sign --path /tiles/slide.svs/0/0/0.jpg --secret "$SECRET" --base-url http://localhost:3000&lt;/code&gt;
    &lt;p&gt;The web viewer handles authentication automatically when enabled.&lt;/p&gt;
    &lt;code&gt;# Check S3 connectivity
wsi-streamer check s3://my-slides

# List available slides
wsi-streamer check s3://my-slides --list-slides

# Test a specific slide
wsi-streamer check s3://my-slides --test-slide sample.svs&lt;/code&gt;
    &lt;p&gt;All options can be set via CLI flags or environment variables:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Env Var&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--host&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_HOST&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;0.0.0.0&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Bind address&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--port&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_PORT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;3000&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;HTTP port&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--s3-bucket&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_S3_BUCKET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;S3 bucket name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--s3-endpoint&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_S3_ENDPOINT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;Custom S3 endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--s3-region&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_S3_REGION&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;us-east-1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;AWS region&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--auth-enabled&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_AUTH_ENABLED&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable authentication&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--auth-secret&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_AUTH_SECRET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;HMAC secret key&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--cache-slides&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_CACHE_SLIDES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;100&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Max slides in cache&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--cache-tiles&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_CACHE_TILES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;100MB&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Tile cache size&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--jpeg-quality&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_JPEG_QUALITY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;80&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JPEG quality (1-100)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--cors-origins&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_CORS_ORIGINS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;any&lt;/cell&gt;
        &lt;cell&gt;Allowed CORS origins&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Run &lt;code&gt;wsi-streamer --help&lt;/code&gt; for full details.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Endpoint&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /health&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Health check&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /view/{slide_id}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Web viewer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /tiles/{slide_id}/{level}/{x}/{y}.jpg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fetch tile&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List slides&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides/{slide_id}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Slide metadata&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides/{slide_id}/thumbnail&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Thumbnail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides/{slide_id}/dzi&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;DZI descriptor&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;See API_SPECIFICATIONS.md for complete documentation.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Extensions&lt;/cell&gt;
        &lt;cell role="head"&gt;Compression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Aperio SVS&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;.svs&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JPEG, JPEG 2000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pyramidal TIFF&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;.tif&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;JPEG, JPEG 2000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Files must be tiled (not stripped) and pyramidal.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;January 17th, 2026: front page of Hacker News and Rust subreddit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT. See LICENSE.&lt;/p&gt;
    &lt;p&gt;Issues and pull requests welcome. See CONTRIBUTING.md.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/PABannier/WSIStreamer"/><published>2026-01-17T08:46:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656552</id><title>ClickHouse acquires Langfuse</title><updated>2026-01-18T00:59:29.968318+00:00</updated><content>&lt;doc fingerprint="2bee90517ddf277e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Langfuse joins ClickHouse&lt;/head&gt;
    &lt;p&gt;Our goal continues to be building the best LLM engineering platform&lt;/p&gt;
    &lt;p&gt;ClickHouse has acquired Langfuse.&lt;/p&gt;
    &lt;p&gt;If you’re reading this as a Langfuse user, your first question is probably: What does this mean for me?&lt;/p&gt;
    &lt;p&gt;Our roadmap stays the same, our goal continues to be building the best LLM engineering platform, and we remain committed to open source and self-hosting. There are no immediate changes to how you use Langfuse and how you can reach out to us.&lt;/p&gt;
    &lt;p&gt;What does change is our ability to move faster. With ClickHouse behind us, we can invest more deeply into performance, reliability, and our roadmap that helps teams build and improve AI applications in production.&lt;/p&gt;
    &lt;head rend="h2"&gt;What stays the same&lt;/head&gt;
    &lt;p&gt;This is the section we would want to read first, too.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Langfuse stays open source and self‑hostable. There are no planned changes to licensing. As you know, we leaned heavily into OSS over the last years.&lt;/item&gt;
      &lt;item&gt;Langfuse Cloud keeps running as‑is. Same product, same endpoints, same experience.&lt;/item&gt;
      &lt;item&gt;Support stays the same. Same channels, same SLAs for existing customers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What gets better now&lt;/head&gt;
    &lt;p&gt;Joining Clickhouse compresses years of operational learning into immediate, real customer benefits.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More engineering leverage on the hardest parts. Langfuse is a data‑intensive product. Working closely with the ClickHouse engineering team helps us push performance and reliability.&lt;/item&gt;
      &lt;item&gt;Faster progress on enhanced enterprise-grade compliance and security, with the help of Clickhouse’s resources.&lt;/item&gt;
      &lt;item&gt;Learning from Clickhouse’s customer success and support playbook. This puts us years ahead and allows us to spend more time on what we really care about: our users.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;A quick look back&lt;/head&gt;
    &lt;p&gt;The longer version of how we got here is in our handbook.&lt;/p&gt;
    &lt;p&gt;Langfuse started the same way many LLM products start: we were building agents ourselves. And we constantly ran into the same problems.&lt;/p&gt;
    &lt;p&gt;Building LLM apps is easy to demo and hard to run in production. Debugging is different, quality is non‑deterministic, and the iteration loop is messy. When we did Y Combinator in early 2023, we saw this every week, both in our own projects and in what other founders in our cohort were working on.&lt;/p&gt;
    &lt;p&gt;So we built a duct tape version of what we wished existed: tracing and evaluation primitives that are easy to add, easy to self‑host, and actually useful for iterating.&lt;/p&gt;
    &lt;p&gt;The very first version was intentionally simple. It ran on Postgres, because speed of shipping mattered more than theoretical scaling. That got us to a real product and a real community fast.&lt;/p&gt;
    &lt;p&gt;Then people actually started to use the product more than we could have imagined.&lt;/p&gt;
    &lt;p&gt;As adoption grew, Postgres became the bottleneck for the workloads Langfuse needed to support (high‑throughput ingestion + fast analytical reads). With Langfuse v3, we switched the core data layer to ClickHouse to make Langfuse scale for production workloads, both in Cloud and self‑hosted deployments.&lt;/p&gt;
    &lt;p&gt;And if you like infrastructure deep dives, here’s the v3 migration write‑up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why join ClickHouse&lt;/head&gt;
    &lt;p&gt;There are a lot of ways this could have gone. We didn’t plan to sell the company. Actually, we had Term Sheets for a great Series A and were looking forward to some days off over Christmas after an intense year.&lt;/p&gt;
    &lt;p&gt;What changed wasn’t our conviction in Langfuse, it was realizing how much faster we can go together with ClickHouse, while staying true to what makes Langfuse work: open source, self-hosting, and a product that’s built for real production workloads.&lt;/p&gt;
    &lt;head rend="h3"&gt;A shared history (before the acquisition)&lt;/head&gt;
    &lt;p&gt;This dialogue didn’t start with a term sheet. Because Langfuse runs on ClickHouse, we naturally ended up collaborating early and often.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We’ve always been closely in touch with many teams at ClickHouse: sharing feedback with the database team, and using new features to make Langfuse more reliable. For example, compute-compute separation helps us to reduce the risk of noisy-neighbours on Langfuse Cloud.&lt;/item&gt;
      &lt;item&gt;Langfuse Cloud is a large customer of ClickHouse Cloud.&lt;/item&gt;
      &lt;item&gt;Teams at ClickHouse use Langfuse to improve their agentic applications.&lt;/item&gt;
      &lt;item&gt;We invested heavily in ClickHouse-backed self-hosting: documentation, templates, and deployment patterns, and collaborated closely with ClickHouse on improving that experience.&lt;/item&gt;
      &lt;item&gt;As a result, Langfuse introduced thousands of teams to ClickHouse when upgrading from Langfuse v2 to v3.&lt;/item&gt;
      &lt;item&gt;We’ve done community meetups together: a ClickHouse meetup at our Berlin office, another one in San Francisco, and an OpenHouse talk in Amsterdam.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Langfuse runs on ClickHouse, ClickHouse uses Langfuse to optimize its agentic products, we share lots of customers and OSS deployments; that gives ClickHouse every incentive to keep Langfuse fast, reliable, and boringly dependable at scale.&lt;/p&gt;
    &lt;p&gt;So in many ways, we operated like long-term partners. This acquisition is a way to make that partnership permanent — and invest aggressively together.&lt;/p&gt;
    &lt;p&gt;Max shared on how we use ClickHouse to keep product performance ahead of demand at ClickHouse Open House (recording) in Amsterdam.&lt;/p&gt;
    &lt;head rend="h3"&gt;Culture and engineering fit&lt;/head&gt;
    &lt;p&gt;The first time we met Aaron, Yury, Alexey, Tanya, Ryadh, and Pete in-person ended up in a long lunch in Amsterdam. It became obvious we share a similar view on building great developer tooling, how that drives everything within our companies, and how fast analytics is increasingly foundational for building and optimizing agentic products.&lt;/p&gt;
    &lt;p&gt;We already knew that ClickHouse is one of the best infrastructure engineering teams in the world. More importantly, the engineering culture feels like an instant match:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;open-source identity and stewardship&lt;/item&gt;
      &lt;item&gt;developer-first product instincts&lt;/item&gt;
      &lt;item&gt;performance and reliability as product features (not afterthoughts)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The whole Langfuse team will join ClickHouse to continue building Langfuse. All of these aspects were important to us and we couldn’t be more excited.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we’re focused on next&lt;/head&gt;
    &lt;p&gt;Our north star doesn’t change: help teams ship useful, reliable agents by closing the loop from production data to better prompts, evaluations, and product decisions.&lt;/p&gt;
    &lt;p&gt;Concretely, we’re investing in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Production monitoring and analytics for real agent systems (not just offline evals).&lt;/item&gt;
      &lt;item&gt;Workflows across tracing, labeling, and experiments so iteration loops get shorter.&lt;/item&gt;
      &lt;item&gt;More performance and scale—especially for large self‑hosted and enterprise deployments.&lt;/item&gt;
      &lt;item&gt;More polish (UI/UX, developer experience, and docs) so the product stays simple even as the space gets more complex.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can always follow along on the public roadmap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thank you&lt;/head&gt;
    &lt;p&gt;Langfuse exists because the community pushed it forward, through GitHub issues, PRs, feedback, and lots of Slack messages and spontaneous calls to dig into a product feature together.&lt;/p&gt;
    &lt;p&gt;We’re grateful for the trust you’ve put in us. Joining ClickHouse is our way of honoring that trust by putting more resources behind the thing we care about most: building a product you can rely on.&lt;/p&gt;
    &lt;p&gt;We’re excited for what’s next!&lt;lb/&gt; Max, Clemens, and Marc&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;Is Langfuse still open source?&lt;lb/&gt;Yes. No licensing changes planned.&lt;/p&gt;
    &lt;p&gt;Can I still self‑host Langfuse?&lt;lb/&gt;Yes. Self‑hosting is a first‑class path.&lt;/p&gt;
    &lt;p&gt;Does anything change for Langfuse Cloud customers today?&lt;lb/&gt;No. Same product, same endpoints, same contracts.&lt;/p&gt;
    &lt;p&gt;Where do I go for support?&lt;lb/&gt;No changes: https://langfuse.com/support&lt;/p&gt;
    &lt;p&gt;Will the Langfuse team stay on Langfuse?&lt;lb/&gt;Yes. The team is joining ClickHouse and will keep building Langfuse. Also, we continue hiring in Berlin and SF.&lt;/p&gt;
    &lt;head rend="h2"&gt;Join the discussion&lt;/head&gt;
    &lt;p&gt;If you have any other questions, let’s discuss together on GitHub Discussions.&lt;/p&gt;
    &lt;p&gt;If you’re an enterprise customer and have additional questions, feel free to reach out to enterprise@langfuse.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://langfuse.com/blog/joining-clickhouse"/><published>2026-01-17T09:15:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46656834</id><title>Map To Poster – Create Art of your favourite city</title><updated>2026-01-18T00:59:29.579180+00:00</updated><content>&lt;doc fingerprint="a8f7d87e9ce1dad8"&gt;
  &lt;main&gt;
    &lt;p&gt;Generate beautiful, minimalist map posters for any city in the world.&lt;/p&gt;
    &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;
    &lt;code&gt;python create_map_poster.py --city &amp;lt;city&amp;gt; --country &amp;lt;country&amp;gt; [options]&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Short&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--city&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-c&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;City name&lt;/cell&gt;
        &lt;cell&gt;required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--country&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-C&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Country name&lt;/cell&gt;
        &lt;cell&gt;required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--theme&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-t&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Theme name&lt;/cell&gt;
        &lt;cell&gt;feature_based&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--distance&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-d&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Map radius in meters&lt;/cell&gt;
        &lt;cell&gt;29000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--list-themes&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List all available themes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;# Iconic grid patterns
python create_map_poster.py -c "New York" -C "USA" -t noir -d 12000           # Manhattan grid
python create_map_poster.py -c "Barcelona" -C "Spain" -t warm_beige -d 8000   # Eixample district

# Waterfront &amp;amp; canals
python create_map_poster.py -c "Venice" -C "Italy" -t blueprint -d 4000       # Canal network
python create_map_poster.py -c "Amsterdam" -C "Netherlands" -t ocean -d 6000  # Concentric canals
python create_map_poster.py -c "Dubai" -C "UAE" -t midnight_blue -d 15000     # Palm &amp;amp; coastline

# Radial patterns
python create_map_poster.py -c "Paris" -C "France" -t pastel_dream -d 10000   # Haussmann boulevards
python create_map_poster.py -c "Moscow" -C "Russia" -t noir -d 12000          # Ring roads

# Organic old cities
python create_map_poster.py -c "Tokyo" -C "Japan" -t japanese_ink -d 15000    # Dense organic streets
python create_map_poster.py -c "Marrakech" -C "Morocco" -t terracotta -d 5000 # Medina maze
python create_map_poster.py -c "Rome" -C "Italy" -t warm_beige -d 8000        # Ancient layout

# Coastal cities
python create_map_poster.py -c "San Francisco" -C "USA" -t sunset -d 10000    # Peninsula grid
python create_map_poster.py -c "Sydney" -C "Australia" -t ocean -d 12000      # Harbor city
python create_map_poster.py -c "Mumbai" -C "India" -t contrast_zones -d 18000 # Coastal peninsula

# River cities
python create_map_poster.py -c "London" -C "UK" -t noir -d 15000              # Thames curves
python create_map_poster.py -c "Budapest" -C "Hungary" -t copper_patina -d 8000  # Danube split

# List available themes
python create_map_poster.py --list-themes&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Distance&lt;/cell&gt;
        &lt;cell role="head"&gt;Best for&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4000-6000m&lt;/cell&gt;
        &lt;cell&gt;Small/dense cities (Venice, Amsterdam center)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8000-12000m&lt;/cell&gt;
        &lt;cell&gt;Medium cities, focused downtown (Paris, Barcelona)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15000-20000m&lt;/cell&gt;
        &lt;cell&gt;Large metros, full city view (Tokyo, Mumbai)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;17 themes available in &lt;code&gt;themes/&lt;/code&gt; directory:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Theme&lt;/cell&gt;
        &lt;cell role="head"&gt;Style&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;feature_based&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Classic black &amp;amp; white with road hierarchy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;gradient_roads&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Smooth gradient shading&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;contrast_zones&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;High contrast urban density&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;noir&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Pure black background, white roads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;midnight_blue&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Navy background with gold roads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;blueprint&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Architectural blueprint aesthetic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;neon_cyberpunk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark with electric pink/cyan&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;warm_beige&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Vintage sepia tones&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;pastel_dream&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Soft muted pastels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;japanese_ink&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Minimalist ink wash style&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;forest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Deep greens and sage&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;ocean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Blues and teals for coastal cities&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;terracotta&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mediterranean warmth&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;sunset&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Warm oranges and pinks&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;autumn&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Seasonal burnt oranges and reds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;copper_patina&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Oxidized copper aesthetic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;monochrome_blue&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Single blue color family&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Posters are saved to &lt;code&gt;posters/&lt;/code&gt; directory with format:&lt;/p&gt;
    &lt;code&gt;{city}_{theme}_{YYYYMMDD_HHMMSS}.png
&lt;/code&gt;
    &lt;p&gt;Create a JSON file in &lt;code&gt;themes/&lt;/code&gt; directory:&lt;/p&gt;
    &lt;code&gt;{
  "name": "My Theme",
  "description": "Description of the theme",
  "bg": "#FFFFFF",
  "text": "#000000",
  "gradient_color": "#FFFFFF",
  "water": "#C0C0C0",
  "parks": "#F0F0F0",
  "road_motorway": "#0A0A0A",
  "road_primary": "#1A1A1A",
  "road_secondary": "#2A2A2A",
  "road_tertiary": "#3A3A3A",
  "road_residential": "#4A4A4A",
  "road_default": "#3A3A3A"
}&lt;/code&gt;
    &lt;code&gt;map_poster/
├── create_map_poster.py          # Main script
├── themes/               # Theme JSON files
├── fonts/                # Roboto font files
├── posters/              # Generated posters
└── README.md
&lt;/code&gt;
    &lt;p&gt;Quick reference for contributors who want to extend or modify the script.&lt;/p&gt;
    &lt;code&gt;┌─────────────────┐     ┌──────────────┐     ┌─────────────────┐
│   CLI Parser    │────▶│  Geocoding   │────▶│  Data Fetching  │
│   (argparse)    │     │  (Nominatim) │     │    (OSMnx)      │
└─────────────────┘     └──────────────┘     └─────────────────┘
                                                     │
                        ┌──────────────┐             ▼
                        │    Output    │◀────┌─────────────────┐
                        │  (matplotlib)│     │   Rendering     │
                        └──────────────┘     │  (matplotlib)   │
                                             └─────────────────┘
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Function&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
        &lt;cell role="head"&gt;Modify when...&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_coordinates()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;City → lat/lon via Nominatim&lt;/cell&gt;
        &lt;cell&gt;Switching geocoding provider&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;create_poster()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Main rendering pipeline&lt;/cell&gt;
        &lt;cell&gt;Adding new map layers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_edge_colors_by_type()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Road color by OSM highway tag&lt;/cell&gt;
        &lt;cell&gt;Changing road styling&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_edge_widths_by_type()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Road width by importance&lt;/cell&gt;
        &lt;cell&gt;Adjusting line weights&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;create_gradient_fade()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Top/bottom fade effect&lt;/cell&gt;
        &lt;cell&gt;Modifying gradient overlay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;load_theme()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JSON theme → dict&lt;/cell&gt;
        &lt;cell&gt;Adding new theme properties&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;z=11  Text labels (city, country, coords)
z=10  Gradient fades (top &amp;amp; bottom)
z=3   Roads (via ox.plot_graph)
z=2   Parks (green polygons)
z=1   Water (blue polygons)
z=0   Background color
&lt;/code&gt;
    &lt;code&gt;# In get_edge_colors_by_type() and get_edge_widths_by_type()
motorway, motorway_link     → Thickest (1.2), darkest
trunk, primary              → Thick (1.0)
secondary                   → Medium (0.8)
tertiary                    → Thin (0.6)
residential, living_street  → Thinnest (0.4), lightest&lt;/code&gt;
    &lt;p&gt;New map layer (e.g., railways):&lt;/p&gt;
    &lt;code&gt;# In create_poster(), after parks fetch:
try:
    railways = ox.features_from_point(point, tags={'railway': 'rail'}, dist=dist)
except:
    railways = None

# Then plot before roads:
if railways is not None and not railways.empty:
    railways.plot(ax=ax, color=THEME['railway'], linewidth=0.5, zorder=2.5)&lt;/code&gt;
    &lt;p&gt;New theme property:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add to theme JSON: &lt;code&gt;"railway": "#FF0000"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Use in code: &lt;code&gt;THEME['railway']&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Add fallback in &lt;code&gt;load_theme()&lt;/code&gt;default dict&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All text uses &lt;code&gt;transform=ax.transAxes&lt;/code&gt; (0-1 normalized coordinates):&lt;/p&gt;
    &lt;code&gt;y=0.14  City name (spaced letters)
y=0.125 Decorative line
y=0.10  Country name
y=0.07  Coordinates
y=0.02  Attribution (bottom-right)
&lt;/code&gt;
    &lt;code&gt;# Get all buildings
buildings = ox.features_from_point(point, tags={'building': True}, dist=dist)

# Get specific amenities
cafes = ox.features_from_point(point, tags={'amenity': 'cafe'}, dist=dist)

# Different network types
G = ox.graph_from_point(point, dist=dist, network_type='drive')  # roads only
G = ox.graph_from_point(point, dist=dist, network_type='bike')   # bike paths
G = ox.graph_from_point(point, dist=dist, network_type='walk')   # pedestrian&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large &lt;code&gt;dist&lt;/code&gt;values (&amp;gt;20km) = slow downloads + memory heavy&lt;/item&gt;
      &lt;item&gt;Cache coordinates locally to avoid Nominatim rate limits&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;network_type='drive'&lt;/code&gt;instead of&lt;code&gt;'all'&lt;/code&gt;for faster renders&lt;/item&gt;
      &lt;item&gt;Reduce &lt;code&gt;dpi&lt;/code&gt;from 300 to 150 for quick previews&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/originalankur/maptoposter"/><published>2026-01-17T10:13:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46657122</id><title>ASCII characters are not pixels: a deep dive into ASCII rendering</title><updated>2026-01-18T00:59:29.209838+00:00</updated><content>&lt;doc fingerprint="74d7db3c781d01ad"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ASCII characters are not pixels: a deep dive into ASCII rendering&lt;/head&gt;
    &lt;p&gt;Recently, I’ve been spending my time building an image-to-ASCII renderer. Below is the result — try dragging it around, the demo is interactive!&lt;/p&gt;
    &lt;p&gt;One thing I spent a lot of effort on is getting edges looking sharp. Take a look at this rotating cube example:&lt;/p&gt;
    &lt;p&gt;Try opening the “split” view. Notice how well the characters follow the contour of the square.&lt;/p&gt;
    &lt;p&gt;This renderer works well for animated scenes, like the ones above, but we can also use it to render static images:&lt;/p&gt;
    &lt;p&gt;The image of Saturn was generated with ChatGPT.&lt;/p&gt;
    &lt;p&gt;Then, to get better separation between different colored regions, I also implemented a cel shading-like effect to enhance contrast between edges. Try dragging the contrast slider below:&lt;/p&gt;
    &lt;p&gt;The contrast enhancement makes the separation between different colored regions far clearer. That was key to making the 3D scene above look as good as it does.&lt;/p&gt;
    &lt;p&gt;I put so much focus on sharp edges because they’re an aspect of ASCII rendering that is often overlooked when programmatically rendering images as ASCII. Consider this animated 3D scene from Cognition’s landing page that is rendered via ASCII characters:&lt;/p&gt;
    &lt;p&gt;Source: cognition.ai&lt;/p&gt;
    &lt;p&gt;It’s a cool effect, especially while in motion, but take a look at those blurry edges! The characters follow the cube contours very poorly, and as a result, the edges look blurry and jagged in places:&lt;/p&gt;
    &lt;p&gt;This blurriness happens because the ASCII characters are being treated like pixels — their shape is ignored. It’s disappointing to see because ASCII art looks so much better when shape is utilized. I don’t believe I’ve ever seen shape utilized in generated ASCII art, and I think that’s because it’s not really obvious how to consider shape when building an ASCII renderer.&lt;/p&gt;
    &lt;p&gt;I started building my ASCII renderer to prove to myself that it’s possible to utilize shape in ASCII rendering. In this post, I’ll cover the techniques and ideas I used to capture shape and build this ASCII renderer in detail.&lt;/p&gt;
    &lt;p&gt;We’ll start with the basics of image-to-ASCII conversion and see where the common issue of blurry edges comes from. After that, I’ll show you the approach I used to fix that and achieve sharp, high-quality ASCII rendering. At the end, we’ll improve on that by implementing the contrast enhancement effect I showed above.&lt;/p&gt;
    &lt;p&gt;Let’s get to it!&lt;/p&gt;
    &lt;head rend="h2"&gt;Image to ASCII conversion&lt;/head&gt;
    &lt;p&gt;ASCII contains 95 printable characters that we can use. Let’s start off by rendering the following image containing a white circle using those ASCII characters:&lt;/p&gt;
    &lt;p&gt;ASCII art is (almost) always rendered using a monospace font. Since every character in a monospace font is equally wide and tall, we can split the image into a grid. Each grid cell will contain a single ASCII character.&lt;/p&gt;
    &lt;p&gt;The image with the circle is &lt;/p&gt;
    &lt;p&gt;Monospace characters are typically taller than they are wide, so I made each grid cell a bit taller than it is wide.&lt;/p&gt;
    &lt;p&gt;Our task is now to pick which character to place in each cell. The simplest approach is to calculate a lightness value for each cell and pick a character based on that.&lt;/p&gt;
    &lt;p&gt;We can get a lightness value for each cell by sampling the lightness of the pixel at the cell’s center:&lt;/p&gt;
    &lt;p&gt;We want each pixel’s lightness as a numeric value between &lt;/p&gt;
    &lt;p&gt;We can use the following formula to convert an RGB color (with component values between &lt;/p&gt;
    &lt;p&gt;See relative luminance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mapping lightness values to ASCII characters&lt;/head&gt;
    &lt;p&gt;Now that we have a lightness value for each cell, we want to use those values to pick ASCII characters. As mentioned before, ASCII has 95 printable characters, but let’s start simple with just these characters:&lt;/p&gt;
    &lt;quote&gt;: - # = + @ * % .&lt;/quote&gt;
    &lt;p&gt;We can sort them in approximate density order like so, with lower-density characters to the left, and high-density characters to the right:&lt;/p&gt;
    &lt;quote&gt;. : - = + * # % @&lt;/quote&gt;
    &lt;p&gt;We’ll put these characters in a &lt;code&gt;CHARS&lt;/code&gt; array:&lt;/p&gt;
    &lt;quote&gt;const CHARS = [" ", ".", ":", "-", "=", "+", "*", "#", "%", "@"]&lt;/quote&gt;
    &lt;p&gt;I added space as the first (least dense) character.&lt;/p&gt;
    &lt;p&gt;We can then map lightness values between &lt;/p&gt;
    &lt;quote&gt;function getCharacterFromLightness(lightness: number) {const index = Math.floor(lightness * (CHARS.length - 1));return CHARS[index];}&lt;/quote&gt;
    &lt;p&gt;This maps low lightness values to low-density characters and high lightness values to high-density characters.&lt;/p&gt;
    &lt;p&gt;Rendering the circle from above with this method gives us:&lt;/p&gt;
    &lt;p&gt;That works... but the result is pretty ugly. We seem to always get &lt;code&gt;@&lt;/code&gt; for cells that fall within the circle and a space for cells that fall outside.&lt;/p&gt;
    &lt;p&gt;That is happening because we’ve pretty much just implemented nearest-neighbor downsampling. Let’s see what that means.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nearest neighbor downsampling&lt;/head&gt;
    &lt;p&gt;Downsampling, in the context of image processing, is taking a larger image (in our case, the &lt;/p&gt;
    &lt;p&gt;The simplest and fastest method of sampling is nearest-neighbor interpolation, where, for each cell (pixel), we only take a single sample from the higher resolution image.&lt;/p&gt;
    &lt;p&gt;Consider the circle example again. Using nearest-neighbor interpolation, every sample either falls inside or outside of the shape, resulting in either &lt;/p&gt;
    &lt;p&gt;If, instead of picking an ASCII character for each grid cell, we color each grid cell (pixel) according to the sampled value, we get the following pixelated rendering:&lt;/p&gt;
    &lt;p&gt;This pixelated rendering is pretty much equivalent to the ASCII rendering from before. The only difference is that instead of &lt;code&gt;@&lt;/code&gt;s we have white pixels, and instead of spaces we have black pixels.&lt;/p&gt;
    &lt;p&gt;These square, jagged looking edges are aliasing artifacts, commonly called jaggies. They’re a common result of using nearest-neighbor interpolation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Supersampling&lt;/head&gt;
    &lt;p&gt;To get rid of jaggies, we can collect more samples for each cell. Consider this line:&lt;/p&gt;
    &lt;p&gt;The line’s slope on the &lt;/p&gt;
    &lt;p&gt;Let’s try to get rid of the jagginess by taking multiple samples within each cell and using the average sampled lightness value as the cell’s lightness. The example below lets you vary the number of samples using the slider:&lt;/p&gt;
    &lt;p&gt;With multiple samples, cells that lie on the edge of a shape will have some of their samples fall within the shape, and some outside of it. Averaging those, we get gray in-between colors that smooth the downsampled image. Below is the same example, but with an overlay showing where the samples are taken:&lt;/p&gt;
    &lt;p&gt;This method of collecting multiple samples from the larger image is called supersampling. It’s a common method of spatial anti-aliasing (avoiding jaggies at edges). Here’s what the rotating square looks like with supersampling (using &lt;/p&gt;
    &lt;p&gt;Let’s look at what supersampling does for the circle example from earlier. Try dragging the sample quality slider:&lt;/p&gt;
    &lt;p&gt;The circle becomes less jagged, but the edges feel blurry. Why’s that?&lt;/p&gt;
    &lt;p&gt;Well, they feel blurry because we’re pretty much just rendering a low-resolution, pixelated image of a circle. Take a look at the pixelated view:&lt;/p&gt;
    &lt;p&gt;The ASCII and pixelated views are mirror images of each other. Both are just low-resolution versions of the original high-resolution image, scaled up to the original’s size — it’s no wonder they both look blurry.&lt;/p&gt;
    &lt;p&gt;Increasing the number of samples is insufficient. No matter how many samples we take per cell, the samples will be averaged into a single lightness value, used to render a single pixel.&lt;/p&gt;
    &lt;p&gt;And that’s the core problem: treating each grid cell as a pixel in an image. It’s an obvious and simple method, but it disregards that ASCII characters have shape.&lt;/p&gt;
    &lt;p&gt;We can make our ASCII renderings far more crisp by picking characters based on their shape. Here’s the circle rendered that way:&lt;/p&gt;
    &lt;p&gt;The characters follow the contour of the circle very well. By picking characters based on shape, we get a far higher effective resolution. The result is also more visually interesting.&lt;/p&gt;
    &lt;p&gt;Let’s see how we can implement this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Shape&lt;/head&gt;
    &lt;p&gt;So what do I mean by shape? Well, consider the characters &lt;code&gt;T&lt;/code&gt;, &lt;code&gt;L&lt;/code&gt;, and &lt;code&gt;O&lt;/code&gt; placed within grid cells:&lt;/p&gt;
    &lt;p&gt;The character &lt;code&gt;T&lt;/code&gt; is top-heavy. Its visual density in the upper half of the grid cell is higher than in the lower half. The opposite can be said for &lt;code&gt;L&lt;/code&gt; — it’s bottom-heavy. &lt;code&gt;O&lt;/code&gt; is pretty much equally dense in the upper and lower halves of the cell.&lt;/p&gt;
    &lt;p&gt;We might also compare characters like &lt;code&gt;L&lt;/code&gt; and &lt;code&gt;J&lt;/code&gt;. The character &lt;code&gt;L&lt;/code&gt; is heavier within the left half of the cell, while &lt;code&gt;J&lt;/code&gt; is heavier in the right half:&lt;/p&gt;
    &lt;p&gt;We also have more “extreme” characters, such as &lt;code&gt;_&lt;/code&gt; and &lt;code&gt;^&lt;/code&gt;, that only occupy the lower or upper portion of the cell, respectively:&lt;/p&gt;
    &lt;p&gt;This is, roughly, what I mean by “shape” in the context of ASCII rendering. Shape refers to which regions of a cell a given character visually occupies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Quantifying shape&lt;/head&gt;
    &lt;p&gt;To pick characters based on their shape, we’ll somehow need to quantify (put numbers to) the shape of each character.&lt;/p&gt;
    &lt;p&gt;Let’s start by only considering how much characters occupy the upper and lower regions of our cell. To do that, we’ll define two “sampling circles” for each grid cell — one placed in the upper half and one in the lower half:&lt;/p&gt;
    &lt;p&gt;It may seem odd or arbitrary to use circles instead of just splitting the cell into two rectangles, but using circles will give us more flexibility later on.&lt;/p&gt;
    &lt;p&gt;A character placed within a cell will overlap each of the cell’s sampling circles to some extent.&lt;/p&gt;
    &lt;p&gt;One can compute that overlap by taking a bunch of samples within the circle (for example, at every pixel). The fraction of samples that land inside the character gives us the overlap as a numeric value between &lt;/p&gt;
    &lt;p&gt;For T, we get an overlap of approximately &lt;/p&gt;
    &lt;p&gt;We can generate such a &lt;/p&gt;
    &lt;p&gt;Below are some ASCII characters and their shape vectors. I’m coloring the sampling circles using the component values of the shape vectors:&lt;/p&gt;
    &lt;p&gt;We can use the shape vectors as 2D coordinates — here’s every ASCII character on a 2D plot:&lt;/p&gt;
    &lt;head rend="h3"&gt;Shape-based lookup&lt;/head&gt;
    &lt;p&gt;Let’s say that we have our ASCII characters and their associated shape vectors in a &lt;code&gt;CHARACTERS&lt;/code&gt; array:&lt;/p&gt;
    &lt;quote&gt;const CHARACTERS: Array&amp;lt;{character: string,shapeVector: number[],}&amp;gt; = [...];&lt;/quote&gt;
    &lt;p&gt;We can then perform a nearest neighbor search like so:&lt;/p&gt;
    &lt;quote&gt;function findBestCharacter(inputVector: number[]) {let bestCharacter = "";let bestDistance = Infinity;for (const { character, shapeVector } of CHARACTERS) {const dist = getDistance(shapeVector, inputVector);if (dist &amp;lt; bestDistance) {bestDistance = dist;bestCharacter = character;}}return bestCharacter;}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;findBestCharacter&lt;/code&gt; function gives us the ASCII character whose shape best matches the input lookup vector.&lt;/p&gt;
    &lt;p&gt;Note: this brute force search is not very performant. This becomes a bottleneck when we start rendering thousands of ASCII characters at &lt;/p&gt;
    &lt;p&gt;To make use of this in our ASCII renderer, we’ll calculate a lookup vector for each cell in the ASCII grid and pass it to &lt;code&gt;findBestCharacter&lt;/code&gt; to determine the character to display.&lt;/p&gt;
    &lt;p&gt;Let’s try it out. Consider the following zoomed-in circle as an example. It is split into three grid cells:&lt;/p&gt;
    &lt;p&gt;Overlaying our sampling circles, we see varying degrees of overlap:&lt;/p&gt;
    &lt;p&gt;When calculating the shape vector of each ASCII character, we took a huge number of samples. We could afford to do that because we only need to calculate those shape vectors once up front. After they’re calculated, we can use them again and again.&lt;/p&gt;
    &lt;p&gt;However, if we’re converting an animated image (e.g. canvas or video) to ASCII, we need to be mindful of performance when calculating the lookup vectors. An ASCII rendering might have hundreds or thousands of cells. Multiplying that by tens or hundreds of samples would be incredibly costly in terms of performance.&lt;/p&gt;
    &lt;p&gt;With that being said, let’s pick a sampling quality of &lt;/p&gt;
    &lt;p&gt;For the top sampling circle of the leftmost cell, we get one white sample and two black, giving us an average lightness of &lt;/p&gt;
    &lt;p&gt;From now on, instead of using the term “lookup vectors”, I’ll call these vectors, sampled from the image that we’re rendering as ASCII, sampling vectors. One sampling vector is calculated for each cell in the grid.&lt;/p&gt;
    &lt;p&gt;Anyway, we can use these sampling vectors to find the best-matching ASCII character. Let’s see what that looks like on our 2D plot — I’ll label the sampling vectors (from left to right) C0, C1, and C2:&lt;/p&gt;
    &lt;p&gt;Hmm... this is not what we want. Since none of the ASCII shape vector components exceed &lt;/p&gt;
    &lt;p&gt;We can fix this by normalizing the shape vectors. We’ll do that by taking the maximum value of each component across all shape vectors, and dividing the components of each shape vector by the maximum. Expressed in code, that looks like so:&lt;/p&gt;
    &lt;quote&gt;const max = [0, 0]for (const vector of characterVectors) {for (const [i, value] of Object.entries(vector)) {if (value &amp;gt; max[i]) {max[i] = value;}}}const normalizedCharacterVectors = characterVectors.map(vector =&amp;gt; vector.map((value, i) =&amp;gt; value / max[i]))&lt;/quote&gt;
    &lt;p&gt;Here’s what the plot looks like with the shape vectors normalized:&lt;/p&gt;
    &lt;p&gt;If we now map the sampling vectors to their nearest neighbors, we get a much more sensible result:&lt;/p&gt;
    &lt;p&gt;We get &lt;code&gt;'&lt;/code&gt;, &lt;code&gt;M&lt;/code&gt; and &lt;code&gt;$&lt;/code&gt;.  Let’s see how well those characters match the circle:&lt;/p&gt;
    &lt;p&gt;Nice! They match very well.&lt;/p&gt;
    &lt;p&gt;Let’s try rendering the full circle from before with the same method:&lt;/p&gt;
    &lt;p&gt;Much better than before! The picked characters follow the contour of the circle very well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limits of a 2D shape vector&lt;/head&gt;
    &lt;p&gt;Using two sampling circles — one upper and one lower — produces a much better result than the &lt;/p&gt;
    &lt;p&gt;For example, two circles don’t capture the shape of characters that fall in the middle of the cell. Consider &lt;code&gt;-&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;-&lt;/code&gt;, we get a shape vector of &lt;/p&gt;
    &lt;p&gt;The two upper-lower sampling circles also don’t capture left-right differences, such as the difference between &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;q&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;We could use such differences to get better character picks, but our two sampling circles don’t capture them. Let’s add more dimensions to our shape to fix that.&lt;/p&gt;
    &lt;head rend="h2"&gt;Increasing to 6 dimensions&lt;/head&gt;
    &lt;p&gt;Since cells are taller than they are wide (at least with the monospace font I’m using), we can use &lt;/p&gt;
    &lt;p&gt;&lt;code&gt;p&lt;/code&gt; and &lt;code&gt;q&lt;/code&gt;, while also capturing differences across the top, bottom, and middle regions of the cell, differentiating &lt;code&gt;^&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, and &lt;code&gt;_&lt;/code&gt;. They also capture the shape of “diagonal” characters like &lt;code&gt;/&lt;/code&gt; to a reasonable degree.&lt;/p&gt;
    &lt;p&gt;One problem with this grid-like configuration for the sampling circles is that there are gaps. For example, &lt;code&gt;.&lt;/code&gt; falls between the sampling circles:&lt;/p&gt;
    &lt;p&gt;To compensate for this, we can stagger the sampling circles vertically (e.g. lowering the left sampling circles and raising the right ones) and make them a bit larger. This causes the cell to be almost fully covered while not causing excessive overlap across the sampling circles:&lt;/p&gt;
    &lt;p&gt;We can use the same procedure as before to generate character vectors using these sampling circles, this time yielding a &lt;code&gt;L&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;L&lt;/code&gt;, we get the vector:&lt;/p&gt;
    &lt;p&gt;I’m presenting &lt;/p&gt;
    &lt;p&gt;The lightness values certainly look L-shaped! The 6D shape vector captures &lt;code&gt;L&lt;/code&gt;’s shape very well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Nearest neighbor lookups in a 6D space&lt;/head&gt;
    &lt;p&gt;Now we have a 6D shape vector for every ASCII character. Does that affect character lookups (how we find the best matching character)?&lt;/p&gt;
    &lt;p&gt;Earlier, in the &lt;code&gt;findBestCharacter&lt;/code&gt; function, I referenced a &lt;code&gt;getDistance&lt;/code&gt; function. That function returns the Euclidean distance between the input points. Given two 2D points &lt;/p&gt;
    &lt;p&gt;This generalizes to higher dimensions:&lt;/p&gt;
    &lt;p&gt;Put into code, this looks like so:&lt;/p&gt;
    &lt;quote&gt;function getDistance(a: number[], b: number[]): number {let sum = 0;for (let i = 0; i &amp;lt; a.length; i++) {sum += (a[i] - b[i]) ** 2;}return Math.sqrt(sum);}&lt;/quote&gt;
    &lt;p&gt;Note: since we’re just using this for the purposes of finding the closest point, we can skip the expensive &lt;code&gt;Math.sqrt()&lt;/code&gt; call and just return the squared distance. It does not affect the result.&lt;/p&gt;
    &lt;p&gt;So, no, the dimensionality of our shape vector does not change lookups at all. We can use the same &lt;code&gt;getDistance&lt;/code&gt; function for both 2D and 6D.&lt;/p&gt;
    &lt;p&gt;With that out of the way, let’s see what the 6D approach yields!&lt;/p&gt;
    &lt;head rend="h3"&gt;Trying out the 6D approach&lt;/head&gt;
    &lt;p&gt;Our new 6D approach works really well for flat shapes, like the circle example we’ve been using:&lt;/p&gt;
    &lt;p&gt;Now let’s see how this approach works when we render a 3D scene with more shades of gray:&lt;/p&gt;
    &lt;p&gt;Firstly, the outer contours look nice and sharp. I also like how well the gradients across the sphere and cone look.&lt;/p&gt;
    &lt;p&gt;However, internally, the objects all kind of blend together. The edges between surfaces with different lightnesses aren’t sharp enough. For example, the lighter faces of the cubes all kind of blend into one solid color. When there is a change in color — like when two faces of a cube meet — I’d like to see more sharpness in the ASCII rendering.&lt;/p&gt;
    &lt;p&gt;To demonstrate what I mean, consider the following split:&lt;/p&gt;
    &lt;p&gt;It’s currently rendered like so:&lt;/p&gt;
    &lt;p&gt;The different shades result in &lt;code&gt;i&lt;/code&gt;s on the left and &lt;code&gt;B&lt;/code&gt;s on the right, but the boundary is not very sharp.&lt;/p&gt;
    &lt;p&gt;By applying some effects to the sampling vector, we can enhance the contrast at the boundary so that it appears sharper:&lt;/p&gt;
    &lt;p&gt;The added contrast makes a big difference in readability for the 3D scene. Let’s look at how we can implement this contrast enhancement effect.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contrast enhancement&lt;/head&gt;
    &lt;p&gt;Consider cells overlapping a color boundary like so:&lt;/p&gt;
    &lt;p&gt;For the cells on the boundary, we get a 6D sampling vector that looks like so:&lt;/p&gt;
    &lt;p&gt;To make future examples easier to visualize, I’ll start drawing the sampling vector using &lt;/p&gt;
    &lt;p&gt;Currently, this sampling vector resolves to the character &lt;code&gt;T&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;That’s a sensible choice. The character &lt;code&gt;T&lt;/code&gt; is visually dense in the top half and less so in the bottom half, so it matches the image fairly well.&lt;/p&gt;
    &lt;p&gt;Still, I want the picked character to emphasize the shape of the boundary better. We can achieve that by enhancing the contrast of the sampling vector.&lt;/p&gt;
    &lt;p&gt;To increase the contrast of our sampling vector, we might raise each component of the vector to the power of some exponent.&lt;/p&gt;
    &lt;p&gt;Consider how an exponent affects values between &lt;/p&gt;
    &lt;p&gt;The level of pull depends on the exponent. Here’s a chart of &lt;/p&gt;
    &lt;p&gt;This effect becomes more pronounced with higher exponents:&lt;/p&gt;
    &lt;p&gt;A higher exponent translates to a stronger pull towards zero.&lt;/p&gt;
    &lt;p&gt;Applying an exponent should make dark values darker more quickly than light ones. The example below allows you to vary the exponent applied to the sampling vector:&lt;/p&gt;
    &lt;p&gt;As the exponent is increased to &lt;/p&gt;
    &lt;p&gt;I don’t want that. I want to increase the contrast between the lighter and darker components of the sampling vector, not the vector in its entirety.&lt;/p&gt;
    &lt;p&gt;To achieve that, we can normalize the sampling vector to the range &lt;/p&gt;
    &lt;p&gt;The normalization to &lt;/p&gt;
    &lt;quote&gt;const maxValue = Math.max(...samplingVector)samplingVector = samplingVector.map((value) =&amp;gt; {value = value / maxValue; // Normalizevalue = Math.pow(value, exponent);value = value * maxValue; // Denormalizereturn value;})&lt;/quote&gt;
    &lt;p&gt;Here’s the same example, but with this normalization applied:&lt;/p&gt;
    &lt;p&gt;Very nice! The lightest component values are retained, and the contrast between the lighter and darker components is increased by “crunching” the lower values.&lt;/p&gt;
    &lt;p&gt;This affects which character is picked. The following example shows how the selected character changes as the contrast is increased:&lt;/p&gt;
    &lt;p&gt;Awesome! The pick of &lt;code&gt;"&lt;/code&gt; over &lt;code&gt;T&lt;/code&gt; emphasizes the separation between the lighter region above and the darker region below!&lt;/p&gt;
    &lt;p&gt;By enhancing the contrast of the sampling vector, we exaggerate its shape. This gives us a character that less faithfully represents the underlying image, but improves readability as a whole by enhancing the separation between different colored regions.&lt;/p&gt;
    &lt;p&gt;Let’s look at another example. Observe how the L-shape of the sampling vector below becomes more pronounced as the exponent increases, and how that affects the picked character:&lt;/p&gt;
    &lt;p&gt;Works really nicely! I love the transition from &lt;code&gt;&amp;amp; -&amp;gt; b -&amp;gt; L&lt;/code&gt; as the L-shape of the vector becomes clearer.&lt;/p&gt;
    &lt;p&gt;What’s nice about applying exponents to normalized sampling vectors is that it barely affects vectors that are uniform in value. If all component values are similar, applying an exponent has a minimal effect:&lt;/p&gt;
    &lt;p&gt;Because the vector is fairly uniform, the exponent only has a slight effect and doesn’t change the picked character.&lt;/p&gt;
    &lt;p&gt;This is a good thing! If we have a smooth gradient in our image, we want to retain it. We very much do not want to introduce unnecessary choppiness.&lt;/p&gt;
    &lt;p&gt;Compare the 3D scene ASCII rendering with and without this contrast enhancement:&lt;/p&gt;
    &lt;p&gt;We do see more contrast at boundaries, but this is not quite there yet. Some edges are still not sharp enough, and we also observe a “staircasing” effect happening at some boundaries.&lt;/p&gt;
    &lt;p&gt;Let’s look at the staircasing effect first. We can reproduce it with a boundary like so:&lt;/p&gt;
    &lt;p&gt;Below is the ASCII rendering of that boundary. Notice how the lower edge (the &lt;code&gt;!&lt;/code&gt;s) becomes “staircase-y” as you increase the exponent:&lt;/p&gt;
    &lt;p&gt;We see a staircase pattern like so:&lt;/p&gt;
    &lt;quote&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;To understand why that’s happening, let’s consider the row in the middle of the canvas, progressing from left to right. As we start off, every sample is equally light, giving us &lt;code&gt;U&lt;/code&gt;s:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUU -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;As we reach the boundary, the lower right samples become a bit darker. Those darker components are crunched by contrast enhancement, giving us some &lt;code&gt;Y&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;So we get:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYY -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;As we progress further right, the middle and lower samples get darker, so we get some &lt;code&gt;f&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;This trend continues towards &lt;code&gt;"&lt;/code&gt;, &lt;code&gt;'&lt;/code&gt;, and finally, &lt;code&gt;`&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Giving us a sequence like so:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYYf""''` -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;That looks good, but at some point we get no light samples. Once we get no light samples, our contrast enhancement has no effect because every component is equally light. This causes us to always get &lt;code&gt;!&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;Making our sequence look like so:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYYf""''`!!!!!!!!!! -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;This sudden stop in contrast enhancement having an effect is what causes the staircasing effect:&lt;/p&gt;
    &lt;quote&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;Let’s see how we can counteract this staircasing effect with another layer of contrast enhancement, this time looking outside of the boundary of each cell.&lt;/p&gt;
    &lt;head rend="h3"&gt;Directional contrast enhancement&lt;/head&gt;
    &lt;p&gt;We currently have sampling circles arranged like so:&lt;/p&gt;
    &lt;p&gt;For each of those sampling circles, we’ll specify an “external sampling circle”, placed outside of the cell’s boundary, like so:&lt;/p&gt;
    &lt;p&gt;Each of those external sampling circles is “reaching” into the region of a neighboring cell. Together, the samples that are collected by the external sampling circles constitute an “external sampling vector”.&lt;/p&gt;
    &lt;p&gt;Let’s simplify the visualization and consider a single example. Imagine that we collected a sampling vector and an external sampling vector that look like so:&lt;/p&gt;
    &lt;p&gt;The circles colored red are the external sampling vector components. Currently, they have no effect.&lt;/p&gt;
    &lt;p&gt;The “internal” sampling vector itself is fairly uniform, with values ranging from &lt;/p&gt;
    &lt;p&gt;To enhance this apparent boundary, we’ll darken the top-left and middle-left components of the sampling vector. We can do that by applying component-wise contrast enhancement using the values from the external vector.&lt;/p&gt;
    &lt;p&gt;In the previous contrast enhancement, we calculated the maximum component value across the sampling vector and normalized the vector using that value:&lt;/p&gt;
    &lt;quote&gt;const maxValue = Math.max(...samplingVector)samplingVector = samplingVector.map((value) =&amp;gt; {value = value / maxValue; // Normalizevalue = Math.pow(value, exponent);value = value * maxValue; // Denormalizereturn value;})&lt;/quote&gt;
    &lt;p&gt;But the new component-wise contrast enhancement will take the maximum value between each component of the sampling vector and the corresponding component in the external sampling vector:&lt;/p&gt;
    &lt;quote&gt;samplingVector = samplingVector.map((value, i) =&amp;gt; {const maxValue = Math.max(value, externalSamplingVector[i])// ...});&lt;/quote&gt;
    &lt;p&gt;Aside from that, the contrast enhancement is performed in the same way:&lt;/p&gt;
    &lt;quote&gt;samplingVector = samplingVector.map((value, i) =&amp;gt; {const maxValue = Math.max(value, externalSamplingVector[i]);value = value / maxValue;value = Math.pow(value, exponent);value = value * maxValue;return value;});&lt;/quote&gt;
    &lt;p&gt;The example below shows how light values in the external sampling vector push values in the sampling vector down:&lt;/p&gt;
    &lt;p&gt;I call this “directional contrast enhancement”, since each of the external sampling circles reaches outside of the cell in the direction of the sampling vector component that it is enhancing the contrast of. I describe the other effect as “global contrast enhancement” since it acts on all of the sampling vector’s components together.&lt;/p&gt;
    &lt;p&gt;Let’s see what this directional contrast enhancement does to get rid of the staircasing effect:&lt;/p&gt;
    &lt;p&gt;Hmm, that’s not doing what I wanted. I wanted to see a sequence like so:&lt;/p&gt;
    &lt;quote&gt;..::!!..::!!!!!!!!..::!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;But we just see &lt;code&gt;!&lt;/code&gt; changing to &lt;code&gt;:&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;This happens because the directional contrast enhancement doesn’t reach far enough into our sampling vector. The light upper values in the external vector do push the upper values of the sampling vector down, but because the lightness of the four bottom components is retained, we don’t get to &lt;code&gt;.&lt;/code&gt;, just &lt;code&gt;:&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Widening the directional contrast enhancement&lt;/head&gt;
    &lt;p&gt;I’d like to “widen” the directional contrast enhancement so that, for example, light external values at the top spread to the middle components of the sampling vector.&lt;/p&gt;
    &lt;p&gt;To do that, I’ll introduce a few more external sampling circles, arranged like so:&lt;/p&gt;
    &lt;p&gt;These are a total of &lt;/p&gt;
    &lt;p&gt;For each component of the internal sampling vector, we’ll calculate the maximum value across the external sampling vector components that affect it, and use that maximum to perform the contrast enhancement.&lt;/p&gt;
    &lt;p&gt;Let’s implement that. I’ll order the internal and external sampling circles like so:&lt;/p&gt;
    &lt;p&gt;We can then define a mapping from the internal circles to the external sampling circles that affect them:&lt;/p&gt;
    &lt;quote&gt;const AFFECTING_EXTERNAL_INDICES = [[0, 1, 2, 4],[0, 1, 3, 5],[2, 4, 6],[3, 5, 7],[4, 6, 8, 9],[5, 7, 8, 9],];&lt;/quote&gt;
    &lt;p&gt;With this, we can change the calculation of &lt;code&gt;maxValue&lt;/code&gt; to take the maximum affecting external value:&lt;/p&gt;
    &lt;quote&gt;// Beforeconst maxValue = Math.max(value, externalSamplingVector[i]);// Afterlet maxValue = value;for (const externalIndex of AFFECTING_EXTERNAL_INDICES[i]) {maxValue = Math.max(value, externalSamplingVector[externalIndex]);}&lt;/quote&gt;
    &lt;p&gt;Now look what happens if the top four external sampling circles are light: it causes the contrast enhancement to reach into the middle of the sampling vector, giving us the desired effect:&lt;/p&gt;
    &lt;p&gt;We now smoothly transition from &lt;code&gt;! -&amp;gt; : -&amp;gt; .&lt;/code&gt; — beautiful stuff!&lt;/p&gt;
    &lt;p&gt;Let’s see if this change resolves the staircasing effect:&lt;/p&gt;
    &lt;p&gt;Oh yeah, looks awesome! We get the desired effect. The boundary is nice and sharp while not being too jagged.&lt;/p&gt;
    &lt;p&gt;Here’s the 3D scene again. The contrast slider now applies both types of contrast enhancement at the same time — try it out:&lt;/p&gt;
    &lt;p&gt;This really enhances the contrast at boundaries, making the image far more readable!&lt;/p&gt;
    &lt;p&gt;Together, the 6D shape vector approach and contrast enhancement techniques have given us a really nice final ASCII rendering.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final words&lt;/head&gt;
    &lt;p&gt;This post was really fun to build and write! I hope you enjoyed reading it.&lt;/p&gt;
    &lt;p&gt;ASCII rendering is perhaps not the most useful topic to write about, but I think the idea of using a high-dimensional vector to capture shape is interesting and could easily be applied to many other problems. There are parallels to be drawn to word embeddings.&lt;/p&gt;
    &lt;p&gt;I started writing this ASCII renderer to see if the idea of using a vector to capture the shape of characters would work at all. That approach turned out to work very well, but the initial prototype was terribly slow — I only got single-digit FPS on my iPhone. To get the ASCII renderer running at a smooth &lt;/p&gt;
    &lt;p&gt;My colleagues, after reading a draft of this post, suggested many alternatives to the approaches I described in this post. For example, why not make the sampling vector &lt;code&gt;T&lt;/code&gt; far better — just look how &lt;code&gt;T&lt;/code&gt;’s stem falls between the two sampling circles in each row:&lt;/p&gt;
    &lt;p&gt;And yeah, he’s right! A &lt;/p&gt;
    &lt;p&gt;It’s really fun how large the solution space to the problem of ASCII rendering is. There are so, so many approaches and trade-offs to explore. I imagine you probably thought of a few yourself while reading this post!&lt;/p&gt;
    &lt;p&gt;One dimension I intentionally did not explore was using different colors or lightnesses for the ASCII characters themselves. This is for many reasons, but the two primary ones are that 1) it would have expanded the scope of this post too much, and 2) it’s just a different effect, and I personally don’t like the look.&lt;/p&gt;
    &lt;p&gt;At the time of writing these final words, around &lt;/p&gt;
    &lt;p&gt;Thanks for reading! And huge thanks to Gunnlaugur Þór Briem and Eiríkur Fannar Torfason for reading and providing feedback on a draft of this post.&lt;/p&gt;
    &lt;p&gt;— Alex Harri&lt;/p&gt;
    &lt;p&gt;To be notified of new posts, subscribe to my mailing list.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix I: Character lookup performance&lt;/head&gt;
    &lt;p&gt;Earlier in this post, I showed how can find the best character by finding the character with the shortest Euclidean distance to our sampling vector.&lt;/p&gt;
    &lt;quote&gt;function findBestCharacter(inputVector: number[]) {let bestCharacter = "";let bestDistance = Infinity;for (const { character, shapeVector } of CHARACTERS) {const dist = getDistance(shapeVector, inputVector);if (dist &amp;lt; bestDistance) {bestDistance = dist;bestCharacter = character;}}return bestCharacter;}&lt;/quote&gt;
    &lt;p&gt;I tried benchmarking this for &lt;/p&gt;
    &lt;p&gt;If we allow ourselves &lt;/p&gt;
    &lt;head rend="h3"&gt;k-d trees&lt;/head&gt;
    &lt;p&gt;Internally, &lt;/p&gt;
    &lt;p&gt;I won’t go into much detail on &lt;/p&gt;
    &lt;p&gt;One could also look at the hierarchical navigable small worlds (HNSW) algorithm, which Eiríkur pointed me to. It is used for approximate nearest neighbor lookups in vector databases, so definitely relevant.&lt;/p&gt;
    &lt;p&gt;Let’s see how it performs! We’ll construct a &lt;/p&gt;
    &lt;quote&gt;const kdTree = new KdTree(CHARACTERS.map(({ character, shapeVector }) =&amp;gt; ({point: shapeVector,data: character,})));&lt;/quote&gt;
    &lt;p&gt;We can now perform nearest-neighbor lookups on the &lt;/p&gt;
    &lt;quote&gt;const result = kdTree.findNearest(samplingVector);&lt;/quote&gt;
    &lt;p&gt;Running &lt;/p&gt;
    &lt;p&gt;That’s a lot of lookups per frame, but again, we’re benchmarking on a powerful machine. This is still not good enough.&lt;/p&gt;
    &lt;p&gt;Let’s see how we can eke out even more performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caching&lt;/head&gt;
    &lt;p&gt;An obvious avenue for speeding up lookups is to cache the result:&lt;/p&gt;
    &lt;quote&gt;function searchCached(samplingVector: number[]) {const key = generateCacheKey(samplingVector)if (cache.has(key)) {return cache.get(key)!;}const result = search(samplingVector);cache.set(key, result);return result;}&lt;/quote&gt;
    &lt;p&gt;But how does one generate a cache key for a &lt;/p&gt;
    &lt;p&gt;Well, one way is to quantize each vector component so that it fits into a set number of bits and packing those bits into a single number. JavaScript numbers give us &lt;/p&gt;
    &lt;p&gt;We can quantize a numeric value between &lt;/p&gt;
    &lt;quote&gt;const BITS = 5;const RANGE = 2 ** BITS;function quantizeTo5Bits(value: number) {return Math.min(RANGE - 1, Math.floor(value * RANGE));}&lt;/quote&gt;
    &lt;p&gt;Applying a max of &lt;code&gt;RANGE - 1&lt;/code&gt; is done so that a &lt;code&gt;value&lt;/code&gt; of exactly &lt;/p&gt;
    &lt;p&gt;We can quantize each of the sampling vector components in this manner and use bit shifting to pack all of the quantized values into a single number like so:&lt;/p&gt;
    &lt;quote&gt;const BITS = 5;const RANGE = 2 ** BITS;function generateCacheKey(vector: number[]): number {let key = 0;for (let i = 0; i &amp;lt; vector.length; i++) {const quantized = Math.min(RANGE - 1, Math.floor(vector[i] * RANGE));key = (key &amp;lt;&amp;lt; BITS) | quantized;}return key;}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;RANGE&lt;/code&gt; is current set to &lt;code&gt;2 ** 5&lt;/code&gt;, but consider how large that makes our key space. Each vector component is one of &lt;/p&gt;
    &lt;p&gt;Alright, &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of keys&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory needed to store keys&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;46,656&lt;/cell&gt;
        &lt;cell&gt;364 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;117,649&lt;/cell&gt;
        &lt;cell&gt;919 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;262,144&lt;/cell&gt;
        &lt;cell&gt;2.00 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;531,441&lt;/cell&gt;
        &lt;cell&gt;4.05 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;1,000,000&lt;/cell&gt;
        &lt;cell&gt;7.63 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;1,771,561&lt;/cell&gt;
        &lt;cell&gt;13.52 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;2,985,984&lt;/cell&gt;
        &lt;cell&gt;22.78 MB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are trade-offs to consider here. As the range gets smaller, the quality of the results drops. If we pick a range of &lt;/p&gt;
    &lt;p&gt;At the same time, if we increase the possible number of keys, we need more memory to store them. Additionally, the cache hit rate might be very low, especially when the cache is relatively empty.&lt;/p&gt;
    &lt;p&gt;I ended up picking a range of &lt;/p&gt;
    &lt;p&gt;Cached lookups are incredibly fast — fast enough that lookup performance just isn’t a concern anymore (&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix II: GPU acceleration&lt;/head&gt;
    &lt;p&gt;Lookups were not the only performance concern. Just collecting the sampling vectors (internal and external) turned out to be terribly expensive.&lt;/p&gt;
    &lt;p&gt;Just consider the sheer amount of samples that need to be collected. The 3D scene I’ve been using as an example uses a &lt;/p&gt;
    &lt;p&gt;And that’s if we use a sampling quality of &lt;/p&gt;
    &lt;p&gt;Collecting these samples absolutely crushed performance on my iPhone, so I needed to either collect fewer samples or speed up the collection of samples. Collecting fewer samples would have meant rendering fewer ASCII characters or removing the directional contrast enhancement, neither of which was an appealing solution.&lt;/p&gt;
    &lt;p&gt;My initial implementation ran on the CPU, which could only collect one sample at a time. To speed this up, I moved the work of sampling collection and applying the contrast enhancement to the GPU. The pipeline for that looks like so (each of the steps listed is a single shader pass):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Collect the raw internal sampling vectors into a &lt;mjx-container/&gt;texture, using the canvas (image) as the input texture.&lt;/item&gt;
      &lt;item&gt;Do the same for the external sampling vectors.&lt;/item&gt;
      &lt;item&gt;Calculate the maximum external value affecting each internal vector component into a &lt;mjx-container/&gt;texture.&lt;/item&gt;
      &lt;item&gt;Apply directional contrast enhancement to each sampling vector component, using the maximum external values texture.&lt;/item&gt;
      &lt;item&gt;Calculate the maximum value for each internal sampling vector into a &lt;mjx-container/&gt;texture.&lt;/item&gt;
      &lt;item&gt;Apply global contrast enhancement to each sampling vector component, using the maximum internal values texture.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m glossing over the details because I could spend a whole other post covering them, but moving work to the GPU made the renderer many times more performant than it was when everything ran on the CPU.&lt;/p&gt;
    &lt;p&gt;To be notified of new posts, subscribe to my mailing list.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alexharri.com/blog/ascii-rendering"/><published>2026-01-17T11:15:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46658345</id><title>The recurring dream of replacing developers</title><updated>2026-01-18T00:59:28.955688+00:00</updated><content>&lt;doc fingerprint="d5de5d392af7abd9"&gt;
  &lt;main&gt;
    &lt;p&gt;07.12.2025, By Stephan Schwab&lt;/p&gt;
    &lt;p&gt;Every decade brings new promises: this time, we'll finally make software development simple enough that we won't need so many developers. From COBOL to AI, the pattern repeats. Business leaders grow frustrated with slow delivery and high costs. Developers feel misunderstood and undervalued. Understanding why this cycle persists for fifty years reveals what both sides need to know about the nature of software work.&lt;/p&gt;
    &lt;p&gt;When Neil Armstrong stepped onto the lunar surface in 1969, the world witnessed what organized human ingenuity could accomplish. Behind that achievement stood Margaret Hamilton and her team, writing Apollo’s guidance software by hand, catching critical errors through careful review, and proving that software could be mission-critical.&lt;/p&gt;
    &lt;p&gt;The Apollo program demonstrated that software development was essential to achieving the impossible. Yet it also revealed something that would frustrate business leaders for decades to come: writing software required specialized knowledge, intense focus, and significant time investment. The dream of making it easier—of needing fewer of these expensive specialists—began almost immediately.&lt;/p&gt;
    &lt;p&gt;The late 1960s and 1970s saw COBOL emerge with an explicit goal stated in its name: Common Business-Oriented Language. The vision was clear: make the language read like English sentences, and business analysts would write their own programs. No need for specialized programmers.&lt;/p&gt;
    &lt;p&gt;This vision had genuine appeal. Software was becoming essential to business operations, yet programmers remained a scarce, expensive resource. COBOL promised to democratize software creation.&lt;/p&gt;
    &lt;p&gt;What happened instead? COBOL became another programming language requiring specialized training. Business analysts who tried to write COBOL quickly discovered that readable syntax didn’t eliminate the complexity of logic, data structures, or system design. A new class of COBOL programmers emerged, and the dream of eliminating specialized developers remained unfulfilled.&lt;/p&gt;
    &lt;p&gt;Yet the dream didn’t die. It simply waited for the next technological wave.&lt;/p&gt;
    &lt;p&gt;Computer-Aided Software Engineering tools arrived in the 1980s with tremendous promise. Draw flowcharts and entity-relationship diagrams, and the tool would generate working code. The marketing message resonated: visual design was more intuitive than typing cryptic commands. Business experts could model their processes, and software would materialize.&lt;/p&gt;
    &lt;p&gt;Organizations invested heavily. Vendors promised productivity increases of 10x or more. Yet most CASE tool initiatives struggled or failed outright.&lt;/p&gt;
    &lt;p&gt;The generated code often required substantial manual intervention. Performance problems emerged. Maintenance became a nightmare when generated code diverged from the visual models. Most critically, drawing accurate diagrams required understanding the same logical complexity that programming demanded. The tool changed the interface but not the fundamental challenge.&lt;/p&gt;
    &lt;p&gt;Once again, the problem proved more stubborn than the solution.&lt;/p&gt;
    &lt;p&gt;The 1990s brought a different approach. Microsoft’s Visual Basic and Borland’s Delphi made building user interfaces dramatically easier. Drag components onto a form, set properties, write event handlers. Suddenly, creating a Windows application felt achievable for developers with modest experience.&lt;/p&gt;
    &lt;p&gt;This wave succeeded differently than COBOL or CASE tools. These environments acknowledged that programming knowledge was still necessary, but they reduced the barrier to entry. A broader range of people could create useful applications.&lt;/p&gt;
    &lt;p&gt;Yet the dream of eliminating developers persisted. “Power users” and “citizen developers” would build departmental applications. IT departments could focus on infrastructure while business units solved their own software needs.&lt;/p&gt;
    &lt;p&gt;Reality proved more nuanced. Simple applications were indeed accessible to more people. But as requirements grew in complexity—integration with existing systems, security considerations, performance under load, long-term maintenance—the need for experienced developers became evident. The tools expanded who could write software, but they didn’t eliminate the expertise required for substantial systems.&lt;/p&gt;
    &lt;p&gt;And so the cycle continued into the new millennium.&lt;/p&gt;
    &lt;p&gt;Each subsequent decade introduced new variations. Ruby on Rails promised convention over configuration. Low-code platforms offered visual development with minimal coding. No-code platforms claimed to eliminate programming entirely for common business applications.&lt;/p&gt;
    &lt;p&gt;Each wave delivered real value. Development genuinely became faster in specific contexts. More people could participate in creating software solutions. Yet professional software developers remained essential, and demand for their skills continued growing rather than shrinking.&lt;/p&gt;
    &lt;p&gt;Which brings us to the question: why does this pattern repeat?&lt;/p&gt;
    &lt;p&gt;The recurring pattern reveals something important about how we think about complexity. Software development looks like it should be simple because we can describe what we want in plain language. “When a customer places an order, check inventory, calculate shipping, process payment, and send a confirmation email.” That description sounds straightforward.&lt;/p&gt;
    &lt;p&gt;The complexity emerges in the details. What happens when inventory is temporarily reserved by another order? How do you handle partial payments? What if the email service is temporarily unavailable? Should you retry? How many times? What if the customer’s session expires during checkout? How do you prevent duplicate orders?&lt;/p&gt;
    &lt;p&gt;Each answer leads to more questions. The accumulated decisions, edge cases, and interactions create genuine complexity that no tool or language can eliminate. Someone must think through these scenarios. That thinking is software development, regardless of whether it’s expressed in COBOL, a CASE tool diagram, Visual Basic, or an AI prompt.&lt;/p&gt;
    &lt;p&gt;Which brings us to today’s excitement.&lt;/p&gt;
    &lt;p&gt;Today’s AI coding assistants represent the most capable attempt yet to assist with software creation. They can generate substantial amounts of working code from natural language descriptions. They can explain existing code, suggest improvements, and help debug problems.&lt;/p&gt;
    &lt;p&gt;This represents genuine progress. The assistance is real and valuable. Experienced developers use these tools to work more efficiently. People learning to code find the interactive guidance helpful.&lt;/p&gt;
    &lt;p&gt;Yet we’re already seeing the familiar pattern emerge. Initial excitement about AI replacing developers is giving way to a more nuanced understanding: AI changes how developers work rather than eliminating the need for their judgment. The complexity remains. Someone must understand the business problem, evaluate whether the generated code solves it correctly, consider security implications, ensure it integrates properly with existing systems, and maintain it as requirements evolve.&lt;/p&gt;
    &lt;p&gt;AI amplifies developer capability. It doesn’t replace the need for people who understand both the problem domain and the technical landscape.&lt;/p&gt;
    &lt;p&gt;Here’s the paradox that makes this pattern particularly poignant. We’ve made extraordinary progress in software capabilities. The Apollo guidance computer had 4KB of RAM. Your smartphone has millions of times more computing power. We’ve built tools and frameworks that genuinely make many aspects of development easier.&lt;/p&gt;
    &lt;p&gt;Yet demand for software far exceeds our ability to create it. Every organization needs more software than it can build. The backlog of desired features and new initiatives grows faster than development teams can address it.&lt;/p&gt;
    &lt;p&gt;This tension—powerful tools yet insufficient capacity—keeps the dream alive. Business leaders look at the backlog and think, “There must be a way to go faster, to enable more people to contribute.” That’s a reasonable thought. It leads naturally to enthusiasm for any tool or approach that promises to democratize software creation.&lt;/p&gt;
    &lt;p&gt;The challenge is that software development isn’t primarily constrained by typing speed or syntax knowledge. It’s constrained by the thinking required to handle complexity well. Faster typing doesn’t help when you’re thinking through how to handle concurrent database updates. Simpler syntax doesn’t help when you’re reasoning about security implications.&lt;/p&gt;
    &lt;p&gt;So what should leaders do with this understanding?&lt;/p&gt;
    &lt;p&gt;Understanding this pattern changes how you evaluate new tools and approaches. When someone promises that their platform will let business users build applications without developers, you can appreciate the aspiration while maintaining realistic expectations.&lt;/p&gt;
    &lt;p&gt;The right question isn’t “Will this eliminate our need for developers?” The right questions are:&lt;/p&gt;
    &lt;p&gt;These questions acknowledge that development involves irreducible complexity while remaining open to tools that provide genuine leverage.&lt;/p&gt;
    &lt;p&gt;And they point to something deeper about the nature of software work.&lt;/p&gt;
    &lt;p&gt;This fifty-year pattern teaches us something fundamental about software development itself. If the problem were primarily mechanical—too much typing, too complex syntax, too many steps—we would have solved it by now. COBOL made syntax readable. CASE tools eliminated typing. Visual tools eliminated syntax. AI can now generate entire functions from descriptions.&lt;/p&gt;
    &lt;p&gt;Each advancement addressed a real friction point. Yet the fundamental challenge persists because it’s not mechanical. It’s intellectual. Software development is thinking made tangible. The artifacts we create—whether COBOL programs, Delphi forms, or Python scripts—are the visible outcome of invisible reasoning about complexity.&lt;/p&gt;
    &lt;p&gt;You can’t shortcut that reasoning any more than you can shortcut the reasoning required to design a building or diagnose a medical condition. Better tools help. Experience helps. But someone must still think it through.&lt;/p&gt;
    &lt;p&gt;So how should we move forward, knowing all this?&lt;/p&gt;
    &lt;p&gt;The next wave of development tools will arrive. Some will provide genuine value. Some will repeat familiar promises with new technology. Having perspective on this recurring pattern helps you engage with new tools productively.&lt;/p&gt;
    &lt;p&gt;Use AI assistants. Evaluate low-code platforms. Experiment with new frameworks. But invest primarily in your people’s ability to think clearly about complexity. That capability remains the constraining factor, just as it was during the Apollo program.&lt;/p&gt;
    &lt;p&gt;The moon landing happened because brilliant people thought carefully about every detail of an extraordinarily complex challenge. They wrote software by hand because that was the available tool. If they’d had better tools, they would have used them gladly. But the tools wouldn’t have eliminated their need to think through the complexity.&lt;/p&gt;
    &lt;p&gt;We’re still in that same fundamental situation. We have better tools—vastly better tools—but the thinking remains essential.&lt;/p&gt;
    &lt;p&gt;Perhaps the recurring dream of replacing developers isn’t a mistake. Perhaps it’s a necessary optimism that drives tool creation. Each attempt to make development more accessible produces tools that genuinely help. The dream doesn’t come true as imagined, but pursuing it creates value.&lt;/p&gt;
    &lt;p&gt;COBOL didn’t let business analysts write programs, but it did enable a generation of developers to build business systems effectively. CASE tools didn’t generate complete applications, but they advanced our thinking about visual modeling. Visual Basic didn’t eliminate professional developers, but it brought application development to more people. AI won’t replace developers, but it will change how we work in meaningful ways.&lt;/p&gt;
    &lt;p&gt;The pattern continues because the dream reflects a legitimate need. We genuinely require faster, more efficient ways to create software. We just keep discovering that the constraint isn’t the tool—it’s the complexity of the problems we’re trying to solve.&lt;/p&gt;
    &lt;p&gt;Understanding this doesn’t mean rejecting new tools. It means using them with clear expectations about what they can provide and what will always require human judgment.&lt;/p&gt;
    &lt;p&gt;Let's talk about your real situation. Want to accelerate delivery, remove technical blockers, or validate whether an idea deserves more investment? Book a short conversation (20 min): I listen to your context and give 1–2 practical recommendations—no pitch, no obligation. If it fits, we continue; if not, you leave with clarity. Confidential and direct.&lt;/p&gt;
    &lt;p&gt;Prefer email? Write me: sns@caimito.net&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.caimito.net/en/blog/2025/12/07/the-recurring-dream-of-replacing-developers.html"/><published>2026-01-17T14:31:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46659219</id><title>There's no single best way to store information</title><updated>2026-01-18T00:59:28.273420+00:00</updated><content>&lt;doc fingerprint="7721186709e90e89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why There’s No Single Best Way To Store Information&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Just as there’s no single best way to organize your bookshelf, there’s no one-size-fits-all solution to storing information.&lt;/p&gt;
    &lt;p&gt;Consider the simple situation where you create a new digital file. Your computer needs to rapidly find a place to put it. If you later want to delete it, the machine must quickly find the right bits to erase. Researchers aim to design storage systems, called data structures, that balance the amount of time it takes to add data, the time it takes to later remove it, and the total amount of memory the system needs.&lt;/p&gt;
    &lt;p&gt;To get a feel for these challenges, imagine you keep all your books in a row on one long shelf. If they’re organized alphabetically, you can quickly pick out any book. But whenever you acquire a new book, it’ll take time to find its proper spot. Conversely, if you place books wherever there’s space, you’ll save time now, but they’ll be hard to find later. This trade-off between insertion time and retrieval time might not be a problem for a single-shelf library, but you can see how it could get cumbersome with thousands of books.&lt;/p&gt;
    &lt;p&gt;Instead of a shelf, you could set up 26 alphabetically labeled bins and assign books to bins based on the first letter of the author’s last name. Whenever you get a new book, you can instantly tell which bin it goes in, and whenever you want to retrieve a book, you will immediately know where to look. In certain situations, both insertion and removal can be a lot faster than they would be if you stored items on one long shelf.&lt;/p&gt;
    &lt;p&gt;Of course, this bin system comes with its own problems. Retrieving books is only instantaneous if you have one book per bin; otherwise, you’ll have to root around to find the right one. In an extreme scenario where all your books are by Asimov, Atwood, and Austen, you’re back to the problem of one long shelf, plus you’ll have a bunch of empty bins cluttering up your living room.&lt;/p&gt;
    &lt;p&gt;Computer scientists often study data structures called hash tables that resemble more sophisticated versions of this simple bin system. Hash tables calculate a storage address for each item from a known property of that item, called the key. In our example, the key for each book is the first letter of the author’s last name. But that simple key makes it likely that some bins will be much fuller than others. (Few authors writing in English have a last name that starts with X, for example.) A better approach is to start with the author’s full name, replace each letter in the name with the number corresponding to its position in the alphabet, add up all these numbers, and divide the sum by 26. The remainder is some number between zero and 25. Use that number to assign the book to a bin.&lt;/p&gt;
    &lt;p&gt;This kind of mathematical rule for transforming a key into a storage address is called a hash function. A cleverly designed hash function ensures that items will usually end up distributed relatively evenly across bins, so you won’t need to spend as much time searching in each bin.&lt;/p&gt;
    &lt;p&gt;If you want to reduce retrieval time further, you can use more bins. But that leads to another trade-off: Those bins will take up space even if they end up empty.&lt;/p&gt;
    &lt;p&gt;This trade-off between space and time is an inherent feature of hash tables — it’s the price you pay for avoiding the tension between insertion and retrieval time that plagues simpler data structures. More than 70 years after hash tables were invented, computer scientists are still discovering new things about their fundamental properties. Recently, they finally devised a version that strikes an ideal balance between space and time. And last year, an undergraduate student disproved a long-standing conjecture about the minimum amount of time needed to find a specific item in a hash table that’s almost full.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Heap of Priorities&lt;/head&gt;
    &lt;p&gt;Hash tables work well when you can’t anticipate which piece of data you’ll need to retrieve next. But that’s not always the case. Imagine you’re trying to complete tasks on a to-do list, but you’re constantly being assigned new tasks with different deadlines. You want to be able to quickly add new items to the to-do list, but you don’t care about retrieving items until they become your top priority.&lt;/p&gt;
    &lt;p&gt;In this case, your best bet is a type of data structure called a heap. As the name suggests, a heap is a somewhat haphazard approach to data storage. It’s basically a mathematical version of a pile of stuff: Some items are stored above others, and these higher items are easier to access. The highest-priority item is always at the top of the heap, where you can instantly pluck it off. Lower layers will be more disorganized, but you don’t need to worry about the relative positions of these low-priority items.&lt;/p&gt;
    &lt;p&gt;The simplest implementation of this basic idea uses a mathematical object called a binary tree, which is a network of nodes with a special shape: There’s a single node at the top, and each node is connected to two nodes directly below it.&lt;/p&gt;
    &lt;p&gt;Let’s imagine a binary tree that contains the items in a to-do list. Each node can store a single item, and each item is labeled with a number that represents its due date. High-priority items get smaller numbers.&lt;/p&gt;
    &lt;p&gt;Each new item is put into an empty slot in the current lowest layer.&lt;/p&gt;
    &lt;p&gt;Once the new item goes in, compare its due date to that of the item in the node directly above it. If the new task is due sooner, swap the items. Keep swapping until the new item ends up directly below an item that’s more urgent.&lt;/p&gt;
    &lt;p&gt;This procedure ensures that the highest-priority item will always rise to the top. What’s more, the procedure is extremely fast. Even in a nightmare scenario where you have 1,000 tasks on your to-do list and keep getting new assignments, storing them in a heap ensures that it takes no more than nine swaps to move each new item up to the appropriate position. Whenever you complete the most urgent task and remove it from the heap, you can quickly pull up your new top priority from the layer below.&lt;/p&gt;
    &lt;p&gt;Within computer science, heaps are widely used in algorithms for finding the shortest path from a given starting point in a network to every other point. In 2024, a team of researchers used an ingenious new heap design to transform a classic shortest-paths algorithm into one that is theoretically optimal for any network layout.&lt;/p&gt;
    &lt;p&gt;There’s no shortage of self-help books filled with contradictory advice about the best way to organize your belongings. If computer science offers any lesson, it’s that there is no perfect solution — every approach comes with trade-offs. But if some items are more important to you than others, don’t be afraid to leave a bit of a mess.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/why-theres-no-single-best-way-to-store-information-20260116/"/><published>2026-01-17T16:17:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46659465</id><title>Apples, Trees, and Quasimodes</title><updated>2026-01-18T00:59:27.594475+00:00</updated><content>&lt;doc fingerprint="fd9b9b1c22e783ab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Apples, Trees, and Quasimodes&lt;/head&gt;
    &lt;p&gt;A while back, Ars Technica published a thoughtful piece about Jef Raskin, tracing his long pursuit of the “humane computer” and the cul-de-sacs where that pursuit ended. It’s a generous, well-told account of the designer who wanted to make machines simpler, kinder, and more aligned with the way people actually think.&lt;/p&gt;
    &lt;p&gt;But part of what makes Raskin interesting is that his story isn’t just Apple’s story. He came out of the same cultural current John Markoff chronicled in What the Dormouse Said—the Bay Area tradition that treated computers not as office appliances but as tools for thought, instruments of liberation. Read that way, the Canon Cat and Raskin’s other projects aren’t just an eccentric side quest from a frustrated Apple veteran. It’s evidence of how far the humane ideal could stretch, and how quickly it ran up against the limits of commercial computing.&lt;/p&gt;
    &lt;p&gt;Apple couldn’t deliver Raskin’s vision then, and it can’t deliver it now. Neither can any other big platform company. If we want to understand why, and what Raskin still tells us about humane computing, we have to put him back in the longer lineage he belonged to, and look at how his version of the dream carried that vision but also narrowed it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prophets and participants&lt;/head&gt;
    &lt;p&gt;What the Dormouse Said documents how the Bay Area counterculture shaped early personal computing. LSD, communes, systems theory, amorphous defense research contracts, and Engelbart’s “augmentation” experiments all swirled together in a weird scene that accidentally (or maybe not so accidentally) created much of the modern world.&lt;/p&gt;
    &lt;p&gt;The story usually gets told with a neat list: Engelbart’s demo, Nelson’s Xanadu hypertext, Kay’s Dynabook, Brand’s Whole Earth. Xerox PARC, Steve Jobs, the World Wide Web. The familiar pantheon. But that version turns a messy, improvisational moment into a plaque. Engelbart’s system needed a whole research staff just to operate; Nelson’s Xanadu was (and is) more sermon than software; Kay’s Dynabook lived mostly on paper; Brand mostly supplied vocabulary and vibe. What bound them together wasn’t working code so much as the conviction that computers could be more than appliances and calculators, even if no one agreed on what “more” meant.&lt;/p&gt;
    &lt;p&gt;Ultimately all these weird white guys had a futurist vision: computers could be liberation machines. They weren’t just for business automation or scientific number-crunching; they could be deployed to expand consciousness and reshape how people thought and worked.&lt;/p&gt;
    &lt;p&gt;Raskin belonged to this current. Before Apple, he was an artist and a musician. He brought a humanist’s suspicion of machine logic into the design lab. He argued for humane interfaces: modeless, predictable, low-friction, focused on the human first. He wasn’t a prophet on his own crying in the wilderness so much as another strand of the same weave.&lt;/p&gt;
    &lt;p&gt;That said, his role was different than that of some of these other figures. He tried to pull those ideals out of the lab and into machines ordinary people might actually use. The Macintosh began under his hand, though what shipped was less a tool for thought than a polished derivative—what you might call a “popular religion” of computing, stripped of the harder doctrines.&lt;/p&gt;
    &lt;p&gt;The Canon Cat and its predecessors were Raskin’s counterargument: humane, text-first systems that tried to carry the spirit of the Dormouse tradition into the commercial world without sanding off everything that made it strange. It sort of worked, but only sort of.&lt;/p&gt;
    &lt;head rend="h2"&gt;Raskin’s Humane vision&lt;/head&gt;
    &lt;p&gt;Raskin’s principles are laid out most clearly in 2000’s The Humane Interface, but he’d been developing them since the late 1970s:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modelessness: eliminate modes generally, and especially when they confuse users or are hard to reason about.&lt;/item&gt;
      &lt;item&gt;Quasimodes: short-lived states (like holding a key down) that don’t trap the user.&lt;/item&gt;
      &lt;item&gt;Humane defaults: undo everywhere, consistent commands, predictable behavior.&lt;/item&gt;
      &lt;item&gt;Low cognitive load: interfaces designed around human memory and perception limits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These ideas are recognizably part of the “Tools for Thought” tradition. Like Engelbart and the others, he wanted to reduce friction between thought and machine. Like Nelson, he believed in fluidity and extension.&lt;/p&gt;
    &lt;p&gt;But there’s a subtle difference. For Engelbart, augmentation meant complexity: bootstrapping a system so wild it demanded co-evolution between user and tool. For Nelson, it meant endless layers of possibility. For Raskin, it often meant protection or constraint. Humane computing wasn’t only about empowerment… often it was about shielding users from mistakes, overload, and confusion.&lt;/p&gt;
    &lt;p&gt;That protective impulse would shape the systems he built.&lt;/p&gt;
    &lt;p&gt;Raskin’s first clear articulation of his humane ideals wasn’t hardware at all but The Macintosh Papers, his internal proposal at Apple for a low-cost, appliance-like computer that would boot straight into a simple, modeless interface. The Mac project that followed eventually diverged—under Steve Jobs it became a graphical machine aimed at competing with the Lisa, for the reasons we’ve all read about—but Raskin’s vision was considerably more radical. He imagined a computer that behaved less like a business workstation and more like a humane, everyday tool.&lt;/p&gt;
    &lt;p&gt;In tone, the Macintosh Papers have more in common with Ted Nelson’s Computer Lib than with any corporate white paper. They read like a manifesto: plainspoken, insistent, arguing that ordinary people deserved machines that bent to them rather than the other way around. Where Nelson declared that “you can and must understand computers now,” Raskin’s papers laid out what such a computer should look like if you started from human needs instead of technical conventions. Both belong to that peculiar genre of the 1970s and early ’80s: the computing manifesto as cultural text, half engineering and half tract.1&lt;/p&gt;
    &lt;p&gt;You could argue that the Swyft, built a few years later by his company Information Appliance Inc., was “the real Macintosh” in that sense. Compact and text-first, it booted instantly, eliminated modes, and introduced the Leap keys for fluid navigation. It was Raskin’s manifesto rendered in hardware. But the Swyft never made it to market; without a manufacturer to back it, Information Appliance pivoted to the SwyftCard, a fallback product that brought the same interface into the Apple II while IA waited to find a dance partner.&lt;/p&gt;
    &lt;p&gt;That partner came briefly in 1987, when Canon released the Canon Cat, the only mass-produced computer to carry Raskin’s humane vision into the world. The Cat retained the Swyft’s defining ideas: instant boot into a blank page, consistent commands, Leap-based navigation. Marketed as a word processor, it was framed as an appliance for the office rather than an exploratory tool for thought.&lt;/p&gt;
    &lt;p&gt;After its failure, Raskin returned to the same design principles in the 1990s with Archy, an unfinished software environment that tried once again to realize his humane interface on contemporary hardware. Archy never reached a finished state, but it shows how Raskin’s ideas kept circling back to the same point: computing stripped down to words, presented as simply and predictably as possible.&lt;/p&gt;
    &lt;p&gt;I’ve always had a real fondness for the Swyft/Cat lineage, and it’s certainly influenced what I think a computer can be. Each one of these attempts embodied humane design: a blank screen for writing, consistent commands, no modes to trip over. The Cat in particular was radical in its way—a computer designed to feel less like a computer and more like a natural extension of the mind. It truly could have changed everything about how we use our computers had it succeeded.&lt;/p&gt;
    &lt;p&gt;Unfortunately for all of us, by 1987, the market for dedicated word processors was already fading. Canon didn’t seem to know what to do with the Cat—whether to sell it as an office appliance, a PC competitor, or something stranger—and the result was that it fit nowhere. Raskin’s design pushed toward humane simplicity, but Canon’s marketing treated it like just another machine for typing memos.&lt;/p&gt;
    &lt;p&gt;It isn’t surprising that it failed, though it’s hard not to wonder how it might have landed a few years earlier, when the ground was more open. As it is, the Cat survives less as a commercial product than as an idea in hardware—a glimpse of what a computer could look like if the whole thing were rebuilt around text, consistency, and genuine care for the user.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Paradox of Openness&lt;/head&gt;
    &lt;p&gt;The Cat also embodies why Raskin’s philosophy was not necessarily on the same wavelength as some of those other visionary systems. On the surface, the Canon Cat looked open. It booted to a blank screen. Everything was text. You could jump anywhere, edit fluidly, undo anything. Compared to the modal labyrinth of DOS or early Mac software, it felt like freedom.&lt;/p&gt;
    &lt;p&gt;But look closer and you see the narrowing. The Cat gave you fewer ways to improvise. Its humane design was also constraining design. It reduced your options in order to keep you safe.&lt;/p&gt;
    &lt;p&gt;The real irony is that the Cat wasn’t even truly closed in the way a smartphone or Chromebook might be considered so today. Underneath, it ran on a Forth environment. You could, if you knew how, drop into Forth and even program directly in 68k assembler. In principle, it was as open as any hacker could want, at least from a software perspective.&lt;/p&gt;
    &lt;p&gt;The catch was cultural, not technical. From the Ars piece:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;IAI’s back door to Forth quietly shipped in every Cat, and the clue was a curious omission in the online help: USE FRONT-ANSWER. This otherwise unexplained and unused key combination was the gateway. If you entered the string&lt;/p&gt;&lt;code&gt;Enable Forth Language&lt;/code&gt;, highlighted it, and evaluated it with USE FRONT-ANSWER (not CALC; usually Control-Backspace in MAME), you’d get a Forth&lt;code&gt;ok&lt;/code&gt;prompt, and the system was now yours. Reset the Cat or type&lt;code&gt;re&lt;/code&gt;to return to the editor.&lt;/quote&gt;
    &lt;p&gt;Canon didn’t provide documentation that would have made that power accessible, and Raskin’s design philosophy treated it as outside the normal use case. Extensibility was there if you knew where to look for it, but it wasn’t encouraged. The humane interface was meant to keep most users away from the hood, even though what was under the hood was remarkably open.&lt;/p&gt;
    &lt;p&gt;That makes the Cat’s paradox sharper: it was a genuinely extensible software environment (up to a point) presented as a sealed appliance. The hardware mostly was a sealed appliance. Contrast this with Emacs or Smalltalk, where openness is the posture of the environment itself. You are expected to extend and reshape as you go, building your tools out of themselves. The Cat offered the same possibility–Forth is a remarkably flexible language, especially for microcomputers–but it discouraged you from taking it.&lt;/p&gt;
    &lt;p&gt;Humane computing, in Raskin’s hands, edged toward hermetic computing. He built openness in, but sealed it away behind an interface designed to keep it out of sight.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cul-de-sacs vs. Branches&lt;/head&gt;
    &lt;p&gt;All of this, to me, is why calling Raskin’s systems thinking a “cul-de-sac” misses the point, and is the wrong way to think about his legacy.&lt;/p&gt;
    &lt;p&gt;If “cul-de-sac” means “product that didn’t sell,” then sure, the Cat and SwyftCard qualify. They were total dead-ends. But by that same measure, Engelbart’s NLS, Nelson’s Xanadu, Kay’s Smalltalk, or even Lotus Agenda are dead-ends, too. By that measure, most of the “Tools for Thought” tradition didn’t lead anywhere.&lt;/p&gt;
    &lt;p&gt;The reality is different. These systems were branches. They were rhizomes, in the Deleuze and Guattari sense. They didn’t reach the mainstream, but they seeded ideas that echoed elsewhere, connecting threads that run throughout the history of computing. Hypertext, graphical interfaces, undo, modeless editing—all of these survived in one form or another.&lt;/p&gt;
    &lt;p&gt;Raskin’s branch is no exception. His machines exposed a fundamental tension inside the tradition: how far do you go in protecting the user from complexity? At what point does “humane” become “hermetic”? Those questions didn’t vanish with the Cat. They’re still with us every time a productivity app promises “simplicity” at the cost of agency.&lt;/p&gt;
    &lt;p&gt;Raskin’s humane ideals live on in obvious ways, to the benefit of anyone using a graphical computer today—undo everywhere, discoverability, and consistent commands and shortcuts are now interface common sense. But the deeper thread, the ethos that inspired him and others in the tradition of computers as tools for thought, survived mostly outside the mainstream. It persists in systems that never had to sell millions of units or satisfy quarterly targets, that never had to justify their existence to the mass of people using PCs—tools that could afford to remain strange, open, and humane on their own terms. Emacs, Oberon, and Smalltalk belong here, but so do newer experiments like Uxn and 9front.&lt;/p&gt;
    &lt;p&gt;The Cat failed partly because it tried to straddle two worlds: commercial appliance and humane machine, whereas something like Emacs survives precisely because it never had to. It’s as complex as you want it to be.&lt;/p&gt;
    &lt;p&gt;This is the sharper point: radical, humane, exploratory computing never survives in the mainstream. The mainstream is built for profit and predictability. Even Engelbart’s work was DARPA-funded, not venture-backed. When you put humane ideals through commercial constraints, they collapse into simplistic appliances, the “For Dummies” version of the original intent. That doesn’t mean the tradition is dead. But it does mean you have to look off to the side, away from the market’s center, to see it alive.&lt;/p&gt;
    &lt;head rend="h2"&gt;The dilemma(s)&lt;/head&gt;
    &lt;p&gt;Raskin’s story sharpens two dilemmas that haven’t gone away.&lt;/p&gt;
    &lt;p&gt;The first is practical: make a system too open, and it risks being overwhelming. Make it too humane, and it risks narrowing into something sealed and hermetic, and not useful enough. The Cat, while also a victim of other factors, tried to balance the two and ended up fitting nowhere.&lt;/p&gt;
    &lt;p&gt;The lesson isn’t that humane computing is impossible. It’s that humane computing can’t just mean protective computing. It has to mean trusting users with both simplicity and openness. That’s why Org mode and even Mac System 7 endure and the Cat does not.&lt;/p&gt;
    &lt;p&gt;The deeper implication is harder, but maybe truer: the true Tools for Thought we still wish existed will never come from Apple, Microsoft, Google, OpenAI, or any other large player in the software or hardware space. They can’t. These companies’ scale and incentives point elsewhere—toward lock-in, surveillance, and products that are safe enough to sell but never open enough to empower. The logic of scale makes them constitutionally incapable of building systems that are truly humane and open. The next humane systems, if they arrive, will have to come from outside those walls, as they always have: from margins, from hobbyists, from research labs, and from stubborn communities of practice. But as those platform companies make it more and more difficult to experiment, how do we keep pushing these philosophies forward?&lt;/p&gt;
    &lt;p&gt;Jef Raskin’s philosophy isn’t a cul-de-sac in computing history. He’s responsible for a branch of the “Tools for Thought” tradition—a branch that shows both the promise and the peril of humane design. His machines make clear how far you can go when you put the human first, and how easily that ideal can collapse into constraint once it’s pushed through commercial channels and turned into walled gardens.&lt;/p&gt;
    &lt;p&gt;The humane thread survives, but only outside the center—in the tools that don’t have to answer to quarterly earnings, in projects that refuse to die just because they don’t fit the market. The Dormouse lineage isn’t gone. It just doesn’t live where the money is, because it can’t. If you want your computer to be humane in the deeper sense—not an appliance, but an instrument for thought—you have to look to the margins. That’s where it has always been, and where it still is today. If it survives, that’s where it’ll still be.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;That genre, “photocopied computer manifesto,” is very much the reason this blog exists. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Published September 18, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://systemstack.dev/2025/09/humane-computing/"/><published>2026-01-17T16:44:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46659550</id><title>An Elizabethan mansion's secrets for staying warm</title><updated>2026-01-18T00:59:27.350489+00:00</updated><content>&lt;doc fingerprint="b42e10265ba205d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'The past is an underused tool': An Elizabethan mansion's secrets for staying warm&lt;/head&gt;
    &lt;p&gt;In a bleak, deadly period of cold weather known as the Little Ice Age, clever Elizabethan designs helped keep a magnificent stately home unusually warm. The house has lessons for how we can heat our homes more efficiently today.&lt;/p&gt;
    &lt;p&gt;England's longest river was usually flowing freely. But on New Year's Eve in 1564, the River Thames was frozen solid, from bank to bank. Bonfires crackled on the stuck-fast surface, oxen roasted on spits, and people danced on the ice. Some accounts say that Queen Elizabeth I even practised archery on the glacial river. This sort of thing wasn't a one off. It had happened before: King Henry VIII and his queen had dashed downriver in a sleigh nearly three decades previously in 1537.&lt;/p&gt;
    &lt;p&gt;These frosty conditions were the result of a climatic plot twist roughly between the 14th and 19th centuries, known today as the Little Ice Age. As well as festivals on the ice, this prolonged cold period brought periods of famine, and frightening unseasonable frosts. Soldiers froze to death in the middle of the European summer.&lt;/p&gt;
    &lt;p&gt;The cold forced Europeans to develop new ways of coping with extreme weather. One of the best-studied examples of architectural adaptation is Hardwick Hall in Derbyshire, England – a building whose design is a carefully choreographed effort to keep as warm as possible.&lt;/p&gt;
    &lt;p&gt;The same tricks for more efficient heating can be used in modern designs, helping reduce our reliance on fossil fuels today. And they can even inspire small changes in our existing homes to keep temperatures cosier through the winter without turning up the thermostat.&lt;/p&gt;
    &lt;head rend="h2"&gt;An 'exceptional' house&lt;/head&gt;
    &lt;p&gt;I drive the long, meandering driveway uphill to the house, confronted by the occasional long-horn cattle grazing between leafless oak trees. At the crest of the hill, I'm met with a striking sight: not one hall, but two.&lt;/p&gt;
    &lt;p&gt;Hardwick "old" Hall is massive, despite its ruinous state. I can tell it's been repeatedly extended over the years, as the bricks are misaligned at the joins of each extension and the windows are mismatched in style and size across the facade.&lt;/p&gt;
    &lt;p&gt;What caused the Little Ice Age?&lt;/p&gt;
    &lt;p&gt;It appears there's no single cause of the Little Ice Age, but a deadly and complex combination. Scientists have found evidence for reduced solar activity, increased volcanic eruptions, changes in ocean circulation and the natural fluctuations within the global climate system. In addition, the arrival of the Europeans in North America in the late 15th Century led to an estimated 56 million deaths of indigenous peoples, resulting in widespread abandonment of farming and regrowth of forests. More trees mean less planet-warming gases were circulating in the atmosphere, reducing the global average temperature.&lt;/p&gt;
    &lt;p&gt;Hardwick "new" Hall is a few dozen metres away. This pale yellow manor was built in the 1590s and is eye-pleasingly symmetrical, complete with three-story turrets and huge expanses of glass. Whoever quipped at the time of its construction that it was "more window than wall" was right. It is a magnificent display of wealth, built in a time when glass was extremely expensive.&lt;/p&gt;
    &lt;p&gt;Elizabeth (Bess), Countess of Shrewsbury, was the woman who had deep enough pockets to build it. She was mid-way through extending the massive, rambling Hardwick "old" Hall, but for some reason or another, stopped midway through and began afresh. The experts I spoke to say we don't know why she did that, but theories range from coming into money when her husband died and feeling the need to have a house in keeping with her elevated status, to using what she'd learnt in previous builds to design a house warm and cosy for a lady approaching her seventies and living through the Little Ice Age.&lt;/p&gt;
    &lt;p&gt;"The late 16th Century is really one of the coldest stretches of the Little Ice Age, and it's bitterly cold in England," says Dagomar Degroot, professor of environmental history at Georgetown University in Washington, DC, and author of The Frigid Golden Age.&lt;/p&gt;
    &lt;p&gt;Global average temperatures during the Little Ice Age dipped "at most" by 0.5C (less than 0.9F), with impacts mostly documented in the northern hemisphere. That figure is an average over about five centuries, so temperatures would have swung more dramatically year to year and region to region.&lt;/p&gt;
    &lt;head rend="h2"&gt;Turning to the Sun&lt;/head&gt;
    &lt;p&gt;A key difference between the old hall and the new hall is their orientation in relation to the Sun. The old hall is just off east-west. The new hall has been rotated by about 90 degrees, which means it can soak up much more sunshine and, therefore, heat.&lt;/p&gt;
    &lt;p&gt;"The incredible thing about Hardwick [new Hall] is… when you set it on the compass, it's almost exactly north-south," says Ranald Lawrence, a lecturer in architecture at the University of Liverpool in the UK. He's also published papers on Hardwick's design and thermal comfort. "And," he adds, "the whole internal planning of the [new] house is then based around that geometry."&lt;/p&gt;
    &lt;p&gt;Bess moved around the rooms, following the Sun's path. Her mornings were spent walking the 63m (200ft) east-facing Long Gallery, where the bright morning light hits. The afternoon and evening Sun illuminates the south-western flank of the building, where Bess' bed chambers were. And the darkest, coldest corner of the house in the north-west was where the kitchens were placed, which would have been handy in keeping food cool and fresh.&lt;/p&gt;
    &lt;p&gt;I experience this first hand as I walk around – the kitchens are much colder. Elena Williams, the senior house and collections manager at The National Trust, a UK charity which preserves historic sites, notices too. "It's a well-designed building that is also designed around comfort and that uses the natural environment to do that," she says.&lt;/p&gt;
    &lt;head rend="h2"&gt;Windows, walls and fireplaces&lt;/head&gt;
    &lt;p&gt;It's not just the orientation that helps keep the house warm. As Williams shows me around, she points out that some of the windows on the north of the building are actually "blind" or fake. She explains that on the outside, there is a window, but on the inside, it's lined with lead and blocked up. Unlike south-facing windows, north-facing windows bring little thermal benefit, even in summer, Lawrence says.&lt;/p&gt;
    &lt;p&gt;Pretty much all the fireplaces I see are also built on the central spine of the building, meaning not much heat would be lost to the windows or exterior wall. It's not until we take a door through this spine that I realise that the girth of it is staggering – 1.37m (4.5ft) thick. This is yet another trick to keep its inhabitants warm.&lt;/p&gt;
    &lt;p&gt;"You have thermal mass, effectively," Lawrence says. "So something heavy like brick or stone, like you have at Hardwick, stores the heat from the fire and gives it out 12 hours later."&lt;/p&gt;
    &lt;p&gt;All these construction techniques appear to have made a difference. Lawrence has measured the temperature difference between inside and outside in modern times and depending on the season and weather, he told me it can feel around 10C (18F) warmer inside on a cold winter's day. Other, typical Elizabethan houses, he estimates, would have only feel 2-3C (3.6-5.4F) warmer.&lt;/p&gt;
    &lt;p&gt;The Tudors had other coping mechanisms, Williams says – like lining the walls with thick tapestries – adding further thermal mass and keeping out the drafts. Curtains were hung around the beds and over some of the windows too. And Elizabethan fashion of giant neck ruffs and layers and layers of linens, thick velvet and fur all helped people like Bess keep warm.&lt;/p&gt;
    &lt;p&gt;Other large and flashy manors at the time were using some of the same solar strategies. But Lawrence believes Hardwick is "exceptional" in the way these elements are carefully integrated and brought together.&lt;/p&gt;
    &lt;p&gt;Though Lawrence says there is no written evidence to suggest that the architectural designs were purposeful, he thinks that "it can't be coincidence". Williams agrees. "I think they definitely thought about using the Sun in the design of Hardwick," she says.&lt;/p&gt;
    &lt;p&gt;All this despite the fact that the Elizabethans may have been unaware that they were living through what is now known as the Little Ice Age, says Degroot. "Why would I expect somebody living 400 years ago would realise that their climate was 0.5C colder than the climate had been in their mother's or father's lifetime?"&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons for modern times&lt;/head&gt;
    &lt;p&gt;There are still lessons we can learn today when we build new homes and in the way we use the ones we've already got – especially with the need to heat and cool our homes more efficiently in order to save money and tackle climate change.&lt;/p&gt;
    &lt;p&gt;"The past is an underused tool," Degroot says. "I think by trying to identify the complex and diverse ways in which people responded to history's climate changes, we can come up with new tools for understanding how we might respond in the future and for identifying responses that are constructive versus destructive."&lt;/p&gt;
    &lt;p&gt;Brutalist architects Peter and Alison Smithson knew of and even admired Hardwick Hall. Some academics claim that it likely inspired their own designs, like the Solar Pavillion, in south-west England, which is only has glass on its east, west and south-facing walls. Sun-soaking designs aren't just the preserve of the rich, though. One of London's most striking council estates is on Alexandria Road in Camden, in the north of the city, and Lawrence says it too features south facing terraces with lots of concrete to store the Sun's heat.&lt;/p&gt;
    &lt;p&gt;But on the whole, he tells me, we generally don't use these Elizabethan building secrets. Instead, we use air conditioning and heating in an attempt to override building designs that are poorly suited to their climate.&lt;/p&gt;
    &lt;p&gt;"Our assumption that the solution to all of our problems is technological," Lawrence says. Glass box skyscrapers, now common in both cold and hot climates, are a good example of this. In winter, heat escapes through the glass, and require a lot of heating. Conversely in summer, the glass traps the heat – like a greenhouse – and require massive amounts of energy for cooling.&lt;/p&gt;
    &lt;p&gt;But without taking apart our existing housing and building it again from scratch, there are also micro-adjustments we can make.&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hacks to keep your home warmer&lt;/item&gt;
      &lt;item&gt;The homes heated without fossil fuels&lt;/item&gt;
      &lt;item&gt;How living in a cold home affects your health&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I get a compass out at my house – for the first time – and begin to think about how I could follow the Sun's path throughout the day. Since it's winter, and cold, I move my desk to a south-eastern window. It brightens the mornings and if I wear another layer, I find I can lower the thermostat by 2C (3.6F). Longer term, I've been thinking about planting a tree just outside. In a couple of decades, it would shade my house from scorching heatwaves that are predicted to be much more common because of climate change.&lt;/p&gt;
    &lt;p&gt;These are modest changes, imperceptible to most, and they won't enable us to forgo active heating and cooling entirely. But they do echo a way of thinking which, today, is oft ignored. Hardwick Hall was designed with Sun, season and temperature in mind. It paid attention to the world outside its walls. As the climate becomes more volatile, architecture that works with its environment feels more urgent than ever.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;For essential climate news and hopeful developments to your inbox, sign up to the Future Earth newsletter, while The Essential List delivers a handpicked selection of features and insights twice a week.&lt;/p&gt;
    &lt;p&gt;For more science, technology, environment and health stories from the BBC, follow us on Facebook and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/future/article/20260116-an-elizabethan-mansions-secrets-for-staying-warm"/><published>2026-01-17T16:53:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46660543</id><title>Raising money fucked me up</title><updated>2026-01-18T00:59:27.134834+00:00</updated><content>&lt;doc fingerprint="7d4e111769e460bd"&gt;
  &lt;main&gt;
    &lt;p&gt;About four months ago I quit my job at Doublepoint and decided to start my own thing.&lt;/p&gt;
    &lt;p&gt;I'd been working on a little project with Pedrique (who would become my co-founder) for a bit over half-a-year and decided I had enough signal to determine he was someone I wanted to start a business with.&lt;/p&gt;
    &lt;p&gt;I was excited about the idea we were working on at the time, but being truly honest about my motivations, I mostly wanted to run my own thing. In a dream world I'd have had the "idea of my life" while working at PostHog or Doublepoint and have gone on to build that with maximum conviction but this wasn't the case, so I got tired of waiting for a spark and decided to go out and make it happen, with the idea we were working on being our best bet at the time.&lt;/p&gt;
    &lt;p&gt;Since I'd just quit my job, I had my finances well in order. Thus, my ideal scenario would have been to work on the idea we had the MVP for, try to get it off the ground, and if that didn't work, try something else, then something else, until something did indeed get off the ground, and only at that point we would consider whether or not to raise VC funding, depending on whether it made sense or not.&lt;/p&gt;
    &lt;p&gt;My ideal scenario wasn't going to work for Pedrique, though. He had told me for a while that the money he had saved up for trying to build his own thing was running out and that soon he'd need to start freelancing or something to make some income in order to sustain the search for a little longer. Prior to us working together, he had a bit of success with his MicroSaaS products but only just enough to increase his personal runway, which was now reasonably short.&lt;/p&gt;
    &lt;p&gt;We had spoken about this before, but with me now being 110% in, we had to do something about it. I had just come in full-time so we weren't about to go back to a dynamic where one person was full-time and the other part-time because they needed to make ends meet. The decision then became clear: we're gonna raise.&lt;/p&gt;
    &lt;p&gt;At that point, it was an easy decision to make. Again, we have two co-founders who have a lot of confidence in each other, and we don't want to let the opportunity pass us by. So while this wasn't my ideal choice, we were a business now and this was the best decision for the company. "Just don't die" goes the advice I think, and Skald had just then been born.&lt;/p&gt;
    &lt;p&gt;And so raise we did. We brought in four phenomenal angels, including, and this is relevant, my last few bosses (PostHog co-founders James and Tim and Doublepoint co-founder Ohto), and then decided to look for an early-stage fund. We eventually landed with Broom Ventures and passed up on a few other opportunities to limit dilution.&lt;/p&gt;
    &lt;p&gt;Great, right? I didn't need a salary yet, but for equality purposes, I now had one. Our investors are amazing. James and Ohto have been particularly helpful as angels (thank you!), and our investors are all founders of successful companies, including Jeff and Dan, the Broom GPs. We're super early, but Broom has been massively helpful and all-around just a great hands-off VC to deal with.&lt;/p&gt;
    &lt;p&gt;Most importantly, none of them put any pressure on us. All understand the nature of pre-seed investing well, and that can't be said about all the potential investors we took meetings with.&lt;/p&gt;
    &lt;p&gt;So some time passes and we decide to pivot. We're really excited about the new idea. We launch and get a bit of early traction. The open source project is doing well, but we're struggling to monetize. We fail to close a few customers and the traction wanes a bit.&lt;/p&gt;
    &lt;p&gt;Then I find myself fucked in the head.&lt;/p&gt;
    &lt;p&gt;And here's where we get to the point that I'm not sure I should be talking publicly about. Does this hurt my image a bit? Maybe. Do I look like I'm not cut for this? Potentially. But I've always appreciated when people share about the process rather than just talking about things in hindsight, and reflecting while things are happening + being super transparent publicly is how I am. You're witnessing my growth, live, as I type these words.&lt;/p&gt;
    &lt;p&gt;Anyway, so what happened is I found myself spending days with my head spinning, searching for ideas. I'm angry, I'm annoyed, and I'm not being super productive.&lt;/p&gt;
    &lt;p&gt;As I dug deeper into these feelings, I realized I was feeling pressured. We weren't making that much money, we weren't growing super fast. Then you look around and see "startup X gets to $1M ARR a month after launch" and shit like that and I'm feeling terrible about how we're barely growing. I'm thinking people that I really respect and admire have placed a bet on me and I'm letting them down.&lt;/p&gt;
    &lt;p&gt;Except they're not saying this, I am.&lt;/p&gt;
    &lt;p&gt;There's an interesting reflection that came up in a discussion between me and my girlfriend a few months prior that I realized applied to me, but in reverse. It's much more comfortable to be the person that "could be X" than to be the person that tries to actually do it. We were speaking about this regarding people who have a clear innate talent for something like music or sports but don't practice at all. Everyone says things like "you'd be the best at this if you just practiced more" but then they never do.&lt;/p&gt;
    &lt;p&gt;The thing is: it's a lot easier to live your life thinking you could have done X if you wanted to, than to "disappoint" these people that believed in you by trying and failing. You can always lean on this idea in your head of what you could have been, and how everyone believed in you so it must be true, but you just chose not to follow that path.&lt;/p&gt;
    &lt;p&gt;In my case, I found myself on the other side of that coin. Throughout my career, I've always had really high ownership roles, and have been actively involved in a couple 0 to 1 journeys. This led me throughout my career to get many comments about how great of a founder I'd be or how I have the "founder profile". I led teams, I wore a bunch of different hats, I worked hard as fuck, and I always thought about the big picture.&lt;/p&gt;
    &lt;p&gt;Those traits led my former bosses to then invest in me, and now suddenly I have to, in my head, live up to all of this. I can no longer take solace in some excuse like "I could have been a founder but working full-time was the best financial decision (it almost always is) so I never started my own thing". I set foot down a path from which there's no return. I've begun my attempt. I can of course stop and try again later. But from now on, I'm either gonna be a successful founder, or I'm not. And if I'm not, I'll have to deal with having broken with the expectations that people had of me.&lt;/p&gt;
    &lt;p&gt;There's a lot to unpack here, including what "success" means, and how most of what I say are other people's expectations are actually my own projected onto them (I've learned this about my relationship with my father too), but this post is already a bit too long so I'll save those for another time.&lt;/p&gt;
    &lt;p&gt;But the whole point here is not just that having raised this money from friends my head got a bit messy, but that I started to actually operate in a way that is counterproductive for my startup, while thinking I was actually doing what was best.&lt;/p&gt;
    &lt;p&gt;Ideas we considered when pivoting were looked more through a lens of "how big does this feel" rather than "what problem does this solve and for who". The slow growth was eating me, and while slow growth is terrible and can be a sign that you're on the wrong path it needs to be looked at from an objectively strategic lens. Didn't we say we were going to build an open source community and only later focus on monetization? Is that a viable strategy? Do we actually have a sound plan? Those were the things I should have been thinking about, rather than looping on "we need something that grows faster".&lt;/p&gt;
    &lt;p&gt;The people who invested in us, invested in us, not whatever idea we pitched them. And the best thing we can do is to follow our own process for building a great business based on what we believe and know, rather than focusing on making numbers look good so I can feel more relieved the next time I send over an investor update.&lt;/p&gt;
    &lt;p&gt;We have a ton to learn, particularly about sales (since we're both engineers), so we can't just be building shit for the sake of building shit because that's our comfort zone. But if our process is slower than company X on TechCrunch, that's fine. It's a marathon after all.&lt;/p&gt;
    &lt;p&gt;So after probably breaking many rules about what a founder should talk about publicly, what was my whole goal here? I mean, the main thing for me with posts like this is to get things off my chest. I've always said that the reason I publish writing that includes poems about my breakup, stories about falling in love, posts about my insecurities, and reflections about my dreams is that by there being the possibility of someone reading them (because technically it could be the case that nobody does) I can truly be who I really am in my day-to-day life. If I'm ok with there being the possibility of a friend I'll meet later today having read about how I felt during my last breakup, I can be myself with them without reservations, because I've made myself available to be seen. That's always been really freeing to me.&lt;/p&gt;
    &lt;p&gt;As a side effect, I'd hope that if this does get read by some people, particularly those starting or looking to start a business, that they can reflect about themselves, their lives, and their companies through listening to my story. I thought about writing a short bullet list about lessons I learned from raising money and dealing with its aftermath here, but honestly, that's best left to the reader to figure out. We're all different, and how one person reacts to a set of circumstances will differ from someone else. Some people don't feel pressure at all, or at least not from friends or investors. Or they only respond positively to pressure (because it certainly has benefits too). Maybe they're better off than me. Maybe they're not.&lt;/p&gt;
    &lt;p&gt;This is my story, after all. I wish you the best of luck with yours.&lt;/p&gt;
    &lt;p&gt;P.S. I'm doing good now. I'm motivated and sharp. If someone finds themselves in a similar situation, feel free to shoot me an email if you're keen to talk. Happy to go over what was useful for me, which fell outside of the scope of this post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.yakkomajuri.com/blog/raising-money-fucked-me-up"/><published>2026-01-17T18:29:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46660663</id><title>The thing that brought me joy</title><updated>2026-01-18T00:59:27.034626+00:00</updated><content/><link href="https://www.stephenlewis.me/blog/the-thing-that-brought-me-joy/"/><published>2026-01-17T18:42:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46661630</id><title>Docker.how – Docker command cheat sheet</title><updated>2026-01-18T00:59:26.921594+00:00</updated><content/><link href="https://docker.how/"/><published>2026-01-17T20:17:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46661897</id><title>A programming language based on grammatical cases of Turkish</title><updated>2026-01-18T00:59:26.586855+00:00</updated><content>&lt;doc fingerprint="4a813f1d0f75a8ca"&gt;
  &lt;main&gt;
    &lt;p&gt;Kip (meaning "grammatical mood" in Turkish) is an experimental programming language that uses Turkish grammatical cases as part of its type system. It demonstrates how natural language morphology—specifically Turkish noun cases and vowel harmony—can be integrated into programming language design.&lt;/p&gt;
    &lt;p&gt;This is a research/educational project exploring the intersection of linguistics and type theory, not a production programming language.&lt;/p&gt;
    &lt;p&gt;There is also a tutorial in Turkish and a tutorial in English that explains how to write Kip programs.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Kip is experimental. Expect changes in syntax and behavior over time.&lt;/p&gt;
    &lt;p&gt;For you to get a taste of what Kip looks like, here is an example program that prompts the user to enter a number and then prints that many of the Fibonacci numbers:&lt;/p&gt;
    &lt;code&gt;(* İlk n Fibonacci sayısını yazdırır. *)
(bu tam-sayıyı) (şu tam-sayıyı) (o tam-sayıyı) işlemek,
  (onla 0'ın eşitliği) doğruysa,
    durmaktır,
  yanlışsa,
    bunu yazıp,
    şunu (bunla şunun toplamını) (onla 1'in farkını) işlemektir.

çalıştırmak,
  "Bir sayı girin:" yazıp,
  isim olarak okuyup,
  ((ismin tam-sayı-hali)
    yokluksa,
      "Geçersiz sayı." yazmaktır,
    n'nin varlığıysa,
      0'ı 1'i n'yi işlemektir).

çalıştır.
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Language Features&lt;/item&gt;
      &lt;item&gt;Installation&lt;/item&gt;
      &lt;item&gt;Example Program&lt;/item&gt;
      &lt;item&gt;WASM Playground&lt;/item&gt;
      &lt;item&gt;Bytecode Cache&lt;/item&gt;
      &lt;item&gt;Project Structure&lt;/item&gt;
      &lt;item&gt;Testing&lt;/item&gt;
      &lt;item&gt;Morphological Analysis&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Kip uses Turkish noun cases (ismin halleri) to determine argument relationships in function calls:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Case&lt;/cell&gt;
        &lt;cell role="head"&gt;Turkish Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Suffix&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nominative&lt;/cell&gt;
        &lt;cell&gt;Yalın hal&lt;/cell&gt;
        &lt;cell&gt;(none)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sıfır&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Accusative&lt;/cell&gt;
        &lt;cell&gt;-i hali&lt;/cell&gt;
        &lt;cell&gt;-i, -ı, -u, -ü&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sayıyı&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Dative&lt;/cell&gt;
        &lt;cell&gt;-e hali&lt;/cell&gt;
        &lt;cell&gt;-e, -a&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sayıya&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Locative&lt;/cell&gt;
        &lt;cell&gt;-de hali&lt;/cell&gt;
        &lt;cell&gt;-de, -da, -te, -ta&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;listede&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Ablative&lt;/cell&gt;
        &lt;cell&gt;-den hali&lt;/cell&gt;
        &lt;cell&gt;-den, -dan, -ten, -tan&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;listeden&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Genitive&lt;/cell&gt;
        &lt;cell&gt;Tamlayan eki&lt;/cell&gt;
        &lt;cell&gt;-in, -ın, -un, -ün&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sayının&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Instrumental&lt;/cell&gt;
        &lt;cell&gt;-le eki&lt;/cell&gt;
        &lt;cell&gt;-le, -la, ile&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sayıyla&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Possessive (3s)&lt;/cell&gt;
        &lt;cell&gt;Tamlanan eki&lt;/cell&gt;
        &lt;cell&gt;-i, -ı, -u, -ü, -si, -sı&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ardılı&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Because Turkish cases mark grammatical relationships explicitly, Kip allows flexible argument ordering. These two calls are equivalent:&lt;/p&gt;
    &lt;code&gt;(5'le 3'ün farkını) yaz.
(3'ün 5'le farkını) yaz.
&lt;/code&gt;
    &lt;p&gt;As long as arguments have different case suffixes or different types, Kip can determine which argument is which.&lt;/p&gt;
    &lt;p&gt;Define algebraic data types with Turkish syntax:&lt;/p&gt;
    &lt;code&gt;Bir doğruluk ya doğru ya da yanlış olabilir.

Bir doğal-sayı
ya sıfır
ya da bir doğal-sayının ardılı
olabilir.
&lt;/code&gt;
    &lt;p&gt;Type variables are supported for generic data structures:&lt;/p&gt;
    &lt;code&gt;Bir (öğe listesi)
ya boş
ya da bir öğenin bir öğe listesine eki
olabilir.
&lt;/code&gt;
    &lt;p&gt;Pattern match using the conditional suffix &lt;code&gt;-sa/-se&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;(bu doğruluğun) tersi,
  bu doğruysa, yanlış,
  yanlışsa, doğrudur.
&lt;/code&gt;
    &lt;p&gt;Supports nested pattern matching, binders, and wildcard patterns (&lt;code&gt;değilse&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;(bu doğal-sayının) kopyası,
  bu sıfırsa, sıfır,
  öncülün ardılıysa, öncülün ardılıdır.
&lt;/code&gt;
    &lt;p&gt;Define named constants with &lt;code&gt;diyelim&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sıfırın ardılına bir diyelim.
birin ardılına iki diyelim.
&lt;/code&gt;
    &lt;p&gt;Sequencing with &lt;code&gt;-ip/-ıp/-up/-üp&lt;/code&gt; suffixes and binding with &lt;code&gt;olarak&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;selamlamak,
  isim olarak okuyup,
  ("Merhaba "yla ismin birleşimini) yazmaktır.
&lt;/code&gt;
    &lt;p&gt;Integers (&lt;code&gt;tam-sayı&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arithmetic: &lt;code&gt;toplamı&lt;/code&gt;,&lt;code&gt;farkı&lt;/code&gt;,&lt;code&gt;çarpımı&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Comparison: &lt;code&gt;eşitliği&lt;/code&gt;,&lt;code&gt;küçüklüğü&lt;/code&gt;,&lt;code&gt;büyüklüğü&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Other: &lt;code&gt;öncülü&lt;/code&gt;,&lt;code&gt;sıfırlığı&lt;/code&gt;,&lt;code&gt;faktöriyeli&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Strings (&lt;code&gt;dizge&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;uzunluğu&lt;/code&gt;- length&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;birleşimi&lt;/code&gt;- concatenation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tam-sayı-hali&lt;/code&gt;- parse as integer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I/O:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;yazmak&lt;/code&gt;/&lt;code&gt;yaz&lt;/code&gt;- print to stdout&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;okumak&lt;/code&gt;/&lt;code&gt;oku&lt;/code&gt;- read from stdin&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;(* This is a comment *)
&lt;/code&gt;
    &lt;code&gt;5'i yaz.              (* Integer literal with case suffix *)
"merhaba"'yı yaz.     (* String literal with case suffix *)
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Foma - finite-state morphology toolkit&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;macOS: &lt;code&gt;brew install foma&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;Debian/Ubuntu: &lt;code&gt;apt install foma libfoma-dev&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;Fedora: &lt;code&gt;dnf install foma foma-devel&lt;/code&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;macOS: &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stack - Haskell build tool&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;See haskellstack.org&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;If you only want to explore the language, you can start with &lt;code&gt;stack exec kip&lt;/code&gt; after a successful build.&lt;/p&gt;
    &lt;p&gt;Clone this repository, then:&lt;/p&gt;
    &lt;code&gt;# Quick install (macOS/Linux)
chmod +x install.sh
./install.sh

# Or manual build
stack build&lt;/code&gt;
    &lt;p&gt;The TRmorph transducer is bundled at &lt;code&gt;vendor/trmorph.fst&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;# Start REPL
stack exec kip

# Execute a file
stack exec kip -- --exec path/to/file.kip

# Install to PATH
stack install&lt;/code&gt;
    &lt;p&gt;A browser playground build is available under &lt;code&gt;playground/&lt;/code&gt;. It compiles the
non-interactive runner (&lt;code&gt;kip-playground&lt;/code&gt;) to &lt;code&gt;wasm32-wasi&lt;/code&gt; and ships a small
HTML/JS harness that runs Kip in the browser.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;playground/README.md&lt;/code&gt; for prerequisites, toolchain setup, and build steps.&lt;/p&gt;
    &lt;p&gt;Kip stores a cached, type-checked version of each &lt;code&gt;.kip&lt;/code&gt; file in a sibling &lt;code&gt;.iz&lt;/code&gt; file. When you run a file again, Kip will reuse the &lt;code&gt;.iz&lt;/code&gt; cache if both the source and its loaded dependencies are unchanged.&lt;/p&gt;
    &lt;p&gt;If you want to force a fresh parse and type-check, delete the &lt;code&gt;.iz&lt;/code&gt; file next to the source.&lt;/p&gt;
    &lt;p&gt;Important&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;.iz&lt;/code&gt; files include a compiler hash. If the compiler changes, the cache is invalidated automatically.&lt;/p&gt;
    &lt;code&gt;(* Natural numbers *)
Bir doğal-sayı
ya sıfır
ya da bir doğal-sayının ardılı
olabilir.

(* Define some constants *)
sıfırın ardılına bir diyelim.
birin ardılına iki diyelim.
ikinin ardılına üç diyelim.

(* Addition function *)
(bu doğal-sayıyla) (şu doğal-sayının) toplamı,
  bu sıfırsa,
    şu,
  öncülün ardılıysa,
    (öncülle) (şunun ardılının) toplamıdır.

(* Print result *)
(ikiyle üçün toplamını) yaz.
&lt;/code&gt;
    &lt;code&gt;app/
└── Main.hs            - CLI entry point

src/
├── Kip/
│   ├── AST.hs         - Abstract syntax tree
│   ├── Cache.hs       - .iz cache handling
│   ├── Eval.hs        - Interpreter
│   ├── Parser.hs      - Parser
│   ├── Render.hs      - Pretty-printing with morphological inflection
│   └── TypeCheck.hs   - Type checker validating grammatical case usage
└── Language/
    └── Foma.hs        - Haskell bindings to Foma via FFI

lib/
├── giriş.kip          - Prelude module loaded by default
├── temel.kip           - Core types
├── temel-doğruluk.kip  - Boolean functions
├── temel-dizge.kip     - String functions
├── temel-etki.kip      - I/O primitives
├── temel-liste.kip     - List functions
└── temel-tam-sayı.kip  - Integer functions

tests/
├── succeed/            - Passing golden tests (.kip + .out + optional .in)
└── fail/               - Failing golden tests (.kip + .err)

vendor/
└── trmorph.fst        - TRmorph transducer
&lt;/code&gt;
    &lt;code&gt;stack test&lt;/code&gt;
    &lt;p&gt;Tests are in &lt;code&gt;tests/succeed/&lt;/code&gt; (expected to pass) and &lt;code&gt;tests/fail/&lt;/code&gt; (expected to fail).&lt;/p&gt;
    &lt;p&gt;Kip uses TRmorph for Turkish morphological analysis. When a word has multiple possible parses (e.g., "takası" could be "taka + possessive" or "takas + accusative"), Kip carries all candidates through parsing and resolves ambiguity during type checking.&lt;/p&gt;
    &lt;p&gt;For intentionally ambiguous words, use an apostrophe to force a specific parse: &lt;code&gt;taka'sı&lt;/code&gt; vs &lt;code&gt;takas'ı&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;See LICENSE file.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/kip-dili/kip"/><published>2026-01-17T20:44:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46662078</id><title>Show HN: ChunkHound, a local-first tool for understanding large codebases</title><updated>2026-01-18T00:59:26.078071+00:00</updated><content>&lt;doc fingerprint="18c0feb517868f1a"&gt;
  &lt;main&gt;
    &lt;p&gt;Local first codebase intelligence&lt;/p&gt;
    &lt;p&gt;Your AI assistant searches code but doesn't understand it. ChunkHound researches your codebase—extracting architecture, patterns, and institutional knowledge at any scale. Integrates via MCP.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;cAST Algorithm - Research-backed semantic code chunking&lt;/item&gt;
      &lt;item&gt;Multi-Hop Semantic Search - Discovers interconnected code relationships beyond direct matches&lt;/item&gt;
      &lt;item&gt;Semantic search - Natural language queries like "find authentication code"&lt;/item&gt;
      &lt;item&gt;Regex search - Pattern matching without API keys&lt;/item&gt;
      &lt;item&gt;Local-first - Your code stays on your machine&lt;/item&gt;
      &lt;item&gt;30 languages with structured parsing &lt;list rend="ul"&gt;&lt;item&gt;Programming (via Tree-sitter): Python, JavaScript, TypeScript, JSX, TSX, Java, Kotlin, Groovy, C, C++, C#, Go, Rust, Haskell, Swift, Bash, MATLAB, Makefile, Objective-C, PHP, Vue, Svelte, Zig&lt;/item&gt;&lt;item&gt;Configuration: JSON, YAML, TOML, HCL, Markdown&lt;/item&gt;&lt;item&gt;Text-based (custom parsers): Text files, PDF&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;MCP integration - Works with Claude, VS Code, Cursor, Windsurf, Zed, etc&lt;/item&gt;
      &lt;item&gt;Real-time indexing - Automatic file watching, smart diffs, seamless branch switching&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Visit chunkhound.github.io for complete guides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3.10+&lt;/item&gt;
      &lt;item&gt;uv package manager&lt;/item&gt;
      &lt;item&gt;API keys (optional - regex search works without any keys) &lt;list rend="ul"&gt;&lt;item&gt;Embeddings: VoyageAI (recommended) | OpenAI | Local with Ollama&lt;/item&gt;&lt;item&gt;LLM (for Code Research): Claude Code CLI or Codex CLI (no API key needed) | Anthropic | OpenAI&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Install uv if needed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install ChunkHound
uv tool install chunkhound&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create &lt;code&gt;.chunkhound.json&lt;/code&gt;in project root&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  "embedding": {
    "provider": "voyageai",
    "api_key": "your-voyageai-key"
  },
  "llm": {
    "provider": "claude-code-cli"
  }
}&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Note: Use&lt;/p&gt;&lt;code&gt;"codex-cli"&lt;/code&gt;instead if you prefer Codex. Both work equally well and require no API key.&lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Index your codebase&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;chunkhound index&lt;/code&gt;
    &lt;p&gt;For configuration, IDE setup, and advanced usage, see the documentation.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Approach&lt;/cell&gt;
        &lt;cell role="head"&gt;Capability&lt;/cell&gt;
        &lt;cell role="head"&gt;Scale&lt;/cell&gt;
        &lt;cell role="head"&gt;Maintenance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Keyword Search&lt;/cell&gt;
        &lt;cell&gt;Exact matching&lt;/cell&gt;
        &lt;cell&gt;Fast&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Traditional RAG&lt;/cell&gt;
        &lt;cell&gt;Semantic search&lt;/cell&gt;
        &lt;cell&gt;Scales&lt;/cell&gt;
        &lt;cell&gt;Re-index files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Knowledge Graphs&lt;/cell&gt;
        &lt;cell&gt;Relationship queries&lt;/cell&gt;
        &lt;cell&gt;Expensive&lt;/cell&gt;
        &lt;cell&gt;Continuous sync&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ChunkHound&lt;/cell&gt;
        &lt;cell&gt;Semantic + Regex + Code Research&lt;/cell&gt;
        &lt;cell&gt;Automatic&lt;/cell&gt;
        &lt;cell&gt;Incremental + realtime&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Ideal for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large monorepos with cross-team dependencies&lt;/item&gt;
      &lt;item&gt;Security-sensitive codebases (local-only, no cloud)&lt;/item&gt;
      &lt;item&gt;Multi-language projects needing consistent search&lt;/item&gt;
      &lt;item&gt;Offline/air-gapped development environments&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Stop recreating code. Start with deep understanding.&lt;/p&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/chunkhound/chunkhound"/><published>2026-01-17T21:03:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46662401</id><title>If writing the code is the easy part, why would I want someone else to write it?</title><updated>2026-01-18T00:59:25.812850+00:00</updated><content>&lt;doc fingerprint="e744dfdc38031ffb"&gt;
  &lt;main&gt;
    &lt;p&gt;This week I wrote an issue on tldraw's repository about a new contributions policy. Due to an influx of low-quality AI pull requests, we would soon begin automatically closing pull requests from external contributors.&lt;/p&gt;
    &lt;p&gt;I was expecting to have to defend my decision but the response has been surprisingly positive: the problem is real, tldraw's solution seems reasonable, maybe we should do the same.&lt;/p&gt;
    &lt;p&gt;The post prompted some good conversation though on Twitter and Hacker News. Much of the discussion focused on recognizing AI code, using AI to evaluate pull requests, and generally preventing people from using AI tools to contribute.&lt;/p&gt;
    &lt;p&gt;I think this misses the point. We already accept code written with AI. I write code with AI tools. I expect my team to use AI tools too. If you know the codebase and know what you're doing, writing great code has never been easier than with these tools.&lt;/p&gt;
    &lt;p&gt;The question is more fundamental. In a world of AI coding assistants, is code from external contributors actually valuable at all?&lt;/p&gt;
    &lt;p&gt;If writing the code is the easy part, why would I want someone else to write it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Context and contribution&lt;/head&gt;
    &lt;p&gt;Every repository gets bad pull requests. A few years ago I submitted a full TypeScript rewrite of a text editor because I thought it would be fun. I hope the maintainers didn't read it. Sorry.&lt;/p&gt;
    &lt;p&gt;One of my first big open source contributions was implementing more arrowheads for Excalidraw. I wrote the code and submitted a PR. The next day, the maintainers kindly pointed me toward their issues-first policy and closed my request.&lt;/p&gt;
    &lt;p&gt;I stuck with it, though. I was sketching state charts, I wanted the little dots on my arrows, and I cared enough to make it happen. So I stayed engaged and got caught up on the history of the problem. It turned out not to be an implementation question but a design problem. How do we let a user pick the start and end arrowheads? Which components can be adapted, which need to be created? Do we need new icons?&lt;/p&gt;
    &lt;p&gt;It took a few rounds of constructive iterations, moved along by my research and design work, before we all aligned on a solution. I wrote a new PR, we landed it, and now you can put a little dot or whatever on the end of your Excalidraw arrows.&lt;/p&gt;
    &lt;p&gt;If this were happening today, what would be different?&lt;/p&gt;
    &lt;p&gt;We would still need to have discussed the problem and designed a solution. It would have still required the sustained attention of a contributor who cared and wanted to see the change go in. My prototypes would still be useful for research and design but their cost to produce would be lower. I would have made more of them.&lt;/p&gt;
    &lt;p&gt;Once we had the context we needed and the alignment on what we would do, the final implementation would have been almost ceremonial. Who wants to push the button?&lt;/p&gt;
    &lt;head rend="h2"&gt;Eternal Sloptember&lt;/head&gt;
    &lt;p&gt;We'd started getting pull requests that purported to fix reported issues but which were, in retrospect, obvious "fix this issue" one-shots by an author using AI coding tools. Without broader knowledge of the codebase or the purpose of the project, the AI agents were taking the issue at face value and producing a diff. Any problems with the issue were multiplied in the pull request, leading to some of the strangest PRs I'd ever seen.&lt;/p&gt;
    &lt;p&gt;These pull requests would have been incredible a few months prior. They all looked good. They were formally correct. Tests and checks passed. We'd even started landing some. Then I started to notice unusual patterns. Authors almost always ignored our PR template. Even large PRs would be abandoned, languishing because their authors had neglected to sign our CLA. Commits would be spaced out strangely, with too-brief gaps between each commit. Checking into the authors revealed dozens of PRs across dozens of repositories. And why not?&lt;/p&gt;
    &lt;p&gt;Many other problematic contributions were more obvious by sheer weirdness. Authors would solve a problem in a way that ignored existing patterns, inlined other parts of the codebase, or went hard into a random direction. Once or twice, I would begin fixing and cleaning up these PRs, often asking my own Claude to make fixes that benefited from my wider knowledge: use this helper, use our existing UI components, etc. All the while thinking that it would have been easier to vibe code this myself.&lt;/p&gt;
    &lt;p&gt;From the discussions this week, it sounds like this is the default experience of every public repository maintainer right now. I could be describing any repo that takes external contributions. To use Excalidraw as another example: they received more than twice as many PRs in Q4 of 2025 than in Q3. While we have much less contribution, our problem recently got worse in a way that forced my hand.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stay away from my trash!&lt;/head&gt;
    &lt;p&gt;More recently, we started getting PRs that were better-formed but still so far off-base that I knew something had changed. These were pull requests that claimed to solve a problem we didn't have or fix a bug that didn't exist. Each was claiming to close an issue.&lt;/p&gt;
    &lt;p&gt;A glance at the linked issue confirmed the problem: one of my own AI scripts, a Claude Code &lt;code&gt;/issue&lt;/code&gt; command, was giving bad directions.&lt;/p&gt;
    &lt;p&gt;As a high-powered tech CEO, I'm constantly running into bugs and small UX nits that I don't have time to fully document but do want to capture for later. Previously, I would fire these into Linear or our issues as empty ticketsâ"fix bug in sidebar"âthen hope I remembered what exactly it was when I came back to it later.&lt;/p&gt;
    &lt;p&gt;To help with this, I'd created a &lt;code&gt;/issue&lt;/code&gt; command that created well-formed issues out of these low-specificity inputs. I would run into a bug and write something quickly like &lt;code&gt;/issue dot menu should stay visible when the menu is open sidebar only desktop&lt;/code&gt;, fire off Claude Code to try and figure it out, and continue with my work. I'd get an issue, then a follow-up reply with a root cause analysis for bugs or suggested implementation.&lt;/p&gt;
    &lt;p&gt;As an example, I'm pretty sure my input for this one was something like &lt;code&gt;/issue you know that paint bucket in google docs i want that for tldraw so that I can copy styles from one shape and paste it to another, if those styles exist in the other shape. i want to like slurp up the styles&lt;/code&gt;. It was probably fewer. "Slurp" sounds like me.&lt;/p&gt;
    &lt;p&gt;When it workedâwhen the issue was obvious or my input was detailed enoughâClaude would go off and make a well-researched issue that was ready to be solved. I'd often follow up with another command to &lt;code&gt;/take&lt;/code&gt; the issue and take a shot at implementing it. But if the issue was complex or my input contained too little information (or if I drew an unlucky seed) then the AI would head off in the wrong direction and produce an issue with an imagined bug or junk solution. I'd close the issue, spend more time refining it, or sometimes decide that the idea was wrong to begin with.&lt;/p&gt;
    &lt;p&gt;In this system, slop is lubrication. My old "fix button" tickets were poor quality but useful. They added entropy and noise to the system but were worth it to capture a certain type of bug or idea. This new system is a similar exchange, introducing noise in exchange for other things. What's different is that this noise is well-formed noise. To an outsider, the result of my fire-and-forget "fix button" might look identical to a professional, well-researched, intellectually serious bug report.&lt;/p&gt;
    &lt;p&gt;In the past, my awful issues would have been ignored. The issues looked wrong and the effort required to produce a fix would have fallen on the author. The noise was never a problem because no one would have tried to write a high-effort diff based on such a low-effort issue.&lt;/p&gt;
    &lt;p&gt;AI changed all of that. My low-effort issues were becoming low-effort pull requests, with AI doing both sides of the work. My poor Claude had produced a nonsense issue causing the contributor's poor Claude to produce a nonsense solution.&lt;/p&gt;
    &lt;p&gt;The thing is, my shitty AI issue was providing value. The contributor's shitty AI solution was not. There was a piece in betweenâa part of the process I would have done but the contributor could not have doneâwhich was to read the issue and decide whether it made sense. Instead, I'd accidentally put out a call for pointless contribution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where this leaves us&lt;/head&gt;
    &lt;p&gt;This leaves us at a place where the best thing to do for our codebase is to shut down external contributions, at least until GitHub offers better support for controlling who can contribute, when, and where. Assuming the social contract of code contribution remains unchanged, then we need better tools to maintain the peace.&lt;/p&gt;
    &lt;p&gt;But if you ask me, the bigger threat to GitHub's model comes from the rapid devaluation of someone else's code. When code was hard to write and low-effort work was easy to identify, it was worth the cost to review the good stuff. If code is easy to write and bad work is virtually indistinguishable from good, then the value of external contribution is probably less than zero.&lt;/p&gt;
    &lt;p&gt;If that's the case, which I'm starting to think it is, then it's better to limit community contribution to the places it still matters: reporting, discussion, perspective, and care. Don't worry about the code, I can push the button myself.&lt;/p&gt;
    &lt;p&gt;Â© 2025 tldraw&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tldraw.dev/blog/stay-away-from-my-trash"/><published>2026-01-17T21:40:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46662662</id><title>Light Mode InFFFFFFlation</title><updated>2026-01-18T00:59:25.638326+00:00</updated><content>&lt;doc fingerprint="7ca4d0675e37ffd9"&gt;
  &lt;main&gt;
    &lt;p&gt;Back in the day, light mode wasn’t called “light mode”. It was just the way that computers were, we didn’t really think about turning everything light or dark. Sure, some applications were often dark (photo editors, IDEs, terminals) but everything else was light, and that was fine.&lt;/p&gt;
    &lt;p&gt;What we didn’t notice is that light mode has been slowly getting lighter, and I’ve got a graph to prove it. I did what any normal person would do, I downloaded the same (or similar) screenshots from the MacOS Screenshot Library on 512 Pixels. This project would have been much more difficult without a single place to get well-organised screenshots from. I cropped each image so just a representative section of the window was present, here shown with a pinkish rectangle:&lt;/p&gt;
    &lt;p&gt;Then used Pillow to get the average lightness of each cropped image:&lt;/p&gt;
    &lt;code&gt;for file in sorted(os.listdir('.')):
  image = Image.open(file)
  greyscale = image.convert('L')
  stat = ImageStat.Stat(greyscale)
  avg_lightness = int(stat.mean[0])
  print(f"{file}\t{avg_lightness}")
&lt;/code&gt;
    &lt;p&gt;This ignores any kind of perceived brightness or the tinting that MacOS has been doing for a while based on your wallpaper colour. I could go down a massive tangent trying to work out exactly what the best way to measure this is, but given that the screenshots aren’t perfectly comparable between versions, comparing the average brightness of a greyscale image seems reasonable.&lt;/p&gt;
    &lt;p&gt;I graphed that on the release year of each OS version, doing the same for dark mode:&lt;/p&gt;
    &lt;p&gt;You can clearly see that the brightness of the UI has been steadily increasing for the last 16 years. The upper line is the default mode/light mode, the lower line is dark mode. When I started using MacOS in 2012, I was running Snow Leopard, the windows had an average brightness of 71%. Since then they’ve steadily increased so that in MacOS Tahoe, they’re at a full 100%.&lt;/p&gt;
    &lt;p&gt;What I’ve graphed here is just the brightness of the window chrome, which isn’t really representative of the actual total screen brightness. A better study would be looking at the overall brightness of a typical set of apps. The default background colour for windows, as well as the colours for inactive windows, would probably give a more complete picture.&lt;/p&gt;
    &lt;p&gt;For example, in Tahoe the darkest colour in a typical light-mode window is the colour of a section in an inactive settings window, at 97% brightness. In Snow Leopard the equivalent colour was 90%, and that was one of the brightest parts of the window, since the window chrome was typically darker than the window content.&lt;/p&gt;
    &lt;p&gt;I tried to remember exactly when I started using dark mode all the time on MacOS. I’ve always used a dark background for my editor and terminal, but I wasn’t sure when I swapped the system theme across. When it first came out I seem to remember thinking that it looked gross.&lt;/p&gt;
    &lt;p&gt;It obviously couldn’t be earlier than 2018, as that’s when dark mode was introduced in MacOS Mojave. I’m pretty sure that when I updated my personal laptop to an M1 MacBook Air at the end of 2020 that I set it to use dark mode. This would make sense, because the Big Sur update bumped the brightness from 85% to 97%, which probably pushed me over the edge.&lt;/p&gt;
    &lt;p&gt;I think the reason this happens is that if you look at two designs, photos, or whatever, it’s really easy to be drawn in to liking the brighter one more. Or if they’re predominantly dark, then the darker one. I’ve done it myself with this very site. If I’m tweaking the colours it’s easy to bump up the brightness on the background and go “ooh wow yeah that’s definitely cleaner”, then swap it back and go “ewww it looks like it needs a good scrub”. If it’s the dark mode colours, then a darker background will just look cooler.&lt;/p&gt;
    &lt;p&gt;I’m not a designer, but I assume that resisting this urge is something you learn in design school. Just like making a website look good with a non-greyscale background.&lt;/p&gt;
    &lt;p&gt;This year in iOS 26, some UI elements use the HDR screen to make some elements and highlights brighter than 100% white.1 This year it’s reasonably subtle, but the inflation potential is there. If you’ve ever looked at an HDR photo on an iPhone (or any other HDR screen) then looked at the UI that’s still being shown in SDR, you’ll know just how grey and sad it looks. If you’re designing a new UI, how tempting will it be to make just a little bit more of it just a little bit brighter?&lt;/p&gt;
    &lt;p&gt;As someone whose job involves looking at MacOS for a lot of the day, I find that I basically have to use dark mode to avoid looking at a display where all the system UI is 100% white blasting in my eyes. But the alternative doesn’t have to be near-black for that, I would happily have a UI that’s a medium grey. In fact what I’ve missed since swapping to using dark mode is that I don’t have contrast between windows. Everything looks the same, whether it’s a text editor, IDE, terminal, web browser, or Finder window. All black, all the time.&lt;/p&gt;
    &lt;p&gt;Somewhat in the spirit of Mavericks Forever2, if I were to pick an old MacOS design to go back to it would probably be Yosemite. I don’t have any nostalgia for skeuomorphic brushed metal or stitched leather, but I do quite like the flattened design and blur effects that Yosemite brought. Ironically Yosemite was a substantial jump in brightness from previous versions.&lt;/p&gt;
    &lt;p&gt;So if you’re making an interface or website, be bold and choose a 50% grey. My eyes will thank you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://willhbr.net/2025/10/20/light-mode-infffffflation/"/><published>2026-01-17T22:19:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46663338</id><title>If you put Apple icons in reverse it looks like someone getting good at design</title><updated>2026-01-18T00:59:25.121330+00:00</updated><link href="https://www.threads.com/@heliographe.studio/post/DTeOwAykwQ1"/><published>2026-01-17T23:47:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46663341</id><title>OpenAI could reportedly run out of cash by mid-2027</title><updated>2026-01-18T00:59:24.826517+00:00</updated><content>&lt;doc fingerprint="2631981438271ead"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenAI could reportedly run out of cash by mid-2027 — analyst paints grim picture after examining the company's finances&lt;/head&gt;
    &lt;p&gt;The circular economy might spiral into Sam Altman's garrote.&lt;/p&gt;
    &lt;p&gt;Given how quickly the evolution of AI has upended technology across the globe and is affecting various markets, it's nigh impossible to accurately predict where anything might be headed. There's no shortage of predictions, ranging from utopia to ultimate doom for established industries. An NYT columnist, however, has one specific bet: OpenAI will be destitute in 18 months in the wake of its AI endeavors.&lt;/p&gt;
    &lt;p&gt;According to an external report last year, OpenAI was projected to burn through $8 billion in 2025, rising to $40 billion in 2028. Given that the company reportedly predicts profitability by 2030, it's not hard to do the math.&lt;/p&gt;
    &lt;p&gt;Altman's venture projects spending $1.4 trillion on datacenters. As Sebastian Mallaby, an economist at the Council on Foreign Relations, notes, even if OpenAI rethinks those limerence-influenced promises and "pays for others with its overvalued shares", there's still a financial chasm to cross. Mallaby isn't the only one thinking along these lines, as Bain &amp;amp; Company reported last year that, even with the best outlook, there's at least a $800 billion black hole in the industry.&lt;/p&gt;
    &lt;p&gt;The financial guru contextualizes the situation adeptly, broadly stating that it's not a matter of whether end-user AI will become technologically entrenched, but rather whether the economics of developing it will make sense in the mid- to long-term.&lt;/p&gt;
    &lt;p&gt;The analyst points out that in theory, investors would "bridge the gap between the emergence of a great technology and eventual profits", except that many AI companies seem to be burning cash far faster than they can generate income. Mallaby remarks that the newcomers are in a much worse position than "legacy" companies like Microsoft or Meta, given that the old-timers already had money-making businesses before AI came about and can (literally) afford to wait out the necessary period until the clankers deliver the fruits.&lt;/p&gt;
    &lt;p&gt;According to him, the majority of people are using free services and have no qualms switching to a competitor once their usual bot adds ads or usage limits, a fact corroborated by the myriad options available right now for all sorts of tasks.&lt;/p&gt;
    &lt;p&gt;He sees this as a temporary problem for AI providers, though, as agentic AI becomes more entrenched in daily people's lives, it'll become harder to switch, as the bots should eventually have all your shopping preferences, aspirations, and emotional profile mapped out —perhaps even better than yourself.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Mallaby does praise OpenAI's CEO Sam Altman's dollar-attracting gravitational field that raised $40 billion in investment, an amount bigger than any other private funding round in history — even more than Saudi Aramco's $30 billion. The difference is that Aramco, along with other IPO'd enterprises, had a business model and profitability, neither of which OpenAI currently enjoys.&lt;/p&gt;
    &lt;p&gt;The AI financial ouroboros certainly looks poised to eat its own tail, but there's an argument that the ophidian will only lose its newer part. There would be some irony in the AI market losing one or more of the players that started it all.&lt;/p&gt;
    &lt;p&gt;Bruno Ferreira is a contributing writer for Tom's Hardware. He has decades of experience with PC hardware and assorted sundries, alongside a career as a developer. He's obsessed with detail and has a tendency to ramble on the topics he loves. When not doing that, he's usually playing games, or at live music shows and festivals.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;justaboutenuf&lt;/header&gt;Reply&lt;quote/&gt;they're expecting a 5 trillion dollar bailout ... oh wait thats just for navidia, make it 50 trillion all in. they'll have their cake and eat it too. memory prices will finally crater right after the depression kicks in ... and havva nice day everyone ...Admin said:OpenAI might be running out of cash as soon as mid-2027&lt;lb/&gt;OpenAI could reportedly run out of cash by mid-2027 — analyst paints grim picture after examining the company's finances : Read more&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;TerryLaze&lt;/header&gt;Reply&lt;quote/&gt;Nvidia has 60bil in cash right now (last quarter) , not in products that they have to sell for them to make that money (which is what they would loose if the bubble would pop) outright cash they have right now.justaboutenuf said:they're expecting a 5 trillion dollar bailout ... oh wait thats just for navidia, make it 50 trillion all in. they'll have their cake and eat it too. memory prices will finally crater right after the depression kicks in ... and havva nice day everyone ...&lt;lb/&gt;They would not qualify for a bail out.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;bit_user&lt;/header&gt;Wow, Bruno Ferreira gettin' downright literary on us!Reply&lt;lb/&gt;: D&lt;lb/&gt;Had to reach for the wiktionary, a couple times:&lt;lb/&gt;garrote - Something, especially a cord or wire, used for strangulation.&lt;lb/&gt;limerence - An involuntary romantic infatuation with another person, especially combined with an overwhelming, obsessive need to have one's feelings reciprocated.&lt;lb/&gt;ouroboros - (mythology) A serpent, dragon or worm that eats its own tail, a representation of the continuous cycle of life and death. (I did know this one)&lt;lb/&gt;ophidian - Of or pertaining to the suborder Serpentes; of, related to, or characteristic of a snake or serpent.&lt;lb/&gt;I like it!&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;bit_user&lt;/header&gt;Reply&lt;quote/&gt;Altman hinted at that, recently.justaboutenuf said:they're expecting a 5 trillion dollar bailout ...&lt;lb/&gt;https://www.tomshardware.com/tech-industry/sam-altman-distances-openai-from-data-center-bailout-talkI think the politicians won't go for it, after all the flak they caught from the 2008-era bank bailouts. Even though the AI industry crashing would be bad, I think it's not nearly as dire as a banking collapse would've been. The prospect of that happening was truly apocalyptic.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;-Fran-&lt;/header&gt;Mid-2027? That's actually later than I thought.Reply&lt;lb/&gt;I read once: "it's really easy to make dangerous bet when it's not your money". Sam Altman is doing exactly that and some are starting to realize the bet was horrible.&lt;lb/&gt;Regards.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;DougMcC&lt;/header&gt;mid-2027 is fine for openAI, though, because they anticipate agi solving their financing problem before that. If they don't achieve AGI on that timeline, then, yes, they will be screwed. They are either right that they are on the right path, or wrong.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Nomadish&lt;/header&gt;Im surprised only because 18 months seems too long. Sam Altman is a liar, he has a long history of it and no one should have trusted him to begin with. Remember loopt? Remember reddit?Reply&lt;lb/&gt;I just hope this Ponzi scheme dies before the pc market does.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;justaboutenuf&lt;/header&gt;Reply&lt;quote/&gt;and the lawbreaking banks didn't qualify for bailouts in 2009 either ... jenson isn't donating to trump for nothing ... watch what happens ... its more sam altman and the data center boys i'm talking about anyways ... and btw i believe its "navidia" lolTerryLaze said:Nvidia has 60bil in cash right now (last quarter) , not in products that they have to sell for them to make that money (which is what they would loose if the bubble would pop) outright cash they have right now.&lt;lb/&gt;They would not qualify for a bail out.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;justaboutenuf&lt;/header&gt;Reply&lt;quote/&gt;they own the politicians ... they will do as they're told ... point being they will get their dream buildout no expense spared for free with our taxes ... ai told ya sobit_user said:Altman hinted at that, recently.&lt;lb/&gt;https://www.tomshardware.com/tech-industry/sam-altman-distances-openai-from-data-center-bailout-talkI think the politicians won't go for it, after all the flak they caught from the 2008-era bank bailouts. Even though the AI industry crashing would be bad, I think it's not nearly as dire as a banking collapse would've been. The prospect of that happening was truly apocalyptic.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;bit_user&lt;/header&gt;Reply&lt;quote/&gt;Only to a point. Politicians know how angry the bank bailouts made people. It was one of the rallying cries of the Teaparty movement and helped fuel the flipping of Congress in 2010.justaboutenuf said:they own the politicians ... they will do as they're told&lt;lb/&gt;Basically, if they think their chances at re-election will be worse if they pass a bailout, then they won't do it.&lt;lb/&gt;Also, I just want to point out that most people got the wrong idea about the bank bailouts. By the time things reached the point where the bailout happened, there was no realistic alternative - the bailouts were the least bad option! The corrupt part about that whole mess was the deregulation that allowed the financial system get to the point where it was dangling at a cliff edge.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/tech-industry/big-tech/openai-could-reportedly-run-out-of-cash-by-mid-2027-nyt-analyst-paints-grim-picture-after-examining-companys-finances"/><published>2026-01-17T23:47:45+00:00</published></entry></feed>