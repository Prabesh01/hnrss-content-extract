<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-17T14:09:59.329450+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45265745</id><title>Denmark close to wiping out cancer-causing HPV strains after vaccine roll-out</title><updated>2025-09-17T14:10:09.329599+00:00</updated><content>&lt;doc fingerprint="2714216008bd02a0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Denmark close to wiping out leading cancer-causing HPV strains after vaccine roll-out&lt;/head&gt;
    &lt;p&gt;A nationwide study suggests infections with human papillomavirus (HPV) types 16 and 18 have been virtually eliminated since vaccination began in 2008 – protecting even unvaccinated women.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2 September 2025&lt;/item&gt;
      &lt;item&gt;3 min read&lt;/item&gt;
      &lt;item&gt;by Linda Geddes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Denmark has effectively eliminated infections with the two biggest cancer-causing strains of human papillomavirus (HPV) since the vaccine was introduced in 2008, data suggests.&lt;/p&gt;
    &lt;p&gt;The research, published in Eurosurveillance, could have implications for how vaccinated populations are screened in the coming years – particularly as people increasingly receive vaccines that protect against multiple high-risk types of HPV virus.&lt;/p&gt;
    &lt;head rend="h3"&gt;Deadly cancer&lt;/head&gt;
    &lt;p&gt;After breast cancer, cervical cancer is the most common type of cancer among women aged 15 to 44 years in Europe, and human papillomavirus (HPV) is the leading cause.&lt;/p&gt;
    &lt;p&gt;At least 14 high-risk types of the virus have been identified, and before Denmark introduced the HPV vaccine in 2008, HPV types 16 and 18 accounted for around three quarters (74%) of cervical cancers in the country.&lt;/p&gt;
    &lt;p&gt;Initially, girls were offered a vaccine that protected against four types of HPV: 16, 18, plus the lower risk types 6 and 11. However, since 2017, Danish girls have been offered a vaccine that protects against nine types of HPV – including those accounting for approximately 90% of cervical cancers.&lt;/p&gt;
    &lt;head rend="h4"&gt;Have you read?&lt;/head&gt;
    &lt;p&gt;To better understand the impact that these vaccination programmes have had on HPV prevalence as vaccinated girls reach cervical screening age (23 to 64 years in Denmark), Dr Mette Hartmann Nonboe at Zealand University Hospital in Nykøbing Falster and colleagues analysed up to three consecutive cervical cell samples collected from Danish women between 2017 and 2024, when they were 22 to 30 years of age.&lt;/p&gt;
    &lt;p&gt;“In 2017, one of the first birth cohorts of women in Denmark who were HPV-vaccinated as teenage girls in 2008 reached the screening age of 23 years,” Nonboe explained.&lt;/p&gt;
    &lt;p&gt;“Compared with previous generations, these women are expected to have a considerably lower risk of cervical cancer, and it is pertinent to assess [their] future need for screening.”&lt;/p&gt;
    &lt;head rend="h3"&gt;High-risk HPV elimination&lt;/head&gt;
    &lt;p&gt;The research found that infection with the high-risk HPV types (HPV16/18) covered by the vaccine has been almost eliminated.&lt;/p&gt;
    &lt;p&gt;“Before vaccination, the prevalence of HPV16/18 was between 15 and 17%, which has decreased in vaccinated women to less than one percent by 2021,” the researchers said.&lt;/p&gt;
    &lt;p&gt;In addition, prevalence of HPV types 16 and 18 in women who had not been vaccinated against HPV was five percent. This strongly suggests that the vaccine has reduced the circulation of these HPV types in general population, to the extent that even unvaccinated women are now less likely to be infected with them – so called “population immunity” – the researchers said.&lt;/p&gt;
    &lt;p&gt;Despite this good news, roughly one third of women screened during the study period still had infection with high-risk HPV types not covered by the original vaccines – and new infections with these types were more frequent among vaccinated women, compared to unvaccinated ones.&lt;/p&gt;
    &lt;p&gt;This is expected to fall once girls who received the more recent ‘nine-valent’ vaccine reach screening age. At this point, the screening guidelines should potentially be reconsidered, Nonboe and colleagues said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.gavi.org/vaccineswork/denmark-close-wiping-out-leading-cancer-causing-hpv-strains-after-vaccine-roll-out"/><published>2025-09-16T18:12:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45266039</id><title>How to make the Framework Desktop run even quieter</title><updated>2025-09-17T14:10:07.597500+00:00</updated><content>&lt;doc fingerprint="1dfb18b60853d5bb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How to make the Framework Desktop run even quieter&lt;/head&gt;
    &lt;head rend="h2"&gt;How to make the Framework Desktop run even quieter&lt;/head&gt;
    &lt;p&gt;Not so long ago, the compact, small form factor PC segment witnessed a significant refreshment with the launch of the Framework desktop PC. If you've missed it and are wondering why this mini-PC is considered so special, it's because it was the first desktop PC to utilise the AMD Ryzen AI Max APU, a processor previously exclusive to laptops.&lt;/p&gt;
    &lt;p&gt;The AMD Ryzen AI Max processor stands out for its exceptional speed and integrated graphics performance, frequently surprising users with its gaming capabilities, even on demanding titles. Users often highlight its powerful integrated GPU, which can leverage a massive memory pool (up to 96GB for AI tasks), allowing it to efficiently handle complex AI and deep learning workloads. The raw performance delivered by this chip makes it a worthwhile choice for intensive tasks and creative workflows.&lt;/p&gt;
    &lt;p&gt;As a collaborator and partner on the Framework Desktop mini-PC project, our first steps involved integrating our NF-A12x25 fan and a fan duct. This way, we could significantly reduce system noise levels while ensuring safe operating temperatures – you can read more about this here. But can the Framework Desktop be made even quieter? We wanted to leave no stone unturned to find out, so we took it a step further by trying to integrate our signature Noctua fan grill design that debuted on the Seasonic Prime TX-1600 Noctua Edition power supply.&lt;/p&gt;
    &lt;p&gt;It must be noted that customer safety and EMC requirements for the mini PC, a standalone electrical item, differ from those for hardware components (such as the PSU) designed to be inside a PC case. The safety standard suggests that ventilation openings on case side panels need to be less than 5mm in diameter. To comply with safety regulations, we created an updated version of the original fan grille as implemented on the Seasonic Prime TX 1600 Noctua Edition power supply featuring more struts and a smaller opening size, ensuring full adherence to these standards. To complement the new grille design, we have also designed a custom, funnel shaped fan duct that makes maximum use of the outermost openings of the custom side panel.&lt;/p&gt;
    &lt;p&gt;In combination, the custom side panel and duct design provided a massive noise reduction compared to the stock configuration, particularly in lower fan speed ranges. We have measured around 7 dB(A) lower noise levels at around 50% fan speed, and up to 5 dB(A) lower at higher fan speeds, when compared at the same APU operating temperature.&lt;/p&gt;
    &lt;p&gt;While the custom side panel with our signature Noctua grill as well as the custom fan duct are not slated for mass production at this point, we are more than happy to share the 3D CAD files for everyone who is looking to make their Framework Desktop run even quieter.&lt;/p&gt;
    &lt;p&gt;Both the custom side panel and the customised fan ductare available to download at Printables.com for you to 3D-print at home:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Custom side panel with Noctua-style fan grill&lt;/item&gt;
      &lt;item&gt;Custom fan duct to make best use of the custom side panel&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fortunately, the quality of 3D printing technologies has advanced so much that you can end up with a nice, clean side panel, which will additionally optimise the sound profile, or bring your APU temperatures down significantly.&lt;/p&gt;
    &lt;p&gt;In addition to redesigning and testing the Noctua fan grill, we also evaluated various other scenarios. These included replacing the NF-A12x25 with its G2 variant and incorporating an additional 8cm fan for exhaust purposes. The findings of these tests may prove surprising. The additional NF-A8 PWM fan, which was added as an exhaust fan at the front of the case, yielded slightly lower temperatures, but at the expense of extra noise emission, so it’s not a setup that we would recommend from a performance-to-noise efficiency point of view.&lt;/p&gt;
    &lt;p&gt;While upgrading to an NF-A12x25 G2 does provide some acoustical benefits compared to the stock setup (around 1 to 1.5 dB(A) lower noise levels at the same temperatures), its maximum speed is limited to 1800 RPM, so it cannot match the performance headroom of the 2400 RPM HS-PWM version of the NF-A12x25 that is supplied with the Framework Desktop PC. This high-speed version of the „G1“ fan is a safeguard that ensures the system can maintain full performance in worst-case conditions with high ambient temperatures. In other words, we would only recommend upgrading to the NF-A12x25 G2 if you seek to lower noise levels as much as possible and if you are willing to sacrifice the maximum performance headroom in worst-case scenarios that the G1 HS-PWM fan provides.&lt;/p&gt;
    &lt;p&gt;In summary, after a lot of simulation, experimenting and testing, we can conclude that not all tweaks to the Framework Desktop’s cooling setup make sense. However, if you have access to a 3D printer, swapping the stock side panel and fan duct for the custom designed ones can help to make your unit run significantly quieter.&lt;/p&gt;
    &lt;p&gt;Related products:&lt;/p&gt;
    &lt;p&gt;Your opinion matters!&lt;/p&gt;
    &lt;p&gt;We are excited to invite you to participate in our short website survey. It will only take 5 minutes of your time!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://noctua.at/en/how-to-make-the-framework-desktop-run-even-quieter"/><published>2025-09-16T18:33:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45266771</id><title>DataTables CDN Outage – post incident review</title><updated>2025-09-17T14:10:07.345227+00:00</updated><content>&lt;doc fingerprint="a50a085124e2b495"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Outage - post incident review&lt;/head&gt;
    &lt;p&gt;On 29th July 2025, the DataTables.net site had a major outage as a result of an attack. This took out the main site with its examples and documentation, the support forum, and the sub-domains, including, most importantly, the DataTables CDN.&lt;/p&gt;
    &lt;p&gt;Digging into such an event, when something has gone so wrong, is not a fun thing to do, but it is important for me to do so I can learn what I can do better in future, for you so you can understand what happened and the challenges faced, and for the community so we can try to stop this sort of thing happening in future.&lt;/p&gt;
    &lt;head rend="h2"&gt;Domain hijacking&lt;/head&gt;
    &lt;p&gt;Any website has a large surface which needs to be protected against hacking attempts - the application software, the HTTP server, the actual server the site resides on, DNS, and of course, the domain registration itself. The attack yesterday came on the domain name, whereby the attacker managed to move the domain out of the account I have with the domain registrar I use, into another controlled by the attacker, at which point they were able to change the name servers, pointing the domain at a different server.&lt;/p&gt;
    &lt;p&gt;To be clear, any account information you have on DataTables.net is safe - the server was never breached. The content on the DataTables server is safe. The DataTables source code is safe. Services are now all normal, but this was a major disruption for the site and anyone using its services, with some caching issues persisting at the time of writing.&lt;/p&gt;
    &lt;head rend="h3"&gt;How?!&lt;/head&gt;
    &lt;p&gt;So how did the attacker manage to execute this domain takeover? Around mid-June, I started receiving an extraordinary amount of email to one of my old email addresses. It was sign-ups for just about every list imaginable - three per minute, which went on continuously. I created a filter and assigned everything from that email address to a folder.&lt;/p&gt;
    &lt;p&gt;The attacker's next step was to create an account with the same registrar &lt;code&gt;datatables.net&lt;/code&gt; is registered with and request a transfer of the domain to their account. They used an email address intentionally crafted to look like it could be mine and submitted a fake driver's license and utility bill with information that could only have been from leaked WHOIS data. The registrar accepted this as proof of identity and started the transfer process. That included sending an email to me to confirm the transfer, an email which I never saw due to the flood of emails (which it is now easy to say was the start of the attack).&lt;/p&gt;
    &lt;p&gt;At the end of a five day period with no reply from me about the transfer, the registrar defaulted to accepting the transfer and the domain was moved to a different account. At no point were any of my accounts or email compromised - the attacker managed to socially engineer the registrar into making the transfer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Events on the day&lt;/head&gt;
    &lt;p&gt;Once the domain was under the attacker's control, events unfolded as follows on 29th July:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;02:57 - The name servers for &lt;code&gt;datatables.net&lt;/code&gt;were changed to point at different CloudFlare name servers. CloudFlare blocked all traffic to the new target with a 1000 error. Some users are still able to load CDN resources due to local cache.&lt;/item&gt;
      &lt;item&gt;07:10 - I read monitoring emails saying DataTables.net when I woke. Dashed to a computer.&lt;/item&gt;
      &lt;item&gt;07:21 - I contacted the registrar, having realised that the domain was no longer under my account.&lt;/item&gt;
      &lt;item&gt;08:45 - While replying to emails and messages saying the site was offline, I realised I could create a mirror domain of the CDN.&lt;/item&gt;
      &lt;item&gt;09:42 - &lt;code&gt;datatables-cdn.com&lt;/code&gt;was deployed as a mirror, allowing those affected to change just the domain - the script paths would still work.&lt;/item&gt;
      &lt;item&gt;11:05 - Submitted evidence that I was the rightful owner to the registrar.&lt;/item&gt;
      &lt;item&gt;12:10 - Submitted further evidence to the registrar (pro-tip, make sure your WHOIS information is up-to-date).&lt;/item&gt;
      &lt;item&gt;13:11 - Domain transferred back to my account - name servers reassigned to what they should be, and services start recovering.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unfortunately, while the &lt;code&gt;datatables.net&lt;/code&gt; server was fully ready at this point, DNS propagation is not instant, and many routers will cache DNS lookups. The DataTables TTL for DNS is all set to 5 minutes ("Auto" in CloudFlare's DNS management), which should allow for reasonably fast propagation, however, many routers appear to incorrectly hold their cache much longer and as a result many users continued to have problems loading resources from the CDN (in some cases this is still the case at the time of writing).&lt;/p&gt;
    &lt;head rend="h2"&gt;CDN&lt;/head&gt;
    &lt;p&gt;Normally if a site like DataTables were to go down, it would be irritating, but we can all survive without documentation for a software library for a day. However, the impact in this case was significant due to the DataTables CDN.&lt;/p&gt;
    &lt;p&gt;DataTables and its extensions (minus Editor, due to its commercial license) are MIT open source software, and to make it easily accessible for developers, it can be obtained in a number of ways - NPM, download for self hosting, or via the DataTables CDN server. Many choose to use the CDN - I provide it for free and it is fast and convenient. I use CloudFlare as a caching service, which makes it super fast and reliable (as long as it has a domain pointing at it!).&lt;/p&gt;
    &lt;p&gt;The image above shows the gap in the requests to DataTables during the attack. The CDN has around 55TB of data transferred per month. At approximately 3'440 million requests, that is a lot of Javascript and CSS! It will also give you an idea of the impact that a disruption to the CDN will have around the web, and what makes DataTables a valuable target for an attack of this type.&lt;/p&gt;
    &lt;head rend="h2"&gt;Credit where it is due&lt;/head&gt;
    &lt;p&gt;I'd like to extend a big thank you to my domain registrar for resolving this issue. It could have been difficult, but they made the process clear, straightforward and explained actions to me along the way.&lt;/p&gt;
    &lt;p&gt;CloudFlare, they blocked requests during the outage with a 1000 error, which could otherwise have been served maliciously.&lt;/p&gt;
    &lt;p&gt;Finally, to you, the DataTables community. I fully recognise how taxing it will have been for many of you trying to work out why the CDN wasn't working yesterday and the impact on your own businesses. Not once did I have people virtually yelling at me once I communicated the issue, I was almost universally wished well while I got on with trying to fix it.&lt;/p&gt;
    &lt;head rend="h3"&gt;And contempt where it belongs&lt;/head&gt;
    &lt;p&gt;The fact that someone would attack an open source product such as DataTables sickens me. I release by far the majority of my work as free open source software, host a free to use CDN, and support the software. I'm not just trying to make the world that little bit better; I really enjoy what I do as well. This attack was an attack on all of the work I've done, and given how it was executed, the attacker doesn't appear to have got out of it other than proof that they could, and to damage my reputation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learning process&lt;/head&gt;
    &lt;p&gt;In online security, it is not a question of if one is going to be attacked; it is a question of when. For me, it was over the last month, culminating in the events of yesterday. It was an attack vector that I had not expected - no compromised accounts, but rather social engineering against the registrar.&lt;/p&gt;
    &lt;p&gt;Immediate actions I've taken:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All email from my domain registrar is now marked as high priority and will never skip the inbox.&lt;/item&gt;
      &lt;item&gt;Put forward my opinion that having the default timeout action of a domain transfer request to accept it is wrong.&lt;/item&gt;
      &lt;item&gt;Added DNS monitoring for DataTables.net (HTTP and ping were already monitored along with various other automated security scanners).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Further actions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I will make the CDN server available for anyone to spin a new one up with checkout from GitHub.&lt;/item&gt;
      &lt;item&gt;Continue to improve my security monitoring of all domains I manage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Actions you can take:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use sub-resource integrity for all external Javascript and CSS you load. Since February this year, the DataTables download builder will automatically calculate the SRI for the resources selected. Use it! It makes sure that what you want to load is what actually gets loaded.&lt;/item&gt;
      &lt;item&gt;Consider if you want to download and host the files yourself, rather than loading them from an external CDN.&lt;/item&gt;
      &lt;item&gt;If you ever get a flood of sign up emails - consider that it is to hide something!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Moving on&lt;/head&gt;
    &lt;p&gt;My plan for yesterday had been to continue work on Editor 2.5 and ColumnControl 1.1. Once the dust has fully settled, I will be back on the case - expect to hear more about them in August.&lt;/p&gt;
    &lt;p&gt;Stay safe out there everyone.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://datatables.net/blog/2025/july-29-outage"/><published>2025-09-16T19:27:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45268269</id><title>Doom crash after 2.5 years of real-world runtime confirmed on real hardware</title><updated>2025-09-17T14:10:06.653505+00:00</updated><content>&lt;doc fingerprint="dc4f80d558235299"&gt;
  &lt;main&gt;
    &lt;p&gt;Two and a half years ago, I started my now longest real-world software experiment. I had read an article about how DOOMs engine works and noticed how a variable for tracking the demo kept being incremented even after the next demo started. This variable was compared with a second one storing its previous value. The issue here being, each incrementation would cause the variable to slowly get closer to an overflow, realistically this would never happen in a normal scenario, although it got me curious on just how long it would take until the game would crash due to this.&lt;lb/&gt; I did a few calculations, I don't remember the specifics of it sadly as it has been over two years since that point and I sadly did not document it back then (or I did, but on a partition I no longer have access to) but I remember having gotten roughly 2 1/2 years of possible runtime before an overflow. Obviously, I wanted to know if this would actually happen in the real game on real hardware.&lt;lb/&gt; So I set up DOOM on a small PDA, powered through a DIY 18650 based UPS which itself was connected to the USB port of my router for a constant 5V supply. I left the system running and mostly forgot about it.&lt;lb/&gt; ... Until today when I noticed a pop-up appearing on the device, not long ago from posting this to the board. The game had crashed, only hours after the two and a half year mark, proving that the variable did indeed overflow and cause the expected hard crash of the game: &lt;/p&gt;
    &lt;head rend="h2"&gt;DOOM crash after 2.5 years of real-world runtime confirmed on real hardware&lt;/head&gt;
    &lt;head rend="h3"&gt;DOOM crash after 2.5 years of real-world runtime confirmed on real hardware&lt;/head&gt;
    &lt;p&gt;~-~-~ MSD - Making your old devices useful again since 2022! ~-~-~&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lenowo.org/viewtopic.php?t=31"/><published>2025-09-16T21:24:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45270087</id><title>AMD Open Source Driver for Vulkan project is discontinued</title><updated>2025-09-17T14:10:06.079296+00:00</updated><content>&lt;doc fingerprint="8fdbcb577bc7b5ab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AMDVLK open-source project is discontinued #416&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;In a move to streamline development and strengthen our commitment to the open-source community, AMD is unifying its Linux Vulkan driver strategy and has decided to discontinue the AMDVLK open-source project, throwing our full support behind the RADV driver as the officially supported open-source Vulkan driver for Radeon™ graphics adapters.&lt;/p&gt;
          &lt;p&gt;This consolidation allows us to focus our resources on a single, high-performance codebase that benefits from the incredible work of the entire open-source community. We invite developers and users alike to utilize the RADV driver and contribute to its future.&lt;/p&gt;
          &lt;p&gt;We are excited about this focused path forward and are committed to the continued success of open-source Vulkan on Radeon.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 6 comments 8 replies&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;What does this mean for AMDPAL and in particular ROCm on Windows?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;To clarify, does this mean that AMD will be allocating more engineering resources towards RADV?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Why pal also got archieved?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Hi all. that is, AMDVLK is no more? and then which Vulkan driver should I use?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Is this a sign that RADV might come to Windows in the future? The Vulkan driver for Windows is based on AMDVLK...&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;How about the pro vulkan driver?&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/GPUOpen-Drivers/AMDVLK/discussions/416"/><published>2025-09-17T00:31:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45270649</id><title>I got the highest score on ARC-AGI again swapping Python for English</title><updated>2025-09-17T14:10:06.023639+00:00</updated><content/><link href="https://jeremyberman.substack.com/p/how-i-got-the-highest-score-on-arc-agi-again"/><published>2025-09-17T01:53:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45271481</id><title>GNU Midnight Commander</title><updated>2025-09-17T14:10:05.825330+00:00</updated><content>&lt;doc fingerprint="cc41321c83a3eac7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Welcome to Midnight Commander&lt;/head&gt;
    &lt;p&gt;GNU Midnight Commander (or &lt;code&gt;mc&lt;/code&gt;) is a visual, dual-pane file manager. It is released under the GNU General Public License and therefore qualifies as Free Software.&lt;/p&gt;
    &lt;p&gt;Midnight Commander is a feature-rich, full-screen, text-mode application that allows you to copy, move, and delete files and entire directory trees, search for files, and execute commands in the subshell. Internal viewer, editor and diff viewer are included.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;mc&lt;/code&gt; uses versatile text interface libraries such as ncurses or S-Lang, which allows it to work on a regular console, inside an X Window terminal, over &lt;code&gt;ssh&lt;/code&gt; connections, and in all kinds of remote shells.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installation&lt;/head&gt;
    &lt;p&gt;The easiest way to install &lt;code&gt;mc&lt;/code&gt; is to use your system package manager:&lt;/p&gt;
    &lt;code&gt;# apt-get install mc
&lt;/code&gt;
    &lt;code&gt;# dnf install mc
&lt;/code&gt;
    &lt;code&gt;# pkg install mc
&lt;/code&gt;
    &lt;code&gt;% brew install midnight-commander
&lt;/code&gt;
    &lt;p&gt;Our source releases are kindly mirrored by OSU OSL. Our canonical repository is hosted on GitHub. See the Source code page for details.&lt;/p&gt;
    &lt;head rend="h2"&gt;Documentation&lt;/head&gt;
    &lt;p&gt;The primary way to learn about &lt;code&gt;mc&lt;/code&gt; is to use the context-sensitive online help available via F1.&lt;/p&gt;
    &lt;p&gt;We also have extensive manual pages, which are the primary source of official documentation:&lt;/p&gt;
    &lt;code&gt;$ man mc
&lt;/code&gt;
    &lt;p&gt;... or read the latest development version online.&lt;/p&gt;
    &lt;code&gt;$ man mcedit
&lt;/code&gt;
    &lt;p&gt;... or read the latest development version online.&lt;/p&gt;
    &lt;code&gt;$ man mcview
&lt;/code&gt;
    &lt;p&gt;... or read the latest development version online.&lt;/p&gt;
    &lt;code&gt;$ man mcdiff
&lt;/code&gt;
    &lt;p&gt;... or read the latest development version online.&lt;/p&gt;
    &lt;head rend="h2"&gt;Color schemes&lt;/head&gt;
    &lt;p&gt;Midnight Commander supports theming! Check out the skins that come with the distribution or develop your own:&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributing &amp;amp; support&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For support, see the Communication page.&lt;/item&gt;
      &lt;item&gt;To contribute to &lt;code&gt;mc&lt;/code&gt;, proceed to the "Development" section.&lt;/item&gt;
      &lt;item&gt;Release notes for the development version are collected on the wiki.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://midnight-commander.org/"/><published>2025-09-17T03:54:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45271484</id><title>The Asus Gaming Laptop ACPI Firmware Bug: A Deep Technical Investigation</title><updated>2025-09-17T14:10:05.066401+00:00</updated><content>&lt;doc fingerprint="a41ebc5c01b4858a"&gt;
  &lt;main&gt;
    &lt;p&gt;You own a high-end ASUS ROG laptop perhaps a Strix, Scar, or Zephyrus. It's specifications are impressive: an RTX 30/40 series GPU, a top-tier Intel processor, and plenty of RAM. Yet, it stutters during basic tasks like watching a YouTube video, audio crackles and pops on Discord calls, the mouse cursor freezes for a split second, just long enough to be infuriating.&lt;/p&gt;
    &lt;p&gt;You've likely tried all the conventional fixes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Updating every driver imaginable, multiple times.&lt;/item&gt;
      &lt;item&gt;Performing a "clean" reinstallation of Windows.&lt;/item&gt;
      &lt;item&gt;Disabling every conceivable power-saving option.&lt;/item&gt;
      &lt;item&gt;Manually tweaking processor interrupt affinities.&lt;/item&gt;
      &lt;item&gt;Following convoluted multi-step guides from Reddit threads.&lt;/item&gt;
      &lt;item&gt;Even installing Linux, only to find the problem persists.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If none of that worked, it's because the issue isn't with the operating system or a driver. The problem is far deeper, embedded in the machine's firmware, the BIOS.&lt;/p&gt;
    &lt;p&gt;The first tool in any performance investigator's toolkit for these symptoms is LatencyMon. It acts as a canary in the coal mine for system-wide latency issues. On an affected ASUS Zephyrus M16, the results are immediate and damning:&lt;/p&gt;
    &lt;code&gt;CONCLUSION
Your system appears to be having trouble handling real-time audio and other tasks. 
You are likely to experience buffer underruns appearing as drop outs, clicks or pops.

HIGHEST MEASURED INTERRUPT TO PROCESS LATENCY
Highest measured interrupt to process latency (μs):   65,816.60
Average measured interrupt to process latency (μs):   23.29

HIGHEST REPORTED ISR ROUTINE EXECUTION TIME
Highest ISR routine execution time (μs):              536.80
Driver with highest ISR routine execution time:       ACPI.sys

HIGHEST REPORTED DPC ROUTINE EXECUTION TIME  
Highest DPC routine execution time (μs):              5,998.83
Driver with highest DPC routine execution time:       ACPI.sys
&lt;/code&gt;
    &lt;p&gt;The data clearly implicates &lt;code&gt;ACPI.sys&lt;/code&gt;. However, the per-CPU data reveals a more specific pattern:&lt;/p&gt;
    &lt;code&gt;CPU 0 Interrupt cycle time (s):                       208.470124
CPU 0 ISR highest execution time (μs):                536.804674
CPU 0 DPC highest execution time (μs):                5,998.834725
CPU 0 DPC total execution time (s):                   90.558238
&lt;/code&gt;
    &lt;p&gt;CPU 0 is taking the brunt of the impact, spending over 90 seconds processing interrupts while other cores remain largely unaffected. This isn't a failure of load balancing; it's a process locked to a single core.&lt;/p&gt;
    &lt;p&gt;A similar test on a Scar 15 from 2022 shows the exact same culprit: high DPC latency originating from &lt;code&gt;ACPI.sys&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It's easy to blame a Windows driver, but &lt;code&gt;ACPI.sys&lt;/code&gt; is not a typical driver. It primarily functions as an interpreter for ACPI Machine Language (AML), the code provided by the laptop's firmware (BIOS). If &lt;code&gt;ACPI.sys&lt;/code&gt; is slow, it's because the firmware is feeding it inefficient or flawed AML code to execute. These slowdowns are often triggered by General Purpose Events (GPEs) and traffic from the Embedded Controller (EC). To find the true source, we must dig deeper.&lt;/p&gt;
    &lt;p&gt;To understand what &lt;code&gt;ACPI.sys&lt;/code&gt; is doing during these latency spikes, we can use Event Tracing for Windows (ETW) to capture detailed logs from the ACPI providers.&lt;/p&gt;
    &lt;code&gt;# Find the relevant ACPI ETW providers
logman query providers | findstr /i acpi
# This returns two key providers:
# Microsoft-Windows-Kernel-Acpi {C514638F-7723-485B-BCFC-96565D735D4A}
# Microsoft-ACPI-Provider {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B}

# Start a comprehensive trace session
logman start ACPITrace -p {DAB01D4D-2D48-477D-B1C3-DAAD0CE6F06B} 0xFFFFFFFF 5 -o C:\Temp\acpi.etl -ets
logman update ACPITrace -p {C514638F-7723-485B-BCFC-96565D735D4A} 0xFFFFFFFF 5 -ets

# Then once we're done we can stop the trace and check the etl file and save the data in csv format aswell.
logman stop ACPITrace -ets
tracerpt C:\Temp\acpi.etl -o C:\Temp\acpi_events.csv -of CSV&lt;/code&gt;
    &lt;p&gt;Analyzing the resulting trace file in the Windows Performance Analyzer reveals a crucial insight. The spikes aren't random; they are periodic, occurring like clockwork every 30 to 60 seconds.&lt;/p&gt;
    &lt;p&gt;Random interruptions often suggest hardware faults or thermal throttling. A perfectly repeating pattern points to a systemic issue, a timer or a scheduled event baked into the system's logic.&lt;/p&gt;
    &lt;p&gt;The raw event data confirms this pattern:&lt;/p&gt;
    &lt;code&gt;Clock-Time (100ns),        Event,                      Kernel(ms), CPU
134024027290917802,       _GPE._L02 started,          13.613820,  0
134024027290927629,       _SB...BAT0._STA started,    0.000000,   4
134024027290932512,       _GPE._L02 finished,         -,          6
&lt;/code&gt;
    &lt;p&gt;The first event, &lt;code&gt;_GPE._L02&lt;/code&gt;, is an interrupt handler that takes 13.6 milliseconds to execute. For a high-priority interrupt, this is an eternity and is catastrophic for real-time system performance.&lt;/p&gt;
    &lt;p&gt;Deeper in the trace, another bizarre behavior emerges; the system repeatedly attempts to power the discrete GPU on and off, even when it's supposed to be permanently active.&lt;/p&gt;
    &lt;code&gt;Clock-Time,                Event,                    Duration
134024027315051227,       _SB.PC00.GFX0._PS0 start, 278μs     # GPU Power On
134024027315155404,       _SB.PC00.GFX0._DOS start, 894μs     # Display Output Switch
134024027330733719,       _SB.PC00.GFX0._PS3 start, 1364μs    # GPU Power Off
[~15 seconds later]
134024027607550064,       _SB.PC00.GFX0._PS0 start, 439μs     # Power On Again!
134024027607657368,       _SB.PC00.GFX0._DOS start, 1079μs    # Display Output Switch
134024027623134006,       _SB.PC00.GFX0._PS3 start, 394μs     # Power Off Again!
...
&lt;/code&gt;
    &lt;p&gt;This power cycling is nonsensical because the laptop is configured for a scenario where it is impossible: The system is in Ultimate Mode (via a MUX switch) with an external display connected.&lt;/p&gt;
    &lt;p&gt;In this mode:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The discrete NVIDIA GPU (dGPU) is the only active graphics processor.&lt;/item&gt;
      &lt;item&gt;The integrated Intel GPU (iGPU) is completely powered down and bypassed.&lt;/item&gt;
      &lt;item&gt;The dGPU is wired directly to the internal and external displays.&lt;/item&gt;
      &lt;item&gt;There is no mechanism for switching between GPUs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yet, the firmware ignores MUX state nudging the iGPU path (GFX0) and, worse, engaging dGPU cut/notify logic (PEGP/PEPD) every 15–30 seconds. The dGPU in mux mode isn't just "preferred" - it's the ONLY path to the display. There's no fallback, and no alternative. When the firmware sends &lt;code&gt;_PS3&lt;/code&gt; (power off), it's attempting something architecturally impossible.&lt;/p&gt;
    &lt;p&gt;Most of the time, hardware sanity checks refuse these nonsensical commands, but even failed attempts introduce latency spikes causing audio dropouts, input lag, and accumulating performance degradation. Games freeze mid-session, videos buffer indefinitely, system responsiveness deteriorates until restart.&lt;/p&gt;
    &lt;p&gt;Sometimes, under specific thermal conditions or race conditions, the power-down actually succeeds. When the firmware manages to power down the GPU that's driving the display, the sequence is predictable and catastrophic:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Firmware OFF attempt - cuts the dgpu path via PEG1.DGCE&lt;/item&gt;
      &lt;item&gt;Hardware complies - safety checks fail or timing aligns&lt;/item&gt;
      &lt;item&gt;Display signal cuts - monitors go black&lt;/item&gt;
      &lt;item&gt;User input triggers wake - mouse/keyboard activity&lt;/item&gt;
      &lt;item&gt;Windows calls &lt;code&gt;PowerOnMonitor()&lt;/code&gt;- attempt display recovery&lt;/item&gt;
      &lt;item&gt;NVIDIA driver executes &lt;code&gt;_PS0&lt;/code&gt;- GPU power on command&lt;/item&gt;
      &lt;item&gt;GPU enters impossible state - firmware insists OFF, Windows needs ON&lt;/item&gt;
      &lt;item&gt;Driver thread blocks indefinitely - waiting for GPU response&lt;/item&gt;
      &lt;item&gt;30-second watchdog expires - Windows gives up&lt;/item&gt;
      &lt;item&gt;System crashes with BSOD&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;5: kd&amp;gt; !analyze -v
*******************************************************************************
*                                                                             *
*                        Bugcheck Analysis                                    *
*                                                                             *
*******************************************************************************

WIN32K_POWER_WATCHDOG_TIMEOUT (19c)
Win32k did not turn the monitor on in a timely manner.
Arguments:
Arg1: 0000000000000050, Calling monitor driver to power on.
Arg2: ffff8685b1463080, Pointer to the power request worker thread.
Arg3: 0000000000000000
Arg4: 0000000000000000
...
STACK_TEXT:  
fffff685`3a767130 fffff800`94767be0     : 00000000`00000047 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSwapContext+0x76
fffff685`3a767270 fffff800`94726051     : ffff8685`b1463080 00000027`00008b94 fffff685`3a767458 fffff800`00000000 : nt!KiSwapThread+0x6a0
fffff685`3a767340 fffff800`94724ed3     : fffff685`00000000 00000000`00000043 00000000`00000002 0000008a`fbf50968 : nt!KiCommitThreadWait+0x271
fffff685`3a7673e0 fffff800`9471baf2     : fffff685`3a7675d0 02000000`0000001b 00000000`00000000 fffff800`94724500 : nt!KeWaitForSingleObject+0x773
fffff685`3a7674d0 fffff800`9471b7d5     : ffff8685`9cbec810 fffff685`3a7675b8 00000000`00010224 fffff800`00000003 : nt!ExpWaitForFastResource+0x92
fffff685`3a767580 fffff800`9471b49d     : 00000000`00000000 ffff8685`9cbec850 ffff8685`b1463080 00000000`00000000 : nt!ExpAcquireFastResourceExclusiveSlow+0x1e5
fffff685`3a767630 fffff800`28faca9b     : fffff800`262ee9c8 00000000`00000003 ffff8685`9cbec810 02000000`00000065 : nt!ExAcquireFastResourceExclusive+0x1bd
fffff685`3a767690 fffff800`28facbe5     : ffff8685`b31de000 00000000`00000000 ffffd31d`9a05244f 00000000`00000000 : win32kbase!&amp;lt;lambda_63b61c2369133a205197eda5bd671ee7&amp;gt;::&amp;lt;lambda_invoker_cdecl&amp;gt;+0x2b
fffff685`3a7676c0 fffff800`28e5f864     : ffffad0c`94d10878 fffff685`3a767769 ffffad0c`94d10830 ffff8685`b31de000 : win32kbase!UserCritInternal::`anonymous namespace'::EnterCritInternalEx+0x4d
fffff685`3a7676f0 fffff800`28e5f4ef     : 00000000`00000000 00000000`00000000 fffff800`262ee9c8 00000000`00000000 : win32kbase!DrvSetWddmDeviceMonitorPowerState+0x354
fffff685`3a7677d0 fffff800`28e2abab     : ffff8685`b31de000 00000000`00000000 ffff8685`b31de000 00000000`00000000 : win32kbase!DrvSetMonitorPowerState+0x2f
fffff685`3a767800 fffff800`28ef22fa     : 00000000`00000000 fffff685`3a7678d9 00000000`00000001 00000000`00000001 : win32kbase!PowerOnMonitor+0x19b
fffff685`3a767870 fffff800`28ef13dd     : ffff8685`94a40700 ffff8685`a2eb31d0 00000000`00000001 00000000`00000020 : win32kbase!xxxUserPowerEventCalloutWorker+0xaaa
fffff685`3a767940 fffff800`4bab21c2     : ffff8685`b1463080 fffff685`3a767aa0 00000000`00000000 00000000`00000020 : win32kbase!xxxUserPowerCalloutWorker+0x13d
fffff685`3a7679c0 fffff800`26217f3a     : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : win32kfull!NtUserUserPowerCalloutWorker+0x22
fffff685`3a7679f0 fffff800`94ab8d55     : 00000000`000005bc 00000000`00000104 ffff8685`b1463080 00000000`00000000 : win32k!NtUserUserPowerCalloutWorker+0x2e
fffff685`3a767a20 00007ff8`ee71ca24     : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : nt!KiSystemServiceCopyEnd+0x25
000000cc`d11ffbc8 00000000`00000000     : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : 0x00007ff8`ee71ca24

...
&lt;/code&gt;
    &lt;p&gt;The crash dump confirms the thread is stuck in &lt;code&gt;win32kbase!DrvSetWddmDeviceMonitorPowerState&lt;/code&gt;, waiting for the NVIDIA driver to respond. It can't because it's caught between a confused power state, windows wanting to turn on the GPU while the firmware is arming the GPU cut off.&lt;/p&gt;
    &lt;p&gt;GPEs are the firmware's mechanism for signaling hardware events to the operating system. They are essentially hardware interrupts that trigger the execution of ACPI code. The trace data points squarely at &lt;code&gt;_GPE._L02&lt;/code&gt; as the source of our latency.&lt;/p&gt;
    &lt;p&gt;A closer look at the timing reveals a consistent and problematic pattern:&lt;/p&gt;
    &lt;code&gt;_GPE._L02 Event Analysis from ROG Strix Trace:

Event 1 @ Clock 134024027290917802
  Duration: 13,613,820 ns (13.61ms)
  Triggered: Battery and AC adapter status checks

Event 2 @ Clock 134024027654496591  
  Duration: 13,647,255 ns (13.65ms)
  Triggered: Battery and AC adapter status checks
  
Event 3 @ Clock 134024028048493318
  Duration: 13,684,515 ns (13.68ms)  
  Triggered: Battery and AC adapter status checks

Interval between events: ~36-39 seconds
Consistency: The duration is remarkably stable and the interval is periodic.
&lt;/code&gt;
    &lt;p&gt;Every single time the lengthy &lt;code&gt;_GPE._L02&lt;/code&gt; event fires, it triggers the exact same sequence of ACPI method calls.&lt;/p&gt;
    &lt;p&gt;The pattern is undeniable:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A hardware interrupt fires &lt;code&gt;_GPE._L02&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The handler executes methods to check battery status.&lt;/item&gt;
      &lt;item&gt;Shortly thereafter, the firmware attempts to change the GPU's power state.&lt;/item&gt;
      &lt;item&gt;The system runs normally for about 30-60 seconds.&lt;/item&gt;
      &lt;item&gt;The cycle repeats.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To analyze the code responsible for this behavior, we must extract and decompile the ACPI tables provided by the BIOS to the operating system.&lt;/p&gt;
    &lt;code&gt;# Extract all ACPI tables into binary .dat files
acpidump -b

# Output includes:
# DSDT.dat - The main Differentiated System Description Table
# SSDT1.dat ... SSDT17.dat - Secondary System Description Tables

# Decompile the main table into human-readable ACPI Source Language (.dsl)
iasl -d DSDT.dsl&lt;/code&gt;
    &lt;p&gt;This decompiled ASL provides a direct view into the firmware's executable logic. It is a precise representation of the exact instructions that the ACPI.sys driver is fed by the firmware and executes at the highest privilege level within the Windows kernel. Any logical flaws found in this code are the direct cause of the system's behavior.&lt;/p&gt;
    &lt;p&gt;Searching the decompiled &lt;code&gt;DSDT.dsl&lt;/code&gt; file, we find the definition for our problematic GPE handler:&lt;/p&gt;
    &lt;code&gt;Scope (_GPE)
{
    Method (_L02, 0, NotSerialized)  // _Lxx: Level-Triggered GPE
    {
        \_SB.PC00.LPCB.ECLV ()
    }
}&lt;/code&gt;
    &lt;p&gt;This code is simple: when the &lt;code&gt;_L02&lt;/code&gt; interrupt occurs, it calls a single method, &lt;code&gt;ECLV&lt;/code&gt;. The "L" prefix in &lt;code&gt;_L02&lt;/code&gt; signifies that this is a level-triggered interrupt, meaning it will continue to fire as long as the underlying hardware condition is active. This is a critical detail.&lt;/p&gt;
    &lt;p&gt;Following the call to &lt;code&gt;ECLV()&lt;/code&gt;, we uncover a deeply flawed implementation that is the direct cause of the system-wide stuttering.&lt;/p&gt;
    &lt;code&gt;Method (ECLV, 0, NotSerialized)  // Starting at line 099244
{
    // Main loop - continues while events exist OR sleep events are pending
    // AND we haven't exceeded our time budget (TI3S &amp;lt; 0x78)
    While (((CKEV() != Zero) || (SLEC != Zero)) &amp;amp;&amp;amp; (TI3S &amp;lt; 0x78))
    {
        Local1 = One
        While (Local1 != Zero)
        {
            Local1 = GEVT()    // Get next event from queue
            LEVN (Local1)      // Process the event
            TIMC += 0x19       // Increment time counter by 25
            
            // This is where it gets really bad
            If ((SLEC != Zero) &amp;amp;&amp;amp; (Local1 == Zero))
            {
                // No events but sleep events pending
                If (TIMC == 0x19)
                {
                    Sleep (0x64)    // Sleep for 100 milliseconds!!!
                    TIMC = 0x64     // Set time counter to 100
                    TI3S += 0x04    // Increment major counter by 4
                }
                Else
                {
                    Sleep (0x19)    // Sleep for 25 milliseconds!!!
                    TI3S++          // Increment major counter by 1
                }
            }
        }
    }
    
    // Here's where it gets even worse
    If (TI3S &amp;gt;= 0x78)  // If we hit our time budget (120)
    {
        TI3S = Zero
        If (EEV0 == Zero)
        {
            EEV0 = 0xFF    // Force another event to be pending!
        }
    }
}&lt;/code&gt;
    &lt;p&gt;This short block of code violates several fundamental principles of firmware and kernel programming.&lt;/p&gt;
    &lt;p&gt;Wtf 1: Sleeping in an Interrupt Context&lt;/p&gt;
    &lt;code&gt;Sleep (0x64)    // 100ms sleep
Sleep (0x19)    // 25ms sleep&lt;/code&gt;
    &lt;p&gt;An interrupt handler runs at a very high priority to service hardware requests quickly. The &lt;code&gt;Sleep()&lt;/code&gt; function completely halts the execution of the CPU core it is running on (CPU 0 in this case). While CPU 0 is sleeping, it cannot:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Process any other hardware interrupts.&lt;/item&gt;
      &lt;item&gt;Allow the kernel to schedule other threads.&lt;/item&gt;
      &lt;item&gt;Update system timers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Clarification: These Sleep() calls live in the ACPI GPE handling path for the GPE L02, these calls get executed at PASSIVE_LEVEL after the SCI/GPE is acknowledged so it's not a raw ISR (because i don't think windows will even allow that) but analyzing this further while the control method runs the GPE stays masked and the ACPI/EC work is serialized. With the Sleep() calls inside that path and the self rearm it seems to have the effect of making ACPI.sys get tied up in long periodic bursts (often on CPU 0) which still have the same effect on the system.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Wtf 2: Time-Sliced Interrupt Processing The entire loop is designed to run for an extended period, processing events in batches. It's effectively a poorly designed task scheduler running inside an interrupt handler, capable of holding a CPU core hostage for potentially seconds at a time.&lt;/p&gt;
    &lt;p&gt;Wtf 3: Self-Rearming Interrupt&lt;/p&gt;
    &lt;code&gt;If (EEV0 == Zero)
{
    EEV0 = 0xFF    // Forces all EC event bits on
}&lt;/code&gt;
    &lt;p&gt;This logic ensures that even if the Embedded Controller's event queue is empty, the code will create a new, artificial event. This guarantees that another interrupt will fire shortly after, creating the perfectly periodic pattern of ACPI spikes observed in the traces.&lt;/p&gt;
    &lt;p&gt;The LEVN() method takes an event and routes it:&lt;/p&gt;
    &lt;code&gt;Method (LEVN, 1, NotSerialized)
  {
      If ((Arg0 != Zero))
      {
          MBF0 = Arg0
          P80B = Arg0
          Local6 = Match (LEGA, MEQ, Arg0, MTR, Zero, Zero)
          If ((Local6 != Ones))
          {
              LGPA (Local6)
          }
      }
  }
&lt;/code&gt;
    &lt;p&gt;The LGPA() method is a giant switch statement handling different events:&lt;/p&gt;
    &lt;code&gt;Method (LGPA, 1, Serialized)  // Line 098862
{
    Switch (ToInteger (Arg0))
    {
        Case (Zero)  // Most common case - power event
        {
            DGD2 ()       // GPU-related function
            ^EC0._QA0 ()  // EC query method
            PWCG ()       // Power change - this is our battery polling
        }
        
        Case (0x18)  // GPU-specific event
        {
            If (M6EF == One)
            {
                Local0 = 0xD2
            }
            Else
            {
                Local0 = 0xD1
            }
            NOD2 (Local0)  // Notify GPU driver
        }
        
        Case (0x1E)  // Another GPU event
        {
            Notify (^^PEG1.PEGP, 0xD5)  // Direct GPU notification
            ROCT = 0x55                  // Sets flag for follow-up
        }
       
    }
}&lt;/code&gt;
    &lt;p&gt;This shows a direct link: a GPE fires, and the dispatch logic calls functions related to battery polling and GPU notifications.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;PWCG()&lt;/code&gt; method, called by multiple event types, is responsible for polling the battery and AC adapter status.&lt;/p&gt;
    &lt;code&gt;Method (PWCG, 0, NotSerialized)
{
    Notify (ADP0, Zero)      // Tell OS to check the AC adapter
    ^BAT0._BST ()            // Execute the Battery Status method
    Notify (BAT0, 0x80)      // Tell OS the battery status has changed
    ^BAT0._BIF ()            // Execute the Battery Information method  
    Notify (BAT0, 0x81)      // Tell OS the battery info has changed
}&lt;/code&gt;
    &lt;p&gt;Which we can see here:&lt;/p&gt;
    &lt;p&gt;Each of these operations requires communication with the Embedded Controller, adding to the workload inside the already-stalled interrupt handler.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;NOD2()&lt;/code&gt; method sends notifications to the GPU driver.&lt;/p&gt;
    &lt;code&gt;Method (NOD2, 1, Serialized)
{
    If ((Arg0 != DNOT))
    {
        DNOT = Arg0
        Notify (^^PEG1.PEGP, Arg0)
    }

    If ((ROCT == 0x55))
    {
        ROCT = Zero
        Notify (^^PEG1.PEGP, 0xD1) // Hardware-Specific
    }
}&lt;/code&gt;
    &lt;p&gt;These notifications (&lt;code&gt;0xD1&lt;/code&gt;, &lt;code&gt;0xD2&lt;/code&gt;, etc.) are hardware-specific signals that tell the NVIDIA driver to re-evaluate its power state, which prompts driver power-state re-evaluation; in traces this surfaces as iGPU GFX0._PSx/_DOS toggles plus dGPU state changes via PEPD._DSM/DGCE.&lt;/p&gt;
    &lt;p&gt;Here's where a simple but catastrophic oversight in the firmware's logic causes system-wide failure. High-end ASUS gaming laptops feature a MUX (Multiplexer) switch, a piece of hardware that lets the user choose between two distinct graphics modes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Optimus Mode: The power-saving default. The integrated Intel GPU (iGPU) is physically connected to the display. The powerful NVIDIA GPU (dGPU) only renders demanding applications when needed, passing finished frames to the iGPU to be drawn on screen.&lt;/item&gt;
      &lt;item&gt;Ultimate/Mux Mode: The high-performance mode. The MUX switch physically rewires the display connections, bypassing the iGPU entirely and wiring the NVIDIA dGPU directly to the screen. In this mode, the dGPU is not optional; it is the only graphics processor capable of outputting an image.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Any firmware managing this hardware must be aware of which mode the system is in. Sending a command intended for one GPU to the other is futile and, in some cases, dangerous. Deep within the ACPI code, a hardware status flag named &lt;code&gt;HGMD&lt;/code&gt; is used to track this state. To understand the flaw, we first need to decipher what &lt;code&gt;HGMD&lt;/code&gt; means, and the firmware itself gives us the key.&lt;/p&gt;
    &lt;p&gt;For screen brightness to work, the command must be sent to the GPU that is physically controlling the display backlight. A command sent to the wrong GPU will simply do nothing. Therefore, the brightness control method (&lt;code&gt;BRTN&lt;/code&gt;) must be aware of the MUX switch state to function at all. It is the firmware's own Rosetta Stone.&lt;/p&gt;
    &lt;code&gt;// Brightness control - CORRECTLY checks for mux mode
Method (BRTN, 1, Serialized)  // Line 034003
{
    If (((DIDX &amp;amp; 0x0F0F) == 0x0400))
    {
        If (HGMD == 0x03)  // 0x03 = Ultimate/Mux mode
        {
            // In mux mode, notify discrete GPU
            Notify (\_SB.PC00.PEG1.PEGP.EDP1, Arg0)
        }
        Else
        {
            // In Optimus, notify integrated GPU
            Notify (\_SB.PC00.GFX0.DD1F, Arg0)
        }
    }
}&lt;/code&gt;
    &lt;p&gt;The logic here is flawless and revealing. The code uses the &lt;code&gt;HGMD&lt;/code&gt; flag to make a binary decision. If &lt;code&gt;HGMD&lt;/code&gt; is &lt;code&gt;0x03&lt;/code&gt;, it sends the command to the NVIDIA GPU. If not, it sends it to the Intel GPU. The firmware itself, through this correct implementation, provides the undeniable definition: &lt;code&gt;HGMD == 0x03&lt;/code&gt; means the system is in Ultimate/Mux Mode.&lt;/p&gt;
    &lt;p&gt;This perfect, platform-aware logic is completely abandoned in the critical code paths responsible for power management. The &lt;code&gt;LGPA&lt;/code&gt; method, which is called by the stutter-inducing interrupt, dispatches power-related commands to the GPU without ever checking the MUX mode.&lt;/p&gt;
    &lt;code&gt;// GPU power notification - NO MUX CHECK!
Case (0x18)
{
    // This SHOULD have: If (HGMD != 0x03)
    // But it doesn't, so it runs even in mux mode
    If (M6EF == One)
    {
        Local0 = 0xD2
    }
    Else
    {
        Local0 = 0xD1
    }
    NOD2 (Local0)  // Notifies GPU regardless of mode
}&lt;/code&gt;
    &lt;p&gt;This is not a single typo. A second, parallel power management system in the firmware exhibits the exact same flaw. The Platform Extension Plug-in Device (&lt;code&gt;PEPD&lt;/code&gt;) is used by Windows to manage system-wide power states, such as turning off displays during modern standby.&lt;/p&gt;
    &lt;code&gt;Device (PEPD)  // Line 071206
{
    Name (_HID, "INT33A1")  // Intel Power Engine Plugin
    
    Method (_DSM, 4, Serialized)  // Device Specific Method
    {
        // ... lots of setup code ...
        
        // Arg2 == 0x05: "All displays have been turned off"
        If ((Arg2 == 0x05))
        {
            // Prepare for aggressive power saving
            If (CondRefOf (\_SB.PC00.PEG1.DHDW))
            {
                ^^PC00.PEG1.DHDW ()         // GPU pre-shutdown work
                ^^PC00.PEG1.DGCE = One      // Set "GPU Cut Enable" flag
            }
            
            If (S0ID == One)  // If system supports S0 idle
            {
                GUAM (One)    // Enter low power mode
            }
            
            ^^PC00.DPOF = One  // Display power off flag
            
            // Tell USB controller about display state
            If (CondRefOf (\_SB.PC00.XHCI.PSLI))
            {
                ^^PC00.XHCI.PSLI (0x05)
            }
        }
        
        // Arg2 == 0x06: "A display has been turned on"
        If ((Arg2 == 0x06))
        {
            // Wake everything back up
            If (CondRefOf (\_SB.PC00.PEG1.DGCE))
            {
                ^^PC00.PEG1.DGCE = Zero     // Clear "GPU Cut Enable"
            }
            
            If (S0ID == One)
            {
                GUAM (Zero)   // Exit low power mode
            }
            
            ^^PC00.DPOF = Zero  // Display power on flag
            
            If (CondRefOf (\_SB.PC00.XHCI.PSLI))
            {
                ^^PC00.XHCI.PSLI (0x06)
            }
        }
    }
}&lt;/code&gt;
    &lt;p&gt;Once again, the firmware prepares to cut power to the discrete GPU without first checking if it's the only GPU driving the displays. This demonstrates that the Mux Mode Confusion is a systemic design flaw. The firmware is internally inconsistent, leading it to issue self-destructive commands that try to cripple the system.&lt;/p&gt;
    &lt;p&gt;Traces from multiple ASUS gaming laptop models confirm this is not an isolated issue.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Trace Duration: 4.1 minutes&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;_GPE._L02&lt;/code&gt;Events: 7&lt;/item&gt;
      &lt;item&gt;Avg. GPE Duration: 1.56ms (lower, but still unacceptably high)&lt;/item&gt;
      &lt;item&gt;Avg. Interval: 39.4 seconds (nearly identical periodic nature)&lt;/item&gt;
      &lt;item&gt;GPU Power Cycles: 8&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Trace Duration: 19.9 minutes&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;_GPE._L02&lt;/code&gt;Events: 3&lt;/item&gt;
      &lt;item&gt;Avg. GPE Duration: 2.94ms&lt;/item&gt;
      &lt;item&gt;GPU Power Cycles: 197 (far more frequent)&lt;/item&gt;
      &lt;item&gt;ASUS WMI Calls: 2,370 (a massive number, indicating software amplification)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Microsoft has a built-in "smooth video" check. It plays HD video in full screen and watches for hiccups. If the PC drops frames, crackles, or any driver pauses for more than a few milliseconds, it fails. That’s Microsoft’s baseline for what "smooth" should look like.&lt;/p&gt;
    &lt;p&gt;Why it matters here:&lt;/p&gt;
    &lt;p&gt;ASUS firmware is causing millisecond-long pauses. Those pauses are exactly the kind that make this test fail i.e., the same stutters and audio pops regular users notice on YouTube/Netflix and games; this firmware violates fundemental standards.&lt;/p&gt;
    &lt;p&gt;Despite being different models, all affected systems exhibit the same core flaws:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;_GPE._L02&lt;/code&gt;handlers take milliseconds to execute instead of microseconds.&lt;/item&gt;
      &lt;item&gt;The GPEs trigger unnecessary battery polling.&lt;/item&gt;
      &lt;item&gt;The firmware attempts to power cycle the GPU while in a fixed MUX mode.&lt;/item&gt;
      &lt;item&gt;The entire process is driven by a periodic, timer-like trigger.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This bug is a cascade of firmware design failures.&lt;/p&gt;
    &lt;p&gt;On windows, the LXX / EXX run at PASSIVE_LEVEL via ACPI.sys but while a GPE control method runs the firing GPE stays masked and ACPI/EC work is serialized. ASUS's dispatch from GPE._L02 to ECLV loops, calls Sleep(25/100ms) and re-arms the EC stretching that masked window into tens of milliseconds (which would explain the 13ms CPU time in ETW (Kernel ms) delay for GPE Events) and producing a periodic ACPI.sys burst that causes the latency problems on the system.The correct behavior is to latch or clear the event, exit the method, and signal a driver with Notify for any heavy work; do not self-rearm or sleep in this path at all.&lt;/p&gt;
    &lt;p&gt;The firmware artificially re-arms the interrupt, creating an endless loop of GPEs instead of clearing the source and waiting for the next legitimate hardware event. This transforms a hardware notification system into a disruptive, periodic timer.&lt;/p&gt;
    &lt;p&gt;The code that sends GPU power notifications does not check if the system is in MUX mode, a critical state check that is correctly performed in other parts of the firmware. This demonstrates inconsistency and a lack of quality control.&lt;/p&gt;
    &lt;p&gt;This issue is not new or isolated. User reports documenting identical symptoms with high ACPI.sys DPC latency, periodic stuttering, and audio crackling have been accumulating since at least 2021 across ASUS's entire gaming laptop lineup.&lt;/p&gt;
    &lt;p&gt;August 2021: The First Major Reports&lt;lb/&gt; The earliest documented cases appear on the official ASUS ROG forums. A G15 Advantage Edition (G513QY) owner reports "severe DPC latency from ACPI.sys" with audio dropouts occurring under any load condition. The thread, last edited in March 2024, shows the issue remains unresolved after nearly three years.&lt;/p&gt;
    &lt;p&gt;Reddit users simultaneously report identical ACPI.sys latency problems alongside NVIDIA driver issues; the exact symptoms described in this investigation.&lt;/p&gt;
    &lt;p&gt;2021-2023: Spreading Across Models&lt;lb/&gt; Throughout this period, the issue proliferates across ASUS's gaming lineup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ROG Strix models experience micro-stutters&lt;/item&gt;
      &lt;item&gt;TUF Gaming series reports throttling for seconds at a time&lt;/item&gt;
      &lt;item&gt;G18 models exhibit the characteristic 45-second periodic stuttering&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2023-2024: The Problem Persists in New Models&lt;lb/&gt; Even the latest generations aren't immune:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2023 Zephyrus G16 owners report persistent audio issues&lt;/item&gt;
      &lt;item&gt;2023 G16 models continue experiencing audio pops/crackles&lt;/item&gt;
      &lt;item&gt;2024 Intel G16 models require workarounds for audio stuttering&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The evidence is undeniable:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Measured Proof: GPE handlers are measured blocking a CPU core for over 13 milliseconds.&lt;/item&gt;
      &lt;item&gt;Code Proof: The decompiled firmware explicitly contains &lt;code&gt;Sleep()&lt;/code&gt;calls within an interrupt handler.&lt;/item&gt;
      &lt;item&gt;Logical Proof: The code lacks critical checks for the laptop's hardware state (MUX mode).&lt;/item&gt;
      &lt;item&gt;Systemic Proof: The issue is reproducible across different models and BIOS versions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Until a fix is implemented, millions of buyers of Asus laptops from approx. 2021 to present day are facing stutters on the simplest of tasks, such as watching YouTube, for the simple mistake of using a sleep call inside of an inefficient interrupt handler and not checking the GPU environment properly.&lt;/p&gt;
    &lt;p&gt;The code is there. The traces prove it. ASUS must fix its firmware.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ASUS has not responded to this investigation or the documented firmware issues at the time of publication, will update this if anything changes.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Investigation conducted using the Windows Performance Toolkit, ACPI table extraction tools, and Intel ACPI Component Architecture utilities. All code excerpts are from official ASUS firmware. Traces were captured on multiple affected systems, all showing consistent behavior. I used an LLM for wording. The research, traces, and AML decomp are mine. Every claim is verified and reproducible if you follow the steps in the article; logs and commands are in the repo. If you think something's wrong, cite the exact timestamp/method/line. "AI wrote it" is not an argument.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Zephkek/Asus-ROG-Aml-Deep-Dive"/><published>2025-09-17T03:54:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45271942</id><title>Notion API importer, with Databases to Bases conversion bounty</title><updated>2025-09-17T14:10:04.898917+00:00</updated><content>&lt;doc fingerprint="b12cdaac20f33688"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 135&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Currently Importer supports converting Notion exports to Markdown via the HTML export #14.&lt;/p&gt;
    &lt;p&gt;Unfortunately Notion's file-based export options don't include necessary data to recreate Databases.&lt;/p&gt;
    &lt;p&gt;This new importer would use the Notion API to download files progressively, and convert Databases to Bases as &lt;code&gt;.base&lt;/code&gt; files using the YAML syntax.&lt;/p&gt;
    &lt;p&gt;Closes #415&lt;/p&gt;
    &lt;head rend="h2"&gt;Bounty&lt;/head&gt;
    &lt;p&gt;See the Contribution guidelines for how to claim this bounty.&lt;/p&gt;
    &lt;p&gt;Bounty: $5,000 USD&lt;lb/&gt; Timeframe: 30 days&lt;/p&gt;
    &lt;head rend="h2"&gt;Requirements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uses Notion API (integration token) incorporating changes from new data source object introduced 2025-09.&lt;/item&gt;
      &lt;item&gt;Properly converts files to Obsidian-flavored Markdown, including tables, to-do lists, etc&lt;/item&gt;
      &lt;item&gt; Support for images and attachments. Embed links converted to Markdown format &lt;code&gt;!()[image.png]&lt;/code&gt;and placed in the user's defined attachment location (Settings → File &amp;amp; links)&lt;/item&gt;
      &lt;item&gt;Provide working test cases, ideally a reproducible data import that can be used on Notion. Alternatively a test account you can share with us via DM.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Databases to Bases&lt;/head&gt;
    &lt;p&gt;Some exploration is required before implementation because Databases and Bases work a bit differently. Notion's Databases start out as empty, whereas a Base starts out with all of the user's files, then narrows down using filters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Determine an approach for importing databases and files&lt;/item&gt;
      &lt;item&gt;Determine what database features can be imported: views, columns, groups, summaries, formulas, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h3"&gt;Assignees&lt;/head&gt;
    &lt;head rend="h3"&gt;Labels&lt;/head&gt;
    &lt;head rend="h3"&gt;Type&lt;/head&gt;
    &lt;head rend="h3"&gt;Projects&lt;/head&gt;
    &lt;p&gt;Status&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/obsidianmd/obsidian-importer/issues/421"/><published>2025-09-17T05:11:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45272480</id><title>Murex – An intuitive and content aware shell for a modern command line</title><updated>2025-09-17T14:10:04.820951+00:00</updated><content>&lt;doc fingerprint="76c4cc9c753539fe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Murex.Rocks&lt;/head&gt;
    &lt;p&gt;An intuitive and content aware shell for a modern command line&lt;/p&gt;
    &lt;head rend="h2"&gt;A Modern shell for the rest of us&lt;/head&gt;
    &lt;p&gt;Murex carries tons of unique features. Some highlights include...&lt;/p&gt;
    &lt;head rend="h3"&gt;Content Aware&lt;/head&gt;
    &lt;p&gt;Native support for manipulating data formats such as JSON, YAML, CSV, and others. This allows for seamless integration and manipulation of data in various formats. &lt;lb/&gt; Data types can be explicitly cast and reformatted, but also inferred if preferred.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expressions&lt;/head&gt;
    &lt;p&gt;Smarter handling of variables and expressions to avoid accidental bugs caused by spaces or incorrect syntax. Resulting in a more reliable and predictable scripting experience. &lt;lb/&gt; Never worry about file names with weird characters, nor running equations in "bc" again.&lt;/p&gt;
    &lt;head rend="h3"&gt;Smartly Interactive&lt;/head&gt;
    &lt;p&gt;A uniquely intuitive interactive shell. With command line hints pulled from man pages, AI LLMs, and other intelligent integrations. &lt;lb/&gt; Navigating the command line is faster, more intuitive and efficient than ever before.&lt;/p&gt;
    &lt;head rend="h3"&gt;Easily Extended&lt;/head&gt;
    &lt;p&gt;The built-in package manager makes it very easy to share your configuration, import other peoples namespaced modules, and port your environment between different machines. &lt;lb/&gt; Configure once, use everywhere.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Read the language tour to get started.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Rosetta Stone is a great cheatsheet for those wishing to skip the tutorials and jump straight in. This guide includes comparisons with Bash.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Interactive Shell guide walks you through using Murex as a command line as opposed to a scripting language.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Screenshots&lt;/head&gt;
    &lt;p&gt;Check out the Language Tour and Interactive Shell guides!&lt;/p&gt;
    &lt;head rend="h2"&gt;Easy to Install&lt;/head&gt;
    &lt;p&gt;Install &lt;code&gt;murex&lt;/code&gt; from your favorite package manager:&lt;/p&gt;
    &lt;code&gt;# via Homebrew:
brew install murex

# via MacPorts:
port install murex
&lt;/code&gt;
    &lt;code&gt;# From AUR: https://aur.archlinux.org/packages/murex
wget -O PKGBUILD 'https://aur.archlinux.org/cgit/aur.git/plain/PKGBUILD?h=murex'
makepkg --syncdeps --install 
&lt;/code&gt;
    &lt;code&gt;pkg install murex
&lt;/code&gt;
    &lt;p&gt;More options are available in the INSTALL document.&lt;/p&gt;
    &lt;p&gt;This document was generated from gen/root/README_doc.yaml.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://murex.rocks/"/><published>2025-09-17T06:32:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45273352</id><title>Why We're Building Stategraph: Terraform State as a Distributed Systems Problem</title><updated>2025-09-17T14:10:04.681703+00:00</updated><content>&lt;doc fingerprint="a51827402d82cda9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Why We're Building Stategraph: Terraform State as a Distributed Systems Problem&lt;/head&gt;&lt;p&gt;The Terraform ecosystem has spent a decade working around a fundamental architectural mismatch: we're using filesystem semantics to solve a distributed systems problem. The result is predictable and painful.&lt;/p&gt;&lt;p&gt;When we started building infrastructure automation at scale, we discovered that Terraform's state management exhibits all the classic symptoms of impedance mismatch between data representation and access patterns. Teams implement increasingly elaborate workarounds: state file splitting, wrapper orchestration, external locking mechanisms. These aren't solutions; they're evidence that we're solving the wrong problem.&lt;/p&gt;&lt;p&gt;Stategraph addresses this by treating state for what it actually is: a directed acyclic graph of resources with partial update semantics, not a monolithic document.&lt;/p&gt;&lt;head rend="h2"&gt;The Pathology of File-Based State&lt;/head&gt;&lt;p&gt;Terraform state, at its core, is a coordination problem. Multiple actors (engineers, CI systems, drift detection) need to read and modify overlapping subsets of infrastructure state concurrently. This is a well-studied problem in distributed systems, with established solutions around fine-grained locking, multi-version concurrency control, and transaction isolation.&lt;/p&gt;&lt;p&gt;Instead, Terraform implements the simplest possible solution: a global mutex on a JSON file.&lt;/p&gt;&lt;head rend="h4"&gt;Observation&lt;/head&gt;&lt;p&gt;The probability of lock contention in a shared state file increases super-linearly with both team size and resource count. At 100 resources and 5 engineers, you're coordinating 500 potential interaction points through a single mutex.&lt;/p&gt;&lt;p&gt;Consider the actual data access patterns in a typical Terraform operation:&lt;/p&gt;&lt;head rend="h4"&gt;Current Model&lt;/head&gt;&lt;p&gt; Read: 100%&lt;lb/&gt;Lock: 100%&lt;lb/&gt;Modify: 0.5% &lt;/p&gt;&lt;head rend="h4"&gt;Actual Requirement&lt;/head&gt;&lt;p&gt; Read: 3%&lt;lb/&gt;Lock: 3%&lt;lb/&gt;Modify: 3% &lt;/p&gt;&lt;p&gt;This mismatch between granularity of operation and granularity of locking is the root cause of every Terraform scaling problem. It violates the fundamental principle of isolation in concurrent systems: non-overlapping operations should not block each other.&lt;/p&gt;&lt;p&gt;The standard response, splitting state files, doesn't solve the problem. It redistributes it. Now you have N coordination problems instead of one, plus the additional complexity of managing cross-state dependencies. You've traded false contention for distributed transaction coordination, which is arguably worse.&lt;/p&gt;&lt;head rend="h2"&gt;State as a Graph: The Natural Representation&lt;/head&gt;&lt;p&gt;Infrastructure state is inherently a directed graph. Resources have dependencies, which form edges. Changes propagate along these edges. Terraform already knows this: the internal representation is a graph, and the planner performs graph traversal. But at the storage layer, we flatten this rich structure into a blob.&lt;/p&gt;&lt;p&gt;This is akin to storing a B-tree in a CSV file. You can do it, but you're destroying the very properties that make the data structure useful.&lt;/p&gt;&lt;p&gt;When state is properly normalized into a graph database, several properties emerge naturally:&lt;/p&gt;&lt;p&gt;Subgraph isolation: Operations on disjoint subgraphs are inherently parallelizable. If Team A is modifying RDS instances and Team B is updating CloudFront distributions, there's no shared state to coordinate.&lt;/p&gt;&lt;p&gt;Precise locking: We can implement row-level locking on resources and edge-level locking on dependencies. Lock acquisition follows the dependency graph, preventing deadlocks through consistent ordering.&lt;/p&gt;&lt;p&gt;Incremental refresh: Given a change set, we can compute the minimal refresh set by traversing the dependency graph. Most changes affect a small cone of resources, not the entire state space.&lt;/p&gt;&lt;head rend="h2"&gt;Concurrency Control Through Proper Abstractions&lt;/head&gt;&lt;p&gt;The distributed systems community solved these problems decades ago. Multi-version concurrency control (MVCC) allows readers to proceed without blocking writers. Write-ahead logging provides durability without sacrificing performance. Transaction isolation levels let operators choose their consistency guarantees.&lt;/p&gt;&lt;p&gt;Stategraph implements these patterns at the Terraform state layer:&lt;/p&gt;&lt;head rend="h4"&gt;Traditional: Global Lock&lt;/head&gt;&lt;head rend="h4"&gt;Stategraph: Subgraph Isolation&lt;/head&gt;&lt;p&gt;Each operation acquires locks only on its subgraph. The lock manager uses the dependency graph to ensure consistent ordering, preventing deadlocks. Readers use MVCC to access consistent snapshots without blocking writers.&lt;/p&gt;&lt;head rend="h4"&gt;Implementation Detail&lt;/head&gt;&lt;p&gt;Lock acquisition follows a strict partial order derived from the resource dependency graph. Resources are locked in topological order, with ties broken by resource ID. This guarantees deadlock freedom without requiring global coordination.&lt;/p&gt;&lt;p&gt;The result is dramatic improvement in concurrent throughput:&lt;/p&gt;&lt;head rend="h5"&gt;Transaction A&lt;/head&gt;&lt;head rend="h5"&gt;Transaction B&lt;/head&gt;&lt;head rend="h5"&gt;Transaction C&lt;/head&gt;&lt;p&gt;Three teams, three transactions, zero contention. This isn't possible with file-based state, regardless of how you split it.&lt;/p&gt;&lt;head rend="h2"&gt;The Refresh Problem&lt;/head&gt;&lt;p&gt;Terraform refresh is O(n) in the number of resources, regardless of change scope. Change one security group rule and you still walk the entire state. That's an algorithmic bottleneck, not just an implementation detail.&lt;/p&gt;&lt;head rend="h4"&gt;File-Based State&lt;/head&gt;&lt;p&gt; Changing 1 resource&lt;lb/&gt; Refreshing all 30 &lt;/p&gt;&lt;head rend="h4"&gt;Graph State&lt;/head&gt;&lt;p&gt; Changing 1 resource&lt;lb/&gt; Refreshing only 3 &lt;/p&gt;&lt;p&gt;With a graph representation, refresh work can be scoped to the affected subgraph instead of the entire state. Most changes touch only a small fraction of resources, not everything.&lt;/p&gt;&lt;head rend="h2"&gt;Why We Built This&lt;/head&gt;&lt;p&gt;At Terrateam, we've watched hundreds of teams struggle with the same fundamental problems. They start with a single state file, hit scaling limits, split their state, discover coordination complexity, build orchestration layers, and eventually resign themselves to living with the pain.&lt;/p&gt;&lt;p&gt;This is a solvable problem. The computer science is well-understood. The implementation is straightforward once you acknowledge that state management is a distributed systems problem, not a file storage problem.&lt;/p&gt;&lt;p&gt;Stategraph isn't revolutionary. It's the application of established distributed systems principles to a problem that's been mischaracterized since its inception. We're not inventing new algorithms; we're applying the right ones.&lt;/p&gt;&lt;head rend="h4"&gt;Design Principle&lt;/head&gt;&lt;p&gt;The storage layer should match the access patterns. Terraform state exhibits graph traversal patterns, partial update patterns, and concurrent access patterns. The storage layer should be a graph database with ACID transactions and fine-grained locking. Anything else is impedance mismatch.&lt;/p&gt;&lt;p&gt;The infrastructure industry has accepted file-based state as an immutable constraint for too long. It's not. It's a choice, and it's the wrong one for systems at scale.&lt;/p&gt;&lt;head rend="h2"&gt;Technical Implementation&lt;/head&gt;&lt;p&gt;Stategraph is implemented as a PostgreSQL schema with a backend that speaks the Terraform/OpenTofu remote backend protocol. We chose PostgreSQL for its robust MVCC, proven scalability, and operational familiarity. The schema normalizes state into three primary relations:&lt;/p&gt;&lt;p&gt;resources: one row per resource, with type, provider, and attribute columns.&lt;lb/&gt; dependencies: edge table representing the resource dependency graph.&lt;lb/&gt; transactions: append-only log of all state mutations with full attribution.&lt;/p&gt;&lt;p&gt;The backend extends Terraform's protocol with graph-aware operations. Lock acquisition and state queries operate directly on the database representation of the graph, enabling precision and concurrency that file-based backends can't provide.&lt;/p&gt;&lt;p&gt;This isn't a wrapper or an orchestrator. It's a replacement for the storage layer that preserves Terraform's execution model while fixing its coordination problems.&lt;/p&gt;&lt;head rend="h2"&gt;Adoption Path&lt;/head&gt;&lt;p&gt;Stategraph reads existing tfstate files and constructs the graph representation automatically. No changes to Terraform configurations are required. The backend protocol is unchanged. From Terraform's perspective, Stategraph is just another backend, like S3 or GCS.&lt;/p&gt;&lt;p&gt;But from an operational perspective, everything changes. Lock contention disappears. Refresh times drop by orders of magnitude. Teams stop blocking each other. State becomes queryable, auditable, and comprehensible.&lt;/p&gt;&lt;p&gt;We're not asking teams to rewrite their infrastructure. We're asking them to store it properly.&lt;/p&gt;&lt;quote&gt;The question isn't whether Terraform state should be a graph. It already is. The question is whether we'll continue pretending it's a file.&lt;/quote&gt;&lt;head rend="h3"&gt;Technical Preview&lt;/head&gt;&lt;p&gt;Stategraph is in active development. We're working with design partners to validate the approach at scale.&lt;/p&gt;Get Updates&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stategraph.dev/blog/why-stategraph/"/><published>2025-09-17T08:38:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45273747</id><title>Alibaba's New AI Chip Unveiled: Key Specifications Comparable to H20</title><updated>2025-09-17T14:10:02.860213+00:00</updated><content>&lt;doc fingerprint="ccca7d4c469478ff"&gt;
  &lt;main&gt;
    &lt;p&gt;On September 16, according to a report aired on CCTV News, a comparison of key parameters between domestically produced cards and NV cards revealed that the latest PPU chip developed by Pingtouge, a subsidiary of Alibaba and designed for artificial intelligence, surpasses NVIDIA's A800 in all major parameter metrics and is comparable to the H20. Compared with other domestic AI chips, Pingtouge’s PPU also mostly leads in these indicators.&lt;/p&gt;
    &lt;p&gt;In terms of specific parameters:&lt;lb/&gt;Memory: Alibaba Pingtouge PPU is equipped with 96GB of HBM2e, surpassing NVIDIA A800's 80GB HBM2e and matching the memory capacity of NVIDIA H20. However, H20 integrates HBM3, which is one generation ahead;**&lt;lb/&gt;Inter-chip interconnect bandwidth: Alibaba Pingtouge PPU reaches up to 700GB/s, higher than A800’s 400GB/s, but slightly lower than H20;&lt;lb/&gt;Interface: Alibaba Pingtouge PPU supports PCIe 5.0×15, superior to A800’s PCIe 4.0×16 and on par with H20;&lt;lb/&gt;Power consumption: Alibaba Pingtouge PPU maintains the same level as NVIDIA A800 at 400W, lower than H20’s 550W.&lt;/p&gt;
    &lt;p&gt;According to the report, the list of signed agreements for China Unicom’s Sanjiangyuan Green Electricity Intelligent Computing Center project showcases collaborations with multiple domestic AI chip brands under signed or proposed agreements.&lt;lb/&gt;Among them, the total number of devices in signed projects amounts to 1,747 units, comprising 22,832 computing cards with a total computing power of 3,479P. Specifically:&lt;lb/&gt;Alibaba Cloud Ten Thousand Cards: A total of 1,024 devices, 16,384 Pingtouge computing cards, with computing power reaching 1,945P;&lt;lb/&gt;Chinese Academy of Sciences: A total of 512 devices, 4,096 Muxi computing cards, with computing power reaching 984P;&lt;lb/&gt;Beijing Jingtai: A total of 83 devices, 1,328 BR100 computing power cards, with a computing power of 450P;&lt;lb/&gt;Zhonghao Core Electronics: A total of 128 devices, with a computing power of 200P.&lt;/p&gt;
    &lt;p&gt;In addition, the total computing power of the proposed contracted projects is 2,002P, including computing power cards from Taichu Yuanjie, Suizhi Technology, and Moore Threads.&lt;/p&gt;
    &lt;p&gt;Editor/Rocky&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.futunn.com/en/post/62202518/alibaba-s-new-ai-chip-unveiled-key-specifications-comparable-to"/><published>2025-09-17T09:45:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45273824</id><title>Oh no, not again a meditation on NPM supply chain attacks</title><updated>2025-09-17T14:10:02.661537+00:00</updated><content/><link href="https://tane.dev/2025/09/oh-no-not-again...-a-meditation-on-npm-supply-chain-attacks/"/><published>2025-09-17T09:57:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45273854</id><title>EU Chat Control: Germany's position has been reverted to UNDECIDED</title><updated>2025-09-17T14:10:01.952794+00:00</updated><content>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mastodon.social/@chatcontrol/115215006562371435"/><published>2025-09-17T10:02:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45273897</id><title>PureVPN IPv6 Leak</title><updated>2025-09-17T14:10:01.111342+00:00</updated><content>&lt;doc fingerprint="fc9cf519ac23292a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;PureVPN IPv6 leak&lt;/head&gt;
    &lt;p&gt;In late August 2025, I submitted two security reports to PureVPN under their VDP. Three weeks later, I’ve received no response, so I decided to publish the findings to inform other users.&lt;/p&gt;
    &lt;p&gt;The issues affect both their GUI (v2.10.0) and CLI (v2.0.1) clients on Linux (tested on Ubuntu 24.04.3 LTS, kernel 6.8.0, iptables-nft backend). Hereâs what I found.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. IPv6 Leaks Off-Tunnel&lt;/head&gt;
    &lt;p&gt;After toggling Wi-Fi or resuming from suspend, the PureVPN client fails to restore IPv6 protections:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;CLI (IKS enabled): The client auto-reconnects and reports status as “connected”, yet the system regains a default IPv6 route via Router Advertisements (&lt;/p&gt;&lt;code&gt;fe80::1&lt;/code&gt;). Since&lt;code&gt;ip6tables&lt;/code&gt;&lt;code&gt;OUTPUT&lt;/code&gt;remains&lt;code&gt;ACCEPT&lt;/code&gt;(default), egress resumes off-tunnel.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;GUI (IKS enabled): When the GUI detects a disconnection, it blocks IPv4 and displays the “VPN session disconnected” dialog. However, IPv6 remains functional until the user explicitly clicks&lt;/p&gt;&lt;code&gt;Reconnect&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Real-world effect: I was able to browse IPv6-preferred sites and send/receive email (Thunderbird) with my ISPâs IPv6 address while the client UI claimed I was protected.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Host Firewall Reset and Not Restored&lt;/head&gt;
    &lt;p&gt;At connect time, PureVPN wipes the user’s &lt;code&gt;iptables&lt;/code&gt; configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;INPUT&lt;/code&gt;is set to&lt;code&gt;ACCEPT&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;All &lt;code&gt;-A&lt;/code&gt;rules are flushed (UFW, Docker jumps, user rules, etc.)&lt;/item&gt;
      &lt;item&gt;After disconnect, these changes are not reverted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Result: the system remains more exposed after using the VPN than before. This defeats the point of using UFW or a local deny policy and contradicts user expectations.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# Baseline protections
$ sudo iptables -P INPUT DROP
$ sudo iptables -I INPUT -p icmp -j DROP

# Connect to VPN
$ purevpn-cli -c US
$ sudo iptables -S | head -3
-P INPUT ACCEPT
-P FORWARD DROP
-P OUTPUT ACCEPT
$ sudo iptables -S | grep icmp
# (no output â rule was wiped)

# Disconnect
$ purevpn-cli -d
$ sudo iptables -S | head -3
-P INPUT ACCEPT
-P FORWARD DROP
-P OUTPUT ACCEPT
# All wiped. INPUT = ACCEPT
&lt;/code&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;PureVPN:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Does not properly implement an IPv6 kill-switch&lt;/item&gt;
      &lt;item&gt;Leaves IPv6 egress open after reconnects or IKS events&lt;/item&gt;
      &lt;item&gt;Wipes your firewall state (&lt;code&gt;iptables&lt;/code&gt;) and does not restore it&lt;/item&gt;
      &lt;item&gt;Applies broad &lt;code&gt;ACCEPT&lt;/code&gt;policies to make things work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both issues have real-world impact. Privacy claims are undermined when your real IPv6 leaks and your firewall state is lost.&lt;/p&gt;
    &lt;p&gt;I submitted full technical reports and screencasts to security@purevpn.com. No acknowledgment to date.&lt;/p&gt;
    &lt;p&gt;Use with caution.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://anagogistis.com/posts/purevpn-ipv6-leak/"/><published>2025-09-17T10:10:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45273999</id><title>Determination of the fifth Busy Beaver value</title><updated>2025-09-17T14:10:00.832493+00:00</updated><content>&lt;doc fingerprint="a8be7401d4f7b0af"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Logic in Computer Science&lt;/head&gt;&lt;p&gt; [Submitted on 15 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Determination of the fifth Busy Beaver value&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We prove that $S(5) = 47,176,870$ using the Coq proof assistant. The Busy Beaver value $S(n)$ is the maximum number of steps that an $n$-state 2-symbol Turing machine can perform from the all-zero tape before halting, and $S$ was historically introduced by Tibor Radó in 1962 as one of the simplest examples of an uncomputable function. The proof enumerates $181,385,789$ Turing machines with 5 states and, for each machine, decides whether it halts or not. Our result marks the first determination of a new Busy Beaver value in over 40 years and the first Busy Beaver value ever to be formally verified, attesting to the effectiveness of massively collaborative online research (bbchallenge$.$org).&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.LO&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2509.12337"/><published>2025-09-17T10:26:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45274277</id><title>Apple Photos App Corrupts Images</title><updated>2025-09-17T14:10:00.594353+00:00</updated><content>&lt;doc fingerprint="7241bf3571248d9f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Apple Photos App Corrupts Images&lt;/head&gt;Sep 17, 2025 @ 8:59 am&lt;p&gt;The Apple Photos app sometimes corrupts images when importing from my camera. I just wanted to make a blog post about it in case anyone else runs into the problem. I’ve seen other references to this online, but most of the people gave up trying to fix it, and none of them went as far as I did to debug the issue.&lt;/p&gt;&lt;p&gt;I’ll try to describe the problem, and the things I’ve tried to do to fix it. But also note that I’ve (sort of) given up on the Photos app too. Since I can’t trust it to import photos from my camera, I switched to a different workflow.&lt;/p&gt;&lt;p&gt;Here is a screenshot of a corrupted image in the Photos app:&lt;/p&gt;&lt;head rend="h2"&gt;How I used to import images&lt;/head&gt;&lt;p&gt;I’ve got an OM System OM-1 camera. I used to shoot in RAW + jpg, then when I would import to Photos app, I would check the “delete photos after import” checkbox in order to empty the SD card. Turns out “delete after import” was a huge mistake.&lt;/p&gt;&lt;head rend="h2"&gt;Getting corrupted images&lt;/head&gt;&lt;p&gt;I’m pretty sure I’d been getting corrupted images for a while, but it would only be 1 or 2 images out of thousands, so I thought nothing of it (it was probably my fault anyway, right?)&lt;/p&gt;&lt;p&gt;But the problem really got me upset when last year I went to a family member’s wedding and took tons of photos. Apple Photos combines RAW + jpg photos so you don’t have a bunch of duplicates, and when you view the images in the photos app, it just shows you the jpg version by default. After I imported all of the wedding photos I noticed some of them were corrupted. Upon closer inspection, I found that it sometimes had corrupted the jpg, sometimes corrupted the RAW file, and sometimes both. Since I had been checking the “delete after import” box, I didn’t know if the images on the SD card were corrupted before importing or not. After all, the files had been deleted so there was no way to check.&lt;/p&gt;&lt;p&gt;I estimate I completely lost about 30% of the images I took that day.&lt;/p&gt;&lt;p&gt;Losing so many photos really rattled me, but I wanted to figure out the problem so I didn’t lose images in the future.&lt;/p&gt;&lt;head rend="h2"&gt;Narrowing down the problem&lt;/head&gt;&lt;p&gt;I was worried this was somehow a hardware problem. Copying files seems so basic, I didn’t think there was any way a massively deployed app like Photos could fuck it up (especially since its main job is managing photo files). So, to narrow down the issue I changed out all of the hardware. Here are all the things I did:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Switched USB-C cables&lt;/item&gt;&lt;item&gt;Bought a new SD card direct from the manufacturer (to eliminate the possibility of buying a bootleg SD card)&lt;/item&gt;&lt;item&gt;Switched to only shooting in RAW (if importing messes up 30% of my images, but I cut the number of images I import by half, then that should be fewer corrupted images right? lol)&lt;/item&gt;&lt;item&gt;Bought a new laptop&lt;/item&gt;&lt;item&gt;Bought a new camera: the OM System OM-1 MKii&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I did each of these steps over time, as to only change one variable at a time, and still the image corruption persisted. I didn’t really want to buy a new camera, the MKii is not really a big improvement over the OM-1, but we had a family trip coming up and the idea that pressing the shutter button on the camera might not actually record the image didn’t sit well with me.&lt;/p&gt;&lt;head rend="h2"&gt;Finally a smoking gun&lt;/head&gt;&lt;p&gt;Since I had replaced literally all of the hardware involved, I knew it must be a software problem. I stopped checking the “delete after import” button, and started reviewing all of the photos after import. After verifying none of them were corrupt, then I would format the SD card. I did this for months without finding any corrupt files. At this point I figured it was somehow a race condition or something when copying the photo files and deleting them at the same time.&lt;/p&gt;&lt;p&gt;However, after I got home from RailsConf and imported my photos, I found one corrupt image (the one above). I was able to verify that the image was not corrupt on the SD card, so the camera was working fine (meaning I probably didn’t need to buy a new camera body at all).&lt;/p&gt;&lt;p&gt;I tried deleting the corrupt file and re-importing the original to see if it was something about that particular image, but it re-imported just fine. In other words, it seems like the Photos app will corrupt files randomly.&lt;/p&gt;&lt;p&gt;I don’t know if this is a problem that is specific to OM System cameras, and I’m not particularly interested in investing in a new camera system just to find out.&lt;/p&gt;&lt;p&gt;If I compare the corrupted image with the non-corrupted image, the file sizes are exactly the same, but the bytes are different:&lt;/p&gt;&lt;p&gt;Checksums:&lt;/p&gt;&lt;code&gt;aaron@tc ~/Downloads&amp;gt; md5sum P7110136-from-camera.ORF Exports/P7110136.ORF 
17ce895fd809a43bad1fe8832c811848  P7110136-from-camera.ORF
828a33005f6b71aea16d9c2f2991a997  Exports/P7110136.ORF
&lt;/code&gt;&lt;p&gt;File sizes:&lt;/p&gt;&lt;code&gt;aaron@tc ~/Downloads&amp;gt; ls -al P7110136-from-camera.ORF Exports/P7110136.ORF
-rw-------@ 1 aaron  staff  18673943 Jul 12 04:38 Exports/P7110136.ORF
-rwx------  1 aaron  staff  18673943 Jul 17 09:29 P7110136-from-camera.ORF*
&lt;/code&gt;&lt;p&gt;The &lt;code&gt;P7110136-from-camera.ORF&lt;/code&gt; is the non-corrupted file, and &lt;code&gt;Exports/P7110136.ORF&lt;/code&gt; is the corrupted file from Photos app.
Here’s a screenshot of the preview of the non-corrupted photo:&lt;/p&gt;&lt;p&gt;Here is the binary diff between the files. I ran both files through &lt;code&gt;xxd&lt;/code&gt; then diffed them.&lt;/p&gt;&lt;head rend="h2"&gt;My new workflow&lt;/head&gt;&lt;p&gt;I’m not going to put any more effort into debugging this problem, but I wanted to blog about it in case anyone else is seeing the issue. I take a lot of photos, and to be frank, most of them are not very good. I don’t want to look through a bunch of bad photos every time I look at my library, so culling photos is important. Culling photos in the Photos app is way too cumbersome, so I’ve switched to using Darktable.&lt;/p&gt;&lt;p&gt;My current process is:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Import images to Darktable&lt;/item&gt;&lt;item&gt;Delete the ones I don’t like&lt;/item&gt;&lt;item&gt;Process ones I do like&lt;/item&gt;&lt;item&gt;Export both the jpg and the original raw file&lt;/item&gt;&lt;item&gt;Import those to the Photos app so they’re easy to view and share&lt;/item&gt;&lt;item&gt;Periodically format my SD card&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I’ve not seen any file corruption when importing to Darktable, so I am convinced this is a problem with the Photos app. But now, since all of my images land in Darktable before making their way to the Photos app, I don’t really care anymore. The bad news is that I’ve spent a lot of time and money trying to debug this. I guess the good news is that now I have redundant hardware!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tenderlovemaking.com/2025/09/17/apple-photos-app-corrupts-images/"/><published>2025-09-17T11:07:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45275049</id><title>Procedural Island Generation (III)</title><updated>2025-09-17T14:10:00.348197+00:00</updated><content>&lt;doc fingerprint="9da72cd84b738ab5"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Procedural Island Generation (III)&lt;/head&gt;&lt;p&gt;This post continues from Part II, where we established the paint map foundation and mountain ridge system. Now we’ll add detailed noise layers, distance-based mountain peaks, and do blending to create the final terrain elevation.&lt;/p&gt;&lt;head rend="h2"&gt;Paint Map (recap)&lt;/head&gt;&lt;p&gt;Before applying noise layers, we start with the foundation established in Part I - the paint map that defines our base land/water distribution:&lt;/p&gt;&lt;p&gt;For visualization throughout this series, we’ll be using the magma palette from matplotlib, which I patched to artificially darken the ocean areas to highlight the coastline:&lt;/p&gt;&lt;p&gt;Note that we’ll be sampling the paint map per Delaunay triangle (at each triangle’s centroid):&lt;/p&gt;&lt;p&gt;Remember that the paint map provides the broad strokes: positive values for land, negative for ocean, with smooth transitions between them. Now we’ll enhance it with noise layers to create realistic terrain detail.&lt;/p&gt;&lt;head rend="h2"&gt;Multi-Scale Noise Layers&lt;/head&gt;&lt;p&gt;We will layer multiple octaves of Simplex noise at different frequencies over the broad strokes provided by the paint map. Each will contribute different detail scales to the final terrain.&lt;/p&gt;&lt;p&gt;mapgen4 by @redblobgames in particular uses six layers:&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Layer&lt;/cell&gt;&lt;cell role="head"&gt;Frequency&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;n₀&lt;/cell&gt;&lt;cell&gt;1x&lt;/cell&gt;&lt;cell&gt;Lowest frequency&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;n₁&lt;/cell&gt;&lt;cell&gt;2x&lt;/cell&gt;&lt;cell&gt;Low frequency&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;n₂&lt;/cell&gt;&lt;cell&gt;4x&lt;/cell&gt;&lt;cell&gt;Medium-low frequency&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;n₄&lt;/cell&gt;&lt;cell&gt;16x&lt;/cell&gt;&lt;cell&gt;Medium-high frequency&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;n₅&lt;/cell&gt;&lt;cell&gt;32x&lt;/cell&gt;&lt;cell&gt;High frequency&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;n₆&lt;/cell&gt;&lt;cell&gt;64x&lt;/cell&gt;&lt;cell&gt;Highest frequency&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Notice the gap in numbering (n₃ is missing). This would correspond to frequency 8x, which we don’t use.&lt;/p&gt;&lt;head rend="h3"&gt;Coastal Noise Enhancement&lt;/head&gt;&lt;p&gt;mapgen4 starts with coastal noise enhancement. This provides control over the variation at coastlines while keeping inland elevation unaffected:&lt;/p&gt;\[e = \text{Paint map from Part I}\] \[e_{coast} = e + \alpha \cdot (1 - e^4) \cdot \left(n_4 + \frac{n_5}{2} + \frac{n_6}{4}\right)\]&lt;p&gt;The term \((1 - e^4)\) creates a bell curve that peaks at \(e=0\) (coastline) and decreases rapidly for \(\lvert e \rvert &amp;gt; 0\). This modulates an fBm-like combination of our three highest frequency noise layers.&lt;/p&gt;&lt;p&gt;What matters here isn’t the exact formula or amplitudes, but the core principle: applying high-frequency detail specifically where land meets water.&lt;/p&gt;\[e_{tmp} = \begin{cases} e &amp;amp; \text{if } e_{coast} &amp;gt; 0 \\ e_{coast} &amp;amp; \text{if } e_{coast} \leq 0 \end{cases}\]&lt;head rend="h2"&gt;Mountain Distance Field&lt;/head&gt;&lt;p&gt;Mountains need special pre-processing. If you remember from Part I in the swarm of seed points we tagged some as mountain peaks. Here we will pre-compute a distance field from every regular seed point to the closest mountain peak point.&lt;/p&gt;&lt;p&gt;We compute distance through the mesh topology of the Delaunay triangulation using BFS (breadth-first search). i.e., we don’t use Euclidean distance. This creates more organic mountain shapes that follow the terrain’s natural connectivity.&lt;/p&gt;&lt;p&gt;The algorithm spreads outward from mountain peaks:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Start at triangles containing mountain seed points (distance = 0)&lt;/item&gt;&lt;item&gt;Visit neighboring triangles, incrementing distance by a randomized amount&lt;/item&gt;&lt;item&gt;The randomization creates natural ridge patterns instead of perfect cones&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Here’s the magic formula used for distance increment in each step:&lt;/p&gt;\[\Delta = s \cdot (1 + j \cdot r)\]&lt;p&gt;Where:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;\(s\) = spacing between triangles (uses configured Poisson disk separation)&lt;/item&gt;&lt;item&gt;\(j\) = jaggedness parameter (0 = true topological distance, 1 = very irregular)&lt;/item&gt;&lt;item&gt;\(r \in [-1,1]\) = random factor using triangular distribution&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The triangular distribution &lt;code&gt;rand() - rand()&lt;/code&gt; clusters values near zero while allowing occasional larger variations. This looks more natural than uniform randomness.&lt;/p&gt;&lt;p&gt;I implemented Fisher-Yates shuffling when visiting neighbor triangles. Instead of processing neighbors in a fixed order (which would create directional bias), the order is randomly shuffled each time. This ensures mountain ridges branch out organically in all directions rather than following predictable patterns.&lt;/p&gt;&lt;p&gt;After computing distances this way, we normalize them (by the max dist, for example):&lt;/p&gt;&lt;head rend="h2"&gt;Elevation Blending&lt;/head&gt;&lt;p&gt;The final elevation combines all components through weighted blending:&lt;/p&gt;\[e_{final} = \begin{cases} \text{lerp}(e_{coast}^2, e_{hill}, e_{mountain}) &amp;amp; \text{if } e_{coast} &amp;gt; 0 \\ e_{coast} \cdot (\rho + n_1) &amp;amp; \text{if } e_{coast} \leq 0 \end{cases}\]&lt;p&gt;Where:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;\(e_{hill} = h \cdot (1 + \text{lerp}(\frac{1 + n_0}{2}, n_4, n_2))\) =&amp;gt; hill elevation with noise-modulated height&lt;/item&gt;&lt;item&gt;\(e_{mountain} = 1 - \frac{\mu}{2^\sigma} \cdot d_m\) =&amp;gt; mountain elevation from distance field&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The quadratic blend weight produces smooth transitions from hills near the coast through mixed terrain at mid-elevations to pure mountains at peaks.&lt;/p&gt;&lt;p&gt;With (editable) parameters:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;\(\alpha\): Coastal noise strength (0.01)&lt;/item&gt;&lt;item&gt;\(h\): Hill height scale (0.02)&lt;/item&gt;&lt;item&gt;\(\rho\): Ocean depth multiplier (1.5)&lt;/item&gt;&lt;item&gt;\(\mu\): Mountain slope (17.6)&lt;/item&gt;&lt;item&gt;\(\sigma\): Mountain sharpness (9.8)&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Interactive Parameter Exploration&lt;/head&gt;&lt;head rend="h2"&gt;Region (vs. Triangle) Elevation&lt;/head&gt;&lt;p&gt;So far we’ve computed elevation for triangles. But our Voronoi regions (from Part I) also need elevations for certain stages in the rest of the series.&lt;/p&gt;&lt;p&gt;Each seed point defines a Voronoi region and serves as a vertex in multiple Delaunay triangles. To assign elevation to a Voronoi region, we average the elevations of all triangles that share its seed point as a vertex.&lt;/p&gt;&lt;head rend="h2"&gt;Next Steps&lt;/head&gt;&lt;p&gt;With elevation complete, our island has shape but lacks the defining features carved by water. Part IV will simulate the hydrological cycle: rainfall patterns influenced by topography, rivers flowing from peaks to ocean, and valleys carved by erosion.&lt;/p&gt;&lt;head rend="h2"&gt;Valuable Resources&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Terrain from Noise - Amit Patel’s Red Blob Games guide to layering noise for terrain&lt;/item&gt;&lt;item&gt;Polygonal Map Generation - Red Blob Games on Voronoi-based terrain (mapgen4 inspiration)&lt;/item&gt;&lt;item&gt;Distance Fields for Terrain - Red Blob Games on using distance fields in terrain generation&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://brashandplucky.com/2025/09/17/procedural-island-generation-iii.html"/><published>2025-09-17T12:29:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45275354</id><title>Tau² Benchmark: How a Prompt Rewrite Boosted GPT-5-Mini by 22%</title><updated>2025-09-17T14:10:00.157623+00:00</updated><content>&lt;doc fingerprint="3f0e65d32093063a"&gt;
  &lt;main&gt;
    &lt;p&gt;In a recent post, we introduced the Tau² benchmark, a framework for benchmaring LLMs. Today we’re sharing a surprising discovery we made while using it: a simple prompt rewrite boosted a small model’s success rate by over 20%. This post is a deep-dive on how we found and fixed this performance bottleneck by making subtle changes to agent policies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarking LLMs with Tau²&lt;/head&gt;
    &lt;p&gt;On the recent OpenAI Summer Update, we have seen that GPT-5 model has made significant strides in agentic tasks. To validate these claims, they’ve turned to the Tau² benchmark, which simulates real-world agent interactions across various domains like telecom, retail, and airlines.&lt;/p&gt;
    &lt;p&gt;Before moving any further, we have to establish that GPT-5 showed significant improvement only in one benchmark domain - which is Telecom. The other ones have been somehow overlooked during model presentation - therefore we won’t bother about them either (😉).&lt;/p&gt;
    &lt;p&gt;In agentic interactions, accuracy is non-negotiable, but model speed is equally vital for user experience. Therefore, it makes sense to consider alternatives to flagship models, such as the recently introduced GPT-5-mini.&lt;/p&gt;
    &lt;p&gt;GPT-5-mini offers significant advantages: it’s roughly twice as fast in latency and noticeably more efficient in throughput. While delivering 85–95% of the full GPT-5’s performance, it is also five times cheaper.&lt;/p&gt;
    &lt;p&gt;Therefore, we ran an experiment to explore two things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How well GPT-5-mini performs on this benchmark.&lt;/item&gt;
      &lt;item&gt;Whether we can improve its results by making subtle changes to the domain, such as modifying agent policies or task descriptions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Baseline: Expect GPT-5-mini to Fail 45% of the Time&lt;/head&gt;
    &lt;p&gt;Firstly, we’re going to establish the benchmark for the GPT-5-mini model. As the telecom benchmark contains over 100 tests, we’ll use their subset. Luckily, the telecom_small task set comes in handy with just 20 test scenarios.&lt;/p&gt;
    &lt;p&gt;Running the benchmark with:&lt;/p&gt;
    &lt;code&gt;tau2 run \
    --domain telecom \ 
    --agent-llm gpt-5-mini \ 
    --user-llm gpt-5-mini \
    --num-trials 2 --task-set-name telecom_small&lt;/code&gt;
    &lt;p&gt;Our results are:&lt;/p&gt;
    &lt;p&gt;We ended up running 40 simulations:&lt;/p&gt;
    &lt;p&gt;The initial success rate was low: just 55%. The GPT-5-mini with its limited reasoning capabilities doesn’t even get close to flagship GPT-5.&lt;/p&gt;
    &lt;p&gt;There’s an additional interesting metric this benchmark has introduced, which is pass^k. This measures how well an agent can perform when it’s challenged with the same task k times. I like to think of it as the reliability of the AI Agent.&lt;lb/&gt; Another intriguing aspect of this benchmark are tasks which failed for all given trials - which could imply that the AI Agent is simply not capable of handling at all. This can happen due to multiple factors - reasoning might be too difficult, user ask could not be specific enough, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Hack: Using Claude to Rewrite Prompts for GPT-5-mini&lt;/head&gt;
    &lt;p&gt;When hacking the AI Agent for &lt;code&gt;GPT-5-mini&lt;/code&gt; model we aimed to answer three questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can I improve the overall success rate when using this limited model?&lt;/item&gt;
      &lt;item&gt;Are we able to “unlock” more tasks which agents are capable of handling?&lt;/item&gt;
      &lt;item&gt;Does agent reliability improve?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beauty of generative AI is that we could offload lots of work and allow ourselves a little laziness. Therefore, we asked Claude to analyze the AI agent policies in the telecom domain, which are the building blocks for the agent prompt. From there, we asked specifically to assume that these policies are going to feed an AI Agent prompt, which will be using faster, non-flagship LLM. Output documents (&lt;code&gt;main_policy.md&lt;/code&gt; and &lt;code&gt;tech_support_manual.md&lt;/code&gt;) will have to be rephrased so that they are easier to follow for the agent and result in improving its behaviour.&lt;/p&gt;
    &lt;p&gt;Here’s the “executive summary” of improvements it made:&lt;/p&gt;
    &lt;code&gt;I've created two AI-optimized versions of the telecom domain documentation:

Key Improvements Made:

1. Structure &amp;amp; Flow
    - Decision Trees: Clear branching logic with ├── and └── notation
    - Sequential Steps: Numbered, ordered procedures instead of scattered explanations
    - Prerequisites: Explicit dependency checks before proceeding

2. AI Agent Optimizations
    - Tool Call Clarity: Exact function names and parameters
    - Binary Decisions: Clear yes/no conditions instead of ambiguous language
    - Error Handling: Specific failure conditions and next steps
    - Verification Steps: "Recheck" instructions after each fix

3. Cognitive Load Reduction
    - Reference Tables: Quick lookup for tools and purposes
    - Pattern Recognition: Common issue combinations and their solutions
    - Critical Reminders: Common AI mistakes section to prevent errors

4. Actionable Language
    - Removed verbose explanations mixed with instructions
    - Consolidated multiple documents' logic into single workflows 
    - Used imperative commands: "Check X", "If Y then Z"
    - Added immediate verification steps&lt;/code&gt;
    &lt;p&gt;We’ve moved from lengthy policy declarations to clear, directive instructions. Instead of wordy statements, the new style uses step-by-step logic, conditions, and patterns. For prompts, this works more like a checklist — “Check X → If Y, then Z” — rather than vague, descriptive policies.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Result: 22% Accuracy Boost and 50% Fewer Unsolvable Tasks&lt;/head&gt;
    &lt;p&gt;Let’s review what our improved AI agent results look like:&lt;/p&gt;
    &lt;p&gt;The new prompts led to a significant performance boost. Pass^k metrics surged:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;k=1 from 0.55 to 0.675 (a 22.73% improvement) → In plain terms, GPT-5-mini now succeeds on 67.5% of tasks instead of 55%.&lt;/item&gt;
      &lt;item&gt;k=2 from 0.4 to 0.5 (a 25% improvement) → Meaning retries became more effective too.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For context, flagship GPT-5 scores ~97% on this benchmark, o3 comes in at 58%, and GPT-4.1 at 34%. With our optimized prompts, GPT-5-mini not only jumped well above its own baseline but also outperformed o3, landing much closer to GPT-5 than before.&lt;/p&gt;
    &lt;p&gt;The side-by-side comparison shows exactly where the gains came from. On the left side of the screen you’ll see the “stock” AI agent results, on the right - our AI agent improved for GPT-5-mini.&lt;/p&gt;
    &lt;p&gt;The screenshot above outlines that with our updated prompts and policies, we managed to “unlock” some of the tests which were previously always failing due to GPT-5-mini’s limited capabilities. Now there are only 3 tasks, which the agent didn’t manage to solve at all within the given 2 trials - compared to 6.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key Takeaways for Your Own Models&lt;/head&gt;
    &lt;p&gt;This experiment shows that thoughtful prompt design can meaningfully boost the performance of smaller models like GPT-5-mini. By restructuring policies into clear, step-by-step instructions, we not only improved success rates but also “unlocked” tasks that previously seemed unsolvable for the model.&lt;/p&gt;
    &lt;p&gt;The key was in simplifying language, reducing ambiguity, and breaking down reasoning into explicit, actionable steps. Smaller models struggle with long-winded or fuzzy policies, but thrive when given structured flows, binary decisions, and lightweight verification steps.&lt;/p&gt;
    &lt;p&gt;The takeaway is clear: using a frontier model to automatically optimize prompts can unlock major improvements for smaller LLMs. With strategic optimization, lightweight models can deliver decent results at a fraction of the cost — making them a compelling alternative when efficiency and affordability matter as much as accuracy.&lt;/p&gt;
    &lt;p&gt;If you found this helpful, let us know! Prompt engineering is still an open playground, and we’re excited to see what creative approaches others are exploring in this space.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://quesma.com/blog/tau2-benchmark-improving-results-smaller-models/"/><published>2025-09-17T13:03:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45275444</id><title>Firefox 143 for Android to introduce DoH</title><updated>2025-09-17T14:10:00.025943+00:00</updated><content>&lt;doc fingerprint="9f6ab3c3bc9540af"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Firefox DNS privacy: Faster than ever, now on Android&lt;/head&gt;
    &lt;p&gt;All web browsing starts with a DNS query to find the IP address for the desired service or website. For much of the internet’s history, this query is sent in the clear. DNS-over-HTTPS (DoH) plugs this privacy leak by encrypting the DNS messages, so no one on the network, not your internet service provider or a free public WiFi provider, can eavesdrop on your browsing.&lt;lb/&gt;In 2020, Firefox became the first browser to roll out DoH by default, starting in the United States and in 2023, we announced the Firefox DoH-by-default rollout in Canada, powered by our trusted partner, the Canadian Internet Registration Authority (CIRA).&lt;/p&gt;
    &lt;p&gt;This year, we’ve built on that foundation and delivered major performance improvements and mobile support, ensuring more Firefox users benefit from privacy without compromise.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing DoH for Android&lt;/head&gt;
    &lt;p&gt;After bringing encrypted DNS protection to millions of desktop users, we’re now extending the same to mobile. Firefox users who have been waiting for DoH on Android can now turn it on and browse with the same privacy protections as on their desktops.&lt;/p&gt;
    &lt;p&gt;Starting with this week’s release of Firefox 143 for Android, users can choose to enable DoH in Firefox on their mobile devices by selecting “Increased Protection” DoH configuration. Performance testing with Firefox DoH partners is currently underway. If DoH is as fast as we expect, we plan to enable it by default for Android users in certain regions, similar to desktop users. Until then, these configuration options provide you the choice to opt in early.&lt;/p&gt;
    &lt;head rend="h2"&gt;DoH performance breakthroughs in 2025&lt;/head&gt;
    &lt;p&gt;DNS resolution speed is critical to the browsing experience — when web pages involve multiple DNS queries, the speed difference compounds and can cause page loads to be slow. Since we first rolled out DoH in Canada, we’ve worked closely with CIRA for reliability and performance measurements. Through our strong collaboration with them and their technology partner Akamai, Firefox DoH lookups are now 61% faster year-to-date for the 75th percentile.&lt;/p&gt;
    &lt;p&gt;With these performance improvements, DoH resolution time is now within a millisecond or two of native DNS resolution. This is a big win because Firefox users in Canada now get the privacy of encrypted DNS with no performance penalty.&lt;/p&gt;
    &lt;p&gt;Although the investigation and analysis started with the desire to improve DoH in Firefox, the benefits didn’t end there. Our collaboration also improved CIRA DoH performance for many of its DNS users, including Canadian universities, as well as other DNS providers relying on CIRA’s or Akamai’s server implementations.&lt;/p&gt;
    &lt;p&gt;This is a win not just for Firefox users, but for the many other users around the globe.&lt;/p&gt;
    &lt;head rend="h2"&gt;Robust privacy on your terms&lt;/head&gt;
    &lt;p&gt;We have always approached DoH with an emphasis on transparency, user choice, and strong privacy safeguards. Firefox gives users meaningful control over how their DNS traffic is handled: Users can opt out, choose their own resolver, or adjust DoH protection levels, and Firefox makes it clear what DoH is doing and why it matters.&lt;/p&gt;
    &lt;p&gt;Firefox enforces strict requirements for DNS resolvers before trusting them with your browsing. Not every DNS provider can become a DoH provider in Firefox — only those that meet and attest to Mozilla’s rigorous Trusted Recursive Resolver (TRR) policy through a legally binding contract.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prioritizing your privacy and speed&lt;/head&gt;
    &lt;p&gt;Our work with DoH this year shows what’s possible when privacy and performance go hand-in-hand. We’ve proven that encrypted DNS can be fast, reliable, and available on desktop and Android. Just as importantly, we’ve shown that partnerships grounded in open standards and accountability can deliver benefits not only to Firefox users but to the wider internet.&lt;/p&gt;
    &lt;p&gt;As we look forward, our commitment stays the same: Privacy should be the default, speed should never be a compromise, and the web should remain open and accessible to everyone. Choosing Firefox means choosing a browser that is built for you and for a better internet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.mozilla.org/en/firefox/dns-android/"/><published>2025-09-17T13:14:04+00:00</published></entry></feed>