<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-30T07:10:32.572303+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45748661</id><title>Tell HN: Azure outage</title><updated>2025-10-30T07:10:39.240315+00:00</updated><content>&lt;doc fingerprint="7e5891552f06a7d5"&gt;
  &lt;main&gt;
    &lt;p&gt;15:45 UTC on 29 October 2025 – Customer impact began.&lt;/p&gt;
    &lt;p&gt;16:04 UTC on 29 October 2025 – Investigation commenced following monitoring alerts being triggered.&lt;/p&gt;
    &lt;p&gt;16:15 UTC on 29 October 2025 – We began the investigation and started to examine configuration changes within AFD.&lt;/p&gt;
    &lt;p&gt;16:18 UTC on 29 October 2025 – Initial communication posted to our public status page.&lt;/p&gt;
    &lt;p&gt;16:20 UTC on 29 October 2025 – Targeted communications to impacted customers sent to Azure Service Health.&lt;/p&gt;
    &lt;p&gt;17:26 UTC on 29 October 2025 – Azure portal failed away from Azure Front Door.&lt;/p&gt;
    &lt;p&gt;17:30 UTC on 29 October 2025 – We blocked all new customer configuration changes to prevent further impact.&lt;/p&gt;
    &lt;p&gt;17:40 UTC on 29 October 2025 – We initiated the deployment of our ‘last known good’ configuration.&lt;/p&gt;
    &lt;p&gt;18:30 UTC on 29 October 2025 – We started to push the fixed configuration globally.&lt;/p&gt;
    &lt;p&gt;18:45 UTC on 29 October 2025 – Manual recovery of nodes commenced while gradual routing of traffic to healthy nodes began after the fixed configuration was pushed globally.&lt;/p&gt;
    &lt;p&gt;23:15 UTC on 29 October 2025 - PowerApps mitigation of dependency, and customers confirm mitigation.&lt;/p&gt;
    &lt;p&gt;00:05 UTC on 30 October 2025 – AFD impact confirmed mitigated for customers.&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. In addition. customers may experience issues accessing the Azure Portal. Customers can attempt to use programmatic methods (PowerShell, CLI, etc.) to access/utilize resources if they are unable to access the portal directly. We have failed the portal away from Azure Front Door (AFD) to attempt to mitigate the portal access issues and are continuing to assess the situation.&lt;/p&gt;
    &lt;p&gt;We are actively assessing failover options of internal services from our AFD infrastructure. Our investigation into the contributing factors and additional recovery workstreams continues. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:57 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Update: 16:35 UTC:&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:18 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. We suspect that an inadvertent configuration change as the trigger event for this issue. We are taking two concurrent actions where we are blocking all changes to the AFD services and at the same time rolling back to our last known good state.&lt;/p&gt;
    &lt;p&gt;We have failed the portal away from Azure Front Door (AFD) to mitigate the portal access issues. Customers should be able to access the Azure management portal directly.&lt;/p&gt;
    &lt;p&gt;We do not have an ETA for when the rollback will be completed, but we will update this communication within 30 minutes or when we have an update.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 17:17 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;"We have initiated the deployment of our 'last known good' configuration. This is expected to be fully deployed in about 30 minutes from which point customers will start to see initial signs of recovery. Once this is completed, the next stage is to start to recover nodes while we route traffic through these healthy nodes."&lt;/p&gt;
    &lt;p&gt;"This message was last updated at 18:11 UTC on 29 October 2025"&lt;/p&gt;
    &lt;p&gt;At this stage, we anticipate full mitigation within the next four hours as we continue to recover nodes. This means we expect recovery to happen by 23:20 UTC on 29 October 2025. We will provide another update on our progress within two hours, or sooner if warranted.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 19:57 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;in many cases: no service health alerts, no status page updates and no confirmations from the support team in tickets. still we can confirm these issues from different customers accross europe. Mostly the issues are regional dependent.&lt;/p&gt;
    &lt;p&gt;This is the single most frustrating thing about these incidents. As you're harmstrung on what you can do or how you can react until Microsoft officially acknowledges a problem. Took nearly 90mins both today and when it happened on 9th October.&lt;/p&gt;
    &lt;p&gt;It's pretty unlikely. AWS published a public 'RCA' https://aws.amazon.com/message/101925/. A race condition in a DNS 'record allocator' causing all DNS records for DDB to be wiped out.&lt;/p&gt;
    &lt;p&gt;I'm simplifying a bit, but I don't think it's likely that Azure has a similar race condition wiping out DNS records on _one_ system than then propagates to all others. The similarity might just end at "it was DNS".&lt;/p&gt;
    &lt;p&gt;That RCA was fun. A distributed system with members that don't know about each other, don't bother with leader elections, and basically all stomp all over each other updating the records. It "worked fine" until one of the members had slightly increased latency and everything cascade-failed down from there. I'm sure there was missing (internal) context but it did not sound like a well-architected system at all.&lt;/p&gt;
    &lt;p&gt;THIS is the real deal. Some say it's always DNS but many times it's some routing fuckup with BGP. two most cursed 3 letter acronym technologies out there&lt;/p&gt;
    &lt;p&gt;Whilst the status message acknowledge's the issue with Front Door (AFD), it seems as though the rest of the actions are about how to get Portal/internal services working without relying on AFD. For those of us using Front Door does that mean we're in for a long haul?&lt;/p&gt;
    &lt;p&gt;Yeah, I am guessing it's just a placeholder till they get more info. I thought I saw somewhere that internally within Microsoft it's seen as a "Sev 1" with "all hands on deck" - Annoyingly I can't remember where I saw it, so if someone spots it before I do, please credit that person :D&lt;/p&gt;
    &lt;p&gt;It's a Sev 0 actually (as one would expect - this isn't a big secret). I was on the engineering bridge call earlier for a bit. The Azure service I work on was minimally impacted (our customer facing dashboard could not load, but APIs and data layer were not impacted) but we found a workaround.&lt;/p&gt;
    &lt;p&gt;yea I saw that, but im not sure on how accurate that is. a few large apps/companies I know to be 100% on AWS in us-east-1 are cranking along just fine.&lt;/p&gt;
    &lt;p&gt;We already had to do it for large files served from Blob Storage since they would cap out at 2MB/s when not in cache of the nearest PoP. If you’ve ever experienced slow Windows Store or Xbox downloads it’s probably the same problem.&lt;/p&gt;
    &lt;p&gt;I had a support ticket open for months about this and in the end the agent said “this is to be expected and we don’t plan on doing anything about it”.&lt;/p&gt;
    &lt;p&gt;We’ve moved to Cloudflare and not only is the performance great, but it costs less.&lt;/p&gt;
    &lt;p&gt;Only thing I need to move off Front Door is a static website for our docs served from Blob Storage, this incident will make us do it sooner rather than later.&lt;/p&gt;
    &lt;p&gt;we are considering the same but because our website uses APEX domain we would need to move all DNS resolver to cloudfront right ? Does it have as a nice "rule set builder" as azure ?&lt;/p&gt;
    &lt;p&gt;Unless you pay for CloudFlare’s Enterpise plan, you’re required to have them host your DNS zone, you can use a different registrar as long as you just point your NS records to Cloudflare.&lt;/p&gt;
    &lt;p&gt;Be aware that if you’re using Azure as your registrar, it’s (probably still) impossible to change your NS records to point to CloudFlare’s DNS server, at least it was for me about 6 months ago.&lt;/p&gt;
    &lt;p&gt;This also makes it impossible to transfer your domain to them either, as CloudFlare’s domain transfer flow requires you set your NS records to point to them before their interface shows a transfer option.&lt;/p&gt;
    &lt;p&gt;In our case we had to transfer to a different registrar, we used Namecheap.&lt;/p&gt;
    &lt;p&gt;However, transferring a domain from Azure was also a nightmare. Their UI doesn’t have any kind of transfer option, I eventually found an obscure document (not on their Learn website) which had an az command which would let you get a transfer code which I could give to Namecheap.&lt;/p&gt;
    &lt;p&gt;Then I had to wait over a week for the transfer timeout to occur because there is no way on Azure side that I could find to accept the transfer immediately.&lt;/p&gt;
    &lt;p&gt;I found CloudFlare’s way of building rules quite easy to use, different from Front Door but I’m not doing anything more complex than some redirects and reverse proxying.&lt;/p&gt;
    &lt;p&gt;I will say that Cloudflare’s UI is super fast, with Front Door I always found it painfully slow when trying to do any kind of configuration.&lt;/p&gt;
    &lt;p&gt;Cloudflare also doesn’t have the problem that Front Door has where it requires a manual process every 6 months or so to renew the APEX certificate.&lt;/p&gt;
    &lt;p&gt;Thanks :). We don't use Azure as our registrar. It seems I'll have to plan for this then, we also had another issue, AFD has a hard 500ms tls handshake timeout (doesn't matter how much you put on the origin timeout settings) which means if our server was slow for some reason we would get 504 origin timeout.&lt;/p&gt;
    &lt;p&gt;They briefly had a statement about using Traffic Manager to work with your AFD to work around this issue, with a link to learn.microsoft.com/...traffic-manager, and the link didn't work. Due to the same issue affecting everyone right now.&lt;/p&gt;
    &lt;p&gt;They quickly updated the message to REMOVE the link. Comical at this point.&lt;/p&gt;
    &lt;p&gt;I noticed that Starbucks mobile ordering was down and thought “welp, I guess I’ll order a bagel and coffee on Grubhub”, then GrubHub was down. My next stop was HN to find the common denominator, and y’all did not disappoint.&lt;/p&gt;
    &lt;p&gt;I’ve seen this up close twice and I’m surprised it’s only twice. Between March and September one year, 6 people on one team had to get new hard drives in their thinkpads and rebuild their systems. All from the same PO but doled out over the course of a project rampup. That was the first project where the onboarding docs were really really good, since we got a lot of practice in a short period of time.&lt;/p&gt;
    &lt;p&gt;Long before that, the first raid array anyone set up for my (teams’) usage, arrived from Sun with 2 dead drives out of 10. They RMA’d us 2 more drives and one of those was also DOA. That was a couple years after Sun stopped burning in hardware for cost savings, which maybe wasn’t that much of a savings all things considered.&lt;/p&gt;
    &lt;p&gt;Many years ago (13?), I was around when Amazon moved SABLE from RAM to SSDs. A whole rack came from a single batch, and something like 128 disks went out at once.&lt;/p&gt;
    &lt;p&gt;I was an intern but everyone seemed very stressed.&lt;/p&gt;
    &lt;p&gt;Why? Starbucks is not providing a critical service. Spending less money and resources and just accepting the risk that occasionally you won't be able to sell coffee for a few hours is a completely valid decision from both management and engineering pov.&lt;/p&gt;
    &lt;p&gt;I noticed it when my Netatmo rigamajig stopped notifying me of bad indoor air quality. Lovely. Why does it need to go through the cloud if the data is right there in the home network…&lt;/p&gt;
    &lt;p&gt;My inner Nelson-from-the-Simpsons wishes I was on your team today, able to flaunt my flask of tea and homemade packed sandwiches. I would tease you by saying 'ha ha!' as your efforts to order coffee with IP packets failed.&lt;/p&gt;
    &lt;p&gt;I always go everywhere adequately prepared for beverages and food. Thanks to your comment, I have a new reason to do so. Take out coffees are actually far from guaranteed. Payment systems could go down, my bank account could be hacked or maybe the coffee shop could be randomly closed. Heck, I might even have an accident crossing the road. Anything could happen. Hence, my humble flask might not have the top beverage in it but at least it works.&lt;/p&gt;
    &lt;p&gt;We all design systems with redundancy, backups and whatnot, but few of us apply this thinking to our food and drink. Maybe get a kettle for the office and a backup kettle, in case the first one fails?&lt;/p&gt;
    &lt;p&gt;It still surprises me how much essential services like public transport are completely reliant on cloud providers, and don't seem to have backups in place.&lt;/p&gt;
    &lt;p&gt;Here in The Netherlands, almost all trains were first delayed significantly, and then cancelled for a few hours because of this, which had real impact because today is also the day we got to vote for the next parlement (I know some who can't get home in time before the polls close, and they left for work before they opened).&lt;/p&gt;
    &lt;p&gt;Is voting there a one day only event? If not, I feel the solution to that particular problem is quite clear. There’s a million things that could go wrong causing you to miss something when you try to do it in a narrow time range (today after work before polls close)&lt;/p&gt;
    &lt;p&gt;If it’s a multi day event, it’s probably that way for a reason. Partially the same as the solution to above.&lt;/p&gt;
    &lt;p&gt;In europe, voting typically happens in one day, where everyone physically goes to their designated voting place and puts papers in a transparent box. You can stay there and wait for the count at the end of the day if you want to. Tom Scott has a very good video about why we don't want electronic/mail voting: https://www.youtube.com/watch?v=w3_0x6oaDmI&lt;/p&gt;
    &lt;p&gt;Well "mail in voting" in Washington state pretty much means you drop off your ballot in a drop box in your neighborhood. Which is pretty much the same thing as putting it in a ballot box.&lt;/p&gt;
    &lt;p&gt;The description of voting in the Netherlands is that you can see your ballot physically go into a clear box and stay to see that exact box be opened and all ballots tallied.&lt;/p&gt;
    &lt;p&gt;Dropping a ballot in a box in tour neighborhood helps ensure nothing with regards to the actually ballot count.&lt;/p&gt;
    &lt;p&gt;Here in NZ when I've been to vote, there are usually a couple of party affiliates at the voting location, doing what one of the parent posts described:&lt;/p&gt;
    &lt;p&gt;&amp;gt; You can stay there and wait for the count at the end of the day if you want to.&lt;/p&gt;
    &lt;p&gt;And if you watch the election night news, you'll see footage of multiple people counting the votes from the ballot boxes, again with various people observing to check that nothing dodgy is going on.&lt;/p&gt;
    &lt;p&gt;Having everyone just put their ballots in a postbox seems like a good way remove public trust from the electoral system, because noone's standing around waiting for the postie to collect the mail, or looking at what happens in the mail truck, or the rest of the mail distribution process.&lt;/p&gt;
    &lt;p&gt;I'm sure I've seen reports in the US of people burning postboxes around election time. Things like this give more excuses to treat election results as illegitimate, which I believe has been an issue over there.&lt;/p&gt;
    &lt;p&gt;(Yes, we do also have advanced voting in NZ, but I think they're considered "special votes" and are counted separately .. the elections are largely determined on the day by in-person votes, with the special votes being confirmed some days later)&lt;/p&gt;
    &lt;p&gt;I’m not sure what’s so special in Oregon’s ballot boxes. But, tampering that is detected (don’t need much special to detect a burning box I guess!) is not a complete failure for a system. If any elections were close enough for a box to matter, they could have rerun them.&lt;/p&gt;
    &lt;p&gt;In Sweden, mail/early votes get sent through the postal system to the official ballot box for those votes. In 2018, a local election had to be redone because the post delivered votes late. Mail delivery occasionally have packaged delayed or lost, and votes are note immune to this problem. In one case the post also gave the votes to an unauthorized person, through the votes did end up at the right place.&lt;/p&gt;
    &lt;p&gt;It is a small but distinct difference between mail/early voting and putting the votes directly into the ballot box.&lt;/p&gt;
    &lt;p&gt;If you wish, you can write a phrase on your ballot. The phrases and their corresponding vote are broadcast (on tv, internet, etc). So if you want to validate that your vote was tallied correctly, write a unique phrase. Or you could pick a random 30 digit number, collisions should be zero-probability, right?&lt;/p&gt;
    &lt;p&gt;I mean, this would be annoying because people would write slurs and advertisements, and the government would have to broadcast them. But, it seems pretty robust.&lt;/p&gt;
    &lt;p&gt;I’d suggest the state handle the number issuing, but then they could record who they issues which numbers to, and the winning party could go about rounding up their opposition, etc.&lt;/p&gt;
    &lt;p&gt;Googling around a bit, it sounds like there are systems that let you verify that your ballot made it, but not necessarily that it was counted correctly. (For this reason, I guess?)&lt;/p&gt;
    &lt;p&gt;You have to trust that whole system. Maybe you do, I don't know the details of how any of that works.&lt;/p&gt;
    &lt;p&gt;When I vote in person, I know all the officials there from various parties are just like...looking at the box for the whole day to make sure everything is counted. It's much easier to understand and trust.&lt;/p&gt;
    &lt;p&gt;Off the top of my head, I can't think of an EU country that does not have some form of advance voting.&lt;/p&gt;
    &lt;p&gt;Here in Latvia the "election day" is usually (always?) on weekend, but the polling stations are open for some (and different!) part of every weekday leading up. Something like couple hours on monday morning, couple hours on tuesday evening, couple around midday wednesday, etc. In my opinion, it's a great system. You have to have a pretty convoluted schedule for at least one window not to line up for you.&lt;/p&gt;
    &lt;p&gt;I think they meant "don't have it" as in except in special circumstances, and that form says:&lt;/p&gt;
    &lt;p&gt;&amp;gt; You may use this form to apply for a postal vote if, due to the circumstances of your work/service or your full-time study in the State, you cannot go to your polling station on polling day.&lt;/p&gt;
    &lt;p&gt;Which seems to indicate that's only for people who can't go to the polling station, otherwise you do have to go there.&lt;/p&gt;
    &lt;p&gt;I think that a lot of Ireland's voting practices come from having a small population but a huge diaspora. I imagine the percentage of people living outside Ireland what would be eligible to vote in many other countries is significant enough to effect elections, certainly if they are close.&lt;/p&gt;
    &lt;p&gt;As someone who spent the first 30 years of my life in Ireland but is now part of that diaspora, it's frustrating but I get it. I don't get to vote, but neither do thousands of plastic paddys who have very little genuine connection to Ireland.&lt;/p&gt;
    &lt;p&gt;That said, I'm sure they could expand the voting window to a couple of days at least without too much issue.&lt;/p&gt;
    &lt;p&gt;Italy has mail-in vote only for citizen residing abroad. The rest vote on the election Sunday (and Monday morning in some cases, at least in the past).&lt;/p&gt;
    &lt;p&gt;You don't have to attribute any name to the transaction, just a voting booth ID and the vote. The actual benefit is just that it is hard to tamper and easy to trace where tampering happened.&lt;/p&gt;
    &lt;p&gt;But I still prefer the paper vote and I usually a blockchain apathetic.&lt;/p&gt;
    &lt;p&gt;Anonymous voting means that you can't sell your vote. Like, if I pay you $5 to vote for X, but I can't actually verify that you voted for X and not Y, then I wouldn't bother trying. Or if I'm your boss and I want you to vote for X... etc.&lt;/p&gt;
    &lt;p&gt;Washington State having full vote-by-mail (there is technically a layer of in-person voting as a fallback for those who need it for accessibility reasons or who missed the registration deadline) has spoiled me rotten, I couldn't imagine having to go back to synchronous on-site voting on a single day like I did in Illinois. Awful. Being able to fill my ballot at my leisure, at home, where I can have all the research material open, and drive it to a ballot drop box whenever is convenient in a 2-3 week window before 20:00 on election night, is a game-changer for democracy. Of course this also means that people who serve to benefit from disenfranchising voters and making it more difficult to vote, absolutely hate our system and continually attack it for one reason or another.&lt;/p&gt;
    &lt;p&gt;As a Dutchman, I have to go vote in person on a specific day. But to be honest: I really don't mind doing so. If you live in a town or city, there'll usually be multiple voting locations you can choose from within 10 minutes walking distance. I've never experienced waiting times more than a couple of minutes. Opening times are pretty good, from 7:30 til 21:00. The people there are friendly. What's not to like? (Except for some of the candidates maybe, but that's a whole different story. :-))&lt;/p&gt;
    &lt;p&gt;Please lookup US voting poll overflow issues that come up every election cycle. Just because you experience a well streamlined process doesn't mean that it's the norm everywhere.&lt;/p&gt;
    &lt;p&gt;We're on year five of one of the two parties telling voters to not trust early voting. Their choice is because of the Fear, Uncertainty, and Doubt created by the propaganda they are fed.&lt;/p&gt;
    &lt;p&gt;"No mail-in or 'Early' Voting, Yes to Voter ID! Watch how totally dishonest the California Prop Vote is! Millions of Ballots being 'shipped.' GET SMART REPUBLICANS, BEFORE IT IS TOO LATE!!!"&lt;/p&gt;
    &lt;p&gt;That's all happening too, but it's honestly a different topic altogether. We have the ability to vote early. Whether you trust it or politicians are trying to undermine your trust in it, etc.... whole other can of worms&lt;/p&gt;
    &lt;p&gt;So, if you have a minor emergency, like a kidney stone and hospitalized for the day - you just miss your chance to vote in that election?&lt;/p&gt;
    &lt;p&gt;If so, I see a lot to dislike. As the point I was making is you can’t anticipate what might come up. Just because it’s worked thus far doesn’t mean it’s designed for resilience. There’s a lot of ways you could miss out in that type of situation. I seems silly to make sure everything else is redundant and fault tolerant in the name of democracy when the democratic process itself isn’t doing the same.&lt;/p&gt;
    &lt;p&gt;How is that an acceptable response? Honestly. You’re in the hospital, in pain, likely having a minor surgery, and having someone cast your vote for you is going to be on your mind too? Do you have your voting card in your pocket just in case this were to play out?&lt;/p&gt;
    &lt;p&gt;That’s just ridiculous in my opinion. Makes me wonder how many well intentioned would be voters end up missing out each election cause shit happens and voting is pretty optional&lt;/p&gt;
    &lt;p&gt;Mild curiosity, no idea whether it would be statistically relevant but asking the question is the first step. If you knew the answer, you might want to extend the voting window even if it wouldn't effect an elections outcome it would be a quantified number of people excluded from the democratic process for simply having bad luck at the wrong time.&lt;/p&gt;
    &lt;p&gt;If India can have voters vote and tally all the votes in one day, then so can everyone else. It’s the best way to avoid fraud and people going with whoever is ahead. I am sympathetic with emergency protocols for deadly pandemics, but for all else, in-person on a given day.&lt;/p&gt;
    &lt;p&gt;&amp;gt; If India can have voters vote and tally all the votes in one day, then so can everyone else.&lt;/p&gt;
    &lt;p&gt;In most countries, in the elections you vote or the member of parliament you want. Presidential elections, and city council elections are held separately, but are also equally simple. But in one election you cast your vote for one person, and that's it.&lt;/p&gt;
    &lt;p&gt;With this kind of elections, many countries manage to hold the elections on paper ballots, count them all by hand, and publish results by midnight.&lt;/p&gt;
    &lt;p&gt;But on an American ballot, you vote for, for example:&lt;/p&gt;
    &lt;p&gt;- US president - US senator - US member of congress - state governor - state senator - state member of congress - several votes for several different state judge positions - several other state officer positions - several votes for several local county officers - local sheriff - local school board member - several yes/no votes for several proposed laws, whether they should be passed or not&lt;/p&gt;
    &lt;p&gt;I don't think it would be possible to calculate all these 20 or 40 votes, if calculated by hand. That's why they use voting machines in America.&lt;/p&gt;
    &lt;p&gt;Say, how many voting stations are there in a typical city/county in the US?&lt;/p&gt;
    &lt;p&gt;Here in Indonesia, in a city of 2 million people there are over 7000 voting stations. While we vote for 5 ballots (President, Legislative (National, Province, and City/Regency), we still use paper ballots and count them by hand.&lt;/p&gt;
    &lt;p&gt;How is it not possible? It's just additional votes, there isn't anything actually stopping counting by hand, is there? How was it counted historically without voting machines?&lt;/p&gt;
    &lt;p&gt;If it's not a national holiday where the vast majority of people don't have to work, and if there aren't polling places reasonably near every voting age citizen, it's voter suppression.&lt;/p&gt;
    &lt;p&gt;In particular India has a law that no one shall be made to walk more than 2km to vote. The Indian military will literally deploy a voting booth into the jungle so that a single caretaker of an old temple can vote.&lt;/p&gt;
    &lt;p&gt;Here in Belgium voting is usually done during the weekend, although it shouldn't matter because voting is a civic duty (unless you have a good reason you have to go vote or you'll be fined), so those who work during the weekend have a valid reason to come in late or leave early.&lt;/p&gt;
    &lt;p&gt;In the US, where I assume a lot of the griping comes from, election day is not a national holiday, nor is it on a weekend (in fact, by law it is defined as "the Tuesday next after the first Monday in November"), and even though it is acknowledged as an important civic duty, only about half of the states have laws on the books that require employers provide time off to vote. There are no federal laws to that effect, so it's left entirely to states to decide.&lt;/p&gt;
    &lt;p&gt;In Australia there are so many places to vote, it is almost popping out to get milk level if convenience. Just detour your dog walk slightly. Always at the weekend.&lt;/p&gt;
    &lt;p&gt;In Australia there are so many places to vote, it is almost popping out to get milk level if convenience. (At least in urbia and suburbia) Just detour your dog walk slightly. Always at the weekend.&lt;/p&gt;
    &lt;p&gt;In the US getting milk involves driving multiple miles, finding parking, walking to the store, finding a shopping cart, finding the grocery department, navigating the aisles to the dairy section, finding the milk, waiting in line to check out, returning the cart if you’re courteous, and driving back. Could take an hour or so.&lt;/p&gt;
    &lt;p&gt;In washington we have a 100% mail-in voting system (for all intents and purposes). I can put my ballot back in the mail or drop at any number of drop-boxes throughout the city (less dropboxes in rural areas i'm sure). I think there are some allowances for in-person voting but I don't think they are often used.&lt;/p&gt;
    &lt;p&gt;There is a ballot tracking system as well, I can see and be notified as my ballot moves through the counting system. It's pretty cool.&lt;/p&gt;
    &lt;p&gt;I actually just got back from dropping off my local elections ballot 15m ago, quick bike trip maybe a mile or so away and back.&lt;/p&gt;
    &lt;p&gt;Of course, because it makes it easy for people to vote, the republicans want to do away with it. If you have to stand in line for several hours (which seems to be very normal in most cities) and potentially miss work to do it that's going to all but guarantee that working people and the less motivated will not vote.&lt;/p&gt;
    &lt;p&gt;So yes in places that only do in person voting, national or state holiday.&lt;/p&gt;
    &lt;p&gt;Yet... deploy on two clouds and you'll get tax payers scream at you for "wasting money" preparing for a black swan event. Can't have both, either reliability or lower cost.&lt;/p&gt;
    &lt;p&gt;i'm not sure this is an easily solvable problem. i remember reading an article arguing that your cloud provider is part of your tech stack and it's close to impossible/a huge PITA to make a non-trivial service provider-agnostic. they'd have to run their own openstack in different datacenters, which would be costly and have their own points of failure.&lt;/p&gt;
    &lt;p&gt;I run non trivial services on EC2, using that service as a VPS. My deploy script works just as well on provisioned Digital Ocean services and on docker containers using docker-compose.&lt;/p&gt;
    &lt;p&gt;I do need a human to provision a few servers and configure e.g. load balancing and when to spin up additional servers under load. But that is far less of a PITA than having my systems tied to a specific provider or down whenever a cloud precipitates.&lt;/p&gt;
    &lt;p&gt;The moment you choose to use S3 instead of hosting your own object store, though, you either use AWS because S3 and IAM already have you or spend more time on the care and feeding of your storage system as opposed to actually doing the thing you customers are paying you to do.&lt;/p&gt;
    &lt;p&gt;It's not impossible, just complicated and difficult for any moderately complex architecture.&lt;/p&gt;
    &lt;p&gt;dang even zealand didn't survive! new zealand got some soul searching with this outage which took down government person ID service, it's called RealME and it can be used to file your taxes apply for passport etc&lt;/p&gt;
    &lt;p&gt;The Flemish bus company (de Lijn) uses Azure and I couldn't activate my ticket when I came home after training a couple of hours ago. I should probably start using physical tickets again, because at least those work properly. It's just stupid that there's so much stuff being moved to digital only (often even only being accessible through an Android or iOS app, despite the parent companies of those two being utterly atrocious) when the physical alternatives are more reliable.&lt;/p&gt;
    &lt;p&gt;Organizations who had their own datacenters were chided for being resistant to modernizing, and now they modernized to use someone else's shared computers and they stopped working.&lt;/p&gt;
    &lt;p&gt;I really do feel the only viable future for clouds is hybrid or agnostic clouds.&lt;/p&gt;
    &lt;p&gt;can't believe it's 2025 and some still need to go to some place to vote. I can vote since I can remember(at least 20 years) by mail for anything, we also vote multiple times a year(4-6 times), we just get 1 Month before the things to vote by mail and then mail in back votes. Hope we can soon vote online to get rid of the paper overhead.&lt;/p&gt;
    &lt;p&gt;For some reason an Azure outage does not faze me in the same way that an AWS outage does.&lt;/p&gt;
    &lt;p&gt;I have never had much confidence in Azure as a cloud provider. The vertical integration of all the things for a Microsoft shop was initially very compelling. I was ready to fight that battle. But, this fantasy was quickly ruined by poor execution on Microsoft's part. They were able to convince me to move back to AWS by simply making it difficult to provision compute resources. Their quota system &amp;amp; availability issues are a nightmare to deal with compared to EC2.&lt;/p&gt;
    &lt;p&gt;At this point I'd rather use GCP over Azure and I have zero seconds of experience with it. The number of things Microsoft gets right in 2025 can be counted single-handedly. The things they do get right are quite good, but everything else tends to be extremely awful.&lt;/p&gt;
    &lt;p&gt;The "Blades" experience [0] where instead of navigating between pages it just kept opening things to the side and expanding horizontally?&lt;/p&gt;
    &lt;p&gt;Yeah, that had some fun ideas but was way more confusing than it needed to be. But also that was quite a few years back now. The Portal ditched that experience relatively quickly. Just long enough to leave a lot of awful first impressions, but not long enough for it to be much more than a distant memory at this point, several redesigns later.&lt;/p&gt;
    &lt;p&gt;[0] The name "Blades" for that came from the early years of the Xbox 360, maybe not the best UX to emulate for a complex control panel/portal.&lt;/p&gt;
    &lt;p&gt;Azure to me has always suffered from a belief that “UI innovations can solve UX complexity if you just try hard enough.”&lt;/p&gt;
    &lt;p&gt;Like, AWS, and GCP to a lesser extent, has a principled approach where simple click-ops goals are simple. You can access the richer metadata/IAM object model at any time, but the wizards you see are dumb enough to make easy things easy.&lt;/p&gt;
    &lt;p&gt;With Azure, those blades allow tremendously complex “you need to build an X Container and a Container Bucket to be able to add an X” flows to coexist on the same page. While this exposes the true complexity, and looks cool/works well for power users, it is exceedingly unintuitive. Inline documentation doesn’t solve this problem.&lt;/p&gt;
    &lt;p&gt;I sometimes wonder if this is by design: like QuickBooks, there’s an entire economy of consultants who need to be Certified and thus will promote your product for their own benefit! Making the interface friendly to them and daunting to mere mortals is a feature, not a bug.&lt;/p&gt;
    &lt;p&gt;But in Azure’s case it’s hard to tell how much this is intentional.&lt;/p&gt;
    &lt;p&gt;(I think that's from near the transition because it has full "windowing" controls of minimize/maximize/close buttons. I recall a period with only close buttons.)&lt;/p&gt;
    &lt;p&gt;All that blue space you could keep filling with more "blades" as you clicked on things until the entire page started scrolling horizontally to switch between "blades". Almost everything you could click opened in a new blade rather than in place in the existing blade. (Like having "Open in New Window" as your browser default.)&lt;/p&gt;
    &lt;p&gt;It was trying to merge the needs of a configurable Dashboard and a "multi-window experience". You could save collections of blades (a bit like Niri workspaces) as named Dashboards. Overall it was somewhere between overkill and underthought.&lt;/p&gt;
    &lt;p&gt;(Also someone reminded me that many "blades" still somewhat exist in the modern Portal, because, of course, Microsoft backwards compatibility. Some of the pages are just "maximized Blades" and you can accidentally unmaximize them and start horizontally scrolling into new blades.)&lt;/p&gt;
    &lt;p&gt;azure likes to open new sections on the same tab / page as opposed to reloading or opening a new page / tab (overlays? modals? I'm lost on graphic terms)&lt;/p&gt;
    &lt;p&gt;depending on the resource you're accessing, you can get 5+ sections each with their own ui/ux on the same page/tab and it can be confusing to understand where you're at in your resources&lt;/p&gt;
    &lt;p&gt;if you're having trouble visualizing it, imagine an url where each new level is a different application with its own ui/ux and purpose all on the same webpage&lt;/p&gt;
    &lt;p&gt;AWS' UI is similarly messy, and to this day. They regularly remove useful data from the UI, or change stuff like the default sort order of database snapshots from last created to initial instance created date.&lt;/p&gt;
    &lt;p&gt;I never understood why a clear and consistent UI and improved UX isn't more of a priority for the big three cloud providers. Even though you talk mostly via platform SDK's, I would consider better UI especially initially, a good way to bind new customers and pick your platform over others.&lt;/p&gt;
    &lt;p&gt;I guess with their bottom line they don't need it (or cynically, you don't want to learn and invest in another cloud if you did it once).&lt;/p&gt;
    &lt;p&gt;It’s more than just the UI itself (which is horrible), it’s the whole thing that is very hostile to new users even if they’re experienced. It’s such an incoherent mess. The UI, the product names, the entire product line itself, with seemingly overlapping or competing products… and now it’s AI this and AI that. If you don’t know exactly what you’re looking for, good luck finding it. It’s like they’re deliberately trying to make things as confusing as possible.&lt;/p&gt;
    &lt;p&gt;For some reason this applies to all AWS, GCP and Azure. Seems like the result of dozens of acquisitions.&lt;/p&gt;
    &lt;p&gt;I still find it much easier to just self host than learn cloud and I’ve tried a few times but it just seems overly complex for the sake of complexity. It seems they tie in all their services to jack up charges, eg. I came for S3 but now I’m paying for 5 other things just to get it working.&lt;/p&gt;
    &lt;p&gt;Any time something is that unintuitive to get started, I automatically assume that if I encounter a problem that I’ll be unable to solve it. That thought alone leads me to bounce every time.&lt;/p&gt;
    &lt;p&gt;100% agree. I've been working in the industry for almost 20 years, I'm a full stack developer and I manage my servers. I've tried signing up for AWS and I noped out.&lt;/p&gt;
    &lt;p&gt;AWS Is a complete mess. Everything is obscured behind other products, and they're all named in the most confusing way possible.&lt;/p&gt;
    &lt;p&gt;Microsoft has the regulatory capture. All the European privacy and regulatory laws are good for Azure. That's why your average European government or baking app runs most likely on Azure. (or Oracle, but more likely Azure)&lt;/p&gt;
    &lt;p&gt;Cloud Run is incredible. It’s one of those things I wish more devs knew about. Even at work where we use GCP all the “smart” devs insist on GKE for their “webscale” services that get dozens of requests a second. Dozens!&lt;/p&gt;
    &lt;p&gt;I know for some people the prospect of losing their Google Cloud access due to an automated terms of service violation on some completely unrelated service is worrisome.&lt;/p&gt;
    &lt;p&gt;I'd hope you can create a Google Cloud account under a completely different email address, but I do as little business with Google as I can get away with, so I have no idea.&lt;/p&gt;
    &lt;p&gt;That's generally speaking a good practice anyways. My Amazon shopping account has a different email than my Amazon Web Services account. I intuited that they need to be different from the get go.&lt;/p&gt;
    &lt;p&gt;The problem is that in some industries, Microsoft is the only option. Many of these regulated industries are just now transitioning from the data center to the cloud, and they've barely managed to get approval for that with all of the Microsoft history in their organization. AWS or GCloud are complete non-starters.&lt;/p&gt;
    &lt;p&gt;I moved a 100% MS shop to AWS circa 2015. We ran our DCs on EC2 instances just as if they were on prem. At some point we installed the AAD connector and bridged some stuff to Azure for office/mail/etc., but it was all effectively in AWS. We were selling software to banks so we had a lot of due diligence to suffer. AWS Artifact did much of the heavy lifting for us. We started with Amazon's compliance documentation and provided our own feedback on top where needed.&lt;/p&gt;
    &lt;p&gt;I feel like compliance is the entire point of using these cloud providers. You get a huge head start. Maintaining something like PCI-DSS when you own the real estate is a much bigger headache than if it's hosted in a provider who is already compliant up through the physical/hardware/networking layers. Getting application-layer checkboxes ticked off is trivial compared to "oops we forgot to hire an armed security team". I just took a look and there are currently 316 certifications and attestations listed under my account.&lt;/p&gt;
    &lt;p&gt;I've found that lift and shifting to EC2 is also generally cheaper than the equivalent VMs on Azure.&lt;/p&gt;
    &lt;p&gt;Microsoft really wants you to use their PaaS offerings, and so things on Azure are priced accordingly. A Microsoft shop just wanting to lift-and-shift, Azure isn't the best choice unless the org has that "nobody ever got fired for buying Microsoft" attitude.&lt;/p&gt;
    &lt;p&gt;Microsoft is better at regulatory capture, so Azure has many customers in the public sector. So an Azure outage probably affects the public sector more (see example above about trains).&lt;/p&gt;
    &lt;p&gt;What Amazon, Azure, and Google are showing with their platform crashes amid layoffs, while they supports governments that are Oppressing's Citizens and Ignoring the Law, is that they do not care about anything other than the bottom line.&lt;/p&gt;
    &lt;p&gt;They think they have the market captured, but I think what their dwindling quality and ethics are really going to drive is adoption of self hosting, distributed computing frameworks. Nerds are the ones who drove adoption of these platforms, and we can eventually end if we put in the work.&lt;/p&gt;
    &lt;p&gt;Seriously with container technology, and a bit more work / adoption on distributed compute systems and file storage (IPFS,FileCoin) there is a future where we dont have to use big brothers compute platform. Fuck these guys.&lt;/p&gt;
    &lt;p&gt;These were my thoughts exactly. I may have my tinfoil hat on, but outages these close together between the largest cloud providers amid social unrest, my wonder is the government / tech companies implementing some update that adds additional spyware / blackout functionality.&lt;/p&gt;
    &lt;p&gt;I really hope this pushes the internet back to how it used to be, self hosted, privacy, anonymity. I truly hope that's where we're headed, but the masses seem to just want to stay comfortable as long as their show is on TV&lt;/p&gt;
    &lt;p&gt;At least some bits of it do. I was writing something to pull logs the other day and the redirect was to an azure bucket. It also returned a 401 with the valid temporary authed redirect in the header. I was a bit worried I'd found a massive security hole but it appears after some testing it just returned the wrong status code.&lt;/p&gt;
    &lt;p&gt;Personally I am thinking more and more about hetzner, yes I know its not an apples to orange comparison. But its honestly so good&lt;/p&gt;
    &lt;p&gt;Someone had created a video where they showed the underlying hardware etc., I am wondering if there is something like https://vpspricetracker.com/ but with geek-benchmarks as well.&lt;/p&gt;
    &lt;p&gt;This video was affiliated with scalahosting but still I don't think that there was too much bias of them and they showed at around 3:37 a graph comparison with prices https://www.youtube.com/watch?v=9dvuBH2Pc1g&lt;/p&gt;
    &lt;p&gt;Now it shows how contabo has better hardware but I am pretty sure that there might be some other issues, and honestly I feel a sense of trust with hetzner I am not sure about others.&lt;/p&gt;
    &lt;p&gt;Either hetzner or self hosting stuff personally or just having a very cheap vps and going to hetzner if need be but hetzner already is pretty cheap or I might use some free service that I know of are good as well.&lt;/p&gt;
    &lt;p&gt;Probably not, but at least you don’t delude yourself into thinking reliability is a solved problem just because you’re paying through the nose for compute and storage.&lt;/p&gt;
    &lt;p&gt;One of recent (4 months ago) Cloudflare outages (I think it was even workers) was caused by Google Cloud being down and Cloudflare hosting an essential service there&lt;/p&gt;
    &lt;p&gt;Hm it seemed that they hosted a critical service for cloudflare kv on google itself, but I wonder about the update.&lt;/p&gt;
    &lt;p&gt;Personally I just trust cloudflare more than google, given how their focus is on security whereas google feels googly...&lt;/p&gt;
    &lt;p&gt;I have heard some good things about google cloud run and the google's interface feels the best out of AWS,Azure,GCloud but I still would just prefer cloudflare/hetzner iirc&lt;/p&gt;
    &lt;p&gt;Another question: Has there ever been a list of all major cloud outages, like I am interested how many times google cloud and all cloud providers went majorly down I guess y'know? is there a website/git project that tracks this?&lt;/p&gt;
    &lt;p&gt;IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door. But it meant that when the system came back online, if the customers card was denied, the customer got free groceries.&lt;/p&gt;
    &lt;p&gt;Yea, good old store and forward. We implemented it in our PoS system. Now, we do non PCI integrations so we arn't in PCI scope, but depending on the processor, it can come with some limitations. Like, you can do store and forward, but only up to X number of transactions. I think for one integration, it's 500-ish store wide (it uses a local gateway that store and forwards to the processors gateway). The other integration we have, its 250, but store and forward on device, per device.&lt;/p&gt;
    &lt;p&gt;In many places it's also possibly just a left over feature from older times. I worked at a major UK supermarket in the mid-00s, and their checkout system had this feature. But it was like that because that's how it was originally built, it wasn't a 'feature' they added.&lt;/p&gt;
    &lt;p&gt;Credit card information would be recorded by the POS, synced to a mini-server in the back office (using store-and-forward to handle network issues) and then in a batch process overnight, sent to HQ where the payment was processed.&lt;/p&gt;
    &lt;p&gt;It wasn't until chip-and-PIN was rolled out that they started supporting "online" (i.e. processed then and there) card transactions, and even then the old method still worked if there was a network issues or power failure (all POSes has their own UPS).&lt;/p&gt;
    &lt;p&gt;The only real risk at the time was that someone tried to pay with a cancelled credit card - the bank would always honour the payment otherwise. But that was pretty uncommon back then, as you'd have to phone your bank to do it, not just press a button in an app.&lt;/p&gt;
    &lt;p&gt;IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door.&lt;/p&gt;
    &lt;p&gt;Chick-fil-a has this.&lt;/p&gt;
    &lt;p&gt;One of the tech people there was on HN a few years ago describing their system. Credit card approval slows down the line, so the cards are automatically "approved" at the terminal, and the transaction is added to a queue.&lt;/p&gt;
    &lt;p&gt;The loss from fraudulent transactions turns out to be less than the loss from customers choosing another restaurant because of the speed of the lines.&lt;/p&gt;
    &lt;p&gt;I was shopping at a mall with a visa vanilla card once. I got it as a gift and didn't know the limit. No matter what I bought the card kept going -- and I never got a balance of what was on the card. Eventually, later that day it stopped. I called customer support and asked how much was left on the balance. They told me they had no idea my balance - but everything I bought was mine.&lt;/p&gt;
    &lt;p&gt;I remember that banks will try to honor the transactions, even if the customer's balance/credit limit is exhausted. It doesn't apply only to some gift cards.&lt;/p&gt;
    &lt;p&gt;There's a Family Dollar by my house that is down at least 2 full days per month because of bad inet connectivity. I live close enough that with a small tower on my roof i can get line of sight to theirs. I've thought about offering them a backup link off my home inet if they give me 50% of sales whenever its in use. It would be a pretty good deal for them, better some sales when their inet is down vs none.&lt;/p&gt;
    &lt;p&gt;It's Family Dollar, margin has to be almost nothing and sales per day is probably &amp;lt; $1k. That's why I said 50% of sales and not profit.&lt;/p&gt;
    &lt;p&gt;I go there daily because it's a nice 30min round trip walk and I wfh. I go up there to get a diet coke or something else just to get out of the house. It amazes me when i see a handwritten sign on the door "closed, system is down". I've gotten to know the cashiers so I asked and it's because the internet connection goes down all the time. That store has to one of the most poorly run things i've ever seen yet it stays in business somehow.&lt;/p&gt;
    &lt;p&gt;I think the point people are trying and failing to make is that asking for half of means sales is half of revenue not half of net and that you’re out of your goddamned mind if you think a store with razor thin margins would sell at a massive loss rather than just close due to connectivity problems.&lt;/p&gt;
    &lt;p&gt;Your responses imply that you think people are questioning whether you would lose money on the deal while we are instead saying you’ll get laughed out of the store, or possibly asked never to come back.&lt;/p&gt;
    &lt;p&gt;Unfortunately they are largely corporate, which is how they can sell items for such a cheap price. The store manager probably has zero say in nearly anything. Even if they wanted to "break the rules," I doubt they could make use of your connection as a backup, but I've also worked for smaller companies that were able to sell internet access to individual locations like Denny's and various large hotels in the US. Being able to somehow share sales would be the difficult part, since all sales are reported back to corporate.&lt;/p&gt;
    &lt;p&gt;Good luck if you make this work for you, it would be exciting to hear about if you're able to get them to work with you.&lt;/p&gt;
    &lt;p&gt;2-3%, bit higher on perishables. Though i'd just ask lump sum payments in cash since it likely has to no go through corporate (as in, avoid the corporation).&lt;/p&gt;
    &lt;p&gt;You'd think any SeriousBusiness would have a backup way to take customers' money. This is the one thing you always want to be able to do: accept payment. If they made it so they can't do that, they deserve the hit to their revenue. People should just walk out of the store with the goods if they're not being charged.&lt;/p&gt;
    &lt;p&gt;Why doesn't someone in the store at least have one of those manual kachunk-kachunk carbon copy card readers in the back that they can resuscitate for a few days until the technology is turned back on? Did they throw them all away?&lt;/p&gt;
    &lt;p&gt;The kachunk-kachunk credit card machines need raised digits on the cards, and I don't think most banks have been issuing those for years at this point. Mine have been smooth for at least 10 years.&lt;/p&gt;
    &lt;p&gt;My card tied to my main financial institution have the raised digits, but most cards you'd sign up for online now no longer have the raised digits (and often allow you to select art to appear on the card face).&lt;/p&gt;
    &lt;p&gt;I think a lot of payment terminals have an option to record transactions offline and upload them later, but apparently it's not enabled by default - probably because it increases your risk that someone pays with a bad card.&lt;/p&gt;
    &lt;p&gt;If they used standalone merchant terminals, then those typically use the local LAN which can rollover to cellular or PoT in the event of a network outage. The store can process a card transaction with the merchant terminal and then reconcile with the end of day chit. This article from 2008 describes their PoS https://www.retailtouchpoints.com/topics/store-operations/ca...&lt;/p&gt;
    &lt;p&gt;These stores appear everywhere, even in areas with high income. You'd be surprised, but often people with those high incomes shop for certain products at very low rates, and that's how they keep their savings. A good example is garbage bags. Most people don't care too much about the quality of their garbage bags, unless they rip on the way to the bin.&lt;/p&gt;
    &lt;p&gt;Just to add - this particular supermarket wasn’t fully down, it took ages for them to press “sub total” and then pick the payment method. I suspect it was slow waiting for a request to timeout perhaps&lt;/p&gt;
    &lt;p&gt;I remember last mechanical cash registers in my country in 90s and when these got replaced by early electronic ones with blue vacuum fluorescent tubes. Then everything got smaller and smaller. Now I'm pestered to "add the item to the cart" by software.&lt;/p&gt;
    &lt;p&gt;Last week I couldn't pay for flowers for grandma's grave because smartphone-sized card terminal refused to work - it stuck on charging-booting loop so I had to get cash. Tho my partner thinks she actually wanted to get cash without a receipt for herself excluding taxes&lt;/p&gt;
    &lt;p&gt;You can, but it's all about risk mitigation. Most processors have some form of store and forward (and it can have limitations like only X number of transactions). Some even have controls to limit the amount you can store-and-forward (for instance, only charges under $50). But ultimately, it's still risk mitigation. You can store-and-forward, but you're trusting that the card/account has the funds. If it doesn't, you loose and ain't shit you can do about it. If you can't tolerate any risk, you don't turn on store and forward systems and then you can't process cards offline.&lt;/p&gt;
    &lt;p&gt;Its not the we are not capable. Its, is the business willing to assume the risk?&lt;/p&gt;
    &lt;p&gt;Currently standing in a half closed supermarket because the tills are down and they cant take payments&lt;/p&gt;
    &lt;p&gt;There's a fairly large supermarket near me that has both kinds of outages.&lt;/p&gt;
    &lt;p&gt;Occasionally it can't take cards because the (fiber? cable?) internet is down, so it's cash only.&lt;/p&gt;
    &lt;p&gt;Occasionally it can't take cash because the safe has its own cellular connection, and the cell tower is down.&lt;/p&gt;
    &lt;p&gt;I was at Frank's Pizza in downtown Houston a few weeks ago and they were giving slices of pizza away because the POS terminal died, and nobody knew enough math to take cash. I tried to give them a $10 and told them to keep the change, but "keep the change" is an unknown phrase these days. They simply couldn't wrap their brains around it. But hey, free pizza!&lt;/p&gt;
    &lt;p&gt;I’ve been migrating our services off of Azure slowly for the past couple of years. The last internet facing things remaining are a static assets bucket and an analytics VM running Matomo. Working with Front Door has been an abysmal experience, and today was the push I needed to finally migrate our assets to Cloudflare.&lt;/p&gt;
    &lt;p&gt;I feel pretty justified in my previous decisions to move away from Azure. Using it feels like building on quicksand…&lt;/p&gt;
    &lt;p&gt;We are very dependent on Azure and Microsoft Authentication and Microsoft 365 and haven’t had weekly or even monthly issues. I can think of maybe three issues this year.&lt;/p&gt;
    &lt;p&gt;I have had intermittent issues with winget today. I use UniGetUI for a front-end, and anything tied to Microsoft has failed for me. Judging by the logs, it's mostly retrieving the listing of versions (I assume similar to what 'apt-get update' does, I'm fairly new to using winget for Windows package management).&lt;/p&gt;
    &lt;p&gt;Pretty much every single Microsoft domain I've tried to access loads for a looooong time before giving me some bare html. I wonder if someone can explain why that's happening.&lt;/p&gt;
    &lt;p&gt;We’re 100% on Azure but so far there’s no impact for us.&lt;/p&gt;
    &lt;p&gt;Luckily, we moved off Azure Front Door about a year ago. We’d had three major incidents tied to Front Door and stopped treating it as a reliable CDN.&lt;/p&gt;
    &lt;p&gt;They weren’t global outages, more like issues triggered by new deployments. In one case, our homepage suddenly showed a huge Microsoft banner about a “post-quantum encryption algorithm” or something along those lines.&lt;/p&gt;
    &lt;p&gt;Kinda wild that a company that big can be so shaky on a CDN, which should be rock solid.&lt;/p&gt;
    &lt;p&gt;We battled https://learn.microsoft.com/en-us/answers/questions/1331370/... for over a year, and finally decided to move off since there was no any resolution. Unfortunately our API servers were still behind AFD so they were affected by today's stuff...&lt;/p&gt;
    &lt;p&gt;And querying https://www.microsoft.com/ results in HTTP 200 on the root document, but the page elements return errors (a 504 on the .css/.js documents, a 404 on some fonts, Name Not Resolved on scripts.clarity.ms, Connection Timed Out on wcpstatic.microsoft.com and mem.gfx.ms). That many different kinds of errors is actually kind of impressive.&lt;/p&gt;
    &lt;p&gt;I'm gonna say this was a networking/routing issue. The CDN stayed up, but everything else non-CDN became unroutable, and different requests traveled through different paths/services, but each eventually hit the bad network path, and that's what created all the different responses. Could also have been a bad deploy or a service stopped running and there's different things trying to access that service in different ways, leading to the weird responses... but that wouldn't explain the failed DNS propagation.&lt;/p&gt;
    &lt;p&gt;I've been doing it since 1998 in my bedroom with a dual T1 (and on to real DCs later). While I've had some outages for sure it makes me feel better I am not that divergent in uptime in the long run vs big clouds.&lt;/p&gt;
    &lt;p&gt;They added a message at the same time as your comment:&lt;/p&gt;
    &lt;p&gt;"We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly."&lt;/p&gt;
    &lt;p&gt;The paradox of cloud provider crashes is that if the provider goes down and takes the whole world with it, it's actually good advertisement. Because, that means so many things rely on it, it's critically important, and has so many big customers. That might be why Amazon stock went up after AWS crash.&lt;/p&gt;
    &lt;p&gt;If Azure goes down and nobody feels it, does Azure really matter?&lt;/p&gt;
    &lt;p&gt;People feel it, but usually not general consumers like they do when AWS goes down.&lt;/p&gt;
    &lt;p&gt;If Azure goes down, it's mostly affecting internal stuff at big old enterprises. Jane in accounting might notice, but the customers don't. Contrast with AWS which runs most of the world's SaaS products.&lt;/p&gt;
    &lt;p&gt;People not being able to do their jobs internally for a day tends not to make headlines like "100 popular internet services down for everyone" does.&lt;/p&gt;
    &lt;p&gt;Yeah just took down the prod site for one of our clients since we host the front-end out of their CDN. Just got wrapped up panic hosting it somewhere else for the past hour, very quickly reminds you about the pain of cookies...&lt;/p&gt;
    &lt;p&gt;Pretty much all Azure services seem to be down. Their status page says it's only the portal since 16:00. It would be nice if these mega-companies could update their status page when they take down a large fraction of the Internet and thousands of services that use them.&lt;/p&gt;
    &lt;p&gt;Same playbook for AWS. When they admitted that Dynamo was inaccessible, they failed to provide context that their internal services are heavily dependent on Dynamo&lt;/p&gt;
    &lt;p&gt;It's only after the fact they are transparent about the impact&lt;/p&gt;
    &lt;p&gt;The Internet is supposed to be decentralized. The big three seem to have all the power now (Amazon, Microsoft, and Google) plus Cloudflare/Oracle.&lt;/p&gt;
    &lt;p&gt;How did we get here? Is it because of scale? Going to market in minutes by using someone else's computers instead of building out your own, like co-location or dedicated servers, like back in the day.&lt;/p&gt;
    &lt;p&gt;A lot of money and years of marketing the cloud as the responsible business decision led us here. Now that the cloud providers have vendor lock-in, few will leave, and customers will continue to wildly overpay for cloud services.&lt;/p&gt;
    &lt;p&gt;Not sure how the current situation is better. Being stranded with no way whatsoever to access most/all of your services sounds way more terrifying than regular issues limited to a couple of services at a time&lt;/p&gt;
    &lt;p&gt;&amp;gt; no way whatsoever to access most/all of your services&lt;/p&gt;
    &lt;p&gt;I work on a product hosted on Azure. That's not the case. Except for front door, everything else is running fine. (Front door is a reverse proxy for static web sites.)&lt;/p&gt;
    &lt;p&gt;The product itself (an iot stormwater management system) is running, but our customers just can't access the website. If they need to do something, they can go out to the sites or call us and we can "rub two sticks together" and bypass the website. (We could also bypass front door if someone twisted our arms.)&lt;/p&gt;
    &lt;p&gt;Most customers only look at the website a few times a year.&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;That being said, our biggest point of failure is a completely different iot vendor who you probably won't hear about on Hacker News when they, or their data networks, have downtime.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Big Tech lobbying is riding the EU’s deregulation wave by spending more, hiring more, and pushing more, according to a new report by NGO’s Corporate Europe Observatory and LobbyControl on Wednesday (29 October).&lt;/p&gt;
    &lt;p&gt;&amp;gt; Based on data from the EU’s transparency register, the NGOs found that tech companies spend the most on lobbying of any sector, spending €151m a year on lobbying — a 33 percent increase from €113m in 2023.&lt;/p&gt;
    &lt;p&gt;Gee whizz, I really do wonder how they end up having all the power!&lt;/p&gt;
    &lt;p&gt;I think the response lies in the surrounding ecosystem.&lt;/p&gt;
    &lt;p&gt;If you have a company it's easier to scale your team if you use AWS (or any other established ecosystem). It's way easier to hire 10 engineers that are competent with AWS tools than it is to hire 10 engineers that are competent with the IBM tools.&lt;/p&gt;
    &lt;p&gt;And from the individuals perspective it also make sense to bet on larger platforms. If you want to increase your odds of getting a new job, learning the AWS tools gives you a better ROI than learning the IBM tools.&lt;/p&gt;
    &lt;p&gt;But the cloud compute market is basically centralized into 2.5 companies at this point. The point of paying companies like Azure here is that they've in theory centralized the knowledge and know-how of running multiple, distributed datacenters, so as to be resilient.&lt;/p&gt;
    &lt;p&gt;But that we keep seeing outages encompassing more than a failure domain, then it should be fair game for engineers / customers to ask "what am I paying for, again?"&lt;/p&gt;
    &lt;p&gt;Moreover, this seems to be a classic case of large barriers to entry (the huge capital costs associated with building out a datacenter) barring new entrants into the market, coupled with "nobody ever got fired for buying IBM" level thinking. Are outages like these truly factored into the napkin math that says externalizing this is worth it?&lt;/p&gt;
    &lt;p&gt;Consolidation is the inevitable outcome of free unregulated markets.&lt;/p&gt;
    &lt;p&gt;In our highly interconnected world, decentralization paradoxically requires a central authority to enforce decentralization by restricting M&amp;amp;A, cartels, etc.&lt;/p&gt;
    &lt;p&gt;A natural monopoly is a monopoly in an industry in which high infrastructure costs and other barriers to entry relative to the size of the market give the largest supplier in an industry, often the first supplier in a market, an overwhelming advantage over potential competitors. Specifically, an industry is a natural monopoly if a single firm can supply the entire market at a lower long-run average cost than if multiple firms were to operate within it. In that case, it is very probable that a company (monopoly) or a minimal number of companies (oligopoly) will form, providing all or most of the relevant products and/or services.&lt;/p&gt;
    &lt;p&gt;The outage was really weird. For me, parts of the portal worked, other parts didn't. I had access to a couple of resource groups, but no resources visible in those groups. Azure Devops Pipelines that needed do download from packages.microsoft.com didn't work.&lt;/p&gt;
    &lt;p&gt;The Microsoft status page mostly referenced the portal outage, but it was more than that.&lt;/p&gt;
    &lt;p&gt;For us, it looks like most services are still working (eastus and eastus2). Our AKS cluster is still running and taking requests. Failures seem limited to management portal.&lt;/p&gt;
    &lt;p&gt;High availability is touted as a reason for their high prices, but I swear I read about major cloud outages far more than I experience any outages at Hetzner.&lt;/p&gt;
    &lt;p&gt;I think the biggest features of the big cloud vendors is that when they are down, not only you but your customers and your competitors usually have issues at the same time so everybody just shrug and have a lazy/off day at the same time. Even on call teams reall just have to wait and stay on standby because there is very little they can do. Doing a failover can be slower than waiting for the recovery, not help at all if outage is spanned accross several region, or bring aditional risks.&lt;/p&gt;
    &lt;p&gt;And more importantly nobody lose any reputation except AWS/Azure/Google.&lt;/p&gt;
    &lt;p&gt;The real reason is that outages are not your fault. Its the new version of "nobody ever got fired for buying IBM" - later it became MS, and now its any big cloud provider.&lt;/p&gt;
    &lt;p&gt;For one it’s statistics - Hetzner simply runs far fewer major services than hyperscalers. And the services they run are also more affluent, with larger customer bases, so downtimes are systemically critical. Therefore it’s louder.&lt;/p&gt;
    &lt;p&gt;On the merits though, I agree, haven’t had any serious issues with Hetzner.&lt;/p&gt;
    &lt;p&gt;DO has been shockingly reliable for me. I shut down a neglected box almost 900 days uptime the other day. In that time AWS has randomly dropped many of my boxes with no warning requiring a manual stop/start action to recover them... But everybody keeps telling me that DO isn't "as reliable" as the big three are.&lt;/p&gt;
    &lt;p&gt;To be fair, in the AWS/Azure outages, I don't think any individual (already created) boxes went down, either. In AWS' case you couldn't start up new EC2 instances, and presumably same for Azure (unless you bypass the management portal, I guess). And obviously services like DynamoDB and Front Door, respectively, went down. Hetzner/DO don't offer those, right? Or at least they're not very popular.&lt;/p&gt;
    &lt;p&gt;Nope, more than the portal. For instance, I just searched for "Azure Front Door" because I hadn't heard of it before (I now know it's a CDN), and neither the product page itself [1] nor the technical docs [2] are coming up for me.&lt;/p&gt;
    &lt;p&gt;we use front door (as does miccrosoft.com) and our website was down, I was able to change the DNS records to point directly to our server and will leave it like that for a few hours until everything is green&lt;/p&gt;
    &lt;p&gt;AWS, now Azure - wasn't this a plot point in Terminator where SkyNet was causing computer systems to have issues much before it finally become self-aware?&lt;/p&gt;
    &lt;p&gt;Funnily enough, AI has been training on its own data as generated by users writing AI conversations back to the internet - there's a feedback loop at play.&lt;/p&gt;
    &lt;p&gt;Do Microsoft still say "If the government has a broader voluntary national security program to gather customer data, we don't participate in it" today (which PRISM proved very false), or are they at least acknowledging they're participating in whatever NSA has deployed today?&lt;/p&gt;
    &lt;p&gt;PRISM wasn't voluntary. Also there are 3 levels here:&lt;/p&gt;
    &lt;p&gt;1. Mandatory&lt;/p&gt;
    &lt;p&gt;2. "Voluntary"&lt;/p&gt;
    &lt;p&gt;3. Voluntary&lt;/p&gt;
    &lt;p&gt;And I suspect that very little of what the NSA does falls into category 3. As Sen Chuck Schumer put it "you take on the intelligence community, they have six ways from Sunday at getting back at you"&lt;/p&gt;
    &lt;p&gt;This is funny but also possibly true because: business/MBA types see these outages as a way to prove how critical some services are, leading to investors deciding to load up on the vendor's stock.&lt;/p&gt;
    &lt;p&gt;I may or may not have been known to temporarily take a database down in the past to make a point to management about how unreliable some old software is.&lt;/p&gt;
    &lt;p&gt;Seeing users having issues with the "Modern Outlook", specifically empty accounts. Switching back to the "Legacy Outlook" which functions largely without the help of the cloud fixes the issue. How ironic.&lt;/p&gt;
    &lt;p&gt;I was having issues a few hours ago. I'm now able to access the portal, although I get lots of errors in the browser console, and things are loading slowly. I have services in the US-East region.&lt;/p&gt;
    &lt;p&gt;I have been having issues with GitHub and the winget tool for updates throughout the day as well. I imagine things are pulling from the same locations on Azure for some of the software I needed to update (NPM dependencies, and some .NET tooling).&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;----&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:18 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;The sad thing is - $MSFT isn't even down by 1%. And IIRC, $AMZN actually went up during their previous outage.&lt;/p&gt;
    &lt;p&gt;So if we look at these companies' bottom lines, all those big wigs are actually doing something right. Sales and lobbying capacity is way more effective than reliability or good engineering (at least in the short term).&lt;/p&gt;
    &lt;p&gt;I think he was implying that those companies think they are so important that it doesnt matter they are down, they wont loose any customers over it because they are too big and important.&lt;/p&gt;
    &lt;p&gt;That's a good thing. Stock prices shouldn't go down because of rare incidents which don't accurately represent how successful a company is likely to be in the future.&lt;/p&gt;
    &lt;p&gt;I looked into this before and the stocks of these large corps simply does not move when outages happens. Maybe intra-day, I don't have that data, but in general no effect.&lt;/p&gt;
    &lt;p&gt;There's no way to tell, and after about 30 minutes, the release process on VS Code Marketplace failed with a cryptic message: "Repository signing for extension file failed.". And there's no way to restart/resume it.&lt;/p&gt;
    &lt;p&gt;"We’re investigating an issue impacting Azure Front Door services. Customers may experience intermittent request failures or latency. Updates will be provided shortly."&lt;/p&gt;
    &lt;p&gt;They admit in their update blurb azure front door is having issues but still report azure front door as having no issues on their status page.&lt;/p&gt;
    &lt;p&gt;And it's very clear from these updates that they're more focused on the portal than the product, their updates haven't even mentioned fixing it yet, just moving off of it, as if it's some third party service that's down.&lt;/p&gt;
    &lt;p&gt;Unsubstantiated idea: So the support contract likely says there is a window between each reporting step and the status page is the last one and the one in the legal documents giving them several more hours before the clauses trigger.&lt;/p&gt;
    &lt;p&gt;Azure goes down all the time. On Friday we had an entire regional service down all day. Two weeks ago same thing different region. You only hear about it when it's something everyone uses like the portal, because in general nobody uses Azure unless they're held hostage.&lt;/p&gt;
    &lt;p&gt;Portal and Azure CDN are down here in the SF Bay Area. Tenant azureedge.net DNS A queries are taking 2-6 seconds and most often return nothing. I got a couple successful A response in the last 10 minutes.&lt;/p&gt;
    &lt;p&gt;Edit: As of 9:19 AM Pacific time, I'm now getting successful A responses but they can take several seconds. The web server at that address is not responding.&lt;/p&gt;
    &lt;p&gt;It is much more than azure. One of my kids needs a key for their laptop and can't reach that either. Great excuse though, 'Azure ate my homework'. What a ridiculous world we are building. Fuck MS and their account requirements for windows.&lt;/p&gt;
    &lt;p&gt;It begs the question from a noob like me... Where should they host the status page? Surely it shouldn't be on the same infra that it's supposed to be monitoring. Am I correct in thinking that?&lt;/p&gt;
    &lt;p&gt;“ Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025”&lt;/p&gt;
    &lt;p&gt;I'd say DNS/Front Door (or some carrier interconnect) is the thing affected, since I can auth just fine in a few places. (I'm at MS, but not looped into anything operational these days, so I'm checking my personal subscription).&lt;/p&gt;
    &lt;p&gt;On our end, our VMs are still working, so our gitlab instance is still up. Our services using Azure App Services are available through their provided url. However, Front Door is failing to resolve any domains that it was responsible for.&lt;/p&gt;
    &lt;p&gt;SSO is down, Azure Portal Down and more, seems like a major outage. Already a lot of services seem to be affected: banks, airlines, consumer apps, etc.&lt;/p&gt;
    &lt;p&gt;The portal is up for me and their status page confirms they did a failover for it. Definitely not disputing that its reach is wide, but a lot of smaller setups probably aren't using Front Door.&lt;/p&gt;
    &lt;p&gt;Part of this outage involves outlook hanging and then blaming random addins. Pretty terrible practice by Microsoft to blame random vendors for their own outage.&lt;/p&gt;
    &lt;p&gt;Looks like MyGet is impacted too. Seems like they use Azure:&lt;/p&gt;
    &lt;p&gt;&amp;gt;What is required to be able to use MyGet? ... MyGet runs its operations from the Microsoft Azure in the West Europe region, near Amsterdam, the Netherlands.&lt;/p&gt;
    &lt;p&gt;This is the eternal tension for early-stage builders, isn't it? Multi-cloud gives you resilience, but adds so much complexity that it can actually slow down shipping features and iterating.&lt;/p&gt;
    &lt;p&gt;I'm curious—at what point did you decide the overhead was worth it? Was it after experiencing an outage, or did you architect for it from day one?&lt;/p&gt;
    &lt;p&gt;As someone launching a product soon (more on the builder/product side than infra-engineer), I keep wrestling with this. The pragmatist in me says "start simple, prove the concept, then layer in resilience." But then you see events like this week and think "what if this happens during launch?"&lt;/p&gt;
    &lt;p&gt;How did you handle the operational complexity? Did you need dedicated DevOps folks, or are there patterns/tools that made it manageable for a smaller team?&lt;/p&gt;
    &lt;p&gt;I don't think it's meant to be serious. It's a comment on Microsoft laying off their staff and stuffing their Azure and Dotnet teams with AI product managers.&lt;/p&gt;
    &lt;p&gt;That said, I don't hear about GCP outages all that often. I do think AWS might be leading in outages, but that's a gut feeling, I didn't look up numbers.&lt;/p&gt;
    &lt;p&gt;Thank you. I was wondering what was going on at a company whose web app I need to access. I just checked with BuiltWith and it seems they are on Azure.&lt;/p&gt;
    &lt;p&gt;Does (should, could) DownDetector also say what customer-facing services are down, when some infrastructure is unworking? Or is that the info that the malefactors are seeking?&lt;/p&gt;
    &lt;p&gt;I absolutely love the utility aspect of LLMs but part of me is curious if moving faster by using AI is going to make these sorts of failure more and more often.&lt;/p&gt;
    &lt;p&gt;Unable to access the portal and any hit to SSO for other corporate accesses is also broken. Seems like there's something wrong in their Identity services.&lt;/p&gt;
    &lt;p&gt;Could be DNS, I'm seeing SERVFAIL trying to resolve what look to be MS servers when I'm hitting (just one example) mygoodtogo.com (trying to pay a road toll bill, and failing).&lt;/p&gt;
    &lt;p&gt;Apologies, but this just reads like a low effort critique of big things.&lt;/p&gt;
    &lt;p&gt;To be clear, they should get criticism. They should be held liable for any damage they cause.&lt;/p&gt;
    &lt;p&gt;But that they remain the biggest cloud offering out there isn't something you'd expect to change from a few outages that, by most all evidence, potential replacements have, as well? More, a lot of the outages potential replacements have are often more global in nature.&lt;/p&gt;
    &lt;p&gt;Yeah, I have non prod environments that don't use FD that are functioning. Routing through FD does not work. And a different app, nonprod doesn't use FD (and is working) but loads assets from the CDN (which is not working).&lt;/p&gt;
    &lt;p&gt;FD and CDN are global resources and are experiencing issues. Probably some other global resources as well.&lt;/p&gt;
    &lt;p&gt;Hate to say it, but DNS is looking like it's still the undisputed champ.&lt;/p&gt;
    &lt;p&gt;HTTPSConnectionPool(host='schemas.xmlsoap.org', port=443): Max retries exceeded with url: /soap/encoding/ (Caused by SSLError(CertificateError("hostname 'schemas.xmlsoap.org' doesn't match '*.azureedge.net'")))&lt;/p&gt;
    &lt;p&gt;A service we rely on that isn't even running on Azure is inaccessible due to this issue. For an asset that probably never changes. Wild for that to be the SPOF.&lt;/p&gt;
    &lt;p&gt;downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?&lt;/p&gt;
    &lt;p&gt;downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?&lt;/p&gt;
    &lt;p&gt;Yeah the graph for that one looks exactly the same shape. I wonder if they were depending on some azure component somehow, or maybe there were things hosted on both and the azure failure made enough things failover to AWS that AWS couldn't cope? If that was the case I'd expect to see something similar with GCP too though.&lt;/p&gt;
    &lt;p&gt;Edit: nope looks like there's actually a spike on GCP as well&lt;/p&gt;
    &lt;p&gt;Definitely also a strong possibility. I wish I had paid more attention during the AWS one earlier to see what other things looked like on there at the time.&lt;/p&gt;
    &lt;p&gt;winget upgrade fabric Failed in attempting to update the source: winget An unexpected error occurred while executing the command: InternetOpenUrl() failed. 0x80072ee7 : unknown error&lt;/p&gt;
    &lt;p&gt;When you look at the scale of the reports, you find they are much lower than Azure's. seeing a bunch of 24-hour sparkline type graphs next to each other can make it look like they are equally impacted, but AWS has 500 reports and Azure has 20,000. The scale is hidden by the choice of graph.&lt;/p&gt;
    &lt;p&gt;In other words, people reporting outages at AWS are probably having trouble with microsoft-run DNS services or caching proxies. It's not that the issues aren't there, it's that the internet is full of intermingled complexity. Just that amount of organic false-positives can make it look like an unrelated major service is impacted.&lt;/p&gt;
    &lt;p&gt;As of now Azure Status page still shows no incident. It must be manually updated, someone has to actively decide to acknowledge an issue, and they're just... not. It undermines confidence in that status page.&lt;/p&gt;
    &lt;p&gt;I know how to fix this but this community is too close minded and argumentative egocentric sensitive pedantic threatened angry etc to bother discussing it&lt;/p&gt;
    &lt;p&gt;I noticed issues on Azure so I went to the status page. It said everything was fine even though the Azure Portal was down. It took more than 10 minutes for that status page to update.&lt;/p&gt;
    &lt;p&gt;How can one of the richest companies in the world not offer a better service?&lt;/p&gt;
    &lt;p&gt;My best guess at the moment is something global like the CDN is having problems affecting things everywhere. I'm able to use a legacy application we have that goes directly to resources in uswest3, but I'm not able to use our more modern application which uses APIM/CDN networks at all.&lt;/p&gt;
    &lt;p&gt;From Azure status page: "Customers can consider implementing failover strategies with Azure Traffic Manager, to fail over from Azure Front Door to your origins".&lt;/p&gt;
    &lt;p&gt;I especially like how Nadella speaks of layoffs as some kind of uncontrollable natural disaster, like a hurricane, caused by no-one in particular. A kind of "God works in mysterious ways".&lt;/p&gt;
    &lt;p&gt;&amp;gt; “Microsoft is being recognized and rewarded at levels never seen before,” Nadella wrote. “And yet, at the same time, we’ve undergone layoffs. This is the enigma of success in an industry that has no franchise value.” &amp;gt; Nadella explained the disconnect between thriving financials and layoffs by stating that “progress isn’t linear” and that it is “sometimes dissonant, and always demanding.”&lt;/p&gt;
    &lt;p&gt;I've read the whole memo and it's actually worse than those excerpts. Nadella doesn't even claim these were low performers:&lt;/p&gt;
    &lt;p&gt;&amp;gt; These decisions are among the most difficult we have to make. They affect people we’ve worked alongside, learned from, and shared countless moments with—our colleagues, teammates, and friends.&lt;/p&gt;
    &lt;p&gt;Ok, so Microsoft is thriving, these were friends and people "we've learned from", but they must go because... uh... "progress isn't linear". Well, thanks Nadella! That explains so much!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45748661"/><published>2025-10-29T16:01:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748725</id><title>Composer: Building a fast frontier model with RL</title><updated>2025-10-30T07:10:38.961225+00:00</updated><content>&lt;doc fingerprint="3d5aedd9e03a0a1a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Composer: Building a fast frontier model with RL&lt;/head&gt;
    &lt;p&gt;Composer is our new agent model designed for software engineering intelligence and speed. On our benchmarks, the model achieves frontier coding results with generation speed four times faster than similar models.&lt;/p&gt;
    &lt;p&gt;We achieve these results by training the model to complete real-world software engineering challenges in large codebases. During training, Composer is given access to a set of production search and editing tools and tasked with efficiently solving a diverse range of difficult problems. The final result is a large-scale model optimized for high-speed use as an agent in Cursor.&lt;/p&gt;
    &lt;p&gt;Our motivation comes from our experience developing Cursor Tab, our custom completion model. We found that often developers want the smartest model that can support interactive use, keeping them in the flow of coding. In our development process, we experimented with a prototype agent model, codenamed Cheetah, to better understand the impact of faster agent models. Composer is a smarter version of this model that keeps coding delightful by being fast enough for an interactive experience.&lt;/p&gt;
    &lt;p&gt;Composer is a mixture-of-experts (MoE) language model supporting long-context generation and understanding. It is specialized for software engineering through reinforcement learning (RL) in a diverse range of development environments. At each iteration of training, the model is given a problem description and instructed to produce the best response, be it a code edit, a plan, or an informative answer. The model has access to simple tools, like reading and editing files, and also more powerful ones like terminal commands and codebase-wide semantic search.&lt;/p&gt;
    &lt;p&gt;To measure progress, we constructed an evaluation that measures a model's usefulness to a software developer as faithfully as possible. Our benchmark, Cursor Bench, consists of real agent requests from engineers and researchers at Cursor, along with hand-curated optimal solutions to these requests. The resulting evaluation measures not just the agent’s correctness, but also its adherence to a codebase's existing abstractions and software engineering practices.&lt;/p&gt;
    &lt;p&gt;Reinforcement learning allows us to actively specialize the model for effective software engineering. Since response speed is a critical component for interactive development, we incentivize the model to make efficient choices in tool use and to maximize parallelism whenever possible. In addition, we train the model to be a helpful assistant by minimizing unnecessary responses and claims made without evidence. We also find that during RL, the model learns useful behaviors on its own like performing complex searches, fixing linter errors, and writing and executing unit tests.&lt;/p&gt;
    &lt;p&gt;Efficient training of large MoE models requires significant investment into building infrastructure and systems research. We built custom training infrastructure leveraging PyTorch and Ray to power asynchronous reinforcement learning at scale. We natively train our models at low precision by combining our MXFP8 MoE kernels with expert parallelism and hybrid sharded data parallelism, allowing us to scale training to thousands of NVIDIA GPUs with minimal communication cost. Additionally, training with MXFP8 allows us to deliver faster inference speeds without requiring post-training quantization.&lt;/p&gt;
    &lt;p&gt;During RL, we want our model to be able to call any tool in the Cursor Agent harness. These tools allow editing code, using semantic search, grepping strings, and running terminal commands. At our scale, teaching the model to effectively call these tools requires running hundreds of thousands of concurrent sandboxed coding environments in the cloud. To support this workload, we adapted existing infrastructure we built for Background Agents, rewriting our virtual machine scheduler to support the bursty nature and scale of training runs. This enabled seamless unification of RL environments with production environments.&lt;/p&gt;
    &lt;p&gt;Cursor builds tools for software engineering, and we make heavy use of the tools we develop. A motivation of Composer development has been developing an agent we would reach for in our own work. In recent weeks, we have found that many of our colleagues were using Composer for their day-to-day software development. With this release, we hope that you also find it to be a valuable tool.&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;¹ Benchmarked on an internal benchmark in the Cursor tool harness. We group models into classes based on score and report the best model in each class. "Fast Frontier" includes models designed for efficient inference such as Haiku 4.5 and Gemini Flash 2.5. "Best Open" includes recent open weight model releases such as Qwen Coder and GLM 4.6. "Frontier 7/2025" is the best model available in July of this year. "Best Frontier" includes GPT-5 and Sonnet 4.5, which both outperform Composer. For the Tokens per Second calculation, tokens are standardized across models to the latest Anthropic tokenizer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cursor.com/blog/composer"/><published>2025-10-29T16:04:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748879</id><title>Minecraft removing obfuscation in Java Edition</title><updated>2025-10-30T07:10:38.641889+00:00</updated><content>&lt;doc fingerprint="370203ca08b7cced"&gt;
  &lt;main&gt;
    &lt;p&gt;Do you like to mod Java, tinker with builds, or take deep dives into Minecraft’s code? Then this article is for you!&lt;/p&gt;
    &lt;p&gt;For a long time, Java Edition has used obfuscation (hiding parts of the code) – a common practice in the gaming industry. Now we’re changing how we ship Minecraft: Java Edition to remove obfuscation completely. We hope that, with this change, we can pave a future for Minecraft: Java Edition where it’s easier to create, update, and debug mods.&lt;/p&gt;
    &lt;head rend="h2"&gt;An obfuscated history&lt;/head&gt;
    &lt;p&gt;Minecraft: Java Edition has been obfuscated since its release. This obfuscation meant that people couldn’t see our source code. Instead, everything was scrambled – and those who wanted to mod Java Edition had to try and piece together what every class and function in the code did.&lt;/p&gt;
    &lt;p&gt;But we encourage people to get creative both in Minecraft and with Minecraft – so in 2019 we tried to make this tedious process a little easier by releasing “obfuscation mappings”. These mappings were essentially a long list that allowed people to match the obfuscated terms to un-obfuscated terms. This alleviated the issue a little, as modders didn’t need to puzzle out what everything did, or what it should be called anymore. But why stop there?&lt;/p&gt;
    &lt;head rend="h2"&gt;Removing obfuscation in Java Edition&lt;/head&gt;
    &lt;p&gt;To make things even easier – and remove these intermediary steps – we’re removing obfuscation altogether! Starting with the first snapshot following the complete Mounts of Mayhem launch, we will no longer obfuscate Minecraft: Java Edition. This means that this build (and all future builds) will have all of our original names* – now with variable names and other names – included by default to make modding even easier.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.minecraft.net/en-us/article/removing-obfuscation-in-java-edition"/><published>2025-10-29T16:12:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749017</id><title>Tailscale Peer Relays</title><updated>2025-10-30T07:10:38.478433+00:00</updated><content>&lt;doc fingerprint="66f1d873751076c5"&gt;
  &lt;main&gt;
    &lt;p&gt;Tailscale Peer Relays provides a customer-deployed and managed traffic relaying mechanism. By advertising itself as a peer relay, a Tailscale node can relay traffic for any peer nodes on the tailnet, even for traffic bound to itself. Tailscale Peer Relays can only relay traffic for nodes on your tailnet, and only for nodes that have access to the peer relay. Because they’re managed entirely by the customer, peer relays are less throughput-constrained than Tailscale’s managed DERP relays, and can provide higher throughput connections for traffic to and from locked-down cloud infrastructure, or behind strict network firewalls.&lt;/p&gt;
    &lt;p&gt;In testing with early design partners, we’ve seen throughputs nearing that of a direct connection; often multiple orders of magnitude higher than Tailscale’s managed DERP fleet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moving past hard NAT&lt;/head&gt;
    &lt;p&gt;Over the past few weeks, you’ve heard us talk about improvements we’ve made to Network Address Translation (NAT) traversal techniques, so that Tailscale can establish direct connections wherever possible (hint: it’s over 90% of the time). However, we’ve also outlined places where this isn’t possible or desirable today for a variety of reasons, especially in cloud environments. And, we’ve postulated a bit about where we think the industry is going.&lt;/p&gt;
    &lt;p&gt;While we’ve been keeping your network reliably connected for years with DERP, we’ve heard from customers that the throughput and performance aspects of a QoS-aware managed relay fleet makes deployments in certain environments difficult or untenable. Furthermore, customers have noted that it’s non-trivial to deploy and manage custom DERP fleets (which run as a separate service and binary).&lt;/p&gt;
    &lt;p&gt;DERP provides an incredibly valuable service, setting up reliable connections between Tailscale clients anywhere in the world (including negotiating connections through peer relays). But often, DERP’s focus as a reliability and NAT traversal tool results in performance tradeoffs.&lt;/p&gt;
    &lt;p&gt;By contrast, Tailscale Peer Relays is designed as a performant connectivity tool, and can perform at a level rivaling direct connections. Peer relays can be placed right next to the resources they serve, and peer relays also run on top of UDP, both characteristics beneficial to lower latency and resource overhead. And, they are built into the Tailscale client itself for ease of deployment.&lt;/p&gt;
    &lt;p&gt;We want to move past even more hard NATs, and put Tailscale’s relaying technology in our customers’ hands, so they can use Tailscale at scale, anywhere, with ease. We believe our new Tailscale Peer Relays connectivity option—unique to Tailscale—gives customers the best performance and flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;Peer relays are configured with a single UDP port that must be available to both sides of a connection. Tailscale Peer Relays is built right into the Tailscale client, and can be enabled with a simple command, using the &lt;code&gt;tailscale set --relay-server-port&lt;/code&gt; flag from the Tailscale CLI. Once enabled via the steps in our documentation, clients can connect to infrastructure in hard NAT environments over the peer relay.&lt;/p&gt;
    &lt;p&gt;And don’t worry, we still prefer to fly direct. Tailscale prefers direct connections wherever possible. Clients can then fall back to available peer relays, and finally leverage Tailscale’s managed DERP fleet, or any customer-deployed custom DERPs, to ensure you have connectivity wherever you need it. All of this traffic, over any connection, is still end-to-end encrypted via WireGuard®.&lt;/p&gt;
    &lt;p&gt;Tailscale Peer Relays is designed for the real world, based on the feedback we’ve received from customers and our own hard-earned networking expertise. It allows customers to make just one firewall exception for connections only coming from their tailnet. Peer relays scale across regions, are resilient to real-world network conditions, and graciously fall back to DERP (Tailscale’s or custom). Your network maintains its shape, but gains all kinds of flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;Connectivity, everywhere, at warp speed&lt;/head&gt;
    &lt;p&gt;Customers can now maintain performance benchmarks even where direct connections aren’t possible, by enabling Tailscale Peer Relays to build a deterministic and high-throughput relay topology.&lt;/p&gt;
    &lt;p&gt;We’ve had customers use peer relays to provide access into unmanaged networks, allowing their partners or customers to provide a controllable and auditable connectivity path without sacrificing performance.&lt;/p&gt;
    &lt;p&gt;In strict private networks, customers can build predictable access paths. Tailscale Peer Relays can be placed in public subnets with VPC peering to private subnets, allowing security teams to efficiently segment their private network infrastructure, while enabling networking teams to roll Tailscale out in full-mesh mode across the subnet.&lt;/p&gt;
    &lt;p&gt;Today, customers are using peer relays to establish relayed connections at near-direct speeds across a variety of environments and settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable high-throughput traffic through cloud NATs, like AWS Managed NAT Gateways: Applications and services behind a Managed NAT Gateway can leverage peer relays to relay traffic that can’t establish direct connections.&lt;/item&gt;
      &lt;item&gt;Relay through network firewalls: Workloads running in strictly firewalled environments can predictably expose a single UDP port, limiting the Tailscale surface area and fast-tracking the approval process for firewall exceptions.&lt;/item&gt;
      &lt;item&gt;Offload from Custom and Managed DERP: Minimize data-in-transit through Tailscale‘s managed DERP network, and remove the need for customer-maintained DERP servers.&lt;/item&gt;
      &lt;item&gt;Provide access to locked down customer networks: Data plane traffic can be relayed through predictable endpoints in customer networks, so that they only need to open minimal numbers of ports to facilitate cross network connections.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;It’s not perfect, but we’re getting there&lt;/head&gt;
    &lt;p&gt;Tailscale Peer Relays is available today as a public beta. We’ve yet to establish all the connectivity paths we want to, and there’s still visibility and debugging improvements to work through. However, we’ve reliably seen our early design partners move to peer relay deployments with relative ease, and we’re ready for you to give it a try on your tailnet.&lt;/p&gt;
    &lt;p&gt;Tailscale Peer Relays can be enabled on all plans, including free (it’s our little way of working through the kinks of the modern internet with our customers). All customers can use two peer relays, for free, forever. As your needs scale, so will the number of available peer relays. To add even more peer relays to your tailnet, come have a chat with us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tailscale.com/blog/peer-relays-beta"/><published>2025-10-29T16:21:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749161</id><title>AOL to be sold to Bending Spoons for $1.5B</title><updated>2025-10-30T07:10:38.379945+00:00</updated><content/><link href="https://www.axios.com/2025/10/29/aol-bending-spoons-deal"/><published>2025-10-29T16:28:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45750425</id><title>OpenAI’s promise to stay in California helped clear the path for its IPO</title><updated>2025-10-30T07:10:38.107442+00:00</updated><content/><link href="https://www.wsj.com/tech/ai/openais-promise-to-stay-in-california-helped-clear-the-path-for-its-ipo-3af1c31c"/><published>2025-10-29T17:44:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45750875</id><title>The Internet runs on free and open source software and so does the DNS</title><updated>2025-10-30T07:10:37.703212+00:00</updated><content>&lt;doc fingerprint="8ac2858004c20ddf"&gt;
  &lt;main&gt;
    &lt;p&gt;Free and open-source software (FOSS) is not merely common on the Internet; it is a deeply embedded and essential foundation of the Domain Name System (DNS), the backbone of how we connect online.&lt;/p&gt;
    &lt;p&gt;The ICANN Security and Stability Advisory Committee (SSAC) is pleased to announce the publication of SAC132: The Domain Name System Runs on Free and Open Source Software (FOSS).&lt;/p&gt;
    &lt;head rend="h3"&gt;Why This Matters Now&lt;/head&gt;
    &lt;p&gt;As governments around the world explore new cybersecurity regulations, the ubiquity of FOSS in DNS operations—from domain registration to retrieval—means that policy decisions made today will have direct implications for the Internet's security and resilience tomorrow. SAC132 provides timely, nontechnical guidance to ensure that new policy and regulation serve to strengthen, rather than inadvertently weaken, this critical infrastructure.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Insights for Policymakers&lt;/head&gt;
    &lt;p&gt;SAC132 is a foundational guide designed to empower policymakers to strategically manage and sustain the FOSS ecosystem. The report provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clear Foundations – An accessible overview of the DNS and the FOSS development model for nontechnical audiences.&lt;/item&gt;
      &lt;item&gt;Policy Assessment – Analysis of cybersecurity regulations in the United States, United Kingdom, and European Union, with a focus on how they account for FOSS in the DNS ecosystem.&lt;/item&gt;
      &lt;item&gt;Practical Guidance – Concrete findings and recommendations to help policymakers support and secure FOSS as a cornerstone of global connectivity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We invite all policymakers, technical experts, and stakeholders to read the full report.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Call to Engage&lt;/head&gt;
    &lt;p&gt;By publishing SAC132, SSAC seeks to raise awareness of the indispensable role of FOSS in maintaining a secure, stable, and resilient Internet. We invite policymakers, technical experts, and all stakeholders to read the full report and join us in conversations about its findings.&lt;/p&gt;
    &lt;p&gt;You can engage with SSAC and the broader community at ICANN84, whether in Dublin or by participating remotely. Together, we can ensure that the FOSS ecosystem—and the Internet it supports—remains strong, sustainable, and open for all.&lt;/p&gt;
    &lt;p&gt;Finally, we thank all SSAC members and invited experts who contributed to this work, especially co-chairs Maarten Aertsen and Barry Leiba, for their leadership.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.icann.org/en/blogs/details/the-internet-runs-on-free-and-open-source-softwareand-so-does-the-dns-23-10-2025-en"/><published>2025-10-29T18:16:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45750954</id><title>Dithering – Part 1</title><updated>2025-10-30T07:10:37.598058+00:00</updated><content>&lt;doc fingerprint="1336983308d69b52"&gt;
  &lt;main&gt;
    &lt;p&gt;Understanding how dithering works, visually.&lt;/p&gt;
    &lt;p&gt;tap/click the right side of the screen to go forward →&lt;/p&gt;
    &lt;p&gt;I’ve always been fascinated by the dithering effect. It has a unique charm that I find so appealing.&lt;/p&gt;
    &lt;p&gt;← tap/click the left side to go back&lt;/p&gt;
    &lt;p&gt;I was even more amazed when I learned how dithering works.&lt;/p&gt;
    &lt;p&gt;← or use arrow keys to navigate →&lt;/p&gt;
    &lt;p&gt;Look closely, and you’ll see this animation is made of alternating black and white pixels.&lt;/p&gt;
    &lt;p&gt;But these black and white pixels are specifically arranged to create the illusion of multiple shades.&lt;/p&gt;
    &lt;p&gt;That’s what dithering does: it simulates more color variations than what are actually used.&lt;/p&gt;
    &lt;p&gt;Here, it uses black and white to give the impression of multiple gray shades.&lt;/p&gt;
    &lt;p&gt;To me, dithering is about creating the most out of what we have, and that's what amazes me the most!&lt;/p&gt;
    &lt;p&gt;It inspired me to learn more about it, and now I want to share what I’ve learned.&lt;/p&gt;
    &lt;p&gt;Please note that this is just part one out of three, so I’ll only scratch the surface here.&lt;/p&gt;
    &lt;p&gt;I’ll go deeper in the next parts, which will come soon. Stay tuned!&lt;/p&gt;
    &lt;p&gt;First, let’s explore the dithering basics with this grayscale image example.&lt;/p&gt;
    &lt;p&gt;A grayscale image has various gray shades, from black to white.&lt;/p&gt;
    &lt;p&gt;Imagine a display that only shows black or white pixels, no grays. We must turn some pixels black and others white—but how?&lt;/p&gt;
    &lt;p&gt;One way is to map each pixel to the closest available color.&lt;/p&gt;
    &lt;p&gt;Pixels darker than medium gray turn black and lighter ones turn white.&lt;/p&gt;
    &lt;p&gt;This splits pixels into black or white groups.&lt;/p&gt;
    &lt;p&gt;However, this creates a harsh image with abrupt black-white transitions.&lt;/p&gt;
    &lt;p&gt;Shadow details vanish as gray pixels become fully black or white.&lt;/p&gt;
    &lt;p&gt;Dithering fixes this by selectively pushing some pixels towards the opposite color.&lt;/p&gt;
    &lt;p&gt;Some light gray pixels that are closer to white turn black.&lt;/p&gt;
    &lt;p&gt;Likewise, some dark grays turn white.&lt;/p&gt;
    &lt;p&gt;And it's done in a way that produces special patterns which simulate shades by varying the black-and-white pixel densities.&lt;/p&gt;
    &lt;p&gt;Denser black pixels are used in darker areas, while denser white pixels are used in lighter ones.&lt;/p&gt;
    &lt;p&gt;Next question: How are these patterns generated?&lt;/p&gt;
    &lt;p&gt;One simple dithering method, known as ordered dithering, uses a threshold map.&lt;/p&gt;
    &lt;p&gt;A threshold map is a grid of values representing brightness levels, from 0 (darkest) to 1 (brightest).&lt;/p&gt;
    &lt;p&gt;To dither, we compare each input pixel’s brightness to a corresponding threshold value.&lt;/p&gt;
    &lt;p&gt;If a pixel’s brightness exceeds the threshold (it’s brighter than the threshold), the pixel turns white. Otherwise, it turns black.&lt;/p&gt;
    &lt;p&gt;Repeating this for all pixels gives us the black-and-white dither patterns.&lt;/p&gt;
    &lt;p&gt;The threshold map is designed to output patterns where the black-and-white pixel density matches the input image’s shades.&lt;/p&gt;
    &lt;p&gt;So brighter input produces patterns with more white, while darker input produces more black.&lt;/p&gt;
    &lt;p&gt;These black-and-white density variations are what create the illusion of gray shades when viewed from a distance.&lt;/p&gt;
    &lt;p&gt;To dither larger images, we extend the threshold map to match the image size and follow the same principle:&lt;/p&gt;
    &lt;p&gt;Compare each pixel’s brightness to the threshold map, then turn it black or white accordingly.&lt;/p&gt;
    &lt;p&gt;The image now uses only two colors, but its overall appearance is preserved.&lt;/p&gt;
    &lt;p&gt;The variations in shades are now replaced by variations in black/white pixel density of the dithering patterns.&lt;/p&gt;
    &lt;p&gt;And that’s how dithering works in a nutshell: it replicates shades with fewer colors, which are strategically placed to maintain the original look.&lt;/p&gt;
    &lt;p&gt;I find it a bit ironic how I used to think dithering ‘adds’ a cool effect, when what it actually does is ‘remove’ colors!&lt;/p&gt;
    &lt;p&gt;That's all for now! We’ve reached the end, but there’s still a lot more to explore.&lt;/p&gt;
    &lt;p&gt;For example, we haven’t explored the algorithm to create a threshold map. (spoiler: there are many ways!)&lt;/p&gt;
    &lt;p&gt;There’s also another algorithm called error diffusion, which doesn’t use a threshold map.&lt;/p&gt;
    &lt;p&gt;Each algorithm creates a distinct, unique look, which I believe deserves its own article.&lt;/p&gt;
    &lt;p&gt;And that's why I decided to break this series into three parts.&lt;/p&gt;
    &lt;p&gt;In the next part, I’ll dive into various algorithms for creating threshold maps.&lt;/p&gt;
    &lt;p&gt;In the final part, I’ll focus on the error diffusion algorithm.&lt;/p&gt;
    &lt;p&gt;We'll dive even deeper into dithering's mechanisms in these next 2 parts, so stay tuned!&lt;/p&gt;
    &lt;p&gt;Thank you for reading!&lt;/p&gt;
    &lt;p&gt;visualrambling.space is a personal project by Damar, someone who loves to learn about different topics and rambling about them visually.&lt;/p&gt;
    &lt;p&gt;If you like this kind of visual article, please consider following me on X/Twitter and sharing this with your friends.&lt;/p&gt;
    &lt;p&gt;I'll keep creating more visual articles like this!&lt;/p&gt;
    &lt;p&gt;https://x.com/damarberlari&lt;/p&gt;
    &lt;p&gt;_blank&lt;/p&gt;
    &lt;p&gt;Topics covered: Three.js, WebGL, dithering, visualization, interactive learning&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://visualrambling.space/dithering-part-1/"/><published>2025-10-29T18:21:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45750995</id><title>Extropic is building thermodynamic computing hardware</title><updated>2025-10-30T07:10:37.396349+00:00</updated><content>&lt;doc fingerprint="27b6766c759d22ac"&gt;
  &lt;main&gt;
    &lt;p&gt;Thermodynamic Computing: From 0 to 1 (Launch Video)&lt;/p&gt;
    &lt;p&gt;October 30th, 2025&lt;/p&gt;
    &lt;p&gt;Extropic is building thermodynamic computing hardware that is radically more energy efficient than GPUs.&lt;/p&gt;
    &lt;p&gt;Our thermodynamic sampling units (TSUs) are inherently probabilistic, the perfect fit for probabilistic AI workloads.&lt;/p&gt;
    &lt;p&gt;Hardware&lt;/p&gt;
    &lt;p&gt;prototype platform&lt;/p&gt;
    &lt;p&gt;XTR-0 enables the development of ultra-efficient AI algorithms by providing low-latency communication between Extropic chips and a traditional processor.&lt;/p&gt;
    &lt;p&gt;Software&lt;/p&gt;
    &lt;p&gt;Our open-source Python library that enables everyone to develop thermodynamic algorithms and simulate running them on TSUs&lt;/p&gt;
    &lt;p&gt;We are hiring engineers and scientists to help us pioneer a new form of computing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://extropic.ai/"/><published>2025-10-29T18:25:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45751400</id><title>Uv is the best thing to happen to the Python ecosystem in a decade</title><updated>2025-10-30T07:10:37.297630+00:00</updated><content>&lt;doc fingerprint="14982e51cdc48290"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;uv is the best thing to happen to the Python ecosystem in a decade&lt;/head&gt;
    &lt;p&gt;23 October 2025 | Reading time: 6 minutes&lt;/p&gt;
    &lt;p&gt;It’s 2025. Does installing Python, managing virtual environments, and synchronizing dependencies between your colleagues really have to be so difficult? Well… no! A brilliant new tool called uv came out recently that revolutionizes how easy installing and using Python can be.&lt;/p&gt;
    &lt;p&gt;uv is a free, open-source tool built by Astral, a small startup that has been churning out Python tools (like the excellent linter Ruff) for the past few years. uv can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install any Python version for you&lt;/item&gt;
      &lt;item&gt;Install packages&lt;/item&gt;
      &lt;item&gt;Manage virtual environments&lt;/item&gt;
      &lt;item&gt;Solve dependency conflicts extremely quickly (very important for big projects.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s best is that it can do all of the above better than any other tool, in my opinion. It’s shockingly fast, written in Rust, and works on almost any operating system or platform.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installing uv&lt;/head&gt;
    &lt;p&gt;uv is straightforward to install. There are a few ways, but the easiest (in my opinion) is this one-liner command — for Linux and Mac, it’s:&lt;/p&gt;
    &lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;
    &lt;p&gt;or on Windows in powershell:&lt;/p&gt;
    &lt;code&gt;powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"&lt;/code&gt;
    &lt;p&gt;You can then access uv with the command &lt;code&gt;uv&lt;/code&gt;. Installing uv will not mess up any of your existing Python installations — it’s a separate tool, so it’s safe to install it just to try it out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Managing Python for a project&lt;/head&gt;
    &lt;p&gt;It’s always a good idea to work with virtual environments for any Python project. It keeps different bits of code and dependencies ringfenced from one another, and in my experience, it can save a lot of hassle to get into the habit of using virtual environments as soon as you can. uv naturally uses virtual environments, so it’s very easy to start using them if you get into using uv.&lt;/p&gt;
    &lt;p&gt;uv will build a Python environment for you based on what’s specified in a &lt;code&gt;pyproject.toml&lt;/code&gt; file in the directory (or parent directories) you’re working in. &lt;code&gt;pyproject.toml&lt;/code&gt; files are a standard, modern format for specifying dependencies for a Python project. A barebones one might look a bit like this:&lt;/p&gt;
    &lt;code&gt;[project]
name = "my_project"
version = "1.0.0"
requires-python = "&amp;gt;=3.9,&amp;lt;3.13"
dependencies = [
  "astropy&amp;gt;=5.0.0",
  "pandas&amp;gt;=1.0.0,&amp;lt;2.0",
]&lt;/code&gt;
    &lt;p&gt;In essence, it just has to specify which Python version to use and some dependencies. Adding a name and version number also aren’t a bad idea.&lt;/p&gt;
    &lt;p&gt;(Sidenote: for projects that you publish as packages, such as to the Python Package Index that pip and uv use, &lt;code&gt;pyproject.toml&lt;/code&gt; files are a modern way to specify everything you need to publish your package.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Making a new project with uv&lt;/head&gt;
    &lt;p&gt;To start a new Python project with uv, you can run&lt;/p&gt;
    &lt;code&gt;uv init&lt;/code&gt;
    &lt;p&gt;Which will create a new project for you, with a &lt;code&gt;pyproject.toml&lt;/code&gt;, a &lt;code&gt;README.md&lt;/code&gt;, and other important bits of boilerplate.&lt;/p&gt;
    &lt;p&gt;There are a lot of different ways to run this command, like &lt;code&gt;uv init --bare&lt;/code&gt; (which only creates a pyproject.toml), &lt;code&gt;uv init --package&lt;/code&gt; (which sets up a new Python package), and more. I recommend running &lt;code&gt;uv init --help&lt;/code&gt; to read about them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Once you have/if you already have a &lt;code&gt;pyproject.toml&lt;/code&gt; file&lt;/head&gt;
    &lt;p&gt;Once you initialize a project — or if you already have a &lt;code&gt;pyproject.toml&lt;/code&gt; file in your project — it’s very easy to start using uv. You just need to do&lt;/p&gt;
    &lt;code&gt;uv sync&lt;/code&gt;
    &lt;p&gt;in the directory that your &lt;code&gt;pyproject.toml&lt;/code&gt; file is in. This command (and in fact, most uv commands if you haven’t ran it already) will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Automatically install a valid version of Python&lt;/item&gt;
      &lt;item&gt;Install all dependencies to a new virtual environment in the directory &lt;code&gt;.venv&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Create a &lt;code&gt;uv.lock&lt;/code&gt;file in your directory, which saves the exact, platform-agnostic version of every package installed — meaning that other colleagues can replicate your Python environment exactly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In principle, you can ‘activate’ this new virtual environment like any typical virtual environment that you may have seen in other tools, but the most ‘uv-onic’ way to use uv is simply to prepend any command with &lt;code&gt;uv run&lt;/code&gt;. This command automatically picks up the correct virtual environment for you and runs your command with it. For instance, to run a script — instead of&lt;/p&gt;
    &lt;code&gt;source .venv/bin/activate
python myscript.py&lt;/code&gt;
    &lt;p&gt;you can just do&lt;/p&gt;
    &lt;code&gt;uv run myscript.py&lt;/code&gt;
    &lt;p&gt;which will have the same effect. Likewise, to use a ‘tool’ like Jupyter Lab, you can just do&lt;/p&gt;
    &lt;code&gt;uv run jupyter lab&lt;/code&gt;
    &lt;p&gt;in your project’s directory, as opposed to first ‘activating’ the environment and then running &lt;code&gt;jupyter lab&lt;/code&gt; separately.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adding dependencies&lt;/head&gt;
    &lt;p&gt;You can always just edit your &lt;code&gt;pyproject.toml&lt;/code&gt; file manually: uv will detect the changes and rebuild your project’s virtual environment. But uv also has easier ways to add dependencies — you can just do&lt;/p&gt;
    &lt;code&gt;uv add numpy&amp;gt;=2.0&lt;/code&gt;
    &lt;p&gt;to add a package, including specifying version constraints (like the above.) This command automatically edits your &lt;code&gt;pyproject.toml&lt;/code&gt; for you. &lt;code&gt;uv add&lt;/code&gt; is also extremely powerful for adding remote dependencies from git or elsewhere on your computer (but I won’t get into that here.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Pinning a Python version&lt;/head&gt;
    &lt;p&gt;Finally, I think that one of the most useful things uv can do is to pin a specific Python version for your project. Doing&lt;/p&gt;
    &lt;code&gt;uv python pin 3.12.9&lt;/code&gt;
    &lt;p&gt;would pin the current project to exactly Python 3.12.9 for you, and anyone else using uv — meaning that you really can replicate the exact same Python install across multiple machines.&lt;/p&gt;
    &lt;head rend="h2"&gt;uvx: ignore all of the above and just run a tool, now!&lt;/head&gt;
    &lt;p&gt;But sometimes, you might just want to run a tool quickly — like using Ruff to lint code somewhere, or starting a Jupyter notebook server without an environment, or even just quickly starting an IPython session with pandas installed so you can open up a file. The &lt;code&gt;uv tool&lt;/code&gt; command, which has a short alias &lt;code&gt;uvx&lt;/code&gt;, makes this insanely easy. Running a command like&lt;/p&gt;
    &lt;code&gt;uvx ruff&lt;/code&gt;
    &lt;p&gt;will automatically download the tool you want to use and run it in a one-off virtual environment. Once the tool has been downloaded before, this is lightning-fast because of how uv uses caches.&lt;/p&gt;
    &lt;p&gt;There are a lot of occasions when I might want to do this — a common one might be to quickly start an IPython session with pandas installed (using &lt;code&gt;--with&lt;/code&gt; to add dependencies) so that I can quickly open &amp;amp; look at a parquet file. For instance:&lt;/p&gt;
    &lt;code&gt;uvx --with pandas,pyarrow ipython&lt;/code&gt;
    &lt;p&gt;Or, maybe just starting a Jupyter Lab server so that I can quickly open a Jupyter notebook that a student sent me:&lt;/p&gt;
    &lt;code&gt;uvx jupyter lab&lt;/code&gt;
    &lt;p&gt;Or honestly just so many other weird, one-off use cases where &lt;code&gt;uvx&lt;/code&gt; is really nice to have around. I don’t feel like I’m missing out by always using virtual environments, because &lt;code&gt;uvx&lt;/code&gt; always gives you a ‘get out of jail free’ card whenever you need it.&lt;/p&gt;
    &lt;head rend="h2"&gt;If that hasn’t sold you: a personal note&lt;/head&gt;
    &lt;p&gt;I first discovered uv last year, while working together with our other lovely developers on building The Astrosky Ecosystem — a wonderful project to build open-source social media integrations for astronomers online. But with multiple developers all working asynchronously on multiple operating systems, managing Python installations quickly became a huge task.&lt;/p&gt;
    &lt;p&gt;uv is an incredibly powerful simplification for us that we use across our entire tech stack. As developers, we can all work with identical Python installations, which is especially important given a number of semi-experimental dependencies that we use that have breaking changes with every version. On GitHub Actions, we’re planning to use uv to quickly build a Python environment and run our unit tests. In production, uv already manages Python for all of our servers.&lt;/p&gt;
    &lt;p&gt;It’s just so nice to always know that Python and package installation will always be handled consistently and correctly across all of our machines. That’s why uv is the best thing to happen to the Python ecosystem in a decade.&lt;/p&gt;
    &lt;head rend="h2"&gt;Find out more&lt;/head&gt;
    &lt;p&gt;There’s a lot more on the uv docs, including a getting started page, more in-depth guides, explanations of important concepts, and a full command reference.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://emily.space/posts/251023-uv"/><published>2025-10-29T18:57:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45751995</id><title>How to Obsessively Tune WezTerm</title><updated>2025-10-30T07:10:37.108843+00:00</updated><content>&lt;doc fingerprint="7add1d0f7ddf6ef5"&gt;
  &lt;main&gt;
    &lt;p&gt;rashil2000 Posted: 6 October 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rashil2000.me/blogs/tune-wezterm"/><published>2025-10-29T19:39:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45753222</id><title>How the U.S. National Science Foundation enabled Software-Defined Networking</title><updated>2025-10-30T07:10:36.818489+00:00</updated><content>&lt;doc fingerprint="b5199159c83903c1"&gt;
  &lt;main&gt;
    &lt;p&gt;The Internet underlies much of modern life, connecting billions of users via access networks across wide-area backbones to countless services running in datacenters. The commercial Internet grew quickly in the 1990s and early 2000s because it was relatively easy for network owners to connect interoperable equipment, such as routers, without relying on a central administrative authority. However, a small number of router vendors controlled both the hardware and the software on these devices, leaving network owners with limited control over how their networks behave. Adding new network capabilities required support from these vendors and a multi-year standardization process to ensure interoperability across vendors. The result was bloated router software with tens of millions of lines of code, networks that were remarkably difficult to manage, and a frustratingly slow pace of innovation.&lt;/p&gt;
    &lt;p&gt;All of this changed with software-defined networking (SDN), where network owners took control over how their networks behaved. The key ideas were simple. First, network devices should offer a common open interface directly to their packet-forwarding logic. This interface allows separate control software to install fine-grained rules that govern how a network device handles different kinds of packets: which packets to drop, where to forward the remaining packets, how to modify the packet headers, and so on. Second, a network should have logically centralized control, where the control software has network-wide visibility and direct control across the distributed collection of network devices. Rather than running on the network devices themselves, the software can run on a separate set of computers that monitor and control the devices of a single network in real time.&lt;/p&gt;
    &lt;p&gt;The first commercial deployments of SDN started around 2008, and its success can be traced back to two intertwined developments that reinforced each other. The first was academic research funded mostly by the U.S. National Science Foundation (NSF). The second was cloud companies starting to build enormous datacenters, which required a new kind of network to interconnect thousands of racks of servers. In a virtuous cycle, the adoption of SDN by the hyperscalers drove further academic research, which in turn created more research, important new innovations, and several successful start-up companies.&lt;/p&gt;
    &lt;p&gt;As a result, SDN revolutionized how networks are built and operated today—the public Internet, private networks in commercial companies, university networks and government networks, and all the way through to the cellular networks that interconnect our smartphones.&lt;/p&gt;
    &lt;p&gt;Early NSF-funded SDN research. In 2001, a National Academies report, Looking Over the Fence at Networks: A Neighbor’s View of Networking Research,30 pointed to the perils of Internet ossification: an inability of networks to change to satisfy new needs. The report highlighted three dimensions of ossification: intellectual (backward compatibility limits creative ideas), infrastructure (it is hard to deploy new ideas into the infrastructure), and system (rigid architecture led to fragile, shoe-horned solutions). In an unprecedented move, the NSF set out to address Internet ossification by investing heavily over the next decade. NSF investments laid the groundwork for SDN. We describe NSF investments here, through the lens of the support we received in our own research groups. Importantly, these and other government-funded research programs fostered a community of researchers that together paved the way for commercial adoption of SDN in the years that followed.&lt;/p&gt;
    &lt;p&gt;100×100 project: In 2003, the NSF launched the 100×100 project as part of its Information Technology Research program. The goal of the 100×100 project was to create communication architectures that could provide 100Mb/s networking for all 100 million American homes. The project brought together researchers from Carnegie Mellon, Stanford, Berkeley, and AT&amp;amp;T. One key aspect of the 100×100 project was the design of better ways to manage large networks. This research led to the 4D architecture for logically centralized network control of a distributed data plane21 (which itself built upon and generalized the routing control platform work at AT&amp;amp;T15), Ethane (a system for logically centralized control of access control in enterprise networks),11 and OpenFlow (an open interface for installing match-action rules in network switches),28 as well as the creation of the first open source network controller, NOX.22&lt;/p&gt;
    &lt;p&gt;Global Environment for Network Innovation (GENI): NSF and researchers wanted to try out new Internet architectures on a nationwide, or global, platform. Computer virtualization was widely used to share a common physical infrastructure, so could we do the same for a network? In 2005, “Overcoming the Internet Impasse through Virtualization” proposed an approach.5 The next year, NSF created the GENI program, with the goal of creating a shared, programmable national infrastructure for researchers to experiment with alternative Internet architectures at scale. GENI funded early OpenFlow deployments on college campuses, sliced by FlowVisor35 to allow multiple experimental networks to run alongside each other on the same production network, each managed by their own experimental controller. This, in turn, led to a proliferation of new open source controllers (Beacon, POX, and Floodlight). GENI also led to a programmable virtualized backbone network platform,6 and an experimental OpenFlow backbone network in Internet2 connecting multiple universities. This led to OpenFlow-enabled switches from Cisco, HP, and NEC. GENI funded the purchase of OpenFlow whitebox switches from ODM manufacturers and the open source software to manage them. NSF funded the NetFPGA project, which enabled experimental OpenFlow switches in Internet2. NSF brought together a community of researchers driven by much more than the desire to create experimental test beds; many researchers came to realize that programmability and virtualization were, in fact, key capabilities needed for future networks.5,16&lt;/p&gt;
    &lt;p&gt;Future Internet Design (FIND): In 2007, NSF started the FIND program to support new Internet architectures that could be prototyped and evaluated on the GENI test bed. The FIND program and its successor Future Internet Architecture (FIA) in 2010 expanded the community, working on clean-slate network architectures and fostering alternative designs. The resulting ideas were bold and exciting, including better support for mobility, content delivery, user privacy, secure cloud computing, and more. NSF’s FIND and FIA programs fostered many clean-slate network designs with prototypes and real-world evaluation, many leveraged SDN and improved its foundations. As momentum for clean-slate networking research grew in the U.S., the rest of the world followed suit, such as the EU Future Internet Research and Experimentation (FIRE) program.&lt;/p&gt;
    &lt;p&gt;Programmable Open Mobile Internet (POMI) Expedition: In 2008, the NSF POMI Expedition at Stanford expanded funding for SDN, including its use in mobile networks. POMI funded the early development of ONOS, an open source distributed controller,8 and the widely used Mininet network emulator for teaching SDN and for testing ideas before deploying them in real networks. POMI also funded the first explorations of programmable forwarding planes, setting the stage for the first fully programmable switch chip10 and the widely used P4 language.9&lt;/p&gt;
    &lt;p&gt;SDN adoption by cloud hyperscalers. In parallel with the early academic research on SDN, large technology companies such as Microsoft, Google, Amazon, and Facebook began building large datacenters full of servers that hosted these companies’ popular Internet services and, increasingly, the services of enterprise customers. Datacenter owners grew frustrated with the cost and complexity of the commercially available networking equipment; a typical datacenter switch cost more than $20,000 and a hyperscaler needed about 10,000 switches per site. They decided they could build their own switch box for about $2,000 using off-the-shelf switching chips from companies such as Broadcom and Marvell, and then use their own armies of software developers to create optimized, tailored software using modern software practices. Reducing cost was good, but it was control they wanted and SDN gave them a quick path to get it.&lt;/p&gt;
    &lt;p&gt;The hyperscalers used SDN to realize two especially important use cases. First, within a single datacenter, cloud providers wanted to virtualize their networks to provide a separate virtual network for each enterprise customer (or “tenant”) with its own IP address space and networking policies. The start-up company Nicira, which emerged from the NSF-funded Ethane project, developed the Network Virtualization Platform (NVP)26 to meet this need. Nicira was later acquired by VMware and NVP became NSX. Nicira also created Open vSwitch (OVS),33 an open source virtual switch for Linux, with an OpenFlow interface. OVS grew rapidly and became the key to enabling network virtualization in datacenters around the world. Second, the hyperscalers wanted to control traffic flows across their new private wide-area networks and between their datacenters. Google adopted SDN to control how traffic is routed in its B4 backbone,23,39 using OpenFlow switches, controlled by ONIX, the first distributed controller platform.27 When Google first described B4 at the Open Network Summit in 2012, it sparked a global surge in research and commercialization of SDN. There were so many papers at ACM SIGCOMM that a separate conference—Hot Topics in Software-Defined Networking (HotSDN, later SOSR) was formed.&lt;/p&gt;
    &lt;p&gt;These two high-profile use cases—multi-tenant virtualization and wide-area traffic engineering—drew significant commercial attention to SDN. Indeed, NSF-funded research led directly to the creation of several successful SDN start-up companies, including Big Switch Networks (open source SDN controllers and management applications, acquired by Arista), Forward Networks (network verification products), Veriflow (developed network verification products, acquired by VMware), and Barefoot Networks (programmable switches, acquired by Intel), to name a few. SDN influenced the large networking vendors, with Cisco, Juniper, Arista, HP, and NEC all creating SDN products. Today, AMD, Nvidia, Intel, and Cisco all sell P4-programmable products, and in 2019 about a third of papers appearing at ACM SIGCOMM were based on P4 or programmable forwarding.&lt;/p&gt;
    &lt;p&gt;The commercial success of SDN drove further interest among academic researchers. The NSF and other government agencies, especially the Defense Advanced Research Project Agency (DARPA), sponsored further research on SDN platforms and use cases that continues to this day. The SDN research community broadened significantly, well beyond computer networking, to include researchers in the neighboring disciplines of programming languages, formal verification, distributed systems, algorithms, security and privacy, and more, all helping lay stronger foundations for future networks.&lt;/p&gt;
    &lt;p&gt;This article summarizes the story of how SDN arose. So many research projects, papers, companies, and products arose because of SDN that it is impossible to include all of them here. The foresight of NSF in the early 2000s, funding a generation of researchers at just the right time, working closely with the rapidly growing hyperscalers, led quite literally to a transformation—a revolution—in how networks are built today.&lt;/p&gt;
    &lt;p&gt;SDN Grew First and Fastest in Datacenters&lt;/p&gt;
    &lt;p&gt;The first large-scale deployments of SDN took place in hyperscale data centers, beginning about 2010. The story is best told by the hyperscaler companies themselves, and so we asked leaders at Google, Microsoft Azure, and Meta to tell their stories about why and how they adopted SDN. As you will see, they all started from the ideas and principles that came from the NSF-funded research; and each tailored SDN to suit their specific needs and culture.&lt;/p&gt;
    &lt;p&gt;The Internet Service Providers (ISPs) and telecommunication companies also had a strong interest in SDN. AT&amp;amp;T played a large role in its definition, engaging in research and early deployments in the mid 2000s. We invited Albert Greenberg, who was at AT&amp;amp;T at the time, to tell the story.&lt;/p&gt;
    &lt;p&gt;Nicira was perhaps the startup that epitomized the SDN movement. It grew out of the NSF-funded program and the Clean Slate Program at Stanford, based on the Ph.D. work of Martín Casado. Nicira developed ONIX, the first distributed control plane, used by Google in its infrastructure; OVS, the first OpenFlow-compliant software switch; and NVP (later NSX), the first network virtualization platform. We invited Teemu Koponen, a principal architect at Nicira, to tell the story.&lt;/p&gt;
    &lt;p&gt;During the early 2010s, the networking industry began to realize that SDN has many big advantages. It lifts complex protocols up and out of the switches into the control plane, where it is written in a modern programming language. This made it possible to reason about the correctness of the protocols simply by examining the software controlling the network and the forwarding state maintained by the switches. For the first time, it became possible to formally verify the behavior of a complete network.&lt;/p&gt;
    &lt;p&gt;Researchers, startups, network equipment vendors, and hyperscalers have all taken advantage of SDN principles to develop new ways to verify network behavior. We invited Professor George Varghese, who has been deeply involved in network verification research, to give us his perspective on network verification.&lt;/p&gt;
    &lt;p&gt;A main benefit of SDN is that it hands over the keys (of control) from the networking equipment vendors—who kept their systems closed and proprietary, and hence tended to evolve slowly—to software programmers, who could define the behavior for themselves, often in open source software. And indeed it happened: Today, most large networks are controlled by software written by those who own and operate networks rather than by networking equipment vendors.&lt;/p&gt;
    &lt;p&gt;But what about the hardware? Switches, routers, firewalls, and network interface cards are all built from special-purpose ASICs—highly integrated, cost-effective, and super-fast. The problem was the features and protocols that operated on packets (for example, forwarding, routing, firewalls, and security) were all baked into hardware at the time the chip was designed, two to three years before it was deployed. What if the network owner and operator needed to change and evolve the behavior in their network, for example to add a new way to measure traffic or a new way to verify behavior? A group of researchers and entrepreneurs set out to make the switches and NICs programmable by the user, to allow more rapid improvement and give the operator greater control. Not only did new programmable devices emerge, but a whole open source movement around the P4 programming language.&lt;/p&gt;
    &lt;p&gt;We invited Professor Nate Foster, who leads the P4 language ecosystem, to tell the story of how programmable forwarding planes came about.&lt;/p&gt;
    &lt;p&gt;So far, we have focused on SDN wireline networks running over electrical and optical cables in datacenters, enterprises, and long-haul WANs. SDN was originally defined with wireline networks in mind.&lt;/p&gt;
    &lt;p&gt;Yet, for cellular networks, the most widely used networks in the world, the need was even greater: Cellular networks have been held back for decades by closed, proprietary, and complex “standards” designed to allow equipment vendors to maintain a strong grip on the market. SDN provides an opportunity to open up networks, introducing well-defined control APIs and interfaces, moving control software to common operating systems running on commodity servers.&lt;/p&gt;
    &lt;p&gt;This story has only just begun, but it started thanks to NSF-funded research in the mid 2000s, then boosted by DARPA-funded programs to support open source software for cellular infrastructure. We invited Guru Parulkar and Oğuz Sunay to tell the story, both of whom developed open source cellular systems at the Open Networking Foundation and for the DARPA-funded Pronto project.&lt;/p&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;The investments NSF made in SDN over the past two decades have paid huge dividends. SDN transformed how companies run their datacenter, enterprise, cellular, and backbone networks, and created a pathway for creative new ideas to see widespread deployment. The biggest beneficiaries are the billions of people who have a much more reliable, more secure, lower-cost, and faster Internet for the services they use every day.&lt;/p&gt;
    &lt;p&gt;NSF invested in the foundations of SDN at a very early stage, back when it seemed unthinkable that network owners—rather than a few incumbent equipment vendors—could decide how networks behave. NSF nurtured the growing interest in SDN over many years, fostering a vibrant research community, critical software building blocks, and key early start-up companies that made SDN technologies available in practice. The Internet, and indeed computing and communication technologies in general, need the kind of bold, ongoing innovation that NSF makes possible.&lt;/p&gt;
    &lt;p&gt;Submit an Article to CACM&lt;/p&gt;
    &lt;p&gt;CACM welcomes unsolicited submissions on topics of relevance and value to the computing community.&lt;/p&gt;
    &lt;p&gt;You Just Read&lt;/p&gt;
    &lt;p&gt;How the U.S. National Science Foundation Enabled Software-Defined Networking&lt;/p&gt;
    &lt;p&gt;Communications of the ACM (CACM) is now a fully Open Access publication.&lt;/p&gt;
    &lt;p&gt;By opening CACM to the world, we hope to increase engagement among the broader computer science community and encourage non-members to discover the rich resources ACM has to offer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cacm.acm.org/federal-funding-of-academic-research/how-the-u-s-national-science-foundation-enabled-software-defined-networking/"/><published>2025-10-29T21:22:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45754390</id><title>A century of reforestation helped keep the eastern US cool (2024)</title><updated>2025-10-30T07:10:36.521324+00:00</updated><content>&lt;doc fingerprint="d2b8118e6db5e9dd"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Much of the U.S. warmed during the 20th century, but the eastern part of the country remained mysteriously cool. The recovery of forests could explain why&lt;/head&gt;
    &lt;p&gt;13 February 2024&lt;/p&gt;
    &lt;p&gt;AGU press contact:&lt;lb/&gt; Liza Lester, +1 (202) 777-7494, [email protected] (UTC-5 hours)&lt;/p&gt;
    &lt;p&gt;Contact information for the researchers:&lt;lb/&gt; Kim Novick, Indiana University, [email protected] (UTC-5 hours)&lt;lb/&gt; Mallory Barnes, Indiana University, [email protected] (UTC-5 hours)&lt;/p&gt;
    &lt;p&gt;WASHINGTON — Widespread 20th-century reforestation in the eastern United States helped counter rising temperatures due to climate change, according to new research. The authors highlight the potential of forests as regional climate adaptation tools, which are needed along with a decrease in carbon emissions.&lt;/p&gt;
    &lt;p&gt;“It’s all about figuring out how much forests can cool down our environment and the extent of the effect,” said Mallory Barnes, lead author of the study and an environmental scientist at Indiana University. “This knowledge is key not only for large-scale reforestation projections aimed at climate mitigation, but also for initiatives like urban tree planting.”&lt;/p&gt;
    &lt;p&gt;The study was published in the AGU journal Earth’s Future, which publishes interdisciplinary research on the past, present and future of our planet and its inhabitants.&lt;/p&gt;
    &lt;head rend="h3"&gt;Return of the trees&lt;/head&gt;
    &lt;p&gt;Before European colonization, the eastern United States was almost entirely covered in temperate forests. From the late 18th to early 20th centuries, timber harvests and clearing for agriculture led to forest losses exceeding 90% in some areas. In the 1930s, efforts to revive the forests, coupled with the abandonment and subsequent reforestation of subpar agricultural fields, kicked off an almost century-long comeback for eastern forests. About 15 million hectares of forest have since grown in these areas.&lt;/p&gt;
    &lt;p&gt;“The extent of the deforestation that happened in the eastern United States is remarkable, and the consequences were grave,” said Kim Novick, an environmental scientist at Indiana University and co-author of the new study. “It was a dramatic land cover change, and not that long ago.”&lt;/p&gt;
    &lt;p&gt;During the period of regrowth, global warming was well underway, with temperatures across North America rising 0.7 degrees Celsius (1.23 degrees Fahrenheit) on average. In contrast, from 1900 to 2000, the East Coast and Southeast cooled by about 0.3 degrees Celsius (0.5 degrees Fahrenheit), with the strongest cooling in the southeast.&lt;/p&gt;
    &lt;p&gt;Previous studies suggested the cooling could be caused by aerosols, agricultural activity or increased precipitation, but many of these factors would only explain highly localized cooling. Despite known relationships between forests and cooling, studies had not considered forests as a possible explanation for the anomalous, widespread cooling.&lt;/p&gt;
    &lt;p&gt;“This widespread history of reforestation, a huge shift in land cover, hasn’t been widely studied for how it could’ve contributed to the anomalous lack of warming in the eastern U.S., which climate scientists call a ‘warming hole,’” Barnes said. “That’s why we initially set out to do this work.”&lt;/p&gt;
    &lt;head rend="h3"&gt;Trees are cool&lt;/head&gt;
    &lt;p&gt;Barnes, Novick and their team used a combination of data from satellites and 58 meteorological towers to compare forests to nearby grasslands and croplands, allowing an examination of how changes in forest cover can influence ground surface temperatures and in the few meters of air right above the surface.&lt;/p&gt;
    &lt;p&gt;The researchers found that forests in the eastern U.S. today cool the land’s surface by 1 to 2 degrees Celsius (1.8 to 3.6 degrees Fahrenheit) annually. The strongest cooling effect occurs at midday in the summer, when trees lower temperatures by 2 to 5 degrees Celsius (3.6 to 9 degrees Fahrenheit) — providing relief when it’s needed most.&lt;/p&gt;
    &lt;p&gt;Using data from a network of gas-measuring towers, the team showed that this cooling effect also extends to the air, with forests lowering the near-surface air temperature by up to 1 degree Celsius (1.8 degrees Fahrenheit) during midday. (Previous work on trees’ cooling effect has focused on land, not air, temperatures.)&lt;/p&gt;
    &lt;p&gt;The team then used historic land cover and daily weather data from 398 weather stations to track the relationship between forest cover and land and near-surface air temperatures from 1900 to 2010. They found that by the end of the 20th century, weather stations surrounded by forests were up to 1 degree Celsius (1.8 degrees Fahrenheit) cooler than locations that did not undergo reforestation. Spots up to 300 meters (984 feet) away were also cooled, suggesting the cooling effect of reforestation could have extended even to unforested parts of the landscape.&lt;/p&gt;
    &lt;p&gt;Other factors, such as changes in agricultural irrigation, may have also had a cooling effect on the study region. The reforestation of the eastern United States in the 20th century likely contributed to, but cannot fully explain, the cooling anomaly, the authors said.&lt;/p&gt;
    &lt;p&gt;“It’s exciting to be able to contribute additional information to the long-standing and perplexing question of, ‘Why hasn’t the eastern United States warmed at a rate commensurate with the rest of the world?’” Barnes said. “We can’t explain all of the cooling, but we propose that reforestation is an important part of the story.”&lt;/p&gt;
    &lt;p&gt;Reforestation in the eastern United States is generally regarded as a viable strategy for climate mitigation due to the capacity of these forests to sequester and store carbon. The authors note that their work suggests that eastern United States reforestation also represents an important tool for climate adaptation. However, in different environments, such as snow-covered boreal regions, adding trees could have a warming effect. In some locations, reforestation can also affect precipitation, cloud cover, and other regional scale processes in ways that may or may not be beneficial. Land managers must therefore consider other environmental factors when evaluating the utility of forests as a climate adaptation tool.&lt;/p&gt;
    &lt;p&gt;***&lt;/p&gt;
    &lt;head rend="h3"&gt;Notes for Journalists:&lt;/head&gt;
    &lt;p&gt;This study is published in Earth’s Future, an open-access journal. Neither the paper nor this press release is under embargo. View and download a pdf of the study here.&lt;/p&gt;
    &lt;p&gt;This study is part of an ongoing special collection, “Quantifying Nature-based Climate Solutions,” from AGU’s publications.&lt;/p&gt;
    &lt;head rend="h3"&gt;Paper title:&lt;/head&gt;
    &lt;p&gt;“A Century of Reforestation Reduced Anthropogenic Warming in the Eastern United States”&lt;/p&gt;
    &lt;head rend="h3"&gt;Authors:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mallory L. Barnes (corresponding author), Indiana University, Indiana, USA&lt;/item&gt;
      &lt;item&gt;Kim Novick (corresponding author), Indiana University, Indiana, USA&lt;/item&gt;
      &lt;item&gt;Quan Zhang, Wuhan University, Wuhan, China&lt;/item&gt;
      &lt;item&gt;Scott M. Robeson, Indiana University, Indiana, USA&lt;/item&gt;
      &lt;item&gt;Lily Young, Indiana University, Indiana, USA&lt;/item&gt;
      &lt;item&gt;Elizabeth A. Burakowski, University of New Hampshire, New Hampshire, USA&lt;/item&gt;
      &lt;item&gt;Christopher. Oishi, USDA Forest Service, North Carolina, USA&lt;/item&gt;
      &lt;item&gt;Paul C. Stoy, University of Wisconsin-Madison, USA&lt;/item&gt;
      &lt;item&gt;Gaby Katul, Duke University, North Carolina, USA&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;AGU (www.agu.org) is a global community supporting more than half a million advocates and professionals in Earth and space sciences. Through broad and inclusive partnerships, we advance discovery and solution science that accelerate knowledge and create solutions that are ethical, unbiased and respectful of communities and their values. Our programs include serving as a scholarly publisher, convening virtual and in-person events and providing career support. We live our values in everything we do, such as our net zero energy renovated building in Washington, D.C. and our Ethics and Equity Center, which fosters a diverse and inclusive geoscience community to ensure responsible conduct.&lt;/p&gt;
    &lt;p&gt;#&lt;/p&gt;
    &lt;p&gt;Contributed by Gabriella Lewis&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.agu.org/press-release/a-century-of-reforestation-helped-keep-the-eastern-us-cool/"/><published>2025-10-29T23:17:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45754439</id><title>Raspberry Pi Pico Bit-Bangs 100 Mbit/S Ethernet</title><updated>2025-10-30T07:10:35.287776+00:00</updated><content>&lt;doc fingerprint="4f9925396be232bf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Raspberry Pi RP2040 or RP2350 Bit-Bangs 100 Mbit/s Ethernet&lt;/head&gt;
    &lt;p&gt;on&lt;/p&gt;
    &lt;p&gt;Three years ago, @kingyoPiyo’s Pico-10BASE-T project drew wide attention right here on Elektor for implementing 10 Mbit/s Ethernet on the Raspberry Pi Pico using just a few resistors. In 2023, another milestone followed with bit-banged USB, showing how far the RP2040’s (and now RP2350) programmable I/O could be pushed.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Can an RP2350 Bit-Bang Next?&lt;/head&gt;
    &lt;p&gt;Now, developer Steve Markgraf (GitHub @steve-m) has extended the concept with Pico-100BASE-TX — a 100 Mbit/s Fast Ethernet transmitter running entirely in software.&lt;lb/&gt; Markgraf’s implementation uses the PIO and DMA to perform MLT-3 encoding, 4B5B line coding, and scrambling at a 125 MHz symbol rate. The result is a functioning 100 Mbit/s link capable of streaming about 11 Mbyte/s over UDP, demonstrated by real-time audio and ADC data streams.&lt;/p&gt;
    &lt;p&gt;As before, this is a transmit-only proof of concept and must not be connected to PoE-enabled hardware. A pulse transformer or intermediary Ethernet switch is recommended for isolation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Check Out the Rest of His Repo&lt;/head&gt;
    &lt;p&gt;Example applications in the repository include a counter, internal-ADC streamer, and an audio demo using a PCM1802 converter at 75 kHz. The library supports both the RP2040 and the newer RP2350 (Pico 2) and builds with the standard Pico SDK.&lt;/p&gt;
    &lt;p&gt;Source: Pico-100BASE-TX on GitHub — check it in action in the video there.&lt;/p&gt;
    &lt;p&gt;Beyond the technical achievement, projects like this hint at new possibilities for low-cost, high-speed data acquisition and streaming using microcontrollers that were never designed for it. A Pico capable of pushing 11 MB/s over Ethernet could form the basis of compact, inexpensive test instruments, remote sensors, or experimental network interfaces — all without a dedicated PHY chip. As these bit-banged interfaces become faster and more capable, the question naturally follows: how far can software-defined hardware really go on a two-dollar microcontroller?&lt;/p&gt;
    &lt;p/&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.elektormagazine.com/news/rp2350-bit-bangs-100-mbit-ethernet"/><published>2025-10-29T23:21:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45754509</id><title>Crunchyroll is destroying its subtitles</title><updated>2025-10-30T07:10:34.877735+00:00</updated><content>&lt;doc fingerprint="6e0102d609bb31b5"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Crunchyroll is destroying its subtitles for no good reason&lt;/head&gt;&lt;p&gt;Since the beginning of the Fall 2025 anime season, a major change has started taking place at the anime streaming service Crunchyroll: the presentation quality for translations of on-screen text has taken a total nosedive compared to what has been on offer for many years, all the way up until the previous Summer 2025 season. Now, more and more subtitles on Crunchyroll are looking like this:&lt;/p&gt;&lt;p&gt;Poor presentation quality like this isnât entirely new to Crunchyroll, as a portion of the subtitles on the site have always been of third-party origin â that is, provided by the licensor â and Crunchyroll just puts them up with zero oversight. This in itself has caused numerous issues over the years, but the pressing issue here is that low quality presentation like this can now be found even in first-party subtitles created by Crunchyrollâs own subtitling staff. For comparison, hereâs the kind of presentation quality that first-party subtitles were providing just earlier this year:&lt;/p&gt;&lt;p&gt;Given the technical capabilities on display in the above screenshots, it should be clear that first-party subtitles for Fall 2025 shows shouldnât look as bad as they do. Yet for some reason, what weâre getting is this low quality presentation reminiscent of third-party subtitles, where translations for dialogue and on-screen text arenât even separated to different sides of the screen â everything is just bunched up together at either the top or the bottom. Lots of on-screen text is even left straight up untranslated.&lt;/p&gt;&lt;head rend="h2"&gt;And thatâs âdestroying subtitlesâ?&lt;/head&gt;&lt;p&gt;It sure is when itâs anime weâre talking about! Anime as a medium has made prominent use of on-screen text basically since its inception. The amount of it varies from series to series, but almost every anime out there makes use of on-screen text at one point or another, with some featuring downright ridiculous amounts of signs (what on-screen text is called for short). With all this on-screen text, it is also very common for there to be text visible on the screen potentially in multiple positions, even when characters are speaking.&lt;/p&gt;&lt;p&gt;As such, if you are in the business of localizing anime for non-Japanese audiences, you need to be able to deal with on-screen text. At bare minimum, when subtitling anime, you should be able to do overlaps (multiple lines of text on the screen at the same time) and positioning (the ability to freely place subtitles anywhere on the screen). Anything less and you are likely to run into trouble the moment you get to something as simple as a next episode preview:&lt;/p&gt;&lt;p&gt;Overlaps and positioning are really just the bare necessities for dealing with on-screen text in anime though â ideally, you should also be able to use different fonts, colors, animate text in various ways, etc. Making use of all these possibilities is an art unto itself, and this art of on-screen text localization is commonly referred to as typesetting. Typesetting is important even when dubbing anime, as all that on-screen text is going to be there in the video all the same!&lt;/p&gt;&lt;head rend="h2"&gt;So why would Crunchyroll get rid of typesetting?&lt;/head&gt;&lt;p&gt;That is a good question. It is no exaggeration to say that up to this point, Crunchyroll with its typesetting was the unambiguous market leader when it came to presentation quality for official anime subtitlesâ¦ though for the most part, other services dealing in anime have never even bothered to try. Sentai Filmworksâ Hidive is just about the only other anime service that even attempts to do typesetting, though they license so few shows per season that they are a tiny player compared to the Big Boys of anime streaming.&lt;/p&gt;&lt;p&gt;And it is very likely the existence of these Big Boys that has played a key part in Crunchyrollâs eradication of its typesetting. Netflix and Amazon Prime Video probably need no introduction to anyone reading this â both are very popular general streaming services. Despite anime being only a minor part of their catalogs, a large chunk of todayâs anime watching worldwide happens through said services thanks to their sheer user counts alone.&lt;/p&gt;&lt;p&gt;Crunchyroll clearly seems to know this, which is why it has been sublicensing its anime properties to both Amazon and Netflix for multiple years at this point. But with such sublicensing comes the matter of dealing with the subtitling standards of general streaming services. Iâm not going to mince words: these standards are awful, at least as far as anime is concerned. Netflix for example insists that you stick to at most two lines of text on screen at once, which makes sense most of the timeâ¦ if youâre talking about dialogue alone. Unfortunately, it becomes completely inadequate when dealing with animeâs plentiful on-screen text. Moreover, the standards of these services actively refuse to give you tools like positioning and overlaps, even though the TTML subtitle format they use supports said features!&lt;/p&gt;&lt;p&gt;With such typesetting-hostile standards to deal with, Crunchyroll had basically two choices for how to make sublicensing to Amazon and Netflix work with their existing subtitles that feature actual typesetting: Either 1) try to negotiate with the services for permission to make use of more TTML capabilities (that the subtitle renderers of said services should already support!) or 2) start mangling subtitles with typesetting into something compatible with the awful subtitling standards of the general streaming services. I am not aware if Crunchyroll ever attempted the former, but I can confirm that it eventually started doing the latter.&lt;/p&gt;&lt;p&gt;Editors among Crunchyrollâs subtitling staff were given an additional job to convert finished high quality subtitles with typesetting into limited low quality TTML subtitles without typesetting, compatible with Amazon &amp;amp; Netflix subtitling standards. They got paid extra for the manual effort required by the process.&lt;/p&gt;&lt;p&gt;Unfortunately, after a couple years of this kind of manual conversion work, the Crunchyroll leadership seems to have decided that it isnât enough, and that Crunchyroll must do away with high quality subtitles with typesetting entirely and only produce low quality TTML subtitles without typesetting from now on. But if they already had a working process for high quality subtitles at home and low quality TTML subtitles elsewhere, why would they just decide to give that up in order to produce exclusively low quality subtitles? It doesnât seem to make very much sense, even as a cost-cutting measure. There should be so much value in being able to advertise best viewed on Crunchyroll to potential audiences for long-term growth, right?&lt;/p&gt;&lt;p&gt;To understand how this is happening, we need to look into some relevant history. Specifically, what happened after Sony bought Crunchyroll and merged it with Funimation, another US anime distributor that Sony had bought previously. But in order to also understand why this is happening, first we need to look at what both Crunchyroll and Funimation were like before this fateful merger happened, as well as how they approached anime subtitling over the years.&lt;/p&gt;&lt;head rend="h2"&gt;A short history of Crunchyroll and its subtitling standards&lt;/head&gt;&lt;p&gt;Crunchyroll launched in 2006 as a pirate streaming site focused on East Asian media content, featuring fansubbed anime, live action drama, music videos, and so on. There was nothing particularly remarkable about the site back then â as a rule of thumb, pirate streaming sites are always worse quality-wise than if you just directly downloaded the pirated releases they use as a base, and the sites mostly exist to make their admins illicit money through ads, begging for donations, and other shady crap. It is important to note though that legal anime streaming basically wasnât a thing at this time.&lt;/p&gt;&lt;p&gt;Things started to change in 2008, when the Japanese anime studio Gonzo started experimenting with legal internet distribution for some of its titles. They struck deals with a couple companies for this, which is how Crunchyroll got its first few legitimate licenses. However, all the pirate material remained on the site while this was going on. Also in 2008: Crunchyroll managed to raise 4 million USD in venture capital funding while still operating as a pirate site, which drew vocal criticism from existing anime distributors at the time (for obvious reasons).&lt;/p&gt;&lt;p&gt;However, it was likely this exact venture capital funding that enabled Crunchyroll to negotiate a major deal with the Japanese broadcasting company TV Tokyo, which was announced at the start of 2009. This announcement brought with it the news that Crunchyroll was going full-time legitimate and getting rid of all its pirate content. With this move, Crunchyroll found itself in a position of having to start producing subtitles of its own (instead of just uploading fansubs) and somehow present said subtitles to its customers.&lt;/p&gt;&lt;p&gt;For the subtitle production part, Crunchyroll managed to strike a deal with a bunch of fansubbers to take on the job. This single decision was a fateful one, as it was the foundation for basically everything that came after â with former fansubbers on the job, the tools of the trade were set according to the standards of fansubbers: the subtitling software of choice was to be Aegisub, and the subtitle format of choice was to be Aegisubâs native format, Advanced SubStation Alpha, or ASS for short.&lt;/p&gt;&lt;p&gt;ASS is an extremely powerful format in terms of formatting and styling capabilities, and with Aegisub, it is easy to produce ASS subtitles that make use of said capabilities. However, as a streaming site, Crunchyroll needed to be able to present these ASS subtitles in the browser somehow, and the only full-fledged ASS renderers that existed were only available in the traditional local media playback environments targeted by fansubbers, which meant that Crunchyroll couldnât make use of said renderers on the web directly.&lt;/p&gt;&lt;p&gt;Now, there are two main ways to subtitle videos, with opposing pros and cons:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Hardsubbing â the subtitles are burned into the video itself. Simple to playback as you only need to be able to play video, but inflexible for updates and multiple languages as you have to recreate your video files over and over again with expensive processing called encoding.&lt;/item&gt;&lt;item&gt;Softsubbing â the subtitles exist as their own separate media track that the video player renders on top of the video in realtime during playback, making softsubs complex to playback, but updates and multiple tracks are very cheap as you only need to deal with tiny subtitle files while the video files remain unchanged.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;As such, one way Crunchyroll could have solved the subtitle presentation problem would have been to simply hardsub its ASS subtitles, but despite the challenges it posed, Crunchyroll decided to go with softsubbing instead (which was also the fansub standard at the time). And so Crunchyroll set out to build its own ASS renderer in Flash, the primary technology used to play video on the web at the time. Hereâs a screenshot of some of the first subtitles ever officially authored by the fully legitimate Crunchyroll, rendered in the current ASS renderer but adhering to the limits of the companyâs very first Flash subtitle renderer:&lt;/p&gt;&lt;p&gt;As can be seen, even the very first version was already capable of handling both overlaps and positioning. Now, the positioning was limited to the eight edges and the center of the screen, making for just nine possible positions total, but even that was enough to handle the humble next episode preview at the very least. Beyond these, the first version also supported fading animations. It wasnât much, but it did cover the bare minimum for dealing with on-screen text in anime.&lt;/p&gt;&lt;p&gt;Over the years, Crunchyroll managed to slowly improve its Flash subtitle renderer to enable the use of more ASS features. Custom colors, multiple fonts, multiple styles, rotation, and full positioning were implemented (albeit in somewhat hacky and unwieldy fashion). This went on until 2018, when Crunchyroll was faced with a major issue: Flash was seeing rapid decline in use, and web streaming was shifting over to HTML5-based technology. However, with a custom ASS renderer built in Flash, Crunchyroll couldnât easily make the change, as it would mean having to essentially rebuild the custom subtitle renderer they had from scratch in HTML5 (as much like in the Flash days, there still were no solutions native to the web available for rendering ASS subtitles).&lt;/p&gt;&lt;p&gt;However, Crunchyroll managed to come up with a way to solve the problem of moving from Flash to HTML5 with the help of another new web technology called WebAssembly, which allowed developers to take code that wasnât developed for the web and compile it for use on the web. With WebAssembly, Crunchyroll could take libass, one of the few fully-featured ASS renderers out there, and use it for their new HTML5 player. Now, not only did all their old ASS subtitles render nicely in HTML5, but the possibilities for typesetting at Crunchyroll had taken a huge leap forward. And the subtitling staff at Crunchyroll was more than happy to make use of this newfound power.&lt;/p&gt;&lt;p&gt;That said, despite having a technically fully-featured ASS renderer to work with, there were still limitations. Code compiled with WebAssembly runs worse compared to its original native counterpart, which limits how heavy the typesetting can be (with the flexible features of ASS, it is very easy to produce typesetting that simply cannot be rendered in realtime even on powerful computers, resulting in notable lag during playback). A commercial service like Crunchyroll will also generally want to keep its content watchable even on lower-end devices, which further reduces how complex any typesetting can be.&lt;/p&gt;&lt;p&gt;And this is the limited but functional standard of typesetting that Crunchyroll users got to enjoy (with first-party subtitles) up until the fateful season of Fall 2025 that prompted the creation of this article.&lt;/p&gt;&lt;p&gt;Before we move to the conclusions for this section, though, it is worth noting that while Crunchyroll currently uses softsubbed ASS subtitles whenever it can, there are platforms and devices (like various TVs) where this kind of ASS rendering simply isnât possible to do. Crunchyroll is available on some platforms like this, which means it has been making additional hardsubbed versions of everything on top of the usual softsubbed ones.&lt;/p&gt;&lt;p&gt;So, what can we learn from all this? At least one thing is abundantly clear: for most of its existence, the leadership at Crunchyroll had at least some respect and understanding for anime as a medium. They understood that it was important to be able to deal with on-screen text in their subtitles, and allocated enough resources to make typesetting possible. The company even managed to improve in this regard over time, albeit very slowly.&lt;/p&gt;&lt;p&gt;That said, anyone familiar with anime fansubs of the 2010s and 2020s probably canât help but feel disappointed that even the highest effort typesetting from Crunchyroll could only ever be on the level of fansub releases from around 2010 at best. Why 2010 specifically? Because from 2011 onwards, fansubbers started widely incorporating advanced motion tracking into their typesetting. Observe an example of such fansub typesetting from over a decade ago, the likes of which has never been seen on Crunchyroll:&lt;/p&gt;&lt;p&gt;Now, while fansubbers giving away their work for free might get away with saying &lt;quote&gt;just get a better computer&lt;/quote&gt; to anyone whose devices canât render softsubbed typesetting like this in realtime, an official service that lots of people pay for doesnât really have the same luxury, which is the main reason why you donât see stuff like this softsubbed on Crunchyroll. But this is not an insurmountable problem, so make no mistake: official anime services could absolutely offer typesetting with similar level of quality to the best of fansubs. The basic solution to the performance problem is very simple, even: you simply hardsub the typesetting. This would work from streaming to physical disc releases and only the sky would be the limit in terms of the typesetting quality you could offer, as realtime rendering would no longer be a concern!&lt;/p&gt;&lt;p&gt;Now, as mentioned earlier, hardsubbing does make things more complicated and expensive on the backend as you need to encode and store multiple copies of video. Crunchyroll is already dealing with this, though! But if costs are an issue, the system is pretty easy to improve in theory: if you keep the dialogue softsubbed, only the parts of the video that actually feature typesetting would be hardsubbed, and with some clever engineering and an understanding of how modern media formats work, you would only have to keep multiple copies of the typeset parts. And since the average anime episode has on-screen text only for a small percentage of its total runtime, combining softsubbed dialogue and hardsubbed typesetting like this would make for a highly cost-effective setup.&lt;/p&gt;&lt;p&gt;And since with a mixed system like this you would only have softsubs for the technically simpler dialogue, you could even convert these dialogue-only ASS subtitles to a simpler but more widely supported subtitle format for playback, which theoretically should do away with the need to keep fully hardsubbed copies around entirely, without any real loss in quality! I actually built a minimal version of a mixed system like this myself when I was doing some anime streaming work a few years back and can confidently say that this would be extremely doable for any official anime serviceâ¦ as long as they just cared enough.&lt;/p&gt;&lt;p&gt;Unfortunately, any interest Crunchyroll had for improving their subtitle rendering for typesetting seemed to run out after the 2018 transition to WebAssembly libass. Not that it actually ever seemed to be all that high to begin with, though, as evident by some of the low-hanging fruit that Crunchyroll never bothered to pick in this regard; the most obvious of which would be Crunchyrollâs dogged insistence to restrict typesetting font choices to Core Fonts for the Web. Free for commercial use fonts have been plentily available since the Flash days, and custom fonts have been well supported on the web for a similarly long time.&lt;/p&gt;&lt;p&gt;Anyway, it would have never been all that hard for Crunchyroll to support custom fonts for typesetting, especially after the 2018 move to HTML5. The underlying technology was there and font files are tiny in size compared to the video files being streamed â this would have been an extremely simple and effective improvement for all typesetting efforts. Yet Crunchyroll never reached for this improvement, which is why Comic Sans has kept appearing in Crunchyroll typesetting with depressing regularity.&lt;/p&gt;&lt;p&gt;It is also disappointing how regularly the anime staples of opening &amp;amp; ending songs are still left untranslated on Crunchyroll, though this issue is admittedly much harder to solve than youâd expect. Still, it is possible to do so, especially with Sonyâs resources behind the company today. That goes double when Sony is involved in anime production in any way, as then the songs being used should be well-known to all relevant parties well in advance of airing for timely rights-clearing. So if Crunchyroll/Sony is in any way involved with an animeâs production, it should basically always be possible for songs to be translated the moment the first episode is released.&lt;/p&gt;&lt;p&gt;But thatâs enough about Crunchyrollâs history. Now itâs time to look at the other company mentioned earlier and see how theyâve fared in comparisonâ¦&lt;/p&gt;&lt;head rend="h2"&gt;A short history of Funimation and its subtitling standards&lt;/head&gt;&lt;p&gt;In the early 90s, Japanese-American businessman Gen Fukunaga was approached by his uncle who was working as a producer for Toei. A proposal was made: if Fukunaga could start an anime company in US, Toei would license the rights to the Dragon Ball franchise to it â a franchise that was already making mad cash in Japan. Sensing an opportunity, Fukunaga found investors, and thus in 1994 Funimation was born. A year later, Dragon Ball was on US TV, dubbed and edited to âconform to American sensibilities and tastesâ.&lt;/p&gt;&lt;p&gt;In the early 2000s, fueled by Dragon Ballâs success, Funimation started expanding its business by getting home video distribution rights for 4Kids Entertainment licenses and non-Japanese kidsâ cartoons, the latter eventually expanding into getting involved in production too. But beyond increased investment in kidsâ cartoons, Funimation also started experimenting with more anime licenses of its own, the 2001 anime adaption for Fruits Basket being one of its early standout releases.&lt;/p&gt;&lt;p&gt;Out of these various expansion attempts, âmore animeâ seemed to be the one to work out best, and towards the end of the 00s that became the main direction of Funimationâs business. This move was helped along by a bunch of licenses obtained from now-defunct US anime publishers Geneon USA and ADV. And in the spring of 2009, hot on the heels of Crunchyroll going legit, Funimation announced that they too were getting into the anime streaming business. The resulting anime streams from Funimation were hardsubbed and looked like this:&lt;/p&gt;&lt;p&gt;What you see here is exactly what you got: plain text at top center or bottom center, with dialogue on bottom, and translations for all on-screen text piled up top. So while overlaps were technically supported, full positioning did not seem to be possible, which made things quite awkward the moment there was more than one sign visible on the screen at the same time. This was also the standard you could expect from Funimationâs DVD and Blu-ray releases. And beyond the way too common dialogue three-liners (which are generally terrible for readability), sometimes you even saw four-liners:&lt;/p&gt;&lt;p&gt;The subtitling software that Funimation was using at the time was Telestream MacCaption. In terms of usability and general authoring features, it was no match for Aegisub, although it was actually capable of doing some overlaps, positioning, and styling â Funimation just never chose to make use of these capabilities for its anime subtitles.&lt;/p&gt;&lt;p&gt;This remained the Funimation subtitle standard all the way until 2016, when Funimation struck a deal with Crunchyroll. Going forward, subtitled releases for Funimation licenses would be found on Crunchyroll, while dubbed releases for said titles would be on Funimationâs new streaming platform, FunimationNow.&lt;/p&gt;&lt;p&gt;However, the only thing that really changed is that instead of Funimation content being hardsubbed on their website, it was now softsubbed on Crunchyroll to the exact same standard: plain text on top center or bottom center, often with three or more lines of dialogue at once, even.&lt;/p&gt;&lt;p&gt;Nothing else of particular note happened during this time period when it comes to Funimationâs subtitles. However, it is worth mentioning that Funimation dubs did have simple hardsubbed typesetting sometimes; this only seemed happen at the whim of the dubbing side of Funimation though, as these hardsubbed signs were never present in the subbed versions, nor were they a consistent feature of Funimation dubs in general.&lt;/p&gt;&lt;p&gt;In 2017, Sony purchased Funimation as part of its growing collection of international anime distributors (Sony had previously bought Madman Anime and AnimeLab in Australia and Wakanim in Europe). As a result of this buyout, towards the end of 2018 the license sharing deal between Funimation and Crunchyroll was dissolved and soon after Funimation started serving new subtitled streams on FunimationNow, which were softsubbed and looked like this:&lt;/p&gt;&lt;p&gt;No longer were the subtitles even making use of overlaps. Where dialogue translation used to go on bottom and sign translation on top when both were present, now all text was stuck on the same side of the screen together, either on top or bottom, but never both at the same time anymore.&lt;/p&gt;&lt;p&gt;How this further reduction in subtitling capabilities came about cannot be said for sure, but there are several possible explanations. For one, another major thing that happened at the end of 2018: Funimation signed a big sublicensing deal with the general streaming service Hulu, which meant dealing with Huluâs subtitling standards and authoring accordingly limited subtitles â because as could be expected, the subtitling standards of a general streaming service did not account for the needs of anime in any real way.&lt;/p&gt;&lt;p&gt;Another possible reason for these less-than-great changes in Funimationâs subtitling standards was that around this time the company started using the cloud-based subtitling toolkit OOONA Tools by the localization service provider OOONA. OOONA Tools, by default, do not allow for the creation of subtitles with overlaps. While it can be done in OOONA today by tweaking the options or by using OOONAâs track features (which are quite similar to those of MacCaption, incidentally), it is possible that at the time these features were either not available or that it wasnât possible to correctly export subtitles with overlaps to the WebVTT subtitle format that was being used on FunimationNow.&lt;/p&gt;&lt;p&gt;Regarding that last possibility in particular, there is this OOONA FAQ entry that mentions how &lt;quote&gt;not all formats support [â¦] overlapping subtitles&lt;/quote&gt; and that &lt;quote&gt;Currently, itâs supported in IMSC1.1, ITT and Videotron Lambda CAP exports&lt;/quote&gt;. However, based on my own testing, OOONA Tools can properly export subtitles with overlaps in more formats today than just the ones mentioned here (including WebVTT), meaning that the FAQ entry is in fact outdated â but it was likely true at some point.&lt;/p&gt;&lt;p&gt;In any case, this was the extremely limited standard of subtitling that Funimation customers had to live with until the service was shut down in 2024 as a result of the Funimation-Crunchyroll merger.&lt;/p&gt;&lt;p&gt;Now, what can we conclude from all this? If nothing else, one thing seems abundantly clear: the Funimation leadership never truly cared about or respected anime as a medium. From the very beginning, itâs clear that Gen Fukunaga (a businessman in his 30s at the time) got into the business with the mindset of making money with kidsâ cartoons, and this only became more evident with how Funimation tried to expand into more types of kidsâ cartoons before eventually realizing that anime is where the money was at.&lt;/p&gt;&lt;p&gt;But even with this eventual focus on more anime, no resources seem to have ever been dedicated to make typesetting an actual thing at Funimation, despite how obviously beneficial it would have been for their key product of localized anime. And the way Funimation never even bothered to figure out how to make the most of MacCaption, the expensive enterprise subtitling software they kept using for over a decadeâ¦ while I speculated about possible technical reasons for Funimation abandoning even overlaps when they started producing softsubs for FunimationNow, there was always one possible additional reason: they just didnât care at all. They ran into a problem, no resources were dedicated to fix the problem, and the subtitles got permanently worse as a result.&lt;/p&gt;&lt;p&gt;The whole move to OOONA was questionable in itself, as while OOONA was capable of exporting subtitles to both WebVTT for FunimationNow and TTML (or SRT, a very limited subtitle format) for Hulu in 2018, so was MacCaption. Why start paying for a monthly subscription service when your existing paid-for enterprise software should be able to deal with your needs just fine? I suspect the primary motivation behind the move (which could have even originated from the new parent company Sony) might have been the fact that it was trendy for companies at the time to move everything they possibly could to The Cloudâ¢, regardless of how much sense it actually madeâ¦ but thatâs enough about OOONA for now.&lt;/p&gt;&lt;p&gt;Ultimately, Funimationâs subtitling standards were extremely poor to begin with, and they only managed to make them worse over time. That is something that only utter indifference or outright disdain for anime as a medium could bring about, which seems to have been the exact attitude that Gen Fukunaga cultivated at the executive levels of Funimation â and his followers appear to have carried the torch even after his departure from the company. But more on that in the next section, when we finally get to the Funimation-Crunchyroll merger.&lt;/p&gt;&lt;head rend="h2"&gt;The Funimation-Crunchyroll merger and its consequences&lt;/head&gt;&lt;p&gt;Following Sonyâs 2017 purchase of Funimation, in 2019 Sony bought out Gen Fukunaga from the company entirely, which led to him stepping down as the General Manager, with Colin Decker taking his place. Soon after, Sony formed the Funimation Global Group to consolidate all the international anime publishing services it had bought, with Decker in charge of the joint venture as the CEO. Then, in late 2020, Sony announced that they were going to buy Crunchyroll, placing it under the executive control of the Funimation Global Group. The acquisition was completed in August 2021, coming with a statement from Sony that their goal is to âcreate a unified anime subscription experience as soon as possibleâ.&lt;/p&gt;&lt;p&gt;Then, in March 2022, the news came that Funimation, Crunchyroll, Wakanim, and VRV (Crunchyrollâs more general streaming service) would all be merged together into a single streaming service that would exist under the name of Crunchyroll (as it had the strongest brand of the lot). Funimation Global Group LLC was renamed to Crunchyroll LLC, with Funimation executives remaining in charge. Soon after, Colin Decker stepped down as the CEO, with Rahul Purini (previously COO) taking his place. The merger was complete.&lt;/p&gt;&lt;p&gt;However, as is often the case with mergers &amp;amp; acquisitions, layoffs were on the horizon. In 2023, 85 people were laid off globally in the name of employee redundancy. More layoffs have happened since then, with the most recent one being from just a couple months back in August 2025.&lt;/p&gt;&lt;p&gt;Things werenât much better for those left behind, as laid out in this Bloomberg article from 2024. Staff from Funimation was notably hostile towards those from Crunchyroll:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Tension between the camps arose almost immediately. In a Zoom meeting announcing [Sonyâs purchase of Crunchyroll], Funimation workers accused Crunchyroll of being pirates, alluding to the siteâs history, according to two people who were present.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;While Crunchyroll workers were quickly frustrated with the new executives from Funimation:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Current or former employees describe Crunchyrollâs new managementâprimarily from Funimationâas out-of-touch with employees and the anime fans the company once prioritized. Some executives write off anime as âkidsâ cartoons,â they said, and resist hiring job candidates who describe themselves as fans.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;And while all these internal troubles were going on, Crunchyroll CEO Rahul Purini was excited to talk about how interested he is in AI-generated subtitles.&lt;/p&gt;&lt;head rend="h3"&gt;How typesetting gets destroyed&lt;/head&gt;&lt;p&gt;In 2025, the executives came up with an idea: Crunchyroll should move away from Aegisub and ASS subtitles with typesetting and start producing exclusively limited TTML subtitles without typesetting in OOONA Tools. The likely end goal of this is to get rid of Crunchyrollâs unique ASS-based subtitle rendering entirely in favor of something more âindustry standardâ like TTML-based subtitle rendering. This would mean no longer having to pay staff for manual ASS-to-TTML conversion, as well as being able to drop the relatively expensive fully hardsubbed encodes for limited playback environments where ASS rendering is not possible (but some sort of TTML rendering usually is).&lt;/p&gt;&lt;p&gt;However, a major change affecting all aspects of the companyâs subtitling pipeline doesnât happen overnight, especially considering Crunchyrollâs large back catalog of ASS subtitles with typesetting that couldnât be automatically converted to limited TTML subtitles without typesetting. So while the subtitling staff was to be (begrudgingly) busy experimenting and onboarding with OOONA and doing manual ASS-to-TTML conversions for back catalog titles, technical work would also need to be done to prepare for this vision of a TTML-only future.&lt;/p&gt;&lt;p&gt;For this purpose, Crunchyroll seems to have decided that it would take its existing manual ASS-to-TTML conversions produced by the subtitling staff and treat them as the new master subtitle files. These TTML âmastersâ would then beâfor the time beingâconverted back to ASS with Closed Caption Converter for use with the current ASS-based subtitle rendering. And so, with the start of the Fall 2025 anime season, a plan like this was pushed to production; while regular ASS subtitles were still being produced by Crunchyrollâs subtitling staff, these ASS subtitles with typesetting were generally left unused, while only limited ASS-to-TTML-to-ASS conversions without typesetting were being presented to customers on most shows.&lt;/p&gt;&lt;p&gt;Implementing this interim pipeline with Closed Caption Converter didnât seem to go exactly as planned, though, as some Fall 2025 shows on Crunchyroll ended up having no subtitles at all on release, including the premieres of the latest seasons of hit shows My Hero Academia and Spy Ã Family.&lt;/p&gt;&lt;p&gt;With the internet taking note of all this, on the 9th of October 2025 Crunchyroll responded to a press inquiry by Anime News Network with the following statement:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Over the past few days, some users experienced delays in accessing the content they wanted and subtitle issues across certain series. These were caused by internal system problems â not by any change in how we create subtitles, use of new vendors or AI. Those internal issues have now been fully resolved.&lt;/p&gt;&lt;p&gt;Quality subtitles are a core part of what makes watching anime on Crunchyroll so special. They connect global fans to the heart of every story, and we take that responsibility seriously.&lt;/p&gt;&lt;p&gt;Thank you for your patience. Weâre committed to continuing to deliver the authenticity, quality, and care that fans deserve.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Following this statement, some of the new Fall 2025 shows have had their ASS-to-TTML-to-ASS subtitles switched out to the previously unused regular ASS subtitles. Other shows havenât. And some shows in the Crunchyroll back catalog have been updated with ASS-to-TTML-to-ASS subtitles, though the exact timing of these back catalog updates is unknown.&lt;/p&gt;&lt;p&gt;With all of this, the future of typesetting on Crunchyroll is unclear.&lt;/p&gt;&lt;p&gt;And thatâs how weâve found ourselves in the situation we face today. Remember what the first Crunchyroll subtitles from 2009 looked like? Yeah, these new subtitles adhering to limited TTML standards are even worse than the subtitles from 2009 in terms of how on-screen text can be handled! In other words: The presentation quality of Crunchyrollâs first-party subtitles has reached an all-time low in 2025.&lt;/p&gt;&lt;p&gt;There is only one conclusion that can be drawn from that: the Funimation-turned-Crunchyroll executives still do not have any respect for anime as a medium. In addition, they seem to be treating Crunchyroll and its ways of doing things as the ways of &lt;quote&gt;pirates&lt;/quote&gt; â which isnât entirely incorrect, as Crunchyrollâs use of Aegisub and ASS did originate from the ways of pirate fansubbers. But fansubbers deeply care about anime as medium (they wouldnât be illegally subtitling it for free as a hobby otherwise), which in turn means that the ways fansubbers have developed to subtitle anime are in fact extremely efficient for the job â much better than basically any âindustry standardsâ for subtitling, even.&lt;/p&gt;&lt;p&gt;But that clearly doesnât matter to the executives. The only thing that seems to be on their mind is how to best make money with kidsâ cartoons that none of them personally watch, and what they seem to consider âbestâ is getting rid of everything positively unique about Crunchyroll in favor of doing things the Funimation way, even if that means ditching Aegisub and ASS in favor of OOONA Tools and TTML and getting rid of typesetting in the process. MacCaption-based workflow around for its Blu-ray releases, with notable reduction in typesetting quality on Blu-ray as a result:&lt;/p&gt;conclusion is further supported by the fact that Crunchyroll has kept Funimationâs old&lt;p&gt;Then thereâs the whole plan of moving to OOONA in general, which is even more questionable than it was back in the Funimation days. Crunchyroll has a lot more to lose in terms of subtitle quality than Funimation ever did, yet the executives seem to want to go back to their âold reliableâ regardless. I canât even see it saving them any money in the long run, considering that Aegisub is completely free software while OOONA will incur constant ongoing costs with its per-user subscription pricing. Rather than authoring limited TTML in OOONA directly, paying the subtitling staff to keep the manual ASS to TTML conversions going would likely be cheaper!&lt;/p&gt;&lt;p&gt;Beyond that, there is also the thing about OOONA being an Israeli company. It is certainly a choice, not only in 2018 but most certainly in 2025, to heavily invest in the services of a company from a country that is actively committing genocide. However, to quell some unsubstantiated internet discourse I have seen in relation to this, I do want to emphasize that OOONA being Israeli is not really directly relevant to the quality issues this article is about.&lt;/p&gt;&lt;p&gt;The reason for this lies in enterprise subtitling software (âindustry standardsâ) being universally poor when it comes to producing high quality typesetting for anime, so it wouldnât really matter which software suite a switch was being made to â no matter what, moving away from Aegisub would destroy typesetting as it currently exists on Crunchyroll. And while Crunchyrollâs CEO has expressed his interest in AI subtitles, at least currently there has been no signs of any kind of AI (Israeli or otherwise) being used to create first-party subtitles on Crunchyroll.&lt;/p&gt;&lt;head rend="h3"&gt;Why Crunchyroll is so confident it will get away with this (or: how capitalism ruins everything)&lt;/head&gt;&lt;p&gt;Finally, I want to talk about the possible reasons for Crunchyroll executives feeling so confident about getting away with making their own primary product so much worse. Ultimately, it comes down to the fact that international anime licensing operates primarily on an exclusive licensing model. This means that generally only one service will be able to offer a specific title in specific language(s) in specific region(s), unless the service voluntarily decides to sublicense it out to others. This in turn upends the assumption that the existence of multiple anime services would be beneficial to consumers, as the services donât actually have to engage in competition on customer-beneficial factors like service quality almost at all â instead, they can just focus on hoarding as many exclusive licenses as possible.&lt;/p&gt;&lt;p&gt;This kind of âcompetitionâ twisted by exclusive licensing is more like a casino, where the customers might occasionally be thrown a bone, but at the end of the day, the house always wins. And the anime companies very much prefer to keep it that way, even if it means never being able to offer full coverage of new anime seasons â a limited amount of exclusives is much more important to them. Dreams of infinite growth are what drives the modern-day game of capitalism, and spending money to please customers rather than shareholders goes directly against said dreams. Itâs all about spending as little money as possible to make as much money as possible.&lt;/p&gt;&lt;p&gt;This is why the capitalists in charge of all the big companies these days are so excited about AI too: nothing gets them going more than the idea of not having for pay for those pesky human employees. This is no doubt the actual reason why Crunchyroll CEO Rahul Purini is interested in AI subtitles. It doesnât matter that anime localization costs are a drop in the bucket compared to the overall costs of anime production, even if you were talking about super high quality work with fansub-level typesetting. Any excuse to cut the wages of real human workers is one step closer to the next yacht purchase for the executive upper class.&lt;/p&gt;&lt;p&gt;â¦Whew, got a bit heated there. Anyway, the most likely reason why Crunchyroll executives believe they can get away with reducing the quality of their own service so much? Because Crunchyroll doesnât have any meaningful competition thanks to the primarily-exclusive licensing model used by the international anime industry. Even if they make the service worse, what can you do about it? Cancel your subscription and not watch the new anime youâre excited about?&lt;/p&gt;&lt;head rend="h2"&gt;What you can do about it&lt;/head&gt;&lt;p&gt;If you are currently subscribed to Crunchyroll, cancel your subscription. When asked for a reason, mention the bad subtitle quality and lack of typesetting. You could even link to this article. Beyond that, and this applies to people who arenât subscribed to Crunchyroll as well: spread the word! Share this article around, talk to people about how Crunchyroll is destroying its subtitles, make it so that Crunchyroll executives canât ignore the issue. And the most important thing: Keep it up until Crunchyroll actually makes a clear public commitment to keep typesetting anime.&lt;/p&gt;&lt;p&gt;Why ask for an explicit commitment? Because back in 2017 when Crunchyroll tried to drastically lower its video quality as a cost-cutting measure, vocal user complaints and subscription cancellations forced them to backtrack on it, eventually leading the company to make a statement and not just one but two technical follow-up posts where it explicitly promised to do better, and in the end, video quality actually improved compared to what was previously available. Ideally, the same would happen with Crunchyrollâs typesetting here.&lt;/p&gt;&lt;p&gt;I also want to emphasize that the recent statement Crunchyroll made about its Fall 2025 subtitles isnât really worth anything. Itâs worded in an intentionally obfuscated manner as to what actually has been &lt;quote&gt;fixed&lt;/quote&gt; â is it the lack of typesetting or just the issues with subtitles not going up for new releases? Then it just outright lies about there being &lt;quote&gt;no changes&lt;/quote&gt; with how subtitles are being handled, before ending on empty platitudes about &lt;quote&gt;quality subtitles&lt;/quote&gt; that mean nothing without concrete actions to back them up.&lt;/p&gt;&lt;p&gt;And so far, the actions of Crunchyroll have made the future of typesetting on the service anything but clear. The lower quality subtitles in the back catalog are especially alarming, as the back catalog was exactly where Crunchyroll also started with its 2017 video quality reduction plans, all the while remaining careful with changes to simulcasts where people were paying closer attention â which is exactly what seems to be happening with subtitles on Crunchyroll right now.&lt;/p&gt;&lt;p&gt;To sum things up: Without a clear public commitment to stick to higher subtitling standards that include typesetting, it is very likely that Crunchyroll executives will just delay their typesetting-killing plans and try again later. Thatâs why you need to cancel your subscription, encourage others to do so, and keep talking about this issue until Crunchyroll explicitly promises to do better.&lt;/p&gt;&lt;p&gt;Together, we can save Crunchyroll from itself!&lt;/p&gt;&lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;&lt;p&gt;This article would have never been as thorough and detailed as it is without the assistance of the following people:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The multiple current and former Crunchyroll and Funimation workers who came forward to indepedently confirm the many previously unpublished details found in this article. Huge thanks, all of you.&lt;/item&gt;&lt;item&gt;BigOnAnime â for his great help with researching the historical technical details of Funimationâs subtitling standards. Thank you.&lt;/item&gt;&lt;item&gt;enonibobble â for his help with various screenshots and technical analysis of Crunchyroll subtitles. Thank you.&lt;/item&gt;&lt;item&gt;FayeÂ Duxovni â for bringing Crunchyrollâs use of old Funimation workflows for Blu-rays to my attention and providing the screenshots of it that are used in the article. Thank you.&lt;/item&gt;&lt;item&gt;Ridley, witchymary, Jhiday â for proofreading this article before release. Thanks, all of you.&lt;/item&gt;&lt;item&gt;People on social media who answered public questions I asked or otherwise helped with various small pieces of research. Thanks, all of you.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;External coverage&lt;/head&gt;&lt;p&gt;Iâm not the only one to have made note of Crunchyrollâs recent subtitle shenanigans, so hereâs some additional reading/watching on the subject elsewhere:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Why did Crunchyrollâs subtitles just get worse? by Miles Atherton (former head of marketing for Crunchyroll), on the newsletter Anime By The Numbers. This includes some additional details (like numbers!) that I didnât go over here (because this article was long enough as-is), so I can recommend giving it a read.&lt;/item&gt;&lt;item&gt;The Absolute State of Crunchyroll by YouTuber Motherâs Basement. This is a good watch just to see how bad the new Crunchyroll subtitles look like in action. Additionally, I didnât really talk about how badly timing quality has been affected by the recent changes too, but this video has some good examples of that as well.&lt;/item&gt;&lt;item&gt;Are Subtitles Getting Smaller? by Jerome Mazandarani on Anime News Network. This Answerman column is nominally about subtitles getting visually smaller, but most of it ends up being about the Crunchyroll subtitle situation. Jerome does keep incorrectly saying that general streaming services use the very bare-bones subtitle format SRT rather than TTML, though, and while these services do support SRT for ingestion (ie. content partners can deliver subtitles as SRT) and anime companies might even be making use of that, TTML is what the services actually use internally. SRT does not officially support any kind of positioning whatsoever, which means that even placing subtitles at the top of the screen would be impossible with it if the normal placement was on bottom.&lt;/item&gt;&lt;item&gt;The Crunchyroll Sub Flub by Lucas DeRuyter and Coop Bicknell, also on Anime News Network. Nothing particularly new in this one if youâre familiar with all the other coverage, but itâs nice to see this get discussed on the This Week in Anime column regardless. The more eyes on the subject, the better.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Iâm Daiz, a digital distribution expert and high quality media enthusiast. I have over a decade of experience with Japanese-to-English media localization, including anime subtitling, and I also care deeply about consumer rights. You can follow me on Bluesky, or drop me a mail.&lt;/p&gt;&lt;p&gt;Iâm working on getting Bluesky comments embedded at the end of the posts. For the time being though, you can read and join the discussion here!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://daiz.moe/crunchyroll-is-destroying-its-subtitles-for-no-good-reason/"/><published>2025-10-29T23:31:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45754660</id><title>OS/2 Warp, PowerPC Edition (2011)</title><updated>2025-10-30T07:10:34.411623+00:00</updated><content>&lt;doc fingerprint="f70569160197a9ca"&gt;
  &lt;main&gt;
    &lt;p&gt;The PowerPC adventure—by far the most exotic release of OS/2&lt;/p&gt;
    &lt;p&gt;In December 1995, after unexpectedly long development (but is that really unexpected?), IBM finally “shipped” OS/2 Warp, PowerPC edition. For brevity, this release will be further referred to as OS/2 PPC. Following years of hype and high expectation, the release was very low key and in fact marked the end of development of OS/2 for PowerPC. The product was only available to a limited number of IBM customers and was never actively marketed. OS/2 PPC may not even had a box, although there were nice looking official CDs.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Hardware&lt;/head&gt;
    &lt;p&gt;OS/2 PPC only supported an extremely limited range of hardware—IBM Personal Power Series machines. Those were desktop models 830 and 850, and OS/2 PPC probably also supported the Power Series ThinkPads 820 and 850, though that can be only inferred from the fact that the graphics chipset employed by these ThinkPads was on the very short list of supported devices in OS/2 PPC.&lt;/p&gt;
    &lt;p&gt;The IBM Power Series computers were IBM’s rather short lived foray into the PowerPC-based desktop personal computer market, circa 1995-1996. The PowerPC CPU aside, the systems were very similar to Intel based hardware of that era. They were designed around the PCI bus, but also included ISA expansion slots and on-board Crystal Audio ISA PnP chips. The desktop Power Series machines were IDE based, ThinkPads used SCSI disks. The computers had standard serial and parallel ports, as well as most of typical PC hardware such as interrupt and DMA controllers. The desktops had onboard S3 864 video, ThinkPads used Western Digital flat panel chipsets. Several optional graphics cards were supported, notably Weitek P9100 based accelerators. The desktops also had onboard Ethernet chips (AMD PCnet).&lt;/p&gt;
    &lt;p&gt;The Power Series systems were closely related to certain IBM RS/6000 workstations. The RS/6000 Model 43P-7248 was nearly identical to the Power Series 850. They used the same motherboard, only the RS/6000 had on-board SCSI controller. Unlike the RS/6000 systems intended for the workstation market and running almost exclusively IBM’s AIX operating system, the Power Series systems were designed for “regular” personal computer users. The machines were supposed to run OS/2, Windows NT, AIX, or Solaris. OS/2 PPC was only semi-finished, and the Solaris for PowerPC port (version 2.5.1) was similarly short-lived. Microsoft dropped PowerPC support in 1996, not long after the Windows NT 4.0 release. Most of the Power Series systems ended up running AIX, which supported them until version 5.1. Linux also supported the Power Series to some extent. Windows NT was clearly the closest competitor of OS/2 PPC.&lt;/p&gt;
    &lt;p&gt;For this article, OS/2 PPC was installed on a Power Series 830, installed by its previous owner in a RS/6000 43P case. The CPU was a 100MHz PowerPC 604 with 256KB L2 cache, and the machine was equipped with 192MB RAM, which was the maximum it could handle. The graphics was an on-board PCI S3 Vision 864 with 2MB video memory and true color S3 SDAC. The machine was equipped with 2.1GB IDE hard drive—AIX can handle up to 8GB and Linux can utilize even larger disks, but OS/2 and NT were not happy with anything over about 2.5GB. The 830 was originally sold with either 500MB or 1GB disks and 16MB RAM. The Power Series 850 systems were equipped with 100 or 120MHz CPUs, slightly more RAM and larger disks.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Software&lt;/head&gt;
    &lt;p&gt;OS/2 Warp, PowerPC edition was delivered on two CDs. The first CD contained the operating system and BonusPak, the second CD was an application sampler with several demo applications.&lt;/p&gt;
    &lt;p&gt;Installation was surprisingly easy and painless. The CD was bootable and there were almost no choices to make during installation—only the disk partitioning was user selectable. The PowerPC operating systems (OS/2, NT, AIX and Linux) generally did not coexist as there was no real equivalent of a boot manager and each OS wanted to install its own boot loader. The OS/2 installer re-partitioned the disk and overwrote any other operating systems. The boot partition had to be FAT. It was possible to create HPFS data partitions, but the HPFS support appeared to be somewhat unstable and likely a last-minute addition.&lt;/p&gt;
    &lt;p&gt;After the OS was copied from the installation CD-ROM and the system booted from fixed disk for the first time, the user was greeted by the following screen:&lt;/p&gt;
    &lt;p&gt;Indeed, OS/2 PPC really looked just like OS/2 Warp, at least at first glance. The system booted up in 640×480 mode with 256 colors, using the accelerated S3 driver. The desktop right after installation looked like this:&lt;/p&gt;
    &lt;p&gt;Still very much like OS/2 Warp, except for that little Systems Management folder. This feature was not present in the Intel OS/2 Warp release, although it was added later. After installing the BonusPak and a few other additions and changing the resolution, the desktop still looked like plain OS/2 Warp, with the exception of the background bitmap of course (click on the picture to see full size screenshot):&lt;/p&gt;
    &lt;p&gt;The system was now running in 1024×768 resolution, but still with 256 colors. The graphics chip supports 64K colors at this resolution, unfortunately the software used to take screenshots (a demo version of Impos/2) was unable to take any screenshots at this resolution. 256 colors it is then, and time to more closely examine the operating system. The README file is a good starting point, and it was quite long in OS/2 PPC. It consisted largely of a list of unimplemented or incomplete features.&lt;/p&gt;
    &lt;p&gt;For example, notice the word “Connect” in the screenshot. OS/2 Warp, PowerPC Edition, doesn’t have any connectivity to speak of. Networking support, in a nutshell, didn’t exist. No LAN Server client, no TCP/IP, nothing. There was just HyperAccess Lite and CompuServe Information Manager, which worked (in theory at least) over a modem. The product name itself seems to have been a last minute change. Programs and documentation in many instances refer to OS/2 Warp Connect, PowerPC Edition, but the final product was called just OS/2 Warp and not “Connect”. One of the README files explains the name change and alludes to networking support in “future versions”.&lt;/p&gt;
    &lt;p&gt;For development versions of OS/2 PPC there was TFTP support which talked directly to the microkernel Ethernet or Token Ring driver and entirely bypassed OS/2. This transport layer also supported remote debugging. This is in sharp contrast to Windows NT which fully supported networking (TCP/IP and SMB file sharing) on the same hardware. Networking was obviously planned for OS/2, but the project was killed before this part was done.&lt;/p&gt;
    &lt;p&gt;Not everything was so blatantly unfinished though. The DOS support in OS/2 PPC was a pleasant surprise:&lt;/p&gt;
    &lt;p&gt;On a closer look, it’s clear that OS/2 PPC included a full-fledged PC emulator, which supplied a virtual x86 CPU as well as common PC hardware. Interestingly, the DOS support in OS/2 PPC was based on PC-DOS 7 and not the outdated DOS 5 level code that OS/2 on Intel was stuck with. The OS/2 PPC DOS boxes thus had for instance the DOS E editor (very similar to TEDIT) or REXX support. Why IBM never updated the DOS support on the Intel side is a mystery. OS/2 PPC supported both windowed and full screen DOS sessions. The full screen sessions always ran in graphics mode, even when the emulated DOS application was using text mode.&lt;/p&gt;
    &lt;p&gt;Not satisfied with “just” DOS emulation, IBM also supported Win-OS/2, both full-screen and windowed:&lt;/p&gt;
    &lt;p&gt;It is difficult to judge how stable the DOS and Win-OS/2 emulation really was, but whatever little utilities came with the OS/2 system seemed to work well, including wave audio in Win-OS/2, and the performance was surprisingly good. IBM must have spent a lot of effort on the x86 emulation support. Documentation hinted at a possibility of future support for native OS/2 x86 applications via emulation.&lt;/p&gt;
    &lt;p&gt;IBM also obviously spent a lot of time on the multimedia support in OS/2 PPC. The multimedia support worked unexpectedly well, especially when contrasted with the problems common on Intel machines.&lt;/p&gt;
    &lt;p&gt;The system played video and audio without problems, with MIDI support either via a software synthesizer or an OPL3 compatible chip (the software synthesizer sounded far better). The application sampler CD came with several videos, mostly ads for OS/2. The PowerPC Toolkit also came with a beta version of OpenGL support, which shared code with IBM’s AIX workstation grade implementation.&lt;/p&gt;
    &lt;p&gt;OS/2 PPC was a hybrid halfway between Warp 3 and Warp 4. The user interface looked like Warp 3, but many of the features of OS/2 PPC later showed in Warp 4 on Intel. One of them was the not very popular Feature Installer:&lt;/p&gt;
    &lt;p&gt;The Feature Installer was used to install the BonusPak, several tools and games, and curiously enough, also the Command Reference which for some odd reason wasn’t part of the base install. Here’s one of those games:&lt;/p&gt;
    &lt;p&gt;Again, there is no real difference from the Intel version, except for the about box text (notice the “Connect” text). And finally the IBM Works text editor—again there is no discernible difference from the Intel version:&lt;/p&gt;
    &lt;head rend="h3"&gt;OS/2 for PowerPCs System Overview&lt;/head&gt;
    &lt;p&gt;OS/2 PPC was a strange OS. In many ways it was exactly identical to the Intel version, yet in other ways it was completely different. The user interface was the same and the entire API practically unchanged. Among the differences were the addition of full Unicode support and 32-bit console API (Kbd/Mou/Vio). The largely unchanged API was the reason why it was relatively easy to port existing OS/2 software to PowerPC. The biggest difference was not even the CPU but rather the compiler—IBM used the MetaWare High C/C++ for PowerPC development (it was allegedly cheaper for the IBM OS/2 division to contract MetaWare rather than IBM’s own compiler group). The MetaWare tool set was only used as a cross compiler hosted on x86 OS/2 systems. IBM used MetaWare’s compiler for embedded PowerPC development in general (IBM’s involvement with MetaWare goes at least as far back as AIX for PS/2), and MetaWare also marketed an OS/2 x86 product. Watcom was at the time working on PowerPC version of their compiler, but OS/2 PPC was killed before that project was finished. The last IBM Developer’s Connection release which contained OS/2 PPC material also included a beta version of IBM’s VisualAge C++ compiler. No release of a compiler (or a debugger) running natively on OS/2 PPC is known.&lt;/p&gt;
    &lt;p&gt;The OS/2 PPC development tools were quite different from their Intel counterparts. To begin with, instead of the LX executable format, OS/2 PPC used the industry standard ELF. Several tools were completely unchanged (IPFC for instance), many were entirely new (linker, librarian, resource compiler). The ABI (Application Binary Interface) used in OS/2 PPC was based on the UNIX SVR4 PowerPC ABI. One notable difference was that OS/2 of course ran in little endian mode, unlike PowerPC UNIX ports but just like Windows NT.&lt;/p&gt;
    &lt;p&gt;Delving deeper into the kernel, OS/2 PPC had precious little in common with the Intel version. The product was based on the IBM microkernel, which was a refinement of the Carnegie Mellon University Mach microkernel. The microkernel bore no resemblance to the Intel OS/2 kernel whatsoever and it was also very different from most other operating systems of the time (NeXTSTEP was also based on the Mach microkernel).&lt;/p&gt;
    &lt;p&gt;The initial grandiose plan was to build the Workplace OS, the One Ring to Bind Them All of operating systems. Workplace OS (or WPOS for short) was supposed to be built on top of the Mach microkernel and support multiple “personalities”. The personalities would implement existing operating systems such as OS/2, AIX, Windows NT and perhaps even Mac OS. In the end this never happened and the only supported personality was OS/2. This was somewhat similar to Windows NT where the the non-Windows personalities (environment subsystems) eventually withered away.&lt;/p&gt;
    &lt;p&gt;The initial plan was still tangible in OS/2 PPC. The OS/2 personality was implemented in the “OS/2 Server” and there were certain “personality neutral” services. Most device drivers were personality neutral and worked directly with the microkernel. This included disk and network drivers. A notable exception were the display drivers, where OS/2 PPC introduced the GRADD model (later ported to Intel OS/2). Documentation on OS/2 PPC internals is somewhat sparse and the online books shipped with PowerPC Toolkit were in many cases either incomplete or simply unmodified copies of OS/2 for Intel documentation. A good source of information is the Redbook titled “OS/2 Warp (Power PC Edition) – A First Look” published by IBM International Technical Support Organization in December 1995, document number SG24-4630-00 for those interested.&lt;/p&gt;
    &lt;head rend="h3"&gt;OS/2 for PowerPC Impressions&lt;/head&gt;
    &lt;p&gt;What was OS/2 Warp, PowerPC Edition like? An unfinished product, rough around the edges but simultaneously technically very interesting and advanced and showing promise. Even though the OS/2 PPC release wasn’t called a beta, it was obvious that this was a beta level product (if even that in some respects). Many features were unfinished or completely missing, notably networking. The kernel code didn’t look much like a production build and printed out quite a lot of debugging output on a serial console, if one was attached. The HPFS support was very unstable, and the stability of Win-OS/2 left a lot to be desired. There were too many clearly unfinished parts of the product—documentation, missing utilities, etc.&lt;/p&gt;
    &lt;p&gt;On the other hand a large portion of the system worked well. The user interface and graphics subsystem in general didn’t exhibit any anomalies. Multitasking was reliable and all things considered, responsiveness quite good for a 100MHz CPU and code that was not likely to have been performance tuned. The multimedia subsystem worked much better than expected. Many things were much improved compared to Intel OS/2—internationalization, graphics subsystem, updated console API, and so on. The system seemed to have enough raw power, even if it wasn’t harnessed too well. Boot time was rather long but once up and running, the system was snappy (with some exceptions, notably the CD-ROM driver). To reach true production quality, the OS would have needed at least additional six months of development, perhaps more.&lt;/p&gt;
    &lt;p&gt;How useful was OS/2 PPC? Not very. In fact, it was almost completely useless. It only ran on three or four models of rather rare IBM machines and supported almost no additional devices. The OS was clearly unfinished and not entirely stable. Worst of all, there were about zero applications. Because OS/2 PPC was never truly in use, PowerPC versions of OS/2 applications were never sold, although several OS/2 ISVs ported their applications to OS/2 PPC as evidenced by the application sampler. Porting wasn’t very difficult and tools for building PowerPC applications were available, but since there was no demand for them, there was little point in porting.&lt;/p&gt;
    &lt;p&gt;OS/2 for PowerPC was undoubtedly an interesting experiment, albeit a failed one. It is impossible to tell whether this failure was caused more by shortcomings of OS/2 for PowerPC or the failure—perhaps just falling far short of expectations—of the PowerPC platform as a whole.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Without the generosity of Mike Kaply and Chris Graham, this article could not be written.&lt;/p&gt;
    &lt;p&gt;Some of the above information was derived from IBM documentation and Redbooks, which may have been inaccurate due to the evolving nature of the OS/2 PPC project. Most of the remaining text is the result of observation and conjecture.&lt;/p&gt;
    &lt;p&gt;If you have any additional information, corrections, or interesting stories about OS/2 for PowerPC, please post a comment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.os2museum.com/wp/os2-history/os2-warp-powerpc-edition/"/><published>2025-10-29T23:52:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45755788</id><title>IRCd service (2024)</title><updated>2025-10-30T07:10:33.899825+00:00</updated><content>&lt;doc fingerprint="3516fd1c7c8c88c8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;IRCd service&lt;/head&gt;&lt;head rend="h2"&gt;10.6.2024&lt;/head&gt; Ok. We have IRC at example.fi&lt;lb/&gt;Internet Relay Chat (IRC) is a form of real-time text communication developed by Jarkko Oikarinen in 1988. Initially created to replace a local BBS system at the University of Oulu in Finland, IRC quickly gained global popularity, becoming a foundational technology for online chat communities and influencing the development of modern instant messaging and social media platforms. Its significance lies in its pioneering role in connecting people across the internet, fostering early online communities, and setting the stage for contemporary digital communication.&lt;lb/&gt;To commemorate this pivotal technology, example.fi provides a simple and limited IRC server. This server is uniquely written in AWK, a scripting language traditionally used for text processing, highlighting the adaptability and enduring legacy of IRC. This creative implementation serves as both an educational tool and a tribute to the foundational role of IRC in the evolution of online communication.&lt;lb/&gt;In the following picture you see Irssi in the background and Hexchat on top of it:&lt;lb/&gt;Note: if you plan to connect to example.fi, make sure you do not use any fancy features. In irssi, use -nocap option. In Windows, use for example hexchat.
As this is written in gawk, most IRC protocol features are not implemented. This includes, for example, channel and user listings, topics, the concept of "operator" etc.
Technical fun fact: Total code count is around 60 lines of awk and a few lines of bash.&lt;quote&gt; $ telnet example.fi ircd Trying 65.108.91.190... Connected to example.fi. Escape character is '^]'. USER foo NICK bar :example.fi 001 bar :Welcome to Internet Relay Network bar!~foo@65.108.91.190 :example.fi 375 test :- example.fi Message of the day - :example.fi 372 test :- Current time is @787.188.beats :example.fi 376 test :End of MOTD command. Connection closed by foreign host. &lt;/quote&gt;&lt;lb/&gt;Don't worry, we'll publish the code when it's "ready" :)&lt;p&gt;This site is HTML 2.0 compliant.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://example.fi/blog/ircd.html"/><published>2025-10-30T02:31:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45755821</id><title>Hello-World iOS App in Assembly</title><updated>2025-10-30T07:10:33.539729+00:00</updated><content>&lt;doc fingerprint="ed3ec50e2b1a7f6b"&gt;
  &lt;main&gt;
    &lt;p&gt; Last active &lt;relative-time&gt;October 30, 2025 07:04&lt;/relative-time&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;tool-tip&gt;Save nicolas17/966a03ce49f949dd17b0123415ef2e31 to your computer and use it in GitHub Desktop.&lt;/tool-tip&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; hello-world iOS app &lt;/p&gt;
    &lt;p&gt; This file contains hidden or bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;.global _main&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;.extern _putchar&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;.align 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;_main:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; prolog; save fp,lr,x19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp x29, x30, [sp, #-0x20]!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str x19, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x29, sp&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; make space for 2 dword local vars&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sub sp, sp, #0x10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; save argc/argv&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp x0, x1, [sp]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; create autorelease pool and save into x19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_autoreleasePoolPush&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x19, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; initialize app delegate class&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl initAppDelegate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; create CFString with delegate class name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, 0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x1, str_AppDelegate@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x1, x1, str_AppDelegate@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x2, 0x0600 ; kCFStringEncodingASCII&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _CFStringCreateWithCString&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 = UIApplicationMain(argc, argv, nil, CFSTR("AppDelegate"));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x3, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldr x0, [sp]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldr x1, [sp, #0x8]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x2, #0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _UIApplicationMain&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x7, x0 ; save retval&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; pop autorelease pool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_autoreleasePoolPop&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; epilog&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; restore stack pointer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add sp, sp, 0x10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; restore saved registers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldr x19, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp x29, x30, [sp], #0x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; get retval&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ret&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;initAppDelegate:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; prolog; save fp,lr,x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp x29, x30, [sp, #-0x20]!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str x20, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x29, sp&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; Class c = objc_allocateClassPair(objc_getClass("NSObject"), "AppDelegate", 0);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_NSObject@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_NSObject@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_getClass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x1, str_AppDelegate@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x1, x1, str_AppDelegate@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x2, 0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_allocateClassPair&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; save the class since we'll clobber x0 several times&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x20, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; class_addProtocol(c, objc_getProtocol("UIApplicationDelegate"));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_UIAppDelegate@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_UIAppDelegate@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_getProtocol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _class_addProtocol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; class_addMethod(c, S("application:didFinishLaunchingWithOptions:"), didFinishLaunching, "B@:@@");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_didFinishLaunchingSel@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_didFinishLaunchingSel@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adr x2, didFinishLaunching&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x3, str_typestr@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x3, x3, str_typestr@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _class_addMethod&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; objc_registerClassPair(c);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_registerClassPair&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; epilog&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldr x20, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp x29, x30, [sp], #0x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ret&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; parameters:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0: self&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x1: _sel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x2: application&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x3: launchOptions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;didFinishLaunching:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; prolog, save fp, lr, x19-x22&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp x29, x30, [sp, #-0x30]!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp x19, x20, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp x21, x22, [sp, #0x20]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x29, sp&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sub sp, sp, 0x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x19 = @selector(mainScreen)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_mainScreen@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_mainScreen@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x19, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; objc_getClass("UIScreen")&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_UIScreen@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_UIScreen@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_getClass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x20 = [UIScreen mainScreen]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x20, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x19 is now free&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x1 = @selector(bounds)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_bounds@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_bounds@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; [x20 bounds]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp d0, d1, [sp]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp d2, d3, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x19 = @selector(initWithFrame:)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_initWithFrame@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_initWithFrame@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x19, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 = UIWindow&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_UIWindow@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_UIWindow@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_getClass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 = class_createInstance(x0)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, #0x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _class_createInstance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 now has the instance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x20 = [x0 initWithFrame:d]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x19 ;initWithFrame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp d0, d1, [sp]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp d2, d3, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x20, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x19 = @selector(init)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_init@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_init@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x19, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 = UIViewController&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_UIViewController@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_UIViewController@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_getClass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 = class_createInstance(UIViewController)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, #0x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _class_createInstance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 now has the instance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x21 = [x0 init]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x19 ;init&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x21, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x19 = @selector(yellowColor)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_yellowColor@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_yellowColor@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x19, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x22 = [UIColor yellowColor]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_UIColor@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_UIColor@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_getClass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x22, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x19 = @selector(setBackgroundColor:)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_setBackgroundColor@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_setBackgroundColor@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x19, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x1 = @selector(view)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_view@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_view@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 = [[controller view] setBackgroundColor: x22];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x21&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x2, x22&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_setRoot@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_setRoot@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; [window setRootViewController:viewController]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x2, x21&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; [x20 makeKeyAndVisible]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_makeKeyAndVisible@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_makeKeyAndVisible@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; return YES&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, #0x1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; epilog&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add sp, sp, 0x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp x19, x20, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp x21, x22, [sp, #0x20]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp x29, x30, [sp], #0x30&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ret&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;.data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_NSObject: .asciz "NSObject"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_AppDelegate: .asciz "AppDelegate"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_UIAppDelegate: .asciz "UIApplicationDelegate"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_UIScreen: .asciz "UIScreen"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_UIWindow: .asciz "UIWindow"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_UIViewController: .asciz "UIViewController"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_UIColor: .asciz "UIColor"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_typestr: .asciz "B@:@@"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_didFinishLaunchingSel: .asciz "application:didFinishLaunchingWithOptions:"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_mainScreen: .asciz "mainScreen"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_bounds: .asciz "bounds"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_initWithFrame: .asciz "initWithFrame:"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_makeKeyAndVisible: .asciz "makeKeyAndVisible"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_init: .asciz "init"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_view: .asciz "view"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_setBackgroundColor: .asciz "setBackgroundColor:"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_yellowColor: .asciz "yellowColor"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;str_setRoot: .asciz "setRootViewController:"&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Could you provide build/deploy steps?&lt;/p&gt;
    &lt;p&gt; Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gist.github.com/nicolas17/966a03ce49f949dd17b0123415ef2e31"/><published>2025-10-30T02:37:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45755911</id><title>One year with Next.js App Router and why we're moving on</title><updated>2025-10-30T07:10:33.114687+00:00</updated><content>&lt;doc fingerprint="3209f5526c85179b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;One Year with Next.js App Router — Why We're Moving On&lt;/head&gt;
    &lt;p&gt;A critique of React Server Components and Next.js 15.&lt;/p&gt;
    &lt;p&gt;As I've been using Next.js professionally on my employer's web app, I find the core design of their App Router and React Server Components (RSC) to be extremely frustrating. And it's not small bugs or that the API is confusing, but large disagreements about the fundamental design decisions that Vercel and the React team made when building it.&lt;/p&gt;
    &lt;p&gt;The more webdev events I go to, the more I see people who dislike Next.js, but still get stuck using it. By the end of this article, I will share how me and my colleagues escaped this hell, seamlessly migrating our entire frontend to TanStack Start.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contents:&lt;/head&gt;
    &lt;head rend="h2"&gt;§A Technical Review, What are Server Components?&lt;/head&gt;
    &lt;p&gt;The pitch of RSC is that components are put into two categories, "server" components and "client" components. Server components don't have &lt;code&gt;useState&lt;/code&gt;, &lt;code&gt;useEffect&lt;/code&gt;, but can be
&lt;code&gt;async function&lt;/code&gt;s and refer to backend tools like directly calling into a
database. Client components are the existing
model, where there is code on the backend to generate HTML text and frontend
code to manage the DOM using &lt;code&gt;window.document.*&lt;/code&gt;.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The first disaster: naming!! React is now using the words "server" and "client" to refer to a very specific things, ignoring their existing definitions. This would be fine, except Client components can run on the backend too! In this article, I'll be using the terms "backend" and "frontend" to describe the two execution environments that web apps exist in: a Node.js process and a Web browser, respectively.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This Server/Client component model is interesting. Since built-ins like &lt;code&gt;&amp;lt;Suspense /&amp;gt;&lt;/code&gt; get serialized across the
network, data fetching can be very trivially modeled with async server components, and the fallback UI works as if it were
client-side.&lt;/p&gt;
    &lt;p&gt;If we ignore the 40kB gzipped bundle size of React itself, the above example has zero JavaScript for the UI and data fetching — it just streams the markup! For example, the imagined markdown parser within the &lt;code&gt;&amp;lt;Markdown /&amp;gt;&lt;/code&gt;
component stays on the backend. When an interactive frontend is needed, Client
components can be created by putting them in a file starting with &lt;code&gt;"use client"&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;§Real-world Pitfalls of the App Router&lt;/head&gt;
    &lt;p&gt;After quitting Bun as a runtime engineer (I implemented Server Components bundling and a RSC template there), I joined a small company working on the front lines: a Next.js app with a Hono backend. The following notes are simplifications from the real world problems I've encountered when trying to maintain and develop new features. As a result of all of these, everyone's time is wasted either working around design flaws, or explaining to each other why what should be a non-issue is an immovable object.&lt;/p&gt;
    &lt;head rend="h3"&gt;§Optimistic Updates are Impossible&lt;/head&gt;
    &lt;p&gt;The Next.js documentation for performing mutations does not mention optimistic updates; it appears this case was not thought about. Components rendered by the React Server, by design, can not be modified after mounting. Elements that could change need to be inside a client component, but data fetching cannot happen on the client components, even during SSR on the backend. This results in awkwardly small server components that only do data fetching and then have a client component that contains a mostly-static version of the page.&lt;/p&gt;
    &lt;p&gt;As more of the page needs interactivity, it gets messier trying to keep the static parts truly server-side. On the work app, nearly every piece of UI displays some dynamic data. A &lt;code&gt;WebSocket&lt;/code&gt; synchronizes data live as it
updates (for example, a user card's online state along with their basic
profile). Since these component setups are harder to understand and maintain
for engineers, almost all of our pages are entirely &lt;code&gt;"use client"&lt;/code&gt; with a
&lt;code&gt;page.tsx&lt;/code&gt; that defines the data fetching.&lt;/p&gt;
    &lt;p&gt;A more concrete example of what this looks like in practice with the data-fetching library we use at work, TanStack Query.&lt;/p&gt;
    &lt;p&gt;This example has to be three separate files because of the rules of server component bundling. (The client component needs &lt;code&gt;"use client"&lt;/code&gt;, and server
component files often can't be imported on the client due to server-only
imports.). In the Pages router, this could've been a single file because of the
tree-shaking that &lt;code&gt;getStaticProps&lt;/code&gt; and &lt;code&gt;getServerSideProps&lt;/code&gt; has.&lt;/p&gt;
    &lt;head rend="h3"&gt;§Every Navigation is Another Fetch&lt;/head&gt;
    &lt;p&gt;Since the App Router starts every page as a server component, with (ideally) small areas of interactivity, a navigation to a new page has to fetch the Next.js server, regardless of what data the client already has available! Even with a a &lt;code&gt;loading.tsx&lt;/code&gt; file, opening &lt;code&gt;/&lt;/code&gt;, navigating to &lt;code&gt;/other&lt;/code&gt;, and then
going back to &lt;code&gt;/&lt;/code&gt; will show the loading state while it re-fetches the homepage.&lt;/p&gt;
    &lt;p&gt;The only case this works is for perfectly static content, where instant navigations and prefetching work great. But web apps are not static, they have lots of dynamic content. Being logged in affects the homepage, which is infuriating because the client literally has everything needed to display the page instantly. It's not like the cookies changed.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;aside: In further testing on a blank project, I observe cases where the Next frontend code would pre-fetch routes, but without any real contents. On the hello world example, this was a 1.8kB RSC payload that pointed to 2 different JS chunks 4 separate times. This is just pure waste of our bandwidth and egress, especially considering all of this information is re-fetched when I actually click the link.&lt;/p&gt;
      &lt;p&gt;In review, I found there is actually some content in here: the loading state. Do you see it?&lt;/p&gt;
      &lt;p&gt;It's still a lot of waste, since all of this data gets re-emitted in the actual page RSC.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The solution to this appears to be &lt;code&gt;staleTime&lt;/code&gt;, but it's marked
experimental and "not recommended for production".  The fact this is a
non-default afterthought configuration option is embarassing. Even if we used
it, you cannot make multiple pages that refer to the same underlying data share
any of it.&lt;/p&gt;
    &lt;p&gt;One form of loading state that cannot be represented with the App Router is having a page such as a page like a git project's issue page, and clicking on a user name to navigate to their profile page. With &lt;code&gt;loading.tsx&lt;/code&gt;, the entire
page is a skeleton, but when modeling these queries with TanStack Query it is
possible to show the username and avatar instantly while the user's bio and
repositories are fetched in. Server components don't support this form of
navigation because the data is only available in rendered components, so it
must be re-fetched.&lt;/p&gt;
    &lt;p&gt;In our Next.js site, we have this line of code on our server component data fetchers to make soft navigations faster by skipping the data fetch phase all together.&lt;/p&gt;
    &lt;p&gt;In addition to this, &lt;code&gt;loading.tsx&lt;/code&gt; should contain the &lt;code&gt;useQuery&lt;/code&gt; calls so that
while the network request for the empty RSC happens, the data is being fetched
if it actually is needed. In fact, the &lt;code&gt;loading.tsx&lt;/code&gt; state can just be the
actual client component, and you'll see the client page.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;At work, we just make our&lt;/p&gt;&lt;code&gt;loading.tsx&lt;/code&gt;files contain the&lt;code&gt;useQuery&lt;/code&gt;calls and show a skeleton. This is because when Next.js loads the actual Server Component, no matter what, the entire page re-mounts. No VDOM diffing here, meaning all hooks (&lt;code&gt;useState&lt;/code&gt;) will reset slightly after the request completes. I tried to reproduce a simple case where I was begging Next.js to just update the existing DOM and preserve state, but it just doesn't. Thankfully, the time the blank RSC call takes is short enough.&lt;/quote&gt;
    &lt;head rend="h3"&gt;§Layouts are Artificially Restricted&lt;/head&gt;
    &lt;p&gt;Layouts can perform data fetching, but they can't observe or alter the request in any way. This is done so that Next.js can fetch and cache layouts whenever they want. In every other framework, layouts are just regular components that have no feature difference compared to page components.&lt;/p&gt;
    &lt;p&gt;Fetching layouts in isolation is a cute idea, but it ends up being silly because it also means that any data fetching has to be re-done per layout. You can't share a &lt;code&gt;QueryClient&lt;/code&gt;; instead, you must rely on their monkey-patched
&lt;code&gt;fetch&lt;/code&gt; to cache the same &lt;code&gt;GET&lt;/code&gt; request like they promise.&lt;/p&gt;
    &lt;p&gt;When a coworker asks me about why Next.js rejects some code, I've given up on explaining the technical intricacies and just say "It's a Next.js Skill Issue, I'm going to blow it up soon don't worry." These rules are too hard for normal developers to understand.&lt;/p&gt;
    &lt;head rend="h3"&gt;§You Still Download All the Content Twice&lt;/head&gt;
    &lt;p&gt;Unlike the "Islands Architecture", Server Components still have to be hydrated on the frontend to support &lt;code&gt;Suspense&lt;/code&gt; and preserving client
component state. When doing soft navigations, the "RSC Payload" (which is not
HTML at all) is retrieved by &lt;code&gt;fetch&lt;/code&gt;. On a fresh reload, HTML is needed for the
first paint, but the information about Client components and &lt;code&gt;Suspense&lt;/code&gt; is
not contained within that HTML. React's solution is to send a second copy of
the entire page's markup. An example of what a Next.js production server
would send in a dynamic page render would be something like this:&lt;/p&gt;
    &lt;p&gt;This solution doubles the size of the initial HTML payload. Except it's worse, because the RSC payload includes JSON quoted in JS string literals, which format is much less efficient than HTML. While it seems to compress fine with brotli and render fast in the browser, this is wasteful. With the hydration pattern, at least the data locally could be re-used for interactivity and other pages.&lt;/p&gt;
    &lt;p&gt;Even on pages that have little to no interactivity, you pay the cost. To use the Next.js documentation as an example, loading its homepage loads an page that is around 750kB (250kB of HTML and the 500kB of script tags), and content is in there twice.&lt;/p&gt;
    &lt;p&gt;You can verify that by pressing Cmd + Opt + u on Mac or Ctrl + u on other platforms. And then Cmd / Ctrl + f to locate any string of the blog, such as "building full-stack web applications". It's there twice. And there is no way around this, since it's a fundamental piece of React Server Components.&lt;/p&gt;
    &lt;p&gt;This RSC format certainly has more waste. But I really don't feel like digging into why the string &lt;code&gt;/_next/static/chunks/6192a3719cda7dcc.js&lt;/code&gt; appears 27 separate
times. What the hell, guys? Is your bandwidth free???&lt;/p&gt;
    &lt;head rend="h3"&gt;§Turbopack Sucks&lt;/head&gt;
    &lt;p&gt;This section is not constructive.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Turbopack isn't fast&lt;/item&gt;
      &lt;item&gt;Turbopack emits code that is hard to debug in a debugger (in development mode)&lt;/item&gt;
      &lt;item&gt;Turbopack throws bad error messages in many cases&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I wouldn't have given this point a section in the blog normally, but I want to point out three actual examples directly from the project.&lt;/p&gt;
    &lt;p&gt;The first is a place where during some refactoring to satisfy the Server/Client component models, I accidentally made a Client component &lt;code&gt;async&lt;/code&gt;. This one was
quite anoying because it didn't say at all where the issue was, but only
contained the server stack trace.&lt;/p&gt;
    &lt;p&gt;Another case of a terrible error message:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;After fixing the underlying issue in this second error (which I cannot recall), the Dev server hung and had to be restarted to recover.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The final one is the dozen times I place a debugger breakpoint and the variable name &lt;code&gt;hello&lt;/code&gt; gets turned into
&lt;code&gt;__TURBOPACK__imported__module__$5b$project$5d2f$client$2f$src$2f$utils$2f$filename$2e$ts__$5b$app$2d$client$5d$__$28$ecmascript$29$__["hello"]&lt;/code&gt;
and other bullshit.&lt;/p&gt;
    &lt;p&gt;Okay. This all sucks. What can we do?&lt;/p&gt;
    &lt;head rend="h2"&gt;§Seamlessly Ditching Next.js and Vercel at Work&lt;/head&gt;
    &lt;p&gt;There are two types of web projects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A web site with mostly static content.&lt;/item&gt;
      &lt;item&gt;A web app with majorly dynamic and interactive components.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And Next.js is the wrong tool for both of these jobs. If you're in the first category with a static web site, go for Astro or Fresh. For everyone who needs the full power of React, this section is about how I replaced the vendor locked Next with TanStack Start, incrementally and seamlessly.&lt;/p&gt;
    &lt;p&gt;It started with this Vite config.&lt;/p&gt;
    &lt;p&gt;Then, I looked for every usage of a Next.js API, and either removed it or made a stub for TanStack. For example, &lt;code&gt;src/tanstack-next/link.tsx&lt;/code&gt; implements
&lt;code&gt;next/link&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Some of these stubs can be extremely simple. Starting out, my implementation of&lt;/p&gt;&lt;code&gt;useRouter&lt;/code&gt;was just&lt;code&gt;return {}&lt;/code&gt;, but later I had to add a couple methods to the object. The code here doesn't have to be clean, because it is temporary.&lt;/quote&gt;
    &lt;p&gt;Now, the new site can import nearly every client component by either stubbing out the Next.js APIs it needs, or by using the &lt;code&gt;.tanstack.ts&lt;/code&gt; extension to
re-implement logic on a file-by-file basis. And shortly after, I got the site's
homepage to work in TanStack Start, and we merged the branch.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;This first PR only supported one of our pages, and was able to do it in a thousand lines of added code, and 40 lines deleted. I had previous patches to remove the few uses of&lt;/p&gt;&lt;code&gt;next/image&lt;/code&gt;and&lt;code&gt;next/font&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;What was left was porting every other route over. The one thing we lose in migrating from Next.js to any other framework is the ability to &lt;code&gt;await&lt;/code&gt;
data-fetching functions in the UI. In practice, moving every route into a
&lt;code&gt;loader&lt;/code&gt; function made it much more clear what happened when a page was SSR'd.
For pages that had multiple fetches, these could be combined into a single,
special API call that would return all of the relevant data for that page.&lt;/p&gt;
    &lt;p&gt;To re-iterate in bold font: The migration path from Server Components is to just simplify your code — RSC inherently drives you down a chaotic road of things you do not need. Nearly every complex part of our site got easier to understand for all engineers. The exception to this was having everyone get used to the new file system routing conventions. With enough examples, we all got the hang of it.&lt;/p&gt;
    &lt;p&gt;With the incremental migration in place, new code did not break the existing deployment. TanStack slowly took over the codebase, and we eventually deleted all of the Next.js stubs and gained all of the beautiful type-safety features that the TanStack Router provides. At the end, the site performed faster from every angle: Development Mode, Production page load times, Soft navigations, and at a lower price than our Next depoyment with Vercel.&lt;/p&gt;
    &lt;p&gt;We're not the only ones seeing the change. While I try and keep myself off of social media, someone sent me the results of Brian Anglin's work at Superwall, showing incredible CPU reductions on TanStack Start. I also recall ChatGPT switching from Next.js to Remix (random online chatter: [1] [2] [3]) a year ago.&lt;/p&gt;
    &lt;head rend="h3"&gt;§&lt;code&gt;next/metadata&lt;/code&gt; is Great&lt;/head&gt;
    &lt;p&gt;In my opinion, this is one of the only good APIs Next.js has, and was the one place in our code where moving to TanStack made things harder to do. Instead of worsening the code, I just ported their metadata API into a regular function, so everyone can use it. Originally, I had a 1:1 port on NPM, but earlier this year I simplified it's API into one short and understandable file. As of this blog post, I have added a TanStack-compatible &lt;code&gt;meta.toTags&lt;/code&gt; API, which can be installed from JSR, NPM,
or simply copied into your project.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;notice: Due to time constraints with writing this article, the library has not yet been updated. I'll probably get around to it by the end of this week (Oct 24th). As a placeholder, I'm able to share the version that is used at work to my website:&lt;/p&gt;&lt;code&gt;meta.tanstack.ts&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;My version wasn't concerned with covering the entire space of Next.js's metadata object, but instead uses inline JSX to fill that gap.&lt;/p&gt;
    &lt;head rend="h3"&gt;§&lt;code&gt;next/og&lt;/code&gt; is Good Too&lt;/head&gt;
    &lt;p&gt;No strong opinions. I just want to remind everyone that the &lt;code&gt;@vercel/og&lt;/code&gt; package exists.&lt;/p&gt;
    &lt;head rend="h2"&gt;§My Experience Feels like the Usual&lt;/head&gt;
    &lt;p&gt;At the Next.js Conf 2024, everyone there was raving about Server Components. I forget exactly who I talked to, but the big people were all in on this. I, having implemented the bundler end of RSC, saw a couple of the problems in the format. With Next 15 "stabilizing" the App Router last year, many companies are building their products on it, realizing these pitfalls first-hand.&lt;/p&gt;
    &lt;p&gt;I came into the Next.js game late, only starting in June with version 15. But everyone I've talked to at events sympathize with my notes. All the people I talked to on the subject at Bun's 1.3 Party agreed with me. Even some people at Vercel told me they don't like how Next.js is to actually use.&lt;/p&gt;
    &lt;p&gt;I hope as TanStack Start stabilizes, it becomes the Next.js replacement everyone wants.&lt;/p&gt;
    &lt;head rend="h2"&gt;§Prefer Tools that Respect You&lt;/head&gt;
    &lt;p&gt;A lot of in the JavaScript ecosystem is a mess. That mess is why web development gets made fun of. There were a lot of times I thought that working with the web was an unrecoverable mess, but the mess was actually just the commonly-used libraries I surrounded myself with. When that is peeled back, modern web development technologies are awesome.&lt;/p&gt;
    &lt;p&gt;I've been making this website from scratch without any framework since late 2024, by writing systems like my own TUI progress widget, static file proxy, incremental build system, and many more components. Working on this code has produced some of my best coding sessions (by happiness) in years. The viewers of paper clover get a better quality website; the mini-libraries I create get extracted for public use, everyone wins.&lt;/p&gt;
    &lt;p&gt;This level of from-scratch is too much for most people, especially at the workplace. I say that at the minimum, we should only give our attention and money to high quality tools that respect us. And Next.js and the company behind it, Vercel, are not that.&lt;/p&gt;
    &lt;p&gt;If you use Next.js, and feel that the experience doesn't remind you of respect too, consider whether you and your colleagues want to continue supporting their serverless empire. The Vite ecosystem seems pretty decent to build on right now, but I still have little experience in using their tools at scale in production. The Vite+ launch from Void0 seems interesting, but only time will tell if these venture-funded tools will respect us (end-users and developers) long term.&lt;/p&gt;
    &lt;p&gt;Next.js Conf 2025, as of writing, is tomorrow. Instead of purchasing a $800 ticket, I decided to put that money toward the TanStack team for respecting and improving the web development ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;What the Future Holds&lt;/head&gt;
    &lt;p&gt;Slowly, I've been replacing many pieces of software that disrespect me with better alternatives. Some examples of this are GitHub, Visual Studio Code, DaVinci Resolve, Discord, Google Drive/Workspace, along many more. I plan to write more on this blog about the technical things I do (that progress library, the purpose of my own site generator, learnings from my current job), including some of my past projects at Bun (details on HMR, the crash reporter, and the crazy system for bundling built-in modules). If it interests you, please subscribe to the email list:&lt;/p&gt;
    &lt;p&gt;click here to send an email to &lt;code&gt;subscribe@paperclover.net&lt;/code&gt;, requesting that you would like to be added to the mailing list. (i manage this mailing list manually)&lt;/p&gt;
    &lt;p&gt;back to top — ask a question about this article&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://paperclover.net/blog/webdev/one-year-next-app-router"/><published>2025-10-30T02:48:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45756445</id><title>Carlo Rovelli’s radical perspective on reality</title><updated>2025-10-30T07:10:32.871891+00:00</updated><content>&lt;doc fingerprint="bf1e0b1729858da9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Carlo Rovelli’s Radical Perspective on Reality&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Sitting outside a Catholic church on the French Riviera, Carlo Rovelli jutted his head forward and backward, imitating a pigeon trotting by. Pigeons bob their heads, he told me, not only to stabilize their vision but also to gauge distances to objects — compensating for their limited binocular vision. “It’s all perspectival,” he said.&lt;/p&gt;
    &lt;p&gt;A theoretical physicist affiliated with Aix-Marseille University, Rovelli studies how we perceive reality from our limited vantage point. His research is wide-ranging, running the gamut from quantum information to black holes, and often delves into the history and philosophy of science. In the late 1980s, he helped develop a theory called loop quantum gravity that aims to describe the quantum underpinnings of space and time. A decade later, he proposed a new “relational” interpretation of quantum mechanics, which goes so far as to suggest that there is no objective reality whatsoever, only perspectives on reality — be they a physicist’s or a pigeon’s.&lt;/p&gt;
    &lt;p&gt;More recently, he’s gained recognition as a best-selling author of popular science books, including Seven Brief Lessons on Physics, which has sold more than 2 million copies worldwide — placing him in a limelight he’s still adjusting to. “I’m very bad at being somewhat famous,” he said. “I’m always getting myself in trouble.” (During my visit, he was fending off criticism from the president of the Italian Physical Society, who accused him of defaming Enrico Fermi as a “bloodthirsty fascist/Nazi.”)&lt;/p&gt;
    &lt;p&gt;Rovelli’s own perspective on physics is heavily influenced by his rebellious, countercultural youth. A student protestor in an attempted political revolution in Bologna in 1977, Rovelli worked at a subversive left-wing radio station, drafted an illegal manifesto, and was later detained for refusing compulsory military service. Disillusioned by societal norms, “I had a sense that we were confused about how to think about reality around us,” he said. At 69, he remains politically engaged (and often enraged). “Part of me is still an old hippie.”&lt;/p&gt;
    &lt;p&gt;After the political unrest in Bologna petered out, Rovelli transferred his deep misgivings to the very fabric of reality. He used the same proclivity for challenging traditional ways of thinking to confront long-standing problems in the foundations of physics — not by rejecting established theories, but by embracing a new perspective on them. His approach centers around a radical openness to abandoning intuitions about how the world works.&lt;/p&gt;
    &lt;p&gt;To confront his own biases, whether about physics or society, Rovelli turns to philosophy. He often publishes on metaphysical topics and advocates for more dialogue between the disciplines. His newest book, published this month in Italian, is a deep dive into the intersection of philosophy and physics, a mash-up he sees as the key to understanding what our existing theories are really telling us.&lt;/p&gt;
    &lt;p&gt;Quanta visited Rovelli at his home overlooking the cliffs of Cassis. Over a 12-hour conversation, held while we lounged on his patio, strolled around town, and cruised on his 100-year-old sailboat, we discussed religion, war, consciousness, media, love, pigeons and, of course, physics. The interview has been condensed and edited for clarity.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is your central question, and how did it lead you to study quantum gravity?&lt;/head&gt;
    &lt;p&gt;My central question has always been: How does the world work? We have two main theories that work incredibly well for different domains: general relativity and quantum mechanics. When I learned about these theories in school, I was impressed by how radical they were. They both challenge very foundational conceptions that we have about the world around us — of space as an empty stage where objects exist, and of time as a steady linear flow. They resonated with this idea I had that if you really want to understand reality, you have to be ready to be radical.&lt;/p&gt;
    &lt;p&gt;All attempts to disprove quantum mechanics and general relativity have failed. But nevertheless, in this picture, there’s clearly a crack. There are phenomena out there — like objects falling into a black hole — that fall outside the domain of both theories. When you try to put the two theories together, they appear to result in all sorts of contradictions and paradoxes. To me, the interface of these two theories — the problem of quantum gravity — was really this deep, profound gap in our fundamental physical picture of the world.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tell me about the approach you’ve taken to fill that gap: loop quantum gravity.&lt;/head&gt;
    &lt;p&gt;Loop quantum gravity is a very conservative approach with a very radical consequence. It’s an attempt to say: Let’s take seriously what we’ve learned from general relativity and quantum mechanics all the way through and see where they lead us. There are no extra fields, extra particles, modifications of the Einstein equations, or other hypotheses about nature. It’s just an effort to make coherent what we know so far.&lt;/p&gt;
    &lt;p&gt;Basically, loop quantum gravity implies that space is not infinitely divisible — it’s made of elementary chunks, which are linked together into loops. The theory is a very simple set of equations, but there’s no time variables and no space variables. Those concepts emerge from the way these quanta of gravity interact and transform. What we call space is the quantity of these loops, and what we call time is how the loops evolve continuously.&lt;/p&gt;
    &lt;head rend="h3"&gt;How do we account for our common experience of time if it’s not fundamental?&lt;/head&gt;
    &lt;p&gt;Our experience of time flowing forward is a product of the second law of thermodynamics — the tendency for physical systems to increase in disorder, or what we call entropy. But this only appears fundamental from our perspective. We happen to be beings that are connected to certain macroscopic variables with respect to which entropy is globally moving in one direction.&lt;/p&gt;
    &lt;p&gt;My intuition is that the overall flow of time really could be like the rotation of the sky every day. It’s a majestic, immense phenomenon, but it’s actually an illusion. This is a totally perspectival understanding of the second law of thermodynamics. It’s real in the same sense that the rotating sky is real, but it’s real only with respect to us.&lt;/p&gt;
    &lt;head rend="h3"&gt;One critique of loop quantum gravity is that it contradicts certain predictions of Einstein, namely that the speed of light is constant for all wavelengths. What do you make of this critique?&lt;/head&gt;
    &lt;p&gt;The theory has evolved a lot over the last 20 years, and the current version is not incompatible with Einstein’s predictions — the speed of light is indeed constant at all physical wavelengths. That said, there are some things about loop quantum gravity that still need resolving. We’re not sure how the different versions of the theory are equivalent to one another. We have a problem in which particle scattering seems to generate infinite amounts of low-energy radiation. And solving the equations is still a very complicated task that we’re working to simplify.&lt;/p&gt;
    &lt;p&gt;The main shortcoming is the lack of experiments supporting it. However, there’s hope on the horizon. There are some proposals to use loop quantum gravity to make sense of signatures in the cosmic microwave background radiation that’s left over from the Big Bang. And there’s another new idea I’m very excited about: If loop quantum gravity is right, there should exist tiny black holes weighing around 10 micrograms that are long-living and that interact only gravitationally. We’re thinking about ways to detect a background “wind” of these particles. And perhaps these tiny black holes are actually what we call dark matter, a mysterious widespread astronomical phenomenon that we have not yet understood.&lt;/p&gt;
    &lt;p&gt;Detection will be difficult, but it’s not out of the game. I’m hopeful there will be some experiment that will make the larger community see loop quantum gravity as the natural explanation. It’s far from clear that we cannot account for all of these phenomena using the existing theories that have worked so well for 100 years.&lt;/p&gt;
    &lt;head rend="h3"&gt;If we are to hold on to our existing theories, what picture do they paint about the nature of reality when taken together?&lt;/head&gt;
    &lt;p&gt;Rethinking space and time pushed me to view reality in a completely different way — not as a universe made of objects with defined properties, but as a network of interactions. This is the “relational” interpretation of quantum mechanics. In some sense, it’s a continuation of the trend in modern physics that we have seen with general relativity and quantum mechanics — a strong push toward perspectivalism.&lt;/p&gt;
    &lt;p&gt;We’re used to velocity being relative: The velocity of this table is different with respect to me, with respect to [that pigeon flying] outside, or with respect to the sun. Einstein showed us that time and length are also relative to different observers. Relational quantum mechanics takes this idea a step further. It argues that all properties of an object — its color, location, size, etc. — are in principle only definable in relation to another system. We need to give up the idea that there are material things which we’re describing from the outside. The best way of conceptualizing reality in light of modern science is in terms of the relative information that pieces of nature have about one another.&lt;/p&gt;
    &lt;p&gt;We can only say how the world looks from our limited, biased perspective. This is very radical, because you can no longer say, “This is a list of things in the world, and this is how they are.” We have to live with this lack of total description over reality.&lt;/p&gt;
    &lt;head rend="h3"&gt;There’s something unsettling about this argument. It seems to undermine the ultimate goal of physics to describe the “true” nature of reality, does it not?&lt;/head&gt;
    &lt;p&gt;It very much does, but if you look at the history of science, the ultimate goal has been changing constantly. It went from describing the rotation of heavenly bodies to tracking the forces that guide particles to following the evolution of fields in space-time. I think that the problem of science is to figure out the right conceptual scheme to best understand nature as we see it. The relational perspective is rooted in a deep awareness that our knowledge about the world is fundamentally limited and that everything we see is partial. We have a much stronger and more honest way of approaching reality without being attached to this misleading idea of there being an ultimate truth. We must not confuse the knowledge we have with the reality of the world.&lt;/p&gt;
    &lt;p&gt;If this leaves you with a sense of emptiness about reality, that’s fair. But it’s precisely by knowing that our knowledge is limited that we are able to learn. Between absolute certainty and ignorance there’s all this interesting space in which we live.&lt;/p&gt;
    &lt;head rend="h3"&gt;You’ve written about how your change in worldview has been guided by philosophers. How do you view the relationship between philosophy and physics?&lt;/head&gt;
    &lt;p&gt;The disciplines desperately need one another. A philosopher who doesn’t think about science is not willing to engage with the knowledge we have, and that’s just silly. And a scientist who refuses to look at philosophy is trapped in ways of thinking from which there may be an escape. Historically, the relationship between physicists and philosophers has been very strong. All scientific revolutions have been strongly influenced by philosophical ideas. Copernicus, Galileo and Newton were all philosophers themselves. Einstein very explicitly credited his insights to philosophers like Immanuel Kant, Ernst Mach and others. And Erwin Schrödinger was likely influenced by his reading of the Upanishads, the sacred Hindu texts, when he came up with wave mechanics.&lt;/p&gt;
    &lt;p&gt;But lately, the relationship between physicists and philosophers has been at an all-time low. Stephen Hawking famously pronounced that “philosophy is dead,” and Richard Feynman said things like “Philosophers are as good for science as ornithologists are good for birds.” What they don’t realize is that, first, they are doing philosophy by commenting on what it means to do science; and second, their whole view of science is already under the influence of American pragmatism thinking and philosophers like Karl Popper and Thomas Kuhn. What the physics community took away from these philosophers was that science is about picking new ideas out of thin air, developing a theory, and testing whether it’s right or wrong. This gives the false impression that scientific progress comes only in paradigm-shifting insights that overturn previous thinking, and that all new hypotheses are equally probable until falsified. But science is so much more than that. It’s a continuous process of building on past knowledge to refine our perspective.&lt;/p&gt;
    &lt;p&gt;In my opinion, this closed-mindedness is precisely the problem with modern theoretical physics. We’re undergoing a colossal jump in knowledge that’s forcing us to rethink notions of reality, information, time and space. Our community has wasted a lot of time searching after speculative ideas. What we need instead is to digest the knowledge we already have. And to do that, we need philosophy. Philosophers help us not to find the right answers to given questions, but to find the right questions to better conceptualize reality.&lt;/p&gt;
    &lt;head rend="h3"&gt;In your book Helgoland, you talk about how the Buddhist philosopher Nagarjuna shaped your work. In what way did his texts open your mind?&lt;/head&gt;
    &lt;p&gt;The core idea of relational quantum mechanics is that when we talk about an object — be it an atom, a person or a galaxy — we are never just referring to the system alone. Rather, we are always referring to the interactions between this system and something else. We can only describe — and in fact understand — a thing as it relates to ourselves, or to our measuring devices.&lt;/p&gt;
    &lt;p&gt;Nagarjuna expresses a very similar idea: that no entity has a proper independent existence — things only exist depending on one another. By renouncing “primary” entities or any “ultimate absolute reality,” we can better make sense of the world in terms of how things manifest themselves to other things.&lt;/p&gt;
    &lt;p&gt;Relational quantum mechanics uses similar ideas to make sense of all quantum paradoxes in a precise mathematical way. The main idea is to give up questions about how things really are, in absolute terms. It’s just like how Galileo taught us that asking “Is this object really moving?” is meaningless, and Einstein taught us that asking “Are these two events really simultaneous?” is meaningless. The confusion about quantum mechanics, I believe, is generated by asking questions that have no meaning. The answer to the riddle is that there is no riddle.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/carlo-rovellis-radical-perspective-on-reality-20251029/"/><published>2025-10-30T04:29:45+00:00</published></entry></feed>