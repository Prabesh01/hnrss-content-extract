<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-29T22:39:10.034944+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45739877</id><title>SwirlDB: Modular-first, CRDT-based embedded database</title><updated>2025-10-29T22:39:17.821187+00:00</updated><content>&lt;doc fingerprint="3cc68a7cc19a3bd9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;üåÄ SwirlDB&lt;/head&gt;
    &lt;p&gt;A modular-first, CRDT-based embedded database built from composable adapters&lt;/p&gt;
    &lt;p&gt;Browser and server are equivalent nodes. No primary platform, no privileged environment. Just pure, swappable components.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design Philosophy&lt;/head&gt;
    &lt;head rend="h3"&gt;Everything is an Adapter&lt;/head&gt;
    &lt;p&gt;SwirlDB is not a monolith with configuration options. It's a composition of swappable adapters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Storage Adapters: localStorage, IndexedDB, redb, SQLite, sharded files, or your own&lt;/item&gt;
      &lt;item&gt;Sync Adapters: WebSocket, HTTP, WebRTC, custom protocols&lt;/item&gt;
      &lt;item&gt;Auth Adapters: JWT, OAuth, ABAC, custom policies&lt;/item&gt;
      &lt;item&gt;Encryption Adapters: AES-GCM, field-level, custom crypto&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No feature flags. No conditional compilation. Each adapter is an independent implementation of a shared trait.&lt;/p&gt;
    &lt;head rend="h2"&gt;Core Principles&lt;/head&gt;
    &lt;head rend="h3"&gt;üîå Adapter-First Architecture&lt;/head&gt;
    &lt;p&gt;Every subsystem is a pluggable adapter. Compose your database from the exact components you need. Runtime swappable with zero recompilation.&lt;/p&gt;
    &lt;head rend="h3"&gt;‚öñÔ∏è Equivalent Nodes&lt;/head&gt;
    &lt;p&gt;Browser and server are peers implementing identical traits. Same CRDT engine, same storage interface, just different adapter implementations.&lt;/p&gt;
    &lt;head rend="h3"&gt;üéØ Path-Level Policies&lt;/head&gt;
    &lt;p&gt;Per-path control over storage, sync, and auth. Different data, different rules. Configure at runtime without rebuilding.&lt;/p&gt;
    &lt;head rend="h3"&gt;üß© Shared Implementations&lt;/head&gt;
    &lt;p&gt;Pure Rust core with unified traits. Browser WASM and native servers share the same CRDT logic, differ only in platform adapters.&lt;/p&gt;
    &lt;head rend="h3"&gt;üîÑ CRDT-Based Sync&lt;/head&gt;
    &lt;p&gt;Built on Automerge for conflict-free replicated data. Multi-user by default, with incremental delta sync and merge semantics.&lt;/p&gt;
    &lt;head rend="h3"&gt;‚ö° Natural APIs&lt;/head&gt;
    &lt;p&gt;Native property access in JavaScript via Proxies. Idiomatic Rust APIs. No learning curve, just write code naturally.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick Example&lt;/head&gt;
    &lt;code&gt;import { SwirlDB } from '@swirldb/js';

// Create database with LocalStorage persistence
const db = await SwirlDB.withLocalStorage('my-app');

// Native property access via Proxies
db.data.user.name = 'Alice';
db.data.user.email = 'alice@example.com';

// Get values with .$value
console.log(db.data.user.name.$value); // 'Alice'

// Observe changes
db.data.user.name.$observe((newValue) =&amp;gt; {
  console.log('Name changed:', newValue);
});

// Enable auto-persistence
db.enableAutoPersist(500); // Debounced 500ms
db.data.settings.theme = 'dark'; // Auto-saves&lt;/code&gt;
    &lt;head rend="h2"&gt;Architecture: The 3-Crate System&lt;/head&gt;
    &lt;p&gt;SwirlDB uses complete separation of concerns with no feature flags:&lt;/p&gt;
    &lt;head rend="h3"&gt;swirldb-core&lt;/head&gt;
    &lt;p&gt;Platform-agnostic Rust library&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CRDT engine (Automerge-based)&lt;/item&gt;
      &lt;item&gt;Storage trait definitions (DocumentStorage + ChangeLog)&lt;/item&gt;
      &lt;item&gt;Policy engine (path-based authorization)&lt;/item&gt;
      &lt;item&gt;Auth providers (extensible)&lt;/item&gt;
      &lt;item&gt;Zero platform dependencies&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;swirldb-browser&lt;/head&gt;
    &lt;p&gt;WASM bindings for browsers&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;JavaScript bindings via wasm-bindgen&lt;/item&gt;
      &lt;item&gt;localStorage adapter (5-10MB)&lt;/item&gt;
      &lt;item&gt;IndexedDB adapter (50MB-1GB)&lt;/item&gt;
      &lt;item&gt;Browser-specific storage implementations&lt;/item&gt;
      &lt;item&gt;~830KB total (~330KB gzipped)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;swirldb-server&lt;/head&gt;
    &lt;p&gt;Pure Rust binary for servers&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HTTP/WebSocket sync server (axum + tokio)&lt;/item&gt;
      &lt;item&gt;redb adapter (embedded, persistent)&lt;/item&gt;
      &lt;item&gt;Memory adapter (volatile, fast)&lt;/item&gt;
      &lt;item&gt;No Node.js dependency&lt;/item&gt;
      &lt;item&gt;Native I/O performance&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Unified Traits, Different Implementations&lt;/head&gt;
    &lt;p&gt;Both browser and server implement the same storage traits:&lt;/p&gt;
    &lt;code&gt;trait DocumentStorage {
    async fn save(&amp;amp;self, key: &amp;amp;str, data: &amp;amp;[u8]) -&amp;gt; Result&amp;lt;()&amp;gt;;
    async fn load(&amp;amp;self, key: &amp;amp;str) -&amp;gt; Result&amp;lt;Option&amp;lt;Vec&amp;lt;u8&amp;gt;&amp;gt;&amp;gt;;
    // ...
}

trait ChangeLog {
    async fn append_change(&amp;amp;self, namespace_id: &amp;amp;str, change: Change) -&amp;gt; Result&amp;lt;()&amp;gt;;
    async fn get_changes_since(&amp;amp;self, namespace_id: &amp;amp;str, since: i64) -&amp;gt; Result&amp;lt;Vec&amp;lt;Change&amp;gt;&amp;gt;;
    // ...
}&lt;/code&gt;
    &lt;p&gt;Browser: localStorage + IndexedDB implementations&lt;/p&gt;
    &lt;p&gt;Server: redb + memory implementations&lt;/p&gt;
    &lt;p&gt;Same workflows. Same APIs. Just different backends.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://docs.swirldb.org/"/><published>2025-10-28T22:07:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45740241</id><title>Detour: Dynamic linking on Linux without Libc</title><updated>2025-10-29T22:39:17.148994+00:00</updated><content>&lt;doc fingerprint="ad37bbb1bb29578d"&gt;
  &lt;main&gt;
    &lt;p&gt;On Linux, the traditional divide between statically and dynamically linked executables can feel like a hard wall. Either you bundle everything into your binary, or you accept full dependency on the system's libc and dynamic linker. But Detour, a tiny static library, blows a hole clean through that wall.&lt;/p&gt;
    &lt;p&gt;Detour lets you build statically linked executables, with no dependency on glibc or musl while still giving you access to dynamic linking at runtime. You can &lt;code&gt;dlopen&lt;/code&gt; libraries, resolve symbols, and even mix multiple C runtimes in the
same process, all without ever linking against libc directly.&lt;/p&gt;
    &lt;p&gt;At its core, Detour is a minimal bootstrap layer that gives your application access to the system dynamic linker &lt;code&gt;ld-linux.so&lt;/code&gt; without requiring libc at all. It allows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dynamically loading libraries without linking libc&lt;/item&gt;
      &lt;item&gt;Capturing &lt;code&gt;libdl&lt;/code&gt;functionality (e.g.,&lt;code&gt;dlopen&lt;/code&gt;,&lt;code&gt;dlsym&lt;/code&gt;) inside a fully static executable&lt;/item&gt;
      &lt;item&gt;Mixing different libcs in one process&lt;/item&gt;
      &lt;item&gt;Creating freestanding, zero-libc ELF executables&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All while remaining entirely under your control, with no extra dependencies or runtime overhead.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Detour is not limited to freestanding or static use. You can also use it in dynamically linked applications that use an alternative libc such as musl. Detour works in both static and dynamic contexts.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Detour only works with x86_64 Linux currently. Other architectures can be supported but will require writing assembly for system calls, setjmp/longjmp, and the indirect jump into the ELF entry point. See loader.c.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While fully static linking may sound appealing, it comes with major tradeoffs. When you bundle everything into your binary, you lose access to essential system components that rely on dynamic linking. This includes things like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GPU drivers (e.g., OpenGL, Vulkan ICDs)&lt;/item&gt;
      &lt;item&gt;Window systems (X11, Wayland)&lt;/item&gt;
      &lt;item&gt;Audio subsystems&lt;/item&gt;
      &lt;item&gt;Input libraries&lt;/item&gt;
      &lt;item&gt;PAM modules and NSS services&lt;/item&gt;
      &lt;item&gt;Almost any plugin-based runtime&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These components expect a working dynamic linker environment. If you statically link a libc, you cannot also have a dynamic linker in the same process. That means &lt;code&gt;dlopen&lt;/code&gt; and &lt;code&gt;dlsym&lt;/code&gt; will not work, and neither will anything that depends on them.&lt;/p&gt;
    &lt;p&gt;Detour solves this by letting you statically link your core application while still setting up a dynamic linker for runtime use.&lt;/p&gt;
    &lt;p&gt;To understand Detour, it helps to understand how dynamic executables work under the hood on Linux.&lt;/p&gt;
    &lt;p&gt;When you run a dynamically linked ELF binary, the kernel does not actually execute your binary. Instead, it reads the ELF Program Header Table to find a segment of type &lt;code&gt;PT_INTERP&lt;/code&gt;. This segment specifies the program interpreter to use, typically &lt;code&gt;/lib64/ld-linux-x86-64.so.2&lt;/code&gt;. The kernel then executes that interpreter, passing it:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The full path to your executable&lt;/item&gt;
      &lt;item&gt;All command-line arguments&lt;/item&gt;
      &lt;item&gt;Environment variables&lt;/item&gt;
      &lt;item&gt;Auxiliary vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From there, the dynamic linker takes over. It maps your executable into memory, resolves shared library dependencies, performs relocations, sets up TLS, runs constructors, and finally jumps to libc's initialization which then jumps to your binary's &lt;code&gt;main&lt;/code&gt; function. In effect, the dynamic
linker is the real program, and your application is just a payload it sets up
and transfers control to after initializing everything.&lt;/p&gt;
    &lt;p&gt;Detour leverages this system by pretending to be the OS.&lt;/p&gt;
    &lt;p&gt;It works like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We provide a tiny stub ELF executable that is dynamically linked against the system dynamic linker.&lt;/item&gt;
      &lt;item&gt;Your actual program (which Detour bootstraps) loads this stub ELF using a minimal ELF loader.&lt;/item&gt;
      &lt;item&gt;Detour reads the stub executable's &lt;code&gt;PT_INTERP&lt;/code&gt;segment and loads the specified dynamic linker, just like the kernel would.&lt;/item&gt;
      &lt;item&gt;Before jumping into the dynamic linker, Detour calls &lt;code&gt;setjmp&lt;/code&gt;to capture its current state.&lt;/item&gt;
      &lt;item&gt;It then jumps into the dynamic linker, forwarding the stub ELF and original arguments as if it were the kernel.&lt;/item&gt;
      &lt;item&gt;The dynamic linker maps in and initializes the stub ELF, then calls its &lt;code&gt;main&lt;/code&gt;function. That&lt;code&gt;main&lt;/code&gt;receives a string argument containing a function pointer encoded as a hex string. It decodes the address, casts it to a function pointer, and calls it.&lt;/item&gt;
      &lt;item&gt;This function captures symbols like &lt;code&gt;dlopen&lt;/code&gt;,&lt;code&gt;dlsym&lt;/code&gt;,&lt;code&gt;dlclose&lt;/code&gt;,&lt;code&gt;dlerror&lt;/code&gt;, and then calls&lt;code&gt;longjmp&lt;/code&gt;to return to the original application.&lt;/item&gt;
      &lt;item&gt;Now, back at your main program's entry point, you have full access to the dynamic linker without ever linking against libc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It is a trampoline: a short, carefully orchestrated detour through the dynamic linker, giving you just enough of its guts to carry on without ever depending on it directly.&lt;/p&gt;
    &lt;p&gt;The helper ELF stub used in the first step is extremely small. It's about 35 lines of C. It is dynamically linked, but uses &lt;code&gt;__asm__(".symver")&lt;/code&gt; to
explicitly pin any symbols it calls to the earliest possible version of glibc
that introduced the dynamic linker (around 2002). This ensures maximum forward
compatibility with any glibc-based Linux system in the wild today. Don't believe
me? Look at the code&lt;/p&gt;
    &lt;p&gt;You can ship this stub alongside your application, compile it at runtime on the user's system, or even embed it directly into your binary and extract it to a temporary file at startup. Its only job is to get the dynamic linker to call a known function pointer. Nothing more.&lt;/p&gt;
    &lt;p&gt;Included is a demo that uses Detour to render a flashing colored window using SDL2 and OpenGL. The demo is a fully freestanding static executable that dynamically loads the system's &lt;code&gt;libc&lt;/code&gt;, &lt;code&gt;libm&lt;/code&gt;, &lt;code&gt;libSDL2&lt;/code&gt;, and &lt;code&gt;libGL&lt;/code&gt; at runtime.&lt;/p&gt;
    &lt;p&gt;It is compiled with:&lt;/p&gt;
    &lt;code&gt;-static -nostartfiles -nodefaultlibs -nostdlib -e detour_start&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Note: When using Detour in a freestanding way (such as this demo), the ELF entry point must be&lt;/p&gt;&lt;code&gt;detour_start&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;Despite being entirely statically linked, the executable dynamically loads everything it needs at runtime. This includes: graphics drivers, windowing system libraries, and more without ever linking against glibc or any dynamic libraries at build time. Provided the system has a &lt;code&gt;libSDL2.so&lt;/code&gt; this will work on any Linux install
from 2002 onwards!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create libc-free executables that still load plugins or shared libraries&lt;/item&gt;
      &lt;item&gt;Avoid dependency hell when shipping portable tools across Linux distributions&lt;/item&gt;
      &lt;item&gt;Experiment with new runtimes that bootstrap their own environment&lt;/item&gt;
      &lt;item&gt;Mix musl and glibc in the same process for advanced compatibility or sandboxing&lt;/item&gt;
      &lt;item&gt;Access graphics drivers, window systems, and hardware-accelerated APIs without linking glibc&lt;/item&gt;
      &lt;item&gt;Maintain compatibility with system components that require a functioning &lt;code&gt;PT_INTERP&lt;/code&gt;chain&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Detour does not hide how Linux works, it uses how Linux works. By repurposing the exact same mechanism the OS uses to launch dynamic binaries, it gives static executables a back door into the dynamic linker.&lt;/p&gt;
    &lt;p&gt;Whether you are building minimal tooling, crafting portable binaries, or writing your own runtime, Detour gives you surgical control over how and when the dynamic linker shows up.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/graphitemaster/detour"/><published>2025-10-28T22:42:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45742456</id><title>Board: New game console recognizes physical pieces, with an open SDK</title><updated>2025-10-29T22:39:16.865788+00:00</updated><content>&lt;doc fingerprint="e9cf1933a9c8cf03"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Board game feel.&lt;lb/&gt; Video game magic. &lt;/head&gt;
    &lt;p&gt;The first ever face-to-face game console designed to bring everyone together.&lt;/p&gt;
    &lt;head rend="h2"&gt;Love it to pieces&lt;/head&gt;
    &lt;head rend="h2"&gt;A new way to play, together&lt;/head&gt;
    &lt;head rend="h2"&gt;12 exclusive games included&lt;/head&gt;
    &lt;p&gt;Arcade, strategy, action, and more ‚Äî the world‚Äôs best game studios crafted an exclusive library of games just for Board, designed for every player and every age.&lt;/p&gt;
    &lt;p&gt;Open Board Arcade when the room shouts ‚Äúgame night.‚Äù Grab the space ships and robots, pick teams and jump between four reimagined arcade classics. Ages 6+&lt;/p&gt;
    &lt;p&gt;Tie on your apron grab your knifes, spoons, spice grinder, sponge, and hit the line! It‚Äôs co-op chaos, culinary strategy, and party-night energy‚Äîwithout the messy clean-up. Ages 6+&lt;/p&gt;
    &lt;p&gt;The Bloogs need a hero. Use cannons, stairs, a shapeshifter ring, and blocks to guide adorable Bloogs across tricky terrain collecting crystals and exploring new worlds. Ages 6+&lt;/p&gt;
    &lt;p&gt;Strata is a multi-layered strategy game where Tetris meets Chess. Place 3D blocks to claim territory and outmaneuver opponents in a beautiful contest of spatial skill. Ages 8+&lt;/p&gt;
    &lt;p&gt;From New York to Tokyo to London, use your Spy Kit and your wits to crack puzzles and unravel an international mystery. Ages 8+&lt;/p&gt;
    &lt;p&gt;Pilot your spaceship, blast asteroids, and scoop up minerals in this fast-paced arcade showdown. Ages 6+&lt;/p&gt;
    &lt;p&gt;Meet Mushka, the digital pet who lives inside your Board. With a watering can, brush, blower, wand, and toy bag, you‚Äôll discover all the delightful things Mushka can do. Ages 6+&lt;/p&gt;
    &lt;p&gt;Use your robot to guide your snek. Snag fruit, dodge rivals, and try not to crash into your own tail in this fast-paced frenzy. Ages 6+&lt;/p&gt;
    &lt;p&gt;Compete head-to-head in a sushi-themed duel using chopsticks and sharp strategy to claim victory. Ages 6+&lt;/p&gt;
    &lt;p&gt;Grab your spaceship and battle it out in the Starfire arena in this pinball-style space game. Ages 6+&lt;/p&gt;
    &lt;p&gt;The aliens have landed and they‚Äôre looking for friends! Use your robots to group them into sets in this cosmically competitive match-3 game. Ages 6+&lt;/p&gt;
    &lt;p&gt;Choose one of the gods of Olympus in a head-to-head strategy duel to conquer your opponent‚Äôs city-state. Ages 8+&lt;/p&gt;
    &lt;head rend="h2"&gt;Gather round&lt;/head&gt;
    &lt;p&gt;"It looks like a screen. It plays like a board game, and it just might save family game night."&lt;/p&gt;
    &lt;p&gt;"It's the gift of the season."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://board.fun/"/><published>2025-10-29T03:58:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45745281</id><title>AWS to bare metal two years later: Answering your questions about leaving AWS</title><updated>2025-10-29T22:39:16.557150+00:00</updated><content>&lt;doc fingerprint="ff74527664b5afab"&gt;
  &lt;main&gt;
    &lt;p&gt;When we published How moving from AWS to Bare-Metal saved us $230,000 /yr. in 2023, the story travelled far beyond our usual readership. The discussion threads on Hacker News and Reddit were packed with sharp questions: did we skip Reserved Instances, how do we fail over a single rack, what about the people cost, and when is cloud still the better answer? This follow-up is our long-form reply.&lt;/p&gt;
    &lt;p&gt;Over the last twenty-four months we:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ran the MicroK8s + Ceph stack in production for 730+ days with 99.993% measured availability.&lt;/item&gt;
      &lt;item&gt;Added a second rack in Frankfurt, joined to our primary Paris cage over redundant DWDM, to kill the ‚Äúsingle rack‚Äù concern.&lt;/item&gt;
      &lt;item&gt;Cut average customer-facing latency by 19% thanks to local NVMe and eliminating noisy neighbours.&lt;/item&gt;
      &lt;item&gt;Reinvested the savings into buying bare metal AI servers to expand LLM-based alert / incident summarisation and auto code fixes based on log / traces and metrics in OneUptime.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Below we tackle the recurring themes from the community feedback, complete with the numbers we use internally.&lt;/p&gt;
    &lt;head rend="h2"&gt;$230,000 / yr savings? That is just an engineers salary.&lt;/head&gt;
    &lt;p&gt;In the US, it is. In the rest of the world. That's 2-5x engineers salary. We used to save $230,000 / yr but now the savings have exponentially grown. We now save over $1.2M / yr and we expect this to grow, as we grow as a business.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúWhy not just buy Savings Plans or Reserved Instances?‚Äù&lt;/head&gt;
    &lt;p&gt;We tried. Long answer: the maths still favoured bare metal once we priced everything in. We see a savings of over 76% if you compare our bare metal setup to AWS.&lt;/p&gt;
    &lt;p&gt;A few clarifications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Savings Plans do not reduce S3, egress, or Direct Connect. 37% off instances still leaves you paying list price for bandwidth, which was 22% of our AWS bill.&lt;/item&gt;
      &lt;item&gt;EKS had an extra $1,260/month control-plane fee plus $600/month for NAT gateways. Those costs disappear once you run Kubernetes yourself.&lt;/item&gt;
      &lt;item&gt;Our workload is 24/7 steady. We were already at &amp;gt;90% reservation coverage; there was no idle burst capacity to ‚Äúright size‚Äù away. If we had the kind of bursty compute profile many commenters referenced, the choice would be different.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;‚ÄúHow much did migration and ongoing ops really cost?‚Äù&lt;/head&gt;
    &lt;p&gt;We spent a week of engineers time (and that is the worst case estimate) on the initial migration, spread across SRE, platform, and database owners. Most of that time was work we needed anyway‚Äîformalising infrastructure-as-code, smoke testing charts, tightening backup policies. The incremental work that existed purely because of bare metal was roughly one week.&lt;/p&gt;
    &lt;p&gt;Ongoing run-cost looks like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hands-on keyboard: ~24 engineer-hours/quarter across the entire platform team, including routine patching and firmware updates. That is comparable to the AWS time we used to burn on cost optimisation, IAM policy churn, and chasing deprecations and updating our VM's on AWS.&lt;/item&gt;
      &lt;item&gt;Remote hands: 2 interventions in 24 months (mainly disks). Mean response time: 27 minutes. We do not staff an on-site team. We rely on co-location provider to physically manage our rack. This means no traditional hardware admins.&lt;/item&gt;
      &lt;item&gt;Automation: We're now moving to Talos. We PXE boot with Tinkerbell, image with Talos, manage configs through Flux and Terraform, and run conformance suites before each Kubernetes upgrade. All of those tools also hardened our AWS estate, so they were not net-new effort.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The opportunity cost question from is fair. We track it the same way we track feature velocity: did the infra team ship less? The answer was ‚Äúno‚Äù‚Äîour release cadence increased because we reclaimed few hours/month we used to spend in AWS ‚Äúcost council‚Äù meetings.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúIsn‚Äôt a single rack a single point of failure?‚Äù&lt;/head&gt;
    &lt;p&gt;We have multiple racks across two different DC / providers. We:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Leased a secondary quarter rack in Frankfurt with a different provider and power utility.&lt;/item&gt;
      &lt;item&gt;Currently: Deployed a second MicroK8s control plane, mirrored Ceph pools with asynchronous replication. Future: We're moving to Talos. Nothing against Microk8s, but we like the Talos way of managing the k8s cluster.&lt;/item&gt;
      &lt;item&gt;Added isolated out-of-band management paths (4G / satellite) so we can reach the gear even during metro fibre events.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The AWS failover cluster we mentioned in 2023 still exists. We rehearse a full cutover quarterly using the same Helm releases we ship to customers. DNS failover remains the slowest leg (resolver caches can ignore TTL), so we added Anycast ingress via BGP with our transit provider to cut traffic shifting to sub-minute.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúWhat about hardware lifecycle and surprise CapEx?‚Äù&lt;/head&gt;
    &lt;p&gt;We amortise servers over five years, but we sized them with 2 √ó AMD EPYC 9654 CPUs, 1 TB RAM, and NVMe sleds. At our current growth rate the boxes will hit CPU saturation before we hit year five. When that happens, the plan is to cascade the older gear into our regional analytics cluster (we use Posthog + Metabase for this) and buy a new batch. Thanks to the savings delta, we can refresh 40% of the fleet every 24 months and still spend less annually than the optimised AWS bill above.&lt;/p&gt;
    &lt;p&gt;We also buy extended warranties from the OEM (Supermicro) and keep three cold spares in the cage. The hardware lasts 7-8 years and not 5, but we wtill count it as 5 to be very conservative.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúAre you reinventing managed services?‚Äù&lt;/head&gt;
    &lt;p&gt;Another strong Reddit critique: why rebuild services AWS already offers? Three reasons we are comfortable with the trade:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Portability is part of our product promise. OneUptime customers self-host in their own environments. Running the same open stack we ship (Postgres, Redis, ClickHouse, etc.) keeps us honest. We eun on Kubernetes and self-hosted customers run on Kubernetes as well.&lt;/item&gt;
      &lt;item&gt;Tooling maturity. Two years ago we relied on Terraform + EKS + RDS. Today we run MicroK8s (Talos in the future), Argo Rollouts, OpenTelemetry Collector, and Ceph dashboards. None of that is bespoke. We do not maintain a fork of anything.&lt;/item&gt;
      &lt;item&gt;Selective cloud use. We still pay AWS for Glacier backups, CloudFront for edge caching, and short-lived burst capacity for load tests. Cloud makes sense when elasticity matters; bare metal wins when baseload dominates.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Managed services are phenomenal when you are short on expertise or need features beyond commodity compute. If we were all-in on DynamoDB streams or Step Functions we would almost certainly still be on AWS.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúHow do bandwidth and DoS scenarios work now?‚Äù&lt;/head&gt;
    &lt;p&gt;We committed to 5 Gbps 95th percentile across two carriers. The same traffic on AWS egress would be 8x expensive in eu-west-1. For DDoS protection we front our ingress with Cloudflare.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúHas reliability suffered?‚Äù&lt;/head&gt;
    &lt;p&gt;Short answer: No. Infact it was better than AWS (compared to recent AWS downtimes)&lt;/p&gt;
    &lt;p&gt;We have 730+ days with 99.993% measured availability and we also escaped AWS region wide downtime that happened a week ago.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúHow do audits and compliance work off-cloud now?‚Äù&lt;/head&gt;
    &lt;p&gt;We stayed SOC 2 Type II and ISO 27001 certified through the transition. The biggest deltas auditors cared about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Physical controls: We provide badge logs from the colo, camera footage on request, and quarterly access reviews. The colo already meets Tier III redundancy, so their reports roll into ours.&lt;/item&gt;
      &lt;item&gt;Change management: Terraform plans, and now Talos machine configs give us immutable evidence of change. Auditors liked that more than AWS Console screenshots.&lt;/item&gt;
      &lt;item&gt;Business continuity: We prove failover by moving workload to other DC.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are in a regulated space (HIPAA for instance), expect the paperwork to grow a little. We worked it in by leaning on the colo providers‚Äô standard compliance packets‚Äîthey slotted straight into our risk register.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúWhy not stay in the cloud but switch providers?‚Äù&lt;/head&gt;
    &lt;p&gt;We priced Hetzner, OVH, Leaseweb, Equinix Metal, and AWS Outposts. The short version:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hyperscaler alternatives were cheaper on compute but still expensive on egress once you hit petabytes/month. Outposts also carried minimum commits that exceeded our needs.&lt;/item&gt;
      &lt;item&gt;European dedicated hosts (Hetzner, OVH) are fantastic for lab clusters. The challenge was multi-100 TB Ceph clusters with redundant uplinks and smart-hands SLAs. Once we priced that tier, the savings narrowed.&lt;/item&gt;
      &lt;item&gt;Equinix Metal got the closest, but bare metal on-demand still carried a 25-30% premium over our CapEx plan. Their global footprint is tempting; we may still use them for short-lived expansion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Owning the hardware also let us plan power density (we run 15 kW racks) and reuse components. For our steady-state footprint, colocation won by a long shot.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúWhat does day-to-day toil look like now?‚Äù&lt;/head&gt;
    &lt;p&gt;We put real numbers to it because Reddit kept us honest:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Weekly: Kernel and firmware patches (Talos makes this a redeploy), Ceph health checks, Total time averages 1 hour/week on average over months.&lt;/item&gt;
      &lt;item&gt;Monthly: Kubernetes control plane upgrades in canary fashion. About 2 engineer-hours. We expect this to reduce when Talos kicks in.&lt;/item&gt;
      &lt;item&gt;Quarterly: Disaster recovery drills, capacity planning, and contract audits with carriers. Roughly 12 hours across three engineers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total toil is ~14 engineer-hours/month, including prep. The AWS era had us spending similar time but on different work: chasing cost anomalies, expanding Security Hub exceptions, and mapping breaking changes in managed services. The toil moved; it did not multiply.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúDo you still use the cloud for anything substantial?‚Äù&lt;/head&gt;
    &lt;p&gt;Absolutely. Cloud still solves problems we would rather not own:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Glacier keeps long-term log archives at a price point local object storage cannot match.&lt;/item&gt;
      &lt;item&gt;CloudFront handles 14 edge PoPs we do not want to build. We terminate TLS at the edge for marketing assets and docs. We will soon move this to Cloudflare as they are cheaper.&lt;/item&gt;
      &lt;item&gt;We spin up short-lived AWS environments for load testing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So yes, we left AWS for the base workload, but we still swipe the corporate card when elasticity or geography outweighs fixed-cost savings.&lt;/p&gt;
    &lt;head rend="h2"&gt;When the cloud is still the right answer&lt;/head&gt;
    &lt;p&gt;It depends on your workload. We still recommend staying put if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your usage pattern is spiky or seasonal and you can auto-scale to near zero between peaks.&lt;/item&gt;
      &lt;item&gt;You lean heavily on managed services (Aurora Serverless, Kinesis, Step Functions) where the operational load is the value prop.&lt;/item&gt;
      &lt;item&gt;You do not have the appetite to build a platform team comfortable with Kubernetes, Ceph, observability, and incident response.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cloud-first was the right call for our first five years. Bare metal became the right call once our compute footprint, data gravity, and independence requirements stabilised.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is next&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We are working on a detailed runbook + Terraform module to help teams do capex forecasting for colo moves. Expect that on the blog later this year.&lt;/item&gt;
      &lt;item&gt;A deep dive on Talos is in the queue, as requested by multiple folks in the HN thread.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Questions we did not cover? Let us know in the discussion threads‚Äîwe are happy to keep sharing the gritty details.&lt;/p&gt;
    &lt;p&gt;Related Reading:&lt;/p&gt;
    &lt;head rend="h3"&gt;Neel Patel&lt;/head&gt;
    &lt;p&gt;@devneelpatel ‚Ä¢ Oct 29, 2025 ‚Ä¢&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://oneuptime.com/blog/post/2025-10-29-aws-to-bare-metal-two-years-later/view"/><published>2025-10-29T11:14:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747018</id><title>Kafka is Fast ‚Äì I'll use Postgres</title><updated>2025-10-29T22:39:16.291612+00:00</updated><content>&lt;doc fingerprint="6f5090de6e0009c6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Intro&lt;/head&gt;
    &lt;p&gt;I feel like the tech world lives in two camps.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;One camp chases buzzwords.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This camp tends to adopt whatever‚Äôs popular without thinking hard about whether it‚Äôs appropriate. They tend to fall for all the purported benefits the sales pitch gives them - real-time, infinitely scale, cutting-edge, cloud-native, serverless, zero-trust, AI-powered, etc.&lt;/p&gt;
    &lt;p&gt;You see this everywhere in the Kafka world: Streaming Lakehouse‚Ñ¢Ô∏è, Kappa‚Ñ¢Ô∏è Architecture, Streaming AI Agents1.&lt;/p&gt;
    &lt;p&gt;This phenomenon is sometimes known as resume-driven design. Modern practices actively encourage this. Consultants push ‚Äúinnovative architectures‚Äù stuffed with vendor tech via ‚Äúinsight‚Äù reports2. System design interviews expect you to design Google-scale architectures that are inevitably at a scale 100x higher than the company you‚Äôre interviewing for would ever need. Career progression rewards you for replatforming to the Hot New Stack‚Ñ¢Ô∏è, not for being resourceful.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The other camp chases common sense&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This camp is far more pragmatic. They strip away unnecessary complexity and steer clear of overengineered solutions. They reason from first principles before making technology choices. They resist marketing hype and approach vendor claims with healthy skepticism.&lt;/p&gt;
    &lt;p&gt;Historically, it has felt like Camp 1 definitively held the upper hand in sheer numbers and noise. Today, it feels like the pendulum may be beginning to swing back, at least a tiny bit. Two recent trends are on the side of Camp 2:&lt;/p&gt;
    &lt;p&gt;Trend 1 - the ‚ÄúSmall Data‚Äù movement. People are realizing two things - their data isn‚Äôt that big and their computers are becoming big too. You can rent a 128-core, 4 TB of RAM instance from AWS. AMD just released 192-core CPUs this summer. That ought to be enough for anybody.3&lt;/p&gt;
    &lt;p&gt;Trend 2 - the Postgres Renaissance. The space is seeing incredible growth and investment4. In the last 2 years, the phrase ‚ÄúJust Use Postgres (for everything)‚Äù has gained a ton of popularity. The basic premise is that you shouldn‚Äôt complicate things with new tech when you don‚Äôt need to, and that Postgres alone solves most problems pretty well. Postgres competes with purpose-built solutions like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Elasticsearch (functionality supported by Postgres‚Äô &lt;code&gt;tsvector&lt;/code&gt;/&lt;code&gt;tsquery&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;MongoDB (&lt;code&gt;jsonb&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Redis (&lt;code&gt;CREATE UNLOGGED TABLE&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;AI Vector Databases (&lt;code&gt;pgvector&lt;/code&gt;,&lt;code&gt;pgai&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Snowflake (&lt;code&gt;pg_mooncake&lt;/code&gt;,&lt;code&gt;pg_duckdb&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and‚Ä¶ Kafka (this blog).&lt;/p&gt;
    &lt;p&gt;The claim isn‚Äôt that Postgres is functionally equivalent to any of these specialized systems. The claim is that it handles 80%+ of their use cases with 20% of the development effort. (Pareto Principle)&lt;/p&gt;
    &lt;p&gt;When you combine the two trends, the appeal becomes obvious. Postgres is a battle-tested, well-known system that is simple, scalable and reliable. Pair it with today‚Äôs powerful hardware and you quickly begin to realize that, more often than not, you do not need the state-of-the-art highly optimized and complex distributed system in order to handle your organization‚Äôs scale.&lt;/p&gt;
    &lt;p&gt;Despite being somebody who is biased towards Kafka, I tend to agree. Kafka is similar to Postgres in that it‚Äôs stable, mature, battle-tested and boasts a strong community. It also scales a lot further. Despite that, I don‚Äôt think it‚Äôs the right choice for a lot of cases. Very often I see it get adopted where it doesn‚Äôt make sense.&lt;/p&gt;
    &lt;p&gt;A 500 KB/s workload should not use Kafka. There is a scalability cargo cult in tech that always wants to choose ‚Äúthe best possible‚Äù tech for a problem - but this misses the forest for the trees. The ‚Äúbest possible‚Äù solution frequently isn‚Äôt a technical question - it‚Äôs a practical one. Adriano makes an airtight case for why you should opt for simple tech in his PG as Queue blog (2023) that originally inspired me to write this.&lt;/p&gt;
    &lt;p&gt;Enough background. In this article, we will do three simple things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Benchmark how far Postgres can scale for pub/sub messaging - # PG as a Pub/Sub&lt;/item&gt;
      &lt;item&gt;Benchmark how far Postgres can scale for queueing - # PG as a Queue&lt;/item&gt;
      &lt;item&gt;Concisely touch upon when Postgres can be a fit for these use cases - # Should You Use Postgres?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I am not aiming for an exhaustive in-depth evaluation. Benchmarks are messy af. Rather, my goal is to publish some reasonable data points which can start a discussion.&lt;/p&gt;
    &lt;p&gt;(while this article is for Postgres, feel free to replace it with your database of choice)&lt;/p&gt;
    &lt;head rend="h1"&gt;Results TL;DR&lt;/head&gt;
    &lt;p&gt;If you‚Äôd like to skip straight to the results, here they are:&lt;/p&gt;
    &lt;head&gt;üî• The Benchmark Results&lt;/head&gt;
    &lt;head rend="h3"&gt;Pub-Sub Results&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Setup&lt;/cell&gt;
        &lt;cell role="head"&gt;‚úçÔ∏è Write&lt;/cell&gt;
        &lt;cell role="head"&gt;üìñ Read&lt;/cell&gt;
        &lt;cell role="head"&gt;üî≠ e2e Latency5 (p99)&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1√ó c7i.xlarge&lt;/cell&gt;
        &lt;cell&gt;4.8 MiB/s&lt;p&gt;5036 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;24.6 MiB/s&lt;p&gt;25 183 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;60 ms&lt;/cell&gt;
        &lt;cell&gt;~60 % CPU; 4 partitions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;3√ó c7i.xlarge (replicated)&lt;/cell&gt;
        &lt;cell&gt;4.9 MiB/s&lt;p&gt;5015 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;24.5 MiB/s&lt;p&gt;25 073 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;186 ms&lt;/cell&gt;
        &lt;cell&gt;~65 % CPU; cross-AZ RF‚âà2.5; 4 partitions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1√ó c7i.24xlarge&lt;/cell&gt;
        &lt;cell&gt;238 MiB/s&lt;p&gt;243,000 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;1.16 GiB/s&lt;p&gt;1,200,000 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;853 ms&lt;/cell&gt;
        &lt;cell&gt;~10 % CPU (idle); 30 partitions&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Queue Results&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Setup&lt;/cell&gt;
        &lt;cell role="head"&gt;üì¨ Throughput (read + write)&lt;/cell&gt;
        &lt;cell role="head"&gt;üî≠ e2e Latency5 (p99)&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1√ó c7i.xlarge&lt;/cell&gt;
        &lt;cell&gt;2.81 MiB/s&lt;p&gt;2885 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;17.7 ms&lt;/cell&gt;
        &lt;cell&gt;~60 % CPU; read-client bottleneck&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3√ó c7i.xlarge (replicated)&lt;/cell&gt;
        &lt;cell&gt;2.34 MiB/s&lt;p&gt;2397 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;920 ms ‚ö†Ô∏è6&lt;/cell&gt;
        &lt;cell&gt;replication lag inflated E2E latency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1√ó c7i.24xlarge&lt;/cell&gt;
        &lt;cell&gt;19.7 MiB/s&lt;p&gt;20,144 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;930 ms ‚ö†Ô∏è6&lt;/cell&gt;
        &lt;cell&gt;~50 % CPU; single-table bottleneck&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Make sure to at least read the last section of the article where we philosophize - # Should You Use Postgres?&lt;/p&gt;
    &lt;head rend="h1"&gt;PG as a Pub/Sub&lt;/head&gt;
    &lt;p&gt;There are dozens of blogs out there using Postgres as a queue, but interestingly enough I haven‚Äôt seen one use it as a pub-sub messaging system.&lt;/p&gt;
    &lt;p&gt;A quick distinction between the two because I often see them get confused:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Queues are meant for point-to-point communication. They‚Äôre widely used for asynchronous background jobs: worker apps (clients) process a task in the queue like sending an e-mail or pushing a notification. The event is consumed once and it‚Äôs done with. A message is immediately deleted (popped) off the queue once it‚Äôs consumed. Queues do not have strict ordering guarantees7.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pub-sub messaging differs from the queue in that it is meant for one-to-many communication. This inherently means there is a large read fanout - more than one reader client is interested in any given message. Good pub-sub systems decouple readers from writers by storing data on disks. This allows them to not impose a max queue depth limit - something in-memory queues need to do in order to prevent them from going OOM.&lt;/p&gt;
        &lt;p&gt;There is also a general expectation that there is strict order - events should be read in the same order that they arrived in the system.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Postgres‚Äô main competitor here is Kafka, which is the standard in pub-sub today. Various (mostly-proprietary) alternatives exist.8&lt;/p&gt;
    &lt;p&gt;Kafka uses the Log data structure to hold messages. You‚Äôll see my benchmark basically reconstructs a log from Postgres primitives.&lt;/p&gt;
    &lt;p&gt;Postgres doesn‚Äôt seem to have any popular libraries for pub-sub9 use cases, so I had to write my own. The Kafka-inspired workflow I opted for is this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Writers produce batches of messages per statement10 (&lt;code&gt;INSERT INTO&lt;/code&gt;). Each transaction carries one batch insert and targets a single&lt;code&gt;topicpartition&lt;/code&gt;table11&lt;/item&gt;
      &lt;item&gt;Each writer is sticky to one table, but in aggregate they produce to multiple tables.&lt;/item&gt;
      &lt;item&gt;Each message has a unique monotonically-increasing offset number. A specific row in a special &lt;code&gt;log_counter&lt;/code&gt;table denotes the latest offset for a given&lt;code&gt;topicpartition&lt;/code&gt;table.&lt;/item&gt;
      &lt;item&gt;Write transactions atomically update both the &lt;code&gt;topicpartition&lt;/code&gt;data and the&lt;code&gt;log_counter&lt;/code&gt;row. This ensures consistent offset tracking across concurrent writers.&lt;/item&gt;
      &lt;item&gt;Readers poll for new messages. They consume the &lt;code&gt;topicpartition&lt;/code&gt;table(s) sequentially, starting from the lowest offset and progressively reading up.&lt;/item&gt;
      &lt;item&gt;Readers are split into consumer groups. Each group performs separate, independent reads and makes progress on the &lt;code&gt;topicpartition&lt;/code&gt;tables.&lt;/item&gt;
      &lt;item&gt;Each group contains 1 reader per &lt;code&gt;topicpartition&lt;/code&gt;table.&lt;/item&gt;
      &lt;item&gt;Readers store their progress in a &lt;code&gt;consumer_offsets&lt;/code&gt;table, with a row for each&lt;code&gt;topicpartition,group&lt;/code&gt;pair.&lt;/item&gt;
      &lt;item&gt;Each reader updates the latest processed offset (claiming the records), selects the records and processes them inside a single transaction.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This ensures Kafka-like semantics - gapless, monotonically-increasing offsets and at-least-once/at-most-once processing. This test in particular uses at-least-once semantics, but neither choice should impact the benchmark results.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pub-Sub Setup&lt;/head&gt;
    &lt;head rend="h4"&gt;Table&lt;/head&gt;
    &lt;head rend="h4"&gt;Writes&lt;/head&gt;
    &lt;p&gt;The benchmark runs &lt;code&gt;N&lt;/code&gt; writer goroutines. These represent writer clients.
Each one loops and atomically inserts &lt;code&gt;$BATCH_SIZE&lt;/code&gt; records while updating the latest offset:&lt;/p&gt;
    &lt;head rend="h4"&gt;Reads&lt;/head&gt;
    &lt;p&gt;The benchmark also runs &lt;code&gt;N&lt;/code&gt; reader goroutines. Each reader is assigned a particular consumer group and partition. The group as a whole reads all partitions while each reader in the group reads only one partition at a time.&lt;/p&gt;
    &lt;p&gt;The reader loops, opens a transaction, optimistically claims &lt;code&gt;$BATCH_SIZE&lt;/code&gt; records (by advancing the offset mark beyond them), selects them and processes the records.
If successful, it commits the transaction and through that advances the offset for the group.&lt;/p&gt;
    &lt;p&gt;It is a pull-based read (just like Kafka), rather than push-based. If the reader has no records to poll, it sleeps for a bit.&lt;/p&gt;
    &lt;p&gt;First it opens a transaction:&lt;/p&gt;
    &lt;p&gt;Then it claims the offsets:&lt;/p&gt;
    &lt;p&gt;Followed by selecting the claimed records:&lt;/p&gt;
    &lt;p&gt;Finally, the data gets processed by the business logic (no-op in our benchmark) and the transaction is closed:&lt;/p&gt;
    &lt;p&gt;If you‚Äôre wondering ‚Äúwhy no &lt;code&gt;NOTIFY/LISTEN&lt;/code&gt;?‚Äù - my understanding of that feature is that it‚Äôs an optimization and cannot be fully relied upon, so polling is required either way12. Given that, I just copied Kafka‚Äôs relatively simple design.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pub-Sub Results&lt;/head&gt;
    &lt;p&gt;The full code and detailed results are all published on GitHub at stanislavkozlovski/pg-queue-pubsub-benchmark. I ran three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The results are the average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.xlarge Postgres server /w 25GB gp3 9000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;mostly default Postgres settings (synchronous commit, fsync); &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;4 topicpartition tables&lt;/item&gt;
      &lt;item&gt;10 writers (2 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;20 reader clients total (4 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 100 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 5036 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 4.8 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 38.7ms p99 / 6.2ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 25,183 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 24.6 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 27.3ms p99 (varied 8.9ms-47ms b/w runs); 4.67ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 60ms p99 / 10.6ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~60% CPU;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;disk was at ~1200 writes/s with iostat claiming 46 MiB/s&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are pretty good results. It‚Äôs funny to think that the majority of people run a complex distributed system like Kafka for similar workloads13.&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Tri-Node&lt;/head&gt;
    &lt;p&gt;Now, a replicated setup to more accurately mimic the durability and availability guarantees of Kafka.&lt;/p&gt;
    &lt;p&gt;The average of two 5-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3x c7i.xlarge Postgres servers /w 25GB gp3 9000 IOPS EBS volume &lt;list rend="ul"&gt;&lt;item&gt;each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)&lt;/item&gt;&lt;item&gt;one &lt;code&gt;sync&lt;/code&gt;replica and one&lt;code&gt;potential&lt;/code&gt;14 replica&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;a few custom Postgres settings like &lt;code&gt;wal_compression&lt;/code&gt;,&lt;code&gt;max_worker_processes&lt;/code&gt;,&lt;code&gt;max_parallel_workers&lt;/code&gt;,&lt;code&gt;max_parallel_workers_per_gather&lt;/code&gt;and of course -&lt;code&gt;hot_standby&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;4 topicpartition tables&lt;/item&gt;
      &lt;item&gt;10 writers (2 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;readers only access the primary DB15; readers are in the same AZ as the primary;&lt;/item&gt;
      &lt;item&gt;20 reader clients total (4 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 100 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 5015 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 4.9 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 153.45ms p99 / 6.8ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 25,073 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 24.5 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 57ms p99; 4.91ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 186ms p99 / 12ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~65% CPU;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;disk was at ~1200 writes/s with iostat claiming 46 MiB/s&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now these are astonishing results! Throughput was not impacted at all. Latency increased but not extremely. Our p99 e2e latency 3x‚Äôd (60ms vs 185ms), but the p95 barely moved from 10.6ms to 12ms.&lt;/p&gt;
    &lt;p&gt;This shows that a simple 3-node Postgres cluster can pretty easily sustain what is a very common Kafka workload - 5 MB/s ingest and 25 MB/s egress. Not only that, but for a cheap cost too. Just $11,514 per year.16&lt;/p&gt;
    &lt;p&gt;Typically, you‚Äôd expect Postgres to run more expensive than Kafka at a certain scale, simply because it wasn‚Äôt designed to be efficient for this use case. Not here though. Running Kafka yourself would cost the same. Running the same workload through a Kafka vendor will cost you at least $50,000 a year. ü§Ø&lt;/p&gt;
    &lt;p&gt;By the way, in Kafka it‚Äôs customary to apply client-side compression on your data. If we assume your messages were 5 KB in size and your clients applied a pretty regular compression ratio of 4x17 - Postgres is actually handling 20 MB/s ingress and 100 MB/s egress.&lt;/p&gt;
    &lt;head rend="h3"&gt;96 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;Ok, let‚Äôs see how far Postgres will go.&lt;/p&gt;
    &lt;p&gt;The results are the average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.24xlarge (96 vCPU, 192 GiB RAM) Postgres server instance /w 250GB io2 12,000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;modified Postgres settings (&lt;code&gt;huge_pages&lt;/code&gt;on, other settings scaled to match the machine);&lt;list rend="ul"&gt;&lt;item&gt;still kept fsync &amp;amp; synchronous_commit on for durability.&lt;/item&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;30 topicpartition tables&lt;/item&gt;
      &lt;item&gt;100 writers (~3.33 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;150 reader clients total (5 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 200 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 243,000 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 238 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 138ms p99 / 47ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 1,200,000 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 1.16 GiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 24.6ms p99&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 853ms p99 / 242ms p95 / 23.4ms p50&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~10% CPU (basically idle);&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;bottleneck: The bottleneck was the write rate per partition. It seems like the test wasn‚Äôt able to write at a higher rate than 8 MiB/s (8k msg/s) per table with this design. I didn‚Äôt push further, but I do wonder now as I write this - how far would writes have scaled?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Reads were trivial to scale. Adding more consumer groups was trivial - I tried with 10x fanout and still ran at low CPU. I didn‚Äôt include it because I didn‚Äôt feel the need to push to an unrealistic read-fanout extreme.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;240 MiB/s ingress and 1.16 GiB/s egress are pretty good! The 96 vCPU machine was overkill for this test - it could have done a lot more, or we could have simply opted for a smaller machine. For what it‚Äôs worth, I do think it‚Äôs worth it to deploy a separate Kafka cluster at this scale. Kafka can save you a lot of money here because it can be more efficient in how it handles cross-zone network traffic with features like Diskless Kafka.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pub-Sub Test Summary&lt;/head&gt;
    &lt;p&gt;The summarized table with the three test results can be found here ‚Üí üëâ stanislavkozlovski/pg-queue-pubsub-benchmark&lt;/p&gt;
    &lt;p&gt;These tests seem to show that Postgres is pretty competitive with Kafka at low scale.&lt;/p&gt;
    &lt;p&gt;You may have noticed none of these tests were particularly long-running. From my understanding, the value in longer-running tests is to test table vacuuming in Postgres, as that can have negative performance effects. In the pub-sub section, vacuuming doesn‚Äôt apply because the tables are append-only. My other reasoning for running shorter tests was to keep costs in check and not spend too much time18.&lt;/p&gt;
    &lt;p&gt;In any case, no benchmark is perfect. My goal wasn‚Äôt to indisputably prove &lt;code&gt;$MY_CLAIM&lt;/code&gt;. Rather, I want to start a discussion by showing that what‚Äôs possible is likely larger than what most people assume. I certainly didn‚Äôt assume I‚Äôd get such good numbers, especially with the pub-sub part.&lt;/p&gt;
    &lt;head rend="h1"&gt;PG as a Queue&lt;/head&gt;
    &lt;p&gt;In Postgres, a queue can be implemented with &lt;code&gt;SELECT FOR UPDATE SKIP LOCKED&lt;/code&gt;. This command selects an unlocked row and locks it. It also skips reading already-locked rows. That‚Äôs how mutual exclusion is achieved - a worker can‚Äôt get other workers‚Äô jobs.&lt;/p&gt;
    &lt;p&gt;Postgres has a very popular pgmq library that offers a slick queue API. To keep it simple and understand the end-to-end flow better, I decided to write my own queue. The basic version of it is very easy. My workflow is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;add job (&lt;code&gt;INSERT&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;lock row &amp;amp; take job (&lt;code&gt;SELECT FOR UPDATE SKIP LOCKED&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;process job (&lt;code&gt;{your business logic}&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;mark job as ‚Äúdone‚Äù (&lt;code&gt;UPDATE&lt;/code&gt;a field or&lt;code&gt;DELETE &amp;amp; INSERT&lt;/code&gt;the row into a separate table)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Postgres competes with RabbitMQ, AWS SQS, NATS, Redis19 and to an extent Kafka20 here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Queue Setup&lt;/head&gt;
    &lt;head rend="h4"&gt;Table&lt;/head&gt;
    &lt;p&gt;We use a simple &lt;code&gt;queue&lt;/code&gt; table. When an element is processed off the queue, it‚Äôs moved into the archive table.&lt;/p&gt;
    &lt;head rend="h4"&gt;Writes&lt;/head&gt;
    &lt;p&gt;We again run &lt;code&gt;N&lt;/code&gt; writer client goroutines.
Each one simply loops and sequentially inserts a single random item into the table:&lt;/p&gt;
    &lt;p&gt;It only inserts one message per statement, which is pretty inefficient at scale.&lt;/p&gt;
    &lt;head rend="h4"&gt;Reads&lt;/head&gt;
    &lt;p&gt;We again run &lt;code&gt;M&lt;/code&gt; reader client goroutines. Each reader loops and processes one message.
The processing is done inside a database transaction.&lt;/p&gt;
    &lt;p&gt;Each reader again only works with one message at a time per transaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Queue Results&lt;/head&gt;
    &lt;p&gt;I again ran the same three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The results are the average of two 15-minute tests. I also ran three 2-minute tests. They all performed similarly. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.xlarge Postgres server /w 25GB gp3 9000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;all default Postgres settings21&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;10 writer clients, 15 reader clients&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 2885 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 2.81 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 2.46ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 4.2ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency5: 17.72ms p99&lt;/item&gt;
      &lt;item&gt;server kept at ~60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What I found Postgres wasn‚Äôt good at was handling client count. The bottleneck in this setup was the read clients. Each client could not read more than ~192 messages a second because of its median read latency and sequential read nature.&lt;/p&gt;
    &lt;p&gt;Increasing client count boosted throughput but violated my ~60% CPU target. Trying to run 50 write and 50 read clients got to 4000 msg/s without increasing the queue depth but pegged the server‚Äôs CPU to 100%. I wanted to keep the benchmarks realistic for what you may run in production, rather than maxing out what a machine can do. This would be easily alleviated with a connection pooler (standard across all prod PG deployments) or a larger machine.&lt;/p&gt;
    &lt;p&gt;Another thing worth mentioning is that the workload could sustain a lot more writes than reads. If I didn‚Äôt throttle the benchmark, it would write at 12,000 msg/s and read at 2,800 msg/s. In the spirit of simplicity, I didn‚Äôt debug further and instead throttled my writes to see at what point I could get a stable 1:1 workload.&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Tri-Node&lt;/head&gt;
    &lt;p&gt;A single 10-minute test. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3x c7i.xlarge Postgres servers /w 25GB gp3 9000 IOPS EBS volume &lt;list rend="ul"&gt;&lt;item&gt;each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)&lt;/item&gt;&lt;item&gt;one &lt;code&gt;sync&lt;/code&gt;replica and one&lt;code&gt;potential&lt;/code&gt;14 replica&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;a few custom Postgres settings like &lt;code&gt;wal_compression&lt;/code&gt;,&lt;code&gt;max_worker_processes&lt;/code&gt;,&lt;code&gt;max_parallel_workers&lt;/code&gt;,&lt;code&gt;max_parallel_workers_per_gather&lt;/code&gt;and of course -&lt;code&gt;hot_standby&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;10 writer clients, 15 reader clients&lt;/item&gt;
      &lt;item&gt;readers only access the primary DB15; readers are in the same AZ as the primary;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 2397 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 2.34 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 3.3ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 7.6ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency5: 920ms p99 ‚ö†Ô∏è6; 536ms p95; 7ms p50&lt;/item&gt;
      &lt;item&gt;server kept at ~60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As expected, throughput and latency were impacted somewhat. But not that much. It‚Äôs still over 2000 messages a second, which is pretty good for an HA queue!&lt;/p&gt;
    &lt;head rend="h3"&gt;96 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.24xlarge Postgres server instance /w 250GB io2 12,000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;modified Postgres settings (&lt;code&gt;huge_pages&lt;/code&gt;on, other settings scaled to match the machine);&lt;list rend="ul"&gt;&lt;item&gt;still kept fsync &amp;amp; synchronous_commit on for durability.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;100 writer clients, 200 reader clients&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 20,144 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 19.67 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 9.42ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 22.6ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency: 930ms p99 ‚ö†Ô∏è6; 709ms p95; 12.6ms p50&lt;/item&gt;
      &lt;item&gt;server at 40-60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This run wasn‚Äôt that impressive. There is some bottleneck in the single-table queue approach at this scale which I didn‚Äôt bother figuring out. I figured that it wasn‚Äôt important to reach absurd numbers on a single table, since all realistic scenarios would have multiple queues and never reach 20,000 msg/s on a single one. The 96 vCPU instance would likely scale far further were we to run a few separate queue tables in parallel.&lt;/p&gt;
    &lt;head rend="h3"&gt;Queue Test Summary&lt;/head&gt;
    &lt;p&gt;The summarized table with the three test results can be found here ‚Üí üëâ stanislavkozlovski/pg-queue-pubsub-benchmark&lt;/p&gt;
    &lt;p&gt;Even a modest Postgres node can durably push thousands of queue ops/sec, which already covers the scale 99% of companies ever hit with a single queue. As I said earlier, the last 2 years have seen the Just Use Postgres slogan become mainstream. The &lt;code&gt;pgmq&lt;/code&gt; library‚Äôs star history captures this trend perfectly:
&lt;/p&gt;
    &lt;head rend="h1"&gt;Should You Use Postgres?&lt;/head&gt;
    &lt;p&gt;Most of the time - yes. You should always default to Postgres until the constraints prove you wrong.&lt;/p&gt;
    &lt;p&gt;Kafka is obviously better optimized for pub-sub workloads. Queue systems are obviously better optimized for queue workloads. The point is that picking a technology based on technical optimization alone is a flawed approach. To throw an analogy:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;a Formula One car is optimized to drive faster, but I still use a sedan to go to work. I am way more comfortable driving my sedan than an F1 car.&lt;/p&gt;
      &lt;p&gt;(seriously, see the steering wheel on these things)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Postgres sedan comes with many quality-of-life comforts that the F1 Kafka does not:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ability to debug messages with regular SQL&lt;/item&gt;
      &lt;item&gt;ability to delete, re-order or edit messages in place&lt;/item&gt;
      &lt;item&gt;ability to join pub-sub data with regular tables&lt;/item&gt;
      &lt;item&gt;ability to trivially read specific data via rich SQL queries (&lt;code&gt;ID=54&lt;/code&gt;,&lt;code&gt;name="John"&lt;/code&gt;,&lt;code&gt;cost&amp;gt;1000&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Giving up these comforts is a justified sacrifice for your F1 car to go at 378 kmh (235 mph), but masochistic if you plan on driving at 25kmh (15 mph).&lt;/p&gt;
    &lt;p&gt;Donald Knuth warned us in 1974 - premature optimization is the root of all evil. Deploying Kafka at small scale is premature optimization. The point of this article is to show you that this ‚Äúsmall scale‚Äù number has grown further than what people remember it to be - it can comfortably mean many megabytes per second.&lt;/p&gt;
    &lt;p&gt;We are in a Postgres Renaissance for a reason: Postgres is frequently good enough. Modern NVMEs and cheap RAM allow it to scale absurdly high.&lt;/p&gt;
    &lt;p&gt;What‚Äôs the alternative?&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Solutions for Everything?&lt;/head&gt;
    &lt;p&gt;Naive engineers tend to adopt a specialized technology at the slightest hint of a need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Need a cache? Redis, of course!&lt;/item&gt;
      &lt;item&gt;Search? Let‚Äôs deploy Elasticsearch!&lt;/item&gt;
      &lt;item&gt;Offline data analysis? BigQuery or Snowflake - that‚Äôs what our data analysts used at their last job.&lt;/item&gt;
      &lt;item&gt;No schemas? We need a NoSQL database like MongoDB.&lt;/item&gt;
      &lt;item&gt;Have to crunch some numbers on S3? Let‚Äôs use Spark!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A good engineer thinks through the bigger picture.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Does this new technology move the needle?&lt;/item&gt;
      &lt;item&gt;Is shaving a few milliseconds off our query worth the extra organizational complexity introduced with the change?&lt;/item&gt;
      &lt;item&gt;Will our users notice?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At small scale, these systems hurt you more than they benefit you. Distributed systems - both in terms of node count and system cardinality - should be respected, feared, avoided and employed only as a weapon of last resort against particularly gnarly problems. Everything with a distributed system becomes more challenging and time-consuming.&lt;/p&gt;
    &lt;p&gt;The problem is the organizational overhead. The organizational overhead of adopting a new system, learning its nuances, configs, establishing monitoring, establishing processes around deployments and upgrades, attaining operational expertise on how to manage it, creating runbooks, testing it, debugging it, adopting its clients and API, using its UI, keeping up with its ecosystem, etc.&lt;/p&gt;
    &lt;p&gt;All of these are real organizational costs that can take months to get right, even if the system in question isn‚Äôt difficult (a lot are). Managed SaaS offerings trade off some of the organizational overhead for greater financial costs - but they still don‚Äôt remove it all. And until you reach the scale where the technology is necessary, you pay these extra {financial, organizational} costs for zero significant gain.&lt;/p&gt;
    &lt;p&gt;If the same can be done with tech for which you‚Äôve already paid the organizational costs for (e.g Postgres), adopting something else prematurely is most definitely an anti-pattern. You don‚Äôt need web-scale technologies when you don‚Äôt have web-scale problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;MVI (a better alternative)&lt;/head&gt;
    &lt;p&gt;What I think is a better approach is to search for the minimum viable infrastructure (MVI): build the smallest amount of system while still providing value.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;choose good-enough technology your org is already familiar with &lt;list rend="ul"&gt;&lt;item&gt;good-enough == meets your users‚Äô needs without being too slow/expensive/insecure&lt;/item&gt;&lt;item&gt;familiar == your org has prior experience, has runbooks/ops setups, monitoring, UI, etc&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;solve a real problem with it&lt;/item&gt;
      &lt;item&gt;use the minimum set of features &lt;list rend="ul"&gt;&lt;item&gt;the fewer features you use, the more flexibility you have to move off the infra in question in the future (e.g if locked in with a vendor)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bonus points if that technology:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;is widely adopted so finding good engineers for it is trivial (Postgres - check)&lt;/item&gt;
      &lt;item&gt;has a strong and growing network effect (Postgres - check)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MVI approach reduces the surface area of your infra. The fewer moving parts you have, the fewer failure modes you worry about and the less glue code you have to maintain.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it‚Äôs human nature to go against this. Just like startups suffer due to MVP bloat (one more feature!), infra teams suffer due to MVI bloat (one more system!)&lt;/p&gt;
    &lt;head rend="h2"&gt;Why are we like this?&lt;/head&gt;
    &lt;p&gt;I won‚Äôt pretend to be able to map out the exact path-dependent outcome, but my guess is this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;the zero interest rate era gave us abundant speculative money that was invested in any company that could grow fast&lt;/item&gt;
      &lt;item&gt;a lot of viral internet companies were growing at speeds that led old infra to become obsolete fast&lt;/item&gt;
      &lt;item&gt;this prompted the next wave of ZIRP investment - specialized data infrastructure companies (in a gold rush, sell shovels!); some of these data infra startups spun off directly from the high-growth companies themselves&lt;/item&gt;
      &lt;item&gt;each well-funded data infra vendor is financially motivated to evangelize their product and have you adopt it even when you don‚Äôt need to (Everyone is Talking Their Book). They had deep pockets for marketing and used them.&lt;/item&gt;
      &lt;item&gt;innovative infrastructure software got engineered. It was exciting - so engineers got nerd-sniped into it&lt;/item&gt;
      &lt;item&gt;a web-scale craze/cargo cult developed, where everybody believed they need to be able to scale from zero to millions of RPS because they may go viral any day.&lt;/item&gt;
      &lt;item&gt;a trend developed to copy whatever solutions the most successful, largest digital-native companies were using (Amazon, Google, Uber, etc.)&lt;/item&gt;
      &lt;item&gt;the trend became a self-perpetuating prophecy: these technologies became a sought-after skill on resumes &lt;list rend="ul"&gt;&lt;item&gt;system design interview questions were adapted to test for knowledge of these systems&lt;/item&gt;&lt;item&gt;within an organization, engineers (knowingly or not) pushed for projects that are exciting and helped build their resumes;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This trend continues to grow while there is no strong competing force that is sufficiently motivated to push the opposite view. Even engineers inside a company, who ought to be motivated to keep things simple, have strong incentives to pursue extra complexity. It benefits their career by giving them a project to use as ammo for their next promotion and improves their resume (cool tech/story on there) for their next job-hop. Plus it‚Äôs simply more fun.&lt;/p&gt;
    &lt;p&gt;This is why I think we, as an industry, don‚Äôt always use the simplest solution available.&lt;/p&gt;
    &lt;p&gt;In most cases, Postgres is that simplest solution that is available.&lt;/p&gt;
    &lt;head rend="h2"&gt;But It Won‚Äôt Scale!&lt;/head&gt;
    &lt;p&gt;I want to wrap this article up, but one rebuttal I can‚Äôt miss addressing is the ‚Äúit won‚Äôt scale argument‚Äù.&lt;/p&gt;
    &lt;p&gt;The argument goes something like this: ‚Äúin today‚Äôs age we can go viral at a moment‚Äôs notice; these viral moments are very valuable for our business so we need to aggressively design in a way that keeps our app stable under traffic spikes‚Äù&lt;/p&gt;
    &lt;p&gt;I have three arguments against this:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Postgres Scales&lt;/head&gt;
    &lt;p&gt;As of 2025, OpenAI still uses an unsharded Postgres architecture with only one primary instance for writes22. OpenAI is the poster-child of rapid viral growth. They hold the record for the fastest startup to reach 100 million users.&lt;/p&gt;
    &lt;p&gt;Bohan Zhang, a member of OpenAI‚Äôs infrastructure team and co-founder of OtterTune (a Postgres tuning service), can be quoted as saying23:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúAt OpenAI, we utilize an unsharded architecture with one writer and multiple readers, demonstrating that PostgreSQL can scale gracefully under massive read loads.‚Äù&lt;/p&gt;
      &lt;p&gt;‚ÄúThe main message of my talk was that if you are not too write heavy, you can scale Postgres to a very high read throughput with read replicas using only a single master! That is exactly the message that needs to be spelled out as that covers the vast majority of apps.‚Äù&lt;/p&gt;
      &lt;p&gt;‚ÄúPostgres is probably the default choice for developers right now. You can use Postgres for a very long time. If you are building a startup with read-heavy workloads, just start with Postgres. If you hit a scalability issue, increase the instance size. You can scale it to a very large scale. If in the future the database becomes a bottleneck, congratulations. You have built a successful startup. It‚Äôs a good problem to have.‚Äù&lt;/p&gt;
      &lt;p&gt;(slightly edited for clarity and grammar)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Despite their rapid growth to a user base of more than 800 million, OpenAI has still NOT opted for a web-scale distributed database. If they haven‚Äôt‚Ä¶ why does your unproven project need to?&lt;/p&gt;
    &lt;head rend="h3"&gt;2. You Have More Time To Scale Than You Think&lt;/head&gt;
    &lt;p&gt;Let‚Äôs say it‚Äôs a good principle to design/test for ~10x your scale. Here are the years of consistent growth rate it takes to get to 10x your current scale:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;annual growth&lt;/cell&gt;
        &lt;cell role="head"&gt;years to hit 10√ó scale&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10 %&lt;/cell&gt;
        &lt;cell&gt;24.16 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25 %&lt;/cell&gt;
        &lt;cell&gt;10.32 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;50 %&lt;/cell&gt;
        &lt;cell&gt;5.68 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;75 %&lt;/cell&gt;
        &lt;cell&gt;4.11 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;100 %&lt;/cell&gt;
        &lt;cell&gt;3.32 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;150 %&lt;/cell&gt;
        &lt;cell&gt;2.51 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;200 %&lt;/cell&gt;
        &lt;cell&gt;2.10 y&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It goes to show that even at extreme growth levels, you still have years to migrate between solutions. The majority of developers, though, work at companies in the 0-50% growth rate. They are more likely to have moved on to another job by the time the solution needs to change (if ever).&lt;/p&gt;
    &lt;head rend="h3"&gt;3. It‚Äôs Overdesign&lt;/head&gt;
    &lt;p&gt;In an ideal world, you would build for scale and any other future problem you may hit in 10 years.&lt;/p&gt;
    &lt;p&gt;In the real world, you have finite bandwidth, so you have to build for the most immediate, highest ROI problem.&lt;/p&gt;
    &lt;p&gt;Commenter snej on lobste.rs captured it well:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Planning your infrastructure around being able to handle that is sort of like buying a huge Marshall stack as your first guitar amp because your garage band might get invited to open for Coldplay.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Just use Postgres until it breaks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disclaimers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Title inspiration comes from a great recent piece - ‚ÄúRedis is fast - I‚Äôll cache in Postgres‚Äù&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I‚Äôm a complete Postgres noob. You may see a lot of dumb mistakes here. Feel free to call me out on them - I‚Äôm happy to learn. I used AI to help a lot with some of the PG tools to use. This both shows how inexperienced I am in the context and how easy it is to start. I am generally skeptical of AI‚Äôs promise (in the short-term), but there‚Äôs no denying it has made a large dent in democratizing niche/low-level knowledge.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you‚Äôd like to reach out to me, you can find me on LinkedIn or X (Twitter).&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Don‚Äôt worry if you don‚Äôt fully understand these terms. I work full-time in the industry that spews these things and I don‚Äôt have a great grasp either. It‚Äôs marketing slop. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gartner and others push embarrassing recommendations that aren‚Äôt tech driven. It‚Äôs frequently the opposite - they‚Äôre profit driven. Gartner makes $6.72B purely off a consulting service that charges organizations $50k per seat solely for access to reports that recommend these slop architectures. It‚Äôs not crazy to believe, hence many people are converging with the idea that it is a pay-to-win racket model. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Seriously, the improvement in hardware is something I find most senior engineers haven‚Äôt properly appreciated. Newest gen AMD CPUs boast 192 cores. Modern SSDs can do 5.5 million random reads a second, or ~28GB/s sequential reads. Both are a 10-20x improvement over the last 10 years alone. Single nodes are more powerful than ever. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Just in the last 6 months - Snowflake acquired Crunchy Data for ~$250M, Databricks acquired Neon for ~$1 billion; In the last 12 months, Supabase more than 5x‚Äôd its valuation from ($900M to $5B), raising $380M across three series (!!!). Within a single year! ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;End-to-end latency here is defined as&lt;/p&gt;&lt;code&gt;now() - event_create_time&lt;/code&gt;; In essence, it tracks how long a brand new persisted event takes to get consumed. It helps show cases where queue lag spikes like when consumers temporarily fall behind the write rate. ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4 ‚Ü©5 ‚Ü©6 ‚Ü©7&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Some queue tests showed higher E2E latencies which I believe was due to a bug. In the pub-sub tests, I ensured readers start before the writers via a 1000ms sleep. For the queue tests, though, I didn‚Äôt do this. The result is that queue tests immediately spike queue depth at startup because the writers manage to get a head start before the readers. I believe the E2E latency is artificially high because of this flaw in the test. ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Actually, things are ordered in the happy path. Only during retries can you get out of order processing. e.g at t=0, client A reads task N; At t=1, client B reads task N+1 and processes it successfully; At t=2, A fails and is unable to process task N; At t=3, client B takes the next available task - which is N. B therefore executes the tasks in order [N+1, N], whereas proper order would have been [N, N+1] ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Open-source projects include Apache Pulsar (open source), RedPanda (source-available), AutoMQ (a fork of Kafka) and a lot of proprietary offerings - AWS Kinesis, Google Pub/Sub, Azure Event Hubs, Confluent Kora, Confluent WarpStream, Bufstream to name a few. What‚Äôs common in 90% of these projects is that they all implement the Apache Kafka API, making Kafka undoubtedly the protocol standard in the space. There‚Äôs also an open-source project which exposes a Kafka API on top of a pluggable Postgres or S3 backend - Tansu (Rust, btw :] ) ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The most popular library I could find is pg-pubsub with 106 stars as of writing (Oct 2025). Its last commit was 3 months ago. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Batching messages per client is very important for scalability here. It is one of Kafka‚Äôs least-talked-about performance ‚Äútricks‚Äù. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;These tables act as different log data structures. You can see them as separate topics, or partitions of one topic (shards). ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Postgres stores all&lt;/p&gt;&lt;code&gt;NOTIFY&lt;/code&gt;events in a single, global queue. If this queue becomes full, transactions calling&lt;code&gt;NOTIFY&lt;/code&gt;will fail when committing. (src) ‚Ü©&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A report by RedPanda found that ~55% of respondents use Kafka for &amp;lt; 1 MB/s. Kafka-vendor Aiven similarly shared that 50% of their Kafka deployments have an ingest rate of below 10 MB/s. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This replication is equivalent to RF=2 in Kafka with one extra non-synchronous replica. Call it RF=2.5. The client receives a response when the one&lt;/p&gt;&lt;code&gt;sync&lt;/code&gt;replica confirms the change. The other&lt;code&gt;potential&lt;/code&gt;replica is replicating asynchronously without blocking the write path. It will become promoted to&lt;code&gt;sync&lt;/code&gt;if the other one was to die. ‚Ü© ‚Ü©2&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The tests didn‚Äôt direct any read traffic to the standbys. This caused extra load on the primary - most production workloads would read from the standbys. Despite that, the results were still good! In my tests, I found that the extra read workload didn‚Äôt seem to have a negative effect on the database - it seems such tail reads were served exclusively from cache. ‚Ü© ‚Ü©2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The node and its disk cost $1826 per year. Since we run three of those, it‚Äôs $5478/yr. The networking in AWS costs $0.02/GB and our setup is replicating 4.9MB/s across two instances - that results in 294.74TB cross-zone networking per year. That‚Äôs $6036 per year in replication networking. Assuming your clients are in the same zone as the database they‚Äôre writing to / reading from, that networking is free. That results in an annual cost of $11,514. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We can realistically achieve a 10x+ compression ratio if operating on compressible data like logs (something Kafka is used for frequently). The only gotcha is that we need to compress larger batches - eg 25KB+ - so that requires a bit of a re-design in the pub-sub data model. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I had already spent enough business days working on this benchmark and re-running tests numerous, numerous times as I iterated on the benchmark and the methodology. On the larger instances, the cost accumulates fast and running longer tests at high MB/s rates requires deploying much larger and more expensive disks in order to store all the accumulated data. The effort spent matches the goal I have with the article. If any Postgres vendor wants to sponsor a more thorough investigation - let me know! ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Surprisingly (to me), Redis is a very popular queue-like backend choice for background jobs. Most popular open-source libraries use it. Although I‚Äôm sure Postgres can do just as good a job, many devs will prefer to use an established library rather than build one from scratch or use something less well-maintained. I do think PG-backed libraries should get developed though! ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kafka has historically never been a queue. To use it as one, you had to develop some difficult workarounds. Today, however, it is in the middle of implementing a first-class Queue-like interface (currently in Preview) ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most importantly, synchronous commit and fsync are both on. This means every write is durably persisted to disk. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The optimizations they did to support this scale are cool, but not novel. See these two talks at a) PGConf.dev 2025 (my transcript) and b) POSETTE (my transcript) ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;From the talks PGConf.dev 2025 (my transcript) and POSETTE (my transcript) ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks"/><published>2025-10-29T14:06:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45747112</id><title>I made a 10¬¢ MCU Talk</title><updated>2025-10-29T22:39:16.020116+00:00</updated><content>&lt;doc fingerprint="a50cd3b339220607"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;TLDR: Yes, you can fit about 7 seconds of audio into 16K of flash and still have room for code. And you can even play LPC encoded audio on a 10 cent MCU.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There‚Äôs quite a lot more detail in this video (and of course you can hear the audio!).&lt;/p&gt;
    &lt;p&gt;In the previous project, I had this ultra-cheap CH32V003 microcontroller playing simple tunes on a tiny SMD buzzer. It was just toggling a GPIO pin at musical note frequencies ‚Äì 1-bit audio output ‚Äì and it sounded surprisingly decent. That was a fun start, but now it‚Äôs time to push this little $0.10 MCU even further: can we make it actually talk?&lt;/p&gt;
    &lt;p&gt;Spoiler: Yes, we can! (well, there wouldn‚Äôt be much of a blog post if we couldn‚Äôt) This 8-pin RISC-V chip is now producing sampled audio data and spoken words. We‚Äôre really stretching the limits of what you can fit in 16 KB of flash.&lt;/p&gt;
    &lt;head rend="h2"&gt;From Beeps to Actual Audio&lt;/head&gt;
    &lt;p&gt;Moving from simple beeps to real audio meant using the microcontroller‚Äôs PWM output as a rudimentary DAC. Instead of just on/off beeping, I‚Äôm driving a waveform at an 8 kHz sample rate using a high-frequency PWM on the output pin. The hardware is the same tiny board as before ‚Äì but I‚Äôve swapped the small SMD buzzer for a small speaker. The buzer works too, but it‚Äôs quieter and very tinny.&lt;/p&gt;
    &lt;p&gt;The sample I wanted to test with is just over 6 seconds in length - it‚Äôs the iconic ‚ÄúOpen the pod bay doors HAL‚Ä¶‚Äù sequence from 2001.&lt;/p&gt;
    &lt;p&gt;If we keep this audio at 16-bit PCM, 8kHZ, we‚Äôd need about 96KB ‚Äì way beyond our 16 KB flash! And remember, that 16 KB has to hold both the audio data and our playback code. Clearly some aggressive compression is required.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Sample Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Bits/Sample&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Fits in 16KB?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CD Quality&lt;/cell&gt;
        &lt;cell&gt;44.1 kHz&lt;/cell&gt;
        &lt;cell&gt;16-bit&lt;/cell&gt;
        &lt;cell&gt;529 KB&lt;/cell&gt;
        &lt;cell&gt;‚ùå 33√ó too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Phone Quality&lt;/cell&gt;
        &lt;cell&gt;16 kHz&lt;/cell&gt;
        &lt;cell&gt;16-bit&lt;/cell&gt;
        &lt;cell&gt;192 KB&lt;/cell&gt;
        &lt;cell&gt;‚ùå 12√ó too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Basic PCM&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;8-bit&lt;/cell&gt;
        &lt;cell&gt;48 KB&lt;/cell&gt;
        &lt;cell&gt;‚ùå 3√ó too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;4-bit ADPCM (IMA)&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;4-bit&lt;/cell&gt;
        &lt;cell&gt;24 KB&lt;/cell&gt;
        &lt;cell&gt;‚ùå 1.5√ó too big&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;QOA (Quite OK Audio)&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;3.2-bit&lt;/cell&gt;
        &lt;cell&gt;19 KB&lt;/cell&gt;
        &lt;cell&gt;‚ùå Still too big!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2-bit ADPCM&lt;/cell&gt;
        &lt;cell&gt;8 kHz&lt;/cell&gt;
        &lt;cell&gt;2-bit&lt;/cell&gt;
        &lt;cell&gt;12 KB&lt;/cell&gt;
        &lt;cell&gt;‚úÖ Fits!&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I considered a few encoding options for compressing the audio.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8-bit PCM: Simply using 8-bit samples at 8 kHz cuts size in half (to ~47 KB for 6s), but that‚Äôs still about 3√ó too large for our flash.&lt;/item&gt;
      &lt;item&gt;4-bit ADPCM: Adaptive Differential PCM is a simple lossy compression that could quarter the size. In theory 6 seconds would be ~24 KB ‚Äì much closer to fitting,&lt;/item&gt;
      &lt;item&gt;‚ÄúQuite OK Audio‚Äù (QOA): This is nice codec that packs audio into about 3.2 bits per sample (roughly 1/5 the size of 16-bit PCM)&lt;/item&gt;
      &lt;item&gt;2-bit ADPCM: Going even further with ADPCM, using only 2 bits per sample gives a 4:1 compression relative to 8-bit audio ‚Äì that‚Äôs 75% storage savings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2-bit ADPCM is definitely the winner here. Our 6-second clip shrinks to under 12 KB, which comfortably fits in flash with room for code. This looked like the winner, provided the audio quality was acceptable. The decoder for 2-bit ADPCM is also very lightweight (my implementation compiled to under just over 1K of code - 1340 bytes!). It‚Äôs definitely low quality - but it actually sounds surprisingly ok.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does 2-bit ADPCM work?&lt;/head&gt;
    &lt;p&gt;It‚Äôs actually a very simple algorithm. Both the encoder and decoder maintain a predicted signal value and a step size index into a predefined table. Each 2-bit code tells the decoder how to adjust the current prediction and the step size index. In essence, we‚Äôre coding the difference between the real audio and our prediction, with only four possible levels (since 2 bits gives 4 values). After each sample, the algorithm adapts: if the prediction error was large, we move to a bigger step size (to allow larger changes); if the error was small, we use a smaller step size for finer resolution. This adaptive step is what makes it ADPCM (Adaptive Differential PCM).&lt;/p&gt;
    &lt;p&gt;Our codes are as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;00&lt;/code&gt;(0): Go down by 1 step - subtract the step size from our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;01&lt;/code&gt;(1): Go up by 1 step - add the step size to our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;10&lt;/code&gt;(2): Go down by 2 steps - subtract the 2 x step size from our current prediction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;11&lt;/code&gt;(3): Go up by 2 steps - add the 2 x step size to our current prediction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even with this very high level of compression, the predicted waveform manages to track the original audio surprisingly well. The above graph shows a small snippet of the audio: the blue line is the original waveform and the yellow line is the ADPCM decoder‚Äôs output.&lt;/p&gt;
    &lt;p&gt;They‚Äôre not identical (and we wouldn‚Äôt expect them to be), but the general shape is preserved. When you play it back through the little speaker, it‚Äôs recognizable and surprisingly good.&lt;/p&gt;
    &lt;p&gt;To make my life easier, I built a quick conversion tool to encode WAV files into this 2-bit ADPCM format. The tool lets me drag-and-drop a WAV, and it gives you the files with the data that can ve dropped into the firmware code.&lt;/p&gt;
    &lt;head rend="h2"&gt;LPC Speech Synthesis&lt;/head&gt;
    &lt;p&gt;Six seconds of audio is cool, but what about longer phrases or even arbitrary speech? Storing anything much longer with raw or ADPCM audio would quickly fill the 16K of flash.&lt;/p&gt;
    &lt;p&gt;For my second experiment, I tried something different: instead of recorded waveform audio, I used an old-school speech synthesis approach. This leverages the fact that spoken language can be encoded very compactly by modeling the human voice, rather than storing the raw sound. Specifically, I integrated a library called Talkie.&lt;/p&gt;
    &lt;p&gt;Talkie is a software implementation of the Texas Instruments LPC speech synthesis architecture from the late 1970s. This was implemented in a variety of chips, most commonly the TMS5220 and TMS5100 speech chips.&lt;/p&gt;
    &lt;p&gt;These were used in things like the original Speak &amp;amp; Spell, arcade games like early Star Wars, and speech add-ons for home computers (e.g. the BBC Micro).&lt;/p&gt;
    &lt;p&gt;The Talkie library (originally by Peter Knight, later added to by Adafruit) comes with a big set of examples and vocabulary. It‚Äôs also possible to extract examples from old ROMs from arcade games.&lt;/p&gt;
    &lt;p&gt;Each phrase or word only takes a few hundred bytes or even less, so you can fit quite a lot of speech into a few kilobytes of flash. The trade-off is that the voice has a very computer-esque timbre ‚Äì think of the Speak &amp;amp; Spell‚Äôs voice. It‚Äôs clearly synthetic, but still understandable.&lt;/p&gt;
    &lt;p&gt;To say custom sentences not in the library, you either concatenate the available words/phonemes (which can be clunky), or you need to generate new LPC data. The original tools for this are a bit obscure ‚Äì there‚Äôs BlueWizard (a classic Mac app) and PythonWizard (a command-line tool with TK GUI) which can analyze WAV files and produce LPC data.&lt;/p&gt;
    &lt;p&gt;I gave both a try with some success (and a few headaches setting them up). In the end, I cheated a bit and used an AI coding assistant to help me create a streamlined online tool for this.&lt;/p&gt;
    &lt;p&gt;The result is a little web app where I can upload a recording of, say, my own voice, and it outputs the LPC data. It even lets me play back the synthesized voice in-browser to check it.&lt;/p&gt;
    &lt;p&gt;So there we have it ‚Äì our 10¬¢ microcontroller now has a voice! By using 2-bit ADPCM compression, we can store short audio clips (up to around 8 seconds) even in 16 KB of flash, and play them back via PWM with decent fidelity.&lt;/p&gt;
    &lt;p&gt;And with the Talkie LPC speech synthesis, we can make the device ‚Äúspeak‚Äù lots of words and phrases with only a tiny memory footprint.&lt;/p&gt;
    &lt;p&gt;If you want to hear it for yourself, check out the video demo linked at the top of this post. In the video, you‚Äôll see (and hear) the WarGames clip and the Star Wars quotes running on the hardware. It‚Äôs honestly amazing what these cheap little MCUs can do. We‚Äôre really pushing the boundaries of cheap hardware here.&lt;/p&gt;
    &lt;p&gt;You can find all my code on GitHub in this repository.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.atomic14.com/2025/10/29/CH32V003-talking"/><published>2025-10-29T14:12:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748661</id><title>Tell HN: Azure outage</title><updated>2025-10-29T22:39:14.868841+00:00</updated><content>&lt;doc fingerprint="7e7891550f0627d5"&gt;
  &lt;main&gt;
    &lt;p&gt;It still surprises me how much essential services like public transport are completely reliant on cloud providers, and don't seem to have backups in place.&lt;/p&gt;
    &lt;p&gt;Here in The Netherlands, almost all trains were first delayed significantly, and then cancelled for a few hours because of this, which had real impact because today is also the day we got to vote for the next parlement (I know some who can't get home in time before the polls close, and they left for work before they opened).&lt;/p&gt;
    &lt;p&gt;Is voting there a one day only event? If not, I feel the solution to that particular problem is quite clear. There‚Äôs a million things that could go wrong causing you to miss something when you try to do it in a narrow time range (today after work before polls close)&lt;/p&gt;
    &lt;p&gt;If it‚Äôs a multi day event, it‚Äôs probably that way for a reason. Partially the same as the solution to above.&lt;/p&gt;
    &lt;p&gt;In europe, voting typically happens in one day, where everyone physically goes to their designated voting place and puts papers in a transparent box. You can stay there and wait for the count at the end of the day if you want to. Tom Scott has a very good video about why we don't want electronic/mail voting: https://www.youtube.com/watch?v=w3_0x6oaDmI&lt;/p&gt;
    &lt;p&gt;Well "mail in voting" in Washington state pretty much means you drop off your ballot in a drop box in your neighborhood. Which is pretty much the same thing as putting it in a ballot box.&lt;/p&gt;
    &lt;p&gt;Italy has mail-in vote only for citizen residing abroad. The rest vote on the election Sunday (and Monday morning in some cases, at least in the past).&lt;/p&gt;
    &lt;p&gt;Off the top of my head, I can't think of an EU country that does not have some form of advance voting.&lt;/p&gt;
    &lt;p&gt;Here in Latvia the "election day" is usually (always?) on weekend, but the polling stations are open for some (and different!) part of every weekday leading up. Something like couple hours on monday morning, couple hours on tuesday evening, couple around midday wednesday, etc. In my opinion, it's a great system. You have to have a pretty convoluted schedule for at least one window not to line up for you.&lt;/p&gt;
    &lt;p&gt;Washington State having full vote-by-mail (there is technically a layer of in-person voting as a fallback for those who need it for accessibility reasons or who missed the registration deadline) has spoiled me rotten, I couldn't imagine having to go back to synchronous on-site voting on a single day like I did in Illinois. Awful. Being able to fill my ballot at my leisure, at home, where I can have all the research material open, and drive it to a ballot drop box whenever is convenient in a 2-3 week window before 20:00 on election night, is a game-changer for democracy. Of course this also means that people who serve to benefit from disenfranchising voters and making it more difficult to vote, absolutely hate our system and continually attack it for one reason or another.&lt;/p&gt;
    &lt;p&gt;As a Dutchman, I have to go vote in person on a specific day. But to be honest: I really don't mind doing so. If you live in a town or city, there'll usually be multiple voting locations you can choose from within 10 minutes walking distance. I've never experienced waiting times more than a couple of minutes. Opening times are pretty good, from 7:30 til 21:00. The people there are friendly. What's not to like? (Except for some of the candidates maybe, but that's a whole different story. :-))&lt;/p&gt;
    &lt;p&gt;So, if you have a minor emergency, like a kidney stone and hospitalized for the day - you just miss your chance to vote in that election?&lt;/p&gt;
    &lt;p&gt;If so, I see a lot to dislike. As the point I was making is you can‚Äôt anticipate what might come up. Just because it‚Äôs worked thus far doesn‚Äôt mean it‚Äôs designed for resilience. There‚Äôs a lot of ways you could miss out in that type of situation. I seems silly to make sure everything else is redundant and fault tolerant in the name of democracy when the democratic process itself isn‚Äôt doing the same.&lt;/p&gt;
    &lt;p&gt;How is that an acceptable response? Honestly. You‚Äôre in the hospital, in pain, likely having a minor surgery, and having someone cast your vote for you is going to be on your mind too? Do you have your voting card in your pocket just in case this were to play out?&lt;/p&gt;
    &lt;p&gt;That‚Äôs just ridiculous in my opinion. Makes me wonder how many well intentioned would be voters end up missing out each election cause shit happens and voting is pretty optional&lt;/p&gt;
    &lt;p&gt;Mild curiosity, no idea whether it would be statistically relevant but asking the question is the first step. If you knew the answer, you might want to extend the voting window even if it wouldn't effect an elections outcome it would be a quantified number of people excluded from the democratic process for simply having bad luck at the wrong time.&lt;/p&gt;
    &lt;p&gt;Please lookup US voting poll overflow issues that come up every election cycle. Just because you experience a well streamlined process doesn't mean that it's the norm everywhere.&lt;/p&gt;
    &lt;p&gt;We're on year five of one of the two parties telling voters to not trust early voting. Their choice is because of the Fear, Uncertainty, and Doubt created by the propaganda they are fed.&lt;/p&gt;
    &lt;p&gt;"No mail-in or 'Early' Voting, Yes to Voter ID! Watch how totally dishonest the California Prop Vote is! Millions of Ballots being 'shipped.' GET SMART REPUBLICANS, BEFORE IT IS TOO LATE!!!"&lt;/p&gt;
    &lt;p&gt;That's all happening too, but it's honestly a different topic altogether. We have the ability to vote early. Whether you trust it or politicians are trying to undermine your trust in it, etc.... whole other can of worms&lt;/p&gt;
    &lt;p&gt;If India can have voters vote and tally all the votes in one day, then so can everyone else. It‚Äôs the best way to avoid fraud and people going with whoever is ahead. I am sympathetic with emergency protocols for deadly pandemics, but for all else, in-person on a given day.&lt;/p&gt;
    &lt;p&gt;&amp;gt; If India can have voters vote and tally all the votes in one day, then so can everyone else.&lt;/p&gt;
    &lt;p&gt;In most countries, in the elections you vote or the member of parliament you want. Presidential elections, and city council elections are held separately, but are also equally simple. But in one election you cast your vote for one person, and that's it.&lt;/p&gt;
    &lt;p&gt;With this kind of elections, many countries manage to hold the elections on paper ballots, count them all by hand, and publish results by midnight.&lt;/p&gt;
    &lt;p&gt;But on an American ballot, you vote for, for example:&lt;/p&gt;
    &lt;p&gt;- US president - US senator - US member of congress - state governor - state senator - state member of congress - several votes for several different state judge positions - several other state officer positions - several votes for several local county officers - local sheriff - local school board member - several yes/no votes for several proposed laws, whether they should be passed or not&lt;/p&gt;
    &lt;p&gt;I don't think it would be possible to calculate all these 20 or 40 votes, if calculated by hand. That's why they use voting machines in America.&lt;/p&gt;
    &lt;p&gt;How is it not possible? It's just additional votes, there isn't anything actually stopping counting by hand, is there? How was it counted historically without voting machines?&lt;/p&gt;
    &lt;p&gt;Yet... deploy on two clouds and you'll get tax payers scream at you for "wasting money" preparing for a black swan event. Can't have both, either reliability or lower cost.&lt;/p&gt;
    &lt;p&gt;i'm not sure this is an easily solvable problem. i remember reading an article arguing that your cloud provider is part of your tech stack and it's close to impossible/a huge PITA to make a non-trivial service provider-agnostic. they'd have to run their own openstack in different datacenters, which would be costly and have their own points of failure.&lt;/p&gt;
    &lt;p&gt;Here in Belgium voting is usually done during the weekend, although it shouldn't matter because voting is a civic duty (unless you have a good reason you have to go vote or you'll be fined), so those who work during the weekend have a valid reason to come in late or leave early.&lt;/p&gt;
    &lt;p&gt;In the US, where I assume a lot of the griping comes from, election day is not a national holiday, nor is it on a weekend (in fact, by law it is defined as "the Tuesday next after the first Monday in November"), and even though it is acknowledged as an important civic duty, only about half of the states have laws on the books that require employers provide time off to vote. There are no federal laws to that effect, so it's left entirely to states to decide.&lt;/p&gt;
    &lt;p&gt;In Australia there are so many places to vote, it is almost popping out to get milk level if convenience. (At least in urbia and suburbia) Just detour your dog walk slightly. Always at the weekend.&lt;/p&gt;
    &lt;p&gt;In Australia there are so many places to vote, it is almost popping out to get milk level if convenience. Just detour your dog walk slightly. Always at the weekend.&lt;/p&gt;
    &lt;p&gt;The Flemish bus company (de Lijn) uses Azure and I couldn't activate my ticket when I came home after training a couple of hours ago. I should probably start using physical tickets again, because at least those work properly. It's just stupid that there's so much stuff being moved to digital only (often even only being accessible through an Android or iOS app, despite the parent companies of those two being utterly atrocious) when the physical alternatives are more reliable.&lt;/p&gt;
    &lt;p&gt;Organizations who had their own datacenters were chided for being resistant to modernizing, and now they modernized to use someone else's shared computers and they stopped working.&lt;/p&gt;
    &lt;p&gt;I really do feel the only viable future for clouds is hybrid or agnostic clouds.&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. In addition. customers may experience issues accessing the Azure Portal. Customers can attempt to use programmatic methods (PowerShell, CLI, etc.) to access/utilize resources if they are unable to access the portal directly. We have failed the portal away from Azure Front Door (AFD) to attempt to mitigate the portal access issues and are continuing to assess the situation.&lt;/p&gt;
    &lt;p&gt;We are actively assessing failover options of internal services from our AFD infrastructure. Our investigation into the contributing factors and additional recovery workstreams continues. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:57 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Update: 16:35 UTC:&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:18 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. We suspect that an inadvertent configuration change as the trigger event for this issue. We are taking two concurrent actions where we are blocking all changes to the AFD services and at the same time rolling back to our last known good state.&lt;/p&gt;
    &lt;p&gt;We have failed the portal away from Azure Front Door (AFD) to mitigate the portal access issues. Customers should be able to access the Azure management portal directly.&lt;/p&gt;
    &lt;p&gt;We do not have an ETA for when the rollback will be completed, but we will update this communication within 30 minutes or when we have an update.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 17:17 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;"We have initiated the deployment of our 'last known good' configuration. This is expected to be fully deployed in about 30 minutes from which point customers will start to see initial signs of recovery. Once this is completed, the next stage is to start to recover nodes while we route traffic through these healthy nodes."&lt;/p&gt;
    &lt;p&gt;"This message was last updated at 18:11 UTC on 29 October 2025"&lt;/p&gt;
    &lt;p&gt;At this stage, we anticipate full mitigation within the next four hours as we continue to recover nodes. This means we expect recovery to happen by 23:20 UTC on 29 October 2025. We will provide another update on our progress within two hours, or sooner if warranted.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 19:57 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;in many cases: no service health alerts, no status page updates and no confirmations from the support team in tickets. still we can confirm these issues from different customers accross europe. Mostly the issues are regional dependent.&lt;/p&gt;
    &lt;p&gt;This is the single most frustrating thing about these incidents. As you're harmstrung on what you can do or how you can react until Microsoft officially acknowledges a problem. Took nearly 90mins both today and when it happened on 9th October.&lt;/p&gt;
    &lt;p&gt;It's pretty unlikely. AWS published a public 'RCA' https://aws.amazon.com/message/101925/. A race condition in a DNS 'record allocator' causing all DNS records for DDB to be wiped out.&lt;/p&gt;
    &lt;p&gt;I'm simplifying a bit, but I don't think it's likely that Azure has a similar race condition wiping out DNS records on _one_ system than then propagates to all others. The similarity might just end at "it was DNS".&lt;/p&gt;
    &lt;p&gt;That RCA was fun. A distributed system with members that don't know about each other, don't bother with leader elections, and basically all stomp all over each other updating the records. It "worked fine" until one of the members had slightly increased latency and everything cascade-failed down from there. I'm sure there was missing (internal) context but it did not sound like a well-architected system at all.&lt;/p&gt;
    &lt;p&gt;THIS is the real deal. Some say it's always DNS but many times it's some routing fuckup with BGP. two most cursed 3 letter acronym technologies out there&lt;/p&gt;
    &lt;p&gt;Whilst the status message acknowledge's the issue with Front Door (AFD), it seems as though the rest of the actions are about how to get Portal/internal services working without relying on AFD. For those of us using Front Door does that mean we're in for a long haul?&lt;/p&gt;
    &lt;p&gt;yea I saw that, but im not sure on how accurate that is. a few large apps/companies I know to be 100% on AWS in us-east-1 are cranking along just fine.&lt;/p&gt;
    &lt;p&gt;Yeah, I am guessing it's just a placeholder till they get more info. I thought I saw somewhere that internally within Microsoft it's seen as a "Sev 1" with "all hands on deck" - Annoyingly I can't remember where I saw it, so if someone spots it before I do, please credit that person :D&lt;/p&gt;
    &lt;p&gt;It's a Sev 0 actually (as one would expect - this isn't a big secret). I was on the engineering bridge call earlier for a bit. The Azure service I work on was minimally impacted (our customer facing dashboard could not load, but APIs and data layer were not impacted) but we found a workaround.&lt;/p&gt;
    &lt;p&gt;We already had to do it for large files served from Blob Storage since they would cap out at 2MB/s when not in cache of the nearest PoP. If you‚Äôve ever experienced slow Windows Store or Xbox downloads it‚Äôs probably the same problem.&lt;/p&gt;
    &lt;p&gt;I had a support ticket open for months about this and in the end the agent said ‚Äúthis is to be expected and we don‚Äôt plan on doing anything about it‚Äù.&lt;/p&gt;
    &lt;p&gt;We‚Äôve moved to Cloudflare and not only is the performance great, but it costs less.&lt;/p&gt;
    &lt;p&gt;Only thing I need to move off Front Door is a static website for our docs served from Blob Storage, this incident will make us do it sooner rather than later.&lt;/p&gt;
    &lt;p&gt;we are considering the same but because our website uses APEX domain we would need to move all DNS resolver to cloudfront right ? Does it have as a nice "rule set builder" as azure ?&lt;/p&gt;
    &lt;p&gt;Unless you pay for CloudFlare‚Äôs Enterpise plan, you‚Äôre required to have them host your DNS zone, you can use a different registrar as long as you just point your NS records to Cloudflare.&lt;/p&gt;
    &lt;p&gt;Be aware that if you‚Äôre using Azure as your registrar, it‚Äôs (probably still) impossible to change your NS records to point to CloudFlare‚Äôs DNS server, at least it was for me about 6 months ago.&lt;/p&gt;
    &lt;p&gt;This also makes it impossible to transfer your domain to them either, as CloudFlare‚Äôs domain transfer flow requires you set your NS records to point to them before their interface shows a transfer option.&lt;/p&gt;
    &lt;p&gt;In our case we had to transfer to a different registrar, we used Namecheap.&lt;/p&gt;
    &lt;p&gt;However, transferring a domain from Azure was also a nightmare. Their UI doesn‚Äôt have any kind of transfer option, I eventually found an obscure document (not on their Learn website) which had an az command which would let you get a transfer code which I could give to Namecheap.&lt;/p&gt;
    &lt;p&gt;Then I had to wait over a week for the transfer timeout to occur because there is no way on Azure side that I could find to accept the transfer immediately.&lt;/p&gt;
    &lt;p&gt;I found CloudFlare‚Äôs way of building rules quite easy to use, different from Front Door but I‚Äôm not doing anything more complex than some redirects and reverse proxying.&lt;/p&gt;
    &lt;p&gt;I will say that Cloudflare‚Äôs UI is super fast, with Front Door I always found it painfully slow when trying to do any kind of configuration.&lt;/p&gt;
    &lt;p&gt;Cloudflare also doesn‚Äôt have the problem that Front Door has where it requires a manual process every 6 months or so to renew the APEX certificate.&lt;/p&gt;
    &lt;p&gt;Thanks :). We don't use Azure as our registrar. It seems I'll have to plan for this then, we also had another issue, AFD has a hard 500ms tls handshake timeout (doesn't matter how much you put on the origin timeout settings) which means if our server was slow for some reason we would get 504 origin timeout.&lt;/p&gt;
    &lt;p&gt;They briefly had a statement about using Traffic Manager to work with your AFD to work around this issue, with a link to learn.microsoft.com/...traffic-manager, and the link didn't work. Due to the same issue affecting everyone right now.&lt;/p&gt;
    &lt;p&gt;They quickly updated the message to REMOVE the link. Comical at this point.&lt;/p&gt;
    &lt;p&gt;I noticed that Starbucks mobile ordering was down and thought ‚Äúwelp, I guess I‚Äôll order a bagel and coffee on Grubhub‚Äù, then GrubHub was down. My next stop was HN to find the common denominator, and y‚Äôall did not disappoint.&lt;/p&gt;
    &lt;p&gt;I‚Äôve seen this up close twice and I‚Äôm surprised it‚Äôs only twice. Between March and September one year, 6 people on one team had to get new hard drives in their thinkpads and rebuild their systems. All from the same PO but doled out over the course of a project rampup. That was the first project where the onboarding docs were really really good, since we got a lot of practice in a short period of time.&lt;/p&gt;
    &lt;p&gt;Long before that, the first raid array anyone set up for my (teams‚Äô) usage, arrived from Sun with 2 dead drives out of 10. They RMA‚Äôd us 2 more drives and one of those was also DOA. That was a couple years after Sun stopped burning in hardware for cost savings, which maybe wasn‚Äôt that much of a savings all things considered.&lt;/p&gt;
    &lt;p&gt;Why? Starbucks is not providing a critical service. Spending less money and resources and just accepting the risk that occasionally you won't be able to sell coffee for a few hours is a completely valid decision from both management and engineering pov.&lt;/p&gt;
    &lt;p&gt;I noticed it when my Netatmo rigamajig stopped notifying me of bad indoor air quality. Lovely. Why does it need to go through the cloud if the data is right there in the home network‚Ä¶&lt;/p&gt;
    &lt;p&gt;My inner Nelson-from-the-Simpsons wishes I was on your team today, able to flaunt my flask of tea and homemade packed sandwiches. I would tease you by saying 'ha ha!' as your efforts to order coffee with IP packets failed.&lt;/p&gt;
    &lt;p&gt;I always go everywhere adequately prepared for beverages and food. Thanks to your comment, I have a new reason to do so. Take out coffees are actually far from guaranteed. Payment systems could go down, my bank account could be hacked or maybe the coffee shop could be randomly closed. Heck, I might even have an accident crossing the road. Anything could happen. Hence, my humble flask might not have the top beverage in it but at least it works.&lt;/p&gt;
    &lt;p&gt;We all design systems with redundancy, backups and whatnot, but few of us apply this thinking to our food and drink. Maybe get a kettle for the office and a backup kettle, in case the first one fails?&lt;/p&gt;
    &lt;p&gt;For some reason an Azure outage does not faze me in the same way that an AWS outage does.&lt;/p&gt;
    &lt;p&gt;I have never had much confidence in Azure as a cloud provider. The vertical integration of all the things for a Microsoft shop was initially very compelling. I was ready to fight that battle. But, this fantasy was quickly ruined by poor execution on Microsoft's part. They were able to convince me to move back to AWS by simply making it difficult to provision compute resources. Their quota system &amp;amp; availability issues are a nightmare to deal with compared to EC2.&lt;/p&gt;
    &lt;p&gt;At this point I'd rather use GCP over Azure and I have zero seconds of experience with it. The number of things Microsoft gets right in 2025 can be counted single-handedly. The things they do get right are quite good, but everything else tends to be extremely awful.&lt;/p&gt;
    &lt;p&gt;The "Blades" experience [0] where instead of navigating between pages it just kept opening things to the side and expanding horizontally?&lt;/p&gt;
    &lt;p&gt;Yeah, that had some fun ideas but was way more confusing than it needed to be. But also that was quite a few years back now. The Portal ditched that experience relatively quickly. Just long enough to leave a lot of awful first impressions, but not long enough for it to be much more than a distant memory at this point, several redesigns later.&lt;/p&gt;
    &lt;p&gt;[0] The name "Blades" for that came from the early years of the Xbox 360, maybe not the best UX to emulate for a complex control panel/portal.&lt;/p&gt;
    &lt;p&gt;Azure to me has always suffered from a belief that ‚ÄúUI innovations can solve UX complexity if you just try hard enough.‚Äù&lt;/p&gt;
    &lt;p&gt;Like, AWS, and GCP to a lesser extent, has a principled approach where simple click-ops goals are simple. You can access the richer metadata/IAM object model at any time, but the wizards you see are dumb enough to make easy things easy.&lt;/p&gt;
    &lt;p&gt;With Azure, those blades allow tremendously complex ‚Äúyou need to build an X Container and a Container Bucket to be able to add an X‚Äù flows to coexist on the same page. While this exposes the true complexity, and looks cool/works well for power users, it is exceedingly unintuitive. Inline documentation doesn‚Äôt solve this problem.&lt;/p&gt;
    &lt;p&gt;I sometimes wonder if this is by design: like QuickBooks, there‚Äôs an entire economy of consultants who need to be Certified and thus will promote your product for their own benefit! Making the interface friendly to them and daunting to mere mortals is a feature, not a bug.&lt;/p&gt;
    &lt;p&gt;But in Azure‚Äôs case it‚Äôs hard to tell how much this is intentional.&lt;/p&gt;
    &lt;p&gt;(I think that's from near the transition because it has full "windowing" controls of minimize/maximize/close buttons. I recall a period with only close buttons.)&lt;/p&gt;
    &lt;p&gt;All that blue space you could keep filling with more "blades" as you clicked on things until the entire page started scrolling horizontally to switch between "blades". Almost everything you could click opened in a new blade rather than in place in the existing blade. (Like having "Open in New Window" as your browser default.)&lt;/p&gt;
    &lt;p&gt;It was trying to merge the needs of a configurable Dashboard and a "multi-window experience". You could save collections of blades (a bit like Niri workspaces) as named Dashboards. Overall it was somewhere between overkill and underthought.&lt;/p&gt;
    &lt;p&gt;(Also someone reminded me that many "blades" still somewhat exist in the modern Portal, because, of course, Microsoft backwards compatibility. Some of the pages are just "maximized Blades" and you can accidentally unmaximize them and start horizontally scrolling into new blades.)&lt;/p&gt;
    &lt;p&gt;azure likes to open new sections on the same tab / page as opposed to reloading or opening a new page / tab (overlays? modals? I'm lost on graphic terms)&lt;/p&gt;
    &lt;p&gt;depending on the resource you're accessing, you can get 5+ sections each with their own ui/ux on the same page/tab and it can be confusing to understand where you're at in your resources&lt;/p&gt;
    &lt;p&gt;if you're having trouble visualizing it, imagine an url where each new level is a different application with its own ui/ux and purpose all on the same webpage&lt;/p&gt;
    &lt;p&gt;AWS' UI is similarly messy, and to this day. They regularly remove useful data from the UI, or change stuff like the default sort order of database snapshots from last created to initial instance created date.&lt;/p&gt;
    &lt;p&gt;I never understood why a clear and consistent UI and improved UX isn't more of a priority for the big three cloud providers. Even though you talk mostly via platform SDK's, I would consider better UI especially initially, a good way to bind new customers and pick your platform over others.&lt;/p&gt;
    &lt;p&gt;I guess with their bottom line they don't need it (or cynically, you don't want to learn and invest in another cloud if you did it once).&lt;/p&gt;
    &lt;p&gt;It‚Äôs more than just the UI itself (which is horrible), it‚Äôs the whole thing that is very hostile to new users even if they‚Äôre experienced. It‚Äôs such an incoherent mess. The UI, the product names, the entire product line itself, with seemingly overlapping or competing products‚Ä¶ and now it‚Äôs AI this and AI that. If you don‚Äôt know exactly what you‚Äôre looking for, good luck finding it. It‚Äôs like they‚Äôre deliberately trying to make things as confusing as possible.&lt;/p&gt;
    &lt;p&gt;For some reason this applies to all AWS, GCP and Azure. Seems like the result of dozens of acquisitions.&lt;/p&gt;
    &lt;p&gt;I still find it much easier to just self host than learn cloud and I‚Äôve tried a few times but it just seems overly complex for the sake of complexity. It seems they tie in all their services to jack up charges, eg. I came for S3 but now I‚Äôm paying for 5 other things just to get it working.&lt;/p&gt;
    &lt;p&gt;Any time something is that unintuitive to get started, I automatically assume that if I encounter a problem that I‚Äôll be unable to solve it. That thought alone leads me to bounce every time.&lt;/p&gt;
    &lt;p&gt;100% agree. I've been working in the industry for almost 20 years, I'm a full stack developer and I manage my servers. I've tried signing up for AWS and I noped out.&lt;/p&gt;
    &lt;p&gt;AWS Is a complete mess. Everything is obscured behind other products, and they're all named in the most confusing way possible.&lt;/p&gt;
    &lt;p&gt;I know for some people the prospect of losing their Google Cloud access due to an automated terms of service violation on some completely unrelated service is worrisome.&lt;/p&gt;
    &lt;p&gt;I'd hope you can create a Google Cloud account under a completely different email address, but I do as little business with Google as I can get away with, so I have no idea.&lt;/p&gt;
    &lt;p&gt;The problem is that in some industries, Microsoft is the only option. Many of these regulated industries are just now transitioning from the data center to the cloud, and they've barely managed to get approval for that with all of the Microsoft history in their organization. AWS or GCloud are complete non-starters.&lt;/p&gt;
    &lt;p&gt;I moved a 100% MS shop to AWS circa 2015. We ran our DCs on EC2 instances just as if they were on prem. At some point we installed the AAD connector and bridged some stuff to Azure for office/mail/etc., but it was all effectively in AWS. We were selling software to banks so we had a lot of due diligence to suffer. AWS Artifact did much of the heavy lifting for us. We started with Amazon's compliance documentation and provided our own feedback on top where needed.&lt;/p&gt;
    &lt;p&gt;I feel like compliance is the entire point of using these cloud providers. You get a huge head start. Maintaining something like PCI-DSS when you own the real estate is a much bigger headache than if it's hosted in a provider who is already compliant up through the physical/hardware/networking layers. Getting application-layer checkboxes ticked off is trivial compared to "oops we forgot to hire an armed security team". I just took a look and there are currently 316 certifications and attestations listed under my account.&lt;/p&gt;
    &lt;p&gt;I've found that lift and shifting to EC2 is also generally cheaper than the equivalent VMs on Azure.&lt;/p&gt;
    &lt;p&gt;Microsoft really wants you to use their PaaS offerings, and so things on Azure are priced accordingly. A Microsoft shop just wanting to lift-and-shift, Azure isn't the best choice unless the org has that "nobody ever got fired for buying Microsoft" attitude.&lt;/p&gt;
    &lt;p&gt;Microsoft is better at regulatory capture, so Azure has many customers in the public sector. So an Azure outage probably affects the public sector more (see example above about trains).&lt;/p&gt;
    &lt;p&gt;What Amazon, Azure, and Google are showing with their platform crashes amid layoffs, while they supports governments that are Oppressing's Citizens and Ignoring the Law, is that they do not care about anything other than the bottom line.&lt;/p&gt;
    &lt;p&gt;They think they have the market captured, but I think what their dwindling quality and ethics are really going to drive is adoption of self hosting, distributed computing frameworks. Nerds are the ones who drove adoption of these platforms, and we can eventually end if we put in the work.&lt;/p&gt;
    &lt;p&gt;Seriously with container technology, and a bit more work / adoption on distributed compute systems and file storage (IPFS,FileCoin) there is a future where we dont have to use big brothers compute platform. Fuck these guys.&lt;/p&gt;
    &lt;p&gt;These were my thoughts exactly. I may have my tinfoil hat on, but outages these close together between the largest cloud providers amid social unrest, my wonder is the government / tech companies implementing some update that adds additional spyware / blackout functionality.&lt;/p&gt;
    &lt;p&gt;I really hope this pushes the internet back to how it used to be, self hosted, privacy, anonymity. I truly hope that's where we're headed, but the masses seem to just want to stay comfortable as long as their show is on TV&lt;/p&gt;
    &lt;p&gt;At least some bits of it do. I was writing something to pull logs the other day and the redirect was to an azure bucket. It also returned a 401 with the valid temporary authed redirect in the header. I was a bit worried I'd found a massive security hole but it appears after some testing it just returned the wrong status code.&lt;/p&gt;
    &lt;p&gt;Personally I am thinking more and more about hetzner, yes I know its not an apples to orange comparison. But its honestly so good&lt;/p&gt;
    &lt;p&gt;Someone had created a video where they showed the underlying hardware etc., I am wondering if there is something like https://vpspricetracker.com/ but with geek-benchmarks as well.&lt;/p&gt;
    &lt;p&gt;This video was affiliated with scalahosting but still I don't think that there was too much bias of them and they showed at around 3:37 a graph comparison with prices https://www.youtube.com/watch?v=9dvuBH2Pc1g&lt;/p&gt;
    &lt;p&gt;Now it shows how contabo has better hardware but I am pretty sure that there might be some other issues, and honestly I feel a sense of trust with hetzner I am not sure about others.&lt;/p&gt;
    &lt;p&gt;Either hetzner or self hosting stuff personally or just having a very cheap vps and going to hetzner if need be but hetzner already is pretty cheap or I might use some free service that I know of are good as well.&lt;/p&gt;
    &lt;p&gt;One of recent (4 months ago) Cloudflare outages (I think it was even workers) was caused by Google Cloud being down and Cloudflare hosting an essential service there&lt;/p&gt;
    &lt;p&gt;Hm it seemed that they hosted a critical service for cloudflare kv on google itself, but I wonder about the update.&lt;/p&gt;
    &lt;p&gt;Personally I just trust cloudflare more than google, given how their focus is on security whereas google feels googly...&lt;/p&gt;
    &lt;p&gt;I have heard some good things about google cloud run and the google's interface feels the best out of AWS,Azure,GCloud but I still would just prefer cloudflare/hetzner iirc&lt;/p&gt;
    &lt;p&gt;Another question: Has there ever been a list of all major cloud outages, like I am interested how many times google cloud and all cloud providers went majorly down I guess y'know? is there a website/git project that tracks this?&lt;/p&gt;
    &lt;p&gt;IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door. But it meant that when the system came back online, if the customers card was denied, the customer got free groceries.&lt;/p&gt;
    &lt;p&gt;Yea, good old store and forward. We implemented it in our PoS system. Now, we do non PCI integrations so we arn't in PCI scope, but depending on the processor, it can come with some limitations. Like, you can do store and forward, but only up to X number of transactions. I think for one integration, it's 500-ish store wide (it uses a local gateway that store and forwards to the processors gateway). The other integration we have, its 250, but store and forward on device, per device.&lt;/p&gt;
    &lt;p&gt;IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door.&lt;/p&gt;
    &lt;p&gt;Chick-fil-a has this.&lt;/p&gt;
    &lt;p&gt;One of the tech people there was on HN a few years ago describing their system. Credit card approval slows down the line, so the cards are automatically "approved" at the terminal, and the transaction is added to a queue.&lt;/p&gt;
    &lt;p&gt;The loss from fraudulent transactions turns out to be less than the loss from customers choosing another restaurant because of the speed of the lines.&lt;/p&gt;
    &lt;p&gt;I remember that banks will try to honor the transactions, even if the customer's balance/credit limit is exhausted. It doesn't apply only to some gift cards.&lt;/p&gt;
    &lt;p&gt;There's a Family Dollar by my house that is down at least 2 full days per month because of bad inet connectivity. I live close enough that with a small tower on my roof i can get line of sight to theirs. I've thought about offering them a backup link off my home inet if they give me 50% of sales whenever its in use. It would be a pretty good deal for them, better some sales when their inet is down vs none.&lt;/p&gt;
    &lt;p&gt;It's Family Dollar, margin has to be almost nothing and sales per day is probably &amp;lt; $1k. That's why I said 50% of sales and not profit.&lt;/p&gt;
    &lt;p&gt;I go there daily because it's a nice 30min round trip walk and I wfh. I go up there to get a diet coke or something else just to get out of the house. It amazes me when i see a handwritten sign on the door "closed, system is down". I've gotten to know the cashiers so I asked and it's because the internet connection goes down all the time. That store has to one of the most poorly run things i've ever seen yet it stays in business somehow.&lt;/p&gt;
    &lt;p&gt;I think the point people are trying and failing to make is that asking for half of means sales is half of revenue not half of net and that you‚Äôre out of your goddamned mind if you think a store with razor thin margins would sell at a massive loss rather than just close due to connectivity problems.&lt;/p&gt;
    &lt;p&gt;Your responses imply that you think people are questioning whether you would lose money on the deal while we are instead saying you‚Äôll get laughed out of the store, or possibly asked never to come back.&lt;/p&gt;
    &lt;p&gt;2-3%, bit higher on perishables. Though i'd just ask lump sum payments in cash since it likely has to no go through corporate (as in, avoid the corporation).&lt;/p&gt;
    &lt;p&gt;You'd think any SeriousBusiness would have a backup way to take customers' money. This is the one thing you always want to be able to do: accept payment. If they made it so they can't do that, they deserve the hit to their revenue. People should just walk out of the store with the goods if they're not being charged.&lt;/p&gt;
    &lt;p&gt;Why doesn't someone in the store at least have one of those manual kachunk-kachunk carbon copy card readers in the back that they can resuscitate for a few days until the technology is turned back on? Did they throw them all away?&lt;/p&gt;
    &lt;p&gt;I think a lot of payment terminals have an option to record transactions offline and upload them later, but apparently it's not enabled by default - probably because it increases your risk that someone pays with a bad card.&lt;/p&gt;
    &lt;p&gt;If they used standalone merchant terminals, then those typically use the local LAN which can rollover to cellular or PoT in the event of a network outage. The store can process a card transaction with the merchant terminal and then reconcile with the end of day chit. This article from 2008 describes their PoS https://www.retailtouchpoints.com/topics/store-operations/ca...&lt;/p&gt;
    &lt;p&gt;The kachunk-kachunk credit card machines need raised digits on the cards, and I don't think most banks have been issuing those for years at this point. Mine have been smooth for at least 10 years.&lt;/p&gt;
    &lt;p&gt;Just to add - this particular supermarket wasn‚Äôt fully down, it took ages for them to press ‚Äúsub total‚Äù and then pick the payment method. I suspect it was slow waiting for a request to timeout perhaps&lt;/p&gt;
    &lt;p&gt;I remember last mechanical cash registers in my country in 90s and when these got replaced by early electronic ones with blue vacuum fluorescent tubes. Then everything got smaller and smaller. Now I'm pestered to "add the item to the cart" by software.&lt;/p&gt;
    &lt;p&gt;Last week I couldn't pay for flowers for grandma's grave because smartphone-sized card terminal refused to work - it stuck on charging-booting loop so I had to get cash. Tho my partner thinks she actually wanted to get cash without a receipt for herself excluding taxes&lt;/p&gt;
    &lt;p&gt;You can, but it's all about risk mitigation. Most processors have some form of store and forward (and it can have limitations like only X number of transactions). Some even have controls to limit the amount you can store-and-forward (for instance, only charges under $50). But ultimately, it's still risk mitigation. You can store-and-forward, but you're trusting that the card/account has the funds. If it doesn't, you loose and ain't shit you can do about it. If you can't tolerate any risk, you don't turn on store and forward systems and then you can't process cards offline.&lt;/p&gt;
    &lt;p&gt;Its not the we are not capable. Its, is the business willing to assume the risk?&lt;/p&gt;
    &lt;p&gt;Currently standing in a half closed supermarket because the tills are down and they cant take payments&lt;/p&gt;
    &lt;p&gt;There's a fairly large supermarket near me that has both kinds of outages.&lt;/p&gt;
    &lt;p&gt;Occasionally it can't take cards because the (fiber? cable?) internet is down, so it's cash only.&lt;/p&gt;
    &lt;p&gt;Occasionally it can't take cash because the safe has its own cellular connection, and the cell tower is down.&lt;/p&gt;
    &lt;p&gt;I was at Frank's Pizza in downtown Houston a few weeks ago and they were giving slices of pizza away because the POS terminal died, and nobody knew enough math to take cash. I tried to give them a $10 and told them to keep the change, but "keep the change" is an unknown phrase these days. They simply couldn't wrap their brains around it. But hey, free pizza!&lt;/p&gt;
    &lt;p&gt;I‚Äôve been migrating our services off of Azure slowly for the past couple of years. The last internet facing things remaining are a static assets bucket and an analytics VM running Matomo. Working with Front Door has been an abysmal experience, and today was the push I needed to finally migrate our assets to Cloudflare.&lt;/p&gt;
    &lt;p&gt;I feel pretty justified in my previous decisions to move away from Azure. Using it feels like building on quicksand‚Ä¶&lt;/p&gt;
    &lt;p&gt;We are very dependent on Azure and Microsoft Authentication and Microsoft 365 and haven‚Äôt had weekly or even monthly issues. I can think of maybe three issues this year.&lt;/p&gt;
    &lt;p&gt;Pretty much every single Microsoft domain I've tried to access loads for a looooong time before giving me some bare html. I wonder if someone can explain why that's happening.&lt;/p&gt;
    &lt;p&gt;And querying https://www.microsoft.com/ results in HTTP 200 on the root document, but the page elements return errors (a 504 on the .css/.js documents, a 404 on some fonts, Name Not Resolved on scripts.clarity.ms, Connection Timed Out on wcpstatic.microsoft.com and mem.gfx.ms). That many different kinds of errors is actually kind of impressive.&lt;/p&gt;
    &lt;p&gt;I'm gonna say this was a networking/routing issue. The CDN stayed up, but everything else non-CDN became unroutable, and different requests traveled through different paths/services, but each eventually hit the bad network path, and that's what created all the different responses. Could also have been a bad deploy or a service stopped running and there's different things trying to access that service in different ways, leading to the weird responses... but that wouldn't explain the failed DNS propagation.&lt;/p&gt;
    &lt;p&gt;We‚Äôre 100% on Azure but so far there‚Äôs no impact for us.&lt;/p&gt;
    &lt;p&gt;Luckily, we moved off Azure Front Door about a year ago. We‚Äôd had three major incidents tied to Front Door and stopped treating it as a reliable CDN.&lt;/p&gt;
    &lt;p&gt;They weren‚Äôt global outages, more like issues triggered by new deployments. In one case, our homepage suddenly showed a huge Microsoft banner about a ‚Äúpost-quantum encryption algorithm‚Äù or something along those lines.&lt;/p&gt;
    &lt;p&gt;Kinda wild that a company that big can be so shaky on a CDN, which should be rock solid.&lt;/p&gt;
    &lt;p&gt;We battled https://learn.microsoft.com/en-us/answers/questions/1331370/... for over a year, and finally decided to move off since there was no any resolution. Unfortunately our API servers were still behind AFD so they were affected by today's stuff...&lt;/p&gt;
    &lt;p&gt;The paradox of cloud provider crashes is that if the provider goes down and takes the whole world with it, it's actually good advertisement. Because, that means so many things rely on it, it's critically important, and has so many big customers. That might be why Amazon stock went up after AWS crash.&lt;/p&gt;
    &lt;p&gt;If Azure goes down and nobody feels it, does Azure really matter?&lt;/p&gt;
    &lt;p&gt;People feel it, but usually not general consumers like they do when AWS goes down.&lt;/p&gt;
    &lt;p&gt;If Azure goes down, it's mostly affecting internal stuff at big old enterprises. Jane in accounting might notice, but the customers don't. Contrast with AWS which runs most of the world's SaaS products.&lt;/p&gt;
    &lt;p&gt;People not being able to do their jobs internally for a day tends not to make headlines like "100 popular internet services down for everyone" does.&lt;/p&gt;
    &lt;p&gt;They added a message at the same time as your comment:&lt;/p&gt;
    &lt;p&gt;"We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly."&lt;/p&gt;
    &lt;p&gt;Yeah just took down the prod site for one of our clients since we host the front-end out of their CDN. Just got wrapped up panic hosting it somewhere else for the past hour, very quickly reminds you about the pain of cookies...&lt;/p&gt;
    &lt;p&gt;I was having issues a few hours ago. I'm now able to access the portal, although I get lots of errors in the browser console, and things are loading slowly. I have services in the US-East region.&lt;/p&gt;
    &lt;p&gt;I have been having issues with GitHub and the winget tool for updates throughout the day as well. I imagine things are pulling from the same locations on Azure for some of the software I needed to update (NPM dependencies, and some .NET tooling).&lt;/p&gt;
    &lt;p&gt;I've been doing it since 1998 in my bedroom with a dual T1 (and on to real DCs later). While I've had some outages for sure it makes me feel better I am not that divergent in uptime in the long run vs big clouds.&lt;/p&gt;
    &lt;p&gt;Pretty much all Azure services seem to be down. Their status page says it's only the portal since 16:00. It would be nice if these mega-companies could update their status page when they take down a large fraction of the Internet and thousands of services that use them.&lt;/p&gt;
    &lt;p&gt;Same playbook for AWS. When they admitted that Dynamo was inaccessible, they failed to provide context that their internal services are heavily dependent on Dynamo&lt;/p&gt;
    &lt;p&gt;It's only after the fact they are transparent about the impact&lt;/p&gt;
    &lt;p&gt;The Internet is supposed to be decentralized. The big three seem to have all the power now (Amazon, Microsoft, and Google) plus Cloudflare/Oracle.&lt;/p&gt;
    &lt;p&gt;How did we get here? Is it because of scale? Going to market in minutes by using someone else's computers instead of building out your own, like co-location or dedicated servers, like back in the day.&lt;/p&gt;
    &lt;p&gt;A lot of money and years of marketing the cloud as the responsible business decision led us here. Now that the cloud providers have vendor lock-in, few will leave, and customers will continue to wildly overpay for cloud services.&lt;/p&gt;
    &lt;p&gt;Not sure how the current situation is better. Being stranded with no way whatsoever to access most/all of your services sounds way more terrifying than regular issues limited to a couple of services at a time&lt;/p&gt;
    &lt;p&gt;&amp;gt; no way whatsoever to access most/all of your services&lt;/p&gt;
    &lt;p&gt;I work on a product hosted on Azure. That's not the case. Except for front door, everything else is running fine. (Front door is a reverse proxy for static web sites.)&lt;/p&gt;
    &lt;p&gt;The product itself (an iot stormwater management system) is running, but our customers just can't access the website. If they need to do something, they can go out to the sites or call us and we can "rub two sticks together" and bypass the website. (We could also bypass front door if someone twisted our arms.)&lt;/p&gt;
    &lt;p&gt;Most customers only look at the website a few times a year.&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;That being said, our biggest point of failure is a completely different iot vendor who you probably won't hear about on Hacker News when they, or their data networks, have downtime.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Big Tech lobbying is riding the EU‚Äôs deregulation wave by spending more, hiring more, and pushing more, according to a new report by NGO‚Äôs Corporate Europe Observatory and LobbyControl on Wednesday (29 October).&lt;/p&gt;
    &lt;p&gt;&amp;gt; Based on data from the EU‚Äôs transparency register, the NGOs found that tech companies spend the most on lobbying of any sector, spending ‚Ç¨151m a year on lobbying ‚Äî a 33 percent increase from ‚Ç¨113m in 2023.&lt;/p&gt;
    &lt;p&gt;Gee whizz, I really do wonder how they end up having all the power!&lt;/p&gt;
    &lt;p&gt;I think the response lies in the surrounding ecosystem.&lt;/p&gt;
    &lt;p&gt;If you have a company it's easier to scale your team if you use AWS (or any other established ecosystem). It's way easier to hire 10 engineers that are competent with AWS tools than it is to hire 10 engineers that are competent with the IBM tools.&lt;/p&gt;
    &lt;p&gt;And from the individuals perspective it also make sense to bet on larger platforms. If you want to increase your odds of getting a new job, learning the AWS tools gives you a better ROI than learning the IBM tools.&lt;/p&gt;
    &lt;p&gt;But the cloud compute market is basically centralized into 2.5 companies at this point. The point of paying companies like Azure here is that they've in theory centralized the knowledge and know-how of running multiple, distributed datacenters, so as to be resilient.&lt;/p&gt;
    &lt;p&gt;But that we keep seeing outages encompassing more than a failure domain, then it should be fair game for engineers / customers to ask "what am I paying for, again?"&lt;/p&gt;
    &lt;p&gt;Moreover, this seems to be a classic case of large barriers to entry (the huge capital costs associated with building out a datacenter) barring new entrants into the market, coupled with "nobody ever got fired for buying IBM" level thinking. Are outages like these truly factored into the napkin math that says externalizing this is worth it?&lt;/p&gt;
    &lt;p&gt;Consolidation is the inevitable outcome of free unregulated markets.&lt;/p&gt;
    &lt;p&gt;In our highly interconnected world, decentralization paradoxically requires a central authority to enforce decentralization by restricting M&amp;amp;A, cartels, etc.&lt;/p&gt;
    &lt;p&gt;A natural monopoly is a monopoly in an industry in which high infrastructure costs and other barriers to entry relative to the size of the market give the largest supplier in an industry, often the first supplier in a market, an overwhelming advantage over potential competitors. Specifically, an industry is a natural monopoly if a single firm can supply the entire market at a lower long-run average cost than if multiple firms were to operate within it. In that case, it is very probable that a company (monopoly) or a minimal number of companies (oligopoly) will form, providing all or most of the relevant products and/or services.&lt;/p&gt;
    &lt;p&gt;There's no way to tell, and after about 30 minutes, the release process on VS Code Marketplace failed with a cryptic message: "Repository signing for extension file failed.". And there's no way to restart/resume it.&lt;/p&gt;
    &lt;p&gt;For us, it looks like most services are still working (eastus and eastus2). Our AKS cluster is still running and taking requests. Failures seem limited to management portal.&lt;/p&gt;
    &lt;p&gt;High availability is touted as a reason for their high prices, but I swear I read about major cloud outages far more than I experience any outages at Hetzner.&lt;/p&gt;
    &lt;p&gt;I think the biggest features of the big cloud vendors is that when they are down, not only you but your customers and your competitors usually have issues at the same time so everybody just shrug and have a lazy/off day at the same time. Even on call teams reall just have to wait and stay on standby because there is very little they can do. Doing a failover can be slower than waiting for the recovery, not help at all if outage is spanned accross several region, or bring aditional risks.&lt;/p&gt;
    &lt;p&gt;And more importantly nobody lose any reputation except AWS/Azure/Google.&lt;/p&gt;
    &lt;p&gt;The real reason is that outages are not your fault. Its the new version of "nobody ever got fired for buying IBM" - later it became MS, and now its any big cloud provider.&lt;/p&gt;
    &lt;p&gt;For one it‚Äôs statistics - Hetzner simply runs far fewer major services than hyperscalers. And the services they run are also more affluent, with larger customer bases, so downtimes are systemically critical. Therefore it‚Äôs louder.&lt;/p&gt;
    &lt;p&gt;On the merits though, I agree, haven‚Äôt had any serious issues with Hetzner.&lt;/p&gt;
    &lt;p&gt;DO has been shockingly reliable for me. I shut down a neglected box almost 900 days uptime the other day. In that time AWS has randomly dropped many of my boxes with no warning requiring a manual stop/start action to recover them... But everybody keeps telling me that DO isn't "as reliable" as the big three are.&lt;/p&gt;
    &lt;p&gt;To be fair, in the AWS/Azure outages, I don't think any individual (already created) boxes went down, either. In AWS' case you couldn't start up new EC2 instances, and presumably same for Azure (unless you bypass the management portal, I guess). And obviously services like DynamoDB and Front Door, respectively, went down. Hetzner/DO don't offer those, right? Or at least they're not very popular.&lt;/p&gt;
    &lt;p&gt;Nope, more than the portal. For instance, I just searched for "Azure Front Door" because I hadn't heard of it before (I now know it's a CDN), and neither the product page itself [1] nor the technical docs [2] are coming up for me.&lt;/p&gt;
    &lt;p&gt;we use front door (as does miccrosoft.com) and our website was down, I was able to change the DNS records to point directly to our server and will leave it like that for a few hours until everything is green&lt;/p&gt;
    &lt;p&gt;Seeing users having issues with the "Modern Outlook", specifically empty accounts. Switching back to the "Legacy Outlook" which functions largely without the help of the cloud fixes the issue. How ironic.&lt;/p&gt;
    &lt;p&gt;Do Microsoft still say "If the government has a broader voluntary national security program to gather customer data, we don't participate in it" today (which PRISM proved very false), or are they at least acknowledging they're participating in whatever NSA has deployed today?&lt;/p&gt;
    &lt;p&gt;PRISM wasn't voluntary. Also there are 3 levels here:&lt;/p&gt;
    &lt;p&gt;1. Mandatory&lt;/p&gt;
    &lt;p&gt;2. "Voluntary"&lt;/p&gt;
    &lt;p&gt;3. Voluntary&lt;/p&gt;
    &lt;p&gt;And I suspect that very little of what the NSA does falls into category 3. As Sen Chuck Schumer put it "you take on the intelligence community, they have six ways from Sunday at getting back at you"&lt;/p&gt;
    &lt;p&gt;This is funny but also possibly true because: business/MBA types see these outages as a way to prove how critical some services are, leading to investors deciding to load up on the vendor's stock.&lt;/p&gt;
    &lt;p&gt;I may or may not have been known to temporarily take a database down in the past to make a point to management about how unreliable some old software is.&lt;/p&gt;
    &lt;p&gt;The sad thing is - $MSFT isn't even down by 1%. And IIRC, $AMZN actually went up during their previous outage.&lt;/p&gt;
    &lt;p&gt;So if we look at these companies' bottom lines, all those big wigs are actually doing something right. Sales and lobbying capacity is way more effective than reliability or good engineering (at least in the short term).&lt;/p&gt;
    &lt;p&gt;I think he was implying that those companies think they are so important that it doesnt matter they are down, they wont loose any customers over it because they are too big and important.&lt;/p&gt;
    &lt;p&gt;I looked into this before and the stocks of these large corps simply does not move when outages happens. Maybe intra-day, I don't have that data, but in general no effect.&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;----&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:18 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;"We‚Äôre investigating an issue impacting Azure Front Door services. Customers may experience intermittent request failures or latency. Updates will be provided shortly."&lt;/p&gt;
    &lt;p&gt;They admit in their update blurb azure front door is having issues but still report azure front door as having no issues on their status page.&lt;/p&gt;
    &lt;p&gt;And it's very clear from these updates that they're more focused on the portal than the product, their updates haven't even mentioned fixing it yet, just moving off of it, as if it's some third party service that's down.&lt;/p&gt;
    &lt;p&gt;Unsubstantiated idea: So the support contract likely says there is a window between each reporting step and the status page is the last one and the one in the legal documents giving them several more hours before the clauses trigger.&lt;/p&gt;
    &lt;p&gt;Part of this outage involves outlook hanging and then blaming random addins. Pretty terrible practice by Microsoft to blame random vendors for their own outage.&lt;/p&gt;
    &lt;p&gt;Portal and Azure CDN are down here in the SF Bay Area. Tenant azureedge.net DNS A queries are taking 2-6 seconds and most often return nothing. I got a couple successful A response in the last 10 minutes.&lt;/p&gt;
    &lt;p&gt;Edit: As of 9:19 AM Pacific time, I'm now getting successful A responses but they can take several seconds. The web server at that address is not responding.&lt;/p&gt;
    &lt;p&gt;Azure goes down all the time. On Friday we had an entire regional service down all day. Two weeks ago same thing different region. You only hear about it when it's something everyone uses like the portal, because in general nobody uses Azure unless they're held hostage.&lt;/p&gt;
    &lt;p&gt;‚Äú Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025‚Äù&lt;/p&gt;
    &lt;p&gt;On our end, our VMs are still working, so our gitlab instance is still up. Our services using Azure App Services are available through their provided url. However, Front Door is failing to resolve any domains that it was responsible for.&lt;/p&gt;
    &lt;p&gt;I'd say DNS/Front Door (or some carrier interconnect) is the thing affected, since I can auth just fine in a few places. (I'm at MS, but not looped into anything operational these days, so I'm checking my personal subscription).&lt;/p&gt;
    &lt;p&gt;It begs the question from a noob like me... Where should they host the status page? Surely it shouldn't be on the same infra that it's supposed to be monitoring. Am I correct in thinking that?&lt;/p&gt;
    &lt;p&gt;Thank you. I was wondering what was going on at a company whose web app I need to access. I just checked with BuiltWith and it seems they are on Azure.&lt;/p&gt;
    &lt;p&gt;SSO is down, Azure Portal Down and more, seems like a major outage. Already a lot of services seem to be affected: banks, airlines, consumer apps, etc.&lt;/p&gt;
    &lt;p&gt;The portal is up for me and their status page confirms they did a failover for it. Definitely not disputing that its reach is wide, but a lot of smaller setups probably aren't using Front Door.&lt;/p&gt;
    &lt;p&gt;Looks like MyGet is impacted too. Seems like they use Azure:&lt;/p&gt;
    &lt;p&gt;&amp;gt;What is required to be able to use MyGet? ... MyGet runs its operations from the Microsoft Azure in the West Europe region, near Amsterdam, the Netherlands.&lt;/p&gt;
    &lt;p&gt;It is much more than azure. One of my kids needs a key for their laptop and can't reach that either. Great excuse though, 'Azure ate my homework'. What a ridiculous world we are building. Fuck MS and their account requirements for windows.&lt;/p&gt;
    &lt;p&gt;Does (should, could) DownDetector also say what customer-facing services are down, when some infrastructure is unworking? Or is that the info that the malefactors are seeking?&lt;/p&gt;
    &lt;p&gt;I absolutely love the utility aspect of LLMs but part of me is curious if moving faster by using AI is going to make these sorts of failure more and more often.&lt;/p&gt;
    &lt;p&gt;That said, I don't hear about GCP outages all that often. I do think AWS might be leading in outages, but that's a gut feeling, I didn't look up numbers.&lt;/p&gt;
    &lt;p&gt;I don't think it's meant to be serious. It's a comment on Microsoft laying off their staff and stuffing their Azure and Dotnet teams with AI product managers.&lt;/p&gt;
    &lt;p&gt;Unable to access the portal and any hit to SSO for other corporate accesses is also broken. Seems like there's something wrong in their Identity services.&lt;/p&gt;
    &lt;p&gt;Could be DNS, I'm seeing SERVFAIL trying to resolve what look to be MS servers when I'm hitting (just one example) mygoodtogo.com (trying to pay a road toll bill, and failing).&lt;/p&gt;
    &lt;p&gt;Apologies, but this just reads like a low effort critique of big things.&lt;/p&gt;
    &lt;p&gt;To be clear, they should get criticism. They should be held liable for any damage they cause.&lt;/p&gt;
    &lt;p&gt;But that they remain the biggest cloud offering out there isn't something you'd expect to change from a few outages that, by most all evidence, potential replacements have, as well? More, a lot of the outages potential replacements have are often more global in nature.&lt;/p&gt;
    &lt;p&gt;Yeah, I have non prod environments that don't use FD that are functioning. Routing through FD does not work. And a different app, nonprod doesn't use FD (and is working) but loads assets from the CDN (which is not working).&lt;/p&gt;
    &lt;p&gt;FD and CDN are global resources and are experiencing issues. Probably some other global resources as well.&lt;/p&gt;
    &lt;p&gt;Hate to say it, but DNS is looking like it's still the undisputed champ.&lt;/p&gt;
    &lt;p&gt;downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?&lt;/p&gt;
    &lt;p&gt;downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?&lt;/p&gt;
    &lt;p&gt;winget upgrade fabric Failed in attempting to update the source: winget An unexpected error occurred while executing the command: InternetOpenUrl() failed. 0x80072ee7 : unknown error&lt;/p&gt;
    &lt;p&gt;HTTPSConnectionPool(host='schemas.xmlsoap.org', port=443): Max retries exceeded with url: /soap/encoding/ (Caused by SSLError(CertificateError("hostname 'schemas.xmlsoap.org' doesn't match '*.azureedge.net'")))&lt;/p&gt;
    &lt;p&gt;A service we rely on that isn't even running on Azure is inaccessible due to this issue. For an asset that probably never changes. Wild for that to be the SPOF.&lt;/p&gt;
    &lt;p&gt;Yeah the graph for that one looks exactly the same shape. I wonder if they were depending on some azure component somehow, or maybe there were things hosted on both and the azure failure made enough things failover to AWS that AWS couldn't cope? If that was the case I'd expect to see something similar with GCP too though.&lt;/p&gt;
    &lt;p&gt;Edit: nope looks like there's actually a spike on GCP as well&lt;/p&gt;
    &lt;p&gt;Definitely also a strong possibility. I wish I had paid more attention during the AWS one earlier to see what other things looked like on there at the time.&lt;/p&gt;
    &lt;p&gt;When you look at the scale of the reports, you find they are much lower than Azure's. seeing a bunch of 24-hour sparkline type graphs next to each other can make it look like they are equally impacted, but AWS has 500 reports and Azure has 20,000. The scale is hidden by the choice of graph.&lt;/p&gt;
    &lt;p&gt;In other words, people reporting outages at AWS are probably having trouble with microsoft-run DNS services or caching proxies. It's not that the issues aren't there, it's that the internet is full of intermingled complexity. Just that amount of organic false-positives can make it look like an unrelated major service is impacted.&lt;/p&gt;
    &lt;p&gt;I know how to fix this but this community is too close minded and argumentative egocentric sensitive pedantic threatened angry etc to bother discussing it&lt;/p&gt;
    &lt;p&gt;As of now Azure Status page still shows no incident. It must be manually updated, someone has to actively decide to acknowledge an issue, and they're just... not. It undermines confidence in that status page.&lt;/p&gt;
    &lt;p&gt;I noticed issues on Azure so I went to the status page. It said everything was fine even though the Azure Portal was down. It took more than 10 minutes for that status page to update.&lt;/p&gt;
    &lt;p&gt;How can one of the richest companies in the world not offer a better service?&lt;/p&gt;
    &lt;p&gt;My best guess at the moment is something global like the CDN is having problems affecting things everywhere. I'm able to use a legacy application we have that goes directly to resources in uswest3, but I'm not able to use our more modern application which uses APIM/CDN networks at all.&lt;/p&gt;
    &lt;p&gt;From Azure status page: "Customers can consider implementing failover strategies with Azure Traffic Manager, to fail over from Azure Front Door to your origins".&lt;/p&gt;
    &lt;p&gt;I especially like how Nadella speaks of layoffs as some kind of uncontrollable natural disaster, like a hurricane, caused by no-one in particular. A kind of "God works in mysterious ways".&lt;/p&gt;
    &lt;p&gt;&amp;gt; ‚ÄúMicrosoft is being recognized and rewarded at levels never seen before,‚Äù Nadella wrote. ‚ÄúAnd yet, at the same time, we‚Äôve undergone layoffs. This is the enigma of success in an industry that has no franchise value.‚Äù &amp;gt; Nadella explained the disconnect between thriving financials and layoffs by stating that ‚Äúprogress isn‚Äôt linear‚Äù and that it is ‚Äúsometimes dissonant, and always demanding.‚Äù&lt;/p&gt;
    &lt;p&gt;I've read the whole memo and it's actually worse than those excerpts. Nadella doesn't even claim these were low performers:&lt;/p&gt;
    &lt;p&gt;&amp;gt; These decisions are among the most difficult we have to make. They affect people we‚Äôve worked alongside, learned from, and shared countless moments with‚Äîour colleagues, teammates, and friends.&lt;/p&gt;
    &lt;p&gt;Ok, so Microsoft is thriving, these were friends and people "we've learned from", but they must go because... uh... "progress isn't linear". Well, thanks Nadella! That explains so much!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45748661"/><published>2025-10-29T16:01:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748725</id><title>Composer: Building a fast frontier model with RL</title><updated>2025-10-29T22:39:14.598435+00:00</updated><content>&lt;doc fingerprint="3d5aedd9e03a0a1a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Composer: Building a fast frontier model with RL&lt;/head&gt;
    &lt;p&gt;Composer is our new agent model designed for software engineering intelligence and speed. On our benchmarks, the model achieves frontier coding results with generation speed four times faster than similar models.&lt;/p&gt;
    &lt;p&gt;We achieve these results by training the model to complete real-world software engineering challenges in large codebases. During training, Composer is given access to a set of production search and editing tools and tasked with efficiently solving a diverse range of difficult problems. The final result is a large-scale model optimized for high-speed use as an agent in Cursor.&lt;/p&gt;
    &lt;p&gt;Our motivation comes from our experience developing Cursor Tab, our custom completion model. We found that often developers want the smartest model that can support interactive use, keeping them in the flow of coding. In our development process, we experimented with a prototype agent model, codenamed Cheetah, to better understand the impact of faster agent models. Composer is a smarter version of this model that keeps coding delightful by being fast enough for an interactive experience.&lt;/p&gt;
    &lt;p&gt;Composer is a mixture-of-experts (MoE) language model supporting long-context generation and understanding. It is specialized for software engineering through reinforcement learning (RL) in a diverse range of development environments. At each iteration of training, the model is given a problem description and instructed to produce the best response, be it a code edit, a plan, or an informative answer. The model has access to simple tools, like reading and editing files, and also more powerful ones like terminal commands and codebase-wide semantic search.&lt;/p&gt;
    &lt;p&gt;To measure progress, we constructed an evaluation that measures a model's usefulness to a software developer as faithfully as possible. Our benchmark, Cursor Bench, consists of real agent requests from engineers and researchers at Cursor, along with hand-curated optimal solutions to these requests. The resulting evaluation measures not just the agent‚Äôs correctness, but also its adherence to a codebase's existing abstractions and software engineering practices.&lt;/p&gt;
    &lt;p&gt;Reinforcement learning allows us to actively specialize the model for effective software engineering. Since response speed is a critical component for interactive development, we incentivize the model to make efficient choices in tool use and to maximize parallelism whenever possible. In addition, we train the model to be a helpful assistant by minimizing unnecessary responses and claims made without evidence. We also find that during RL, the model learns useful behaviors on its own like performing complex searches, fixing linter errors, and writing and executing unit tests.&lt;/p&gt;
    &lt;p&gt;Efficient training of large MoE models requires significant investment into building infrastructure and systems research. We built custom training infrastructure leveraging PyTorch and Ray to power asynchronous reinforcement learning at scale. We natively train our models at low precision by combining our MXFP8 MoE kernels with expert parallelism and hybrid sharded data parallelism, allowing us to scale training to thousands of NVIDIA GPUs with minimal communication cost. Additionally, training with MXFP8 allows us to deliver faster inference speeds without requiring post-training quantization.&lt;/p&gt;
    &lt;p&gt;During RL, we want our model to be able to call any tool in the Cursor Agent harness. These tools allow editing code, using semantic search, grepping strings, and running terminal commands. At our scale, teaching the model to effectively call these tools requires running hundreds of thousands of concurrent sandboxed coding environments in the cloud. To support this workload, we adapted existing infrastructure we built for Background Agents, rewriting our virtual machine scheduler to support the bursty nature and scale of training runs. This enabled seamless unification of RL environments with production environments.&lt;/p&gt;
    &lt;p&gt;Cursor builds tools for software engineering, and we make heavy use of the tools we develop. A motivation of Composer development has been developing an agent we would reach for in our own work. In recent weeks, we have found that many of our colleagues were using Composer for their day-to-day software development. With this release, we hope that you also find it to be a valuable tool.&lt;/p&gt;
    &lt;p&gt;‚Äî&lt;/p&gt;
    &lt;p&gt;¬π Benchmarked on an internal benchmark in the Cursor tool harness. We group models into classes based on score and report the best model in each class. "Fast Frontier" includes models designed for efficient inference such as Haiku 4.5 and Gemini Flash 2.5. "Best Open" includes recent open weight model releases such as Qwen Coder and GLM 4.6. "Frontier 7/2025" is the best model available in July of this year. "Best Frontier" includes GPT-5 and Sonnet 4.5, which both outperform Composer. For the Tokens per Second calculation, tokens are standardized across models to the latest Anthropic tokenizer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cursor.com/blog/composer"/><published>2025-10-29T16:04:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45748879</id><title>Minecraft removing obfuscation in Java Edition</title><updated>2025-10-29T22:39:13.835115+00:00</updated><content>&lt;doc fingerprint="370203ca08b7cced"&gt;
  &lt;main&gt;
    &lt;p&gt;Do you like to mod Java, tinker with builds, or take deep dives into Minecraft‚Äôs code? Then this article is for you!&lt;/p&gt;
    &lt;p&gt;For a long time, Java Edition has used obfuscation (hiding parts of the code) ‚Äì a common practice in the gaming industry. Now we‚Äôre changing how we ship Minecraft: Java Edition to remove obfuscation completely. We hope that, with this change, we can pave a future for Minecraft: Java Edition where it‚Äôs easier to create, update, and debug mods.&lt;/p&gt;
    &lt;head rend="h2"&gt;An obfuscated history&lt;/head&gt;
    &lt;p&gt;Minecraft: Java Edition has been obfuscated since its release. This obfuscation meant that people couldn‚Äôt see our source code. Instead, everything was scrambled ‚Äì and those who wanted to mod Java Edition had to try and piece together what every class and function in the code did.&lt;/p&gt;
    &lt;p&gt;But we encourage people to get creative both in Minecraft and with Minecraft ‚Äì so in 2019 we tried to make this tedious process a little easier by releasing ‚Äúobfuscation mappings‚Äù. These mappings were essentially a long list that allowed people to match the obfuscated terms to un-obfuscated terms. This alleviated the issue a little, as modders didn‚Äôt need to puzzle out what everything did, or what it should be called anymore. But why stop there?&lt;/p&gt;
    &lt;head rend="h2"&gt;Removing obfuscation in Java Edition&lt;/head&gt;
    &lt;p&gt;To make things even easier ‚Äì and remove these intermediary steps ‚Äì we‚Äôre removing obfuscation altogether! Starting with the first snapshot following the complete Mounts of Mayhem launch, we will no longer obfuscate Minecraft: Java Edition. This means that this build (and all future builds) will have all of our original names* ‚Äì now with variable names and other names ‚Äì included by default to make modding even easier.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.minecraft.net/en-us/article/removing-obfuscation-in-java-edition"/><published>2025-10-29T16:12:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749017</id><title>Tailscale Peer Relays</title><updated>2025-10-29T22:39:13.668508+00:00</updated><content>&lt;doc fingerprint="66f1d873751076c5"&gt;
  &lt;main&gt;
    &lt;p&gt;Tailscale Peer Relays provides a customer-deployed and managed traffic relaying mechanism. By advertising itself as a peer relay, a Tailscale node can relay traffic for any peer nodes on the tailnet, even for traffic bound to itself. Tailscale Peer Relays can only relay traffic for nodes on your tailnet, and only for nodes that have access to the peer relay. Because they‚Äôre managed entirely by the customer, peer relays are less throughput-constrained than Tailscale‚Äôs managed DERP relays, and can provide higher throughput connections for traffic to and from locked-down cloud infrastructure, or behind strict network firewalls.&lt;/p&gt;
    &lt;p&gt;In testing with early design partners, we‚Äôve seen throughputs nearing that of a direct connection; often multiple orders of magnitude higher than Tailscale‚Äôs managed DERP fleet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moving past hard NAT&lt;/head&gt;
    &lt;p&gt;Over the past few weeks, you‚Äôve heard us talk about improvements we‚Äôve made to Network Address Translation (NAT) traversal techniques, so that Tailscale can establish direct connections wherever possible (hint: it‚Äôs over 90% of the time). However, we‚Äôve also outlined places where this isn‚Äôt possible or desirable today for a variety of reasons, especially in cloud environments. And, we‚Äôve postulated a bit about where we think the industry is going.&lt;/p&gt;
    &lt;p&gt;While we‚Äôve been keeping your network reliably connected for years with DERP, we‚Äôve heard from customers that the throughput and performance aspects of a QoS-aware managed relay fleet makes deployments in certain environments difficult or untenable. Furthermore, customers have noted that it‚Äôs non-trivial to deploy and manage custom DERP fleets (which run as a separate service and binary).&lt;/p&gt;
    &lt;p&gt;DERP provides an incredibly valuable service, setting up reliable connections between Tailscale clients anywhere in the world (including negotiating connections through peer relays). But often, DERP‚Äôs focus as a reliability and NAT traversal tool results in performance tradeoffs.&lt;/p&gt;
    &lt;p&gt;By contrast, Tailscale Peer Relays is designed as a performant connectivity tool, and can perform at a level rivaling direct connections. Peer relays can be placed right next to the resources they serve, and peer relays also run on top of UDP, both characteristics beneficial to lower latency and resource overhead. And, they are built into the Tailscale client itself for ease of deployment.&lt;/p&gt;
    &lt;p&gt;We want to move past even more hard NATs, and put Tailscale‚Äôs relaying technology in our customers‚Äô hands, so they can use Tailscale at scale, anywhere, with ease. We believe our new Tailscale Peer Relays connectivity option‚Äîunique to Tailscale‚Äîgives customers the best performance and flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;Peer relays are configured with a single UDP port that must be available to both sides of a connection. Tailscale Peer Relays is built right into the Tailscale client, and can be enabled with a simple command, using the &lt;code&gt;tailscale set --relay-server-port&lt;/code&gt; flag from the Tailscale CLI. Once enabled via the steps in our documentation, clients can connect to infrastructure in hard NAT environments over the peer relay.&lt;/p&gt;
    &lt;p&gt;And don‚Äôt worry, we still prefer to fly direct. Tailscale prefers direct connections wherever possible. Clients can then fall back to available peer relays, and finally leverage Tailscale‚Äôs managed DERP fleet, or any customer-deployed custom DERPs, to ensure you have connectivity wherever you need it. All of this traffic, over any connection, is still end-to-end encrypted via WireGuard¬Æ.&lt;/p&gt;
    &lt;p&gt;Tailscale Peer Relays is designed for the real world, based on the feedback we‚Äôve received from customers and our own hard-earned networking expertise. It allows customers to make just one firewall exception for connections only coming from their tailnet. Peer relays scale across regions, are resilient to real-world network conditions, and graciously fall back to DERP (Tailscale‚Äôs or custom). Your network maintains its shape, but gains all kinds of flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;Connectivity, everywhere, at warp speed&lt;/head&gt;
    &lt;p&gt;Customers can now maintain performance benchmarks even where direct connections aren‚Äôt possible, by enabling Tailscale Peer Relays to build a deterministic and high-throughput relay topology.&lt;/p&gt;
    &lt;p&gt;We‚Äôve had customers use peer relays to provide access into unmanaged networks, allowing their partners or customers to provide a controllable and auditable connectivity path without sacrificing performance.&lt;/p&gt;
    &lt;p&gt;In strict private networks, customers can build predictable access paths. Tailscale Peer Relays can be placed in public subnets with VPC peering to private subnets, allowing security teams to efficiently segment their private network infrastructure, while enabling networking teams to roll Tailscale out in full-mesh mode across the subnet.&lt;/p&gt;
    &lt;p&gt;Today, customers are using peer relays to establish relayed connections at near-direct speeds across a variety of environments and settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable high-throughput traffic through cloud NATs, like AWS Managed NAT Gateways: Applications and services behind a Managed NAT Gateway can leverage peer relays to relay traffic that can‚Äôt establish direct connections.&lt;/item&gt;
      &lt;item&gt;Relay through network firewalls: Workloads running in strictly firewalled environments can predictably expose a single UDP port, limiting the Tailscale surface area and fast-tracking the approval process for firewall exceptions.&lt;/item&gt;
      &lt;item&gt;Offload from Custom and Managed DERP: Minimize data-in-transit through Tailscale‚Äòs managed DERP network, and remove the need for customer-maintained DERP servers.&lt;/item&gt;
      &lt;item&gt;Provide access to locked down customer networks: Data plane traffic can be relayed through predictable endpoints in customer networks, so that they only need to open minimal numbers of ports to facilitate cross network connections.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;It‚Äôs not perfect, but we‚Äôre getting there&lt;/head&gt;
    &lt;p&gt;Tailscale Peer Relays is available today as a public beta. We‚Äôve yet to establish all the connectivity paths we want to, and there‚Äôs still visibility and debugging improvements to work through. However, we‚Äôve reliably seen our early design partners move to peer relay deployments with relative ease, and we‚Äôre ready for you to give it a try on your tailnet.&lt;/p&gt;
    &lt;p&gt;Tailscale Peer Relays can be enabled on all plans, including free (it‚Äôs our little way of working through the kinks of the modern internet with our customers). All customers can use two peer relays, for free, forever. As your needs scale, so will the number of available peer relays. To add even more peer relays to your tailnet, come have a chat with us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tailscale.com/blog/peer-relays-beta"/><published>2025-10-29T16:21:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749161</id><title>AOL to be sold to Bending Spoons for $1.5B</title><updated>2025-10-29T22:39:13.548354+00:00</updated><content/><link href="https://www.axios.com/2025/10/29/aol-bending-spoons-deal"/><published>2025-10-29T16:28:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45749690</id><title>Upwave (YC S12) is hiring software engineers</title><updated>2025-10-29T22:39:13.296899+00:00</updated><content>&lt;doc fingerprint="355c16590c952eaa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Senior Software Engineer&lt;/head&gt;
    &lt;p&gt;Upwave: The Brand Outcomes Measurement Platform&lt;/p&gt;
    &lt;p&gt;Upwave is a leading measurement company entirely focused on measuring and optimizing upper funnel campaigns.. The world‚Äôs leading advertisers, agencies, and media partners trust Upwave‚Äôs robust, AI-driven platform to bring science to the top of the funnel.&lt;/p&gt;
    &lt;p&gt;With Upwave, marketers maximize the effectiveness of brand spend. Upwave measures Brand Lift, validates Brand Reach, and surfaces Brand Optimization opportunities in one, dynamic platform with cross-channel brand measurement for CTV, Digital, Social, Linear, Addressable, Retail Media, Streaming Audio and more.&lt;/p&gt;
    &lt;p&gt;We‚Äôre a profitable, growth-stage company backed by leading venture investors (Y Combinator, Uncork Capital, Bloomberg Beta, Initialized Capital, PivotNorth, Ridge Ventures, Industry Ventures, Conductive Ventures,) and leading AdTechfounders &amp;amp; CEOs.&lt;/p&gt;
    &lt;p&gt;We‚Äôre a humble but ambitious team that takes its work seriously but never ourselves. Come join us.&lt;/p&gt;
    &lt;p&gt;As a Senior Software Engineer at Upwave, you‚Äôll be a full-stack problem solver with a backend focus‚Äîbuilding the APIs, data pipelines, and systems that power our brand measurement platform. Your work will process billions of ad impressions, manage complex data workflows, and deliver insights that inform marketing decisions for the world‚Äôs biggest brands.&lt;/p&gt;
    &lt;p&gt;You‚Äôll collaborate across engineering, product, and data science to ship high-impact features end-to-end, scale our platform for the next phase of growth, and help define the next generation of brand measurement.&lt;/p&gt;
    &lt;p&gt;What you will do:&lt;/p&gt;
    &lt;p&gt;Build AI-powered customer experiences ‚Äî integrate LLMs and advanced causal inference techniques into production workflows that automatically generate data visualizations, synthesize campaign performance into natural language insights, and help enterprise customers understand and optimize their advertising through our AI analyst "Bayes."&lt;/p&gt;
    &lt;p&gt;Design and build scalable backend systems ‚Äîdevelop microservices and RESTful APIs that power the analytics platform behind the world‚Äôs top brand campaigns.&lt;/p&gt;
    &lt;p&gt;Contribute across the stack ‚Äî work from backend APIs to Python analytics services to React frontends, delivering complete features that combine sophisticated data analysis with intuitive user experiences.&lt;/p&gt;
    &lt;p&gt;Engineer data pipelines at scale ‚Äî design and operate systems that process massive volumes of ad and survey data with MySQL, DynamoDB, and AWS (S3, Lambda, EMR, Kinesis Firehose).&lt;/p&gt;
    &lt;p&gt;Improve reliability and performance ‚Äî deploy services on Kubernetes and AWS, automate deployments via CI/CD, monitor with DataDog and Sentry, and continuously raise the bar for operational excellence&lt;/p&gt;
    &lt;p&gt;Collaborate deeply ‚Äî work closely with Product and Data Science to productionize statistical models, integrate advanced analytics into customer-facing tools, and bring cutting-edge AI capabilities to enterprise customers.&lt;/p&gt;
    &lt;p&gt;Deliver insights that move millions ‚Äî enable brand lift analytics and real-time campaign insights by building reliable, high-throughput systems. Multi-million dollar advertising decisions hinge on our recommendations.&lt;/p&gt;
    &lt;p&gt;About you:&lt;/p&gt;
    &lt;p&gt;You‚Äôre an experienced engineer (5+ years) who thrives on solving complex problems across APIs, data systems, and distributed infrastructure. You care about clean architecture, reliable systems, and measurable customer impact.&lt;/p&gt;
    &lt;p&gt;You‚Äôve built powerful, intuitive, API-driven products for professional users.. You‚Äôre comfortable across the stack, with experience in RDBMS-backed backends using Spring Boot, Django, Rails, or Express, and single-page frontends built in React, Vue, or Angular.&lt;/p&gt;
    &lt;p&gt;You understand and enjoy programming. You‚Äôre fluent in the modern landscape of UI frameworks, API and microservice architectures, databases, and cloud platforms‚Äîand know when to use the right tool for the job.&lt;/p&gt;
    &lt;p&gt;You embrace modern AI-powered development tools to move faster and code smarter. You use technologies like Claude Code, Cursor, and GitHub CoPilot to automate routine work, explore ideas quickly, and focus your time on higher-value system design and innovation.&lt;/p&gt;
    &lt;p&gt;You value structured software development practices‚Äîtesting, documentation, CI/CD, and code review‚Äîand care about building maintainable systems that scale.&lt;/p&gt;
    &lt;p&gt;You believe developers should operate what they build. You think about observability, cost, and reliability from day one, and design systems that are easy to deploy and maintain. You‚Äôve built in the cloud and know both its power and pitfalls.&lt;/p&gt;
    &lt;p&gt;You like turning ideas into tools that make real customers more effective. You collaborate closely with Product to design features that solve real-world problems and delight users.&lt;/p&gt;
    &lt;p&gt;You mentor others, share knowledge freely, and understand that healthy human systems are the foundation of healthy technical systems. Teammates look to you for guidance.&lt;/p&gt;
    &lt;p&gt;You want to understand how things work and why. You care more about the best idea winning than whose idea it is.&lt;/p&gt;
    &lt;p&gt;You take responsibility, move quickly to fix problems, and take pride in establishing areas of deep expertise in a fast-changing environment.&lt;/p&gt;
    &lt;p&gt;You believe high-trust, inclusive teams outperform individuals. You communicate clearly and compassionately, and contribute to a culture where people enjoy working together.&lt;/p&gt;
    &lt;p&gt;Bonus points:&lt;/p&gt;
    &lt;p&gt;Have worked with modern backend ecosystems like Java/Kotlin/Groovy (Spring Boot or Grails) and know how to design APIs that scale elegantly.&lt;/p&gt;
    &lt;p&gt;Are fluent with data systems such as MySQL, DynamoDB, and Presto, and understand the tradeoffs between relational and NoSQL storage.&lt;/p&gt;
    &lt;p&gt;Have built cloud-native applications on AWS, especially using Kubernetes and Terraform for automation and scalability.&lt;/p&gt;
    &lt;p&gt;Know your way around modern front-end frameworks like React/Redux and enjoy collaborating up and down the stack.&lt;/p&gt;
    &lt;p&gt;Have startup DNA‚Äîyou‚Äôre comfortable with ambiguity, iterate fast, and make pragmatic technical decisions.&lt;/p&gt;
    &lt;p&gt;Bring experience from AdTech, MarTech, or measurement platforms, or are excited to learn how AI and large-scale data intersect in this space.&lt;/p&gt;
    &lt;p&gt;Why You‚Äôll Like Working Here:&lt;/p&gt;
    &lt;p&gt;Engineering-first company: Upwave‚Äôs success depends on high-velocity innovation, and we believe high velocity comes from high efficiency, not high effort. We set priorities rather than deadlines, we don‚Äôt crunch, we work reasonable hours, and engineers actually take vacations.&lt;/p&gt;
    &lt;p&gt;Modern tech stack: Python analytics, Kotlin/Java APIs, event streaming (100k+ RPM), DynamoDB, Kubernetes, AWS, Terraform, LLM orchestration.&lt;/p&gt;
    &lt;p&gt;Impact at scale: Your code processes billions of advertising events and directly influences multi-million dollar decisions by Fortune 500 brands.&lt;/p&gt;
    &lt;p&gt;Autonomy and ownership: Our engineers lead projects from design through deployment and monitoring.&lt;/p&gt;
    &lt;p&gt;Ambitious but humble culture: We take our work seriously but never ourselves. Upwavers collaborate hard and support each other generously. We screen for people who are both exceptionally talented and genuinely kind.&lt;/p&gt;
    &lt;p&gt;Remote-first team: Our diverse team spans half the globe (but only one half, to ensure everyone can talk live when we need to). We balance synchronous core hours with flexibility to create a work environment that enables both deep collaboration and deep work.&lt;/p&gt;
    &lt;p&gt;Additional Information:&lt;/p&gt;
    &lt;p&gt;The annual base salary range for this role is $150,000 - $175,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for the new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits.&lt;/p&gt;
    &lt;p&gt;Upwave is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.upwave.com/job/8228849002/"/><published>2025-10-29T17:00:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45750425</id><title>OpenAI‚Äôs promise to stay in California helped clear the path for its IPO</title><updated>2025-10-29T22:39:13.117891+00:00</updated><content/><link href="https://www.wsj.com/tech/ai/openais-promise-to-stay-in-california-helped-clear-the-path-for-its-ipo-3af1c31c"/><published>2025-10-29T17:44:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45750501</id><title>Encoding x86 Instructions</title><updated>2025-10-29T22:39:12.088444+00:00</updated><content>&lt;doc fingerprint="fb91fb8d464572e0"&gt;
  &lt;main&gt;Source: CIS-77 Home
http://www.c-jump.com/CIS77/CIS77syllabus.htm&lt;head rend="h1"&gt;Encoding x86 Instructions &lt;list rend="ul"&gt;&lt;item&gt;It is time to take a look that the actual machine instruction format of the x86 CPU family. &lt;/item&gt;&lt;item&gt;They don't call the x86 CPU a Complex Instruction Set Computer (CISC) for nothing! &lt;/item&gt;&lt;item&gt;Although more complex instruction encodings exist, no one is going to challenge that the x86 has a complex instruction encoding! &lt;/item&gt;&lt;/list&gt; 1. x86 Instructions Overview &lt;div&gt;&lt;p&gt; x86 Instruction Encoding: &lt;/p&gt;&lt;/div&gt; Although the diagram seems to imply that instructions can be up to 16 bytes long, in actuality the x86 will not allow instructions greater than 15 bytes in length.&lt;/head&gt;&lt;p&gt; The prefix bytes are not the opcode expansion prefix discussed earlier - they are special bytes to modify the behavior of existing instructions. &lt;/p&gt;&lt;head rend="h2"&gt;2. x86 Instruction Format Reference&lt;/head&gt;&lt;div&gt;&lt;p&gt; Another view of the x86 instruction format: &lt;/p&gt;&lt;/div&gt;Additional reference:&lt;head rend="h2"&gt;3. x86 Opcode Sizes&lt;/head&gt; The x86 CPU supports two basic opcode sizes: &lt;list rend="ol"&gt;&lt;item&gt;standard one-byte opcode &lt;/item&gt;&lt;item&gt;two-byte opcode consisting of a 0Fh opcode expansion prefix byte. &lt;lb/&gt;The second byte then specifies the actual instruction. &lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;The x86 opcode bytes are 8-bit equivalents of iii field that we discussed in simplified encoding. &lt;/item&gt;&lt;item&gt;This provides for up to 512 different instruction classes, although the x86 does not yet use them all. &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;3.1. x86 ADD Instruction Opcode&lt;/head&gt;&lt;div&gt;&lt;p&gt; x86 ADD &lt;/p&gt;instruction opcode&lt;p&gt;:&lt;/p&gt;&lt;p&gt;Bit number one, marked d, specifies the direction of the data transfer: &lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;If d = 0 then the destination operand is a memory location, e.g. &lt;quote&gt; add [ebx], al&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;If d = 1 then the destination operand is a register, e.g. &lt;quote&gt; add al, [ebx]&lt;/quote&gt;&lt;/item&gt;&lt;/list&gt;&lt;/div&gt;Bit number zero marked s specifies
the size of the operands the ADD instruction operates upon:&lt;list rend="ul"&gt;&lt;item&gt;If s = 0 then the operands are 8-bit registers and memory locations. &lt;/item&gt;&lt;item&gt;If s = 1 then the operands are either 16-bits or 32-bits: &lt;list rend="ul"&gt;&lt;item&gt;Under 32-bit operating systems the default is 32-bit operands if s = 1. &lt;/item&gt;&lt;item&gt;To specify a 16-bit operand (under Windows or Linux) you must insert a special operand-size prefix byte in front of the instruction (example of this later.) &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;lb/&gt;You'll soon see that this direction bit d creates a
problem that results in one instruction have two different possible
opcodes.&lt;head rend="h2"&gt;4. Encoding x86 Instruction Operands, MOD-REG-R/M Byte&lt;/head&gt; The MOD-REG-R/M byte specifies instruction operands and their addressing mode(*): &lt;div&gt;&lt;p&gt; The MOD field specifies x86 addressing mode:&lt;/p&gt;&lt;table&gt;&lt;row style="background:#C0C0C0"&gt;&lt;cell role="head"&gt;MOD&lt;/cell&gt;&lt;cell role="head"&gt;Meaning &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;00&lt;/cell&gt;&lt;cell&gt;Register indirect addressing mode or SIB with no displacement (when R/M = 100) or Displacement only addressing mode (when R/M = 101). &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;01&lt;/cell&gt;&lt;cell&gt;One-byte signed displacement follows addressing mode byte(s). &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;10&lt;/cell&gt;&lt;cell&gt;Four-byte signed displacement follows addressing mode byte(s). &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;11&lt;/cell&gt;&lt;cell&gt;Register addressing mode. &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; The REG field specifies source or destination register:&lt;/p&gt;&lt;table&gt;&lt;row style="background:#C0C0C0"&gt;&lt;cell role="head"&gt;REG Value &lt;/cell&gt;&lt;cell role="head"&gt;Register if data size is eight bits &lt;/cell&gt;&lt;cell role="head"&gt;Register if data size is 16-bits &lt;/cell&gt;&lt;cell role="head"&gt;Register if data size is 32 bits &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;000&lt;/cell&gt;&lt;cell&gt;al&lt;/cell&gt;&lt;cell&gt;ax&lt;/cell&gt;&lt;cell&gt;eax &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;001&lt;/cell&gt;&lt;cell&gt;cl&lt;/cell&gt;&lt;cell&gt;cx&lt;/cell&gt;&lt;cell&gt;ecx &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;010&lt;/cell&gt;&lt;cell&gt;dl&lt;/cell&gt;&lt;cell&gt;dx&lt;/cell&gt;&lt;cell&gt;edx &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;011&lt;/cell&gt;&lt;cell&gt;bl&lt;/cell&gt;&lt;cell&gt;bx&lt;/cell&gt;&lt;cell&gt;ebx &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;100&lt;/cell&gt;&lt;cell&gt;ah&lt;/cell&gt;&lt;cell&gt;sp&lt;/cell&gt;&lt;cell&gt;esp &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;101&lt;/cell&gt;&lt;cell&gt;ch&lt;/cell&gt;&lt;cell&gt;bp&lt;/cell&gt;&lt;cell&gt;ebp &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;110&lt;/cell&gt;&lt;cell&gt;dh&lt;/cell&gt;&lt;cell&gt;si&lt;/cell&gt;&lt;cell&gt;esi &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;111&lt;/cell&gt;&lt;cell&gt;bh&lt;/cell&gt;&lt;cell&gt;di&lt;/cell&gt;&lt;cell&gt;edi &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/div&gt;&lt;p&gt; The R/M field, combined with MOD, specifies either &lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;the second operand in a two-operand instruction, or &lt;/item&gt;&lt;item&gt;the only operand in a single-operand instruction like NOT or NEG. &lt;/item&gt;&lt;/list&gt;The d bit in the opcode determines which operand is
the source, and which is the destination:&lt;list class="n" rend="ul"&gt;&lt;item&gt;d=0: MOD R/M &amp;lt;- REG, REG is the source &lt;/item&gt;&lt;item&gt;d=1: REG &amp;lt;- MOD R/M, REG is the destination &lt;/item&gt;&lt;/list&gt;&lt;lb/&gt;(*) Technically, registers do not have an
address, but we apply the term addressing mode to registers
nonetheless.&lt;head rend="h2"&gt;5. General-Purpose Registers&lt;/head&gt;&lt;div&gt;&lt;p&gt; Since the processor accesses registers more quickly than it accesses memory, you can make your programs run faster by keeping the most-frequently used data in registers. &lt;/p&gt;&lt;/div&gt;&lt;list rend="ul"&gt;&lt;item&gt;The EAX, EDX, ECX, EBX, EBP, EDI, and ESI registers are 32-bit general-purpose registers, used for temporary data storage and memory access. &lt;/item&gt;&lt;item&gt;The AX, DX, CX, BX, BP, DI, and SI registers are 16-bit equivalents of the above, they represent the low-order 16 bits of 32-bit registers. &lt;/item&gt;&lt;item&gt;The AH, DH, CH, and BH registers represent the high-order 8 bits of the corresponding registers. &lt;/item&gt;&lt;item&gt;Similarly, AL, DL, CL, and BL represent the low-order 8 bits of the registers. &lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;6. REG Field of the MOD-REG-R/M Byte&lt;/head&gt; See MOD-REG-R/M Byte.&lt;p&gt; Depending on the instruction, this can be either the source or the destination operand.&lt;/p&gt;&lt;p&gt; Many instructions have the d (direction) field in their opcode to choose REG operand role: &lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;If d=0, REG is the source, &lt;lb/&gt;MOD R/M &amp;lt;- REG. &lt;/item&gt;&lt;item&gt;If d=1, REG is the destination, &lt;lb/&gt;REG &amp;lt;- MOD R/M. &lt;/item&gt;&lt;/list&gt;&lt;lb/&gt;(*) For certain (often single-operand or
immediate-operand) instructions, the REG field may contain an
opcode extension rather than the register bits. The
R/M field will specify the operand in such case.&lt;head rend="h3"&gt;9. MOD R/M Byte and Addressing Modes &lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell valign="top"&gt;&lt;quote&gt; MOD R/M Addressing Mode === === ================================ 00 000 [ eax ] 01 000 [ eax + disp8 ] (1) 10 000 [ eax + disp32 ] 11 000 register ( al / ax / eax ) (2) 00 001 [ ecx ] 01 001 [ ecx + disp8 ] 10 001 [ ecx + disp32 ] 11 001 register ( cl / cx / ecx ) 00 010 [ edx ] 01 010 [ edx + disp8 ] 10 010 [ edx + disp32 ] 11 010 register ( dl / dx / edx ) 00 011 [ ebx ] 01 011 [ ebx + disp8 ] 10 011 [ ebx + disp32 ] 11 011 register ( bl / bx / ebx ) 00 100 SIB Mode (3) 01 100 SIB + disp8 Mode 10 100 SIB + disp32 Mode 11 100 register ( ah / sp / esp ) 00 101 32-bit Displacement-Only Mode (4) 01 101 [ ebp + disp8 ] 10 101 [ ebp + disp32 ] 11 101 register ( ch / bp / ebp ) 00 110 [ esi ] 01 110 [ esi + disp8 ] 10 110 [ esi + disp32 ] 11 110 register ( dh / si / esi ) 00 111 [ edi ] 01 111 [ edi + disp8 ] 10 111 [ edi + disp32 ] 11 111 register ( bh / di / edi ) &lt;cell valign="top"&gt;&lt;list rend="ol"&gt;&lt;item&gt;Addressing modes with 8-bit displacement fall in the range -128..+127 and require only a single byte displacement after the opcode (Faster!) &lt;/item&gt;&lt;item&gt;The size bit in the opcode specifies 8 or 32-bit register size. To select a 16-bit register requires a prefix byte. &lt;/item&gt;&lt;item&gt;The so-called scaled indexed addressing modes, SIB = scaled index byte mode. &lt;/item&gt;&lt;item&gt;Note that there is no [ ebp ] addressing. It's slot is occupied by the 32-bit displacement only addressing mode. Intel decided that programmers can use [ ebp+ disp8 ] addressing mode instead, with its 8-bit displacement set equal to zero (instruction is a little longer, though.) &lt;/item&gt;&lt;/list&gt;&lt;/cell&gt;&lt;/quote&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;8. SIB (Scaled Index Byte) Layout&lt;/head&gt;&lt;div&gt;&lt;p&gt; Scaled index byte layout:&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;table&gt;&lt;row style="background:#C0C0C0"&gt;&lt;cell role="head"&gt;Scale Value&lt;/cell&gt;&lt;cell role="head"&gt;Index*Scale Value &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;00&lt;/cell&gt;&lt;cell&gt;Index*1 &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;01&lt;/cell&gt;&lt;cell&gt;Index*2 &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;10&lt;/cell&gt;&lt;cell&gt;Index*4 &lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell align="center"&gt;11&lt;/cell&gt;&lt;cell&gt;Index*8 &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/cell&gt;&lt;cell&gt;&lt;table&gt;&lt;row style="background:#C0C0C0"&gt;&lt;cell role="head"&gt;Index&lt;/cell&gt;&lt;cell role="head"&gt;Register &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;000&lt;/cell&gt;&lt;cell&gt;EAX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;001&lt;/cell&gt;&lt;cell&gt;ECX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;010&lt;/cell&gt;&lt;cell&gt;EDX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;011&lt;/cell&gt;&lt;cell&gt;EBX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;100&lt;/cell&gt;&lt;cell&gt;Illegal &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;101&lt;/cell&gt;&lt;cell&gt;EBP &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;110&lt;/cell&gt;&lt;cell&gt;ESI &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;111&lt;/cell&gt;&lt;cell&gt;EDI &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/cell&gt;&lt;cell&gt;&lt;table&gt;&lt;row style="background:#C0C0C0"&gt;&lt;cell role="head"&gt;Base&lt;/cell&gt;&lt;cell role="head"&gt;MOD&lt;/cell&gt;&lt;cell role="head"&gt;Register &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;000&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;EAX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;001&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;ECX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;010&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;EDX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;011&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;EBX &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;100&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;ESP &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell rowspan="2"&gt;101&lt;/cell&gt;&lt;cell&gt;00&lt;/cell&gt;&lt;cell&gt;Displacement-only &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;01, 10&lt;/cell&gt;&lt;cell&gt;EBP &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;110&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;ESI &lt;/cell&gt;&lt;/row&gt;&lt;row align="center"&gt;&lt;cell&gt;111&lt;/cell&gt;&lt;cell&gt;xx&lt;/cell&gt;&lt;cell&gt;EDI &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/div&gt;&lt;list rend="ul"&gt;&lt;item&gt;Scaled indexed addressing mode uses the second byte (namely, SIB byte) that follows the MOD-REG-R/M byte in the instruction format. &lt;/item&gt;&lt;item&gt;The MOD field still specifies the displacement size of zero, one, or four bytes. &lt;list class="n" rend="ul"&gt;&lt;item&gt;The MOD-REG-R/M and SIB bytes are complex, because Intel reused 16-bit addressing circuitry in the 32-bit mode, rather than simply abandoning the 16-bit format in the 32-bit mode. &lt;/item&gt;&lt;item&gt;There are good hardware reasons for this, but the end result is a complex scheme for specifying addressing modes in the opcodes. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;8.1. Scaled Indexed Addressing Mode&lt;/head&gt;&lt;table&gt;&lt;row valign="top"&gt;&lt;cell&gt;&lt;quote&gt; [ reg32 + eax*n ] MOD = 00 [ reg32 + ebx*n ] [ reg32 + ecx*n ] [ reg32 + edx*n ] [ reg32 + ebp*n ] [ reg32 + esi*n ] [ reg32 + edi*n ] [ disp + reg8 + eax*n ] MOD = 01 [ disp + reg8 + ebx*n ] [ disp + reg8 + ecx*n ] [ disp + reg8 + edx*n ] [ disp + reg8 + ebp*n ] [ disp + reg8 + esi*n ] [ disp + reg8 + edi*n ] [ disp + reg32 + eax*n ] MOD = 10 [ disp + reg32 + ebx*n ] [ disp + reg32 + ecx*n ] [ disp + reg32 + edx*n ] [ disp + reg32 + ebp*n ] [ disp + reg32 + esi*n ] [ disp + reg32 + edi*n ] [ disp + eax*n ] MOD = 00, and [ disp + ebx*n ] BASE field = 101 [ disp + ecx*n ] [ disp + edx*n ] [ disp + ebp*n ] [ disp + esi*n ] [ disp + edi*n ] &lt;cell&gt; Note: n = 1, 2, 4, or 8.&lt;p&gt; In each scaled indexed addressing mode the MOD field in MOD-REG-R/M byte specifies the size of the displacement. It can be zero, one, or four bytes:&lt;/p&gt;&lt;quote&gt; MOD R/M Addressing Mode --- --- --------------------------- 00 100 SIB 01 100 SIB + disp8 10 100 SIB + disp32 &lt;/quote&gt; The Base and Index fields of the SIB byte select the base and index registers, respectively.&lt;p&gt; Note that this addressing mode does not allow the use of the ESP register as an index register. Presumably, Intel left this particular mode undefined to provide the ability to extend the addressing modes in a future version of the CPU. &lt;/p&gt;&lt;/cell&gt;&lt;/quote&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;9. Examples&lt;/head&gt;&lt;head rend="h3"&gt;9.1. Encoding ADD Instruction Example&lt;/head&gt;&lt;list class="n" rend="ul"&gt;&lt;item&gt;&lt;p&gt;The ADD opcode can be decimal 0, 1, 2, or 3, depending on the direction and size bits in the opcode: &lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;How could we encode various forms of the ADD instruction using different addressing modes? &lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;9.2 Encoding ADD CL, AL Instruction&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Interesting side effect of the direction bit and the MOD-REG-R/M byte organization: some instructions can have two different opcodes, and both are legal! &lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;For example, encoding of &lt;/p&gt;&lt;quote&gt; add cl, al &lt;/quote&gt;&lt;p&gt;could be 00 C1 (if d=0), or 02 C8, if d bit is set to 1. &lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The possibility of opcode duality issue here applies to all instructions with two register operands. &lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;9.3. Encoding ADD ECX, EAX Instruction&lt;/head&gt;&lt;head rend="h3"&gt;9.4. Encoding ADD EDX, DISPLACEMENT Instruction&lt;/head&gt;&lt;head rend="h3"&gt;9.5. Encoding ADD EDI, [EBX] Instruction&lt;/head&gt;&lt;head rend="h3"&gt;9.6. Encoding ADD EAX, [ ESI + disp8 ] Instruction&lt;/head&gt;&lt;head rend="h3"&gt;9.7. Encoding ADD EBX, [ EBP + disp32 ] Instruction&lt;/head&gt;&lt;list class="n" rend="ul"&gt;&lt;item&gt;&lt;p&gt;Encoding the ADD EBX, [ EBP + disp32 ] instruction: &lt;/p&gt;&lt;quote&gt; add ebx, [ ebp + disp32 ] &lt;/quote&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;9.8. Encoding ADD EBP, [ disp32 + EAX*1 ] Instruction&lt;/head&gt;&lt;list class="n" rend="ul"&gt;&lt;item&gt;&lt;p&gt;Encoding the ADD EBP, [ disp32 + EAX*1 ] Instruction &lt;/p&gt;&lt;quote&gt; add ebp, [ disp32 + eax*1 ] &lt;/quote&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;9.9. Encoding ADD ECX, [ EBX + EDI*4 ] Instruction&lt;/head&gt;&lt;head rend="h2"&gt;10. Encoding ADD Immediate Instruction&lt;/head&gt;&lt;div&gt;&lt;p&gt; Encoding x86 immediate operands: &lt;/p&gt;&lt;/div&gt;MOD-REG-R/M and SIB bytes have no
bit combinations to specify an immediate operand.&lt;p&gt; Instead, x86 uses a entirely different instruction format to specify instruction with an immediate operand.&lt;/p&gt;&lt;p&gt; There are three rules that apply: &lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;If opcode high-order bit set to 1, then instruction has an immediate constant. &lt;/item&gt;&lt;item&gt;There is no direction bit in the opcode: &lt;list class="n" rend="ul"&gt;&lt;item&gt;: indeed, you cannot specify a constant as a destination operand! &lt;/item&gt;&lt;item&gt;Therefore, destination operand is always the location encoded in the MOD-R/M bits of the the MOD-REG-R/M byte. &lt;/item&gt;&lt;item&gt;In place of the direction bit d, the opcode has a sign extension x bit instead: &lt;list rend="ul"&gt;&lt;item&gt;For 8-bit operands, the CPU ignores x bit. &lt;/item&gt;&lt;item&gt;For 16-bit and 32-bit operands, x bit specifies the size of the Constant following at the end of the instruction: &lt;list rend="ul"&gt;&lt;item&gt;If x bit contains zero, the Constant is the same size as the operand (i.e., 16 or 32 bits). &lt;/item&gt;&lt;item&gt;If x bit contains one, the Constant is a signed 8-bit value, and the CPU sign-extends this value to the appropriate size before adding it to the operand. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;This little x trick often makes programs shorter, because adding small-value constants to 16 or 32 bit operands is very common. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;The third difference between the ADD-immediate and the standard ADD instruction is the meaning of the REG field in the MOD-REG-R/M byte: &lt;list rend="ul"&gt;&lt;item&gt;Since the instruction implies that &lt;list rend="ul"&gt;&lt;item&gt;the source operand is a constant, and &lt;/item&gt;&lt;item&gt;MOD-R/M fields specify the destination operand, &lt;/item&gt;&lt;/list&gt; the instruction does not need to use the REG field to specify an operand. &lt;/item&gt;&lt;item&gt;Instead, the x86 CPU uses these three bits as an opcode extension. &lt;/item&gt;&lt;item&gt;For the ADD-immediate instruction the REG bits must contain zero. &lt;/item&gt;&lt;item&gt;Other bit patterns would correspond to a different instruction. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;quote&gt; Note that when adding a constant to a memory location, the displacement (if any) immediately precedes the immediate (constant) value in the opcode sequence. &lt;/quote&gt;&lt;head rend="h2"&gt;11. Encoding Eight, Sixteen, and Thirty-Two Bit Operands&lt;/head&gt;&lt;div&gt;&lt;p&gt; x86 ADD Opcode: &lt;/p&gt;&lt;/div&gt;&lt;list class="n" rend="ul"&gt;&lt;item&gt;When Intel designed the 8086, one bit in the opcode, s, selected between 8 and 16 bit integer operand sizes. &lt;/item&gt;&lt;item&gt;Later, when CPU added 32-bit integers to its architecture on 80386 chip, there was a problem: &lt;list class="n" rend="ul"&gt;&lt;item&gt;three encodings were needed to support 8, 16, and 32 bit sizes. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Solution was an operand size prefix byte. &lt;/item&gt;&lt;item&gt;Intel studied x86 instruction set and came to the conclusion: &lt;list class="n" rend="ul"&gt;&lt;item&gt;in a 32-bit environment, programs were more likely to use 8-bit and 32-bit operands far more often than 16-bit operands. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;So Intel decided to let the size bit s in the opcode select between 8- and 32-bit operands. &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;11.1. Encoding Sixteen Bit Operands&lt;/head&gt;&lt;div&gt;&lt;p&gt; x86 instruction format: &lt;/p&gt;&lt;/div&gt;32-bit programs don't use 16-bit operands that often, but they do
need them now and then.&lt;p&gt; To allow for 16-bit operands, Intel added prefix a 32-bit mode instruction with the operand size prefix byte with value 66h.&lt;/p&gt;&lt;p&gt; This prefix byte tells the CPU to operand on 16-bit data rather than 32-bit data. &lt;/p&gt;&lt;head rend="h2"&gt;12. x86 Instruction Prefix Bytes&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;x86 instruction can have up to 4 prefixes. &lt;/item&gt;&lt;item&gt;Each prefix adjusts interpretation of the opcode: &lt;list rend="ol"&gt;&lt;item&gt;Repeat/lock prefix byte guarantees that instruction will have exclusive use of all shared memory, until the instruction completes execution:&lt;quote&gt; F0h = LOCK&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;String manipulation instruction prefixes&lt;quote&gt; F3h = REP, REPE F2h = REPNE&lt;/quote&gt; where &lt;list rend="ul"&gt;&lt;item&gt;REP repeats instruction the number of times specified by iteration count ECX. &lt;/item&gt;&lt;item&gt;REPE and REPNE prefixes allow to terminate loop on the value of ZF CPU flag. &lt;/item&gt;&lt;/list&gt; Related string manipulation instructions are: &lt;list rend="ul"&gt;&lt;item&gt;MOVS, move string &lt;/item&gt;&lt;item&gt;STOS, store string &lt;/item&gt;&lt;item&gt;SCAS, scan string &lt;/item&gt;&lt;item&gt;CMPS, compare string, etc. &lt;/item&gt;&lt;/list&gt; See also string manipulation sample program: rep_movsb.asm &lt;/item&gt;&lt;item&gt;Segment override prefix causes memory access to use specified segment instead of default segment designated for instruction operand.&lt;quote&gt; 2Eh = CS 36h = SS 3Eh = DS 26h = ES 64h = FS 65h = GS&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;Operand override, 66h. Changes size of data expected by default mode of the instruction e.g. 16-bit to 32-bit and vice versa. &lt;/item&gt;&lt;item&gt;Address override, 67h. Changes size of address expected by the instruction. 32-bit address could switch to 16-bit and vice versa. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;13. Alternate Encodings for Instructions&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;To shorten program code, Intel created alternate (shorter) encodings of some very commonly used instructions. &lt;/item&gt;&lt;item&gt;For example, x86 provides a single byte opcode for&lt;quote&gt; add al, constant ; one-byte opcode and no MOD-REG-R/M byte add eax, constant ; one-byte opcode and no MOD-REG-R/M byte&lt;/quote&gt; the opcodes are 04h and 05h, respectively. Also, &lt;/item&gt;&lt;item&gt;These instructions are one byte shorter than their standard ADD immediate counterparts. &lt;/item&gt;&lt;item&gt;Note that&lt;quote&gt; add ax, constant ; operand size prefix byte + one-byte opcode, no MOD-REG-R/M byte&lt;/quote&gt; requires an operand size prefix just as a standard ADD AX, constant instruction, yet is still one byte shorter than the corresponding standard version of ADD immediate. &lt;/item&gt;&lt;item&gt;Any decent assembler will automatically choose the shortest possible instruction when translating program into machine code. &lt;/item&gt;&lt;item&gt;Intel only provides alternate encodings only for the accumulator registers AL, AX, EAX. &lt;/item&gt;&lt;item&gt;This is a good reason to use accumulator registers if you have a choice &lt;quote&gt; (also a good reason to take some time and study encodings of the x86 instructions.) &lt;/quote&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;14. x86 Opcode Summary&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;x86 opcodes are represented by one or two bytes. &lt;/item&gt;&lt;item&gt;Opcode could extend into unused bits of MOD-REG-R/M byte. &lt;/item&gt;&lt;item&gt;Opcode encodes information about &lt;list rend="ul"&gt;&lt;item&gt;operation type, &lt;/item&gt;&lt;item&gt;operands, &lt;/item&gt;&lt;item&gt;size of each operand, including the size of an immediate operand. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;14.1. MOD-REG-R/M Byte Summary&lt;/head&gt;&lt;div&gt;&lt;p&gt; MOD-REG-R/M Byte: &lt;/p&gt;&lt;/div&gt;&lt;list rend="ul"&gt;&lt;item&gt;MOD-REG-R/M byte follows one or two opcode bytes of the instruction &lt;/item&gt;&lt;item&gt;It provides addressing mode information for one or two operands. &lt;/item&gt;&lt;item&gt;If operand is in memory, or operand is a register: &lt;list rend="ul"&gt;&lt;item&gt;MOD field (bits [7:6]), combined with the R/M field (bits [2:0]), specify memory/register operand, as well as its addressing mode. &lt;/item&gt;&lt;item&gt;REG field (bits [5:3]) specifies another register operand in of the two-operand instruction. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;15. ISA Design Considerations&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Instruction set architecture design that can stand the test of time is a true intellectual challenge. &lt;/item&gt;&lt;item&gt;It takes several compromises between space and efficiency to assign opcodes and encode instruction formats. &lt;/item&gt;&lt;item&gt;Today people are using Intel x86 instruction set for purposes never intended by original designers. &lt;/item&gt;&lt;item&gt;Extending the CPU is a very difficult task. &lt;/item&gt;&lt;item&gt;The instruction set can become extremely complex. &lt;/item&gt;&lt;item&gt;If x86 CPU was designed from scratch today, it would have a totally different ISA! &lt;/item&gt;&lt;item&gt;Software developers usually don't have a problem adapting to a new architecture when writing new software... &lt;quote&gt; ...but they are very resistant to moving existing software from one platform to another. &lt;/quote&gt;&lt;/item&gt;&lt;item&gt;This is the primary reason the Intel x86 platform remains so popular to this day. &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;15.1. ISA Design Challenges &lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Allowing for future expansion of the chip requires some undefined opcodes. &lt;/item&gt;&lt;item&gt;From the beginning there should be a balance between the number of undefined opcodes and &lt;list rend="ol"&gt;&lt;item&gt;the number of initial instructions, and &lt;/item&gt;&lt;item&gt;the size of your opcodes (including special assignments.) &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Hard decisions: &lt;list rend="ul"&gt;&lt;item&gt;Reduce the number of instructions in the initial instruction set? &lt;/item&gt;&lt;item&gt;Increase the size of the opcode? &lt;/item&gt;&lt;item&gt;Rely on an opcode prefix byte(s), which makes later added instructions longer? &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;There are no easy answers to these challenges for CPU designers! &lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;16. Intel Architecture Software Developer's Manual&lt;/head&gt; Classic Intel Pentium II Architecture Software Developer's Manual contains three parts: &lt;list rend="ol"&gt;&lt;item&gt;Volume 1 , Intel Basic Architecture: Order Number 243190 , PDF, 2.6 MB. &lt;/item&gt;&lt;item&gt;Volume 2 , Instruction Set Reference: Order Number 243191 , PDF, 6.6 MB. &lt;/item&gt;&lt;item&gt;Volume 3 , System Programing Guide: Order Number 243192 , PDF, 5.1 MB. &lt;/item&gt;&lt;/list&gt;It is highly recommended that you download the above manuals and use them
as a reference.&lt;head rend="h3"&gt;16.1. Intel Instruction Set Reference (Volume2)&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Chapter 3 of the Instruction Set Reference describes &lt;list rend="ul"&gt;&lt;item&gt;each Intel instruction in detail &lt;/item&gt;&lt;item&gt;algorithmic description of each operation &lt;/item&gt;&lt;item&gt;effect on flags &lt;/item&gt;&lt;item&gt;operand(s), their sizes and attributes &lt;/item&gt;&lt;item&gt;CPU exceptions that may be generated. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;The instructions are arranged in alphabetical order. &lt;/item&gt;&lt;item&gt;Appendix A provides opcode map for the entire Intel Architecture instruction set. &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.2. Chapter 3 of Intel Instruction Set Reference&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Chapter 3 begins with instruction format example and explains the Opcode column encoding. &lt;/item&gt;&lt;item&gt;The Opcode column gives the complete machine codes as it is understood by the CPU. &lt;/item&gt;&lt;item&gt;When possible, the actual machine code bytes are given as exact hexadecimal bytes, in the same order in which they appear in memory. &lt;/item&gt;&lt;item&gt;However, there are opcode definitions other than hexadecimal bytes... &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.3. Intel Reference Opcode Bytes&lt;/head&gt;&lt;list class="n" rend="ul"&gt;&lt;item&gt;Fow example, &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.4. Intel Reference Opcode Bytes, Cont.&lt;/head&gt;&lt;head rend="h3"&gt;16.5. Intel Reference Opcode Bytes, Cont.&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;/r - Indicates that the instruction uses the Mod R/M byte of the instruction. &lt;/item&gt;&lt;item&gt;Mod R/M byte contains both &lt;list rend="ul"&gt;&lt;item&gt;a register operand reg and &lt;/item&gt;&lt;item&gt;an r/m (register or memory) operand. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.6. Intel Reference Opcode Bytes, Cont. &lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;cb, cw, cd, cp - A 1-byte (cb), 2-byte (cw), 4-byte (cd), or 6-byte (cp) value, &lt;lb/&gt;following the opcode, is used to specify &lt;list rend="ul"&gt;&lt;item&gt;a code offset, &lt;/item&gt;&lt;item&gt;and possibly a new value for the code segment register CS. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.7. Intel Reference Opcode Bytes, Cont.&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;ib, iw, id - A 1-byte (ib), 2-byte (iw), or 4-byte (id) indicates presence of the immediate operand in the instruction. &lt;/item&gt;&lt;item&gt;Typical order of opcode bytes is &lt;list rend="ul"&gt;&lt;item&gt;opcode &lt;/item&gt;&lt;item&gt;Mod R/M byte (optional) &lt;/item&gt;&lt;item&gt;SIB scale-indexing byte (optional) &lt;/item&gt;&lt;item&gt;immediate operand. &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;The opcode determines if the operand is a signed value. &lt;/item&gt;&lt;item&gt;All words and doublewords are given with the low-order byte first (little endian). &lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.8. Intel Reference Opcode Bytes, Cont.&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;+rb, +rw, +rd - A register code, from 0 through 7, added to the hexadecimal byte given at the left of the plus sign to form a single opcode byte. &lt;/item&gt;&lt;item&gt;Register Encodings Associated with the +rb, +rw, and +rd: &lt;p&gt;For example, &lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;16.9. Intel Reference Instruction Column&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;The Instruction column gives the syntax of the instruction statement as it would appear in a 386 Assembly program. &lt;/item&gt;&lt;item&gt;For example, &lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www-user.tu-chemnitz.de/~heha/hs/chm/x86.chm/x86.htm"/><published>2025-10-29T17:50:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45750875</id><title>The Internet Runs on Free and Open Source Software‚Äìand So Does the DNS</title><updated>2025-10-29T22:39:11.929348+00:00</updated><content>&lt;doc fingerprint="8ac2858004c20ddf"&gt;
  &lt;main&gt;
    &lt;p&gt;Free and open-source software (FOSS) is not merely common on the Internet; it is a deeply embedded and essential foundation of the Domain Name System (DNS), the backbone of how we connect online.&lt;/p&gt;
    &lt;p&gt;The ICANN Security and Stability Advisory Committee (SSAC) is pleased to announce the publication of SAC132: The Domain Name System Runs on Free and Open Source Software (FOSS).&lt;/p&gt;
    &lt;head rend="h3"&gt;Why This Matters Now&lt;/head&gt;
    &lt;p&gt;As governments around the world explore new cybersecurity regulations, the ubiquity of FOSS in DNS operations‚Äîfrom domain registration to retrieval‚Äîmeans that policy decisions made today will have direct implications for the Internet's security and resilience tomorrow. SAC132 provides timely, nontechnical guidance to ensure that new policy and regulation serve to strengthen, rather than inadvertently weaken, this critical infrastructure.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Insights for Policymakers&lt;/head&gt;
    &lt;p&gt;SAC132 is a foundational guide designed to empower policymakers to strategically manage and sustain the FOSS ecosystem. The report provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clear Foundations ‚Äì An accessible overview of the DNS and the FOSS development model for nontechnical audiences.&lt;/item&gt;
      &lt;item&gt;Policy Assessment ‚Äì Analysis of cybersecurity regulations in the United States, United Kingdom, and European Union, with a focus on how they account for FOSS in the DNS ecosystem.&lt;/item&gt;
      &lt;item&gt;Practical Guidance ‚Äì Concrete findings and recommendations to help policymakers support and secure FOSS as a cornerstone of global connectivity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We invite all policymakers, technical experts, and stakeholders to read the full report.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Call to Engage&lt;/head&gt;
    &lt;p&gt;By publishing SAC132, SSAC seeks to raise awareness of the indispensable role of FOSS in maintaining a secure, stable, and resilient Internet. We invite policymakers, technical experts, and all stakeholders to read the full report and join us in conversations about its findings.&lt;/p&gt;
    &lt;p&gt;You can engage with SSAC and the broader community at ICANN84, whether in Dublin or by participating remotely. Together, we can ensure that the FOSS ecosystem‚Äîand the Internet it supports‚Äîremains strong, sustainable, and open for all.&lt;/p&gt;
    &lt;p&gt;Finally, we thank all SSAC members and invited experts who contributed to this work, especially co-chairs Maarten Aertsen and Barry Leiba, for their leadership.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.icann.org/en/blogs/details/the-internet-runs-on-free-and-open-source-softwareand-so-does-the-dns-23-10-2025-en"/><published>2025-10-29T18:16:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45750954</id><title>Dithering ‚Äì Part 1</title><updated>2025-10-29T22:39:11.652431+00:00</updated><content>&lt;doc fingerprint="1336983308d69b52"&gt;
  &lt;main&gt;
    &lt;p&gt;Understanding how dithering works, visually.&lt;/p&gt;
    &lt;p&gt;tap/click the right side of the screen to go forward ‚Üí&lt;/p&gt;
    &lt;p&gt;I‚Äôve always been fascinated by the dithering effect. It has a unique charm that I find so appealing.&lt;/p&gt;
    &lt;p&gt;‚Üê tap/click the left side to go back&lt;/p&gt;
    &lt;p&gt;I was even more amazed when I learned how dithering works.&lt;/p&gt;
    &lt;p&gt;‚Üê or use arrow keys to navigate ‚Üí&lt;/p&gt;
    &lt;p&gt;Look closely, and you‚Äôll see this animation is made of alternating black and white pixels.&lt;/p&gt;
    &lt;p&gt;But these black and white pixels are specifically arranged to create the illusion of multiple shades.&lt;/p&gt;
    &lt;p&gt;That‚Äôs what dithering does: it simulates more color variations than what are actually used.&lt;/p&gt;
    &lt;p&gt;Here, it uses black and white to give the impression of multiple gray shades.&lt;/p&gt;
    &lt;p&gt;To me, dithering is about creating the most out of what we have, and that's what amazes me the most!&lt;/p&gt;
    &lt;p&gt;It inspired me to learn more about it, and now I want to share what I‚Äôve learned.&lt;/p&gt;
    &lt;p&gt;Please note that this is just part one out of three, so I‚Äôll only scratch the surface here.&lt;/p&gt;
    &lt;p&gt;I‚Äôll go deeper in the next parts, which will come soon. Stay tuned!&lt;/p&gt;
    &lt;p&gt;First, let‚Äôs explore the dithering basics with this grayscale image example.&lt;/p&gt;
    &lt;p&gt;A grayscale image has various gray shades, from black to white.&lt;/p&gt;
    &lt;p&gt;Imagine a display that only shows black or white pixels, no grays. We must turn some pixels black and others white‚Äîbut how?&lt;/p&gt;
    &lt;p&gt;One way is to map each pixel to the closest available color.&lt;/p&gt;
    &lt;p&gt;Pixels darker than medium gray turn black and lighter ones turn white.&lt;/p&gt;
    &lt;p&gt;This splits pixels into black or white groups.&lt;/p&gt;
    &lt;p&gt;However, this creates a harsh image with abrupt black-white transitions.&lt;/p&gt;
    &lt;p&gt;Shadow details vanish as gray pixels become fully black or white.&lt;/p&gt;
    &lt;p&gt;Dithering fixes this by selectively pushing some pixels towards the opposite color.&lt;/p&gt;
    &lt;p&gt;Some light gray pixels that are closer to white turn black.&lt;/p&gt;
    &lt;p&gt;Likewise, some dark grays turn white.&lt;/p&gt;
    &lt;p&gt;And it's done in a way that produces special patterns which simulate shades by varying the black-and-white pixel densities.&lt;/p&gt;
    &lt;p&gt;Denser black pixels are used in darker areas, while denser white pixels are used in lighter ones.&lt;/p&gt;
    &lt;p&gt;Next question: How are these patterns generated?&lt;/p&gt;
    &lt;p&gt;One simple dithering method, known as ordered dithering, uses a threshold map.&lt;/p&gt;
    &lt;p&gt;A threshold map is a grid of values representing brightness levels, from 0 (darkest) to 1 (brightest).&lt;/p&gt;
    &lt;p&gt;To dither, we compare each input pixel‚Äôs brightness to a corresponding threshold value.&lt;/p&gt;
    &lt;p&gt;If a pixel‚Äôs brightness exceeds the threshold (it‚Äôs brighter than the threshold), the pixel turns white. Otherwise, it turns black.&lt;/p&gt;
    &lt;p&gt;Repeating this for all pixels gives us the black-and-white dither patterns.&lt;/p&gt;
    &lt;p&gt;The threshold map is designed to output patterns where the black-and-white pixel density matches the input image‚Äôs shades.&lt;/p&gt;
    &lt;p&gt;So brighter input produces patterns with more white, while darker input produces more black.&lt;/p&gt;
    &lt;p&gt;These black-and-white density variations are what create the illusion of gray shades when viewed from a distance.&lt;/p&gt;
    &lt;p&gt;To dither larger images, we extend the threshold map to match the image size and follow the same principle:&lt;/p&gt;
    &lt;p&gt;Compare each pixel‚Äôs brightness to the threshold map, then turn it black or white accordingly.&lt;/p&gt;
    &lt;p&gt;The image now uses only two colors, but its overall appearance is preserved.&lt;/p&gt;
    &lt;p&gt;The variations in shades are now replaced by variations in black/white pixel density of the dithering patterns.&lt;/p&gt;
    &lt;p&gt;And that‚Äôs how dithering works in a nutshell: it replicates shades with fewer colors, which are strategically placed to maintain the original look.&lt;/p&gt;
    &lt;p&gt;I find it a bit ironic how I used to think dithering ‚Äòadds‚Äô a cool effect, when what it actually does is ‚Äòremove‚Äô colors!&lt;/p&gt;
    &lt;p&gt;That's all for now! We‚Äôve reached the end, but there‚Äôs still a lot more to explore.&lt;/p&gt;
    &lt;p&gt;For example, we haven‚Äôt explored the algorithm to create a threshold map. (spoiler: there are many ways!)&lt;/p&gt;
    &lt;p&gt;There‚Äôs also another algorithm called error diffusion, which doesn‚Äôt use a threshold map.&lt;/p&gt;
    &lt;p&gt;Each algorithm creates a distinct, unique look, which I believe deserves its own article.&lt;/p&gt;
    &lt;p&gt;And that's why I decided to break this series into three parts.&lt;/p&gt;
    &lt;p&gt;In the next part, I‚Äôll dive into various algorithms for creating threshold maps.&lt;/p&gt;
    &lt;p&gt;In the final part, I‚Äôll focus on the error diffusion algorithm.&lt;/p&gt;
    &lt;p&gt;We'll dive even deeper into dithering's mechanisms in these next 2 parts, so stay tuned!&lt;/p&gt;
    &lt;p&gt;Thank you for reading!&lt;/p&gt;
    &lt;p&gt;visualrambling.space is a personal project by Damar, someone who loves to learn about different topics and rambling about them visually.&lt;/p&gt;
    &lt;p&gt;If you like this kind of visual article, please consider following me on X/Twitter and sharing this with your friends.&lt;/p&gt;
    &lt;p&gt;I'll keep creating more visual articles like this!&lt;/p&gt;
    &lt;p&gt;https://x.com/damarberlari&lt;/p&gt;
    &lt;p&gt;_blank&lt;/p&gt;
    &lt;p&gt;Topics covered: Three.js, WebGL, dithering, visualization, interactive learning&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://visualrambling.space/dithering-part-1/"/><published>2025-10-29T18:21:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45750995</id><title>Extropic is building thermodynamic computing hardware</title><updated>2025-10-29T22:39:11.241913+00:00</updated><content>&lt;doc fingerprint="27b6766c759d22ac"&gt;
  &lt;main&gt;
    &lt;p&gt;Thermodynamic Computing: From 0 to 1 (Launch Video)&lt;/p&gt;
    &lt;p&gt;October 30th, 2025&lt;/p&gt;
    &lt;p&gt;Extropic is building thermodynamic computing hardware that is radically more energy efficient than GPUs.&lt;/p&gt;
    &lt;p&gt;Our thermodynamic sampling units (TSUs) are inherently probabilistic, the perfect fit for probabilistic AI workloads.&lt;/p&gt;
    &lt;p&gt;Hardware&lt;/p&gt;
    &lt;p&gt;prototype platform&lt;/p&gt;
    &lt;p&gt;XTR-0 enables the development of ultra-efficient AI algorithms by providing low-latency communication between Extropic chips and a traditional processor.&lt;/p&gt;
    &lt;p&gt;Software&lt;/p&gt;
    &lt;p&gt;Our open-source Python library that enables everyone to develop thermodynamic algorithms and simulate running them on TSUs&lt;/p&gt;
    &lt;p&gt;We are hiring engineers and scientists to help us pioneer a new form of computing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://extropic.ai/"/><published>2025-10-29T18:25:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45751400</id><title>Uv is the best thing to happen to the Python ecosystem in a decade</title><updated>2025-10-29T22:39:11.122811+00:00</updated><content>&lt;doc fingerprint="14982e51cdc48290"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;uv is the best thing to happen to the Python ecosystem in a decade&lt;/head&gt;
    &lt;p&gt;23 October 2025 | Reading time: 6 minutes&lt;/p&gt;
    &lt;p&gt;It‚Äôs 2025. Does installing Python, managing virtual environments, and synchronizing dependencies between your colleagues really have to be so difficult? Well‚Ä¶ no! A brilliant new tool called uv came out recently that revolutionizes how easy installing and using Python can be.&lt;/p&gt;
    &lt;p&gt;uv is a free, open-source tool built by Astral, a small startup that has been churning out Python tools (like the excellent linter Ruff) for the past few years. uv can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install any Python version for you&lt;/item&gt;
      &lt;item&gt;Install packages&lt;/item&gt;
      &lt;item&gt;Manage virtual environments&lt;/item&gt;
      &lt;item&gt;Solve dependency conflicts extremely quickly (very important for big projects.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What‚Äôs best is that it can do all of the above better than any other tool, in my opinion. It‚Äôs shockingly fast, written in Rust, and works on almost any operating system or platform.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installing uv&lt;/head&gt;
    &lt;p&gt;uv is straightforward to install. There are a few ways, but the easiest (in my opinion) is this one-liner command ‚Äî for Linux and Mac, it‚Äôs:&lt;/p&gt;
    &lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;
    &lt;p&gt;or on Windows in powershell:&lt;/p&gt;
    &lt;code&gt;powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"&lt;/code&gt;
    &lt;p&gt;You can then access uv with the command &lt;code&gt;uv&lt;/code&gt;. Installing uv will not mess up any of your existing Python installations ‚Äî it‚Äôs a separate tool, so it‚Äôs safe to install it just to try it out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Managing Python for a project&lt;/head&gt;
    &lt;p&gt;It‚Äôs always a good idea to work with virtual environments for any Python project. It keeps different bits of code and dependencies ringfenced from one another, and in my experience, it can save a lot of hassle to get into the habit of using virtual environments as soon as you can. uv naturally uses virtual environments, so it‚Äôs very easy to start using them if you get into using uv.&lt;/p&gt;
    &lt;p&gt;uv will build a Python environment for you based on what‚Äôs specified in a &lt;code&gt;pyproject.toml&lt;/code&gt; file in the directory (or parent directories) you‚Äôre working in. &lt;code&gt;pyproject.toml&lt;/code&gt; files are a standard, modern format for specifying dependencies for a Python project. A barebones one might look a bit like this:&lt;/p&gt;
    &lt;code&gt;[project]
name = "my_project"
version = "1.0.0"
requires-python = "&amp;gt;=3.9,&amp;lt;3.13"
dependencies = [
  "astropy&amp;gt;=5.0.0",
  "pandas&amp;gt;=1.0.0,&amp;lt;2.0",
]&lt;/code&gt;
    &lt;p&gt;In essence, it just has to specify which Python version to use and some dependencies. Adding a name and version number also aren‚Äôt a bad idea.&lt;/p&gt;
    &lt;p&gt;(Sidenote: for projects that you publish as packages, such as to the Python Package Index that pip and uv use, &lt;code&gt;pyproject.toml&lt;/code&gt; files are a modern way to specify everything you need to publish your package.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Making a new project with uv&lt;/head&gt;
    &lt;p&gt;To start a new Python project with uv, you can run&lt;/p&gt;
    &lt;code&gt;uv init&lt;/code&gt;
    &lt;p&gt;Which will create a new project for you, with a &lt;code&gt;pyproject.toml&lt;/code&gt;, a &lt;code&gt;README.md&lt;/code&gt;, and other important bits of boilerplate.&lt;/p&gt;
    &lt;p&gt;There are a lot of different ways to run this command, like &lt;code&gt;uv init --bare&lt;/code&gt; (which only creates a pyproject.toml), &lt;code&gt;uv init --package&lt;/code&gt; (which sets up a new Python package), and more. I recommend running &lt;code&gt;uv init --help&lt;/code&gt; to read about them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Once you have/if you already have a &lt;code&gt;pyproject.toml&lt;/code&gt; file&lt;/head&gt;
    &lt;p&gt;Once you initialize a project ‚Äî or if you already have a &lt;code&gt;pyproject.toml&lt;/code&gt; file in your project ‚Äî it‚Äôs very easy to start using uv. You just need to do&lt;/p&gt;
    &lt;code&gt;uv sync&lt;/code&gt;
    &lt;p&gt;in the directory that your &lt;code&gt;pyproject.toml&lt;/code&gt; file is in. This command (and in fact, most uv commands if you haven‚Äôt ran it already) will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Automatically install a valid version of Python&lt;/item&gt;
      &lt;item&gt;Install all dependencies to a new virtual environment in the directory &lt;code&gt;.venv&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Create a &lt;code&gt;uv.lock&lt;/code&gt;file in your directory, which saves the exact, platform-agnostic version of every package installed ‚Äî meaning that other colleagues can replicate your Python environment exactly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In principle, you can ‚Äòactivate‚Äô this new virtual environment like any typical virtual environment that you may have seen in other tools, but the most ‚Äòuv-onic‚Äô way to use uv is simply to prepend any command with &lt;code&gt;uv run&lt;/code&gt;. This command automatically picks up the correct virtual environment for you and runs your command with it. For instance, to run a script ‚Äî instead of&lt;/p&gt;
    &lt;code&gt;source .venv/bin/activate
python myscript.py&lt;/code&gt;
    &lt;p&gt;you can just do&lt;/p&gt;
    &lt;code&gt;uv run myscript.py&lt;/code&gt;
    &lt;p&gt;which will have the same effect. Likewise, to use a ‚Äòtool‚Äô like Jupyter Lab, you can just do&lt;/p&gt;
    &lt;code&gt;uv run jupyter lab&lt;/code&gt;
    &lt;p&gt;in your project‚Äôs directory, as opposed to first ‚Äòactivating‚Äô the environment and then running &lt;code&gt;jupyter lab&lt;/code&gt; separately.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adding dependencies&lt;/head&gt;
    &lt;p&gt;You can always just edit your &lt;code&gt;pyproject.toml&lt;/code&gt; file manually: uv will detect the changes and rebuild your project‚Äôs virtual environment. But uv also has easier ways to add dependencies ‚Äî you can just do&lt;/p&gt;
    &lt;code&gt;uv add numpy&amp;gt;=2.0&lt;/code&gt;
    &lt;p&gt;to add a package, including specifying version constraints (like the above.) This command automatically edits your &lt;code&gt;pyproject.toml&lt;/code&gt; for you. &lt;code&gt;uv add&lt;/code&gt; is also extremely powerful for adding remote dependencies from git or elsewhere on your computer (but I won‚Äôt get into that here.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Pinning a Python version&lt;/head&gt;
    &lt;p&gt;Finally, I think that one of the most useful things uv can do is to pin a specific Python version for your project. Doing&lt;/p&gt;
    &lt;code&gt;uv python pin 3.12.9&lt;/code&gt;
    &lt;p&gt;would pin the current project to exactly Python 3.12.9 for you, and anyone else using uv ‚Äî meaning that you really can replicate the exact same Python install across multiple machines.&lt;/p&gt;
    &lt;head rend="h2"&gt;uvx: ignore all of the above and just run a tool, now!&lt;/head&gt;
    &lt;p&gt;But sometimes, you might just want to run a tool quickly ‚Äî like using Ruff to lint code somewhere, or starting a Jupyter notebook server without an environment, or even just quickly starting an IPython session with pandas installed so you can open up a file. The &lt;code&gt;uv tool&lt;/code&gt; command, which has a short alias &lt;code&gt;uvx&lt;/code&gt;, makes this insanely easy. Running a command like&lt;/p&gt;
    &lt;code&gt;uvx ruff&lt;/code&gt;
    &lt;p&gt;will automatically download the tool you want to use and run it in a one-off virtual environment. Once the tool has been downloaded before, this is lightning-fast because of how uv uses caches.&lt;/p&gt;
    &lt;p&gt;There are a lot of occasions when I might want to do this ‚Äî a common one might be to quickly start an IPython session with pandas installed (using &lt;code&gt;--with&lt;/code&gt; to add dependencies) so that I can quickly open &amp;amp; look at a parquet file. For instance:&lt;/p&gt;
    &lt;code&gt;uvx --with pandas,pyarrow ipython&lt;/code&gt;
    &lt;p&gt;Or, maybe just starting a Jupyter Lab server so that I can quickly open a Jupyter notebook that a student sent me:&lt;/p&gt;
    &lt;code&gt;uvx jupyter lab&lt;/code&gt;
    &lt;p&gt;Or honestly just so many other weird, one-off use cases where &lt;code&gt;uvx&lt;/code&gt; is really nice to have around. I don‚Äôt feel like I‚Äôm missing out by always using virtual environments, because &lt;code&gt;uvx&lt;/code&gt; always gives you a ‚Äòget out of jail free‚Äô card whenever you need it.&lt;/p&gt;
    &lt;head rend="h2"&gt;If that hasn‚Äôt sold you: a personal note&lt;/head&gt;
    &lt;p&gt;I first discovered uv last year, while working together with our other lovely developers on building The Astrosky Ecosystem ‚Äî a wonderful project to build open-source social media integrations for astronomers online. But with multiple developers all working asynchronously on multiple operating systems, managing Python installations quickly became a huge task.&lt;/p&gt;
    &lt;p&gt;uv is an incredibly powerful simplification for us that we use across our entire tech stack. As developers, we can all work with identical Python installations, which is especially important given a number of semi-experimental dependencies that we use that have breaking changes with every version. On GitHub Actions, we‚Äôre planning to use uv to quickly build a Python environment and run our unit tests. In production, uv already manages Python for all of our servers.&lt;/p&gt;
    &lt;p&gt;It‚Äôs just so nice to always know that Python and package installation will always be handled consistently and correctly across all of our machines. That‚Äôs why uv is the best thing to happen to the Python ecosystem in a decade.&lt;/p&gt;
    &lt;head rend="h2"&gt;Find out more&lt;/head&gt;
    &lt;p&gt;There‚Äôs a lot more on the uv docs, including a getting started page, more in-depth guides, explanations of important concepts, and a full command reference.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://emily.space/posts/251023-uv"/><published>2025-10-29T18:57:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45751995</id><title>How to Obsessively Tune WezTerm</title><updated>2025-10-29T22:39:10.900279+00:00</updated><content>&lt;doc fingerprint="7add1d0f7ddf6ef5"&gt;
  &lt;main&gt;
    &lt;p&gt;rashil2000 Posted: 6 October 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rashil2000.me/blogs/tune-wezterm"/><published>2025-10-29T19:39:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45753222</id><title>How the U.S. National Science Foundation Enabled Software-Defined Networking</title><updated>2025-10-29T22:39:10.561935+00:00</updated><content>&lt;doc fingerprint="b5199159c83903c1"&gt;
  &lt;main&gt;
    &lt;p&gt;The Internet underlies much of modern life, connecting billions of users via access networks across wide-area backbones to countless services running in datacenters. The commercial Internet grew quickly in the 1990s and early 2000s because it was relatively easy for network owners to connect interoperable equipment, such as routers, without relying on a central administrative authority. However, a small number of router vendors controlled both the hardware and the software on these devices, leaving network owners with limited control over how their networks behave. Adding new network capabilities required support from these vendors and a multi-year standardization process to ensure interoperability across vendors. The result was bloated router software with tens of millions of lines of code, networks that were remarkably difficult to manage, and a frustratingly slow pace of innovation.&lt;/p&gt;
    &lt;p&gt;All of this changed with software-defined networking (SDN), where network owners took control over how their networks behaved. The key ideas were simple. First, network devices should offer a common open interface directly to their packet-forwarding logic. This interface allows separate control software to install fine-grained rules that govern how a network device handles different kinds of packets: which packets to drop, where to forward the remaining packets, how to modify the packet headers, and so on. Second, a network should have logically centralized control, where the control software has network-wide visibility and direct control across the distributed collection of network devices. Rather than running on the network devices themselves, the software can run on a separate set of computers that monitor and control the devices of a single network in real time.&lt;/p&gt;
    &lt;p&gt;The first commercial deployments of SDN started around 2008, and its success can be traced back to two intertwined developments that reinforced each other. The first was academic research funded mostly by the U.S. National Science Foundation (NSF). The second was cloud companies starting to build enormous datacenters, which required a new kind of network to interconnect thousands of racks of servers. In a virtuous cycle, the adoption of SDN by the hyperscalers drove further academic research, which in turn created more research, important new innovations, and several successful start-up companies.&lt;/p&gt;
    &lt;p&gt;As a result, SDN revolutionized how networks are built and operated today‚Äîthe public Internet, private networks in commercial companies, university networks and government networks, and all the way through to the cellular networks that interconnect our smartphones.&lt;/p&gt;
    &lt;p&gt;Early NSF-funded SDN research. In 2001, a National Academies report, Looking Over the Fence at Networks: A Neighbor‚Äôs View of Networking Research,30 pointed to the perils of Internet ossification: an inability of networks to change to satisfy new needs. The report highlighted three dimensions of ossification: intellectual (backward compatibility limits creative ideas), infrastructure (it is hard to deploy new ideas into the infrastructure), and system (rigid architecture led to fragile, shoe-horned solutions). In an unprecedented move, the NSF set out to address Internet ossification by investing heavily over the next decade. NSF investments laid the groundwork for SDN. We describe NSF investments here, through the lens of the support we received in our own research groups. Importantly, these and other government-funded research programs fostered a community of researchers that together paved the way for commercial adoption of SDN in the years that followed.&lt;/p&gt;
    &lt;p&gt;100√ó100 project: In 2003, the NSF launched the 100√ó100 project as part of its Information Technology Research program. The goal of the 100√ó100 project was to create communication architectures that could provide 100Mb/s networking for all 100 million American homes. The project brought together researchers from Carnegie Mellon, Stanford, Berkeley, and AT&amp;amp;T. One key aspect of the 100√ó100 project was the design of better ways to manage large networks. This research led to the 4D architecture for logically centralized network control of a distributed data plane21 (which itself built upon and generalized the routing control platform work at AT&amp;amp;T15), Ethane (a system for logically centralized control of access control in enterprise networks),11 and OpenFlow (an open interface for installing match-action rules in network switches),28 as well as the creation of the first open source network controller, NOX.22&lt;/p&gt;
    &lt;p&gt;Global Environment for Network Innovation (GENI): NSF and researchers wanted to try out new Internet architectures on a nationwide, or global, platform. Computer virtualization was widely used to share a common physical infrastructure, so could we do the same for a network? In 2005, ‚ÄúOvercoming the Internet Impasse through Virtualization‚Äù proposed an approach.5 The next year, NSF created the GENI program, with the goal of creating a shared, programmable national infrastructure for researchers to experiment with alternative Internet architectures at scale. GENI funded early OpenFlow deployments on college campuses, sliced by FlowVisor35 to allow multiple experimental networks to run alongside each other on the same production network, each managed by their own experimental controller. This, in turn, led to a proliferation of new open source controllers (Beacon, POX, and Floodlight). GENI also led to a programmable virtualized backbone network platform,6 and an experimental OpenFlow backbone network in Internet2 connecting multiple universities. This led to OpenFlow-enabled switches from Cisco, HP, and NEC. GENI funded the purchase of OpenFlow whitebox switches from ODM manufacturers and the open source software to manage them. NSF funded the NetFPGA project, which enabled experimental OpenFlow switches in Internet2. NSF brought together a community of researchers driven by much more than the desire to create experimental test beds; many researchers came to realize that programmability and virtualization were, in fact, key capabilities needed for future networks.5,16&lt;/p&gt;
    &lt;p&gt;Future Internet Design (FIND): In 2007, NSF started the FIND program to support new Internet architectures that could be prototyped and evaluated on the GENI test bed. The FIND program and its successor Future Internet Architecture (FIA) in 2010 expanded the community, working on clean-slate network architectures and fostering alternative designs. The resulting ideas were bold and exciting, including better support for mobility, content delivery, user privacy, secure cloud computing, and more. NSF‚Äôs FIND and FIA programs fostered many clean-slate network designs with prototypes and real-world evaluation, many leveraged SDN and improved its foundations. As momentum for clean-slate networking research grew in the U.S., the rest of the world followed suit, such as the EU Future Internet Research and Experimentation (FIRE) program.&lt;/p&gt;
    &lt;p&gt;Programmable Open Mobile Internet (POMI) Expedition: In 2008, the NSF POMI Expedition at Stanford expanded funding for SDN, including its use in mobile networks. POMI funded the early development of ONOS, an open source distributed controller,8 and the widely used Mininet network emulator for teaching SDN and for testing ideas before deploying them in real networks. POMI also funded the first explorations of programmable forwarding planes, setting the stage for the first fully programmable switch chip10 and the widely used P4 language.9&lt;/p&gt;
    &lt;p&gt;SDN adoption by cloud hyperscalers. In parallel with the early academic research on SDN, large technology companies such as Microsoft, Google, Amazon, and Facebook began building large datacenters full of servers that hosted these companies‚Äô popular Internet services and, increasingly, the services of enterprise customers. Datacenter owners grew frustrated with the cost and complexity of the commercially available networking equipment; a typical datacenter switch cost more than $20,000 and a hyperscaler needed about 10,000 switches per site. They decided they could build their own switch box for about $2,000 using off-the-shelf switching chips from companies such as Broadcom and Marvell, and then use their own armies of software developers to create optimized, tailored software using modern software practices. Reducing cost was good, but it was control they wanted and SDN gave them a quick path to get it.&lt;/p&gt;
    &lt;p&gt;The hyperscalers used SDN to realize two especially important use cases. First, within a single datacenter, cloud providers wanted to virtualize their networks to provide a separate virtual network for each enterprise customer (or ‚Äútenant‚Äù) with its own IP address space and networking policies. The start-up company Nicira, which emerged from the NSF-funded Ethane project, developed the Network Virtualization Platform (NVP)26 to meet this need. Nicira was later acquired by VMware and NVP became NSX. Nicira also created Open vSwitch (OVS),33 an open source virtual switch for Linux, with an OpenFlow interface. OVS grew rapidly and became the key to enabling network virtualization in datacenters around the world. Second, the hyperscalers wanted to control traffic flows across their new private wide-area networks and between their datacenters. Google adopted SDN to control how traffic is routed in its B4 backbone,23,39 using OpenFlow switches, controlled by ONIX, the first distributed controller platform.27 When Google first described B4 at the Open Network Summit in 2012, it sparked a global surge in research and commercialization of SDN. There were so many papers at ACM SIGCOMM that a separate conference‚ÄîHot Topics in Software-Defined Networking (HotSDN, later SOSR) was formed.&lt;/p&gt;
    &lt;p&gt;These two high-profile use cases‚Äîmulti-tenant virtualization and wide-area traffic engineering‚Äîdrew significant commercial attention to SDN. Indeed, NSF-funded research led directly to the creation of several successful SDN start-up companies, including Big Switch Networks (open source SDN controllers and management applications, acquired by Arista), Forward Networks (network verification products), Veriflow (developed network verification products, acquired by VMware), and Barefoot Networks (programmable switches, acquired by Intel), to name a few. SDN influenced the large networking vendors, with Cisco, Juniper, Arista, HP, and NEC all creating SDN products. Today, AMD, Nvidia, Intel, and Cisco all sell P4-programmable products, and in 2019 about a third of papers appearing at ACM SIGCOMM were based on P4 or programmable forwarding.&lt;/p&gt;
    &lt;p&gt;The commercial success of SDN drove further interest among academic researchers. The NSF and other government agencies, especially the Defense Advanced Research Project Agency (DARPA), sponsored further research on SDN platforms and use cases that continues to this day. The SDN research community broadened significantly, well beyond computer networking, to include researchers in the neighboring disciplines of programming languages, formal verification, distributed systems, algorithms, security and privacy, and more, all helping lay stronger foundations for future networks.&lt;/p&gt;
    &lt;p&gt;This article summarizes the story of how SDN arose. So many research projects, papers, companies, and products arose because of SDN that it is impossible to include all of them here. The foresight of NSF in the early 2000s, funding a generation of researchers at just the right time, working closely with the rapidly growing hyperscalers, led quite literally to a transformation‚Äîa revolution‚Äîin how networks are built today.&lt;/p&gt;
    &lt;p&gt;SDN Grew First and Fastest in Datacenters&lt;/p&gt;
    &lt;p&gt;The first large-scale deployments of SDN took place in hyperscale data centers, beginning about 2010. The story is best told by the hyperscaler companies themselves, and so we asked leaders at Google, Microsoft Azure, and Meta to tell their stories about why and how they adopted SDN. As you will see, they all started from the ideas and principles that came from the NSF-funded research; and each tailored SDN to suit their specific needs and culture.&lt;/p&gt;
    &lt;p&gt;The Internet Service Providers (ISPs) and telecommunication companies also had a strong interest in SDN. AT&amp;amp;T played a large role in its definition, engaging in research and early deployments in the mid 2000s. We invited Albert Greenberg, who was at AT&amp;amp;T at the time, to tell the story.&lt;/p&gt;
    &lt;p&gt;Nicira was perhaps the startup that epitomized the SDN movement. It grew out of the NSF-funded program and the Clean Slate Program at Stanford, based on the Ph.D. work of Mart√≠n Casado. Nicira developed ONIX, the first distributed control plane, used by Google in its infrastructure; OVS, the first OpenFlow-compliant software switch; and NVP (later NSX), the first network virtualization platform. We invited Teemu Koponen, a principal architect at Nicira, to tell the story.&lt;/p&gt;
    &lt;p&gt;During the early 2010s, the networking industry began to realize that SDN has many big advantages. It lifts complex protocols up and out of the switches into the control plane, where it is written in a modern programming language. This made it possible to reason about the correctness of the protocols simply by examining the software controlling the network and the forwarding state maintained by the switches. For the first time, it became possible to formally verify the behavior of a complete network.&lt;/p&gt;
    &lt;p&gt;Researchers, startups, network equipment vendors, and hyperscalers have all taken advantage of SDN principles to develop new ways to verify network behavior. We invited Professor George Varghese, who has been deeply involved in network verification research, to give us his perspective on network verification.&lt;/p&gt;
    &lt;p&gt;A main benefit of SDN is that it hands over the keys (of control) from the networking equipment vendors‚Äîwho kept their systems closed and proprietary, and hence tended to evolve slowly‚Äîto software programmers, who could define the behavior for themselves, often in open source software. And indeed it happened: Today, most large networks are controlled by software written by those who own and operate networks rather than by networking equipment vendors.&lt;/p&gt;
    &lt;p&gt;But what about the hardware? Switches, routers, firewalls, and network interface cards are all built from special-purpose ASICs‚Äîhighly integrated, cost-effective, and super-fast. The problem was the features and protocols that operated on packets (for example, forwarding, routing, firewalls, and security) were all baked into hardware at the time the chip was designed, two to three years before it was deployed. What if the network owner and operator needed to change and evolve the behavior in their network, for example to add a new way to measure traffic or a new way to verify behavior? A group of researchers and entrepreneurs set out to make the switches and NICs programmable by the user, to allow more rapid improvement and give the operator greater control. Not only did new programmable devices emerge, but a whole open source movement around the P4 programming language.&lt;/p&gt;
    &lt;p&gt;We invited Professor Nate Foster, who leads the P4 language ecosystem, to tell the story of how programmable forwarding planes came about.&lt;/p&gt;
    &lt;p&gt;So far, we have focused on SDN wireline networks running over electrical and optical cables in datacenters, enterprises, and long-haul WANs. SDN was originally defined with wireline networks in mind.&lt;/p&gt;
    &lt;p&gt;Yet, for cellular networks, the most widely used networks in the world, the need was even greater: Cellular networks have been held back for decades by closed, proprietary, and complex ‚Äústandards‚Äù designed to allow equipment vendors to maintain a strong grip on the market. SDN provides an opportunity to open up networks, introducing well-defined control APIs and interfaces, moving control software to common operating systems running on commodity servers.&lt;/p&gt;
    &lt;p&gt;This story has only just begun, but it started thanks to NSF-funded research in the mid 2000s, then boosted by DARPA-funded programs to support open source software for cellular infrastructure. We invited Guru Parulkar and Oƒüuz Sunay to tell the story, both of whom developed open source cellular systems at the Open Networking Foundation and for the DARPA-funded Pronto project.&lt;/p&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;The investments NSF made in SDN over the past two decades have paid huge dividends. SDN transformed how companies run their datacenter, enterprise, cellular, and backbone networks, and created a pathway for creative new ideas to see widespread deployment. The biggest beneficiaries are the billions of people who have a much more reliable, more secure, lower-cost, and faster Internet for the services they use every day.&lt;/p&gt;
    &lt;p&gt;NSF invested in the foundations of SDN at a very early stage, back when it seemed unthinkable that network owners‚Äîrather than a few incumbent equipment vendors‚Äîcould decide how networks behave. NSF nurtured the growing interest in SDN over many years, fostering a vibrant research community, critical software building blocks, and key early start-up companies that made SDN technologies available in practice. The Internet, and indeed computing and communication technologies in general, need the kind of bold, ongoing innovation that NSF makes possible.&lt;/p&gt;
    &lt;p&gt;Submit an Article to CACM&lt;/p&gt;
    &lt;p&gt;CACM welcomes unsolicited submissions on topics of relevance and value to the computing community.&lt;/p&gt;
    &lt;p&gt;You Just Read&lt;/p&gt;
    &lt;p&gt;How the U.S. National Science Foundation Enabled Software-Defined Networking&lt;/p&gt;
    &lt;p&gt;Communications of the ACM (CACM) is now a fully Open Access publication.&lt;/p&gt;
    &lt;p&gt;By opening CACM to the world, we hope to increase engagement among the broader computer science community and encourage non-members to discover the rich resources ACM has to offer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cacm.acm.org/federal-funding-of-academic-research/how-the-u-s-national-science-foundation-enabled-software-defined-networking/"/><published>2025-10-29T21:22:50+00:00</published></entry></feed>