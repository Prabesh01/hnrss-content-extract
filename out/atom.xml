<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-19T14:50:51.815034+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46667101</id><title>Keystone (YC S25) Is Hiring</title><updated>2026-01-19T14:51:02.520688+00:00</updated><content>&lt;doc fingerprint="8d21d67548473058"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Keystone builds infrastructure for autonomous coding agents. We give agents sandboxed environments that mirror production, event-based triggers (Sentry, Linear, GitHub), and verification workflows so they can ship code end-to-end— not just write it. We're hiring a founding engineer to work directly with me (solo founder) on core product. Stack is TypeScript, React (Next.js), Python, Postgres, Redis, AWS.&lt;/p&gt;
      &lt;p&gt;In-person in SoMa. $150K-$350K + 0.5-3% equity.&lt;/p&gt;
      &lt;p&gt;https://www.workatastartup.com/jobs/88801&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46667101"/><published>2026-01-18T12:00:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46670024</id><title>Gaussian Splatting – A$AP Rocky "Helicopter" music video</title><updated>2026-01-19T14:51:02.165292+00:00</updated><content>&lt;doc fingerprint="2ff1eb95032d70b0"&gt;
  &lt;main&gt;
    &lt;p&gt;Michael Rubloff&lt;/p&gt;
    &lt;p&gt;Jan 13, 2026&lt;/p&gt;
    &lt;p&gt;Believe it or not, A$AP Rocky is a huge fan of radiance fields.&lt;/p&gt;
    &lt;p&gt;Yesterday, when A$AP Rocky released the music video for Helicopter, many viewers focused on the chaos, the motion, and the unmistakable early MTV energy of the piece. Whatâs easier to miss, unless you know what youâre looking at, is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats.&lt;/p&gt;
    &lt;p&gt;I spoke with Evercoast, the team responsible for capturing the performances, as well as Chris Rutledge, the projectâs CG Supervisor at Grin Machine, and Wilfred Driscoll of WildCapture and FitsÅ«.ai, to understand how Helicopter came together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date.&lt;/p&gt;
    &lt;p&gt;The decision to shoot Helicopter volumetrically wasnât driven by technology for technologyâs sake. According to the team, the director Dan Strait approached the project in July with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. This would have been either impractical or prohibitively expensive using conventional filming and VFX pipelines.&lt;/p&gt;
    &lt;p&gt;Chris told me heâd been tracking volumetric performance capture for years, fascinated by emerging techniques that could enable visuals that simply werenât possible before. Two years ago, he began pitching the idea to directors in his circle, including Dan, as a âsomedayâ workflow. When Dan came back this summer and said he wanted to use volumetric capture for the entire video, the proliferation of gaussian splatting enabled them to take it on.&lt;/p&gt;
    &lt;p&gt;The aesthetic leans heavily into kinetic motion. Dancers colliding, bodies suspended in midair, chaotic fight scenes, and performers interacting with props that later dissolve into something else entirely. Every punch, slam, pull-up, and fall you see was physically performed and captured in 3D.&lt;/p&gt;
    &lt;p&gt;Almost every human figure in the video, including Rocky himself, was recorded volumetrically using Evercoastâs system. Itâs all real performance, preserved spatially.&lt;/p&gt;
    &lt;p&gt;This is not the first time that A$AP Rocky has featured a radiance field in one of his music videos. The 2023 music video for Shittinâ Me featured several NeRFs and even the GUI for Instant-NGP, which you can spot throughout the piece.&lt;/p&gt;
    &lt;p&gt;The primary shoot for Helicopter took place in August in Los Angeles. Evercoast deployed a 56 camera RGB-D array, synchronized across two Dell workstations. Performers were suspended from wires, hanging upside down, doing pull-ups on ceiling-mounted bars, swinging props, and performing stunts, all inside the capture volume.&lt;/p&gt;
    &lt;p&gt;Scenes that appear surreal in the final video were, in reality, grounded in very physical setups, such as wooden planks standing in for helicopter blades, real wire rigs, and real props. The volumetric data allowed those elements to be removed, recomposed, or entirely recontextualized later without losing the authenticity of the human motion.&lt;/p&gt;
    &lt;p&gt;Over the course of the shoot, Evercoast recorded more than 10 terabytes of raw data, ultimately rendering roughly 30 minutes of final splatted footage, exported as PLY sequences totaling around one terabyte.&lt;/p&gt;
    &lt;p&gt;That data was then brought into Houdini, where the post production team used CG Nomads GSOPs for manipulation and sequencing, and OTOYâs OctaneRender for final rendering. Thanks to this combination, the production team was also able to relight the splats.&lt;/p&gt;
    &lt;p&gt;One of the more powerful aspects of the workflow was Evercoastâs ability to preview volumetric captures at multiple stages. The director could see live spatial feedback on set, generate quick mesh based previews seconds after a take, and later review fully rendered splats through Evercoastâs web player before downloading massive PLY sequences for Houdini.&lt;/p&gt;
    &lt;p&gt;In practice, this meant creative decisions could be made rapidly and cheaply, without committing to heavy downstream processing until the team knew exactly what they wanted. Itâs a workflow that more closely resembles simulation than traditional filming.&lt;/p&gt;
    &lt;p&gt;Chris also discovered that Octaneâs Houdini integration had matured, and that Octaneâs early splat support was far enough along to enable relighting. According to the team, the ability to relight splats, introduce shadowing, and achieve a more dimensional â3D videoâ look was a major reason the final aesthetic lands the way it does.&lt;/p&gt;
    &lt;p&gt;The team also used Blender heavily for layout and previs, converting splat sequences into lightweight proxy caches for scene planning. Wilfred described how WildCaptureâs internal tooling was used selectively to introduce temporal consistency. In his words, the team derived primitive pose estimation skeletons that could be used to transfer motion, support collision setups, and allow Houdiniâs simulation toolset to handle rigid body, soft body, and more physically grounded interactions.&lt;/p&gt;
    &lt;p&gt;One recurring reaction to the video has been confusion. Viewers assume the imagery is AI-generated. According to Evercoast, that couldnât be further from the truth. Every stunt, every swing, every fall was physically performed and captured in real space. What makes it feel synthetic is the freedom volumetric capture affords. You arenât limited by the cameraâs composition. You have free rein to explore, reposition cameras after the fact, break spatial continuity, and recombine performances in ways that 2D simply canât.&lt;/p&gt;
    &lt;p&gt;In other words, radiance field technology isnât replacing reality. Itâs preserving everything.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting"/><published>2026-01-18T17:40:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46670279</id><title>Flux 2 Klein pure C inference</title><updated>2026-01-19T14:51:01.917424+00:00</updated><content>&lt;doc fingerprint="2e5c8b38193e188c"&gt;
  &lt;main&gt;
    &lt;p&gt;This program generates images from text prompts (and optionally from other images) using the FLUX.2-klein-4B model from Black Forest Labs. It can be used as a library as well, and is implemented entirely in C, with zero external dependencies beyond the C standard library. MPS and BLAS acceleration are optional but recommended.&lt;/p&gt;
    &lt;p&gt;I (the human here, Salvatore) wanted to test code generation with a more ambitious task, over the weekend. This is the result. It is my first open source project where I wrote zero lines of code. I believe that inference systems not using the Python stack (which I do not appreciate) are a way to free open models usage and make AI more accessible. There is already a project doing the inference of diffusion models in C / C++ that supports multiple models, and is based on GGML. I wanted to see if, with the assistance of modern AI, I could reproduce this work in a more concise way, from scratch, in a weekend. Looks like it is possible.&lt;/p&gt;
    &lt;p&gt;This code base was written with Claude Code, using the Claude Max plan, the small one of ~80 euros per month. I almost reached the limits but this plan was definitely sufficient for such a large task, which was surprising. In order to simplify the usage of this software, no quantization is used, nor do you need to convert the model. It runs directly with the safetensors model as input, using floats.&lt;/p&gt;
    &lt;p&gt;Even if the code was generated using AI, my help in steering towards the right design, implementation choices, and correctness has been vital during the development. I learned quite a few things about working with non trivial projects and AI.&lt;/p&gt;
    &lt;code&gt;# Build (choose your backend)
make mps       # Apple Silicon (fastest)
# or: make blas    # Intel Mac / Linux with OpenBLAS
# or: make generic # Pure C, no dependencies

# Download the model (~16GB)
pip install huggingface_hub
python download_model.py

# Generate an image
./flux -d flux-klein-model -p "A woman wearing sunglasses" -o output.png&lt;/code&gt;
    &lt;p&gt;That's it. No Python runtime, no PyTorch, no CUDA toolkit required at inference time.&lt;/p&gt;
    &lt;p&gt;Generated with: &lt;code&gt;./flux -d flux-klein-model -p "A picture of a woman in 1960 America. Sunglasses. ASA 400 film. Black and White." -W 250 -H 250 -o /tmp/woman.png&lt;/code&gt;, and later processed with image to image generation via &lt;code&gt;./flux -d flux-klein-model -i /tmp/woman.png -o /tmp/woman2.png -p "oil painting of woman with sunglasses" -v -H 256 -W 256&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero dependencies: Pure C implementation, works standalone. BLAS optional for ~30x speedup (Apple Accelerate on macOS, OpenBLAS on Linux)&lt;/item&gt;
      &lt;item&gt;Metal GPU acceleration: Automatic on Apple Silicon Macs&lt;/item&gt;
      &lt;item&gt;Text-to-image: Generate images from text prompts&lt;/item&gt;
      &lt;item&gt;Image-to-image: Transform existing images guided by prompts&lt;/item&gt;
      &lt;item&gt;Integrated text encoder: Qwen3-4B encoder built-in, no external embedding computation needed&lt;/item&gt;
      &lt;item&gt;Memory efficient: Automatic encoder release after encoding (~8GB freed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./flux -d flux-klein-model -p "A fluffy orange cat sitting on a windowsill" -o cat.png&lt;/code&gt;
    &lt;p&gt;Transform an existing image based on a prompt:&lt;/p&gt;
    &lt;code&gt;./flux -d flux-klein-model -p "oil painting style" -i photo.png -o painting.png -t 0.7&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;-t&lt;/code&gt; (strength) parameter controls how much the image changes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0.0&lt;/code&gt;= no change (output equals input)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;1.0&lt;/code&gt;= full generation (input only provides composition hint)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.7&lt;/code&gt;= good balance for style transfer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required:&lt;/p&gt;
    &lt;code&gt;-d, --dir PATH        Path to model directory
-p, --prompt TEXT     Text prompt for generation
-o, --output PATH     Output image path (.png or .ppm)
&lt;/code&gt;
    &lt;p&gt;Generation options:&lt;/p&gt;
    &lt;code&gt;-W, --width N         Output width in pixels (default: 256)
-H, --height N        Output height in pixels (default: 256)
-s, --steps N         Sampling steps (default: 4)
-S, --seed N          Random seed for reproducibility
&lt;/code&gt;
    &lt;p&gt;Image-to-image options:&lt;/p&gt;
    &lt;code&gt;-i, --input PATH      Input image for img2img
-t, --strength N      How much to change the image, 0.0-1.0 (default: 0.75)
&lt;/code&gt;
    &lt;p&gt;Output options:&lt;/p&gt;
    &lt;code&gt;-q, --quiet           Silent mode, no output
-v, --verbose         Show detailed config and timing info
&lt;/code&gt;
    &lt;p&gt;Other options:&lt;/p&gt;
    &lt;code&gt;-e, --embeddings PATH Load pre-computed text embeddings (advanced)
-h, --help            Show help
&lt;/code&gt;
    &lt;p&gt;The seed is always printed to stderr, even when random:&lt;/p&gt;
    &lt;code&gt;$ ./flux -d flux-klein-model -p "a landscape" -o out.png
Seed: 1705612345
out.png
&lt;/code&gt;
    &lt;p&gt;To reproduce the same image, use the printed seed:&lt;/p&gt;
    &lt;code&gt;$ ./flux -d flux-klein-model -p "a landscape" -o out.png -S 1705612345
&lt;/code&gt;
    &lt;p&gt;Choose a backend when building:&lt;/p&gt;
    &lt;code&gt;make            # Show available backends
make generic    # Pure C, no dependencies (slow)
make blas       # BLAS acceleration (~30x faster)
make mps        # Apple Silicon Metal GPU (fastest, macOS only)&lt;/code&gt;
    &lt;p&gt;Recommended:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS Apple Silicon: &lt;code&gt;make mps&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;macOS Intel: &lt;code&gt;make blas&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux with OpenBLAS: &lt;code&gt;make blas&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux without OpenBLAS: &lt;code&gt;make generic&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For &lt;code&gt;make blas&lt;/code&gt; on Linux, install OpenBLAS first:&lt;/p&gt;
    &lt;code&gt;# Ubuntu/Debian
sudo apt install libopenblas-dev

# Fedora
sudo dnf install openblas-devel&lt;/code&gt;
    &lt;p&gt;Other targets:&lt;/p&gt;
    &lt;code&gt;make clean      # Clean build artifacts
make info       # Show available backends for this platform
make test       # Run reference image test&lt;/code&gt;
    &lt;p&gt;The model weights are downloaded from HuggingFace:&lt;/p&gt;
    &lt;code&gt;pip install huggingface_hub
python download_model.py&lt;/code&gt;
    &lt;p&gt;This downloads approximately 16GB to &lt;code&gt;./flux-klein-model&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VAE (~300MB)&lt;/item&gt;
      &lt;item&gt;Transformer (~4GB)&lt;/item&gt;
      &lt;item&gt;Qwen3-4B Text Encoder (~8GB)&lt;/item&gt;
      &lt;item&gt;Tokenizer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FLUX.2-klein-4B is a rectified flow transformer optimized for fast inference:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Architecture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Transformer&lt;/cell&gt;
        &lt;cell&gt;5 double blocks + 20 single blocks, 3072 hidden dim, 24 attention heads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;VAE&lt;/cell&gt;
        &lt;cell&gt;AutoencoderKL, 128 latent channels, 8x spatial compression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Text Encoder&lt;/cell&gt;
        &lt;cell&gt;Qwen3-4B, 36 layers, 2560 hidden dim&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Inference steps: This is a distilled model that produces good results with exactly 4 sampling steps.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Phase&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text encoding&lt;/cell&gt;
        &lt;cell&gt;~8GB (encoder weights)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Diffusion&lt;/cell&gt;
        &lt;cell&gt;~8GB (transformer ~4GB + VAE ~300MB + activations)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Peak&lt;/cell&gt;
        &lt;cell&gt;~16GB (if encoder not released)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The text encoder is automatically released after encoding, reducing peak memory during diffusion. If you generate multiple images with different prompts, the encoder reloads automatically.&lt;/p&gt;
    &lt;p&gt;Benchmarks on Apple M3 Max (128GB RAM), generating a 4-step image:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;C (MPS)&lt;/cell&gt;
        &lt;cell role="head"&gt;C (BLAS)&lt;/cell&gt;
        &lt;cell role="head"&gt;C (Generic)&lt;/cell&gt;
        &lt;cell role="head"&gt;PyTorch (MPS)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;512x512&lt;/cell&gt;
        &lt;cell&gt;49.6s&lt;/cell&gt;
        &lt;cell&gt;51.9s&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;5.4s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;256x256&lt;/cell&gt;
        &lt;cell&gt;32.4s&lt;/cell&gt;
        &lt;cell&gt;29.7s&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;3.0s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;64x64&lt;/cell&gt;
        &lt;cell&gt;25.0s&lt;/cell&gt;
        &lt;cell&gt;23.5s&lt;/cell&gt;
        &lt;cell&gt;605.6s&lt;/cell&gt;
        &lt;cell&gt;2.2s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The C implementation uses float32 throughout, while PyTorch uses bfloat16 with highly optimized MPS kernels. The next step of this project is likely to implement such an optimization, in order to reach similar speed, or at least try to approach it.&lt;/item&gt;
      &lt;item&gt;The generic (pure C) backend is extremely slow and only practical for testing at small sizes.&lt;/item&gt;
      &lt;item&gt;Times include text encoding, denoising (4 steps), and VAE decode.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Maximum resolution: 1024x1024 pixels. Higher resolutions require prohibitive memory for the attention mechanisms.&lt;/p&gt;
    &lt;p&gt;Minimum resolution: 64x64 pixels.&lt;/p&gt;
    &lt;p&gt;Dimensions should be multiples of 16 (the VAE downsampling factor).&lt;/p&gt;
    &lt;p&gt;The library can be integrated into your own C/C++ projects. Link against &lt;code&gt;libflux.a&lt;/code&gt; and include &lt;code&gt;flux.h&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here's a complete program that generates an image from a text prompt:&lt;/p&gt;
    &lt;code&gt;#include "flux.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    /* Load the model. This loads VAE, transformer, and text encoder. */
    flux_ctx *ctx = flux_load_dir("flux-klein-model");
    if (!ctx) {
        fprintf(stderr, "Failed to load model: %s\n", flux_get_error());
        return 1;
    }

    /* Configure generation parameters. Start with defaults and customize. */
    flux_params params = FLUX_PARAMS_DEFAULT;
    params.width = 512;
    params.height = 512;
    params.seed = 42;  /* Use -1 for random seed */

    /* Generate the image. This handles text encoding, diffusion, and VAE decode. */
    flux_image *img = flux_generate(ctx, "A fluffy orange cat in a sunbeam", &amp;amp;params);
    if (!img) {
        fprintf(stderr, "Generation failed: %s\n", flux_get_error());
        flux_free(ctx);
        return 1;
    }

    /* Save to file. Format is determined by extension (.png or .ppm). */
    flux_image_save(img, "cat.png");
    printf("Saved cat.png (%dx%d)\n", img-&amp;gt;width, img-&amp;gt;height);

    /* Clean up */
    flux_image_free(img);
    flux_free(ctx);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;Compile with:&lt;/p&gt;
    &lt;code&gt;gcc -o myapp myapp.c -L. -lflux -lm -framework Accelerate  # macOS
gcc -o myapp myapp.c -L. -lflux -lm -lopenblas              # Linux&lt;/code&gt;
    &lt;p&gt;Transform an existing image guided by a text prompt. The &lt;code&gt;strength&lt;/code&gt; parameter controls how much the image changes:&lt;/p&gt;
    &lt;code&gt;#include "flux.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    flux_ctx *ctx = flux_load_dir("flux-klein-model");
    if (!ctx) return 1;

    /* Load the input image */
    flux_image *photo = flux_image_load("photo.png");
    if (!photo) {
        fprintf(stderr, "Failed to load image\n");
        flux_free(ctx);
        return 1;
    }

    /* Set up parameters. Output size defaults to input size. */
    flux_params params = FLUX_PARAMS_DEFAULT;
    params.strength = 0.7;  /* 0.0 = no change, 1.0 = full regeneration */
    params.seed = 123;

    /* Transform the image */
    flux_image *painting = flux_img2img(ctx, "oil painting, impressionist style",
                                         photo, &amp;amp;params);
    flux_image_free(photo);  /* Done with input */

    if (!painting) {
        fprintf(stderr, "Transformation failed: %s\n", flux_get_error());
        flux_free(ctx);
        return 1;
    }

    flux_image_save(painting, "painting.png");
    printf("Saved painting.png\n");

    flux_image_free(painting);
    flux_free(ctx);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;Strength values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0.3&lt;/code&gt;- Subtle style transfer, preserves most details&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.5&lt;/code&gt;- Moderate transformation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.7&lt;/code&gt;- Strong transformation, good for style transfer&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.9&lt;/code&gt;- Almost complete regeneration, keeps only composition&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When generating multiple images with different seeds but the same prompt, you can avoid reloading the text encoder:&lt;/p&gt;
    &lt;code&gt;flux_ctx *ctx = flux_load_dir("flux-klein-model");
flux_params params = FLUX_PARAMS_DEFAULT;
params.width = 256;
params.height = 256;

/* Generate 5 variations with different seeds */
for (int i = 0; i &amp;lt; 5; i++) {
    flux_set_seed(1000 + i);

    flux_image *img = flux_generate(ctx, "A mountain landscape at sunset", &amp;amp;params);

    char filename[64];
    snprintf(filename, sizeof(filename), "landscape_%d.png", i);
    flux_image_save(img, filename);
    flux_image_free(img);
}

flux_free(ctx);&lt;/code&gt;
    &lt;p&gt;Note: The text encoder (~8GB) is automatically released after the first generation to save memory. It reloads automatically if you use a different prompt.&lt;/p&gt;
    &lt;p&gt;All functions that can fail return NULL on error. Use &lt;code&gt;flux_get_error()&lt;/code&gt; to get a description:&lt;/p&gt;
    &lt;code&gt;flux_ctx *ctx = flux_load_dir("nonexistent-model");
if (!ctx) {
    fprintf(stderr, "Error: %s\n", flux_get_error());
    /* Prints something like: "Failed to load VAE - cannot generate images" */
    return 1;
}&lt;/code&gt;
    &lt;p&gt;Core functions:&lt;/p&gt;
    &lt;code&gt;flux_ctx *flux_load_dir(const char *model_dir);   /* Load model, returns NULL on error */
void flux_free(flux_ctx *ctx);                     /* Free all resources */

flux_image *flux_generate(flux_ctx *ctx, const char *prompt, const flux_params *params);
flux_image *flux_img2img(flux_ctx *ctx, const char *prompt, const flux_image *input,
                          const flux_params *params);&lt;/code&gt;
    &lt;p&gt;Image handling:&lt;/p&gt;
    &lt;code&gt;flux_image *flux_image_load(const char *path);     /* Load PNG or PPM */
int flux_image_save(const flux_image *img, const char *path);  /* 0=success, -1=error */
flux_image *flux_image_resize(const flux_image *img, int new_w, int new_h);
void flux_image_free(flux_image *img);&lt;/code&gt;
    &lt;p&gt;Utilities:&lt;/p&gt;
    &lt;code&gt;void flux_set_seed(int64_t seed);                  /* Set RNG seed for reproducibility */
const char *flux_get_error(void);                  /* Get last error message */
void flux_release_text_encoder(flux_ctx *ctx);     /* Manually free ~8GB (optional) */&lt;/code&gt;
    &lt;code&gt;typedef struct {
    int width;              /* Output width in pixels (default: 256) */
    int height;             /* Output height in pixels (default: 256) */
    int num_steps;          /* Denoising steps, use 4 for klein (default: 4) */
    float guidance_scale;   /* CFG scale, use 1.0 for klein (default: 1.0) */
    int64_t seed;           /* Random seed, -1 for random (default: -1) */
    float strength;         /* img2img only: 0.0-1.0 (default: 0.75) */
} flux_params;

/* Initialize with sensible defaults */
#define FLUX_PARAMS_DEFAULT { 256, 256, 4, 1.0f, -1, 0.75f }&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/antirez/flux2.c"/><published>2026-01-18T18:01:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46671731</id><title>Dead Internet Theory</title><updated>2026-01-19T14:51:01.168235+00:00</updated><content>&lt;doc fingerprint="32e0135d28e5b5bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Dead Internet Theory&lt;/head&gt;
    &lt;p&gt;The other day I was browsing my one-and-only social network — which is not a social network, but I’m tired of arguing with people online about it — HackerNews. It’s like this dark corner of the internet, where anonymous tech-enthusiasts, scientists, entrepreneurs, and internet-trolls, like to lurk. I like HackerNews. It helps me stay up-to-date about recent tech news (like Cloudflare acquiring Astro which makes me happy for the Astro team, but also sad and worried since I really like Astro, and big-tech has a tendency to ruin things); it mostly avoids politics; and it’s not a social network.&lt;/p&gt;
    &lt;p&gt;And, in the fashion of HackerNews, I stumbled upon someone sharing their open-source project. It’s great to see people work on their projects and decide to show them to the world. I think people underestimate the fear of actually shipping stuff, which involves sharing it with the world.&lt;/p&gt;
    &lt;p&gt;Upon glancing at the comment section, I started to see other anonymous participants questioning the validity of said open-source project in terms of how much of it was AI-generated. I grabbed my popcorn, and started to follow this thread. More accusations started to appear: the commit timeline does not make sense; the code has AI-generated comments; etc. And at the same time, the author tried to reply to every comment claiming that they wrote this 100% without using AI.&lt;/p&gt;
    &lt;p&gt;I don’t mind people using AI to write code, even though I tried to resist it myself, until eventually succumbing to it. But I think it’s fair to disclose the use of AI, especially in open-source software. People on the internet are, mostly, anonymous, and it’s not always possible to verify the claims or expertise of particular individuals. But as the amount of code is growing, considering that everyone is using AI to generate whatever-app they want, it’s impossible to verify every piece of code we are going to use. So it’s fair to know, I think, if some project is AI generated and to what extent. In the end, LLMs are just probabilistic next-token generators. And while they are getting extremely good at most simple tasks, they have the potential to wreak havoc with harder problems or edge-cases (especially if there are no experienced engineers, with domain knowledge, to review the generated code).&lt;/p&gt;
    &lt;p&gt;As I was following this thread, I started to see a pattern: the comments of the author looked AI generated too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The use of em-dashes, which on most keyboard require a special key-combination that most people don’t know, and while in markdown two dashes will render as em-dash, this is not true of HackerNews (hence, you often see &lt;code&gt;--&lt;/code&gt;in HackerNews comments, where the author is probably used to Markdown renderer turning it into em-dash)&lt;/item&gt;
      &lt;item&gt;The notorious “you are absolutely right”, which no living human ever used before, at least not that I know of&lt;/item&gt;
      &lt;item&gt;The other notorious “let me know if you want to [do that thing] or [explore this other thing]” at the end of the sentence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was sitting there, refreshing the page, seeing the author being confronted with use of AI in both their code and their comments, while the author claiming to have not used AI at all. Honestly, I was thinking I was going insane. Am I wrong to suspect them? What if people DO USE em-dashes in real life? What if English is not their native language and in their native language it’s fine to use phrases like “you are absolutely right”? Is this even a real person? Are the people who are commenting real?&lt;/p&gt;
    &lt;p&gt;And then it hit me. We have reached the Dead Internet. The Dead Internet Theory claims that since around 2016 (a whooping 10 years already), the internet is mainly dead, i.e. most interactions are between bots, and most content is machine generated to either sell you stuff, or game the SEO game (in order to sell you stuff).&lt;/p&gt;
    &lt;p&gt;I’m &lt;del&gt;ashamed&lt;/del&gt; proud to say that I spent a good portion of my teenage years on the internet, chatting and learning from real people who knew more than me. Back in the early 2000s, there were barely bots on the internet. The average non-tech human didn’t know anything about phpBB forums, and the weird people with pseudonyms who hanged-out in there. I spent countless hours inside IRC channels, and on phpBB forums, learning things like network programming, OS-development, game-development, and of course web-development (which became my profession for almost two decades now). I’m basically a graduate of the Internet University. Back then, nobody had doubts that they were talking to a human-being. Sure, you could think that you spoke to a hot girl, who in reality was a fat guy, but hey, at least they were real!&lt;/p&gt;
    &lt;p&gt;But today, I no longer know what is real. I saw a picture on LinkedIn, from a real tech company, posting about their “office vibes” and their happy employees. And then I went to the comment section, and sure enough this picture is AI generated (mangled text that does not make sense, weird hand artifacts). It was posted by an employee of the company, it showed other employees of said company, and it was altered with AI to showcase a different reality. Hell, maybe the people on the picture do not even exist!&lt;/p&gt;
    &lt;p&gt;And these are mild examples. I don’t use social networks (and no, HackerNews is not a social network), but I hear horror stories about AI generated content on Facebook, Xitter, TikTok, ranging from photos of giants that built the pyramids in Egypt, all the way to short videos of pretty girls saying that the EU is bad for Poland.&lt;/p&gt;
    &lt;p&gt;I honestly got sad that day. Hopeless, if I could say. AI is easily available to the masses, which allow them to generate shitload of AI-slop. People no longer need to write comments or code, they can just feed this to AI agents who will generate the next “you are absolutely right” masterpiece.&lt;/p&gt;
    &lt;p&gt;I like technology. I like software engineering, and the concept of the internet where people could share knowledge and create communities. Were there malicious actors back then on the internet? For sure. But what I am seeing today, makes me question whether the future we are headed to is a future where technology is useful anymore. Or, rather, it’s a future where bots talk with bots, and human knowledge just gets recycled and repackaged into “10 step to fix your [daily problem] you are having” for the sake of selling you more stuff.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kudmitry.com/articles/dead-internet-theory/"/><published>2026-01-18T20:19:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46672181</id><title>Show HN: Beats, a web-based drum machine</title><updated>2026-01-19T14:51:00.106679+00:00</updated><content>&lt;doc fingerprint="32e55a1cb8f697b3"&gt;
  &lt;main&gt;
    &lt;p&gt;Share your beat with this URL:&lt;/p&gt;
    &lt;p&gt;BEATS&lt;/p&gt;
    &lt;p&gt;A web-based drum machine inspired by the Teenage Engineering Pocket Operators.&lt;/p&gt;
    &lt;p&gt;CREDITS:&lt;/p&gt;
    &lt;p&gt;• Wrote by @kinduff&lt;/p&gt;
    &lt;p&gt;• Built with Tone.js and Stimulus.js&lt;/p&gt;
    &lt;p&gt;• With the awesome VT323 font&lt;/p&gt;
    &lt;p&gt;THANKS TO:&lt;/p&gt;
    &lt;p&gt;• andiam03 for transposing patterns and inspiring!&lt;/p&gt;
    &lt;p&gt;• ethanhein for the original idea!&lt;/p&gt;
    &lt;p&gt;• all beta reviewers!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://beats.lasagna.pizza"/><published>2026-01-18T21:10:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46673453</id><title>High-speed train collision in Spain kills at least 39</title><updated>2026-01-19T14:50:59.862326+00:00</updated><content>&lt;doc fingerprint="66f391d8ae52b2f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;High-speed train crash in Spain kills at least 39&lt;/head&gt;
    &lt;p&gt;At least 39 people have died in a train collision in southern Spain and dozens more have been injured in the country's worst rail crash in more than a decade, Spain's Civil Guard has said.&lt;/p&gt;
    &lt;p&gt;Carriages on a Madrid-bound train derailed and crossed over to the opposite tracks, colliding with an oncoming train in Adamuz on Sunday evening.&lt;/p&gt;
    &lt;p&gt;Four hundred passengers and staff were onboard both trains, the rail networks said. Emergency services treated 122 people, with 43, including four children, still in hospital. Of those, 12 adults and one child are in intensive care.&lt;/p&gt;
    &lt;p&gt;Spanish Transport Minister Óscar Puente said the death toll "is not yet final", as officials launched an investigation.&lt;/p&gt;
    &lt;p&gt;Puente described the incident as "extremely strange". All the railway experts consulted by the government "are extremely baffled by the accident", he told reporters in Madrid.&lt;/p&gt;
    &lt;p&gt;Rail network operator Adif said the collision happened at 19:45 local time (18:45 GMT), about an hour after the train left Málaga heading north to Madrid, when it derailed on a straight stretch of track near the city of Córdoba.&lt;/p&gt;
    &lt;p&gt;The force of the crash pushed the carriages of the second train into an embankment, Puente said. He added that most of those killed and injured were in the front carriages of the second train, which was travelling south from Madrid to Huelva.&lt;/p&gt;
    &lt;p&gt;The type of train involved in the crash was a Freccia 1000, which can reach top speeds of 400 km/h (250 mph), a spokesperson for the Italian rail company Ferrovie dello Stato told Reuters news agency.&lt;/p&gt;
    &lt;p&gt;Rescue teams said the twisted wreckage of the trains made it difficult to recover people trapped inside the carriages.&lt;/p&gt;
    &lt;p&gt;Córdoba fire chief Francisco Carmona told Spanish public broadcaster RTVE: "We have even had to remove a dead person to be able to reach someone alive. It is hard, tricky work."&lt;/p&gt;
    &lt;p&gt;Salvador Jimenez, a journalist with RTVE who was on one of the trains, said the impact felt like an "earthquake".&lt;/p&gt;
    &lt;p&gt;"I was in the first carriage. There was a moment when it felt like an earthquake and the train had indeed derailed," Jimenez said.&lt;/p&gt;
    &lt;p&gt;Footage from the scene appears to show some train carriages had tipped over on their sides. Rescue workers can be seen scaling the train to pull people out of the lopsided train doors and windows.&lt;/p&gt;
    &lt;p&gt;A Madrid-bound passenger, José, told public broadcaster Canal Sur: "There were people and screaming, calling for doctors."&lt;/p&gt;
    &lt;p&gt;All rail services between Madrid and Andalusia were suspended following the accident and are expected to remain closed all day on Monday.&lt;/p&gt;
    &lt;p&gt;Iryo, a private rail company that operated the journey from Málaga, said around 300 passengers were on board the train that first derailed, while the other train – operated by the state-funded firm Renfe – had around 100 passengers.&lt;/p&gt;
    &lt;p&gt;The official cause is not yet known. An investigation is not expected to determine what happened for at least a month, according to the transport minister.&lt;/p&gt;
    &lt;p&gt;Spain's Prime Minister, Pedro Sánchez, said the country will endure a "night of deep pain".&lt;/p&gt;
    &lt;p&gt;The mayor of Adamuz, Rafael Moreno, was one of the first people on the scene of the accident, describing it as "a nightmare".&lt;/p&gt;
    &lt;p&gt;King Felipe VI and Queen Letizia said they were following news of the disaster "with great concern".&lt;/p&gt;
    &lt;p&gt;"We extend our most heartfelt condolences to the relatives and loved ones of the dead, as well as our love and wishes for a swift recovery to the injured," the royal palace said on X.&lt;/p&gt;
    &lt;p&gt;The emergency agency in the region of Andalusia urged any crash survivors to contact their families or post on social media that they are alive.&lt;/p&gt;
    &lt;p&gt;Advanced medical posts were set up for impacted passengers to be treated for injuries and transferred to hospital. Adif said it set up spaces for relatives of the victims at Atocha, Seville, Córdoba, Málaga and Huelva stations.&lt;/p&gt;
    &lt;p&gt;The Spanish Red Cross has deployed emergency support services to the scene, while also offering counselling to families nearby.&lt;/p&gt;
    &lt;p&gt;Miguel Ángel Rodríguez from the Red Cross told RNE radio: "The families are going through a situation of great anxiety due to the lack of information. These are very distressing moments."&lt;/p&gt;
    &lt;p&gt;French President Emmanuel Macron, Italian Prime Minister Giorgia Meloni and European Commission chief Ursula von der Leyen have published statements offering condolences.&lt;/p&gt;
    &lt;p&gt;"My thoughts are with the victims, their families and the entire Spanish people. France stands by your side," Macron wrote on social media.&lt;/p&gt;
    &lt;p&gt;In 2013, Spain suffered its worst high-speed train derailment in Galicia, north-west Spain, which left 80 people dead and 140 others injured.&lt;/p&gt;
    &lt;p&gt;Spain's high-speed rail network is the second largest in the world, behind China, connecting more than 50 cities across the country. Adif data shows the Spanish rail is more than 4,000km long (2,485 miles).&lt;/p&gt;
    &lt;p&gt;Get our flagship newsletter with all the headlines you need to start the day. Sign up here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/cedw6ylpynyo"/><published>2026-01-18T23:54:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46673809</id><title>Show HN: I quit coding years ago. AI brought me back</title><updated>2026-01-19T14:50:59.404941+00:00</updated><content>&lt;doc fingerprint="74dbd8bd7c3cba78"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Compound Interest Calculator&lt;/head&gt;
    &lt;p&gt;Calculate how your investments grow over time with compound interest.&lt;/p&gt;
    &lt;head rend="h3"&gt;How much are you investing?&lt;/head&gt;
    &lt;head rend="h3"&gt;What return do you expect?&lt;/head&gt;
    &lt;head rend="h3"&gt;How long will you invest?&lt;/head&gt;
    &lt;head rend="h2"&gt;You May Also Like&lt;/head&gt;
    &lt;head rend="h2"&gt;Related Articles&lt;/head&gt;
    &lt;head rend="h3"&gt;Beyond the Nest Egg: Finding Your Financial 'Crossover Point' with Compound Interest&lt;/head&gt;
    &lt;p&gt;Discover the Crossover Point: the milestone where interest earnings exceed your contributions. A guide to compound interest for late-start investors.&lt;/p&gt;
    &lt;head rend="h3"&gt;The 'Wait Tax': Quantifying the Exact Cost of Delaying Your Investments&lt;/head&gt;
    &lt;p&gt;Stop waiting for the 'perfect time' to invest. Learn how to calculate your 'Wait Tax'—the massive financial penalty of delaying your portfolio by just 12–24 months.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Compound Interest?&lt;/head&gt;
    &lt;p&gt;Compound interest is interest calculated on both the initial principal and the accumulated interest from previous periods. Unlike simple interest, which only earns interest on the original amount, compound interest allows your money to grow exponentially over time.&lt;/p&gt;
    &lt;p&gt;Albert Einstein reportedly called compound interest "the eighth wonder of the world," saying: "He who understands it, earns it; he who doesn't, pays it."&lt;/p&gt;
    &lt;head rend="h2"&gt;The Compound Interest Formula&lt;/head&gt;
    &lt;p&gt;The basic formula for compound interest is:&lt;/p&gt;
    &lt;p&gt;Where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A = Final amount (principal + interest)&lt;/item&gt;
      &lt;item&gt;P = Principal (initial investment)&lt;/item&gt;
      &lt;item&gt;r = Annual interest rate (as a decimal)&lt;/item&gt;
      &lt;item&gt;n = Number of times interest compounds per year&lt;/item&gt;
      &lt;item&gt;t = Time in years&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For continuous compounding, the formula becomes:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Rule of 72&lt;/head&gt;
    &lt;p&gt;A quick mental math trick to estimate how long it takes to double your money:&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;At 6% interest: 72 ÷ 6 = 12 years to double&lt;/item&gt;
      &lt;item&gt;At 8% interest: 72 ÷ 8 = 9 years to double&lt;/item&gt;
      &lt;item&gt;At 12% interest: 72 ÷ 12 = 6 years to double&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Rule of 72 is a quick approximation. For more precise calculations, use the formula above or our calculator!&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Compound Frequency Matters&lt;/head&gt;
    &lt;p&gt;The more frequently interest compounds, the more you earn. Think of it as: how often the bank calculates and adds interest to your balance.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Annual compounding: Interest added once per year&lt;/item&gt;
      &lt;item&gt;Monthly compounding: Interest added 12 times per year&lt;/item&gt;
      &lt;item&gt;Daily compounding: Interest added 365 times per year&lt;/item&gt;
      &lt;item&gt;Continuous compounding: Interest added infinitely (theoretical maximum)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At a 10% annual rate on $10,000 over 10 years:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Annual compounding: $25,937&lt;/item&gt;
      &lt;item&gt;Monthly compounding: $27,070&lt;/item&gt;
      &lt;item&gt;Daily compounding: $27,179&lt;/item&gt;
      &lt;item&gt;Continuous compounding: $27,183&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Real vs Nominal Returns: Understanding Inflation&lt;/head&gt;
    &lt;p&gt;When planning long-term investments, it's crucial to understand the difference between nominal returns (the number you see) and real returns (actual purchasing power).&lt;/p&gt;
    &lt;p&gt;Nominal Return: The raw percentage your investment grows – what your account statement shows.&lt;/p&gt;
    &lt;p&gt;Real Return: Your return after accounting for inflation – what your money can actually buy.&lt;/p&gt;
    &lt;p&gt;A simpler approximation:&lt;/p&gt;
    &lt;p&gt;Example: You invest $10,000 at 10% annual return for 20 years.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nominal value: $67,275 (what your account shows)&lt;/item&gt;
      &lt;item&gt;With 3% inflation: $37,278 in today's purchasing power&lt;/item&gt;
      &lt;item&gt;Inflation loss: $29,997 – nearly half your "gains"!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use the "Adjust for inflation" toggle in our calculator to see what your future money will actually be worth in today's dollars. This helps set realistic expectations for retirement planning.&lt;/p&gt;
    &lt;p&gt;Historical inflation rates vary by country, but a common assumption for developed economies is 2-3% annually. During high-inflation periods, this can exceed 5-10%.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tips for Maximizing Compound Interest&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start early – Time is your greatest ally. Even small amounts grow significantly over decades.&lt;/item&gt;
      &lt;item&gt;Be consistent – Regular contributions amplify the effect of compounding.&lt;/item&gt;
      &lt;item&gt;Reinvest returns – Don't withdraw interest; let it compound.&lt;/item&gt;
      &lt;item&gt;Seek higher rates – Even a 1% difference compounds to significant amounts over time.&lt;/item&gt;
      &lt;item&gt;Minimize fees – High fees erode your compounding gains.&lt;/item&gt;
      &lt;item&gt;Beat inflation – Ensure your real return is positive; otherwise, you're losing purchasing power.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://calquio.com/finance/compound-interest"/><published>2026-01-19T00:50:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46674416</id><title>The Code-Only Agent</title><updated>2026-01-19T14:50:59.076676+00:00</updated><content>&lt;doc fingerprint="540ea85569ca9a17"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Code-Only Agent&lt;/head&gt;&lt;p&gt;When Code Execution Really is All You Need&lt;/p&gt;&lt;p&gt;If you're building an agent, you're probably overwhelmed. Tools. MCP. Subagents. Skills. The ecosystem pushes you toward complexity, toward "the right way" to do things. You should know: Concepts like "Skills" and "MCP" are actually outcomes of an ongoing learning process of humans figuring stuff out. The space is wide open for exploration. With this mindset I wanted to try something different. Simplify the assumptions.&lt;/p&gt;&lt;p&gt; What if the agent only had &lt;code&gt;one tool&lt;/code&gt;? Not just any tool, but the most powerful one. The
            &lt;code&gt;Turing-complete&lt;/code&gt; one: execute code.
          &lt;/p&gt;&lt;p&gt; Truly one tool means: no `bash`, no `ls`, no `grep`. Only &lt;code&gt;execute_code&lt;/code&gt;. And you enforce it.
          &lt;/p&gt;&lt;p&gt;When you watch an agent run, you might think: "I wonder what tools it'll use to figure this out. Oh look, it ran `ls`. That makes sense. Next, `grep`. Cool."&lt;/p&gt;&lt;p&gt;The simpler Code-Only paradigm makes that question irrelevant. The question shifts from "what tools?" to "what code will it produce?" And that's when things get interesting.&lt;/p&gt;&lt;head rend="h2"&gt;&lt;code&gt;execute_code&lt;/code&gt;: One Tool to Rule Them All&lt;/head&gt;&lt;p&gt;Traditional prompting works like this:&lt;/p&gt;&lt;p&gt; &amp;gt; Agent, do thing &lt;lb/&gt; &amp;gt; Agent responds with thing &lt;/p&gt;&lt;p&gt;Contrast with:&lt;/p&gt;&lt;p&gt; &amp;gt; Agent, do thing &lt;lb/&gt; &amp;gt; Agent creates and runs code to do thing &lt;/p&gt;&lt;p&gt; It does this every time. No, really, &lt;code&gt;every&lt;/code&gt;
            time. Pick a runtime for our Code-Only agent, say Python. It needs
            to find a file? It writes Python code to find the file and executes
            the code. Maybe it runs rglob. Maybe it does os.walk.
          &lt;/p&gt;&lt;p&gt;It needs to create a script that crawls a website? It doesn't write the script to your filesystem (reminder: there's no create_file tool to do that!). It writes code to output a script that crawls a website.1&lt;/p&gt;&lt;p&gt;We make it so that there is literally no way for the agent to do anything productive without writing code.&lt;/p&gt;&lt;p&gt;So what? Why do this? You're probably thinking, how is this useful? Just give it `bash` tool already man.&lt;/p&gt;&lt;p&gt;Let's think a bit more deeply what's happening. Traditional agents respond with something. Tell it to find some DNA pattern across 100 files. It might `ls` and `grep`, it might do that in some nondeterministic order, it'll figure out an answer and maybe you continue interacting because it missed a directory or you added more files. After some time, you end up with a conversation of tool calls, responses, and an answer.&lt;/p&gt;&lt;p&gt; At some point the agent might even write a Python script to do this DNA pattern finding. That would be a lucky happy path, because we could rerun that script or update it later... Wait, that's handy... actually, more than handy... isn't that &lt;code&gt;ideal&lt;/code&gt;? Wouldn't it be better if we told it to write a script at the
            start? You see, the Code-Only agent doesn't need to be told to write
            a script. It
            &lt;code&gt;has&lt;/code&gt;
            to, because that's literally the only way for it to do anything of
            substance.
          &lt;/p&gt;&lt;p&gt;The Code-Only agent produces something more precise than an answer in natural language. It produces a code witness of an answer. The answer is the output from running the code. The agent can interpret that output in natural language (or by writing code), but the "work" is codified in a very literal sense. The Code-Only agent doesn't respond with something. It produces a code witness that outputs something.&lt;/p&gt;&lt;p&gt;Try ❯❯ Code-Only plugin for Claude Code&lt;/p&gt;&lt;head rend="h2"&gt;Code witnesses are semantic guarantees&lt;/head&gt;&lt;p&gt;Let's follow the consequences. The code witness must abide by certain rules: The rules imposed by the language runtime semantics (e.g., of Python). That's not a "next token" process. That's not a "LLM figures out sequence of tool calls, no that's not what I wanted". It's piece of code. A piece of code! Our one-tool agent has a wonderful property: It went through latent space to produce something that has a defined semantics, repeatably runnable, and imminently comprehensible (for humans or agents alike to reason about). This is nondeterministic LLM token-generation projected into the space of Turing-complete code, an executable description of behavior as we best understand it.&lt;/p&gt;&lt;p&gt; Is a Code-Only agent really enough, or too extreme? I'll be frank: I pursued this extreme after two things (1) inspiration from articles in Further Reading below (2) being annoyed at agents for not comprehensively and exhaustively analyzing 1000s of files on my laptop. They would skip, take shortcuts, hallucinate. I knew how to solve part of that problem: create a &lt;code&gt;programmatic&lt;/code&gt;
            loop and try have fresh instances/prompts to do the work
            comprehensively. I can rely on the semantics of a loop written in
            Python. Take this idea further, and you realize that for anything
            long-running and computable (e.g., bash or some tool), you actually
            want the real McCoy: the full witness of code, a trace of why things
            work or don't work. The Code-Only agent
            &lt;code&gt;enforces&lt;/code&gt;
            that principle.
          &lt;/p&gt;&lt;p&gt; Code-Only agents are not too extreme. I think they're the only way forward for computable things. If you're writing travel blog posts, you accept the LLMs answer (and you don't need to run tools for that). When something is computable though, Code-Only is the only path to a &lt;code&gt;fully trustworthy&lt;/code&gt;
            way to make progress where you need guarantees (subject to
            the semantics that your language of choice guarantees, of course). When I say
            guarantees, I mean that in the looser sense, and also in a
            Formal
            sense. Which beckons: What happens when we use a language like
            Lean with some of the
            strongest guarantees? Did we not observe that
            programs are proofs?
          &lt;/p&gt;&lt;p&gt;This lens says the Code-Only agent is a producer of proofs, witnesses of computational behavior in the world of proofs-as-programs. An LLM in a loop forced to produce proofs, run proofs, interpret proof results. That's all.&lt;/p&gt;&lt;head rend="h2"&gt;Going Code-Only&lt;/head&gt;&lt;p&gt;So you want to go Code-Only. What happens? The paradigm is simple, but the design choices are surprising.&lt;/p&gt;&lt;p&gt;First, the harness. The LLM's output is code, and you execute that code. What should be communicated back? Exit code makes sense. What about output? What if the output is very large? Since you're running code, you can specify the result type that running the code should return.&lt;/p&gt;&lt;p&gt;I've personally, e.g., had the tool return results directly if under a certain threshold (1K bytes). This would go into the session context. Alternatively, write the results to a JSON file on disk if it exceeds the threshold. This avoids context blowup and the result tells the agent about the output file path written to disk. How best to pass results, persist them, and optimize for size and context fill are open questions. You also want to define a way to deal with `stdout` and `stderr`: Do you expose these to the agent? Do you summarize before exposing?&lt;/p&gt;&lt;p&gt;Next, enforcement. Let's say you're using Claude Code. It's not enough to persuade it to always create and run code. It turns out it's surprisingly twisty to force Claude Code into a single tool (maybe support for this will improve). The best plugin-based solution I found is a tool PreHook that catches banned tool uses. This wastes some iterations when Claude Code tries to use a tool that's not allowed, but it learns to stop attempting filesystem reads/writes. An initial prompt helps direct.&lt;/p&gt;&lt;p&gt;Next, the language runtime. Python, TypeScript, Rust, Bash. Any language capable of being executed is fair game, but you'll need to think through whether it works for your domain. Dynamic languages like Python are interesting because you can run code natively in the agent's own runtime, rather than through subprocess calls. Likewise TypeScript/JS can be injected into TypeScript-based agents (see Further Reading).&lt;/p&gt;&lt;p&gt;Once you get into the Code-Only mindset, you'll see the potential for composition and reuse. Claude Skills define reusable processes in natural language. What's the equivalent for a Code-Only agent? I'm not sure a Skills equivalent exists yet, but I anticipate it will take shape soon: code as building blocks for specific domains where Code-Only agents compose programmatic patterns. How is that different from calling APIs? APIs form part of the reusable blocks, but their composition (loops, parallelism, asynchrony) is what a Code-Only agent generates.&lt;/p&gt;&lt;p&gt;What about heterogeneous languages and runtimes for our `execute_tool`? I don't think we've thought that far yet.&lt;/p&gt;&lt;head rend="h2"&gt;Further Reading&lt;/head&gt;&lt;p&gt;The agent landscape is quickly evolving. My thoughts on how the Code-Only paradigm fits into inspiring articles and trends, from most recent and going back:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;prose.md (Jan 2026) — Code-Only reduces prompts to executable code (with loops and statement sequences). Prose expands prompts into natural language with program-like constructs (also loops, sequences, parallelism). The interplay of natural language for agent orchestration and rigid semantics for agent execution could be extremely powerful.&lt;/item&gt;&lt;item&gt;Welcome to Gas Town (Jan 2026) — Agent orchestration gone berserk. Tool running is the low-level operation at the bottom of the agent stack. Code-Only fits as the primitive: no matter how many agents you orchestrate, each one reduces to generating and executing code.&lt;/item&gt;&lt;item&gt;Anthropic Code Execution with MCP article (Nov 2025) — MCP-centric view of exposing MCP servers as code API and not tool calls. Code-Only is simpler and more general. It doesn't care about MCP, and casting the MCP interface as an API is a mechanical necessity that acknowledges the power of going Code-Only.&lt;/item&gt;&lt;item&gt;Anthropic Agent Skills article (Oct 2025) — Skills embody reusable processes framed in natural language. They can generate and run code, but that's not their only purpose. Code-Only is narrower (but computationally all-powerful): the reusable unit is always executable. The analog to Skills manifests as pluggable executable pieces: functions, loops, composable routines over APIs.&lt;/item&gt;&lt;item&gt;Cloudflare Code Mode article (Sep 2025) — Possibly the earliest concrete single-code-tool implementation. Code Mode converts MCP tools into a TypeScript API and gives the agent one tool: execute TypeScript. Their insight is pragmatic: LLMs write better code than tool calls because of training data. In its most general sense, going Code-Only doesn't need to rely on MCP or APIs, and encapsulates all code execution concerns.&lt;/item&gt;&lt;item&gt;Ralph Wiggum as a "software engineer" (Jul 2025) — A programmatic loop over agents (agent orchestration). Huntley describes it as "deterministically bad in a nondeterministic world". Code-Only inverts this a bit: projection of a nondeterministic model into deterministic execution. Agent orchestration on top of an agent's Code-Only inner-loop could be a powerful combination.&lt;/item&gt;&lt;item&gt;Tools: Code is All You Need (Jul 2025) — Raises code as a first-order concern for agents. Ronacher's observation: asking an LLM to write a script to transform markdown makes it possible to reason about and trust the process. The script is reviewable, repeatable, composable. Code-Only takes this further where every action becomes a script you can reason about.&lt;/item&gt;&lt;item&gt;How to Build an Agent (Apr 2025) — The cleanest way to achieve a Code-Only agent today may be to build it from scratch. Tweaking current agents like Claude Code to enforce a single tool means friction. Thorsten's article is a lucid account for building an agent loop with tool calls. If you want to enforce Code-Only, this makes it easy to do it yourself.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;What's Next&lt;/head&gt;&lt;p&gt;Two directions feel inevitable. First, agent orchestration. Tools like prose.md let you compose agents in natural language with program-like constructs. What happens when those agents are Code-Only in their inner loop? You get natural language for coordination, rigid semantics for execution. The best of both.&lt;/p&gt;&lt;p&gt;Second, hybrid tooling. Skills work well for processes that live in natural language. Code-Only works well for processes that need guarantees. We'll see agents that fluidly mix both: Skills for orchestration and intent, Code-Only for computation and precision. The line between "prompting an agent" and "programming an agent" will blur until it disappears.&lt;/p&gt;&lt;p&gt;Try ❯❯ Code-Only plugin for Claude Code&lt;/p&gt;&lt;p&gt;1There is something beautifully quine-like about this agent. I've always loved quines.&lt;/p&gt;Timestamped 9 Jan 2026&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rijnard.com/blog/the-code-only-agent"/><published>2026-01-19T02:27:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46675853</id><title>A decentralized peer-to-peer messaging application that operates over Bluetooth</title><updated>2026-01-19T14:50:58.731038+00:00</updated><content>&lt;doc fingerprint="eb88e4ab8a1a3419"&gt;
  &lt;main&gt;
    &lt;quote&gt;##\ ##\ ##\ ##\ ##\ ## | \__| ## | ## | ## | #######\ ##\ ######\ #######\ #######\ ######\ ######\ ## __##\ ## |\_## _| ## _____|## __##\ \____##\\_## _| ## | ## |## | ## | ## / ## | ## | ####### | ## | ## | ## |## | ## |##\ ## | ## | ## |## __## | ## |##\ ####### |## | \#### |\#######\ ## | ## |\####### | \#### | \_______/ \__| \____/ \_______|\__| \__| \_______| \____/&lt;/quote&gt;
    &lt;p&gt;bitchat is a decentralized peer-to-peer messaging application that operates over bluetooth mesh networks. no internet required, no servers, no phone numbers.&lt;/p&gt;
    &lt;p&gt;traditional messaging apps depend on centralized infrastructure that can be monitored, censored, or disabled. bitchat creates ad-hoc communication networks using only the devices present in physical proximity. each device acts as both client and server, automatically discovering peers and relaying messages across multiple hops to extend the network's reach.&lt;/p&gt;
    &lt;p&gt;this approach provides censorship resistance, surveillance resistance, and infrastructure independence. the network remains functional during internet outages, natural disasters, protests, or in regions with limited connectivity.&lt;/p&gt;
    &lt;p&gt; ios/macos version:&lt;lb/&gt; appstore: bitchat mesh&lt;lb/&gt; source code: https://github.com/permissionlesstech/bitchat&lt;lb/&gt; supports ios 16.0+ and macos 13.0+. build using xcode with xcodegen or swift package manager. &lt;/p&gt;
    &lt;p&gt; android version:&lt;lb/&gt; play store: bitchat&lt;lb/&gt; source code: https://github.com/permissionlesstech/bitchat-android&lt;lb/&gt; apk releases: https://github.com/permissionlesstech/bitchat-android/releases&lt;lb/&gt; supports android 8.0+ (api 26). full protocol compatibility with ios version. &lt;/p&gt;
    &lt;p&gt;technical whitepaper: whitepaper.md&lt;/p&gt;
    &lt;p&gt;the software is released into the public domain.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bitchat.free/"/><published>2026-01-19T07:14:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46676276</id><title>Radboud University selects Fairphone as standard smartphone for employees</title><updated>2026-01-19T14:50:57.137057+00:00</updated><content>&lt;doc fingerprint="6765084cc8137f49"&gt;
  &lt;main&gt;
    &lt;p&gt;The Fairphone is a sustainable smartphone with easily replaceable parts such as the battery and screen. This makes the device last longer. Fair and recycled materials, such as plastic and aluminium, are used as much as possible in the production of this smartphone. Fairphone also pays attention to good and safe working conditions in its factories.&lt;/p&gt;
    &lt;p&gt;Fairphones are issued to employees by the Information &amp;amp; Library Services (ILS) division. In addition to new Fairphones, the university can also reissue used Samsung devices where possible. These are Samsung devices that have already been returned and still meet the technical and age requirements. As long as these devices are still available, not every employee will receive a Fairphone immediately. Employees who have an iPhone from Radboud University can continue to use it as long as the device is still functioning. However, returned iPhones will no longer be reissued.&lt;/p&gt;
    &lt;p&gt;Employees who prefer to use their private phone for work can request an RU SIM card for this purpose. The costs for using your own device will not be reimbursed. Naturally, smartphone models that have already been issued will continue to be supported by ILS colleagues, as will privately purchased smartphone models used for work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cost-effective and easier management&lt;/head&gt;
    &lt;p&gt;Due to its longer lifespan, the total cost of a Fairphone is lower than that of comparable devices. In addition, Radboud University only needs to purchase, manage and support one standard model. This results in smaller stock, easier management and faster support. Manuals and instructions also only need to be maintained for one device.&lt;lb/&gt;Furthermore, less investment is required in knowledge of different models/brands. This also helps to speed up incident handling and, where necessary, smartphone replacement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Circularity strategy&lt;/head&gt;
    &lt;p&gt;Fairphone offers a five-year warranty and long-term software support for up to eight years. This means that devices need to be replaced less quickly. This fits in with Radboud University's circularity strategy, which focuses on the longest possible use and reuse of ICT hardware.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees"/><published>2026-01-19T08:23:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46677106</id><title>Wikipedia: WikiProject AI Cleanup</title><updated>2026-01-19T14:50:56.825502+00:00</updated><content>&lt;doc fingerprint="fa34da17339063c6"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Wikipedia:WikiProject AI Cleanup&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Main page&lt;/cell&gt;&lt;cell&gt;Discussion&lt;/cell&gt;&lt;cell&gt;Noticeboard&lt;/cell&gt;&lt;cell&gt;Guide&lt;/cell&gt;&lt;cell&gt;Resources&lt;/cell&gt;&lt;cell&gt;Policies&lt;/cell&gt;&lt;cell&gt;Research&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;This is a WikiProject, an open group of Wikipedia editors. New participants are welcome; feel free to talk to us! &lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Wikipedia editors are making a guide to identifying AI-generated writing and the kinds of problems it tends to introduce. Your contributions are welcome!&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Welcome to WikiProject AI Cleanup, a collaboration to combat the increasing problem of unsourced, poorly written AI-generated content on Wikipedia. If you would like to help, add yourself as a participant in the project, inquire on the talk page, and see the to-do list.&lt;/p&gt;&lt;head rend="h2"&gt;Goals&lt;/head&gt;[edit]&lt;p&gt;Since 2022, large language models (LLMs) like GPTs have become a convenient tool for writing at scale. Unfortunately, these models virtually always fail to properly source claims and often introduce errors. Essays like WP:LLM strongly encourage care in using them for editing articles. These are the project's goals:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;To identify text written by AI, and proofread such text to make sure they follow Wikipedia's policies. Any unsourced or likely inaccurate claims should be removed.&lt;/item&gt;&lt;item&gt;To identify AI-generated images and ensure appropriate usage.&lt;/item&gt;&lt;item&gt;To help and keep track of AI-using editors who may not realize the deficiencies of AI as a writing tool.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The purpose of this project is not to restrict or ban the use of AI in articles, but to verify that its output is acceptable and constructive, and to fix or remove it otherwise.&lt;/p&gt;&lt;head rend="h3"&gt;Editing advice&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Tag articles with appropriate templates, remove unsourced information and warn users who add unsourced AI-generated content to articles.&lt;/item&gt;&lt;item&gt;Articles that are clearly entirely LLM-generated pages without human review can be nominated for speedy deletion under WP:G15.&lt;/item&gt;&lt;item&gt;Identifying AI-assisted edits is difficult in most cases since the generated text is often indistinguishable from human text. The signs of AI writing page provides a list of characteristics that are associated with text generated by AI chatbots. &lt;list rend="ul"&gt;&lt;item&gt;Text that was present in an article before November 30, 2022 (the release date of ChatGPT) is very unlikely to be AI-generated.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;AI content is not always "unsourced"—sometimes it has real sources that are unrelated to the article's topic, sometimes it creates its own fake sources, and sometimes it uses legitimate sources to create the AI content. Be careful when removing bad AI content not to remove legitimate sources, and always check the cited sources for legitimacy. &lt;list rend="ul"&gt;&lt;item&gt;Example: the article Leninist historiography was entirely written by AI included a list of completely fake sources in Russian and Hungarian at the bottom of the page. Google turned up no results for these sources.&lt;/item&gt;&lt;item&gt;Other example: the article Estola albosignata, about a beetle species, had paragraphs written by AI sourced to actual German and French sources. While the sourced articles were real, they were completely off-topic, with the French one discussing a completely unrelated lifeform.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Sometimes entire articles are AI-generated, and in such a case, make sure to check that the topic is legitimate and notable. Occasionally, WP:HOAXes have made it onto Wikipedia because AI tools can create fake citations that may appear legitimate. &lt;list rend="ul"&gt;&lt;item&gt;Example: the article Amberlihisar was created in January 2023, passed articles for creation, and was not discovered to be entirely fictional until December 2023. It has since now been deleted.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Open tasks&lt;/head&gt;[edit]&lt;p&gt;See Category:Articles containing suspected AI-generated texts for all articles that have been tagged as possibly {{AI-generated}}. The tasks page recommends ways to handle articles, talk page discussions, and sources that use AI-generated content.&lt;/p&gt;&lt;head rend="h2"&gt;Participants&lt;/head&gt;[edit]&lt;p&gt;Primary contacts:&lt;/p&gt;&lt;p&gt;Feel free to add yourself here!&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;3df (talk) 02:59, 4 December 2023 (UTC) - founding member&lt;/item&gt;&lt;item&gt;Chaotıċ Enby (talk · contribs) 03:00, 4 December 2023 (UTC) - founding member&lt;/item&gt;&lt;item&gt;charlotte 👸♥ - founding member&lt;/item&gt;&lt;item&gt;ARandomName123 (talk · contribs) 03:02, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Fermiboson (talk) 03:03, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Kline • talk to me! • contribs 03:04, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;sawyer / talk 03:04, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;LilianaUwU (talk / contributions) 03:15, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Ca talk to me! 03:45, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Neonorange (talk to Phil) (he, they) 09:02, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Jondvdsn1 (talk) 11:40, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Chlod (say hi!) 16:59, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;TheBritinator (talk) 17:03, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Generalissima (talk) 17:55, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Anemonemma (talk) 18:39, 4 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Vermont (🐿️—🏳️🌈) 00:30, 5 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Est. 2021 (talk · contribs) 11:19, 5 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Alalch E. 23:56, 5 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;🌙Eclipse (talk) (contribs) 18:05, 6 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;jp×g🗯️ 01:29, 7 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Fuzheado | Talk 11:37, 8 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Aurodea108 (talk) 05:04, 13 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Cremastra (talk) 22:11, 14 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;DrowssapSMM 23:40, 19 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;EspWikiped (talk) 15:34, 20 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;Logie1 (talk) 01:58, 23 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;skarz (talk) 19:57, 24 December 2023 (UTC)&lt;/item&gt;&lt;item&gt;DoubleGrazing (talk) 12:31, 15 January 2024 (UTC)&lt;/item&gt;&lt;item&gt;Remsense诉 03:13, 8 February 2024 (UTC)&lt;/item&gt;&lt;item&gt;Geardona (talk to me?) 23:59, 12 February 2024 (UTC)&lt;/item&gt;&lt;item&gt;Elsa_Versailles (talk) 22:11, 23 February 2024 (UTC)&lt;/item&gt;&lt;item&gt;Davidvacca 13:24, 24 February 2024 (UTC)&lt;/item&gt;&lt;item&gt;Adleid (talk) 08:10, 12 March 2024 (UTC)&lt;/item&gt;&lt;item&gt;Ljleppan (talk) 08:12, 12 March 2024 (UTC)&lt;/item&gt;&lt;item&gt;Yamantakks (talk) 03:26, 19 March 2024 (UTC)&lt;/item&gt;&lt;item&gt;GraziePrego (talk) 05:53, 3 April 2024 (UTC)&lt;/item&gt;&lt;item&gt;neonmoon227(talk)10:27, 28 April 2024 (UTC)&lt;/item&gt;&lt;item&gt;Florificapis (talk) 15:00, 24 May 2024 (UTC)&lt;/item&gt;&lt;item&gt;CaroleHenson (talk) 04:23, 26 May 2024 (UTC)&lt;/item&gt;&lt;item&gt;Awhellnawr123214 (talk) 23:29, 26 May 2024 (UTC)&lt;/item&gt;&lt;item&gt;The WordsmithTalk to me 23:31, 29 May 2024 (UTC)&lt;/item&gt;&lt;item&gt;Acebulf (talk | contribs) 01:33, 17 June 2024 (UTC)&lt;/item&gt;&lt;item&gt;CycoMa2&lt;/item&gt;&lt;item&gt;Epsilon02 (talk) 00:59, 23 July 2024 (UTC)&lt;/item&gt;&lt;item&gt;Rxp392 18 Aug 2024 (EST)&lt;/item&gt;&lt;item&gt;SecretSpectre (talk) 07:30, 30 August 2024 (UTC)&lt;/item&gt;&lt;item&gt;Miniapolis 21:10, 26 September 2024 (UTC)&lt;/item&gt;&lt;item&gt;Dan Leonard • talk • contribs 20:17, 27 September 2024 (UTC)&lt;/item&gt;&lt;item&gt;rsjaffe 🗣️ 15:47, 2 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Ravinesgal (talk) 13:48, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;DJ Cane (he/him) (Talk) 14:26, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;GreatBritant (talk) 14:30, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Logical Luna (talk) 14:34, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;GordonGlottal (talk) 14:57, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Spinixster (trout me!) 15:17, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Ebishirl (talk) 16:00, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Corundum Conundrum (CC) 20:11, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;ElectronicsForDogs (talk) 23:13, 9 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;OsFish (talk) 05:36, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Wil540 art (talk) 09:37, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;W0nderhat (talk) 11:19, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Matt Heard (talk) 11:46, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Jambutheplant (talk) 12:21, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Cmrc23 ʕ•ᴥ•ʔ 18:38, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Tantomile (talk) 22:19, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;SirMemeGod 23:02, 10 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Lunaroxas (talk) 06:30, 11 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Yaris678 (talk) 14:20, 11 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Northern-Virginia-Photographer (talk) 15:10, 11 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Boredintheevening (talk) 19:33, 11 October (UTC)&lt;/item&gt;&lt;item&gt;Q T C 20:02, 11 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Svampesky (talk) 17:30, 12 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Lalalalala7 (talk) 02:50, 13 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Delabrede (talk) 18:55, 13 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Alecasa (talk) 14:59, 14 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;podstawko ●talk 20:28, 14 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Smallangryplanet (talk) 08:31, 15 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;&amp;lt;&amp;gt;Plasticwonder (talk) 20:03, 15 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;jlwoodwa (talk) 17:10, 16 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Sohom (talk) 15:27, 19 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;ABG (Talk/Report any mistakes here) 13:38, 20 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;The Cunctator (talk) 19:15, 22 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Jenny8lee (talk) 22:36, 22 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;SamHolt6 (talk) 02:35, 23 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Heylenny (talk) 07:31, 24 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Junemoon19 (talk) 09:08, 24 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;scope_creepTalk 13:53, 24 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;Imconfused3456talk 01:25, 25 October 2024 (UTC)&lt;/item&gt;&lt;item&gt;K.Yuzen 67854 (talk) 13:01, 3 November 2024 (UTC)&lt;/item&gt;&lt;item&gt;LaMèreVeille (talk) 10:41, 4 November 2024 (UTC)&lt;/item&gt;&lt;item&gt;ランボル (talk) 11:45, 9 November 2024 (UTC)&lt;/item&gt;&lt;item&gt;StartGrammarTime (talk) 12:52, 13 November 2024 (UTC)&lt;/item&gt;&lt;item&gt;&lt;code&gt;/etc/owuh $ (💬 | she/her)&lt;/code&gt;01:35, 27 November 2024 (UTC)&lt;/item&gt;&lt;item&gt;User:Milchsee 06:42, December 2 2024 (UTC)&lt;/item&gt;&lt;item&gt;GastelEtzwane (talk) 08:24, 5 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;User:NekokCH 06:42, December 2 2024 (UTC)&lt;/item&gt;&lt;item&gt;user:Skeletons are the axiom 8:22 December 5 (CST)&lt;/item&gt;&lt;item&gt;LordXavier15 (talk) 19:48, 9 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Vulcan❯❯❯Sphere! 02:43, 11 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;ForsythiaJo (talk) 18:36, 14 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Doug Weller talk 13:13, 23 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Fantastic Mr. Fox (talk) 14:00, 23 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Especially as this keeps popping up at WP:GAN, CMD (talk) 06:41, 27 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Sangsangaplaz (Talk to me! I'm willing to help) 10:59, 29 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Qcne I see so much AI slop when reviewing drafts. qcne (talk) 22:32, 29 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;Marleeashton 19 January 2026 T 12:13 (UTC)&lt;/item&gt;&lt;item&gt;--A09|(talk) 16:57, 31 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;—pythoncoder (talk | contribs) 21:34, 31 December 2024 (UTC)&lt;/item&gt;&lt;item&gt;PrismEnder 01:59, 4 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;DM5Pedia 03:13, 5 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;Apocheir (talk) 03:58, 5 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;Belbury (talk) 13:31, 7 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;UnlatchedCursor (talk) 06:12, 9 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;Cdjp1 (talk) 15:06, 22 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;Kwlla (talk to me?? :3) 15:02, 24 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;S. Perquin (talk) – 10:07, 25 January 2025 (UTC), I find it interesting to think about whether or not to allow certain AI-generated images.&lt;/item&gt;&lt;item&gt;Heart6008 (talk) 00:40, 30 January 2025 (UTC)&lt;/item&gt;&lt;item&gt;Dronebogus (talk) 22:14, 17 March 2025 (UTC)&lt;/item&gt;&lt;item&gt;Zentavious (talk) 13:49, 20 March 2025 (UTC)&lt;/item&gt;&lt;item&gt;Seanbhean-chríonna-caite (talk to me/slap me with a fish as needed) 14:56, 22 March 2025 (UTC)&lt;/item&gt;&lt;item&gt;jellyfish ✉ 21:45, 4 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;User:MMr.Bat message: I am new but I will do my best 17:10, 5 April 2025 (JTC)&lt;/item&gt;&lt;item&gt;Rkieferbaum (talk) 14:18, 6 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;Autarch (talk) 11:23, 8 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;Somajyoti ✉ 08:19, 11 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;Rafstr (talk) 08:23, 11 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;~/Bunnypranav:&amp;lt;ping&amp;gt; 11:11, 11 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;Sophisticatedevening🍷(talk) 00:09, 23 April 2025 (UTC)&lt;/item&gt;&lt;item&gt;Dandykong1 (talk) 17:13, 1 May 2025 (UTC)&lt;/item&gt;&lt;item&gt;Redivy (talk) 09:44, 4 May 2025 (UTC)&lt;/item&gt;&lt;item&gt;— Newslinger talk 08:51, 19 May 2025 (UTC)&lt;/item&gt;&lt;item&gt;CoffeeCrumbs 14:47, 21 May 2025 (UTC)&lt;/item&gt;&lt;item&gt;QuillThrills (talk) 14:33, 24 May 2025 (UTC)&lt;/item&gt;&lt;item&gt;— BE243 (about | talk) 09:44, 6 June 2025 (UTC)&lt;/item&gt;&lt;item&gt;Altoids0 (talk) 02:54, 9 June 2025 (UTC), lets mop some slop!&lt;/item&gt;&lt;item&gt;DrShirleyTempleton (talk) 10:17, 12 June 2025 (UTC)&lt;/item&gt;&lt;item&gt;Paprikaiser (talk · contribs) 21:49:32, 12 June 2025 (UTC)&lt;/item&gt;&lt;item&gt;Trojan Dreadnought (talk) 00:47, 13 June 2025 (UTC), because I hate "AI".&lt;/item&gt;&lt;item&gt;MeowsyCat99 (talk) 15:11, 13 June 2025 (UTC)&lt;/item&gt;&lt;item&gt;User:Tankishguy Chat 06:55, 27 June 2025 (UTC) i hate ai.&lt;/item&gt;&lt;item&gt;User: ChrisWBer (talk)16:52, 30 June 2025 (UTC) AI is stupid.&lt;/item&gt;&lt;item&gt;Hdialk (talk) 23:03, 2 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;Giorgx12 (msg) 09:15, 4 July 2025&lt;/item&gt;&lt;item&gt;SuperPianoMan9167 (talk) I already contributed to this project as an IP and finally got around to making an account :) 05:30, 5 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;David Palmer//cloventt (talk) 01:36, 11 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;Guettarda (talk) 14:36, 11 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;GoldRomean (talk) 23:30, 13 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;Enjoyman (talk) 10:48, 17 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;SunloungerFrog (talk) 13:29, 20 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;16kTheFox (talk to me!) 22:44, 23 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;Knowswheretheirtowelis 19:57, 28 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;Vacant0 (talk • contribs) 20:05, 28 July 2025 (UTC)&lt;/item&gt;&lt;item&gt;Nhlesser (talk) 15:55, 1 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Patient Zerotalk 23:07, 4 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Rosaece ♡ talk ♡ contributions 22:01, 8 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Gnomingstuff (talk) 04:31, 11 August 2025 (UTC) -- I plan to mostly focus on undetected edits.&lt;/item&gt;&lt;item&gt;Jay 💬 15:44, 11 August 2025 (UTC) - Separating Good from Evil!&lt;/item&gt;&lt;item&gt;scenography (talk) 00:43, 12 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;CoconutOctopus talk 10:00, 12 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Augmented Seventh🎱 17:06, 13 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Best wishes, Macaw*! 02:15, 17 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;DolyaIskrina (talk) 19:31, 18 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;The Wonk (talk) 16:20, 20 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;ʊnƌer◙swamȹᵗᵅᵜᵏ 17:09, 20 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;OXScience_Pat (talk) 08:21, 21 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Nil🥝 05:07, 25 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;OneOfYour5ADay 22:07, 25 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Hayasynth (talk) 02:28, 26 August 2025 (UTC)&lt;/item&gt;&lt;item&gt;Sarsenet•he/they•(talk) 12:25, 27 August 2025 (UTC) - officially join, I've been doing AI cleanup in the wild for a bit&lt;/item&gt;&lt;item&gt;User:Oliver.ophb 04:02, 30 August 2025 (UTC) - synthesize and generate with AI, revise with eyes&lt;/item&gt;&lt;item&gt;Vanilla Wizard 💙 16:03, 30 August 2025 (UTC) - focusing on AI slop at AfC &amp;amp; edits that trip filter #1325.&lt;/item&gt;&lt;item&gt;Trivialist (talk) 22:51, 15 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;NicheSports (talk) 20:09, 16 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;Lijil (talk) 06:55, 19 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;Sodium hypobromite (talk) 17:36, 22 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;RandFreeman (talk) 03:21, 26 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;Muktee1494 (talk) 15:51, 27 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;Umar2z(💬) 18:41, 27 September 2025 (UTC)&lt;/item&gt;&lt;item&gt;EatingCarBatteries (contributions, talk) 05:51, 1 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Gurkubondinn (talk) 14:34, 4 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Orange sticker (talk) 12:05, 16 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;– Quinn ΘΔ 12:36, 16 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Dr vulpes (Talk) 18:25, 20 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Merko (talk) 19:45, 20 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Athanelar (talk) 19:14, 22 October 2025 (UTC) - staunch LLM abolitionist. I'm doing my part!&lt;/item&gt;&lt;item&gt;sjones23 (talk - contributions) 19:26, 22 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Z E T A3 21:55, 22 October 2025 (UTC) Here goes nothing&lt;/item&gt;&lt;item&gt;〜 Festucalex • talk 04:03, 26 October 2025 (UTC) I support a full, total, comprehensive, all-encompassing, draconian, butlerian-jihadist ban on LLM usage on Wikipedia. No wiggle room for vandals.&lt;/item&gt;&lt;item&gt;Lovelyfurball (talk) 02:27, 28 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;mwwv converse∫edits 13:52, 28 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;-Samoht27 (talk) 20:57, 28 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;~ Argenti Aertheri(Chat?) 21:52, 28 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;ScrabbleTiles (talk) 09:40, 29 October 2025 (UTC)&lt;/item&gt;&lt;item&gt;Викидим (talk) 02:56, 3 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;DarklitShadow (talk) 19:37, 6 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;Go D. Usopp (talk) 06:20, 7 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;&lt;code&gt;dot.py&lt;/code&gt;04:23, 8 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;Knight U (talk) 07:31, 12 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;-- .nhals8 (puhLEASE ping when responding) 11:01, 12 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;Eurostarguage(talk) 19:38, 12 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;Lazman321 (talk) 01:08, 16 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;EvenTwist41 (talk) 16:48, 28 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;N7fty (talk) 18:46, 29 November 2025 (UTC)&lt;/item&gt;&lt;item&gt;Bababashqort (talk) 14:35, 9 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;NenadWeber (talk) 15:08, 9 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;Smallangryplanet (talk) 17:26, 10 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;Jībanmṛtamessage 17:33, 12 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;GreenRedFlag (talk) 15:08, 29 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;Zygmeyer (talk) 15:12, 29 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;Valjean (talk) (PING me) 15:37, 29 December 2025 (UTC)&lt;/item&gt;&lt;item&gt;rfqii talk! 07:21, 1 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;Kqol (talk) 18:48, 5 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;Revolving Doormat (talk) 03:10, 5 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;nf utvol (talk) 14:45, 8 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;ThatTrainGuy1945 (talk) 2:12, 9 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;User:Cr0dhonn (talk) 21:35, 14 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;User:Mtbbk (talk) 13:47, 19 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;User:Pedro_Silva_Santos (talk) 13:55, 19 January 2026 (UTC)&lt;/item&gt;&lt;item&gt;RadicalHarmony (talk) 12:13, 19 January 2026 (UTC)&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Resources&lt;/head&gt;[edit]&lt;head rend="h3"&gt;Essays&lt;/head&gt;[edit]&lt;head rend="h3"&gt;Information&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;AI - Article text generation&lt;/item&gt;&lt;item&gt;Perennial sources - Large language models&lt;/item&gt;&lt;item&gt;LLM dungeon, a list of LLM-created articles with bogus sources maintained by JPxG&lt;/item&gt;&lt;item&gt;LLM demonstration 1 &amp;amp; LLM demonstration 2, experiments with AI and Wikipedia done by JPxG&lt;/item&gt;&lt;item&gt;AI Images and German Wikipedia&lt;/item&gt;&lt;item&gt;Academic sources regarding synthetic content&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Relevant discussions&lt;/head&gt;[edit]&lt;p&gt;These threads may be useful for editors seeking information about how AI has previously been handled on Wikipedia.&lt;/p&gt;&lt;p&gt;Want to update this table? Try using the visual editor to edit this page.&lt;/p&gt;&lt;head rend="h3"&gt;Project resources&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;List of uses of ChatGPT at Wikipedia&lt;/item&gt;&lt;item&gt;Articles using ChatGPT as a reference&lt;/item&gt;&lt;item&gt;AI images in non-AI contexts&lt;/item&gt;&lt;item&gt;Wikipedia:Signs of AI writing&lt;/item&gt;&lt;item&gt;AI cleanup thread in the Wikimedia discord&lt;/item&gt;&lt;item&gt;Wikipedia:WikiProject AI Cleanup/VWF bot log, an automated log of images categorised as AI/upscaled on Commons which are in use on Wikipedia. It updates every Sunday, using the script at User:DreamRimmer/commonsfileusage.py, and has an ignore list for AI-related articles.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup"/><published>2026-01-19T10:09:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46677212</id><title>RISC-V is coming along quite speedily: Milk-V Titan Mini-ITX 8-core board</title><updated>2026-01-19T14:50:56.520015+00:00</updated><content>&lt;doc fingerprint="d9b3d334348f0de2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Milk-V Titan Mini-IX board with UR-DP1000 processor shows RISC-V ecosystem taking shape — M.2, DDR4, and PCIe card support form a kit that you can use out of the box&lt;/head&gt;
    &lt;p&gt;RISC-V is coming along quite speedily.&lt;/p&gt;
    &lt;p&gt;The RISC-V ecosystem might still be a nascent one, but it's definitely starting to take shape. You can now order the Milk-V Titan full-featured Mini-ITX motherboard kit with an integrated Ultra-RISC UR-DP1000 CPU (RISC-V), all with standard hardware, and ready to roll.&lt;/p&gt;
    &lt;p&gt;Although this isn't strictly the first such offering, it's one of the few on the market that combines complete feature, out-of-the-box usability, and a reasonable price. The motherboard is a pretty plain Mini-ITX model, but in a good way. It supports up to 64 GB of DDR4 RAM in a dual-channel setup at up to 3200 MT/s, and has one M.2 slot, USB-A and USB-C ports, Gigabit Ethernet, and BMC (out-of-band management) ports.&lt;/p&gt;
    &lt;p&gt;The only notable omission is integrated graphics, as you'll have to make use of the available PCIe x16 slot to plug in your own graphics card. As RISC-V is for practical purposes an entirely new platform, graphics driver support is still somewhat spotty. Older Radeons (7000 series and previous) are known to work well, but this very statement is likely to change quite quickly.&lt;/p&gt;
    &lt;p&gt;There are no audio ports on the board, but that's unlikely to be a deal-breaker as you can always use USB audio devices. Besides, these boards are aimed at development work anyway. In fact, there are even 3-pin UART and USB-C connector for CPU debugging purposes. The idle power consumption is apparently pretty high at 14 W, but that's not likely to matter for development purposes.&lt;/p&gt;
    &lt;p&gt;As for the Ultra-RISC UR-DP1000 CPU itself, it's an eight-core setup with four two-cluster cores, each loaded with 4 MB of L3 cache, for a total of 16 MB. It is fully compliant with the RVA22 profile (RVA specs are CPU instruction sets), and there's support for the RVA23 except for the V (vector) extension. It's important to note this CPU supports hardware virtualization, so you can use hypervisors with it. And, of course, at only 2 GHz on a nascent platform, keep your performance expectations tempered.&lt;/p&gt;
    &lt;p&gt;You can run Ubuntu on the The Milk-V Titan right out of the box. The kit available is for preorder now at Arace Tech. The standard price is $329 or 288€, but there's a $50 discount for preorders, so make that $279 in practice, a pretty reasonable amount. And since it uses DDR4 RAM, you might be able to get the memory for less than the entire board.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Bruno Ferreira is a contributing writer for Tom's Hardware. He has decades of experience with PC hardware and assorted sundries, alongside a career as a developer. He's obsessed with detail and has a tendency to ramble on the topics he loves. When not doing that, he's usually playing games, or at live music shows and festivals.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;bit_user&lt;/header&gt;Found some performance claims, here:Reply&lt;lb/&gt;https://www.cnx-software.com/2025/07/22/three-high-performance-risc-v-processors-to-watch-in-h2-2025-ultrarisc-ur-dp1000-zizhe-a210-and-spacemit-k3/&lt;lb/&gt;Unfortunately, it's not very easy to find SPEC2006 data on other common CPUs, but you can at least use it to compare integer performance of two CPU listed above.&lt;lb/&gt;Also, they claim the PCIe port is electrically x16 and PCIe 4.0.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Findecanor&lt;/header&gt;Reply&lt;quote&gt;there's support for the RVA23 except for the V (vector) extension.&lt;/quote&gt;No. The vector extension is mandatory in RVA23.&lt;lb/&gt;What is true is that "It would have supported RVA23 if only it had the V (vector) extension".&lt;lb/&gt;I have seen an experiment with emulating a few scalar extensions in RVA23 by trapping to emulation in software. It works... with some performance penalty compared to code compiled not to use those extensions.&lt;lb/&gt;The Vector extension, however, is quite a big extension and I think it would require some significant engineering effort to create an emulator for, at even lower performance compared to not compiling to use it.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/pc-components/cpus/milk-v-titan-mini-ix-board-with-ur-dp1000-processor-shows-risc-v-ecosystem-taking-shape-m-2-ddr4-and-pcie-card-support-form-a-kit-that-you-can-use-out-of-the-box"/><published>2026-01-19T10:20:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46677436</id><title>Nuclear elements detected in West Philippine Sea</title><updated>2026-01-19T14:50:56.005037+00:00</updated><content>&lt;doc fingerprint="56c8e5bcdd0ca59c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Nuclear elements detected in West Philippine Sea&lt;/head&gt;
    &lt;p&gt;MANILA, Philippines — The University of the Philippines Marine Science Institute (UP MSI) has detected elevated levels of iodine-129 – an isotope commonly used as an indicator of nuclear activity – in seawater samples from the West Philippine Sea (WPS).&lt;/p&gt;
    &lt;p&gt;UP MSI said the concentrations found in the WPS were higher than in any other part of the country, despite the Philippines having no active nuclear power plant or nuclear weapons program.&lt;/p&gt;
    &lt;p&gt;The findings are based on an analysis of 119 seawater samples collected from the WPS, the Philippine Rise, the Sulu Sea and other areas across the archipelago.&lt;/p&gt;
    &lt;p&gt;Researchers found iodine-129 levels in the WPS to be about 1.5 to 1.7 times higher than those recorded in other sampling sites.&lt;/p&gt;
    &lt;p&gt;The study was conducted by experts from the Department of Science and Technology-Philippine Nuclear Research Institute, UP MSI’s Geological Oceanography Laboratory and the University of Tokyo.&lt;/p&gt;
    &lt;p&gt;The team traced the likely source of the isotope to the Yellow Sea.&lt;/p&gt;
    &lt;p&gt;UP MSI said the results were consistent with recent Chinese studies linking iodine-129 in the Yellow Sea to decades-old nuclear weapons tests and nuclear fuel reprocessing facilities in Europe, which released the isotope into soils and rivers in northeastern China.&lt;/p&gt;
    &lt;p&gt;The study added that iodine-129 may have reached Philippine waters through ocean circulation systems, particularly the Yellow Sea Coastal Current and the Chinese Coastal Current, though further oceanographic modeling is needed to confirm the transport pathways.&lt;/p&gt;
    &lt;p&gt;While iodine-129 is radioactive, the researchers said its current levels in the WPS pose no threat to human health or the environment.&lt;/p&gt;
    &lt;p&gt;They also underscored the need to strengthen monitoring and regulation of radioactive materials, especially those that cross national boundaries.&lt;/p&gt;
    &lt;p&gt;The research was funded by the DOST-National Research Council of the Philippines and the DOST-Philippine Council for Agriculture and Resources Research and Development.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Latest&lt;/item&gt;
      &lt;item&gt;Trending&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.philstar.com/headlines/2026/01/18/2501750/nuclear-elements-detected-west-philippine-sea"/><published>2026-01-19T10:46:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46677628</id><title>Nvidia Contacted Anna's Archive to Access Books</title><updated>2026-01-19T14:50:55.788413+00:00</updated><content>&lt;doc fingerprint="8e658bccde9911c9"&gt;
  &lt;main&gt;
    &lt;p&gt;Chip giant NVIDIA has been one of the main financial beneficiaries in the artificial intelligence boom.&lt;/p&gt;
    &lt;p&gt;Revenue surged due to high demand for its AI-learning chips and data center services, and the end doesn’t appear to be in sight.&lt;/p&gt;
    &lt;p&gt;Besides selling the most sought-after hardware, NVIDIA is also developing its own models, including NeMo, Retro-48B, InstructRetro, and Megatron. These are trained using their own hardware and with help from large text libraries, much like other tech giants do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Authors Sue NVIDIA for Copyright Infringement&lt;/head&gt;
    &lt;p&gt;Like other tech companies, NVIDIA has also seen significant legal pushback from copyright holders in response to its training methods. This includes authors, who, in various lawsuits, accused tech companies of training their models on pirated books.&lt;/p&gt;
    &lt;p&gt;In early 2024, for example, several authors sued NVIDIA over alleged copyright infringement.&lt;/p&gt;
    &lt;p&gt;Through the class action lawsuit, they claimed that the company’s AI models were trained on the Books3 dataset that included copyrighted works taken from the ‘pirate’ site Bibliotik. Since this happened without permission, the authors demanded compensation.&lt;/p&gt;
    &lt;p&gt;In response, NVIDIA defended its actions as fair use, noting that books are nothing more than statistical correlations to its AI models. However, the allegations didn’t go away. On the contrary, the plaintiffs found more evidence during discovery.&lt;/p&gt;
    &lt;head rend="h2"&gt;‘NVIDIA Contacted Anna’s Archive’&lt;/head&gt;
    &lt;p&gt;Last Friday, the authors filed an amended complaint that significantly expands the scope of the lawsuit. In addition to adding more books, authors, and AI models, it also includes broader “shadow library” claims and allegations.&lt;/p&gt;
    &lt;p&gt;The authors, including Abdi Nazemian, now cite various internal Nvidia emails and documents, suggesting that the company willingly downloaded millions of copyrighted books.&lt;/p&gt;
    &lt;p&gt;The new complaint alleges that “competitive pressures drove NVIDIA to piracy”, which allegedly included collaborating with the controversial Anna’s Archive library.&lt;/p&gt;
    &lt;p&gt;According to the amended complaint, a member of Nvidia’s data strategy team reached out to Anna’s Archive to find out what the pirate library could offer the trillion-dollar company&lt;/p&gt;
    &lt;p&gt;“Desperate for books, NVIDIA contacted Anna’s Archive—the largest and most brazen of the remaining shadow libraries—about acquiring its millions of pirated materials and ‘including Anna’s Archive in pre-training data for our LLMs’,” the complaint notes.&lt;/p&gt;
    &lt;p&gt;“Because Anna’s Archive charged tens of thousands of dollars for ‘high-speed access’ to its pirated collections […] NVIDIA sought to find out what “high-speed access” to the data would look like.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Anna’s Archive Points Out Legal ‘Concern’&lt;/head&gt;
    &lt;p&gt;According to the complaint, Anna’s Archive then warned Nvidia that its library was illegally acquired and maintained. Because the site previously wasted time on other AI companies, the pirate library asked NVIDIA executives if they had internal permission to move forward.&lt;/p&gt;
    &lt;p&gt;This permission was allegedly granted within a week, after which Anna’s Archive provided the chip giant with access to its pirated books.&lt;/p&gt;
    &lt;p&gt;“Within a week of contacting Anna’s Archive, and days after being warned by Anna’s Archive of the illegal nature of their collections, NVIDIA management gave ‘the green light’ to proceed with the piracy. Anna’s Archive offered NVIDIA millions of pirated copyrighted books.”&lt;/p&gt;
    &lt;p&gt;The complaint states that Anna’s Archive promised to provide NVIDIA with access to roughly 500 terabytes of data. This included millions of books that are usually only accessible through Internet Archive’s digital lending system, which itself has been targeted in court.&lt;/p&gt;
    &lt;p&gt;The complaint does not explicitly mention whether NVIDIA ended up paying Anna’s Archive for access to the data.&lt;/p&gt;
    &lt;p&gt;Additionally, it’s worth mentioning that NVIDIA also stands accused of using other pirated sources. In addition to the previously included Books3 database, the new complaint also alleges that the company downloaded books from LibGen, Sci-Hub, and Z-Library.&lt;/p&gt;
    &lt;head rend="h2"&gt;Direct and Vicarious Copyright Infringement&lt;/head&gt;
    &lt;p&gt;In addition to downloading and using pirated books for its own AI training, the authors allege NVIDIA distributed scripts and tools that allowed its corporate customers to automatically download “The Pile“, which contains the Books3 pirated dataset.&lt;/p&gt;
    &lt;p&gt;These allegations lead to new claims of vicarious and contributory infringement, alleging that NVIDIA generated revenue from customers by facilitating access to these pirated datasets.&lt;/p&gt;
    &lt;p&gt;Based on these and other claims, the authors request to be compensated for the damages they suffered. This applies to the named authors, but also to potentially hundreds of others who may later join the class action lawsuit.&lt;/p&gt;
    &lt;p&gt;As far as we know, this is the first time that correspondence between a major U.S. tech company and Anna’s Archive was revealed in public. This will only raise the profile of the pirate library, which just lost several domain names, even further.&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;A copy of the first consolidated and amended complaint, filed at the U.S. District Court for the Northern District of California, is available here (pdf). The named authors include Abdi Nazemian, Brian Keene, Stewart O’Nan, Andre Dubus III, and Susan Orlean.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://torrentfreak.com/nvidia-contacted-annas-archive-to-secure-access-to-millions-of-pirated-books/"/><published>2026-01-19T11:11:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46677918</id><title>Robust Conditional 3D Shape Generation from Casual Captures</title><updated>2026-01-19T14:50:55.659134+00:00</updated><content>&lt;doc fingerprint="6521ebef3f7d29a6"&gt;
  &lt;main&gt;
    &lt;p&gt;Robust Conditional 3D Shape Generation from Casual Captures&lt;/p&gt;
    &lt;p&gt;From an input image sequence, ShapeR preprocesses per-object multimodal data (SLAM points, images, captions). A rectified flow transformer then conditions on these inputs to generate meshes object-centrically, producing a full metric scene reconstruction.&lt;/p&gt;
    &lt;p&gt;ShapeR performs generative, object-centric 3D reconstruction from image sequences by leveraging multimodal inputs and robust training strategies. First, off-the-shelf SLAM and 3D instance detection are used to compute 3D points and object instances. For each object, sparse points, relevant images, 2D projections, and VLM captions are extracted to condition a rectified flow model, which denoises a latent VecSet to produce the 3D shape. The use of multimodal conditioning, along with heavy on-the-fly compositional augmentations and curriculum training, ensures the robustness of ShapeR in real-world scenarios.&lt;/p&gt;
    &lt;p&gt;ShapeR conditions on a range of modalities, including the object's posed multiview images, SLAM points, text descriptions, and 2D point projections.&lt;/p&gt;
    &lt;p&gt;ShapeR leverages single-object pretraining with extensive augmentations, simulating realistic backgrounds, occlusions, and noise across images and SLAM inputs.&lt;/p&gt;
    &lt;p&gt;ShapeR is fine-tuned on object-centric crops from Aria Synthetic Environment scenes, which feature realistic image occlusions, SLAM point cloud noise, and inter-object interaction.&lt;/p&gt;
    &lt;p&gt;For even more detail, refer to the paper.&lt;/p&gt;
    &lt;p&gt;ShapeR comes with a new evaluation dataset of in-the-wild sequences with paired posed multi-view images, SLAM point clouds, and individually complete 3D shape annotations for 178 objects across 7 diverse scenes. In contrast to existing real-world 3D reconstruction datasets which are either captured in controlled setups or have merged object and background geometries or incomplete shapes, this dataset is designed to capture real-world challenges like occlusions, clutter, and variable resolution and viewpoints to enable realistic, in-the-wild evaluation.&lt;/p&gt;
    &lt;p&gt;SAM 3D Objects marks a significant improvement in shape generation, but it lacks metric accuracy and requires interaction. Since it can only exploit a single view, it can sometimes fail to preserve correct aspect ratios, relative scales, and object layouts in complex scenes such as shown in the example here.&lt;/p&gt;
    &lt;p&gt;ShapeR solves this by leveraging image sequences and multimodal data (such as SLAM points). By integrating multiple posed views, ShapeR automatically produces metrically accurate and consistent reconstructions. Unlike interactive single-image methods, ShapeR robustly handles casually captured real-world scenes, generating high-quality metric shapes and arrangements without requiring user interaction.&lt;/p&gt;
    &lt;p&gt;Notably, ShapeR achieves this while trained entirely on synthetic data, whereas SAM 3D exploits large-scale labeled real image-to-3D data. This highlights two different axes of progress: where SAM 3D uses large-scale real data for robust single-view inference, ShapeR utilizes multi-view geometric constraints to achieve robust, metric scene reconstruction.&lt;/p&gt;
    &lt;p&gt;The two approaches can be combined. By conditioning the second stage of SAM 3D with the output of ShapeR, we can merge the best of both worlds: the metric accuracy and robust layout of ShapeR, and the textures and robust real-world priors of SAM 3D.&lt;/p&gt;
    &lt;p&gt;Although trained on simulated data with visual-inertial SLAM points, ShapeR generalizes to other data sources without finetuning. For instance, it can reconstruct complete objects in ScanNet++ scenes. Furthermore, by leveraging tools like MapAnything to generate metric points, ShapeR can even produce metric 3D shapes from monocular images without retraining.&lt;/p&gt;
    &lt;p&gt;If you find this research helpful, please consider citing our paper:&lt;/p&gt;
    &lt;code&gt;@misc{siddiqui2026shaperrobustconditional3d,
      title={ShapeR: Robust Conditional 3D Shape Generation from Casual Captures}, 
      author={Yawar Siddiqui and Duncan Frost and Samir Aroudj and Armen Avetisyan and Henry Howard-Jenkins and Daniel DeTone and Pierre Moulon and Qirui Wu and Zhengqin Li and Julian Straub and Richard Newcombe and Jakob Engel},
      year={2026},
      eprint={2601.11514},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2601.11514}, 
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://facebookresearch.github.io/ShapeR/"/><published>2026-01-19T11:48:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46678205</id><title>Amazon is ending all inventory commingling as of March 31, 2026</title><updated>2026-01-19T14:50:55.276949+00:00</updated><content>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/ghhughes/status/2012824754319753456"/><published>2026-01-19T12:24:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46678430</id><title>Article by article, how Big Tech shaped the EU's roll-back of digital rights</title><updated>2026-01-19T14:50:54.025812+00:00</updated><content>&lt;doc fingerprint="a7f4913c1aabc5b5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Article by article, how Big Tech shaped the EU’s roll-back of digital rights&lt;/head&gt;
    &lt;p&gt;In a new analysis by Corporate Europe Observatory and LobbyControl, we trace Big Tech's fingerprints on the Digital Omnibus proposals - a major deregulation of EU digital laws including the GDPR and the AI Act. They are helped in this attempt by the Trump administration and the European far right.&lt;/p&gt;
    &lt;p&gt;At the end of November 2025, Ursula von der Leyen gave Trump and his tech oligarchs an early Christmas present: an unprecedented attack on digital rights. In its so-called Digital Omnibus, the European Commission proposed weakening important rules designed to protect us from Big Tech’s abuses of power.&lt;/p&gt;
    &lt;p&gt;These are the protections that keep everyone's data safe, governments and companies accountable, protect people from having artificial intelligence (AI) systems decide their life opportunities, and ultimately keep our societies free from unchecked surveillance.&lt;/p&gt;
    &lt;p&gt;At the same time, the Digital Omnibus is part of the European Commission's deregulation agenda, which threatens key social and environmental standards in Europe. Ironically this deregulation agenda is being promoted by the Commission as a way to make the EU 'competitive' – despite in reality actively empowering US Big Tech companies that dominate the field.&lt;/p&gt;
    &lt;p&gt;The Digital Omnibus was immediately heavily criticised by numerous civil society organisations. Politico even called it the end of the ‘Brussels effect’ – that is, that European tech regulations are adopted in other countries – and wrote that “Washington is [now] setting the pace on deregulation in Europe.”&lt;/p&gt;
    &lt;p&gt;To show the extent of Big Tech’s influence on the Digital Omnibus, we compared the Commission’s proposals with the lobbying positions from Big Tech and its associations.&lt;/p&gt;
    &lt;p&gt;The proposals in the Digital Omnibus concern both data protection and rules for AI. While the EU mistakenly speaks of benefits for European corporations, it is clear that weak digital rules strengthen the power of Google, Microsoft, Meta etc, thereby jeopardising the goal of becoming more independent from Big Tech and the US.&lt;/p&gt;
    &lt;p&gt;In the past, Big Tech has repeatedly spread the one-sided lobbying message that data protection hinders economic growth and innovation, especially with regard to AI. This includes exceptions for SMEs and a fundamental focus on making more use of data instead of protecting it.&lt;/p&gt;
    &lt;p&gt;Tech companies are spreading these messages with a record-breaking lobbying budget, a huge lobbying network, and support from the Trump administration. The digital industry’s annual lobby spending has grown from €113 million in 2023 to €151 million today – an increase of 33.6 percent in just two years.&lt;/p&gt;
    &lt;p&gt;Now, the European Commission appears to be bowing to this lobbying pressure and adopting key lobbying messages from Google, Microsoft, Meta and their many lobby organisations in its Digital Omnibus.&lt;/p&gt;
    &lt;p&gt;Here we break down these industry lobbying messages, how they have been adopted by the Commission as proposed text changes, and what the real world impacts could be.&lt;/p&gt;
    &lt;head rend="h2"&gt;How the Commission aims to weaken the GDPR and ePrivacy&lt;/head&gt;
    &lt;p&gt;The General Data Protection Regulation (GDPR) is the backbone of the EU’s digital rulebook. While the Commission claims it is only giving the GDPR a “face-lift”, its proposed changes - from the definition of personal data to the use of data for training AI - will have far-reaching consequences to people’s rights, and will benefit Big Tech’s problematic business model based on massive data extraction.&lt;/p&gt;
    &lt;head rend="h3"&gt;Limiting the definition of personal data&lt;/head&gt;
    &lt;p&gt;The Commission intends to stop classifying pseudonymised data (ie swapping out a user's identifiable name for a code or number) as personal data if a company claims it cannot identify a person, thereby exempting it from GDPR protection. This rule would also apply even when other actors ( for instance data brokers) can still identify individuals based on the pseudonymised data.&lt;/p&gt;
    &lt;p&gt;As the digital rights organisations Noyb and EDRi have pointed out, this change turns a universal rule into a subjective one. GDPR protections will only apply when a company has the means to identify a person based on the data it holds. This gives huge leeway to companies to decide not to apply the GDPR arguing that they can’t identify a person. Worse, data can be sold to other companies or data brokers that do have the means to re-identify individuals.&lt;/p&gt;
    &lt;p&gt;But even if data is never sold or passed on to third parties, the proposed subjective approach would still severely narrow the scope of the GDPR. Big Tech companies such as Meta and Google for instance could use personal data for online tracking by claiming that the data cannot be traced back to a natural person and is therefore not covered by the GDPR.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Digital Omnibus&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;Proposed changed text to article 4(1) of the GDPR in the digital omnibus in italics: “Information relating to a natural person is not necessarily personal data for every other person or entity, merely because another entity can identify that natural person.&lt;/p&gt;
        &lt;p&gt;Information shall not be personal for a given entity where that entity cannot identify the natural person to whom the information relates, taking into account the means reasonably likely to be used by that entity. Such information does not become personal for that entity merely because a potential subsequent recipient has means reasonably likely to be used to identify the natural person to whom the information relates.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Big Tech’s lobby position&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;This move closely reflects Big Tech's lobby position. The industry has long been calling for greater commercial use of personal data. The use of anonymous and pseudonymous data in particular would contribute to this.&lt;/p&gt;
        &lt;p&gt;DigitalEurope, (which counts all Big Tech companies among its members), wrote: “Clarify that pseudonymised data is not personal data when recipients cannot reasonably re-identify individuals.”&lt;/p&gt;
        &lt;p&gt;Microsoft Germany also lobbied for weakening the definition along similar lines.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Limiting your right to access your own data&lt;/head&gt;
    &lt;p&gt;Summary: Currently, anyone can request a copy of their personal data from any company or organisation that holds it. However, the Commission intends to limit this right if a person ‘abuses’ it.&lt;/p&gt;
    &lt;p&gt;This will severely limit the rights of individuals to know which of their data is being held by Big Tech. For instance, in 2023 Uber and Ola drivers who were ‘robo-fired’ won a court case against the company after it refused access to their work-related information. Ola tried to argue that the drivers requests for data amounted to an abuse of data protection rights, an excuse that the Commission now wants to give a legal basis.&lt;/p&gt;
    &lt;p&gt;This will make it harder to hold Big Tech to account and to contest their unlawful practices. “The proposal threatens to dismantle a tool of counter-power”, as the academic René Mahieu writes.&lt;/p&gt;
    &lt;p&gt;Contrary to the claims made by industry, and adopted by the German Government, it is not citizens who have ‘abused’ their right to access their own data, but tech companies that have disregarded this right. According to the privacy organisation NOYB 90 percent of data access requests are not respected. In one case, it took more than five years for Youtube to respect a particular data access request.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Digital Omnibus&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;Proposed changed text to article 12(5) of the GDPR in the digital omnibus in italics: “Where requests from a data subject are manifestly unfounded or excessive, in particular because of their repetitive character or also, for requests under Article 15 because the data subject abuses the rights conferred by this regulation for purposes other than the protection of their data, the controller may either: a) charge a reasonable fee [...] or refuse to act on the request.&lt;/p&gt;
        &lt;p&gt;The controller shall bear the burden of demonstrating that the request is manifestly unfounded or that there are reasonable grounds to believe that it is excessive.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Big Tech’s lobby position&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;The German Government lobbied for this change in an influential but controversial position paper. What has largely gone under the radar, however, is that these proposals were actually pushed by Big Tech companies.&lt;/p&gt;
        &lt;p&gt;In a lobby paper dated 16 August 2025, Google called on the German Government to "Introduce a ‘disproportionate efforts’ exemption to compliance with Articles 15-22 GDPR". With regard to Article 12(5), Google proposed the following addition highlighted in bold:&lt;/p&gt;
        &lt;p&gt;“Where requests from a data subject are manifestly unfounded or excessive, in particular because of their repetitive character, or, taking into account the scope of the processing and the cost of implementation, where responding to the request would involve a disproportionate effort, the controller may either: (a) charge a reasonable fee taking into account the administrative costs of providing the information or communication or taking the action requested; or (b) refuse to act on the request.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Using your personal data for training AI&lt;/head&gt;
    &lt;p&gt;Generative AI models are being trained on enormous amounts of data. The Commission intends to permit the training of AI models with personal data, including highly sensitive data such as sexuality, political beliefs, or ethnicity, without active consent. People’s data will only be protected from being used for training AI models if they explicitly opt-out.&lt;/p&gt;
    &lt;p&gt;Tech companies can basically hoover up any personal data on the internet to train their AI models without active consent (opt-out would still be possible). The protection of sensitive data for training AI such as political beliefs, union membership or sexuality is also weakened.&lt;/p&gt;
    &lt;p&gt;There is a risk of ‘data leakage’ whereby AI systems reproduce the personal data it has been trained on or produce fake information. In one such case a journalist was falsely accused by a Microsoft chatbot of child abuse when in fact he had just published articles on criminal court cases about it. The AI system, in essence a statistical programme, had conflated this information and had made him out to be a criminal.&lt;/p&gt;
    &lt;p&gt;Major tech companies such as Meta, Google and X stand to benefit as they can train their AI models with massive troves of personal data collected through their platforms.&lt;/p&gt;
    &lt;p&gt;Big Tech companies are spending enormous amounts, possibly as much as US$550 billion in 2026, to dominate the AI market. Loosening rules on AI data collection plays directly into their hands.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Digital Omnibus&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;Proposed text:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;The digital omnibus introduces a new article 88c in the GDPR introducing the use of personal data for AI training as a legitimate interest: “Where the processing of personal data is necessary for the interests of the controller in the context of the development and operation of an AI system such processing may be pursued for legitimate interests within the meaning of Article 6(1)(f)”&lt;/item&gt;
          &lt;item&gt;The digital omnibus also waters down protections on using sensitive data for AI training by introducing article 9(5) to the GDPR: “For processing referred to in point (k) of paragraph 2, appropriate organisational and technical measures shall be implemented to avoid the collection and otherwise processing of special categories of personal data. Where, despite the implementation of such measures, the controller identifies special categories of personal data in the datasets used for training, testing or validation or in the AI system or AI model, the controller shall remove such data. If removal of those data requires disproportionate effort, the controller shall in any event effectively protect without undue delay such data from being used to produce outputs, from being disclosed or otherwise made available to third parties.”&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;The digital omnibus introduces a new article 88c in the GDPR introducing the use of personal data for AI training as a legitimate interest: “Where the processing of personal data is necessary for the interests of the controller in the context of the development and operation of an AI system such processing may be pursued for legitimate interests within the meaning of Article 6(1)(f)”&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Big Tech’s lobby position&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;This has been a top priority of Big Tech lobbying. Almost every trade association and company has lobbied both the Commission and member states on that topic.&lt;/p&gt;
        &lt;p&gt;Big Tech lobby organisation CCIA: “It is crucial to reaffirm the role of legitimate interest as a lawful basis under the GDPR for responsible AI innovation, moving beyond the non-binding EDPB opinion to provide harmonised legal certainty for AI training.”&lt;/p&gt;
        &lt;p&gt;DigitalEurope: “Reinforce the use of ‘legitimate interest’ as a ground to process personal data for key use cases such as product development – including of AI models – and security.”&lt;/p&gt;
        &lt;p&gt;Big Tech lobby organisation Dot Europe (in a lobby letter to the Danish Government): “GDPR Article 9 strictly limits the processing of special category data (e.g., race, ethnicity, health), posing challenges for AI development, particularly in healthcare. AI models need access to sensitive data to ensure accuracy, fairness, and cultural relevance.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Weakening rules on automated decision-making&lt;/head&gt;
    &lt;p&gt;Currently, automated systems cannot be used to make decisions with legal effect or for online profiling. A human must be in the loop. The Commission’s proposal is a structural shift from a general prohibition on automated decision-making but with a few narrow exceptions towards an authorisation regime where a company can employ automated decision-making whenever it thinks this is “necessary”.&lt;/p&gt;
    &lt;p&gt;Important decisions including credit scoring, ‘robo-firings’, profiling, and welfare benefits could in the future be taken by automated decision-making without human intervention. This change will increasingly expose people to possibly flawed and biased algorithms which could make life-changing decisions, including if you get a loan or are fired from your job. Moreover these algorithms are generally black boxes, meaning it can be hard to uncover evidence of bias. Scandals in the Netherlands and Australia already show how thousands of people can be wrongly targeted with devastating effects.&lt;/p&gt;
    &lt;p&gt;In 2024, a subsidiary of the food delivery platform Glovo was fined €5 million by the Italian data protection authority under article 22 of the GDPR for violating workers' rights. The platform had used its rating system to automatically assign orders or ‘deactivate’ (read: ‘fire’) workers based on their ratings.&lt;/p&gt;
    &lt;p&gt;While the drastic weakening of article 22 will benefit a range of different sectors, from the insurance and banking sector to gig economy companies, Big Tech is also set to profit.&lt;/p&gt;
    &lt;p&gt;At the moment, social media giants employ thousands of underpaid workers to review harmful or illegal content on social media. This change will allow Big Tech companies to fully automate content moderation, cutting these costs essentially down to zero. Since the inauguration of Trump, Meta has fired thousands of content moderators. Amnesty International has warned that replacing content moderators with automated systems could amplify the most harmful content including content inciting racial hatred.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Digital Omnibus&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;Proposed text to Article 22 of the GDPR in italics: “A decision which produces legal effects for a data subject or similarly significantly affects him or her may be based solely on automated processing, including profiling, only where that decision: (a) is necessary for entering into, or performance of, a contract between the data subject and a data controller regardless of whether the decision could be taken otherwise than by solely automated means.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Big Tech’s lobby position&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;While Big Tech companies have been complaining about the overlap between article 22 of the GDPR with the AI Act and the Platform Work Directive, it seems it was mainly insurance sector lobbying that was decisive in rolling back the protection on automated decision-making (Big Tech is however still set to benefit from this change). In 2023, the European Court of Justice ruled in a landmark case that credit scores based on profiling cannot be used by banks and insurance companies to decide on granting a loan or other financial products. The Digital Omnibus might now undermine that ruling.&lt;/p&gt;
        &lt;p&gt;Insurance Europe: "Automated-decision making should be allowed as long as it is subject to safeguard mechanisms. To ensure that Art. 22 does not become an obstacle to the development of new digital solutions, it should be clarified that it is a right of the data subject and not an ex-ante prohibition."&lt;/p&gt;
        &lt;p&gt;Big Tech lobby organisation CCIA: “The definitions of the General Data Protection Regulation’s (GDPR) ‘automated individual decision-making’ (Article 22), the AI Act’s ‘AI system’ (Article 3(1)), and the Platform Work Directive’s (PWD) for automated decision-making systems often overlap.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Folding parts of ePrivacy into the GDPR&lt;/head&gt;
    &lt;p&gt;Cookies are the backbone of the AdTech industry, used to trace our online activities in order to target us with personalised ads. Article 5(3) of the ePrivacy directive requires websites and apps to ask for prior consent before storing cookies. The Commission now wants to ‘fold’ parts of article 5(3) into the GDPR. This replaces a categorical, consent-based mechanism with a more flexible framework based on balancing and exceptions.&lt;/p&gt;
    &lt;p&gt;Folding ePrivacy into the GDPR creates a more permissive system that allows companies to use exceptions to track behaviour. The Databroker Files demonstrated that commercial datasets which contain millions of locations could actually be used to spy on the public in Europe. These and other examples show the risks to our privacy are real: reporting shows how the vast trade in location data from smartphones can be traced back to individuals showing where they were at a specific time.&lt;/p&gt;
    &lt;p&gt;It will allow them to do even more of what they already do: track you without your consent. Big Tech firms have been lobbying for years against ePrivacy as it could undermine their invasive business model based on surveillance ads.&lt;/p&gt;
    &lt;p&gt;Several Big Tech firms have moreover faced fines for tracking users without consent. This change might let these companies get away with their most problematic practices.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Digital Omnibus&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;New text added to article 5(3) of the ePrivacy directive in italics: “This paragraph shall not apply if the subscriber or user is a natural person, and the information stored or accessed constitutes or leads to the processing of personal data.”&lt;/p&gt;
        &lt;p&gt;A new GDPR article 88a takes over instead which also introduces a series of exceptions to ask for consent including when “creating aggregated information about the usage of an online service to measure the audience of such a service, where it is carried out by the controller of that online service solely for its own use”.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Big Tech’s lobby position&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;The telecom sector, publishers and the tech industry have lobbied for years against strong privacy protections as guaranteed by the ePrivacy directive. In 2018 a major Big Tech driven lobby campaign prevented efforts to strengthen the ePrivacy Directive. A court document showed Google revealing that “we have been successful in slowing down and delaying the [ePrivacy Regulation] process and have been working behind the scenes hand in hand with the other companies.” The digital omnibus is another step in dismantling ePrivacy protections with all major players pushing for the changes as proposed by the Commission.&lt;/p&gt;
        &lt;p&gt;Google: “The most effective simplification is to delete Article 5(3) from the ePrivacy directive and govern all data processing related to cookies under the GDPR risk-based framework. Alternatively, a significant step toward simplification would be to amend Article 5(3) to extend the scope of permitted exemptions to allow specific, low-risk processing activities that are essential both for the functioning of a safe and sustainable digital ecosystem as well as for user experience. This would create clear exemptions for functions such as first-party audience measurement, ad frequency capping, and anti-fraud measures—allowing them to operate without generating unnecessary consent requests.”&lt;/p&gt;
        &lt;p&gt;Microsoft: “The “cookie rule” in article 5 (3) eP[rivacy] D[irective] could be moved to the GDPR or, if kept in, rendered more flexible by allowing cookie placement without consent in a wider range of circumstances, e.g. for security, software updates, anti-fraud, and analytics.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How the Commission aims to weaken the AI Act&lt;/head&gt;
    &lt;p&gt;"Europe is open for AI and for business!" Ursula von der Leyen tweeted during the AI Action Summit in Paris. In its single-minded priority to “win the global AI race”, the Commission is slashing rules and protections against risky AI systems. A year-long lobby campaign by the Trump administration and Big Tech to delay the implementation of the AI Act has clearly paid off.&lt;/p&gt;
    &lt;head rend="h3"&gt;No Checks and Balances for risky AI systems&lt;/head&gt;
    &lt;p&gt;A controversial win for Big Tech firms during the AI Act negotiations was allowing companies to “self-assess” if they believe an AI system is high-risk. To compensate for that loophole, industry had to register these AI systems in a public database. Now this transparency failsafe will also be removed, basically giving tech companies a free hand in deciding if an AI system is risky without any public oversight.&lt;/p&gt;
    &lt;p&gt;The risk to fundamental rights these high-risk AI systems pose are far from hypothetical. From algorithmic-powered employee firings to biased algorithms that disadvantage students based on their socio-economic background, highly problematic AI systems are already in circulation. The AI Act lets companies self-assess if these AI systems are high-risk or not, and should therefore comply with requirements such as proper risk management, accuracy, and transparency.&lt;/p&gt;
    &lt;p&gt;The digital omnibus will worsen an already huge loophole in the AI Act with potentially disastrous impacts on our rights.&lt;/p&gt;
    &lt;p&gt;Not only can AI companies already self-assess if their AI systems are risky, the digital omnibus will remove any possibility of public oversight of that assessment, giving these companies a blank check to do as they please without any accountability mechanism.&lt;/p&gt;
    &lt;p&gt;In a reaction on LinkedIn Daniel Leufer from the NGO Access Now called this “the biggest, most ridiculous loophole in the AI Act that will let unscrupulous providers unilaterally exempt themselves from the AI Act's obligations with oversight”.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Digital Omnibus&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;Paragraph 2 of article 49 of the AI Act is deleted.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Big Tech’s lobby position&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;The Commission’s proposals are completely in line with the lobby position of the two lobby organisations Dot Europe and DigitalEurope that count Big Tech members as its members.&lt;/p&gt;
        &lt;p&gt;DigitalEurope: “Abolish the mandatory registration of AI systems, along with the related EU and Member State databases.”&lt;/p&gt;
        &lt;p&gt;Dot Europe: “when a provider of AI systems provides concrete justifications that its AI system does not pose a significant risk of harm to the health, safety or fundamental rights of natural persons per Article 6(3), it should not be required to register its system in the high-risk AI database per Article 49.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Delay in the implementation of the AI Act&lt;/head&gt;
    &lt;p&gt;The Commission intends to postpone the implementation of part of the AI Regulation by almost a year and a half. This means giving Big Tech more than 12 months to continue releasing potentially risky systems onto the market without any safeguards.&lt;/p&gt;
    &lt;p&gt;This proposal would enable companies to continue to release risky AI systems for at least a year onto the market without any safeguards. Moreover, as the Center for Democracy and Technology points out, delaying the parts of the AI Act on high-risk AI systems, will also obstruct the ban of the most dangerous AI systems, leaving dangerous practices such as emotion recognition systems and facial recognition AI used in public spaces on the market for longer.&lt;/p&gt;
    &lt;p&gt;Delaying is a tried and tested industry lobbying tactic. It will give Big Tech more time to further water down the AI Act. Already, tech lobbyists are calling for the further deregulation of the AI Act.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Big Tech’s lobby position&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;A delay in the implementation of the AI Act is a central demand in a year-long tech lobby campaign which was backed by the Trump administration.&lt;/p&gt;
        &lt;p&gt;CCIA: “The first priority should be to delay AI Act implementation until at least 12 months after relevant guidance, codes of practice, or technical standards become available.”&lt;/p&gt;
        &lt;p&gt;DigitalEurope: “Delay the application of high-risk AI requirements until at least 12 months after relevant harmonised standards are published, allowing sufficient time for adaptation.”&lt;/p&gt;
        &lt;p&gt;Meta: “It is critical to first pause the implementation and enforcement of the [AI Act]. This pause will provide the necessary time to undertake meaningful reforms without risking the EU falling behind in the global AI race.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Using your sensitive data to train AI&lt;/head&gt;
    &lt;p&gt;The AI Act under narrow circumstances allowed the use of sensitive data for mitigation of high-risk AI models to prevent bias and discrimination. This exception is now expanded to all AI systems based on the assessment of companies if the processing is necessary (see also above as part of the changes to the GDPR).&lt;/p&gt;
    &lt;p&gt;This will allow intrusive gathering of your most sensitive personal data to train AI systems. Also see above “Using your personal data for training AI.&lt;/p&gt;
    &lt;p&gt;While Big Tech claims that more data is necessary for detecting bias, research suggests that debiasing - certain statistical techniques to ‘correct’ bias in databases that are used to train AI - is often ineffective and is unable to detect the many forms and contexts in which discrimination and bias manifests. Instead, it is a technical fix that enables Big Tech companies to collect yet more sensitive personal data to train their AI models while creating the illusion of ethical AI, all while encouraging the widespread adoption of AI across all sectors of society.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Digital Omnibus&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;The digital omnibus introduces article 4(a) to the AI Act: “To the extent necessary to ensure bias detection and correction in relation to high-risk AI systems in accordance with Article 10 (2), points (f) and (g), of this Regulation, providers of such systems may exceptionally process special categories of personal data.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Big Tech’s lobby position&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;The tech lobby constantly portrays data protection as a major obstacle to AI training and has therefore repeatedly lobbied, either specifically or in general terms, for the weakening of data protection.&lt;/p&gt;
        &lt;p&gt;Google: “We propose extending the allowance in Article 10(5) to permit the necessary data processing for bias detection and correction across all AI systems and general purpose AI models. Extending this provision will provide a harmonized legal basis for developers to proactively build the fair, representative, and trustworthy AI that aligns with the EU’s core values and benefits all citizens. It will also reduce the risk of AI models and systems perpetuating or amplifying societal discrimination, irrespective of their specific AI Act risk classification.”&lt;/p&gt;
        &lt;p&gt;Big Tech lobby organisation Information Technology Industry Council (ITI): “The AI Act's Article 10(5) allowance for special categories of personal data processing for bias mitigation should be extended to the training of all AI systems and GPAI models, not just those classified as "high-risk.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;A Big Tech-far right alliance in the making?&lt;/head&gt;
    &lt;p&gt;The Commission’s digital omnibus received widespread criticism. Civil society organisations, think tanks, experts, and political groups in the European Parliament from the left to the centre all perceived the Commission’s proposals as handouts to Big Tech and the Trump administration.&lt;/p&gt;
    &lt;p&gt;But while the Social Democrats in the Parliament called the digital omnibus unacceptable deregulation, far right parties quickly came to the support of the Commission.&lt;/p&gt;
    &lt;p&gt;Big Tech lobbying of the European Parliament also shifted in higher gear. Lobbying of the far-right seems to have become a particular priority for Meta, and to a lesser extent Google. While during the previous parliamentary mandate, Meta only met once with a far-right MEP, during this parliamentary mandate it has already met 38 times with MEPs from the ECR, the Patriots and the Europe of Sovereign Nations Group. The digital omnibus is a key priority in those meetings. In the week of 8 December 2025, Meta met with four far right MEPs with most of those meetings mentioning the digital omnibus.&lt;/p&gt;
    &lt;p&gt;Google has also not shied away from meeting far-right MEPs. A few days after the launch of the digital omnibus, the Head of Public Affairs of Google France joined a dinner party in Strasbourg hosted by six French MEPs from the far right Rassemblement National.&lt;/p&gt;
    &lt;p&gt;Big Tech's lobbying strategy in the US, where it has aligned itself with the Trump administration, now appears to have been extended to the European Parliament.&lt;/p&gt;
    &lt;p&gt;As outlined in this article, the digital omnibus is not just an unprecedented attack on our digital rights – it also closely mirrors Big Tech lobbying positions. The Commission’s deregulation agenda threatens to undermine years of progress in reining these tech giants and protecting our privacy.&lt;/p&gt;
    &lt;p&gt;The emerging far right - Big Tech alliance in the European Parliament points towards an even more alarming trend. It should now be clear to all that the Commission’s deregulation agenda isn't just opening the door to Big Tech, it's inviting the far right in.&lt;/p&gt;
    &lt;p&gt;However, this outcome is not inevitable. The European Parliament now has a crucial opportunity to stop this dangerous proposal and defend the hard-won data protection safeguards.&lt;/p&gt;
    &lt;p&gt;The Digital Omnibus has received massive pushback, from civil society organsations, from within parliament and from member states, including Malta, which recently requested more time to scrutinise the proposal.&lt;/p&gt;
    &lt;p&gt;What happens next depends on whether we manage to increase the pressure.&lt;/p&gt;
    &lt;p&gt;Now is the time to make our voices heard and make it crystal clear to the European Parliament and national governments that they must stand up for our privacy, freedom of expression and democratic control over technology, and reject the Digital Omnibus.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://corporateeurope.org/en/2026/01/article-article-how-big-tech-shaped-eus-roll-back-digital-rights"/><published>2026-01-19T12:53:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46678550</id><title>Ask HN: COBOL devs, how are AI coding affecting your work?</title><updated>2026-01-19T14:50:53.631023+00:00</updated><content>&lt;doc fingerprint="564ac0bc2b0511d7"&gt;
  &lt;main&gt;
    &lt;p&gt;I've not found it that great at programming in cobol, at least in comparison to its ability with other languages it seems to be noticeably worse, though we aren't using any models that were specifically trained on cobol. It is still useful for doing simple and tedious tasks, for example constructing a file layout based on info I fed it can be a time saver, otherwise I feel it's pretty limited by the necessary system specifics and really large context window needed to understand what is actually going on in these systems. I do really like being able to feed it a whole manual and let it act as a sort of advanced find. Working in a mainframe environment often requires looking for some obscure info, typically in a large PDF that's not always easy to find what you need, so this is pretty nice.&lt;/p&gt;
    &lt;p&gt;AI isn’t particularly great with C, Zig, or Rust either in my experience. It can certainly help with snippets of code and elucidate complex bitwise mathematics, and I’ll use it for those tedious tasks. And it’s a great research assistant, helping with referencing documentation. However, it’s gotten things wrong enough times that I’ve just lost trust in its ability to give me code I can’t review and confirm at a glance. Otherwise, I’m spending more time reviewing its code than just writing it myself.&lt;/p&gt;
    &lt;p&gt;Not COBOL but I sometimes have to maintain a large ColdFusion app. The early LLMs were pretty bad at it but these days, I can let AI write code and I "just" review it.&lt;/p&gt;
    &lt;p&gt;I've also used AI to convert a really old legacy app to something more modern. It works surprisingly well.&lt;/p&gt;
    &lt;p&gt;Heard an excellent COBOL talk this summer that really helped me to understand it. The speaker was fairly confident that COBOL wasn't going away anytime soon.&lt;/p&gt;
    &lt;p&gt;No, it doesn't. For example, you could use an AI agent just to aid you in code search and understanding or for filling out well specified functions which you then do QA on.&lt;/p&gt;
    &lt;p&gt;You 100% can use it this way. But it takes a lot of discipline to keep the slop out of the code base. The same way it took discipline to keep human slop out.&lt;/p&gt;
    &lt;p&gt;There has always been a class of devs who throw things at the wall and see what sticks. They copy paste from other parts of the application, or from stack overflow. They write half assed tests or no tests at all and they try their best to push it thought the review process with pleas about how urgent it is (there are developers on the opposite side of this spectrum who are also bad).&lt;/p&gt;
    &lt;p&gt;The new problem is that this class of developer is the exact kind of developer who AI speeds up the most, and they are the most experienced at getting shit code through review.&lt;/p&gt;
    &lt;p&gt;The point about the mass of code running the economy being untouched by AI agents is so real. During my years as a developer, I've often faced the skepticism surrounding automation technologies, especially when it comes to legacy languages like COBOL. There’s a perception that as AI becomes more capable, it might threaten specialized roles. However, I believe that the intricacies and context of legacy systems often require human insight that AI has yet to master fully.&lt;/p&gt;
    &lt;p&gt;I wonder if the OP's question is motivated by there being less public examples of COBOL code to train LLM's on compared to newer languages (so a different experience is expected), or something else. If the prior, it'd be interesting to see if having a language spec and a few examples leads to even better results from an LLM, since less examples could also mean less bad examples that deviate from the spec :) if there are any dev's that use AI with COBOL and other more common languages, please share your comparative experience&lt;/p&gt;
    &lt;p&gt;I am in banking and it's fine we have some finetuned models to work with our code base. I think COBOL is a good language for LLM use. It's verbose and English like syntax aligns naturally with the way language models process text. Can't complain.&lt;/p&gt;
    &lt;p&gt;Can you elaborate? See questions about what kind of use in sibling thread.&lt;/p&gt;
    &lt;p&gt;And in addition to the type of development you are doing in COBOL, I'm wondering if you also have used LLMs to port existing code to (say) Java, C# or whatever is current in (presumably) banking?&lt;/p&gt;
    &lt;p&gt;This is implied but I guess needs to be made explicit: people are looking for answers from devs with direct knowledge of the question at hand, not what random devs suspect.&lt;/p&gt;
    &lt;p&gt;I see it as a complete opposite for sure, I will tell you why.&lt;/p&gt;
    &lt;p&gt;it could have been a threat if it was something you cannot control, but you can control it, you can learn to control it, and controlling it in the right direction would enable anyone to actually secure your position or even advance it.&lt;/p&gt;
    &lt;p&gt;And, about the COBOL, well i dont know what the heck this is.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46678550"/><published>2026-01-19T13:05:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46678612</id><title>Fire Shuts GTA 6 Developer Rockstar North, Following Report of Explosion</title><updated>2026-01-19T14:50:53.110275+00:00</updated><content>&lt;doc fingerprint="db3d80de7b8cc3d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Fire services attended the offices of Grand Theft Auto 6 developer Rockstar North this morning and secured "structural damage" following a reported boiler explosion.&lt;/p&gt;
    &lt;p&gt;Seven vehicles were mobilized to attend the main Rockstar North building in Edinburgh, Scotland at 5.02am local time (just after midnight Eastern), following an incident that local news outlet Edinburgh Live described as an "explosion in a boiler room."&lt;/p&gt;
    &lt;p&gt;Crews remained on-site for over four hours but have now left the scene, Scottish newspaper The Herald has reported.&lt;/p&gt;
    &lt;p&gt;"We were alerted at 5.02am on Monday, 19 January to attend an incident on Holyrood Road, Edinburgh," a Scottish Fire and Rescue Service spokesperson said. "Operations Control mobilised three fire appliances and specialist resources to the scene, where firefighters worked to secure structural damage at a commercial building.&lt;/p&gt;
    &lt;p&gt;"There were no reported casualties and crews left the scene at 9.21am."&lt;/p&gt;
    &lt;p&gt;Rockstar Games' Edinburgh office has long served as the heart of the company, with development of every major Grand Theft Auto and Red Dead Redemption game taking place within its walls. Most recently, the building has been the site of protests over Rockstar's sudden firing of workers amid an increasingly bitter dispute that the layoffs were due to the employees' union membership — a claim that Rockstar itself has vehemently denied.&lt;/p&gt;
    &lt;p&gt;It's believed that office remains shut today, though it's too early to say how this setback might impact the release of the twice-delayed GTA 6. Rockstar's hugely-anticipated game is currently set to launch on November 19.&lt;/p&gt;
    &lt;p&gt;IGN has contacted Rockstar Games for more.&lt;/p&gt;
    &lt;p&gt;Tom Phillips is IGN's News Editor. You can reach Tom at tom_phillips@ign.com or find him on Bluesky @tomphillipseg.bsky.social&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ign.com/articles/fire-shuts-gta-6-developer-rockstar-north-following-report-of-explosion"/><published>2026-01-19T13:12:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46679515</id><title>Idiocracy</title><updated>2026-01-19T14:50:52.334028+00:00</updated><content>&lt;doc fingerprint="383186429a4684bc"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Idiocracy&lt;/head&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;&lt;p&gt;Idiocracy&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Terra de Idiotas (prt)&lt;p&gt;Idiocracia (bra)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Pôster promocional&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; Estados Unidos&lt;p&gt;2006 • cor • 84 min&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Gênero&lt;/cell&gt;&lt;cell&gt;comédia&lt;p&gt;ficção científica&lt;/p&gt;&lt;p&gt;humor negro&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Direção&lt;/cell&gt;&lt;cell&gt;Mike Judge&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Produção&lt;/cell&gt;&lt;cell&gt;Mike Judge&lt;p&gt;Elysa Koplovitz&lt;/p&gt;&lt;p&gt;Michael Nelson&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Roteiro&lt;/cell&gt;&lt;cell&gt;Mike Judge&lt;p&gt;Etan Cohen&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;História&lt;/cell&gt;&lt;cell&gt;Mike Judge&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Narração&lt;/cell&gt;&lt;cell&gt;Earl Mann&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Elenco&lt;/cell&gt;&lt;cell&gt;Luke Wilson&lt;p&gt;Maya Rudolph&lt;/p&gt;&lt;p&gt;Dax Shepard&lt;/p&gt;&lt;p&gt;Terry Crews&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Música&lt;/cell&gt;&lt;cell&gt;Theodore Shapiro&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Cinematografia&lt;/cell&gt;&lt;cell&gt;Tim Suhrstedt&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Edição&lt;/cell&gt;&lt;cell&gt;David Rennie&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Companhia produtora&lt;/cell&gt;&lt;cell&gt;Judgemental Films&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Distribuição&lt;/cell&gt;&lt;cell&gt;20th Century Fox&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Lançamento&lt;/cell&gt;&lt;cell&gt;1 de setembro de 2006&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Idioma&lt;/cell&gt;&lt;cell&gt;inglês&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Orçamento&lt;/cell&gt;&lt;cell&gt;US$2-4 milhões&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Receita&lt;/cell&gt;&lt;cell&gt;US$495,303 (mundialmente)[1]&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Idiocracy, tendo os títulos provisórios de The United States of Uhh-merica e 3001 (no Brasil, Idiocracia, e em Portugal, Terra de Idiotas) é um filme de humor negro de 2006. É uma comédia de ficção científica dirigida por Mike Judge, e estrelando Luke Wilson, Maya Rudolph, Dax Shepard, e Terry Crews. O filme conta a história de um soldado que participa de um experimento científico militar de hibernação que dá errado, e ele desperta 500 anos no futuro, em 2505. Descobre então, que o mundo agora vive numa sociedade distópica em que a publicidade, o marketing, o consumismo, o mercantilismo, e anti-intelectualismo cultural funcionam desenfreadamente e que a pressão disgênica resultou numa sociedade humana uniformemente estúpida, insensível ao meio ambiente, desprovida de curiosidade intelectual, responsabilidade social, e noções coerentes de justiça e direitos humanos.&lt;/p&gt;&lt;p&gt;O diretor e roteirista do filme, Mike Judge, escreveu Idiocracy como um meio para cumprir o seu contrato de dois filmes na 20th Century Fox e encontrar um local para expressar seu ponto de vista sobre a trajetória da sociedade estadunidense. Imaginado como um "2001 que deu errado", no qual a sociedade piorou em vez de melhorar, Judge atraiu Luke Wilson para o projeto. A Fox, no entanto, não o lançou quando concluído e não fez nada com ele por quase um ano, não havendo trailers, kits de imprensa, ou projeções para os críticos,[2] antes de liberá-lo apenas em cinemas suficientes para cumprir o acordo de distribuição. Na maioria dos lugares nos EUA onde foi lançado, o título não foi nem enviado, e foi listado como "comédia de Mike Judge sem título".[3][4] Nem a Fox e nem Mike Judge deram explicações até então,[2] mas o estúdio posteriormente alegou que o filme fora engavetado por não ter sido bem recebido em exibições-teste.[5] Uma outra razão seria que a Fox evitou promover o filme, porque a empresa não quis ofender nem os seus telespectadores nem seus potenciais anunciantes retratados negativamente no filme,[6] e a própria Fox foi alvo de sátira, como na cena do noticiário da Fox News Channel em que um casal de apresentadores mostram uma reportagem completamente nus. Como consequência, a bilheteria foi de apenas US$495,303 em todo o mundo.[7]&lt;/p&gt;&lt;p&gt;Apesar de sua falta de um grande lançamento nos cinemas, o filme atingiu um culto de seguidores.[3][4][8] Em 2017, Judge disse ao The New York Times que a falta de marketing e o lançamento em massa do filme foram o resultado de testes negativos.[9] Ele acrescentou que a Fox decidiu posteriormente para não dar ao filme uma campanha de marketing forte, porque o distribuidor acreditava que iria desenvolver um culto de seguidores através do boca-a-boca e recuperar o seu orçamento através do vídeo caseiro de vendas, como teve filme anterior de Judge Office Space.[9]&lt;/p&gt;&lt;head rend="h2"&gt;Enredo&lt;/head&gt;[editar | editar código]&lt;p&gt;Em 2005, o bibliotecário do exército dos Estados Unidos, Cabo Joe Bauers (Luke Wilson), é selecionado para um experimento de animação suspensa por ser o indivíduo "mais comum" de todas as Forças Armadas dos Estados Unidos. Na falta de uma candidata adequada do exército, eles contratam uma prostituta chamada Rita (Maya Rudolph), cujo cafetão Upgrayedd (Brad Jordan) foi subornado para permitir que ela participasse. Quando o oficial responsável (Michael McCafferty) é preso por ter iniciado sua própria rede de prostituição sob a tutela de Upgrayedd, o experimento é esquecido. Nos próximos cinco séculos, as expectativas da sociedade levam os humanos mais inteligentes a optar por não ter filhos enquanto os menos inteligentes se reproduzem indiscriminadamente, criando gerações que se tornam cada vez mais burras e viris a cada século que passa. Em 2505, as câmaras de suspensão de Joe e Rita são desenterradas pelo colapso de uma pilha de lixo do tamanho de uma montanha; A câmara de suspensão de Joe bate no apartamento de Frito Pendejo (Dax Shepard), que o expulsa.&lt;/p&gt;&lt;p&gt;O que antes era Washington, D.C. perdeu a maior parte de sua infraestrutura, com pessoas vivendo em cabanas de plástico chamadas de "domistiles". A população humana tornou-se morbidamente estúpida, falando apenas registros baixos de inglês com competência, e são profundamente anti-intelectualistas, com indivíduos recebendo nomes de produtos corporativos. Suspeitando de alucinação, Joe entra em um hospital, onde é diagnosticado de forma incompetente e percebe o que aconteceu com ele e a sociedade. Ele é preso por não ter uma tatuagem de código de barras para pagar sua consulta médica e é enviado para a prisão depois de ter sido designado o grosseiramente incompetente Frito como seu advogado. Enquanto isso, Rita foge de seu quarto e pretende retomar o trabalho como prostituta, mas logo percebe que as pessoas se tornaram tão estúpidas que ela pode cobrar dinheiro dos clientes sem fazer nada por eles.&lt;/p&gt;&lt;p&gt;Joe é renomeado "não sei" por uma máquina de tatuagem de reconhecimento de fala com defeito. Ele faz um teste de QI e engana um guarda, dizendo-lhe que está sendo liberado e simplesmente sai pela porta. Uma vez livre, Joe encontra Frito e pergunta se existe uma máquina do tempo para devolvê-lo a 2005, subornando Frito com promessas de riquezas por meio de juros compostos em uma conta bancária que Joe abrirá para ele no século 21. Enquanto conduz Joe e Rita para a máquina do tempo, Frito os leva a uma loja gigante da Costco, onde um scanner de tatuagem identifica Joe. Ele é preso, mas fica surpreso quando é levado à Casa Branca, onde é nomeado Secretário do Interior dos Estados Unidos com base no fato de que seu teste de QI o identificou como a pessoa mais inteligente viva. Em discurso, o presidente Camacho (Terry Crews) dá a Joe a tarefa de resolver, dentro de uma semana, a escassez de alimentos do país, que se formou numa Dust Bowl, um fenômeno climático de tempestade de areia, e a economia paralisada; após Joe descobrir que as plantações do país são irrigadas com uma bebida esportiva denominada "Brawndo", cuja corporação pai epónimo tinha comprado o FDA, FCC e USDA. Quando Joe o substituí com água, sem melhorar visivelmente as plantações, as ações da Brawndo caem para zero, e metade da população perde seus empregos, provocando distúrbios em massa.&lt;/p&gt;&lt;p&gt;Joe é condenado a morrer em um caminhão monstro numa corrida de demolição contra um invicto reabilitado (Andrew Wilson), quando Rita descobre que a reintrodução de água de Joe ao solo fez aparecer a vegetação nos campos. Frito mostra as colheitas na tela de exibição do estádio, e Camacho dá a Joe o perdão total, nomeando-o vice-presidente. Joe e Rita descobrem que a máquina do tempo que Frito falou de é na verdade, um parque temático de viagem histórica imprecisa. Após a aposentadoria de Camacho, Joe é eleito Presidente. Joe e Rita se casam e ela engravida das três crianças mais inteligentes do mundo, enquanto Frito, agora vice-presidente, teve oito esposas e virou pai das 32 crianças mais estúpidas do mundo. Uma cena pós-créditos mostra uma terceira câmara de suspensão liberando Upgrayedd, com a intenção de trazer Rita de volta, exatamente como ela havia previsto antes.&lt;/p&gt;&lt;head rend="h2"&gt;Elenco&lt;/head&gt;[editar | editar código]&lt;list rend="ul"&gt;&lt;item&gt;Luke Wilson — Cabo Joe Bauers&lt;/item&gt;&lt;item&gt;Maya Rudolph — Rita&lt;/item&gt;&lt;item&gt;Dax Shepard — Frito Pendejo&lt;/item&gt;&lt;item&gt;Terry Crews — Presidente Dwayne Elizondo Mountain Dew Herbert Camacho&lt;/item&gt;&lt;item&gt;Anthony Campos — Secretário de Defesa&lt;/item&gt;&lt;item&gt;David Herman — Secretário de Estado&lt;/item&gt;&lt;item&gt;Sara Rue — Procuradora-geral&lt;/item&gt;&lt;item&gt;Kevin McAfee — Bailiff&lt;/item&gt;&lt;item&gt;Robert Musgrave — Sargento Keller&lt;/item&gt;&lt;item&gt;Michael McCafferty — Tenente Coronel Collins&lt;/item&gt;&lt;item&gt;Justin Long — Dr. Lexus&lt;/item&gt;&lt;item&gt;Andrew Wilson — Beef Supreme&lt;/item&gt;&lt;item&gt;Kevin Klee — Hormel Chavez&lt;/item&gt;&lt;item&gt;Randal Reeder — Bandido do Serviço Secreto&lt;/item&gt;&lt;item&gt;Scarface — Upgrayedd&lt;/item&gt;&lt;item&gt;Thomas Haden Church — CEO de Brawndo&lt;/item&gt;&lt;item&gt;Stephen Root — Juiz Hank "The Hangman" BMW&lt;/item&gt;&lt;item&gt;Matt Bruce&lt;/item&gt;&lt;item&gt;Brendan Hill — Secretário de Energia&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Produção&lt;/head&gt;[editar | editar código]&lt;p&gt;Títulos inicial do filme seria The United States of Uhh-merica[10] e 3001. Filmagem ocorreram em 2004 em vários estágios na Austin Studios[11][12] e nas cidades de Austin, San Marcos, Pflugerville, e Round Rock, Texas.[13] O ator Artie Lange fez um teste para participar do filme.[14]&lt;/p&gt;&lt;p&gt;Sessões de teste começaram em março de 2005 produzindo relatórios não oficiais de reações fracas do público. Depois de alguma refilmagem no verão de 2005, uma exibição de teste no Reino Unido em agosto produziu um relatório de uma impressão mais positiva.[15]&lt;/p&gt;&lt;head rend="h2"&gt;Lançamento&lt;/head&gt;[editar | editar código]&lt;p&gt;Data de lançamento original de Idiocracy foi em 5 de agosto de 2005, de acordo com Mike Judge.[16] Em abril de 2006, a data de lançamento foi marcada para 1 de setembro de 2006. Em agosto, inúmeros artigos[17] revelaram que o lançamento era para ser colocada em espera indefinidamente. Idiocracy foi lançado, como previsto, mas apenas em sete cidades (Los Angeles, Atlanta, Toronto, Chicago, Dallas, Houston, e de na cidade natal de Mike Judge, Austin, Texas),[12] e se expandiu para apenas 130 cinemas,[18] não fazendo uso das usuais ampliações de lançamento de 600 ou mais cinemas.[19] De acordo com o Austin American-Statesman, 20th Century Fox, distribuidora do filme, não fez nada para promover o filme;[12] enquanto cartazes foram lançado nos cinemas, "não há trailers do filme, sem anúncios, e apenas duas fotografias"[20] e não há kits de imprensa foram lançados.[21]&lt;/p&gt;&lt;p&gt;O filme não foi exibido para os críticos.[22] A falta de informações concretas de Fox levaram à especulação de que o distribuidor pode ter tentado ativamente manter o filme de ser visto por um grande público, ao cumprir uma obrigação contratual da versão do cinema estar à frente de um lançamento do DVD, de acordo com Ryan Pearson da AP.[18] Essa especulação foi seguida de crítica aberta à falta de apoio do estúdio de Ain't It Cool News, Time, e Esquire.[23][24][25] Joel Stein da Time escreveu "anúncios e trailers do filme testados escandalosamente", mas, "ainda, abandonar Idiocracy parece particularmente injusto, uma vez que Judge fez um monte de dinheiro para a Fox".[24]&lt;/p&gt;&lt;p&gt;No The New York Times, Dan Mitchell argumentou que a Fox pode estar se afastando do conto de advertência sobre baixa inteligência disgênica, porque a empresa não quis ofender nem os seus telespectadores ou potenciais anunciantes retratado negativamente no filme[6] notando que no filme, serviço Starbucks oferece punhetas, e o lema da Carl's Jr. foi degenerado de "Não me incomode. Eu estou comendo." para "Foda-se! eu estou comendo!"[26]&lt;/p&gt;&lt;head rend="h3"&gt;Desempenho nas bilheterias&lt;/head&gt;[editar | editar código]&lt;table&gt;&lt;row span="9"&gt;&lt;cell role="head"&gt;Filme&lt;/cell&gt;&lt;cell role="head"&gt;Data de lançamento&lt;/cell&gt;&lt;cell role="head"&gt;Receitas de bilheteria&lt;/cell&gt;&lt;cell role="head"&gt;Ranking de bilheteria&lt;/cell&gt;&lt;cell role="head"&gt;Orçamento&lt;/cell&gt;&lt;cell role="head"&gt;Referência&lt;/cell&gt;&lt;/row&gt;&lt;row span="9"&gt;&lt;cell&gt;Estados Unidos&lt;/cell&gt;&lt;cell&gt;Estados Unidos&lt;/cell&gt;&lt;cell&gt;Internacional&lt;/cell&gt;&lt;cell&gt;Mundial&lt;/cell&gt;&lt;cell&gt;Apenas nos Estados Unidos&lt;/cell&gt;&lt;cell&gt;No resto do mundo&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Idiocracy&lt;/cell&gt;&lt;cell&gt;Setembro de 2006&lt;/cell&gt;&lt;cell&gt;$444,093&lt;/cell&gt;&lt;cell&gt;$51,210&lt;/cell&gt;&lt;cell&gt;$495,303&lt;/cell&gt;&lt;cell&gt;#6,914&lt;/cell&gt;&lt;cell&gt;Desconhecido&lt;/cell&gt;&lt;cell&gt;Desconhecido&lt;/cell&gt;&lt;cell&gt;[27]&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Receitas de bilheteira totalizaram $444,093 em 135 cinemas nos Estados Unidos[7]&lt;/p&gt;&lt;head rend="h3"&gt;Recepção crítica&lt;/head&gt;[editar | editar código]&lt;p&gt;Embora não tenha sido exibido com antecedência para os críticos, Idiocracy recebeu críticas positivas. Em Rotten Tomatoes, o filme tem 74% de aprovação, com base em 43 avaliações, com uma classificação média de 6.5/10. O consenso crítico do site diz: "Idiocracy apresenta a hilaridade e a sátira mordaz que só poderiam vir de Mike Judge".[28] Em Metacritic, o filme tem uma pontuação de 66 em 100, com base em 12 críticos, indicando "revisões geralmente favoráveis".[29]&lt;/p&gt;&lt;p&gt;Idiocracy não foi exibido para os críticos, mas o filme recebeu críticas favoráveis. Positivamente focado em conceito, elenco, e humor; a maior parte da crítica foi direcionada a questões do lançamento do filme ou em efeitos especiais e problemas de enredo.[30] Escrevendo para o Entertainment Weekly, Joshua Rich afirmou que "Mike Judge nos implora para refletir sobre um futuro em que a Britney e K-Fed são como o novo Adão e Eva".[31] Nathan Rabin do A.V. Club encontrou Luke Wilson "perfeitamente moldado [...] como um homem comum por excelência"; e escreveu sobre o filme: "Como a ficção científica muito superior, Idiocracy usa um futuro fantástico para comentar sobre um presente".[22]&lt;/p&gt;&lt;p&gt;Em outros países, o filme foi revisado positivamente. John Patterson, crítico para o The Guardian, escreveu que "Idiocracy não é uma obra-prima, mas é infinitamente engraçado", e da popularidade do filme, descreveu ter visto o filme "em uma casa meio vazia. Dois dias mais tarde, no mesmo lugar, mesmo show—lotado".[32]&lt;/p&gt;&lt;p&gt;No Brasil, a revista Veja chamou o filme de "politicamente incorreto", recomendou que os leitores vejam o DVD, e escreveu "...o filme passou voando pelos cinemas americanos e nem estreou nos brasileiros. Prova de que o futuro vislumbrado por Judge não está assim tão distante".[33] Crítico Alexandre Koball do CinePlayers, deu ao filme uma pontuação de 5/10, juntamente, escrevendo, "Idiocracy não é exatamente um filme recomendável. Nem para os amantes de comédias, pois não é exatamente engraçado, e muito menos para quem gosta de ficção científica e filmes sobre a vida no futuro, pois não é inovador na apresentação do mundo que cria. Mas é um filme feito para pensar, nem que seja por meros cinco minutos. E por isso, consegue ficar um ponto acima em relação à horrível média das comédias lançadas nos últimos anos no cinema norte-americano".[34] Já o site JoBlo em sua coluna O Melhor Filme Que Você Nunca Viu disse que o filme é ótimo e uma das melhores sátiras sociais modernas já feitas, escrevendo que "No entanto, acho que Idiocracy é mais uma comédia de advertência, que fala muito sobre onde estamos, para onde vamos e como contornar um mundo onde ver Big Brother e sobreviver apenas pelo alfabetismo funcional é mais importante do que ler livros".[3][4]&lt;/p&gt;&lt;p&gt;Escrevendo para o Salon.com Adam Johnson acusou o filme de apoiar a eugenia, dizendo: "Enquanto o filme é inteligente o suficiente para evitar o racismo, ele mergulha primeiro no classismo bruto".[35]&lt;/p&gt;&lt;p&gt;O filme tornou-se objeto de adoração por jornalistas e sociólogos das Américas. De tempos em tempos e com o passar dos anos, análises e lembranças surgem sobre as questões sociais ali abordadas e sobre como algumas delas se tornaram reais.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;A revista Time, em um artigo digital datado de 12 de maio de 2012[36] e classificada sob o tema do humor, fala sobre a banalização da campanha presidencial nos Estados Unidos e a linguagem chula usada nela. Ele também fala sobre a previsão feita no filme de que palavras são substituídas por pictogramas na televisão e nas redes sociais. Joel Stein, autor do relatório, propositadamente entrevistou os cineastas do filme, especialmente Mike Judge (diretor), que confessou que não se considerava um profeta, mas tinha medo das semelhanças que o mundo e seu trabalho tomaram.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;A revista Rolling Stone publicou em 2014[37] uma série de artigos especiais sobre filmes que por diversos motivos foram esquecidos ou que, pelo menos, não receberam o seu justo reconhecimento na época. No capítulo dedicado à "Idiocracy", escrito por David Fear, é detalhado que o slogan do filme passou a ser, na verdade, "um filme que originalmente era uma comédia, mas se tornou um documentário". Salienta que, nas palavras do diretor Mike Judge, a inspiração para o trabalho cinematográfico foi obtida em 2001 na fila para entrar com sua família em um parque de diversões e ele observou uma briga verbal muito forte entre duas mulheres atrás dele que também estavam com seus próprios filhos, o que ele considerou um péssimo exemplo para a infância e o levou a pensar sobre aonde a sociedade americana está indo. O trabalho enfrentou a recusa de exibi-lo por temor de possíveis processos legais de marcas muito semelhantes aos reais e que foram parodiados ao extremo. O artigo conclui que, embora considerem que ainda não se tornou um documentário, com a passagem de uma ou duas gerações, o fará, sugerindo que ainda há muito material a ser cortado neste trabalho cinematográfico.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;A revista digital "Crave" em 2013[38] dedicou uma análise intitulada "10 coisas que a 'Idiocracy' previu poderia acontecer e infelizmente nós as temos" e o autor, Cory Dudak, numerou precisamente:&lt;/item&gt;&lt;/list&gt;&lt;list rend="ol"&gt;&lt;item&gt;A tendência da mídia para o mercado e ganhar dinheiro com pornografia e até mesmo aqueles que assim expressamente objeto para fazer isso, mas usar as pessoas em seu livre arbítrio, tornar seus produtos tecnológicos e que inevitavelmente os faz ganhar dinheiro indiretamente com pornografia.&lt;/item&gt;&lt;item&gt;Publicidade onipresente: em qualquer lugar na Internet, e com o avanço da tecnologia, publicidade exagerada e indesejada é apresentada em todos os momentos e lugares e até mesmo em telefones celulares por meio de aplicativos que não são gratuitos e, ainda por cima, mostram publicidade não desejado ou relacionado ao aplicativo instalado.&lt;/item&gt;&lt;item&gt;Linguagem grosseira na publicidade: para o ponto anterior, o uso de palavrões e/ou sua associação com eles é jogado no jogo de palavras com material audiovisual, o que constitui uma falta de respeito para o cliente.&lt;/item&gt;&lt;item&gt;Obesidade: A tendência da sociedade americana para a obesidade é empurrada para os tamanhos mínimos de alimentos nas cadeias de fast food, que são sobrecarregados em calorias, gorduras e açúcares e oferecidos a um preço mais baixo, o que leva a um beco em cidadãos que trabalham das 9 às 5 e são forçados a comer na rua e não em suas próprias casas.&lt;/item&gt;&lt;item&gt;Avalanches do lixo: embora essa questão tenha sido levantada em outros filmes como WALL·E a forma como eles a apresentam em “Idiocracia” através de telejornais tornou-se realidade como ocorreu na Guatemala em 2008, onde 20 pessoas perderam suas vidas.[39]&lt;/item&gt;&lt;item&gt;Linguagem humana: observa e alcança a mesma conclusão alcançada pela revista Time e critica o uso desnecessário e abusivo de contrações e abreviações de palavras.&lt;/item&gt;&lt;item&gt;veículo desligamento remoto: no filme a polícia fora, de longe, o veículo onde fogem os protagonistas e hoje essa possibilidade está disponível para as autoridades eo que é pior, nas mãos de hackers e crackers que tiveram acesso a dita característica em certas marcas e modelos de veículos.[40]&lt;/item&gt;&lt;item&gt;Sistema de posicionamento global GPS: destaca que atualmente muito poucas pessoas têm a capacidade de usar e interpretar um mapa em papel para ir a qualquer lugar e que mesmo esses dispositivos não mais apenas orientam o motorista, mas fazem pedidos diretamente que trouxeram acidentes infelizes.[41]&lt;/item&gt;&lt;item&gt;A popularização de programas baseados em pessoas que batem uns nos outros na tela, que são baratos para produzir e passar para o riso fácil no telespectador e até desenvolveram aplicativos para telefones celulares para visualizá-los em todos os momentos e lugares.&lt;/item&gt;&lt;item&gt;No filme eles mostram um filme de grande sucesso que ganha vários Oscars e são apenas 90 minutos de um par de nádegas expostas, incluindo flatulência. Na vida real, o autor diz que não estamos longe deles e refere-se ao vídeo lançado pela banda de música chilena Astro e foi censurado pela empresa Youtube, mas volta a ser publicado por centenas de outros fãs da banda, cumprindo e o que foi colocado pela revista Rolling Stone em seu artigo, a comercialização de pornografia direta e indiretamente.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;O crítico de cinema mexicano Carlos de la Rosa[42] em seu blog comenta e também concorda com os pontos indicados acima e afirma que já estamos vivendo uma "idiocracia". Acrescente-se ao tema a sinalização da tendência da grande mídia jornalística de usar homens e mulheres "com pouca roupa e sem inteligência ou critérios" repetindo repetidas vezes a mesma notícia que os torna obstinados à redundância da mídia. Ele ressalta que a realidade ultrapassa o filme, pois há noticiários em que as meninas gradualmente tiram suas roupas enquanto informam o público, que são os casos do Desnudando la Notícia da Venezuela e o Naked News do Canadá.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Home media&lt;/head&gt;[editar | editar código]&lt;p&gt;Idiocracy foi lançado em DVD em 9 de janeiro de 2007 com cenas e proporções de tela mudadas, cenas deletadas, faixas de idiomas inglês e espanhol, e legendas em inglês, espanhol e francês. Em fevereiro de 2007, ele tinha ganhado $9 milhões em aluguel de DVD, mais de 20 vezes a sua receita de bilheteria nos EUA de menos de $450,000.[43]&lt;/p&gt;&lt;p&gt;No Reino Unido, as versões sem cortes do filme foram mostrados no canal por satélite Sky Comedy em 26 de fevereiro de 2009, com a estreia por televisão digital terrestre exibida na Film4 no dia 26 de abril de 2009.&lt;/p&gt;&lt;p&gt;No Brasil, o filme foi lançado diretamente em vídeo.[2]&lt;/p&gt;&lt;head rend="h2"&gt;Spin-off&lt;/head&gt;[editar | editar código]&lt;p&gt;Em agosto de 2012, Crews disse que estava em negociações com o diretor Judge e Fox sobre uma possível spin-off de Idiocracy com seu personagem Presidente Camacho, inicialmente concebido como uma websérie.[44] Uma semana antes das eleições estadunidenses de 2012, ele repetiu o personagem em um série de curtas para o site Funny or Die.&lt;/p&gt;&lt;head rend="h2"&gt;Análise&lt;/head&gt;[editar | editar código]&lt;p&gt;A ideia de uma sociedade distópica baseada na disgenia não é nova. No romance de ficção científica The Time Machine de H. G. Wells postula uma sociedade descentralizada dos seres humanos, assim como o conto "The Marching Morons" de Cyril M. Kornbluth, semelhante ao "Epsilon-minus Semi-Morons" do livro de Aldous Huxley, Admirável Mundo Novo.[45][46]&lt;/p&gt;&lt;head rend="h2"&gt;Ver também&lt;/head&gt;[editar | editar código]&lt;list rend="ul"&gt;&lt;item&gt;Paradoxo demográfico-econômico&lt;/item&gt;&lt;item&gt;Degeneração&lt;/item&gt;&lt;item&gt;Falácia biológica&lt;/item&gt;&lt;item&gt;William Shockley&lt;/item&gt;&lt;item&gt;Sleeper&lt;/item&gt;&lt;item&gt;Lista de filmes distópicos&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Referências&lt;/head&gt;&lt;list rend="ol"&gt;&lt;item&gt;↑ Box Office Summary (BoxOfficeMojo)&lt;/item&gt;&lt;item&gt;↑ a b c Jones Rossi (14 de fevereiro de 2014). «"Idiocracy": o melhor filme do ano que ninguém viu». G1. Globo.com. Consultado em 30 de novembro de 2014&lt;/item&gt;&lt;item&gt;↑ a b c Paul Shirey (15 de junho de 2013). «The Best Movie You Never Saw: Idiocracy». JoBloc.com. Consultado em 30 de novembro de 2014&lt;/item&gt;&lt;item&gt;↑ a b c Clare Neumann. «Idiocracia – Um dos melhores filmes que você provavelmente nunca viu». Legião dos Heróis. Consultado em 30 de novembro de 2014. Arquivado do original em 5 de dezembro de 2014&lt;/item&gt;&lt;item&gt;↑ Sérgio Dávila (1 de outubro de 2006). «O mundo será dos idiotas». Revista da Folha da Folha de S.Paulo. UOL. Consultado em 30 de novembro de 2014&lt;/item&gt;&lt;item&gt;↑ a b Mitchell, Dan (9 de setembro de 2006). «Shying away from Degeneracy». New York Times. Consultado em 25 de novembro de 2006&lt;/item&gt;&lt;item&gt;↑ a b «Idiocracy». Box Office Mojo. Amazon.com. Consultado em 2 de fevereiro de 2007&lt;/item&gt;&lt;item&gt;↑ Walker, Rob (4 de maio de 2008). «This Joke's for You». The New York Times. Consultado em 26 de maio de 2009&lt;/item&gt;&lt;item&gt;↑ a b Staley, Willy (13 de abril de 2017). «Mike Judge, the Bard of Suck». The New York Times. Consultado em 14 de abril de 2017. Cópia arquivada em 13 de abril de 2017&lt;/item&gt;&lt;item&gt;↑ Pierce, Thomas (11 de janeiro de 2007). «So What Idiot Kept This Movie Out of Theaters? (3rd item)». NPR. Consultado em 9 de fevereiro de 2007&lt;/item&gt;&lt;item&gt;↑ «Idiocracy at Austin Studios. Facilities usage.». Austin Studios;. Austin Film Society. Consultado em 18 de junho de 2010. Arquivado do original em 8 de outubro de 2007&lt;/item&gt;&lt;item&gt;↑ a b c Garcia, Chris (30 de agosto de 2006). «Was 'Idiocracy' treated idiotically?». Austin American-Statesman. Consultado em 9 de fevereiro de 2007&lt;/item&gt;&lt;item&gt;↑ «Texas Film Commission Filmography (2000-2007)». Office of the Governor. Consultado em 20 de junho de 2010. Arquivado do original em 22 de agosto de 2008&lt;/item&gt;&lt;item&gt;↑ «Idiocracy». AdoroCinema. Consultado em 30 de novembro de 2014&lt;/item&gt;&lt;item&gt;↑ «Mike Judge's Idiocracy Tests! (etc.)». Eric Vespe quoting anonymous contributor. AintItCoolNews.com. 22 de agosto de 2005. Consultado em 9 de fevereiro de 2007&lt;/item&gt;&lt;item&gt;↑ Franklin, Garth (28 de fevereiro de 2005). «Mike Judge Still Not In "3001"». Dark Horizons. Consultado em 21 de agosto de 2010. Arquivado do original em 5 de fevereiro de 2008&lt;/item&gt;&lt;item&gt;↑ Carroll, Larry (30 de agosto de 2006). «MTV Movie File». MTV. Viacom. Consultado em 9 de fevereiro de 2007&lt;/item&gt;&lt;item&gt;↑ a b Pearson, Ryan (8 de setembro de 2006). «The mystery of 'Idiocracy'». Associated Press. Consultado em 25 de novembro de 2006&lt;/item&gt;&lt;item&gt;↑ About Movie Box Office Tracking and Terms. Box Office Mojo. Retrieved 2010-08-28.&lt;/item&gt;&lt;item&gt;↑ Kernion, Jette (22 de outubro de 2006). «Time for Mike Judge to go Indie». Cinematical&lt;/item&gt;&lt;item&gt;↑ Patel, Nihar (8 de setembro de 2006). «A Paucity of Publicity for 'Idiocracy'». Day to Day. NPR Transcript.&lt;/item&gt;&lt;item&gt;↑ a b Rabin, Nathan (6 de setembro de 2006). «Idiocracy (review)». The A.V. Club. The Onion. Consultado em 8 de fevereiro de 2007&lt;/item&gt;&lt;item&gt;↑ Vespe, Eric (2 de setembro de 2006). «Open Letter to Fox re: IDIOCRACY!!!». Ain't It Cool News&lt;/item&gt;&lt;item&gt;↑ a b Stein, Joel (10 de setembro de 2006). «Dude, Where's My Film?». Time Magazine&lt;/item&gt;&lt;item&gt;↑ Raftery, Brian (1 de junho de 2006). «Mike Judge Is Getting Screwed (Again)». Esquire&lt;/item&gt;&lt;item&gt;↑ Adawi, Kamal (8 de agosto de 2008). «Idiocracy is Pure Genius». MBAcasestudysolutions.com. Consultado em 10 de agosto de 2008&lt;/item&gt;&lt;item&gt;↑ «Idiocracy (2006)». Box Office Mojo. Consultado em 19 de agosto de 2011&lt;/item&gt;&lt;item&gt;↑ «Idiocracy». Rotten Tomatoes. Consultado em 7 de maio de 2016. Cópia arquivada em 5 de outubro de 2006&lt;/item&gt;&lt;item&gt;↑ «Idiocracy». Metacritic. Consultado em 8 de setembro de 2009. Cópia arquivada em 16 de junho de 2010&lt;/item&gt;&lt;item&gt;↑ Chocano, Carina (4 de setembro de 2006). «Movie review : 'Idiocracy'». Los Angeles Times. calendarlive.com. Consultado em 29 de setembro de 2010&lt;/item&gt;&lt;item&gt;↑ Rich, Joshua (30 de agosto de 2006). «Idiocracy (2006)». ew.com. Consultado em 29 de setembro de 2010&lt;/item&gt;&lt;item&gt;↑ Patterson, John (8 de setembro de 2006). «On film : Stupid Fox». The Guardian. UK. Consultado em 28 de setembro de 2010&lt;/item&gt;&lt;item&gt;↑ «Idiocracy». Veja. Editora Abril. 21 de março de 2014. Consultado em 30 de novembro de 2014. Arquivado do original em 23 de julho de 2010&lt;/item&gt;&lt;item&gt;↑ Alexandre Koball (12 de abril de 2007). «Idiocracy». Cineplayers. Consultado em 30 de novembro de 2014&lt;/item&gt;&lt;item&gt;↑ Adam Johnson. «'Idiocracy's' curdled politics: The beloved dystopian comedy is really a celebration of eugenics». Salon. Consultado em 7 de março de 2016. Cópia arquivada em 8 de março de 2016&lt;/item&gt;&lt;item&gt;↑ "We Have Become an Idiocracy" por Joel Stein - [1]&lt;/item&gt;&lt;item&gt;↑ "Darwin, Dar-lose: The Genius of 'Idiocracy'" por David Fear -[2]&lt;/item&gt;&lt;item&gt;↑ "10 Things ‘Idiocracy’ Predicted Would Happen, and Sadly Already Have" por Corik Dudak -"&lt;/item&gt;&lt;item&gt;↑ "Tons of trash kill 4 at Guatemala dump, 20 missing" (Agencia de noticias "Ruters") - [3]&lt;/item&gt;&lt;item&gt;↑ "Hackers Remotely Kill a Jeep on the Highway—With Me in It" por Andy Greenberg- [4]&lt;/item&gt;&lt;item&gt;↑ "9 Car Accidents Caused by Google Maps &amp;amp; GPS" por Robert Wabash - [5]&lt;/item&gt;&lt;item&gt;↑ "Idiocracy" por Carlos de la Rosa -[6]&lt;/item&gt;&lt;item&gt;↑ DVD/ Home Video summary (BoxOfficeMojo)&lt;/item&gt;&lt;item&gt;↑ Yamato, Jen (6 de agosto de 2012). «Idiocracy Spin-Off In The Works? Terry Crews Talks». Movieline. Consultado em 8 de outubro de 2012&lt;/item&gt;&lt;item&gt;↑ Tremblay, Ronald Michel (4 de novembro de 2009). «Humankind's future: social and political Utopia or Idiocracy?». Atlantic Free Press. Consultado em 8 de maio de 2010. Arquivado do original em 27 de julho de 2011&lt;/item&gt;&lt;item&gt;↑ Grigg, William Norman (14 de maio de 2010). «Idiocracy Rising». Lew Rockwell. Consultado em 2 de outubro de 2010&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Ligações externas&lt;/head&gt;[editar | editar código]&lt;list rend="ul"&gt;&lt;item&gt;Idiocracy no IMDb&lt;/item&gt;&lt;item&gt;Idiocracy (em inglês). no Box Office Mojo.&lt;/item&gt;&lt;item&gt;«Idiocracy» (em inglês) no Rotten Tomatoes&lt;/item&gt;&lt;item&gt;«Idiocracy» (em inglês). no Metacritic&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;«idiocracy» (em inglês). no TCM Movie Database&lt;/item&gt;&lt;item&gt;«Idiocracy» (em inglês). no AFI Catalog of Feature Films&lt;/item&gt;&lt;item&gt;Idiocracy no AdoroCinema (em português)&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;Filmes dos Estados Unidos de 2006&lt;/item&gt;&lt;item&gt;Filmes de comédia dos Estados Unidos&lt;/item&gt;&lt;item&gt;Filmes de comédia da década de 2000&lt;/item&gt;&lt;item&gt;Filmes de ficção científica dos Estados Unidos&lt;/item&gt;&lt;item&gt;Filmes de ficção científica da década de 2000&lt;/item&gt;&lt;item&gt;Filmes ambientados em 2005&lt;/item&gt;&lt;item&gt;Filmes ambientados no futuro&lt;/item&gt;&lt;item&gt;Filmes ambientados no século XXVI&lt;/item&gt;&lt;item&gt;Filmes ambientados na Casa Branca&lt;/item&gt;&lt;item&gt;Filmes de humor negro&lt;/item&gt;&lt;item&gt;Filmes satíricos&lt;/item&gt;&lt;item&gt;Filmes distópicos&lt;/item&gt;&lt;item&gt;Filmes sobre televisão&lt;/item&gt;&lt;item&gt;Filmes sobre prostituição&lt;/item&gt;&lt;item&gt;Filmes sobre presidentes fictícios dos Estados Unidos&lt;/item&gt;&lt;item&gt;Filmes da 20th Century Studios&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://pt.wikipedia.org/wiki/Idiocracy"/><published>2026-01-19T14:42:01+00:00</published></entry></feed>