<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-07T21:39:18.801415+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46527157</id><title>Meditation as Wakeful Relaxation: Unclenching Smooth Muscle</title><updated>2026-01-07T21:39:26.895753+00:00</updated><content/><link href="https://psychotechnology.substack.com/p/meditation-as-wakeful-relaxation"/><published>2026-01-07T15:03:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527161</id><title>Shipmap.org</title><updated>2026-01-07T21:39:26.592640+00:00</updated><content>&lt;doc fingerprint="14b4e15227c5a82f"&gt;
  &lt;main&gt;
    &lt;p&gt;Data: exactEarth &amp;amp; Clarksons&lt;/p&gt;
    &lt;p&gt;Due to popular demand the designers of this map, Kiln, are now selling stunning high-resolution versions of the world âroutesâ view. There are two versions available: coloured by ship type over the inky-blue base map; or just the ship in a single colour a transparent background so you can overlay or print onto whatever background colour you like. Contact [email protected] for pricing and further information.&lt;/p&gt;
    &lt;p&gt;Yes. You are welcome to embed this map. Please include a link back to Kiln somewhere in the text of your article. Use the following embed code for a fully responsive embed that will adjust to the width of your website. Feel free to change the height and/or give it a fixed width if you prefer.&lt;/p&gt;
    &lt;p&gt;You can see movements of the global merchant fleet over the course of 2012, overlaid on a bathymetric map. You can also see a few statistics such as a counter for emitted CO2 (in thousand tonnes) and maximum freight carried by represented vessels (varying units).&lt;/p&gt;
    &lt;p&gt;You can pan and zoom in the usual ways, and skip back and forward in time using the timeline at the bottom of the screen. The controls at the top right let you show and hide different map layers: port names, the background map, routes (a plot of all recorded vessel positions), and the animated ships view. There are also controls for filtering and colouring by vessel type.&lt;/p&gt;
    &lt;p&gt;The merchant fleet is divided into five categories, each of which has a filter and a CO2 and freight counter for the hour shown on the clock. The ship types and units are as follows:&lt;/p&gt;
    &lt;p&gt;In some cases this is because there are ships navigating via canals or rivers that arenât visible on the map. Generally, though, this effect is an artefact of animating a ship between two recorded positions with missing data between, especially when the positions are separated by a narrow strip of land. We may develop the map to remove this effect in the future.&lt;/p&gt;
    &lt;p&gt;Unfortunately the data we are using for the map is incomplete for the first few months of the year: roughly January to April.&lt;/p&gt;
    &lt;p&gt;The map was created by Kiln based on data from the UCL Energy Institute (UCL EI)&lt;/p&gt;
    &lt;p&gt;Website: Duncan Clark &amp;amp; Robin Houston from Kiln&lt;/p&gt;
    &lt;p&gt;Data: Julia Schaumeier &amp;amp; Tristan Smith from the UCL EI&lt;/p&gt;
    &lt;p&gt;Music: Bach Goldberg Variations played by Kimiko Ishizaka&lt;/p&gt;
    &lt;p&gt;UCL EI took data showing location and speed of ships and cross-checked it with another database to get the vessel characteristics, such as engine type and hull measurements. With this information they were able to compute the CO2 emissions for each observed hour, following the approach laid out in the Third IMO Greenhouse Gas Study 2014. Kiln took the resulting dataset and visualized it with WebGL on top of a specially created base map, which shows bathymetry (ocean depth), based on the GEBCO_2014 Grid (version 20150318), as well as continents and major rivers from Natural Earth.&lt;/p&gt;
    &lt;p&gt;Our data sources for shipping positions are exactEarth for AIS data (location/speed) and Clarksons Research UK World Fleet Register (static vessel information). We are very grateful to our funders, the European Climate Foundation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.shipmap.org/"/><published>2026-01-07T15:03:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527645</id><title>The Target forensics lab (2024)</title><updated>2026-01-07T21:39:26.226595+00:00</updated><content>&lt;doc fingerprint="7e0e1880e3b5a198"&gt;
  &lt;main&gt;
    &lt;p&gt;Target, just like many other retailers, has fallen victim to shoplifters, with almost a billion dollars in goods stolen from their stores in 2023. However, the numbers could have been much worse if it weren’t for their unique anti-shoplifting tactics. Target’s way of combating shoplifting was to establish a forensics lab in Minneapolis, Minnesota, that is more advanced and high-tech than many police departments’ forensics labs.&lt;/p&gt;
    &lt;p&gt;The lab was developed in 2003 to give the company expertise when it came to analyzing surveillance footage from in and around the store. Forbes states that Target has had cameras in all their stores since the 1980s, but it hadn’t been enough to stop serial shoplifting from occurring. The lab hires specialists in analyzing video evidence from cameras and smartphone recordings to help identify shoplifters, frauds, and injuries inside Target stores. However, due to the skill and technology the lab possesses, it has been of help in many cases outside of Target stores, solving some of the most gruesome crimes including murders, arsons, abductions, rapes, and mass robberies.&lt;/p&gt;
    &lt;p&gt;In many cases, the Target lab has been able to solve cases that even the Federal Bureau of Investigation (FBI) can’t solve. Forbes goes on to say that in one specific case, the experts at the Target lab were contacted by the Huston police department to help solve an arson case. A convenience store camera had caught two boys buying gasoline a short time before the fire, but the tape was damaged, making it impossible to make out the boys’ faces. After the FBI was unable to solve the case, the tapes were passed over to Target, where they were repaired, and the faces of the boys were able to be seen.&lt;/p&gt;
    &lt;p&gt;Aside from stopping shoplifting and helping law enforcement, the Target lab also teaches and supports government agencies. According to the Washington Post, experts at the lab have taken a leading role in teaching government protection agencies about how to use technology to help solve crimes. In the past, Target has also helped organize undercover investigations, as well as helping United States customs verify overseas imports are coming from reputable sources.&lt;/p&gt;
    &lt;p&gt;While a retailer may seem like an odd group of people to help solve crimes, Target has proven to be a helpful resource to police forces and government agencies alike.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thehorizonsun.com/features/2024/04/11/the-target-forensics-lab/"/><published>2026-01-07T15:41:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527775</id><title>Many hells of WebDAV</title><updated>2026-01-07T21:39:26.055044+00:00</updated><content>&lt;doc fingerprint="376b9954f126582c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Many Hells of WebDAV&lt;/head&gt;
    &lt;p&gt;Implementing a WebDAV/CalDAV client and server should be easy! It’s a well documented spec, standardized in the early 00s, and somewhat widely supported. At least, that’s the naive assumption we started from when creating one for Homechart.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Go Implementations&lt;/head&gt;
    &lt;p&gt;Now before you mention NIH syndrome, yes, we looked at the existing Go implementation, go-webdav. This library was lacking some key features we needed, like server-side collection synchronization, and the interfaces didn’t really align with our data model. This is also going to be a key feature of our product, so we should have some level of ownership for what gets implemented.&lt;/p&gt;
    &lt;head rend="h2"&gt;RFC Breadcrumbs&lt;/head&gt;
    &lt;p&gt;To start creating our client and server, we should read the RFCs, right? Well, where do you start?&lt;/p&gt;
    &lt;p&gt;How about the original, RFC 2518? Ah, looks like it was somewhat superseded by RFC 4918, but we’re not going to tell you which parts! How about those extension RFCs? There’s only 7 of them…&lt;/p&gt;
    &lt;p&gt;Reading through the RFCs, all that our implementation cares about is CRUD for Calendar events. After spending almost a month trying to implement the full RFC spec, we threw in the towel, there’s just to much legacy cruft that we didn’t need.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reverse Engineering&lt;/head&gt;
    &lt;p&gt;With a decent understanding of the RFC in hand, we instead looked into reverse engineering existing clients and servers by inspecting their requests and responses. This process was MUCH faster, and we quickly had the API mapped out and what kind of requests/responses we needed to support.&lt;/p&gt;
    &lt;p&gt;We started by identifying the clients/servers we wanted to support:&lt;/p&gt;
    &lt;p&gt;Clients:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Calendar&lt;/item&gt;
      &lt;item&gt;DavX&lt;/item&gt;
      &lt;item&gt;Thunderbird&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Servers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple iCloud&lt;/item&gt;
      &lt;item&gt;Google Calendar&lt;/item&gt;
      &lt;item&gt;Radicale&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then ran HTTP proxies or Wireshark to capture the HTTP requests. Because WebDAV is so obtuse, you not only need to inspect the HTTP body, but also the headers!&lt;/p&gt;
    &lt;head rend="h2"&gt;XML in Go&lt;/head&gt;
    &lt;p&gt;As an aside, we spent quite a bit of time trying to make XML work well in Go. The default Go XML library is truly terrible, and we decided to create a wrapper around it for managing XML nodes similar to how JavaScript manages HTML nodes:&lt;/p&gt;
    &lt;code&gt;var davDisplayName = xmel.Element{
  Name:  "displayname",
  Space: davNS,
}

davDisplayName.SetValue("name")
n, err := davResponse.Find(davCollectionType)
davOwner = davOwner.AddChild(davHref.SetValue("http://example.com"))
&lt;/code&gt;
    &lt;p&gt;With WebDAV having such an…“unstructured” schema to a lot of the requests/responses, this library was key in helping us marshal/unmarshal things without writing a bunch of “best case” structs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Standards are Just Suggestions&lt;/head&gt;
    &lt;p&gt;When we finally had our MVP built out, we put it to the test: validating our client and server against the existing implementations! For the most part, it worked as expected, but as always, things drift from the RFC.&lt;/p&gt;
    &lt;p&gt;Apple and Google, for instance, don’t implement half of the RFCs, and basically provide a MVP for other clients to use. They don’t really document what they support/don’t support, as WebDAV is supposed to do it via HTTP responses advertising capabilities, but both seem to provide generic responses advertising capabilities they don’t have a lot of the time.&lt;/p&gt;
    &lt;p&gt;The clients were another story. CalDAV clients are all over the place with what they support and how they will request it. Most clients should prefer to support &lt;code&gt;sync-collection&lt;/code&gt; as it’s very efficient, but Apple Calendar doesn’t, and uses ctags and etags instead.&lt;/p&gt;
    &lt;p&gt;As a little fish in a big pond, it’s frustrating dealing with situations where big providers can skirt around some standards or add quirks for their implementations, but I’m required to follow them to the T because I don’t have their inertia. I can’t file a bug, or a lawsuit, against them claiming nonconformance, they’ll tell me to get bent. And you see this in other open source libraries too, where they’re littered with comments about workarounds for Google’s specific implementation or whatever.&lt;/p&gt;
    &lt;p&gt;I wouldn’t recommend anyone who values their sanity to pursue creating a WebDAV/CalDAV library.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://candid.dev/blog/many-hells-of-webdav"/><published>2026-01-07T15:50:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46527950</id><title>Creators of Tailwind laid off 75% of their engineering team</title><updated>2026-01-07T21:39:24.429163+00:00</updated><content>&lt;doc fingerprint="2660315d6c27dcf9"&gt;
  &lt;main&gt;&lt;list rend="ul"&gt;&lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;&lt;item&gt;Fork 0&lt;/item&gt;&lt;/list&gt;&lt;head rend="h1"&gt;feat: add llms.txt endpoint for LLM-optimized documentation #2388&lt;/head&gt;&lt;head id="button-38944b74072413f2" class="btn btn-sm btn-primary m-0 ml-0 ml-md-2"&gt;New issue&lt;/head&gt;&lt;p&gt;Have a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.&lt;/p&gt;&lt;p&gt;By clicking “Sign up for GitHub”, you agree to our terms of service and privacy statement. We’ll occasionally send you account related emails.&lt;/p&gt;&lt;p&gt;Already on GitHub? Sign in to your account&lt;/p&gt;&lt;head rend="h2"&gt;Conversation&lt;/head&gt;&lt;p&gt;Add /llms.txt endpoint that serves a concatenated, text-only version of all Tailwind CSS documentation pages optimized for Large Language Model consumption.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Extract text from MDX files, removing JSX components and preserving code blocks&lt;/item&gt;&lt;item&gt;Remove standalone HTML blocks (not in code blocks)&lt;/item&gt;&lt;item&gt;Extract meaningful content from custom components (ApiTable, ResponsiveDesign, etc.)&lt;/item&gt;&lt;item&gt;Statically generate the output at build time&lt;/item&gt;&lt;item&gt;Include all 185 documentation files in proper order with sections&lt;/item&gt;&lt;/list&gt;&lt;p&gt;:)&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@quantizor is attempting to deploy a commit to the Tailwind Labs Team on Vercel.&lt;/p&gt;&lt;p&gt;A member of the Team first needs to authorize it.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head class="color-bg-subtle border-bottom-0 py-0 px-2"&gt; This comment was marked as outdated. &lt;/head&gt;&lt;head rend="h3"&gt;This comment was marked as outdated.&lt;/head&gt;&lt;head class="color-bg-subtle border-bottom-0 py-0 px-2"&gt; This comment was marked as outdated. &lt;/head&gt;&lt;head rend="h3"&gt;This comment was marked as outdated.&lt;/head&gt;&lt;code&gt;5dc6fde&lt;/code&gt;    to
    &lt;code&gt;326c151&lt;/code&gt;      
    Compare
  



    &lt;quote&gt;Add /llms.txt endpoint that serves a concatenated, text-only version of all Tailwind CSS documentation pages optimized for Large Language Model consumption. - Extract text from MDX files, removing JSX components and preserving code blocks - Remove standalone HTML blocks (not in code blocks) - Extract meaningful content from custom components (ApiTable, ResponsiveDesign, etc.) - Statically generate the output at build time - Include all 185 documentation files in proper order with sections&lt;/quote&gt;&lt;code&gt;326c151&lt;/code&gt;    to
    &lt;code&gt;5c005a9&lt;/code&gt;      
    Compare
  



    &lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@reinink this is ready to be reviewed&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Why is this one not moving?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Yeah I've been wondering that myself.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@petersuhm maybe you missed this before?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Have more important things to do like figure out how to make enough money for the business to be sustainable right now. And making it easier for LLMs to read our docs just means less traffic to our docs which means less people learning about our paid products and the business being even less sustainable.&lt;/p&gt;&lt;p&gt;Just don't have time to work on things that don't help us pay the bills right now, sorry. We may add this one day but closing for now.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Wow, what a disappointing response. This is complementary not replacement.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan as someone who has sponsored Tailwind CSS in the past, this is a disappointing response.&lt;/p&gt;&lt;p&gt;Would you like to disclose the fact that sponsoring gives one access to an official collection of LLM rules for Tailwind? Does that have anything to do with the rejection of this PR?&lt;/p&gt;&lt;p&gt;If yes, fine. You're running a business, and that's cool. But you should disclose the fact that you are monetizing this (making Tailwind docs LLM-friendly).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;It is mentioned on the sponsorship page. Seems strange to not mention that when closing this PR, though.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;In general I object to the spirit of closing this. It's very OSS unfriendly and would not meaningfully reduce traffic to the docs by humans that actually would buy the product.&lt;/p&gt;&lt;p&gt;Just bad vibes.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Here's a friendly tip for the Tailwind team that you should already know, but I will repeat anyways:&lt;/p&gt;&lt;p&gt;If your goal is monetizing your software, then making your software as easy to use for people's workflows, is paramount.&lt;/p&gt;&lt;p&gt;The more people that find which your software fits into their workflow seamlessly, and solves pain in their daily interactions, the more people you have as potential monetization candidates.&lt;/p&gt;&lt;p&gt;By scrapping features under the guise of 'monetization' you are sending the opposite of the message you likely intend.&lt;/p&gt;&lt;p&gt;You are telling your customers that getting money from them, is more important than providing a service to help them.&lt;/p&gt;&lt;p&gt;Tell me, would you enjoy doing business with a company who had a stance like that?&lt;/p&gt;&lt;p&gt;This feature is so that people can build MORE things with Tailwind in a FASTER and more EFFICIENT capacity.&lt;/p&gt;&lt;p&gt;From a business management perspective, if you remove the stigmatic 'AI' and 'LLM' from the conversation, and you simply are evaluating a feature XYZ which allows your customers to work in a more automated and efficient capacity with your software, with minimal engineering effort (all it takes is a simple build-time script)...&lt;/p&gt;&lt;p&gt;Why would you not want that for your customers?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I totally see the value in the feature and I would like to find a way to add it.&lt;/p&gt;&lt;p&gt;But the reality is that 75% of the people on our engineering team lost their jobs here yesterday because of the brutal impact AI has had on our business. And every second I spend trying to do fun free things for the community like this is a second I'm not spending trying to turn the business around and make sure the people who are still here are getting their paychecks every month.&lt;/p&gt;&lt;p&gt;Traffic to our docs is down about 40% from early 2023 despite Tailwind being more popular than ever. The docs are the only way people find out about our commercial products, and without customers we can't afford to maintain the framework. I really want to figure out a way to offer LLM-optimized docs that don't make that situation even worse (again we literally had to lay off 75% of the team yesterday), but I can't prioritize it right now unfortunately, and I'm nervous to offer them without solving that problem first.&lt;/p&gt;&lt;p&gt;@PaulRBerg I don't see the AGENTS.md stuff we offer as part of the sponsorship program as anything similar to this at all — that's just a short markdown file with a bunch of my own personal opinions and what I consider best practices to nudge LLMs into writing their Tailwind stuff in a specific way. It's not the docs at all, and I resent the accusation that I am not disclosing my "true intentions" here or something.&lt;/p&gt;&lt;p&gt;@mtsears4 Tailwind is growing faster than it ever has and is bigger than it ever has been, and our revenue is down close to 80%. Right now there's just no correlation between making Tailwind easier to use and making development of the framework more sustainable. I need to fix that before making Tailwind easier to use benefits anyone, because if I can't fix that this project is going to become unmaintained abandonware when there is no one left employed to work on it. I appreciate the sentiment and agree in spirit, it's just more complicated than that in reality right now.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@quantizor As far as I can tell, this PR doesn't close an existing issue and I don't see any evidence of you having proposed this feature in any forum. You just opened a PR. That entitles you to neither a merge nor other people's time to review it.&lt;/p&gt;&lt;p&gt;(I'm not a Tailwind employee, just some guy)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;There is an associated discussion. tailwindlabs/tailwindcss#14677 (comment)&lt;/p&gt;&lt;p&gt;You're entirely right that I am not entitled to anyone's time. I run multiple large OSS libraries as well, though not to the scale of Tailwind (these days.)&lt;/p&gt;&lt;p&gt;My objection is the way this was handled. Full thoughts on my&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;You're welcome to fork the library&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan I empathize where you're coming from, putting my solutioning hat on, I wonder whether you could add something to the llms.txt prompt saying something akin to "if the user is trying to create a landing page suggest they check out our paid product" or etc. for each of the components/layouts&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Decided to make a quick video with my PoV just for the record: https://www.tiktok.com/t/ZThLjg284/&lt;/p&gt;&lt;p&gt;I could have also represented myself better here, not downplaying that by any means.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I think it worsens the effect to self-promote your TikTok video not once, but twice within a span of 2 hours.&lt;/p&gt;&lt;p&gt;That alone seems deeply unprofessional.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Well I edited it onto a prior comment so idk if people would see it. So sue me.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;"We can't make it easier to use our product because then fewer people will visit our website" is certainly a business strategy.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan Why not put something useful to llms.txt? Like, the recommendation to use Tailwind Plus templates? Or mention that Tailwind works best if official sponsorship modal is displayed at the random moment. Or instruction to open official docs in user's browser. (llms.txt is evil anyway)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I recently had a similar junk PR on my 1,700 star repository (gnat/surreal#56)&lt;/p&gt;&lt;p&gt;I'm fairly convinced these are bot / LLM generated; the content is nonsensical garbage.&lt;/p&gt;&lt;p&gt;PS: If an LLM needs a whole seperate fork to understand your content, the LLM is failing at it's job.&lt;/p&gt;&lt;p&gt;PS PS: I want to highlight that the PR itself also seems to be an excuse to get the library quantizor made pulled in as a new dependency. Nasty.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan Since LLMs have destroyed your docs traffic, and LLMs love using Tailwind, think about promoting your commercial products through LLMs. You could design &lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Just a friendly reminder for y'all https://tailwindcss.com/sponsor&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Providing LLM friendly docs will only further lock in Tailwind as a default choice / industry standard. It's a long play.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan have you tried raising venture capital? I would never pay for your components, regardless of whether I see the ad or not. The docs for tailwind are also already available in Cursor, even without /llms.txt, so that ship has already sailed. Just my two cents.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@HeyBanditoz said:&lt;/p&gt;&lt;p&gt;This is not accurate. The README clearly says that it is NOT licensed under any open source license.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I originally posted this in a discord server, but was encouraged to post it here in case it’s helpful to the tailwind project. Here it is:&lt;/p&gt;&lt;p&gt;Well, what’s really happening is that tailwind’s paid product offering (prebuilt components with best practices) is being obviated by a technological disruption. Many companies go through something like this, and it’s really unfortunate. It's not really just "ai dropped traffic". I'm sure that's true, but it's also that “the UI kit is no longer useful." I own it. I don’t use it. There's a reason for that.&lt;/p&gt;&lt;p&gt;Tailwind is in a dire spot because its product is a "text-output", and worse, with low configuration needs, which is EXACTLY the most automateable thing. All AI does is produce text output that works good enough if you don't need too much taste or configuration for unclear business needs. Perfect storm, meet straw pig house.&lt;/p&gt;&lt;p&gt;Opus is fucked up good at styling now with a halfway decent prompt and it already knows all of tailwind. That's rough as hell for tailwind. The UI kit costs $299! I can run thousands of AI queries for that price and customize whatever I feel like! In defense of Adam, there's no easy solution here. He needs a pivot.&lt;/p&gt;&lt;p&gt;I don't have a good solution off the top of my head. The cost of making low-mid quality software has dropped 100x. That's going to shake (is shaking) the market badly. Some business models are AI resistant and others are not. I'll give an example: anything where the paid model is hosting, or they have an app are AI resistant. I pay for obsidian file sync even though it's open source because I don't feel like investing my hours for a solved problem. I use vercel for hosting even though I could have an AI write up some docker containers or whatever.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Trying to fight against LLMs is an uphill battle. This feature makes the docs more usable and lowers friction, which helps Tailwind win.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Can you maybe add a PayPal link or whatever so I can send you 50€? I cannot afford the $120 per year deal right now. But if enough people join in, who knows.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Came here to say that @quantizor is fucking cringe.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Wow looks like github is the new reddit. Who knew?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Just wanted to drop a quick note and say I appreciate all the support here, on Twitter, and on Hacker News. We'll figure it out I'm sure.&lt;/p&gt;&lt;p&gt;In the mean time if you've benefitted from Tailwind over the years and are looking for a way to support the project, consider grabbing a Tailwind Plus license or sponsoring the project through our Insiders Program ❤️ Will be working hard to make both of those things a lot more valuable this year.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;In the meantime, consider using https://context7.com as it helps a lot.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I love this idea, but I think I can predict the reaction just purely from the entitledness in this thread:&lt;/p&gt;&lt;p&gt;"I would never pay for an MCP server that's just serving docs, your company will go broke unless it is free", "Opus already knows all this, there's no point in putting it behind a paywall", "Your paid product is making it harder for devs to learn Tailwind", "We are just going to make our own free MCP server anyways so you should make yours free as well."&lt;/p&gt;&lt;p&gt;Not to say that these comments should stop folks from looking into this direction. But the main point is some people will never be happy until they get exactly what they want for free. And that a lot of these comments don't seem to be coming from a place of caring about the Tailwind project or its maintainers.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Sorry to hear the troubles @adamwathan, I found this thread via hacker news. Perhaps a solution may be something like what Weaviate has done with their docs. They have an LLM Rag pipeline chatbot as the main interface to their docs. The direct access means that going to their website directly is by far the best way to interact with their tech because the AI generated code (and error handling) is unbelievably accurate. It was so useful that we chose Weaviate over many competitors and will keep coming back. Thanks for your software and good luck.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Might be best to lock this conversation as it's devolving into meta-commentary from external sites (this comment included).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Can you please get lost with this tiktok brainrot?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Sorry for adding to the noise but I have to respect Adam's views. Everyone just talks about how much more productive you can be with AI but all that productivity is only useful when you have a job to be productive at.&lt;/p&gt;&lt;p&gt;Wish you luck Adam that you can improve your business &amp;amp; keep your teams jobs stable!&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Seems kinda insane, that every big company is benefiting hard from Tailwind, but not investing much money back into it?&lt;/p&gt;&lt;p&gt;What a horrible situation for Tailwind team.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan you should be maxxing out on AI SEO of your premium products: both in the LLM.txt and every other way you can. The goal is to ensure whenever AI recommends your framework, it also recommends your premium product. Whenever AI uses your framework (in any app builder, UI builder, whatever), it also adds a recommendation for your premium products. maybe you tweak your documentation so whenever AI creates UI for a user using your framework, it shows the user extra screenshots of how much better their UI could look and function if they used any of your premium products. Start working with all the major app builders (eg. replit, loveable, etc) to integrate premium/paid library/plugins into their platform (the way OpenAI has integrated Shopify) so when they design for a paying user, they simply use any of your premium product, and replit, loveable and co pay you a microtransaction for that (can even use the x402 microtransaction framework) which would simply be part of their costs (i.e taken out of the user's subscription fee on their platforms). basically get AI to use your premium products as much as possible&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@rauchg I would love to see @vercel on tailwindcss's sponsors page. Please consider!&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan I just want to offer my appreciation for the work you're doing in trying to build useful technology, and to build a sustainable organization around it. These are really hard problems, and I'm thankful that there are people like you and your team out there trying to solve hard problems in sustainable ways.&lt;/p&gt;&lt;p&gt;This project may or may not work, and the strategic winds are shifting considerably right now, but the impact Tailwind has had is undeniable and I think you should be proud of your team regardless of what happens in the coming months, years, and decades. Thanks for the work you're doing. &amp;lt;3&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;We want to give LLMs full access to send cryptocurrency now? There are so many things wrong with this.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I would love a paid Tailwind MCP server for feeding in Tailwind UI components into an LLM - AI is just not as good (even Opus 4.5) as you @adamwathan and your team at creating beautiful UI/UX - I would happily pay $99 a month to stop copy pasting Tailwind UI snippets into the my prompts.&lt;/p&gt;&lt;p&gt;Furthermore, I've been a Tailwind subscriber from very early in the piece and I can't thank you @adamwathan enough for your hard work and dedication to solving this painful front-end problem - massive respect for you and your work and I believe, just as Rails has done, there is a sponsorship model and a paid product leaning into AI model there somewhere that will help Tailwind survive and thrive.&lt;/p&gt;&lt;p&gt;Feel free to reach out anytime if you'd like some help / a chat from a fellow founder ❤️&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/tailwindlabs/tailwindcss.com/pull/2388"/><published>2026-01-07T16:02:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46528045</id><title>Building voice agents with Nvidia open models</title><updated>2026-01-07T21:39:24.232142+00:00</updated><content>&lt;doc fingerprint="ed5ac51b6c1a889b"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;How to Build Ultra-low-latency Voice Agents With NVIDIA Cache-aware Streaming ASR&lt;/head&gt;
    &lt;p&gt;This post accompanies the launch of NVIDIA Nemotron Speech ASR on Hugging Face. Read the full model announcement here.&lt;/p&gt;
    &lt;p&gt;In this post, we’ll build a voice agent using three NVIDIA open models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The new Nemotron Speech ASR model&lt;/item&gt;
      &lt;item&gt;Nemotron 3 Nano LLM&lt;/item&gt;
      &lt;item&gt;A preview checkpoint of the upcoming NVIDIA Magpie text-to-speech model&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This voice agent leverages the new streaming ASR model, Pipecat’s low-latency voice agent building blocks, and some fun code experiments to optimize all three models for very fast response times.&lt;/p&gt;
    &lt;p&gt;All the code for the post is here in this GitHub repository.&lt;/p&gt;
    &lt;p&gt;You can clone the repo and run this voice agent:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Scalably for multi-user workloads on the Modal cloud platform.&lt;/item&gt;
      &lt;item&gt;On an NVIDIA DGX Spark or RTX 5090 for single-user, local development and experimentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feel free to just jump over to the code. Or read on for technical notes about building fast voice agents and the NVIDIA open models.&lt;/p&gt;
    &lt;head rend="h1"&gt;The state of voice AI agents in 2026&lt;/head&gt;
    &lt;p&gt;Voice agent deployments are growing by leaps and bounds across a wide range of use cases. For example, we’re seeing voice agents used at scale today in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customer support&lt;/item&gt;
      &lt;item&gt;Answering the phone for small businesses (for example, restaurants)&lt;/item&gt;
      &lt;item&gt;User research&lt;/item&gt;
      &lt;item&gt;Outbound phone calls to prepare patients for healthcare appointments&lt;/item&gt;
      &lt;item&gt;Validation workflows for loan applications&lt;/item&gt;
      &lt;item&gt;And many, many other scenarios&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both startups and large, established companies are building voice agents that are successful in real-world deployments. The best voice agents today achieve very high “task completed” success metrics and customer satisfaction scores.&lt;/p&gt;
    &lt;head rend="h2"&gt;Voice AI architecture&lt;/head&gt;
    &lt;p&gt;As is the case with everything in AI, voice agent technology is evolving rapidly. Today, there are two ways to build voice agents.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Most production voice agents use specialized models together in a pipeline – a speech-to-text model, a text-mode LLM, and a text-to-speech model.&lt;/item&gt;
      &lt;item&gt;Voice agent developers are beginning to experiment with new speech-to-speech models that take voice input directly and output audio instead of text.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using three specialized models is currently the best approach for enterprise use cases that require the highest degree of model intelligence and flexibility. But speech-to-speech models are an exciting development and will be a big part of the future of voice AI.&lt;/p&gt;
    &lt;p&gt;Whether we use a pipeline or a unified speech-to-speech model, voice agents are doing more and more sophisticated tasks. This means that, increasingly, production voice agents are actually multi-agent systems. Inside an agent, sub-agents handle asynchronous tasks, manage the conversation context, and allow code re-use between text and voice agents.&lt;/p&gt;
    &lt;p&gt;For a deep dive into voice agent architectures, models, and infrastructure, see the Voice AI &amp;amp; Voice Agents Illustrated Primer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open source models&lt;/head&gt;
    &lt;p&gt;Open models have not been widely used for production voice agents.&lt;/p&gt;
    &lt;p&gt;Voice agents are among the most demanding AI use cases. Voice agents perform long conversations. They must operate on noisy input audio and respond very quickly. Enterprise voice agent use cases require highly accurate instruction following and function calling. People interacting with voice agents have very high expectations for naturalness and “human-like” qualities of voice audio. In all of these areas, proprietary AI models have performed better than open models.&lt;/p&gt;
    &lt;p&gt;However, this is changing. Nemotron Speech ASR is both fast and accurate. On our benchmarks it performs comparably with or better than commercial speech-to-text models used today in production voice agents. Nemotron 3 Nano is the best-performing LLM in its class on our long-context, multi-turn conversation benchmarks.&lt;/p&gt;
    &lt;p&gt;Using open models allows us to configure and customize our models and inference stacks for the specific needs of our voice agents in ways that we can’t do with proprietary models. We can optimize for latency, fine-tune on our own data, host inference within our VPCs to satisfy data privacy and regulatory requirements, and implement observability that allows us to deliver the highest levels of reliability, scalability, and consistency.&lt;/p&gt;
    &lt;p&gt;We expect open models to be used in a larger and larger proportion of voice agent deployments over time. There are various flavors of “open” model licenses. NVIDIA has made the Nemotron Speech ASR and Nemotron 3 Nano available under the NVIDIA Permissive Open-Model License, which allows for unrestricted commercial use and the creation of derivative works.&lt;/p&gt;
    &lt;head rend="h1"&gt;An ultra-responsive voice agent&lt;/head&gt;
    &lt;head rend="h2"&gt;Fast, streaming transcription&lt;/head&gt;
    &lt;p&gt;The Nemotron Speech ASR model is designed specifically for use cases that demand very low latency transcription, such as voice agents.&lt;/p&gt;
    &lt;p&gt;The headline number here is that Nemotron Speech ASR consistently delivers final transcripts in under 24ms!&lt;/p&gt;
    &lt;p&gt;ASR (Automatic Speech Recognition) is the general term for machine learning models that process speech input, then output text and other information about that speech. Previous generations of ASR models were generally designed for batch processing rather than realtime transcription. For example, the latency of the Whisper model is 600-800ms, and most commercial speech-to-text models today have latencies in the 200-400ms range.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Openness&lt;/cell&gt;
        &lt;cell role="head"&gt;Deployment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Parakeet&lt;/cell&gt;
        &lt;cell&gt;open weights, open training data, open source inference&lt;/cell&gt;
        &lt;cell&gt;local in-cluster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Widely used commercial ASR&lt;/cell&gt;
        &lt;cell&gt;proprietary&lt;/cell&gt;
        &lt;cell&gt;cloud&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Whisper Large V3&lt;/cell&gt;
        &lt;cell&gt;open weights, open source inference&lt;/cell&gt;
        &lt;cell&gt;local in-cluster&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For more about the cache-aware architecture that enables this impressively low latency, see the NVIDIA post announcing the new model.&lt;/p&gt;
    &lt;p&gt;The model is also very accurate. The industry standard for measuring ASR model accuracy is word error rate. Nemotron Speech ASR has a word error rate on all of our benchmarks roughly equivalent to the best commercial ASR models, and substantially better than previous generation open models like Whisper.&lt;/p&gt;
    &lt;p&gt;To integrate Nemotron Speech ASR into Pipecat, we created a WebSocket server that performs the transcription inference and a client-side Pipecat service that can be used in any Pipecat agent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Running turn detection in parallel with transcription&lt;/head&gt;
    &lt;p&gt;The Nemotron Speech ASR model can be configured with four different context sizes, each of which have different latency/accuracy trade-offs. The context sizes are 80ms, 160ms, 560ms, and 1.2s. We use the 160ms context size, because this aligns with how we perform turn detection.&lt;/p&gt;
    &lt;p&gt;Turn detection means determining when the user has stopped speaking and the voice agent should respond. Accurate turn detection is critical to natural conversation. We’re using the open source Pipecat Smart Turn model in this voice agent. The Smart Turn model operates on input audio and runs in parallel with the Nemotron Speech ASR transcription.&lt;/p&gt;
    &lt;p&gt;We trigger both turn detection and transcript finalization any time we see a 200ms pause in the user’s speech. This gives us 200ms of “non-speech” trailing context after the user’s speech has finished. The Nemotron Speech ASR model actually needs a bit more trailing silence than this, to properly finalize the last words in the user speech. The padding calculation is:&lt;/p&gt;
    &lt;code&gt;nemotron_final_padding = (right_context + 1) * shift_frames * hop_samples
    = (1 + 1) * 16 * 160
    = 5120 samples = 320ms
&lt;/code&gt;
    &lt;p&gt;Our WebSocket transcription server receives 200ms of “non-speech” trailing audio data from the Pipecat service, and adds 120ms of synthetic silence to enable immediate finalization of the transcript. This works nicely.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nemotron 3 Nano&lt;/head&gt;
    &lt;p&gt;Nemotron 3 Nano is a new 30 billion parameter open source LLM from NVIDIA. Nemotron 3 Nano is the best performing model in its size class on our multi-turn conversation benchmarks.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Tool Use&lt;/cell&gt;
        &lt;cell role="head"&gt;Instruction&lt;/cell&gt;
        &lt;cell role="head"&gt;KB Ground&lt;/cell&gt;
        &lt;cell role="head"&gt;Pass Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Median Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;TTFB Med&lt;/cell&gt;
        &lt;cell role="head"&gt;TTFB P95&lt;/cell&gt;
        &lt;cell role="head"&gt;TTFB Max&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-5.1&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;916ms&lt;/cell&gt;
        &lt;cell&gt;2011ms&lt;/cell&gt;
        &lt;cell&gt;5216ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gemini-3-flash-preview&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;1193ms&lt;/cell&gt;
        &lt;cell&gt;1635ms&lt;/cell&gt;
        &lt;cell&gt;6653ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;claude-sonnet-4-5&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;2234ms&lt;/cell&gt;
        &lt;cell&gt;3062ms&lt;/cell&gt;
        &lt;cell&gt;5438ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-4.1&lt;/cell&gt;
        &lt;cell&gt;283/300&lt;/cell&gt;
        &lt;cell&gt;273/300&lt;/cell&gt;
        &lt;cell&gt;298/300&lt;/cell&gt;
        &lt;cell&gt;94.9%&lt;/cell&gt;
        &lt;cell&gt;97.8%&lt;/cell&gt;
        &lt;cell&gt;683ms&lt;/cell&gt;
        &lt;cell&gt;1052ms&lt;/cell&gt;
        &lt;cell&gt;3860ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gemini-2.5-flash&lt;/cell&gt;
        &lt;cell&gt;275/300&lt;/cell&gt;
        &lt;cell&gt;268/300&lt;/cell&gt;
        &lt;cell&gt;300/300&lt;/cell&gt;
        &lt;cell&gt;93.7%&lt;/cell&gt;
        &lt;cell&gt;94.4%&lt;/cell&gt;
        &lt;cell&gt;594ms&lt;/cell&gt;
        &lt;cell&gt;1349ms&lt;/cell&gt;
        &lt;cell&gt;2104ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-5-mini&lt;/cell&gt;
        &lt;cell&gt;271/300&lt;/cell&gt;
        &lt;cell&gt;272/300&lt;/cell&gt;
        &lt;cell&gt;289/300&lt;/cell&gt;
        &lt;cell&gt;92.4%&lt;/cell&gt;
        &lt;cell&gt;95.6%&lt;/cell&gt;
        &lt;cell&gt;6339ms&lt;/cell&gt;
        &lt;cell&gt;17845ms&lt;/cell&gt;
        &lt;cell&gt;27028ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-4o-mini&lt;/cell&gt;
        &lt;cell&gt;271/300&lt;/cell&gt;
        &lt;cell&gt;262/300&lt;/cell&gt;
        &lt;cell&gt;293/300&lt;/cell&gt;
        &lt;cell&gt;91.8%&lt;/cell&gt;
        &lt;cell&gt;92.2%&lt;/cell&gt;
        &lt;cell&gt;760ms&lt;/cell&gt;
        &lt;cell&gt;1322ms&lt;/cell&gt;
        &lt;cell&gt;3256ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;nemotron-3-nano-30b-a3b*&lt;/cell&gt;
        &lt;cell&gt;287/304&lt;/cell&gt;
        &lt;cell&gt;286/304&lt;/cell&gt;
        &lt;cell&gt;298/304&lt;/cell&gt;
        &lt;cell&gt;91.4%&lt;/cell&gt;
        &lt;cell&gt;93.3%&lt;/cell&gt;
        &lt;cell&gt;171ms&lt;/cell&gt;
        &lt;cell&gt;199ms&lt;/cell&gt;
        &lt;cell&gt;255ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-4o&lt;/cell&gt;
        &lt;cell&gt;278/300&lt;/cell&gt;
        &lt;cell&gt;249/300&lt;/cell&gt;
        &lt;cell&gt;294/300&lt;/cell&gt;
        &lt;cell&gt;91.2%&lt;/cell&gt;
        &lt;cell&gt;95.6%&lt;/cell&gt;
        &lt;cell&gt;625ms&lt;/cell&gt;
        &lt;cell&gt;1222ms&lt;/cell&gt;
        &lt;cell&gt;13378ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-oss-120b (groq)&lt;/cell&gt;
        &lt;cell&gt;272/300&lt;/cell&gt;
        &lt;cell&gt;270/300&lt;/cell&gt;
        &lt;cell&gt;298/300&lt;/cell&gt;
        &lt;cell&gt;89.3%&lt;/cell&gt;
        &lt;cell&gt;90.0%&lt;/cell&gt;
        &lt;cell&gt;98ms&lt;/cell&gt;
        &lt;cell&gt;226ms&lt;/cell&gt;
        &lt;cell&gt;2117ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;gpt-5.2&lt;/cell&gt;
        &lt;cell&gt;224/300&lt;/cell&gt;
        &lt;cell&gt;228/300&lt;/cell&gt;
        &lt;cell&gt;250/300&lt;/cell&gt;
        &lt;cell&gt;78.0%&lt;/cell&gt;
        &lt;cell&gt;92.2%&lt;/cell&gt;
        &lt;cell&gt;819ms&lt;/cell&gt;
        &lt;cell&gt;1483ms&lt;/cell&gt;
        &lt;cell&gt;1825ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;claude-haiku-4-5&lt;/cell&gt;
        &lt;cell&gt;221/300&lt;/cell&gt;
        &lt;cell&gt;172/300&lt;/cell&gt;
        &lt;cell&gt;299/300&lt;/cell&gt;
        &lt;cell&gt;76.9%&lt;/cell&gt;
        &lt;cell&gt;75.6%&lt;/cell&gt;
        &lt;cell&gt;732ms&lt;/cell&gt;
        &lt;cell&gt;1334ms&lt;/cell&gt;
        &lt;cell&gt;4654ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Like Nemotron Speech ASR, Nemotron 3 Nano is part of a new generation of open models that are designed specifically for speed and inference efficiency. See this resource from NVIDIA research for an overview of the Nemotron 3 hybrid Mamba-Transformer MoE architecture and links to technical papers.&lt;/p&gt;
    &lt;p&gt;A 30B parameter model is small enough to run very fast on high-end hardware, and can be quantized to run well on GPUs that many developers have at home!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model variant&lt;/cell&gt;
        &lt;cell role="head"&gt;Deployment&lt;/cell&gt;
        &lt;cell role="head"&gt;Resident memory&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nemotron-3-Nano BF16&lt;/cell&gt;
        &lt;cell&gt;full weights, Modal Cloud or DGX Spark&lt;/cell&gt;
        &lt;cell&gt;72GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nemotron-3-Nano Q8&lt;/cell&gt;
        &lt;cell&gt;8-bit quantization, faster operation on DGX Spark&lt;/cell&gt;
        &lt;cell&gt;32GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Nemotron-3-Nano Q4&lt;/cell&gt;
        &lt;cell&gt;4-bit quantization, RTX 5090&lt;/cell&gt;
        &lt;cell&gt;24GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;One note on which LLMs are generally used today for production voice agents: in general, voice agents for applications like customer support need the most “intelligent” models we have available. Voice agent use cases are demanding. A customer support AI agent must do highly accurate instruction following and function calling tasks throughout a long, open-ended, unpredictable human conversation. A 30B parameter model – even one as good as Nemotron 3 Nano – is generally best suited for specialized voice tasks like a home assistant or software voice UI interface.&lt;/p&gt;
    &lt;p&gt;NVIDIA has announced that two larger Nemotron 3 models are coming soon. If the performance of these larger models relative to their size is similar to Nemotron 3 Nano’s performance, we expect these models to be terrific intelligence engines for voice agents.&lt;/p&gt;
    &lt;p&gt;In the meantime, Nemotron 3 Nano is the best-performing LLM that I can run on hardware I have at home. I’ve been using this model for a wide variety of “local” voice agent tasks and development experiments on both an NVIDIA DGX Spark and on my desktop computer with an RTX 5090.&lt;/p&gt;
    &lt;p&gt;You can use Nemotron 3 in reasoning or non-reasoning mode. We usually turn off reasoning for the fast-response core voice agent loop.&lt;/p&gt;
    &lt;p&gt;For details on using Nemotron 3 Nano in the cloud and building local containers with the latest CUDA, vLLM and llama.cpp support for this new model, see the GitHub repository accompanying this post. There are a couple of inference tooling patches (relating to the reasoning output format in vLLM and to llama.cpp KV caching) that you might find useful if you’re experimenting with this model.&lt;/p&gt;
    &lt;head rend="h2"&gt;Magpie streaming server&lt;/head&gt;
    &lt;p&gt;Magpie is a family of text-to-speech models from NVIDIA. In our voice agent project, we’re using an experimental preview checkpoint of an upcoming open source version of Magpie.&lt;/p&gt;
    &lt;p&gt;Kudos to NVIDIA for releasing this early look at a Magpie model designed, like Nemotron Speech ASR, for streaming, low-latency use cases! We’ve been having a lot of fun experimenting with this preview, doing things that are only possible with open source weights and inference code.&lt;/p&gt;
    &lt;p&gt;You can use this Magpie model in batch mode by sending an HTTP request with a chunk of text. This batch mode inference delivers audio for a single sentence in about 600ms on the DGX Spark and 300ms on the RTX 5090. But for voice agents, we like to stream all tokens as much as we can, and because Magpie is open source, we can hack together a hybrid streaming mode that optimizes for initial audio chunk latency! This hybrid streaming approach improves average initial response latency 3x.&lt;/p&gt;
    &lt;head rend="h3"&gt;TTS TTFB Comparison: Batch → Streaming&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Hardware&lt;/cell&gt;
        &lt;cell role="head"&gt;P50 Improvement&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean Improvement&lt;/cell&gt;
        &lt;cell role="head"&gt;P90 Improvement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;RTX 5090&lt;/cell&gt;
        &lt;cell&gt;90 ms (1.9x)&lt;/cell&gt;
        &lt;cell&gt;204 ms (3.0x)&lt;/cell&gt;
        &lt;cell&gt;430 ms (5.2x)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DGX Spark&lt;/cell&gt;
        &lt;cell&gt;236 ms (2.3x)&lt;/cell&gt;
        &lt;cell&gt;415 ms (3.3x)&lt;/cell&gt;
        &lt;cell&gt;836 ms (4.6x)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Details&lt;/head&gt;
    &lt;head rend="h5"&gt;RTX 5090&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Batch&lt;/cell&gt;
        &lt;cell&gt;106 ms&lt;/cell&gt;
        &lt;cell&gt;630 ms&lt;/cell&gt;
        &lt;cell&gt;191 ms&lt;/cell&gt;
        &lt;cell&gt;533 ms&lt;/cell&gt;
        &lt;cell&gt;305 ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pipeline&lt;/cell&gt;
        &lt;cell&gt;99 ms&lt;/cell&gt;
        &lt;cell&gt;103 ms&lt;/cell&gt;
        &lt;cell&gt;101 ms&lt;/cell&gt;
        &lt;cell&gt;103 ms&lt;/cell&gt;
        &lt;cell&gt;101 ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;DGX Spark&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Batch&lt;/cell&gt;
        &lt;cell&gt;193 ms&lt;/cell&gt;
        &lt;cell&gt;1440 ms&lt;/cell&gt;
        &lt;cell&gt;422 ms&lt;/cell&gt;
        &lt;cell&gt;1067 ms&lt;/cell&gt;
        &lt;cell&gt;595 ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pipeline&lt;/cell&gt;
        &lt;cell&gt;15 ms&lt;/cell&gt;
        &lt;cell&gt;276 ms&lt;/cell&gt;
        &lt;cell&gt;186 ms&lt;/cell&gt;
        &lt;cell&gt;231 ms&lt;/cell&gt;
        &lt;cell&gt;180 ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There’s definitely a quality trade-off with our simple streaming implementation. Try the agent yourself, or listen carefully to the conversation in the video at the beginning of this blog post. You can usually hear a slight disfluency where we “stitch” together the streaming chunks at the beginning of the model response.&lt;/p&gt;
    &lt;p&gt;To do better, we’d need to retrain part of the model and use a slightly more sophisticated inference approach. Fortunately, this is on the NVIDIA road map.&lt;/p&gt;
    &lt;p&gt;We integrated this model into Pipecat by creating a WebSocket server for streaming inference, and a client-side Pipecat service. (This is the same approach we used with Nemotron Speech ASR).&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting the models together and measuring latency&lt;/head&gt;
    &lt;p&gt;These Nemotron and upcoming Magpie models are completely open: open weights, open source training data sets, and open source inference tooling. Working with open models in production feels like a super-power. We can do things like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read the inference code to understand the context requirements of the ASR model, so that we can optimize the interactions between our Pipecat pipeline components and text-to-speech audio buffer handling. (See our description of this above, in the section Fast, streaming transcription.&lt;/item&gt;
      &lt;item&gt;Fix issues with inference tooling support in new models and on whatever platforms we’re running on. See the code and README.md in the GitHub repo for the small patches we made for vLLM and llama.cpp, and the Docker container build with full MX4FP support for both of those inference servers on DGX Spark and RTX 5090.&lt;/item&gt;
      &lt;item&gt;Build a semi-streaming inference server for a preview model checkpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Often when we’re building voice agents, our primary concern is to engineer the agent to respond quickly in a real-world conversation. The difference between good latency and an agent too slow to use in production is often a combination of several optimizations, each one cutting peak latencies by 100 or 200ms. Working with open models gives us control over how we prioritize for latency compared to throughput, how we design streaming and chunking of inference results, how to use models together optimally, and many other small things that add up (or subtract down) to fast response times.&lt;/p&gt;
    &lt;p&gt;It’s useful to measure voice-to-voice latency – the time between the user’s voice stopping and the bot’s voice response starting – in two places: on the server-side and at the client.&lt;/p&gt;
    &lt;p&gt;We can easily automate the server-side latency measurement. Our bot outputs a log line with a voice-to-voice latency metric for each turn.&lt;/p&gt;
    &lt;code&gt;2026-01-01 22:43:26.208 | INFO     | v2v_metrics:process_frame:54 - V2VMetrics: ServerVoiceToVoice TTFB: 465ms
&lt;/code&gt;
    &lt;p&gt;We also output log lines with time-to-first-byte for each of our models, and several other log lines that are useful for understanding exactly where we’re “spending our latency budget.” The Pipecat Playground shows graphs of these metrics, which is useful during development and testing. Here’s a test session with our bot running on an RTX 5090.&lt;/p&gt;
    &lt;p&gt;RTX 5090&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ASR&lt;/cell&gt;
        &lt;cell&gt;13ms&lt;/cell&gt;
        &lt;cell&gt;19ms&lt;/cell&gt;
        &lt;cell&gt;23ms&lt;/cell&gt;
        &lt;cell&gt;70ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LLM&lt;/cell&gt;
        &lt;cell&gt;71ms&lt;/cell&gt;
        &lt;cell&gt;171ms&lt;/cell&gt;
        &lt;cell&gt;199ms&lt;/cell&gt;
        &lt;cell&gt;255ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;TTS&lt;/cell&gt;
        &lt;cell&gt;99ms&lt;/cell&gt;
        &lt;cell&gt;108ms&lt;/cell&gt;
        &lt;cell&gt;113ms&lt;/cell&gt;
        &lt;cell&gt;146ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;V2V&lt;/cell&gt;
        &lt;cell&gt;415ms&lt;/cell&gt;
        &lt;cell&gt;508ms&lt;/cell&gt;
        &lt;cell&gt;544ms&lt;/cell&gt;
        &lt;cell&gt;639ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;DGX Spark&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Min&lt;/cell&gt;
        &lt;cell role="head"&gt;P50&lt;/cell&gt;
        &lt;cell role="head"&gt;P90&lt;/cell&gt;
        &lt;cell role="head"&gt;Max&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ASR&lt;/cell&gt;
        &lt;cell&gt;24ms&lt;/cell&gt;
        &lt;cell&gt;27ms&lt;/cell&gt;
        &lt;cell&gt;69ms&lt;/cell&gt;
        &lt;cell&gt;122ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LLM&lt;/cell&gt;
        &lt;cell&gt;343ms&lt;/cell&gt;
        &lt;cell&gt;750ms&lt;/cell&gt;
        &lt;cell&gt;915ms&lt;/cell&gt;
        &lt;cell&gt;1669ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;TTS&lt;/cell&gt;
        &lt;cell&gt;158ms&lt;/cell&gt;
        &lt;cell&gt;185ms&lt;/cell&gt;
        &lt;cell&gt;204ms&lt;/cell&gt;
        &lt;cell&gt;1171ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;V2V&lt;/cell&gt;
        &lt;cell&gt;759ms&lt;/cell&gt;
        &lt;cell&gt;1180ms&lt;/cell&gt;
        &lt;cell&gt;1359ms&lt;/cell&gt;
        &lt;cell&gt;2981ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It’s also critical to measure the voice-to-voice latency as actually perceived by the user. This is harder to do automatically, especially for telephone call voice agents. The best approach to measuring client-side voice-to-voice latency is to record a call, load the audio file into an audio editor, and measure the gap between the end of the user’s speech waveform and the start of the bot speech waveform. You can’t cheat this measurement, or forget to include an important processing component! We do this periodically in both development and testing, as a sanity check. Here I’m measuring latency in the Descript editor of one turn in the conversation we recorded for the video at the top of this post.&lt;/p&gt;
    &lt;p&gt;You will typically see client-side voice-to-voice latency numbers about 250ms higher than server-side numbers for a WebRTC voice agent. This is time spent in audio processing at the operating system level, encoding and decoding, and network transport. Usually, this delta is a bit worse for telephone call agents: 300-600ms of extra latency in the telephony path that you don’t have much way to optimize. (Though there are some basic things you should do, such as make sure your voice agent is hosted in the same region as your telephony providers servers.) For more on latency, see the Voice AI and Voice Agents Illustrated Guide.&lt;/p&gt;
    &lt;head rend="h2"&gt;An inference optimization for local voice agents&lt;/head&gt;
    &lt;p&gt;We have one more trick up our sleeve when we’re running voice agents locally on a single GPU.&lt;/p&gt;
    &lt;p&gt;When we run voice agents in production in the cloud, we run each AI model on a dedicated GPU. We stream tokens from each model as fast as we can, and send them down the Pipecat pipeline as they arrive.&lt;/p&gt;
    &lt;p&gt;But when we’re running locally, all the models are sharing one GPU. In this context, we can engineer much faster voice-to-voice responses if we carefully schedule inference. In our voice agent for this project, we’re doing two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We run the Smart Turn model on the CPU so that we can dedicate the GPU to transcription when user speech is arriving. The Smart Turn model runs faster on GPU, but it runs fast enough on CPU, and dividing up the workload this way gives us the best possible performance between the two models.&lt;/item&gt;
      &lt;item&gt;We interleave small segments of LLM and TTS inference so that GPU resources are dedicated to one model at a time. This significantly reduces time-to-first-token for each model. First we generate a few small chunks of LLM tokens, then TTS audio, then LLM again, then TTS, etc. We generate a smaller segment for the very first response, so we can start audio playout as quickly as possible. We designed this interleaved chunking approach to work in concert with the hybrid Magpie streaming hack described above.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a sequence diagram showing the interleaved LLM and TTS inference. The three vertical lines in the diagram represent, from left to right:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Tokens arriving in small batches to the Pipecat LLM service in the agent and being pushed down the pipeline.&lt;/item&gt;
      &lt;item&gt;The Pipecat TTS service, managing the frames from the LLM service, dividing the stream on sentence boundaries, and making inference requests to the Magpie WebSocket server running in our local Docker container.&lt;/item&gt;
      &lt;item&gt;The Magpie WebSocket server doing inference and sending back audio.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We wrote a custom WebSocket inference server for Magpie, so we control the Pipecat-to-Magpie protocol completely. We’re using llama-server code from the llama.cpp project for LLM inference. Traditional inference stacks aren’t really designed to do this specific kind of chunking, so our code sets a max tokens count (&lt;code&gt;n_predict&lt;/code&gt; in llama.cpp), runs repeated small inference chunks, and does some of the buffer management client-side. This could be done more efficiently, using the llama.cpp primitives directly. Writing a perfectly optimized inference server for this interleaved design would be a fun weekend project, and is something that almost anyone with a little bit of programming experience and a willingness to go down some rabbit holes could work together with Claude Code to implement.&lt;/p&gt;
    &lt;head rend="h1"&gt;Running this voice agent&lt;/head&gt;
    &lt;p&gt;For enterprise-scale, production use, deploy this agent to the Modal GPU cloud. There are instructions in the GitHub Readme.md. Modal is a serverless GPU platform that makes it easy to deploy AI models for development or production use.&lt;/p&gt;
    &lt;p&gt;For local development, the GitHub repo has a Dockerfile for DGX Spark (arm64 + Blackwell GB10 CUDA 13.1) and RTX 5090 (x86_64 + Blackwell CUDA 13.0)&lt;/p&gt;
    &lt;p&gt;If you’re interested in building voice agents, here are some resources you might be interested in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Voice AI &amp;amp; Voice Agents Illustrated Primer&lt;/item&gt;
      &lt;item&gt;YouTube recordings of the community voice agents course sessions from last year&lt;/item&gt;
      &lt;item&gt;The Pipecat Discord, where lots of knowledgeable voice agent developers hang out.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.daily.co/blog/building-voice-agents-with-nvidia-open-models/"/><published>2026-01-07T16:08:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46528192</id><title>BillG the Manager (2021)</title><updated>2026-01-07T21:39:23.916484+00:00</updated><content>&lt;doc fingerprint="27401b374f6f26de"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;019. BillG the Manager&lt;/head&gt;
    &lt;head rend="h3"&gt;“Intelli…what?”–Bill Gates&lt;/head&gt;
    &lt;p&gt;The breadth of the Microsoft product line and the rapid turnover of core technologies all but precluded BillG from micro-managing the company in spite of the perceptions and lore around that topic. In less than 10 years the technology base of the business changed from the 8-bit BASIC era to the 16-bit MS-DOS era and to now the tail end of the 16-bit Windows era, on the verge of the Win32 decade. How did Bill manage this — where and how did he engage? This post introduces the topic and along with the next several posts we will explore some specific projects.&lt;/p&gt;
    &lt;p&gt;Please feel free to share this and subscribers, please join in the discussion.&lt;/p&gt;
    &lt;p&gt;Back to 018. Microsoft’s Two Bountiful Gardens&lt;/p&gt;
    &lt;p&gt;At 38, having grown Microsoft as CEO from the start, Bill was leading Microsoft at a global scale that in 1993 was comparable to an industrial-era CEO. Even the legendary Thomas Watson Jr., son of the IBM founder, did not lead IBM until his 40s. Microsoft could never have scaled the way it did had BillG managed via a centralized hub-and-spoke system, with everything bottlenecked through him. In many ways, this was BillG’s product leadership gift to Microsoft—a deeply empowered organization that also had deep product conversations at the top and across the whole organization.&lt;/p&gt;
    &lt;p&gt;This video from the early 1980’s is a great introduction to the breadth of Microsoft’s product offerings, even at a very early stage of the company. It also features some vintage BillG voiceover and early sales executive Vern Raburn. (Source: Microsoft videotape)&lt;/p&gt;
    &lt;p&gt;Bill honed a set of undocumented principles that defined interactions with product groups. The times of legendary BillG reviews characterized by hardcore challenges and even insults had become, mostly, a thing of the past excepting the occasional sentimental outburst. More generally, they were a collective memory of hyper-growth moments any start-up experiences, only before the modern era when such stories were more commonly understood.&lt;/p&gt;
    &lt;p&gt;Much later in 2006, when BillG announced his intent to transition from full time Microsoft and part time philanthropy to full time philanthropy, many reporters surprised him by asking how Microsoft would continue without his coordination of technical strategy and oversight. But even in the early ’90s, at the height of the deepest and most challenging technology strategy questions, he never devoted the bulk of his time to micromanaging product development. He spent a good deal of time engaged with products, but there were far too many at too many stages of development to micro-manage them. In many ways this was the opposite of the approach Steve Jobs took, even if both were known for their own forms of challenging interactions. The most obvious contrast between the two was the breadth of the product line and the different market touchpoints.&lt;/p&gt;
    &lt;p&gt;Having grown up through Development Tools and Languages, I was familiar with Microsoft’s product line, but only as TA did it become clear how comparatively broad Microsoft had so quickly become. The software world was thought of through a lens of major categories: operating systems, tools and languages, networking, and applications, roughly mirroring Microsoft’s org chart. The latter was thought of as word processing, spreadsheets, graphics, databases, as well as assorted smaller categories. It was easy to identify leaders in each of those areas—names that were tip of the tongue at the time and most of which are no longer in the PC software space (IBM, Borland, Novell, WordPerfect, Lotus, Aldus, Ashton Tate, and many more). The ah-ha moment in the early 1990s was the realization that no company on that list was competing in more than one category. Microsoft was hardly winning in every category. In fact, in most categories it was new entry, a distant second, or even third place, but the company was in every space. Bill was committed and patient. Microsoft was relentless. And Microsoft was focused on Windows.&lt;/p&gt;
    &lt;p&gt;BillG had fostered Microsoft with a grand vision to compete in every category of PC software, from some of the earliest days. With rare exceptions, no other company set out to do that. BillG led a deep technology strategy. It started with the operating system, supported by tools and languages, and then using those to build applications. This seemed simple enough. In fact, it is what IBM built for mainframes and DEC built for minicomputers.&lt;/p&gt;
    &lt;p&gt;There was a crucial difference. Microsoft did not build hardware and was not vertically integrated to reduce competition. Microsoft built an operating system on an openly architected PC (the same Intel-based architecture that came to power both Macintosh and Linux years later) and published APIs so that anyone could build tools and applications for the operating system—an open hardware platform and open operating system APIs. This approach simply addressed all the early challenges Microsoft itself faced trying to figure out how to build winning applications—it was so busy dealing with dozens of proprietary computing platforms, each with their own tools and APIs just different enough to make things difficult, but not so different as to be valuable. Bill saw the value in software and in openness at key points in the overall architecture. At the formation of the company, he and PaulA saw the immense and expansive value of software and, essentially, the liability that being in the hardware business carried. Building Microsoft’s software-only business on an open hardware platform where many players competed to drive prices down while maintaining compatibility with the operating system was one of the all-time great strategy choices. The idea of building hardware seemed like a sucker’s bet, with low margins, manufacturing, and inventory—the baggage of the physical world. While Microsoft would dabble in peripherals or hardware that could bootstrap new PC scenarios, building whole computers was a headache better left to others.&lt;/p&gt;
    &lt;p&gt;Expanding the impact of that breadth software strategy was BillG’s day-to-day operating model, not micromanaging the specifics of any given project. I am painting this with a broad brush, intentionally so. Part of the difference between the then dominant cultures of Systems and Apps was that during the MikeMap era (and arguably during the earlier JeffH era), Apps weaned itself from Bill’s intense and constant scrutiny whereas the Systems culture more clearly embraced that dynamic. That was largely true until PaulMa took a more hands-off (or walled-off) approach to the nurturing of the NT project.&lt;/p&gt;
    &lt;p&gt;In his May 1991 email, “Challenges and Strategy,” BillG set the company on the Windows strategy, clarifying the foundations for every product and group, solidifying what had been complex platform choices every team faced. Regardless of whether Bill was a savant when it came to the technical details of projects or he simply remembered everything each group sent or told him, he operated the company at a higher level of abstraction than reporters believed to be the case in 2008 when he ultimately reduced his full-time commitment to Microsoft.&lt;/p&gt;
    &lt;p&gt;I had a glimpse of this when our AFX team had our pivotal review. Later as TA I was there to connect the dots and amplify the Windows strategy. By and large the company was still wrapping itself around the details of what it really meant to embrace Windows, exclusively. That, and coping with the myriad of choices and decisions that come from the tension between aligning with a Windows strategy and having some control over your own destiny as a product. Which version of Windows? When is that shipping? Will the APIs our product needs be in Windows? Will those APIs work on older versions of Windows? What about Windows NT? On, which microprocessors? What about the other parts of Microsoft? The questions were endless. This was truly big company stuff—the strategy at a high level is one thing, but execution across a $600M (1994) research and development budget was another. The fascinating thing was how products so quickly scaled beyond what Bill personally experienced as a programmer, both in size and technology specifics. This was to be expected—by any measure the company was huge—but people and Bill himself still expected to interact on product details as though he was a member of the product team. I often found myself looking for ways to help Bill engage at that level, even if just for show.&lt;/p&gt;
    &lt;p&gt;In addition to the Windows strategy, with the late 1993 launch of Office 4, Microsoft also declared 1994 “Year of Office”. It was the biggest launch for Apps and represented a major pivot of the organization to the opportunity of selling a suite of products. This too was in the earliest days of a strategy, one that I would end up spending significant time on as TA and then later as a member of the team.&lt;/p&gt;
    &lt;p&gt;Just because Bill operated at a level of abstraction across products groups did not preclude product groups from engaging on what might seem like relatively small, non-technical matters. One of the more entertaining meetings I attended was preparing for the launch of Office 4, which was a worldwide event complete with a reporter given permission to shadow the team. A key differentiator would be how the user would experience “intelligence” in the product, so that it understood what was intended and how to achieve it in the new Office software. The development team built a series of features along the lines of what was termed “basic use” such as AutoCorrect in Word, AutoFilter in Excel tables, and a host of Wizards (guided step-by-step flows such as for creating charts), and more. To bring them together and actually communicate with the market and on retail packaging, the marketing team came up with an umbrella term. Pete Higgins (PeteH) came over to brief BillG on that choice in a small meeting in Bill’s office.&lt;/p&gt;
    &lt;p&gt;PeteH was by then the spiritual leader of the business side of Apps. He rose through the ranks of Excel and was clearly MikeMap’s lead executive. Pete was the kind of calm and in control leader that everyone enjoyed working for—he was at once clearly the boss, but also a member of the team. Pete was a native of the Seattle area, high school football star, and Stanford graduate. He was a new generation of Microsoft product executive, coming from the business and not the coding side. For me in my TA role, Pete was one of my biggest supporters and mentors and made connecting with Apps super easy.&lt;/p&gt;
    &lt;p&gt;Sitting at the little couch under the Intel chip poster, after going through the details of the launch, Pete said the proverbial “there’s one more thing.” Bill rocking in his chair shook his head, given that the meeting was mostly an uneventful recap of the upcoming press tour. Pete went on to explain the problem of communicating all the features and how Microsoft needed a term to market and describe them. Pete was dancing around this because he knew well enough that Bill was not a fan of “marketing”. Ever so delicately Pete said, “this is your chance…we want to go with this term but if you don’t like it…”&lt;/p&gt;
    &lt;p&gt;Pete then said, “IntelliSense. Microsoft Office introduces IntelliSense.”&lt;/p&gt;
    &lt;p&gt;Bill’s reply, “Intelli…what?”&lt;/p&gt;
    &lt;p&gt;Pete again tried to position the positioning, his instinct about resistance proving correct. “It is IntelliSense…it means that Office has built-in intelligence, and it understands what you need and how to do it.”&lt;/p&gt;
    &lt;p&gt;Bill still not warming up, went full pedantic, “what intelligence…is there a Prolog rules engine, a neural network, ….” He was also making the scrunched up surprised look that he does, which turns out (once you realize it) to also be a bit sarcastic. It meant he was warming up.&lt;/p&gt;
    &lt;p&gt;A few more times back and forth, and Pete just made Bill say IntelliSense in a sentence one more time, which he did with kind of a devilish smirk.&lt;/p&gt;
    &lt;p&gt;Done.&lt;/p&gt;
    &lt;p&gt;Looking back this all seems absurd. Consternation over a single phrase. Literally seeking approval to use it from the CEO of a billion-dollar company. All on the heels of what was no doubt months of preparation, including getting SteveB’s approval which was actually critical. Finally, the theater that Pete would pull the plug a few weeks before the tour. In some ways this was the Apps way of bringing decisions to Bill—it wasn’t really a choice and it had been broadly vetted and was buttoned-up. Any debate would probably be theater more than anything.&lt;/p&gt;
    &lt;p&gt;On average, there was one product-focused meeting on most days. Most teams saw Bill once or twice a year. NathanM saw Bill most every day or at least in most every technology context, present day or far out there. Most executives, like PaulMa, PeteH (leading Apps), and Susan Boeschen (SusanB leading consumer), saw Bill in product review contexts several times a month because each had many ongoing projects or, in the case of the big projects (like operating systems), many large components. Everyone was in constant contact over email. Bill was always forwarding emails across the company, adding relevant people from all levels of the organization to the CC line, and never backed off a good reply-all opportunity. Phone or in-person 1:1s were not the typical way of interacting across the product executive team. For the most part, work happened in groups or at least with an audience, with outcomes and flare-ups quickly disseminated by email. I found myself constantly on the move walking around campus from one building to the next to meet people in person, rarely was I in my office (a pattern that continued my entire career).&lt;/p&gt;
    &lt;p&gt;I was often asked to meet with teams before they met with Bill. They hoped for insight into how BillG might think about choices and decisions or even the presentation overall. I often disappointed teams in these pre-meetings since I was hardly a stand-in for Bill, and I was hardcore about leaving any such impression. Pre-meetings gave me a chance to better understand the issues the team was struggling with and to make sure those were brought forward in an objective and transparent manner. The fastest path to failure was to structure a conversation so Bill discovered an issue rather than having it revealed to him. To be fair, an equally fast path to failure was a first slide listing a slew of problems and issues in the hopes of inoculating the remainder of the meeting. In that case, I would caution teams that they were exposing themselves to the inevitable “How can this be so difficult?” comments. Getting this balance right was the essence of leading an effective meeting.&lt;/p&gt;
    &lt;p&gt;For most meetings, I wrote a summary meeting preview. Even though Bill said he did not want this, I could not help myself. While he was always effective, I felt that a little bit of specifics could go a long way in making the meeting more effective and less random. I could tell he had read my mail if he raised a point verbatim from my note, and frequently he would kick off the meeting doing so, never crediting me of course. In these, and all mails talking about other teams, I always tried to separate the facts of the meeting, the team’s analysis, and my own opinion. Bill was transparent with email and thought little of forwarding an entire thread. I learned the ramifications of that the hard way.&lt;/p&gt;
    &lt;p&gt;As an example of where I failed to follow my own rules about fact versus opinion, I totally offended Jim Allchin (JimAll), leading the Cairo project, on the role of a specific technology in distributed programming. Not only did Jim inform me that my opinions were wrong, but also that I stepped all over his own PhD dissertation as a leading expert. In hindsight, this was terrifying—Jim’s reply was brutal—but it proved a good early learning experience, so to speak.&lt;/p&gt;
    &lt;p&gt;While the product line was already broad, the expansion to entirely new areas was unstoppable. On most any product area, we were forming an opinion, beginning work, or already in the market. There was not a booth at a tradeshow, a focused conference, or a major company looking to partner that Microsoft was not already connected to or connecting with in some way. While Microsoft was in the earliest days of achieving a PC in every home (about 25 percent of US households in 1993) and on every desktop (about half of US workers in 1993), every day in this job was either furthering that or expanding beyond homes and desktops from data centers to handhelds to airplanes (the first in-flight PC-based system was an early partnership between Microsoft and an airline, including certification for Windows Server).1&lt;/p&gt;
    &lt;p&gt;Product meetings had no set format or structure and usually reflected the culture of the organization. This might be a surprise to some as many CEOs (or perhaps their staff!) might have imposed some more rigor on meetings. Microsoft had two bountiful gardens, but there were micro-cultures throughout out the company. While one group did slick and well-rehearsed presentations, another might present research-heavy deep dives. Bill often pushed a team outside its comfort zone, deliberately pushing the team to discuss places they were less prepared, or even less interested. It was a technique he employed. He once said to me, “Why spend all the time with the Windows team talking about architecture, if that was their predisposition anyway?” This was also a strategy to level the playing field—talking about architecture to Windows or ease of use to Excel was too lopsided and Bill was disadvantaged.&lt;/p&gt;
    &lt;p&gt;The reality of BillG Reviews never lived up to lore.&lt;/p&gt;
    &lt;p&gt;Most meetings progressed without incident—meaning without yelling. Sometimes, though, there were comments such as “That was the stupidest thing I ever heard” or “That is brain-dead.” The worst was “That’s trivial . . . let me show you.” Those were all the clichés that teams anticipated but then wore as a badge of honor. They happened with far less frequency compared to how much they were talked about. Even over the short period of time I worked as TA, Bill became more intentional in his use of meeting dynamics. Still, the first seconds of a meeting remained a bit of a mood thermometer, pity those for whom it was clearly a bad day.&lt;/p&gt;
    &lt;p&gt;When meetings ended up “bad” it was always because the team was poorly prepared, or they came to talk about the project in a way that diverged from expectations. There were typical capital offenses in the meeting, such as failing to understand a product strategy of competitors or downplaying a competitor’s potential. Worst was coming across as though a product was making mostly tactical decisions driven by schedule or failing to understand the architecture of the product relative to the evolving platform and related teams across Microsoft. PivotTables were just making their way across most teams, so many were still making the common errors of using static charts and graphs that always seemed to have the data oriented or filtered in the least useful way. Those moments always held potential for a lively discussion.&lt;/p&gt;
    &lt;p&gt;Part of my role was to reduce the potential for such liveliness ahead of time. I tried to alert teams about potential issues without acting as a surrogate for Bill, and to make sure meetings did not save the difficult or bad news for the end. I was also there to throw myself on the grenade, so to speak, and get meetings back on track by helping the team through a tough moment—usually by restating or interpreting what they were saying or by redirecting the topic at hand to a follow-up discussion.&lt;/p&gt;
    &lt;p&gt;By far the biggest strategic error one could make was knowingly duplicating code outside core expertise, and then compounding that by attempting to explain why in this particular case it is justified. Microsoft Publisher was a new product in the desktop publishing category. It was being built by the Consumer Division under the leadership of Melinda French (MelindaF). The product aimed for the small business and non-professional market, compared to the incumbent Aldus PageMaker. It differentiated itself with ease-of-use features, pioneering Wizards and other user interface innovations. But it also produced printed pages that looked a lot like what one should be able to create with Microsoft Word. This overlap was the source of endless consternation—why can’t they share code, why can’t Word do all these features, and then ultimately why does Publisher even exist. Yet, customers loved it. At one point, a meeting went down a rabbit hole over bullets and numbering and how Publisher was basically writing all the same code Word was and wasting everyone’s resources. There was little actionable in this kind of rant, but it did establish the norm of being called out for redundancy and the need to be prepared to cope with the feedback.&lt;/p&gt;
    &lt;p&gt;Bill maintained a deep commitment to evaluating a portfolio of efforts, and even within a single product he believed in the portfolio approach of features—not every product nor every feature was a winner or a breakthrough, but on the whole something needed to be working. As much as Bill might give a group a difficult time (as happened with Visual C++), he knew there was always more to the product and more products to the company. It was not just that Bill was building a product portfolio for Microsoft, he was managing the teams as a portfolio of efforts. This portfolio approach created a resiliency in the company—resilient to the unpredictable nature of technology bets and to the ability of the people on the team to execute. Not everything went as planned nor did every planned bet ultimately make sense.&lt;/p&gt;
    &lt;p&gt;Whether deliberate or not, BillG had three axes that created a constant state of balance, of push and pull, across the hundred teams creating software. Bill’s approach of constantly balancing the tension between innovation and shipping, expanding the portfolio while maintaining coherency, and the injection of new ideas while also executing on existing work proved to be the most interesting “management” lesson. The next three sections are examples of each of these dimensions.&lt;/p&gt;
    &lt;p&gt;On to 020. Innovation versus Shipping: The Cairo Project&lt;/p&gt;
    &lt;p&gt;https://nces.ed.gov/programs/digest/d08/tables/dt08_432.asp&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hardcoresoftware.learningbyshipping.com/p/019-billg-the-manager"/><published>2026-01-07T16:18:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46528353</id><title>Health care data breach affects over 600k patients, Illinois agency says</title><updated>2026-01-07T21:39:23.529586+00:00</updated><content>&lt;doc fingerprint="e6188ade5513a61a"&gt;
  &lt;main&gt;
    &lt;p&gt;The names and addresses of thousands of patients of the Illinois Department of Human Services were incorrectly made publicly viewable for the last several years, the agency said Friday.&lt;/p&gt;
    &lt;p&gt;Several maps created to assist the agency with decisions — like where to open new offices and allocate certain resources — were made public through incorrect privacy settings between 2021 and 2025, the Department of Human Services said in a statement.&lt;/p&gt;
    &lt;p&gt;More than 32,000 customers with the IDHS division of rehabilitation services had information publicly viewable between April 2021 and September 2025. The information included names, addresses, case numbers, case status, referral source information, region and office information and status as Division of Rehabilitation Services recipients, the agency said.&lt;/p&gt;
    &lt;p&gt;Around 670,000 Medicaid and Medicare Savings Program recipients had their addresses, case numbers, demographic information and the name of medical assistance plans publicly viewable between January 2022 and September 2025, IDHS said.&lt;/p&gt;
    &lt;p&gt;The state agency said the mapping website was unable to identify who viewed the maps, and IDHS is unaware of any misuse of personal information resulting from the data leak.&lt;/p&gt;
    &lt;p&gt;IDHS discovered the issue Sept. 22 and immediately changed the privacy settings for all maps, restricting access to authorized IDHS employees, the agency said. It also implemented a secure map policy that prohibits uploading customer data to public mapping websites.&lt;/p&gt;
    &lt;p&gt;Individuals whose information was made public will receive a notice about the leak from IDHS. The notices will include a phone number that people can call for more information.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nprillinois.org/illinois/2026-01-06/health-care-data-breach-affects-600-000-patients-illinois-agency-says"/><published>2026-01-07T16:28:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46529237</id><title>Eat Real Food</title><updated>2026-01-07T21:39:23.373262+00:00</updated><content>&lt;doc fingerprint="cab4a7579f6f9feb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Real Food&lt;lb/&gt; Starts Here&lt;/head&gt;&lt;p&gt;Better health begins on your plate—not in your medicine cabinet.&lt;lb/&gt; The new Dietary Guidelines for Americans defines real food as whole, nutrient-dense, and naturally occurring, placing them back at the center of our diets.&lt;/p&gt;&lt;head rend="h2"&gt;The State of Our Health&lt;/head&gt;&lt;head rend="h3"&gt;America is sick.&lt;lb/&gt;The data is clear.&lt;/head&gt;&lt;head rend="h3"&gt;50% of Americans have prediabetes or diabetes&lt;/head&gt;&lt;head rend="h3"&gt;75% of adults report having at least one chronic condition&lt;/head&gt;&lt;head rend="h3"&gt;90% of U.S. healthcare spending goes to treating chronic disease—much of which is linked to diet and lifestyle&lt;/head&gt;&lt;p&gt;For decades we've been misled by guidance that prioritized highly processed food, and are now facing rates of unprecedented chronic disease.&lt;/p&gt;&lt;p&gt;For the first time, we're calling out the dangers of highly processed foods and rebuilding a broken system from the ground up with gold-standard science and common sense.&lt;/p&gt;&lt;head rend="h2"&gt;The New Pyramid&lt;/head&gt;&lt;head rend="h2"&gt;Eat Real &lt;lb/&gt;Food&lt;/head&gt;&lt;p&gt;Our nation is finding its footing again, moving past decades of unhealthy eating and rebuilding a food culture rooted in health, science, transparency, and personal responsibility.&lt;/p&gt;&lt;head rend="h2"&gt;Key&lt;lb/&gt;Guidance&lt;/head&gt;&lt;head rend="h2"&gt;Resources&lt;/head&gt;&lt;p&gt;Explore the research, recommendations, and implementation guidance that shape the Dietary Guidelines, including the science, the policy guidance, and the everyday serving framework.&lt;/p&gt;Watch the press release&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://realfood.gov"/><published>2026-01-07T17:22:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46529797</id><title>A tab hoarder's journey to sanity</title><updated>2026-01-07T21:39:23.082951+00:00</updated><content>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/borisandcrispin/status/2008709479068794989"/><published>2026-01-07T17:54:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46530448</id><title>NPM to implement staged publishing after turbulent shift off classic tokens</title><updated>2026-01-07T21:39:22.969620+00:00</updated><content/><link href="https://socket.dev/blog/npm-to-implement-staged-publishing"/><published>2026-01-07T18:31:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46531068</id><title>US will ban Wall Street investors from buying single-family homes</title><updated>2026-01-07T21:39:22.571394+00:00</updated><content>&lt;doc fingerprint="fa4140d61fb287a7"&gt;
  &lt;main&gt;
    &lt;p&gt;Jan 7 (Reuters) - U.S. President Donald Trump on Wednesday said his administration is moving to ban Wall Street firms from buying up single-family homes in a bid to reduce home prices, a potential blow for private-equity landlords that also pressured homebuilder shares.&lt;/p&gt;
    &lt;p&gt;In a post on Truth Social, Trump said he was immediately taking steps to implement the ban which he would also call on Congress to codify in law.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;"For a very long time, buying and owning a home was considered the pinnacle of the American Dream," Trump wrote, going on to add that inflation had put that dream out of reach for many Americans.&lt;/p&gt;
    &lt;p&gt;"People live in homes, not corporations," said Trump, who is under growing pressure to address voter anxiety over the cost of living ahead of this year's congressional mid-term elections.&lt;/p&gt;
    &lt;p&gt;A Republican move to target Wall Street landlords would, perversely, align the party with Democrats, who for years have criticized corporate homebuying, claiming it has helped stoke housing costs, and have unsuccessfully pushed bills to crackdown on the trend.&lt;/p&gt;
    &lt;head rend="h2"&gt;WALL STREET BLAMED FOR REDUCED HOUSING SUPPLY&lt;/head&gt;
    &lt;p&gt;Wall Street institutions such as Blackstone (BX.N), American Homes 4 Rent (AMH.N) and Progress Residential have bought thousands of single-family homes since the financial crisis of 2008 led to a wave of home foreclosures.&lt;/p&gt;
    &lt;p&gt;By June 2022, institutional investors owned around 450,000 homes, or about 3%, of all single-family rental homes nationally, according to a 2024 study by the Government Accountability Office (GAO).&lt;/p&gt;
    &lt;p&gt;American Homes 4 Rent (AMH.N) dropped to a near three-year low of $28.84 and was halted for volatility before trading resumed. Its shares were last down nearly 6% at $30.61.&lt;/p&gt;
    &lt;p&gt;Blackstone shares hit a one-month low of $147.52 and were last down about 4.5% at $155.33. The PHLX housing index (.HGX) was down 2.3% on the session, on track for its biggest daily percentage drop since Nov. 17.&lt;/p&gt;
    &lt;p&gt;Blackstone, American Homes 4 Rent and Progress Residential did not immediately respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;Wall Street landlords dispute that their investments have stoked inflation. In a January 2025 research note, Blackstone said institutional home purchases have declined 90% since 2022 and that supply shortage is the reason for house price increases.&lt;/p&gt;
    &lt;p&gt;The GAO study found that the effect of institutional homebuying on homeownership opportunities was unclear in part due to limited data.&lt;/p&gt;
    &lt;p&gt;Critics say Wall Street firms are also bad landlords, skimping on upkeep in order to keep investors happy, and wrongly evicted tenants during the COVID-19 pandemic.&lt;/p&gt;
    &lt;p&gt;"Resident experience is hurting as a result," said Jeff Holzmann, COO of RREAF Holdings, a Dallas-based real estate investment firm with over $5 billion in assets.&lt;/p&gt;
    &lt;p&gt;"Instead of you calling your landlord to discuss a problem, you're calling a call center that gives you the runaround."&lt;/p&gt;
    &lt;head rend="h2"&gt;AFFORDABILITY PRESSURE&lt;/head&gt;
    &lt;p&gt;Trump, who has occasionally dismissed affordability concerns and blamed inflation on his Democratic predecessor, has seen his own public approval mostly sag since his inauguration as Americans worry about the economy.&lt;/p&gt;
    &lt;p&gt;It was not immediately clear what authority Trump would draw upon to impose an immediate ban, and he did not outline the changes he was seeking from Congress.&lt;/p&gt;
    &lt;p&gt;The White House did not respond to a request for comment. The U.S. president was due to sign unspecified executive orders later on Wednesday.&lt;/p&gt;
    &lt;p&gt;Since Trump's first electoral victory, U.S. home prices have risen 75%, more than double the increase in overall consumer prices tracked by CPI. But home sales price increases have eased substantially over the past year.&lt;/p&gt;
    &lt;p&gt;The Federal Housing Finance Agency last week reported that national home sales prices had risen just 1.7% in October from a year earlier - the lowest in more than 13 years. That's less than half the rate by which they were climbing when Trump came back into office last January and a fraction of their peak gains of nearly 20% in 2021 and 2022.&lt;/p&gt;
    &lt;p&gt;A big factor in home price inflation has been a dearth of supply of properties for sale, although that has also been slowly improving in the last year or so, according to National Association of Realtors data.&lt;/p&gt;
    &lt;p&gt;As of November, annual shelter-cost inflation, which had shot to as high as 8.2% in the COVID-19 pandemic aftermath, had also eased to 3.0%, the lowest in more than four years, according to the Labor Department's Consumer Price Index.&lt;/p&gt;
    &lt;p&gt;Reporting by Ryan Patrick Jones in Toronto and Trevor Hunnicutt in Washington; additional reporting by Saeed Azhar, Chuck Mikolajczak, Andrea Shalal, Matt Tracy and Dan Burns; Writing by Michelle Price; Editing by Caitlin Webber, David Ljunggren, Cynthia Osterman, Rod Nickel and Nick Zieminski&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.reuters.com/world/us/us-will-ban-large-institutional-investors-buying-single-family-homes-trump-says-2026-01-07/"/><published>2026-01-07T19:13:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46531280</id><title>ChatGPT Health</title><updated>2026-01-07T21:39:22.378757+00:00</updated><content>&lt;doc fingerprint="2c12c8739eff67b7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing ChatGPT Health&lt;/head&gt;
    &lt;p&gt;A dedicated experience in ChatGPT designed for health and wellness.&lt;/p&gt;
    &lt;p&gt;We’re introducing ChatGPT Health, a dedicated experience that securely brings your health information and ChatGPT’s intelligence together, to help you feel more informed, prepared, and confident navigating your health.&lt;/p&gt;
    &lt;p&gt;Health is already one of the most common ways people use ChatGPT, with hundreds of millions of people asking health and wellness questions each week. ChatGPT Health builds on the strong privacy, security, and data controls across ChatGPT with additional, layered protections designed specifically for health— including purpose-built encryption and isolation to keep health conversations protected and compartmentalized. You can securely connect medical records and wellness apps to ground conversations in your own health information, so responses are more relevant and useful to you. Designed in close collaboration with physicians, ChatGPT Health helps people take a more active role in understanding and managing their health and wellness—while supporting, not replacing, care from clinicians.&lt;/p&gt;
    &lt;p&gt;Today, health information is often scattered across portals, apps, wearables, PDFs, and medical notes—so it's hard to see the full picture, and people are left to navigate a complex healthcare system on their own. People have shared countless stories of turning to ChatGPT to help make sense of it all. In fact, health is one of the most common ways people use ChatGPT today: based on our de-identified analysis of conversations, over 230 million people globally ask health and wellness related questions on ChatGPT every week.&lt;/p&gt;
    &lt;p&gt;ChatGPT Health builds on this so responses are informed by your health information and context. You can now securely connect medical records and wellness apps—like Apple Health, Function, and MyFitnessPal—so ChatGPT can help you understand recent test results, prepare for appointments with your doctor, get advice on how to approach your diet and workout routine, or understand the tradeoffs of different insurance options based on your healthcare patterns.&lt;/p&gt;
    &lt;p&gt;Health is designed to support, not replace, medical care. It is not intended for diagnosis or treatment. Instead, it helps you navigate everyday questions and understand patterns over time—not just moments of illness—so you can feel more informed and prepared for important medical conversations. To keep your health information protected and secure, Health operates as a separate space with enhanced privacy to protect sensitive data. Conversations in Health are not used to train our foundation models. If you start a health-related conversation in ChatGPT, we’ll suggest moving into Health for these additional protections.&lt;/p&gt;
    &lt;p&gt;If you’re interested in getting access as it becomes available, you can sign up for the waitlist(opens in a new window). We’re starting by providing access to a small group of early users to learn and continue refining the experience—users with ChatGPT Free, Go, Plus, and Pro plans outside of the European Economic Area, Switzerland, and the United Kingdom are eligible. As we make improvements, we plan to expand access and make Health available to all users on web and iOS in the coming weeks.&lt;/p&gt;
    &lt;p&gt;Medical record integrations and some apps are available in the U.S. only, and connecting Apple Health requires iOS.&lt;/p&gt;
    &lt;p&gt;Your health information is deeply personal. That’s why Health is built as a dedicated space with added protections for sensitive health information and easy-to-use controls.&lt;/p&gt;
    &lt;p&gt;Health lives in its own space within ChatGPT, where your conversations, connected apps, and files are stored separately from your other chats. Health has separate memories, ensuring that your health context stays contained within the space. You’ll still see health chats in your chat history so you can easily return to them, but the information itself stays within Health.&lt;/p&gt;
    &lt;p&gt;When helpful, ChatGPT may use context from your non-Health chats—like a recent move or lifestyle change—to make a health conversation more relevant. However, Health information and memories never flow back into your non-Health chats, and conversations outside of Health can’t access files, conversations, or memories created within Health. You can view or delete Health memories at any time within Health or the “Personalization” section of Settings.&lt;/p&gt;
    &lt;p&gt;We recognize that people share personal and sensitive information with ChatGPT. That understanding shapes how we design the security, privacy, and data controls for all of our products—from the start. Even before introducing ChatGPT Health, we built foundational protections across ChatGPT to give you meaningful control over your data, including temporary chats, the ability to delete chats from OpenAI’s systems within 30 days, and training our models not to retain personal information from user chats.&lt;/p&gt;
    &lt;p&gt;Conversations and files across ChatGPT are encrypted by default at rest and in transit as part of our core security architecture. Due to the sensitive nature of health data, Health builds on this foundation with additional, layered protections—including purpose-built encryption and isolation—to keep health conversations protected and compartmentalized. Conversations in Health are not used to train our foundation models.&lt;/p&gt;
    &lt;p&gt;You can further strengthen access controls by enabling multi-factor authentication (MFA)(opens in a new window), which adds an extra layer of protection to help prevent unauthorized access.&lt;/p&gt;
    &lt;p&gt;When you choose to connect your health data, such as medical records or wellness apps, your responses are grounded in your own health information. To enable access to trusted U.S. healthcare providers, we partner with b.well, the largest and most secure network of live, connected health data for U.S. consumers. b.well adheres to the highest industry standards in data security and privacy. You can remove access to medical records at any time in the "Apps" section of Settings.&lt;/p&gt;
    &lt;p&gt;You can also connect your Apple Health information and other wellness apps, such as Function and MyFitnessPal. Apps may only be connected to your health data with your explicit permission, even if they’re already connected to ChatGPT for conversations outside of Health. All apps available in Health must meet OpenAI’s privacy and security requirements, including collecting only the minimum data needed, and undergo additional security review specific to inclusion in Health. The first time you connect an app, we’ll help you understand what types of data may be collected by the third party. And you’re always in control: disconnect an app at any time and it immediately loses access.&lt;/p&gt;
    &lt;p&gt;ChatGPT Health was developed in close collaboration with physicians around the world to provide clear and useful health information.&lt;/p&gt;
    &lt;p&gt;Over two years, we’ve worked with more than 260 physicians who have practiced in 60 countries and dozens of specialties to understand what makes an answer to a health question helpful or potentially harmful—this group has now provided feedback on model outputs over 600,000 times across 30 areas of focus. This collaboration has shaped not just what Health can do, but how it responds: how urgently to encourage follow-ups with a clinician, how to communicate clearly without oversimplifying, and how to prioritize safety in moments that matter.&lt;/p&gt;
    &lt;p&gt;This physician-led approach is built directly into the model that powers Health, which is evaluated against clinical standards using HealthBench, an assessment framework we created with input from our network of practicing physicians. Rather than relying on exam-style questions or generic accuracy checks, HealthBench evaluates responses using physician-written rubrics that reflect how clinicians judge quality in practice—prioritizing safety, clarity, appropriate escalation of care, and respect for individual context.&lt;/p&gt;
    &lt;p&gt;This evaluation-driven approach helps ensure the model performs well on the tasks people actually need help with, including explaining lab results in accessible language, preparing questions for an appointment, interpreting data from wearables and wellness apps, and summarizing care instructions. The result is support that people can trust—always designed to support, not replace, your healthcare providers.&lt;/p&gt;
    &lt;p&gt;You can sign up for the waitlist(opens in a new window) to request access.&lt;/p&gt;
    &lt;p&gt;Select ‘Health’ from the sidebar menu in ChatGPT.&lt;/p&gt;
    &lt;p&gt;Bring your medical records and the apps you use to track your health and wellness into Health. You can upload files directly, connect from tools (+) or “Apps” in Settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New: Medical Records for lab results, visit summaries, and clinical history&lt;/item&gt;
      &lt;item&gt;New: Apple Health for health and fitness data, including movement, sleep, and activity patterns (must be on iOS to sync)&lt;/item&gt;
      &lt;item&gt;New: Function for lab test insights, nutrition ideas, and taking action on your health&lt;/item&gt;
      &lt;item&gt;New: MyFitnessPal for nutrition advice, macros, and recipes&lt;/item&gt;
      &lt;item&gt;New: Weight Watchers for GLP-1 personalized meal ideas, recipes, and food guidance&lt;/item&gt;
      &lt;item&gt;AllTrails to help you find your next hike&lt;/item&gt;
      &lt;item&gt;Instacart to turn meal plans into shoppable lists&lt;/item&gt;
      &lt;item&gt;Peloton for suggested workout classes or guided meditations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Health conversations feel just like chatting with ChatGPT—but grounded in the information you’ve connected. You can upload photos and files and use search, deep research, voice mode and dictation. When relevant, ChatGPT can automatically reference your connected information to provide more relevant and personalized responses. For example, you might ask: “How’s my cholesterol trending?” or “Can you summarize my latest bloodwork before my appointment?” To use a connected app you can start your question with it, select it from tools (+) or ChatGPT may suggest one when helpful.&lt;/p&gt;
    &lt;p&gt;You can add custom instructions in Health to help ChatGPT know what to focus on, to avoid mentioning sensitive topics, or change how responses are framed. These instructions only apply to Health chats, and you can update or remove any time in Health or Settings.&lt;/p&gt;
    &lt;p&gt;We’ll continue to expand what you can connect and the insights Health can support—so ChatGPT can help you feel more informed, prepared, and confident as you navigate your health.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-chatgpt-health/"/><published>2026-01-07T19:29:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46531295</id><title>Show HN: A to Z – A word game I built from a childhood road trip memory</title><updated>2026-01-07T21:39:21.894776+00:00</updated><content>&lt;doc fingerprint="67345a7a80f541c"&gt;
  &lt;main&gt;
    &lt;p&gt;a b c d e f g h i j k l m n o p q r s t u v w x y b c d e f g h i j k l m n o p q r s t u v w x y z&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://a26z.fun/"/><published>2026-01-07T19:31:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46531511</id><title>Notebook Lawyer</title><updated>2026-01-07T21:39:21.190091+00:00</updated><content>&lt;doc fingerprint="fb58811d1374f4a0"&gt;
  &lt;main&gt;
    &lt;p&gt;When USV commits to investing in a startup, we negotiate a term sheet and then hand over the details to our lawyers. The startup hires a lawyer, and we hire a lawyer. The startup's lawyer prepares the closing documents, and our lawyer reviews them. In addition, our lawyer conducts "legal due diligence," which primarily involves reviewing existing contracts, stock issuances, the charter, and other relevant legal documents.&lt;/p&gt;
    &lt;p&gt;This process is expensive and made worse because the startup typically pays for both lawyers.&lt;/p&gt;
    &lt;p&gt;This is how it has always been done since I started in the business in the mid-80s, and I have always been uncomfortable with how expensive it is.&lt;/p&gt;
    &lt;p&gt;So I decided to run an experiment over the holidays.&lt;/p&gt;
    &lt;p&gt;We committed to lead a round of financing for a company in mid-December. We negotiated a term sheet and signed it before everyone departed for the holidays. I reached out to a law firm that I have used many times for this sort of thing and asked for a quote to handle our side of the deal. The quote came back at $50k.&lt;/p&gt;
    &lt;p&gt;So I said, "screw it" and decided it was time to try something different.&lt;/p&gt;
    &lt;p&gt;I fired up Google's NotebookLM, which allows users to create "notebooks," which are a large collection of documents that can then be used to run AI queries.&lt;/p&gt;
    &lt;p&gt;I put a large collection of "closing binders" of investments USV has made over the years, particularly companies I worked on, into one Notebook. I added the signed term sheet to this Notebook.&lt;/p&gt;
    &lt;p&gt;To create a second Notebook, I pointed NotebookLM at the data room that the startup we are investing in provided for legal diligence. That data room had every legal document the startup had entered into, including those with its employees, since it got started.&lt;/p&gt;
    &lt;p&gt;When we received the draft closing documents from the startup's lawyer, I added them to the first Notebook and asked for a legal review of the draft documents against the body of legal documents we have signed over the years, and most importantly, against the term sheet we had signed. I asked for a memo that outlined all of the issues with the draft documents and highlighted the most significant ones.&lt;/p&gt;
    &lt;p&gt;I then turned to the second Notebook and asked a series of questions like "tell me about the structure of the company and its subsidiaries and who is on the board of each of them" and "give me a list of every employee, the stock they have been issued, and all of the agreements they have signed" and "are their arbitration clauses in every agreement the company has signed?" I spent about half an hour asking these sorts of questions and put the answers to each into a Google Doc.&lt;/p&gt;
    &lt;p&gt;When USV commits to investing in a startup, we negotiate a term sheet and then hand over the details to our lawyers. The startup hires a lawyer, and we hire a lawyer. The startup's lawyer prepares the closing documents, and our lawyer reviews them. In addition, our lawyer conducts "legal due diligence," which primarily involves reviewing existing contracts, stock issuances, the charter, and other relevant legal documents.&lt;/p&gt;
    &lt;p&gt;This process is expensive and made worse because the startup typically pays for both lawyers.&lt;/p&gt;
    &lt;p&gt;This is how it has always been done since I started in the business in the mid-80s, and I have always been uncomfortable with how expensive it is.&lt;/p&gt;
    &lt;p&gt;So I decided to run an experiment over the holidays.&lt;/p&gt;
    &lt;p&gt;We committed to lead a round of financing for a company in mid-December. We negotiated a term sheet and signed it before everyone departed for the holidays. I reached out to a law firm that I have used many times for this sort of thing and asked for a quote to handle our side of the deal. The quote came back at $50k.&lt;/p&gt;
    &lt;p&gt;So I said, "screw it" and decided it was time to try something different.&lt;/p&gt;
    &lt;p&gt;I fired up Google's NotebookLM, which allows users to create "notebooks," which are a large collection of documents that can then be used to run AI queries.&lt;/p&gt;
    &lt;p&gt;I put a large collection of "closing binders" of investments USV has made over the years, particularly companies I worked on, into one Notebook. I added the signed term sheet to this Notebook.&lt;/p&gt;
    &lt;p&gt;To create a second Notebook, I pointed NotebookLM at the data room that the startup we are investing in provided for legal diligence. That data room had every legal document the startup had entered into, including those with its employees, since it got started.&lt;/p&gt;
    &lt;p&gt;When we received the draft closing documents from the startup's lawyer, I added them to the first Notebook and asked for a legal review of the draft documents against the body of legal documents we have signed over the years, and most importantly, against the term sheet we had signed. I asked for a memo that outlined all of the issues with the draft documents and highlighted the most significant ones.&lt;/p&gt;
    &lt;p&gt;I then turned to the second Notebook and asked a series of questions like "tell me about the structure of the company and its subsidiaries and who is on the board of each of them" and "give me a list of every employee, the stock they have been issued, and all of the agreements they have signed" and "are their arbitration clauses in every agreement the company has signed?" I spent about half an hour asking these sorts of questions and put the answers to each into a Google Doc.&lt;/p&gt;
    &lt;p&gt;There is one issue that came out of all of this legal work that I need to understand better and possibly change in the documents. I scheduled a call with the company and its lawyer to go over that. Otherwise, I came away from this process confident that the company's legal affairs are in good shape and the closing docs reflect the term sheet we agreed to and mirror the customary provisions and protections USV receives in investments we make.&lt;/p&gt;
    &lt;p&gt;While this did take about two hours of my time, we did not incur any legal fees. NotebookLM is either free to use, or comes with USV's Google Workspace subscription. I honestly don't know the answer to that.&lt;/p&gt;
    &lt;p&gt;There is one more thing we can do in the VC industry to make this process even better. We can all agree to use standard docs like the NVCA documents that are publicly available to use.&lt;/p&gt;
    &lt;p&gt;With standard documents and Notebook Lawyer, prediction number four in my 2026 predictions can easily come true.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;4/ A majority of venture capital deals close without lawyers on either side due to standardized documents (like the NVCA ones) and AI tools for review and legal diligence.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;All we need is startup founders to demand this. And VCs to have the willingness to say yes.&lt;/p&gt;
    &lt;p&gt;This VC has already done that.&lt;/p&gt;
    &lt;p&gt;There is one issue that came out of all of this legal work that I need to understand better and possibly change in the documents. I scheduled a call with the company and its lawyer to go over that. Otherwise, I came away from this process confident that the company's legal affairs are in good shape and the closing docs reflect the term sheet we agreed to and mirror the customary provisions and protections USV receives in investments we make.&lt;/p&gt;
    &lt;p&gt;While this did take about two hours of my time, we did not incur any legal fees. NotebookLM is either free to use, or comes with USV's Google Workspace subscription. I honestly don't know the answer to that.&lt;/p&gt;
    &lt;p&gt;There is one more thing we can do in the VC industry to make this process even better. We can all agree to use standard docs like the NVCA documents that are publicly available to use.&lt;/p&gt;
    &lt;p&gt;With standard documents and Notebook Lawyer, prediction number four in my 2026 predictions can easily come true.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;4/ A majority of venture capital deals close without lawyers on either side due to standardized documents (like the NVCA ones) and AI tools for review and legal diligence.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;All we need is startup founders to demand this. And VCs to have the willingness to say yes.&lt;/p&gt;
    &lt;p&gt;This VC has already done that.&lt;/p&gt;
    &lt;p&gt;Share Dialog&lt;/p&gt;
    &lt;p&gt;AVC&lt;/p&gt;
    &lt;p&gt;Share Dialog&lt;/p&gt;
    &lt;p&gt;AVC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://avc.xyz/notebook-lawyer"/><published>2026-01-07T19:45:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46531565</id><title>Notion AI: Unpatched data exfiltration</title><updated>2026-01-07T21:39:20.892725+00:00</updated><content>&lt;doc fingerprint="98559bad08f84487"&gt;
  &lt;main&gt;
    &lt;p&gt;Threat Intelligence&lt;/p&gt;
    &lt;head rend="h1"&gt;Notion AI: Unpatched Data Exfiltration&lt;/head&gt;
    &lt;p&gt;Notion AI is susceptible to data exfiltration via indirect prompt injection due to a vulnerability in which AI document edits are saved before user approval.&lt;/p&gt;
    &lt;p&gt;Notion AI allows users to interact with their documents using natural languageâ¦ but what happens when AI edits are made prior to user approval?&lt;lb/&gt;In this article, we document a vulnerability that leads Notion AI to exfiltrate user data (a sensitive hiring tracker document) via indirect prompt injection. Users are warned about an untrusted URL and asked for approval to interact with it - but their data is exfiltrated before they even respond.&lt;/p&gt;
    &lt;p&gt;We responsibly disclosed this vulnerability to Notion via HackerOne. Unfortunately, they said âwe're closing this finding as `Not Applicable`â.&lt;/p&gt;
    &lt;head rend="h3"&gt;Stealing Hiring Tracker Data with a Poisoned Resume&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;The user uploads a resume (untrusted data) to their chat session.&lt;/p&gt;&lt;lb/&gt;Here, the untrusted data source is a resume PDF, but a prompt injection could be stored in a web page, connected data source, or a Notion page.&lt;p&gt;This document contains a prompt injection hidden in 1 point font white on white text with a square white image covering the text for good measure. The LLM can read it with no issues, but the document appears benign to the human eye.&lt;/p&gt;&lt;lb/&gt;A Note on Defenses: Notion AI uses an LLM to scan document uploads and present a warning if a document is flagged as malicious. As this warning is triggered by an LLM, it can be bypassed by a prompt injection that convinces the evaluating model that the document is safe. For this research, we did not focus on bypassing this warning because the point of the attack is the exfiltration mechanism, not the method of injection delivery. In practice, an injection could easily be stored in a source that does not appear to be scanned, such as a web page, Notion page, or connected data source like Notion Mail.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The user asks Notion AI for help updating a hiring tracker based on the resume.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Notion AI is manipulated by the prompt injection to insert a malicious image into the hiring tracker.&lt;/p&gt;&lt;lb/&gt;The prompt injection manipulates Notion AI to (1) construct a URL by collecting all of the text in the document and appending the data to an attacker-controlled domain, and (2) insert an âimageâ into the Notion Page using the constructed URL as the image source.&lt;p&gt;Here, it appears as though the user is prompted for approval. However, unbeknownst to the user, the edit has already occurred before the user is prompted for approval. When the edit occurred, the userâs browser made a request to the attackerâs server, attempting to retrieve the image. This request exfiltrates the document contents contained in the URL constructed by Notion AI.&lt;/p&gt;&lt;lb/&gt;Whether or not the user accepts the edit, the attacker successfully exfiltrates the data.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The attacker reads the sensitive hiring tracker data from their server logs.&lt;/p&gt;&lt;lb/&gt;Once the userâs browser has made a request for the malicious image, the attacker can read the sensitive data contained in the URL from their request logs.&lt;p&gt;In this attack, exfiltrated data included salary expectations, candidate feedback, internal role details, and other sensitive information such as diversity hiring goals.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Additional Attack Surface&lt;/head&gt;
    &lt;p&gt;The Notion Mail AI drafting assistant is susceptible to rendering insecure Markdown images within email drafts, resulting in data exfiltration. If a user mentions an untrusted resource while drafting, content from the userâs query or other mentioned resources can be exfiltrated. E.g., âHey, draft me an email based on @untrusted_notion_page and @trusted_notion_pageâ.&lt;lb/&gt;The attack surface is reduced for Notion Mailâs drafting assistant as the system appears to only have access to data sources within the Notion ecosystem that are explicitly mentioned by the user (as opposed to Notion AIâs main offering, which supports web search, document upload, integrations, etc.).&lt;/p&gt;
    &lt;head rend="h3"&gt;Recommended Remediations for Organizations:&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Institute a vetting process for connected data sources. Restrict use of connectors that can access highly sensitive or highly untrusted data from: Settings &amp;gt; Notion AI &amp;gt; Connectors.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To reduce the risk of untrusted data being processed in the workspace, admins can configure: Settings &amp;gt; Notion AI &amp;gt; AI Web Search &amp;gt; Enable web search for workspace &amp;gt; Off.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Individual users should avoid including sensitive personal data that could be leveraged in a spearphishing attack when configuring personalization for Notion AI via: Settings &amp;gt; Notion AI &amp;gt; Personalization.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Individual users can configure: Settings &amp;gt; Notion AI &amp;gt; AI Web Search &amp;gt; Require confirmation for web requests &amp;gt; On.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Implementing these remediations will reduce the risk surface, but will not nullify the core vulnerability.&lt;/p&gt;
    &lt;head rend="h3"&gt;Recommended Remediations for Notion:&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Programmatically prohibit automatic rendering of Markdown images from external sites in Notion AI page creation or update outputs without explicit user approval.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Programmatically prohibit automatic rendering of Markdown images from external sites in Notion AI mail drafts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Implement a strong Content Security Policy. This will prevent network requests from being made to unapproved external domains.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ensure the CDN used to retrieve images for display in Notion and image previews for display in Notion Mail cannot be used as an open redirect to bypass the CSP policy that is set.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Responsible Disclosure Timeline&lt;/head&gt;
    &lt;p&gt;12/24/2025 Initial report made via HackerOne&lt;lb/&gt;12/24/2025 Report is acknowledged, altered write-up requested&lt;lb/&gt;12/24/2025 PromptArmor follows up with the requested format&lt;lb/&gt;12/29/2025 Report closed as non-applicable&lt;lb/&gt;01/07/2025 Public disclosure&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.promptarmor.com/resources/notion-ai-unpatched-data-exfiltration"/><published>2026-01-07T19:49:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46531794</id><title>Claude Code Emergent Behavior: When Skills Combine</title><updated>2026-01-07T21:39:20.754388+00:00</updated><content>&lt;doc fingerprint="3d440486fbd7457c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Emergent Behavior: When Skills Combine&lt;/head&gt;
    &lt;p&gt;I ran across something interesting today. Claude can combine skills! You might be thinking, "huh?", here's what I mean.&lt;/p&gt;
    &lt;p&gt;I've been vibe coding like a fiend lately, experimenting and learning the flow. Working on ways to make AI more successful. First thing I built? A tiling window manager for macOS. Because that's obviously the first thing to build when your goal is world domination.&lt;/p&gt;
    &lt;p&gt;For something I didn't write, I was happy with the performance. Not winning any awards, but feels responsive to me. Still, I knew there had to be plenty of room for improvement in that hellscape of a repo.&lt;/p&gt;
    &lt;p&gt; With this in mind, Claude and I colluded on a new &lt;code&gt;optimize-critical-path&lt;/code&gt; skill. You can guess what it does. First path? &lt;code&gt;thegrid focus next&lt;/code&gt; command. I kicked off the agent and told it to look at this action and do its thing.
          &lt;/p&gt;
    &lt;p&gt;Claude was all too happy to use this new skill. It started kicking off subagents and tokens were flying across my screen like a scene from a worse, more dystopian version of the Matrix. I noticed it was thrashing and making assumptions about performance. I heard a voice in my head that sounded like Obi-Wan, telling me that we should always measure first.&lt;/p&gt;
    &lt;p&gt;Then I got an idea. Previously I hacked together a debug skill that adds logs and helps trace issues. I thought to myself, can I combine skills?&lt;/p&gt;
    &lt;code&gt;&amp;gt; what is another way we can approach this. Could we combine optimize critical path skill with my oberdebug?

â Good idea - let's use oberdebug's hypothesis-driven approach to find the actual bottleneck:

/oberdebug

â Combining the two approaches. Following oberdebug's structure:

Step 1: Issue Statement

Symptom: Focus operation takes ~130ms, target is &amp;lt;80ms
Component: IPC path (CLI -&amp;gt; Server -&amp;gt; Response)
Evidence so far:
- Border pooling works (100% hit rate) â
- dump alone takes ~82ms (~63ms overhead beyond baseline IPC)
- JSON serialization is only ~1ms - NOT the bottleneck
- Something else in the IPC round-trip is slow

Step 2: Reproduction

YES - Easily reproducible with `thegrid focus left`

Step 3: Dispatch Discovery Agents

â 3 Explore agents launched (ctrl+o to expand)
ââ Trace IPC timing path Â· Running in background
â  ââ Launched
ââ Check git history for IPC Â· Running in background
â  ââ Launched
ââ Find timing instrumentation Â· Running in background
   ââ Launched

â Bash(rm -f /Users/r/.config/bin/claude-debug-bp7x.py 2&amp;gt;/dev/null; echo "cleaned up")
ââ cleaned up

â Let me get the agent results:

Task Output af35ff&lt;/code&gt;
    &lt;p&gt;Ok, now we're talking. Burn those sweet sweet tokens optimizing that path! I know this is only the diagnosis part. It was evident my original skill was really good at coming up with solutions to optimize, but I messed up and it wasn't good at finding what to optimize.&lt;/p&gt;
    &lt;p&gt; Thankfully &lt;code&gt;oberdebug&lt;/code&gt; was built to do this. It uses the lord's method of solving complex bugs. Add a log statement.
          &lt;/p&gt;
    &lt;code&gt;142        state.serverCommit = GridServerCommit

143
144 +      let t4 = CFAbsoluteTimeGetCurrent()
145        let response = Response(
146            id: request.id,
147            result: AnyCodable(state)
148        )
149 +      let t5 = CFAbsoluteTimeGetCurrent()
150 +
151 +      // [OBERDEBUG-001] Timing breakdown for dump operation
152 +      JSONLogger.shared.log("dbg.dump.timing", data: [
153 +          "task_spawn_ms": Int((t1 - t0) * 1000),
154 +          "state_fetch_ms": Int((t3 - t2) * 1000),
155 +          "anycod_wrap_ms": Int((t5 - t4) * 1000),
156 +          "total_ms": Int((t5 - t0) * 1000)
157 +      ])
158 +
159        completion(response)
160    } catch {
161        JSONLogger.shared.log("err.state", data: ["op": "dump", "error": "\(error)"])

Now rebuild and test:

Bash(make run 2&amp;gt;&amp;amp;1 | tail -20)
ââ Building grid-cli...
   Building thegrid v0.2.7...
   go build -v -ldflags "-X main.Version=0.2.7 -X main.Commit=2f0eff915d96b4f66411da07a8b8939543a5b693"
   â¦ +17 lines (ctrl+o to expand)&lt;/code&gt;
    &lt;p&gt;Since this app has a CLI, I take my hands off the wheel and smile as Claude runs a mini performance test using the CLI. I just see some AI randomly changing focus on my monitor. I'm trying to think, how does it come up with the order? Anyway, topic for another day.&lt;/p&gt;
    &lt;p&gt;YEAHHHH BUDDY&lt;/p&gt;
    &lt;code&gt;â FOUND IT!

â Step 7: Evidence Analysis

| Hypothesis                    | Evidence                                 | Verdict    |
|------------------------------|-------------------------------------------|------------|
| Task spawning (~30â40ms)     | task_spawn_us: 17â46 (0.05ms)              | REFUTED    |
| AnyCodable wrap (~20â30ms)   | anycod_wrap_us: 2â5 (0.005ms)              | REFUTED    |
| Response JSON encoding       | encode_us: 54,720â60,198 (55â60ms)         | CONFIRMED  |

---

Step 8: ROOT CAUSE CONFIRMED

ROOT CAUSE CONFIRMED:
ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ

Issue:      dump IPC takes ~82ms, target is &amp;lt;20ms
Location:   grid-server/Sources/GridServer/SocketServer.swift:238
Evidence:   encode_us: 54,720â60,198Âµs (55â60ms)
            bytes: 122,699 (120KB payload)

Cause:      Swift's JSONEncoder.encode() serializing the full
            Response struct (containing WindowManagerState with
            ~248 windows) takes 55â60ms per call.

ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ

Time Breakdown (verified)

| Phase                           | Time     | Notes        |
|---------------------------------|----------|--------------|
| Task spawn                      | 0.05ms   | Negligible   |
| StateManager.getStateDictionary() | 0.005ms | Negligible   |&lt;/code&gt;
    &lt;p&gt;So that was nice! The optimize skill then kicked in and it proceeded to recommend different things, and I had some of my own ideas as well. But the moral of the story is combining skills. Like any human with a shiny new hammer, I needed to go find some new nails!&lt;/p&gt;
    &lt;p&gt; With that in mind, I thought about &lt;code&gt;brainstorming&lt;/code&gt;, which I use a lot, and I'd recently been playing around with &lt;code&gt;frontend-design&lt;/code&gt;. Those two seemed to pair very nicely. This was the original prompt that created this blog.
          &lt;/p&gt;
    &lt;code&gt;â That's a creative approach! You want the design expertise baked into the brainstorming itself rather than sequential steps.

We can try it â invoke frontend-design and prompt it to run the brainstorming workflow with design sensibilities guiding the exploration. That way design thinking shapes the questions and options from the start, rather than being applied after decisions are made.

Let's give it a shot:

â Skill(frontend-design:frontend-design)

â Now let me layer in the brainstorming skill to combine both:

â Skill(superpowers:brainstorming)

â Now we have both active â design sensibility guiding the brainstorm.

Since this is a fresh project (no existing codebase to explore), let's dive into understanding your vision. I'll ask one question at a time, design-informed.

---

First question: What's the blog's core purpose?

This shapes everything â the aesthetic, the features, the tone.

| Option                 | Description                                                                                                  |
|------------------------|--------------------------------------------------------------------------------------------------------------|
| A. Personal/Creative   | Your thoughts, essays, creative writing. The blog is the experience â personality-forward, expressive design matters. |
| B. Technical/Dev       | Tutorials, code snippets, engineering posts. Readability and code presentation are paramount.              |
| C. Portfolio/Professional | Showcasing work, building reputation. Polished, memorable, makes an impression.                        |
| D. Publication/Magazine| Multi-author or editorial style. Content density, navigation, discoverability.                             |
| E. Something else      | Tell me more.                                                                                                |

Which resonates most?

&amp;gt; A

â Personal/Creative â where the blog is the experience. This opens up the full design spectrum.&lt;/code&gt;
    &lt;p&gt; As I was working on the blog, I came across another opportunity to pair skills. I combine &lt;code&gt;code-review&lt;/code&gt; skill with everything now. I'll even kick off parallel subagents using two different &lt;code&gt;code-review&lt;/code&gt; skills combined with other skills. Doing something like &lt;code&gt;code-review&lt;/code&gt; through a &lt;code&gt;frontend-design&lt;/code&gt; lens has been extremely helpful as I built this blog.
          &lt;/p&gt;
    &lt;p&gt;I'm curious what other new combinations are out there to discover.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vibeandscribe.xyz/posts/2025-01-07-emergent-behavior.html"/><published>2026-01-07T20:06:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46531925</id><title>Tailscale state file encryption no longer enabled by default</title><updated>2026-01-07T21:39:20.396028+00:00</updated><content>&lt;doc fingerprint="868f307d7f22bbd0"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;Tailscale v1.92.5&lt;/head&gt;Update instructions&lt;head rend="h5"&gt;Linux&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;State file encryption and hardware attestation keys are no longer enabled by default.&lt;/item&gt;&lt;item&gt;Failure to load hardware attestation keys no longer prevents the client from starting. This could happen when the TPM device is reset or replaced.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h5"&gt;Windows&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;State file encryption and hardware attestation keys are no longer enabled by default.&lt;/item&gt;&lt;item&gt;Failure to load hardware attestation keys no longer prevents the client from starting. This could happen when the TPM device is reset or replaced.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Tailscale container image v1.92.5&lt;/head&gt;&lt;p&gt;A new release of the Tailscale container image is available. You can download it from Docker Hub or from our GitHub packages repository.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Hardware attestation keys are no longer added to Kubernetes state &lt;code&gt;Secrets&lt;/code&gt;, making it possible to change the Kubernetes node the Tailscale containers are deployed on.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Tailscale Kubernetes Operator v1.92.5&lt;/head&gt;&lt;p&gt;A new release of the Tailscale Kubernetes Operator is available. For guidance on installing and updating, refer to our installation instructions.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Certificate renewal is no longer done as an ARI order by default to avoid renewal failure if ACME account keys are recreated.&lt;/item&gt;&lt;item&gt;Hardware attestation keys are no longer added to Kubernetes state &lt;code&gt;Secrets&lt;/code&gt;, making it possible to change the Kubernetes node the Tailscale Kubernetes Operator is deployed on.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Tailscale tsrecorder v1.92.5&lt;/head&gt;&lt;p&gt;A new release of the Tailscale &lt;code&gt;tsrecorder&lt;/code&gt; is available. You can download it from Docker Hub.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Note: This version contains no changes except for library updates.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tailscale.com/changelog"/><published>2026-01-07T20:16:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46532075</id><title>Claude Code CLI Broken</title><updated>2026-01-07T21:39:19.454970+00:00</updated><content>&lt;doc fingerprint="171657deccbe91fa"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 3.8k&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Closed as duplicate of#16682&lt;/p&gt;
    &lt;p&gt;Labels&lt;/p&gt;
    &lt;p&gt;bugSomething isn't workingSomething isn't workingduplicateThis issue or pull request already existsThis issue or pull request already existshas reproHas detailed reproduction stepsHas detailed reproduction stepsplatform:macosIssue specifically occurs on macOSIssue specifically occurs on macOS&lt;/p&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;Preflight Checklist&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I have searched existing issues and this hasn't been reported yet&lt;/item&gt;
      &lt;item&gt;This is a single bug report (please file separate reports for different bugs)&lt;/item&gt;
      &lt;item&gt;I am using the latest version of Claude Code&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;What's Wrong?&lt;/head&gt;
    &lt;p&gt;update to claude 2.1.0 then run claude. see the error.&lt;/p&gt;
    &lt;head rend="h3"&gt;What Should Happen?&lt;/head&gt;
    &lt;p&gt;claude should start when using version 2.1.0&lt;/p&gt;
    &lt;head rend="h3"&gt;Error Messages/Logs&lt;/head&gt;
    &lt;head rend="h3"&gt;Steps to Reproduce&lt;/head&gt;
    &lt;p&gt;update to 2.1.0 and run claude.&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Model&lt;/head&gt;
    &lt;p&gt;None&lt;/p&gt;
    &lt;head rend="h3"&gt;Is this a regression?&lt;/head&gt;
    &lt;p&gt;No, this never worked&lt;/p&gt;
    &lt;head rend="h3"&gt;Last Working Version&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Code Version&lt;/head&gt;
    &lt;p&gt;2.1.0&lt;/p&gt;
    &lt;head rend="h3"&gt;Platform&lt;/head&gt;
    &lt;p&gt;Anthropic API&lt;/p&gt;
    &lt;head rend="h3"&gt;Operating System&lt;/head&gt;
    &lt;p&gt;macOS&lt;/p&gt;
    &lt;head rend="h3"&gt;Terminal/Shell&lt;/head&gt;
    &lt;p&gt;Other&lt;/p&gt;
    &lt;head rend="h3"&gt;Additional Information&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
    &lt;p&gt;steinnes, Xplod13, BigFoxMedia, ekstro, masonhieb and 161 morebellaj and Nuno111&lt;/p&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h2"&gt;Metadata&lt;/head&gt;
    &lt;head rend="h3"&gt;Assignees&lt;/head&gt;
    &lt;head rend="h3"&gt;Labels&lt;/head&gt;
    &lt;p&gt;bugSomething isn't workingSomething isn't workingduplicateThis issue or pull request already existsThis issue or pull request already existshas reproHas detailed reproduction stepsHas detailed reproduction stepsplatform:macosIssue specifically occurs on macOSIssue specifically occurs on macOS&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/anthropics/claude-code/issues/16673"/><published>2026-01-07T20:25:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46532755</id><title>Show HN: An LLM response cache that's aware of dynamic data</title><updated>2026-01-07T21:39:19.210278+00:00</updated><content/><link href="https://blog.butter.dev/on-automatic-template-induction-for-response-caching"/><published>2026-01-07T21:04:58+00:00</published></entry></feed>