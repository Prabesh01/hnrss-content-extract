<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-19T20:40:53.889385+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46319657</id><title>1.5 TB of VRAM on Mac Studio – RDMA over Thunderbolt 5</title><updated>2025-12-19T20:41:00.836739+00:00</updated><content>&lt;doc fingerprint="ef70d13f3fb601fc"&gt;
  &lt;main&gt;
    &lt;p&gt;Apple gave me access to this Mac Studio cluster to test RDMA over Thunderbolt, a new feature in macOS 26.2. The easiest way to test it is with Exo 1.0, an open source private AI clustering tool. RDMA lets the Macs all act like they have one giant pool of RAM, which speeds up things like massive AI models.&lt;/p&gt;
    &lt;p&gt;The stack of Macs I tested, with 1.5 TB of unified memory, costs just shy of $40,000, and if you're wondering, no I cannot justify spending that much money for this. Apple loaned the Mac Studios for testing. I also have to thank DeskPi for sending over the 4-post mini rack containing the cluster.&lt;/p&gt;
    &lt;p&gt;The last time I remember hearing anything interesting about Apple and HPC (High Performance Computing), was back in the early 2000s, when they still made the Xserve.&lt;/p&gt;
    &lt;p&gt;They had a proprietary clustering solution called Xgrid... that landed with a thud. A few universities built some clusters, but it never really caught on, and now Xserve is a distant memory.&lt;/p&gt;
    &lt;p&gt;I'm not sure if its by accident or Apple's playing the long game, but the M3 Ultra Mac Studio hit a sweet spot for running local AI models. And with RDMA support lowering memory access latency from 300μs down to &amp;lt; 50μs, clustering now adds to the performance, especially running huge models.&lt;/p&gt;
    &lt;p&gt;They also hold their own for creative apps and at least small-scale scientific computing, all while running under 250 watts and almost whisper-quiet.&lt;/p&gt;
    &lt;p&gt;The two Macs on the bottom have 512 GB of unified memory and 32 CPU cores, and cost $11,699 each. The two on top, with half the RAM, are $8,099 each1.&lt;/p&gt;
    &lt;p&gt;They're not cheap.&lt;/p&gt;
    &lt;p&gt;But with Nvidia releasing their DGX Spark and AMD with their AI Max+ 395 systems, both of which have a fourth the memory (128 GB maximum), I thought I'd put this cluster through its paces.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video&lt;/head&gt;
    &lt;p&gt;This blog post is the reformatted text version of my latest YouTube video, which you can watch below.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Mini Mac Rack&lt;/head&gt;
    &lt;p&gt;In a stroke of perfect timing, DeskPi sent over a new 4-post mini rack called the TL1 the day before these Macs showed up.&lt;/p&gt;
    &lt;p&gt;I kicked off Project MINI RACK earlier this year, but the idea is you can have the benefits of rackmount gear, but in a form factor that'll fit on your desk, or tucked away in a corner.&lt;/p&gt;
    &lt;p&gt;Right now, I haven't seen any solutions for mounting Mac Studios in 10" racks besides this 3D printable enclosure, so I just put them on some 10" rack shelves.&lt;/p&gt;
    &lt;p&gt;The most annoying thing about racking any non-Pro Macs is the power button. On a Mac Studio it's located in the back left, on a rounded surface, which means rackmount solutions need to have a way to get to it.&lt;/p&gt;
    &lt;p&gt;The open sides on the mini rack allow me to reach in and press the power button, but I still have to hold onto the Mac Studio while doing so, to prevent it from sliding out the front!&lt;/p&gt;
    &lt;p&gt;It is nice to have the front ports on the Studio to plug in a keyboard and monitor:&lt;/p&gt;
    &lt;p&gt;For power, I'm glad Apple uses an internal power supply. Too many 'small' PCs are small only because they punt the power supply into a giant brick outside the case. Not so, here, but you do have to deal with Apple's non-C13 power cables (which means it's harder to find cables in the perfect length to reduce cabling to be managed).&lt;/p&gt;
    &lt;p&gt;The DGX Spark does better than Apple on networking. They have these big rectangle QSFP ports (pictured above). The plugs hold in better, while still being easy to plug in and pull out.&lt;/p&gt;
    &lt;p&gt;The Mac Studios have 10 Gbps Ethernet, but the high speed networking (something like 50-60 Gbps real-world throughput) on the Macs comes courtesy of Thunderbolt. Even with premium Apple cables costing $70 each, I don't feel like the mess of plugs would hold up for long in many environments.&lt;/p&gt;
    &lt;p&gt;There's tech called ThunderLok-A, which adds a little screw to each cable to hold it in, but I wasn't about to drill out and tap the loaner Mac Studios, to see if I could make them work.&lt;/p&gt;
    &lt;p&gt;Also, AFAICT, Thunderbolt 5 switches don't exist, so you can't plug in multiple Macs to one central switch—you have to plug every Mac into every other Mac, which adds to the cabling mess. Right now, you can only cross-connect up to four Macs, but I think that may not be a hard limit for the current Mac Studio (Apple said all five TB5 ports are RDMA-enabled).&lt;/p&gt;
    &lt;p&gt;The bigger question is: do you need a full cluster of Mac Studios at all? Because just one is already a beast, matching four maxed-out DGX Sparks or AI Max+ 395 systems. Managing clusters can be painful.&lt;/p&gt;
    &lt;head rend="h2"&gt;M3 Ultra Mac Studio - Baseline&lt;/head&gt;
    &lt;p&gt;To inform that decision, I ran some baseline benchmarks, and posted all my results (much more than I highlight in this blog post) to my sbc-reviews project.&lt;/p&gt;
    &lt;p&gt;I'll compare the M3 Ultra Mac Studio to a:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dell Pro Max with GB10 (similar to the Nvidia DGX Spark, but with better thermals)&lt;/item&gt;
      &lt;item&gt;Framework Desktop Mainboard (with AMD's AI Max+ 395 chip)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First, Geekbench. The M3 Ultra, running two-generations-old CPU cores, beats the other two in both single and multi-core performance (and even more handily in Geekbench 5, which is more suitable for CPUs with many cores).&lt;/p&gt;
    &lt;p&gt;Switching over to a double-precision FP64 test, my classic top500 HPL benchmark, the M3 Ultra is the first small desktop I've tested that breaks 1 Tflop FP64. It's almost double Nvidia's GB10, and the AMD AI Max chip is left in the dust.&lt;/p&gt;
    &lt;p&gt;Efficiency on the CPU is also great, though that's been the story with Apple since the A-series, with all their chips. And related to that, idle power draw on here is less than 10 watts:&lt;/p&gt;
    &lt;p&gt;I mean, I've seen SBC's idle over 10 watts, much less something that could be considered a personal supercomputer.&lt;/p&gt;
    &lt;p&gt;Regarding AI Inference, the M3 Ultra stands out, both for small and large models:&lt;/p&gt;
    &lt;p&gt;Of course, the truly massive models (like DeepSeek R1 or Kimi K2 Thinking) won't even run on a single node of the other two systems.&lt;/p&gt;
    &lt;p&gt;But this is a $10,000 system. You expect more when you pay more.&lt;/p&gt;
    &lt;p&gt;But consider this: a single M3 Ultra Mac Studio has more horsepower than my entire Framework Desktop cluster, using half the power. I also compared it to a tiny 2-node cluster of Dell Pro Max with GB10 systems, and a single M3 Ultra still comes ahead in performance and efficiency, with double the memory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mini Stack, Maxi Mac&lt;/head&gt;
    &lt;p&gt;But with four Macs, how's clustering and remote management?&lt;/p&gt;
    &lt;p&gt;The biggest hurdle for me is macOS itself. I automate everything I can on my Macs. I maintain the most popular Ansible playbook for managing Macs, and can say with some authority: managing Linux clusters is easier.&lt;/p&gt;
    &lt;p&gt;Every cluster has hurdles, but there are a bunch of small struggles when managing a cluster of Macs without additional tooling like MDM. For example: did you know there's no way to run a system upgrade (like to 26.2) via SSH? You have to click buttons in the UI.&lt;/p&gt;
    &lt;p&gt;Instead of plugging a KVM into each Mac remotely, I used Screen Sharing (built into macOS) to connect to each Mac and complete certain operations via the GUI.&lt;/p&gt;
    &lt;head rend="h2"&gt;HPL and Llama.cpp&lt;/head&gt;
    &lt;p&gt;With everything set up, I tested HPL over 2.5 Gigabit Ethernet, and llama.cpp over that and Thunderbolt 5.&lt;/p&gt;
    &lt;p&gt;For HPL, I got 1.3 Teraflops with a single M3 Ultra. With all four put together, I got 3.7, which is less than a 3x speedup. But keep in mind, the top two Studios only have half the RAM of the bottom two, so a 3x speedup is probably around what I'd expect.&lt;/p&gt;
    &lt;p&gt;I tried running HPL through Thunderbolt (not using RDMA, just TCP), but after a minute or so, both Macs I had configured in a cluster would crash and reboot. I looked into using Apple's MLX wrapper for &lt;code&gt;mpirun&lt;/code&gt;, but I couldn't get that done in time for this post.&lt;/p&gt;
    &lt;p&gt;Next I tested llama.cpp running AI models over 2.5 gigabit Ethernet versus Thunderbolt 5:&lt;/p&gt;
    &lt;p&gt;Thunderbolt definitely wins for latency, even if you're not using RDMA.&lt;/p&gt;
    &lt;p&gt;All my llama.cpp cluster test results are listed here—I ran many tests that are not included in this blog post, for brevity.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enabling RDMA&lt;/head&gt;
    &lt;p&gt;Exo 1.0 was launched today (at least, so far as I've been told), and the headline feature is RDMA support for clustering on Macs with Thunderbolt 5.&lt;/p&gt;
    &lt;p&gt;To enable RDMA, though, you have to boot into recovery mode and run a command:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Shut down the Mac Studio&lt;/item&gt;
      &lt;item&gt;Hold down the power button for 10 seconds (you'll see a boot menu appear)&lt;/item&gt;
      &lt;item&gt;Go into Options, then when the UI appears, open Terminal from the Utilities menu&lt;/item&gt;
      &lt;item&gt;Type in &lt;code&gt;rdma_ctl enable&lt;/code&gt;, and press enter&lt;/item&gt;
      &lt;item&gt;Reboot the Mac Studio&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once that was done, I ran a bunch of HUGE models, including Kimi K2 Thinking, which at 600+ GB, is too big to run on a single Mac.&lt;/p&gt;
    &lt;p&gt;I can run models like that across multiple Macs using both llama.cpp and Exo, but the latter is so far the only one to support RDMA. Llama.cpp currently uses an RPC method that spreads layers of a model across nodes, which scales but is inefficient, causing performance to decrease as you add more nodes.&lt;/p&gt;
    &lt;p&gt;This benchmark of Qwen3 235B illustrates that well:&lt;/p&gt;
    &lt;p&gt;Exo speeds up as you add more nodes, hitting 32 tokens per second on the full cluster. That's definitely fast enough for vibe coding, if that's your thing, but it's not mine.&lt;/p&gt;
    &lt;p&gt;So I moved on to testing DeepSeek V3.1, a 671 billion parameter model:&lt;/p&gt;
    &lt;p&gt;I was a little surprised to see llama.cpp get a little speedup. Maybe the network overhead isn't so bad running on two nodes? I'm not sure.&lt;/p&gt;
    &lt;p&gt;Let's move to the biggest model I've personally run on anything, Kimi K2 Thinking:&lt;/p&gt;
    &lt;p&gt;This is a 1 trillion parameter model, though there's only 32 billion 'active' at any given time—that's what the A is for in the A32B there.&lt;/p&gt;
    &lt;p&gt;But we're still getting around 30 tokens per second.&lt;/p&gt;
    &lt;p&gt;Working with some of these huge models, I can see how AI has some use, especially if it's under my own local control. But it'll be a long time before I put much trust in what I get out of it—I treat it like I do Wikipedia. Maybe good for a jumping-off point, but don't ever let AI replace your ability to think critically!&lt;/p&gt;
    &lt;p&gt;But this post isn't about the merits of AI, it's about a Mac Studio Cluster, RDMA, and Exo.&lt;/p&gt;
    &lt;p&gt;They performed great... when they performed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stability Issues&lt;/head&gt;
    &lt;p&gt;First a caveat: I was working with prerelease software while testing. A lot of bugs were worked out in the course of testing.&lt;/p&gt;
    &lt;p&gt;But it was obvious RDMA over Thunderbolt is new. When it works, it works great. When it doesn't... well, let's just say I was glad I had Ansible set up so I could shut down and reboot the whole cluster quickly.&lt;/p&gt;
    &lt;p&gt;I also mentioned HPL crashing when I ran it over Thunderbolt. Even if I do get that working, I've only seen clusters of 4 Macs with RDMA (as of late 2025). Apple says all five Thunderbolt 5 ports are enabled for RDMA, though, so maybe more Macs could be added?&lt;/p&gt;
    &lt;p&gt;Besides that, I still have some underlying trust issues with Exo, since the developers went AWOL for a while.&lt;/p&gt;
    &lt;p&gt;They are keeping true to their open source roots, releasing Exo 1.0 under the Apache 2.0 license, but I wish they didn't have to hole up and develop it in secrecy; that's probably a side effect of working so closely with Apple.&lt;/p&gt;
    &lt;p&gt;I mean, it's their right, but as someone who maybe develops too much in the open, I dislike layers of secrecy around any open source project.&lt;/p&gt;
    &lt;p&gt;I am excited to see where it goes next. They teased putting a DGX Spark in front of a Mac Studio cluster to speed up prompt processing... maybe they'll get support re-added for Raspberry Pi's, too? Who knows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unanswered Questions / Topics to Explore Further&lt;/head&gt;
    &lt;p&gt;But I'm left with more questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Where's the M5 Ultra? If Apple released one, it would be a lot faster for machine learning.&lt;/item&gt;
      &lt;item&gt;Could Apple revive the Mac Pro to give me all the PCIe bandwidth I desire for faster clustering, without being held back by Thunderbolt?&lt;/item&gt;
      &lt;item&gt;Could Macs get SMB Direct? Network file shares would behave as if attached directly to the Mac, which'd be amazing for video editing or other latency-sensitive, high-bandwidth applications.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally, what about other software? Llama.cpp and other apps could get a speed boost with RDMA support, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Unlike most AI-related hardware, I'm kinda okay with Apple hyping this up. When the AI bubble goes bust, Mac Studios are still fast, silent, and capable workstations for creative work (I use an M4 Max at my desk!).&lt;/p&gt;
    &lt;p&gt;But it's not all rainbows and sunshine in Apple-land. Besides being more of a headache to manage Mac clusters, Thunderbolt 5 holds these things back from their true potential. QSFP would be better, but it would make the machine less relevant for people who 'just want a computer'.&lt;/p&gt;
    &lt;p&gt;Maybe as a consolation prize, they could replace the Ethernet jack and one or two Thunderbolt ports on the back with QSFP? That way we could use network switches, and cluster more than four of these things at a time...&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;As configured. Apple put in 8 TB of SSD storage on the 512GB models, and 4TB on the 256GB models. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;Thank you for the great post, Jeff. Has there been any indication they'll backport support for RDMA over TB to the older models?&lt;/p&gt;
    &lt;p&gt;It seems rather strange that Exo disappeared for a few months and has now come out with a totally new rewrite of the project (in some kind of backroom deal with Apple) that exclusively supports only the newest generation of Apple Silicon computers (M3/M4) while the older ones (M1/M2) are apparently left in the dust wrt RDMA.&lt;/p&gt;
    &lt;p&gt;I'm not trying to blow smoke or complain; there are a lot of people who took Alex Cheema, Awni Hannun, and Georgi Gerganov at their word when they pointed out that the M2 series is really great for inference. Georgi himself has an M2 Ultra 192GB; is he going to quietly trade it in for an M3 Ultra and eat a $7,000 loss because... Apple doesn't feel like issuing a microcode patch that enables RDMA on the M2? It all feels so fake.&lt;/p&gt;
    &lt;p&gt;It almost feels like this is a big marketing stunt by Apple to get the home computing hobbyist community to spend a few more $B on new Apple Silicon.&lt;/p&gt;
    &lt;p&gt;And of course, in the time between MLX/Exo coming out and the present, we completely lost all the main developers of Asahi Linux.&lt;/p&gt;
    &lt;p&gt;I don't know anything that's happened behind closed doors, but I have seen many times when an AI startup that does something interesting/promising get gobbled up and just kinda disappear from the face of the planet.&lt;/p&gt;
    &lt;p&gt;At least this time Exo re-surfaced! I'm more interested in the HPC aspects, than LLM to be honest. It'd be neat to build a true beowulf cluster with RDMA of a Mac, an Nvidia node, an AMD server, etc. and see what kind of fun I could have :)&lt;/p&gt;
    &lt;p&gt;Hey Andrej,&lt;lb/&gt; What's your reasoning for saying M1/M2 is not supported, is the requirement TB5 specifically (in which case some of the M4 and M5 machines are not supported as well)? Didn't really find any source and I was hoping I can mix and match whatever M1 Max with M1 Pro and M4 and M3 Ultra to my liking, so to speak. If that's not the case then… it's disappointing.&lt;/p&gt;
    &lt;p&gt;Cheers&lt;/p&gt;
    &lt;p&gt;Have you tried with thunderbolt 5 hosts with thunderbolt 4 hosts? I wanted to try this clustering for local LLM.&lt;/p&gt;
    &lt;p&gt;I've been emailing with Deskpi about the TL1, do you know if it is able to fit 10"x10" rack like this one?&lt;lb/&gt; https://www.printables.com/model/1176409-10-x10-minirack-now-with-micro…&lt;lb/&gt; The rails looks slightly oddly shaped but it seems like it should work.&lt;lb/&gt; Makes it way cheaper when getting a MOBO for your rack if you can fit a microATX instead of mini&lt;/p&gt;
    &lt;p&gt;It would make my current setup MUCH less janky&lt;/p&gt;
    &lt;p&gt;No only up to like 8.75" I think... 220mm?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;did you know there's no way to run a system upgrade (like to 26.2) via SSH? You have to click buttons in the UI.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;/usr/sbin/softwareupdate&lt;/code&gt; can't do this? I don't have any pending updates to test with, but it looks like &lt;code&gt;--install --os-only --restart&lt;/code&gt; should suffice.&lt;/p&gt;
    &lt;p&gt;A few people mentioned this — I had tried with the 26.0 update and it didn't seem to work. I may try again once 26.3 is out (I could maybe test on a beta...).&lt;/p&gt;
    &lt;p&gt;I remember the Xserve days and VFX render clusters. But these days software update cli is deprecated as Apple pushes us all to MDM (DDM for software updates) but other ways to update existing for enterprise admins like Graham Pugh’s erase-install on github which leverages Nindi Gill’s mist-cli. A lot of crafty Mac Admins&lt;/p&gt;
    &lt;p&gt;Just setting-up RDMA across 2 x M3 Ultra Studios (1024GB RAM)&lt;lb/&gt; I got tons of models on local drive - most &amp;gt; 700GB - I don't want to download them again from Huggingface.&lt;/p&gt;
    &lt;p&gt;Is there a way to get EXO to use local model directory?&lt;/p&gt;
    &lt;p&gt;I have seen some comments -&lt;/p&gt;
    &lt;p&gt;from GitHub repo ...&lt;/p&gt;
    &lt;p&gt;How to use the downloaded local model #190&lt;/p&gt;
    &lt;p&gt;- but its all a bit cryptic&lt;/p&gt;
    &lt;p&gt;Since you have a "working system" - have you tried to see what is the "fix" (if at all) to use locally stored model?&lt;/p&gt;
    &lt;p&gt;I appreciate that the supported models / tokenises are baked into EXO at the moment - I am using same models - but they are LOCALY stored.&lt;/p&gt;
    &lt;p&gt;Any feedback / testing would be most welcome by those of us who have access to the expensive compute (the M3 Ultras)&lt;/p&gt;
    &lt;p&gt;Thanks&lt;/p&gt;
    &lt;p&gt;Hi, thank you for the great article!&lt;lb/&gt; I have a question: is it possible to measure the actual throughput of Thunderbolt 5 in RDMA mode?&lt;lb/&gt; Specifically, can we monitor what the real transfer rate is when using RDMA over Thunderbolt 5, and if so, what tools or methods do you recommend to observe that actual throughput?&lt;/p&gt;
    &lt;p&gt;Great work Jeff! I'm wondering what version of HPL are you using? It's a bit peculiar to see you didn't have results for HPL over RDMA -- would OpenMPI need to add support for this RDMA transport?&lt;lb/&gt; Cheers, Pengcheng&lt;/p&gt;
    &lt;p&gt;Jeff, great work as always, and thank you for your Ansible contributions; I use them nearly every day. Came here to comment that I feel your pain w.r.t. provisioning macOS hardware. One thing that has helped *tremendously* is using Claude Computer Use to automate the parts that cannot otherwise be automated through programmatic means, in particular OS reinstallations and the granting of sensitive permissions, e.g. screen recording and camera. Together w/ Ansible and MDM, it enables me to wipe and reprovision a mac end-to-end w/o any human intervention. Just thought I'd mention it here as it's genuinely useful in this scenario.&lt;/p&gt;
    &lt;p&gt;Great blog post/video as always! Thank you :)&lt;/p&gt;
    &lt;p&gt;There is another open source clustering sofware available at https://github.com/GradientHQ/parallax Wondering what differences between exo and that one. Also it looks like exo is not providing an OpenAI-compatible API, is it?&lt;/p&gt;
    &lt;p&gt;Supposedly they are, but I have not tested it.&lt;/p&gt;
    &lt;p&gt;The OpenAI-compatible API, is baked into the EXO software - just set-up the endpoints in your client (openWEBUI) and use the local EXO IP with port 8000 - it works well!&lt;/p&gt;
    &lt;p&gt;What if you use thunderbolds for ethernet on AMD 385max&lt;lb/&gt; It should be much faster&lt;/p&gt;
    &lt;p&gt;So far I've only been able to get 9-10 Gbps on the 40 Gbps TB4 ports on the Max+ 395 boards from Framework. Still working on it though...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"/><published>2025-12-18T22:23:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46321947</id><title>Brown/MIT shooting suspect found dead, officials say</title><updated>2025-12-19T20:41:00.700453+00:00</updated><content>&lt;doc fingerprint="ff5221384375c71"&gt;
  &lt;main&gt;
    &lt;p&gt;A man suspected in the fatal shooting at Brown University and killing days later of a professor from the Massachusetts Institute of Technology was found dead of a self-inflicted gunshot wound inside a New Hampshire storage facility, authorities announced Thursday evening.&lt;/p&gt;
    &lt;p&gt;Democracy Dies in Darkness&lt;/p&gt;
    &lt;head rend="h1"&gt;Brown U. and MIT professor shootings are linked; suspect found dead, officials say&lt;/head&gt;
    &lt;p&gt;The alleged shooter, Claudio Manuel Neves Valente, was a legal permanent U.S. resident and had been a graduate student at Brown two decades ago, authorities said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.washingtonpost.com/nation/2025/12/18/brown-university-shooting-person-of-interest/"/><published>2025-12-19T03:19:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46322391</id><title>Rust's Block Pattern</title><updated>2025-12-19T20:41:00.472803+00:00</updated><content>&lt;doc fingerprint="a620394777b53911"&gt;
  &lt;main&gt;
    &lt;p&gt;Here’s a little idiom that I haven’t really seen discussed anywhere, that I think makes Rust code much cleaner and more robust.&lt;/p&gt;
    &lt;p&gt;I don’t know if there’s an actual name for this idiom; I’m calling it the “block pattern” for lack of a better word. I find myself reaching for it frequently in code, and I think other Rust code could become cleaner if it followed this pattern. If there’s an existing name for this, please let me know!&lt;/p&gt;
    &lt;p&gt;The pattern comes from blocks in Rust being valid expressions. For example, this code:&lt;/p&gt;
    &lt;code&gt;let foo = { 1 + 2 };
&lt;/code&gt;
    &lt;p&gt;…is equal to this code:&lt;/p&gt;
    &lt;code&gt;let foo = 1 + 2;
&lt;/code&gt;
    &lt;p&gt;…which is, in turn, equal to this code:&lt;/p&gt;
    &lt;code&gt;let foo = {
    let x = 1;
    let y = 2;
    x + y
};
&lt;/code&gt;
    &lt;head rend="h2"&gt;So, why does this matter?&lt;/head&gt;
    &lt;p&gt;Let’s say you have a function that loads a configuration file, then sends a few HTTP requests based on that config file. In order to load that config file, first you need to load the raw bytes of that file from the disk. Then you need to parse whatever the format of the configuration file is. For the sake of having a complex enough program to demonstrate the value of this pattern, let’s say it’s JSON with comments. You would need to remove the comments first using the &lt;code&gt;regex&lt;/code&gt; crate,
then parse the resulting JSON with something like &lt;code&gt;serde-json&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Such a function would look like this:&lt;/p&gt;
    &lt;code&gt;use regex::{Regex, RegexBuilder};
use std::{fs, sync::LazyLock};

/// Format of the configuration file.
#[derive(serde::Deserialize)]
struct Config { /* ... */ }

// Always make sure to cache your regexes!
static STRIP_COMMENTS: LazyLock&amp;lt;Regex&amp;gt; = LazyLock::new(|| {
    RegexBuilder::new(r"//.*").multi_line(true).build().expect("regex build failed")
});

/// Function to load the config and send some HTTP requests.
fn foo(cfg_file: &amp;amp;str) -&amp;gt; anyhow::Result&amp;lt;()&amp;gt; {
    // Load the raw bytes of the file.
    let config_data = fs::read(cfg_file)?;

    // Convert to a string to the regex can work on it.
    let config_string = String::from_utf8(&amp;amp;config_data)?;

    // Strip out all comments.
    let stripped_data = STRIP_COMMENTS.replace(&amp;amp;config_string, "");

    // Parse as JSON.
    let config = serde_json::from_str(&amp;amp;stripped_data)?;

    // Do some work based on this data.
    send_http_request(&amp;amp;config.url1)?;
    send_http_request(&amp;amp;config.url2)?;
    send_http_request(&amp;amp;config.url3)?;

    Ok(())
}
&lt;/code&gt;
    &lt;p&gt;This is fairly simple, and just leverages a few Rust crates and language features to parse JSON and then do something with it.&lt;/p&gt;
    &lt;p&gt;However, there are a few weaknesses here. In the &lt;code&gt;foo&lt;/code&gt; function, we declare four new variables (&lt;code&gt;config_data&lt;/code&gt;, &lt;code&gt;config_string&lt;/code&gt;,
&lt;code&gt;stripped_data&lt;/code&gt;, &lt;code&gt;config&lt;/code&gt;) only for only one of those variables to be used after the configuration parsing (&lt;code&gt;config&lt;/code&gt;). In addition,
let’s say you didn’t know what this code was for going in, and you didn’t have these comments (or you had bad comments). One might
ask why you’re declaring the regular expression &lt;code&gt;STRIP_COMMENTS&lt;/code&gt;, or why you’re loading data from a file.&lt;/p&gt;
    &lt;p&gt;When I write code, I try to make it immediately obvious what the purpose of the code is, and why it’s written that way. This is why I generally avoid C’s “bottom-up” strategy for organizing code. It’s like being given a few screws and being expected to implicitly understand that it should be built into a chair. In Rust, I like that you are able to define your top-level functions first, and then go down and define all the bits and pieces after.&lt;/p&gt;
    &lt;p&gt;Although, we can do a little bit better. What if we organized the &lt;code&gt;foo&lt;/code&gt; function like this:&lt;/p&gt;
    &lt;code&gt;/// Function to load the config and send some HTTP requests.
fn foo(cfg_file: &amp;amp;str) -&amp;gt; anyhow::Result&amp;lt;()&amp;gt; {
    // Load the configuration from the file.
    let config = {
        // Cached regular expression for stripping comments.
        static STRIP_COMMENTS: LazyLock&amp;lt;Regex&amp;gt; = LazyLock::new(|| {
            RegexBuilder::new(r"//.*").multi_line(true).build().expect("regex build failed")
        });

        // Load the raw bytes of the file.
        let raw_data = fs::read(cfg_file)?;

        // Convert to a string to the regex can work on it.
        let data_string = String::from_utf8(&amp;amp;raw_data)?;

        // Strip out all comments.
        let stripped_data = STRIP_COMMENTS.replace(&amp;amp;config_string, "");

        // Parse as JSON.
        serde_json::from_str(&amp;amp;stripped_data)?
    };

    // Do some work based on this data.
    send_http_request(&amp;amp;config.url1)?;
    send_http_request(&amp;amp;config.url2)?;
    send_http_request(&amp;amp;config.url3)?;

    Ok(())
}
&lt;/code&gt;
    &lt;p&gt;In this function, we’ve moved all of the configuration-related code (parsing, loading, even the static regex) into the block. This works because Rust lets you have items, statements and expressions inside of a block, hence why we were able to move everything inside. This pattern has three immediate advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The block starts with the intent of the code (&lt;code&gt;let config = ...&lt;/code&gt;). We can see that we’re working to resolve some kind of configuration object right off the bat. Only then do we move into the implementation details of the code.&lt;/item&gt;
      &lt;item&gt;It reduces pollution of the namespace of both the &lt;code&gt;foo&lt;/code&gt;function and the top-level module. Now in&lt;code&gt;foo&lt;/code&gt;, the variable names&lt;code&gt;config_data&lt;/code&gt;,&lt;code&gt;config_string&lt;/code&gt;et al are no longer used. In addition to allowing these variable names to be re-used, it makes this code a lot more “idiot-proof”. If someone else were to edit the&lt;code&gt;foo&lt;/code&gt;function, they would only be able to use&lt;code&gt;config&lt;/code&gt;. They wouldn’t be able to use the&lt;code&gt;raw_data&lt;/code&gt;or&lt;code&gt;STRIP_COMMENTS&lt;/code&gt;items, which are only meant to be used by the&lt;code&gt;config&lt;/code&gt;parser.&lt;/item&gt;
      &lt;item&gt;The variables &lt;code&gt;raw_data&lt;/code&gt;and&lt;code&gt;data_string&lt;/code&gt;go out of scope at the end of the block, which means they are dropped, freeing up resources.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As an aside, all three of these advantages also come if you were to refactor the block out into its own function. However, this pattern has two key advantages over that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The code flow is still inline with the rest of the function. For shorter blocks, this improves reading comprehension, since it means you don’t have to go to a different part of the code to fully understand the function.&lt;/item&gt;
      &lt;item&gt;If there are a lot of variables that the block would use, it prevents needing to explicitly name those variables as parameters.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There is one more benefit that’s not exposed in the above example: erasure of mutability. Let’s say you construct some object for use in a later part of the function:&lt;/p&gt;
    &lt;code&gt;let mut data = vec![];
data.push(1);
data.extend_from_slice(&amp;amp;[4, 5, 6, 7]);

data.iter().for_each(|x| println!("{x}"));
return data[2];
&lt;/code&gt;
    &lt;p&gt;The issue is that &lt;code&gt;data&lt;/code&gt; is declared as mutable, which means the rest of the function can mutate it. Since a lot of bugs come from
data being mutated when it isn’t supposed to be mutated, we’d like to restrict the mutability of the data to a certain area of the
function. This is also possible with the block pattern:&lt;/p&gt;
    &lt;code&gt;let data = {
    let mut data = vec![];
    data.push(1);
    data.extend_from_slice(&amp;amp;[4, 5, 6, 7]);
    data
};

data.iter().for_each(|x| println!("{x}"));
return data[2];
&lt;/code&gt;
    &lt;p&gt;This effectively “closes” the mutability to a certain section of the function.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Thoughts&lt;/head&gt;
    &lt;p&gt;I don’t know if this pattern is already well known to the Rust community. Even if it isn’t, I figure it’s still a good idea to bring it to people who may be inexperienced in Rust.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://notgull.net/block-pattern/"/><published>2025-12-19T04:56:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46324078</id><title>Amazon will allow ePub and PDF downloads for DRM-free eBooks</title><updated>2025-12-19T20:41:00.094389+00:00</updated><content>&lt;doc fingerprint="336f507caffaf299"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading × Sorry to interrupt CSS Error Refresh&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.kdpcommunity.com/s/article/New-eBook-Download-Options-for-Readers-Coming-in-2026?language=en_US"/><published>2025-12-19T10:03:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46324543</id><title>GotaTun -- Mullvad's WireGuard Implementation in Rust</title><updated>2025-12-19T20:40:59.541094+00:00</updated><content>&lt;doc fingerprint="77aac445c1b43610"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing GotaTun, the future of WireGuard at Mullvad VPN&lt;/head&gt;
    &lt;p&gt;GotaTun is a WireGuard® implementation written in Rust aimed at being fast, efficient and reliable.&lt;/p&gt;
    &lt;p&gt;GotaTun is a fork of the BoringTun project from Cloudflare. This is not a new protocol or connection method, just WireGuard® written in Rust. The name GotaTun is a combination of the original project, BoringTun, and Götatunneln, a physical tunnel located in Gothenburg. We have integrated privacy enhancing features like DAITA &amp;amp; Multihop, added first-class support for Android and used Rust to achieve great performance by using safe multi-threading and zero-copy memory strategies.&lt;/p&gt;
    &lt;p&gt;Last month we rolled it out to all our Android users, and we aim to ship it to the remaining platforms next year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why GotaTun?&lt;/head&gt;
    &lt;p&gt;Our mobile apps have relied on wireguard-go for several years, a cross-platform userspace implementation of WireGuard® in Go. wireguard-go has been the de-facto userspace implementation of WireGuard® to this date, and many VPN providers besides Mullvad use it. Since mid-2024 we have been maintaining a fork of &lt;lb/&gt;wireguard-go to support features like DAITA &amp;amp; Multihop. While wireguard-go has served its purpose for many years it has not been without its challenges.&lt;/p&gt;
    &lt;p&gt;For Android apps distributed via the Google Play Store, Google collects crash reports and makes them available to developers. In the developer console we have seen that more than 85% of all crashes reported have stemmed from the wireguard-go. We have managed to solve some of the obscure issues over the years (#6727 and #7728 to name two examples), but many still remain. For these reasons we chose Android as the first platform to release GotaTun on, allowing us to see the impact right away.&lt;/p&gt;
    &lt;p&gt;Another challenge we have faced is interoperating Rust and Go. Currently, most of the service components of the Mullvad VPN app are written in Rust with the exception of wireguard-go. Crossing the boundary between Rust and Go is done using a foreign function interface (FFI), which is inherently unsafe and complex. Since Go is a managed language with its own separate runtime, how it executes is opaque to the Rust code. If wireguard-go were to hang or crash, recovering stacktraces is not always possible which makes debugging the code cumbersome. Limited visibility insight into crashes stemming from Go has made troubleshooting and long-term maintenance tedious.&lt;/p&gt;
    &lt;head rend="h3"&gt;Outcome&lt;/head&gt;
    &lt;p&gt;The impact has been immediate. So far not a single crash has stemmed from GotaTun, meaning that all our old crashes from wireguard-go are now gone. Since rolling out GotaTun on Android with version 2025.10 in the end of November we’ve seen a big drop in the metric user-perceived crash rate, from 0.40% to 0.01%, when comparing to previous releases. The feedback from users' have also been positive, with reports of better speeds and lower battery usage.&lt;/p&gt;
    &lt;p&gt;User-perceived crash rate&lt;/p&gt;
    &lt;head rend="h3"&gt;Looking ahead&lt;/head&gt;
    &lt;p&gt;We’ve reached the first major milestone with the release of GotaTun on Android, but we have a lot more exciting things in store for 2026.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A third-party security audit will take place early next year.&lt;/item&gt;
      &lt;item&gt;We will replace wireguard-go with GotaTun across all platforms, including desktop and iOS.&lt;/item&gt;
      &lt;item&gt;More effort will be put into improving performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We hope you are as excited as we are for 2026!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mullvad.net/en/blog/announcing-gotatun-the-future-of-wireguard-at-mullvad-vpn"/><published>2025-12-19T11:16:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46324702</id><title>8-bit Boléro</title><updated>2025-12-19T20:40:58.984453+00:00</updated><content>&lt;doc fingerprint="ad1238a56919468b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Boléro&lt;/head&gt;
    &lt;p&gt;I perform Maurice Ravel's Boléro on a variety of homemade 8-bit instruments.&lt;/p&gt;
    &lt;head rend="h2"&gt;Download&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linus Akesson - Maurice Ravel - Boléro.mp3 (MP3, 26.2 MB)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Some stats and details&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;9 hours and 42 minutes of footage&lt;/item&gt;
      &lt;item&gt;52 mixer channels&lt;/item&gt;
      &lt;item&gt;13 neck- and bowties&lt;/item&gt;
      &lt;item&gt;9 different instruments&lt;/item&gt;
      &lt;item&gt;1 crazy automaton&lt;/item&gt;
      &lt;item&gt;0 regrets&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project took me a bit over half a year to finish.&lt;/p&gt;
    &lt;p&gt;I hope you'll enjoy the video as much as I enjoyed making it! There are many little details that I'll let you discover on your own.&lt;/p&gt;
    &lt;p&gt;It was fun to put my tools and methods to the test with this huge undertaking. When I started out I had no idea if my mixing and video-editing process would work at this scale, but it turned out to only need a few tweaks here and there.&lt;/p&gt;
    &lt;p&gt;The nine instruments are: The Qweremin (breadbin / regular C64C / dark C64C), Qwertuoso (breadbin), the Paulimba, the Tenor Commodordion, the Family Bass (albeit not as a bass this time), my still unnamed floppy-drive noise instrument (1541 / 1541-II), the C=TAR, the Chipophone, and a newcomer: NES timpani.&lt;/p&gt;
    &lt;p&gt;The timpani sound is based on the famous NES staircase triangle wave. But there is no register for controlling its volume, and yet we can hear an envelope with a release phase. To achieve this, I rely on a neat trick that was used already in Super Mario Bros.: It turns out that the triangle channel is mixed with the ADPCM sample-playback channel and then fed through a non-linear resistor network (the output is proportional to ((adpcm + triangle)⁻¹ + C)⁻¹). Therefore, adding a constant DC offset via the sample channel makes the triangle more or less compressed.&lt;/p&gt;
    &lt;p&gt;In nearly every part of this video, what you see is what you hear; the audio and video were recorded at the same time. But the automaton is different: Thanks to its 100% repeatable performance I could capture the sound of each individual section of the hardware, with the microphone up close, and then mix the parts according to taste and combine them with the visuals from a separate take.&lt;/p&gt;
    &lt;p&gt;Fun fact: You can't see it in the video but the automaton is supported by original C64 boxes:&lt;/p&gt;
    &lt;p&gt;Posted Friday 19-Dec-2025 08:00&lt;/p&gt;
    &lt;head rend="h3"&gt;Discuss this page&lt;/head&gt;
    &lt;p&gt;There are no comments here yet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://linusakesson.net/music/bolero/index.php"/><published>2025-12-19T11:38:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46326506</id><title>Building a Transparent Keyserver</title><updated>2025-12-19T20:40:58.725202+00:00</updated><content>&lt;doc fingerprint="e629631b3ec43ed3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Building a Transparent Keyserver&lt;/head&gt;
    &lt;p&gt;Today, we are going to build a keyserver to lookup age public keys. That part is boring. What’s interesting is that we’ll apply the same transparency log technology as the Go Checksum Database to keep the keyserver operator honest and unable to surreptitiously inject malicious keys, while still protecting user privacy and delivering a smooth UX. You can see the final result at keyserver.geomys.org. We’ll build it step-by-step, using modern tooling from the tlog ecosystem, integrating transparency in less than 500 lines.&lt;/p&gt;
    &lt;p&gt;I am extremely excited to write this post: it demonstrates how to use a technology that I strongly believe is key in protecting users and holding centralized services accountable, and it’s the result of years of effort by me, the TrustFabric team at Google, the Sigsum team at Glasklar, and many others.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This article is being cross-posted on the Transparency.dev Community Blog.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Let’s start by defining the goal: we want a secure and convenient way to fetch age public keys for other people and services.1&lt;/p&gt;
    &lt;p&gt;The easiest and most usable way to achieve that is to build a centralized keyserver: a web service where you log in with your email address to set your public key, and other people can look up public keys by email address.&lt;/p&gt;
    &lt;p&gt;Trusting the third party that operates the keyserver lets you solve identity, authentication, and spam by just delegating the responsibilities of checking email ownership and implementing rate limiting. The keyserver can send a link to the email address, and whoever receives it is authorized to manage the public key(s) bound to that address.&lt;/p&gt;
    &lt;p&gt;I had Claude Code build the base service, because it’s simple and not the interesting part of what we are doing today. There’s nothing special in the implementation: just a Go server, an SQLite database,2 a lookup API, a set API protected by a CAPTCHA that sends an email authentication link,3 and a Go CLI that calls the lookup API.&lt;/p&gt;
    &lt;head rend="h2"&gt;Transparency logs and accountability for centralized services&lt;/head&gt;
    &lt;p&gt;A lot of problems are shaped like this and are much more solvable with a trusted third party: PKIs, package registries, voting systems… Sometimes the trusted third party is encapsulated behind a level of indirection, and we talk about Certificate Authorities, but it’s the same concept.&lt;/p&gt;
    &lt;p&gt;Centralization is so appealing that even the OpenPGP ecosystem embraced it: after the SKS pool was killed by spam, a new OpenPGP keyserver was built which is just a centralized, email-authenticated database of public keys. Its FAQ claims they don’t wish to be a CA, but also explains they don’t support the (dubiously effective) Web-of-Trust at all, so effectively they can only act as a trusted third party.&lt;/p&gt;
    &lt;p&gt;The obvious downside of a trusted third party is, well, trust. You need to trust the operator, but also whoever will control the operator in the future, and also the operator’s security practices. That’s asking a lot, especially these days, and a malicious or compromised keyserver could provide fake public keys to targeted victims with little-to-no chance of detection.&lt;/p&gt;
    &lt;p&gt;Transparency logs are a technology for applying cryptographic accountability to centralized systems with no UX sacrifices.&lt;/p&gt;
    &lt;p&gt;A transparency log or tlog is an append-only, globally consistent list of entries, with efficient cryptographic proofs of inclusion and consistency. The log operator appends entries to the log, which can be tuples like (package, version, hash) or (email, public key). The clients verify an inclusion proof before accepting an entry, guaranteeing that the log operator will have to stand by that entry in perpetuity and to the whole world, with no way to hide it or disown it. As long as someone who can check the authenticity of the entry will eventually check (or “monitor”) the log, the client can trust that malfeasance will be caught.&lt;/p&gt;
    &lt;p&gt;Effectively, a tlog lets the log operator stake their reputation to borrow time for collective, potentially manual verification of the log’s entries. This is a middle-ground between impractical local verification mechanisms like the Web of Trust, and fully trusted mechanisms like centralized X.509 PKIs.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you’d like a longer introduction, my Real World Crypto 2024 talk presents both the technical functioning and abstraction of modern transparency logs.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There is a whole ecosystem of interoperable tlog tools and publicly available infrastructure built around C2SP specifications. That’s what we are going to use today to add a tlog to our keyserver.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you want to catch up with the tlog ecosystem, my 2025 Transparency.dev Summit Keynote maps out the tools, applications, and specifications.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;tlogs vs Certificate Transparency vs Key Transparency&lt;/head&gt;
    &lt;p&gt;If you are familiar with Certificate Transparency, tlogs are derived from CT, but with a few major differences. Most importantly, there is no separate entry producer (in CT, the CAs) and log operator; moreover, clients check actual inclusion proofs instead of SCTs; finally, there are stronger split-view protections, as we will see below. The Static CT API and Sunlight CT log implementation were a first successful step in moving CT towards the tlog ecosystem, and a proposed design called Merkle Tree Certificates redesigns the WebPKI to have tlog-like and tlog-interoperable transparency.&lt;/p&gt;
    &lt;p&gt;In my experience, it’s best not to think about CT when learning about tlogs. A better production example of a tlog is the Go Checksum Database, where Google logs the module name, version, and hash for every module version observed by the Go Modules Proxy. The module fetches happen over regular HTTPS, so there is no publicly-verifiable proof of their authenticity. Instead, the central party appends every observation to the tlog, so that any misbehavior can be caught. The &lt;code&gt;go get&lt;/code&gt; command verifies inclusion proofs for every module it downloads, protecting 100% of the ecosystem, without requiring module authors to manage keys.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Katie Hockman gave a great talk on the Go Checksum Database at GopherCon 2019.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You might also have heard of Key Transparency. KT is an overlapping technology that was deployed by Apple, WhatsApp, and Signal amongst others. It has similar goals, but picks different tradeoffs that involve significantly more complexity, in exchange for better privacy and scalability in some settings.&lt;/p&gt;
    &lt;head rend="h2"&gt;A tlog for our keyserver&lt;/head&gt;
    &lt;p&gt;Ok, so how do we apply a tlog to our email-based keyserver?&lt;/p&gt;
    &lt;p&gt;It’s pretty simple, and we can do it with a 250-line diff using Tessera and Torchwood. Tessera is a general-purpose tlog implementation library, which can be backed by object storage or a POSIX filesystem. For our keyserver, we’ll use the latter backend, which stores the whole tlog in a directory according to the c2sp.org/tlog-tiles specification.&lt;/p&gt;
    &lt;code&gt;s, err := note.NewSigner(os.Getenv("LOG_KEY"))
if err != nil {
    log.Fatalln("failed to create checkpoint signer:", err)
}
v, err := torchwood.NewVerifierFromSigner(os.Getenv("LOG_KEY"))
if err != nil {
    log.Fatalln("failed to create checkpoint verifier:", err)
}
policy := torchwood.ThresholdPolicy(2, torchwood.OriginPolicy(v.Name()),
    torchwood.SingleVerifierPolicy(v))

driver, err := posix.New(ctx, posix.Config{
    Path: *logPath,
})
if err != nil {
    log.Fatalln("failed to create log storage driver:", err)
}

// Since this is a low-traffic but interactive server, disable batching to
// remove integration latency for the first request. Keep a 1s checkpoint
// interval not to hit the witnesses too often; this will be observed only
// if two requests come in quick succession. Finally, only publish a
// checkpoint once a day if there are no new entries, making the average qps
// on witnesses low. Poll for new checkpoints quickly since it should be
// just a read from a hot filesystem cache.
checkpointInterval := 1 * time.Second
if testing.Testing() {
    checkpointInterval = 100 * time.Millisecond
}
appender, shutdown, logReader, err := tessera.NewAppender(ctx, driver, tessera.NewAppendOptions().
    WithCheckpointSigner(s).
    WithBatching(1, tessera.DefaultBatchMaxAge).
    WithCheckpointInterval(checkpointInterval).
    WithCheckpointRepublishInterval(24*time.Hour))
if err != nil {
    log.Fatalln("failed to create log appender:", err)
}
defer shutdown(context.Background())
awaiter := tessera.NewPublicationAwaiter(ctx, logReader.ReadCheckpoint, 25*time.Millisecond)
&lt;/code&gt;
    &lt;p&gt;Every time a user sets their key, we append an encoded (email, public key) entry to the tlog, and we store the tlog entry index in the database.&lt;/p&gt;
    &lt;code&gt;+    // Add to transparency log
+    if strings.ContainsAny(email, "\n") {
+        http.Error(w, "Invalid email format", http.StatusBadRequest)
+        return
+    }
+    entry := tessera.NewEntry(fmt.Appendf(nil, "%s\n%s\n", email, pubkey))
+    index, _, err := s.awaiter.Await(r.Context(), s.appender.Add(r.Context(), entry))
+    if err != nil {
+        http.Error(w, "Failed to add to transparency log", http.StatusInternalServerError)
+        log.Printf("transparency log error: %v", err)
+        return
+    }
+
     // Store in database
-    if err := s.storeKey(email, pubkey); err != nil {
+    if err := s.storeKey(email, pubkey, int64(index.Index)); err != nil {
         http.Error(w, "Failed to store key", http.StatusInternalServerError)
         log.Printf("database error: %v", err)
         return
     }
&lt;/code&gt;
    &lt;p&gt;The lookup API produces a proof from the index and provides it to the client.&lt;/p&gt;
    &lt;code&gt;func (s *Server) makeSpicySignature(ctx context.Context, index int64) ([]byte, error) {
    checkpoint, err := s.reader.ReadCheckpoint(ctx)
    if err != nil {
        return nil, fmt.Errorf("failed to read checkpoint: %v", err)
    }
    c, _, err := torchwood.VerifyCheckpoint(checkpoint, s.policy)
    if err != nil {
        return nil, fmt.Errorf("failed to parse checkpoint: %v", err)
    }
    p, err := tlog.ProveRecord(c.N, index, torchwood.TileHashReaderWithContext(
        ctx, c.Tree, tesserax.NewTileReader(s.reader)))
    if err != nil {
        return nil, fmt.Errorf("failed to create proof: %v", err)
    }
    return torchwood.FormatProof(index, p, checkpoint), nil
}
&lt;/code&gt;
    &lt;p&gt;The proof follows the c2sp.org/tlog-proof specification. It looks like this&lt;/p&gt;
    &lt;code&gt;c2sp.org/tlog-proof@v1
index 1
CJdjppwZSa2A60oEpcdj/OFjVQyrkP3fu/Ot2r6smg0=

keyserver.geomys.org
2
HtFreYGe2VBtaf3Vf0AG0DAwEZ+H92HQqrx4dkrzk0U=

— keyserver.geomys.org FrMVCWmHnYfHReztLams2F3HUY6UMub3c5xu7+e8R8SAk9cxPKAB1fsQ6gFM16xwkvZ8p5aWaBf8km+M20eHErSfGwI=
&lt;/code&gt;
    &lt;p&gt;and it combines a checkpoint (a signed snapshot of the log at a certain size), the index of the entry in the log, and a proof of inclusion of the entry in the checkpoint.&lt;/p&gt;
    &lt;p&gt;The client CLI receives the proof from the lookup API, checks the signature on the checkpoint from the built-in log public key, hashes the expected entry, and checks the inclusion proof for that hash and checkpoint. It can do all this without interacting further with the log.&lt;/p&gt;
    &lt;code&gt;vkey := os.Getenv("AGE_KEYSERVER_PUBKEY")
if vkey == "" {
    vkey = defaultKeyserverPubkey
}
v, err := note.NewVerifier(vkey)
if err != nil {
    fmt.Fprintf(os.Stderr, "Error: invalid keyserver public key: %v\n", err)
    os.Exit(1)
}
policy := torchwood.ThresholdPolicy(2, torchwood.OriginPolicy(v.Name()),
    torchwood.SingleVerifierPolicy(v))
&lt;/code&gt;
    &lt;code&gt;// Verify spicy signature
entry := fmt.Appendf(nil, "%s\n%s\n", result.Email, result.Pubkey)
if err := torchwood.VerifyProof(policy, tlog.RecordHash(entry), []byte(result.Proof)); err != nil {
    return "", fmt.Errorf("failed to verify key proof: %w", err)
}
&lt;/code&gt;
    &lt;p&gt;If you squint, you can see that the proof is really a “fat signature” for the entry, which you verify with the log’s public key, just like you’d verify an Ed25519 or RSA signature for a message. I like to call them spicy signatures to stress how tlogs can be deployed anywhere you can deploy regular digital signatures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Monitoring&lt;/head&gt;
    &lt;p&gt;What’s the point of all this though? The point is that anyone can look through the log to make sure the keyserver is not serving unauthorized keys for their email address! Indeed, just like backups are useless without restores and signatures are useless without verification, tlogs are useless without monitoring. That means we need to build tooling to monitor the log.&lt;/p&gt;
    &lt;p&gt;On the server side, it takes two lines of code, to expose the Tessera POSIX log directory.&lt;/p&gt;
    &lt;code&gt;// Serve tlog-tiles log
fs := http.StripPrefix("/tlog/", http.FileServer(http.Dir(*logPath)))
mux.Handle("GET /tlog/", fs)
&lt;/code&gt;
    &lt;p&gt;On the client side, we add an &lt;code&gt;-all&lt;/code&gt; flag to the CLI that reads all matching entries in the log.&lt;/p&gt;
    &lt;code&gt;func monitorLog(serverURL string, policy torchwood.Policy, email string) ([]string, error) {
    f, err := torchwood.NewTileFetcher(serverURL+"/tlog", torchwood.WithUserAgent("age-keylookup/1.0"))
    if err != nil {
        return nil, fmt.Errorf("failed to create tile fetcher: %w", err)
    }
    c, err := torchwood.NewClient(f)
    if err != nil {
        return nil, fmt.Errorf("failed to create torchwood client: %w", err)
    }

    // Fetch and verify checkpoint
    signedCheckpoint, err := f.ReadEndpoint(context.Background(), "checkpoint")
    if err != nil {
        return nil, fmt.Errorf("failed to read checkpoint: %w", err)
    }
    checkpoint, _, err := torchwood.VerifyCheckpoint(signedCheckpoint, policy)
    if err != nil {
        return nil, fmt.Errorf("failed to parse checkpoint: %w", err)
    }

    // Fetch all entries up to the checkpoint size
    var pubkeys []string
    for i, entry := range c.AllEntries(context.Background(), checkpoint.Tree, 0) {
        e, rest, ok := strings.Cut(string(entry), "\n")
        if !ok {
            return nil, fmt.Errorf("malformed log entry %d: %q", i, string(entry))
        }
        k, rest, ok := strings.Cut(rest, "\n")
        if !ok || rest != "" {
            return nil, fmt.Errorf("malformed log entry %d: %q", i, string(entry))
        }
        if e == email {
            pubkeys = append(pubkeys, k)
        }
    }
    if c.Err() != nil {
        return nil, fmt.Errorf("error fetching log entries: %w", c.Err())
    }

    return pubkeys, nil
}
&lt;/code&gt;
    &lt;p&gt;To enable effective monitoring, we also normalize email addresses by trimming spaces and lowercasing them, since users are unlikely to monitor all the variations. We do it before sending the login link, so normalization can’t lead to impersonation.&lt;/p&gt;
    &lt;code&gt;// Normalize email
email = strings.TrimSpace(strings.ToLower(email))
&lt;/code&gt;
    &lt;p&gt;A complete monitoring story would involve 3rd party services that monitor the log for you and email you if new keys are added, like gopherwatch and Source Spotter do for the Go Checksum Database, but the &lt;code&gt;-all&lt;/code&gt; flag is a start.&lt;/p&gt;
    &lt;p&gt;The full change involves 5 files changed, 251 insertions(+), 6 deletions(-), plus tests, and includes a new keygen helper binary, the required database schema and help text and API changes, and web UI changes to show the proof.&lt;/p&gt;
    &lt;head rend="h2"&gt;Privacy with VRFs&lt;/head&gt;
    &lt;p&gt;We created a problem by implementing this tlog, though: now all the email addresses of our users are public! While this is ok for module names in the Go Checksum Database, allowing email address enumeration in our keyserver is a non-starter for privacy and spam reasons.&lt;/p&gt;
    &lt;p&gt;We could hash the email addresses, but that would still allow offline brute-force attacks. The right tool for the job is a Verifiable Random Function. You can think of a VRF as a hash with a private and public key: only you can produce a hash value, using the private key, but anyone can check that it’s the correct (and unique) hash value, using the public key.&lt;/p&gt;
    &lt;p&gt;Overall, implementing VRFs takes less than 130 lines using the c2sp.org/vrf-r255 instantiation based on ristretto255, implemented by filippo.io/mostly-harmless/vrf-r255 (pending a more permanent location). Instead of the email address, we include the VRF hash in the log entry, and we save the VRF proof in the database.&lt;/p&gt;
    &lt;code&gt;+       // Compute VRF hash and proof
+       vrfProof := s.vrf.Prove([]byte(email))
+       vrfHash := base64.StdEncoding.EncodeToString(vrfProof.Hash())
+
        // Add to transparency log
-       entry := tessera.NewEntry(fmt.Appendf(nil, "%s\n%s\n", email, pubkey))
+       entry := tessera.NewEntry(fmt.Appendf(nil, "%s\n%s\n", vrfHash, pubkey))
        index, _, err := s.awaiter.Await(r.Context(), s.appender.Add(r.Context(), entry))
        if err != nil {
            http.Error(w, "Failed to add to transparency log", http.StatusInternalServerError)
        }

        // [...]

        // Store in database
-       if err := s.storeKey(email, pubkey, int64(index.Index)); err != nil {
+       if err := s.storeKey(email, pubkey, int64(index.Index), vrfProof.Bytes()); err != nil {
            http.Error(w, "Failed to store key", http.StatusInternalServerError)
            log.Printf("database error: %v", err)
            return
        }
&lt;/code&gt;
    &lt;p&gt;The tlog proof format has space for application-specific opaque extra data, so we can store the VRF proof there, to keep the tlog proof self-contained.&lt;/p&gt;
    &lt;code&gt;-   return torchwood.FormatProof(index, p, checkpoint), nil
+   return torchwood.FormatProofWithExtraData(index, vrfProof, p, checkpoint), nil
&lt;/code&gt;
    &lt;p&gt;In the client CLI, we extract the VRF hash from the tlog proof’s extra data and verify it’s the correct hash for the email address.&lt;/p&gt;
    &lt;code&gt;+   // Compute and verify VRF hash
+   vrfProofBytes, err := torchwood.ProofExtraData([]byte(result.Proof))
+   if err != nil {
+       return "", fmt.Errorf("failed to extract VRF proof: %w", err)
+   }
+   vrfProof, err := vrf.NewProof(vrfProofBytes)
+   if err != nil {
+       return "", fmt.Errorf("failed to parse VRF proof: %w", err)
+   }
+   vrfHash, err := vrfKey.Verify(vrfProof, []byte(email))
+   if err != nil {
+       return "", fmt.Errorf("failed to verify VRF proof: %w", err)
+   }
+
    // Verify spicy signature
-   entry := fmt.Appendf(nil, "%s\n%s\n", result.Email, result.Pubkey)
+   vrfHashB64 := base64.StdEncoding.EncodeToString(vrfHash)
+   entry := fmt.Appendf(nil, "%s\n%s\n", vrfHashB64, result.Pubkey)
    if err := torchwood.VerifyProof(policy, tlog.RecordHash(entry), []byte(result.Proof)); err != nil {
        return "", fmt.Errorf("failed to verify key proof: %w", err)
    }
&lt;/code&gt;
    &lt;p&gt;How do we do monitoring now, though? We need to add a new API that provides the VRF hash (and proof) for an email address.&lt;/p&gt;
    &lt;code&gt;    mux.HandleFunc("GET /manage", srv.handleManage)
    mux.HandleFunc("POST /setkey", srv.handleSetKey)
    mux.HandleFunc("GET /api/lookup", srv.handleLookup)
+   mux.HandleFunc("GET /api/monitor", srv.handleMonitor)
    mux.HandleFunc("POST /api/verify-token", srv.handleVerifyToken)
&lt;/code&gt;
    &lt;code&gt;func (s *Server) handleMonitor(w http.ResponseWriter, r *http.Request) {
    email := r.URL.Query().Get("email")
    if email == "" {
        http.Error(w, "Email parameter required", http.StatusBadRequest)
        return
    }

    // Return as JSON
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(map[string]any{
        "email":     email,
        "vrf_proof": s.vrf.Prove([]byte(email)).Bytes(),
    })
}
&lt;/code&gt;
    &lt;p&gt;On the client side, we use that API to obtain the VRF proof, we verify it, and we look for the VRF hash in the log instead of looking for the email address.&lt;/p&gt;
    &lt;p&gt;Attackers can still enumerate email addresses by hitting the public lookup or monitor API, but they’ve always been able to do that: serving such a public API is the point of the keyserver! With VRFs, we restored the original status quo: enumeration requires brute-forcing the online, rate-limited API, instead of having a full list of email addresses in the tlog (or hashes that can be brute-forced offline).&lt;/p&gt;
    &lt;p&gt;VRFs have a further benefit: if a user requests to be deleted from the service, we can’t remove their entries from the tlog, but we can stop serving the VRF for their email address4 from the lookup and monitor APIs. This makes it impossible to obtain the key history for that user, or even to check if they ever used the keyserver, but doesn’t impact monitoring for other users.&lt;/p&gt;
    &lt;p&gt;The full change adding VRFs involves 3 files changed, 125 insertions(+), 13 deletions(-), plus tests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anti-poisoning&lt;/head&gt;
    &lt;p&gt;We have one last marginal risk to mitigate: since we can’t ever remove entries from the tlog, what if someone inserts some unsavory message in the log by smuggling it in as a public key, like &lt;code&gt;age1llllllllllllllrustevangellsmstrlkef0rcellllllllllllq574n08&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Protecting against this risk is called anti-poisoning. The risk to our log is relatively small, public keys have to be Bech32-encoded and short, so an attacker can’t usefully embed images or malware. Still, it’s easy enough to neutralize it: instead of the public keys, we put their hashes in the tlog entry, keeping the original public keys in a new table in the database, and serving them as part of the monitor API.&lt;/p&gt;
    &lt;code&gt;         // Compute VRF hash and proof
         vrfProof := s.vrf.Prove([]byte(email))
-        vrfHash := base64.StdEncoding.EncodeToString(vrfProof.Hash())
+
+        // Keep track of the unhashed key
+        if err := s.storeHistory(email, pubkey); err != nil {
+            http.Error(w, "Failed to store key history", http.StatusInternalServerError)
+            log.Printf("database error: %v", err)
+            return
+        }

         // Add to transparency log
-        entry := tessera.NewEntry(fmt.Appendf(nil, "%s\n%s\n", vrfHash, pubkey))
+        h := sha256.New()
+        h.Write([]byte(pubkey))
+        entry := tessera.NewEntry(h.Sum(vrfProof.Hash())) // vrf-r255(email) || SHA-256(pubkey)
         index, _, err := s.awaiter.Await(r.Context(), s.appender.Add(r.Context(), entry))
&lt;/code&gt;
    &lt;p&gt;It’s very important that we persist the original key in the database before adding the entry to the tlog. Losing the original key would be indistinguishable from refusing to provide a malicious key to monitors.&lt;/p&gt;
    &lt;p&gt;On the client side, to do a lookup we just hash the public key when verifying the inclusion proof. To monitor in &lt;code&gt;-all&lt;/code&gt; mode, we match the hashes against the list of original public keys provided by the server through the monitor API.&lt;/p&gt;
    &lt;code&gt;     var result struct {
         Email    string   `json:"email"`
         VRFProof []byte   `json:"vrf_proof"`
+        History  []string `json:"history"`
     }
&lt;/code&gt;
    &lt;code&gt;+    // Prepare map of hashes of historical keys
+    historyHashes := make(map[[32]byte]string)
+    for _, pk := range result.History {
+        h := sha256.Sum256([]byte(pk))
+        historyHashes[h] = pk
+    }
&lt;/code&gt;
    &lt;code&gt;     // Fetch all entries up to the checkpoint size
     var pubkeys []string
     for i, entry := range c.AllEntries(context.Background(), checkpoint.Tree, 0) {
-        e, rest, ok := strings.Cut(string(entry), "\n")
-        if !ok {
-            return nil, fmt.Errorf("malformed log entry %d: %q", i, string(entry))
-        }
-        k, rest, ok := strings.Cut(rest, "\n")
-        if !ok || rest != "" {
-            return nil, fmt.Errorf("malformed log entry %d: %q", i, string(entry))
-        }
-        if e == base64.StdEncoding.EncodeToString(vrfHash) {
-            pubkeys = append(pubkeys, k)
-        }
+        if len(entry) != 64+32 {
+            return nil, fmt.Errorf("invalid entry size at index %d", i)
+        }
+        if !bytes.Equal(entry[:64], vrfHash) {
+            continue
+        }
+        pk, ok := historyHashes[([32]byte)(entry[64:])]
+        if !ok {
+            return nil, fmt.Errorf("found unknown public key hash in log at index %d", i)
+        }
+        pubkeys = append(pubkeys, pk)
     }
&lt;/code&gt;
    &lt;p&gt;Our final log entry format is &lt;code&gt;vrf-r255(email) || SHA-256(pubkey)&lt;/code&gt;. Designing the tlog entry is the most important part of deploying a tlog: it needs to include enough information to let monitors isolate all the entries relevant to them, but not enough information to pose privacy or poisoning threats.&lt;/p&gt;
    &lt;p&gt;The full change providing anti-poisoning involves 2 files changed, 93 insertions(+), 19 deletions(-), plus tests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-equivocation and the Witness Network&lt;/head&gt;
    &lt;p&gt;We’re almost done! There’s still one thing to fix, and it used to be the hardest part.&lt;/p&gt;
    &lt;p&gt;To get the delayed, collective verification we need, all clients and monitors must see consistent views of the same log, where the log maintains its append-only property. This is called non-equivocation, or split-view protection. In other words, how do we stop the log operator from showing an inclusion proof for log A to a client, and then a different log B to the monitors?&lt;/p&gt;
    &lt;p&gt;Just like logging without a monitoring story is like signing without verification, logging without a non-equivocation story is just a complicated signature algorithm with no strong transparency properties.&lt;/p&gt;
    &lt;p&gt;This is the hard part because in the general case you can’t do it alone. Instead, the tlog ecosystem has the concept of witness cosigners: third-party operated services which cosign a checkpoint to attest that it is consistent with all the other checkpoints the witness observed for that log. Clients check these witness cosignatures to get assurance that—unless a quorum of witnesses is colluding with the log—they are not being presented a split-view of the log.&lt;/p&gt;
    &lt;p&gt;These witnesses are extremely efficient to operate: the log provides the O(log N) consistency proof when requesting a cosignature, and the witness only needs to store the O(1) latest checkpoint it observed. All the potentially intensive verification is deferred and delegated to monitors, which can be sure to have the same view as all clients thanks to the witness cosignatures.&lt;/p&gt;
    &lt;p&gt;This efficiency makes it possible to operate witnesses for free as public benefit infrastructure. The Witness Network collects public witnesses and maintains an open list of tlogs that the witnesses automatically configure.&lt;/p&gt;
    &lt;p&gt;For the Geomys instance of the keyserver, I generated a tlog key and then I sent a PR to the Witness Network to add the following lines to the testing log list.&lt;/p&gt;
    &lt;code&gt;vkey keyserver.geomys.org+16b31509+ARLJ+pmTj78HzTeBj04V+LVfB+GFAQyrg54CRIju7Nn8
qpd 1440
contact keyserver-tlog@geomys.org
&lt;/code&gt;
    &lt;p&gt;This got my log configured in a handful of witnesses, from which I picked three to build the default keyserver witness policy.&lt;/p&gt;
    &lt;code&gt;log keyserver.geomys.org+16b31509+ARLJ+pmTj78HzTeBj04V+LVfB+GFAQyrg54CRIju7Nn8
witness TrustFabric transparency.dev/DEV:witness-little-garden+d8042a87+BCtusOxINQNUTN5Oj8HObRkh2yHf/MwYaGX4CPdiVEPM https://api.transparency.dev/dev/witness/little-garden/
witness Mullvad witness.stagemole.eu+67f7aea0+BEqSG3yu9YrmcM3BHvQYTxwFj3uSWakQepafafpUqklv https://witness.stagemole.eu/
witness Geomys witness.navigli.sunlight.geomys.org+a3e00fe2+BNy/co4C1Hn1p+INwJrfUlgz7W55dSZReusH/GhUhJ/G https://witness.navigli.sunlight.geomys.org/
group public 2 TrustFabric Mullvad Geomys
quorum public
&lt;/code&gt;
    &lt;p&gt;The policy format is based on Sigsum’s policies, and it encodes the log’s public key and the witnesses’ public keys (for the clients) and submission URLs (for the log).&lt;/p&gt;
    &lt;p&gt;Tessera supports these policies directly. When minting a new checkpoint, it will reach out in parallel to all the witnesses, and return the checkpoint once it satisfies the policy. Configuration is trivial, and the added latency is minimal (less than one second).&lt;/p&gt;
    &lt;code&gt;+    witnessPolicy := defaultWitnessPolicy
+    if path := os.Getenv("LOG_WITNESS_POLICY"); path != "" {
+        witnessPolicy, err = os.ReadFile(path)
+        if err != nil {
+            log.Fatalln("failed to read witness policy file:", err)
+        }
+    }
+    witnesses, err := tessera.NewWitnessGroupFromPolicy(witnessPolicy)
+    if err != nil {
+        log.Fatalln("failed to create witness group from policy:", err)
+    }

     // [...]

     appender, shutdown, logReader, err := tessera.NewAppender(ctx, driver, tessera.NewAppendOptions().
         WithCheckpointSigner(s).
         WithBatching(1, tessera.DefaultBatchMaxAge).
         WithCheckpointInterval(checkpointInterval).
-        WithCheckpointRepublishInterval(24*time.Hour))
+        WithCheckpointRepublishInterval(24*time.Hour).
+        WithWitnesses(witnesses, nil))
&lt;/code&gt;
    &lt;p&gt;On the client side, we can use Torchwood to parse the policy and use it directly with VerifyProof in place of the policy we were manually constructing from the log’s public key.&lt;/p&gt;
    &lt;code&gt;-    vkey := os.Getenv("AGE_KEYSERVER_PUBKEY")
-    if vkey == "" {
-        vkey = defaultKeyserverPubkey
-    }
+    policyBytes := defaultPolicy
+    if policyPath := os.Getenv("AGE_KEYSERVER_POLICY"); policyPath != "" {
+        p, err := os.ReadFile(policyPath)
+        if err != nil {
+            fmt.Fprintf(os.Stderr, "Error: failed to read policy file: %v\n", err)
+            os.Exit(1)
+        }
+        policyBytes = p
+    }
-    v, err := note.NewVerifier(vkey)
-    if err != nil {
-        fmt.Fprintf(os.Stderr, "Error: invalid keyserver public key: %v\n", err)
-        os.Exit(1)
-    }
-    policy := torchwood.ThresholdPolicy(2, torchwood.OriginPolicy(v.Name()), torchwood.SingleVerifierPolicy(v))
+    policy, err := torchwood.ParsePolicy(policyBytes)
+    if err != nil {
+        fmt.Fprintf(os.Stderr, "Error: invalid policy: %v\n", err)
+        os.Exit(1)
+    }
&lt;/code&gt;
    &lt;p&gt;Again, if you squint you can see that just like tlog proofs are spicy signatures, the policy is a spicy public key. Verification is a deterministic, offline function that takes a policy/public key and a proof/signature, just like digital signature verification!&lt;/p&gt;
    &lt;p&gt;The policies are a DAG that can get complex to match even the strictest uptime requirements. For example, you can require 3 out of 10 witness operators to cosign a checkpoint, where each operator can use any 1 out of N witness instances to do so.&lt;/p&gt;
    &lt;p&gt;The full change implementing witnessing involves 5 files changed, 43 insertions(+), 11 deletions(-), plus tests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summing up&lt;/head&gt;
    &lt;p&gt;We started with a simple centralized email-authenticated5 keyserver, and we turned it into a transparent, privacy-preserving, anti-poisoning, and witness-cosigned service.&lt;/p&gt;
    &lt;p&gt;We did that in four small steps using Tessera, Torchwood, and various C2SP specifications.&lt;/p&gt;
    &lt;code&gt;cmd/age-keyserver: add transparency log of stored keys
    5 files changed, 259 insertions(+), 8 deletions(-)
cmd/age-keyserver: use VRFs to hide emails in the log
    3 files changed, 125 insertions(+), 13 deletions(-)
cmd/age-keyserver: hash age public key to prevent log poisoning
    2 files changed, 93 insertions(+), 19 deletions(-)
cmd/age-keyserver: add witness cosigning to prevent split-views
    5 files changed, 43 insertions(+), 11 deletions(-)
&lt;/code&gt;
    &lt;p&gt;Overall, it took less than 500 lines.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;7 files changed, 472 insertions(+), 9 deletions(-)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The UX is completely unchanged: there are no keys for users to manage, and the web UI and CLI work exactly like they did before. The only difference is the new &lt;code&gt;-all&lt;/code&gt; functionality of the CLI, which allows holding the log operator accountable for all the public keys it could ever have presented for an email address.&lt;/p&gt;
    &lt;p&gt;The result is deployed live at keyserver.geomys.org.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work: efficient monitoring and revocation&lt;/head&gt;
    &lt;p&gt;This tlog system still has two limitations:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;To monitor the log, the monitor needs to download it all. This is probably fine for our little keyserver, and even for the Go Checksum Database, but it’s a scaling problem for the Certificate Transparency / Merkle Tree Certificates ecosystem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The inclusion proof guarantees that the public key is in the log, not that it’s the latest entry in the log for that email address. Similarly, the Go Checksum Database can’t efficiently prove the Go Modules Proxy&lt;/p&gt;&lt;code&gt;/list&lt;/code&gt;response is complete.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are working on a design called Verifiable Indexes which plugs on top of a tlog to provide verifiable indexes or even map-reduce operations over the log entries. We expect VI to be production-ready before the end of 2026, while everything above is ready today.&lt;/p&gt;
    &lt;p&gt;Even without VI, the tlog provides strong accountability for our keyserver, enabling a secure UX that would have simply not been possible without transparency.&lt;/p&gt;
    &lt;p&gt;I hope this step-by-step demo will help you apply tlogs to your own systems. If you need help, you can join the Transparency.dev Slack. You might also want to follow me on Bluesky at @filippo.abyssdomain.expert or on Mastodon at @filippo@abyssdomain.expert.&lt;/p&gt;
    &lt;head rend="h2"&gt;The picture&lt;/head&gt;
    &lt;p&gt;Growing up, I used to drive my motorcycle around the hills near my hometown, trying to reach churches I could spot from hilltops. This was one of my favorite spots.&lt;/p&gt;
    &lt;p&gt;Geomys, my Go open source maintenance organization, is funded by Smallstep, Ava Labs, Teleport, Tailscale, and Sentry. Through our retainer contracts they ensure the sustainability and reliability of our open source maintenance work and get a direct line to my expertise and that of the other Geomys maintainers. (Learn more in the Geomys announcement.)&lt;/p&gt;
    &lt;p&gt;Here are a few words from some of them!&lt;/p&gt;
    &lt;p&gt;Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.&lt;/p&gt;
    &lt;p&gt;Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;age is not really meant to encrypt messages to strangers, nor does it encourage long-term keys. Instead, keys are simple strings that can be exchanged easily through any semi-trusted (i.e. safe against active attackers) channel. Still, a keyserver could be useful in some cases, and it will serve as a decent example for what we are doing today. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I like to use the SQLite built-in JSON support as a simple document database, to avoid tedious table migrations when adding columns. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ok, one thing is special, but it doesn’t have anything to do with transparency. I strongly prefer email magic links that authenticate your original tab, where you have your browsing session history, instead of making you continue in the new tab you open from the email. However, intermediating that flow via a server introduces a phishing risk: if you click the link you risk authenticating the attacker’s session. This implementation uses the JavaScript Broadcast Channel API to pass the auth token locally to the original tab, if it’s open in the same browser, and otherwise authenticates the new tab. Another advantage of this approach is that there are no authentication cookies. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Someone who stored the VRF for that email address could continue to match the tlog entries, but since we won’t be adding any new entries to the tlog for that email address, they can’t learn anything they didn’t already know. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Something cool about tlogs is that they are often agnostic to the mechanism by which entries are added to the log. For example, instead of email identities and verification we could have used OIDC identities, with our centralized server checking OIDC bearer tokens, held accountable by the tlog. Everything would have worked exactly the same. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://words.filippo.io/keyserver-tlog/"/><published>2025-12-19T14:54:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46326519</id><title>The FreeBSD Foundation's Laptop Support and Usability Project</title><updated>2025-12-19T20:40:58.397941+00:00</updated><content>&lt;doc fingerprint="f28eead3e9bb85fb"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Program Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Laptop Support and Usability&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Funding Body&lt;/cell&gt;
        &lt;cell&gt;FreeBSD Foundation, and Quantum Leap Research&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Funding Status&lt;/cell&gt;
        &lt;cell&gt;Approved on September 27, 2024&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Program Sponsor&lt;/cell&gt;
        &lt;cell&gt;Ed Maste&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Program Manager&lt;/cell&gt;
        &lt;cell&gt;Alice Sowerby&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Objectives&lt;/cell&gt;
        &lt;cell&gt;Deliver a package of improved or new FreeBSD functionality that, together, will ensure that it runs well “out of the box” on a broad range of personal computing devices.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Organization goals&lt;/cell&gt;
        &lt;cell&gt;Laptop support and accessibility is a strategic priority for the FreeBSD Foundation to accelerate developer and corporate adoption, through: &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Output&lt;/cell&gt;
        &lt;cell&gt;Updates to FreeBSD 14.x and/or above that deliver contemporary WiFi, full audio, modern suspend and resume, improved graphics, Bluetooth, and other identified features. Documentation, and how-to guides for the new functionality.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Explore scope by area of functionality&lt;/p&gt;
    &lt;p&gt;Laptop and Desktop Working Group - (community owned)&lt;/p&gt;
    &lt;p&gt;Foundation blog about the Laptop Project&lt;/p&gt;
    &lt;p&gt;We have created discussion threads in the Desktop mailing list for key areas of the project:&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Power Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Hardware Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Audio Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Graphics Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] WiFi Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] System Management Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Security Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] User Testing Discussion Thread&lt;/p&gt;
    &lt;p&gt;Please come and join the discussion!&lt;/p&gt;
    &lt;p&gt;In total, $750,000 has been committed to a program of work to improve the experience of laptop users who run FreeBSD.&lt;/p&gt;
    &lt;p&gt;The program will start in Q4, 2024 and will likely run for 1-2 years.&lt;/p&gt;
    &lt;p&gt;The high-level scope was outlined by the FreeBSD Foundation with input from the community, including users such as program co-funder, Quantum Leap Research, and from laptop vendors including Dell, AMD and Framework.&lt;/p&gt;
    &lt;p&gt;The scope will be unpacked month by month as we make progress, focusing on where the most high-value functionality can be achieved with the resources and support that we have available. Our roadmap will contain work items that are candidates for future months.&lt;/p&gt;
    &lt;p&gt;No, these are high-level placeholders to help us visualise our intended order of work and to help share our plans with the community. The actual date of delivery on any item will be subject to change based on project progress and other factors.&lt;/p&gt;
    &lt;p&gt;The Foundation will be managing staff and a group of contracted FreeBSD developers to work on different functional areas to deliver regular updates to the laptop experience.&lt;/p&gt;
    &lt;p&gt;The FreeBSD community hosts a Laptop and Desktop Working Group where all interested parties can share their experiences, work in progress, and offer and receive help and support. You can also join the Desktop mailing list for more general updates. At present there is not a dedicated Laptop mailing list, this may change if there is community support for it.&lt;/p&gt;
    &lt;p&gt;Our target user is developers. However, we hope to be able to improve the experience for all users by reducing the need to "go under the hood" to set up, manage, and use FreeBSD on a laptop.&lt;/p&gt;
    &lt;p&gt;Broadly speaking this work is focused on laptop user experience. However, many of the areas that apply to laptops will also benefit the desktop user experience. We recommend engaging with the Laptop and Desktop Working Group to advocate for any desktop-specific work items.&lt;/p&gt;
    &lt;p&gt;We are mindful that UX is an important part of making FreeBSD functional and enjoyable for laptop users. We are framing the work as “user stories” that describe what a user wants to be able to accomplish and why. This is a user-focused approach to defining functional requirements.&lt;/p&gt;
    &lt;p&gt;There are several ways to keep yourself in the loop.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read the monthly updates that are posted into this repo.&lt;/item&gt;
      &lt;item&gt;Attend the Laptop and Desktop Working Group meetings. Work done as part of the program will be shared in these calls (these will also be recorded).&lt;/item&gt;
      &lt;item&gt;Check out the public roadmap on GitHub. We are developing a practice of keeping the program work up to date and available for anyone to see.&lt;/item&gt;
      &lt;item&gt;Sign up to the Desktop mailing list.&lt;/item&gt;
      &lt;item&gt;Sign up to the FreeBSD Foundation newsletter. All announcements about the program will be included in our updates.&lt;/item&gt;
      &lt;item&gt;Attend, or watch recordings of, the FreeBSD Foundation's Technology Team updates that are given at developer summits cohosted at conferences such as BSDCan, EuroBSDCon, and AsiaBSDCon.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are using this repo and associated GH project board as tools for capturing the roadmap and progress on work items at a high-level. We are not using it for source code management. The repo and project are read-only for the public.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/FreeBSDFoundation/proj-laptop"/><published>2025-12-19T14:56:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46326984</id><title>Garage – An S3 object store so reliable you can run it outside datacenters</title><updated>2025-12-19T20:40:57.782695+00:00</updated><content>&lt;doc fingerprint="dd8215339a80dd6"&gt;
  &lt;main&gt;
    &lt;p&gt;An S3 object store so reliable you can run it outside datacenters&lt;/p&gt;
    &lt;p&gt;Made for redundancy&lt;/p&gt;
    &lt;p&gt;Each chunk of data is replicated in 3 zones&lt;/p&gt;
    &lt;p&gt;We made it lightweight and kept the efficiency in mind:&lt;/p&gt;
    &lt;p&gt;We ship a single dependency-free binary that runs on all Linux distributions&lt;/p&gt;
    &lt;p&gt;We are sysadmins, we know the value of operator-friendly software&lt;/p&gt;
    &lt;p&gt;We do not have a dedicated backbone, and neither do you,&lt;lb/&gt; so we made software that run over the Internet across multiple datacenters&lt;/p&gt;
    &lt;p&gt;We worked hard to keep requirements as low as possible:&lt;/p&gt;
    &lt;p&gt;We built Garage to suit your existing infrastructure:&lt;/p&gt;
    &lt;p&gt; Garage implements the Amazon S3 API&lt;lb/&gt;and thus is already compatible with many applications. &lt;/p&gt;
    &lt;p&gt;Garage leverages insights from recent research in distributed systems:&lt;/p&gt;
    &lt;p&gt;Garage has benefitted multiple times from public funding:&lt;/p&gt;
    &lt;p&gt;If you want to participate in funding Garage development, either through donation or support contract, please get in touch with us.&lt;/p&gt;
    &lt;p&gt;This project has received funding from the European Union's Horizon 2021 research and innovation programme within the framework of the NGI-POINTER Project funded under grant agreement NÂ° 871528.&lt;/p&gt;
    &lt;p&gt;This project has received funding from the NGI Zero Entrust Fund, a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme, under the aegis of DG Communications Networks, Content and Technology under grant agreement No 101069594.&lt;/p&gt;
    &lt;p&gt;This project has received funding from the NGI Zero Commons Fund, a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme, under the aegis of DG Communications Networks, Content and Technology under grant agreement No 101135429.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://garagehq.deuxfleurs.fr/"/><published>2025-12-19T15:40:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46327133</id><title>Believe the Checkbook</title><updated>2025-12-19T20:40:57.636261+00:00</updated><content>&lt;doc fingerprint="b9a5135f4d93fc97"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Believe the Checkbook&lt;/head&gt;&lt;p&gt;AI companies talk as if engineering is over. Their acquisitions say the opposite.&lt;/p&gt;&lt;p&gt;Anthropic’s AI agent was the most prolific code contributor to Bun’s GitHub repository, submitting more merged pull requests than any human developer. Then Anthropic paid millions to acquire the human team anyway. The code was MIT-licensed; they could have forked it for free. Instead, they bought the people.&lt;/p&gt;&lt;p&gt;Everyone’s heard the line: “AI will write all the code; engineering as you know it is finished.”&lt;/p&gt;&lt;p&gt;Boards repeat it. CFOs love it. Some CTOs quietly use it to justify hiring freezes and stalled promotion paths.&lt;/p&gt;&lt;p&gt;The Bun acquisition blows a hole in that story.&lt;/p&gt;&lt;p&gt;Here’s a team whose project was open source, whose most active contributor was an AI agent, whose code Anthropic legally could have copied overnight. No negotiations. No equity. No retention packages.&lt;/p&gt;&lt;p&gt;Anthropic still fought competitors for the right to buy that group.&lt;/p&gt;&lt;p&gt;Publicly, AI companies talk like engineering is being automated away. Privately, they deploy millions of dollars to acquire engineers who already work with AI at full tilt. That contradiction is not a PR mistake. It is a signal.&lt;/p&gt;&lt;p&gt;The key constraint is obvious once you say it out loud. The bottleneck isn’t code production, it is judgment.&lt;/p&gt;&lt;p&gt;Anthropic’s own announcement barely talked about Bun’s existing codebase. It praised the team’s ability to rethink the JavaScript toolchain “from first principles”.&lt;/p&gt;&lt;p&gt;That’s investor-speak for: we’re paying for how these people think, what they choose not to build, which tradeoffs they make under pressure. They didn’t buy a pile of code. They bought a track record of correct calls in a complex, fast-moving domain.&lt;/p&gt;&lt;p&gt;AI drastically increases the volume of code you can generate. It does almost nothing to increase your supply of people who know which ten lines matter, which pull request should never ship, and which “clever” optimization will explode your latency or your reliability six months from now.&lt;/p&gt;&lt;p&gt;So when Anthropic’s own AI tops the contribution charts and they still decide the scarce asset is the human team, pay attention. That’s revealed preference.&lt;/p&gt;&lt;p&gt;Leaders don’t express their true beliefs in blog posts or conference quotes. They express them in hiring plans, acquisition targets, and compensation bands. If you want to understand what AI companies actually believe about engineering, follow the cap table, not the keynote.&lt;/p&gt;&lt;p&gt;So what do you do with this as a technical leader?&lt;/p&gt;&lt;p&gt;Stop using AI as an excuse to devalue your best knowledge workers. Use it to give them more leverage.&lt;/p&gt;&lt;p&gt;Treat AI as force multiplication for your highest-judgment people. The ones who can design systems, navigate ambiguity, shape strategy, and smell risk before it hits. They’ll use AI to move faster, explore more options, and harden their decisions with better data.&lt;/p&gt;&lt;p&gt;Double down on developing judgment, not just syntax speed: architecture, performance modeling, incident response, security thinking, operational literacy. The skills Anthropic implicitly paid for when it bought a team famous for rethinking the stack, not just writing another bundler.&lt;/p&gt;&lt;p&gt;Be careful about starving your junior pipeline based on “coding is over” narratives. As AI pushes routine work down, the gap between senior and everyone else widens. Companies that maintain a healthy apprenticeship ladder will own the next generation of high-judgment engineers while everyone else hunts the same shrinking senior pool at auction.&lt;/p&gt;&lt;p&gt;Most important: calibrate your strategy to revealed preferences, not marketing copy. When someone’s AI writes more code than their engineers but they still pay millions for the engineers, believe the transaction, not the tweet.&lt;/p&gt;&lt;p&gt;How did you like this article?&lt;/p&gt;&lt;p&gt;Enjoyed this article? Subscribe to get weekly insights on AI, technology strategy, and leadership. Completely free.&lt;/p&gt;Subscribe for Free&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://robertgreiner.com/believe-the-checkbook/"/><published>2025-12-19T15:51:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46327206</id><title>Graphite Is Joining Cursor</title><updated>2025-12-19T20:40:57.431142+00:00</updated><content>&lt;doc fingerprint="2586635ddc0e4d77"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Graphite is joining Cursor&lt;/head&gt;
    &lt;p&gt;The way developers write code looks different than it did a few years ago. But reviewing those changes, merging them safely, and collaborating on them has increasingly become the bottleneck for building production-grade software.&lt;/p&gt;
    &lt;p&gt;The team at Graphite has spent the past few years thinking deeply about these workflows and have built a code review platform used by hundreds of thousands of engineers at top engineering organizations. The boundary between where you write code and where you collaborate on it feels increasingly arbitrary, and there's a lot we think we can build by collapsing that distance.&lt;/p&gt;
    &lt;p&gt;We are excited to announce that Graphite has entered into a definitive agreement to be acquired by Cursor.&lt;/p&gt;
    &lt;p&gt;Graphite will continue to operate independently with the same team and product. Over the coming months, we'll explore connecting the two products in ways that we hope will feel natural: tighter integrations between local development and pull requests, smarter code review that learns from both systems, and some more radical ideas we can't share just yet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cursor.com/blog/graphite"/><published>2025-12-19T15:57:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46328109</id><title>Prepare for That Stupid World</title><updated>2025-12-19T20:40:56.923111+00:00</updated><content>&lt;doc fingerprint="423251d38bc5df2f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Prepare for That Stupid World&lt;/head&gt;
    &lt;p&gt;by Ploum on 2025-12-19&lt;/p&gt;
    &lt;p&gt;You probably heard about the Wall Street Journal story where they had a snack-vending machine run by a chatbot created by Anthropic.&lt;/p&gt;
    &lt;p&gt;At first glance, it is funny and it looks like journalists doing their job criticising the AI industry. If you are curious, the video is there (requires JS).&lt;/p&gt;
    &lt;p&gt;But what appears to be journalism is, in fact, pure advertising. For both WSJ and Anthropic. Look at how WSJ journalists are presented as "world class", how no-subtle the Anthropic guy is when telling them they are the best and how the journalist blush at it. If you are taking the story at face value, you are failing for the trap which is simple: "AI is not really good but funny, we must improve it."&lt;/p&gt;
    &lt;p&gt;The first thing that blew my mind was how stupid the whole idea is. Think for one second. One full second. Why do you ever want to add a chatbot to a snack vending machine? The video states it clearly: the vending machine must be stocked by humans. Customers must order and take their snack by themselves. The AI has no value at all.&lt;/p&gt;
    &lt;p&gt;Automated snack vending machine is a solved problem since nearly a century. Why do you want to make your vending machine more expensive, more error-prone, more fragile and less efficient for your customers?&lt;/p&gt;
    &lt;p&gt;What this video is really doing is normalising the fact that "even if it is completely stupid, AI will be everywhere, get used to it!"&lt;/p&gt;
    &lt;p&gt;The Anthropic guy himself doesn’t seem to believe his own lies, to the point of making me uncomfortable. Toward the ends, he even tries to warn us: "Claude AI could run your business but you don’t want to come one day and see you have been locked out." At which the journalist adds, "Or has ordered 100 PlayStations."&lt;/p&gt;
    &lt;p&gt;And then he gives up:&lt;/p&gt;
    &lt;p&gt;"Well, the best you can do is probably prepare for that world."&lt;/p&gt;
    &lt;p&gt;None of the world class journalists seemed to care. They are probably too badly paid for that. I was astonished to see how proud they were, having spent literally hours chatting with a bot just to get a free coke, even queuing for the privilege of having a free coke. A coke that cost a few minutes of minimum-wage work.&lt;/p&gt;
    &lt;p&gt;So the whole thing is advertising a world where chatbots will be everywhere and where world-class workers will do long queue just to get a free soda.&lt;/p&gt;
    &lt;p&gt;And the best advice about it is that you should probably prepare for that world.&lt;/p&gt;
    &lt;p&gt;I’m Ploum, a writer and an engineer. I like to explore how technology impacts society. You can subscribe by email or by rss. I value privacy and never share your adress.&lt;/p&gt;
    &lt;p&gt;I write science-fiction novels in French. For Bikepunk, my new post-apocalyptic-cyclist book, my publisher is looking for contacts in other countries to distribute it in languages other than French. If you can help, contact me!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ploum.net/2025-12-19-prepare-for-that-world.html"/><published>2025-12-19T17:01:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46328203</id><title>Show HN: I Made Loom for Mobile</title><updated>2025-12-19T20:40:56.804152+00:00</updated><content>&lt;doc fingerprint="eb963945dd1ed6b1"&gt;
  &lt;main&gt;
    &lt;p&gt;Record or stream any mobile website with your face cam and touch indicators. The only iOS app purpose-built for creating professional mobile demos.&lt;/p&gt;
    &lt;p&gt;Watch Demo Scope create a professional mobile demo in seconds.&lt;/p&gt;
    &lt;p&gt;Purpose-built for the mobile web. No compromises.&lt;/p&gt;
    &lt;p&gt;Show your face while demoing. Drag to reposition, pinch to resize, tap to change shape. Circle, square, or rectangle - you choose.&lt;/p&gt;
    &lt;p&gt;Every tap, swipe, and gesture is visible. Customizable colors and sizes. Your viewers will never lose track.&lt;/p&gt;
    &lt;p&gt;Load any URL directly in the app. Your product, your favorite sites, anything on the web - it all works seamlessly.&lt;/p&gt;
    &lt;p&gt;Stream directly to Twitch, YouTube, Facebook, or any custom RTMP server. Go live with face cam and touch indicators.&lt;/p&gt;
    &lt;p&gt;Web browser is just the start. Use photos or videos as your source. Perfect for reaction content.&lt;/p&gt;
    &lt;p&gt;From startup founders to professional streamers, Demo Scope is the tool you've been missing.&lt;/p&gt;
    &lt;p&gt;Demo your mobile web product with confidence. Show investors and users exactly how your app works, with your face explaining every feature.&lt;/p&gt;
    &lt;p&gt;Stream mobile web games, react to content, or share your browsing live. Face cam, touch indicators, direct to your audience.&lt;/p&gt;
    &lt;p&gt;Create engaging mobile tutorials for YouTube, TikTok, or your courses. Your face keeps viewers connected while touches guide them.&lt;/p&gt;
    &lt;p&gt;Demo Scope is specifically designed for mobile web content. If your app has a mobile web version, you can demo that. For native iOS apps, Apple's built-in screen recording is your best bet - but you won't get face cam or touch indicators.&lt;/p&gt;
    &lt;p&gt;Demo Scope supports YouTube Live, Twitch, Facebook Live, and any custom RTMP server. Just enter your stream key and RTMP URL, and you're live with face cam and touch indicators.&lt;/p&gt;
    &lt;p&gt;No! Demo Scope Pro is a one-time purchase. Buy once, use forever. No recurring fees, no annual renewals. We hate subscriptions too.&lt;/p&gt;
    &lt;p&gt;Everything! The free version includes all features - face cam, touch indicators, streaming, customization. The only limits are a 5-minute recording/streaming cap and a small watermark. Try it out, make sure it works for your use case, then upgrade when ready.&lt;/p&gt;
    &lt;p&gt;Absolutely. Once you upgrade to Pro (removing the watermark), your recordings are 100% yours to use however you want - YouTube, courses, client work, anything.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://demoscope.app"/><published>2025-12-19T17:08:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46328769</id><title>Show HN: Linggen – A local-first memory layer for your AI (Cursor, Zed, Claude)</title><updated>2025-12-19T20:40:56.305402+00:00</updated><content>&lt;doc fingerprint="1f15a051e1c0806e"&gt;
  &lt;main&gt;
    &lt;p&gt;The free and local app for your AI’s memory.&lt;/p&gt;
    &lt;p&gt;Linggen indexes your codebases and tribal knowledge so your AI (Cursor, Zed, Claude, etc.) can actually understand your architecture, cross-project dependencies, and long-term decisions.&lt;/p&gt;
    &lt;p&gt;Website • VS Code Extension • Documentation&lt;/p&gt;
    &lt;p&gt;Traditional AI chat is "blind" to anything you haven't manually copy-pasted. Linggen bridges this "context gap" by providing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🧠 Persistent Memory: Store architectural decisions in &lt;code&gt;.linggen/memory&lt;/code&gt;as Markdown. AI recalls them via semantic search.&lt;/item&gt;
      &lt;item&gt;🌐 Cross-Project Intelligence: Work on Project A while your AI learns design patterns or auth logic from Project B.&lt;/item&gt;
      &lt;item&gt;📊 System Map (Graph): Visualize file dependencies and "blast radius" before you refactor.&lt;/item&gt;
      &lt;item&gt;🔒 Local-First &amp;amp; Private: All indexing and vector search (via LanceDB) happens on your machine. Your code and embeddings never leave your side. No accounts required.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install the CLI in seconds and start indexing:&lt;/p&gt;
    &lt;code&gt;curl -sSL https://linggen.dev/install-cli.sh | bash
linggen start
linggen index .&lt;/code&gt;
    &lt;p&gt;Windows &amp;amp; Linux support coming soon.&lt;/p&gt;
    &lt;p&gt;Once Linggen is running and your project is indexed, simply talk to your MCP-enabled IDE (like Cursor or Zed):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Call Linggen MCP, find out how project-sender sends out messages, and ingest it."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;"Call Linggen MCP, load memory from Project-B, learn its code style and design pattern."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;"Load memory from Linggen, find out what is the goal of this piece of code."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;linggen: The core engine and CLI runtime.&lt;/item&gt;
      &lt;item&gt;linggen-vscode: VS Code extension for Graph View and automatic MCP setup.&lt;/item&gt;
      &lt;item&gt;linggensite: (This Repo) The landing page and documentation site.&lt;/item&gt;
      &lt;item&gt;linggen-releases: Pre-built binaries and distribution scripts.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linggen is open-source under the MIT License.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;100% Free for Individuals: Use it for all your personal and open-source projects.&lt;/item&gt;
      &lt;item&gt;Local-First: Your code and your "memory" never leave your machine.&lt;/item&gt;
      &lt;item&gt;Commercial Support: If you are a team (5+ users) or a company using Linggen in a professional environment, we ask that you support the project's development by purchasing a Commercial License.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For more details on future enterprise features (SSO, Team Sync, RBAC), visit our Pricing Page or get in touch via email.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Core Engine: Local indexing and semantic search (LanceDB).&lt;/item&gt;
      &lt;item&gt;MCP Support: Use with Cursor, Zed, and Claude.&lt;/item&gt;
      &lt;item&gt;Visual System Map: Graph visualization of your codebase.&lt;/item&gt;
      &lt;item&gt;Team Memory Sync: Share architectural decisions across your team.&lt;/item&gt;
      &lt;item&gt;Deep Integration: More IDEs and specialized agents.&lt;/item&gt;
      &lt;item&gt;Windows Support: Bringing the local engine to more platforms.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT © 2025 Linggen&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/linggen/linggen"/><published>2025-12-19T17:54:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46328992</id><title>Reverse Engineering US Airline's PNR System and Accessing All Reservations</title><updated>2025-12-19T20:40:56.198732+00:00</updated><content>&lt;doc fingerprint="1c9e33032c1e679d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Brute-Forceable Airline Reservation API Left Millions of Passenger Records Vulnerable&lt;/head&gt;
    &lt;head rend="h2"&gt;A 6-hour brute-force attack could have downloaded every Avelo Airline passenger's PII, Known Traveler Number, and payment data.&lt;/head&gt;
    &lt;p&gt;Timeline &amp;amp; Responsible Disclosure&lt;/p&gt;
    &lt;p&gt;Initial Contact: Upon discovering this vulnerability on October 15, 2025, I immediately reached out to security contacts at Avelo Airlines via email.&lt;/p&gt;
    &lt;p&gt;October 16, 2025: The Avelo cybersecurity team responded quickly and professionally. We had productive email exchanges where I detailed the vulnerability, including the lack of last name verification and rate limiting on reservation endpoints.&lt;/p&gt;
    &lt;p&gt;November 13, 2025: Avelo pushed a fix to production and notified me that the vulnerabilities were patched. I independently verified the fixes were in place before publication, and informed the Avelo team of my intention to write a technical blog post about this vulnerability, highlighting their cooperative and responsive approach to security disclosure.&lt;/p&gt;
    &lt;p&gt;Publication: November 20, 2025.&lt;/p&gt;
    &lt;p&gt;The Avelo team was responsive, professional, and took the findings seriously throughout the disclosure process. They acknowledged the severity, worked quickly to remediate the issues, and maintained clear communication. This is a model example of how organizations should handle security disclosures.&lt;/p&gt;
    &lt;p&gt;After my 9 AM Akkadian class, I sat down to change my flight out of New Haven with Avelo Airlines, and noticed that my computer was making some unusual requests. After digging a little further, I stepped into a landmine of customer information exposure. In the wrong hands, this critical vulnerability could allow an attacker to access full reservation details, including PII, government ID numbers, and partial payment info, for every Avelo passenger, past and present.&lt;/p&gt;
    &lt;p&gt;Before I walk you through my work on that Tuesday morning, let’s establish how airlines generally manage their reservations.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Airline Logins Should Work&lt;/head&gt;
    &lt;p&gt;Normally, to access a flight reservation (which often contains sensitive information like passport numbers, Known Traveler Numbers, and partial credit card data), you need at least two pieces of information: a confirmation code and the passenger’s last name.&lt;/p&gt;
    &lt;p&gt;This two-factor system is generally secure. The space of all 6-character alphanumeric confirmation codes combined with all possible last names is astronomically large, making it impossible to “guess” a valid pair.&lt;/p&gt;
    &lt;p&gt;But what if the last name check was missing?&lt;/p&gt;
    &lt;p&gt;Suddenly, the problem becomes much simpler. The entire keyspace an attacker needs to guess is just the confirmation code. In Avelo’s case, their codes are 6-character alphanumeric strings (&lt;code&gt;[A-Z0-9]&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Let’s do the math:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Keyspace: 36 characters (26 letters + 10 digits)&lt;/item&gt;
      &lt;item&gt;Length: 6&lt;/item&gt;
      &lt;item&gt;Total Combinations: 36^6 = 2,176,782,336 (~2.18 billion)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That’s a big number, but it’s not “astronomically large.” It’s well within the reach of a modern brute-force attack.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Attack Timeline&lt;/head&gt;
    &lt;p&gt;How long would it take to try all 2.18 billion combinations? The time is just &lt;code&gt;2.18 billion / (requests per second)&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;At 1,000 req/s (a modest script): 2.18 million seconds, or ~25 days.&lt;/item&gt;
      &lt;item&gt;At 10,000 req/s (a decent server): 218,000 seconds, or ~2.5 days.&lt;/item&gt;
      &lt;item&gt;At 100,000 req/s (a small cluster of servers, costing $400-$700)1: 21,800 seconds, or ~6 hours.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bottom line: If Avelo’s flight system has no rate limiting and doesn’t require a last name, an adversary could extract all passenger data in about 6 hours for less than a thousand dollars.&lt;/p&gt;
    &lt;head rend="h3"&gt;Even Faster Than 6 Hours&lt;/head&gt;
    &lt;p&gt;Even worse, they don’t need to run for 6 hours. With an estimated 8 million tickets sold, the “hit rate” is roughly 1 in every 270 guesses (2.18B / 8M). An attacker would start getting valid PII back in seconds.&lt;/p&gt;
    &lt;head rend="h2"&gt;Back to the Story: Finding the Flaw&lt;/head&gt;
    &lt;p&gt;This was all just theory until I looked at my network traffic. As I was changing my reservation, I saw a GET request to an API endpoint:&lt;/p&gt;
    &lt;code&gt;https://www.aveloair.com/payment/services/reservation/{code}
&lt;/code&gt;
    &lt;p&gt;The parameter at the end didn’t seem like a reservation code, but the response contained all relevant reservation data, so I decided to probe further. On a hunch, I swapped that token for my actual 6-character code and re-sent the request.&lt;/p&gt;
    &lt;p&gt;Voila. The server responded with a massive JSON object containing my entire reservation.&lt;/p&gt;
    &lt;p&gt;This endpoint wasn’t asking for my last name. The only other security was a standard authentication cookie… but was that cookie tied to my reservation?&lt;/p&gt;
    &lt;p&gt;I quickly texted a friend for their old Avelo confirmation code. I plugged it into the URL, kept my own cookie, and hit send. But there was no way it could poss-&lt;/p&gt;
    &lt;p&gt;It worked.&lt;/p&gt;
    &lt;p&gt;I was looking at their full reservation. Any valid authentication cookie could be used to query any reservation, using only the 6-character code. The theoretical flaw was real.&lt;/p&gt;
    &lt;head rend="h2"&gt;Executing the Attack: No Rate Limiting&lt;/head&gt;
    &lt;p&gt;The only remaining (partial) defense was rate-limiting. I wrote a quick multi-threaded Python script to generate random 6-character codes and hit the endpoint.&lt;/p&gt;
    &lt;p&gt;The requests flew. There was no WAF, no IP blocking, no CAPTCHA.&lt;/p&gt;
    &lt;p&gt;The script quickly finding valid reservation codes&lt;/p&gt;
    &lt;p&gt;Within minutes, my script was logging hundreds of valid reservations. Troves of data were being returned, including from passengers flying on government business with &lt;code&gt;@dot.gov&lt;/code&gt; and &lt;code&gt;@faa.gov&lt;/code&gt; email addresses.&lt;/p&gt;
    &lt;p&gt;A successful hit returned the entire reservation object. This was a complete data breach for each passenger – including myself!&lt;/p&gt;
    &lt;p&gt;(Note: During further testing, I discovered a similar vulnerability on a different reservation endpoint. I promptly notified the Avelo team, and they patched that endpoint as well before publication.)&lt;/p&gt;
    &lt;head rend="h2"&gt;What Data Was Leaked?&lt;/head&gt;
    &lt;p&gt;For every valid code, the API returned:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full Passenger PII: &lt;code&gt;FullName&lt;/code&gt;,&lt;code&gt;DateOfBirth&lt;/code&gt;,&lt;code&gt;Gender&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Government IDs: &lt;code&gt;IDDocuments.IDNumber&lt;/code&gt;(this field contained Known Traveler Numbers (KNTs) and, in other cases, Passport Numbers)&lt;/item&gt;
      &lt;item&gt;Contact Info: phone numbers, email addresses&lt;/item&gt;
      &lt;item&gt;Full Itinerary: Flight numbers, dates, times, and &lt;code&gt;SeatLocation&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Payment Details: &lt;code&gt;CardNumber&lt;/code&gt;(masked:&lt;code&gt;************8&lt;/code&gt;),&lt;code&gt;DateTimeExpiration&lt;/code&gt;, and billing&lt;code&gt;Address.PostalCode&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Vouchers: &lt;code&gt;PaymentInternals.AccountNumber&lt;/code&gt;and&lt;code&gt;Amount.Value&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;PCI Data: &lt;code&gt;PaymentCards.TrackData&lt;/code&gt;— This field seemed to contain partial magnetic-stripe data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example of exposed payment card data returned by the API&lt;/p&gt;
    &lt;p&gt;Example of exposed Known Traveler Number (KNT) and other PII in API response&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fallout&lt;/head&gt;
    &lt;p&gt;This flaw was critical. An attacker could:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run the 6-hour brute-force attack to enumerate millions of valid passenger reservation codes (PNRs) — or simply run the script for a few minutes and start harvesting valid passenger data immediately&lt;/item&gt;
      &lt;item&gt;Extract comprehensive PII including full names, dates of birth, contact information, flight itineraries, and government ID numbers (Known Traveler Numbers and passport numbers) for identity theft and fraud&lt;/item&gt;
      &lt;item&gt;Access partial payment card data including last 4 digits, expiration dates, and billing zip codes&lt;/item&gt;
      &lt;item&gt;View complete travel history and passenger boarding status&lt;/item&gt;
      &lt;item&gt;Modify or cancel all Avelo passengers’ reservations, causing widespread travel disruption&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I immediately disclosed this to the Avelo team. They were responsive, professional, and took the findings seriously, patching the issues promptly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key Takeaways&lt;/head&gt;
    &lt;p&gt;This incident is a stark reminder of how critical simple security checks are. A single missing &lt;code&gt;lastName&lt;/code&gt; check and an absent rate-limit configuration exposed millions of sensitive passenger records to trivial enumeration.&lt;/p&gt;
    &lt;p&gt;For developers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Always require multiple factors for accessing sensitive data (e.g., confirmation code + last name)&lt;/item&gt;
      &lt;item&gt;Implement rate limiting on all enumerable endpoints&lt;/item&gt;
      &lt;item&gt;Ensure authentication cookies are properly scoped to user sessions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m glad we could get this fixed, and I hope this write-up helps other developers avoid similar pitfalls.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;AWS Lambda: requests billed at $0.20 per million plus compute billed per GB‑second; at 2.18B requests, request charges are about 2,176.8 million × $0.20 ≈ $435 ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://alexschapiro.com/security/vulnerability/2025/11/20/avelo-airline-reservation-api-vulnerability"/><published>2025-12-19T18:15:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46329038</id><title>TP-Link Tapo C200: Hardcoded Keys, Buffer Overflows and Privacy</title><updated>2025-12-19T20:40:55.979948+00:00</updated><content>&lt;doc fingerprint="2540d911803d63cc"&gt;
  &lt;main&gt;
    &lt;p&gt;Hi friends and welcome to the last post for this year! Whenever someone asks me how to get started with reverse engineering, I always give the same advice: buy the cheapest IP camera you can find. These devices are self-contained little ecosystems - they have firmware you can extract, network protocols you can sniff, and mobile apps you can decompile. Chances are, you’ll find something interesting. At worst, you’ll learn a lot about assembly and embedded systems. At best, you’ll find some juicy vulnerability and maybe learn how to exploit it!&lt;/p&gt;
    &lt;p&gt;I own several TP-Link Tapo C200 cameras myself. They’re cheap (less than 20 EUR from Italy), surprisingly stable, and I genuinely like them - they just work. One weekend, I decided just for fun to take my own advice. The Tapo C200 has been around for a while and has had a few CVEs discovered and more or less patched over the years, so I honestly wasn’t expecting to find much in the latest firmware. However, I wanted to use this chance to perform some AI assisted reverse engineering and test whether I could still find anything at all.&lt;/p&gt;
    &lt;p&gt;I documented the entire process live on Arcadia - my thought process, the dead ends, the AI prompts that worked and the ones that didn’t. If you want the raw, unfiltered version with screenshots and videos of things crashing, go check that out.&lt;/p&gt;
    &lt;p&gt;This post is the cleaned-up version of that journey, where I wanted to show how I approach firmware analysis these days, now that we have AI. You will notice that in several instances I will be particularly lazy and delegate to AI things I could have done manually and/or inferred myself after some more work. Keep in mind that while I am generally lazy, this was also an experiment in integrating and documenting how effective AI can be for security research and reverse engineering, and especially in making them accessible to less experienced/sophisticated researchers/attackers.&lt;/p&gt;
    &lt;p&gt;What started as a lazy weekend project turned into finding a few security vulnerabilities that affect about 25,000 of these devices directly exposed on the internet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting the Firmware&lt;/head&gt;
    &lt;head rend="h3"&gt;Tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Old friend JD-GUI to reverse the Android app and get a sense of things&lt;/item&gt;
      &lt;item&gt;The AWS CLI to download the firmware image.&lt;/item&gt;
      &lt;item&gt;binwalk for firmware inspection.&lt;/item&gt;
      &lt;item&gt;Grok to give a quick AI assisted look into prior research.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first step is always obtaining the firmware binary file and this time it was super easy! After some basic reversing of the Tapo Android app, I found out that TP-Link have their entire firmware repository in an open S3 bucket. No authentication required. So, you can list and download every version of every firmware they’ve ever released for any device they ever produced:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;$ aws s3 ls s3://download.tplinkcloud.com/ --no-sign-request --recursive&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The entire output is here, for the curious. This provides access to the firmware image of every TP-Link device - routers, cameras, smart plugs, you name it. A reverse engineer’s candy store.&lt;/p&gt;
    &lt;p&gt;I grabbed version 1.4.2 Build 250313 Rel.40499n for the C200 (Hardware Revision 3), named &lt;code&gt;Tapo_C200v3_en_1.4.2_Build_250313_Rel.40499n_up_boot-signed_1747894968535.bin&lt;/code&gt;, and started poking around. However, the first attempt at identifying its format via binwalk was not successful, indicating that some sort of encryption or obfuscation was in place.&lt;/p&gt;
    &lt;p&gt;And here is where I started using AI. I used Grok to do some deep research on how to decrypt the firmware for these cameras. Since I knew other hackers worked on this before, I delegated searching into hundreds of relevant web pages to the AI:&lt;/p&gt;
    &lt;head rend="h3"&gt;Decrypting the Firmware&lt;/head&gt;
    &lt;head rend="h3"&gt;Tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The tp-link-decrypt tool to decrypt the firmware image.&lt;/item&gt;
      &lt;item&gt;binwalk for firmware inspection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks to Grok, the tp-link-decrypt tool and the fact that every firmware image for every device seems to be encrypted the same exact way, we can now decrypt the firmware. The tool extracts RSA keys from TP-Link’s own GPL code releases - they publish the decryption keys themselves as part of their open source obligations.&lt;/p&gt;
    &lt;p&gt;Credits to @watchfulip for the original extensive TP-Link firmware research and @tangrs for finding that the relevant binaries are published in TP-Link GPL code dumps and how to extract keys from them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;$ git clone https://github.com/robbins/tp-link-decrypt&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;After decryption, the firmware revealed a fairly standard structure: a bootloader, a kernel, and a SquashFS root filesystem.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;$ binwalk -e Tapo_C200_v3_1.4.2_decrypted.bin&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Hunting for Bugs&lt;/head&gt;
    &lt;head rend="h3"&gt;Tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ghidra to decompile and understand the MIPS binaries&lt;/item&gt;
      &lt;item&gt;GhidraMCP to let an AI connect to my running Ghidra instance and support me in the process.&lt;/item&gt;
      &lt;item&gt;Cline to ask AI to explore the filesystem and find interesting components.&lt;/item&gt;
      &lt;item&gt;A mix of Anthropic's Opus and Sonnet 4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once extracted, I used AI and Cline to explore the filesystem in search of which components handle the discovery protocol, camera web API, video streaming, etc all discovered earlier while reversing the Android app.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Claude Opus 4: "this is the firmware of an ipcam, i'm trying to find where the webapp that serves the API is managed" pic.twitter.com/NrgtKGUD8h&lt;/p&gt;— Simone Margaritelli (@evilsocket) July 18, 2025&lt;/quote&gt;
    &lt;p&gt;Loading Ghidra and giving a quick look at the &lt;code&gt;tp_manage&lt;/code&gt; binary, revealed the first interesting thing:&lt;/p&gt;
    &lt;p&gt;This private key is not generated at boot. Similarly to CVE-2025-1099 for the C500, the C200 embeds in its firmware the private key that serves the SSL for a few APIs. If you’re on the same network as a camera, you can MitM and decrypt their HTTPS traffic with keys you extracted from the firmware image - without ever touching the hardware. For a security camera streaming video of people’s homes, this is… not ideal.&lt;/p&gt;
    &lt;p&gt;I kept loading the other interesting binaries and exploring them in Ghidra using AI to quickly get a sense of the main features and possible entry points for an attacker.&lt;/p&gt;
    &lt;p&gt;Asking AI to explain a function and its relation to the other functions proved to be very useful for instance to understand encryption / obfuscation routines and network protocol handlers. This allows you to go from here:&lt;/p&gt;
    &lt;p&gt;To a higher level understanding that the AI can provide:&lt;/p&gt;
    &lt;p&gt;Another technique I found particularly effective is asking the AI to analyze a given function of interest and rename its variables and parameters to something meaningful based on context. Then do the same for the functions it calls, recursively following the branches you’re interested in. After a few iterations, what started as &lt;code&gt;FUN_0042eb7c(undefined2 *param_1, undefined4 param_2, int param_3)&lt;/code&gt; becomes &lt;code&gt;handleConnectAp(connection *conn, int flags, json *params)&lt;/code&gt; - and suddenly the decompiled code reads almost like the original source. &lt;/p&gt;
    &lt;p&gt;This iterative refinement approach, which I find a great example of human-AI collaboration where neither alone would be as efficient, is how I mapped most of the HTTP handlers, discovery protocol, and so on. What follows is the bottom line of my findings. For more details on the process, refer to the original Discord thread.&lt;/p&gt;
    &lt;p&gt;As a side note, I did not investigate (much) the exploitability of the following bugs to achieve code execution, mostly because I’m not familiar with MIPS, and it was not my intent. You can however do it relatively easily once obtained a shell via physical access, due to the presence of the &lt;code&gt;/bin/gdbserver&lt;/code&gt; binary in the firmware.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bug 1: Pre-Auth ONVIF SOAP XML Parser Memory Overflow&lt;/head&gt;
    &lt;p&gt;The Tapo C200 exposes an ONVIF service via the &lt;code&gt;/bin/main&lt;/code&gt; server listening on port 2020 for interoperability with standard video management systems. The problem is in how it parses SOAP XML requests.&lt;/p&gt;
    &lt;p&gt;When processing XML elements, the parser (&lt;code&gt;soap_parse_and_validate_request&lt;/code&gt; at &lt;code&gt;0x0045ae8c&lt;/code&gt;) calls &lt;code&gt;ds_parse&lt;/code&gt; without any bounds checking on the number of elements or total memory allocation. Send it enough XML elements, and you’ll overflow allocated memory.&lt;/p&gt;
    &lt;p&gt;Here’s the PoC:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;#!/usr/bin/env python3&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Send this, and the camera crashes, requiring a power cycle to recover.&lt;/p&gt;
    &lt;quote&gt;— Simone Margaritelli (@evilsocket) July 19, 2025&lt;/quote&gt;
    &lt;head rend="h2"&gt;Bug 2: Pre-Auth HTTPS Content-Length Integer Overflow&lt;/head&gt;
    &lt;p&gt;The HTTPS server routine running on port 443 has a classic integer overflow in its &lt;code&gt;Content-Length&lt;/code&gt; header parsing. The vulnerable function at &lt;code&gt;0x004bd054&lt;/code&gt; does this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;iVar1 = atoi(value);&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That’s it. No bounds checking. No validation. Just raw &lt;code&gt;atoi()&lt;/code&gt; on user input.&lt;/p&gt;
    &lt;p&gt;On a 32-bit system, &lt;code&gt;atoi("4294967295")&lt;/code&gt; causes integer overflow, resulting in undefined behavior. In this case, the camera crashes:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;#!/usr/bin/env python3&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;&lt;p&gt;And two pic.twitter.com/tt7eL7MA27&lt;/p&gt;— Simone Margaritelli (@evilsocket) July 19, 2025&lt;/quote&gt;
    &lt;p&gt;Another crash 💪&lt;/p&gt;
    &lt;head rend="h2"&gt;Bug 3: Pre-Auth WiFi Hijacking&lt;/head&gt;
    &lt;p&gt;The camera exposes an API endpoint called &lt;code&gt;connectAp&lt;/code&gt; that’s used during initial setup to configure WiFi. The problem? It’s accessible without any authentication. Even after the camera is fully set up and connected to your network.&lt;/p&gt;
    &lt;p&gt;The vulnerable handler at &lt;code&gt;0x0042eb7c&lt;/code&gt; processes the request without any auth checks:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;void connectApHandler(undefined2 *param_1,undefined4 param_2,int json_params)&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;&lt;p&gt;And three! 🚀 pic.twitter.com/2GZiG4bTm0&lt;/p&gt;— Simone Margaritelli (@evilsocket) July 22, 2025&lt;/quote&gt;
    &lt;p&gt;The exploit is trivial:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;#!/usr/bin/env python3&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This allows a remote attacker to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disconnect the camera from its legitimate network (DoS)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If in WiFi range proximity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Force it to connect to an attacker-controlled network (MitM)&lt;/item&gt;
      &lt;item&gt;Intercept all video traffic once on the malicious network (not that we really needed this since the HTTPS private key is shared by all devices, as mentioned earlier XD)&lt;/item&gt;
      &lt;item&gt;Maintain persistent access even if the owner changes their WiFi password&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Bug 4: Pre-Auth Nearby WiFi Network Scanning&lt;/head&gt;
    &lt;p&gt;Related to Bug 3, the &lt;code&gt;scanApList&lt;/code&gt; method is also accessible without authentication - even when the device is not in onboarding mode. This endpoint returns a list of all WiFi networks visible to the camera:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;#!/usr/bin/env python3&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A test on one of the devices exposed on the internet:&lt;/p&gt;
    &lt;p&gt;This is particularly concerning given the number of these devices exposed on the internet. An attacker can remotely enumerate WiFi networks in the camera’s vicinity, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SSIDs of nearby networks&lt;/item&gt;
      &lt;item&gt;BSSIDs (MAC addresses of access points)&lt;/item&gt;
      &lt;item&gt;Signal strength (useful for triangulation)&lt;/item&gt;
      &lt;item&gt;Security configurations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s where it gets worse: tools like apple_bssid_locator can query Apple’s location services API with a BSSID and return precise GPS coordinates.&lt;/p&gt;
    &lt;p&gt;This means an attacker can:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find an exposed Tapo camera via services like ZoomEye, Shodan or similar indexes&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;scanApList&lt;/code&gt;to retrieve nearby WiFi BSSIDs&lt;/item&gt;
      &lt;item&gt;Query Apple’s location database with those BSSIDs&lt;/item&gt;
      &lt;item&gt;Pinpoint the camera’s physical location to within a few meters&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Remote attackers can not only see what WiFi networks exist around a camera - they can determine exactly where that camera (and by extension, the home or business it’s monitoring) is located on a map.&lt;/p&gt;
    &lt;head rend="h2"&gt;Disclosure&lt;/head&gt;
    &lt;p&gt;I’ve decided to follow the industry standard 90+30 days responsible disclosure process; here’s the timeline:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;July 22, 2025: Sent initial report to TP-Link’s security team (security@tp-link.com) with full technical details, PoC exploits and videos. All compiled according to their guidelines.&lt;/item&gt;
      &lt;item&gt;July 22, 2025: Acknowledgment received.&lt;/item&gt;
      &lt;item&gt;August 22, 2025: TP-Link confirms they’re still reviewing the report&lt;/item&gt;
      &lt;item&gt;September 27, 2025: TP-Link responds and sets the timeline for the remediation patch to the end of November 2025.&lt;/item&gt;
      &lt;item&gt;November 2025: Nothing happens.&lt;/item&gt;
      &lt;item&gt;December 1, 2025: Sent follow up email, no response.&lt;/item&gt;
      &lt;item&gt;December 4, 2025: Sent another follow up email, which TP-Link responds to, further postponing the patch to the following week.&lt;/item&gt;
      &lt;item&gt;The following week: Nothing happens.&lt;/item&gt;
      &lt;item&gt;December 19, 2025: Public disclosure after 150 days.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The 90+30 period has long passed, so I decided to publish this writeup.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conflict Of Interest&lt;/head&gt;
    &lt;p&gt;As of April 25, TP-Link is a CVE Numbering Authority (CNA). This means they have the authority to assign CVE identifiers for vulnerabilities in their own products - at least for the ones reported directly to them. And they actively encourage responsible disclosure directly to their security team, which means they control a considerable pipeline of vulnerability reports.&lt;/p&gt;
    &lt;p&gt;On their Security Commitment page, TP-Link prominently displays charts comparing their CVE count to competitors. They explicitly market themselves as having fewer CVEs than Cisco, Netgear, and D-Link. They state they “aim to patch vulnerabilities within 90 days.”&lt;/p&gt;
    &lt;p&gt;There’s an obvious and structural conflict of interest when a vendor is allowed to be their own CNA while simultaneously using their CVE count as a marketing metric.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.evilsocket.net/2025/12/18/TP-Link-Tapo-C200-Hardcoded-Keys-Buffer-Overflows-and-Privacy-in-the-Era-of-AI-Assisted-Reverse-Engineering/"/><published>2025-12-19T18:19:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46329530</id><title>Performance Hints – Jeff Dean and Sanjay Ghemawat</title><updated>2025-12-19T20:40:55.460927+00:00</updated><content>&lt;doc fingerprint="ee11ba790e091688"&gt;
  &lt;main&gt;&lt;p&gt;Original version: 2023/07/27, last updated: 2025/12/16&lt;/p&gt;&lt;p&gt;Over the years, we (Jeff &amp;amp; Sanjay) have done a fair bit of diving into performance tuning of various pieces of code, and improving the performance of our software has been important from the very earliest days of Google, since it lets us do more for more users. We wrote this document as a way of identifying some general principles and specific techniques that we use when doing this sort of work, and tried to pick illustrative source code changes (change lists, or CLs) that provide examples of the various approaches and techniques. Most of the concrete suggestions below reference C++ types and CLs, but the general principles apply to other languages. The document focuses on general performance tuning in the context of a single binary, and does not cover distributed systems or machine learning (ML) hardware performance tuning (huge areas unto themselves). We hope others will find this useful.&lt;/p&gt;&lt;p&gt;Many of the examples in the document have code fragments that demonstrate the techniques (click the little triangles!). Note that some of these code fragments mention various internal Google codebase abstractions. We have included these anyway if we felt like the examples were self-contained enough to be understandable to those unfamiliar with the details of those abstractions.&lt;/p&gt;&lt;p&gt;Knuth is often quoted out of context as saying premature optimization is the root of all evil. The full quote reads: “We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.” This document is about that critical 3%, and a more compelling quote, again from Knuth, reads:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The improvement in speed from Example 2 to Example 2a is only about 12%, and many people would pronounce that insignificant. The conventional wisdom shared by many of today’s software engineers calls for ignoring efficiency in the small; but I believe this is simply an overreaction to the abuses they see being practiced by penny-wise-and-pound-foolish programmers, who can’t debug or maintain their “optimized” programs. In established engineering disciplines a 12% improvement, easily obtained, is never considered marginal; and I believe the same viewpoint should prevail in software engineering. Of course I wouldn’t bother making such optimizations on a one-shot job, but when it’s a question of preparing quality programs, I don’t want to restrict myself to tools that deny me such efficiencies.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Many people will say “let’s write down the code in as simple a way as possible and deal with performance later when we can profile”. However, this approach is often wrong:&lt;/p&gt;&lt;p&gt;Instead, we suggest that when writing code, try to choose the faster alternative if it does not impact readability/complexity of the code significantly.&lt;/p&gt;&lt;p&gt;If you can develop an intuition for how much performance might matter in the code you are writing, you can make a more informed decision (e.g., how much extra complexity is warranted in the name of performance). Some tips on estimating performance while you are writing code:&lt;/p&gt;&lt;p&gt;You can do a slightly deeper analysis when picking between options with potentially different performance characteristics by relying on back of the envelope calculations. Such calculations can quickly give a very rough estimate of the performance of different alternatives, and the results can be used to discard some of the alternatives without having to implement them.&lt;/p&gt;&lt;p&gt;Here is how such an estimation might work:&lt;/p&gt;&lt;p&gt;The following table, which is an updated version of a table from a 2007 talk at Stanford University (video of the 2007 talk no longer exists, but there is a video of a related 2011 Stanford talk that covers some of the same content) may be useful since it lists the types of operations to consider, and their rough cost:&lt;/p&gt;&lt;code&gt;L1 cache reference                             0.5 ns
L2 cache reference                             3 ns
Branch mispredict                              5 ns
Mutex lock/unlock (uncontended)               15 ns
Main memory reference                         50 ns
Compress 1K bytes with Snappy              1,000 ns
Read 4KB from SSD                         20,000 ns
Round trip within same datacenter         50,000 ns
Read 1MB sequentially from memory         64,000 ns
Read 1MB over 100 Gbps network           100,000 ns
Read 1MB from SSD                      1,000,000 ns
Disk seek                              5,000,000 ns
Read 1MB sequentially from disk       10,000,000 ns
Send packet CA-&amp;gt;Netherlands-&amp;gt;CA      150,000,000 ns
&lt;/code&gt;&lt;p&gt;The preceding table contains rough costs for some basic low-level operations. You may find it useful to also track estimated costs for higher-level operations relevant to your system. E.g., you might want to know the rough cost of a point read from your SQL database, the latency of interacting with a Cloud service, or the time to render a simple HTML page. If you don’t know the relevant cost of different operations, you can’t do decent back-of-the-envelope calculations!&lt;/p&gt;&lt;p&gt;As a rough approximation, a good quicksort algorithm makes log(N) passes over an array of size N. On each pass, the array contents will be streamed from memory into the processor cache, and the partition code will compare each element once to a pivot element. Let’s add up the dominant costs:&lt;/p&gt;&lt;p&gt;If necessary, we could refine our analysis to account for processor caches. This refinement is probably not needed since branch mispredictions are the dominant cost according to the analysis above, but we include it here anyway as another example. Let’s assume we have a 32MB L3 cache, and that the cost of transferring data from L3 cache to the processor is negligible. The L3 cache can hold 2^23 numbers, and therefore the last 22 passes can operate on the data resident in the L3 cache (the 23rd last pass brings data into the L3 cache and the remaining passes operate on that data.) That cuts down the memory transfer cost to 2.5 seconds (10 memory transfers of 4GB at 16GB/s) instead of 7.5 seconds (30 memory transfers).&lt;/p&gt;&lt;p&gt;Let’s compare two potential designs where the original images are stored on disk, and each image is approximately 1MB in size.&lt;/p&gt;&lt;p&gt;The preceding section gives some tips about how to think about performance when writing code without worrying too much about how to measure the performance impact of your choices. However, before you actually start making improvements, or run into a tradeoff involving various things like performance, simplicity, etc. you will want to measure or estimate potential performance benefits. Being able to measure things effectively is the number one tool you’ll want to have in your arsenal when doing performance-related work.&lt;/p&gt;&lt;p&gt;As an aside, it’s worth pointing out that profiling code that you’re unfamiliar with can also be a good way of getting a general sense of the structure of the codebase and how it operates. Examining the source code of heavily involved routines in the dynamic call graph of a program can give you a high level sense of “what happens” when running the code, which can then build your own confidence in making performance-improving changes in slightly unfamiliar code.&lt;/p&gt;&lt;p&gt;Many useful profiling tools are available. A useful tool to reach for first is pprof since it gives good high level performance information and is easy to use both locally and for code running in production. Also try perf if you want more detailed insight into performance.&lt;/p&gt;&lt;p&gt;Some tips for profiling:&lt;/p&gt;&lt;p&gt;Use a benchmark library to emit performance counter readings both for better precision, and to get more insight into program behavior.&lt;/p&gt;&lt;p&gt;You will often run into situations where your CPU profile is flat (there is no obvious big contributor to slowness). This can often happen when all low-hanging fruit has been picked. Here are some tips to consider if you find yourself in this situation:&lt;/p&gt;&lt;p&gt;Some of the techniques suggested below require changing data structures and function signatures, which may be disruptive to callers. Try to organize code so that the suggested performance improvements can be made inside an encapsulation boundary without affecting public interfaces. This will be easier if your modules are deep (significant functionality accessed via a narrow interface).&lt;/p&gt;&lt;p&gt;Widely used APIs come under heavy pressure to add features. Be careful when adding new features since these will constrain future implementations and increase cost unnecessarily for users who don’t need the new features. E.g., many C++ standard library containers promise iterator stability, which in typical implementations increases the number of allocations significantly, even though many users do not need pointer stability.&lt;/p&gt;&lt;p&gt;Some specific techniques are listed below. Consider carefully the performance benefits vs. any API usability issues introduced by such changes.&lt;/p&gt;&lt;p&gt;Provide bulk ops to reduce expensive API boundary crossings or to take advantage of algorithmic improvements.&lt;/p&gt;&lt;p&gt;Add bulk MemoryManager::LookupMany interface.&lt;/p&gt;&lt;p&gt;In addition to adding a bulk interface, this also simplified the signature for the new bulk variant: it turns out clients only needed to know if all the keys were found, so we can return a bool rather than a Status object.&lt;/p&gt;&lt;p&gt;memory_manager.h&lt;/p&gt;&lt;code&gt;class MemoryManager {
 public:
  ...
  util::StatusOr&amp;lt;LiveTensor&amp;gt; Lookup(const TensorIdProto&amp;amp; id);
&lt;/code&gt;&lt;code&gt;class MemoryManager {
 public:
  ...
  util::StatusOr&amp;lt;LiveTensor&amp;gt; Lookup(const TensorIdProto&amp;amp; id);

  // Lookup the identified tensors
  struct LookupKey {
    ClientHandle client;
    uint64 local_id;
  };
  bool LookupMany(absl::Span&amp;lt;const LookupKey&amp;gt; keys,
                  absl::Span&amp;lt;tensorflow::Tensor&amp;gt; tensors);
&lt;/code&gt;&lt;p&gt;Add bulk ObjectStore::DeleteRefs API to amortize locking overhead.&lt;/p&gt;&lt;p&gt;object_store.h&lt;/p&gt;&lt;code&gt;template &amp;lt;typename T&amp;gt;
class ObjectStore {
 public:
  ...
  absl::Status DeleteRef(Ref);
&lt;/code&gt;&lt;code&gt;template &amp;lt;typename T&amp;gt;
class ObjectStore {
 public:
  ...
  absl::Status DeleteRef(Ref);

  // Delete many references.  For each ref, if no other Refs point to the same
  // object, the object will be deleted.  Returns non-OK on any error.
  absl::Status DeleteRefs(absl::Span&amp;lt;const Ref&amp;gt; refs);
  ...
template &amp;lt;typename T&amp;gt;
absl::Status ObjectStore&amp;lt;T&amp;gt;::DeleteRefs(absl::Span&amp;lt;const Ref&amp;gt; refs) {
  util::Status result;
  absl::MutexLock l(&amp;amp;mu_);
  for (auto ref : refs) {
    result.Update(DeleteRefLocked(ref));
  }
  return result;
}
&lt;/code&gt;&lt;p&gt;memory_tracking.cc&lt;/p&gt;&lt;code&gt;void HandleBatch(int, const plaque::Batch&amp;amp; input) override {
  for (const auto&amp;amp; t : input) {
    auto in = In(t);
    PLAQUE_OP_ASSIGN_OR_RETURN(const auto&amp;amp; handles, in.handles());
    for (const auto handle : handles.value-&amp;gt;handles()) {
      PLAQUE_OP_RETURN_IF_ERROR(in_buffer_store_
                                    ? bstore_-&amp;gt;DeleteRef(handle)
                                    : tstore_-&amp;gt;DeleteRef(handle));
    }
  }
}
&lt;/code&gt;&lt;code&gt;void HandleBatch(int, const plaque::Batch&amp;amp; input) override {
  for (const auto&amp;amp; t : input) {
    auto in = In(t);
    PLAQUE_OP_ASSIGN_OR_RETURN(const auto&amp;amp; handles, in.handles());
    if (in_buffer_store_) {
      PLAQUE_OP_RETURN_IF_ERROR(
          bstore_-&amp;gt;DeleteRefs(handles.value-&amp;gt;handles()));
    } else {
      PLAQUE_OP_RETURN_IF_ERROR(
          tstore_-&amp;gt;DeleteRefs(handles.value-&amp;gt;handles()));
    }
  }
}
&lt;/code&gt;&lt;p&gt;Use Floyd's heap construction for efficient initialization.&lt;/p&gt;&lt;p&gt;Bulk initialization of a heap can be done in O(N) time, whereas adding one element at a time and updating the heap property after each addition requires O(N lg(N)) time.&lt;/p&gt;&lt;p&gt;Sometimes it is hard to change callers to use a new bulk API directly. In that case it might be beneficial to use a bulk API internally and cache the results for use in future non-bulk API calls:&lt;/p&gt;&lt;p&gt;Cache block decode results for use in future calls.&lt;/p&gt;&lt;p&gt;Each lookup needs to decode a whole block of K entries. Store the decoded entries in a cache and consult the cache on future lookups.&lt;/p&gt;&lt;p&gt;lexicon.cc&lt;/p&gt;&lt;code&gt;void GetTokenString(int pos, std::string* out) const {
  ...
  absl::FixedArray&amp;lt;LexiconEntry, 32&amp;gt; entries(pos + 1);

  // Decode all lexicon entries up to and including pos.
  for (int i = 0; i &amp;lt;= pos; ++i) {
    p = util::coding::TwoValuesVarint::Decode32(p, &amp;amp;entries[i].remaining,
                                                &amp;amp;entries[i].shared);
    entries[i].remaining_str = p;
    p += entries[i].remaining;  // remaining bytes trail each entry.
  }
&lt;/code&gt;&lt;code&gt;mutable std::vector&amp;lt;absl::InlinedVector&amp;lt;std::string, 16&amp;gt;&amp;gt; cache_;
...
void GetTokenString(int pos, std::string* out) const {
  ...
  DCHECK_LT(skentry, cache_.size());
  if (!cache_[skentry].empty()) {
    *out = cache_[skentry][pos];
    return;
  }
  ...
  // Init cache.
  ...
  const char* prev = p;
  for (int i = 0; i &amp;lt; block_sz; ++i) {
    uint32 shared, remaining;
    p = TwoValuesVarint::Decode32(p, &amp;amp;remaining, &amp;amp;shared);
    auto&amp;amp; cur = cache_[skentry].emplace_back();
    gtl::STLStringResizeUninitialized(&amp;amp;cur, remaining + shared);

    std::memcpy(cur.data(), prev, shared);
    std::memcpy(cur.data() + shared, p, remaining);
    prev = cur.data();
    p += remaining;
  }
  *out = cache_[skentry][pos];
&lt;/code&gt;&lt;p&gt;Prefer view types (e.g., &lt;code&gt;std::string_view&lt;/code&gt;, &lt;code&gt;std::Span&amp;lt;T&amp;gt;&lt;/code&gt;,
&lt;code&gt;absl::FunctionRef&amp;lt;R(Args...)&amp;gt;&lt;/code&gt;) for function arguments (unless ownership of the
data is being transferred). These types reduce copying, and allow callers to
pick their own container types (e.g., one caller might use &lt;code&gt;std::vector&lt;/code&gt; whereas
another one uses &lt;code&gt;absl::InlinedVector&lt;/code&gt;).&lt;/p&gt;&lt;p&gt;For frequently called routines, sometimes it is useful to allow higher-level callers to pass in a data structure that they own or information that the called routine needs that the client already has. This can avoid the low-level routine being forced to allocate its own temporary data structure or recompute already-available information.&lt;/p&gt;&lt;p&gt;Add RPC_Stats::RecordRPC variant allowing client to pass in already available WallTime value.&lt;/p&gt;&lt;p&gt;rpc-stats.h&lt;/p&gt;&lt;code&gt;static void RecordRPC(const Name &amp;amp;name, const RPC_Stats_Measurement&amp;amp; m);
&lt;/code&gt;&lt;code&gt;static void RecordRPC(const Name &amp;amp;name, const RPC_Stats_Measurement&amp;amp; m,
                      WallTime now);
&lt;/code&gt;&lt;p&gt;clientchannel.cc&lt;/p&gt;&lt;code&gt;const WallTime now = WallTime_Now();
...
RPC_Stats::RecordRPC(stats_name, m);
&lt;/code&gt;&lt;code&gt;const WallTime now = WallTime_Now();
...
RPC_Stats::RecordRPC(stats_name, m, now);
&lt;/code&gt;&lt;p&gt;A type may be either thread-compatible (synchronized externally) or thread-safe (synchronized internally). Most generally used types should be thread-compatible. This way callers who do not need thread-safety don’t pay for it.&lt;/p&gt;&lt;p&gt;Make a class thread-compatible since callers are already synchronized.&lt;/p&gt;&lt;p&gt;hitless-transfer-phase.cc&lt;/p&gt;&lt;code&gt;TransferPhase HitlessTransferPhase::get() const {
  static CallsiteMetrics cm("HitlessTransferPhase::get");
  MonitoredMutexLock l(&amp;amp;cm, &amp;amp;mutex_);
  return phase_;
}
&lt;/code&gt;&lt;code&gt;TransferPhase HitlessTransferPhase::get() const { return phase_; }
&lt;/code&gt;&lt;p&gt;hitless-transfer-phase.cc&lt;/p&gt;&lt;code&gt;bool HitlessTransferPhase::AllowAllocate() const {
  static CallsiteMetrics cm("HitlessTransferPhase::AllowAllocate");
  MonitoredMutexLock l(&amp;amp;cm, &amp;amp;mutex_);
  return phase_ == TransferPhase::kNormal || phase_ == TransferPhase::kBrownout;
}
&lt;/code&gt;&lt;code&gt;bool HitlessTransferPhase::AllowAllocate() const {
  return phase_ == TransferPhase::kNormal || phase_ == TransferPhase::kBrownout;
}
&lt;/code&gt;&lt;p&gt;However if the typical use of a type needs synchronization, prefer to move the synchronization inside the type. This allows the synchronization mechanism to be tweaked as necessary to improve performance (e.g., sharding to reduce contention) without affecting callers.&lt;/p&gt;&lt;p&gt;The most critical opportunities for performance improvements come from algorithmic improvements, e.g., turning an O(N²) algorithm to O(N lg(N)) or O(N), avoiding potentially exponential behavior, etc. These opportunities are rare in stable code, but are worth paying attention to when writing new code. A few examples that show such improvements to pre-existing code:&lt;/p&gt;&lt;p&gt;Add nodes to cycle detection structure in reverse post-order.&lt;/p&gt;&lt;p&gt;We were previously adding graph nodes and edges one at a time to a cycle-detection data structure, which required expensive work per edge. We now add the entire graph in reverse post-order, which makes cycle-detection trivial.&lt;/p&gt;&lt;p&gt;graphcycles.h&lt;/p&gt;&lt;code&gt;class GraphCycles : public util_graph::Graph {
 public:
  GraphCycles();
  ~GraphCycles() override;

  using Node = util_graph::Node;
&lt;/code&gt;&lt;code&gt;class GraphCycles : public util_graph::Graph {
 public:
  GraphCycles();
  ~GraphCycles() override;

  using Node = util_graph::Node;

  // InitFrom adds all the nodes and edges from src, returning true if
  // successful, false if a cycle is encountered.
  // REQUIRES: no nodes and edges have been added to GraphCycles yet.
  bool InitFrom(const util_graph::Graph&amp;amp; src);
&lt;/code&gt;&lt;p&gt;graphcycles.cc&lt;/p&gt;&lt;code&gt;bool GraphCycles::InitFrom(const util_graph::Graph&amp;amp; src) {
  ...
  // Assign ranks in topological order so we don't need any reordering during
  // initialization. For an acyclic graph, DFS leaves nodes in reverse
  // topological order, so we assign decreasing ranks to nodes as we leave them.
  Rank last_rank = n;
  auto leave = [&amp;amp;](util_graph::Node node) {
    DCHECK(r-&amp;gt;rank[node] == kMissingNodeRank);
    NodeInfo* nn = &amp;amp;r-&amp;gt;nodes[node];
    nn-&amp;gt;in = kNil;
    nn-&amp;gt;out = kNil;
    r-&amp;gt;rank[node] = --last_rank;
  };
  util_graph::DFSAll(src, std::nullopt, leave);

  // Add all the edges (detect cycles as we go).
  bool have_cycle = false;
  util_graph::PerEdge(src, [&amp;amp;](util_graph::Edge e) {
    DCHECK_NE(r-&amp;gt;rank[e.src], kMissingNodeRank);
    DCHECK_NE(r-&amp;gt;rank[e.dst], kMissingNodeRank);
    if (r-&amp;gt;rank[e.src] &amp;gt;= r-&amp;gt;rank[e.dst]) {
      have_cycle = true;
    } else if (!HasEdge(e.src, e.dst)) {
      EdgeListAddNode(r, &amp;amp;r-&amp;gt;nodes[e.src].out, e.dst);
      EdgeListAddNode(r, &amp;amp;r-&amp;gt;nodes[e.dst].in, e.src);
    }
  });
  if (have_cycle) {
    return false;
  } else {
    DCHECK(CheckInvariants());
    return true;
  }
}
&lt;/code&gt;&lt;p&gt;graph_partitioner.cc&lt;/p&gt;&lt;code&gt;absl::Status MergeGraph::Init() {
  const Graph&amp;amp; graph = *compiler_-&amp;gt;graph();
  clusters_.resize(graph.NodeLimit());
  graph.PerNode([&amp;amp;](Node node) {
    graph_-&amp;gt;AddNode(node);
    NodeList* n = new NodeList;
    n-&amp;gt;push_back(node);
    clusters_[node] = n;
  });
  absl::Status s;
  PerEdge(graph, [&amp;amp;](Edge e) {
    if (!s.ok()) return;
    if (graph_-&amp;gt;HasEdge(e.src, e.dst)) return;  // already added
    if (!graph_-&amp;gt;InsertEdge(e.src, e.dst)) {
      s = absl::InvalidArgumentError("cycle in the original graph");
    }
  });
  return s;
}
&lt;/code&gt;&lt;code&gt;absl::Status MergeGraph::Init() {
  const Graph&amp;amp; graph = *compiler_-&amp;gt;graph();
  if (!graph_-&amp;gt;InitFrom(graph)) {
    return absl::InvalidArgumentError("cycle in the original graph");
  }
  clusters_.resize(graph.NodeLimit());
  graph.PerNode([&amp;amp;](Node node) {
    NodeList* n = new NodeList;
    n-&amp;gt;push_back(node);
    clusters_[node] = n;
  });
  return absl::OkStatus();
}
&lt;/code&gt;&lt;p&gt;Replace the deadlock detection system built into a mutex implementation with a better algorithm.&lt;/p&gt;&lt;p&gt;Replaced deadlock detection algorithm by one that is ~50x as fast and scales to millions of mutexes without problem (the old algorithm relied on a 2K limit to avoid a performance cliff). The new code is based on the following paper: A dynamic topological sort algorithm for directed acyclic graphs David J. Pearce, Paul H. J. Kelly Journal of Experimental Algorithmics (JEA) JEA Homepage archive Volume 11, 2006, Article No. 1.7&lt;/p&gt;&lt;p&gt;The new algorithm takes O(|V|+|E|) space (instead of the O(|V|^2) bits needed by the older algorithm). Lock-acquisition order graphs are very sparse, so this is much less space. The algorithm is also quite simple: the core of it is ~100 lines of C++. Since the code now scales to much larger number of Mutexes, we were able to relax an artificial 2K limit, which uncovered a number of latent deadlocks in real programs.&lt;/p&gt;&lt;p&gt;Benchmark results: these were run in DEBUG mode since deadlock detection is mainly enabled in debug mode. The benchmark argument (/2k etc.) is the number of tracked nodes. At the default 2k limit of the old algorithm, the new algorithm takes only 0.5 microseconds per InsertEdge compared to 22 microseconds for the old algorithm. The new algorithm also easily scales to much larger graphs without problems whereas the old algorithm keels over quickly.&lt;/p&gt;&lt;code&gt;DEBUG: Benchmark            Time(ns)    CPU(ns) Iterations
----------------------------------------------------------
DEBUG: BM_StressTest/2k        23553      23566      29086
DEBUG: BM_StressTest/4k        45879      45909      15287
DEBUG: BM_StressTest/16k      776938     777472        817
&lt;/code&gt;&lt;code&gt;DEBUG: BM_StressTest/2k          392        393   10485760
DEBUG: BM_StressTest/4k          392        393   10485760
DEBUG: BM_StressTest/32k         407        407   10485760
DEBUG: BM_StressTest/256k        456        456   10485760
DEBUG: BM_StressTest/1M          534        534   10485760
&lt;/code&gt;&lt;p&gt;Replace an IntervalMap (with O(lg N) lookups) with a hash table (O(1) lookups).&lt;/p&gt;&lt;p&gt;The initial code was using IntervalMap because it seemed like the right data structure to support coalescing of adjacent blocks, but a hash table suffices since the adjacent block can be found by a hash table lookup. This (plus other changes in the CL) improve the performance of tpu::BestFitAllocator by ~4X.&lt;/p&gt;&lt;p&gt;best_fit_allocator.h&lt;/p&gt;&lt;code&gt;using Block = gtl::IntervalMap&amp;lt;int64, BlockState&amp;gt;::Entry;
...
// Map of pairs (address range, BlockState) with one entry for each allocation
// covering the range [0, allocatable_range_end_).  Adjacent kFree and
// kReserved blocks are coalesced. Adjacent kAllocated blocks are not
// coalesced.
gtl::IntervalMap&amp;lt;int64, BlockState&amp;gt; block_list_;

// Set of all free blocks sorted according to the allocation policy. Adjacent
// free blocks are coalesced.
std::set&amp;lt;Block, BlockSelector&amp;gt; free_list_;
&lt;/code&gt;&lt;code&gt;// A faster hash function for offsets in the BlockTable
struct OffsetHash {
  ABSL_ATTRIBUTE_ALWAYS_INLINE size_t operator()(int64 value) const {
    uint64 m = value;
    m *= uint64_t{0x9ddfea08eb382d69};
    return static_cast&amp;lt;uint64_t&amp;gt;(m ^ (m &amp;gt;&amp;gt; 32));
  }
};

// Hash table maps from block start address to block info.
// We include the length of the previous block in this info so we
// can find the preceding block to coalesce with.
struct HashTableEntry {
  BlockState state;
  int64 my_length;
  int64 prev_length;  // Zero if there is no previous block.
};
using BlockTable = absl::flat_hash_map&amp;lt;int64, HashTableEntry, OffsetHash&amp;gt;;
&lt;/code&gt;&lt;p&gt;Replace sorted-list intersection (O(N log N)) with hash table lookups (O(N)).&lt;/p&gt;&lt;p&gt;Old code to detect whether or not two nodes share a common source would get the sources for each node in sorted order and then do a sorted intersection. The new code places the sources for one node in a hash-table and then iterates over the other node's sources checking the hash-table.&lt;/p&gt;&lt;code&gt;name             old time/op  new time/op  delta
BM_CompileLarge   28.5s ± 2%   22.4s ± 2%  -21.61%  (p=0.008 n=5+5)
&lt;/code&gt;&lt;p&gt;Implement good hash function so that things are O(1) instead of O(N).&lt;/p&gt;&lt;p&gt;location.h&lt;/p&gt;&lt;code&gt;// Hasher for Location objects.
struct LocationHash {
  size_t operator()(const Location* key) const {
    return key != nullptr ? util_hash::Hash(key-&amp;gt;address()) : 0;
  }
};
&lt;/code&gt;&lt;code&gt;size_t HashLocation(const Location&amp;amp; loc);
...
struct LocationHash {
  size_t operator()(const Location* key) const {
    return key != nullptr ? HashLocation(*key) : 0;
  }
};
&lt;/code&gt;&lt;p&gt;location.cc&lt;/p&gt;&lt;code&gt;size_t HashLocation(const Location&amp;amp; loc) {
  util_hash::MurmurCat m;

  // Encode some simpler features into a single value.
  m.AppendAligned((loc.dynamic() ? 1 : 0)                    //
                  | (loc.append_shard_to_address() ? 2 : 0)  //
                  | (loc.is_any() ? 4 : 0)                   //
                  | (!loc.any_of().empty() ? 8 : 0)          //
                  | (loc.has_shardmap() ? 16 : 0)            //
                  | (loc.has_sharding() ? 32 : 0));

  if (loc.has_shardmap()) {
    m.AppendAligned(loc.shardmap().output() |
                    static_cast&amp;lt;uint64_t&amp;gt;(loc.shardmap().stmt()) &amp;lt;&amp;lt; 20);
  }
  if (loc.has_sharding()) {
    uint64_t num = 0;
    switch (loc.sharding().type_case()) {
      case Sharding::kModShard:
        num = loc.sharding().mod_shard();
        break;
      case Sharding::kRangeSplit:
        num = loc.sharding().range_split();
        break;
      case Sharding::kNumShards:
        num = loc.sharding().num_shards();
        break;
      default:
        num = 0;
        break;
    }
    m.AppendAligned(static_cast&amp;lt;uint64_t&amp;gt;(loc.sharding().type_case()) |
                    (num &amp;lt;&amp;lt; 3));
  }

  auto add_string = [&amp;amp;m](absl::string_view s) {
    if (!s.empty()) {
      m.Append(s.data(), s.size());
    }
  };

  add_string(loc.address());
  add_string(loc.lb_policy());

  // We do not include any_of since it is complicated to compute a hash
  // value that is not sensitive to order and duplication.
  return m.GetHash();
}
&lt;/code&gt;&lt;p&gt;Careful consideration of memory footprint and cache footprint of important data structures can often yield big savings. The data structures below focus on supporting common operations by touching fewer cache lines. Care taken here can (a) avoid expensive cache misses (b) reduce memory bus traffic, which speeds up both the program in question and anything else running on the same machine. They rely on some common techniques you may find useful when designing your own data structures.&lt;/p&gt;&lt;p&gt;Use compact representations for data that will be accessed often or that comprises a large portion of the application’s memory usage. A compact representation can significantly reduce memory usage and improve performance by touching fewer cache lines and reducing memory bus bandwidth usage. However, watch out for cache-line contention.&lt;/p&gt;&lt;p&gt;Carefully consider the memory layout of types that have a large memory or cache footprint.&lt;/p&gt;&lt;code&gt;enum class OpType : uint8_t { ...
}&lt;/code&gt; instead of &lt;code&gt;enum class OpType { ... }&lt;/code&gt;).&lt;p&gt;On modern 64-bit machines, pointers take up 64 bits. If you have a pointer-rich data structure, you can easily chew up lots of memory with indirections of T*. Instead, consider using integer indices into an array T[] or other data structure. Not only will the references be smaller (if the number of indices is small enough to fit in 32 or fewer bits), but the storage for all the T[] elements will be contiguous, often leading to better cache locality.&lt;/p&gt;&lt;p&gt;Avoid data structures that allocate a separate object per stored element (e.g., &lt;code&gt;std::map&lt;/code&gt;, &lt;code&gt;std::unordered_map&lt;/code&gt; in C++). Instead, consider types that use
chunked or flat representations to store multiple elements in close proximity in
memory (e.g., &lt;code&gt;std::vector&lt;/code&gt;, &lt;code&gt;absl::flat_hash_{map,set}&lt;/code&gt; in C++). Such types
tend to have much better cache behavior. Furthermore, they encounter less
allocator overhead.&lt;/p&gt;&lt;p&gt;One useful technique is to partition elements into chunks where each chunk can hold a fixed number of elements. This technique can reduce the cache footprint of a data structure significantly while preserving good asymptotic behavior.&lt;/p&gt;&lt;p&gt;For some data structures, a single chunk suffices to hold all elements (e.g., strings and vectors). Other types (e.g., &lt;code&gt;absl::flat_hash_map&lt;/code&gt;) also use this
technique.&lt;/p&gt;&lt;p&gt;Some container types are optimized for storing a small number of elements. These types provide space for a small number of elements at the top level and completely avoid allocations when the number of elements is small. This can be very helpful when instances of such types are constructed often (e.g., as stack variables in frequently executed code), or if many instances are live at the same time. If a container will typically contain a small number of elements consider using one of the inlined storage types, e.g., InlinedVector.&lt;/p&gt;&lt;p&gt;Caveat: if &lt;code&gt;sizeof(T)&lt;/code&gt; is large, inlined storage containers may not be the best
choice since the inlined backing store will be large.&lt;/p&gt;&lt;p&gt;Sometimes a nested map data structure can be replaced with a single-level map with a compound key. This can reduce the cost of lookups and insertions significantly.&lt;/p&gt;&lt;p&gt;Reduce allocations and improve cache footprint by converting btree&amp;lt;a,btree&amp;lt;b,c&amp;gt;&amp;gt; to btree&amp;lt;pair&amp;lt;a,b&amp;gt;,c&amp;gt;.&lt;/p&gt;&lt;p&gt;graph_splitter.cc&lt;/p&gt;&lt;code&gt;absl::btree_map&amp;lt;std::string, absl::btree_map&amp;lt;std::string, OpDef&amp;gt;&amp;gt; ops;
&lt;/code&gt;
&lt;code&gt;// The btree maps from {package_name, op_name} to its const Opdef*.
absl::btree_map&amp;lt;std::pair&amp;lt;absl::string_view, absl::string_view&amp;gt;,
                const OpDef*&amp;gt;
    ops;
&lt;/code&gt;
&lt;p&gt;Caveat: if the first map key is big, it might be better to stick with nested maps:&lt;/p&gt;&lt;p&gt;Switch to a nested map leads to 76% performance improvement in microbenchmark.&lt;/p&gt;&lt;p&gt;We previously had a single-level hash table where the key consisted of a (string) path and some other numeric sub-keys. Each path occurred in approximately 1000 keys on average. We split the hash table into two levels where the first level was keyed by the path and each second level hash table kept just the sub-key to data mapping for a particular path. This reduced the memory usage for storing paths by a factor of 1000, and also sped up accesses where many sub-keys for the same path were accessed together.&lt;/p&gt;&lt;p&gt;Arenas can help reduce memory allocation cost, but they also have the benefit of packing together independently allocated items next to each other, typically in fewer cache lines, and eliminating most destruction costs. They are likely most effective for complex data structures with many sub-objects. Consider providing an appropriate initial size for the arena since that can help reduce allocations.&lt;/p&gt;&lt;p&gt;Caveat: it is easy to misuse arenas by putting too many short-lived objects in a long-lived arena, which can unnecessarily bloat memory footprint.&lt;/p&gt;&lt;p&gt;If the domain of a map can be represented by a small integer or is an enum, or if the map will have very few elements, the map can sometimes be replaced by an array or a vector of some form.&lt;/p&gt;&lt;p&gt;Use an array instead of flat_map.&lt;/p&gt;&lt;p&gt;rtp_controller.h&lt;/p&gt;&lt;code&gt;const gtl::flat_map&amp;lt;int, int&amp;gt; payload_type_to_clock_frequency_;
&lt;/code&gt;
&lt;code&gt;// A map (implemented as a simple array) indexed by payload_type to clock freq
// for that paylaod type (or 0)
struct PayloadTypeToClockRateMap {
  int map[128];
};
...
const PayloadTypeToClockRateMap payload_type_to_clock_frequency_;
&lt;/code&gt;
&lt;p&gt;If the domain of a set can be represented by a small integer, the set can be replaced with a bit vector (InlinedBitVector is often a good choice). Set operations can also be nicely efficient on these representations using bitwise boolean operations (OR for union, AND for intersection, etc.).&lt;/p&gt;&lt;p&gt;Spanner placement system. Replace dense_hash_set&amp;lt;ZoneId&amp;gt; with a bit-vector with one bit per zone.&lt;/p&gt;&lt;p&gt;zone_set.h&lt;/p&gt;&lt;code&gt;class ZoneSet: public dense_hash_set&amp;lt;ZoneId&amp;gt; {
 public:
  ...
  bool Contains(ZoneId zone) const {
    return count(zone) &amp;gt; 0;
  }
&lt;/code&gt;
&lt;code&gt;class ZoneSet {
  ...
  // Returns true iff "zone" is contained in the set
  bool ContainsZone(ZoneId zone) const {
    return zone &amp;lt; b_.size() &amp;amp;&amp;amp; b_.get_bit(zone);
  }
  ...
 private:
  int size_;          // Number of zones inserted
  util::bitmap::InlinedBitVector&amp;lt;256&amp;gt; b_;
&lt;/code&gt;
&lt;p&gt;Benchmark results:&lt;/p&gt;&lt;code&gt;CPU: AMD Opteron (4 cores) dL1:64KB dL2:1024KB
Benchmark                          Base (ns)  New (ns) Improvement
------------------------------------------------------------------
BM_Evaluate/1                            960       676    +29.6%
BM_Evaluate/2                           1661      1138    +31.5%
BM_Evaluate/3                           2305      1640    +28.9%
BM_Evaluate/4                           3053      2135    +30.1%
BM_Evaluate/5                           3780      2665    +29.5%
BM_Evaluate/10                          7819      5739    +26.6%
BM_Evaluate/20                         17922     12338    +31.2%
BM_Evaluate/40                         36836     26430    +28.2%
&lt;/code&gt;

&lt;p&gt;Use bit matrix to keep track of reachability properties between operands instead of hash table.&lt;/p&gt;&lt;p&gt;hlo_computation.h&lt;/p&gt;&lt;code&gt;using TransitiveOperandMap =
    std::unordered_map&amp;lt;const HloInstruction*,
                       std::unordered_set&amp;lt;const HloInstruction*&amp;gt;&amp;gt;;
&lt;/code&gt;
&lt;code&gt;class HloComputation::ReachabilityMap {
  ...
  // dense id assignment from HloInstruction* to number
  tensorflow::gtl::FlatMap&amp;lt;const HloInstruction*, int&amp;gt; ids_;
  // matrix_(a,b) is true iff b is reachable from a
  tensorflow::core::Bitmap matrix_;
};
&lt;/code&gt;
&lt;p&gt;Memory allocation adds costs:&lt;/p&gt;&lt;p&gt;Garbage-collection runtimes sometimes obviate issue #3 by placing consecutive allocations sequentially in memory.&lt;/p&gt;&lt;p&gt;Reducing allocations increases benchmark throughput by 21%.&lt;/p&gt;&lt;p&gt;memory_manager.cc&lt;/p&gt;&lt;code&gt;LiveTensor::LiveTensor(tf::Tensor t, std::shared_ptr&amp;lt;const DeviceInfo&amp;gt; dinfo,
                       bool is_batched)
    : tensor(std::move(t)),
      device_info(dinfo ? std::move(dinfo) : std::make_shared&amp;lt;DeviceInfo&amp;gt;()),
      is_batched(is_batched) {
&lt;/code&gt;
&lt;code&gt;static const std::shared_ptr&amp;lt;DeviceInfo&amp;gt;&amp;amp; empty_device_info() {
  static std::shared_ptr&amp;lt;DeviceInfo&amp;gt;* result =
      new std::shared_ptr&amp;lt;DeviceInfo&amp;gt;(new DeviceInfo);
  return *result;
}

LiveTensor::LiveTensor(tf::Tensor t, std::shared_ptr&amp;lt;const DeviceInfo&amp;gt; dinfo,
                       bool is_batched)
    : tensor(std::move(t)), is_batched(is_batched) {
  if (dinfo) {
    device_info = std::move(dinfo);
  } else {
    device_info = empty_device_info();
  }
&lt;/code&gt;
&lt;p&gt;Use statically-allocated zero vector when possible rather than allocating a vector and filling it with zeroes.&lt;/p&gt;&lt;p&gt;embedding_executor_8bit.cc&lt;/p&gt;&lt;code&gt;// The actual implementation of the EmbeddingLookUpT using template parameters
// instead of object members to improve the performance.
template &amp;lt;bool Mean, bool SymmetricInputRange&amp;gt;
static tensorflow::Status EmbeddingLookUpT(...) {
    ...
  std::unique_ptr&amp;lt;tensorflow::quint8[]&amp;gt; zero_data(
      new tensorflow::quint8[max_embedding_width]);
  memset(zero_data.get(), 0, sizeof(tensorflow::quint8) * max_embedding_width);
&lt;/code&gt;
&lt;code&gt;// A size large enough to handle most embedding widths
static const int kTypicalMaxEmbedding = 256;
static tensorflow::quint8 static_zero_data[kTypicalMaxEmbedding];  // All zeroes
...
// The actual implementation of the EmbeddingLookUpT using template parameters
// instead of object members to improve the performance.
template &amp;lt;bool Mean, bool SymmetricInputRange&amp;gt;
static tensorflow::Status EmbeddingLookUpT(...) {
    ...
  std::unique_ptr&amp;lt;tensorflow::quint8[]&amp;gt; zero_data_backing(nullptr);

  // Get a pointer to a memory area with at least
  // "max_embedding_width" quint8 zero values.
  tensorflow::quint8* zero_data;
  if (max_embedding_width &amp;lt;= ARRAYSIZE(static_zero_data)) {
    // static_zero_data is big enough so we don't need to allocate zero data
    zero_data = &amp;amp;static_zero_data[0];
  } else {
    // static_zero_data is not big enough: we need to allocate zero data
    zero_data_backing =
        absl::make_unique&amp;lt;tensorflow::quint8[]&amp;gt;(max_embedding_width);
    memset(zero_data_backing.get(), 0,
           sizeof(tensorflow::quint8) * max_embedding_width);
    zero_data = zero_data_backing.get();
  }
&lt;/code&gt;

&lt;p&gt;Also, prefer stack allocation over heap allocation when object lifetime is bounded by the scope (although be careful with stack frame sizes for large objects).&lt;/p&gt;&lt;p&gt;When the maximum or expected maximum size of a vector (or some other container types) is known in advance, pre-size the container’s backing store (e.g., using &lt;code&gt;resize&lt;/code&gt; or &lt;code&gt;reserve&lt;/code&gt; in C++).&lt;/p&gt;&lt;p&gt;Pre-size a vector and fill it in, rather than N push_back operations.&lt;/p&gt;&lt;p&gt;indexblockdecoder.cc&lt;/p&gt;&lt;code&gt;for (int i = 0; i &amp;lt; ndocs-1; i++) {
  uint32 delta;
  ERRORCHECK(b-&amp;gt;GetRice(rice_base, &amp;amp;delta));
  docs_.push_back(DocId(my_shard_ + (base + delta) * num_shards_));
  base = base + delta + 1;
}
docs_.push_back(last_docid_);
&lt;/code&gt;
&lt;code&gt;docs_.resize(ndocs);
DocId* docptr = &amp;amp;docs_[0];
for (int i = 0; i &amp;lt; ndocs-1; i++) {
  uint32 delta;
  ERRORCHECK(b.GetRice(rice_base, &amp;amp;delta));
  *docptr = DocId(my_shard_ + (base + delta) * num_shards_);
  docptr++;
  base = base + delta + 1;
}
*docptr = last_docid_;
&lt;/code&gt;
&lt;p&gt;Caveat: Do not use &lt;code&gt;resize&lt;/code&gt; or &lt;code&gt;reserve&lt;/code&gt; to grow one element at a time since
that may lead to quadratic behavior. Also, if element construction is expensive,
prefer an initial &lt;code&gt;reserve&lt;/code&gt; call followed by several &lt;code&gt;push_back&lt;/code&gt; or
&lt;code&gt;emplace_back&lt;/code&gt; calls instead of an initial &lt;code&gt;resize&lt;/code&gt; since that will double the
number of constructor calls.&lt;/p&gt;&lt;p&gt;Avoid an extra copy when receiving a tensor via gRPC.&lt;/p&gt;&lt;p&gt;A benchmark that sends around 400KB tensors speeds up by ~10-15%:&lt;/p&gt;&lt;code&gt;Benchmark              Time(ns)    CPU(ns) Iterations
-----------------------------------------------------
BM_RPC/30/98k_mean    148764691 1369998944       1000
&lt;/code&gt;
&lt;code&gt;Benchmark              Time(ns)    CPU(ns) Iterations
-----------------------------------------------------
BM_RPC/30/98k_mean    131595940 1216998084       1000
&lt;/code&gt;

&lt;p&gt;Move large options structure rather than copying it.&lt;/p&gt;&lt;p&gt;index.cc&lt;/p&gt;&lt;code&gt;return search_iterators::DocPLIteratorFactory::Create(opts);
&lt;/code&gt;
&lt;code&gt;return search_iterators::DocPLIteratorFactory::Create(std::move(opts));
&lt;/code&gt;
&lt;p&gt;Use std::sort instead of std::stable_sort, which avoids an internal copy inside the stable sort implementation.&lt;/p&gt;&lt;p&gt;encoded-vector-hits.h&lt;/p&gt;&lt;code&gt;std::stable_sort(hits_.begin(), hits_.end(),
                 gtl::OrderByField(&amp;amp;HitWithPayloadOffset::docid));
&lt;/code&gt;
&lt;code&gt;struct HitWithPayloadOffset {
  search_iterators::LocalDocId64 docid;
  int first_payload_offset;  // offset into the payload vector.
  int num_payloads;

  bool operator&amp;lt;(const HitWithPayloadOffset&amp;amp; other) const {
    return (docid &amp;lt; other.docid) ||
           (docid == other.docid &amp;amp;&amp;amp;
            first_payload_offset &amp;lt; other.first_payload_offset);
  }
};
    ...
    std::sort(hits_.begin(), hits_.end());
&lt;/code&gt;
&lt;p&gt;A container or an object declared inside a loop will be recreated on every loop iteration. This can lead to expensive construction, destruction, and resizing. Hoisting the declaration outside the loop enables reuse and can provide a significant performance boost. (Compilers are often unable to do such hoisting on their own due to language semantics or their inability to ensure program equivalence.)&lt;/p&gt;&lt;p&gt;Hoist variable definition outside of loop iteration.&lt;/p&gt;&lt;p&gt;autofdo_profile_utils.h&lt;/p&gt;&lt;code&gt;auto iterator = absl::WrapUnique(sstable-&amp;gt;GetIterator());
while (!iterator-&amp;gt;done()) {
  T profile;
  if (!profile.ParseFromString(iterator-&amp;gt;value_view())) {
    return absl::InternalError(
        "Failed to parse mem_block to specified profile type.");
  }
  ...
  iterator-&amp;gt;Next();
}
&lt;/code&gt;
&lt;code&gt;auto iterator = absl::WrapUnique(sstable-&amp;gt;GetIterator());
T profile;
while (!iterator-&amp;gt;done()) {
  if (!profile.ParseFromString(iterator-&amp;gt;value_view())) {
    return absl::InternalError(
        "Failed to parse mem_block to specified profile type.");
  }
  ...
  iterator-&amp;gt;Next();
}
&lt;/code&gt;
&lt;p&gt;Define a protobuf variable outside a loop so that its allocated storage can be reused across loop iterations.&lt;/p&gt;&lt;p&gt;stats-router.cc&lt;/p&gt;&lt;code&gt;for (auto&amp;amp; r : routers_to_update) {
  ...
  ResourceRecord record;
  {
    MutexLock agg_lock(r.agg-&amp;gt;mutex());
    r.agg-&amp;gt;AddResourceRecordUsages(measure_indices, &amp;amp;record);
  }
  ...
}
&lt;/code&gt;
&lt;code&gt;ResourceRecord record;
for (auto&amp;amp; r : routers_to_update) {
  ...
  record.Clear();
  {
    MutexLock agg_lock(r.agg-&amp;gt;mutex());
    r.agg-&amp;gt;AddResourceRecordUsages(measure_indices, &amp;amp;record);
  }
  ...
}
&lt;/code&gt;
&lt;p&gt;Serialize to same std::string repeatedly.&lt;/p&gt;&lt;p&gt;program_rep.cc&lt;/p&gt;&lt;code&gt;std::string DeterministicSerialization(const proto2::Message&amp;amp; m) {
  std::string result;
  proto2::io::StringOutputStream sink(&amp;amp;result);
  proto2::io::CodedOutputStream out(&amp;amp;sink);
  out.SetSerializationDeterministic(true);
  m.SerializePartialToCodedStream(&amp;amp;out);
  return result;
}
&lt;/code&gt;
&lt;code&gt;absl::string_view DeterministicSerializationTo(const proto2::Message&amp;amp; m,
                                               std::string* scratch) {
  scratch-&amp;gt;clear();
  proto2::io::StringOutputStream sink(scratch);
  proto2::io::CodedOutputStream out(&amp;amp;sink);
  out.SetSerializationDeterministic(true);
  m.SerializePartialToCodedStream(&amp;amp;out);
  return absl::string_view(*scratch);
}
&lt;/code&gt;
&lt;p&gt;Caveat: protobuf, string, vector, containers etc. tend to grow to the size of the largest value ever stored in them. Therefore reconstructing them periodically (e.g., after every N uses) can help reduce memory requirements and reinitialization costs.&lt;/p&gt;&lt;p&gt;Perhaps one of the most effective categories of improving performance is avoiding work you don’t have to do. This can take many forms, including creating specialized paths through code for common cases that avoid more general expensive computation, precomputation, deferring work until it is really needed, hoisting work into less-frequently executed pieces of code, and other similar approaches. Below are many examples of this general approach, categorized into a few representative categories.&lt;/p&gt;&lt;p&gt;Often, code is written to cover all cases, but some subset of the cases are much simpler and more common than others. E.g., &lt;code&gt;vector::push_back&lt;/code&gt; usually has
enough space for the new element, but contains code to resize the underlying
storage when it does not. Some attention paid to the structure of code can help
make the common simple case faster without hurting uncommon case performance
significantly.&lt;/p&gt;&lt;p&gt;Make fast path cover more common cases.&lt;/p&gt;&lt;p&gt;Add handling of trailing single ASCII bytes, rather than only handling multiples of four bytes with this routine. This avoids calling the slower generic routine for all-ASCII strings that are, for example, 5 bytes.&lt;/p&gt;&lt;p&gt;utf8statetable.cc&lt;/p&gt;&lt;code&gt;// Scan a UTF-8 stringpiece based on state table.
// Always scan complete UTF-8 characters
// Set number of bytes scanned. Return reason for exiting
// OPTIMIZED for case of 7-bit ASCII 0000..007f all valid
int UTF8GenericScanFastAscii(const UTF8ScanObj* st, absl::string_view str,
                             int* bytes_consumed) {
                             ...
  int exit_reason;
  do {
    //  Skip 8 bytes of ASCII at a whack; no endianness issue
    while ((src_limit - src &amp;gt;= 8) &amp;amp;&amp;amp;
           (((UNALIGNED_LOAD32(src + 0) | UNALIGNED_LOAD32(src + 4)) &amp;amp;
             0x80808080) == 0)) {
      src += 8;
    }
    //  Run state table on the rest
    int rest_consumed;
    exit_reason = UTF8GenericScan(
        st, absl::ClippedSubstr(str, src - initial_src), &amp;amp;rest_consumed);
    src += rest_consumed;
  } while (exit_reason == kExitDoAgain);

  *bytes_consumed = src - initial_src;
  return exit_reason;
}
&lt;/code&gt;
&lt;code&gt;// Scan a UTF-8 stringpiece based on state table.
// Always scan complete UTF-8 characters
// Set number of bytes scanned. Return reason for exiting
// OPTIMIZED for case of 7-bit ASCII 0000..007f all valid
int UTF8GenericScanFastAscii(const UTF8ScanObj* st, absl::string_view str,
                             int* bytes_consumed) {
                             ...
  int exit_reason = kExitOK;
  do {
    //  Skip 8 bytes of ASCII at a whack; no endianness issue
    while ((src_limit - src &amp;gt;= 8) &amp;amp;&amp;amp;
           (((UNALIGNED_LOAD32(src + 0) | UNALIGNED_LOAD32(src + 4)) &amp;amp;
             0x80808080) == 0)) {
      src += 8;
    }
    while (src &amp;lt; src_limit &amp;amp;&amp;amp; Is7BitAscii(*src)) { // Skip ASCII bytes
      src++;
    }
    if (src &amp;lt; src_limit) {
      //  Run state table on the rest
      int rest_consumed;
      exit_reason = UTF8GenericScan(
          st, absl::ClippedSubstr(str, src - initial_src), &amp;amp;rest_consumed);
      src += rest_consumed;
    }
  } while (exit_reason == kExitDoAgain);

  *bytes_consumed = src - initial_src;
  return exit_reason;
}
&lt;/code&gt;
&lt;p&gt;Simpler fast paths for InlinedVector.&lt;/p&gt;&lt;p&gt;inlined_vector.h&lt;/p&gt;&lt;code&gt;auto Storage&amp;lt;T, N, A&amp;gt;::Resize(ValueAdapter values, size_type new_size) -&amp;gt; void {
  StorageView storage_view = MakeStorageView();

  IteratorValueAdapter&amp;lt;MoveIterator&amp;gt; move_values(
      MoveIterator(storage_view.data));

  AllocationTransaction allocation_tx(GetAllocPtr());
  ConstructionTransaction construction_tx(GetAllocPtr());

  absl::Span&amp;lt;value_type&amp;gt; construct_loop;
  absl::Span&amp;lt;value_type&amp;gt; move_construct_loop;
  absl::Span&amp;lt;value_type&amp;gt; destroy_loop;

  if (new_size &amp;gt; storage_view.capacity) {
  ...
  } else if (new_size &amp;gt; storage_view.size) {
    construct_loop = {storage_view.data + storage_view.size,
                      new_size - storage_view.size};
  } else {
    destroy_loop = {storage_view.data + new_size, storage_view.size - new_size};
  }
&lt;/code&gt;
&lt;code&gt;auto Storage&amp;lt;T, N, A&amp;gt;::Resize(ValueAdapter values, size_type new_size) -&amp;gt; void {
  StorageView storage_view = MakeStorageView();
  auto* const base = storage_view.data;
  const size_type size = storage_view.size;
  auto* alloc = GetAllocPtr();
  if (new_size &amp;lt;= size) {
    // Destroy extra old elements.
    inlined_vector_internal::DestroyElements(alloc, base + new_size,
                                             size - new_size);
  } else if (new_size &amp;lt;= storage_view.capacity) {
    // Construct new elements in place.
    inlined_vector_internal::ConstructElements(alloc, base + size, &amp;amp;values,
                                               new_size - size);
  } else {
  ...
  }
&lt;/code&gt;
&lt;p&gt;Fast path for common cases of initializing 1-D to 4-D tensors.&lt;/p&gt;&lt;p&gt;tensor_shape.cc&lt;/p&gt;&lt;code&gt;template &amp;lt;class Shape&amp;gt;
TensorShapeBase&amp;lt;Shape&amp;gt;::TensorShapeBase(gtl::ArraySlice&amp;lt;int64&amp;gt; dim_sizes) {
  set_tag(REP16);
  set_data_type(DT_INVALID);
  set_ndims_byte(0);
  set_num_elements(1);
  for (int64 s : dim_sizes) {
    AddDim(internal::SubtleMustCopy(s));
  }
}
&lt;/code&gt;
&lt;code&gt;template &amp;lt;class Shape&amp;gt;
void TensorShapeBase&amp;lt;Shape&amp;gt;::InitDims(gtl::ArraySlice&amp;lt;int64&amp;gt; dim_sizes) {
  DCHECK_EQ(tag(), REP16);

  // Allow sizes that are under kint64max^0.25 so that 4-way multiplication
  // below cannot overflow.
  static const uint64 kMaxSmall = 0xd744;
  static_assert(kMaxSmall * kMaxSmall * kMaxSmall * kMaxSmall &amp;lt;= kint64max,
                "bad overflow check");
  bool large_size = false;
  for (auto s : dim_sizes) {
    if (s &amp;gt; kMaxSmall) {
      large_size = true;
      break;
    }
  }

  if (!large_size) {
    // Every size fits in 16 bits; use fast-paths for dims in {1,2,3,4}.
    uint16* dst = as16()-&amp;gt;dims_;
    switch (dim_sizes.size()) {
      case 1: {
        set_ndims_byte(1);
        const int64 size = dim_sizes[0];
        const bool neg = Set16(kIsPartial, dst, 0, size);
        set_num_elements(neg ? -1 : size);
        return;
      }
      case 2: {
        set_ndims_byte(2);
        const int64 size0 = dim_sizes[0];
        const int64 size1 = dim_sizes[1];
        bool neg = Set16(kIsPartial, dst, 0, size0);
        neg |= Set16(kIsPartial, dst, 1, size1);
        set_num_elements(neg ? -1 : (size0 * size1));
        return;
      }
      case 3: {
      ...
      }
      case 4: {
      ...
      }
    }
  }

  set_ndims_byte(0);
  set_num_elements(1);
  for (int64 s : dim_sizes) {
    AddDim(internal::SubtleMustCopy(s));
  }
}
&lt;/code&gt;
&lt;p&gt;Make varint parser fast path cover just the 1-byte case, instead of covering 1-byte and 2-byte cases.&lt;/p&gt;&lt;p&gt;Reducing the size of the (inlined) fast path reduces code size and icache pressure, which leads to improved performance.&lt;/p&gt;&lt;p&gt;parse_context.h&lt;/p&gt;&lt;code&gt;template &amp;lt;typename T&amp;gt;
PROTOBUF_NODISCARD const char* VarintParse(const char* p, T* out) {
  auto ptr = reinterpret_cast&amp;lt;const uint8_t*&amp;gt;(p);
  uint32_t res = ptr[0];
  if (!(res &amp;amp; 0x80)) {
    *out = res;
    return p + 1;
  }
  uint32_t byte = ptr[1];
  res += (byte - 1) &amp;lt;&amp;lt; 7;
  if (!(byte &amp;amp; 0x80)) {
    *out = res;
    return p + 2;
  }
  return VarintParseSlow(p, res, out);
}
&lt;/code&gt;
&lt;code&gt;template &amp;lt;typename T&amp;gt;
PROTOBUF_NODISCARD const char* VarintParse(const char* p, T* out) {
  auto ptr = reinterpret_cast&amp;lt;const uint8_t*&amp;gt;(p);
  uint32_t res = ptr[0];
  if (!(res &amp;amp; 0x80)) {
    *out = res;
    return p + 1;
  }
  return VarintParseSlow(p, res, out);
}
&lt;/code&gt;
&lt;p&gt;parse_context.cc&lt;/p&gt;&lt;code&gt;std::pair&amp;lt;const char*, uint32_t&amp;gt; VarintParseSlow32(const char* p,
                                                   uint32_t res) {
  for (std::uint32_t i = 2; i &amp;lt; 5; i++) {
  ...
}
...
std::pair&amp;lt;const char*, uint64_t&amp;gt; VarintParseSlow64(const char* p,
                                                   uint32_t res32) {
  uint64_t res = res32;
  for (std::uint32_t i = 2; i &amp;lt; 10; i++) {
  ...
}
&lt;/code&gt;
&lt;code&gt;std::pair&amp;lt;const char*, uint32_t&amp;gt; VarintParseSlow32(const char* p,
                                                   uint32_t res) {
  for (std::uint32_t i = 1; i &amp;lt; 5; i++) {
  ...
}
...
std::pair&amp;lt;const char*, uint64_t&amp;gt; VarintParseSlow64(const char* p,
                                                   uint32_t res32) {
  uint64_t res = res32;
  for (std::uint32_t i = 1; i &amp;lt; 10; i++) {
  ...
}
&lt;/code&gt;
&lt;p&gt;Skip significant work in RPC_Stats_Measurement addition if no errors have occurred.&lt;/p&gt;&lt;p&gt;rpc-stats.h&lt;/p&gt;&lt;code&gt;struct RPC_Stats_Measurement {
  ...
  double errors[RPC::NUM_ERRORS];
&lt;/code&gt;
&lt;code&gt;struct RPC_Stats_Measurement {
  ...
  double get_errors(int index) const { return errors[index]; }
  void set_errors(int index, double value) {
    errors[index] = value;
    any_errors_set = true;
  }
 private:
  ...
  // We make this private so that we can keep track of whether any of
  // these values have been set to non-zero values.
  double errors[RPC::NUM_ERRORS];
  bool any_errors_set;  // True iff any of the errors[i] values are non-zero
&lt;/code&gt;
&lt;p&gt;rpc-stats.cc&lt;/p&gt;&lt;code&gt;void RPC_Stats_Measurement::operator+=(const RPC_Stats_Measurement&amp;amp; x) {
  ...
  for (int i = 0; i &amp;lt; RPC::NUM_ERRORS; ++i) {
    errors[i] += x.errors[i];
  }
}
&lt;/code&gt;
&lt;code&gt;void RPC_Stats_Measurement::operator+=(const RPC_Stats_Measurement&amp;amp; x) {
  ...
  if (x.any_errors_set) {
    for (int i = 0; i &amp;lt; RPC::NUM_ERRORS; ++i) {
      errors[i] += x.errors[i];
    }
    any_errors_set = true;
  }
}
&lt;/code&gt;
&lt;p&gt;Do array lookup on first byte of string to often avoid fingerprinting full string.&lt;/p&gt;&lt;p&gt;soft-tokens-helper.cc&lt;/p&gt;&lt;code&gt;bool SoftTokensHelper::IsSoftToken(const StringPiece&amp;amp; token) const {
  return soft_tokens_.find(Fingerprint(token.data(), token.size())) !=
      soft_tokens_.end();
}
&lt;/code&gt;
&lt;p&gt;soft-tokens-helper.h&lt;/p&gt;&lt;code&gt;class SoftTokensHelper {
 ...
 private:
  ...
  // Since soft tokens are mostly punctuation-related, for performance
  // purposes, we keep an array filter_.  filter_[i] is true iff any
  // of the soft tokens start with the byte value 'i'.  This avoids
  // fingerprinting a term in the common case, since we can just do an array
  // lookup based on the first byte, and if filter_[b] is false, then
  // we can return false immediately.
  bool          filter_[256];
  ...
};

inline bool SoftTokensHelper::IsSoftToken(const StringPiece&amp;amp; token) const {
  if (token.size() &amp;gt;= 1) {
    char first_char = token.data()[0];
    if (!filter_[first_char]) {
      return false;
    }
  }
  return IsSoftTokenFallback(token);
}
&lt;/code&gt;
&lt;p&gt;soft-tokens-helper.cc&lt;/p&gt;&lt;code&gt;bool SoftTokensHelper::IsSoftTokenFallback(const StringPiece&amp;amp; token) const {
  return soft_tokens_.find(Fingerprint(token.data(), token.size())) !=
      soft_tokens_.end();
}
&lt;/code&gt;
&lt;p&gt;Precompute a TensorFlow graph execution node property that allows us to quickly rule out certain unusual cases.&lt;/p&gt;&lt;p&gt;executor.cc&lt;/p&gt;&lt;code&gt;struct NodeItem {
  ...
  bool kernel_is_expensive = false;  // True iff kernel-&amp;gt;IsExpensive()
  bool kernel_is_async = false;      // True iff kernel-&amp;gt;AsAsync() != nullptr
  bool is_merge = false;             // True iff IsMerge(node)
  ...
  if (IsEnter(node)) {
  ...
  } else if (IsExit(node)) {
  ...
  } else if (IsNextIteration(node)) {
  ...
  } else {
    // Normal path for most nodes
    ...
  }
&lt;/code&gt;
&lt;code&gt;struct NodeItem {
  ...
  bool kernel_is_expensive : 1;  // True iff kernel-&amp;gt;IsExpensive()
  bool kernel_is_async : 1;      // True iff kernel-&amp;gt;AsAsync() != nullptr
  bool is_merge : 1;             // True iff IsMerge(node)
  bool is_enter : 1;             // True iff IsEnter(node)
  bool is_exit : 1;              // True iff IsExit(node)
  bool is_control_trigger : 1;   // True iff IsControlTrigger(node)
  bool is_sink : 1;              // True iff IsSink(node)
  // True iff IsEnter(node) || IsExit(node) || IsNextIteration(node)
  bool is_enter_exit_or_next_iter : 1;
  ...
  if (!item-&amp;gt;is_enter_exit_or_next_iter) {
    // Fast path for nodes types that don't need special handling
    DCHECK_EQ(input_frame, output_frame);
    ...
  } else if (item-&amp;gt;is_enter) {
  ...
  } else if (item-&amp;gt;is_exit) {
  ...
  } else {
    DCHECK(IsNextIteration(node));
    ...
  }
&lt;/code&gt;
&lt;p&gt;Precompute 256 element array and use during trigram initialization.&lt;/p&gt;&lt;p&gt;byte_trigram_classifier.cc&lt;/p&gt;&lt;code&gt;void ByteTrigramClassifier::VerifyModel(void) const {
  ProbT class_sums[num_classes_];
  for (int cls = 0; cls &amp;lt; num_classes_; cls++) {
    class_sums[cls] = 0;
  }
  for (ByteNgramId id = 0; id &amp;lt; trigrams_.num_trigrams(); id++) {
    for (int cls = 0; cls &amp;lt; num_classes_; ++cls) {
      class_sums[cls] += Prob(trigram_probs_[id].log_probs[cls]);
    }
  }
  ...
}                         
&lt;/code&gt;
&lt;code&gt;void ByteTrigramClassifier::VerifyModel(void) const {
  CHECK_EQ(sizeof(ByteLogProbT), 1);
  ProbT fast_prob[256];
  for (int b = 0; b &amp;lt; 256; b++) {
    fast_prob[b] = Prob(static_cast&amp;lt;ByteLogProbT&amp;gt;(b));
  }

  ProbT class_sums[num_classes_];
  for (int cls = 0; cls &amp;lt; num_classes_; cls++) {
    class_sums[cls] = 0;
  }
  for (ByteNgramId id = 0; id &amp;lt; trigrams_.num_trigrams(); id++) {
    for (int cls = 0; cls &amp;lt; num_classes_; ++cls) {
      class_sums[cls] += fast_prob[trigram_probs_[id].log_probs[cls]];
    }
  }
  ...
}                         
&lt;/code&gt;
&lt;p&gt;General advice: check for malformed inputs at module boundaries instead of repeating checks internally.&lt;/p&gt;&lt;p&gt;Move bounds computation outside loop.&lt;/p&gt;&lt;p&gt;literal_linearizer.cc&lt;/p&gt;&lt;code&gt;for (int64 i = 0; i &amp;lt; src_shape.dimensions(dimension_numbers.front());
     ++i) {
&lt;/code&gt;
&lt;code&gt;int64 dim_front = src_shape.dimensions(dimension_numbers.front());
const uint8* src_buffer_data = src_buffer.data();
uint8* dst_buffer_data = dst_buffer.data();
for (int64 i = 0; i &amp;lt; dim_front; ++i) {
&lt;/code&gt;
&lt;p&gt;Defer GetSubSharding call until needed, which reduces 43 seconds of CPU time to 2 seconds.&lt;/p&gt;&lt;p&gt;sharding_propagation.cc&lt;/p&gt;&lt;code&gt;HloSharding alternative_sub_sharding =
    user.sharding().GetSubSharding(user.shape(), {i});
if (user.operand(i) == &amp;amp;instruction &amp;amp;&amp;amp;
    hlo_sharding_util::IsShardingMoreSpecific(alternative_sub_sharding,
                                              sub_sharding)) {
  sub_sharding = alternative_sub_sharding;
}
&lt;/code&gt;
&lt;code&gt;if (user.operand(i) == &amp;amp;instruction) {
  // Only evaluate GetSubSharding if this operand is of interest,
  // as it is relatively expensive.
  HloSharding alternative_sub_sharding =
      user.sharding().GetSubSharding(user.shape(), {i});
  if (hlo_sharding_util::IsShardingMoreSpecific(
          alternative_sub_sharding, sub_sharding)) {
    sub_sharding = alternative_sub_sharding;
  }
}
&lt;/code&gt;
&lt;p&gt;Don't update stats eagerly; compute them on demand.&lt;/p&gt;&lt;p&gt;Do not update stats on the very frequent allocation/deallocation calls. Instead, compute stats on demand when the much less frequently called Stats() method is invoked.&lt;/p&gt;&lt;p&gt;Preallocate 10 nodes not 200 for query handling in Google's web server.&lt;/p&gt;&lt;p&gt;A simple change that reduced web server's CPU usage by 7.5%.&lt;/p&gt;&lt;p&gt;querytree.h&lt;/p&gt;&lt;code&gt;static const int kInitParseTreeSize = 200;   // initial size of querynode pool
&lt;/code&gt;
&lt;code&gt;static const int kInitParseTreeSize = 10;   // initial size of querynode pool
&lt;/code&gt;
&lt;p&gt;Change search order for 19% throughput improvement.&lt;/p&gt;&lt;p&gt;An old search system (circa 2000) had two tiers: one contained a full-text index, and the other tier contained just the index for the title and anchor terms. We used to search the smaller title/anchor tier first. Counter-intuitively, we found that it is cheaper to search the larger full-text index tier first since if we reach the end of the full-text tier, we can entirely skip searching the title/anchor tier (a subset of the full-text tier). This happened reasonably often and allowed us to reduce the average number of disk seeks to process a query.&lt;/p&gt;&lt;p&gt;See discussion of title and anchor text handling in The Anatomy of a Large-Scale Hypertextual Web Search Engine for background information.&lt;/p&gt;&lt;p&gt;A particular performance-sensitive call-site may not need the full generality provided by a general-purpose library. Consider writing specialized code in such cases instead of calling the general-purpose code if it provides a performance improvement.&lt;/p&gt;&lt;p&gt;Custom printing code for Histogram class is 4x as fast as sprintf.&lt;/p&gt;&lt;p&gt;This code is performance sensitive because it is invoked when monitoring systems gather statistics from various servers.&lt;/p&gt;&lt;p&gt;histogram_export.cc&lt;/p&gt;&lt;code&gt;void Histogram::PopulateBuckets(const string &amp;amp;prefix,
                                expvar::MapProto *const var) const {
                                ...
  for (int i = min_bucket; i &amp;lt;= max_bucket; ++i) {
    const double count = BucketCount(i);
    if (!export_empty_buckets &amp;amp;&amp;amp; count == 0.0) continue;
    acc += count;
    // The label format of exported buckets for discrete histograms
    // specifies an inclusive upper bound, which is the same as in
    // the original Histogram implementation.  This format is not
    // applicable to non-discrete histograms, so a half-open interval
    // is used for them, with "_" instead of "-" as a separator to
    // make possible to distinguish the formats.
    string key =
        options_.export_cumulative_counts() ?
            StringPrintf("%.12g", boundaries_-&amp;gt;BucketLimit(i)) :
        options_.discrete() ?
            StringPrintf("%.0f-%.0f",
                         ceil(boundaries_-&amp;gt;BucketStart(i)),
                         ceil(boundaries_-&amp;gt;BucketLimit(i)) - 1.0) :
            StringPrintf("%.12g_%.12g",
                         boundaries_-&amp;gt;BucketStart(i),
                         boundaries_-&amp;gt;BucketLimit(i));
    EscapeMapKey(&amp;amp;key);
    const double value = options_.export_cumulative_counts() ? acc : count;
    expvar::AddMapFloat(StrCat(prefix,
                               options_.export_bucket_key_prefix(),
                               key),
                        value * count_mult,
                        var);
  }
&lt;/code&gt;
&lt;code&gt;// Format "val" according to format.  If "need_escape" is true, then the
// format can produce output with a '.' in it, and the result will be escaped.
// If "need_escape" is false, then the caller guarantees that format is
// such that the resulting number will not have any '.' characters and
// therefore we can avoid calling EscapeKey.
// The function is free to use "*scratch" for scratch space if necessary,
// and the resulting StringPiece may point into "*scratch".
static StringPiece FormatNumber(const char* format,
                                bool need_escape,
                                double val, string* scratch) {
  // This routine is specialized to work with only a limited number of formats
  DCHECK(StringPiece(format) == "%.0f" || StringPiece(format) == "%.12g");

  scratch-&amp;gt;clear();
  if (val == trunc(val) &amp;amp;&amp;amp; val &amp;gt;= kint32min &amp;amp;&amp;amp; val &amp;lt;= kint32max) {
    // An integer for which we can just use StrAppend
    StrAppend(scratch, static_cast&amp;lt;int32&amp;gt;(val));
    return StringPiece(*scratch);
  } else if (isinf(val)) {
    // Infinity, represent as just 'inf'.
    return StringPiece("inf", 3);
  } else {
    // Format according to "format", and possibly escape.
    StringAppendF(scratch, format, val);
    if (need_escape) {
      EscapeMapKey(scratch);
    } else {
      DCHECK(!StringPiece(*scratch).contains("."));
    }
    return StringPiece(*scratch);
  }
}
...
void Histogram::PopulateBuckets(const string &amp;amp;prefix,
                                expvar::MapProto *const var) const {
                                ...
  const string full_key_prefix = StrCat(prefix,
                                        options_.export_bucket_key_prefix());
  string key = full_key_prefix;  // Keys will start with "full_key_prefix".
  string start_scratch;
  string limit_scratch;
  const bool cumul_counts = options_.export_cumulative_counts();
  const bool discrete = options_.discrete();
  for (int i = min_bucket; i &amp;lt;= max_bucket; ++i) {
    const double count = BucketCount(i);
    if (!export_empty_buckets &amp;amp;&amp;amp; count == 0.0) continue;
    acc += count;
    // The label format of exported buckets for discrete histograms
    // specifies an inclusive upper bound, which is the same as in
    // the original Histogram implementation.  This format is not
    // applicable to non-discrete histograms, so a half-open interval
    // is used for them, with "_" instead of "-" as a separator to
    // make possible to distinguish the formats.
    key.resize(full_key_prefix.size());  // Start with full_key_prefix.
    DCHECK_EQ(key, full_key_prefix);

    const double limit = boundaries_-&amp;gt;BucketLimit(i);
    if (cumul_counts) {
      StrAppend(&amp;amp;key, FormatNumber("%.12g", true, limit, &amp;amp;limit_scratch));
    } else {
      const double start = boundaries_-&amp;gt;BucketStart(i);
      if (discrete) {
        StrAppend(&amp;amp;key,
                  FormatNumber("%.0f", false, ceil(start), &amp;amp;start_scratch),
                  "-",
                  FormatNumber("%.0f", false, ceil(limit) - 1.0,
                               &amp;amp;limit_scratch));
      } else {
        StrAppend(&amp;amp;key,
                  FormatNumber("%.12g", true, start, &amp;amp;start_scratch),
                  "_",
                  FormatNumber("%.12g", true, limit, &amp;amp;limit_scratch));
      }
    }
    const double value = cumul_counts ? acc : count;

    // Add to map var
    expvar::AddMapFloat(key, value * count_mult, var);
  }
}
&lt;/code&gt;
&lt;p&gt;Add specializations for VLOG(1), VLOG(2), … for speed and smaller code size.&lt;/p&gt;&lt;p&gt;&lt;code&gt;VLOG&lt;/code&gt; is a heavily used macro throughout the code base. This change avoids
passing an extra integer constant at nearly every call site (if the log level is
constant at the call site, as it almost always is, as in &lt;code&gt;VLOG(1) &amp;lt;&amp;lt; ...&lt;/code&gt;),
which saves code space.&lt;/p&gt;&lt;p&gt;vlog_is_on.h&lt;/p&gt;&lt;code&gt;class VLogSite final {
 public:
  ...
  bool IsEnabled(int level) {
    int stale_v = v_.load(std::memory_order_relaxed);
    if (ABSL_PREDICT_TRUE(level &amp;gt; stale_v)) {
      return false;
    }

    // We put everything other than the fast path, i.e. vlogging is initialized
    // but not on, behind an out-of-line function to reduce code size.
    return SlowIsEnabled(stale_v, level);
  }
  ...
 private:
  ...
  ABSL_ATTRIBUTE_NOINLINE
  bool SlowIsEnabled(int stale_v, int level);
  ...
};
&lt;/code&gt;
&lt;code&gt;class VLogSite final {
 public:
  ...
  bool IsEnabled(int level) {
    int stale_v = v_.load(std::memory_order_relaxed);
    if (ABSL_PREDICT_TRUE(level &amp;gt; stale_v)) {
      return false;
    }

    // We put everything other than the fast path, i.e. vlogging is initialized
    // but not on, behind an out-of-line function to reduce code size.
    // "level" is almost always a call-site constant, so we can save a bit
    // of code space by special-casing for levels 1, 2, and 3.
#if defined(__has_builtin) &amp;amp;&amp;amp; __has_builtin(__builtin_constant_p)
    if (__builtin_constant_p(level)) {
      if (level == 0) return SlowIsEnabled0(stale_v);
      if (level == 1) return SlowIsEnabled1(stale_v);
      if (level == 2) return SlowIsEnabled2(stale_v);
      if (level == 3) return SlowIsEnabled3(stale_v);
      if (level == 4) return SlowIsEnabled4(stale_v);
      if (level == 5) return SlowIsEnabled5(stale_v);
    }
#endif
    return SlowIsEnabled(stale_v, level);
    ...
 private:
  ...
  ABSL_ATTRIBUTE_NOINLINE
  bool SlowIsEnabled(int stale_v, int level);
  ABSL_ATTRIBUTE_NOINLINE bool SlowIsEnabled0(int stale_v);
  ABSL_ATTRIBUTE_NOINLINE bool SlowIsEnabled1(int stale_v);
  ABSL_ATTRIBUTE_NOINLINE bool SlowIsEnabled2(int stale_v);
  ABSL_ATTRIBUTE_NOINLINE bool SlowIsEnabled3(int stale_v);
  ABSL_ATTRIBUTE_NOINLINE bool SlowIsEnabled4(int stale_v);
  ABSL_ATTRIBUTE_NOINLINE bool SlowIsEnabled5(int stale_v);
  ...
};
&lt;/code&gt;
&lt;p&gt;vlog_is_on.cc&lt;/p&gt;&lt;code&gt;bool VLogSite::SlowIsEnabled0(int stale_v) { return SlowIsEnabled(stale_v, 0); }
bool VLogSite::SlowIsEnabled1(int stale_v) { return SlowIsEnabled(stale_v, 1); }
bool VLogSite::SlowIsEnabled2(int stale_v) { return SlowIsEnabled(stale_v, 2); }
bool VLogSite::SlowIsEnabled3(int stale_v) { return SlowIsEnabled(stale_v, 3); }
bool VLogSite::SlowIsEnabled4(int stale_v) { return SlowIsEnabled(stale_v, 4); }
bool VLogSite::SlowIsEnabled5(int stale_v) { return SlowIsEnabled(stale_v, 5); }
&lt;/code&gt;
&lt;p&gt;Replace RE2 call with a simple prefix match when possible.&lt;/p&gt;&lt;p&gt;read_matcher.cc&lt;/p&gt;&lt;code&gt;enum MatchItemType {
  MATCH_TYPE_INVALID,
  MATCH_TYPE_RANGE,
  MATCH_TYPE_EXACT,
  MATCH_TYPE_REGEXP,
};
&lt;/code&gt;
&lt;code&gt;enum MatchItemType {
  MATCH_TYPE_INVALID,
  MATCH_TYPE_RANGE,
  MATCH_TYPE_EXACT,
  MATCH_TYPE_REGEXP,
  MATCH_TYPE_PREFIX,   // Special type for regexp ".*"
};
&lt;/code&gt;
&lt;p&gt;read_matcher.cc&lt;/p&gt;&lt;code&gt;p-&amp;gt;type = MATCH_TYPE_REGEXP;
&lt;/code&gt;
&lt;code&gt;term.NonMetaPrefix().CopyToString(&amp;amp;p-&amp;gt;prefix);
if (term.RegexpSuffix() == ".*") {
  // Special case for a regexp that matches anything, so we can
  // bypass RE2::FullMatch
  p-&amp;gt;type = MATCH_TYPE_PREFIX;
} else {
  p-&amp;gt;type = MATCH_TYPE_REGEXP;
&lt;/code&gt;
&lt;p&gt;Use StrCat rather than StringPrintf to format IP addresses.&lt;/p&gt;&lt;p&gt;ipaddress.cc&lt;/p&gt;&lt;code&gt;string IPAddress::ToString() const {
  char buf[INET6_ADDRSTRLEN];

  switch (address_family_) {
    case AF_INET:
      CHECK(inet_ntop(AF_INET, &amp;amp;addr_.addr4, buf, INET6_ADDRSTRLEN) != NULL);
      return buf;
    case AF_INET6:
      CHECK(inet_ntop(AF_INET6, &amp;amp;addr_.addr6, buf, INET6_ADDRSTRLEN) != NULL);
      return buf;
    case AF_UNSPEC:
      LOG(DFATAL) &amp;lt;&amp;lt; "Calling ToString() on an empty IPAddress";
      return "";
    default:
      LOG(FATAL) &amp;lt;&amp;lt; "Unknown address family " &amp;lt;&amp;lt; address_family_;
  }
}
...
string IPAddressToURIString(const IPAddress&amp;amp; ip) {
  switch (ip.address_family()) {
    case AF_INET6:
      return StringPrintf("[%s]", ip.ToString().c_str());
    default:
      return ip.ToString();
  }
}
...
string SocketAddress::ToString() const {
  return IPAddressToURIString(host_) + StringPrintf(":%u", port_);
}
&lt;/code&gt;
&lt;code&gt;string IPAddress::ToString() const {
  char buf[INET6_ADDRSTRLEN];

  switch (address_family_) {
    case AF_INET: {
      uint32 addr = gntohl(addr_.addr4.s_addr);
      int a1 = static_cast&amp;lt;int&amp;gt;((addr &amp;gt;&amp;gt; 24) &amp;amp; 0xff);
      int a2 = static_cast&amp;lt;int&amp;gt;((addr &amp;gt;&amp;gt; 16) &amp;amp; 0xff);
      int a3 = static_cast&amp;lt;int&amp;gt;((addr &amp;gt;&amp;gt; 8) &amp;amp; 0xff);
      int a4 = static_cast&amp;lt;int&amp;gt;(addr &amp;amp; 0xff);
      return StrCat(a1, ".", a2, ".", a3, ".", a4);
    }
    case AF_INET6:
      CHECK(inet_ntop(AF_INET6, &amp;amp;addr_.addr6, buf, INET6_ADDRSTRLEN) != NULL);
      return buf;
    case AF_UNSPEC:
      LOG(DFATAL) &amp;lt;&amp;lt; "Calling ToString() on an empty IPAddress";
      return "";
    default:
      LOG(FATAL) &amp;lt;&amp;lt; "Unknown address family " &amp;lt;&amp;lt; address_family_;
  }
}
...
string IPAddressToURIString(const IPAddress&amp;amp; ip) {
  switch (ip.address_family()) {
    case AF_INET6:
      return StrCat("[", ip.ToString(), "]");
    default:
      return ip.ToString();
  }
}
...
string SocketAddress::ToString() const {
  return StrCat(IPAddressToURIString(host_), ":", port_);
}
&lt;/code&gt;
&lt;p&gt;Cache based on precomputed fingerprint of large serialized proto.&lt;/p&gt;&lt;p&gt;dp_ops.cc&lt;/p&gt;&lt;code&gt;InputOutputMappingProto mapping_proto;
PLAQUE_OP_REQUIRES(
    mapping_proto.ParseFromStringPiece(GetAttrMappingProto(state)),
    absl::InternalError("Failed to parse InputOutputMappingProto"));
ParseMapping(mapping_proto);
&lt;/code&gt;
&lt;code&gt;uint64 mapping_proto_fp = GetAttrMappingProtoFp(state);
{
  absl::MutexLock l(&amp;amp;fp_to_iometa_mu);
  if (fp_to_iometa == nullptr) {
    fp_to_iometa =
        new absl::flat_hash_map&amp;lt;uint64, std::unique_ptr&amp;lt;ProgramIOMetadata&amp;gt;&amp;gt;;
  }
  auto it = fp_to_iometa-&amp;gt;find(mapping_proto_fp);
  if (it != fp_to_iometa-&amp;gt;end()) {
    io_metadata_ = it-&amp;gt;second.get();
  } else {
    auto serial_proto = GetAttrMappingProto(state);
    DCHECK_EQ(mapping_proto_fp, Fingerprint(serial_proto));
    InputOutputMappingProto mapping_proto;
    PLAQUE_OP_REQUIRES(
        mapping_proto.ParseFromStringPiece(GetAttrMappingProto(state)),
        absl::InternalError("Failed to parse InputOutputMappingProto"));
    auto io_meta = ParseMapping(mapping_proto);
    io_metadata_ = io_meta.get();
    (*fp_to_iometa)[mapping_proto_fp] = std::move(io_meta);
  }
}
&lt;/code&gt;
&lt;p&gt;The compiler may have trouble optimizing through layers of abstractions because it must make conservative assumptions about the overall behavior of the code, or may not make the right speed vs. size tradeoffs. The application programmer will often know more about the behavior of the system and can aid the compiler by rewriting the code to operate at a lower level. However, only do this when profiles show an issue since compilers will often get things right on their own. Looking at the generated assembly code for performance critical routines can help you understand if the compiler is “getting it right”. Pprof provides a very helpful display of source code interleaved with disassembly and annotated with performance data.&lt;/p&gt;&lt;p&gt;Some techniques that may be useful:&lt;/p&gt;&lt;p&gt;Speed up ShapeUtil::ForEachState by replacing absl::Span with raw pointers to the underlying arrays.&lt;/p&gt;&lt;p&gt;shape_util.h&lt;/p&gt;&lt;code&gt;struct ForEachState {
  ForEachState(const Shape&amp;amp; s, absl::Span&amp;lt;const int64_t&amp;gt; b,
               absl::Span&amp;lt;const int64_t&amp;gt; c, absl::Span&amp;lt;const int64_t&amp;gt; i);
  ~ForEachState();

  const Shape&amp;amp; shape;
  const absl::Span&amp;lt;const int64_t&amp;gt; base;
  const absl::Span&amp;lt;const int64_t&amp;gt; count;
  const absl::Span&amp;lt;const int64_t&amp;gt; incr;
&lt;/code&gt;
&lt;code&gt;struct ForEachState {
  ForEachState(const Shape&amp;amp; s, absl::Span&amp;lt;const int64_t&amp;gt; b,
               absl::Span&amp;lt;const int64_t&amp;gt; c, absl::Span&amp;lt;const int64_t&amp;gt; i);
  inline ~ForEachState() = default;

  const Shape&amp;amp; shape;
  // Pointers to arrays of the passed-in spans
  const int64_t* const base;
  const int64_t* const count;
  const int64_t* const incr;
&lt;/code&gt;
&lt;p&gt;Hand unroll cyclic redundancy check (CRC) computation loop.&lt;/p&gt;&lt;p&gt;crc.cc&lt;/p&gt;&lt;code&gt;void CRC32::Extend(uint64 *lo, uint64 *hi, const void *bytes, size_t length)
                      const {
                      ...
  // Process bytes 4 at a time
  while ((p + 4) &amp;lt;= e) {
    uint32 c = l ^ WORD(p);
    p += 4;
    l = this-&amp;gt;table3_[c &amp;amp; 0xff] ^
        this-&amp;gt;table2_[(c &amp;gt;&amp;gt; 8) &amp;amp; 0xff] ^
        this-&amp;gt;table1_[(c &amp;gt;&amp;gt; 16) &amp;amp; 0xff] ^
        this-&amp;gt;table0_[c &amp;gt;&amp;gt; 24];
  }

  // Process the last few bytes
  while (p != e) {
    int c = (l &amp;amp; 0xff) ^ *p++;
    l = this-&amp;gt;table0_[c] ^ (l &amp;gt;&amp;gt; 8);
  }
  *lo = l;
}
&lt;/code&gt;
&lt;code&gt;void CRC32::Extend(uint64 *lo, uint64 *hi, const void *bytes, size_t length)
                      const {
                      ...
#define STEP {                                  \
    uint32 c = l ^ WORD(p);                     \
    p += 4;                                     \
    l = this-&amp;gt;table3_[c &amp;amp; 0xff] ^               \
        this-&amp;gt;table2_[(c &amp;gt;&amp;gt; 8) &amp;amp; 0xff] ^        \
        this-&amp;gt;table1_[(c &amp;gt;&amp;gt; 16) &amp;amp; 0xff] ^       \
        this-&amp;gt;table0_[c &amp;gt;&amp;gt; 24];                 \
}

  // Process bytes 16 at a time
  while ((e-p) &amp;gt;= 16) {
    STEP;
    STEP;
    STEP;
    STEP;
  }

  // Process bytes 4 at a time
  while ((p + 4) &amp;lt;= e) {
    STEP;
  }
#undef STEP

  // Process the last few bytes
  while (p != e) {
    int c = (l &amp;amp; 0xff) ^ *p++;
    l = this-&amp;gt;table0_[c] ^ (l &amp;gt;&amp;gt; 8);
  }
  *lo = l;
}

&lt;/code&gt;
&lt;p&gt;Handle four characters at a time when parsing Spanner keys.&lt;/p&gt;&lt;p&gt;Hand unroll loop to deal with four characters at a time rather than using memchr&lt;/p&gt;&lt;p&gt;Manually unroll loop for finding separated sections of name&lt;/p&gt;&lt;p&gt;Go backwards to find separated portions of a name with '#' separators (rather than forwards) since the first part is likely the longest in the name.&lt;/p&gt;&lt;p&gt;key.cc&lt;/p&gt;&lt;code&gt;void Key::InitSeps(const char* start) {
  const char* base = &amp;amp;rep_[0];
  const char* limit = base + rep_.size();
  const char* s = start;

  DCHECK_GE(s, base);
  DCHECK_LT(s, limit);

  for (int i = 0; i &amp;lt; 3; i++) {
    s = (const char*)memchr(s, '#', limit - s);
    DCHECK(s != NULL);
    seps_[i] = s - base;
    s++;
  }
}
&lt;/code&gt;
&lt;code&gt;inline const char* ScanBackwardsForSep(const char* base, const char* p) {
  while (p &amp;gt;= base + 4) {
    if (p[0] == '#') return p;
    if (p[-1] == '#') return p-1;
    if (p[-2] == '#') return p-2;
    if (p[-3] == '#') return p-3;
    p -= 4;
  }
  while (p &amp;gt;= base &amp;amp;&amp;amp; *p != '#') p--;
  return p;
}

void Key::InitSeps(const char* start) {
  const char* base = &amp;amp;rep_[0];
  const char* limit = base + rep_.size();
  const char* s = start;

  DCHECK_GE(s, base);
  DCHECK_LT(s, limit);

  // We go backwards from the end of the string, rather than forwards,
  // since the directory name might be long and definitely doesn't contain
  // any '#' characters.
  const char* p = ScanBackwardsForSep(s, limit - 1);
  DCHECK(*p == '#');
  seps_[2] = p - base;
  p--;

  p = ScanBackwardsForSep(s, p);
  DCHECK(*p == '#');
  seps_[1] = p - base;
  p--;

  p = ScanBackwardsForSep(s, p);
  DCHECK(*p == '#');
  seps_[0] = p - base;
}
&lt;/code&gt;
&lt;p&gt;Avoid frame setup costs by converting ABSL_LOG(FATAL) to ABSL_DCHECK(false).&lt;/p&gt;&lt;p&gt;arena_cleanup.h&lt;/p&gt;&lt;code&gt;inline ABSL_ATTRIBUTE_ALWAYS_INLINE size_t Size(Tag tag) {
  if (!EnableSpecializedTags()) return sizeof(DynamicNode);

  switch (tag) {
    case Tag::kDynamic:
      return sizeof(DynamicNode);
    case Tag::kString:
      return sizeof(TaggedNode);
    case Tag::kCord:
      return sizeof(TaggedNode);
    default:
      ABSL_LOG(FATAL) &amp;lt;&amp;lt; "Corrupted cleanup tag: " &amp;lt;&amp;lt; static_cast&amp;lt;int&amp;gt;(tag);
      return sizeof(DynamicNode);
  }
}
&lt;/code&gt;
&lt;code&gt;inline ABSL_ATTRIBUTE_ALWAYS_INLINE size_t Size(Tag tag) {
  if (!EnableSpecializedTags()) return sizeof(DynamicNode);

  switch (tag) {
    case Tag::kDynamic:
      return sizeof(DynamicNode);
    case Tag::kString:
      return sizeof(TaggedNode);
    case Tag::kCord:
      return sizeof(TaggedNode);
    default:
      ABSL_DCHECK(false) &amp;lt;&amp;lt; "Corrupted cleanup tag: " &amp;lt;&amp;lt; static_cast&amp;lt;int&amp;gt;(tag);
      return sizeof(DynamicNode);
  }
}
&lt;/code&gt;
&lt;p&gt;Balance the utility of stats and other behavioral information about a system against the cost of maintaining that information. The extra information can often help people to understand and improve high-level behavior, but can also be costly to maintain.&lt;/p&gt;&lt;p&gt;Stats that are not useful can be dropped altogether.&lt;/p&gt;&lt;p&gt;Stop maintaining expensive stats about number of alarms and closures in SelectServer.&lt;/p&gt;&lt;p&gt;Part of changes that reduce time for setting an alarm from 771 ns to 271 ns.&lt;/p&gt;&lt;p&gt;selectserver.h&lt;/p&gt;&lt;code&gt;class SelectServer {
 public:
 ...
 protected:
  ...
  scoped_ptr&amp;lt;MinuteTenMinuteHourStat&amp;gt; num_alarms_stat_;
  ...
  scoped_ptr&amp;lt;MinuteTenMinuteHourStat&amp;gt; num_closures_stat_;
  ...
};
&lt;/code&gt;
&lt;code&gt;// Selectserver class
class SelectServer {
 ...
 protected:
 ...
};
&lt;/code&gt;
&lt;p&gt;/selectserver.cc&lt;/p&gt;&lt;code&gt;void SelectServer::AddAlarmInternal(Alarmer* alarmer,
                                    int offset_in_ms,
                                    int id,
                                    bool is_periodic) {
                                    ...
  alarms_-&amp;gt;insert(alarm);
  num_alarms_stat_-&amp;gt;IncBy(1);
  ...
}
&lt;/code&gt;
&lt;code&gt;void SelectServer::AddAlarmInternal(Alarmer* alarmer,
                                    int offset_in_ms,
                                    int id,
                                    bool is_periodic) {
                                    ...
  alarms_-&amp;gt;Add(alarm);
  ...
}
&lt;/code&gt;
&lt;p&gt;/selectserver.cc&lt;/p&gt;&lt;code&gt;void SelectServer::RemoveAlarm(Alarmer* alarmer, int id) {
      ...
      alarms_-&amp;gt;erase(alarm);
      num_alarms_stat_-&amp;gt;IncBy(-1);
      ...
}
&lt;/code&gt;
&lt;code&gt;void SelectServer::RemoveAlarm(Alarmer* alarmer, int id) {
      ...
      alarms_-&amp;gt;Remove(alarm);
      ...
}
&lt;/code&gt;
&lt;p&gt;Often, stats or other properties can be maintained for a sample of the elements handled by the system (e.g., RPC requests, input records, users). Many subsystems use this approach (tcmalloc allocation tracking, /requestz status pages, Dapper samples).&lt;/p&gt;&lt;p&gt;When sampling, consider reducing the sampling rate when appropriate.&lt;/p&gt;&lt;p&gt;Maintain stats for just a sample of doc info requests.&lt;/p&gt;&lt;p&gt;Sampling allows us to avoid touching 39 histograms and MinuteTenMinuteHour stats for most requests.&lt;/p&gt;&lt;p&gt;generic-leaf-stats.cc&lt;/p&gt;&lt;code&gt;... code that touches 39 histograms to update various stats on every request ...
&lt;/code&gt;
&lt;code&gt;// Add to the histograms periodically
if (TryLockToUpdateHistogramsDocInfo(docinfo_stats, bucket)) {
  // Returns true and grabs bucket-&amp;gt;lock only if we should sample this
  // request for maintaining stats
  ... code that touches 39 histograms to update various stats ...
  bucket-&amp;gt;lock.Unlock();
}
&lt;/code&gt;

&lt;p&gt;Reduce sampling rate and make faster sampling decisions.&lt;/p&gt;&lt;p&gt;This change reduces the sampling rate from 1 in 10 to 1 in 32. Furthermore, we now keep execution time stats just for the sampled events and speed up sampling decisions by using a power of two modulus. This code is called on every packet in the Google Meet video conferencing system and needed performance work to keep up with capacity demands during the first part of the COVID outbreak as users rapidly migrated to doing more online meetings.&lt;/p&gt;&lt;p&gt;packet_executor.cc&lt;/p&gt;&lt;code&gt;class ScopedPerformanceMeasurement {
 public:
  explicit ScopedPerformanceMeasurement(PacketExecutor* packet_executor)
      : packet_executor_(packet_executor),
        tracer_(packet_executor-&amp;gt;packet_executor_trace_threshold_,
                kClosureTraceName) {
    // ThreadCPUUsage is an expensive call. At the time of writing,
    // it takes over 400ns, or roughly 30 times slower than absl::Now,
    // so we sample only 10% of closures to keep the cost down.
    if (packet_executor-&amp;gt;closures_executed_ % 10 == 0) {
      thread_cpu_usage_start_ = base::ThreadCPUUsage();
    }

    // Sample start time after potentially making the above expensive call,
    // so as not to pollute wall time measurements.
    run_start_time_ = absl::Now();
  }

  ~ScopedPerformanceMeasurement() {
&lt;/code&gt;
&lt;code&gt;ScopedPerformanceMeasurement::ScopedPerformanceMeasurement(
    PacketExecutor* packet_executor)
    : packet_executor_(packet_executor),
      tracer_(packet_executor-&amp;gt;packet_executor_trace_threshold_,
              kClosureTraceName) {
  // ThreadCPUUsage is an expensive call. At the time of writing,
  // it takes over 400ns, or roughly 30 times slower than absl::Now,
  // so we sample only 1 in 32 closures to keep the cost down.
  if (packet_executor-&amp;gt;closures_executed_ % 32 == 0) {
    thread_cpu_usage_start_ = base::ThreadCPUUsage();
  }

  // Sample start time after potentially making the above expensive call,
  // so as not to pollute wall time measurements.
  run_start_time_ = absl::Now();
}
&lt;/code&gt;
&lt;p&gt;packet_executor.cc&lt;/p&gt;&lt;code&gt;~ScopedPerformanceMeasurement() {
  auto run_end_time = absl::Now();
  auto run_duration = run_end_time - run_start_time_;

  if (thread_cpu_usage_start_.has_value()) {
  ...
  }

  closure_execution_time-&amp;gt;Record(absl::ToInt64Microseconds(run_duration));
&lt;/code&gt;
&lt;code&gt;ScopedPerformanceMeasurement::~ScopedPerformanceMeasurement() {
  auto run_end_time = absl::Now();
  auto run_duration = run_end_time - run_start_time_;

  if (thread_cpu_usage_start_.has_value()) {
    ...
    closure_execution_time-&amp;gt;Record(absl::ToInt64Microseconds(run_duration));
  }
&lt;/code&gt;
&lt;p&gt;Benchmark results:&lt;/p&gt;&lt;code&gt;Run on (40 X 2793 MHz CPUs); 2020-03-24T20:08:19.991412535-07:00
CPU: Intel Ivybridge with HyperThreading (20 cores) dL1:32KB dL2:256KB dL3:25MB
Benchmark                                      Base (ns)    New (ns) Improvement
----------------------------------------------------------------------------
BM_PacketOverhead_mean                               224          85    +62.0%
&lt;/code&gt;

&lt;p&gt;Logging statements can be costly, even if the logging-level for the statement doesn’t actually log anything. E.g., &lt;code&gt;ABSL_VLOG&lt;/code&gt;’s implementation requires at
least a load and a comparison, which may be a problem in hot code paths. In
addition, the presence of the logging code may inhibit compiler optimizations.
Consider dropping logging entirely from hot code paths.&lt;/p&gt;&lt;p&gt;Remove logging from guts of memory allocator.&lt;/p&gt;&lt;p&gt;This was a small part of a larger change.&lt;/p&gt;&lt;p&gt;gpu_bfc_allocator.cc&lt;/p&gt;&lt;code&gt;void GPUBFCAllocator::SplitChunk(...) {
  ...
  VLOG(6) &amp;lt;&amp;lt; "Adding to chunk map: " &amp;lt;&amp;lt; new_chunk-&amp;gt;ptr;
  ...
}
...
void GPUBFCAllocator::DeallocateRawInternal(void* ptr) {
  ...
  VLOG(6) &amp;lt;&amp;lt; "Chunk at " &amp;lt;&amp;lt; c-&amp;gt;ptr &amp;lt;&amp;lt; " no longer in use";
  ...
}
&lt;/code&gt;
&lt;code&gt;void GPUBFCAllocator::SplitChunk(...) {
...
}
...
void GPUBFCAllocator::DeallocateRawInternal(void* ptr) {
...
}
&lt;/code&gt;

&lt;p&gt;Precompute whether or not logging is enabled outside a nested loop.&lt;/p&gt;&lt;p&gt;image_similarity.cc&lt;/p&gt;&lt;code&gt;for (int j = 0; j &amp;lt; output_subimage_size_y; j++) {
  int j1 = j - rad + output_to_integral_subimage_y;
  int j2 = j1 + 2 * rad + 1;
  // Create a pointer for this row's output, taking into account the offset
  // to the full image.
  double *image_diff_ptr = &amp;amp;(*image_diff)(j + min_j, min_i);

  for (int i = 0; i &amp;lt; output_subimage_size_x; i++) {
    ...
    if (VLOG_IS_ON(3)) {
    ...
    }
    ...
  }
}
&lt;/code&gt;
&lt;code&gt;const bool vlog_3 = DEBUG_MODE ? VLOG_IS_ON(3) : false;

for (int j = 0; j &amp;lt; output_subimage_size_y; j++) {
  int j1 = j - rad + output_to_integral_subimage_y;
  int j2 = j1 + 2 * rad + 1;
  // Create a pointer for this row's output, taking into account the offset
  // to the full image.
  double *image_diff_ptr = &amp;amp;(*image_diff)(j + min_j, min_i);

  for (int i = 0; i &amp;lt; output_subimage_size_x; i++) {
    ...
    if (vlog_3) {
    ...
    }
  }
}
&lt;/code&gt;
&lt;code&gt;Run on (40 X 2801 MHz CPUs); 2016-05-16T15:55:32.250633072-07:00
CPU: Intel Ivybridge with HyperThreading (20 cores) dL1:32KB dL2:256KB dL3:25MB
Benchmark                          Base (ns)  New (ns) Improvement
------------------------------------------------------------------
BM_NCCPerformance/16                   29104     26372     +9.4%
BM_NCCPerformance/64                  473235    425281    +10.1%
BM_NCCPerformance/512               30246238  27622009     +8.7%
BM_NCCPerformance/1k              125651445  113361991     +9.8%
BM_NCCLimitedBoundsPerformance/16       8314      7498     +9.8%
BM_NCCLimitedBoundsPerformance/64     143508    132202     +7.9%
BM_NCCLimitedBoundsPerformance/512   9335684   8477567     +9.2%
BM_NCCLimitedBoundsPerformance/1k   37223897  34201739     +8.1%
&lt;/code&gt;

&lt;p&gt;Precompute whether logging is enabled and use the result in helper routines.&lt;/p&gt;&lt;p&gt;periodic_call.cc&lt;/p&gt;&lt;code&gt;  VLOG(1) &amp;lt;&amp;lt; Logid()
          &amp;lt;&amp;lt; "MaybeScheduleAlarmAtNextTick. Time until next real time: "
          &amp;lt;&amp;lt; time_until_next_real_time;
          ...
  uint64 next_virtual_time_ms =
      next_virtual_time_ms_ - num_ticks * kResolutionMs;
  CHECK_GE(next_virtual_time_ms, 0);
  ScheduleAlarm(now, delay, next_virtual_time_ms);
}

void ScheduleNextAlarm(uint64 current_virtual_time_ms)
    ABSL_EXCLUSIVE_LOCKS_REQUIRED(mutex_) {
  if (calls_.empty()) {
    VLOG(1) &amp;lt;&amp;lt; Logid() &amp;lt;&amp;lt; "No calls left, entering idle mode";
    next_real_time_ = absl::InfiniteFuture();
    return;
  }
  uint64 next_virtual_time_ms = FindNextVirtualTime(current_virtual_time_ms);
  auto delay =
      absl::Milliseconds(next_virtual_time_ms - current_virtual_time_ms);
  ScheduleAlarm(GetClock().TimeNow(), delay, next_virtual_time_ms);
}

// An alarm scheduled by this function supersedes all previously scheduled
// alarms. This is ensured through `scheduling_sequence_number_`.
void ScheduleAlarm(absl::Time now, absl::Duration delay,
                   uint64 virtual_time_ms)
    ABSL_EXCLUSIVE_LOCKS_REQUIRED(mutex_) {
  next_real_time_ = now + delay;
  next_virtual_time_ms_ = virtual_time_ms;
  ++ref_count_;  // The Alarm holds a reference.
  ++scheduling_sequence_number_;
  VLOG(1) &amp;lt;&amp;lt; Logid() &amp;lt;&amp;lt; "ScheduleAlarm. Time : "
          &amp;lt;&amp;lt; absl::FormatTime("%M:%S.%E3f", now, absl::UTCTimeZone())
          &amp;lt;&amp;lt; ", delay: " &amp;lt;&amp;lt; delay &amp;lt;&amp;lt; ", virtual time: " &amp;lt;&amp;lt; virtual_time_ms
          &amp;lt;&amp;lt; ", refs: " &amp;lt;&amp;lt; ref_count_
          &amp;lt;&amp;lt; ", seq: " &amp;lt;&amp;lt; scheduling_sequence_number_
          &amp;lt;&amp;lt; ", executor: " &amp;lt;&amp;lt; executor_;

  executor_-&amp;gt;AddAfter(
      delay, new Alarm(this, virtual_time_ms, scheduling_sequence_number_));
}
&lt;/code&gt;
&lt;code&gt;  const bool vlog_1 = VLOG_IS_ON(1);

  if (vlog_1) {
    VLOG(1) &amp;lt;&amp;lt; Logid()
            &amp;lt;&amp;lt; "MaybeScheduleAlarmAtNextTick. Time until next real time: "
            &amp;lt;&amp;lt; time_until_next_real_time;
  }
  ...
  uint64 next_virtual_time_ms =
      next_virtual_time_ms_ - num_ticks * kResolutionMs;
  CHECK_GE(next_virtual_time_ms, 0);
  ScheduleAlarm(now, delay, next_virtual_time_ms, vlog_1);
}

void ScheduleNextAlarm(uint64 current_virtual_time_ms, bool vlog_1)
    ABSL_EXCLUSIVE_LOCKS_REQUIRED(mutex_) {
  if (calls_.empty()) {
    if (vlog_1) {
      VLOG(1) &amp;lt;&amp;lt; Logid() &amp;lt;&amp;lt; "No calls left, entering idle mode";
    }
    next_real_time_ = absl::InfiniteFuture();
    return;
  }
  uint64 next_virtual_time_ms = FindNextVirtualTime(current_virtual_time_ms);
  auto delay =
      absl::Milliseconds(next_virtual_time_ms - current_virtual_time_ms);
  ScheduleAlarm(GetClock().TimeNow(), delay, next_virtual_time_ms, vlog_1);
}

// An alarm scheduled by this function supersedes all previously scheduled
// alarms. This is ensured through `scheduling_sequence_number_`.
void ScheduleAlarm(absl::Time now, absl::Duration delay,
                   uint64 virtual_time_ms,
                   bool vlog_1)
    ABSL_EXCLUSIVE_LOCKS_REQUIRED(mutex_) {
  next_real_time_ = now + delay;
  next_virtual_time_ms_ = virtual_time_ms;
  ++ref_count_;  // The Alarm holds a reference.
  ++scheduling_sequence_number_;
  if (vlog_1) {
    VLOG(1) &amp;lt;&amp;lt; Logid() &amp;lt;&amp;lt; "ScheduleAlarm. Time : "
            &amp;lt;&amp;lt; absl::FormatTime("%M:%S.%E3f", now, absl::UTCTimeZone())
            &amp;lt;&amp;lt; ", delay: " &amp;lt;&amp;lt; delay &amp;lt;&amp;lt; ", virtual time: " &amp;lt;&amp;lt; virtual_time_ms
            &amp;lt;&amp;lt; ", refs: " &amp;lt;&amp;lt; ref_count_
            &amp;lt;&amp;lt; ", seq: " &amp;lt;&amp;lt; scheduling_sequence_number_
            &amp;lt;&amp;lt; ", executor: " &amp;lt;&amp;lt; executor_;
  }

  executor_-&amp;gt;AddAfter(
      delay, new Alarm(this, virtual_time_ms, scheduling_sequence_number_));
}
&lt;/code&gt;
&lt;p&gt;Performance encompasses more than just runtime speed. Sometimes it is worth considering the effects of software choices on the size of generated code. Large code size means longer compile and link times, bloated binaries, more memory usage, more icache pressure, and other sometimes negative effects on microarchitectural structures like branch predictors, etc. Thinking about these issues is especially important when writing low-level library code that will be used in many places, or when writing templated code that you expect will be instantiated for many different types.&lt;/p&gt;&lt;p&gt;The techniques that are useful for reducing code size vary significantly across programming languages. Here are some techniques that we have found useful for C++ code (which can suffer from an over-use of templates and inlining).&lt;/p&gt;&lt;p&gt;Widely called functions combined with inlining can have a dramatic effect on code size.&lt;/p&gt;&lt;p&gt;Speed up TF_CHECK_OK.&lt;/p&gt;&lt;p&gt;Avoid creating Ok object, and save code space by doing complex formatting of fatal error message out of line instead of at every call site.&lt;/p&gt;&lt;p&gt;status.h&lt;/p&gt;&lt;code&gt;#define TF_CHECK_OK(val) CHECK_EQ(::tensorflow::Status::OK(), (val))
#define TF_QCHECK_OK(val) QCHECK_EQ(::tensorflow::Status::OK(), (val))
&lt;/code&gt;
&lt;code&gt;extern tensorflow::string* TfCheckOpHelperOutOfLine(
    const ::tensorflow::Status&amp;amp; v, const char* msg);
inline tensorflow::string* TfCheckOpHelper(::tensorflow::Status v,
                                           const char* msg) {
  if (v.ok()) return nullptr;
  return TfCheckOpHelperOutOfLine(v, msg);
}
#define TF_CHECK_OK(val)                                           \
  while (tensorflow::string* _result = TfCheckOpHelper(val, #val)) \
  LOG(FATAL) &amp;lt;&amp;lt; *(_result)
#define TF_QCHECK_OK(val)                                          \
  while (tensorflow::string* _result = TfCheckOpHelper(val, #val)) \
  LOG(QFATAL) &amp;lt;&amp;lt; *(_result)
&lt;/code&gt;
&lt;p&gt;status.cc&lt;/p&gt;&lt;code&gt;string* TfCheckOpHelperOutOfLine(const ::tensorflow::Status&amp;amp; v,
                                 const char* msg) {
  string r("Non-OK-status: ");
  r += msg;
  r += " status: ";
  r += v.ToString();
  // Leaks string but this is only to be used in a fatal error message
  return new string(r);
}
&lt;/code&gt;
&lt;p&gt;Shrink each RETURN_IF_ERROR call site by 79 bytes of code.&lt;/p&gt;&lt;p&gt;Improve performance of CHECK_GE by 4.5X and shrink code size from 125 bytes to 77 bytes.&lt;/p&gt;&lt;p&gt;logging.h&lt;/p&gt;&lt;code&gt;struct CheckOpString {
  CheckOpString(string* str) : str_(str) { }
  ~CheckOpString() { delete str_; }
  operator bool() const { return str_ == NULL; }
  string* str_;
};
...
#define DEFINE_CHECK_OP_IMPL(name, op) \
  template &amp;lt;class t1, class t2&amp;gt; \
  inline string* Check##name##Impl(const t1&amp;amp; v1, const t2&amp;amp; v2, \
                                   const char* names) { \
    if (v1 op v2) return NULL; \
    else return MakeCheckOpString(v1, v2, names); \
  } \
  string* Check##name##Impl(int v1, int v2, const char* names);
DEFINE_CHECK_OP_IMPL(EQ, ==)
DEFINE_CHECK_OP_IMPL(NE, !=)
DEFINE_CHECK_OP_IMPL(LE, &amp;lt;=)
DEFINE_CHECK_OP_IMPL(LT, &amp;lt; )
DEFINE_CHECK_OP_IMPL(GE, &amp;gt;=)
DEFINE_CHECK_OP_IMPL(GT, &amp;gt; )
#undef DEFINE_CHECK_OP_IMPL
&lt;/code&gt;
&lt;code&gt;struct CheckOpString {
  CheckOpString(string* str) : str_(str) { }
  // No destructor: if str_ is non-NULL, we're about to LOG(FATAL),
  // so there's no point in cleaning up str_.
  operator bool() const { return str_ == NULL; }
  string* str_;
};
...
extern string* MakeCheckOpStringIntInt(int v1, int v2, const char* names);

template&amp;lt;int, int&amp;gt;
string* MakeCheckOpString(const int&amp;amp; v1, const int&amp;amp; v2, const char* names) {
  return MakeCheckOpStringIntInt(v1, v2, names);
}
...
#define DEFINE_CHECK_OP_IMPL(name, op) \
  template &amp;lt;class t1, class t2&amp;gt; \
  inline string* Check##name##Impl(const t1&amp;amp; v1, const t2&amp;amp; v2, \
                                   const char* names) { \
    if (v1 op v2) return NULL; \
    else return MakeCheckOpString(v1, v2, names); \
  } \
  inline string* Check##name##Impl(int v1, int v2, const char* names) { \
    if (v1 op v2) return NULL; \
    else return MakeCheckOpString(v1, v2, names); \
  }
DEFINE_CHECK_OP_IMPL(EQ, ==)
DEFINE_CHECK_OP_IMPL(NE, !=)
DEFINE_CHECK_OP_IMPL(LE, &amp;lt;=)
DEFINE_CHECK_OP_IMPL(LT, &amp;lt; )
DEFINE_CHECK_OP_IMPL(GE, &amp;gt;=)
DEFINE_CHECK_OP_IMPL(GT, &amp;gt; )
#undef DEFINE_CHECK_OP_IMPL
&lt;/code&gt;
&lt;p&gt;logging.cc&lt;/p&gt;&lt;code&gt;string* MakeCheckOpStringIntInt(int v1, int v2, const char* names) {
  strstream ss;
  ss &amp;lt;&amp;lt; names &amp;lt;&amp;lt; " (" &amp;lt;&amp;lt; v1 &amp;lt;&amp;lt; " vs. " &amp;lt;&amp;lt; v2 &amp;lt;&amp;lt; ")";
  return new string(ss.str(), ss.pcount());
}
&lt;/code&gt;
&lt;p&gt;Inlining can often improve performance, but sometimes it can increase code size without a corresponding performance payoff (and in some case even a performance loss due to increased instruction cache pressure).&lt;/p&gt;&lt;p&gt;Reduce inlining in TensorFlow.&lt;/p&gt;&lt;p&gt;The change stops inlining many non-performance-sensitive functions (e.g., error paths and op registration code). Furthermore, slow paths of some performance-sensitive functions are moved into non-inlined functions.&lt;/p&gt;&lt;p&gt;These changes reduces the size of tensorflow symbols in a typical binary by 12.2% (8814545 bytes down to 7740233 bytes)&lt;/p&gt;&lt;p&gt;Protocol buffer library change. Avoid expensive inlined code space for encoding message length for messages ≥ 128 bytes and instead do a procedure call to a shared out-of-line routine.&lt;/p&gt;&lt;p&gt;Not only makes important large binaries smaller but also faster.&lt;/p&gt;&lt;p&gt;Bytes of generated code per line of a heavily inlined routine in one large binary. First number represents the total bytes generated for a particular source line including all locations where that code has been inlined.&lt;/p&gt;&lt;p&gt;Before:&lt;/p&gt;&lt;code&gt;.           0   1825 template &amp;lt;typename MessageType&amp;gt;
.           0   1826 inline uint8* WireFormatLite::InternalWriteMessage(
.           0   1827     int field_number, const MessageType&amp;amp; value, uint8* target,
.           0   1828     io::EpsCopyOutputStream* stream) {
&amp;gt;&amp;gt;&amp;gt;    389246   1829   target = WriteTagToArray(field_number, WIRETYPE_LENGTH_DELIMITED, target);
&amp;gt;&amp;gt;&amp;gt;   5454640   1830   target = io::CodedOutputStream::WriteVarint32ToArray(
&amp;gt;&amp;gt;&amp;gt;    337837   1831       static_cast&amp;lt;uint32&amp;gt;(value.GetCachedSize()), target);
&amp;gt;&amp;gt;&amp;gt;   1285539   1832   return value._InternalSerialize(target, stream);
.           0   1833 }
&lt;/code&gt;
&lt;p&gt;The new codesize output with this change looks like:&lt;/p&gt;&lt;code&gt;.           0   1825 template &amp;lt;typename MessageType&amp;gt;
.           0   1826 inline uint8* WireFormatLite::InternalWriteMessage(
.           0   1827     int field_number, const MessageType&amp;amp; value, uint8* target,
.           0   1828     io::EpsCopyOutputStream* stream) {
&amp;gt;&amp;gt;&amp;gt;    450612   1829   target = WriteTagToArray(field_number, WIRETYPE_LENGTH_DELIMITED, target);
&amp;gt;&amp;gt;       9609   1830   target = io::CodedOutputStream::WriteVarint32ToArrayOutOfLine(
&amp;gt;&amp;gt;&amp;gt;    434668   1831       static_cast&amp;lt;uint32&amp;gt;(value.GetCachedSize()), target);
&amp;gt;&amp;gt;&amp;gt;   1597394   1832   return value._InternalSerialize(target, stream);
.           0   1833 }
&lt;/code&gt;
&lt;p&gt;coded_stream.h&lt;/p&gt;&lt;code&gt;class PROTOBUF_EXPORT CodedOutputStream {
  ...
  // Like WriteVarint32()  but writing directly to the target array, and with the
  // less common-case paths being out of line rather than inlined.
  static uint8* WriteVarint32ToArrayOutOfLine(uint32 value, uint8* target);
  ...
};
...
inline uint8* CodedOutputStream::WriteVarint32ToArrayOutOfLine(uint32 value,
                                                               uint8* target) {
  target[0] = static_cast&amp;lt;uint8&amp;gt;(value);
  if (value &amp;lt; 0x80) {
    return target + 1;
  } else {
    return WriteVarint32ToArrayOutOfLineHelper(value, target);
  }
}
&lt;/code&gt;
&lt;p&gt;coded_stream.cc&lt;/p&gt;&lt;code&gt;uint8* CodedOutputStream::WriteVarint32ToArrayOutOfLineHelper(uint32 value,
                                                              uint8* target) {
  DCHECK_GE(value, 0x80);
  target[0] |= static_cast&amp;lt;uint8&amp;gt;(0x80);
  value &amp;gt;&amp;gt;= 7;
  target[1] = static_cast&amp;lt;uint8&amp;gt;(value);
  if (value &amp;lt; 0x80) {
    return target + 2;
  }
  target += 2;
  do {
    // Turn on continuation bit in the byte we just wrote.
    target[-1] |= static_cast&amp;lt;uint8&amp;gt;(0x80);
    value &amp;gt;&amp;gt;= 7;
    *target = static_cast&amp;lt;uint8&amp;gt;(value);
    ++target;
  } while (value &amp;gt;= 0x80);
  return target;
}
&lt;/code&gt;
&lt;p&gt;Reduce absl::flat_hash_set and absl::flat_hash_map code size.&lt;/p&gt;&lt;p&gt;Reduces sizes of some large binaries by ~0.5%.&lt;/p&gt;&lt;p&gt;Do not inline string allocation and deallocation when not using protobuf arenas.&lt;/p&gt;&lt;p&gt;public/arenastring.h&lt;/p&gt;&lt;code&gt;  if (IsDefault(default_value)) {
    std::string* new_string = new std::string();
    tagged_ptr_.Set(new_string);
    return new_string;
  } else {
    return UnsafeMutablePointer();
  }
}
&lt;/code&gt;
&lt;code&gt;  if (IsDefault(default_value)) {
    return SetAndReturnNewString();
  } else {
    return UnsafeMutablePointer();
  }
}
&lt;/code&gt;
&lt;p&gt;internal/arenastring.cc&lt;/p&gt;&lt;code&gt;std::string* ArenaStringPtr::SetAndReturnNewString() {
  std::string* new_string = new std::string();
  tagged_ptr_.Set(new_string);
  return new_string;
}
&lt;/code&gt;
&lt;p&gt;Avoid inlining some routines. Create variants of routines that take 'const char*' rather than 'const std::string&amp;amp;' to avoid std::string construction code at every call site.&lt;/p&gt;&lt;p&gt;op.h&lt;/p&gt;&lt;code&gt;class OpDefBuilderWrapper {
 public:
  explicit OpDefBuilderWrapper(const char name[]) : builder_(name) {}
  OpDefBuilderWrapper&amp;amp; Attr(std::string spec) {
    builder_.Attr(std::move(spec));
    return *this;
  }
  OpDefBuilderWrapper&amp;amp; Input(std::string spec) {
    builder_.Input(std::move(spec));
    return *this;
  }
  OpDefBuilderWrapper&amp;amp; Output(std::string spec) {
    builder_.Output(std::move(spec));
    return *this;
  }
&lt;/code&gt;
&lt;code&gt;class OpDefBuilderWrapper {
 public:
  explicit OpDefBuilderWrapper(const char name[]) : builder_(name) {}
  OpDefBuilderWrapper&amp;amp; Attr(std::string spec) {
    builder_.Attr(std::move(spec));
    return *this;
  }
  OpDefBuilderWrapper&amp;amp; Attr(const char* spec) TF_ATTRIBUTE_NOINLINE {
    return Attr(std::string(spec));
  }
  OpDefBuilderWrapper&amp;amp; Input(std::string spec) {
    builder_.Input(std::move(spec));
    return *this;
  }
  OpDefBuilderWrapper&amp;amp; Input(const char* spec) TF_ATTRIBUTE_NOINLINE {
    return Input(std::string(spec));
  }
  OpDefBuilderWrapper&amp;amp; Output(std::string spec) {
    builder_.Output(std::move(spec));
    return *this;
  }
  OpDefBuilderWrapper&amp;amp; Output(const char* spec) TF_ATTRIBUTE_NOINLINE {
    return Output(std::string(spec));
  }
&lt;/code&gt;
&lt;p&gt;Templated code can be duplicated for every possible combination of template arguments when it is instantiated.&lt;/p&gt;&lt;p&gt;Replace template argument with a regular argument.&lt;/p&gt;&lt;p&gt;Changed a large routine templated on a bool to instead take the bool as an extra argument. (The bool was only being used once to select one of two string constants, so a run-time check was just fine.) This reduced the # of instantiations of the large routine from 287 to 143.&lt;/p&gt;&lt;p&gt;sharding_util_ops.cc&lt;/p&gt;&lt;code&gt;template &amp;lt;bool Split&amp;gt;
Status GetAndValidateAttributes(OpKernelConstruction* ctx,
                                std::vector&amp;lt;int32&amp;gt;&amp;amp; num_partitions,
                                int&amp;amp; num_slices, std::vector&amp;lt;int32&amp;gt;&amp;amp; paddings,
                                bool&amp;amp; has_paddings) {
  absl::string_view num_partitions_attr_name =
      Split ? kNumSplitsAttrName : kNumConcatsAttrName;
      ...
  return OkStatus();
}
&lt;/code&gt;
&lt;code&gt;Status GetAndValidateAttributes(bool split, OpKernelConstruction* ctx,
                                std::vector&amp;lt;int32&amp;gt;&amp;amp; num_partitions,
                                int&amp;amp; num_slices, std::vector&amp;lt;int32&amp;gt;&amp;amp; paddings,
                                bool&amp;amp; has_paddings) {
  absl::string_view num_partitions_attr_name =
      split ? kNumSplitsAttrName : kNumConcatsAttrName;
      ...
  return OkStatus();
}
&lt;/code&gt;
&lt;p&gt;Move bulky code from templated constructor to a non-templated shared base class constructor.&lt;/p&gt;&lt;p&gt;Also reduce number of template instantiations from one for every combination of &lt;code&gt;&amp;lt;T, Device, Rank&amp;gt;&lt;/code&gt; to one for every &lt;code&gt;&amp;lt;T&amp;gt;&lt;/code&gt; and every &lt;code&gt;&amp;lt;Rank&amp;gt;&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;sharding_util_ops.cc&lt;/p&gt;&lt;code&gt;template &amp;lt;typename Device, typename T&amp;gt;
class XlaSplitNDBaseOp : public OpKernel {
 public:
  explicit XlaSplitNDBaseOp(OpKernelConstruction* ctx) : OpKernel(ctx) {
    OP_REQUIRES_OK(
        ctx, GetAndValidateAttributes(/*split=*/true, ctx, num_splits_,
                                      num_slices_, paddings_, has_paddings_));
  }
&lt;/code&gt;
&lt;code&gt;// Shared base class to save code space
class XlaSplitNDShared : public OpKernel {
 public:
  explicit XlaSplitNDShared(OpKernelConstruction* ctx) TF_ATTRIBUTE_NOINLINE
      : OpKernel(ctx),
        num_slices_(1),
        has_paddings_(false) {
    GetAndValidateAttributes(/*split=*/true, ctx, num_splits_, num_slices_,
                             paddings_, has_paddings_);
  }
&lt;/code&gt;
&lt;p&gt;Reduce generated code size for absl::flat_hash_set and absl::flat_hash_map.&lt;/p&gt;&lt;p&gt;Consider the impact of map and other container operations since each call to such and operation can produce large amounts of generated code.&lt;/p&gt;&lt;p&gt;Turn many map insertion calls in a row to initialize a hash table of emoji characters into a single bulk insert operation (188KB of text down to 360 bytes in library linked into many binaries). 😊&lt;/p&gt;&lt;p&gt;textfallback_init.h&lt;/p&gt;&lt;code&gt;inline void AddEmojiFallbacks(TextFallbackMap *map) {
  (*map)[0xFE000] = &amp;amp;kFE000;
  (*map)[0xFE001] = &amp;amp;kFE001;
  (*map)[0xFE002] = &amp;amp;kFE002;
  (*map)[0xFE003] = &amp;amp;kFE003;
  (*map)[0xFE004] = &amp;amp;kFE004;
  (*map)[0xFE005] = &amp;amp;kFE005;
  ...
  (*map)[0xFEE7D] = &amp;amp;kFEE7D;
  (*map)[0xFEEA0] = &amp;amp;kFEEA0;
  (*map)[0xFE331] = &amp;amp;kFE331;
};
&lt;/code&gt;
&lt;code&gt;inline void AddEmojiFallbacks(TextFallbackMap *map) {
#define PAIR(x) {0x##x, &amp;amp;k##x}
  // clang-format off
  map-&amp;gt;insert({
    PAIR(FE000),
    PAIR(FE001),
    PAIR(FE002),
    PAIR(FE003),
    PAIR(FE004),
    PAIR(FE005),
    ...
    PAIR(FEE7D),
    PAIR(FEEA0),
    PAIR(FE331)});
  // clang-format on
#undef PAIR
};
&lt;/code&gt;
&lt;p&gt;Stop inlining a heavy user of InlinedVector operations.&lt;/p&gt;&lt;p&gt;Moved very long routine that was being inlined from .h file to .cc (no real performance benefit from inlining this).&lt;/p&gt;&lt;p&gt;reduction_ops_common.h&lt;/p&gt;&lt;code&gt;Status Simplify(const Tensor&amp;amp; data, const Tensor&amp;amp; axis,
                const bool keep_dims) {
  ... Eighty line routine body ...
}
&lt;/code&gt;
&lt;code&gt;Status Simplify(const Tensor&amp;amp; data, const Tensor&amp;amp; axis, const bool keep_dims);
&lt;/code&gt;

&lt;p&gt;Modern machines have many cores, and they are often underutilized. Expensive work may therefore be completed faster by parallelizing it. The most common approach is to process different items in parallel and combine the results when done. Typically, the items are first partitioned into batches to avoid paying the cost of running something in parallel per item.&lt;/p&gt;&lt;p&gt;Four-way parallelization improves the rate of encoding tokens by ~3.6x.&lt;/p&gt;&lt;p&gt;blocked-token-coder.cc&lt;/p&gt;&lt;code&gt;MutexLock l(&amp;amp;encoder_threads_lock);
if (encoder_threads == NULL) {
  encoder_threads = new ThreadPool(NumCPUs());
  encoder_threads-&amp;gt;SetStackSize(262144);
  encoder_threads-&amp;gt;StartWorkers();
}
encoder_threads-&amp;gt;Add
    (NewCallback(this,
                 &amp;amp;BlockedTokenEncoder::EncodeRegionInThread,
                 region_tokens, N, region,
                 stats,
                 controller_-&amp;gt;GetClosureWithCost
                 (NewCallback(&amp;amp;DummyCallback), N)));
&lt;/code&gt;
&lt;p&gt;Parallelization improves decoding performance by 5x.&lt;/p&gt;&lt;p&gt;coding.cc&lt;/p&gt;&lt;code&gt;for (int c = 0; c &amp;lt; clusters-&amp;gt;size(); c++) {
  RET_CHECK_OK(DecodeBulkForCluster(...);
}
&lt;/code&gt;
&lt;code&gt;struct SubTask {
  absl::Status result;
  absl::Notification done;
};

std::vector&amp;lt;SubTask&amp;gt; tasks(clusters-&amp;gt;size());
for (int c = 0; c &amp;lt; clusters-&amp;gt;size(); c++) {
  options_.executor-&amp;gt;Schedule([&amp;amp;, c] {
    tasks[c].result = DecodeBulkForCluster(...);
    tasks[c].done.Notify();
  });
}
for (int c = 0; c &amp;lt; clusters-&amp;gt;size(); c++) {
  tasks[c].done.WaitForNotification();
}
for (int c = 0; c &amp;lt; clusters-&amp;gt;size(); c++) {
  RETURN_IF_ERROR(tasks[c].result);
}
&lt;/code&gt;

&lt;p&gt;The effect on system performance should be measured carefully – if spare CPU is not available, or if memory bandwidth is saturated, parallelization may not help, or may even hurt.&lt;/p&gt;&lt;p&gt;Avoid fine-grained locking to reduce the cost of Mutex operations in hot paths. Caveat: this should only be done if the change does not increase lock contention.&lt;/p&gt;&lt;p&gt;Acquire lock once to free entire tree of query nodes, rather than reacquiring lock for every node in tree.&lt;/p&gt;&lt;p&gt;mustang-query.cc&lt;/p&gt;&lt;code&gt;// Pool of query nodes
ThreadSafeFreeList&amp;lt;MustangQuery&amp;gt; pool_(256);
...
void MustangQuery::Release(MustangQuery* node) {
  if (node == NULL)
    return;
  for (int i=0; i &amp;lt; node-&amp;gt;children_-&amp;gt;size(); ++i)
    Release((*node-&amp;gt;children_)[i]);
  node-&amp;gt;children_-&amp;gt;clear();
  pool_.Delete(node);
}
&lt;/code&gt;
&lt;code&gt;// Pool of query nodes
Mutex pool_lock_;
FreeList&amp;lt;MustangQuery&amp;gt; pool_(256);
...
void MustangQuery::Release(MustangQuery* node) {
  if (node == NULL)
    return;
  MutexLock l(&amp;amp;pool_lock_);
  ReleaseLocked(node);
}

void MustangQuery::ReleaseLocked(MustangQuery* node) {
#ifndef NDEBUG
  pool_lock_.AssertHeld();
#endif
  if (node == NULL)
    return;
  for (int i=0; i &amp;lt; node-&amp;gt;children_-&amp;gt;size(); ++i)
    ReleaseLocked((*node-&amp;gt;children_)[i]);
  node-&amp;gt;children_-&amp;gt;clear();
  pool_.Delete(node);
}
&lt;/code&gt;
&lt;p&gt;Avoid expensive work inside critical sections. In particular, watch out for innocuous looking code that might be doing RPCs or accessing files.&lt;/p&gt;&lt;p&gt;Reduce number of cache lines touched in critical section.&lt;/p&gt;&lt;p&gt;Careful data structure adjustments reduce the number of cache lines accessed significantly and improve the performance of an ML training run by 3.3%.&lt;/p&gt;&lt;code&gt;~2 + O(num_outgoing edges)&lt;/code&gt;
(and for large graphs with many cores executing them there is also less TLB
pressure).&lt;p&gt;Avoid RPC while holding Mutex.&lt;/p&gt;&lt;p&gt;trainer.cc&lt;/p&gt;&lt;code&gt;{
  // Notify the parameter server that we are starting.
  MutexLock l(&amp;amp;lock_);
  model_ = model;
  MaybeRecordProgress(last_global_step_);
}
&lt;/code&gt;
&lt;code&gt;bool should_start_record_progress = false;
int64 step_for_progress = -1;
{
  // Notify the parameter server that we are starting.
  MutexLock l(&amp;amp;lock_);
  model_ = model;
  should_start_record_progress = ShouldStartRecordProgress();
  step_for_progress = last_global_step_;
}
if (should_start_record_progress) {
  StartRecordProgress(step_for_progress);
}
&lt;/code&gt;
&lt;p&gt;Also, be wary of expensive destructors that will run before a Mutex is unlocked (this can often happen when the Mutex unlock is triggered by a &lt;code&gt;~MutexUnlock&lt;/code&gt;.)
Declaring objects with expensive destructors before MutexLock may help (assuming
it is thread-safe).&lt;/p&gt;&lt;p&gt;Sometimes a data structure protected by a Mutex that is exhibiting high contention can be safely split into multiple shards, each shard with its own Mutex. (Note: this requires that there are no cross-shard invariants between the different shards.)&lt;/p&gt;&lt;p&gt;Shard a cache 16 ways which improves throughput under a multi-threaded load by ~2x.&lt;/p&gt;&lt;p&gt;cache.cc&lt;/p&gt;&lt;code&gt;class ShardedLRUCache : public Cache {
 private:
  LRUCache shard_[kNumShards];
  port::Mutex id_mutex_;
  uint64_t last_id_;

  static inline uint32_t HashSlice(const Slice&amp;amp; s) {
    return Hash(s.data(), s.size(), 0);
  }

  static uint32_t Shard(uint32_t hash) {
    return hash &amp;gt;&amp;gt; (32 - kNumShardBits);
  }
  ...
  virtual Handle* Lookup(const Slice&amp;amp; key) {
    const uint32_t hash = HashSlice(key);
    return shard_[Shard(hash)].Lookup(key, hash);
  }
&lt;/code&gt;
&lt;p&gt;Shard spanner data structure for tracking calls.&lt;/p&gt;&lt;p&gt;transaction_manager.cc&lt;/p&gt;&lt;code&gt;absl::MutexLock l(&amp;amp;active_calls_in_mu_);
ActiveCallMap::const_iterator iter = active_calls_in_.find(m-&amp;gt;tid());
if (iter != active_calls_in_.end()) {
  iter-&amp;gt;second.ExtractElements(&amp;amp;m-&amp;gt;tmp_calls_);
}
&lt;/code&gt;
&lt;code&gt;ActiveCalls::LockedShard shard(active_calls_in_, m-&amp;gt;tid());
const ActiveCallMap&amp;amp; active_calls_map = shard.active_calls_map();
ActiveCallMap::const_iterator iter = active_calls_map.find(m-&amp;gt;tid());
if (iter != active_calls_map.end()) {
  iter-&amp;gt;second.ExtractElements(&amp;amp;m-&amp;gt;tmp_calls_);
}
&lt;/code&gt;
&lt;p&gt;If the data structure in question is a map, consider using a concurrent hash map implementation instead.&lt;/p&gt;&lt;p&gt;Be careful with the information used for shard selection. If, for example, you use some bits of a hash value for shard selection and then those same bits end up being used again later, the latter use may perform poorly since it sees a skewed distribution of hash values.&lt;/p&gt;&lt;p&gt;Fix information used for shard selection to prevent hash table issues.&lt;/p&gt;&lt;p&gt;netmon_map_impl.h&lt;/p&gt;&lt;code&gt;ConnectionBucket* GetBucket(Index index) {
  // Rehash the hash to make sure we are not partitioning the buckets based on
  // the original hash. If num_buckets_ is a power of 2 that would drop the
  // entropy of the buckets.
  size_t original_hash = absl::Hash&amp;lt;Index&amp;gt;()(index);
  int hash = absl::Hash&amp;lt;size_t&amp;gt;()(original_hash) % num_buckets_;
  return &amp;amp;buckets_[hash];
}
&lt;/code&gt;
&lt;code&gt;ConnectionBucket* GetBucket(Index index) {
  absl::Hash&amp;lt;std::pair&amp;lt;Index, size_t&amp;gt;&amp;gt; hasher{};
  // Combine the hash with 42 to prevent shard selection using the same bits
  // as the underlying hashtable.
  return &amp;amp;buckets_[hasher({index, 42}) % num_buckets_];
}
&lt;/code&gt;
&lt;p&gt;Shard Spanner data structure used for tracking calls.&lt;/p&gt;&lt;p&gt;This CL partitions the ActiveCallMap into 64 shards. Each shard is protected by a separate mutex. A given transaction will be mapped to exactly one shard. A new interface LockedShard(tid) is added for accessing the ActiveCallMap for a transaction in a thread-safe manner. Example usage:&lt;/p&gt;&lt;p&gt;transaction_manager.cc&lt;/p&gt;&lt;code&gt;{
  absl::MutexLock l(&amp;amp;active_calls_in_mu_);
  delayed_locks_timer_ring_.Add(delayed_locks_flush_time_ms, tid);
}
&lt;/code&gt;
&lt;code&gt;{
  ActiveCalls::LockedShard shard(active_calls_in_, tid);
  shard.delayed_locks_timer_ring().Add(delayed_locks_flush_time_ms, tid);
}
&lt;/code&gt;
&lt;p&gt;The results show a 69% reduction in overall wall-clock time when running the benchmark with 8192 fibers&lt;/p&gt;&lt;code&gt;Benchmark                   Time(ns)        CPU(ns)     Iterations
------------------------------------------------------------------
BM_ActiveCalls/8k        11854633492     98766564676            10
BM_ActiveCalls/16k       26356203552    217325836709            10
&lt;/code&gt;
&lt;code&gt;Benchmark                   Time(ns)        CPU(ns)     Iterations
------------------------------------------------------------------
BM_ActiveCalls/8k         3696794642     39670670110            10
BM_ActiveCalls/16k        7366284437     79435705713            10
&lt;/code&gt;

&lt;p&gt;Explore whether handling multiple items at once using SIMD instructions available on modern CPUs can give speedups (e.g., see &lt;code&gt;absl::flat_hash_map&lt;/code&gt; discussion below in Bulk Operations
section).&lt;/p&gt;&lt;p&gt;If different threads access different mutable data, consider placing the different data items on different cache lines, e.g., in C++ using the &lt;code&gt;alignas&lt;/code&gt;
directive. However, these directives are easy to misuse and may increase object
sizes significantly, so make sure performance measurements justify their use.&lt;/p&gt;&lt;p&gt;Segregate commonly mutated fields in a different cache line than other fields.&lt;/p&gt;&lt;p&gt;histogram.h&lt;/p&gt;&lt;code&gt;HistogramOptions options_;
...
internal::HistogramBoundaries *boundaries_;
...
std::vector&amp;lt;double&amp;gt; buckets_;

double min_;             // Minimum.
double max_;             // Maximum.
double count_;           // Total count of occurrences.
double sum_;             // Sum of values.
double sum_of_squares_;  // Sum of squares of values.
...
RegisterVariableExporter *exporter_;
&lt;/code&gt;
&lt;code&gt;  HistogramOptions options_;
  ...
  internal::HistogramBoundaries *boundaries_;
  ...
  RegisterVariableExporter *exporter_;
  ...
  // Place the following fields in a dedicated cacheline as they are frequently
  // mutated, so we can avoid potential false sharing.
  ...
#ifndef SWIG
  alignas(ABSL_CACHELINE_SIZE)
#endif
  std::vector&amp;lt;double&amp;gt; buckets_;

  double min_;             // Minimum.
  double max_;             // Maximum.
  double count_;           // Total count of occurrences.
  double sum_;             // Sum of values.
  double sum_of_squares_;  // Sum of squares of values.
&lt;/code&gt;
&lt;p&gt;Process small work items inline instead of on device thread pool.&lt;/p&gt;&lt;p&gt;cast_op.cc&lt;/p&gt;&lt;code&gt;template &amp;lt;typename Device, typename Tout, typename Tin&amp;gt;
void CastMaybeInline(const Device&amp;amp; d, typename TTypes&amp;lt;Tout&amp;gt;::Flat o,
                     typename TTypes&amp;lt;Tin&amp;gt;::ConstFlat i) {
  if (o.size() * (sizeof(Tin) + sizeof(Tout)) &amp;lt; 16384) {
    // Small cast on a CPU: do inline
    o = i.template cast&amp;lt;Tout&amp;gt;();
  } else {
    o.device(d) = i.template cast&amp;lt;Tout&amp;gt;();
  }
}
&lt;/code&gt;
&lt;p&gt;Channels can be unbuffered which means that a writer blocks until a reader is ready to pick up an item. Unbuffered channels can be useful when the channel is being used for synchronization, but not when the channel is being used to increase parallelism.&lt;/p&gt;&lt;p&gt;Sometimes lock-free data structures can make a difference over more conventional mutex-protected data structures. However, direct atomic variable manipulation can be dangerous. Prefer higher-level abstractions.&lt;/p&gt;&lt;p&gt;Use lock-free map to manage a cache of RPC channels.&lt;/p&gt;&lt;p&gt;Entries in an RPC stub cache are read thousands of times a second and modified rarely. Switching to an appropriate lock-free map reduces search latency by 3%-5%.&lt;/p&gt;&lt;p&gt;Use a fixed lexicon+lock-free hash map to speed-up determining IsValidTokenId.&lt;/p&gt;&lt;p&gt;dynamic_token_class_manager.h&lt;/p&gt;&lt;code&gt;mutable Mutex mutex_;

// The density of this hash map is guaranteed by the fact that the
// dynamic lexicon reuses previously allocated TokenIds before trying
// to allocate new ones.
dense_hash_map&amp;lt;TokenId, common::LocalTokenClassId&amp;gt; tid_to_cid_
    GUARDED_BY(mutex_);
&lt;/code&gt;
&lt;code&gt;// Read accesses to this hash-map should be done using
// 'epoch_gc_'::(EnterFast / LeaveFast). The writers should periodically
// GC the deleted entries, by simply invoking LockFreeHashMap::CreateGC.
typedef util::gtl::LockFreeHashMap&amp;lt;TokenId, common::LocalTokenClassId&amp;gt;
    TokenIdTokenClassIdMap;
TokenIdTokenClassIdMap tid_to_cid_;
&lt;/code&gt;
&lt;p&gt;Protobufs are a convenient representation of data, especially if the data will be sent over the wire or stored persistently. However, they can have significant performance costs. For example, a piece of code that fills in a list of 1000 points and then sums up the Y coordinates, speeds up by a factor of 20 when converted from protobufs to a C++ std::vector of structs!&lt;/p&gt;&lt;p&gt;Benchmark code for both versions.&lt;/p&gt;&lt;code&gt;name                old time/op  new time/op  delta
BenchmarkIteration  17.4µs ± 5%   0.8µs ± 1%  -95.30%  (p=0.000 n=11+12)
&lt;/code&gt;
&lt;p&gt;Protobuf version:&lt;/p&gt;&lt;code&gt;message PointProto {
  int32 x = 1;
  int32 y = 2;
}
message PointListProto {
  repeated PointProto points = 1;
}
&lt;/code&gt;
&lt;code&gt;void SumProto(const PointListProto&amp;amp; vec) {
  int sum = 0;
  for (const PointProto&amp;amp; p : vec.points()) {
    sum += p.y();
  }
  ABSL_VLOG(1) &amp;lt;&amp;lt; sum;
}

void BenchmarkIteration() {
  PointListProto points;
  points.mutable_points()-&amp;gt;Reserve(1000);
  for (int i = 0; i &amp;lt; 1000; i++) {
    PointProto* p = points.add_points();
    p-&amp;gt;set_x(i);
    p-&amp;gt;set_y(i * 2);
  }
  SumProto(points);
}
&lt;/code&gt;
&lt;p&gt;Non-protobuf version:&lt;/p&gt;&lt;code&gt;struct PointStruct {
  int x;
  int y;
};

void SumVector(const std::vector&amp;lt;PointStruct&amp;gt;&amp;amp; vec) {
  int sum = 0;
  for (const PointStruct&amp;amp; p : vec) {
    sum += p.y;
  }
  ABSL_VLOG(1) &amp;lt;&amp;lt; sum;
}

void BenchmarkIteration() {
  std::vector&amp;lt;PointStruct&amp;gt; points;
  points.reserve(1000);
  for (int i = 0; i &amp;lt; 1000; i++) {
    points.push_back({i, i * 2});
  }
  SumVector(points);
}
&lt;/code&gt;

&lt;p&gt;In addition, the protobuf version adds a few kilobytes of code and data to the binary, which may not seem like much, but adds up quickly in systems with many protobuf types. This increased size creates performance problems by creating i-cache and d-cache pressure.&lt;/p&gt;&lt;p&gt;Here are some tips related to protobuf performance:&lt;/p&gt;&lt;p&gt;Do not use protobufs unnecessarily.&lt;/p&gt;&lt;p&gt;Given the factor of 20 performance difference described above, if some data is never serialized or parsed, you probably should not put it in a protocol buffer. The purpose of protocol buffers is to make it easy to serialize and deserialize data structures, but they can have significant code-size, memory, and CPU overheads. Do not use them if all you want are some of the other niceties like &lt;code&gt;DebugString&lt;/code&gt; and copyability.&lt;/p&gt;&lt;p&gt;Avoid unnecessary message hierarchies.&lt;/p&gt;&lt;p&gt;Message hierarchy can be useful to organize information in a more readable fashion. However, the extra level of message hierarchy incurs overheads like memory allocations, function calls, cache misses, larger serialized messages, etc.&lt;/p&gt;&lt;p&gt;E.g., instead of:&lt;/p&gt;&lt;code&gt;message Foo {
  optional Bar bar = 1;
}
message Bar {
  optional Baz baz = 1;
}
message Baz {
  optional int32 count = 1;
}
&lt;/code&gt;
&lt;p&gt;Prefer:&lt;/p&gt;&lt;code&gt;message Foo {
  optional int32 count = 1;
}
&lt;/code&gt;
&lt;p&gt;A protocol buffer message corresponds to a message class in C++ generated code and emits a tag and the length of the payload on the wire. To carry an integer, the old form requires more allocations (and deallocations) and emits a larger amount of generated code. As a result, all protocol buffer operations (parsing, serialization, size, etc.) become more expensive, having to traverse the message tree. The new form does not have such overhead and is more efficient.&lt;/p&gt;&lt;p&gt;Use small field numbers for frequently occurring fields.&lt;/p&gt;&lt;p&gt;Protobufs use a variable length integer representation for the combination of field number and wire format (see the protobuf encoding documentation). This representation is 1 byte for field numbers between 1 and 15, and two bytes for field numbers between 16 and 2047. (Field numbers 2048 or greater should typically be avoided.)&lt;/p&gt;&lt;p&gt;Consider pre-reserving some small field numbers for future extension of performance-sensitive protobufs.&lt;/p&gt;&lt;p&gt;Choose carefully between int32, sint32, fixed32, and uint32 (and similarly for the 64 bit variants).&lt;/p&gt;&lt;p&gt;Generally, use &lt;code&gt;int32&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt;, but use &lt;code&gt;fixed32&lt;/code&gt; or &lt;code&gt;fixed64&lt;/code&gt; for large
values like hash codes and &lt;code&gt;sint32&lt;/code&gt; or &lt;code&gt;sint64&lt;/code&gt; for values are that are often
negative.&lt;/p&gt;&lt;p&gt;A varint occupies fewer bytes to encode small integers and can save space at the cost of more expensive decoding. However, it can take up more space for negative or large values. In that case, using fixed32 or fixed64 (instead of uint32 or uint64) reduces size with much cheaper encoding and decoding. For small negative integers, use sint32 or sint64 instead of int32 or int64.d&lt;/p&gt;&lt;p&gt;For proto2, pack repeated numeric fields by annotating them with &lt;code&gt;[packed=true]&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In proto2, repeated values are serialized as a sequence of (tag, value) pairs by default. This is inefficient because tags have to be decoded for every element.&lt;/p&gt;&lt;p&gt;Packed repeated primitives are serialized with the length of the payload first followed by values without tags. When using fixed-width values, we can avoid reallocations by knowing the final size the moment we start parsing; i.e., no reallocation cost. We still don't know how many varints are in the payload and may have to pay the reallocation cost.&lt;/p&gt;&lt;p&gt;In proto3, repeated fields are packed by default.&lt;/p&gt;&lt;p&gt;Packed works best with fixed-width values like fixed32, fixed64, float, double, etc. since the entire encoded length can be predetermined by multiplying the number of elements by the fixed value size, instead of having to calculate the length of each individual element.&lt;/p&gt;&lt;p&gt;Use &lt;code&gt;bytes&lt;/code&gt; instead for &lt;code&gt;string&lt;/code&gt; for binary data
and large values.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;string&lt;/code&gt; type holds UTF8-encoded text, and can sometimes require validation.
The &lt;code&gt;bytes&lt;/code&gt; type can hold an arbitrary sequence of bytes (non-text data) and is
often more appropriate as well as more efficient than &lt;code&gt;string&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Consider &lt;code&gt;string_type = VIEW&lt;/code&gt; to avoid copying.&lt;/p&gt;&lt;p&gt;Copying a big string or bytes field during parsing is expensive. Such cost can often be avoided by marking the field with &lt;code&gt;string_type = VIEW&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;message Image {
  ...
  bytes jpeg_encoding = 4 [features.(pb.cpp).string_type=VIEW];
}
&lt;/code&gt;
&lt;p&gt;Without the &lt;code&gt;VIEW&lt;/code&gt; annotation, when the protocol buffer is parsed, the
potentially large field contents are copied from the serialized protocol buffer
to a string object in memory. Depending on the number of string or bytes fields
and the size of those fields, the overhead of copying can be significant.&lt;/p&gt;&lt;p&gt;Instead of copying the big binary blobs, routines like &lt;code&gt;ParseFromStringWithAliasing&lt;/code&gt; use &lt;code&gt;absl::string_view&lt;/code&gt; to reference the original
backing string. Note that the backing string (the serialized protocol buffer)
must outlive the protocol buffer instance that contains the alias.&lt;/p&gt;&lt;p&gt;Consider using &lt;code&gt;Cord&lt;/code&gt; for large fields to reduce copying
costs.&lt;/p&gt;&lt;p&gt;Annotating large &lt;code&gt;bytes&lt;/code&gt; and &lt;code&gt;string&lt;/code&gt; fields with &lt;code&gt;[ctype=CORD]&lt;/code&gt; may reduce
copying costs. This annotation changes the representation of the field from
&lt;code&gt;std::string&lt;/code&gt; to &lt;code&gt;absl::Cord&lt;/code&gt;. &lt;code&gt;absl::Cord&lt;/code&gt; uses reference counting and
tree-based storage to reduce copying and appending costs. If a protocol buffer
is serialized to a cord, parsing a string or bytes field with &lt;code&gt;[ctype=CORD]&lt;/code&gt; can
avoid copying the field contents.&lt;/p&gt;&lt;code&gt;message Document {
  ...
  bytes html = 4 [ctype = CORD];
}
&lt;/code&gt;
&lt;p&gt;Performance of a Cord field depends on length distribution and access patterns. Use benchmarks to validate such changes.&lt;/p&gt;&lt;p&gt;Use protobuf arenas in C++ code.&lt;/p&gt;&lt;p&gt;Consider using arenas to save allocation and deallocation costs, especially for protobufs containing repeated, string, or message fields.&lt;/p&gt;&lt;p&gt;Message and string fields are heap-allocated (even if the top-level protocol buffer object is stack-allocated). If a protocol buffer message has a lot of sub message fields and string fields, allocation and deallocation cost can be significant. Arenas amortize allocation costs and makes deallocation virtually free. It also improves memory locality by allocating from contiguous chunks of memory.&lt;/p&gt;&lt;p&gt;Keep .proto files small&lt;/p&gt;&lt;p&gt;Do not put too many messages in a single .proto file. Once you rely on anything at all from a .proto file, the entire file will get pulled in by the linker even if it's mostly unused. This increases build times and binary sizes. You can use extensions and &lt;code&gt;Any&lt;/code&gt; to avoid creating hard dependencies on big
.proto files with many message types.&lt;/p&gt;&lt;p&gt;Consider storing protocol buffers in serialized form, even in memory.&lt;/p&gt;&lt;p&gt;In-memory protobuf objects have a large memory footprint (often 5x the wire format size), potentially spread across many cache lines. So if your application is going to keep many protobuf objects live for long periods of time, consider storing them in serialized form.&lt;/p&gt;&lt;p&gt;Avoid protobuf map fields.&lt;/p&gt;&lt;p&gt;Protobuf map fields have performance problems that usually outweigh the small syntactic convenience they provide. Prefer using non-protobuf maps initialized from protobuf contents:&lt;/p&gt;&lt;p&gt;msg.proto&lt;/p&gt;&lt;code&gt;map&amp;lt;string, bytes&amp;gt; env_variables = 5;
&lt;/code&gt;
&lt;code&gt;message Var {
  string key = 1;
  bytes value = 2;
}
repeated Var env_variables = 5;
&lt;/code&gt;

&lt;p&gt;Use protobuf message definition with a subset of the fields.&lt;/p&gt;&lt;p&gt;If you want to access only a few fields of a large message type, consider defining your own protocol buffer message type that mimics the original type, but only defines the fields that you care about. Here's an example:&lt;/p&gt;&lt;code&gt;message FullMessage {
  optional int32 field1 = 1;
  optional BigMessage field2 = 2;
  optional int32 field3 = 3;
  repeater AnotherBigMessage field4 = 4;
  ...
  optional int32 field100 = 100;
}
&lt;/code&gt;
&lt;code&gt;message SubsetMessage {
  optional int32 field3 = 3;
  optional int32 field88 = 88;
}
&lt;/code&gt;
&lt;p&gt;By parsing a serialized &lt;code&gt;FullMessage&lt;/code&gt; into a &lt;code&gt;SubsetMessage&lt;/code&gt;, only two out of a
hundred fields are parsed and others are treated as unknown fields. Consider
using APIs that discard unknown fields to improve performance even more when
appropriate.&lt;/p&gt;&lt;p&gt;Reuse protobuf objects when possible.&lt;/p&gt;&lt;p&gt;Declare protobuf objects outside loops so that their allocated storage can be reused across loop iterations.&lt;/p&gt;&lt;p&gt;Absl hash tables usually out-perform C++ standard library containers such as &lt;code&gt;std::map&lt;/code&gt; and
&lt;code&gt;std::unordered_map&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Speed up LanguageFromCode (use absl::flat_hash_map instead of a __gnu_cxx::hash_map).&lt;/p&gt;&lt;p&gt;languages.cc&lt;/p&gt;&lt;code&gt;class CodeToLanguage
    ...
    : public __gnu_cxx::hash_map&amp;lt;absl::string_view, i18n::languages::Language,
                                 CodeHash, CodeCompare&amp;gt; {
&lt;/code&gt;
&lt;code&gt;class CodeToLanguage
    ...
    : public absl::flat_hash_map&amp;lt;absl::string_view, i18n::languages::Language,
                                 CodeHash, CodeCompare&amp;gt; {
&lt;/code&gt;
&lt;p&gt;Benchmark results:&lt;/p&gt;&lt;code&gt;name               old time/op  new time/op  delta
BM_CodeToLanguage  19.4ns ± 1%  10.2ns ± 3%  -47.47%  (p=0.000 n=8+10)
&lt;/code&gt;

&lt;p&gt;Speed up stats publish/unpublish (an older change, so uses dense_hash_map instead of absl::flat_hash_map, which did not exist at the time).&lt;/p&gt;&lt;p&gt;publish.cc&lt;/p&gt;&lt;code&gt;typedef hash_map&amp;lt;uint64, Publication*&amp;gt; PublicationMap;
static PublicationMap* publications = NULL;
&lt;/code&gt;
&lt;code&gt;typedef dense_hash_map&amp;lt;uint64, Publication*&amp;gt; PublicationMap;;
static PublicationMap* publications GUARDED_BY(mu) = NULL;
&lt;/code&gt;
&lt;p&gt;Use dense_hash_map instead of hash_map for keeping track of SelectServer alarms (would use absl::flat_hash_map today).&lt;/p&gt;&lt;p&gt;alarmer.h&lt;/p&gt;&lt;code&gt;typedef hash_map&amp;lt;int, Alarm*&amp;gt; AlarmList;
&lt;/code&gt;
&lt;code&gt;typedef dense_hash_map&amp;lt;int, Alarm*&amp;gt; AlarmList;
&lt;/code&gt;
&lt;p&gt;absl::btree_map and absl::btree_set store multiple entries per tree node. This has a number of advantages over ordered C++ standard library containers such as &lt;code&gt;std::map&lt;/code&gt;. First, the pointer overhead of pointing to child tree nodes is often
significantly reduced. Second, because the entries or key/values are stored
consecutively in memory for a given btree tree node, cache efficiency is often
significantly better.&lt;/p&gt;&lt;p&gt;Use btree_set instead of std::set to represent a very heavily used work-queue.&lt;/p&gt;&lt;p&gt;register_allocator.h&lt;/p&gt;&lt;code&gt;using container_type = std::set&amp;lt;WorklistItem&amp;gt;;
&lt;/code&gt;
&lt;code&gt;using container_type = absl::btree_set&amp;lt;WorklistItem&amp;gt;;
&lt;/code&gt;

&lt;p&gt;&lt;code&gt;util::bitmap::InlinedBitvector&lt;/code&gt; can store short bit-vectors inline, and
therefore can often be a better choice than &lt;code&gt;std::vector&amp;lt;bool&amp;gt;&lt;/code&gt; or other bitmap
types.&lt;/p&gt;&lt;p&gt;Use InlinedBitVector instead of std::vector&amp;lt;bool&amp;gt;, and then use FindNextBitSet to find the next item of interest.&lt;/p&gt;&lt;p&gt;block_encoder.cc&lt;/p&gt;&lt;code&gt;vector&amp;lt;bool&amp;gt; live_reads(nreads);
...
for (int offset = 0; offset &amp;lt; b_.block_width(); offset++) {
  ...
  for (int r = 0; r &amp;lt; nreads; r++) {
    if (live_reads[r]) {
&lt;/code&gt;
&lt;code&gt;util::bitmap::InlinedBitVector&amp;lt;4096&amp;gt; live_reads(nreads);
...
for (int offset = 0; offset &amp;lt; b_.block_width(); offset++) {
  ...
  for (size_t r = 0; live_reads.FindNextSetBit(&amp;amp;r); r++) {
    DCHECK(live_reads[r]);
&lt;/code&gt;
&lt;p&gt;absl::InlinedVector stores a small number of elements inline (configurable via the second template argument). This enables small vectors up to this number of elements to generally have better cache efficiency and also to avoid allocating a backing store array at all when the number of elements is small.&lt;/p&gt;&lt;p&gt;Use InlinedVector instead of std::vector in various places.&lt;/p&gt;&lt;p&gt;bundle.h&lt;/p&gt;&lt;code&gt;class Bundle {
 public:
 ...
 private:
  // Sequence of (slotted instruction, unslotted immediate operands).
  std::vector&amp;lt;InstructionRecord&amp;gt; instructions_;
  ...
};
&lt;/code&gt;
&lt;code&gt;class Bundle {
 public:
 ...
 private:
  // Sequence of (slotted instruction, unslotted immediate operands).
  absl::InlinedVector&amp;lt;InstructionRecord, 2&amp;gt; instructions_;
  ...
};
&lt;/code&gt;

&lt;p&gt;Saves space by using a customized vector type that only supports sizes that fit in 32 bits.&lt;/p&gt;&lt;p&gt;Simple type change saves ~8TiB of memory in Spanner.&lt;/p&gt;&lt;p&gt;table_ply.h&lt;/p&gt;&lt;code&gt;class TablePly {
    ...
    // Returns the set of data columns stored in this file for this table.
    const std::vector&amp;lt;FamilyId&amp;gt;&amp;amp; modified_data_columns() const {
      return modified_data_columns_;
    }
    ...
   private:
    ...
    std::vector&amp;lt;FamilyId&amp;gt; modified_data_columns_;  // Data columns in the table.
&lt;/code&gt;
&lt;code&gt;#include "util/gtl/vector32.h"
    ...
    // Returns the set of data columns stored in this file for this table.
    absl::Span&amp;lt;const FamilyId&amp;gt; modified_data_columns() const {
      return modified_data_columns_;
    }
    ...

    ...
    // Data columns in the table.
    gtl::vector32&amp;lt;FamilyId&amp;gt; modified_data_columns_;
&lt;/code&gt;
&lt;p&gt;gtl::small_map uses an inline array to store up to a certain number of unique key-value-pair elements, but upgrades itself automatically to be backed by a user-specified map type when it runs out of space.&lt;/p&gt;&lt;p&gt;Use gtl::small_map in tflite_model.&lt;/p&gt;&lt;p&gt;tflite_model.cc&lt;/p&gt;&lt;code&gt;using ChoiceIdToContextMap = gtl::flat_hash_map&amp;lt;int, TFLiteContext*&amp;gt;;
&lt;/code&gt;
&lt;code&gt;using ChoiceIdToContextMap =
    gtl::small_map&amp;lt;gtl::flat_hash_map&amp;lt;int, TFLiteContext*&amp;gt;&amp;gt;;
&lt;/code&gt;
&lt;p&gt;gtl::small_ordered_set is an optimization for associative containers (such as std::set or absl::btree_multiset). It uses a fixed array to store a certain number of elements, then reverts to using a set or multiset when it runs out of space. For sets that are typically small, this can be considerably faster than using something like set directly, as set is optimized for large data sets. This change shrinks cache footprint and reduces critical section length.&lt;/p&gt;&lt;p&gt;Use gtl::small_ordered_set to hold set of listeners.&lt;/p&gt;&lt;p&gt;broadcast_stream.h&lt;/p&gt;&lt;code&gt;class BroadcastStream : public ParsedRtpTransport {
 ...
 private:
  ...
  std::set&amp;lt;ParsedRtpTransport*&amp;gt; listeners_ ABSL_GUARDED_BY(listeners_mutex_);
};
&lt;/code&gt;
&lt;code&gt;class BroadcastStream : public ParsedRtpTransport {
 ...
 private:
  ...
  using ListenersSet =
      gtl::small_ordered_set&amp;lt;std::set&amp;lt;ParsedRtpTransport*&amp;gt;, 10&amp;gt;;
  ListenersSet listeners_ ABSL_GUARDED_BY(listeners_mutex_);
&lt;/code&gt;
&lt;p&gt;&lt;code&gt;gtl::intrusive_list&amp;lt;T&amp;gt;&lt;/code&gt; is a doubly-linked list where the link pointers are
embedded in the elements of type T. It saves one cache line+indirection per
element when compared to &lt;code&gt;std::list&amp;lt;T*&amp;gt;&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Use intrusive_list to keep track of inflight requests for each index row update.&lt;/p&gt;&lt;p&gt;row-update-sender-inflight-set.h&lt;/p&gt;&lt;code&gt;std::set&amp;lt;int64&amp;gt; inflight_requests_ GUARDED_BY(mu_);
&lt;/code&gt;
&lt;code&gt;class SeqNum : public gtl::intrusive_link&amp;lt;SeqNum&amp;gt; {
  ...
  int64 val_ = -1;
  ...
};
...
gtl::intrusive_list&amp;lt;SeqNum&amp;gt; inflight_requests_ GUARDED_BY(mu_);
&lt;/code&gt;
&lt;p&gt;Even though &lt;code&gt;absl::Status&lt;/code&gt; and &lt;code&gt;absl::StatusOr&lt;/code&gt; types are fairly efficient, they
have a non-zero overhead even in the success path and should therefore be
avoided for hot routines that don’t need to return any meaningful error details
(or perhaps never even fail!):&lt;/p&gt;&lt;p&gt;Avoid StatusOr&amp;lt;int64&amp;gt; return type for RoundUpToAlignment() function.&lt;/p&gt;&lt;p&gt;best_fit_allocator.cc&lt;/p&gt;&lt;code&gt;absl::StatusOr&amp;lt;int64&amp;gt; BestFitAllocator::RoundUpToAlignment(int64 bytes) const {
  TPU_RET_CHECK_GE(bytes, 0);

  const int64 max_aligned = MathUtil::RoundDownTo&amp;lt;int64&amp;gt;(
      std::numeric_limits&amp;lt;int64&amp;gt;::max(), alignment_in_bytes_);
  if (bytes &amp;gt; max_aligned) {
    return util::ResourceExhaustedErrorBuilder(ABSL_LOC)
           &amp;lt;&amp;lt; "Attempted to allocate "
           &amp;lt;&amp;lt; strings::HumanReadableNumBytes::ToString(bytes)
           &amp;lt;&amp;lt; " which after aligning to "
           &amp;lt;&amp;lt; strings::HumanReadableNumBytes::ToString(alignment_in_bytes_)
           &amp;lt;&amp;lt; " cannot be expressed as an int64.";
  }

  return MathUtil::RoundUpTo&amp;lt;int64&amp;gt;(bytes, alignment_in_bytes_);
}
&lt;/code&gt;
&lt;p&gt;best_fit_allocator.h&lt;/p&gt;&lt;code&gt;// Rounds bytes up to nearest multiple of alignment_.
// REQUIRES: bytes &amp;gt;= 0.
// REQUIRES: result does not overflow int64.
// REQUIRES: alignment_in_bytes_ is a power of 2 (checked in constructor).
int64 RoundUpToAlignment(int64 bytes) const {
  DCHECK_GE(bytes, 0);
  DCHECK_LE(bytes, max_aligned_bytes_);
  int64 result =
      ((bytes + (alignment_in_bytes_ - 1)) &amp;amp; ~(alignment_in_bytes_ - 1));
  DCHECK_EQ(result, MathUtil::RoundUpTo&amp;lt;int64&amp;gt;(bytes, alignment_in_bytes_));
  return result;
}
&lt;/code&gt;
&lt;p&gt;Add ShapeUtil::ForEachIndexNoStatus to avoid creating a Status return object for every element of a tensor.&lt;/p&gt;&lt;p&gt;shape_util.h&lt;/p&gt;&lt;code&gt;using ForEachVisitorFunction =
    absl::FunctionRef&amp;lt;StatusOr&amp;lt;bool&amp;gt;(absl::Span&amp;lt;const int64_t&amp;gt;)&amp;gt;;
    ...
static void ForEachIndex(const Shape&amp;amp; shape, absl::Span&amp;lt;const int64_t&amp;gt; base,
                         absl::Span&amp;lt;const int64_t&amp;gt; count,
                         absl::Span&amp;lt;const int64_t&amp;gt; incr,
                         const ForEachVisitorFunction&amp;amp; visitor_function);

&lt;/code&gt;
&lt;code&gt;using ForEachVisitorFunctionNoStatus =
    absl::FunctionRef&amp;lt;bool(absl::Span&amp;lt;const int64_t&amp;gt;)&amp;gt;;
    ...
static void ForEachIndexNoStatus(
    const Shape&amp;amp; shape, absl::Span&amp;lt;const int64_t&amp;gt; base,
    absl::Span&amp;lt;const int64_t&amp;gt; count, absl::Span&amp;lt;const int64_t&amp;gt; incr,
    const ForEachVisitorFunctionNoStatus&amp;amp; visitor_function);
&lt;/code&gt;
&lt;p&gt;literal.cc&lt;/p&gt;&lt;code&gt;ShapeUtil::ForEachIndex(
    result_shape, [&amp;amp;](absl::Span&amp;lt;const int64_t&amp;gt; output_index) {
      for (int64_t i = 0, end = dimensions.size(); i &amp;lt; end; ++i) {
        scratch_source_index[i] = output_index[dimensions[i]];
      }
      int64_t dest_index = IndexUtil::MultidimensionalIndexToLinearIndex(
          result_shape, output_index);
      int64_t source_index = IndexUtil::MultidimensionalIndexToLinearIndex(
          shape(), scratch_source_index);
      memcpy(dest_data + primitive_size * dest_index,
             source_data + primitive_size * source_index, primitive_size);
      return true;
    });
&lt;/code&gt;
&lt;code&gt;ShapeUtil::ForEachIndexNoStatus(
    result_shape, [&amp;amp;](absl::Span&amp;lt;const int64_t&amp;gt; output_index) {
      // Compute dest_index
      int64_t dest_index = IndexUtil::MultidimensionalIndexToLinearIndex(
          result_shape, result_minor_to_major, output_index);

      // Compute source_index
      int64_t source_index;
      for (int64_t i = 0, end = dimensions.size(); i &amp;lt; end; ++i) {
        scratch_source_array[i] = output_index[dimensions[i]];
      }
      if (src_shape_dims == 1) {
        // Fast path for this case
        source_index = scratch_source_array[0];
        DCHECK_EQ(source_index,
                  IndexUtil::MultidimensionalIndexToLinearIndex(
                      src_shape, src_minor_to_major, scratch_source_span));
      } else {
        source_index = IndexUtil::MultidimensionalIndexToLinearIndex(
            src_shape, src_minor_to_major, scratch_source_span);
      }
      // Move one element from source_index in source to dest_index in dest
      memcpy(dest_data + PRIMITIVE_SIZE * dest_index,
             source_data + PRIMITIVE_SIZE * source_index, PRIMITIVE_SIZE);
      return true;
    });
&lt;/code&gt;
&lt;p&gt;In TF_CHECK_OK, avoid creating Ok object in order to test for ok().&lt;/p&gt;&lt;p&gt;status.h&lt;/p&gt;&lt;code&gt;#define TF_CHECK_OK(val) CHECK_EQ(::tensorflow::Status::OK(), (val))
#define TF_QCHECK_OK(val) QCHECK_EQ(::tensorflow::Status::OK(), (val))
&lt;/code&gt;
&lt;code&gt;extern tensorflow::string* TfCheckOpHelperOutOfLine(
    const ::tensorflow::Status&amp;amp; v, const char* msg);
inline tensorflow::string* TfCheckOpHelper(::tensorflow::Status v,
                                           const char* msg) {
  if (v.ok()) return nullptr;
  return TfCheckOpHelperOutOfLine(v, msg);
}
#define TF_CHECK_OK(val)                                           \
  while (tensorflow::string* _result = TfCheckOpHelper(val, #val)) \
  LOG(FATAL) &amp;lt;&amp;lt; *(_result)
#define TF_QCHECK_OK(val)                                          \
  while (tensorflow::string* _result = TfCheckOpHelper(val, #val)) \
  LOG(QFATAL) &amp;lt;&amp;lt; *(_result)
&lt;/code&gt;
&lt;p&gt;Remove StatusOr from the hot path of remote procedure calls (RPCs).&lt;/p&gt;&lt;p&gt;Removal of StatusOr from a hot path eliminated a 14% CPU regression in RPC benchmarks caused by an earlier change.&lt;/p&gt;&lt;p&gt;privacy_context.h&lt;/p&gt;&lt;code&gt;absl::StatusOr&amp;lt;privacy::context::PrivacyContext&amp;gt; GetRawPrivacyContext(
    const CensusHandle&amp;amp; h);
&lt;/code&gt;
&lt;p&gt;privacy_context_statusfree.h&lt;/p&gt;&lt;code&gt;enum class Result {
  kSuccess,
  kNoRootScopedData,
  kNoPrivacyContext,
  kNoDDTContext,
  kDeclassified,
  kNoPrequestContext
};
...
Result GetRawPrivacyContext(const CensusHandle&amp;amp; h,
                            PrivacyContext* privacy_context);
&lt;/code&gt;
&lt;p&gt;If possible, handle many items at once rather than just one at a time.&lt;/p&gt;&lt;p&gt;absl::flat_hash_map compares one hash byte per key from a group of keys using a single SIMD instruction.&lt;/p&gt;&lt;p&gt;See Swiss Table Design Notes and related CppCon 2017 and CppCon 2019 talks by Matt Kulukundis.&lt;/p&gt;&lt;p&gt;raw_hash_set.h&lt;/p&gt;&lt;code&gt;// Returns a bitmask representing the positions of slots that match hash.
BitMask&amp;lt;uint32_t&amp;gt; Match(h2_t hash) const {
  auto ctrl = _mm_loadu_si128(reinterpret_cast&amp;lt;const __m128i*&amp;gt;(pos));
  auto match = _mm_set1_epi8(hash);
  return BitMask&amp;lt;uint32_t&amp;gt;(_mm_movemask_epi8(_mm_cmpeq_epi8(match, ctrl)));
}
&lt;/code&gt;
&lt;p&gt;Do single operations to deal with many bytes and fix things up, rather than checking every byte what to do.&lt;/p&gt;&lt;p&gt;ordered-code.cc&lt;/p&gt;&lt;code&gt;int len = 0;
while (val &amp;gt; 0) {
  len++;
  buf[9 - len] = (val &amp;amp; 0xff);
  val &amp;gt;&amp;gt;= 8;
}
buf[9 - len - 1] = (unsigned char)len;
len++;
FastStringAppend(dest, reinterpret_cast&amp;lt;const char*&amp;gt;(buf + 9 - len), len);
&lt;/code&gt;
&lt;code&gt;BigEndian::Store(val, buf + 1);  // buf[0] may be needed for length
const unsigned int length = OrderedNumLength(val);
char* start = buf + 9 - length - 1;
*start = length;
AppendUpto9(dest, start, length + 1);
&lt;/code&gt;
&lt;p&gt;Improve Reed-Solomon processing speed by handling multiple interleaved input buffers more efficiently in chunks.&lt;/p&gt;&lt;code&gt;Run on (12 X 3501 MHz CPUs); 2016-09-27T16:04:55.065995192-04:00
CPU: Intel Haswell with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:15MB
Benchmark                          Base (ns)  New (ns) Improvement
------------------------------------------------------------------
BM_OneOutput/3/2                      466867    351818    +24.6%
BM_OneOutput/4/2                      563130    474756    +15.7%
BM_OneOutput/5/3                      815393    688820    +15.5%
BM_OneOutput/6/3                      897246    780539    +13.0%
BM_OneOutput/8/4                     1270489   1137149    +10.5%
BM_AllOutputs/3/2                     848772    642942    +24.3%
BM_AllOutputs/4/2                    1067647    638139    +40.2%
BM_AllOutputs/5/3                    1739135   1151369    +33.8%
BM_AllOutputs/6/3                    2045817   1456744    +28.8%
BM_AllOutputs/8/4                    3012958   2484937    +17.5%
BM_AllOutputsSetUpOnce/3/2            717310    493371    +31.2%
BM_AllOutputsSetUpOnce/4/2            833866    600060    +28.0%
BM_AllOutputsSetUpOnce/5/3           1537870   1137357    +26.0%
BM_AllOutputsSetUpOnce/6/3           1802353   1398600    +22.4%
BM_AllOutputsSetUpOnce/8/4           3166930   2455973    +22.4%
&lt;/code&gt;

&lt;p&gt;Decode four integers at a time (circa 2004).&lt;/p&gt;&lt;p&gt;Introduced a GroupVarInt format that encodes/decodes groups of 4 variable-length integers at a time in 5-17 bytes, rather than one integer at a time. Decoding one group of 4 integers in the new format takes ~1/3rd the time of decoding 4 individually varint-encoded integers.&lt;/p&gt;&lt;p&gt;groupvarint.cc&lt;/p&gt;&lt;code&gt;const char* DecodeGroupVar(const char* p, int N, uint32* dest) {
  assert(groupvar_initialized);
  assert(N % 4 == 0);
  while (N) {
    uint8 tag = *p;
    p++;

    uint8* lenptr = &amp;amp;groupvar_table[tag].length[0];

#define GET_NEXT                                        \
    do {                                                \
      uint8 len = *lenptr;                              \
      *dest = UNALIGNED_LOAD32(p) &amp;amp; groupvar_mask[len]; \
      dest++;                                           \
      p += len;                                         \
      lenptr++;                                         \
    } while (0)
    GET_NEXT;
    GET_NEXT;
    GET_NEXT;
    GET_NEXT;
#undef GET_NEXT

    N -= 4;
  }
  return p;
}
&lt;/code&gt;
&lt;p&gt;Encode groups of 4 k-bit numbers at a time.&lt;/p&gt;&lt;p&gt;Added KBitStreamEncoder and KBitStreamDecoder classes to encode/decode 4 k-bit numbers at a time into a bit stream. Since K is known at compile time, the encoding and decoding can be quite efficient. E.g., since four numbers are encoded at a time, the code can assume that the stream is always byte-aligned (for even k), or nibble-aligned (for odd k).&lt;/p&gt;&lt;p&gt;Sometimes a single CL contains a number of performance-improving changes that use many of the preceding techniques. Looking at the kinds of changes in these CLs is sometimes a good way to get in the mindset of making general changes to speed up the performance of some part of a system after that has been identified as a bottleneck.&lt;/p&gt;&lt;p&gt;Speed up GPU memory allocator by ~40%.&lt;/p&gt;&lt;p&gt;36-48% speedup in allocation/deallocation speed for GPUBFCAllocator:&lt;/p&gt;&lt;p&gt;Identify chunks by a handle number, rather than by a pointer to a Chunk. Chunk data structures are now allocated in a &lt;code&gt;vector&amp;lt;Chunk&amp;gt;&lt;/code&gt;, and a handle
is an index into this vector to refer to a particular chunk. This allows the
next and prev pointers in Chunk to be ChunkHandle (4 bytes), rather than
&lt;code&gt;Chunk*&lt;/code&gt; (8 bytes).&lt;/p&gt;&lt;p&gt;When a Chunk object is no longer in use, we maintain a free list of Chunk objects, whose head is designated by ChunkHandle &lt;code&gt;free_chunks_list_&lt;/code&gt;, and
with the &lt;code&gt;Chunk-&amp;gt;next&lt;/code&gt; pointing to the next free list entry. Together with
(1), this allows us to avoid heap allocation/deallocation of Chunk objects
in the allocator, except (rarely) when the &lt;code&gt;vector&amp;lt;Chunk&amp;gt;&lt;/code&gt; grows. It also
makes all the memory for Chunk objects contiguous.&lt;/p&gt;&lt;p&gt;Rather than having the bins_ data structure be a std::set and using lower_bound to locate the appropriate bin given a byte_size, we instead have an array of bins, indexed by a function that is log₂(byte_size/256). This allows the bin to be located with a few bit operations, rather than a binary search tree lookup. It also allows us to allocate the storage for all the Bin data structures in a contiguous array, rather than in many different cache lines. This reduces the number of cache lines that must be moved around between cores when multiple threads are doing allocations.&lt;/p&gt;&lt;p&gt;Added fast path to GPUBFCAllocator::AllocateRaw that first tries to allocate memory without involving the retry_helper_. If an initial attempt fails (returns nullptr), then we go through the retry_helper_, but normally we can avoid several levels of procedure calls as well as the allocation/deallocation of a std::function with several arguments.&lt;/p&gt;&lt;p&gt;Commented out most of the VLOG calls. These can be reenabled selectively when needed for debugging purposes by uncommenting and recompiling.&lt;/p&gt;&lt;p&gt;Added multi-threaded benchmark to test allocation under contention.&lt;/p&gt;&lt;p&gt;Speeds up ptb_word_lm on my desktop machine with a Titan X card from 8036 words per second to 8272 words per second (+2.9%).&lt;/p&gt;&lt;code&gt;Run on (40 X 2801 MHz CPUs); 2016/02/16-15:12:49
CPU: Intel Ivybridge with HyperThreading (20 cores) dL1:32KB dL2:256KB dL3:25MB
Benchmark                          Base (ns)  New (ns) Improvement
------------------------------------------------------------------
BM_Allocation                            347       184    +47.0%
BM_AllocationThreaded/1                  351       181    +48.4%
BM_AllocationThreaded/4                 2470      1975    +20.0%
BM_AllocationThreaded/16               11846      9507    +19.7%
BM_AllocationDelayed/1                   392       199    +49.2%
BM_AllocationDelayed/10                  285       169    +40.7%
BM_AllocationDelayed/100                 245       149    +39.2%
BM_AllocationDelayed/1000                238       151    +36.6%
&lt;/code&gt;

&lt;p&gt;Speed up Pathways throughput by ~20% via a set of miscellaneous changes.&lt;/p&gt;&lt;p&gt;Unified a bunch of special fast descriptor parsing functions into a single ParsedDescriptor class and use this class in more places to avoid expensive full parse calls.&lt;/p&gt;&lt;p&gt;Change several protocol buffer fields from string to bytes (avoids unnecessary utf-8 checks and associated error handling code).&lt;/p&gt;&lt;p&gt;DescriptorProto.inlined_contents is now a string, not a Cord (it is expected to be used only for small-ish tensors). This necessitated the addition of a bunch of copying helpers in tensor_util.cc (need to now support both strings and Cords).&lt;/p&gt;&lt;p&gt;Use flat_hash_map instead of std::unordered_map in a few places.&lt;/p&gt;&lt;p&gt;Added MemoryManager::LookupMany for use by Stack op instead of calling Lookup per batch element. This change reduces setup overhead like locking.&lt;/p&gt;&lt;p&gt;Removed some unnecessary string creation in TransferDispatchOp.&lt;/p&gt;&lt;p&gt;Performance results for transferring a batch of 1000 1KB tensors from one component to another in the same process:&lt;/p&gt;&lt;code&gt;Before: 227.01 steps/sec
After:  272.52 steps/sec (+20% throughput)
&lt;/code&gt;

&lt;p&gt;~15% XLA compiler performance improvement through a series of changes.&lt;/p&gt;&lt;p&gt;Some changes to speed up XLA compilation:&lt;/p&gt;&lt;p&gt;In SortComputationsByContent, return false if a == b in comparison function, to avoid serializing and fingerprinting long computation strings.&lt;/p&gt;&lt;p&gt;Turn CHECK into DCHECK to avoid touching an extra cache line in HloComputation::ComputeInstructionPostOrder&lt;/p&gt;&lt;p&gt;Avoid making an expensive copy of the front instruction in CoreSequencer::IsVectorSyncHoldSatisfied().&lt;/p&gt;&lt;p&gt;Rework 2-argument HloComputation::ToString and HloComputation::ToCord routines to do the bulk of the work in terms of appending to std::string, rather than appending to a Cord.&lt;/p&gt;&lt;p&gt;Change PerformanceCounterSet::Increment to just do a single hash table lookup rather than two.&lt;/p&gt;&lt;p&gt;Streamline Scoreboard::Update code&lt;/p&gt;&lt;p&gt;Overall speedup of 14% in XLA compilation time for one important model.&lt;/p&gt;&lt;p&gt;Speed up low level logging in Google Meet application code.&lt;/p&gt;&lt;p&gt;Speed up ScopedLogId, which is on the critical path for each packet.&lt;/p&gt;&lt;code&gt;LOG_EVERY_N(ERROR, ...)&lt;/code&gt; messages that seemed to be there only
to see if invariants were violated.&lt;code&gt;LOG_EVERY_N_SECONDS(ERROR, ...)&lt;/code&gt; statements, they are now small enough to
inline.&lt;code&gt;InlinedVector&amp;lt;...&amp;gt;&lt;/code&gt; for maintaining the thread local state. Since we
never were growing beyond size 4 anyway, the InlinedVector's functionality
was more general than needed.&lt;code&gt;Base: Baseline plus the code in scoped_logid_test.cc to add the benchmark
New: This changelist

CPU: Intel Ivybridge with HyperThreading (20 cores) dL1:32KB dL2:256KB dL3:25MB
Benchmark                                      Base (ns)    New (ns) Improvement
----------------------------------------------------------------------------
BM_ScopedLogId/threads:1                               8           4    +52.6%
BM_ScopedLogId/threads:2                               8           4    +51.9%
BM_ScopedLogId/threads:4                               8           4    +52.9%
BM_ScopedLogId/threads:8                               8           4    +52.1%
BM_ScopedLogId/threads:16                             11           6    +44.0%

&lt;/code&gt;

&lt;p&gt;Reduce XLA compilation time by ~31% by improving Shape handling.&lt;/p&gt;&lt;p&gt;Several changes to improve XLA compiler performance:&lt;/p&gt;&lt;p&gt;Improved performance of ShapeUtil::ForEachIndex... iteration in a few ways:&lt;/p&gt;&lt;p&gt;In ShapeUtil::ForEachState, save just pointers to the arrays represented by the spans, rather than the full span objects.&lt;/p&gt;&lt;p&gt;Pre-form a ShapeUtil::ForEachState::indexes_span pointing at the ShapeUtil::ForEachState::indexes vector, rather than constructing this span from the vector on every loop iteration.&lt;/p&gt;&lt;p&gt;Save a ShapeUtil::ForEachState::indexes_ptr pointer to the backing store of the ShapeUtil::ForEachState::indexes vector, allowing simple array operations in ShapeUtil::ForEachState::IncrementDim(), rather than more expensive vector::operator[] operations.&lt;/p&gt;&lt;p&gt;Save a ShapeUtil::ForEachState::minor_to_major array pointer initialized in the constructor by calling shape.layout().minor_to_major().data() rather than calling LayoutUtil::Minor(...) for each dimension for each iteration.&lt;/p&gt;&lt;p&gt;Inlined the ShapeUtil::ForEachState constructor and the ShapeUtil::ForEachState::IncrementDim() routines&lt;/p&gt;&lt;p&gt;Improved the performance of ShapeUtil::ForEachIndex iteration for call sites that don't need the functionality of returning a Status in the passed in function. Did this by introducing ShapeUtil::ForEachIndexNoStatus variants, which accept a ForEachVisitorFunctionNoStatus (which returns a plain bool). This is faster than the ShapeUtil::ForEachIndex routines, which accept a ForEachVisitorFunction (which returns a &lt;code&gt;StatusOr&amp;lt;bool&amp;gt;&lt;/code&gt;, which requires an
expensive &lt;code&gt;StatusOr&amp;lt;bool&amp;gt;&lt;/code&gt; destructor call per element that we iterate
over).&lt;/p&gt;&lt;p&gt;Improved performance of LiteralBase::Broadcast in several ways:&lt;/p&gt;&lt;p&gt;Introduced templated BroadcastHelper routine in literal.cc that is specialized for different primitive byte sizes (without this, primitive_size was a runtime variable and so the compiler couldn't do a very good job of optimizing the memcpy that occurred per element, and would invoke the general memcpy path that assumes the byte count is fairly large, even though in our case it is a tiny power of 2 (typically 1, 2, 4, or 8)).&lt;/p&gt;&lt;p&gt;Avoided all but one of ~(5 + num_dimensions + num_result_elements) virtual calls per Broadcast call by making a single call to 'shape()' at the beginning of the LiteralBase::Broadcast routine. The innocuous looking 'shape()' calls that were sprinkled throughout end up boiling down to "root_piece().subshape()", where subshape() is a virtual function.&lt;/p&gt;&lt;p&gt;In the BroadcastHelper routine, Special-cased the source dimensions being one and avoided a call to IndexUtil::MultiDimensionalIndexToLinearIndex for this case.&lt;/p&gt;&lt;p&gt;In BroadcastHelper, used a scratch_source_array pointer variable that points into the backing store of the scratch_source_index vector, and used that directly to avoid vector::operator[] operations inside the per-element code. Also pre-computed a scratch_source_span that points to the scratch_source_index vector outside the per-element loop in BroadcastHelper, to avoid constructing a span from the vector on each element.&lt;/p&gt;&lt;p&gt;Introduced new three-argument variant of IndexUtil::MultiDimensionalIndexToLinearIndex where the caller passes in the minor_to_major span associated with the shape argument. Used this in BroadcastHelper to compute this for the src and dst shapes once per Broadcast, rather than once per element copied.&lt;/p&gt;&lt;p&gt;In ShardingPropagation::GetShardingFromUser, for the HloOpcode::kTuple case, only call user.sharding().GetSubSharding(...) if we have found the operand to be of interest. Avoiding calling it eagerly reduces CPU time in this routine for one lengthy compilation from 43.7s to 2.0s.&lt;/p&gt;&lt;p&gt;Added benchmarks for ShapeUtil::ForEachIndex and Literal::Broadcast and for the new ShapeUtil::ForEachIndexNoStatus.&lt;/p&gt;&lt;code&gt;Base is with the benchmark additions of
BM_ForEachIndex and BM_BroadcastVectorToMatrix (and BUILD file change to add
benchmark dependency), but no other changes.

New is this cl

Run on (72 X 1357.56 MHz CPU s) CPU Caches: L1 Data 32 KiB (x36)
L1 Instruction 32 KiB (x36) L2 Unified 1024 KiB (x36) L3 Unified 25344 KiB (x2)

Benchmark                                      Base (ns)    New (ns) Improvement
----------------------------------------------------------------------------
BM_MakeShape                                       18.40       18.90     -2.7%
BM_MakeValidatedShape                              35.80       35.60     +0.6%
BM_ForEachIndex/0                                  57.80       55.80     +3.5%
BM_ForEachIndex/1                                  90.90       85.50     +5.9%
BM_ForEachIndex/2                               1973606     1642197     +16.8%
&lt;/code&gt;
&lt;p&gt;The newly added ForEachIndexNoStatus is considerably faster than the ForEachIndex variant (it only exists in this new cl, but the benchmark work that is done by BM_ForEachIndexNoStatus/NUM is comparable to the BM_ForEachIndex/NUM results above).&lt;/p&gt;&lt;code&gt;Benchmark                                      Base (ns)    New (ns) Improvement
----------------------------------------------------------------------------
BM_ForEachIndexNoStatus/0                             0        46.90    ----
BM_ForEachIndexNoStatus/1                             0        65.60    ----
BM_ForEachIndexNoStatus/2                             0     1001277     ----
&lt;/code&gt;
&lt;p&gt;Broadcast performance improves by ~58%.&lt;/p&gt;&lt;code&gt;Benchmark                                      Base (ns)    New (ns) Improvement
----------------------------------------------------------------------------
BM_BroadcastVectorToMatrix/16/16                   5556        2374     +57.3%
BM_BroadcastVectorToMatrix/16/1024               319510      131075     +59.0%
BM_BroadcastVectorToMatrix/1024/1024           20216949     8408188     +58.4%
&lt;/code&gt;
&lt;p&gt;Macro results from doing ahead-of-time compilation of a large language model (program does more than just the XLA compilation, but spends a bit less than half its time in XLA-related code):&lt;/p&gt;&lt;p&gt;Baseline program overall: 573 seconds With this cl program overall: 465 seconds (+19% improvement)&lt;/p&gt;&lt;p&gt;Time spent in compiling the two largest XLA programs in running this program:&lt;/p&gt;&lt;p&gt;Baseline: 141s + 143s = 284s With this CL: 99s + 95s = 194s (+31% improvement)&lt;/p&gt;&lt;p&gt;Reduce compilation time for large programs by ~22% in Plaque (a distributed execution framework).&lt;/p&gt;&lt;p&gt;Small tweaks to speed up compilation by ~22%.&lt;/p&gt;&lt;code&gt;pair&amp;lt;package, opname&amp;gt;&lt;/code&gt; instead of a btree of btrees.&lt;p&gt;Measurement of speed on large programs (~45K ops):&lt;/p&gt;&lt;code&gt;name             old time/op  new time/op  delta
BM_CompileLarge   28.5s ± 2%   22.4s ± 2%  -21.61%  (p=0.008 n=5+5)
&lt;/code&gt;

&lt;p&gt;MapReduce improvements (~2X speedup for wordcount benchmark).&lt;/p&gt;&lt;p&gt;Mapreduce speedups:&lt;/p&gt;&lt;p&gt;The combiner data structures for the SafeCombinerMapOutput class have been changed. Rather than using a &lt;code&gt;hash_multimap&amp;lt;SafeCombinerKey, StringPiece&amp;gt;&lt;/code&gt;,
which had a hash table entry for each unique key/value inserted in the
table, we instead use a &lt;code&gt;hash_map&amp;lt;SafeCombinerKey, ValuePtr*&amp;gt;&lt;/code&gt; (where
ValuePtr is a linked list of values and repetition counts). This helps in
three ways:&lt;/p&gt;&lt;p&gt;It significantly reduces memory usage, since we only use "sizeof(ValuePtr) + value_len" bytes for each value, rather than "sizeof(SafeCombinerKey) + sizeof(StringPiece) + value_len + new hash table entry overhead" for each value. This means that we flush the reducer buffer less often.&lt;/p&gt;&lt;p&gt;It's significantly faster, since we avoid extra hash table entries when we're inserting a new value for a key that already exists in the table (and instead we just hook the value into the linked list of values for that key).&lt;/p&gt;&lt;p&gt;Since we associate a repetition count with each value in the linked list, we can represent this sequence:&lt;/p&gt;&lt;code&gt;Output(key, "1");
Output(key, "1");
Output(key, "1");
Output(key, "1");
Output(key, "1");
&lt;/code&gt;
&lt;p&gt;as a single entry in the linked list for "key" with a repetition count of 5. Internally we yield "1" five times to the user-level combining function. (A similar trick could be applied on the reduce side, perhaps).&lt;/p&gt;&lt;p&gt;(Minor) Added a test for "nshards == 1" to the default MapReductionBase::KeyFingerprintSharding function that avoids fingerprinting the key entirely if we are just using 1 reduce shard (since we can just return 0 directly in that case without examining the key).&lt;/p&gt;&lt;p&gt;Turned some VLOG(3) statements into DVLOG(3) in the code path that is called for each key/value added to the combiner.&lt;/p&gt;&lt;p&gt;Reduces time for one wordcount benchmark from 12.56s to 6.55s.&lt;/p&gt;&lt;p&gt;Rework the alarm handling code in the SelectServer to significantly improve its performance (adding+removing an alarm from 771 ns to 271 ns).&lt;/p&gt;&lt;p&gt;Reworked the alarm handling code in the SelectServer to significantly improve its performance.&lt;/p&gt;&lt;p&gt;Changes:&lt;/p&gt;&lt;p&gt;Switched to using &lt;code&gt;AdjustablePriorityQueue&amp;lt;Alarm&amp;gt;&lt;/code&gt; instead of a a
&lt;code&gt;set&amp;lt;Alarm*&amp;gt;&lt;/code&gt; for the &lt;code&gt;AlarmQueue&lt;/code&gt;. This significantly speeds up alarm
handling, reducing the time taken to add and remove an alarm from 771
nanoseconds to 281 nanoseconds. This change avoids an
allocation/deallocation per alarm setup (for the red-black tree node in the
STL set object), and also gives much better cache locality (since the
AdjustablePriorityQueue is a heap implemented in a vector, rather than a
red-black tree), there are fewer cache lines touched when manipulating the
&lt;code&gt;AlarmQueue&lt;/code&gt; on every trip through the selectserver loop.&lt;/p&gt;&lt;p&gt;Converted AlarmList in Alarmer from a hash_map to a dense_hash_map to avoid another allocation/deallocation per alarm addition/deletion (this also improves cache locality when adding/removing alarms).&lt;/p&gt;&lt;p&gt;Removed the &lt;code&gt;num_alarms_stat_&lt;/code&gt; and &lt;code&gt;num_closures_stat_&lt;/code&gt;
MinuteTenMinuteHourStat objects, and the corresponding exported variables.
Although monitoring these seems nice, in practice they add significant
overhead to critical networking code. If I had left these variables in as
Atomic32 variables instead of MinuteTenMinuteHourStat, they would have still
increased the cost of adding and removing alarms from 281 nanoseconds to 340
nanoseconds.&lt;/p&gt;&lt;p&gt;Benchmark results&lt;/p&gt;&lt;code&gt;Benchmark                      Time(ns)  CPU(ns) Iterations
-----------------------------------------------------------
BM_AddAlarm/1                       902      771     777777
&lt;/code&gt;
&lt;p&gt;With this change&lt;/p&gt;&lt;code&gt;Benchmark                      Time(ns)  CPU(ns) Iterations
-----------------------------------------------------------
BM_AddAlarm/1                       324      281    2239999
&lt;/code&gt;

&lt;p&gt;3.3X performance in index serving speed!&lt;/p&gt;&lt;p&gt;We found a number of performance issues when planning a switch from on-disk to in-memory index serving in 2001. This change fixed many of these problems and took us from 150 to over 500 in-memory queries per second (for a 2 GB in-memory index on dual processor Pentium III machine).&lt;/p&gt;&lt;p&gt;In no particular order, a list of performance related books and articles that the authors have found helpful:&lt;/p&gt;&lt;p&gt;If you want to cite this document, we suggest:&lt;/p&gt;&lt;code&gt;Jeffrey Dean &amp;amp; Sanjay Ghemawat, Performance Hints, 2025, https://abseil.io/fast/hints.html
&lt;/code&gt;&lt;p&gt;Or in BibTeX:&lt;/p&gt;&lt;code&gt;@misc{DeanGhemawatPerformance2025,
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  title = {Performance Hints},
  year = {2025},
  howpublished = {\url{https://abseil.io/fast/hints.html}},
}
&lt;/code&gt;&lt;p&gt;Many colleagues have provided helpful feedback on this document, including:&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://abseil.io/fast/hints.html"/><published>2025-12-19T18:59:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46329536</id><title>Wall Street Ruined the Roomba and Then Blamed Lina Khan</title><updated>2025-12-19T20:40:55.327940+00:00</updated><content/><link href="https://www.thebignewsletter.com/p/how-wall-street-ruined-the-roomba"/><published>2025-12-19T18:59:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46329696</id><title>You can now play Grand Theft Auto Vice City in the browser</title><updated>2025-12-19T20:40:54.625527+00:00</updated><content>&lt;doc fingerprint="f1f8459e56b609a"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Grand Theft Auto: Vice City&lt;/head&gt;
    &lt;p&gt;The open-source implementation of the classic GTA engine, known as reVC, is now running directly in the browser. On DOS.Zone, you can explore a technology demo that showcases how this iconic game behaves in a modern web environment.&lt;/p&gt;
    &lt;p&gt;The engine has been completely reworked and carefully adapted to run smoothly inside the browser. Low-level systems such as rendering, input handling, audio, and file access were redesigned to work efficiently with WebAssembly and modern browser APIs, delivering stable performance without native installation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Disclaimer&lt;/head&gt;
    &lt;p&gt;This project is an independent, non-commercial technology demonstration. It is not affiliated with, endorsed by, sponsored by, or connected in any way to the original developers, publishers, or rights holders of the Grand Theft Auto series or any related trademarks, brands, or intellectual property.&lt;/p&gt;
    &lt;p&gt;All trademarks, game names, and references are the property of their respective owners and are used solely for informational and descriptive purposes. The reimplementation showcased here does not include or distribute original game assets. To experience the full version, users must legally own and provide their own original game resources.&lt;/p&gt;
    &lt;p&gt;This demo is provided for educational and research purposes only, to showcase technical capabilities of running complex game engines in a modern web environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Copyright Compliance&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;This project is operated in accordance with applicable copyright laws and respects the intellectual property rights of all rights holders.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The software component provided in this project (WebAssembly executable) is built from publicly available open-source code (including the re3 / reVC projects hosted on GitHub) and is distributed in compliance with the terms of their respective licenses. No proprietary or copyrighted game assets are included in the executable.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The project does not distribute full game content. Any game assets required for demonstration purposes are limited in scope, obtained from lawful sources, and technically restricted to operate only within the DOS.Zone domain. Public indexing, direct listing, or unrestricted access to these assets is not provided.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The demonstration version uses only a minimal and incomplete set of assets and is intended solely for educational and technological demonstration purposes, including showcasing modern web-browser capabilities and engine functionality.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Access to the full version of the game is possible only if the user independently provides original game assets obtained legally. Ownership of such assets is verified through cryptographic checksum validation (SHA-256). The project does not provide, host, or facilitate the distribution of full original game data.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This project is not affiliated with, endorsed by, sponsored by, or connected to the original developers, publishers, or rights holders of the game or related trademarks.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you are a copyright holder or an authorized representative of a copyright holder and believe that the use of materials within this project infringes your rights, please contact us by email so that we can promptly review your request.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dos.zone/grand-theft-auto-vice-city/"/><published>2025-12-19T19:12:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46330013</id><title>Show HN: Stickerbox, a kid-safe, AI-powered voice to sticker printer</title><updated>2025-12-19T20:40:54.342738+00:00</updated><content>&lt;doc fingerprint="4beca723d4b63860"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;IMAGINATION unboxedIMAGINATION unboxed&lt;/head&gt;
    &lt;p&gt; Get Your &lt;/p&gt;
    &lt;head rend="h2"&gt;StickerboxStickerbox&lt;/head&gt;
    &lt;p&gt; Stickerbox &lt;/p&gt;
    &lt;head rend="h2"&gt;In the wildIn the wild&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; @littlelorenzfamm&lt;p&gt;“Best invention EVER! Thank you!!!!”&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“This might just be the coolest creative tool I've seen for kids”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt; @bbbritnee&lt;p&gt;“Its such a good approach to AI.. It's def a present I'll give”&lt;/p&gt;@eateachlove&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://stickerbox.com/"/><published>2025-12-19T19:44:48+00:00</published></entry></feed>