<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-30T22:10:24.763624+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46432916</id><title>Approachable Swift Concurrency</title><updated>2025-12-30T22:10:33.591251+00:00</updated><content>&lt;doc fingerprint="7470a7176c2d9444"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fucking Approachable&lt;lb/&gt;Swift Concurrency&lt;/head&gt;
    &lt;p&gt;Finally understand async/await, Tasks, and why the compiler keeps yelling at you.&lt;/p&gt;
    &lt;p&gt;Huge thanks to Matt Massicotte for making Swift concurrency understandable. Put together by Pedro Piñera. Found an issue? [email protected]&lt;/p&gt;
    &lt;p&gt;In the tradition of fuckingblocksyntax.com and fuckingifcaseletsyntax.com&lt;/p&gt;
    &lt;head rend="h2"&gt;Async Code: async/await&lt;/head&gt;
    &lt;p&gt;Most of what apps do is wait. Fetch data from a server - wait for the response. Read a file from disk - wait for the bytes. Query a database - wait for the results.&lt;/p&gt;
    &lt;p&gt;Before Swift's concurrency system, you'd express this waiting with callbacks, delegates, or Combine. They work, but nested callbacks get hard to follow, and Combine has a steep learning curve.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;async/await&lt;/code&gt; gives Swift a new way to handle waiting. Instead of callbacks, you write code that looks sequential - it pauses, waits, and resumes. Under the hood, Swift's runtime manages these pauses efficiently. But making your app actually stay responsive while waiting depends on where code runs, which we'll cover later.&lt;/p&gt;
    &lt;p&gt;An async function is one that might need to pause. You mark it with &lt;code&gt;async&lt;/code&gt;, and when you call it, you use &lt;code&gt;await&lt;/code&gt; to say "pause here until this finishes":&lt;/p&gt;
    &lt;code&gt;func fetchUser(id: Int) async throws -&amp;gt; User {
    let url = URL(string: "https://api.example.com/users/\(id)")!
    let (data, _) = try await URLSession.shared.data(from: url)  // Suspends here
    return try JSONDecoder().decode(User.self, from: data)
}

// Calling it
let user = try await fetchUser(id: 123)
// Code here runs after fetchUser completes&lt;/code&gt;
    &lt;p&gt;Your code pauses at each &lt;code&gt;await&lt;/code&gt; - this is called suspension. When the work finishes, your code resumes right where it left off. Suspension gives Swift the opportunity to do other work while waiting.&lt;/p&gt;
    &lt;head rend="h3"&gt;Waiting for them&lt;/head&gt;
    &lt;p&gt;What if you need to fetch several things? You could await them one by one:&lt;/p&gt;
    &lt;code&gt;let avatar = try await fetchImage("avatar.jpg")
let banner = try await fetchImage("banner.jpg")
let bio = try await fetchBio()&lt;/code&gt;
    &lt;p&gt;But that's slow - each waits for the previous one to finish. Use &lt;code&gt;async let&lt;/code&gt; to run them in parallel:&lt;/p&gt;
    &lt;code&gt;func loadProfile() async throws -&amp;gt; Profile {
    async let avatar = fetchImage("avatar.jpg")
    async let banner = fetchImage("banner.jpg")
    async let bio = fetchBio()

    // All three are fetching in parallel!
    return Profile(
        avatar: try await avatar,
        banner: try await banner,
        bio: try await bio
    )
}&lt;/code&gt;
    &lt;p&gt;Each &lt;code&gt;async let&lt;/code&gt; starts immediately. The &lt;code&gt;await&lt;/code&gt; collects the results.&lt;/p&gt;
    &lt;head rend="h4"&gt;await needs async&lt;/head&gt;
    &lt;p&gt;You can only use &lt;code&gt;await&lt;/code&gt; inside an &lt;code&gt;async&lt;/code&gt; function.&lt;/p&gt;
    &lt;head rend="h2"&gt;Managing Work: Tasks&lt;/head&gt;
    &lt;p&gt;A Task is a unit of async work you can manage. You've written async functions, but a Task is what actually runs them. It's how you start async code from synchronous code, and it gives you control over that work: wait for its result, cancel it, or let it run in the background.&lt;/p&gt;
    &lt;p&gt;Let's say you're building a profile screen. Load the avatar when the view appears using the &lt;code&gt;.task&lt;/code&gt; modifier, which cancels automatically when the view disappears:&lt;/p&gt;
    &lt;code&gt;struct ProfileView: View {
    @State private var avatar: Image?

    var body: some View {
        avatar
            .task { avatar = await downloadAvatar() }
    }
}&lt;/code&gt;
    &lt;p&gt;If users can switch between profiles, use &lt;code&gt;.task(id:)&lt;/code&gt; to reload when the selection changes:&lt;/p&gt;
    &lt;code&gt;struct ProfileView: View {
    var userID: String
    @State private var avatar: Image?

    var body: some View {
        avatar
            .task(id: userID) { avatar = await downloadAvatar(for: userID) }
    }
}&lt;/code&gt;
    &lt;p&gt;When the user taps "Save", create a Task manually:&lt;/p&gt;
    &lt;code&gt;Button("Save") {
    Task { await saveProfile() }
}&lt;/code&gt;
    &lt;p&gt;What if you need to load the avatar, bio, and stats all at once? Use a &lt;code&gt;TaskGroup&lt;/code&gt; to fetch them in parallel:&lt;/p&gt;
    &lt;code&gt;try await withThrowingTaskGroup(of: Void.self) { group in
    group.addTask { avatar = try await downloadAvatar(for: userID) }
    group.addTask { bio = try await fetchBio(for: userID) }
    group.addTask { stats = try await fetchStats(for: userID) }
    try await group.waitForAll()
}&lt;/code&gt;
    &lt;p&gt;Tasks inside a group are child tasks, linked to the parent. A few things to know:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cancellation propagates: cancel the parent, and all children get cancelled too&lt;/item&gt;
      &lt;item&gt;Errors: a thrown error cancels siblings and rethrows, but only when you consume results with &lt;code&gt;next()&lt;/code&gt;,&lt;code&gt;waitForAll()&lt;/code&gt;, or iteration&lt;/item&gt;
      &lt;item&gt;Completion order: results arrive as tasks finish, not the order you added them&lt;/item&gt;
      &lt;item&gt;Waits for all: the group doesn't return until every child completes or is cancelled&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is structured concurrency: work organized in a tree that's easy to reason about and clean up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Things Run: From Threads to Isolation Domains&lt;/head&gt;
    &lt;p&gt;So far we've talked about when code runs (async/await) and how to organize it (Tasks). Now: where does it run, and how do we keep it safe?&lt;/p&gt;
    &lt;head rend="h4"&gt;Most apps just wait&lt;/head&gt;
    &lt;p&gt;Most app code is I/O-bound. You fetch data from a network, await a response, decode it, and display it. If you have multiple I/O operations to coordinate, you resort to tasks and task groups. The actual CPU work is minimal. The main thread can handle this fine because &lt;code&gt;await&lt;/code&gt; suspends without blocking.&lt;/p&gt;
    &lt;p&gt;But sooner or later, you'll have CPU-bound work: parsing a giant JSON file, processing images, running complex calculations. This work doesn't wait for anything external. It just needs CPU cycles. If you run it on the main thread, your UI freezes. This is where "where does code run" actually matters.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Old World: Many Options, No Safety&lt;/head&gt;
    &lt;p&gt;Before Swift's concurrency system, you had several ways to manage execution:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Approach&lt;/cell&gt;
        &lt;cell role="head"&gt;What it does&lt;/cell&gt;
        &lt;cell role="head"&gt;Tradeoffs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Thread&lt;/cell&gt;
        &lt;cell&gt;Direct thread control&lt;/cell&gt;
        &lt;cell&gt;Low-level, error-prone, rarely needed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GCD&lt;/cell&gt;
        &lt;cell&gt;Dispatch queues with closures&lt;/cell&gt;
        &lt;cell&gt;Simple but no cancellation, easy to cause thread explosion&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;OperationQueue&lt;/cell&gt;
        &lt;cell&gt;Task dependencies, cancellation, KVO&lt;/cell&gt;
        &lt;cell&gt;More control but verbose and heavyweight&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Combine&lt;/cell&gt;
        &lt;cell&gt;Reactive streams&lt;/cell&gt;
        &lt;cell&gt;Great for event streams, steep learning curve&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All of these worked, but safety was entirely on you. The compiler couldn't help if you forgot to dispatch to main, or if two queues accessed the same data simultaneously.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Problem: Data Races&lt;/head&gt;
    &lt;p&gt;A data race happens when two threads access the same memory at the same time, and at least one is writing:&lt;/p&gt;
    &lt;code&gt;var count = 0

DispatchQueue.global().async { count += 1 }
DispatchQueue.global().async { count += 1 }

// Undefined behavior: crash, memory corruption, or wrong value&lt;/code&gt;
    &lt;p&gt;Data races are undefined behavior. They can crash, corrupt memory, or silently produce wrong results. Your app works fine in testing, then crashes randomly in production. Traditional tools like locks and semaphores help, but they're manual and error-prone.&lt;/p&gt;
    &lt;head rend="h4"&gt;Concurrency amplifies the problem&lt;/head&gt;
    &lt;p&gt;The more concurrent your app is, the more likely data races become. A simple iOS app might get away with sloppy thread safety. A web server handling thousands of simultaneous requests will crash constantly. This is why Swift's compile-time safety matters most in high-concurrency environments.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Shift: From Threads to Isolation&lt;/head&gt;
    &lt;p&gt;Swift's concurrency model asks a different question. Instead of "which thread should this run on?", it asks: "who is allowed to access this data?"&lt;/p&gt;
    &lt;p&gt;This is isolation. Rather than manually dispatching work to threads, you declare boundaries around data. The compiler enforces these boundaries at build time, not runtime.&lt;/p&gt;
    &lt;head rend="h4"&gt;Under the hood&lt;/head&gt;
    &lt;p&gt;Swift Concurrency is built on top of libdispatch (the same runtime as GCD). The difference is the compile-time layer: actors and isolation are enforced by the compiler, while the runtime handles scheduling on a cooperative thread pool limited to your CPU's core count.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Three Isolation Domains&lt;/head&gt;
    &lt;p&gt;1. MainActor&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;@MainActor&lt;/code&gt; is a global actor that represents the main thread's isolation domain. It's special because UI frameworks (UIKit, AppKit, SwiftUI) require main thread access.&lt;/p&gt;
    &lt;code&gt;@MainActor
class ViewModel {
    var items: [Item] = []  // Protected by MainActor isolation
}&lt;/code&gt;
    &lt;p&gt;When you mark something &lt;code&gt;@MainActor&lt;/code&gt;, you're not saying "dispatch this to the main thread." You're saying "this belongs to the main actor's isolation domain." The compiler enforces that anything accessing it must either be on MainActor or &lt;code&gt;await&lt;/code&gt; to cross the boundary.&lt;/p&gt;
    &lt;head rend="h4"&gt;When in doubt, use @MainActor&lt;/head&gt;
    &lt;p&gt;For most apps, marking your ViewModels with &lt;code&gt;@MainActor&lt;/code&gt; is the right choice. Performance concerns are usually overblown. Start here, optimize only if you measure actual problems.&lt;/p&gt;
    &lt;p&gt;2. Actors&lt;/p&gt;
    &lt;p&gt;An actor protects its own mutable state. It guarantees that only one piece of code can access its data at a time:&lt;/p&gt;
    &lt;code&gt;actor BankAccount {
    var balance: Double = 0

    func deposit(_ amount: Double) {
        balance += amount  // Safe: actor guarantees exclusive access
    }
}

// From outside, you must await to cross the boundary
await account.deposit(100)&lt;/code&gt;
    &lt;p&gt;Actors are not threads. An actor is an isolation boundary. The Swift runtime decides which thread actually executes actor code. You don't control that, and you don't need to.&lt;/p&gt;
    &lt;p&gt;3. Nonisolated&lt;/p&gt;
    &lt;p&gt;Code marked &lt;code&gt;nonisolated&lt;/code&gt; opts out of actor isolation. It can be called from anywhere without &lt;code&gt;await&lt;/code&gt;, but it cannot access the actor's protected state:&lt;/p&gt;
    &lt;code&gt;actor BankAccount {
    var balance: Double = 0

    nonisolated func bankName() -&amp;gt; String {
        "Acme Bank"  // No actor state accessed, safe to call from anywhere
    }
}

let name = account.bankName()  // No await needed&lt;/code&gt;
    &lt;head rend="h4"&gt;Approachable Concurrency: Less Friction&lt;/head&gt;
    &lt;p&gt;Approachable Concurrency simplifies the mental model with two Xcode build settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SWIFT_DEFAULT_ACTOR_ISOLATION&lt;/code&gt;=&lt;code&gt;MainActor&lt;/code&gt;: Everything runs on MainActor unless you say otherwise&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SWIFT_APPROACHABLE_CONCURRENCY&lt;/code&gt;=&lt;code&gt;YES&lt;/code&gt;:&lt;code&gt;nonisolated&lt;/code&gt;async functions stay on the caller's actor instead of jumping to a background thread&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;New Xcode 26 projects have both enabled by default. When you need CPU-intensive work off the main thread, use &lt;code&gt;@concurrent&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;// Runs on MainActor (the default)
func updateUI() async { }

// Runs on background thread (opt-in)
@concurrent func processLargeFile() async { }&lt;/code&gt;
    &lt;head rend="h4"&gt;The Office Building&lt;/head&gt;
    &lt;p&gt;Think of your app as an office building. Each isolation domain is a private office with a lock on the door. Only one person can be inside at a time, working with the documents in that office.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;MainActor&lt;/code&gt;is the front desk - where all customer interactions happen. There's only one, and it handles everything the user sees.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;actor&lt;/code&gt;types are department offices - Accounting, Legal, HR. Each protects its own sensitive documents.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;nonisolated&lt;/code&gt;code is the hallway - shared space anyone can walk through, but no private documents live there.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can't just barge into someone's office. You knock (&lt;code&gt;await&lt;/code&gt;) and wait for them to let you in.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Can Cross Isolation Domains: Sendable&lt;/head&gt;
    &lt;p&gt;Isolation domains protect data, but eventually you need to pass data between them. When you do, Swift checks if it's safe.&lt;/p&gt;
    &lt;p&gt;Think about it: if you pass a reference to a mutable class from one actor to another, both actors could modify it simultaneously. That's exactly the data race we're trying to prevent. So Swift needs to know: can this data be safely shared?&lt;/p&gt;
    &lt;p&gt;The answer is the &lt;code&gt;Sendable&lt;/code&gt; protocol. It's a marker that tells the compiler "this type is safe to pass across isolation boundaries":&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sendable types can cross safely (value types, immutable data, actors)&lt;/item&gt;
      &lt;item&gt;Non-Sendable types can't (classes with mutable state)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;// Sendable - it's a value type, each place gets a copy
struct User: Sendable {
    let id: Int
    let name: String
}

// Non-Sendable - it's a class with mutable state
class Counter {
    var count = 0  // Two places modifying this = disaster
}&lt;/code&gt;
    &lt;head rend="h3"&gt;Making Types Sendable&lt;/head&gt;
    &lt;p&gt;Swift automatically infers &lt;code&gt;Sendable&lt;/code&gt; for many types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Structs and enums with only &lt;code&gt;Sendable&lt;/code&gt;properties are implicitly&lt;code&gt;Sendable&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Actors are always &lt;code&gt;Sendable&lt;/code&gt;because they protect their own state&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@MainActor&lt;/code&gt;types are&lt;code&gt;Sendable&lt;/code&gt;because MainActor serializes access&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For classes, it's harder. A class can conform to &lt;code&gt;Sendable&lt;/code&gt; only if it's &lt;code&gt;final&lt;/code&gt; and all its stored properties are immutable:&lt;/p&gt;
    &lt;code&gt;final class APIConfig: Sendable {
    let baseURL: URL      // Immutable
    let timeout: Double   // Immutable
}&lt;/code&gt;
    &lt;p&gt;If you have a class that's thread-safe through other means (locks, atomics), you can use &lt;code&gt;@unchecked Sendable&lt;/code&gt; to tell the compiler "trust me":&lt;/p&gt;
    &lt;code&gt;final class ThreadSafeCache: @unchecked Sendable {
    private let lock = NSLock()
    private var storage: [String: Data] = [:]
}&lt;/code&gt;
    &lt;head rend="h4"&gt;@unchecked Sendable is a promise&lt;/head&gt;
    &lt;p&gt;The compiler won't verify thread safety. If you're wrong, you'll get data races. Use sparingly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Approachable Concurrency: Less Friction&lt;/head&gt;
    &lt;p&gt;With Approachable Concurrency, Sendable errors become much rarer:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If code doesn't cross isolation boundaries, you don't need Sendable&lt;/item&gt;
      &lt;item&gt;Async functions stay on the caller's actor instead of hopping to a background thread&lt;/item&gt;
      &lt;item&gt;The compiler is smarter about detecting when values are used safely&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Enable it by setting &lt;code&gt;SWIFT_DEFAULT_ACTOR_ISOLATION&lt;/code&gt; to &lt;code&gt;MainActor&lt;/code&gt; and &lt;code&gt;SWIFT_APPROACHABLE_CONCURRENCY&lt;/code&gt; to &lt;code&gt;YES&lt;/code&gt;. New Xcode 26 projects have both enabled by default. When you do need parallelism, mark functions &lt;code&gt;@concurrent&lt;/code&gt; and then think about Sendable.&lt;/p&gt;
    &lt;head rend="h4"&gt;Photocopies vs. Original Documents&lt;/head&gt;
    &lt;p&gt;Back to the office building. When you need to share information between departments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Photocopies are safe - If Legal makes a copy of a document and sends it to Accounting, both have their own copy. They can scribble on them, modify them, whatever. No conflict.&lt;/item&gt;
      &lt;item&gt;Original signed contracts must stay put - If two departments could both modify the original, chaos ensues. Who has the real version?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;Sendable&lt;/code&gt; types are like photocopies: safe to share because each place gets its own independent copy (value types) or because they're immutable (nobody can modify them). Non-&lt;code&gt;Sendable&lt;/code&gt; types are like original contracts: passing them around creates the potential for conflicting modifications.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Isolation Is Inherited&lt;/head&gt;
    &lt;p&gt;You've seen that isolation domains protect data, and Sendable controls what crosses between them. But how does code end up in an isolation domain in the first place?&lt;/p&gt;
    &lt;p&gt;When you call a function or create a closure, isolation flows through your code. With Approachable Concurrency, your app starts on &lt;code&gt;MainActor&lt;/code&gt;, and that isolation propagates to the code you call, unless something explicitly changes it. Understanding this flow helps you predict where code runs and why the compiler sometimes complains.&lt;/p&gt;
    &lt;head rend="h3"&gt;Function Calls&lt;/head&gt;
    &lt;p&gt;When you call a function, its isolation determines where it runs:&lt;/p&gt;
    &lt;code&gt;@MainActor func updateUI() { }      // Always runs on MainActor
func helper() { }                    // Inherits caller's isolation
@concurrent func crunch() async { }  // Explicitly runs off-actor&lt;/code&gt;
    &lt;p&gt;With Approachable Concurrency, most of your code inherits &lt;code&gt;MainActor&lt;/code&gt; isolation. The function runs where the caller runs, unless it explicitly opts out.&lt;/p&gt;
    &lt;head rend="h3"&gt;Closures&lt;/head&gt;
    &lt;p&gt;Closures inherit isolation from the context where they're defined:&lt;/p&gt;
    &lt;code&gt;@MainActor
class ViewModel {
    func setup() {
        let closure = {
            // Inherits MainActor from ViewModel
            self.updateUI()  // Safe, same isolation
        }
        closure()
    }
}&lt;/code&gt;
    &lt;p&gt;This is why SwiftUI's &lt;code&gt;Button&lt;/code&gt; action closures can safely update &lt;code&gt;@State&lt;/code&gt;: they inherit MainActor isolation from the view.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tasks&lt;/head&gt;
    &lt;p&gt;A &lt;code&gt;Task { }&lt;/code&gt; inherits actor isolation from where it's created:&lt;/p&gt;
    &lt;code&gt;@MainActor
class ViewModel {
    func doWork() {
        Task {
            // Inherits MainActor isolation
            self.updateUI()  // Safe, no await needed
        }
    }
}&lt;/code&gt;
    &lt;p&gt;This is usually what you want. The task runs on the same actor as the code that created it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Breaking Inheritance: Task.detached&lt;/head&gt;
    &lt;p&gt;Sometimes you want a task that doesn't inherit any context:&lt;/p&gt;
    &lt;code&gt;@MainActor
class ViewModel {
    func doHeavyWork() {
        Task.detached {
            // No actor isolation, runs on cooperative pool
            let result = await self.expensiveCalculation()
            await MainActor.run {
                self.data = result  // Explicitly hop back
            }
        }
    }
}&lt;/code&gt;
    &lt;head rend="h4"&gt;Task.detached is usually wrong&lt;/head&gt;
    &lt;p&gt;The Swift team recommends Task.detached as a last resort. It doesn't inherit priority, task-local values, or actor context. Most of the time, regular &lt;code&gt;Task&lt;/code&gt; is what you want. If you need CPU-intensive work off the main actor, mark the function &lt;code&gt;@concurrent&lt;/code&gt; instead.&lt;/p&gt;
    &lt;head rend="h4"&gt;Walking Through the Building&lt;/head&gt;
    &lt;p&gt;When you're in the front desk office (MainActor), and you call someone to help you, they come to your office. They inherit your location. If you create a task ("go do this for me"), that assistant starts in your office too.&lt;/p&gt;
    &lt;p&gt;The only way someone ends up in a different office is if they explicitly go there: "I need to work in Accounting for this" (&lt;code&gt;actor&lt;/code&gt;), or "I'll handle this in the back office" (&lt;code&gt;@concurrent&lt;/code&gt;).&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting It All Together&lt;/head&gt;
    &lt;p&gt;Let's step back and see how all the pieces fit.&lt;/p&gt;
    &lt;p&gt;Swift Concurrency can feel like a lot of concepts: &lt;code&gt;async/await&lt;/code&gt;, &lt;code&gt;Task&lt;/code&gt;, actors, &lt;code&gt;MainActor&lt;/code&gt;, &lt;code&gt;Sendable&lt;/code&gt;, isolation domains. But there's really just one idea at the center of it all: isolation is inherited by default.&lt;/p&gt;
    &lt;p&gt;With Approachable Concurrency enabled, your app starts on &lt;code&gt;MainActor&lt;/code&gt;. That's your starting point. From there:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Every function you call inherits that isolation&lt;/item&gt;
      &lt;item&gt;Every closure you create captures that isolation&lt;/item&gt;
      &lt;item&gt;Every &lt;code&gt;Task { }&lt;/code&gt;you spawn inherits that isolation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You don't have to annotate anything. You don't have to think about threads. Your code runs on &lt;code&gt;MainActor&lt;/code&gt;, and the isolation just propagates through your program automatically.&lt;/p&gt;
    &lt;p&gt;When you need to break out of that inheritance, you do it explicitly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;@concurrent&lt;/code&gt;says "run this on a background thread"&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;actor&lt;/code&gt;says "this type has its own isolation domain"&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Task.detached { }&lt;/code&gt;says "start fresh, inherit nothing"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And when you pass data between isolation domains, Swift checks that it's safe. That's what &lt;code&gt;Sendable&lt;/code&gt; is for: marking types that can safely cross boundaries.&lt;/p&gt;
    &lt;p&gt;That's it. That's the whole model:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Isolation propagates from &lt;code&gt;MainActor&lt;/code&gt;through your code&lt;/item&gt;
      &lt;item&gt;You opt out explicitly when you need background work or separate state&lt;/item&gt;
      &lt;item&gt;Sendable guards the boundaries when data crosses between domains&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the compiler complains, it's telling you one of these rules was violated. Trace the inheritance: where did the isolation come from? Where is the code trying to run? What data is crossing a boundary? The answer is usually obvious once you ask the right question.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where to Go From Here&lt;/head&gt;
    &lt;p&gt;The good news: you don't need to master everything at once.&lt;/p&gt;
    &lt;p&gt;Most apps only need the basics. Mark your ViewModels with &lt;code&gt;@MainActor&lt;/code&gt;, use &lt;code&gt;async/await&lt;/code&gt; for network calls, and create &lt;code&gt;Task { }&lt;/code&gt; when you need to kick off async work from a button tap. That's it. That handles 80% of real-world apps. The compiler will tell you if you need more.&lt;/p&gt;
    &lt;p&gt;When you need parallel work, reach for &lt;code&gt;async let&lt;/code&gt; to fetch multiple things at once, or &lt;code&gt;TaskGroup&lt;/code&gt; when the number of tasks is dynamic. Learn to handle cancellation gracefully. This covers apps with complex data loading or real-time features.&lt;/p&gt;
    &lt;p&gt;Advanced patterns come later, if ever. Custom actors for shared mutable state, &lt;code&gt;@concurrent&lt;/code&gt; for CPU-intensive processing, deep &lt;code&gt;Sendable&lt;/code&gt; understanding. This is framework code, server-side Swift, complex desktop apps. Most developers never need this level.&lt;/p&gt;
    &lt;head rend="h4"&gt;Start simple&lt;/head&gt;
    &lt;p&gt;Don't optimize for problems you don't have. Start with the basics, ship your app, and add complexity only when you hit real problems. The compiler will guide you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Watch Out: Common Mistakes&lt;/head&gt;
    &lt;head rend="h3"&gt;Thinking async = background&lt;/head&gt;
    &lt;code&gt;// This STILL blocks the main thread!
@MainActor
func slowFunction() async {
    let result = expensiveCalculation()  // Synchronous work = blocking
    data = result
}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;async&lt;/code&gt; means "can pause." The actual work still runs wherever it runs. Use &lt;code&gt;@concurrent&lt;/code&gt; (Swift 6.2) or &lt;code&gt;Task.detached&lt;/code&gt; for CPU-heavy work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Creating too many actors&lt;/head&gt;
    &lt;code&gt;// Over-engineered
actor NetworkManager { }
actor CacheManager { }
actor DataManager { }

// Better - most things can live on MainActor
@MainActor
class AppState { }&lt;/code&gt;
    &lt;p&gt;You need a custom actor only when you have shared mutable state that can't live on &lt;code&gt;MainActor&lt;/code&gt;. Matt Massicotte's rule: introduce an actor only when (1) you have non-&lt;code&gt;Sendable&lt;/code&gt; state, (2) operations on that state must be atomic, and (3) those operations can't run on an existing actor. If you can't justify it, use &lt;code&gt;@MainActor&lt;/code&gt; instead.&lt;/p&gt;
    &lt;head rend="h3"&gt;Making everything Sendable&lt;/head&gt;
    &lt;p&gt;Not everything needs to cross boundaries. If you're adding &lt;code&gt;@unchecked Sendable&lt;/code&gt; everywhere, step back and ask if the data actually needs to move between isolation domains.&lt;/p&gt;
    &lt;head rend="h3"&gt;Using MainActor.run when you don't need it&lt;/head&gt;
    &lt;code&gt;// Unnecessary
Task {
    let data = await fetchData()
    await MainActor.run {
        self.data = data
    }
}

// Better - just make the function @MainActor
@MainActor
func loadData() async {
    self.data = await fetchData()
}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;MainActor.run&lt;/code&gt; is rarely the right solution. If you need MainActor isolation, annotate the function with &lt;code&gt;@MainActor&lt;/code&gt; instead. It's clearer and the compiler can help you more. See Matt's take on this.&lt;/p&gt;
    &lt;head rend="h3"&gt;Blocking the cooperative thread pool&lt;/head&gt;
    &lt;code&gt;// NEVER do this - risks deadlock
func badIdea() async {
    let semaphore = DispatchSemaphore(value: 0)
    Task {
        await doWork()
        semaphore.signal()
    }
    semaphore.wait()  // Blocks a cooperative thread!
}&lt;/code&gt;
    &lt;p&gt;Swift's cooperative thread pool has limited threads. Blocking one with &lt;code&gt;DispatchSemaphore&lt;/code&gt;, &lt;code&gt;DispatchGroup.wait()&lt;/code&gt;, or similar calls can cause deadlocks. If you need to bridge sync and async code, use &lt;code&gt;async let&lt;/code&gt; or restructure to stay fully async.&lt;/p&gt;
    &lt;head rend="h3"&gt;Creating unnecessary Tasks&lt;/head&gt;
    &lt;code&gt;// Unnecessary Task creation
func fetchAll() async {
    Task { await fetchUsers() }
    Task { await fetchPosts() }
}

// Better - use structured concurrency
func fetchAll() async {
    async let users = fetchUsers()
    async let posts = fetchPosts()
    await (users, posts)
}&lt;/code&gt;
    &lt;p&gt;If you're already in an async context, prefer structured concurrency (&lt;code&gt;async let&lt;/code&gt;, &lt;code&gt;TaskGroup&lt;/code&gt;) over creating unstructured &lt;code&gt;Task&lt;/code&gt;s. Structured concurrency handles cancellation automatically and makes the code easier to reason about.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cheat Sheet: Quick Reference&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Keyword&lt;/cell&gt;
        &lt;cell role="head"&gt;What it does&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;async&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Function can pause&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;await&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Pause here until done&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Task { }&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start async work, inherits context&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Task.detached { }&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start async work, no inherited context&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;@MainActor&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Runs on main thread&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;actor&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Type with isolated mutable state&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;nonisolated&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Opts out of actor isolation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Sendable&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Safe to pass between isolation domains&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;@concurrent&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Always run on background (Swift 6.2+)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;async let&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start parallel work&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;TaskGroup&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dynamic parallel work&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Further Reading&lt;/head&gt;
    &lt;head rend="h4"&gt;Matt Massicotte's Blog (Highly Recommended)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Swift Concurrency Glossary - Essential terminology&lt;/item&gt;
      &lt;item&gt;An Introduction to Isolation - The core concept&lt;/item&gt;
      &lt;item&gt;When should you use an actor? - Practical guidance&lt;/item&gt;
      &lt;item&gt;Non-Sendable types are cool too - Why simpler is better&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fuckingapproachableswiftconcurrency.com/en/"/><published>2025-12-30T13:01:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46432999</id><title>The British empire's resilient subsea telegraph network</title><updated>2025-12-30T22:10:33.368467+00:00</updated><content>&lt;doc fingerprint="a5d313b1b0391af3"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;The British Empire's Resilient Subsea Telegraph Network&lt;/head&gt;
    &lt;p&gt;The British empire had largely completed its Red Line cable network by 1902. This network allowed news and messages to be delivered in a few minutes or several hours at most depending on the message queue's length. It spanned the globe and formed a network ring so traffic could be routed in the opposite direction in case of disruption. It was, as Dr. Michael Delaunay has argued, a highly resilient network. Besides the ring configuration, the network relied on multiple cables between any pair of given end points to ensure uptime. The British military believed it would be impossible for an enemy to cut enough cables on any route to sever all communications between any given pair of end points. The Committee of Imperial Defense concluded that 57 cables must be shut down to isolate the British Isles from the Red Line network. The figure was 15 for Canada and 7 for South Africa. The Empire was self sufficient in terms of manufacturing the components for a subsea telegraph cable and repairing it. Its navy had no peers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://subseacables.blogspot.com/2025/12/the-british-empires-resilient-subsea.html"/><published>2025-12-30T13:10:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46433352</id><title>Igniting the GPU: From Kernel Plumbing to 3D Rendering on RISC-V</title><updated>2025-12-30T22:10:33.071316+00:00</updated><content>&lt;doc fingerprint="8fddc85941f0b928"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction: Enabling the Hardware&lt;/head&gt;
    &lt;p&gt;For years, PowerVR GPUs ubiquitous in the embedded world relied entirely on out of tree vendor drivers (often named &lt;code&gt;pvrsrvkm&lt;/code&gt;). While source code was provided in Board Support Packages, these drivers were never accepted into the mainline kernel due to their non standard architecture.&lt;/p&gt;
    &lt;p&gt;That changed when Imagination Technologies announced their commitment to an upstream, open source driver. The resulting &lt;code&gt;drm/imagination&lt;/code&gt; driver has been upstream for some time, but it wasn’t usable on RISC-V platforms like the T-HEAD TH1520 (used in the Lichee Pi 4A).&lt;/p&gt;
    &lt;p&gt;This marks a significant milestone: with the enablement work described below, the TH1520 becomes the first RISC-V SoC to feature fully mainline, hardware accelerated 3D graphics support.&lt;/p&gt;
    &lt;p&gt;This effort has followed a long road of development, generating significant community interest along the way from the initial driver support discussions to the power sequencing challenges, and finally culminating in the official upstream merge in Linux 6.18.&lt;/p&gt;
    &lt;p&gt;While the GPU driver itself is generic, the hardware surrounding the GPU on this SoC specifically the power, clock, and reset controllers required significant enablement work before the GPU could actually be probed.&lt;/p&gt;
    &lt;p&gt;This post details the architectural “plumbing” required to bring up the full graphics stack on the TH1520. This involved implementing the necessary platform drivers to handle the SoC’s power sequencing, enabling the mainline &lt;code&gt;drm/imagination&lt;/code&gt; driver for RISC-V, and validating the stack with a modern, Vulkan based userspace.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1: The Dependency Chain&lt;/head&gt;
    &lt;p&gt;Enabling the GPU wasn’t just a matter of changing a Kconfig entry. The TH1520 GPU subsystem is gated behind a chain of hardware dependencies that had no existing Linux drivers.&lt;/p&gt;
    &lt;p&gt;To reach the point where I could submit the final patch enabling the PowerVR driver for RISC-V, I first had to implement and upstream the drivers for these underlying subsystems.&lt;/p&gt;
    &lt;p&gt;The hierarchy looks like this, from the bottom up:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Mailbox (&lt;code&gt;mailbox-th1520&lt;/code&gt;): The SoC uses a safety coprocessor (E902) to manage power. The first step was writing a mailbox driver to establish a physical communication link between the main CPUs and this coprocessor.&lt;/item&gt;
      &lt;item&gt;Firmware Protocol (&lt;code&gt;thead-aon-protocol&lt;/code&gt;): On top of the mailbox, I implemented the AON (Always-On) firmware protocol. This driver handles the specific message format required to request power state changes from the coprocessor.&lt;/item&gt;
      &lt;item&gt;Power Domains (&lt;code&gt;pmdomain-thead&lt;/code&gt;): With the protocol active, I could expose the GPU’s power rail as a standard Linux Generic Power Domain (GenPD). This allows the kernel to manage the GPU’s power state generically.&lt;/item&gt;
      &lt;item&gt;Resets and Clocks: Finally, I extended the clock driver (&lt;code&gt;clk-th1520-vo&lt;/code&gt;) and implemented a new reset controller (&lt;code&gt;reset-th1520&lt;/code&gt;) to handle the specific requirements of the Video Output (VO) subsystem where the GPU resides.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The Power Sequencer: A Novel Application&lt;/head&gt;
    &lt;p&gt;With the platform drivers in place, one integration challenge remained. The TH1520 requires a specific, time sensitive sequence to power up the GPU: enable the power domain, wait for voltage stabilization, and then de-assert resets in a specific order.&lt;/p&gt;
    &lt;p&gt;Historically, power sequencing in the kernel was mostly confined to the MMC/Bluetooth subsystems (for toggling GPIOs on WiFi chips). However, the kernel recently introduced a generic Power Sequencing (&lt;code&gt;pwrseq&lt;/code&gt;) subsystem (authored by Bartosz Golaszewski) to standardize this problem.&lt;/p&gt;
    &lt;p&gt;During the upstream review process, Ulf Hansson (the Power Management subsystem maintainer) suggested that the TH1520’s GPU was the perfect candidate for this new framework. It behaves almost like an external component: it needs a dedicated “manager” to orchestrate its wake-up routine before the main driver can even touch it.&lt;/p&gt;
    &lt;p&gt;I implemented this in &lt;code&gt;pwrseq-thead-gpu&lt;/code&gt;. The most interesting part of this driver is the &lt;code&gt;match&lt;/code&gt; function, which allows the sequencer to “adopt” the GPU’s resources:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;The Integration in &lt;code&gt;drm/imagination&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;To make this work, I also had to introduce a small but strategic change to the generic &lt;code&gt;drm/imagination&lt;/code&gt; driver (see commit &lt;code&gt;e38e8391f30b&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Following a suggestion from Matt Coster, I implemented a new abstraction, &lt;code&gt;pvr_power_sequence_ops&lt;/code&gt;. This interface allows the driver to select its power strategy at runtime based on the device compatible string, keeping the core driver logic generic while accommodating platform specific needs.&lt;/p&gt;
    &lt;p&gt;For the TH1520, the driver simply selects the &lt;code&gt;pwrseq&lt;/code&gt; backend:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This architecture offers three major benefits:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clean Abstraction: The GPU driver doesn’t need to know about T-HEAD’s specific reset order or microsecond delays. It simply calls the generic &lt;code&gt;pwr_ops-&amp;gt;power_on()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Inversion of Control: The sequencer “steals” the resource handles (clocks and resets) from the GPU’s device tree node during the match phase (lines 19-24 in the first snippet). This allows the sequencer to control resources that conceptually belong to the GPU, ensuring the correct power up order without modifying the GPU driver logic.&lt;/item&gt;
      &lt;item&gt;Strict Ordering: By centralizing this logic in a dedicated driver, we guarantee that the &lt;code&gt;clkgen&lt;/code&gt;reset (controlled by the parent node) and the&lt;code&gt;gpu_core&lt;/code&gt;reset (controlled by the consumer node) are de-asserted in the exact order required by the hardware manual.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Part 2: The Display Pipeline (Connecting the Pixels)&lt;/head&gt;
    &lt;p&gt;Powering up the GPU is a massive victory, but it solves only half the problem. A GPU can render beautiful 3D scenes into memory, but without a Display Controller to scan those buffers out to a screen, you’re still looking at a black terminal.&lt;/p&gt;
    &lt;p&gt;On the TH1520, the display duties are handled by a Verisilicon DC8200 IP block, connected to a Synopsys DesignWare HDMI bridge.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Ecosystem Note: If you are following the RISC-V space, this IP might sound familiar. The StarFive JH7110 (used in the VisionFive 2) uses the exact same Verisilicon DC8200 display controller.&lt;/p&gt;
      &lt;p&gt;I am actually working on enabling the display stack for the JH7110 in parallel. While the IP is the same, the integration is vastly different the JH7110 has a complex circular dependency between the HDMI PHY and the clock generator that requires a complete architectural rethink. But that is a story for a future blog post.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;The Collaborative Puzzle&lt;/head&gt;
    &lt;p&gt;While I focused on the TH1520 power sequencing and GPU enablement, the display driver work here was led by Icenowy Zheng, another brilliant engineer in the RISC-V ecosystem.&lt;/p&gt;
    &lt;p&gt;This is the beauty of upstream kernel development: you don’t have to build the world alone. Icenowy has been working on a generic DRM driver for Verisilicon display controllers, adapting it to support the specific HDMI PHY found on the TH1520.&lt;/p&gt;
    &lt;p&gt;Since these patches are currently in the review process (v4), they aren’t in mainline yet. To build the working demo, I applied Icenowy’s patch series on top of mainline kernel.&lt;/p&gt;
    &lt;p&gt;With Icenowy’s display driver handling the “scan out” and my infrastructure handling the “power up,” we finally had a complete pipeline: Memory -&amp;gt; GPU Render -&amp;gt; Memory -&amp;gt; Display Controller -&amp;gt; HDMI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 3: The “Vulkan-Only” Future&lt;/head&gt;
    &lt;p&gt;Now that the kernel could talk to the hardware, we needed a userspace stack to render graphics.&lt;/p&gt;
    &lt;p&gt;Historically, enabling a new GPU meant writing two massive drivers for Mesa: one for Vulkan and one for OpenGL. But the open-source graphics world has shifted. The &lt;code&gt;drm/imagination&lt;/code&gt; driver is designed to be Vulkan-native.&lt;/p&gt;
    &lt;p&gt;Instead of writing a complex, legacy OpenGL driver, we use Zink.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Stack: Rendering vs. Display&lt;/head&gt;
    &lt;p&gt;Since the TH1520 uses a split DRM architecture, the flow isn’t just a straight line. The GPU and Display Controller are separate devices that share data via memory (DMA-BUF).&lt;/p&gt;
    &lt;code&gt;      [ Application (glmark2) ]
                 │
                 ▼
      [    Zink (OpenGL)      ]
                 │
                 ▼
      [ Mesa PowerVR (Vulkan) ]
                 │
      ┌──────────┴──────────┐
      │     Linux Kernel    │
      ▼                     ▼
[ GPU Driver ]       [ Display Driver ]
 (Render Node)         (KMS/Card Node)
      │                     │
      ▼        DMA-BUF      ▼
 [ GPU HW ] ──(Memory)──▶ [ Display HW ] ──▶ HDMI&lt;/code&gt;
    &lt;p&gt;This separation is why the kernel plumbing in Part 1 (GPU) and Part 2 (Display) had to be done independently before they could work together.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building the Stack (Reproduction Guide)&lt;/head&gt;
    &lt;p&gt;For those who want to reproduce this on their own Lichee Pi 4A, exact version matching is critical.&lt;/p&gt;
    &lt;p&gt;1. The Kernel I used Linux 6.19 as the base, with unmerged Display Controller patches applied on top. You can find the exact tree here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kernel Branch: &lt;code&gt;github.com/mwilczy/linux/tree/blog_code&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2. Mesa (Userspace) I used a fork of Icenowy Zheng’s work, which includes the necessary glue to make Zink play nicely with this specific hardware combination.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mesa Branch: &lt;code&gt;github.com/mwilczy/mesa&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here is the exact Meson configuration I used to build a pure Vulkan+Zink stack:&lt;/p&gt;
    &lt;code&gt;meson setup build \
    -D buildtype=release \
    -D platforms=x11,wayland \
    -D vulkan-drivers=imagination \
    -D gallium-drivers=zink \
    -D glx=disabled \
    -D gles1=disabled \
    -D gles2=enabled \
    -D egl=enabled \
    -D tools=imagination \
    -D glvnd=disabled&lt;/code&gt;
    &lt;head rend="h2"&gt;Part 4: The Result&lt;/head&gt;
    &lt;p&gt;With the kernel compiled (including the pending display patches) and the Mesa stack built, we can finally run accelerated 3D workloads.&lt;/p&gt;
    &lt;head rend="h3"&gt;The “Secret Sauce” (Environment Variables)&lt;/head&gt;
    &lt;p&gt;Because the driver is still in active development and not yet fully conformant, we need to pass a few flags to convince Mesa to run.&lt;/p&gt;
    &lt;p&gt;The most important one is &lt;code&gt;PVR_I_WANT_A_BROKEN_VULKAN_DRIVER=1&lt;/code&gt;. Without this, the driver safeguards would prevent loading. We also force the use of the Zink driver and explicitly select our device:&lt;/p&gt;
    &lt;code&gt;export PVR_I_WANT_A_BROKEN_VULKAN_DRIVER=1
export GALLIUM_DRIVER=zink
export MESA_VK_DEVICE_SELECT=1010:36104182!&lt;/code&gt;
    &lt;head rend="h3"&gt;The Benchmark&lt;/head&gt;
    &lt;p&gt;I started a Weston compositor session using the DRM backend:&lt;/p&gt;
    &lt;code&gt;weston --backend=drm-backend.so --continue-without-input &amp;amp;&lt;/code&gt;
    &lt;p&gt;And then, the moment of truth - running &lt;code&gt;glmark2-es2-wayland&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Above: glmark2 running on the Lichee Pi 4A.&lt;/p&gt;
    &lt;p&gt;Here is the output, confirming we are running fully accelerated on the PowerVR GPU via Zink:&lt;/p&gt;
    &lt;code&gt;root@revyos-lpi4a:~/test_mesa/Vulkan/build4/bin# glmark2-es2-wayland
MESA: warning: Core count fetching is unimplemented. Setting 1 for now.
WARNING: powervr is not a conformant Vulkan implementation, testing use only.
=======================================================
    glmark2 2023.01
=======================================================
    OpenGL Information
    GL_VENDOR:      Mesa
    GL_RENDERER:    zink Vulkan 1.2(PowerVR B-Series BXM-4-64 MC1 (IMAGINATION_OPEN_SOURCE_MESA))
    GL_VERSION:     OpenGL ES 2.0 Mesa 26.0.0-devel (git-601d20e81e)
    Surface Config: buf=32 r=8 g=8 b=8 a=8 depth=24 stencil=0 samples=0
    Surface Size:   800x600 windowed
=======================================================
[build] use-vbo=false:[  510.861554] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 524288 bytes), total 32768 (slots), used 4 (slots)
[  510.887018] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 1847296 bytes), total 32768 (slots), used 36 (slots)
[  510.900771] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 1847296 bytes), total 32768 (slots), used 36 (slots)
[  510.923956] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 1921024 bytes), total 32768 (slots), used 0 (slots)
[  510.954656] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 458752 bytes), total 32768 (slots), used 0 (slots)
[  510.966931] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 458752 bytes), total 32768 (slots), used 0 (slots)
[  511.038586] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 1921024 bytes), total 32768 (slots), used 0 (slots)
[  512.165193] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 1900544 bytes), total 32768 (slots), used 10 (slots)
[  512.187871] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 1900544 bytes), total 32768 (slots), used 10 (slots)
[  512.209524] verisilicon-dc ffef600000.display: swiotlb buffer is full (sz: 745472 bytes), total 32768 (slots), used 0 (slots)
 FPS: 67 FrameTime: 14.960 ms
[build] use-vbo=true: FPS: 98 FrameTime: 10.252 ms
[texture] texture-filter=nearest: FPS: 97 FrameTime: 10.332 ms
[texture] texture-filter=linear: FPS: 93 FrameTime: 10.868 ms
[texture] texture-filter=mipmap: FPS: 101 FrameTime: 9.957 ms
[shading] shading=gouraud: FPS: 93 FrameTime: 10.851 ms
[shading] shading=blinn-phong-inf: FPS: 98 FrameTime: 10.274 ms
[shading] shading=phong: FPS: 97 FrameTime: 10.356 ms
[shading] shading=cel: FPS: 91 FrameTime: 11.086 ms
[bump] bump-render=high-poly: FPS: 75 FrameTime: 13.404 ms
[bump] bump-render=normals: FPS: 97 FrameTime: 10.356 ms
[bump] bump-render=height: FPS: 88 FrameTime: 11.449 ms
[effect2d] kernel=0,1,0;1,-4,1;0,1,0;: FPS: 94 FrameTime: 10.646 ms
[effect2d] kernel=1,1,1,1,1;1,1,1,1,1;1,1,1,1,1;: FPS: 57 FrameTime: 17.590 ms
[pulsar] light=false:quads=5:texture=false: FPS: 92 FrameTime: 10.953 ms
[desktop] blur-radius=5:effect=blur:passes=1:separable=true:windows=4: FPS: 10 FrameTime: 105.957 ms
[desktop] effect=shadow:windows=4: FPS: 37 FrameTime: 27.465 ms

...&lt;/code&gt;
    &lt;p&gt;We have successfully turned “dark silicon” into a modern, Vulkan capable graphics platform.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Bringing a new GPU architecture to life in the mainline kernel is never a solo effort. It requires navigating complex subsystems - from power domains to clocks and relies heavily on the patience and expertise of subsystem maintainers.&lt;/p&gt;
    &lt;p&gt;This work went through many iterations, and the code is significantly better thanks to the rigorous feedback from the community.&lt;/p&gt;
    &lt;p&gt;A huge thank you to everyone who helped review the code, suggested architectural improvements, and tested the stack:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Marek Szyprowski - For the guidance and mentorship throughout the upstreaming process.&lt;/item&gt;
      &lt;item&gt;Drew Fustini - For his long standing work maintaining the TH1520 platform.&lt;/item&gt;
      &lt;item&gt;Krzysztof Kozłowski - For ensuring the Device Tree bindings were strictly compliant.&lt;/item&gt;
      &lt;item&gt;Ulf Hansson - For his guidance on the AON power domains and for suggesting the use of the Power Sequencing framework, which simplified the architecture significantly.&lt;/item&gt;
      &lt;item&gt;Bartosz Gołaszewski - For creating the Power Sequencing subsystem and helping merge the TH1520 driver.&lt;/item&gt;
      &lt;item&gt;Matt Coster - For reviewing the driver changes and helping navigate the PowerVR internals.&lt;/item&gt;
      &lt;item&gt;Stephen Boyd - For the feedback on the video output clock controller.&lt;/item&gt;
      &lt;item&gt;Philipp Zabel - For reviewing the reset controller implementation.&lt;/item&gt;
      &lt;item&gt;Icenowy Zheng - For the incredible work on the display controller and Mesa/Zink integration.&lt;/item&gt;
      &lt;item&gt;Jassi Brar - For reviewing the mailbox driver implementation.&lt;/item&gt;
      &lt;item&gt;Conor Dooley - For reviewing Device Tree patches.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mwilczynski.dev/posts/riscv-gpu-zink/"/><published>2025-12-30T13:55:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46433661</id><title>Hive (YC S14) Is Hiring a Staff Software Engineer (Data Systems)</title><updated>2025-12-30T22:10:32.909154+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jobs.ashbyhq.com/hive.co/cb0dc490-0e32-4734-8d91-8b56a31ed497"/><published>2025-12-30T14:31:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46434580</id><title>Show HN: Tidy Baby is a SET game but with words</title><updated>2025-12-30T22:10:32.502711+00:00</updated><content>&lt;doc fingerprint="1e8bfdcd154f754e"&gt;
  &lt;main&gt;
    &lt;p&gt;If you know how to play SET you basically know how to play Tidy Baby — the "dimensions" are just:&lt;/p&gt;
    &lt;p&gt;If you don't know how to play SET, we recommend checking out the How To Play page.&lt;/p&gt;
    &lt;p&gt;Make sets. Clean board.&lt;/p&gt;
    &lt;p&gt;No sets guessed yet...&lt;/p&gt;
    &lt;p&gt;Final Score:&lt;/p&gt;
    &lt;p&gt;Total Time:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tidy.baby"/><published>2025-12-30T15:57:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46435308</id><title>Show HN: 22 GB of Hacker News in SQLite</title><updated>2025-12-30T22:10:32.092131+00:00</updated><content>&lt;doc fingerprint="60e736960d5d7ecf"&gt;
  &lt;main&gt;
    &lt;p&gt;Hacker Book new | front | start | ask | show | jobs | query Someday, Month 00, 0000 &amp;lt; &amp;gt; ARCHIVE Loading… Y Combinator | Apply | Companies | Blog | Live HN | Contact &amp;lt; &amp;gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://hackerbook.dosaygo.com"/><published>2025-12-30T17:01:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46435418</id><title>Toro: Deploy Applications as Unikernels</title><updated>2025-12-30T22:10:31.892720+00:00</updated><content>&lt;doc fingerprint="3069f0a7771d385"&gt;
  &lt;main&gt;
    &lt;p&gt;Toro is a unikernel dedicated to deploy applications as microVMs. Toro leverages on virtio-fs and virtio-vsocket to provide a minimalistic architecture.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support x86-64 architecture&lt;/item&gt;
      &lt;item&gt;Support up to 512GB of RAM&lt;/item&gt;
      &lt;item&gt;Support QEMU-KVM microvm and Firecracker&lt;/item&gt;
      &lt;item&gt;Cooperative and I/O bound threading scheduler&lt;/item&gt;
      &lt;item&gt;Support virtio-vsocket for networking&lt;/item&gt;
      &lt;item&gt;Support virtio-fs for filesystem&lt;/item&gt;
      &lt;item&gt;Fast boot up&lt;/item&gt;
      &lt;item&gt;Tiny image&lt;/item&gt;
      &lt;item&gt;Built-in gdbstub&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can try Toro by running the HelloWorld example using a Docker image that includes all the required tools. To do so, execute the following commands in a console (these steps require you to install before KVM and Docker):&lt;/p&gt;
    &lt;code&gt;wget https://raw.githubusercontent.com/torokernel/torokernel/master/ci/Dockerfile
sudo docker build -t torokernel-dev .
sudo docker run --privileged --rm -it torokernel-dev
cd examples/HelloWorld
python3 ../CloudIt.py -a HelloWorld&lt;/code&gt;
    &lt;p&gt;If these commands execute successfully, you will get the output of the HelloWorld example. You can also pull the image from dockerhub instead of building it:&lt;/p&gt;
    &lt;code&gt;sudo docker pull torokernel/torokernel-dev:latest
sudo docker run --privileged --rm -it torokernel/torokernel-dev:latest&lt;/code&gt;
    &lt;p&gt;You can share a directory from the host by running:&lt;/p&gt;
    &lt;code&gt;sudo docker run --privileged --rm --mount type=bind,source="$(pwd)",target=/root/torokernel -it torokernel/torokernel-dev:latest&lt;/code&gt;
    &lt;p&gt;You will find $pwd from host at &lt;code&gt;/root/torokernel&lt;/code&gt; in the container.&lt;/p&gt;
    &lt;p&gt;Execute the commands in &lt;code&gt;ci/Dockerfile&lt;/code&gt; to install the required components locally. Then, Go to &lt;code&gt;torokernel/examples&lt;/code&gt; and edit &lt;code&gt;CloudIt.py&lt;/code&gt; to set the correct paths to Qemu and fpc. Optionally, you can install vsock-socat from here and virtio-fs from here. You need to set the correct path to virtiofsd and socat.&lt;/p&gt;
    &lt;p&gt;Go to &lt;code&gt;examples/HelloWorld/&lt;/code&gt; and execute:&lt;/p&gt;
    &lt;code&gt;python3 ../CloudIt.py -a HelloWorld&lt;/code&gt;
    &lt;p&gt;To run the StaticWebserver, you require virtiofsd and socat. To compile socat, execute the following commands:&lt;/p&gt;
    &lt;code&gt;git clone git@github.com:stefano-garzarella/socat-vsock.git
cd socat-vsock
autoreconf -fiv
./configure
make socat&lt;/code&gt;
    &lt;p&gt;Set the path to socat binary in CloudIt.py and then execute:&lt;/p&gt;
    &lt;code&gt;python3 ../CloudIt.py -a StaticWebServer -r -d /path-to-directory/ -f 4000:80&lt;/code&gt;
    &lt;p&gt;You have to replace the &lt;code&gt;/path-to-directory/&lt;/code&gt; to a directory that containing the files, e.g., index.html. To try it, you can execute:&lt;/p&gt;
    &lt;code&gt;wget http://127.0.0.1:4000/index.html
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;-f&lt;/code&gt; parameter indicates a forwarding of the 4000 port from the host to the 80 port in the guest using vsock.&lt;/p&gt;
    &lt;p&gt;This example shows how cores can communicate by using the VirtIOBus device. In this example, core #0 sends a packet to every core in the system with the ping string. Each core responds with a packet that contains the message pong. This example is configured to use three cores. To launch it, simply executes the following commands in the context of the container presented above:&lt;/p&gt;
    &lt;code&gt;python3 ../CloudIt.py -a InterCoreComm&lt;/code&gt;
    &lt;p&gt;You will get the following output:&lt;/p&gt;
    &lt;p&gt;You have many ways to contribute to Toro. One of them is by joining the Google Group here. In addition, you can find more information here.&lt;/p&gt;
    &lt;p&gt;GPLv3&lt;/p&gt;
    &lt;p&gt;[0] A Dedicated Kernel named Toro. Matias Vara. FOSDEM 2015.&lt;/p&gt;
    &lt;p&gt;[1] Reducing CPU usage of a Toro Appliance. Matias Vara. FOSDEM 2018.&lt;/p&gt;
    &lt;p&gt;[2] Toro, a Dedicated Kernel for Microservices. Matias Vara and Cesar Bernardini. Open Source Summit Europe 2018.&lt;/p&gt;
    &lt;p&gt;[3] Speeding Up the Booting Time of a Toro Appliance. Matias Vara. FOSDEM 2019.&lt;/p&gt;
    &lt;p&gt;[4] Developing and Deploying Microservices with Toro Unikernel. Matias Vara. Open Source Summit Europe 2019.&lt;/p&gt;
    &lt;p&gt;[5] Leveraging Virtio-fs and Virtio-vsocket in Toro Unikernel. Matias Vara. DevConfCZ 2020.&lt;/p&gt;
    &lt;p&gt;[6] Building a Cloud Infrastructure to Deploy Microservices as Microvm Guests. Matias Vara. KVM Forum 2020.&lt;/p&gt;
    &lt;p&gt;[7] Running MPI applications on Toro unikernel. Matias Vara. FOSDEM 2023.&lt;/p&gt;
    &lt;p&gt;[8] Is Toro unikernel faster for MPI?. Matias Vara. FOSDEM 2024.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/torokernel/torokernel"/><published>2025-12-30T17:09:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46435614</id><title>A Vulnerability in Libsodium</title><updated>2025-12-30T22:10:30.942639+00:00</updated><content>&lt;doc fingerprint="1c17b053a8636291"&gt;
  &lt;main&gt;
    &lt;p&gt;Libsodium is now 13 years old!&lt;/p&gt;
    &lt;p&gt;I started that project to pursue Dan Bernsteinâs desire to make cryptography simple to use. That meant exposing a limited set of high-level functions and parameters, providing a simple API, and writing documentation for users, not cryptographers. Libsodiumâs goal was to expose APIs to perform operations, not low-level functions. Users shouldnât even have to know or care about what algorithms are used internally. This is how Iâve always viewed libsodium.&lt;/p&gt;
    &lt;p&gt;Never breaking the APIs is also something Iâm obsessed with. APIs may not be great, and if I could start over from scratch, I would have made them very different, but as a developer, the best APIs are not the most beautifully designed ones, but the ones that you donât have to worry about because they donât change and upgrades donât require any changes in your application either. Libsodium started from the NaCl API, and still adheres to it.&lt;/p&gt;
    &lt;p&gt;These APIs exposed high-level functions, but also some lower-level functions that high-level functions wrap or depend on. Over the years, people started using these low-level functions directly. Libsodium started to be used as a toolkit of algorithms and low-level primitives.&lt;/p&gt;
    &lt;p&gt;That made me sad, especially since it is clearly documented that only APIs from builds with &lt;code&gt;--enable-minimal&lt;/code&gt; are guaranteed to be tested and stable. But after all, it makes sense. When building custom protocols, having a single portable library with a consistent interface for different functions is far better than importing multiple dependencies, each with their own APIs and sometimes incompatibilities between them.&lt;/p&gt;
    &lt;p&gt;Thatâs a lot of code to maintain. It includes features and target platforms I donât use but try to support for the community. I also maintain a large number of other open source projects.&lt;/p&gt;
    &lt;p&gt;Still, the security track record of libsodium is pretty good, with zero CVEs in 13 years even though it has gotten a lot of scrutiny.&lt;/p&gt;
    &lt;p&gt;However, while recently experimenting with adding support for batch signatures, I noticed inconsistent results with code originally written in Zig. The culprit was a check that was present in a function in Zig, but that I forgot to add in libsodium.&lt;/p&gt;
    &lt;head rend="h2"&gt;The bug&lt;/head&gt;
    &lt;p&gt;The function &lt;code&gt;crypto_core_ed25519_is_valid_point()&lt;/code&gt;, a low-level function used to check if a given elliptic curve point is valid, was supposed to reject points that arenât in the main cryptographic group, but some points were slipping through.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this matter?&lt;/head&gt;
    &lt;p&gt;Edwards25519 is like a special mathematical playground where cryptographic operations happen.&lt;/p&gt;
    &lt;p&gt;It is used internally for Ed25519 signatures, and includes multiple subgroups of different sizes (order):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Order 1: just the identity (0, 1)&lt;/item&gt;
      &lt;item&gt;Order 2: identity + point (0, -1)&lt;/item&gt;
      &lt;item&gt;Order 4: 4 points&lt;/item&gt;
      &lt;item&gt;Order 8: 8 points&lt;/item&gt;
      &lt;item&gt;Order L: the âmain subgroupâ (L = ~2^252 points) where all operations are expected to happen&lt;/item&gt;
      &lt;item&gt;Order 2L, 4L, 8L: very large, but not prime order subgroups&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The validation function was designed to reject points not in the main subgroup. It properly rejected points in the small-order subgroups, but not points in the mixed-order subgroups.&lt;/p&gt;
    &lt;head rend="h2"&gt;What went wrong technically?&lt;/head&gt;
    &lt;p&gt;To check if a point is in the main subgroup (the one of order L), the function multiplies it by L. If the order is L, multiplying any point by L gives the identity point (the mathematical equivalent of zero). So, the code does the multiplication and checks that we ended up with the identity point.&lt;/p&gt;
    &lt;p&gt;Points are represented by coordinates. In the internal representation used here, there are three coordinates: X, Y, and Z. The identity point is represented internally with coordinates where X = 0 and Y = Z. Z can be anything depending on previous operations; it doesnât have to be 1.&lt;/p&gt;
    &lt;p&gt;The old code only checked X = 0. It forgot to verify Y = Z. This meant some invalid points (where X = 0 but Y â Z after the multiplication) were incorrectly accepted as valid.&lt;/p&gt;
    &lt;p&gt;Concretely: take any main-subgroup point Q (for example, the output of &lt;code&gt;crypto_core_ed25519_random&lt;/code&gt;) and add the order-2 point (0, -1), or equivalently negate both coordinates. Every such Q + (0, -1) would have passed validation before the fix, even though itâs not in the main subgroup.&lt;/p&gt;
    &lt;head rend="h2"&gt;The fix&lt;/head&gt;
    &lt;p&gt;The fix is trivial and adds the missing check:&lt;/p&gt;
    &lt;code&gt;// OLD:
return fe25519_iszero(pl.X);
&lt;/code&gt;
    &lt;code&gt;// NEW:
fe25519_sub(t, pl.Y, pl.Z);
return fe25519_iszero(pl.X) &amp;amp; fe25519_iszero(t);
&lt;/code&gt;
    &lt;p&gt;Now it properly verifies both conditions: X must be zero and Y must equal Z.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who is affected?&lt;/head&gt;
    &lt;p&gt;You may be affected if you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use a point release &amp;lt;= &lt;code&gt;1.0.20&lt;/code&gt;or a version of&lt;code&gt;libsodium&lt;/code&gt;released before December 30, 2025.&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;crypto_core_ed25519_is_valid_point()&lt;/code&gt;to validate points from untrusted sources&lt;/item&gt;
      &lt;item&gt;Implement custom cryptography using arithmetic over the Edwards25519 curve&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But donât panic. Most users are not affected.&lt;/p&gt;
    &lt;p&gt;None of the high-level APIs (&lt;code&gt;crypto_sign_*&lt;/code&gt;) are affected; they donât even use or need that function. Scalar multiplication using &lt;code&gt;crypto_scalarmult_ed25519&lt;/code&gt; wonât leak anything even if the public key is not on the main subgroup. And public keys created with the regular &lt;code&gt;crypto_sign_keypair&lt;/code&gt; and &lt;code&gt;crypto_sign_seed_keypair&lt;/code&gt; functions are guaranteed to be on the correct subgroup.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recommendation&lt;/head&gt;
    &lt;p&gt;Support for the Ristretto255 group was added to libsodium in 2019 specifically to solve cofactor-related issues. With Ristretto255, if a point decodes, itâs safe. No further validation is required.&lt;/p&gt;
    &lt;p&gt;If you implement custom cryptographic schemes doing arithmetic over a finite field group, using Ristretto255 is recommended. Itâs easier to use, and as a bonus, low-level operations will run faster than over Edwards25519.&lt;/p&gt;
    &lt;p&gt;If you canât update libsodium and need an application-level workaround, use the following function:&lt;/p&gt;
    &lt;code&gt;int is_on_main_subgroup(const unsigned char p[crypto_core_ed25519_BYTES])
{
    /* l - 1 (group order minus 1) */
    static const unsigned char L_1[crypto_core_ed25519_SCALARBYTES] = {
        0xec, 0xd3, 0xf5, 0x5c, 0x1a, 0x63, 0x12, 0x58,
        0xd6, 0x9c, 0xf7, 0xa2, 0xde, 0xf9, 0xde, 0x14,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x10
    };
    /* Identity point encoding: (x=0, y=1) */
    static const unsigned char ID[crypto_core_ed25519_BYTES] = {
        0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
    };
    unsigned char t[crypto_core_ed25519_BYTES];
    unsigned char r[crypto_core_ed25519_BYTES];
    if (crypto_scalarmult_ed25519_noclamp(t, L_1, p) != 0 ||
        crypto_core_ed25519_add(r, t, p) != 0) {
        return 0;
    }
    return sodium_memcmp(r, ID, sizeof ID) == 0;
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Fixed packages&lt;/head&gt;
    &lt;p&gt;This issue was fixed immediately after discovery. All &lt;code&gt;stable&lt;/code&gt; packages released after December 30, 2025 include the fix:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;official tarballs&lt;/item&gt;
      &lt;item&gt;binaries for Visual Studio&lt;/item&gt;
      &lt;item&gt;binaries for MingW&lt;/item&gt;
      &lt;item&gt;NuGet packages for all architectures including Android&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;swift-sodium&lt;/code&gt;xcframework (but&lt;code&gt;swift-sodium&lt;/code&gt;doesnât expose low-level functions anyway)&lt;/item&gt;
      &lt;item&gt;rust &lt;code&gt;libsodium-sys-stable&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;libsodium.js&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A new point release is also going to be tagged.&lt;/p&gt;
    &lt;p&gt;If &lt;code&gt;libsodium&lt;/code&gt; is useful to you, please keep in mind that it is maintained by one person, for free, in time I could spend with my family or on other projects. The best way to help the project would be to consider sponsoring it, which helps me dedicate more time to improving it and making it great for everyone, for many more years to come.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://00f.net/2025/12/30/libsodium-vulnerability/"/><published>2025-12-30T17:24:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46436127</id><title>Electrolysis can solve one of our biggest contamination problems</title><updated>2025-12-30T22:10:30.278623+00:00</updated><content>&lt;doc fingerprint="cb108501eca1c785"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Electrolysis can solve one of our biggest contamination problems&lt;/head&gt;
    &lt;p&gt;ETH Zurich researchers have developed a process that can be used on site to render environmental toxins such as DDT and lindane harmless and convert them into valuable chemicals – a breakthrough for the remediation of contaminated sites and a sustainable circular economy.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read&lt;/item&gt;
      &lt;item&gt;Number of comments&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;In brief&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Persistent organic pollutants such as DDT and lindane still pollute the environment and affect humans decades after their use.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ETH researchers have developed a new electrochemical process that completely dehalogenates these long-lived toxins and converts them into valuable industrial chemicals.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The method uses cheap equipment, prevents side reactions and could be used on contaminated landfills, soils or sludge.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mobile systems could be used on site in the future – an important step towards the remediation of contaminated sites and the creation of a sustainable circular economy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They were once considered miracle workers – insecticides such as lindane or DDT were produced and used millions of times during the 20th century. But what was hailed as progress led to a global environmental catastrophe: persistent organic pollutants (POPs) are so chemically stable that they remain in soil, water and organisms for decades. They accumulate in the fatty tissue of animals and thus enter the human food chain. Many of these substances were banned long ago, but their traces can still be found today – even in human blood.&lt;/p&gt;
    &lt;p&gt;How to remediate such contaminated sites, be they soils, bodies of water or landfills, is one of the major unresolved questions of environmental protection. How can highly stable poisons be rendered harmless without creating new problems? Researchers at ETH Zurich, led by Bill Morandi, Professor of Synthetic Organic Chemistry, have now found a promising approach. Using an innovative electrochemical method, they are not only able to break down these long-lived pollutants but also to convert them into valuable raw materials for the chemical industry.&lt;/p&gt;
    &lt;head rend="h2"&gt;Converting pollutants into raw materials&lt;/head&gt;
    &lt;p&gt;A key distinction between this and previous work is that the carbon skeleton of the pollutants is recycled and made reusable, while the halide component is sequestered as a harmless inorganic salt. “The previous methods were also energetically inefficient,” says Patrick Domke, a doctoral student in Morandi’s group. He explains: “The processes were expensive and still led to outcomes that were harmful to the environment.”&lt;/p&gt;
    &lt;p&gt;Together with electrochemistry specialist Alberto Garrido-Castro, a former postdoc in this group, Domke developed a process that renders the pollutants in question completely harmless. During this project, the two researchers were able to draw on the many years of experience of ETH professor Morandi, who has been working on the transformation of such compounds for years. “The key advance of this new technology is the use of alternating current to sequester the problematic halogen atoms as innocuous salts such as NaCl (table salt), while still generating valuable hydrocarbons,” says Morandi.&lt;/p&gt;
    &lt;head rend="h2"&gt;Using electricity to break down toxins&lt;/head&gt;
    &lt;p&gt;Electrolysis enables almost complete dehalogenation of pollutants under mild, environmentally friendly and cost-effective conditions. It cleaves the stable carbon-halogen bonds, leaving behind only harmless salts such as table salt and useful hydrocarbons such as benzene, diphenylethane or cyclododecatriene. These are actually sought-after intermediates in the chemical industry, for example, for plastics, varnishes, coatings and pharmaceutical applications. In this way, the technology not only contributes to the remediation of contaminated sites but also to the sustainable circular economy.&lt;/p&gt;
    &lt;p&gt;“What makes our process so special from a technical point of view is that we supply electricity using alternating current, similar to the electrical waveform delivered to households. It is one of the most cost-effective resources in chemistry,” explains Garrido-Castro. “Alternating current protects the electrodes from wear, which is why we can reuse them for many subsequent electrolysis cycles. In addition, the alternating current suppresses unwanted side reactions and the formation of poisonous chlorine gas, allowing the pollutant’s halogen atoms to be fully converted to inorganic salts.” The reactor used by the researchers consists of an undivided electrolysis cell in which dimethyl sulfoxide (DMSO) is used as a solvent – itself a by-product of the pulp process in paper production.&lt;/p&gt;
    &lt;head rend="h2"&gt;A fully thought-out circular economy&lt;/head&gt;
    &lt;p&gt;The process can be applied not only to pure substances but also to mixtures from contaminated soils. Soil or sludge can therefore be treated without pre-treatment or further separation processes. A prototype of the reactor has already been successfully tested on classic environmental toxins such as lindane and DDT. “Our system is mobile and can be assembled on site. This eliminates the need to transport these hazardous substances,” explains Domke.&lt;/p&gt;
    &lt;quote&gt;“Our motivation was to solve one of the biggest environmental problems of the last century. We cannot simply leave the pollution to future generations.”Alberto Garrido-Castro&lt;/quote&gt;
    &lt;head rend="h2"&gt;Spark Award 2025 – these projects have made it to the finals&lt;/head&gt;
    &lt;p&gt;On 27 November 2025 at ETH Zurich @ Open-i, ETH Zurich will award the Spark Award for the best invention of the year for the 14th time. The criteria for this award are originality, patent strength and market potential.&lt;/p&gt;
    &lt;p&gt;Click here to find all the Spark Award nominees of 2025.&lt;/p&gt;
    &lt;p&gt;Spark Award ceremony, Industry Day @ Open-i, Thursday, 27 November 2025, 1.30 p.m., Zurich Convention Center. Registration is required.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ethz.ch/en/news-and-events/eth-news/news/2025/11/electrolysis-can-solve-one-of-our-biggest-contamination-problems.html"/><published>2025-12-30T18:08:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46436128</id><title>Show HN: I remade my website in the Sith Lord Theme and I hope it's fun</title><updated>2025-12-30T22:10:29.908247+00:00</updated><content>&lt;doc fingerprint="b9ae31118bfb75fd"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Latest Modified Projects&lt;/head&gt;
    &lt;head rend="h4"&gt;RogueBerry One&lt;/head&gt;
    &lt;p&gt; RogueBerry One - KDE Plasma Mobile based OS for the Hackberry Pi &lt;lb/&gt; Read More | GitHub &lt;/p&gt;
    &lt;head rend="h4"&gt;Forensics Tools&lt;/head&gt;
    &lt;p&gt; Forensics Tools - Cookie's Forensics Tools &lt;lb/&gt; Read More | GitHub &lt;/p&gt;
    &lt;head rend="h4"&gt;Gooey Framework&lt;/head&gt;
    &lt;p&gt; Gooey - Opinionated WebASM Bindings and Web Components Framework &lt;lb/&gt; Read More | GitHub &lt;/p&gt;
    &lt;head rend="h4"&gt;ZIMdex&lt;/head&gt;
    &lt;p&gt; ZIMdex - Self-hostable Offline ZIM search and web spider &lt;lb/&gt; Read More | GitHub &lt;/p&gt;
    &lt;head rend="h4"&gt;Gooey CLI&lt;/head&gt;
    &lt;p&gt; Gooey CLI - CLI Wizard for easier Gooey project creation and management &lt;lb/&gt; Read More | GitHub &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cookie.engineer/index.html"/><published>2025-12-30T18:08:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46436409</id><title>A faster heart for F-Droid. Our new server is here</title><updated>2025-12-30T22:10:29.241867+00:00</updated><content>&lt;doc fingerprint="fdf7b599e0066ed7"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;A faster heart for F-Droid. Our new server is here!&lt;/head&gt;Posted on Dec 30, 2025 by F-Droid&lt;p&gt;Donations are a key part of what keeps F-Droid independent and reliable and our latest hardware update is a direct result of your support. Thanks to donations from our incredible community, F-Droid has replaced one of its most critical pieces of infrastructure, our core server hardware. It was overdue for a refresh, and now we are happy to give you an update on the new server and how it impacts the project.&lt;/p&gt;&lt;p&gt;This upgrade touches a core part of the infrastructure that builds and publishes apps for the main F-Droid repository. If the server is slow, everything downstream gets slower too. If it is healthy, the entire ecosystem benefits.&lt;/p&gt;&lt;head rend="h2"&gt;Why did we wait?&lt;/head&gt;&lt;p&gt;This server replacement took a bit longer than we would have liked. The biggest reason is that sourcing reliable parts right now is genuinely hard. Ongoing global trade tensions have made supply chains unpredictable, and that hit the specific components we needed. We had to wait for quotes, review, replan, and wait again when quotes turned out to have unexpected long waits, before we finally managed to receive hardware that met our requirements.&lt;/p&gt;&lt;p&gt;Even with the delays, the priority never changed. We were looking for the right server set up for F-Droid, built to last for the long haul.&lt;/p&gt;&lt;head rend="h2"&gt;A note about the host&lt;/head&gt;&lt;p&gt;Another important part of this story is where the server lives and how it is managed. F-Droid is not hosted in just any data center where commodity hardware is managed by some unknown staff. We worked out a special arrangement so that this server is physically held by a long time contributor with a proven track record of securely hosting services. We can control it remotely, we know exactly where it is, and we know who has access. That level of transparency and trust is not common in infrastructure, but it is central to how we think about resilience and stewardship.&lt;/p&gt;&lt;p&gt;This was not the easiest path, and it required careful coordination and negotiation. But we are glad we did it this way. It fits our values and our threat model, and it keeps the project grounded in real people rather than anonymous systems.&lt;/p&gt;&lt;head rend="h2"&gt;Old hardware, new momentum&lt;/head&gt;&lt;p&gt;The previous server was 12 year old hardware and had been running for about five years. In infrastructure terms, that is a lifetime. It served F-Droid well, but it was reaching the point where speed and maintenance overhead were becoming a daily burden.&lt;/p&gt;&lt;p&gt;The new system is already showing a huge improvement. Stats of the running cycles from the last two months suggest it can handle the full build and publish actions much faster than before. E.g. this year, between January and September, we published updates once every 3 or 4 days, that got down to once every 2 days in October, to every day in November and itâs reaching twice a day in December. (You can see this in the frequency of index publishing after October 18, 2025 in our f-droid.org transparency log). That extra capacity gives us more breathing room and helps shorten the gap between when apps are updated and when those updates reach users. We can now build all the auto-updated apps in the (UTC) morning in one cycle, and all the newly included apps, fixed apps and manually updated apps, through the day, in the evening cycle.&lt;/p&gt;&lt;p&gt;We are being careful here, because real world infrastructure always comes with surprises. But the performance gains are real, and they are exciting.&lt;/p&gt;&lt;head rend="h2"&gt;What donations make possible&lt;/head&gt;&lt;p&gt;This upgrade exists because of community support, pooled over time, turned into real infrastructure, benefiting everyone who relies on F-Droid.&lt;/p&gt;&lt;p&gt;A faster server does not just make our lives easier. It helps developers get timely builds. It reduces maintenance risk. It strengthens the health of the entire repository.&lt;/p&gt;&lt;p&gt;So thank you. Every donation, whether large or small, is part of how this project stays reliable, independent, and aligned with free software values.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://f-droid.org/2025/12/30/a-faster-heart-for-f-droid.html"/><published>2025-12-30T18:36:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46436828</id><title>Escaping Containment: A Security Analysis of FreeBSD Jails [video]</title><updated>2025-12-30T22:10:27.994141+00:00</updated><content>&lt;doc fingerprint="1512f8d2f0f5cfeb"&gt;
  &lt;main&gt;
    &lt;p&gt;ilja and Michael Smith&lt;/p&gt;
    &lt;p&gt;FreeBSD’s jail mechanism promises strong isolation—but how strong is it really? &lt;lb/&gt;In this talk, we explore what it takes to escape a compromised FreeBSD jail by auditing the kernel’s attack surface, identifying dozens of vulnerabilities across exposed subsystems, and developing practical proof-of-concept exploits. We’ll share our findings, demo some real escapes, and discuss what they reveal about the challenges of maintaining robust OS isolation.&lt;/p&gt;
    &lt;p&gt;FreeBSD’s jail feature is one of the oldest and most mature OS-level isolation mechanisms in use today, powering hosting environments, container frameworks, and security sandboxes. But as with any large and evolving kernel feature, complexity breeds opportunity. This research asks a simple but critical question: If an attacker compromises root inside a FreeBSD jail, what does it take to break out?&lt;/p&gt;
    &lt;p&gt;To answer that, we conducted a large-scale audit of FreeBSD kernel code paths accessible from within a jail. We systematically examined privileged operations, capabilities, and interfaces that a jailed process can still reach, hunting for memory safety issues, race conditions, and logic flaws. The result: roughly 50 distinct issues uncovered across multiple kernel subsystems, ranging from buffer overflows and information leaks to unbounded allocations and reference counting errors—many of which could crash the system or provide vectors for privilege escalation beyond the jail.&lt;/p&gt;
    &lt;p&gt;We’ve developed proof-of-concept exploits and tools to demonstrate some of these vulnerabilities in action. We’ve responsibly disclosed our findings to the FreeBSD security team and are collaborating with them on fixes. Our goal isn’t to break FreeBSD, but to highlight the systemic difficulty of maintaining strict isolation in a large, mature codebase.&lt;/p&gt;
    &lt;p&gt;This talk will present our methodology, tooling, and selected demos of real jail escapes. We’ll close with observations about kernel isolation boundaries, lessons learned for other OS container systems, and a call to action for hardening FreeBSD’s jail subsystem against the next generation of threats.&lt;/p&gt;
    &lt;p&gt;Licensed to the public under http://creativecommons.org/licenses/by/4.0&lt;/p&gt;
    &lt;head rend="h3"&gt;Download&lt;/head&gt;
    &lt;head rend="h4"&gt;Video&lt;/head&gt;
    &lt;head rend="h4"&gt;These files contain multiple languages.&lt;/head&gt;
    &lt;p&gt;This Talk was translated into multiple languages. The files available for download contain all languages as separate audio-tracks. Most desktop video players allow you to choose between them.&lt;/p&gt;
    &lt;p&gt;Please look for "audio tracks" in your desktop video player.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://media.ccc.de/v/39c3-escaping-containment-a-security-analysis-of-freebsd-jails"/><published>2025-12-30T19:15:05+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46436889</id><title>FediMeteo: A €4 FreeBSD VPS Became a Global Weather Service</title><updated>2025-12-30T22:10:27.320840+00:00</updated><content>&lt;doc fingerprint="b20d9871206db6ab"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Personal Introduction&lt;/head&gt;
    &lt;p&gt;Weather has always significantly influenced my life. When I was a young athlete, knowing the forecast in advance would have allowed me to better plan my training sessions. As I grew older, I could choose whether to go to school on my motorcycle or, for safety reasons, have my grandfather drive me. And it was him, my grandfather, who was my go-to meteorologist. He followed all weather patterns and forecasts, a remnant of his childhood in the countryside and his life on the move. It's to him that I dedicate FediMeteo.&lt;/p&gt;
    &lt;p&gt;The idea for FediMeteo started almost by chance while I was checking the holiday weather forecast to plan an outing. Suddenly, I thought how nice it would be to receive regular weather updates for my city directly in my timeline. After reflecting for a few minutes, I registered a domain and started planning.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design Principles&lt;/head&gt;
    &lt;p&gt;The choice of operating system was almost automatic. The idea was to separate instances by country, and FreeBSD jails are one of the most useful tools for this purpose.&lt;/p&gt;
    &lt;p&gt;I initially thought the project would generate little interest. I was wrong. After all, weather affects many of our lives, directly or indirectly. So I decided to structure everything in this way:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I would use a test VPS to see how things would go. The VPS was a small VM on a German provider with 4 shared cores, 4GB of RAM, 120GB of SSD disk space, and a 1Gbit/sec internet connection and now is a 4 euro per month VPS in Milano, Italy - 4 shared cores, 8 GB RAM and 75GB disk space.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would separate various countries into different instances, for both management and security reasons, as well as to have the possibility of relocating just some of them if needed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Weather data would come from a reliable and open-source friendly source. I narrowed it down to two options: wttr.in and Open-Meteo, two solutions I know and that have always given me reliable results.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would pay close attention to accessibility: forecasts would be in local languages, consultable via text browsers, with emojis to give an idea even to those who don't speak local languages, and everything would be accessible without JavaScript or other requirements. One's mother tongue is always more "familiar" than a second language, even if you're fluent.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would manage everything according to Unix philosophy: small pieces working together. The more years pass, the more I understand how valuable this approach is.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The software chosen to manage the instances is snac. Snac embodies my philosophy of minimal and effective software, perfect for this purpose. It provides clear web pages for those who want to consult via the web, "speaks" the ActivityPub protocol perfectly, produces RSS feeds for each user (i.e., city), has extremely low RAM and CPU consumption, compiles in seconds, and is stable. The developer is an extremely helpful and positive person, and in my opinion, this carries equal weight as everything else.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I would do it for myself. If there was no interest, I would have kept it running anyway, without expanding it. So no anxiety or fear of failure.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Technical Implementation&lt;/head&gt;
    &lt;p&gt;I started setting up the first "pieces" during the days around Christmas 2024. The scheme was clear: each jail would handle everything internally. A Python script would download data, city by city, and produce markdown. The city coordinates would be calculated via the geopy library and passed to wttr.in and Open-Meteo. No data would be stored locally. This approach gives the ability to process all cities together. Just pass the city and country to the script, and the markdown would be served. At that point, snac comes into play: without the need to use external utilities, the "snac note" command allows posting from stdin by specifying the instance directory and the user to post from. No need to make API calls with external utilities, having to manage API keys, permissions, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Setting Up for Italy&lt;/head&gt;
    &lt;p&gt;To simplify things, I first structured the jail for Italy. I made a list of the main cities, normalizing them. For example, La Spezia became la_spezia. ForlÃ¬, with an accent, became forli - this for maximum compatibility since each city would be a snac user. I then created a script that takes this list and creates snac users via "snac adduser." At that point, after creating all the users, the script would modify the JSON of each user to convert the city name to uppercase, insert the bio (a standard text), activate the "bot" flag, and set the avatar, which was the same for all users at the time. This script is also able to add a new city: just run the script with the (normalized) name of the city, and it will add it - also adding it to the "cities.txt" file, so it will be updated in the next weather update cycle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core Application Development&lt;/head&gt;
    &lt;p&gt;I then created the heart of the service. A Python application (initially only in Italian, then multilingual, separating the operational part from the text) able to receive (via command line) the name of a city and a country code (corresponding to the file with texts in the local language). The script determines the coordinates and then, using API calls, requests the current weather conditions, those for the next 12 hours, and the next 7 days. I conducted experiments with both wttr.in and Open-Meteo, and both gave good results. However, I settled on Open-Meteo because, for my uses, it has always provided very reliable results. This application directly provides an output in Markdown since snac supports it, at least partially.&lt;/p&gt;
    &lt;p&gt;The cities.txt file is also crucial for updates. I created a script - post.sh, in pure sh, that scrolls through all cities, and for each one, launches the FediMeteo application and publishes its output using snac directly via command line. Once the job is finished, it makes a call to my instance of Uptime-Kuma, which keeps an eye on the situation. In case of failure, the monitoring will alert me that there have been no recent updates, and I can check.&lt;/p&gt;
    &lt;p&gt;At this point, the system cron takes care of launching post.sh every 6 hours. The requests are serialized, so the cities will update one at a time, and the posts will be sent to followers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Growth and Unexpected Success&lt;/head&gt;
    &lt;p&gt;After listing all Italian provincial capitals, I started testing everything. It worked perfectly. Of course, I had to make some adjustments at all levels. For example, one of the problems encountered was that snac did not set the language of the posts, and some users could have missed them. The developer was very quick and, as soon as I exposed the problem, immediately modified the program so that the post could keep the system language, set as an environment variable in the sh script.&lt;/p&gt;
    &lt;p&gt;After two days, I decided to start adding other countries and announce the project. And the announcement was unexpectedly well received: there were many boosts, and people started asking me to add their cities or countries. I tried to do what I could, within the limits of my physical condition, as in those days, I had the flu that kept me at home with a fever and illness for several days. I started adding many countries in the heart of Europe, translating the main indications into local languages but maintaining emojis so that everything would be understandable even to those who don't speak the local language. There were some small problems reported by some users. One of them: not all weather conditions had been translated, so sometimes they appeared in Italian - as well as errors. In bilingual countries, I tried to include all local languages. Sometimes, unfortunately, making mistakes as I encountered dynamics unknown to me or difficult to interpret. For example, in Ireland, forecasts were published in Irish, but it was pointed out to me that not everyone speaks it, so I modified and published in English.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Turning Point&lt;/head&gt;
    &lt;p&gt;The turning point was when FediFollows (@FediFollows@social.growyourown.services - who also manages the site Fedi Directory) started publishing the list of countries and cities, highlighting the project. Many people became aware of FediMeteo and started following the various accounts, the various cities. And from here came requests to add new countries and some new information, such as wind speed. Moreover, I was asked (rightly, to avoid flooding timelines) to publish posts as unlisted - this way, followers would see the posts, but they wouldn't fill local timelines. Snac didn't support this, but again, the snac dev came to my rescue in a few hours.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scaling Challenges&lt;/head&gt;
    &lt;p&gt;But with new countries came new challenges. For example, in my original implementation, all units of measurement were in metric/decimal/Celsius - and this doesn't adapt well to realities like the USA. Moreover, focusing on Europe, almost all countries were located in a single timezone, while for larger countries (such as Australia, USA, Canada, etc.), this is totally different. So I started developing a more complete and global version and, in the meantime, added almost all of Europe. The new version would have to be backward compatible, would have to take into account timezone differences for each city, different measurements (e.g., degrees C and F), as well as, initially more difficult part, being able to separate cities with the same name based on states or provinces. I had already seen a similar problem with the implementation of support for Germany, so it had to be addressed properly.&lt;/p&gt;
    &lt;p&gt;The original goal was to have a VPS for each continent, but I soon realized that thanks to the quality of snac's code and FreeBSD's efficient management, even keeping countries in separate jails, the load didn't increase much. So I decided to challenge myself and the limits of the economical 4 euros per month VPS. That is, to insert as much as possible until seeing what the limits were. Limits that, to date, I have not yet reached. I would also soon exhaust the available API calls for Open-Meteo's free accounts, so I tried to contact the team and explain everything. I was positively surprised to read that they appreciated the project and provided me with a dedicated API key.&lt;/p&gt;
    &lt;p&gt;Compatible with my free time, I managed to complete the richer and more complete version of my Python program. I'm not a professional dev, I'm more oriented towards systems, so the code is probably quite poor in the eyes of an expert dev. But, in the end, it just needs to take an input and give me an output. It's not a daemon, it's not a service that responds on the network. For that, snac takes care of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Expansion to North America&lt;/head&gt;
    &lt;p&gt;So I decided to start with a very important launch: the USA and Canada. A non-trivial part was identifying the main cities in order to cover, state by state, all the territory. In the end, I identified more than 1200 cities. A number that, by itself, exceeded the sum of all other countries (at that time). And the program, now, is able to take an input with a separator (two underscores: __) between city and state. In this way, it's possible to perfectly understand the differences between city and state: new_york__new_york is an example I like to make, but there are many.&lt;/p&gt;
    &lt;p&gt;The launch of the USA was interesting: despite having had many previous requests, the reception was initially quite lukewarm, to my extreme surprise. The number of followers in Canada, in a few hours, far exceeded that of the USA. On the contrary, the country with the most followers (in a few days, more than 1000) was Germany. Followed by the UK - which I expected would have been the first.&lt;/p&gt;
    &lt;head rend="h2"&gt;System Performance&lt;/head&gt;
    &lt;p&gt;The VPS held up well. Except for the moments when FediFollows launched (after fixing some FreeBSD tuning, the service slowed slightly but didn't crash), the load remained extremely low. So I continued to expand: Japan, Australia, New Zealand, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Current Status&lt;/head&gt;
    &lt;p&gt;At the time of the last update of this article (30 December 2025), the supported countries are 38: Argentina, Australia, Austria, Belgium, Brazil, Bulgaria, Canada, Croatia, Czechia, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, India, Ireland, Italy, Japan, Latvia, Lithuania, Malta, Mexico, Netherlands, New Zealand, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Sweden, Switzerland, Taiwan, the United Kingdom, and the United States of America (with more regions coming soon!).&lt;/p&gt;
    &lt;p&gt;Direct followers in the Fediverse are around 7,707 and growing daily, excluding those who follow hashtags or cities via RSS, whose number I can't estimate. However, a quick look at the logs suggests there are many more.&lt;/p&gt;
    &lt;p&gt;The cities currently covered are 2937 - growing based on new countries and requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenges Encountered&lt;/head&gt;
    &lt;p&gt;There have been some problems. The most serious, by my fault, was the API key leak: I had left a debug code active and, the first time Open-Meteo had problems, the error message also included the API call - including the API key. Some users reported it to me (others just mocked) and I fixed the code and immediately reported everything to the Open-Meteo team, who kindly gave me a new API Key and deactivated the old one.&lt;/p&gt;
    &lt;p&gt;A further problem was related to geopy. It makes a call to Nominatim to determine coordinates. One of the times Nominatim didn't respond, my program wasn't able to determine the position and went into error. I solved this by introducing coordinate caching: now the program, the first time it encounters a city, requests and saves the coordinates. If present, they will be used in the future without making a new request via geopy. This is both lighter on their servers and faster and safer for us.&lt;/p&gt;
    &lt;head rend="h2"&gt;Infrastructure Details&lt;/head&gt;
    &lt;p&gt;And the VPS? It has no problems and is surprisingly fast and effective. FreeBSD 14.3-RELEASE, BastilleBSD to manage the jails. Currently, there are 39 jails - one for haproxy, the FediMeteo website, so nginx, and the snac instance for FediMeteo announcements and support - the other 38 for the individual instances. Each of them, therefore, has its autonomous ZFS dataset. Every 15 minutes, there is a local snapshot of all datasets. Every hour, the homepage is regenerated: a small script calculates the number of followers (counting, instance by instance, the followers of individual cities, since I don't publish except in aggregate to avoid possible triangulations and privacy leaks of users). Every hour, moreover, an external backup is made via zfs-autobackup (on encrypted at rest dataset), and once a day, a further backup is made in my datacenter, on disks encrypted with geli. The occupied RAM is 501 MB (yes, exactly: 501 MB), which rises slightly when updates are in progress. Updates normally occur every 6 hours. I have tried, as much as possible, to space them out to avoid overloads in timelines (or on the server itself). Only for the USA, I added a sleep of 5 seconds between one city and another, to give snac the opportunity to better organize the sending of messages. It probably wouldn't be necessary, with the current numbers, but better safe than sorry. In this way, the USA is processed in about 2 and a half hours, but the other jails (thus countries) can work autonomously and send their updates.&lt;/p&gt;
    &lt;p&gt;The average load of the VPS (taking as reference both the last 24 hours and the last two weeks) is about 25%, as it rises to 70/75% when updates occur for larger instances (such as the USA), or when it is announced by FediFollows. Otherwise, it is on average less than 10%. So, the VPS still has huge margin, and new instances, with new nations, will still be inside it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This article, although in some parts very conversational, aims to demonstrate how it's possible to build solid, valid, and efficient solutions without the need to use expensive and complex services. Moreover, this is the demonstration of how it's possible to have your online presence without the need to put your data in the hands of third parties or without necessarily having to resort to complex stacks. Sometimes, less is more.&lt;/p&gt;
    &lt;p&gt;The success of this project demonstrates, once again, that my grandfather was right: weather forecasts interest everyone. He worried about my health and, thanks to his concerns, we spent time together. In the same way, I see many followers and friends talking to me or among themselves about the weather, their experiences, what happens. Again, in my life, weather forecasts have helped sociality and socialization.&lt;/p&gt;
    &lt;p&gt;Thank you, Grandpa.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://it-notes.dragas.net/2025/02/26/fedimeteo-how-a-tiny-freebsd-vps-became-a-global-weather-service-for-thousands/"/><published>2025-12-30T19:21:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46437288</id><title>Zpdf: PDF text extraction in Zig – 5x faster than MuPDF</title><updated>2025-12-30T22:10:27.124264+00:00</updated><content>&lt;doc fingerprint="6242b261e60c20ca"&gt;
  &lt;main&gt;
    &lt;p&gt;A PDF text extraction library written in Zig.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Memory-mapped file reading for efficient large file handling&lt;/item&gt;
      &lt;item&gt;Streaming text extraction (no intermediate allocations)&lt;/item&gt;
      &lt;item&gt;Multiple decompression filters: FlateDecode, ASCII85, ASCIIHex, LZW, RunLength&lt;/item&gt;
      &lt;item&gt;Font encoding support: WinAnsi, MacRoman, ToUnicode CMap&lt;/item&gt;
      &lt;item&gt;XRef table and stream parsing (PDF 1.5+)&lt;/item&gt;
      &lt;item&gt;Configurable error handling (strict or permissive)&lt;/item&gt;
      &lt;item&gt;Multi-threaded parallel page extraction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Text extraction performance vs MuPDF 1.26 (&lt;code&gt;mutool convert -F text&lt;/code&gt;) on Apple M4 Pro:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Document&lt;/cell&gt;
        &lt;cell role="head"&gt;Pages&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;zpdf&lt;/cell&gt;
        &lt;cell role="head"&gt;MuPDF&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;C++ Standard Draft&lt;/cell&gt;
        &lt;cell&gt;2,134&lt;/cell&gt;
        &lt;cell&gt;8 MB&lt;/cell&gt;
        &lt;cell&gt;250 ms&lt;/cell&gt;
        &lt;cell&gt;968 ms&lt;/cell&gt;
        &lt;cell&gt;3.9x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Pandas Documentation&lt;/cell&gt;
        &lt;cell&gt;3,743&lt;/cell&gt;
        &lt;cell&gt;15 MB&lt;/cell&gt;
        &lt;cell&gt;395 ms&lt;/cell&gt;
        &lt;cell&gt;1,112 ms&lt;/cell&gt;
        &lt;cell&gt;2.8x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Intel SDM&lt;/cell&gt;
        &lt;cell&gt;5,252&lt;/cell&gt;
        &lt;cell&gt;25 MB&lt;/cell&gt;
        &lt;cell&gt;451 ms&lt;/cell&gt;
        &lt;cell&gt;2,099 ms&lt;/cell&gt;
        &lt;cell&gt;4.7x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Document&lt;/cell&gt;
        &lt;cell role="head"&gt;Pages&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;zpdf&lt;/cell&gt;
        &lt;cell role="head"&gt;MuPDF&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;C++ Standard Draft&lt;/cell&gt;
        &lt;cell&gt;2,134&lt;/cell&gt;
        &lt;cell&gt;8 MB&lt;/cell&gt;
        &lt;cell&gt;131 ms&lt;/cell&gt;
        &lt;cell&gt;966 ms&lt;/cell&gt;
        &lt;cell&gt;7.4x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Pandas Documentation&lt;/cell&gt;
        &lt;cell&gt;3,743&lt;/cell&gt;
        &lt;cell&gt;15 MB&lt;/cell&gt;
        &lt;cell&gt;218 ms&lt;/cell&gt;
        &lt;cell&gt;1,117 ms&lt;/cell&gt;
        &lt;cell&gt;5.1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Intel SDM&lt;/cell&gt;
        &lt;cell&gt;5,252&lt;/cell&gt;
        &lt;cell&gt;25 MB&lt;/cell&gt;
        &lt;cell&gt;117 ms&lt;/cell&gt;
        &lt;cell&gt;2,098 ms&lt;/cell&gt;
        &lt;cell&gt;17.9x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Peak throughput: 45,000 pages/sec (Intel SDM, parallel)&lt;/p&gt;
    &lt;p&gt;Build with &lt;code&gt;zig build -Doptimize=ReleaseFast&lt;/code&gt; for these results.&lt;/p&gt;
    &lt;p&gt;zpdf uses SIMD-accelerated routines for hot paths:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Whitespace skipping (content streams are whitespace-heavy)&lt;/item&gt;
      &lt;item&gt;Delimiter detection (tokenization)&lt;/item&gt;
      &lt;item&gt;Keyword search (&lt;code&gt;stream&lt;/code&gt;,&lt;code&gt;endstream&lt;/code&gt;,&lt;code&gt;startxref&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;String boundary scanning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Auto-detects: NEON (ARM64), AVX2/SSE4.2 (x86_64), or scalar fallback.&lt;/p&gt;
    &lt;p&gt;Note: MuPDF's threading (&lt;code&gt;-T&lt;/code&gt; flag) is for rendering/rasterization only. Text extraction via &lt;code&gt;mutool convert -F text&lt;/code&gt; is single-threaded by design. zpdf parallelizes text extraction across pages.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zig 0.15.2 or later&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;zig build              # Build library and CLI
zig build test         # Run tests&lt;/code&gt;
    &lt;code&gt;const zpdf = @import("zpdf");

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const doc = try zpdf.Document.open(allocator, "file.pdf");
    defer doc.close();

    var buf: [4096]u8 = undefined;
    var writer = std.fs.File.stdout().writer(&amp;amp;buf);
    defer writer.interface.flush() catch {};

    for (0..doc.pages.items.len) |page_num| {
        try doc.extractText(page_num, &amp;amp;writer.interface);
    }
}&lt;/code&gt;
    &lt;code&gt;zpdf extract document.pdf           # Extract all pages to stdout
zpdf extract -p 1-10 document.pdf   # Extract pages 1-10
zpdf extract -o out.txt document.pdf # Output to file
zpdf info document.pdf              # Show document info
zpdf bench document.pdf             # Run benchmark&lt;/code&gt;
    &lt;code&gt;import zpdf

with zpdf.Document("file.pdf") as doc:
    print(doc.page_count)

    # Single page
    text = doc.extract_page(0)

    # All pages (parallel by default)
    all_text = doc.extract_all()

    # Page info
    info = doc.get_page_info(0)
    print(f"{info.width}x{info.height}")&lt;/code&gt;
    &lt;p&gt;Build the shared library first:&lt;/p&gt;
    &lt;code&gt;zig build -Doptimize=ReleaseFast
PYTHONPATH=python python3 examples/basic.py&lt;/code&gt;
    &lt;code&gt;src/
├── root.zig         # Document API and core types
├── capi.zig         # C ABI exports for FFI
├── parser.zig       # PDF object parser
├── xref.zig         # XRef table/stream parsing
├── pagetree.zig     # Page tree resolution
├── decompress.zig   # Stream decompression filters
├── encoding.zig     # Font encoding and CMap parsing
├── interpreter.zig  # Content stream interpreter
├── simd.zig         # SIMD string operations
└── main.zig         # CLI

python/zpdf/         # Python bindings (cffi)
examples/            # Usage examples
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;zpdf&lt;/cell&gt;
        &lt;cell role="head"&gt;MuPDF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Text Extraction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reading order / layout analysis&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Two-column detection&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Paragraph grouping&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Word/line bounding boxes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Font Support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WinAnsi/MacRoman&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ToUnicode CMap&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CID fonts (Type0)&lt;/cell&gt;
        &lt;cell&gt;Partial&lt;/cell&gt;
        &lt;cell&gt;Full&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Embedded fonts&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Compression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;FlateDecode, LZW, ASCII85/Hex&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;JBIG2, JPEG2000&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PDF Features&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Incremental updates&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Encrypted PDFs&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Forms / Annotations&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Rendering&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Use zpdf when: Speed matters, simple layouts, batch processing raw text.&lt;/p&gt;
    &lt;p&gt;Use MuPDF when: Complex layouts, encrypted PDFs, accurate reading order needed.&lt;/p&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Lulzx/zpdf"/><published>2025-12-30T19:57:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46437381</id><title>Everything as Code: How We Manage Our Company in One Monorepo</title><updated>2025-12-30T22:10:26.614915+00:00</updated><content>&lt;doc fingerprint="b79cf4185d653cf8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Everything as Code: How We Manage Our Company In One Monorepo&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Last week, I updated our pricing limits. One JSON file. The backend started enforcing the new caps, the frontend displayed them correctly, the marketing site showed them on the pricing page, and our docs reflected the change—all from a single commit.&lt;/p&gt;
    &lt;p&gt;No sync issues. No "wait, which repo has the current pricing?" No deploy coordination across three teams. Just one change, everywhere, instantly.&lt;/p&gt;
    &lt;p&gt;At Kasava, our entire platform lives in a single repository. Not just the code—everything:&lt;/p&gt;
    &lt;code&gt;kasava/                              # 5,470+ files TypeScript files
├── frontend/                       # Next.js 16 + React 19 application
│   └── src/
│       ├── app/                   # 25+ route directories
│       └── components/            # 45+ component directories
├── backend/                        # Cloudflare Workers API
│   └── src/
│       ├── services/              # 55+ business logic services
│       └── workflows/             # Mastra AI workflows
├── website/                        # Marketing site (kasava.ai)
├── docs/                           # Public documentation (Mintlify)
├── docs-internal/                  # 12+ architecture docs &amp;amp; specs
├── marketing/
│   ├── blogs/                     # Blog pipeline (drafts → review → published)
│   ├── investor-deck/             # Next.js site showing investment proposal
│   └── email/                     # MJML templates for Loops.so campaigns
├── external/
│   ├── chrome-extension/          # WXT + React bug capture tool
│   ├── google-docs-addon/         # @helper AI assistant (Apps Script)
│   └── google-cloud-functions/
│       ├── tree-sitter-service/   # AST parsing for 10+ languages
│       └── mobbin-research-service/
├── scripts/                        # Deployment &amp;amp; integration testing
├── infra-tester/                   # Integration test harness
└── github-simulator/               # Mock GitHub API for local dev

&lt;/code&gt;
    &lt;head rend="h2"&gt;Why This Matters: AI-Native Development&lt;/head&gt;
    &lt;p&gt;This isn't about abstract philosophies on design patterns for 'how we should work.' It's about velocity in an era where products change fast and context matters.&lt;/p&gt;
    &lt;p&gt;AI is all about context. And this monorepo is our company—not just the product.&lt;/p&gt;
    &lt;p&gt;When our AI tools help us write documentation, they have immediate access to the actual code being documented. When we update our marketing website, the AI can verify claims against the real implementation. When we write blog posts like this one, the AI can fact-check every code example, every number, every architectural claim against the source of truth.&lt;/p&gt;
    &lt;p&gt;This means we move faster:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Documentation updates faster because the AI sees code changes and suggests doc updates in the same context&lt;/item&gt;
      &lt;item&gt;Website updates faster because pricing, features, and capabilities are pulled from the same config files that power the app&lt;/item&gt;
      &lt;item&gt;Blog posts ship faster because the AI can run self-referential checks—validating that our "5,470+ TypeScript files" claim is accurate by actually counting them&lt;/item&gt;
      &lt;item&gt;Nothing goes out of sync because there's only one source of truth, and AI has access to all of it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you ask Claude to "update the pricing page to reflect the new limits," it can:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read the backend service that enforces limits&lt;/item&gt;
      &lt;item&gt;Check the frontend that displays them&lt;/item&gt;
      &lt;item&gt;Update the marketing site&lt;/item&gt;
      &lt;item&gt;Verify the docs are consistent&lt;/item&gt;
      &lt;item&gt;Flag any blog posts that might mention outdated numbers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All in one conversation. All in one repository.&lt;/p&gt;
    &lt;p&gt;This is what "AI-native development" actually means: structuring your work so AI can be maximally helpful, not fighting against fragmentation.&lt;/p&gt;
    &lt;p&gt;And it reinforces a shipping culture.&lt;/p&gt;
    &lt;p&gt;Everything-as-code means everything ships the same way: &lt;code&gt;git push&lt;/code&gt;. Want to update the website pricing page? &lt;code&gt;git push&lt;/code&gt;. New blog post ready to go live? &lt;code&gt;git push&lt;/code&gt;. Fix a typo in the docs? &lt;code&gt;git push&lt;/code&gt;. Deploy a backend feature? &lt;code&gt;git push&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;No separate CMSs to log into. No WordPress admin panels. No waiting for marketing tools to sync. No "can someone with Contentful access update this?" The same Git workflow that ships code also ships content, documentation, and marketing. Everyone on the team can ship anything, and it all goes through the same review process, the same CI/CD, the same audit trail.&lt;/p&gt;
    &lt;p&gt;This uniformity removes friction and removes excuses. Shipping becomes muscle memory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Everything in One Repo?&lt;/head&gt;
    &lt;head rend="h3"&gt;1. Atomic Changes Across Boundaries (That AI Can Understand)&lt;/head&gt;
    &lt;p&gt;When a backend API changes, the frontend type definitions update in the same commit. When we add a new feature, the documentation can ship alongside it. No version mismatches. No "which version of the API does this frontend need?"&lt;/p&gt;
    &lt;p&gt;AI can see and validate the entire change in context.&lt;/p&gt;
    &lt;p&gt;When we ask Claude to add a feature, it doesn't just write backend code. It sees the frontend that will consume it, the docs that need updating, and the marketing site that might reference it. All in one view. All in one conversation.&lt;/p&gt;
    &lt;p&gt;Real example from our codebase—adding Asana integration:&lt;/p&gt;
    &lt;code&gt;commit: "feat: add Asana integration"
├── backend/src/services/AsanaService.ts
├── backend/src/routes/api/integrations/asana.ts
├── frontend/src/components/integrations/asana/
├── frontend/src/app/integrations/asana/
├── docs/integrations/asana.mdx
└── website/src/app/integrations/page.tsx
&lt;/code&gt;
    &lt;p&gt;One PR. One review. One merge. Everything ships together.&lt;/p&gt;
    &lt;p&gt;Another example—keeping pricing in sync:&lt;/p&gt;
    &lt;p&gt;We have a single &lt;code&gt;billing-plans.json&lt;/code&gt; that defines all plan limits and features:&lt;/p&gt;
    &lt;code&gt;// frontend/src/config/billing-plans.json (also copied to website/src/config/)
{
  "plans": {
    "free": { "limits": { "repositories": 1, "aiChatMessagesPerDay": 10 } },
    "starter": {
      "limits": { "repositories": 10, "aiChatMessagesPerDay": 100 }
    },
    "professional": {
      "limits": { "repositories": 50, "aiChatMessagesPerDay": 1000 }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;The backend enforces these limits. The frontend displays them in settings. The marketing website shows them on the pricing page. When we change a limit, one JSON update propagates everywhere—no "the website says 50 repos but the app shows 25" bugs.&lt;/p&gt;
    &lt;p&gt;And AI validates all of it. When we update &lt;code&gt;billing-plans.json&lt;/code&gt;, we can ask Claude to verify that the backend, frontend, and website are all consistent. It reads all three implementations and confirms they match—or tells us what needs fixing.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Cross-Project Refactoring&lt;/head&gt;
    &lt;p&gt;Renaming a function? Your IDE finds all usages across frontend, backend, docs examples, and blog code snippets. One find-and-replace. One commit.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Single Source of Truth&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dependencies: Shared tooling configured once&lt;/item&gt;
      &lt;item&gt;CI/CD: One pipeline to understand&lt;/item&gt;
      &lt;item&gt;Search: Find anything with one &lt;code&gt;grep&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Structure: What Lives Where&lt;/head&gt;
    &lt;head rend="h3"&gt;Core Application&lt;/head&gt;
    &lt;code&gt;frontend/                        # Customer-facing Next.js app
├── src/
│   ├── app/                    # Next.js 15 App Router
│   │   ├── analytics/         # Semantic commit analysis
│   │   ├── bug-reports/       # AI-powered bug tracking
│   │   ├── chat/              # AI assistant interface
│   │   ├── code-search/       # Semantic code search
│   │   ├── dashboard/         # Main dashboard
│   │   ├── google-docs-assistant/
│   │   ├── integrations/      # GitHub, Linear, Jira, Asana
│   │   ├── prd/               # PRD management
│   │   └── ...                # 25+ route directories total
│   ├── components/            # 45+ component directories
│   │   ├── ai-elements/      # AI-specific UI
│   │   ├── bug-reports/      # Bug tracking UI
│   │   ├── dashboard/        # Dashboard widgets
│   │   ├── google-docs/      # Google Docs integration
│   │   ├── onboarding/       # User onboarding flow
│   │   └── ui/               # shadcn/ui base components
│   ├── mastra/               # Frontend Mastra integration
│   └── lib/                  # SDK, utilities, hooks

backend/                        # Cloudflare Workers API
├── src/
│   ├── routes/               # Hono API endpoints
│   ├── services/             # 55+ business logic services
│   ├── workflows/            # Mastra AI workflows
│   │   ├── steps/           # Reusable workflow steps
│   │   └── RepositoryIndexingWorkflow.ts
│   ├── db/                   # Drizzle ORM schema
│   ├── durable-objects/      # Stateful edge computing
│   ├── workers/              # Queue consumers
│   └── mastra/               # AI agents and tools
&lt;/code&gt;
    &lt;p&gt;These two talk to each other constantly. Having them in the same repo means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;API changes include frontend updates&lt;/item&gt;
      &lt;item&gt;Type safety across the boundary&lt;/item&gt;
      &lt;item&gt;Shared testing utilities&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Marketing Properties&lt;/head&gt;
    &lt;code&gt;website/                        # kasava.ai marketing site
├── src/
│   ├── app/                   # Landing pages, blog
│   ├── components/            # Shared marketing components
│   └── lib/                   # Utilities

marketing/
├── blogs/
│   ├── queue/
│   │   └── drafts/           # Ideas and drafts
│   ├── review/               # Ready for editing
│   └── published/            # Live on the site
├── investor-deck/            # Next.js presentation (not PowerPoint!)
└── email/
    ├── CLAUDE.md             # Email writing guidelines
    └── mjml/                 # 7+ email campaign loops
        ├── loop-1-welcome/
        ├── loop-2-github-connected/
        ├── loop-3-trial-conversion/
        └── ...
&lt;/code&gt;
    &lt;p&gt;Yes, even blog posts are code. They're Markdown files with frontmatter, versioned in Git, reviewed in PRs. Email templates are MJML that version controls our entire customer communication system.&lt;/p&gt;
    &lt;p&gt;Even our investor deck is code — a Next.js 16 static site with 17 React slide components, keyboard navigation, and PDF export. No PowerPoint, no Google Slides. When we update metrics or messaging, it's a code change with full Git history, reviewed in a PR, and deployed with &lt;code&gt;git push&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Why this matters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Marketing can update copy without engineering&lt;/item&gt;
      &lt;item&gt;Changes are reviewed and tracked&lt;/item&gt;
      &lt;item&gt;Rollback is one &lt;code&gt;git revert&lt;/code&gt;away&lt;/item&gt;
      &lt;item&gt;Email campaigns are testable and diffable&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Documentation&lt;/head&gt;
    &lt;code&gt;docs/                           # Public docs (Mintlify)
├── index.mdx                  # Landing page
├── quickstart.mdx             # Getting started
├── demo-mode.mdx              # Demo mode guide
├── features/                  # Product features
│   ├── ai-chat.mdx
│   ├── code-intelligence.mdx
│   ├── code-search.mdx
│   └── prds.mdx
├── integrations/              # Integration guides
│   ├── github.mdx
│   ├── linear.mdx
│   ├── jira.mdx
│   └── asana.mdx
└── bug-tracking/              # Bug tracking docs

docs-internal/                  # Engineering knowledge base
├── GITHUB_CHAT_ARCHITECTURE.md
├── QUEUE_ARCHITECTURE_SUMMARY.md
├── UNIFIED_TASK_ANALYTICS_QUEUE.md
├── features/                  # Feature specs
├── migrations/                # Migration guides
├── plans/                     # Implementation plans
└── research/                  # Research notes
&lt;/code&gt;
    &lt;p&gt;Public docs deploy automatically when we push. Internal docs are searchable alongside code—when someone asks "how does the queue work?", they find the actual architecture document, not a stale wiki page.&lt;/p&gt;
    &lt;head rend="h3"&gt;External Services&lt;/head&gt;
    &lt;code&gt;external/
├── chrome-extension/          # WXT-based bug capture tool
│   ├── entrypoints/          # popup, content scripts, background
│   ├── lib/                  # Screen capture, console logging
│   ├── components/           # React UI components
│   └── wxt.config.ts         # WXT configuration
│
├── google-docs-addon/        # @helper mentions in Docs
│   ├── Code.gs              # Main Apps Script (18KB)
│   ├── Sidebar.html         # React-like UI (26KB)
│   ├── Settings.html        # Configuration UI
│   └── appsscript.json      # Manifest
│
└── google-cloud-functions/
    ├── tree-sitter-service/  # AST parsing
    │   └── Supports: JS, TS, Python, Go, Rust,
    │       Java, C, C++, Ruby, PHP, C#
    └── mobbin-research-service/  # UX research
&lt;/code&gt;
    &lt;p&gt;These deploy to completely different platforms (Chrome Web Store, Google Apps Script, GCP) but live together because:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They share API contracts with the main app&lt;/item&gt;
      &lt;item&gt;Changes often span boundaries&lt;/item&gt;
      &lt;item&gt;One team maintains everything&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Development Infrastructure&lt;/head&gt;
    &lt;code&gt;github-simulator/              # Mock GitHub API for local dev
infra-tester/                  # Integration test harness
scripts/
├── google-cloud/             # GCP deployment scripts
├── test-credentials.ts       # Credential testing
└── test-webhook-integration.ts
&lt;/code&gt;
    &lt;p&gt;Local development shouldn't require external services. Mock servers live with the code they simulate.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Deploys Where&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Tech Stack&lt;/cell&gt;
        &lt;cell role="head"&gt;Deploys To&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Frontend&lt;/cell&gt;
        &lt;cell&gt;Next.js 15, React 19, Tailwind v4&lt;/cell&gt;
        &lt;cell&gt;Vercel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Backend&lt;/cell&gt;
        &lt;cell&gt;Cloudflare Workers, Hono, Mastra&lt;/cell&gt;
        &lt;cell&gt;Cloudflare&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Website&lt;/cell&gt;
        &lt;cell&gt;Next.js, custom components&lt;/cell&gt;
        &lt;cell&gt;Vercel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Investor Deck&lt;/cell&gt;
        &lt;cell&gt;Next.js, custom components&lt;/cell&gt;
        &lt;cell&gt;Vercel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Docs&lt;/cell&gt;
        &lt;cell&gt;Mintlify MDX&lt;/cell&gt;
        &lt;cell&gt;Mintlify&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Chrome Extension&lt;/cell&gt;
        &lt;cell&gt;WXT, React, Tailwind&lt;/cell&gt;
        &lt;cell&gt;Chrome Web Store&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Google Docs Add-on&lt;/cell&gt;
        &lt;cell&gt;Apps Script, HTML&lt;/cell&gt;
        &lt;cell&gt;Google Workspace Marketplace&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tree-sitter Service&lt;/cell&gt;
        &lt;cell&gt;Node.js, GCP Functions&lt;/cell&gt;
        &lt;cell&gt;Google Cloud&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Email Templates&lt;/cell&gt;
        &lt;cell&gt;MJML&lt;/cell&gt;
        &lt;cell&gt;Loops.so&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;How We Make It Work&lt;/head&gt;
    &lt;head rend="h3"&gt;No Workspaces (And That's Fine)&lt;/head&gt;
    &lt;p&gt;We deliberately don't use npm/yarn workspaces. (Well, we do in one specific use case but that's for another post.) Each directory is its own independent npm project:&lt;/p&gt;
    &lt;code&gt;cd frontend &amp;amp;&amp;amp; npm install    # Frontend dependencies
cd backend &amp;amp;&amp;amp; npm install     # Backend dependencies
cd external/chrome-extension &amp;amp;&amp;amp; npm install  # Extension dependencies
&lt;/code&gt;
    &lt;p&gt;Why? Simplicity. No hoisting confusion. No "which version of React am I actually getting?" Each project is isolated and predictable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Selective CI/CD&lt;/head&gt;
    &lt;p&gt;We run 5 GitHub Actions workflows, each triggered by specific paths:&lt;/p&gt;
    &lt;code&gt;# .github/workflows/frontend-tests.yml
name: Frontend Tests
on:
  push:
    paths:
      - "frontend/**"
      - ".github/workflows/frontend-tests.yml"
# Runs: type-check, lint, demo data validation, tests with coverage
&lt;/code&gt;
    &lt;code&gt;# .github/workflows/backend-tests.yml
name: Backend Tests
on:
  push:
    paths:
      - "backend/**"
      - ".github/workflows/backend-tests.yml"
# Runs: unit tests, integration tests, e2e tests
&lt;/code&gt;
    &lt;code&gt;# .github/workflows/tree-sitter-tests.yml
name: Tree-sitter Tests
on:
  push:
    paths:
      - "external/google-cloud-functions/tree-sitter-service/**"
# Runs: parsing tests for all 10+ supported languages
&lt;/code&gt;
    &lt;p&gt;Change the Chrome extension? Only relevant tests run. Update the backend? Backend tests plus any integration tests that depend on it.&lt;/p&gt;
    &lt;head rend="h3"&gt;The CLAUDE.md Convention&lt;/head&gt;
    &lt;p&gt;Every major directory has a CLAUDE.md file that documents:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What this code does&lt;/item&gt;
      &lt;item&gt;Tech stack and versions&lt;/item&gt;
      &lt;item&gt;Quick start commands&lt;/item&gt;
      &lt;item&gt;Architecture decisions&lt;/item&gt;
      &lt;item&gt;Common patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;CLAUDE.md                          # Root-level overview
├── frontend/CLAUDE.md            # Next.js 15, React 19, Tailwind v4
├── backend/CLAUDE.md             # Cloudflare Workers, Hono, Mastra
├── external/chrome-extension/CLAUDE.md
├── external/google-cloud-functions/CLAUDE.md
└── marketing/email/CLAUDE.md     # MJML email guidelines
&lt;/code&gt;
    &lt;p&gt;This isn't just for humans—AI coding assistants read these files. When Claude Code works on our frontend, it reads &lt;code&gt;frontend/CLAUDE.md&lt;/code&gt; and knows we're using Next.js 15 with React 19, npm (not pnpm), and specific patterns.&lt;/p&gt;
    &lt;head rend="h3"&gt;Consistent Tooling&lt;/head&gt;
    &lt;p&gt;One configuration, everywhere:&lt;/p&gt;
    &lt;code&gt;.prettierrc              # Formatting (all JS/TS)
.eslintrc               # Linting (shared rules)
tsconfig.json           # TypeScript base config
&lt;/code&gt;
    &lt;p&gt;New developer? &lt;code&gt;npm install&lt;/code&gt; in the directory you're working on. Everything works.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Challenges (And How We Handle Them)&lt;/head&gt;
    &lt;head rend="h3"&gt;Challenge: Repository Size&lt;/head&gt;
    &lt;p&gt;Why it's not a problem (yet):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clone time: ~20 seconds&lt;/item&gt;
      &lt;item&gt;Git operations: still snappy&lt;/item&gt;
      &lt;item&gt;We haven't needed sparse checkout, LFS, or shallow clones&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When we might need to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large binary assets would go to R2/S3, not git&lt;/item&gt;
      &lt;item&gt;If we hit 1GB+, we'd look at shallow clones for CI&lt;/item&gt;
      &lt;item&gt;Truly independent services could be extracted&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Challenge: Build Times&lt;/head&gt;
    &lt;p&gt;Problem: If everything is connected, does everything rebuild?&lt;/p&gt;
    &lt;p&gt;Reality: No. Each project builds independently:&lt;/p&gt;
    &lt;code&gt;# Frontend build (only rebuilds frontend)
cd frontend &amp;amp;&amp;amp; npm run build

# Backend build (only rebuilds backend)
cd backend &amp;amp;&amp;amp; npm run build

# Extension build (only rebuilds extension)
cd external/chrome-extension &amp;amp;&amp;amp; npm run build
&lt;/code&gt;
    &lt;p&gt;We use Turbopack for frontend dev (fast HMR), Wrangler for backend dev (fast reload), and WXT for extension dev (fast rebuild).&lt;/p&gt;
    &lt;head rend="h3"&gt;Challenge: Permission Boundaries&lt;/head&gt;
    &lt;p&gt;Problem: Not everyone should see everything.&lt;/p&gt;
    &lt;p&gt;Our situation: We're a small team. Everyone can see everything. That's a feature, not a bug—it enables cross-pollination.&lt;/p&gt;
    &lt;p&gt;If we grew and needed boundaries:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub CODEOWNERS for review requirements&lt;/item&gt;
      &lt;item&gt;Branch protection rules&lt;/item&gt;
      &lt;item&gt;Potentially split truly sensitive codebases (but we'd resist this)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Challenge: Context Switching&lt;/head&gt;
    &lt;p&gt;Problem: Jumping between TypeScript (frontend), TypeScript (backend), Apps Script (Google add-on), and MJML (emails) feels disorienting.&lt;/p&gt;
    &lt;p&gt;Solutions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consistent patterns across projects (same linting, same formatting)&lt;/item&gt;
      &lt;item&gt;CLAUDE.md files explain context immediately&lt;/item&gt;
      &lt;item&gt;IDE workspace configurations&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Our monorepo isn't about following a trend. It's about removing friction between things that naturally belong together, something that is critical when related context is everything.&lt;/p&gt;
    &lt;p&gt;When a feature touches the backend API, the frontend component, the documentation, and the marketing site—why should that be four repositories, four PRs, four merge coordination meetings?&lt;/p&gt;
    &lt;p&gt;The monorepo isn't a constraint. It's a force multiplier.&lt;/p&gt;
    &lt;p&gt;Kasava is built as a unified platform. See what we've built&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.kasava.dev/blog/everything-as-code-monorepo"/><published>2025-12-30T20:05:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46437391</id><title>Prof. Software Developers Don't Vibe, They Control: AI Agent Coding Use in 2025</title><updated>2025-12-30T22:10:26.319295+00:00</updated><content>&lt;doc fingerprint="8ddd161bc8db90cf"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Software Engineering&lt;/head&gt;&lt;p&gt; [Submitted on 16 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.SE&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2512.14012"/><published>2025-12-30T20:06:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46438122</id><title>How the "Marvelization" of Cinema Accelerates the Decline of Filmmaking</title><updated>2025-12-30T22:10:25.961383+00:00</updated><content>&lt;doc fingerprint="b88213d5adec8428"&gt;
  &lt;main&gt;
    &lt;p&gt;As hard as it may be to believe, some of us have never seen a movie belonging to the Marvel Cinematic Universe. If you’re one of those uninitiated, none of the countless clips incorporated into the Like Stories of Old video essay above will tempt you to get initiated. Nor will the laments aired by host Tom van der Linden, who, despite once enjoying the MCU himself, eventually came to wonder why keeping up with its releases had begun to feel less like a thrill than a chore. As if their CGI-laden sound and fury weren’t trying enough, there’s also “the constant quipping, the annoying self-awareness, the fact that everything has to be a franchise now.”&lt;/p&gt;
    &lt;p&gt;Van der Linden labels a central factor in the decline of the MCU “storytelling entropy.” Classic films, you may have noticed, concentrate practically all the energy in every facet of their production toward the expression of specific themes, stories, and characters; at their best, their every line, gesture, cut, and invention represents the tip of an artistic iceberg. Take, to use a popular example, the lightsaber introduced in Star Wars, which Van der Linden calls “not just a weapon, but a metaphor” that “symbolically communicates a lot about the philosophy of its wielder, and about the larger world that it exists in,” condensing “a multitude of meanings and ideas into a simple, singular object.”&lt;/p&gt;
    &lt;p&gt;It does so in the first two or three movies, at any rate. In the decades since, as the Star Wars universe has grown ever vaster, more complex, and conceptually unwieldy, so the proliferation and modification of the once-marvelous lightsaber has turned it into something mundane, even banal. So it goes with storytelling entropy, a phenomenon that afflicts every narrative franchise commercially compelled to grow without end. That process of expansion eventually turns even the most captivating original materials diffuse and uninvolving to all but the hardest-core fans — by which point it has usually become obvious that creators themselves have long since lost their own passion for the stories.&lt;/p&gt;
    &lt;p&gt;Most MCU viewers will admit that it has produced misses as well as hits. But Marvelization, as Van der Linden calls it, has also inspired other, imitative corporate franchises to pump out globally marketable content fiercely protected by intellectual property lawyers — and has even drained the interest out of realms of film and television that have nothing to do with superheroes, swords, or sci-fi. Hollywood has always been about the bottom line, of course, but only in recent decades have market saturation, cross-platform strategy, and maximum crossover potential come to dominate its priorities so completely. From the MCU or otherwise, a Marvelized movie is one that, at bottom, has no pressing need to be made — and that we, ultimately, feel no pressing need to see.&lt;/p&gt;
    &lt;p&gt;Related content:&lt;/p&gt;
    &lt;p&gt;Why Movies Don’t Feel Real Anymore: A Close Look at Changing Filmmaking Techniques&lt;/p&gt;
    &lt;p&gt;Based in Seoul, Colin Marshall writes and broadcasts on cities, language, and culture. His projects include the Substack newsletter Books on Cities and the book The Stateless City: a Walk through 21st-Century Los Angeles. Follow him on the social network formerly known as Twitter at @colinmarshall.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.openculture.com/2025/11/how-the-marvelization-of-cinema-accelerates-the-decline-of-filmmaking.html"/><published>2025-12-30T21:18:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46438216</id><title>Tatiana Schlossberg, granddaughter of John F Kennedy, dies aged 35</title><updated>2025-12-30T22:10:25.805021+00:00</updated><content>&lt;doc fingerprint="df5e711f23ea4b17"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tatiana Schlossberg, granddaughter of John F Kennedy, dies aged 35&lt;/head&gt;
    &lt;p&gt;Tatiana Schlossberg, the granddaughter of former US President John F Kennedy, has died aged 35.&lt;/p&gt;
    &lt;p&gt;Her family announced her death in a social media post shared by the John F Kennedy Library Foundation, writing: "Our beautiful Tatiana passed away this morning. She will always be in our hearts."&lt;/p&gt;
    &lt;p&gt;In November, Schlossberg, a climate journalist, announced her diagnosis of an aggressive form of cancer. She said in an essay that she had been given less than a year to live.&lt;/p&gt;
    &lt;p&gt;Schlossberg was the daughter of designer Edwin Schlossberg and diplomat Caroline Kennedy.&lt;/p&gt;
    &lt;p&gt;In an article published last month in The New Yorker titled, "A Battle With My Blood," Schlossberg revealed she had been diagnosed with acute myeloid leukaemia in May 2024, after giving birth to her second child.&lt;/p&gt;
    &lt;p&gt;"My first thought was that my kids, whose faces live permanently on the inside of my eyelids, wouldn't remember me," she wrote.&lt;/p&gt;
    &lt;p&gt;Schlossberg described the treatments she received, including chemotherapy and a bone marrow transplant, but shared that doctors did not give her a good prognosis.&lt;/p&gt;
    &lt;p&gt;She also wrote of the pain she feared her passing would cause for her family, which has endured multiple personal tragedies. Her grandfather, President Kennedy, was assassinated in 1963 and her uncle, John F Kennedy Jr, died in a plane crash in 1999.&lt;/p&gt;
    &lt;p&gt;Her younger brother, Jack Schlossberg, is running for Congress in New York.&lt;/p&gt;
    &lt;p&gt;"For my whole life, I have tried to be good, to be a good student and a good sister and a good daughter, and to protect my mother and never make her upset or angry," Schlossberg wrote.&lt;/p&gt;
    &lt;p&gt;"Now I have added a new tragedy to her life, to our family's life, and there's nothing I can do to stop it," she said.&lt;/p&gt;
    &lt;p&gt;In her essay, Schlossberg also expressed disappointment in her uncle Robert F Kennedy Jr's appointment to lead the Department of Health and Human Services.&lt;/p&gt;
    &lt;p&gt;Before her widely-read essay about her diagnosis, Schlossberg forged a successful career as a climate journalist.&lt;/p&gt;
    &lt;p&gt;Schlossberg authored the book "Inconspicuous Consumption: The Environmental Impact You Don't Know You Have". She also wrote about climate and other beats for the New York Times.&lt;/p&gt;
    &lt;p&gt;In December 2021, she reported on local experiments to harness the energy of the London Underground to provide heat to homes, in an effort to fight climate change.&lt;/p&gt;
    &lt;p&gt;"I think climate change is the biggest story in the world, and it's a story about everything," she told NBC News in 2019. "It's about science and nature, but it's also about politics and health and business. To me, looking at this as a journalist, it seemed like a really important story to tell."&lt;/p&gt;
    &lt;p&gt;"And if I could help communicate about it, that might inspire other people to get involved and work on the issue," she said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/c338ne3relzo"/><published>2025-12-30T21:27:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46438252</id><title>The moment GMV is labeled ARR, the business is built on sand</title><updated>2025-12-30T22:10:25.522301+00:00</updated><content>&lt;doc fingerprint="d6c8208893aad4c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The ARR Illusion in the Age of AI&lt;/head&gt;
    &lt;head rend="h3"&gt;— The moment GMV is labeled Annual Recurring Revenue, the business is built on sand&lt;/head&gt;
    &lt;p&gt;In recent feeds and headlines covering AI startups, a certain phrase repeats with exhausting frequency: "Reached $X million ARR in just n months."&lt;/p&gt;
    &lt;p&gt;At first, it is impressive. The second time, it invites skepticism. By the third, one inevitably arrives at the question:&lt;/p&gt;
    &lt;p&gt;"Is that actually ARR?"&lt;/p&gt;
    &lt;p&gt;This is not merely a matter of accounting tricks or semantic hair-splitting. It is a fundamental question about the structure of AI businesses, their sustainability, and how we ought to interpret the numbers presented to us.&lt;/p&gt;
    &lt;head rend="h2"&gt;What ARR Actually Is — A Concept Strict in its Simplicity&lt;/head&gt;
    &lt;p&gt;ARR stands for Annual Recurring Revenue. The two operative words here carry weight.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Annual: A full year must effectively elapse or be contractually guaranteed.&lt;/item&gt;
      &lt;item&gt;Recurring: The revenue must be structurally proven to repeat, not merely happen by chance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Therefore, ARR is not:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Last month's revenue × 12"&lt;/item&gt;
      &lt;item&gt;"This month's usage annualized"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are run-rates. ARR is not a prediction; it is a result verified by time.&lt;/p&gt;
    &lt;p&gt;Fundamentally, For a company less than a year old to claim ARR is, conceptually, a contradiction.&lt;/p&gt;
    &lt;p&gt;The year has not passed. The recurrence has not been proven.&lt;/p&gt;
    &lt;head rend="h2"&gt;Yet Everyone Speaks in ARR. Why?&lt;/head&gt;
    &lt;p&gt;The reason is simple.&lt;/p&gt;
    &lt;p&gt;ARR is the lingua franca of SaaS. And the language of SaaS invariably connotes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stability&lt;/item&gt;
      &lt;item&gt;Predictability&lt;/item&gt;
      &lt;item&gt;High Valuation Multiples&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus, many AI companies, even those far from reaching that stage, borrow the term ARR to describe their finances.&lt;/p&gt;
    &lt;p&gt;The problem begins exactly there.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Real Numbers of AI Startups — Closer to GMV than ARR&lt;/head&gt;
    &lt;p&gt;Peer slightly beneath the surface of many AI startup revenue structures, and a pattern emerges.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A significant portion of what the customer pays&lt;/item&gt;
      &lt;item&gt;Flows immediately out the door.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For instance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LLM API costs&lt;/item&gt;
      &lt;item&gt;GPU and compute expenses&lt;/item&gt;
      &lt;item&gt;Costs for external human-in-the-loop contractors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this structure, the portion the company actually retains is but a fraction of the total payment.&lt;/p&gt;
    &lt;p&gt;Here, the correct concept is GMV (Gross Merchandise Value).&lt;/p&gt;
    &lt;p&gt;GMV represents the total value of merchandise sold through a platform. The critical distinction is this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GMV is the "flow of money."&lt;/item&gt;
      &lt;item&gt;It is not the money the company earned.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If a platform mediates a transaction and takes a commission, GMV ≠ Revenue.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Core of the Problem — When GMV Wears the Mask of ARR&lt;/head&gt;
    &lt;p&gt;The illusion specific to the AI industry arises here.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Contracts are signed monthly or annually.&lt;/item&gt;
      &lt;item&gt;Billing is recurring based on usage.&lt;/item&gt;
      &lt;item&gt;Dashboards display "how much was spent this month."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On the surface, it mimics SaaS perfectly.&lt;/p&gt;
    &lt;p&gt;But in reality:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cost control lies with the model provider.&lt;/item&gt;
      &lt;item&gt;Pricing power resides externally.&lt;/item&gt;
      &lt;item&gt;The company takes only a thin slice from the middle.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this structure, calling GMV "ARR" is, An act of consuming an unproven future as a present achievement.&lt;/p&gt;
    &lt;p&gt;To put it more bluntly, it borders on a bluff designed to induce a financing illusion.&lt;/p&gt;
    &lt;head rend="h2"&gt;On the Counterargument: "Won't Compute Costs eventually fall?"&lt;/head&gt;
    &lt;p&gt;A common rebuttal exists.&lt;/p&gt;
    &lt;p&gt;"Model costs are dropping, so even if margins are thin now, they will improve."&lt;/p&gt;
    &lt;p&gt;This statement is only half true.&lt;/p&gt;
    &lt;p&gt;Technology costs do decrease. However, demand always migrates to the newest, most powerful models.&lt;/p&gt;
    &lt;p&gt;People do not:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Choose GPT-3.5 because it is cheap.&lt;/item&gt;
      &lt;item&gt;Select a lower-tier Claude model on purpose.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course, low-cost models have their place—simple tasks, non-competitive domains.&lt;/p&gt;
    &lt;p&gt;But the question remains: Is the product you are building truly that kind of product?&lt;/p&gt;
    &lt;p&gt;We are cognitively greedy. Given the same time and money, we desire the smarter brain.&lt;/p&gt;
    &lt;p&gt;Therefore,&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The assumption that "costs will fall"&lt;/item&gt;
      &lt;item&gt;Relies on the premise that "lower performance models will suffice."&lt;/item&gt;
      &lt;item&gt;A premise that rarely holds in reality.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;This is Not a Numbers Problem; It is an Identity Problem&lt;/head&gt;
    &lt;p&gt;Ultimately, the discussion boils down to one thing.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Is this a Software Company?&lt;/item&gt;
      &lt;item&gt;Or is it a low-margin Reseller/Broker?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Neither is inherently right or wrong. But how they are valued must be completely different.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For SaaS, every dollar of revenue can be a unit of value.&lt;/item&gt;
      &lt;item&gt;For Reseller/Broker models, one must judge by:&lt;/item&gt;
      &lt;item&gt;The actual take rate (net revenue).&lt;/item&gt;
      &lt;item&gt;Contribution margin.&lt;/item&gt;
      &lt;item&gt;Control over costs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A large GMV does not equate to a solid foundation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing — Growing Numbers vs. Growing a Business&lt;/head&gt;
    &lt;p&gt;The spectacle of GMV-first growth often reminds me of a familiar scene.&lt;/p&gt;
    &lt;p&gt;The investor with $10,000 of capital, who trades dozens of times a day, boasting, "I moved millions in transaction volume this month."&lt;/p&gt;
    &lt;p&gt;The trades are many. The numbers are large. But what actually remains?&lt;/p&gt;
    &lt;p&gt;Mostly transaction fees paid to the exchange. The volume swells, but the asset base barely moves.&lt;/p&gt;
    &lt;p&gt;AI startups obsessed with GMV are not so different.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transactions abound.&lt;/item&gt;
      &lt;item&gt;Money moves fast.&lt;/item&gt;
      &lt;item&gt;The numbers on the dashboard keep growing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the vast majority of that flow passes through to model providers and infrastructure giants, leaking straight out of the building.&lt;/p&gt;
    &lt;p&gt;To discuss GMV as if it were ARR in this context is not a demonstration of capital control, but rather an attempt to prove competence via the sheer volume of money that passed through one's hands.&lt;/p&gt;
    &lt;p&gt;From a consulting perspective, the questions are simple.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Is this company controlling its recurring revenue?&lt;/item&gt;
      &lt;item&gt;Or is it merely brokering atop recurring costs?&lt;/item&gt;
      &lt;item&gt;Does the margin structure improve over time?&lt;/item&gt;
      &lt;item&gt;Or does the fee burden simply grow with volume?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ARR is not a marketing slogan. It is a report card proving that a financial structure has withstood the test of time.&lt;/p&gt;
    &lt;p&gt;GMV can be a signal of growth. But it is, at best, a process metric. It cannot substitute for the foundation.&lt;/p&gt;
    &lt;p&gt;In the AI era, numbers have become larger and faster. That is precisely why we must be more honest with our terminology.&lt;/p&gt;
    &lt;p&gt;The moment you slap the label of ARR onto GMV, the business begins to look solid, but in reality, it is being built on sand.&lt;/p&gt;
    &lt;p&gt;Time will eventually reveal the truth. Whether you were a company that made many trades, or a company that accumulated value.&lt;/p&gt;
    &lt;p&gt;Time always distinguishes the two.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://oswarld.com/eng/insight/250816_ai-arr-illusion-gmv-vs-arr"/><published>2025-12-30T21:30:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46438255</id><title>U.S. cybersecurity experts plead guilty for ransomware attacks</title><updated>2025-12-30T22:10:25.194589+00:00</updated><content>&lt;doc fingerprint="376032db9a13041c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;U.S. cybersecurity experts plead guilty for ransomware attacks, face 20 years in prison each — group demanded up to $10 million from each victim&lt;/head&gt;
    &lt;p&gt;These cybersecurity experts were hired to prevent the exact attacks they perpetrated.&lt;/p&gt;
    &lt;p&gt;Two former cybersecurity experts pled guilty to conspiracy to obstruct commerce by extortion and are set to be sentenced up to 20 years in prison each for attacking several U.S. companies with ransomware and holding their data hostage for up to $10 million. BleepingComputer reported that the two offenders were former employees of Sygnia and DigitalMint — cybersecurity incident response firms that help companies that have been affected by ransomware and other cyberattacks.&lt;/p&gt;
    &lt;p&gt;“These defendants used their sophisticated cybersecurity training and experience to commit ransomware attacks — the very type of crime that they should have been working to stop,” Assistant Attorney General A. Tysen Duva said in a statement. “Extortion via the internet victimizes innocent citizens every bit as much as taking money directly out of their pockets. The Department of Justice is committed to using all tools available to identify and arrest perpetrators of ransomware attacks wherever we have jurisdiction.”&lt;/p&gt;
    &lt;p&gt;Ryan Clifford Goldberg, 40, of Watkinsville, Georgia, was a Sygnia incident response manager, while Kevin Tyler Martin, 36, of Roanoke, Texas, was a ransomware threat negotiator for DigitalMint. Another unnamed co-conspirator had the same position as Martin at DigitalMint, but they haven't been identified yet. According to the Justice Department, the three people tapped the ALPHV BlackCat ransomware-as-a-service for their activities, paying its administrators a 20% cut of their proceeds.&lt;/p&gt;
    &lt;p&gt;The three conspirators attacked several U.S. companies across different states, including those based in Maryland, California, Florida, and Virginia. Of all the victims, court records show that only a Florida-based medical device maker paid a ransom of $1.27 million — a fraction of the $10 million the group demanded from the company. After paying BlackCat’s 20% cut, the group split the remainder three ways and laundered the Bitcoin through different channels.&lt;/p&gt;
    &lt;p&gt;Neither the DOJ nor the United States Southern District Court of Florida mentioned how the two were caught, so we don’t know what led to their arrest — nevertheless, Goldberg has been in federal custody since September 2023. The FBI Miami Field Office was the main agency behind this investigation, and has been assisted by the U.S. Secret Service. Aside from their arrest, indictment, and sentencing, the Southern District of Florida is also handling the asset forfeiture case, meaning the perpetrators will likely yield the proceeds of their crime to the victims or the state.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Jowi Morales is a tech enthusiast with years of experience working in the industry. He’s been writing with several tech publications since 2021, where he’s been interested in tech hardware and consumer electronics.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/tech-industry/cyber-security/u-s-cybersecurity-experts-plead-guilty-for-ransomware-attacks-face-20-years-in-prison-each-group-demanded-up-to-usd10-million-from-each-victim"/><published>2025-12-30T21:31:00+00:00</published></entry></feed>