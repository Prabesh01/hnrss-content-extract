<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-30T23:09:25.450170+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46096556</id><title>Windows drive letters are not limited to A-Z</title><updated>2025-11-30T23:09:34.714790+00:00</updated><content>&lt;doc fingerprint="c2841e11e92da09d"&gt;
  &lt;main&gt;&lt;p&gt;On its own, the title of this post is just a true piece of trivia, verifiable with the built-in &lt;code&gt;subst&lt;/code&gt; tool (among other methods).&lt;/p&gt;&lt;p&gt;Here's an example creating the drive &lt;code&gt;+:\&lt;/code&gt; as an alias for a directory at &lt;code&gt;C:\foo&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;subst +: C:\foo
&lt;/code&gt;&lt;p&gt;The &lt;code&gt;+:\&lt;/code&gt; drive then works as normal (at least in cmd.exe, this will be discussed more later):&lt;/p&gt;&lt;code&gt;&amp;gt; cd /D +:\

+:\&amp;gt; tree .
Folder PATH listing
Volume serial number is 00000001 12AB:23BC
+:\
√¢√¢√¢√¢bar
&lt;/code&gt;&lt;p&gt;However, understanding why it's true elucidates a lot about how Windows works under the hood, and turns up a few curious behaviors.&lt;/p&gt;&lt;p&gt;The paths that most people are familiar with are Win32 namespace paths, e.g. something like &lt;code&gt;C:\foo&lt;/code&gt; which is a drive-absolute Win32 path. However, the high-level APIs that take Win32 paths like &lt;code&gt;CreateFileW&lt;/code&gt; ultimately will convert a path like &lt;code&gt;C:\foo&lt;/code&gt; into a NT namespace path before calling into a lower level API within &lt;code&gt;ntdll.dll&lt;/code&gt; like &lt;code&gt;NtCreateFile&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;This can be confirmed with NtTrace, where a call to &lt;code&gt;CreateFileW&lt;/code&gt; with &lt;code&gt;C:\foo&lt;/code&gt; ultimately leads to a call of &lt;code&gt;NtCreateFile&lt;/code&gt; with &lt;code&gt;\??\C:\foo&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;NtCreateFile( FileHandle=0x40c07ff640 [0xb8], DesiredAccess=SYNCHRONIZE|GENERIC_READ|0x80, ObjectAttributes="\??\C:\foo", IoStatusBlock=0x40c07ff648 [0/1], AllocationSize=null, FileAttributes=0, ShareAccess=7, CreateDisposition=1, CreateOptions=0x4000, EaBuffer=null, EaLength=0 ) =&amp;gt; 0
NtClose( Handle=0xb8 ) =&amp;gt; 0
&lt;/code&gt;&lt;p&gt;&lt;code&gt;createfilew.zig&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;const std = @import("std");
const windows = std.os.windows;
const L = std.unicode.wtf8ToWtf16LeStringLiteral;

pub extern "kernel32" fn CreateFileW(
    lpFileName: windows.LPCWSTR,
    dwDesiredAccess: windows.DWORD,
    dwShareMode: windows.DWORD,
    lpSecurityAttributes: ?*windows.SECURITY_ATTRIBUTES,
    dwCreationDisposition: windows.DWORD,
    dwFlagsAndAttributes: windows.DWORD,
    hTemplateFile: ?windows.HANDLE,
) callconv(.winapi) windows.HANDLE;

pub fn main() !void {
    const path = L("C:\\foo");
    const dir_handle = CreateFileW(
        path,
        windows.GENERIC_READ,
        windows.FILE_SHARE_DELETE | windows.FILE_SHARE_READ | windows.FILE_SHARE_WRITE,
        null,
        windows.OPEN_EXISTING,
        windows.FILE_FLAG_BACKUP_SEMANTICS | windows.FILE_FLAG_OVERLAPPED,
        null,
    );
    if (dir_handle == windows.INVALID_HANDLE_VALUE) return error.FailedToOpenDir;
    defer windows.CloseHandle(dir_handle);
}
&lt;/code&gt;&lt;p&gt;Built with:&lt;/p&gt;&lt;code&gt;zig build-exe createfilew.zig
&lt;/code&gt;&lt;p&gt;To run with NtTrace:&lt;/p&gt;&lt;code&gt;nttrace createfilew.exe &amp;gt; createfilew.log
&lt;/code&gt;&lt;p&gt;That &lt;code&gt;\??\C:\foo&lt;/code&gt; is a NT namespace path, which is what &lt;code&gt;NtCreateFile&lt;/code&gt; expects. To understand this path, though, we need to talk about the Object Manager, which is responsible for handling NT paths.&lt;/p&gt;&lt;p&gt;The Object Manager is responsible for keeping track of named objects, which we can explore using the WinObj tool. The &lt;code&gt;\??&lt;/code&gt; part of the &lt;code&gt;\??\C:\foo&lt;/code&gt; path is actually a special virtual folder within the Object Manager that combines the &lt;code&gt;\GLOBAL??&lt;/code&gt; folder and a per-user &lt;code&gt;DosDevices&lt;/code&gt; folder together.&lt;/p&gt;&lt;p&gt;For me, the object &lt;code&gt;C:&lt;/code&gt; is within &lt;code&gt;\GLOBAL??&lt;/code&gt;, and is actually a symbolic link to &lt;code&gt;\Device\HarddiskVolume4&lt;/code&gt;:&lt;/p&gt;&lt;p&gt;So, &lt;code&gt;\??\C:\foo&lt;/code&gt; ultimately resolves to &lt;code&gt;\Device\HarddiskVolume4\foo&lt;/code&gt;, and then it's up to the actual device to deal with the &lt;code&gt;foo&lt;/code&gt; part of the path.&lt;/p&gt;&lt;p&gt;The important thing here, though, is that &lt;code&gt;\??\C:\foo&lt;/code&gt; is just one way of referring to the device path &lt;code&gt;\Device\HarddiskVolume4\foo&lt;/code&gt;. For example, volumes will also get a named object created using their GUID with the format &lt;code&gt;Volume{18123456-abcd-efab-cdef-1234abcdabcd}&lt;/code&gt; that is also a symlink to something like &lt;code&gt;\Device\HarddiskVolume4&lt;/code&gt;, so a path like &lt;code&gt;\??\Volume{18123456-abcd-efab-cdef-1234abcdabcd}\foo&lt;/code&gt; is effectively equivalent to &lt;code&gt;\??\C:\foo&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;All this is to say that there's nothing innately special about the named object &lt;code&gt;C:&lt;/code&gt;; the Object Manager treats it just like any other symbolic link and resolves it accordingly.&lt;/p&gt;&lt;p&gt;How I see it, drive letters are essentially just a convention borne out of the conversion of a Win32 path into a NT path. In particular, that would be down to the implementation of &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In other words, since &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; converts &lt;code&gt;C:\foo&lt;/code&gt; to &lt;code&gt;\??\C:\foo&lt;/code&gt;, then an object named &lt;code&gt;C:&lt;/code&gt; will behave like a drive letter. To give an example of what I mean by that: in an alternate universe, &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; could convert the path &lt;code&gt;FOO:\bar&lt;/code&gt; to &lt;code&gt;\??\FOO:\bar&lt;/code&gt; and then &lt;code&gt;FOO:&lt;/code&gt; could behave like a drive letter.&lt;/p&gt;&lt;p&gt;So, getting back to the title, how does &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; treat something like &lt;code&gt;+:\foo&lt;/code&gt;? Well, exactly the same as &lt;code&gt;C:\foo&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;&amp;gt; paths.exe C:\foo
path type: .DriveAbsolute
  nt path: \??\C:\foo

&amp;gt; paths.exe +:\foo
path type: .DriveAbsolute
  nt path: \??\+:\foo
&lt;/code&gt;&lt;p&gt;&lt;code&gt;paths.zig&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;const std = @import("std");
const windows = std.os.windows;

pub fn main() !void {
    var arena_state = std.heap.ArenaAllocator.init(std.heap.page_allocator);
    defer arena_state.deinit();
    const arena = arena_state.allocator();

    const args = try std.process.argsAlloc(arena);
    if (args.len &amp;lt;= 1) return error.ExpectedArg;

    const path = try std.unicode.wtf8ToWtf16LeAllocZ(arena, args[1]);

    const path_type = RtlDetermineDosPathNameType_U(path);
    std.debug.print("path type: {}\n", .{path_type});
    const nt_path = try RtlDosPathNameToNtPathName_U(path);
    std.debug.print("  nt path: {f}\n", .{std.unicode.fmtUtf16Le(nt_path.span())});
}

const RTL_PATH_TYPE = enum(c_int) {
    Unknown,
    UncAbsolute,
    DriveAbsolute,
    DriveRelative,
    Rooted,
    Relative,
    LocalDevice,
    RootLocalDevice,
};

pub extern "ntdll" fn RtlDetermineDosPathNameType_U(
    Path: [*:0]const u16,
) callconv(.winapi) RTL_PATH_TYPE;

fn RtlDosPathNameToNtPathName_U(path: [:0]const u16) !windows.PathSpace {
    var out: windows.UNICODE_STRING = undefined;
    const rc = windows.ntdll.RtlDosPathNameToNtPathName_U(path, &amp;amp;out, null, null);
    if (rc != windows.TRUE) return error.BadPathName;
    defer windows.ntdll.RtlFreeUnicodeString(&amp;amp;out);

    var path_space: windows.PathSpace = undefined;
    const out_path = out.Buffer.?[0 .. out.Length / 2];
    @memcpy(path_space.data[0..out_path.len], out_path);
    path_space.len = out.Length / 2;
    path_space.data[path_space.len] = 0;

    return path_space;
}
&lt;/code&gt;&lt;p&gt;Therefore, if an object with the name &lt;code&gt;+:&lt;/code&gt; is within the virtual folder &lt;code&gt;\??&lt;/code&gt;, we can expect the Win32 path &lt;code&gt;+:\&lt;/code&gt; to behave like any other drive-absolute path, which is exactly what we see.&lt;/p&gt;&lt;p&gt;This section only focuses on a few things that were relevant to what I was working on. I encourage others to investigate the implications of this further if they feel so inclined.&lt;/p&gt;&lt;code&gt;explorer.exe&lt;/code&gt; doesn't play ball√∞&lt;p&gt;Drives with a drive-letter other than A-Z do not appear in File Explorer, and cannot be navigated to in File Explorer.&lt;/p&gt;&lt;code&gt;+:\&lt;/code&gt; in File Explorer
&lt;p&gt;For the "do not appear" part, my guess as to what's happening is that &lt;code&gt;explorer.exe&lt;/code&gt; is walking &lt;code&gt;\??&lt;/code&gt; and looking specifically for objects named &lt;code&gt;A:&lt;/code&gt; through &lt;code&gt;Z:&lt;/code&gt;. For the "cannot be navigated to" part, that's a bit more mysterious, but my guess is that &lt;code&gt;explorer.exe&lt;/code&gt; has a lot of special logic around handling paths typed into the location bar, and part of that restricts drive letters to &lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; (i.e. it's short-circuiting before it ever tries to actually open the path).&lt;/p&gt;&lt;p&gt;PowerShell seems to reject non-&lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; drives as well:&lt;/p&gt;&lt;code&gt;PS C:\&amp;gt; cd +:\
cd : Cannot find drive. A drive with the name '+' does not exist.
At line:1 char:1
+ cd +:\
+ ~~~~~~
    + CategoryInfo          : ObjectNotFound: (+:String) [Set-Location], DriveNotFoundException
    + FullyQualifiedErrorId : DriveNotFound,Microsoft.PowerShell.Commands.SetLocationCommand
&lt;/code&gt;
&lt;p&gt;Drive letters don't have to be within the ASCII range at all; they can also be non-ASCII characters.&lt;/p&gt;&lt;code&gt;&amp;gt; subst √¢¬¨: C:\foo

&amp;gt; cd /D √¢¬¨:\

√¢¬¨:\&amp;gt; tree .
Folder PATH listing
Volume serial number is 000000DE 12AB:23BC
√¢¬¨:\
√¢√¢√¢√¢bar
&lt;/code&gt;
&lt;p&gt;Non-ASCII drive letters are even case-insensitive like &lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; are:&lt;/p&gt;&lt;code&gt;&amp;gt; subst √é: C:\foo

&amp;gt; cd /D √é¬ª:\

√é¬ª:\&amp;gt; tree .
Folder PATH listing
Volume serial number is 000000DE 12AB:23BC
√é¬ª:\
√¢√¢√¢√¢bar
&lt;/code&gt;
&lt;p&gt;However, drive-letters cannot be arbitrary Unicode graphemes or even arbitrary code points; they are restricted to a single WTF-16 code unit (a &lt;code&gt;u16&lt;/code&gt;, so &amp;lt;= &lt;code&gt;U+FFFF&lt;/code&gt;). The tool that we've been using so far (&lt;code&gt;subst.exe&lt;/code&gt;) errors with &lt;code&gt;Invalid parameter&lt;/code&gt; if you try to use a drive letter with a code point larger than &lt;code&gt;U+FFFF&lt;/code&gt;, but you can get around that by going through the &lt;code&gt;MountPointManager&lt;/code&gt; directly:&lt;/p&gt;&lt;code&gt;√∞¬§¬¢:&lt;/code&gt; symlink&lt;code&gt;const std = @import("std");
const windows = std.os.windows;
const L = std.unicode.wtf8ToWtf16LeStringLiteral;

const MOUNTMGR_CREATE_POINT_INPUT = extern struct {
    SymbolicLinkNameOffset: windows.USHORT,
    SymbolicLinkNameLength: windows.USHORT,
    DeviceNameOffset: windows.USHORT,
    DeviceNameLength: windows.USHORT,
};

pub fn main() !void {
    const mgmt_handle = try windows.OpenFile(L("\\??\\MountPointManager"), .{
        .access_mask = windows.SYNCHRONIZE | windows.GENERIC_READ | windows.GENERIC_WRITE,
        .share_access = windows.FILE_SHARE_READ | windows.FILE_SHARE_WRITE | windows.FILE_SHARE_DELETE,
        .creation = windows.FILE_OPEN,
    });
    defer windows.CloseHandle(mgmt_handle);

    const volume_name = L("\\Device\\HarddiskVolume4");
    const mount_point = L("\\DosDevices\\√∞¬§¬¢:");

    const buf_size = @sizeOf(MOUNTMGR_CREATE_POINT_INPUT) + windows.MAX_PATH * 2 + windows.MAX_PATH * 2;
    var input_buf: [buf_size]u8 align(@alignOf(MOUNTMGR_CREATE_POINT_INPUT)) = [_]u8{0} ** buf_size;

    var input_struct: *MOUNTMGR_CREATE_POINT_INPUT = @ptrCast(&amp;amp;input_buf[0]);
    input_struct.SymbolicLinkNameOffset = @sizeOf(MOUNTMGR_CREATE_POINT_INPUT);
    input_struct.SymbolicLinkNameLength = mount_point.len * 2;
    input_struct.DeviceNameOffset = input_struct.SymbolicLinkNameOffset + input_struct.SymbolicLinkNameLength;
    input_struct.DeviceNameLength = volume_name.len * 2;

    @memcpy(input_buf[input_struct.SymbolicLinkNameOffset..][0..input_struct.SymbolicLinkNameLength], @as([*]const u8, @ptrCast(mount_point)));
    @memcpy(input_buf[input_struct.DeviceNameOffset..][0..input_struct.DeviceNameLength], @as([*]const u8, @ptrCast(volume_name)));

    const IOCTL_MOUNTMGR_CREATE_POINT = windows.CTL_CODE(windows.MOUNTMGRCONTROLTYPE, 0, .METHOD_BUFFERED, windows.FILE_READ_ACCESS | windows.FILE_WRITE_ACCESS);
    try windows.DeviceIoControl(mgmt_handle, IOCTL_MOUNTMGR_CREATE_POINT, &amp;amp;input_buf, null);
}
&lt;/code&gt;
&lt;p&gt;(the compiled executable must be run as administrator)&lt;/p&gt;&lt;p&gt;However, having the symlink in place doesn't solve anything on its own:&lt;/p&gt;&lt;code&gt;&amp;gt; cd /D √∞¬§¬¢:\
The filename, directory name, or volume label syntax is incorrect.
&lt;/code&gt;
&lt;p&gt;This is because there's no way to get the drive-absolute Win32 path &lt;code&gt;√∞¬§¬¢:\&lt;/code&gt; to end up as the relevant NT path. As mentioned earlier, the behavior of &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; is what matters, and we can verify that it will not convert a drive-absolute path with a drive letter bigger than &lt;code&gt;U+FFFF&lt;/code&gt; to the relevant NT path:&lt;/p&gt;&lt;code&gt;C:\foo&amp;gt; paths.exe √∞¬§¬¢:\foo
path type: .Relative
  nt path: \??\C:\foo\√∞¬§¬¢:\foo
&lt;/code&gt;

&lt;p&gt;It's very common for path-related functions to be written without the use of system-specific APIs, which means that there's high potential for a mismatch between how &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; treats a file path and how something like a particular implementation of &lt;code&gt;path.isAbsolute&lt;/code&gt; treats a file path.&lt;/p&gt;&lt;p&gt;As a random example, Rust only considers paths with &lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; drive letters as absolute:&lt;/p&gt;&lt;code&gt;use std::path::Path;

fn main() {
    println!("C:\\ {}", Path::new("C:\\foo").is_absolute());
    println!("+:\\ {}", Path::new("+:\\foo").is_absolute());
    println!("√¢¬¨:\\ {}", Path::new("√¢¬¨:\\foo").is_absolute());
}
&lt;/code&gt;
&lt;code&gt;&amp;gt; rustc test.rs

&amp;gt; test.exe
C:\ true
+:\ false
√¢¬¨:\ false
&lt;/code&gt;
&lt;p&gt;Whether or not this represents a problem worth fixing is left as an exercise for the reader (I genuinely don't know if it is a problem), but there's a second wrinkle (hinted at previously) involving text encoding that can make something like an &lt;code&gt;isAbsolute&lt;/code&gt; implementation return different results for the same path. This wrinkle is the reason I looked into this whole thing in the first place, as when I was doing some work on Zig's path-related functions recently I realized that looking at &lt;code&gt;path[0]&lt;/code&gt;, &lt;code&gt;path[1]&lt;/code&gt;, and &lt;code&gt;path[2]&lt;/code&gt; for a pattern like &lt;code&gt;C:\&lt;/code&gt; will look at different parts of the path depending on the encoding. That is, for something like &lt;code&gt;√¢¬¨:\&lt;/code&gt; (which is made up of the code points &lt;code&gt;&amp;lt;U+20AC&amp;gt;&amp;lt;U+003A&amp;gt;&amp;lt;U+005C&amp;gt;&lt;/code&gt;):&lt;/p&gt;&lt;code&gt;U+20AC&lt;/code&gt; can be encoded as the single &lt;code&gt;u16&lt;/code&gt; code unit &lt;code&gt;0x20AC&lt;/code&gt;, that'd mean &lt;code&gt;path[0]&lt;/code&gt; will be &lt;code&gt;0x20AC&lt;/code&gt;, &lt;code&gt;path[1]&lt;/code&gt; will be &lt;code&gt;0x3A&lt;/code&gt; (&lt;code&gt;:&lt;/code&gt;), and &lt;code&gt;path[2]&lt;/code&gt; will be &lt;code&gt;0x5C&lt;/code&gt; (&lt;code&gt;\&lt;/code&gt;), which looks like a drive-absolute path&lt;code&gt;U+20AC&lt;/code&gt; is encoded as three &lt;code&gt;u8&lt;/code&gt; code units (&lt;code&gt;0xE2 0x82 0xAC&lt;/code&gt;), that'd mean &lt;code&gt;path[0]&lt;/code&gt; will be &lt;code&gt;0xE2&lt;/code&gt;, &lt;code&gt;path[1]&lt;/code&gt; will be &lt;code&gt;0x82&lt;/code&gt;, and &lt;code&gt;path[2]&lt;/code&gt; will be &lt;code&gt;0xAC&lt;/code&gt;, meaning it will look nothing like a drive-absolute path&lt;p&gt;So, to write an implementation that treats paths the same regardless of encoding, some decision has to be made:&lt;/p&gt;&lt;code&gt;RtlDetermineDosPathNameType_U&lt;/code&gt;/&lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; is desired, decode the first code point and check for &lt;code&gt;&amp;lt;= 0xFFFF&lt;/code&gt; when dealing with WTF-8 (this is the option I went with for the Zig standard library, but I'm not super happy about it)&lt;code&gt;path[0]&lt;/code&gt;/&lt;code&gt;path[1]&lt;/code&gt;/&lt;code&gt;path[2]&lt;/code&gt; and don't care about non-ASCII drive letters, check for &lt;code&gt;path[0] &amp;lt;= 0x7F&lt;/code&gt; regardless of encoding&lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; drive letters, then check for that explicitly (this is what Rust does)&lt;p&gt;Something bizarre that I found with this whole thing is that the &lt;code&gt;kernel32.dll&lt;/code&gt; API &lt;code&gt;SetVolumeMountPointW&lt;/code&gt; has it's own unique quirk when dealing with non-ASCII drive letters. Specifically, this code (attempting to create the drive &lt;code&gt;√¢¬¨:\&lt;/code&gt;) will succeed:&lt;/p&gt;&lt;code&gt;const std = @import("std");
const windows = std.os.windows;
const L = std.unicode.wtf8ToWtf16LeStringLiteral;

extern "kernel32" fn SetVolumeMountPointW(
    VolumeMountPoint: windows.LPCWSTR,
    VolumeName: windows.LPCWSTR,
) callconv(.winapi) windows.BOOL;

pub fn main() !void {
    const volume_name = L("\\\\?\\Volume{18123456-abcd-efab-cdef-1234abcdabcd}\\");
    const mount_point = L("√¢¬¨:\\");
    if (SetVolumeMountPointW(mount_point, volume_name) == 0) {
        const err = windows.GetLastError();
        std.debug.print("{any}\n", .{err});
        return error.Failed;
    }
}
&lt;/code&gt;
&lt;p&gt;However, when we look at the Object Manager, the &lt;code&gt;√¢¬¨:&lt;/code&gt; symlink won't exist... but &lt;code&gt;√Ç¬¨:&lt;/code&gt; will:&lt;/p&gt;&lt;p&gt;My time dealing extensively with Windows quirks made me recognize what might be happening here: &lt;code&gt;0x20AC&lt;/code&gt; is likely being truncated to &lt;code&gt;0xAC&lt;/code&gt; by &lt;code&gt;SetVolumeMountPointW&lt;/code&gt;, and &lt;code&gt;U+00AC&lt;/code&gt; happens to be &lt;code&gt;√Ç¬¨&lt;/code&gt;. If that is indeed what's going on, it seems pretty strange to truncate the drive letter instead of reject the path, but it also makes sense that non-ASCII drive letters are an edge case that no one has really thought about at all.&lt;/p&gt;&lt;p&gt;I have no idea if anything I wrote about here is novel, although my cursory searches didn't turn up much. The only mention of non-&lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; drive letters I'm currently aware of is from the article The Definitive Guide on Win32 to NT Path Conversion which says:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;it's natural to assume that drive "letters" can only be A through Z. It turns out the&lt;/p&gt;&lt;code&gt;RtlGetFullPathName_U&lt;/code&gt;API does not enforce this requirement, although the Explorer shell and command prompt almost certainly do. Therefore as long as the second character of a path is a colon, the conversion will treat it as a Drive Absolute or Drive Relative path. Of course if the DosDevices object directory doesn't have an appropriate symbolic link it's not going to do you much good.&lt;/quote&gt;&lt;p&gt;Well, it turns out that the command prompt also doesn't enforce the requirement, and I'd guess that there's at least some more weirdness around this quirk that's waiting to be discovered.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ryanliptak.com/blog/windows-drive-letters-are-not-limited-to-a-z/"/><published>2025-11-30T13:40:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46096800</id><title>Migrating Dillo from GitHub</title><updated>2025-11-30T23:09:34.077517+00:00</updated><content>&lt;doc fingerprint="af008dd98a923a96"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Migrating Dillo from GitHub&lt;/head&gt;Written on 2025-11-30 by Rodrigo Arias Mallo&lt;p&gt;I would like to migrate the Dillo project away from GitHub into a new home which is more friendly to be used with Dillo and solves some of its problems. This page summarizes the current situation with GitHub and why I decided to move away from it into a self-hosted server with multiple mirrors in other forges.&lt;/p&gt;&lt;head rend="h2"&gt;Background&lt;/head&gt;&lt;p&gt;Before we dive into the details, I would like to briefly mention what happened with the old site. The original Dillo website was at dillo.org, which also had the source code of Dillo in a mercurial repository at hg.dillo.org. But it also included the mail server used to reach the developers, a bug tracker and archives for the mailing list. However, in 2022 the domain was lost and someone else decided to buy it to put a similar site but plaged with AI generated ads. The original developers are no longer active, but luckily I had a copy of the mercurial repository and with some help I was able to recover a lot of material from the original server (some parts are still missing to this day).&lt;/p&gt;&lt;p&gt;I want to avoid this situation as much as possible, so we cannot rely on a single site that can go down and the whole project become lost. Initially, I uploaded the Dillo source and website to git repositories on GitHub, but I no longer think this is a good idea.&lt;/p&gt;&lt;head rend="h2"&gt;The situation with GitHub&lt;/head&gt;&lt;p&gt;GitHub has been useful to store all repositories of the Dillo project, as well as to run the CI workflows for platforms in which I don't have a machine available (like Windows, Mac OS or some BSDs).&lt;/p&gt;&lt;p&gt;However, it has several problems that make it less suitable to develop Dillo anymore. The most annoying problem is that the frontend barely works without JavaScript, so we cannot open issues, pull requests, source code or CI logs in Dillo itself, despite them being mostly plain HTML, which I don't think is acceptable. In the past, it used to gracefully degrade without enforcing JavaScript, but now it doesn't. Additionally, the page is very resource hungry, which I don't think is needed to render mostly static text.&lt;/p&gt;&lt;p&gt;Another big problem is that it is a single point of failure. I don't mean that GitHub is stored in a single machine, but it is controlled by a single entity which can unilateraly ban our repository or account and we would lose the ability to notify in that URL what happened. This can cause data loss if we don't have a local copy of all the data.&lt;/p&gt;&lt;p&gt;On the usability side, the platform has become more and more slow over time, which is affecting the development process. It also requires you to have a fast Internet connection at all times, which is not the case for me sometimes. Additionally, GitHub seems to encourage a "push model" in which you are notified when a new event occurs in your project(s), but I don't want to work with that model. Instead, I prefer it to work as a "pull model", so I only get updates when I specifically look for them. This model would also allow me to easily work offline. Unfortunately, I see that the same push model has been copied to alternative forges.&lt;/p&gt;&lt;p&gt;On the social side, I feel that it doesn't have the right tools to moderate users, specially for projects where the ratio of non-technical users to developers is high. This is specially problematic when active issues with developer notes begin to be filled with comments from users that have never contributed to the project and usually do more harm than good. This situation ends up causing burnout in developers.&lt;/p&gt;&lt;p&gt;Lastly, GitHub seem to follow the current trend of over-focusing on LLMs and generative AI, which are destroying the open web (or what remains of it) among other problems. It has a direct impact on us because sites protect themseves with a JavaScript wall (or worse, browser fingerprinting) to prevent aggresive LLM crawler bots from overloading the site, but they also leave Dillo users out. So I would prefer not to encourage this trend. Despite my intentions, moving Dillo away won't change much their capability to train their model with our code, but at least I won't be actively helping.&lt;/p&gt;&lt;head rend="h2"&gt;Self-hosting Dillo&lt;/head&gt;&lt;p&gt;After researching the available options, it seems that none of the current forges would allow us to have a redundant system that can prevent the forge from becoming a single point of failure and solve the rest of the problems with GitHub. Therefore, I decided to self-host Dillo myself, move all important data to git repositories and keep them synchronized in multiple git mirrors.&lt;/p&gt;&lt;p&gt;I decided to buy the dillo-browser.org domain name and setup a very small VPS. Initially, I was very skeptical that it would be able to survive on today's web, but it seems to be doing an acceptable job at handling it (mostly AI bot traffic masquerading as users). The Dillo website is available here:&lt;/p&gt;&lt;p&gt;I researched which git frontends may suit our needs, and I discovered that most options are very complicated to self-host and require a lot of server resources and JavaScript on the frontend. I ended up testing cgit, which is written in C and it seems to be very lightweight both on RAM and CPU usage. Furthermore, the web frontend doesn't require JS, so I can use it from Dillo (I modified cgit CSS slightly to work well on Dillo). It is available on this URL:&lt;/p&gt;&lt;p&gt;https://git.dillo-browser.org/&lt;/p&gt;&lt;p&gt;Regarding the bug tracker, I also took a look at the available options. They are all too complicated for what I would like to have and they seem to centralize the data into a database that can get lost. This is precisely the case that happened with the old dillo bug tracker and we are still unable to recover the original bug entries.&lt;/p&gt;&lt;p&gt;To avoid this problem, I created my own bug tracker software, buggy, which is a very simple C tool that parses plain Markdown files and creates a single HTML page for each bug. All bugs are stored in a git repository and a git hook regenerates the bug pages and the index on each new commit. As it is simply plain text, I can edit the bugs locally and only push them to the remote when I have Internet back, so it works nice offline. Also, as the output is just an static HTML site, I don't need to worry about having any vulnerabilities in my code, as it will only run at build time. You can see it live here, with the exported issues from GitHub:&lt;/p&gt;&lt;p&gt;https://bug.dillo-browser.org/&lt;/p&gt;&lt;p&gt;The mailing list archives are stored by three independent external services, but I might include a copy with our own archives in the future.&lt;/p&gt;&lt;head rend="h2"&gt;Setting up mirrors&lt;/head&gt;&lt;p&gt;As all the important data is now stored in git repositories, we can mirror them in any forge, without having to rely on their custom storage format for the issues or other data. If a forge goes down (or goes rogue) we can simply switch to another site with low switching cost. To this end, I have created git mirrors in Codeberg and Sourcehut that are synced with our git server:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Codeberg: https://codeberg.org/dillo/&lt;/item&gt;&lt;item&gt;Sourcehut: https://git.sr.ht/~dillo/&lt;/item&gt;&lt;/list&gt;&lt;p&gt;However, we still have a single point of failure: the DNS entry of the dillo-browser.org domain. If we lose the DNS entry (like with dillo.org) it would cause a problem as all services will be unreachable. We could recover from such situation by relying on alternative ways to reach users, by the mailing list, fediverse or IRC, as well as updating the mirrors to reflect the current situation. It is not ideal, but I don't think it would cause a catastrophic data loss (like it happened before) as all the data is now stored in git and replicated across independent locations.&lt;/p&gt;&lt;head rend="h2"&gt;OpenPGP signature&lt;/head&gt;&lt;p&gt;In order for this page to have some authority, the HTML file is signed with my GPG key (32E65EC501A1B6FDF8190D293EE6BA977EB2A253), which is the same that I use to sign the last releases of Dillo and is also listed in my GitHub user. The signature is available here and is linked to the page with the &lt;code&gt;&amp;lt;link&amp;gt;&lt;/code&gt; tag using the &lt;code&gt;rel=signature&lt;/code&gt;
relation. You can find more information and how to verify the signature in the
Dillo RFC-006.

&lt;/p&gt;&lt;p&gt;Using OpenPGP signatures is robust against losing the DNS entry, as the authority is not given by the TLS certificate chain but by the trust in the OpenPGP signature, so we could move the site elsewhere and still claim that is owned by us. Additionally, as we can store the signatures inside all git mirrors, they are also resilient against data loss.&lt;/p&gt;&lt;head rend="h2"&gt;Closing remarks&lt;/head&gt;&lt;p&gt;Keep in mind that the migration process requires several moving parts and it will take a while for it to stabilize (switching costs). The GitHub repositories won't be removed at any point in time and they will continue to be updated until we finish the migration. When the migration process is completed, I will mark the Dillo repositories as archived and properly comunicate it in our site. It is important that we don't remove any commit or tarball release to avoid breaking downstream builds that still rely on the GitHub URL.&lt;/p&gt;&lt;p&gt;Lastly, I'm glad that we can have our own fully independent and self-hosted site with relatively low expenses and very little energy cost (which is good for the environment, but probably not even noticeable at large scale). With the current DNS and server costs and our current donations I consider that it is likely that we can continue covering the expenses for at least the next 3 years in the worst case scenario. If you are interested in keeping us afloat, you can help via Liberapay.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dillo-browser.org/news/migration-from-github/"/><published>2025-11-30T14:11:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46097671</id><title>Langjam Gamejam: Build a programming language then make a game with it</title><updated>2025-11-30T23:09:33.900384+00:00</updated><content>&lt;doc fingerprint="dda86e398bfb3013"&gt;
  &lt;main&gt;
    &lt;p&gt;Langjam Gamejam is a 7-day challenge to create a programming language and then use that language to build a game. You set the rules. Be as creative as possible, use any technologies you want, and have fun. There will be prizes for the most creative submissions!&lt;/p&gt;
    &lt;head rend="h2"&gt;Rules&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The hackathon runs for 7 days&lt;/item&gt;
      &lt;item&gt;You must design and implement a programming language&lt;/item&gt;
      &lt;item&gt;You must design and implement a game using your language&lt;/item&gt;
      &lt;item&gt;You can use any language, engine, libraries, and technologies&lt;/item&gt;
      &lt;item&gt;You define what a programming language is and what a game is&lt;/item&gt;
      &lt;item&gt;Work solo or on a team&lt;/item&gt;
      &lt;item&gt;Documentation and instructions are encouraged&lt;/item&gt;
      &lt;item&gt;Bonus points: Write a blog post about your language, game, and design process&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Join the jam and submit on Itch.io.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://langjamgamejam.com/"/><published>2025-11-30T15:57:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46097773</id><title>The Thinking Game Film ‚Äì Google DeepMind documentary</title><updated>2025-11-30T23:09:33.604749+00:00</updated><content>&lt;doc fingerprint="72a77c532d96d321"&gt;
  &lt;main&gt;
    &lt;p&gt;Stay Updated Sign up with your email address to receive news and updates. Email Address Sign Up We respect your privacy. Thank you!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thinkinggamefilm.com"/><published>2025-11-30T16:07:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46097829</id><title>GitHub to Codeberg: my experience</title><updated>2025-11-30T23:09:33.006909+00:00</updated><content>&lt;doc fingerprint="9a9c101d0be75ac8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GitHub ‚Üí Codeberg: my experience&lt;/head&gt;
    &lt;p&gt;Published . Estimated reading time: 11 minutes.&lt;/p&gt;
    &lt;p&gt;In which I talk about the process involved in switching forges, and how well that went.&lt;/p&gt;
    &lt;p&gt;Spoiler alert: this very site that you‚Äôre reading this on is not served from GitHub Pages anymore! At this point, I‚Äôd call my migration successful. But it took more than clicking a single button, so let‚Äôs talk about the steps involved, at least for me. I‚Äôm hoping that it can help be an example for other people, and show that it‚Äôs actually not that complicated.&lt;/p&gt;
    &lt;head rend="h2"&gt;(My) migration process&lt;/head&gt;
    &lt;p&gt;First, I took an hour or so to set up my profile picture, email address(es), SSH keys‚Ä¶&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 1: migrating the repos&lt;/head&gt;
    &lt;p&gt;This wasn‚Äôt difficult, because Forgejo (the forge software that powers Codeberg) offers a ‚Äúmigrate from GitHub‚Äù functionality. You need to generate a PAT on GitHub to import things like issues (which is awesome!), and as a bonus it also speeds up the process.&lt;/p&gt;
    &lt;p&gt;It was, however, tedious, because the process was entirely manual (perhaps there‚Äôs a way to automate it, like by using some Forgejo CLI tool, but I didn‚Äôt bother looking into that). And, due to GitHub API rate limits, whenever I tried importing two repos at the same time, one or both would fail. (It wasn‚Äôt too bad, though, since I could fill out the migration page for the next while one was in progress; and generally, it took me roughly as long to fill it out as it took Codeberg to perform the import.)&lt;/p&gt;
    &lt;p&gt;I‚Äôm really happy that issues, PRs, wikis, and releases can be imported flawlessly: this makes it possible to not have to refer to GitHub anymore!&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 2: repointing links to Codeberg&lt;/head&gt;
    &lt;p&gt;Of course I don‚Äôt control all links that point to my stuff, but I could at least run &lt;code&gt;rg -F github.com/ISSOtm&lt;/code&gt; in my home directory, to catch those within my own repos. It‚Äôs possible to automate the replacing process:&lt;/p&gt;
    &lt;p&gt;‚Ä¶and if you‚Äôre feeling like bulk-replacing all files in a directory:&lt;/p&gt;
    &lt;p&gt;Repositories, however, may still be pointing to GitHub:&lt;/p&gt;
    &lt;code&gt; 
 )
 )
&lt;/code&gt;
    &lt;p&gt;You can either manually &lt;code&gt;git remote set-url origin git@codeberg.org:ISSOtm/rsgbds.git&lt;/code&gt; (or the equivalent if you‚Äôre using HTTPS), or use one of the replace commands above, since remote URLs are stored textually:&lt;/p&gt;
    &lt;code&gt;# Within a single repo:

# For all repos within the current directory: (requires `shopt -s globstar` if using Bash)
&lt;/code&gt;
    &lt;p&gt;‚Ä¶then it‚Äôs a matter of pushing the changes to all of the repos.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 3: stubbing out the GitHub repos&lt;/head&gt;
    &lt;p&gt;I also wanted to make it clear that my repos were now living on Codeberg; so, I created a little script in an empty directory:&lt;/p&gt;
    &lt;code&gt;#!/bin/bash
 

 
 
 
 
 
 
 
&lt;/code&gt;
    &lt;p&gt;Then, to run it:&lt;/p&gt;
    &lt;code&gt; 
 
 
 
 
# ...etc.
&lt;/code&gt;
    &lt;p&gt;The automation made it not painful, so this went pretty well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 4: porting CI&lt;/head&gt;
    &lt;p&gt;Now, onto the harder stuff :)&lt;/p&gt;
    &lt;p&gt;The first interesting thing that I noticed is this section of Codeberg‚Äôs CI documentation:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Running CI/CD pipelines can use significant amounts of energy. As much as it is tempting to have green checkmarks everywhere, running the jobs costs real money and has environmental costs.&lt;/p&gt;
      &lt;p&gt;Unlike other giant platforms, we do not encourage you to write ‚Äúheavy‚Äù pipelines and charge you for the cost later. We expect you to carefully consider the costs and benefits from your pipelines and reduce CI/CD usage to a minimum amount necessary to guarantee consistent quality for your projects.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That got me to think about which projects of mine really need CI, and ultimately, I decided that I would only need CI for publishing my website, and the documentation of gb-starter-kit and fortISSimO; the rest of my projects don‚Äôt get contributions anyway, so I can live without CI on them, at least for now.&lt;/p&gt;
    &lt;p&gt;Anyway, Codeberg actually has two different CI solutions: Woodpecker, and Forgejo Actions; the former seems to be more powerful, but you need to apply for access, and the latter is very close to GitHub Actions, which should facilitate the migration. So I picked Forgejo Actions, even though it‚Äôs marked as being in beta.&lt;/p&gt;
    &lt;p&gt;It‚Äôs not very difficult to port a YAML file from GHA to Forgejo Actions; for example, look at the commit porting gb-starter-kit‚Äôs publishing CI. (This doesn‚Äôt really appear as a diff, since I‚Äôve moved the file; but it‚Äôs small, so it‚Äôs easy to compare manually.)&lt;/p&gt;
    &lt;p&gt;Here are some salient points:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Actions are normally just referred to as &lt;code&gt;owner/repo&lt;/code&gt;, but Forgejo supports cloning any Git repo, especially across forges. It‚Äôs actually recommended to use full URLs always, so you don‚Äôt rely on the default prefix, which is configurable by the instance admin and thus not necessarily portable.&lt;/item&gt;
      &lt;item&gt;I could have kept the files in &lt;code&gt;.github/workflows&lt;/code&gt;, since Forgejo picks up that directory automatically if&lt;code&gt;.forgejo/workflows&lt;/code&gt;doesn‚Äôt exist; however, I think it‚Äôs more convenient to keep un-migrated scripts in&lt;code&gt;.github&lt;/code&gt;and migrated ones in&lt;code&gt;.forgejo&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Most Actions (the individual steps, not the workflow files) actually work out of the box on Forgejo Actions. Nice!&lt;/item&gt;
      &lt;item&gt;Codeberg‚Äôs runners differ from GitHub‚Äôs significantly: they have way less software installed by default, fewer resources, and only Linux runners are provided (Ubuntu by default, but you can use any Docker container image). macOS and Windows being non-free OSes, Codeberg has no plans to offer either of those! For both philosophical and financial reasons. If this is a deal-breaker for you, consider cross-compiling, or bringing your own runner.&lt;/item&gt;
      &lt;item&gt;Unless low latency is crucial, consider using the lazy runners for better load balancing and possibly greener energy consumption. In practice I haven‚Äôt seen delays beyond a few minutes, which is acceptable to me.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I actually spent some extra time trying to use less compute to perform my CI jobs, somewhat motivated by the small size of the runners, and because I‚Äôm guessing that the smaller the runner you‚Äôre picking, the faster your job will be able to be scheduled. Here is one such commit; note in particular line 50, where I tried1 using a Docker image with LaTeX preinstalled, which saves the time taken by &lt;code&gt;apt install&lt;/code&gt; and requires fewer writes to the filesystem, freeing up RAM.&lt;/p&gt;
    &lt;p&gt;Unfortunately, due to a version discrepancy with &lt;code&gt;noweb&lt;/code&gt;, I had to revert to the base Ubuntu image; but a ‚Äúregular‚Äù LaTeX workflow would have had no problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 5: re-hosting my website&lt;/head&gt;
    &lt;p&gt;All of the previous steps were done within the span of a few days; however, since my website (this very website) was hosted using GitHub Pages, I couldn‚Äôt migrate its repos (yes, plural: you can configure individual repos to be published separately, which is how e.g. https://eldred.fr/fortISSimO is published, despite not being in the website‚Äôs main repo).&lt;/p&gt;
    &lt;p&gt;Nominally, Codeberg has an equivalent, Codeberg Pages; however, as mentioned on that page, &lt;quote&gt;the software behind this feature is currently in maintenance mode&lt;/quote&gt;, because of complexity and performance issues2. So I left it at that for roughly a month, hoping there‚Äôll eventually be an update. Also, subprojects are published as subdomains instead of subdirectories, which would have broken links (e.g. &lt;code&gt;http://eldred.fr/fortISSimO&lt;/code&gt; would have become &lt;code&gt;http://fortISSimO.eldred.fr&lt;/code&gt;). Meh‚Ä¶&lt;/p&gt;
    &lt;p&gt;And then (by chance lol) I discovered git-pages and its public instance Grebedoc3! It functions much like GitHub Pages, though with a bit more setup since it‚Äôs not integrated within the forge itself.&lt;/p&gt;
    &lt;p&gt;git-pages actually has several niceties:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;My website had zero downtime during the entire migration, as git-pages supports uploading your website before updating your DNS records!&lt;/item&gt;
      &lt;item&gt;It also supports server-side redirects, which lets me redirect people who still go to http://eldred.fr/gb-asm-tutorial/* to its new home, for example. People have been getting 404s because of incomplete client-side coverage on my side, but no more!&lt;/item&gt;
      &lt;item&gt;It also also supports custom headers; I‚Äôm not particularly interested in CORS, but I‚Äôve used that file to pay my respects4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Oh, and also, Codeberg‚Äôs November 2025 newsletter mentions that &lt;quote&gt;Codeberg is planning to gradually migrate to [git-pages]&lt;/quote&gt;. Exciting!&lt;/p&gt;
    &lt;p&gt;I‚Äôm actually much happier using this than GitHub Pages; so, I‚Äôve joined Catherine‚Äôs Patreon, because I want to see this go far.&lt;/p&gt;
    &lt;p&gt;To quote Catherine‚Äôs motivation for creating git-pages: &lt;quote&gt;I started out wanting to just use Codeberg Pages and then I found out that Codeberg Pages is in maintenance mode and has such poor architecture that not only does it have rather low uptime but it also regularly crashes Codeberg‚Äôs Forgejo instance itself.&lt;/quote&gt;&lt;/p&gt;
    &lt;p&gt;ü¶É. It is intensely looking at you‚Ä¶‚Ä¶.&lt;/p&gt;
    &lt;p&gt;Here is some context as to what this means.&lt;/p&gt;
    &lt;head rend="h3"&gt;Time tracking&lt;/head&gt;
    &lt;p&gt;Steps 1 through 3 (migrating the repos) took me the better part of an afternoon; step 4 (porting CI) took me another afternoon, mostly to learn the new CI system; and step 5 (the website) took me‚Ä¶ well, it should have taken an afternoon, but I used the opportunity to also pay down some tech debt (merging my slides repo into my main website), which took a few days due to required rearchitecting.&lt;/p&gt;
    &lt;p&gt;All in all, even with 45 repos migrated, this basically took a weekend. And I didn‚Äôt find it annoying!&lt;/p&gt;
    &lt;p&gt;Since the task seemed really daunting, my anxiety caused me to procrastinate this a lot, but in the end it was little work. One of the reasons I‚Äôm writing this is to let other people know that, so they can overcome their own anxiety. Maybe. :P&lt;/p&gt;
    &lt;head rend="h2"&gt;What now?&lt;/head&gt;
    &lt;p&gt;All in all, I‚Äôm very happy with this migration! As far as I can tell, nothing on this website has broken, and I‚Äôve tried reasonably containing the breakage over on GitHub: I have truncated the &lt;code&gt;master&lt;/code&gt; branches, but all other branches and tags remain in place (mostly due to laziness lol), permalinks (e.g. &lt;code&gt;https://github.com/ISSOtm/gb-bootroms/blob/c8ed9e106e0ab1193a57071820e46358006c79d0/src/dmg.asm&lt;/code&gt;) still work, only non-perma links (e.g. &lt;code&gt;https://github.com/ISSOtm/gb-bootroms/blob/master/src/dmg.asm&lt;/code&gt;) are broken, but those are unreliable in the first place anyway.&lt;/p&gt;
    &lt;p&gt;Since that means that all of my code is still on GitHub, I want to delete my repos; but that would be a bad idea at this point, due to leaving no redirects or anything. I‚Äôll consider that again in‚Ä¶ idk, a year or something. I would also like to delete my GitHub account (like I have deleted my Twitter account when‚Ä¶ *gestures vaguely*), but not only do I need my repos to be up, I also need my account to contribute to projects that are still on GitHub.&lt;/p&gt;
    &lt;p&gt;One downside of this migration is that since I‚Äôm moving off of The Main Forge, my projects are likely to get fewer contributions‚Ä¶ But I wasn‚Äôt getting many in the first place, and some people have already made accounts on Codeberg to keep contributing to my stuff. Likewise, I‚Äôm not really worried about discoverability. We‚Äôll see I guess lol ü§∑‚ôÇÔ∏è&lt;/p&gt;
    &lt;p&gt;Lastly, I‚Äôm writing this after the migration, and I haven‚Äôt really taken notes during it; so, if I‚Äôve forgotten any steps, feel free to let me know in the comments below or by opening an issue, and I‚Äôll edit this article.&lt;/p&gt;
    &lt;p&gt;Cheers!&lt;/p&gt;
    &lt;head rend="h2"&gt;Special thanks&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Catherine ‚Äòwhitequark‚Äô for her work on git-pages and for being part of the ops team for Grebedoc&lt;/item&gt;
      &lt;item&gt;SERVFAIL network (domi, Merlin, famfo, aprl, and all of #servfail) for being my awesome DNS providers&lt;/item&gt;
      &lt;item&gt;Codeberg team and Forgejo contributors for making all of this possible in the first place&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eldred.fr/blog/forge-migration/"/><published>2025-11-30T16:12:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46098336</id><title>RetailReady (YC W24) Is Hiring Associate Product Manager</title><updated>2025-11-30T23:09:31.739048+00:00</updated><content>&lt;doc fingerprint="c194fec3c124afea"&gt;
  &lt;main&gt;
    &lt;p&gt;An AI-powered supply chain compliance engine&lt;/p&gt;
    &lt;p&gt;San Francisco - In Person&lt;/p&gt;
    &lt;p&gt;We‚Äôre RetailReady (YC W24), an AI-powered supply chain compliance engine shaking up an antiquated (and yes, unsexy) industry. Since YC, we raised a $3.3M seed round and signed over 15 enterprise customers‚Ä¶ we‚Äôre officially in scaling mode.&lt;/p&gt;
    &lt;p&gt;RetailReady is the first AI-powered compliance engine designed for retail supply chains. We automate the messy web of compliance requirements between brands, warehouses, and retailers, turning weeks of manual work into minutes of automation. Our platform integrates deeply with warehouse operations and connects with customer systems via EDI and APIs.&lt;/p&gt;
    &lt;p&gt;We don‚Äôt just build dashboards. We build the nervous system of compliance inside a warehouse.&lt;/p&gt;
    &lt;p&gt;We‚Äôre hiring an Associate Product Manager to work directly with our Technical Product Lead and drive day-to-day product execution. You‚Äôll own QA, write clear specs, organize feedback, and keep features moving across engineering and customers. This role is built for someone who wants to grow into a full PM.&lt;/p&gt;
    &lt;p&gt;Bonus: supply chain or 3PL experience, QA background, implementation experience, or exposure to EDI/APIs.&lt;/p&gt;
    &lt;p&gt;RetailReady is building an AI-powered supply chain compliance engine. Supply chains are still heavily reliant on paper processes and tribal knowledge, causing costly shipping mistakes that jeopardize the longevity of businesses. RetailReady is the first-to-market with our retail compliance packing software, leveraging camera vision to direct warehouses to ship orders without error. We are positioning our compliance data models to become the operating system that will power the next wave of warehouse robotics and automation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/retailready/jobs/KPKDu3D-associate-product-manager"/><published>2025-11-30T17:01:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46098359</id><title>LLVM-MOS ‚Äì Clang LLVM fork targeting the 6502</title><updated>2025-11-30T23:09:31.445711+00:00</updated><content/><link href="https://llvm-mos.org/wiki/Welcome"/><published>2025-11-30T17:02:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46098673</id><title>ESA Sentinel-1D delivers first high-resolution images</title><updated>2025-11-30T23:09:30.616550+00:00</updated><content>&lt;doc fingerprint="a0002ffb95f93d86"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Sentinel-1D delivers first images: from Antarctica to Bremen&lt;/head&gt;
    &lt;p&gt;The first high-resolution images have been received from Copernicus Sentinel-1D and were shared publicly for the first time at the European Space Agency‚Äôs Ministerial Council, held today in Bremen, Germany. Glaciers in Antarctica, the tip of South America, as well as the city of Bremen, are visible in these stunning radar images.&lt;/p&gt;
    &lt;p&gt;The groundbreaking Copernicus Sentinel-1 mission saw the arrival in orbit of its latest satellite earlier in November: Sentinel-1D was launched on 4 November, on board an Ariane 6 launcher from Europe‚Äôs Spaceport in French Guiana.&lt;/p&gt;
    &lt;p&gt;Once in orbit, the satellite and its instruments ‚Äì it carries a 12 m-long synthetic aperture radar (SAR) instrument ‚Äì were switched on, ready to capture images during a pass over the Antarctic and South America two days after launch. On the night of 6 November (European time), the first images were captured over the Antarctic Peninsula, the Tierra del Fuego and the Thwaites Glacier. Some six hours later, on the morning of 7 November, Sentinel-1D also captured images over Bremen, in Germany. The data was then transmitted, or ‚Äòdownlinked‚Äô, from the satellite to the ground station, in Matera (Italy), which is part of the Copernicus Ground Segment. All this was done within 50 hours of launch, which is likely to be the shortest time from launch to data delivery for a radar-based Earth observation satellite.&lt;/p&gt;
    &lt;p&gt;According to Nuno Miranda, ESA‚Äôs Sentinel-1 Mission Manager, the images are of unprecedented data quality for a ‚Äòfirst light‚Äô acquisition. They are very similar to the images captured not so long ago by Sentinel-1C, which, according to Nuno, is very promising for the commissioning phase. He noted, ‚ÄúThese images have been downlinked and processed within an exceptionally short timeframe. Some of us remember that when Sentinel-1B was launched, it delivered its first radar images within two hours of activation. Sentinel-1D achieved this in an even faster time, setting what we believe is a new record for space radar. This remarkable performance reflects the dedication and exceptional preparation of all the teams involved.‚Äù&lt;/p&gt;
    &lt;p&gt;Radar instruments can image Earth‚Äôs surface through clouds, precipitation, regardless of sunlight, making them particularly well suited for monitoring polar regions. The Sentinel-1C and -1D satellites also carry an Automatic Identification System (AIS) instrument ‚Äì improving the mission capacity to detect ships and sea pollution. The Sentinel-1D AIS was also activated as the satellite passed over Antarctica capturing the presence of ships in these extreme areas.&lt;/p&gt;
    &lt;head rend="h2"&gt;First images show fragility of glaciers&lt;/head&gt;
    &lt;p&gt;The Antarctic Peninsula (above) is part of the larger peninsula of West Antarctica, protruding 1300 km. It is an ice sheet resting on a string of rocky islands and its tip is just 1000 km from the southern tip of South America. The Antarctic Peninsula ice sheet is one of the smallest ice sheets in Antarctica but is perhaps the most vulnerable to climate change as its glaciers are small and in a region of rapid warming. Observable changes such as collapsing ice shelves, thinning and accelerating glaciers are all key indicators of climate change in the region.&lt;/p&gt;
    &lt;p&gt;This image is in black and white, showing the contrast between the ocean and the peninsula‚Äôs icy landscape.&lt;/p&gt;
    &lt;p&gt;Tierra del Fuego (above) is an archipelago off the southern tip of the South American continent. It covers territory in both Argentina to the east and in Chile to the west and is separated from the mainland by the Magellan Strait. The most southerly point of Tierra del Fuego is Cape Horn.&lt;/p&gt;
    &lt;p&gt;The bright contrasting colours in this image are created by using multiple types of radar wave, known polarisations. In this image the ocean and snowy peaks are shown in shades of blue, while the land appears yellow.&lt;/p&gt;
    &lt;p&gt;The Thwaites Glacier, and the adjacent Pine Island Glacier (above), are located west of the Antarctic Peninsula. Both are vulnerable to climate change. Thwaites is one of the most unstable glaciers in Antarctica and is at risk of rapid retreat. The details shown in this image from Sentinel-1D remind us of the fragility of glaciers in the Antarctic. And since 2025 is the United Nation‚Äôs International Year of Glaciers' Preservation, it is timely to see this image, captured on 6 November 2025.&lt;/p&gt;
    &lt;p&gt;This image also uses multiple radar polarisations to capture enhanced data on the landscape. In this image, the sea ice in the water is visible in tones of purple or violet, while the glacier appears white.&lt;/p&gt;
    &lt;p&gt;The publication of these images also follows the 30th meeting of the Conference of Parties, or COP30, where the consequences of climate change and the mitigating actions needed were discussed. The World Meteorological Organization‚Äôs State of the Climate Update for COP30 notes that glaciers, from October 2023 to September 2024, lost the largest amount of ice on record back to 1950. The report states this is equivalent to 1.2 mm of global mean sea-level rise. The report also notes that, on 24 February 2025, the extent of Antarctic sea-ice reached its third lowest extent since satellite records began in 1978, the lowest being in 2023.&lt;/p&gt;
    &lt;p&gt;Simonetta Cheli, Director of ESA‚Äôs Earth Observation Programmes, said, ‚ÄúThis is a great achievement and I am so pleased to see these results from Sentinel-1D. It really places the data we receive from our innovative missions in the spotlight ‚Äì it is data that we as a society rely upon as we continue to discuss and take action on climate change, and also data that we need in applications for understanding and studying our planet.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe Sentinel-1 team has done an amazing job and I would like to thank everyone within ESA, together with our partners in the space industry and European institutions, for delivering work of such high quality. It‚Äôs an honour to deliver this mission for the Copernicus Earth observation programme, and we thank the Commission for their support and collaboration. We look forward also to developing the Sentinel missions of the future, to further extend the capacity and potential of Copernicus for Europe.‚Äù&lt;/p&gt;
    &lt;p&gt;ESA‚Äôs Sentinel-1 Project Manager, Ram√≥n Torres, expressed the whole team‚Äôs pride, ‚ÄúUnveiling the first images from Sentinel-1D is an incredibly emotional milestone for all of us. The sense of awe and fulfilment goes beyond the thrill of liftoff itself, because seeing those breathtaking images from the SAR instrument brings our hard work to life. They are not just pictures ‚Äì they are proof of our vision becoming reality, underlining how cutting-edge this mission truly is. The fact that these stunning images also confirm the satellite‚Äôs health and flawless operation fills us with relief and joy. And to have achieved all of this within an astonishingly short time ‚Äì just over two days after launch ‚Äì makes this moment even more unforgettable for our team.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-1/Sentinel-1D_delivers_first_images_from_Antarctica_to_Bremen"/><published>2025-11-30T17:37:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46098747</id><title>ETH-Zurich: Digital Design and Computer Architecture; 227-0003-10L, Spring, 2025</title><updated>2025-11-30T23:09:30.014570+00:00</updated><content>&lt;doc fingerprint="5c71730e58600a24"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Table of Contents&lt;/head&gt;
    &lt;head rend="h1"&gt;Digital Design and Computer Architecture&lt;/head&gt;
    &lt;head rend="h1"&gt;Spring 2025 (227-0003-10L)&lt;/head&gt;
    &lt;p&gt;Welcome to the wiki for Digital Design and Computer Architecture for Spring 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Announcements&lt;/head&gt;
    &lt;head rend="h2"&gt;Course Information&lt;/head&gt;
    &lt;head rend="h3"&gt;Description&lt;/head&gt;
    &lt;p&gt;The class provides a first introduction to the design of digital circuits and computer architecture. It covers technical foundations of how a computing platform is designed from the bottom up. It introduces various execution paradigms, hardware description languages, and principles in digital design and computer architecture. The focus is on fundamental techniques employed in the design of modern microprocessors and their hardware/software interface.&lt;/p&gt;
    &lt;head rend="h3"&gt;Objectives&lt;/head&gt;
    &lt;p&gt;This class provides a first approach to Computer Architecture. The students learn the design of digital circuits in order to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;understand the basics,&lt;/item&gt;
      &lt;item&gt;understand the principles (of design),&lt;/item&gt;
      &lt;item&gt;understand the precedents (in computer architecture).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Based on such understanding, the students are expected to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;learn how a modern computer works underneath, from the bottom up,&lt;/item&gt;
      &lt;item&gt;evaluate tradeoffs of different designs and ideas,&lt;/item&gt;
      &lt;item&gt;implement a principled design (a simple microprocessor),&lt;/item&gt;
      &lt;item&gt;learn to systematically debug increasingly complex systems,&lt;/item&gt;
      &lt;item&gt;hopefully be prepared to develop novel, out-of-the-box designs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The focus is on basics, principles, precedents, and how to use them to create/implement good designs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lectures&lt;/head&gt;
    &lt;p&gt; Thursday, 14:15-16:00, in HG F7 (Overflow room: HG F5) &lt;lb/&gt; Friday, 14:15-16:00, in HG F7 (Overflow room: HG F5) &lt;/p&gt;
    &lt;p&gt;Watch the lectures in YouTube livestream:&lt;/p&gt;
    &lt;head rend="h3"&gt;Lab sessions&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;When?&lt;/cell&gt;
        &lt;cell role="head"&gt;Where?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Tuesday, 16:15-18:00&lt;/cell&gt;
        &lt;cell&gt;labs in HG E19, HG E26.1, HG E26.3, HG E27&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Wednesday, 16:15-18:00&lt;/cell&gt;
        &lt;cell&gt;labs in HG E19, HG E26.1, HG E26.3, HG E27&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Friday, 08:15-10:00&lt;/cell&gt;
        &lt;cell&gt;labs in HG D11, HG D12, HG E26.3, HG E27&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Friday, 10:15-12:00&lt;/cell&gt;
        &lt;cell&gt;labs in HG E19, HG E26.1, HG E26.3, HG E27&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Prerequisites: None.&lt;/p&gt;
    &lt;head rend="h2"&gt;Staff Information&lt;/head&gt;
    &lt;head rend="h3"&gt;Contact&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mailing List: digitaltechnik@lists.inf.ethz.ch (sent to instructor and TAs)&lt;/item&gt;
      &lt;item&gt;Office Hours: TBD&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Office&lt;/cell&gt;
        &lt;cell role="head"&gt;Phone&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Instructor&lt;/cell&gt;
        &lt;cell&gt;Onur Mutlu&lt;/cell&gt;
        &lt;cell&gt;onur.mutlu@safari.ethz.ch&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Instructor&lt;/cell&gt;
        &lt;cell&gt;Mohammad Sadrosadati&lt;/cell&gt;
        &lt;cell&gt;mohammad.sadrosadati@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ F76&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Head Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Ataberk Olgun&lt;/cell&gt;
        &lt;cell&gt;ataberk.olgun@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ H61.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Giray Yaglikci&lt;/cell&gt;
        &lt;cell&gt;giray.yaglikci@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ H61.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Can Firtina&lt;/cell&gt;
        &lt;cell&gt;can.firtina@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Geraldo De Oliveira Junior&lt;/cell&gt;
        &lt;cell&gt;geraldod@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ 61.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Rahul Bera&lt;/cell&gt;
        &lt;cell&gt;rahbera@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ H64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Konstantinos Kanellopoulos&lt;/cell&gt;
        &lt;cell&gt;kanellok@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ H61.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Nika Mansouri Ghiasi&lt;/cell&gt;
        &lt;cell&gt;mnika@ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ 61.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Nisa Bostancƒ±&lt;/cell&gt;
        &lt;cell&gt;nisa.bostanci@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ 61.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Rakesh Nadig&lt;/cell&gt;
        &lt;cell&gt;rakesh.nadig@safari.ethz.ch&lt;/cell&gt;
        &lt;cell&gt;ETZ H64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;ƒ∞smail Emir Y√ºksel&lt;/cell&gt;
        &lt;cell&gt;ETZ H61.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Teaching Assistant&lt;/cell&gt;
        &lt;cell&gt;Haocong Luo&lt;/cell&gt;
        &lt;cell&gt;ETZ H61.2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://safari.ethz.ch/ddca/spring2025/doku.php?id=start"/><published>2025-11-30T17:45:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46098838</id><title>Writing a good Claude.md</title><updated>2025-11-30T23:09:29.802493+00:00</updated><content>&lt;doc fingerprint="474c0a5f0dbfd7fe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;# Writing a good CLAUDE.md&lt;/head&gt;
    &lt;p&gt;Kyle Mistele ¬∑ November 25, 2025 ¬∑ &amp;lt; 10 min read&lt;/p&gt;
    &lt;p&gt;Note: this post is also applicable to &lt;code&gt;AGENTS.md&lt;/code&gt;, the open-source equivalent of &lt;code&gt;CLAUDE.md&lt;/code&gt; for agents and harnesses like OpenCode, Zed, Cursor and Codex.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Principle: LLMs are (mostly) stateless&lt;/head&gt;
    &lt;p&gt;LLMs are stateless functions. Their weights are frozen by the time they're used for inference, so they don't learn over time. The only thing that the model knows about your codebase is the tokens you put into it.&lt;/p&gt;
    &lt;p&gt;Similarly, coding agent harnesses such as Claude Code usually require you to manage agents' memory explicitly. &lt;code&gt;CLAUDE.md&lt;/code&gt; (or &lt;code&gt;AGENTS.md&lt;/code&gt;) is the only file that by default goes into every single conversation you have with the agent.&lt;/p&gt;
    &lt;p&gt;This has three important implications:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Coding agents know absolutely nothing about your codebase at the beginning of each session.&lt;/item&gt;
      &lt;item&gt;The agent must be told anything that's important to know about your codebase each time you start a session.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;is the preferred way of doing this.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;## &lt;code&gt;CLAUDE.md&lt;/code&gt; onboards Claude to your codebase&lt;/head&gt;
    &lt;p&gt;Since Claude doesn't know anything about your codebase at the beginning of each session, you should use &lt;code&gt;CLAUDE.md&lt;/code&gt; to onboard Claude into your codebase. At a high level, this means it should cover:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WHAT: tell Claude about the tech, your stack, the project structure. Give Claude a map of the codebase. This is especially important in monorepos! Tell Claude what the apps are, what the shared packages are, and what everything is for so that it knows where to look for things&lt;/item&gt;
      &lt;item&gt;WHY: tell Claude the purpose of the project and what everything is doing in the repository. What are the purpose and function of the different parts of the project?&lt;/item&gt;
      &lt;item&gt;HOW: tell Claude how it should work on the project. For example, do you use &lt;code&gt;bun&lt;/code&gt;instead of&lt;code&gt;node&lt;/code&gt;? You want to include all the information it needs to actually do meaningful work on the project. How can Claude verify Claude's changes? How can it run tests, typechecks, and compilation steps?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the way you do this is important! Don't try to stuff every command Claude could possibly need to run in your &lt;code&gt;CLAUDE.md&lt;/code&gt; file - you will get sub-optimal results.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Claude often ignores &lt;code&gt;CLAUDE.md&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Regardless of which model you're using, you may notice that Claude frequently ignores your &lt;code&gt;CLAUDE.md&lt;/code&gt; file's contents.&lt;/p&gt;
    &lt;p&gt;You can investigate this yourself by putting a logging proxy between the claude code CLI and the Anthropic API using &lt;code&gt;ANTHROPIC_BASE_URL&lt;/code&gt;. Claude code injects the following system reminder with your &lt;code&gt;CLAUDE.md&lt;/code&gt; file in the user message to the agent:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;&amp;lt;system-reminder&amp;gt; IMPORTANT: this context may or may not be relevant to your tasks. You should not respond to this context unless it is highly relevant to your task. &amp;lt;/system-reminder&amp;gt;&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;As a result, Claude will ignore the contents of your &lt;code&gt;CLAUDE.md&lt;/code&gt; if it decides that it is not relevant to its current task. The more information you have in the file that's not universally applicable to the tasks you have it working on, the more likely it is that Claude will ignore your instructions in the file.&lt;/p&gt;
    &lt;p&gt;Why did Anthropic add this? It's hard to say for sure, but we can speculate a bit. Most &lt;code&gt;CLAUDE.md&lt;/code&gt; files we come across include a bunch of instructions in the file that aren't broadly applicable. Many users treat the file as a way to add "hotfixes" to behavior they didn't like by appending lots of instructions that weren't necessarily broadly applicable.&lt;/p&gt;
    &lt;p&gt;We can only assume that the Claude Code team found that by telling Claude to ignore the bad instructions, the harness actually produced better results.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Creating a good &lt;code&gt;CLAUDE.md&lt;/code&gt; file&lt;/head&gt;
    &lt;p&gt;The following section provides a number of recommendations on how to write a good &lt;code&gt;CLAUDE.md&lt;/code&gt; file following context engineering best practices.&lt;/p&gt;
    &lt;p&gt;Your mileage may vary. Not all of these rules are necessarily optimal for every setup. Like anything else, feel free to break the rules once...&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;you understand when &amp;amp; why it's okay to break them&lt;/item&gt;
      &lt;item&gt;you have a good reason to do so&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;### Less (instructions) is more&lt;/head&gt;
    &lt;p&gt;It can be tempting to try and stuff every single command that claude could possibly need to run, as well as your code standards and style guidelines into &lt;code&gt;CLAUDE.md&lt;/code&gt;. We recommend against this.&lt;/p&gt;
    &lt;p&gt;Though the topic hasn't been investigated in an incredibly rigorous manner, some research has been done which indicates the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Frontier thinking LLMs can follow ~ 150-200 instructions with reasonable consistency. Smaller models can attend to fewer instructions than larger models, and non-thinking models can attend to fewer instructions than thinking models.&lt;/item&gt;
      &lt;item&gt;Smaller models get MUCH worse, MUCH more quickly. Specifically, smaller models tend to exhibit an expotential decay in instruction-following performance as the number of instructions increase, whereas larger frontier thinking models exhibit a linear decay (see below). For this reason, we recommend against using smaller models for multi-step tasks or complicated implementation plans.&lt;/item&gt;
      &lt;item&gt;LLMs bias towards instructions that are on the peripheries of the prompt: at the very beginning (the Claude Code system message and &lt;code&gt;CLAUDE.md&lt;/code&gt;), and at the very end (the most-recent user messages)&lt;/item&gt;
      &lt;item&gt;As instruction count increases, instruction-following quality decreases uniformly. This means that as you give the LLM more instructions, it doesn't simply ignore the newer ("further down in the file") instructions - it begins to ignore all of them uniformly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our analysis of the Claude Code harness indicates that Claude Code's system prompt contains ~50 individual instructions. Depending on the model you're using, that's nearly a third of the instructions your agent can reliably follow already - and that's before rules, plugins, skills, or user messages.&lt;/p&gt;
    &lt;p&gt;This implies that your &lt;code&gt;CLAUDE.md&lt;/code&gt; file should contain as few instructions as possible - ideally only ones which are universally applicable to your task.&lt;/p&gt;
    &lt;head rend="h3"&gt;### &lt;code&gt;CLAUDE.md&lt;/code&gt; file length &amp;amp; applicability&lt;/head&gt;
    &lt;p&gt;All else being equal, an LLM will perform better on a task when its' context window is full of focused, relevant context including examples, related files, tool calls, and tool results compared to when its context window has a lot of irrelevant context.&lt;/p&gt;
    &lt;p&gt;Since &lt;code&gt;CLAUDE.md&lt;/code&gt; goes into every single session, you should ensure that its contents are as universally applicable as possible.&lt;/p&gt;
    &lt;p&gt;For example, avoid including instructions about (for example) how to structure a new database schema - this won't matter and will distract the model when you're working on something else that's unrelated!&lt;/p&gt;
    &lt;p&gt;Length-wise, the less is more principle applies as well. While Anthropic does not have an official recommendation on how long your &lt;code&gt;CLAUDE.md&lt;/code&gt; file should be, general consensus is that &amp;lt; 300 lines is best, and shorter is even better.&lt;/p&gt;
    &lt;p&gt;At HumanLayer, our root &lt;code&gt;CLAUDE.md&lt;/code&gt; file is less than sixty lines.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Progressive Disclosure&lt;/head&gt;
    &lt;p&gt;Writing a concise &lt;code&gt;CLAUDE.md&lt;/code&gt; file that covers everything you want Claude to know can be challenging, especially in larger projects.&lt;/p&gt;
    &lt;p&gt;To address this, we can leverage the principle of Progressive Disclosure to ensure that claude only sees task- or project-specific instructions when it needs them.&lt;/p&gt;
    &lt;p&gt;Instead of including all your different instructions about building your project, running tests, code conventions, or other important context in your &lt;code&gt;CLAUDE.md&lt;/code&gt; file, we recommend keeping task-specific instructions in separate markdown files with self-descriptive names somewhere in your project.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;agent_docs/ |- building_the_project.md |- running_tests.md |- code_conventions.md |- service_architecture.md |- database_schema.md |- service_communication_patterns.md&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Then, in your &lt;code&gt;CLAUDE.md&lt;/code&gt; file, you can include a list of these files with a brief description of each, and instruct Claude to decide which (if any) are relevant and to read them before it starts working. Or, ask Claude to present you with the files it wants to read for aproval first before reading them.&lt;/p&gt;
    &lt;p&gt;Prefer pointers to copies. Don't include code snippets in these files if possible - they will become out-of-date quickly. Instead, include &lt;code&gt;file:line&lt;/code&gt; references to point Claude to the authoritative context.&lt;/p&gt;
    &lt;p&gt;Conceptually, this is very similar to how Claude Skills are intended to work, although skills are more focused on tool use than instructions.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Claude is (not) an expensive linter&lt;/head&gt;
    &lt;p&gt;One of the most common things that we see people put in their &lt;code&gt;CLAUDE.md&lt;/code&gt; file is code style guidelines. Never send an LLM to do a linter's job. LLMs are comparably expensive and incredibly slow compared to traditional linters and formatters. We think you should always use deterministic tools whenever you can.&lt;/p&gt;
    &lt;p&gt;Code style guidelines will inevitably add a bunch of instructions and mostly-irrelevant code snippets into your context window, degrading your LLM's performance and instruction-following and eating up your context window.&lt;/p&gt;
    &lt;p&gt;LLMs are in-context learners! If your code follows a certain set of style guidelines or patterns, you should find that armed with a few searches of your codebase (or a good research document!) your agent should tend to follow existing code patterns and conventions without being told to.&lt;/p&gt;
    &lt;p&gt;If you feel very stronly about this, you might even consider setting up a Claude Code &lt;code&gt;Stop&lt;/code&gt; hook that runs your formatter &amp;amp; linter and presents errors to Claude for it to fix. Don't make Claude find the formatting issues itself.&lt;/p&gt;
    &lt;p&gt;Bonus points: use a linter that can automatically fix issues (we like Biome), and carefully tune your rules about what can safely be auto-fixed for maximum (safe) coverage.&lt;/p&gt;
    &lt;p&gt;You could also create a Slash Command that includes your code guidelines and which points claude at the changes in version control, or at your &lt;code&gt;git status&lt;/code&gt;, or similar. This way, you can handle implementation and formatting separately. You will see better results with both as a result.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Don't use &lt;code&gt;/init&lt;/code&gt; or auto-generate your &lt;code&gt;CLAUDE.md&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Both Claude Code and other harnesses with OpenCode come with ways to auto-generate your &lt;code&gt;CLAUDE.md&lt;/code&gt; file (or &lt;code&gt;AGENTS.md&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Because &lt;code&gt;CLAUDE.md&lt;/code&gt; goes into every single session with Claude code, it is one of the highest leverage points of the harness - for better or for worse, depending on how you use it.&lt;/p&gt;
    &lt;p&gt;A bad line of code is a bad line of code. A bad line of an implementation plan has the potential to create a lot of bad lines of code. A bad line of a research that misunderstands how the system works has the potential to result in a lot of bad lines in the plan, and therefore a lot more bad lines of code as a result.&lt;/p&gt;
    &lt;p&gt;But the &lt;code&gt;CLAUDE.md&lt;/code&gt; file affects every single phase of your workflow and every single artifact produced by it. As a result, we think you should spend some time thinking very carefully about every single line that goes into it:&lt;/p&gt;
    &lt;head rend="h2"&gt;## In Conclusion&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;is for onboarding Claude into your codebase. It should define your project's WHY, WHAT, and HOW.&lt;/item&gt;
      &lt;item&gt;Less (instructions) is more. While you shouldn't omit necessary instructions, you should include as few instructions as reasonably possible in the file.&lt;/item&gt;
      &lt;item&gt;Keep the contents of your &lt;code&gt;CLAUDE.md&lt;/code&gt;concise and universally applicable.&lt;/item&gt;
      &lt;item&gt;Use Progressive Disclosure - don't tell Claude all the information you could possibly want it to know. Rather, tell it how to find important information so that it can find and use it, but only when it needs to to avoid bloating your context window or instruction count.&lt;/item&gt;
      &lt;item&gt;Claude is not a linter. Use linters and code formatters, and use other features like Hooks and Slash Commands as necessary.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;is the highest leverage point of the harness, so avoid auto-generating it. You should carefully craft its contents for best results.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.humanlayer.dev/blog/writing-a-good-claude-md"/><published>2025-11-30T17:56:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46098979</id><title>There is No Quintic Formula [video]</title><updated>2025-11-30T23:09:28.443825+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket ¬© 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=9HIy5dJE-zQ"/><published>2025-11-30T18:15:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46098992</id><title>Don't push AI down our throats</title><updated>2025-11-30T23:09:28.349651+00:00</updated><content/><link href="https://gpt3experiments.substack.com/p/dont-push-ai-down-our-throats"/><published>2025-11-30T18:17:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46099022</id><title>NixOS 25.11 released</title><updated>2025-11-30T23:09:28.053523+00:00</updated><content>&lt;doc fingerprint="ff65515b8a10e7d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Hey everyone, we are jopejoe1 and Leona Maroni, the release managers of the newest release of NixOS. We are very proud to announce the public availability of NixOS 25.11 ‚ÄúXantusia‚Äù.&lt;/p&gt;
    &lt;p&gt;NixOS is a Linux distribution. Its underlying package repository Nixpkgs can also be used on other Linux systems and macOS with the Nix package manager.&lt;/p&gt;
    &lt;p&gt;This release will receive bugfixes and security updates for seven months (up until 2026-06-30). The old release 25.05 ‚ÄúWarbler‚Äù is now officially deprecated and will reach its end-of-life and stop receiving security updates after 2025-12-31.&lt;/p&gt;
    &lt;p&gt;The 25.11 release was made possible due to the efforts of 2742 contributors, who authored 59430 commits since the previous release.&lt;/p&gt;
    &lt;head rend="h2"&gt;Highlights&lt;/head&gt;
    &lt;p&gt;Our vast and routinely maintained set of packages has also been updated. This release of Nixpkgs&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Added 7002 new packages&lt;/item&gt;
      &lt;item&gt;Updated 25252 existing packages&lt;/item&gt;
      &lt;item&gt;Removed 6338 outdated packages, in an effort to keep the package set maintainable and secure.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In addition to packages from Nixpkgs, the NixOS Linux distribution also features composable configuration modules and integration tests for distributed systems. This release of NixOS&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Added 107 new modules and 1778 configuration options&lt;/item&gt;
      &lt;item&gt;Removed 41 outdated modules and 807 configuration options.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;GNOME 49&lt;/head&gt;
    &lt;p&gt;GNOME has been updated to version 49 ‚ÄúBrescia‚Äù, which removes X11 session support, introduces a new video player, a new document viewer, a redesigned calender and more changes. Refer to the release notes for more details.&lt;/p&gt;
    &lt;head rend="h3"&gt;C compilers&lt;/head&gt;
    &lt;p&gt;LLVM has been updated to version 21. GCC remains at version 14. CMake was updated to version 4.&lt;/p&gt;
    &lt;head rend="h2"&gt;Special Thanks&lt;/head&gt;
    &lt;p&gt;We want to personally thank&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Seth and dish for editorializing the release notes&lt;/item&gt;
      &lt;item&gt;Yohann Boniface for the release logo&lt;/item&gt;
      &lt;item&gt;The NixOS infrastucture team for their dutifully tending to our build infrastructure&lt;/item&gt;
      &lt;item&gt;The Nixpkgs staging team for supporting our staging cycles and the patient fixing of many build errors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We also want to thank all contributors who made this release possible!&lt;/p&gt;
    &lt;head rend="h2"&gt;Reflections and closing&lt;/head&gt;
    &lt;p&gt;We are grateful for the opportunity to support the community as release managers and to learn about and participate in the release process. Seeing all the contributors working in their area of the project to improve it has been an exciting experience. We would like to thank everyone in the community for that. We are looking forward to the next release, NixOS 26.05 ‚ÄúYarara‚Äù.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nixos.org/blog/announcements/2025/nixos-2511/"/><published>2025-11-30T18:21:45+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46099108</id><title>Program-of-Thought Prompting Outperforms Chain-of-Thought by 15% (2022)</title><updated>2025-11-30T23:09:27.717824+00:00</updated><content>&lt;doc fingerprint="e8bcc40accf7908a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computation and Language&lt;/head&gt;&lt;p&gt; [Submitted on 22 Nov 2022 (v1), last revised 23 Oct 2023 (this version, v4)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks&lt;/head&gt;View PDF&lt;quote&gt;Abstract:Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github this https URL&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Wenhu Chen [view email]&lt;p&gt;[v1] Tue, 22 Nov 2022 21:06:00 UTC (8,689 KB)&lt;/p&gt;&lt;p&gt;[v2] Fri, 25 Nov 2022 01:49:50 UTC (8,689 KB)&lt;/p&gt;&lt;p&gt;[v3] Tue, 29 Nov 2022 03:46:29 UTC (8,689 KB)&lt;/p&gt;&lt;p&gt;[v4] Mon, 23 Oct 2023 01:27:38 UTC (4,047 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2211.12588"/><published>2025-11-30T18:34:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46099367</id><title>You want microservices, but do you need them?</title><updated>2025-11-30T23:09:27.386771+00:00</updated><content>&lt;doc fingerprint="3558b3710334ca8f"&gt;
  &lt;main&gt;
    &lt;p&gt;Do you know who managed to cut costs by a staggering 90% by abandoning microservices for a monolith in May 2023? Not a cash-strapped startup or an indie project‚ÄîAmazon itself, for its Prime Video service. The same AWS that earns billions every year by selling microservices infrastructure admitted that, sometimes, a good old monolith wins.&lt;/p&gt;
    &lt;p&gt;This reversal from the company that practically wrote the playbook on distributed systems sent shockwaves through the cloud-native community. Amazon later removed the original blog post, but the internet never forgets, as you‚Äôll see later.&lt;/p&gt;
    &lt;p&gt;I‚Äôve been speaking up against unnecessary or premature use of microservices architecture for five, six years now. After Amazon Prime Video went back to a monolith, I came across several eminent architects who are also speaking against microservices as default.&lt;/p&gt;
    &lt;p&gt;And yet in most tech circles, microservices are still viewed as the only way to build modern software. They dominate conferences, blogs, and job listings. Teams adopt them not because their requirements justify it, but because it feels like the obvious (and r√©sum√©-boosting) choice. ‚ÄúCloud-native‚Äù has become synonymous with ‚Äúmicroservices-by-default‚Äù, as if other approaches are as obsolete as floppy disks.&lt;/p&gt;
    &lt;p&gt;Microservices do solve real problems, but at a massive scale. Most teams don‚Äôt actually operate at that scale.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;With this article, I urge you to reflect on the question the industry has mostly stopped asking: Should microservices be the default choice for building at scale? We‚Äôll look at reversal stories and insights from seasoned architects, and weigh the trade-offs and alternatives. After considering all of this, you can decide whether your problem really needs a constellation of microservices.&lt;/p&gt;
    &lt;head rend="h2"&gt;Microservices: The Agility-Complexity Trade-Off&lt;/head&gt;
    &lt;p&gt;On paper, microservices look impressive. Instead of one big monolith, you split your application into many small services. Each one can be written in any language, owned by a small team, and deployed on its own schedule. If you need more capacity, you can scale only the part that‚Äôs under load. The promise is elegant: independent deployability, autonomous teams, multi-language stacks, and elastic scaling.&lt;/p&gt;
    &lt;p&gt;But the catch is that every split creates a seam, and every seam is a potential failure point. Inside a monolith, function calls are instant and predictable. Across services, those same calls become network requests: slower, failure-prone, sometimes returning inconsistent data. With dozens (or hundreds) of services, you need version management, schema evolution, distributed transactions, tracing, centralized logging, and heavy-duty CI/CD pipelines just to keep things running.&lt;/p&gt;
    &lt;p&gt;This Gartner diagram captures the trade-off perfectly: microservices exchange the simplicity of one codebase for the complexity of many.&lt;/p&gt;
    &lt;p&gt;At a massive scale (think Netflix), that trade-off may be worth it. But when operational benefits don‚Äôt outweigh the costs, teams end up paying a steep price in debugging, coordination, and glue code just to hold their product together.&lt;/p&gt;
    &lt;p&gt;Microservices make sense in very specific scenarios where distinct business capabilities need independent scaling and deployment. For example, payment processing (security-critical, rarely updated) differs fundamentally from recommendation engine (memory-intensive, constantly A/B tested). These components have different scaling patterns, deployment cycles, and risk profiles, which justify separate services.&lt;/p&gt;
    &lt;p&gt;The success of microservices hinges on clear business domain boundaries that match your team structure, as Conway‚Äôs Law predicts. If your organization naturally splits into autonomous teams that own distinct capabilities, microservices might work. (So, most ‚Äúone-and-a-half pizza‚Äù startups don‚Äôt qualify, do they?)&lt;/p&gt;
    &lt;p&gt;That‚Äôs why microservices work effectively for companies like Amazon and Uber‚Äîalthough not always.&lt;/p&gt;
    &lt;p&gt;In fact, most organizations lack the prerequisites: dedicated service ownership, mature CI/CD, robust monitoring, and crucially, scale that justifies the operational overhead. Startups that adapt microservices prematurely often regret their decision.&lt;/p&gt;
    &lt;p&gt;So ask yourself:&lt;/p&gt;
    &lt;p&gt;Are you using microservices to solve an independent scaling problem, or are you inviting more complexity than your solution needs?&lt;/p&gt;
    &lt;head rend="h2"&gt;The Great Microservices Reversal&lt;/head&gt;
    &lt;p&gt;Ironically, even though tech giants are the ones that are most likely to benefit from microservices, many of these very same companies are walking back their microservices architectures, and the results are eye-opening.&lt;/p&gt;
    &lt;head rend="h3"&gt;Amazon Prime Video: 90% Cost Reduction with a Monolith&lt;/head&gt;
    &lt;p&gt;In May 2023, Amazon engineers admitted the unthinkable: Prime Video had abandoned microservices for a monolith. Their Video Quality Analysis (VQA) team had built what looked like a textbook distributed system: AWS Step Functions and Lambda monitored thousands of video streams through independent, scalable components. On paper, it was serverless perfection.&lt;/p&gt;
    &lt;p&gt;In practice, it was a disaster. ‚ÄúWe realized that distributed approach wasn‚Äôt bringing a lot of benefits in our specific use case,‚Äù said Marcin Kolny in the now-archived Prime Video Engineering blog. Their ‚Äúinfinitely scalable‚Äù system crumbled at just 5% of expected load due to orchestration overhead.&lt;/p&gt;
    &lt;p&gt;The fix was embarrassingly simple: collapse everything into a single process. It resulted in 90% lower costs and faster performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Twilio Segment: From 140 Services to One Fast Monolith&lt;/head&gt;
    &lt;p&gt;Back in 2018, Twilio Segment, a customer data platform, documented a similar reversal in their brutally honest post ‚ÄúGoodbye Microservices‚Äù.&lt;/p&gt;
    &lt;p&gt;Their system had sprawled into 140+ services, creating operational chaos. At one point, three full-time engineers spent most of their time firefighting instead of building. As they admitted, ‚ÄúInstead of enabling us to move faster, the small team found themselves mired in exploding complexity. Essential benefits of this architecture became burdens. As our velocity plummeted, our defect rate exploded.‚Äù&lt;/p&gt;
    &lt;p&gt;Their solution was radical but effective: collapse all 140+ services into a single monolith. The impact was immediate. Test suites that once took an hour now finished in milliseconds. Developer productivity soared: they shipped 46 improvements to shared libraries in a year, up from 32 in the microservices era.&lt;/p&gt;
    &lt;head rend="h3"&gt;Shopify: Sanity over Hype&lt;/head&gt;
    &lt;p&gt;Shopify runs one of the largest Ruby on Rails codebases in the world (2.8M+ lines). Instead of chasing microservices, they deliberately chose a modular monolith: a single codebase with clear component boundaries.&lt;/p&gt;
    &lt;p&gt;Shopify‚Äôs engineers concluded that ‚Äúmicroservices would bring their own set of challenges‚Äù, so they chose modularity without the operational overhead.&lt;/p&gt;
    &lt;p&gt;All these examples beg the question:&lt;/p&gt;
    &lt;p&gt;If even the pioneers of microservices are retreating, why are we still treating it as gospel?&lt;/p&gt;
    &lt;head rend="h2"&gt;Expert Voices against Microservices Mania&lt;/head&gt;
    &lt;p&gt;Some of the most respected voices in software architecture‚Äîpeople behind many of the systems we all admire‚Äîare also cautioning against microservices and repeating mistakes they‚Äôve seen play out at scale. (After all, cheerleaders don‚Äôt play the game; cloud DevRels rarely build at scale.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Rails Creator: Simplicity over Sophistication&lt;/head&gt;
    &lt;p&gt;David Heinemeier Hansson (DHH), the creator of Ruby on Rails, has long advocated simplicity over architectural trends. His analysis of the Amazon Prime Video reversal puts it bluntly:&lt;/p&gt;
    &lt;p&gt;‚ÄúThe real-world results of all this theory are finally in, and it‚Äôs clear that in practice, microservices pose perhaps the biggest siren song for needlessly complicating your system.‚Äù&lt;/p&gt;
    &lt;p&gt;DHH‚Äôs image of a siren song is apt: microservices promise elegance but leave teams wrecked on the rocks of complexity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Microservices: Mistake of The Decade?&lt;/head&gt;
    &lt;p&gt;Jason Warner, former CTO of GitHub, doesn‚Äôt mince words while commenting on microservices:&lt;/p&gt;
    &lt;p&gt;‚ÄúI‚Äôm convinced that one of the biggest architectural mistakes of the past decade was going full microservice.‚Äù&lt;/p&gt;
    &lt;p&gt;Warner understands scale: GitHub runs at internet scale, and he‚Äôs led engineering at Heroku and Canonical. His critique cuts deeper because it‚Äôs lived experience, beyond theoretical advice:&lt;/p&gt;
    &lt;p&gt;‚Äú90% of all companies in the world could probably just be a monolith running against a primary db cluster with db backups, some caches and proxies and be done with it.‚Äù&lt;/p&gt;
    &lt;head rend="h3"&gt;GraphQL Co-Creator: ‚ÄúDon‚Äôt‚Äù&lt;/head&gt;
    &lt;p&gt;Then there‚Äôs Nick Schrock, co-creator of GraphQL. If anyone had a reason to cheer for distributed systems, it‚Äôd be him. Instead, he says:&lt;/p&gt;
    &lt;p&gt;‚ÄúMicroservices are such a fundamentally and catastrophically bad idea that there are going to be an entire cohort of multi-billion companies built that do nothing but contain the damage that they have wrought.‚Äù&lt;/p&gt;
    &lt;p&gt;He goes on to describe microservices as organizational gambles:&lt;/p&gt;
    &lt;p&gt;‚Äú[Y]ou end up with these services that you have to maintain forever that match the org structure and the product requirements from five years ago. Today, they don‚Äôt make a lot of sense.‚Äù&lt;/p&gt;
    &lt;p&gt;The person who literally built tools to fix distributed system pain says don‚Äôt distribute unless you must, maybe it‚Äôs time to listen.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other Voices Questioning Microservice Maximalism&lt;/head&gt;
    &lt;p&gt;Other engineering leaders are also reconsidering microservice maximalism.&lt;/p&gt;
    &lt;p&gt;At Uber, Gergely Orosz admitted:&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôre moving many of our microservices to macroservices (well-sized services). Exactly b/c testing and maintaining thousands of microservices is not only hard ‚Äì it can cause more trouble long-term than it solves the short-term.‚Äù&lt;/p&gt;
    &lt;p&gt;Uber still runs microservices where they‚Äôre justified, but they‚Äôre choosing their battles.&lt;/p&gt;
    &lt;p&gt;Kelsey Hightower, known for his work with Kubernetes and Google Cloud, cut through the microservices hype with CS101:&lt;/p&gt;
    &lt;p&gt;‚ÄúI‚Äôm willing to wager a monolith will outperform every microservice architecture. Just do the math on the network latency between each service and the amount of serialization and deserialization of each request.‚Äù&lt;/p&gt;
    &lt;p&gt;He subsequently deleted this tweet, but the network math still grades microservices.&lt;/p&gt;
    &lt;p&gt;When pioneers like these, including those who actually solved distributed systems at scale, start waving red flags, it‚Äôs worth taking note.&lt;/p&gt;
    &lt;p&gt;My question here is:&lt;/p&gt;
    &lt;p&gt;If GitHub‚Äôs CTO thinks 90% of companies don‚Äôt need microservices, are you sure yours is part of the 10%?&lt;/p&gt;
    &lt;head rend="h2"&gt;The Hidden Costs of Microservices&lt;/head&gt;
    &lt;p&gt;Microservices demand such caution because of these hidden costs that teams often underestimate.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operational Costs&lt;/head&gt;
    &lt;p&gt;A monolith is simple: in-process function calls.&lt;/p&gt;
    &lt;p&gt;Microservices replace that with networks. Every request now travels across machines, through load balancers, service meshes, and authentication layers, creating more failure points and infrastructure needs. You suddenly need service discovery (how services find each other), distributed tracing (tracking requests across services), centralized logging (aggregating logs from multiple services), and monitoring systems that understand service topology.&lt;/p&gt;
    &lt;p&gt;Each of these is necessary, but together they‚Äôre complex and expensive. Duplicated data requires extra storage. Constant service-to-service calls rack up network egress fees. Cloud costs scale faster than the apps they host. Prime Video‚Äôs workflow spent more on orchestrating S3 data transfers between services than on actual processing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Developer Productivity Drain&lt;/head&gt;
    &lt;p&gt;In microservices, the hard part isn‚Äôt writing code; it‚Äôs navigating distributed system interactions.&lt;/p&gt;
    &lt;p&gt;In ‚ÄúThe macro problem with microservices‚Äú, Stack Overflow identifies a critical productivity drain: distributed state forces developers to write defensive code that constantly checks for partial failures.&lt;/p&gt;
    &lt;p&gt;In a monolith, a developer can follow a code path end-to-end within one repo. In microservices, one feature might span four or five repos with different dependencies and deploy cycles. Adding a single field triggers weeks of coordination: you need to update one service, then wait for consumers to adopt, version your APIs, manage rollouts, and so on. Different teams will also typically maintain different microservices using different tech stacks, so there‚Äôs a risk that they unintentionally break something as well. Breaking changes that a compiler would catch in a monolith now surface as runtime errors in production.&lt;/p&gt;
    &lt;head rend="h3"&gt;Testing and Deployment Complexity&lt;/head&gt;
    &lt;p&gt;Monolith integration and end-to-end tests are faster because they run locally, in memory. Distributed systems don‚Äôt allow that luxury: real confidence requires integration and end-to-end tests across numerous service boundaries. So these tests are slower, more brittle, and require staging environments that resemble production, all of which effectively double infrastructure costs and slow feedback loops.&lt;/p&gt;
    &lt;p&gt;Many teams discover this only after their test suite becomes a bottleneck. Deployment orchestration adds another layer. Rolling updates across interdependent services require careful sequencing to avoid breaking contracts. Version incompatibility disturbs frequently: Service A works with Service B v2.1 but breaks with v2.2.&lt;/p&gt;
    &lt;p&gt;Failed deployments leave systems partially updated and difficult to recover.&lt;/p&gt;
    &lt;head rend="h3"&gt;Data Management and Consistency&lt;/head&gt;
    &lt;p&gt;The most underestimated complexity of microservices lies in data consistency across service boundaries.&lt;/p&gt;
    &lt;p&gt;Monoliths benefit from ACID transactions: operations complete entirely or fail entirely. Microservices split that across services, forcing you to build distributed saga (multi-step workflows with rollback logic), live with eventual consistency (data only becomes correct after a delay), or write compensation logic (extra code to undo partial failures). What was once a single database transaction now spans network hops, retries, and partial failures. Debugging inconsistent orders or payments gets much harder when state is duplicated across services.&lt;/p&gt;
    &lt;p&gt;As research confirms, data duplication, correctness challenges, and transactional complexity are the top pain points in microservice systems.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Compounding Effect&lt;/head&gt;
    &lt;p&gt;These complexities multiply. Operational overhead makes debugging harder, which slows testing, which makes deployments riskier, which creates more incidents. Microservices don‚Äôt just shift complexity from code to operations; they tax every part of your engineering process.&lt;/p&gt;
    &lt;p&gt;Unless your scale demands it, that tax often outweighs the benefits.&lt;/p&gt;
    &lt;p&gt;Think about it:&lt;/p&gt;
    &lt;p&gt;If every network hop adds complexity and cost, does your use case really justify the price?&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond Microservices: Smarter Architectural Alternatives&lt;/head&gt;
    &lt;p&gt;Before defaulting to microservices, it‚Äôs worth considering how simpler, well-structured architectures can deliver comparable scalability without the distributed complexity tax. Two noteworthy alternatives are modular monoliths and service-oriented architectures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Modular Monoliths: Structure without Distribution&lt;/head&gt;
    &lt;p&gt;Unlike traditional monoliths that become tangled messes, modular monoliths enforce strict internal boundaries through clear module APIs and disciplined separation. Each module exposes well-defined interfaces, enabling teams to work independently while deploying a single, coherent system.&lt;/p&gt;
    &lt;p&gt;As Kent Beck explains in ‚ÄúMonolith -&amp;gt; Services: Theory &amp;amp; Practice‚Äù, modular monoliths manage coupling through organizational discipline rather than distributed networks. The key difference: modules still communicate via explicit contracts like microservices, but they use fast, reliable function calls instead of HTTP requests that are vulnerable to network latency and partial failures.&lt;/p&gt;
    &lt;p&gt;Why does it work?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simpler operations: microservices-level organization with monolithic simplicity&lt;/item&gt;
      &lt;item&gt;Stronger consistency: full ACID transactions&lt;/item&gt;
      &lt;item&gt;Easier debugging: one traceable system, no hunting for bugs in the ELK haystack&lt;/item&gt;
      &lt;item&gt;Better performance: function calls beat network hops&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here‚Äôs some real-world proof: Shopify‚Äôs 2.8 million-line codebase handles 30TB per minute with separate teams owning distinct modules, yet everything deploys together. Facebook runs similarly. (And principal architect Keith Adams jokes that if you want to be talked out of microservices, he‚Äôs your guy.)&lt;/p&gt;
    &lt;p&gt;With recent developments in frameworks like Spring Modulith, Django, Laravel, and Rails (as seen at scale with Shopify), modular monoliths are poised to gain wider traction in the years ahead.&lt;/p&gt;
    &lt;head rend="h3"&gt;Service-Oriented Architecture: The Middle Ground&lt;/head&gt;
    &lt;p&gt;Service-oriented architecture (SOA) sits between monoliths and microservices, favoring larger, domain-driven services instead of dozens or hundreds of tiny ones. These services often communicate via an enterprise service bus (ESB), which reduces orchestration overhead while preserving separation of concerns.&lt;/p&gt;
    &lt;p&gt;Instead of splitting authentication, user preferences, and notifications into separate microservices, SOA might combine them into a single ‚ÄúUser Service‚Äù, simplifying coordination while preserving autonomy and targeted scaling. SOA provides enterprise-grade modularity without ultra-fine-grained distribution overhead.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs why it works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Right-sized boundaries: fewer, domain-aligned services instead of sprawl&lt;/item&gt;
      &lt;item&gt;Targeted scalability: scale services tied to real business domains&lt;/item&gt;
      &lt;item&gt;Pragmatic complexity: avoids ultra-fine-grained overhead while retaining modular reasoning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SOA has also been proven to work at scale. Norwegian Air Shuttle, Europe‚Äôs 9th-largest airline, used SOA to boost agility across complex flight operations. Credit Suisse‚Äôs SOA rollout powered millions of service calls per day back in the early 2000s.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choosing Wisely: Fit over Hype&lt;/head&gt;
    &lt;p&gt;The problem you‚Äôre solving should justify your architecture.&lt;/p&gt;
    &lt;p&gt;I often use this analogy in consulting: You don‚Äôt need a sword to cut a lemon‚Äîa knife suffices. And as timeless wisdom reminds us, simplicity is the ultimate sophistication.&lt;/p&gt;
    &lt;p&gt;In all likelihood, you‚Äôre not Google (you don‚Äôt need Google-level fault tolerance), or Amazon (you don‚Äôt need massive write availability), or LinkedIn (you don‚Äôt handle billions of events a day). Most applications don‚Äôt operate at that scale, demanding fundamentally different solutions than ultra-distributed architectures.&lt;/p&gt;
    &lt;p&gt;For most systems, well-structured modular monoliths (for most common applications, including startups) or SOA (enterprises) deliver comparable scalability and resilience as microservices, without the distributed complexity tax. Alternatively, you may also consider well-sized services (macroservices, or what Gartner proposed as miniservices) instead of tons of microservices.&lt;/p&gt;
    &lt;p&gt;It‚Äôs worth asking:&lt;/p&gt;
    &lt;p&gt;If simpler architectures can deliver comparable scalability, why are you choosing the complexity of microservices?&lt;/p&gt;
    &lt;head rend="h2"&gt;Docker: Built for Any Architecture&lt;/head&gt;
    &lt;p&gt;Docker isn‚Äôt just for microservices‚Äîit works great across all kinds of architectures like monoliths, SOA, APIs, and event-driven systems. The real benefit is that Docker gives you consistent performance, easier deployment, and flexibility to scale up your apps no matter what architectural approach you‚Äôre using.&lt;/p&gt;
    &lt;p&gt;Docker packages applications cleanly, keeps environments consistent from laptop to production, simplifies dependency management, and isolates applications from the host system. A Dockerized monolith offers all these benefits, minus the orchestration overhead of microservices.&lt;/p&gt;
    &lt;p&gt;Microsoft‚Äôs guidance on containerizing monoliths clarifies that scaling containers is ‚Äúfar faster and easier than deploying additional VMs‚Äù, whether you run one service or fifty. Twilio Segment observed that containerized monoliths can ‚Äúhorizontally scale your environment easily by spinning up more containers and shutting them down when demand subsides.‚Äù For many applications, scaling the whole app is exactly what‚Äôs needed.&lt;/p&gt;
    &lt;p&gt;As for DevOps, a monolith in Docker is lighter to operate than a full-blown microservices setup. Logging aggregation becomes simpler when you‚Äôre collecting from identical containers rather than disparate services with different formats. Monitoring and debugging remain centralized, and troubleshooting avoids tracing requests across service boundaries.&lt;/p&gt;
    &lt;p&gt;So, it‚Äôs definitely worth considering:&lt;/p&gt;
    &lt;p&gt;Even without the complexity of microservices, Docker gives you the same advantages ‚Äî clean deployments, easy scaling, and consistent environments. So why not keep it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping Up&lt;/head&gt;
    &lt;p&gt;A few years ago, my then-8-year-old wanted a bicycle. He‚Äôd mostly ride around our apartment complex, maybe venture into the nearby lane. He didn‚Äôt need 21 gears, but those shiny shifters had him smitten‚Äîimagine riding faster by changing those gears! He absolutely wanted that mechanically complex beauty. (It‚Äôs hard to argue with a starry-eyed kid‚Ä¶ or a founder :P).&lt;/p&gt;
    &lt;p&gt;Once he started riding the new bike, the gears slipped, the chain jammed, and the bicycle spent more time broken than on the road. Eventually, we had to dump it.&lt;/p&gt;
    &lt;p&gt;I wasn‚Äôt able to convince him back then that a simpler bicycle could‚Äôve served him better, but maybe this article will convince a few grown-ups making architectural decisions.&lt;/p&gt;
    &lt;p&gt;We techies love indulging in complex systems. (Check: were you already thinking, What‚Äôs complex about bicycles with gears??) But the more moving parts you add, the more often they break. Complexity often creates more problems than it solves.&lt;/p&gt;
    &lt;p&gt;The point I‚Äôm making isn‚Äôt to dump microservices entirely‚Äîit‚Äôs to pick an architecture that fits your actual needs, not what the cloud giant is pushing (while quietly rolling back their own commit). Most likely, modular monoliths or well-designed SOA will serve your needs better and make your team more productive.&lt;/p&gt;
    &lt;p&gt;So here‚Äôs the million-dollar question:&lt;/p&gt;
    &lt;p&gt;Will you design for cloud-native hype or for your own business requirements?&lt;/p&gt;
    &lt;p&gt;Do you really need microservices?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.docker.com/blog/do-you-really-need-microservices/"/><published>2025-11-30T19:02:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46099985</id><title>People keep flocking to Linux, not just to escape Windows</title><updated>2025-11-30T23:09:27.042217+00:00</updated><content>&lt;doc fingerprint="159ab95bfb87b4ee"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why people keep flocking to Linux in 2025 (and it's not just to escape Windows)&lt;/head&gt;
    &lt;p&gt;Follow ZDNET: Add us as a preferred source on Google.&lt;/p&gt;
    &lt;head rend="h3"&gt;ZDNET key takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Linux desktop has continued its slow growth.&lt;/item&gt;
      &lt;item&gt;Linux has been making gains in no small part because of Microsoft Windows' blunders.&lt;/item&gt;
      &lt;item&gt;Users and governments have been losing trust in Windows and Microsoft.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My colleague Jack Wallen and I have been telling you for a while now that you should switch from Windows to the Linux desktop. Sounds like some of you have been listening.&lt;/p&gt;
    &lt;p&gt;The proof of the pudding comes from various sources. First, with Windows 10 nearing the end of its supported life, we told you to consider switching from Windows to Linux Mint or another Windows-like Linux distribution. What do we find now?&lt;/p&gt;
    &lt;p&gt;Also: The most beautiful Linux distributions for 2025&lt;/p&gt;
    &lt;p&gt;Zorin OS, an excellent Linux desktop, reports that its latest release, "Zorin OS 18 has amassed 1 million downloads in just over a month since its release." What makes it especially interesting is that over "78% of these downloads came from Windows" users.&lt;/p&gt;
    &lt;p&gt;Now, that's got my attention... 780,000 Windows users don't download a 3.5 Gigabyte Linux desktop distribution if they're not giving it serious consideration. Linux desktop fans download different distros all the time. For them, it's a hobby.&lt;/p&gt;
    &lt;p&gt;For Windows users? You have to think they are considering making the Linux switch.&lt;/p&gt;
    &lt;head rend="h2"&gt;The real top dog operating system&lt;/head&gt;
    &lt;p&gt;Many have already been making the leap. By May 2025, StatCounter data showed the Linux desktop had grown from a minute 1.5% global desktop share in 2020 to above 4% in 2024, and was at a new American high of above 5% by 2025.&lt;/p&gt;
    &lt;p&gt;In StatCounter's latest US numbers, which cover through October, Linux shows up as only 3.49%. But if you look closer, "unknown" accounts for 4.21%. Allow me to make an educated guess here: I suspect those unknown desktops are actually running Linux. What else could it be? FreeBSD? Unix? OS/2? Unlikely.&lt;/p&gt;
    &lt;p&gt;Also: What Linus Torvalds really thinks about AI and software development might surprise you&lt;/p&gt;
    &lt;p&gt;In addition, ChromeOS comes in at 3.67%, which strikes me as much too low. Leaving that aside, ChromeOS is a Linux variant. It just uses the Chrome web browser for its interface rather than KDE Plasma, Cinnamon, or another Linux desktop environment. Put all these together, and you get a Linux desktop market share of 11.37%. Now we're talking.&lt;/p&gt;
    &lt;p&gt;If you want to look at the broader world of end-user operating systems, including phones and tablets, Linux comes out even better. In the US, where we love our Apple iPhones, Android -- yes, another Linux distro -- boasts 41.71% of the market share, according to StatCounter's latest numbers. Globally, however, Android rules with 72.55% of the market.&lt;/p&gt;
    &lt;p&gt;Yes, that's right, if you widen the Linux end-user operating system metric to include PC, tablets, and smartphones, you can make a reasonable argument that Linux, and not Windows, is already the top dog operating system. Take that, Redmond!&lt;/p&gt;
    &lt;head rend="h2"&gt;The view from DAP&lt;/head&gt;
    &lt;p&gt;Now, of course, StatCounter's numbers, as Ed Bott has pointed out, have their problems. So I also looked at my preferred data source for operating system numbers: the US federal government's Digital Analytics Program (DAP).&lt;/p&gt;
    &lt;p&gt;Also: Inside Canonical's plan to make Ubuntu 26.04 the Linux desktop that finally goes mainstream&lt;/p&gt;
    &lt;p&gt;This site gives a running count of US government website visits and an analysis. On average, there are 1.6 billion sessions over the last 30 days, with millions of users per day. In short, DAP gives a detailed view of what people use without massaging the data.&lt;/p&gt;
    &lt;p&gt;DAP gets its raw data from a Google Analytics account. DAP has open-sourced the code, which displays the data on the web, and its data-collection code. You can download its data in JavaScript Object Notation (JSON) format so you can analyze the raw numbers yourself.&lt;/p&gt;
    &lt;p&gt;By DAP's count, the Linux desktop now has a 5.8% market share. That may not sound impressive, but when I started looking at DAP's numbers a decade ago, the Linux desktop had a mere 0.67% share. We've come a long way.&lt;/p&gt;
    &lt;p&gt;If you add Chrome OS (1.7%) and Android (15.8%), 23.3% of all people accessing the US government's websites are Linux users. The Linux kernel's user-facing footprint is much larger than the "desktop Linux" label suggests.&lt;/p&gt;
    &lt;p&gt;Also: 5 factors steadily fueling Linux's desktop rise&lt;/p&gt;
    &lt;p&gt;I'll also note that although Windows 10 should be heading into retirement by now, DAP numbers show it's still the most popular version of Windows, with 16.9% compared to Windows 11's 13.5%. StatCounter, however, has Windows 11 leading in the US, with 64.83% of all Windows users, while Windows 10 is well behind with 31.92%.&lt;/p&gt;
    &lt;p&gt;That's still way too many Windows 10 users taking chances with their security.&lt;/p&gt;
    &lt;p&gt;But wait, there's more data. According to Lansweeper, an IT asset discovery and inventory company, in its analysis of over 15 million identified consumer desktop operating systems, Linux desktops currently account for just over 6% of PC market share.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why is Linux growing?&lt;/head&gt;
    &lt;p&gt;Earlier this year, I identified five drivers for people switching from Windows to Linux. These are: Microsoft's shift of focus from Windows as a product to Microsoft 365 and cloud services, the increased viability of gaming via Steam and Proton, drastically improved ease of use in mainstream distros, broader hardware support, and rising concern about privacy and data control.&lt;/p&gt;
    &lt;p&gt;Three others have emerged since then. One is that many companies and users still have perfectly good Windows 10 machines that can't "upgrade" to Windows 11. ControlUp, a company that would love to help you move to Windows 11, has found that about 25% of consumer and business Windows 10 PCs can't be moved to Windows 11.&lt;/p&gt;
    &lt;p&gt;Also: How to upgrade your 'incompatible' Windows 10 PC to Windows 11 for free - today&lt;/p&gt;
    &lt;p&gt;This hardware lockout is a key reason many users plan to ride Windows 10 beyond the end of support instead of paying for a new machine just to meet Microsoft's specs.&lt;/p&gt;
    &lt;p&gt;Another is that many people really, really don't want to move to Windows 11. A UK survey by consumer group Which? in September 2025 found that 26% of respondents intended to keep using Windows 10 even after updates stopped. Interestingly, 6% plan to go to an alternative operating system such as Linux.&lt;/p&gt;
    &lt;p&gt;Why so stubborn? Well, besides wanting to save money, surveys and vendor analyses consistently mention three main reasons users hesitate: Windows 10 is "good enough," Windows 11 is not seen as meaningfully better, and they don't like the forced interface changes (e.g., the centered Start menu, context menus, default app behavior, and Copilot integration).&lt;/p&gt;
    &lt;p&gt;Also: Linux's remarkable journey from one dev's hobby to 40 million lines of code - and counting&lt;/p&gt;
    &lt;p&gt;Gaming users are also afraid that Windows 11 will slow down their games or come with compatibility bugs. For example, Windows 11's October update came with a bug that hurt gaming performance on some NVIDIA-equipped gaming PCs.&lt;/p&gt;
    &lt;p&gt;Finally, not everyone is thrilled with Windows 11 being turned into an AI-agentic operating system. Despite all the AI hype, some people don't want AI second-guessing their every move or reporting on their work to Microsoft.&lt;/p&gt;
    &lt;p&gt;After Microsoft president Pavan Davulur tweeted on Nov. 10 that "Windows is evolving into an agentic OS, connecting devices, cloud, and AI to unlock intelligent productivity and secure work anywhere," he probably expected Windows users to be happy with this vision. They weren't.&lt;/p&gt;
    &lt;p&gt;Also: Can't upgrade to Windows 11? This Linux distro is the best alternative for your Windows 10 PC&lt;/p&gt;
    &lt;p&gt;Instead, the top response from one person on X was "It's evolving into a product that's driving people to Mac and Linux." Exactly so. If you want a traditional desktop where you control what's what on your PC without an AI Big Brother looking over your shoulder, Linux will be almost your only choice going forward.&lt;/p&gt;
    &lt;head rend="h2"&gt;Don't forget digital sovereignty&lt;/head&gt;
    &lt;p&gt;My last reason for people looking to Linux from Windows doesn't matter much to users in the US, but it matters a lot to people outside the US. You see, the European Union (EU) governments don't trust Microsoft to deliver on its service promises under potential US political pressure.&lt;/p&gt;
    &lt;p&gt;This has resulted in the rise of Digital Sovereignty initiatives, where EU companies and not American tech giants are seen as much more trustworthy. As a result, many EU states have dropped Microsoft programs and have switched to open-source software.&lt;/p&gt;
    &lt;p&gt;Also: Yet another European government is ditching Microsoft for Linux - here's why&lt;/p&gt;
    &lt;p&gt;That includes the desktop. Indeed, one EU group has created EU OS. This is a proof-of-concept Linux desktop for a Fedora-based distro that uses the KDE Plasma desktop environment.&lt;/p&gt;
    &lt;p&gt;It's not just the EU. The UK also no longer trusts Microsoft with its data. A 2024 Computer Weekly report revealed that Microsoft told Scottish police it could not guarantee that data in Microsoft 365 and Azure would remain in the UK.&lt;/p&gt;
    &lt;p&gt;Nicky Stewart, former UK Cabinet Office ICT chief, told Computer Weekly at the time: "You've got Microsoft touting what they describe as sovereign cloud, but what do they mean by sovereign...because truly sovereign data would not be offshored under any circumstances."&lt;/p&gt;
    &lt;p&gt;With Windows 11 data potentially being sent back and forth to US data centers, more and more governments will be wary about trusting Windows.&lt;/p&gt;
    &lt;p&gt;Also: Another European agency shifts off Big Tech, as digital sovereignty movement gains steam&lt;/p&gt;
    &lt;p&gt;Taken together, all these shifts make Linux less of a tinker's special and more of a pragmatic option for people who want out of the Windows upgrade treadmill or subscription model.&lt;/p&gt;
    &lt;p&gt;Desktop Linux is moving from perennial underdog to a small but meaningful slice of everyday computing, especially among technically inclined users, non-American public-sector agencies, and ordinary consumer and business users who want a cheaper, more trustworthy desktop.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.zdnet.com/article/why-people-keep-flocking-to-linux-in-2025-and-its-not-just-to-escape-windows/"/><published>2025-11-30T20:14:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46100323</id><title>"Boobs check" ‚Äì Technique to verify if sites behind CDN are hosted in Iran</title><updated>2025-11-30T23:09:26.588923+00:00</updated><content>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://twitter.com/hkashfi/status/1995109785679573167"/><published>2025-11-30T20:54:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46100566</id><title>Stackoverflow Outage</title><updated>2025-11-30T23:09:26.354634+00:00</updated><content/><link href="https://www.stackstatus.net/"/><published>2025-11-30T21:24:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46100887</id><title>The reality of life after getting your brand stocked in a national retailer</title><updated>2025-11-30T23:09:26.238491+00:00</updated><content/><link href="https://old.reddit.com/r/ausbusiness/comments/1pa94j9/ink_nurse_how_our_small_aussie_business_performed/"/><published>2025-11-30T22:04:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46100892</id><title>A Love Letter to FreeBSD</title><updated>2025-11-30T23:09:25.704852+00:00</updated><content>&lt;doc fingerprint="378a913fe1694f6e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Love Letter to FreeBSD&lt;/head&gt;
    &lt;p&gt;Dear FreeBSD,&lt;/p&gt;
    &lt;p&gt;I‚Äôm still the new person here, learning your ways, stumbling over the occasional quirk, smiling when I find the small touches that make you different. You remind me of what computing felt like before the noise. Before hype cycles and performance theatre. Before every tool needed a plugin system and a logo. You are coherent. You are deliberate. You are the kind of system that doesn‚Äôt have to shout to belong.&lt;/p&gt;
    &lt;p&gt;You carry the quiet strength of the greats, like a mainframe humming in a locked room, not chasing attention, just doing its work, year after year. Your base system feels like it was built by people who cared about the whole picture, not just the pieces. Your boot environments are like an old IBM i‚Äôs ‚Äúside A / side B‚Äù IPL, a built-in escape hatch that says, we‚Äôve thought ahead for you. You could be, you should be, the open-source mainframe: aligned with hardware lifecycles of three to five years or more, built for long-term trust, a platform people bet their uptime on. Your core design reminds me of Solaris in its best days: a stable base that commercial and community software could rely on without fear of shifting foundations.&lt;/p&gt;
    &lt;p&gt;And make uptime a design goal: a thousand-day uptime shouldn‚Äôt be folklore, it should be normal. Not a party trick, not a screenshot to boast about, but simply the natural consequence of a system built to endure. Mainframes never apologised for uptime measured in years, and neither should you. Apply updates without fear, reboot only when the kernel truly demands it, and let administrators see longevity as a feature, not a gamble.&lt;/p&gt;
    &lt;p&gt;I know you are reaching further into the desktop now. I understand why, and I can see how it might widen your reach. But here I find myself wondering: how do you keep the heartbeat of a rock-solid server while also embracing the quicker pulse of a modern desktop? I don‚Äôt pretend to have all the answers, I‚Äôm too new to you for that, but my first instinct is to lean on what you already have: the natural separation between CURRENT and RELEASE. Let those worlds move at their own pace, without asking one to carry the other‚Äôs compromises.&lt;/p&gt;
    &lt;p&gt;And now, with pkgbase in play, the stability of packages matters as much as the base system itself. The base must remain untouchable in its reliability, but I dream of a world where the package ecosystem is available in clear stability channels: from a rock-solid ‚Äúproduction tier‚Äù you can stake a business on, to faster-moving streams where new features can flow without fear of breaking mission-critical systems. Too many times in the past, packages vanished or broke unexpectedly. I understand the core is sacred, but I wouldn‚Äôt mind if some of the wider ecosystem inherited that same level of care.&lt;/p&gt;
    &lt;p&gt;Culture matters too. One reason I stepped away from Linux was the noise, the debates that drowned out the joy of building. Please keep FreeBSD the kind of place where thoughtful engineering is welcome without ego battles, where enterprise focus and technical curiosity can sit at the same table. That spirit, the calm, shared purpose that carried Unix from the PDP-11 labs to the backbone of the Internet, is worth protecting.&lt;/p&gt;
    &lt;p&gt;There‚Äôs also the practical side: keep the doors open with hardware vendors like Dell and HPE, so FreeBSD remains a first-class citizen. Give me the tools to flash firmware without having to borrow Linux or Windows. Make hardware lifecycle alignment part of your story, major releases paced with the real world, point releases treated as refinement rather than disruption.&lt;/p&gt;
    &lt;p&gt;My hope is simple: that you stay different. Not in the way that shouts for attention, but in the way that earns trust. If someone wants hype or the latest shiny thing every month, they have Linux. If they want a platform that feels like it could simply run, and keep running, the way the best of Unix always did, they should know they can find it here. And I still dream of a future where a purpose-built ‚Äúopen-source mainframe‚Äù exists: a modern, reliable hardware system running FreeBSD with the same quiet presence as Sun‚Äôs Enterprise 10k once did.&lt;/p&gt;
    &lt;p&gt;And maybe, one day, someone will walk past a rack of servers, hear the steady, unhurried rhythm of a FreeBSD system still running, and smile, knowing that in a world that burns through trends, there is still something built to last.&lt;/p&gt;
    &lt;p&gt;With gratitude,&lt;lb/&gt; and with the wish to stay for the long run,&lt;lb/&gt; A newcomer who finally feels at home.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tara.sh/posts/2025/2025-11-25_freebsd_letter/"/><published>2025-11-30T22:05:07+00:00</published></entry></feed>