<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-01-14T00:56:56.010231+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46603111</id><title>Are two heads better than one?</title><updated>2026-01-14T00:57:07.228109+00:00</updated><content>&lt;doc fingerprint="e262c3369163d09e"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Are Two Heads Better Than One?&lt;/head&gt;
    &lt;p&gt;Three heads are certainly more fun&lt;/p&gt;
    &lt;p&gt;Dec 9, 2025&lt;/p&gt;
    &lt;p&gt;You’re playing a game with your lying friends Alice and Bob.&lt;/p&gt;
    &lt;p&gt;Bob flips a coin and shows it to Alice. Alice tells you what she saw - but she lies 20% of the time. Then you take your best guess on whether the coin is heads or tails.&lt;/p&gt;
    &lt;p&gt;Your best strategy is to trust whatever Alice says. You’re right 80% of the time.&lt;/p&gt;
    &lt;p&gt;Now Bob joins in. He makes up his mind independent of Alice, and he also lies 20% of the time 1.&lt;/p&gt;
    &lt;p&gt;Your friends are all liars!&lt;/p&gt;
    &lt;p&gt;You were right 80% of the time by trusting Alice.&lt;/p&gt;
    &lt;p&gt;How much better can you do with Bob’s help?&lt;/p&gt;
    &lt;head rend="h2"&gt;Here’s some empty space for you to think&lt;/head&gt;
    &lt;p&gt;I’m going to give you the answer below. So here’s some empty space for you to think in case you want to do the math yourself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Alright, let’s do some math&lt;/head&gt;
    &lt;p&gt;The answer is 0% - you don’t do any better! You’re still exactly 80% to get the right answer.&lt;/p&gt;
    &lt;p&gt;To establish this, let’s write a simple simulation. We’ll flip a coin a million times, ask our friends what they saw, and observe the results.&lt;/p&gt;
    &lt;p&gt;For our strategy, we’ll look at a fact pattern (like “Alice says heads”), figure out what’s most likely (“the coin is heads”), and say “we guess the coin flip correctly whenever the most likely outcome occurs for this fact pattern” 2.&lt;/p&gt;
    &lt;p&gt;Given that Bob and Alice decide independently and aren’t playing adversarially, “guess the most likely outcome” is optimal here. It may not be optimal if the game was adversarial, although I think that’s a trickier question than it might first seem.&lt;/p&gt;
    &lt;p&gt;Here’s the code for that simulation. We’ll start with the easy case (just Alice):&lt;/p&gt;
    &lt;head class="sc-4d1d4ca-1 bowwWe"&gt;The simulation code&lt;/head&gt;
    &lt;code&gt;# heads.py

from random import random
from collections import defaultdict

table = defaultdict(lambda: [0, 0])
LYING_PROB = 0.2
LYING_FRIENDS = ["Alice"]
ITERATIONS = 1_000_000

for _ in range(ITERATIONS):
    is_heads = random() &amp;gt; 0.5
    keys = []
    for lying_friend in LYING_FRIENDS:
        lied = random() &amp;lt; LYING_PROB
        answer = None
        if is_heads: answer = "T" if lied else "H"
        else: answer = "H" if lied else "T"
        keys.append(f"{lying_friend[0]}:{answer}")

    key = ", ".join(keys)

    table_idx = 0 if is_heads else 1
    table[key][table_idx] += 1

total_times_we_are_right = 0
for key, (times_heads, times_tails) in table.items():
    total = times_heads + times_tails
    heads_chance = 100 * round(times_heads / total, 2)
    tails_chance = 100 * round(times_tails / total, 2)
    pattern_chance = 100 * round(total / ITERATIONS, 2)

    print(f"{key} - chances -  H {heads_chance:4}% | T {tails_chance:4}% | occurs {pattern_chance}% of the time")

    # We look at key, and guess whichever outcome is more likely. So we're right, on average,
    # the max of times_heads and times_tails
    total_times_we_are_right += max(times_heads, times_tails)

accuracy = round(total_times_we_are_right / ITERATIONS, 2)
print(f"\nOur accuracy: {100*accuracy}%")
&lt;/code&gt;
    &lt;p&gt;This gives us:&lt;/p&gt;
    &lt;code&gt;% python heads.py
A:T - chances -  H 20.0% | T 80.0% | occurs 50.0% of the time
A:H - chances -  H 80.0% | T 20.0% | occurs 50.0% of the time

Our accuracy: 80.0%
&lt;/code&gt;
    &lt;p&gt;Now let’s add Bob to the simulation. We see something like this:&lt;/p&gt;
    &lt;code&gt;% python heads.py
A:T, B:T - chances -  H  6.0% | T 94.0% | occurs 34.0% of the time
A:H, B:T - chances -  H 50.0% | T 50.0% | occurs 16.0% of the time
A:H, B:H - chances -  H 94.0% | T  6.0% | occurs 34.0% of the time
A:T, B:H - chances -  H 50.0% | T 50.0% | occurs 16.0% of the time

Our accuracy: 80.0%
&lt;/code&gt;
    &lt;p&gt;That’s weird! But perhaps this gives you an intuition for what’s happening. By introducing a second player, we introduce the possibility of a tie.&lt;/p&gt;
    &lt;p&gt;A decent amount of the time, Alice and Bob agree. Most (~94%) of the time when that happens, they’re telling the truth. Occasionally they’re both lying, but that’s pretty unlikely.&lt;/p&gt;
    &lt;p&gt;But a meaningful portion of the time (32%) Alice says heads and Bob says tails, or vice versa. And in that case we don’t know anything at all! Alice and Bob are equally trustworthy and they disagreed - we’d be better off if we’d just gone and asked Alice 3!&lt;/p&gt;
    &lt;p&gt;I am deeply curious whether anyone else was read the book “Go Ask Alice” by their middle school science teacher in order to scare them straight or whether that was specific to my middle school experience.&lt;/p&gt;
    &lt;head rend="h3"&gt;Let’s prove it&lt;/head&gt;
    &lt;p&gt;Now that we’ve simulated this result, let’s walk through each case assuming that the coin landed on heads.&lt;/p&gt;
    &lt;code&gt;- both tell the truth
Alice: Heads (80%), Bob: Heads (80%)
happens 80% * 80% = 64% of the time
we always guess correctly in this case

- both lie
Alice: Tails (20%), Bob: Tails (20%)
happens 20% * 20% = 4% of the time
we never guess correctly in this case

- alice tells the truth, bob lies
Alice: Heads (80%), Bob: Tails (20%)
happens 80% * 20% = 16% of the time
we guess at random in this case; we're right 50% of the time

- alice lies, bob tells the truth
Alice: Tails (20%), Bob: Heads (80%)
happens 20% * 80% = 16% of the time
we guess at random in this case; we're right 50% of the time

Our total chance to guess correctly is:
64% + 16% / 2 + 16% / 2 = 64% + 8% + 8% = 80%
&lt;/code&gt;
    &lt;p&gt;There’s something beautiful here. Our total chance to guess remains at 80% because our additional chance to guess correctly when Alice and Bob agree is perfectly offset by the chance that Alice and Bob disagree!&lt;/p&gt;
    &lt;head rend="h3"&gt;Meet Charlie (and David)&lt;/head&gt;
    &lt;p&gt;If our friend Charlie - who also lies 20% of the time - joins the fun, our odds improve substantially. If Bob and Alice disagree, Charlie can act as a tiebreaker.&lt;/p&gt;
    &lt;code&gt;% python heads.py
A:H, B:H, C:H - chances -  H 98.0% | T  2.0% | occurs 26.0% of the time
A:T, B:T, C:T - chances -  H  2.0% | T 98.0% | occurs 26.0% of the time
A:T, B:H, C:H - chances -  H 80.0% | T 20.0% | occurs 8.0% of the time
A:H, B:T, C:T - chances -  H 20.0% | T 80.0% | occurs 8.0% of the time
A:H, B:H, C:T - chances -  H 80.0% | T 20.0% | occurs 8.0% of the time
A:H, B:T, C:H - chances -  H 80.0% | T 20.0% | occurs 8.0% of the time
A:T, B:T, C:H - chances -  H 20.0% | T 80.0% | occurs 8.0% of the time
A:T, B:H, C:T - chances -  H 20.0% | T 80.0% | occurs 8.0% of the time

Our accuracy: 90.0%
&lt;/code&gt;
    &lt;p&gt;But if David joins, the pattern repeats. David introduces the possibility of a 2-2 split, and our odds don’t improve at all!&lt;/p&gt;
    &lt;code&gt;% python heads.py
A:T, B:T, C:T, D:T - chances -  H  0.0% | T 100.0% | occurs 21.0% of the time
A:T, B:H, C:H, D:H - chances -  H 94.0% | T  6.0% | occurs 5.0% of the time
A:T, B:H, C:T, D:T - chances -  H  6.0% | T 94.0% | occurs 5.0% of the time
A:H, B:H, C:H, D:H - chances -  H 100.0% | T  0.0% | occurs 21.0% of the time
A:H, B:T, C:H, D:T - chances -  H 50.0% | T 50.0% | occurs 3.0% of the time
A:T, B:T, C:H, D:H - chances -  H 50.0% | T 50.0% | occurs 3.0% of the time
A:H, B:T, C:H, D:H - chances -  H 94.0% | T  6.0% | occurs 5.0% of the time
A:T, B:T, C:T, D:H - chances -  H  6.0% | T 94.0% | occurs 5.0% of the time
A:H, B:T, C:T, D:T - chances -  H  6.0% | T 94.0% | occurs 5.0% of the time
A:H, B:H, C:H, D:T - chances -  H 94.0% | T  6.0% | occurs 5.0% of the time
A:H, B:H, C:T, D:T - chances -  H 50.0% | T 50.0% | occurs 3.0% of the time
A:T, B:T, C:H, D:T - chances -  H  6.0% | T 94.0% | occurs 5.0% of the time
A:H, B:H, C:T, D:H - chances -  H 94.0% | T  6.0% | occurs 5.0% of the time
A:T, B:H, C:T, D:H - chances -  H 50.0% | T 50.0% | occurs 3.0% of the time
A:H, B:T, C:T, D:H - chances -  H 50.0% | T 50.0% | occurs 3.0% of the time
A:T, B:H, C:H, D:T - chances -  H 50.0% | T 50.0% | occurs 3.0% of the time

Our accuracy: 90.0%
&lt;/code&gt;
    &lt;p&gt;And this continues, on and on, forever (as long as we have enough friends). If our number &lt;code&gt;N&lt;/code&gt; of friends is odd, our chances of guessing correctly don’t improve when we move to &lt;code&gt;N+1&lt;/code&gt; friends.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is there a name for this?&lt;/head&gt;
    &lt;p&gt;As far as I can tell, there’s no name for this weird little phenomenon. But it does appear, implicitly, in voting literature.&lt;/p&gt;
    &lt;p&gt;Condorcet’s jury theorem is a famous theorem in political science. It states:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you have a group of voters of size &lt;code&gt;N&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;…and they all vote, independently, on an issue with a correct answer&lt;/item&gt;
      &lt;item&gt;…and each voter votes the “right” way with probability &lt;code&gt;P&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;…and we make whatever decision the majority of the voters vote for&lt;/item&gt;
      &lt;item&gt;…then if &lt;code&gt;P &amp;gt; 50%&lt;/code&gt;, the chance that we make the right decision approaches 100% as we add more voters&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sounds a fair bit like our coin flipping problem. Here’s a simplifying assumption that Wikipedia makes when proving the theorem:&lt;/p&gt;
    &lt;p&gt;Hah! The proof explicitly recognizes (and dodges) the even-voter case precisely because that voter doesn’t add any information.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why did I write this?&lt;/head&gt;
    &lt;p&gt;I stumbled upon this result while writing a simulation for a more complex problem. I was so surprised at the simulation results that I assumed that I had a bug in my code. And when I walked through the math by hand I was absolutely delighted.&lt;/p&gt;
    &lt;p&gt;I suspect some of the surprise for me was because I typically encounter problems like these in the context of betting, not voting. If we’re betting on coin flips, we’re certainly excited to bet more if Alice and Bob agree than if we’re just listening to Alice.&lt;/p&gt;
    &lt;p&gt;But voting is a different beast; our outcome is binary. There’s no way to harvest the additional EV from the increased confidence Bob sometimes gives us.&lt;/p&gt;
    &lt;p&gt;By the way - I encountered this problem while working with a friend at The Recurse Center (a writers retreat for programmers). It’s a great place to get nerd sniped by silly math problems; If you enjoyed this blog consider applying!&lt;/p&gt;
    &lt;p&gt;Anyway. I hope this delights you like it did me.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://eieio.games/blog/two-heads-arent-better-than-one/"/><published>2026-01-13T16:22:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46603535</id><title>Influencers and OnlyFans models are dominating U.S. O-1 visa requests</title><updated>2026-01-14T00:57:06.993828+00:00</updated><content>&lt;doc fingerprint="32439b71b1a3afb9"&gt;
  &lt;main&gt;
    &lt;p&gt;Content creators and influencers in the US are now increasingly applying for O-1 work visas. Astoundingly, the number of O-1 visas granted each year increased by 50% between 2014 and 2024, as noted by recent reporting in the Financial Times.&lt;/p&gt;
    &lt;p&gt;These visas allow non-immigrants to work temporarily in the US. The O-1 category includes the O-1A, which is designated for individuals with extraordinary ability in the sciences, education, business or athletics and the O-1B, reserved for those with “extraordinary ability or achievement”.&lt;/p&gt;
    &lt;p&gt;The Guardian spoke with some influencers who have had success in obtaining or are still trying to obtain the coveted O-1 visa and talked about what was involved in their process.&lt;/p&gt;
    &lt;p&gt;Julia Ain decided to post some videos of herself on social media at the height of the Covid-19 lockdown, when she was a student at McGill University.&lt;/p&gt;
    &lt;p&gt;“I was bored during the pandemic – like everyone else – and started posting on TikTok,” she told the Guardian. “I started livestreaming, and I grew a fanbase kind of quickly.”&lt;/p&gt;
    &lt;p&gt;Five years later, the 25-year-old Canadian content creator now has 1.3 million followers combined across various social media platforms. Her influencer success led her to an O-1 visa.&lt;/p&gt;
    &lt;p&gt;“It became really obvious that you could make a lot of money doing this in a short period of time,” she said. “It felt like a very time-sensitive thing. Nobody knows how long this is going to last for.”&lt;/p&gt;
    &lt;p&gt;Ain posts photos and videos across Instagram, TikTok, X and Snapchat, sometimes in collaboration with other creators. Of her brand, she says: “My whole thing is being the funny Jewish girl with big boobs.” The majority of Ain’s income is from Fanfix, a safe-for-work subscription based platform for influencers to monetize their content. She first applied for the O-1B Visa after launching on the platform in August 2023, and the company ended up sponsoring her application. She now says she makes five figures per month on the platform.&lt;/p&gt;
    &lt;p&gt;Luca Mornet also began making content during the pandemic while he was a student at the Fashion Institute of Technology in New York. Mornet, who is from France, realized soon that his F-1 student visa was holding him back from making money as an influencer.&lt;/p&gt;
    &lt;p&gt;“I became friends with so many [other influencers], and I would always see them work with so many people and brands and agencies. And I always was so annoyed that I couldn’t because I was a student,” he said.&lt;/p&gt;
    &lt;p&gt;He applied for the O-1B Visa shortly after graduating, during which he could finally make money from influencing while on his OPT, a 12-month work authorization for international students post-graduation.&lt;/p&gt;
    &lt;p&gt;The O-1B visa, once reserved for Hollywood titans and superstar musicians, has evolved over the years.&lt;/p&gt;
    &lt;p&gt;“We started doing [O-1 visa applications] for kids who are e-sport players and influencers and the OnlyFans crew,” said Michael Wildes, an immigration attorney and managing partner of Wildes &amp;amp; Weinberg. “It’s the new, sexy medium for people to be a part of.”&lt;/p&gt;
    &lt;p&gt;Wildes has worked with the likes of musician Sinéad O’Connor, soccer star Pelé, and restaurateur Jean-Georges Vongerichten. His father, Leon Wildes, who started the firm in 1960, defended John Lennon and Yoko Ono against deportation during the Nixon administration, and helped facilitate the creation of the O-1B visa, which was established by the Immigration Act of 1990. Wildes’s client roster now includes social media influencers and Twitch streamers.&lt;/p&gt;
    &lt;p&gt;To qualify for an O-1B visa, applicants must submit evidence of at least three of the six regulatory criteria, which include performing in a distinguished production or event, national or international recognition for achievements, and a record of commercial or critically acclaimed successes. In 2026, though, these criteria are being stretched to encompass the accolades of an influencer.&lt;/p&gt;
    &lt;p&gt;In Ain’s application, she highlighted her sizable income and social media metrics.&lt;/p&gt;
    &lt;p&gt;“Part of my application was: ‘I have 200,000 followers on this app, 300,000 followers on this app, 10 million people watch me here every month,’” she said. “This isn’t just, ‘Oh, you had one viral video and people watched that.’ No, you’ve got a following now that are not only watching you, but also paying for your content actively month after month.”&lt;/p&gt;
    &lt;p&gt;Social media was an integral part of the O-1B visa application of Dina Belenkaya, a Russian Israeli chess player and content creator – which was approved in December 2023.&lt;/p&gt;
    &lt;p&gt;“My followings on Instagram (1.2 million), Twitch (108,000) and YouTube (799,000) were included as part of my profile, and I listed my follower counts on each platform,” she said. After her visa approval, she moved to Charlotte, North Carolina – widely considered the chess capital of the United States.&lt;/p&gt;
    &lt;p&gt;While a certain number of followers may not be an automatic ticket to the US, one viral music group has been trying their luck. Boy Throb, comprising Anthony Key, Evan Papier, Zachary Sobania and Darshan Magdum, spent the past few months campaigning to reach 1 million followers on TikTok so that Magdum could use the stat on his O-1 visa application. Clad in matching pink jumpsuits, the three US-based bandmates danced together on screen to parody lyrics of hit songs, while Magdum was edited in from India.&lt;/p&gt;
    &lt;p&gt;Within a month of their first post, Boy Throb reached their goal of 1 million followers. Whether it will help Magdum get a visa remains unclear.&lt;/p&gt;
    &lt;p&gt;“Honestly, the entire immigration process has been so complicated and there have been so many people who don’t believe us when we say we’re doing everything in our power to get Darshan here,” the group said.&lt;/p&gt;
    &lt;p&gt;“We’re not sure how much longer we want to keep going without Darshan here and the process has been really expensive,” they added. In total, the band has spent more than $10,000 in legal and processing fees.&lt;/p&gt;
    &lt;p&gt;The rise in content creators applying for visas given out on the basis of “extraordinary ability” has garnered a variety of reactions. Dominic Michael Tripi, a political analyst and writer, posted on X that the trend was indicative of “end-stage empire conditions. It’s sad.” Legal professionals like Wildes, however, argue that the creator economy is the next frontier of American exceptionalism.&lt;/p&gt;
    &lt;p&gt;“Influencers are filling a large gap in the retail and commercial interests of the world,” he said. “They’re moving content and purchases like no other. Immigration has to keep up with this.”&lt;/p&gt;
    &lt;p&gt;Ain also takes issue with the criticism of influencers applying for O-1 visas, as well as the notion that influencing is not a legitimate profession.&lt;/p&gt;
    &lt;p&gt;“I don’t think [people] realize how much work actually goes into it,” she said. “You might not agree with the way the money is being made, or what people are watching, but people are still watching and paying for it.”&lt;/p&gt;
    &lt;p&gt;She continued: “Maybe 50 years ago, this isn’t what people imagined the American dream would look like. But this is what the American dream is now.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theguardian.com/us-news/2026/jan/11/onlyfans-influencers-us-o-1-visa"/><published>2026-01-13T16:47:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46603829</id><title>Legion Health (YC S21) Hiring Cracked Founding Eng for AI-Native Ops</title><updated>2026-01-14T00:57:06.828138+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jobs.ashbyhq.com/legionhealth/ffdd2b52-eb21-489e-b124-3c0804231424"/><published>2026-01-13T17:01:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46603995</id><title>The Tulip Creative Computer</title><updated>2026-01-14T00:57:06.058051+00:00</updated><content>&lt;doc fingerprint="204746de23261bd9"&gt;
  &lt;main&gt;
    &lt;p&gt;Welcome to the Tulip Creative Computer (Tulip CC)!&lt;/p&gt;
    &lt;p&gt;Tulip is a low power and affordable self-contained portable computer, with a touchscreen display and sound. It's fully programmable - you write code to define your music, games or anything else you can think of. It boots instantaneously into a Python prompt with a lot of built in support for music synthesis, fast graphics and text, hardware MIDI, network access and external sensors. Dive right into making something without distractions or complications.&lt;/p&gt;
    &lt;p&gt;The entire system is dedicated to your code, the display and sound, running in real time, on specialized hardware. The hardware and software are fully open source and anyone can buy one or build one. You can use Tulip to make music, code, art, games, or just write.&lt;/p&gt;
    &lt;p&gt;You can now even run Tulip on the web and share your creations with anyone!&lt;/p&gt;
    &lt;p&gt;Tulip is powered by MicroPython, AMY, and LVGL. The Tulip hardware runs on the ESP32-S3 chip using the ESP-IDF.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Get a Tulip from our friends at Makerfabs for only US$59&lt;/item&gt;
      &lt;item&gt;Just got a Tulip CC? Check out our getting started guide!&lt;/item&gt;
      &lt;item&gt;Want to make music with your Tulip? See our music tutorial&lt;/item&gt;
      &lt;item&gt;See the full Tulip API&lt;/item&gt;
      &lt;item&gt;Try out Tulip on the web!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Chat about Tulip on our Discord!&lt;/p&gt;
    &lt;p&gt;Check out this video!&lt;/p&gt;
    &lt;p&gt;You can use Tulip one of three ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tulip is available both as an off the shelf or DIY hardware project (Tulip CC)&lt;/item&gt;
      &lt;item&gt;Tulip runs on the web with (almost) all the same features.&lt;/item&gt;
      &lt;item&gt;Tulip can also run as a native app for Mac or Linux (or WSL in Windows) as Tulip Desktop&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you're nervous about getting or building the hardware, try it out on the web!&lt;/p&gt;
    &lt;p&gt;The hardware Tulip CC supports:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8.5MB of RAM - 2MB is available to MicroPython, and 1.5MB is available for OS memory. The rest is used for the graphics framebuffers (which you can use as storage) and the firmware cache.&lt;/item&gt;
      &lt;item&gt;32MB flash storage, as a filesystem accesible in Python (24MB left over after OS in ROM)&lt;/item&gt;
      &lt;item&gt;An AMY stereo 120-voice synthesizer engine running locally, or as a wireless controller for an Alles mesh. Tulip's synth supports additive and subtractive oscillators, an excellent FM synthesis engine, samplers, karplus-strong, high quality analog style filters, a sequencer, and much more. We ship Tulip with a drum machine, voices / patch app, and Juno-6 editor.&lt;/item&gt;
      &lt;item&gt;Text frame buffer layer, 128 x 50, with ANSI support for 256 colors, inverse, bold, underline, background color&lt;/item&gt;
      &lt;item&gt;Up to 32 sprites on screen, drawn per scanline, with collision detection, from a total of 32KB of bitmap memory (1 byte per pixel)&lt;/item&gt;
      &lt;item&gt;A 1024 (+128 overscan) by 600 (+100 overscan) background frame buffer to draw arbitrary bitmaps to, or use as RAM, and which can scroll horizontally / vertically&lt;/item&gt;
      &lt;item&gt;WiFi, access http via Python requests or TCP / UDP sockets&lt;/item&gt;
      &lt;item&gt;Adjustable display clock and resolution, defaults to 30 FPS at 1024x600.&lt;/item&gt;
      &lt;item&gt;256 colors&lt;/item&gt;
      &lt;item&gt;Can load PNGs from disk to set sprites or background, or generate bitmap data from code&lt;/item&gt;
      &lt;item&gt;Built in code and text editor&lt;/item&gt;
      &lt;item&gt;Built in BBS chat room and file transfer area called TULIP ~ WORLD&lt;/item&gt;
      &lt;item&gt;USB keyboard, MIDI and mouse support, including hubs&lt;/item&gt;
      &lt;item&gt;Capactive multi-touch support (mouse on Tulip Desktop and Tulip Web)&lt;/item&gt;
      &lt;item&gt;MIDI input and output&lt;/item&gt;
      &lt;item&gt;I2C / Grove / Mabee connector, compatible with many I2C devices like joysticks, keyboard, GPIO, DACs, ADCs, hubs&lt;/item&gt;
      &lt;item&gt;575mA power usage @ 5V including display, at medium display brightness, can last for hours on LiPo, 18650s, or USB battery pack&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I've been working on Tulip on and off for years over many hardware iterations and hope that someone out there finds it as fun as I have, either making things with Tulip or working on Tulip itself. I'd love feedback, your own Tulip experiments or pull requests to improve the system.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Any issues with your Tulip CC? Here's our troubleshooting guide&lt;/item&gt;
      &lt;item&gt;Learn about our roadmap and find out what we're working on next&lt;/item&gt;
      &lt;item&gt;Build your own Tulip&lt;/item&gt;
      &lt;item&gt;You can read more about the "why" or "how" of Tulip on my website!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A new small option: get yourself a T-Deck and install Tulip CC on it directly! Check out our T-Deck page for more detail.&lt;/p&gt;
    &lt;p&gt;Once you've bought a Tulip, opened Tulip Web, built a Tulip or installed Tulip Desktop, you'll see that Tulip boots right into a Python prompt and all interaction with the system happens there. You can make your own Python programs with Tulip's built in editor and execute them, or just experiment on the Tulip REPL prompt in real time.&lt;/p&gt;
    &lt;p&gt;See the full Tulip API for more details on all the graphics, sound and input functions.&lt;/p&gt;
    &lt;p&gt;Below are a few getting started tips and small examples. The full API page has more detail on everything you can do on a Tulip. See a more complete getting started page or a music making tutorial as well!&lt;/p&gt;
    &lt;code&gt;# Run a saved Python file. Control-C stops it
cd('ex') # The ex folder has a few examples and graphics in it
execfile("parallax.py")
# If you want to run a Tulip package (folder with other files in it)
run("game")&lt;/code&gt;
    &lt;p&gt;Tulip ships with a text editor, based on pico/nano. It supports syntax highlighting, search, save/save-as.&lt;/p&gt;
    &lt;code&gt;# Opens the Tulip editor to the given filename. 
edit("game.py")&lt;/code&gt;
    &lt;p&gt;Tulip supports USB keyboard and mice input as well as touch input. (On Tulip Desktop and Web, mouse clicks act as touch points.) It also comes with UI elements like buttons and sliders to use in your applications, and a way to run mulitple applications as once using callbacks. More in the full API.&lt;/p&gt;
    &lt;code&gt;(x0, y0, x1, y1, x2, y2) = tulip.touch()&lt;/code&gt;
    &lt;p&gt;Tulip CC has the capability to connect to a Wi-Fi network, and Python's native requests library will work to access TCP and UDP. We ship a few convenience functions to grab data from URLs as well. More in the full API.&lt;/p&gt;
    &lt;code&gt;# Join a wifi network (not needed on Tulip Desktop or Web)
tulip.wifi("ssid", "password")

# Get IP address or check if connected
ip_address = tulip.ip() # returns None if not connected

# Save the contents of a URL to disk (needs wifi)
bytes_read = tulip.url_save("https://url", "filename.ext")&lt;/code&gt;
    &lt;p&gt;Tulip comes with the AMY synthesizer, a very full featured 120-oscillator synth that supports FM, PCM, additive synthesis, partial synthesis, filters, and much more. We also provide a useful "music computer" for scales, chords and progressions. More in the full API and in the music tutorial. Tulip's version of AMY comes with stereo sound, which you can set per oscillator with the &lt;code&gt;pan&lt;/code&gt; parameter.&lt;/p&gt;
    &lt;code&gt;amy.drums() # plays a test song
amy.send(volume=4) # change volume
amy.reset() # stops all music / sounds playing&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;music.mov&lt;/head&gt;
    &lt;p&gt;Tulip supports MIDI in and out to connect to external music hardware. You can set up a Python callback to respond immediately to any incoming MIDI message. You can also send messages out to MIDI out. More in the full API and music tutorial.&lt;/p&gt;
    &lt;code&gt;m = tulip.midi_in() # returns bytes of the last MIDI message received
tulip.midi_out((144,60,127)) # sends a note on message
tulip.midi_out(bytes) # Can send bytes or list&lt;/code&gt;
    &lt;p&gt;The Tulip GPU supports a scrolling background layer, hardware sprites, and a text layer. Much more in the full API.&lt;/p&gt;
    &lt;code&gt;# Set or get a pixel on the BG
pal_idx = tulip.bg_pixel(x,y)

# Set the contents of a PNG file on the background.
tulip.bg_png(png_filename, x, y)

tulip.bg_scroll(line, x_offset, y_offset, x_speed, y_speed)&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;scroll.mov&lt;/head&gt;
    &lt;p&gt;Hardware sprites are supported. They draw over the background and text layer per scanline per frame:&lt;/p&gt;
    &lt;code&gt;(w, h, bytes) = tulip.sprite_png("filename.png", mem_pos)

...

# Set a sprite x and y position
tulip.sprite_move(12, x, y)&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;game.mov&lt;/head&gt;
    &lt;p&gt;Still very much early days, but Tulip supports a native chat and file sharing BBS called TULIP ~ WORLD where you can hang out with other Tulip owners. You're able to pull down the latest messages and files and send messages and files yourself. More in the full API.&lt;/p&gt;
    &lt;code&gt;import world
world.post_message("hello!!") # Sends a message to Tulip World. username required. will prompt if not set
world.upload(filename) # Uploads a file to Tulip World. username required
world.ls() # lists most recent unique filenames/usernames&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Get a Tulip!&lt;/item&gt;
      &lt;item&gt;Build your own Tulip Creative Computer with FOUR different options.&lt;/item&gt;
      &lt;item&gt;How to compile and flash Tulip hardware&lt;/item&gt;
      &lt;item&gt;How to run or compile Tulip Desktop&lt;/item&gt;
      &lt;item&gt;The full Tulip API&lt;/item&gt;
      &lt;item&gt;File any code issues or pull requests!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Chat about Tulip on our Discord!&lt;/p&gt;
    &lt;p&gt;Two important development guidelines if you'd like to help contribute!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Be nice and helpful and don't be afraid to ask questions! We're all doing this for fun and to learn.&lt;/item&gt;
      &lt;item&gt;Any change or feature must be equivalent across Tulip Desktop and Tulip CC. There are of course limited exceptions to this rule, but please test on hardware before proposing a new feature / change.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Have fun!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/shorepine/tulipcc"/><published>2026-01-13T17:10:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46604250</id><title>How to make a damn website (2024)</title><updated>2026-01-14T00:57:05.485103+00:00</updated><content>&lt;doc fingerprint="260e12710bf61f95"&gt;
  &lt;main&gt;
    &lt;p&gt;A lot of people want to make a website but donât know where to start or they get stuck. Thatâs in part because our perception of what websites should be has changed so dramatically over the last 20 years.&lt;/p&gt;
    &lt;p&gt;Itâs easy to forget how simple a website can be. A website can be just one page. It doesnât even need CSS. You donât need a content management system like Wordpress. All you have to do is write some HTML and drag that file to a server over FTP.&lt;/p&gt;
    &lt;p&gt;For years now, people have tried to convince us that this is the âhardâ way of making a website, but in reality, it may be the easiest.&lt;/p&gt;
    &lt;p&gt;It doesnât have to be super complicated. However, with this post, I will assume youâve written at least some HTML and CSS before, and that you know how to upload files to a server. If youâve never done these things, it may seem like Iâm skipping over some things. I am.&lt;/p&gt;
    &lt;p&gt;Let me begin with what I think you shouldnât start with. Donât shop around for a CMS. Donât even design or outline your website. Donât buy a domain or hosting yet. Donât set up a GitHub repository; I donât care how fast you can make one.&lt;/p&gt;
    &lt;p&gt;Instead, just write your first blog post. The very first thing I did was open TextEdit and write my first post with HTML, ye olde way. Not with Markdown. Not with Nova or BBEdit or another code editor. Just TextEdit (in plain text). Try it, even if just this once. Itâs kinda refreshing. You can go back to using a code editor later.&lt;/p&gt;
    &lt;p&gt;Hereâs what a draft of this blog post looks like:&lt;/p&gt;
    &lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang="en"&amp;gt;
	&amp;lt;head&amp;gt;
		&amp;lt;meta charset="utf-8"&amp;gt;
		&amp;lt;title&amp;gt;How to Make a Damn Website&amp;lt;/title&amp;gt;
	&amp;lt;/head&amp;gt;
	&amp;lt;body&amp;gt;

		&amp;lt;h1&amp;gt;&amp;lt;a href="how-to-make-a-damn-website.html"&amp;gt;How to Make a Damn Website&amp;lt;/a&amp;gt;&amp;lt;/h1&amp;gt;
		&amp;lt;p&amp;gt;A lot of people want to make a website but donât know where to start or they get stuck.&amp;lt;/p&amp;gt;

	&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;&lt;/code&gt;
    &lt;p&gt;This is honestly all you need. Itâs kind of charming.&lt;/p&gt;
    &lt;p&gt;Make sure you rely exclusively on HTML elements for your formatting. Your page should render clearly with raw HTML. Do not let yourself get distracted by writing CSS. Donât even imagine the CSS youâll use later. Donât write in IDs or classes yet. Do yourself a favor and donât make a single &lt;code&gt;div&lt;/code&gt; element.&lt;/p&gt;
    &lt;p&gt;Just write the post in the plainest HTML. And donât you dare write a âHello Worldâ post or a âLorem Ipsumâ post. Write an actual blog post. If you want, make it about why youâre making a website.&lt;/p&gt;
    &lt;p&gt;Writing this way helps you stay focused on writing for the web. The most important thing here is shipping something. You can (and should) update your site later. Now, name the HTML file something sensible, like the post name.&lt;/p&gt;
    &lt;code&gt;how-to-make-a-damn-website.html&lt;/code&gt;
    &lt;p&gt;Finished? Great. If you have a domain and hosting, make a new folder on your server called blog and upload your first post in there. Donât worry about index pages yet. You have only one post, thereâs not much to index. Weâll get there.&lt;/p&gt;
    &lt;p&gt;If you donât have a domain or hosting yet, nowâs the time to buckle down and do that. Unfortunately, I donât have good advice for you here. Just know that itâs going to be stupid and tedious and bad and unfun. Thatâs just the way this is.&lt;/p&gt;
    &lt;p&gt;Try not to let it deter you. Once you have the ability to upload files to an FTP server, youâve reached the âset it and forget itâ phase.&lt;/p&gt;
    &lt;p&gt;Direct your web browser to the HTML file you uploaded. Wow! There it is. A real, actual page on the web! You shipped it. Congratulations. Times New Roman, black on white. Hyperlinks that are blue and underlined. Useful. Classic.&lt;/p&gt;
    &lt;p&gt;Look at your unstyled HTML page and appreciate it for what it is. Always remember, this is all a website has to be. Good websites can be reduced to this and still work.&lt;/p&gt;
    &lt;p&gt;A broken escalator is just stairs. Even if itâs a little less convenient, it remains functional. This is important.&lt;/p&gt;
    &lt;p&gt;If you get this far, I want you to know this is truly the hardest part. Some people will ignore what Iâve said. They will spend significant time designing a website, hunting around for a good CMS, doing a wide variety of busywork, neglecting the part where they write actual content for their site. But if you shipped a single blog post, you have a website, and they donât.&lt;/p&gt;
    &lt;p&gt;A website is nothing without content. You can spend months preparing to make a website, tacking up what Iâm sure was intended to be a âtemporaryâ page telling people that youâre âworking on a new website,â but it will inevitably become a permanent reminder that you havenât done it yet. So focus on what matters, and ship one blog post. Do the rest later.&lt;/p&gt;
    &lt;p&gt;You may think CSS is the next logical step, or maybe an index page, but I donât think so. It takes only a few minutes to hand-write an XML file, and once itâs done, people will be able to read your blog via an RSS reader.&lt;/p&gt;
    &lt;p&gt;On your site, youâre in control of publishing now. When you post to your blog, part of the process is syndicating it to those who want to stay updated. If you provide an RSS feed, people can follow it. If you donât, they canât.&lt;/p&gt;
    &lt;p&gt;While the best time to make an RSS feed was 20 years ago, the second best time is now.&lt;/p&gt;
    &lt;p&gt;It should be noted that most people who have an RSS feed are probably not making it manually, so you wonât find a lot of documentation out there for doing it this way. But itâs not too hard. And once you make a habit, itâll be a totally reasonable component of your publishing flow.&lt;/p&gt;
    &lt;p&gt;Hereâs what my XML file looks like (without any entries):&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="utf-8"?&amp;gt;
&amp;lt;rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"&amp;gt;
	&amp;lt;channel&amp;gt;

		&amp;lt;title&amp;gt;LMNT&amp;lt;/title&amp;gt;
		&amp;lt;link&amp;gt;https://lmnt.me/&amp;lt;/link&amp;gt;
		&amp;lt;description&amp;gt;Louie Mantiaâs weblog.&amp;lt;/description&amp;gt;
		&amp;lt;language&amp;gt;en-us&amp;lt;/language&amp;gt;
		&amp;lt;atom:link href="https://lmnt.me/feed.xml" rel="self" type="application/rss+xml" /&amp;gt;

	&amp;lt;/channel&amp;gt;
&amp;lt;/rss&amp;gt;&lt;/code&gt;
    &lt;p&gt;The elements inside the &lt;code&gt;channel&lt;/code&gt; element are for your feed as a whole (&lt;code&gt;title&lt;/code&gt;, &lt;code&gt;link&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;, &lt;code&gt;language&lt;/code&gt;, and &lt;code&gt;atom:link&lt;/code&gt;). After the ones about your feedâs metadata, we can add a blog post to the XML file, which will look like this:&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="utf-8"?&amp;gt;
&amp;lt;rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"&amp;gt;
	&amp;lt;channel&amp;gt;
		&amp;lt;title&amp;gt;LMNT&amp;lt;/title&amp;gt;
		&amp;lt;link&amp;gt;https://lmnt.me/&amp;lt;/link&amp;gt;
		&amp;lt;description&amp;gt;Louie Mantiaâs weblog.&amp;lt;/description&amp;gt;
		&amp;lt;language&amp;gt;en-us&amp;lt;/language&amp;gt;
		&amp;lt;atom:link href="https://lmnt.me/feed.xml" rel="self" type="application/rss+xml" /&amp;gt;

		&amp;lt;item&amp;gt;
			&amp;lt;title&amp;gt;How to Make a Damn Website&amp;lt;/title&amp;gt;
			&amp;lt;pubDate&amp;gt;Mon, 25 Mar 2024 09:05:00 GMT&amp;lt;/pubDate&amp;gt;
			&amp;lt;guid&amp;gt;C5CC4199-E380-4851-B621-2C1AEF2CE7A1&amp;lt;/guid&amp;gt;
			&amp;lt;link&amp;gt;https://lmnt.me/blog/how-to-make-a-damn-website.html&amp;lt;/link&amp;gt;
			&amp;lt;description&amp;gt;&amp;lt;![CDATA[

				&amp;lt;h1&amp;gt;&amp;lt;a href="how-to-make-a-damn-website.html"&amp;gt;How to Make a Damn Website&amp;lt;/a&amp;gt;&amp;lt;/h1&amp;gt;
				&amp;lt;p&amp;gt;A lot of people want to make a website but donât know where to start or they get stuck.&amp;lt;/p&amp;gt;

			]]&amp;gt;&amp;lt;/description&amp;gt;
		&amp;lt;/item&amp;gt;

	&amp;lt;/channel&amp;gt;
&amp;lt;/rss&amp;gt;&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;item&lt;/code&gt; element represents an entry, and goes inside the &lt;code&gt;channel&lt;/code&gt; element as well. There are a few self-explanatory elements for the post metadata (&lt;code&gt;title&lt;/code&gt;, &lt;code&gt;pubDate&lt;/code&gt;, &lt;code&gt;guid&lt;/code&gt;, and &lt;code&gt;link&lt;/code&gt;), but the content inside the &lt;code&gt;description&lt;/code&gt; element can be the same HTML from your actual post. Handy!&lt;/p&gt;
    &lt;p&gt;Writing your first post with HTML and understanding how it looks âunstyledâ really works in your favor here, because RSS readers use their own stylesheets. How they render pages will not be too different from how a raw HTML page is rendered in your browser. If you make your own stylesheet too early, you may neglect how the raw HTML could be parsed in an RSS reader.&lt;/p&gt;
    &lt;p&gt;For the &lt;code&gt;pubDate&lt;/code&gt;, you can use GMT time. Ask Siri what time it is in Reykjavik, and enter that. You can use your local time zone instead, but be sure itâs formatted correctly. Also, note that it needs to be 24-hour time.&lt;/p&gt;
    &lt;p&gt;If you have images or other media in your post, be sure to use the absolute URL to a resource rather than a relative one. Relative URLs are fine for content that only lives on your site, but when you syndicate via RSS, that content loads outside of your website. Absolute URLs are better for content inside your blog posts, especially in the XML.&lt;/p&gt;
    &lt;p&gt;Once youâve got your first post in the XML file, upload it to the root folder of your website. If you donât already have an RSS reader, get one. I recommend NetNewsWire. Go to the XML file in your browser, and it should automatically open in your RSS reader and let you subscribe.&lt;/p&gt;
    &lt;p&gt;There it is! Your blog post is on the web and now also available via RSS! You can share that link now.&lt;/p&gt;
    &lt;p&gt;Now would be a good time to reference your RSS feed in your HTML. Youâll want to do this on all pages going forward, too. It helps browsers and plugins detect that thereâs an RSS feed for people to subscribe to.&lt;/p&gt;
    &lt;code&gt;&amp;lt;link rel="alternate" type="application/rss+xml" title="LMNT" href="https://lmnt.me/feed.xml" /&amp;gt;&lt;/code&gt;
    &lt;p&gt;When you add a new &lt;code&gt;item&lt;/code&gt; (a new blog post), put it above the previous one in your XML file. Keep in mind that your XML file will be updated periodically from devices that subscribe to it. RSS readers will be downloading this file when updating, so keep an eye on the file size. It probably wonât ever be that big, because itâs just text, but itâs customary to keep only a certain amount of recent entries in the XML file, or a certain time period. But thereâs no rule here.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;guid&lt;/code&gt; should be a unique string. Some people use URLs thinking theyâre unique, but those can change. The right way is to generate a unique string for each post, which you can do easily with my app Tulip.&lt;/p&gt;
    &lt;p&gt;Changing the &lt;code&gt;guid&lt;/code&gt; (unique identifier) for your posts makes an RSS reader think itâs a different entry, resulting in a post being marked âunread.â If you go the route of using a URL as your &lt;code&gt;guid&lt;/code&gt; for each post, youâll want to think harder about the file structure of your website, right? Itâs probably fine if you change your file structure once or twice (I did), but just be sure to update your &lt;code&gt;link&lt;/code&gt; elements in the RSS feed, and redirect old URLs to new ones with an .htaccess file. Just donât change the contents of the &lt;code&gt;guid&lt;/code&gt; element.&lt;/p&gt;
    &lt;p&gt;Alright, we can make index pages now. This is going to be super easy, because you donât have a lot to index yet.&lt;/p&gt;
    &lt;p&gt;At the root, you want a link to the blog directory, and at the blog directory, you want a link to your first post. Put titles on each page, maybe a link back to the home page from your blog index. If you want, write a little description of your site on the root index.&lt;/p&gt;
    &lt;p&gt;Keep using basic HTML! Titles can be &lt;code&gt;h1&lt;/code&gt;, and descriptions can be &lt;code&gt;p&lt;/code&gt;. Keep it simple.
		&lt;/p&gt;
    &lt;p&gt;Once you got those uploaded, you got three pages and an RSS feed. Youâre doing great!&lt;/p&gt;
    &lt;p&gt;I recommend writing a couple more posts next. Try using some HTML elements that you didnât use in the first post, maybe an &lt;code&gt;hr&lt;/code&gt; element. Fancy! &lt;code&gt;ol&lt;/code&gt; and &lt;code&gt;ul&lt;/code&gt;. Maybe some &lt;code&gt;img&lt;/code&gt;, &lt;code&gt;video&lt;/code&gt;, and &lt;code&gt;audio&lt;/code&gt; elements.&lt;/p&gt;
    &lt;p&gt;In addition to being more posts for your blog, these will also help prioritize which elements need styling, providing you with a few sample pages to check while you write CSS.&lt;/p&gt;
    &lt;p&gt;Upload the posts as you write them, one after the next, adding them to your XML file. Donât forget to update your index pages, too. Always check your links and your feed.&lt;/p&gt;
    &lt;p&gt;Before you get ahead of yourself with layout, I recommend first styling the basic HTML elements you already defined in your first few posts: &lt;code&gt;h1&lt;/code&gt;, &lt;code&gt;h2&lt;/code&gt;, &lt;code&gt;h3&lt;/code&gt;, &lt;code&gt;hr&lt;/code&gt;, &lt;code&gt;p&lt;/code&gt;, &lt;code&gt;strong&lt;/code&gt;, &lt;code&gt;em&lt;/code&gt;, &lt;code&gt;ol&lt;/code&gt;, &lt;code&gt;ul&lt;/code&gt;. Define the &lt;code&gt;body&lt;/code&gt; font and width, text sizes, and colors.&lt;/p&gt;
    &lt;p&gt;Like the rest of your site, stylesheets are mutable. Expect them to change with your website. Incremental updates are what makes this whole process work. Ship tiny updates to your CSS. You can upload your stylesheet in a second. Heck, work directly on the server if you want. I did that.&lt;/p&gt;
    &lt;p&gt;If youâve done all this, then youâve cleared the hurdle. Now you get to just keep doing the fun stuff. Write more blog posts. Make more web pages. Itâs your website, you can make pages for anything you want. You can style them however you want. You can update people via RSS whenever you make something new.&lt;/p&gt;
    &lt;p&gt;Manually making a website like this may seem silly to engineers who would rather build or rely on systems that automate this stuff. But it doesnât seem like thereâs actually a whole lot that needs automation, does it?&lt;/p&gt;
    &lt;p&gt;A lot of modern solutions may not save time as much as they introduce complexity and reliance on more tools than you need. This whole process is not that complex.&lt;/p&gt;
    &lt;p&gt;Itâs not doing this manually thatâs hard.&lt;/p&gt;
    &lt;p&gt;The hard part is just shipping.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lmnt.me/blog/how-to-make-a-damn-website.html"/><published>2026-01-13T17:23:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46604862</id><title>Show HN: Ayder – HTTP-native durable event log written in C (curl as client)</title><updated>2026-01-14T00:57:02.835044+00:00</updated><content>&lt;doc fingerprint="8858ee55c525499a"&gt;
  &lt;main&gt;
    &lt;p&gt;HTTP-native durable event log / message bus — written in C&lt;/p&gt;
    &lt;p&gt;A single-binary event streaming system where &lt;code&gt;curl&lt;/code&gt; is your client. No JVM, no ZooKeeper, no thick client libraries.&lt;/p&gt;
    &lt;code&gt;# Produce
curl -X POST 'localhost:1109/broker/topics/orders/produce?partition=0' \
  -H 'Authorization: Bearer dev' \
  -d '{"item":"widget"}'

# Consume
curl 'localhost:1109/broker/consume/orders/mygroup/0?encoding=b64' \
  -H 'Authorization: Bearer dev'&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sustained: ~50K msg/s (wrk2 @ 50K req/s)&lt;/item&gt;
      &lt;item&gt;Client P99: 3.46ms&lt;/item&gt;
      &lt;item&gt;Server P99.999: 1.22ms (handler only)&lt;/item&gt;
      &lt;item&gt;Recovery after SIGKILL: 40–50s (8M offsets)&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Kafka&lt;/cell&gt;
        &lt;cell role="head"&gt;Redis Streams&lt;/cell&gt;
        &lt;cell role="head"&gt;Ayder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Protocol&lt;/cell&gt;
        &lt;cell&gt;Binary (requires thick client)&lt;/cell&gt;
        &lt;cell&gt;RESP&lt;/cell&gt;
        &lt;cell&gt;HTTP (curl works)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Durability&lt;/cell&gt;
        &lt;cell&gt;✅ Replicated log&lt;/cell&gt;
        &lt;cell&gt;✅ Raft consensus (sync-majority)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Operations&lt;/cell&gt;
        &lt;cell&gt;ZooKeeper/KRaft + JVM tuning&lt;/cell&gt;
        &lt;cell&gt;Single node or Redis Cluster&lt;/cell&gt;
        &lt;cell&gt;Single binary, zero dependencies&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Latency (P99)&lt;/cell&gt;
        &lt;cell&gt;10-50ms&lt;/cell&gt;
        &lt;cell&gt;N/A (async only)&lt;/cell&gt;
        &lt;cell&gt;3.5ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Recovery time&lt;/cell&gt;
        &lt;cell&gt;2+ hours (unclean shutdown)&lt;/cell&gt;
        &lt;cell&gt;Minutes&lt;/cell&gt;
        &lt;cell&gt;40-50 seconds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;First message&lt;/cell&gt;
        &lt;cell&gt;~30 min setup&lt;/cell&gt;
        &lt;cell&gt;~5 min setup&lt;/cell&gt;
        &lt;cell&gt;~60 seconds&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Kafka is battle-tested but operationally heavy. JVM tuning, partition rebalancing, and config sprawl add up.&lt;/p&gt;
    &lt;p&gt;Redis Streams is simple and fast, but replication is async-only — no majority quorum, no strong durability guarantees.&lt;/p&gt;
    &lt;p&gt;Ayder sits in the middle: Kafka-grade durability (Raft sync-majority) with Redis-like simplicity (single binary, HTTP API). Think of it as what Nginx did to Apache — same pattern applied to event streaming.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Append-only logs with per-partition offsets&lt;/item&gt;
      &lt;item&gt;Consumer groups with committed offsets&lt;/item&gt;
      &lt;item&gt;Durability via sealed append-only files (AOF) + crash recovery&lt;/item&gt;
      &lt;item&gt;HA replication with Raft consensus (3 / 5 / 7 node clusters)&lt;/item&gt;
      &lt;item&gt;KV store with CAS and TTL&lt;/item&gt;
      &lt;item&gt;Stream processing with filters, aggregations, and windowed joins (including cross-format Avro+Protobuf joins)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All benchmarks use real network (not loopback). Numbers are real, not marketing.&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3-node Raft cluster on DigitalOcean (8 vCPU AMD)&lt;/item&gt;
      &lt;item&gt;Sync-majority writes (2/3 nodes confirm before ACK)&lt;/item&gt;
      &lt;item&gt;64B payload&lt;/item&gt;
      &lt;item&gt;Separate machines, real network&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Client-side&lt;/cell&gt;
        &lt;cell role="head"&gt;Server-side&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;49,871 msg/s&lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P50&lt;/cell&gt;
        &lt;cell&gt;1.60ms&lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;3.46ms&lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P99.9&lt;/cell&gt;
        &lt;cell&gt;12.94ms&lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;P99.999&lt;/cell&gt;
        &lt;cell&gt;154.49ms&lt;/cell&gt;
        &lt;cell&gt;1.22ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Server-side breakdown at P99.999:&lt;/p&gt;
    &lt;code&gt;Handler:     1.22ms
Queue wait:  0.47ms
HTTP parse:  0.41ms
&lt;/code&gt;
    &lt;p&gt;The 154ms client-side tail is network/kernel scheduling — the broker itself stays under 2ms even at P99.999. HTTP is not the bottleneck.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;93,807 msg/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P50&lt;/cell&gt;
        &lt;cell&gt;3.78ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;10.22ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Max&lt;/cell&gt;
        &lt;cell&gt;224.51ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Full wrk output (3-node cluster, max throughput)&lt;/head&gt;
    &lt;code&gt;Running 1m test @ http://10.114.0.3:8001
  12 threads and 400 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     4.26ms    2.97ms 224.51ms   93.76%
    Req/Sec     7.86k     1.19k   13.70k    67.61%
  Latency Distribution
     50%    3.78ms
     75%    4.93ms
     90%    6.44ms
     99%   10.22ms
  5634332 requests in 1.00m, 2.99GB read
Requests/sec:  93807.95
Transfer/sec:     50.92MB
&lt;/code&gt;
    &lt;head&gt;Full wrk2 output (3-node cluster, rate-limited)&lt;/head&gt;
    &lt;code&gt;Running 1m test @ http://10.114.0.2:9001
  12 threads and 400 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.72ms    1.19ms 216.19ms   96.39%
    Req/Sec     4.35k     1.17k    7.89k    79.58%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.60ms
 75.000%    2.03ms
 90.000%    2.52ms
 99.000%    3.46ms
 99.900%   12.94ms
 99.990%   31.76ms
 99.999%  154.49ms
100.000%  216.32ms

  2991950 requests in 1.00m, 1.80GB read
Requests/sec:  49871.12

SERVER  server_us p99.999=1219us (1.219ms)
SERVER  queue_us p99.999=473us (0.473ms)
SERVER  recv_parse_us p99.999=411us (0.411ms)
&lt;/code&gt;
    &lt;p&gt;Ayder runs natively on ARM64. Here's a benchmark on consumer hardware:&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Snapdragon X Elite laptop (1.42 kg)&lt;/item&gt;
      &lt;item&gt;WSL2 Ubuntu, 16GB RAM&lt;/item&gt;
      &lt;item&gt;Running on battery (unplugged)&lt;/item&gt;
      &lt;item&gt;3-node Raft cluster (same machine — testing code efficiency)&lt;/item&gt;
      &lt;item&gt;wrk: 12 threads, 400 connections, 60 seconds&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Client-side&lt;/cell&gt;
        &lt;cell role="head"&gt;Server-side&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;106,645 msg/s&lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P50&lt;/cell&gt;
        &lt;cell&gt;3.57ms&lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;7.62ms&lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;P99.999&lt;/cell&gt;
        &lt;cell&gt;250.84ms&lt;/cell&gt;
        &lt;cell&gt;0.65ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Server-side breakdown at P99.999:&lt;/p&gt;
    &lt;code&gt;Handler:     0.65ms
Queue wait:  0.29ms
HTTP parse:  0.29ms
&lt;/code&gt;
    &lt;p&gt;Comparison: Snapdragon vs Cloud VMs (Server-side P99.999)&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Environment&lt;/cell&gt;
        &lt;cell role="head"&gt;Throughput&lt;/cell&gt;
        &lt;cell role="head"&gt;Server P99.999&lt;/cell&gt;
        &lt;cell role="head"&gt;Hardware&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Snapdragon X Elite (WSL2, battery)&lt;/cell&gt;
        &lt;cell&gt;106,645/s&lt;/cell&gt;
        &lt;cell&gt;0.65ms&lt;/cell&gt;
        &lt;cell&gt;1.42kg laptop&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DigitalOcean (8-vCPU AMD, 3 VMs)&lt;/cell&gt;
        &lt;cell&gt;93,807/s&lt;/cell&gt;
        &lt;cell&gt;1.22ms&lt;/cell&gt;
        &lt;cell&gt;Cloud infrastructure&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The laptop's server-side latency is 47% faster while handling 14% more throughput — on battery, in WSL2.&lt;/p&gt;
    &lt;p&gt;What this proves:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ARM64 is ready for server workloads&lt;/item&gt;
      &lt;item&gt;Efficient C code runs beautifully on Snapdragon&lt;/item&gt;
      &lt;item&gt;WSL2 overhead is minimal for async I/O&lt;/item&gt;
      &lt;item&gt;You can test full HA clusters on your laptop&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Full wrk output (Snapdragon X Elite)&lt;/head&gt;
    &lt;code&gt;Running 1m test @ http://172.31.76.127:7001
  12 threads and 400 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     3.81ms    3.80ms 289.49ms   99.00%
    Req/Sec     8.94k     1.16k   22.81k    80.11%
  Latency Distribution
     50%    3.57ms
     75%    4.01ms
     90%    4.51ms
     99%    7.62ms
  6408525 requests in 1.00m, 3.80GB read
Requests/sec: 106645.65
Transfer/sec:     64.83MB

CLIENT  p99.999=250843us (250.843ms)  max=289485us (289.485ms)
SERVER  server_us p99.999=651us (0.651ms)  max=11964us (11.964ms)
SERVER  queue_us p99.999=285us (0.285ms)  max=3920us (3.920ms)
SERVER  recv_parse_us p99.999=293us (0.293ms)  max=4149us (4.149ms)
&lt;/code&gt;
    &lt;p&gt;Kafka in 2025 is like starting a car with a hand crank. It works, but why are we still doing this?&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Scenario&lt;/cell&gt;
        &lt;cell role="head"&gt;Kafka&lt;/cell&gt;
        &lt;cell role="head"&gt;Ayder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cluster restart (unclean)&lt;/cell&gt;
        &lt;cell&gt;2+ hours (reported in production)&lt;/cell&gt;
        &lt;cell&gt;40-50 seconds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Broker sync after failure&lt;/cell&gt;
        &lt;cell&gt;181 minutes for 1TB data&lt;/cell&gt;
        &lt;cell&gt;Auto catch-up in seconds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50+ broker rolling restart&lt;/cell&gt;
        &lt;cell&gt;2+ hours (2 min per broker)&lt;/cell&gt;
        &lt;cell&gt;N/A — single binary&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Tested crash recovery:&lt;/p&gt;
    &lt;code&gt;# 3-node cluster with 8 million offsets
1. SIGKILL a follower mid-write
2. Leader continues, follower misses offsets
3. Restart follower
4. Follower replays local AOF → asks leader for missing offsets
5. Leader streams missing data → follower catches up
6. Cluster fully healthy in 40-50 seconds
7. Zero data loss&lt;/code&gt;
    &lt;p&gt;No manual intervention. No partition reassignment. No ISR drama.&lt;/p&gt;
    &lt;code&gt;# Clone and run with Docker Compose (includes Prometheus + Grafana)
git clone https://github.com/A1darbek/ayder.git
cd ayder
docker compose up -d --build

# Or build and run standalone
docker build -t ayder .
docker run -p 1109:1109 --shm-size=2g ayder

# That's it. Now produce:
curl -X POST localhost:1109/broker/topics \
  -H 'Authorization: Bearer dev' \
  -H 'Content-Type: application/json' \
  -d '{"name":"events","partitions":4}'

curl -X POST 'localhost:1109/broker/topics/events/produce?partition=0' \
  -H 'Authorization: Bearer dev' \
  -d 'hello world'&lt;/code&gt;
    &lt;code&gt;# Dependencies: libuv 1.51+, openssl, zlib, liburing
make clean &amp;amp;&amp;amp; make
./ayder --port 1109&lt;/code&gt;
    &lt;p&gt;The included &lt;code&gt;docker-compose.yml&lt;/code&gt; brings up:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ayder on port &lt;code&gt;1109&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Prometheus on port &lt;code&gt;9090&lt;/code&gt;(metrics scraping)&lt;/item&gt;
      &lt;item&gt;Grafana on port &lt;code&gt;3000&lt;/code&gt;(dashboards, default password:&lt;code&gt;admin&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# docker-compose.yml
services:
  ayder:
    build: .
    ports:
      - "1109:1109"
    shm_size: 2g
    environment:
      - RF_BEARER_TOKENS=dev&lt;/code&gt;
    &lt;code&gt;# Create a topic
curl -X POST localhost:1109/broker/topics \
  -H 'Authorization: Bearer dev' \
  -H 'Content-Type: application/json' \
  -d '{"name":"events","partitions":8}'

# Produce a message
curl -X POST 'localhost:1109/broker/topics/events/produce?partition=0' \
  -H 'Authorization: Bearer dev' \
  -d 'hello world'

# Consume messages (binary-safe with base64)
curl 'localhost:1109/broker/consume/events/mygroup/0?limit=10&amp;amp;encoding=b64' \
  -H 'Authorization: Bearer dev'

# Commit offset
curl -X POST localhost:1109/broker/commit \
  -H 'Authorization: Bearer dev' \
  -H 'Content-Type: application/json' \
  -d '{"topic":"events","group":"mygroup","partition":0,"offset":10}'&lt;/code&gt;
    &lt;p&gt;A topic contains N partitions. Each partition is an independent append-only log with its own offset sequence.&lt;/p&gt;
    &lt;p&gt;Consumers read from &lt;code&gt;/broker/consume/{topic}/{group}/{partition}&lt;/code&gt;. Progress is tracked per &lt;code&gt;(topic, group, partition)&lt;/code&gt; tuple via explicit commits.&lt;/p&gt;
    &lt;p&gt;If you consume without specifying &lt;code&gt;?offset=&lt;/code&gt;, Ayder resumes from the last committed offset for that consumer group.&lt;/p&gt;
    &lt;p&gt;Ayder acknowledges writes in two modes:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;batch_id&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;durable&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sealed&lt;/cell&gt;
        &lt;cell&gt;Non-zero&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;true&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Appended to AOF, survives crashes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Rocket&lt;/cell&gt;
        &lt;cell&gt;Zero&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;In-memory fast path, not persisted&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Use &lt;code&gt;timeout_ms&lt;/code&gt; to wait for sync confirmation.&lt;/p&gt;
    &lt;code&gt;GET  /health      # → {"ok":true}
GET  /ready       # → {"ready":true}
GET  /metrics     # → Prometheus format
GET  /metrics_ha  # → HA cluster metrics&lt;/code&gt;
    &lt;p&gt;Create topic&lt;/p&gt;
    &lt;code&gt;POST /broker/topics
{"name":"events","partitions":8}&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true,"topic":"events","partitions":8}&lt;/code&gt;
    &lt;p&gt;Single message (raw bytes in body)&lt;/p&gt;
    &lt;code&gt;POST /broker/topics/{topic}/produce
&lt;/code&gt;
    &lt;p&gt;Query parameters:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;partition&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Target partition (optional; auto-assigned if omitted)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;key&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Message key, URL-encoded (optional)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;idempotency_key&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Deduplication key, URL-encoded (optional)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;timeout_ms&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Wait for sync confirmation (optional)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;timing&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set to &lt;code&gt;1&lt;/code&gt; to include timing breakdown (optional)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{
  "ok": true,
  "offset": 123,
  "partition": 0,
  "batch_id": 9991,
  "sealed": true,
  "durable": true,
  "mode": "sealed",
  "synced": true
}&lt;/code&gt;
    &lt;p&gt;Duplicate detection (when &lt;code&gt;idempotency_key&lt;/code&gt; matches):&lt;/p&gt;
    &lt;code&gt;{"ok":true,"offset":123,"partition":0,"sealed":true,"synced":null,"duplicate":true}&lt;/code&gt;
    &lt;p&gt;Batch produce (NDJSON — one message per line)&lt;/p&gt;
    &lt;code&gt;POST /broker/topics/{topic}/produce-ndjson
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{
  "ok": true,
  "first_offset": 1000,
  "count": 250,
  "partition": 0,
  "batch_id": 424242,
  "sealed": true,
  "durable": true,
  "mode": "sealed",
  "synced": false
}&lt;/code&gt;
    &lt;code&gt;GET /broker/consume/{topic}/{group}/{partition}
&lt;/code&gt;
    &lt;p&gt;Query parameters:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;offset&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start offset, inclusive (resumes from commit if omitted)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;limit&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Max messages to return (default: 100, max: 1000)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;encoding&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set to &lt;code&gt;b64&lt;/code&gt; for binary-safe base64 encoding&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{
  "messages": [
    {"offset": 0, "partition": 0, "value_b64": "aGVsbG8=", "key_b64": "a2V5"}
  ],
  "count": 1,
  "next_offset": 1,
  "committed_offset": 0,
  "truncated": false
}&lt;/code&gt;
    &lt;p&gt;Use &lt;code&gt;next_offset&lt;/code&gt; as the &lt;code&gt;?offset=&lt;/code&gt; parameter for subsequent reads.&lt;/p&gt;
    &lt;code&gt;POST /broker/commit
{"topic":"events","group":"g1","partition":0,"offset":124}
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true}&lt;/code&gt;
    &lt;p&gt;Commits are stored per &lt;code&gt;(topic, group, partition)&lt;/code&gt;. Backward commits are ignored.&lt;/p&gt;
    &lt;p&gt;Delete before offset (hard floor)&lt;/p&gt;
    &lt;code&gt;POST /broker/delete-before
{"topic":"events","partition":0,"before_offset":100000}
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true,"deleted_count":12345,"freed_bytes":987654}&lt;/code&gt;
    &lt;p&gt;Set retention policy&lt;/p&gt;
    &lt;code&gt;POST /broker/retention
&lt;/code&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;// TTL + size cap for specific partition
{"topic":"events","partition":0,"ttl_ms":60000,"max_bytes":104857600}

// TTL for all topics
{"topic":"*","ttl_ms":300000}&lt;/code&gt;
    &lt;p&gt;Ayder includes a key-value store with CAS (compare-and-swap) and TTL support.&lt;/p&gt;
    &lt;p&gt;Put&lt;/p&gt;
    &lt;code&gt;POST /kv/{namespace}/{key}?cas=&amp;lt;u64&amp;gt;&amp;amp;ttl_ms=&amp;lt;u64&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Body contains raw value bytes.&lt;/p&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true,"cas":2,"sealed":true,"durable":true,"mode":"sealed","synced":true,"batch_id":123}&lt;/code&gt;
    &lt;p&gt;Get&lt;/p&gt;
    &lt;code&gt;GET /kv/{namespace}/{key}
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"value":"&amp;lt;base64&amp;gt;","cas":2}&lt;/code&gt;
    &lt;p&gt;Get metadata&lt;/p&gt;
    &lt;code&gt;GET /kv/{namespace}/{key}/meta
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"cas":2,"ttl_ms":12345}&lt;/code&gt;
    &lt;p&gt;Delete&lt;/p&gt;
    &lt;code&gt;DELETE /kv/{namespace}/{key}?cas=&amp;lt;u64&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true,"deleted":true,"sealed":true,"durable":true,"mode":"sealed","synced":false,"batch_id":456}&lt;/code&gt;
    &lt;p&gt;Built-in stream processing — no separate service required.&lt;/p&gt;
    &lt;code&gt;POST /broker/query
&lt;/code&gt;
    &lt;p&gt;Consume JSON objects from a topic/partition with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Row filtering (eq, ne, lt, gt, in, contains)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;group_by&lt;/code&gt;with aggregations (count, sum, avg, min, max)&lt;/item&gt;
      &lt;item&gt;Field projection&lt;/item&gt;
      &lt;item&gt;Tumbling windows&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;POST /broker/join
&lt;/code&gt;
    &lt;p&gt;Windowed join between two sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Join types: inner / left / right / full&lt;/item&gt;
      &lt;item&gt;Composite keys&lt;/item&gt;
      &lt;item&gt;Window size and allowed lateness&lt;/item&gt;
      &lt;item&gt;Optional &lt;code&gt;dedupe_once&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Cross-format support (Avro + Protobuf in same join)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ayder supports 3, 5, or 7 node clusters with Raft-based replication.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Acknowledgment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;async&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Leader appends locally, replicates in background&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;sync-majority&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Waits for majority (e.g., 2/3 nodes)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;sync-all&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Waits for all nodes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Writes must go to the leader. If you send a write to a follower, it returns an HTTP redirect with the leader's address in the &lt;code&gt;Location&lt;/code&gt; header.&lt;/p&gt;
    &lt;p&gt;Options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Follow redirects automatically&lt;/item&gt;
      &lt;item&gt;Discover the leader via &lt;code&gt;/metrics_ha&lt;/code&gt;and pin writes to it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When a follower rejoins after downtime:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Replays local AOF&lt;/item&gt;
      &lt;item&gt;Connects to leader&lt;/item&gt;
      &lt;item&gt;Requests missing offsets&lt;/item&gt;
      &lt;item&gt;Leader streams missing data&lt;/item&gt;
      &lt;item&gt;Follower catches up automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example scenario:&lt;/p&gt;
    &lt;code&gt;# 3-node cluster
node1 (7001) = LEADER
node2 (8001) = FOLLOWER
node3 (9001) = FOLLOWER

# Write to leader
curl -X POST 'localhost:7001/broker/topics/test/produce?partition=0' \
  -H 'Authorization: Bearer dev' -d 'msg-0'
# → offset 0

# Kill node2
kill -9 $(pgrep -f "port 8001")

# Write while node2 is down
curl -X POST 'localhost:7001/broker/topics/test/produce?partition=0' \
  -H 'Authorization: Bearer dev' -d 'msg-1'
# → offset 1

curl -X POST 'localhost:7001/broker/topics/test/produce?partition=0' \
  -H 'Authorization: Bearer dev' -d 'msg-2'
# → offset 2

# Restart node2 — automatically catches up to offset 2

# Verify all data is present on recovered node
curl 'localhost:8001/broker/consume/test/g1/0?offset=0&amp;amp;limit=10' \
  -H 'Authorization: Bearer dev'
# → offsets 0, 1, 2 all present&lt;/code&gt;
    &lt;code&gt;# Default port is 1109
./ayder --port 1109

# Or specify custom port
./ayder --port 7001&lt;/code&gt;
    &lt;p&gt;Ayder uses Raft for consensus. Here's a complete 3-node setup:&lt;/p&gt;
    &lt;p&gt;Environment variables:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_ENABLED&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable HA mode (&lt;code&gt;1&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_NODE_ID&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Unique node identifier&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_NODES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cluster topology: &lt;code&gt;id:host:raft_port:priority,...&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_BOOTSTRAP_LEADER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set to &lt;code&gt;1&lt;/code&gt; on initial leader only&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_WRITE_CONCERN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Nodes to wait for: &lt;code&gt;1&lt;/code&gt;=leader only, &lt;code&gt;2&lt;/code&gt;=majority, &lt;code&gt;N&lt;/code&gt;=all&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_DEDICATED_WORKER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set to &lt;code&gt;0&lt;/code&gt; for best P99 latency (highly recommended)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_TLS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable mTLS for Raft (&lt;code&gt;1&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_TLS_CA&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to CA certificate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_TLS_CERT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to node certificate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_TLS_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to node private key&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_BEARER_TOKENS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;HTTP auth tokens (format: &lt;code&gt;token1@scope:token2:...&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;3-Node Example:&lt;/p&gt;
    &lt;code&gt;# Node 1 (bootstrap leader)
export RF_HA_ENABLED=1
export RF_HA_NODE_ID=node1
export RF_HA_BOOTSTRAP_LEADER=1
export RF_HA_NODES='node1:10.0.0.1:7000:100,node2:10.0.0.2:8000:50,node3:10.0.0.3:9000:25'
export RF_HA_WRITE_CONCERN=2  # sync-majority (2/3 nodes)
export RF_HA_DEDICATED_WORKER=0  # critical for low P99
export RF_BEARER_TOKENS='dev@scope:token2:token3'
export RF_HA_TLS=1
export RF_HA_TLS_CA=./certs/ca.crt
export RF_HA_TLS_CERT=./certs/node1.crt
export RF_HA_TLS_KEY=./certs/node1.key
./ayder --port 7001

# Node 2
export RF_HA_ENABLED=1
export RF_HA_NODE_ID=node2
export RF_HA_NODES='node1:10.0.0.1:7000:100,node2:10.0.0.2:8000:50,node3:10.0.0.3:9000:25'
export RF_HA_WRITE_CONCERN=2
export RF_HA_DEDICATED_WORKER=0
export RF_BEARER_TOKENS='dev@scope:token2:token3'
export RF_HA_TLS=1
export RF_HA_TLS_CA=./certs/ca.crt
export RF_HA_TLS_CERT=./certs/node2.crt
export RF_HA_TLS_KEY=./certs/node2.key
./ayder --port 8001

# Node 3 (same pattern, port 9001)&lt;/code&gt;
    &lt;p&gt;5-Node and 7-Node Clusters:&lt;/p&gt;
    &lt;code&gt;# 5-node topology
export RF_HA_NODES='node1:host1:7000:100,node2:host2:8000:80,node3:host3:9000:60,node4:host4:10000:40,node5:host5:11000:20'
export RF_HA_WRITE_CONCERN=3  # majority of 5

# 7-node topology
export RF_HA_NODES='node1:host1:7000:100,node2:host2:8000:90,node3:host3:9000:80,node4:host4:10000:70,node5:host5:11000:60,node6:host6:12000:50,node7:host7:13000:40'
export RF_HA_WRITE_CONCERN=4  # majority of 7&lt;/code&gt;
    &lt;p&gt;Generate TLS certificates:&lt;/p&gt;
    &lt;code&gt;# Create CA
openssl req -x509 -newkey rsa:4096 -keyout ca.key -out ca.crt \
  -days 365 -nodes -subj "/CN=ayder-ca"

# Create node certificate (repeat for each node)
openssl req -newkey rsa:2048 -nodes -keyout node1.key -out node1.csr \
  -subj "/CN=node1" -addext "subjectAltName=DNS:node1,IP:10.0.0.1"

openssl x509 -req -in node1.csr -CA ca.crt -CAkey ca.key -CAcreateserial \
  -out node1.crt -days 365 -copy_extensions copy&lt;/code&gt;
    &lt;p&gt;Write concern tradeoffs:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;RF_HA_WRITE_CONCERN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;Durability&lt;/cell&gt;
        &lt;cell role="head"&gt;Latency&lt;/cell&gt;
        &lt;cell role="head"&gt;Survives&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;~1ms&lt;/cell&gt;
        &lt;cell&gt;Nothing (leader only)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;2&lt;/code&gt; (3-node)&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;~3ms&lt;/cell&gt;
        &lt;cell&gt;1 node failure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;3&lt;/code&gt; (5-node)&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;~3ms&lt;/cell&gt;
        &lt;cell&gt;2 node failures&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;N&lt;/code&gt; (all nodes)&lt;/cell&gt;
        &lt;cell&gt;Maximum&lt;/cell&gt;
        &lt;cell&gt;Higher&lt;/cell&gt;
        &lt;cell&gt;N-1 failures, but blocks if any node slow&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Built by Aydarbek Romanuly — solo founder from Kazakhstan 🇰🇿&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub: @A1darbek&lt;/item&gt;
      &lt;item&gt;Email: aidarbekromanuly@gmail.com&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Errors follow a consistent format:&lt;/p&gt;
    &lt;code&gt;{
  "ok": false,
  "error": "missing_topic",
  "message": "Topic name is required",
  "docs": "https://ayder.dev/docs/api/produce"
}&lt;/code&gt;
    &lt;p&gt;✅ HTTP-native event log with partitions and offsets&lt;lb/&gt; ✅ Fast writes with cursor-based consumption&lt;lb/&gt; ✅ Durable with crash recovery&lt;lb/&gt; ✅ Horizontally scalable with Raft replication&lt;lb/&gt; ✅ Built-in stream processing with cross-format joins&lt;lb/&gt; ✅ ARM64-native (tested on Snapdragon X Elite)&lt;/p&gt;
    &lt;p&gt;❌ Kafka protocol compatible&lt;lb/&gt; ❌ A SQL database&lt;lb/&gt; ❌ Magic exactly-once without client-side idempotency discipline&lt;/p&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/A1darbek/ayder"/><published>2026-01-13T17:55:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46605490</id><title>AI Generated Music Barred from Bandcamp</title><updated>2026-01-14T00:57:02.693361+00:00</updated><content/><link href="https://old.reddit.com/r/BandCamp/comments/1qbw8ba/ai_generated_music_on_bandcamp/"/><published>2026-01-13T18:31:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46605675</id><title>Show HN: Nogic – VS Code extension that visualizes your codebase as a graph</title><updated>2026-01-14T00:57:02.563913+00:00</updated><content>&lt;doc fingerprint="1cccba6509fd4c3a"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;table&gt;
              &lt;row&gt;
                &lt;cell class="ux-itemdetails-left"&gt;
                  &lt;div&gt;
                    &lt;div&gt;
                      &lt;head rend="h1"&gt;🔍 Nogic&lt;/head&gt;
                      &lt;p&gt;Visualize your codebase structure with interactive diagrams&lt;/p&gt;
                      &lt;head rend="h2"&gt;📦 Supported Languages&lt;/head&gt;
                      &lt;p&gt;More languages and frameworks coming soon! 🎉&lt;/p&gt;
                      &lt;head rend="h2"&gt;🚀 Getting Started&lt;/head&gt;
                      &lt;list rend="ol"&gt;
                        &lt;item&gt;Open the Command Palette (&lt;code&gt;Cmd+Shift+P&lt;/code&gt; / &lt;code&gt;Ctrl+Shift+P&lt;/code&gt;)&lt;/item&gt;
                        &lt;item&gt;Run &lt;code&gt;Nogic: Open Visualizer&lt;/code&gt;&lt;/item&gt;
                        &lt;item&gt;Right-click files or folders in the Explorer and select &lt;code&gt;Add to Nogic Board&lt;/code&gt;&lt;/item&gt;
                      &lt;/list&gt;
                      &lt;p&gt;Your codebase is automatically indexed when you open the visualizer, if given permission.&lt;/p&gt;
                      &lt;head rend="h2"&gt;✨ Features&lt;/head&gt;
                      &lt;list rend="ul"&gt;
                        &lt;item&gt;🌲 Unified View — Browse files, classes, and functions in an interactive hierarchical graph&lt;/item&gt;
                        &lt;item&gt;📋 Boards — Create custom boards to organize and focus on specific parts of your codebase&lt;/item&gt;
                        &lt;item&gt;🎯 Class Diagrams — View class relationships, inheritance, and method structures&lt;/item&gt;
                        &lt;item&gt;🔄 Call Graphs — Trace function calls and dependencies across your codebase&lt;/item&gt;
                        &lt;item&gt;🔍 Quick Search — Find elements instantly with &lt;code&gt;Cmd/Ctrl+K&lt;/code&gt;&lt;/item&gt;
                        &lt;item&gt;⚡ Auto-sync — Changes to your code are automatically reflected in the visualization&lt;/item&gt;
                      &lt;/list&gt;
                      &lt;head rend="h2"&gt;📖 Commands&lt;/head&gt;
                      &lt;table&gt;
                        &lt;row&gt;
                          &lt;cell role="head"&gt;Command&lt;/cell&gt;
                          &lt;cell role="head"&gt;Description&lt;/cell&gt;
                        &lt;/row&gt;
                        &lt;row&gt;
                          &lt;cell&gt;
                            &lt;code&gt;Nogic: Open Visualizer&lt;/code&gt;
                          &lt;/cell&gt;
                          &lt;cell&gt;Open the interactive visualizer&lt;/cell&gt;
                        &lt;/row&gt;
                        &lt;row&gt;
                          &lt;cell&gt;
                            &lt;code&gt;Nogic: Create New Board&lt;/code&gt;
                          &lt;/cell&gt;
                          &lt;cell&gt;Create a new board&lt;/cell&gt;
                        &lt;/row&gt;
                        &lt;row&gt;
                          &lt;cell&gt;
                            &lt;code&gt;Add to Nogic Board&lt;/code&gt;
                          &lt;/cell&gt;
                          &lt;cell&gt;Add a file/folder to a board (right-click menu)&lt;/cell&gt;
                        &lt;/row&gt;
                      &lt;/table&gt;
                      &lt;head rend="h2"&gt;💡 Tips&lt;/head&gt;
                      &lt;list rend="ul"&gt;
                        &lt;item&gt;🖱️ Right-click files or folders in the Explorer to add them to a board&lt;/item&gt;
                        &lt;item&gt;👆 Double-click nodes to open files in the editor&lt;/item&gt;
                        &lt;item&gt;📂 Click nodes to expand and see methods&lt;/item&gt;
                        &lt;item&gt;🖐️ Drag to pan, scroll to zoom&lt;/item&gt;
                      &lt;/list&gt;
                    &lt;/div&gt;
                  &lt;/div&gt;
                &lt;/cell&gt;
              &lt;/row&gt;
            &lt;/table&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://marketplace.visualstudio.com/items?itemName=Nogic.nogic"/><published>2026-01-13T18:43:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46605716</id><title>Choosing learning over autopilot</title><updated>2026-01-14T00:57:02.215326+00:00</updated><content>&lt;doc fingerprint="3750761f0e740db1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;choosing learning over autopilot&lt;/head&gt;
    &lt;p&gt;I use ai coding tools a lot. I love them. I’m all-in on ai tools. They unlock doors that let me do things that I cannot do with my human hands alone.&lt;/p&gt;
    &lt;p&gt;But they also scare me.&lt;/p&gt;
    &lt;p&gt;As I see it, they offer me two paths:&lt;/p&gt;
    &lt;head rend="h4"&gt;✨ The glittering vision ✨&lt;/head&gt;
    &lt;p&gt;The glittering vision is they let me build systems in the way that the version of me who is a better engineer would build them. Experimentation, iteration and communication have become cheaper. This enables me to learn by doing at a speed that was prohibitive before. I can make better decisions about what and how to build because I can try out a version and learn where some of the sharp edges are in practice instead of guessing. I can also quickly loop in others for feedback and context. All of this leads to building a better version of the system than I would have otherwise.&lt;/p&gt;
    &lt;head rend="h4"&gt;☠️ The cursed vision ☠️&lt;/head&gt;
    &lt;p&gt;The cursed vision is I am lazy, and I build systems of ai slop that I do not understand. There’s a lot of ink spilled about perils and pains of ai slop, especially working on a team that has to maintain the resulting code.&lt;/p&gt;
    &lt;p&gt;What scares me most is an existential fear that I won’t learn anything if I work in the “lazy” way. There is no substitute for experiential learning, and it accumulates over time. There are things that are very hard for me to do today, and I will feel sad if all of those things feel equally hard in a year, two years, five years. I am motivated by an emotional response to problems I find interesting, and I like problems that have to do with computers. I am afraid of drowning that desire by substituting engaging a problem with semi-conscious drifting on autopilot.&lt;/p&gt;
    &lt;p&gt;And part of why this is scary to me is that even if my goal is to be principled, to learn, to engage, to satisfy my curiosity with understanding, it is really easy for me to coast with an llm and not notice. There are times when I am tired and I am distracted and I have a thing that I need to get done at work. I just want it done, because then I have another thing I need to do. There are a lot of reasons to be lazy.&lt;/p&gt;
    &lt;head rend="h4"&gt;So I think the crux here is about experiential learning:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ai tools make it so much easier to learn by doing, which can lead to much better results&lt;/item&gt;
      &lt;item&gt;but it’s also possible to use them take a shortcut and get away without learning &lt;list rend="ul"&gt;&lt;item&gt;I deeply believe that the shortcut is a trap&lt;/item&gt;&lt;item&gt;I also believe it is harder than it seems to notice and be honest about when I’m doing this&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And so, I’ve been thinking about guidelines &amp;amp; guardrails– how do I approach my work to escape the curse, such that llms are a tool for understanding, rather than a replacement for thinking?&lt;/p&gt;
    &lt;head rend="h4"&gt;Here’s my current working model:&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;use ai-tooling to learn, in loops&lt;/item&gt;
      &lt;item&gt;ai-generated code is cheap and not precious; throw it away and start over several times&lt;/item&gt;
      &lt;item&gt;be very opinionated about how to break down a problem&lt;/item&gt;
      &lt;item&gt;“textbook” commits &amp;amp; PRs&lt;/item&gt;
      &lt;item&gt;write my final docs / pr descriptions / comments with my human hands&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The rest of the blog post is a deeper look at these topics, in a way that I hope is pretty concrete and grounded.&lt;/p&gt;
    &lt;head rend="h1"&gt;but first, let me make this more concrete&lt;/head&gt;
    &lt;head rend="h3"&gt;Things I now get to care less about:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the mechanics of figuring out how things are hooked together&lt;/item&gt;
      &lt;item&gt;the mechanics of translating pseudocode into code&lt;/item&gt;
      &lt;item&gt;figuring out what the actual code looks like&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The times I’m using ai tools to disengage a problem and go fast are the times I’m only doing the things in this first category and getting away with skipping doing the things in the other two.&lt;/p&gt;
    &lt;head rend="h3"&gt;Things I cared about before and should still care about:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;deciding which libraries are used&lt;/item&gt;
      &lt;item&gt;how the code is organized: files &amp;amp; function signatures&lt;/item&gt;
      &lt;item&gt;leaving comments that explain why something is set up in a way if there’s complication behind it&lt;/item&gt;
      &lt;item&gt;leaving docs explaining how things work&lt;/item&gt;
      &lt;item&gt;understanding when I need to learn something more thoroughly to get unblocked&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Things I now get to care about that were expensive before:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;more deeply understanding how a system works&lt;/item&gt;
      &lt;item&gt;adding better observability like nicely structured outputs for debugging&lt;/item&gt;
      &lt;item&gt;running more experiments&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The times when I’m using ai tools to enhance my learning and understanding I’m doing the things in the latter two categories.&lt;/p&gt;
    &lt;p&gt;I will caveat that the appropriate amount of care and effort in an implementation depends, of course, on the problem and context. More is not always better. Moving slow can carry engineering risk and I know from experienced that it’s possible for a team to mistake micromanagement for code quality.&lt;/p&gt;
    &lt;p&gt;I like to work on problems somewhere in the middle of the “how correct does this have to be” spectrum and so that’s where my intuition is tuned to. I don’t need things clean down to the bits, but how the system is built matters so care is worth the investment.&lt;/p&gt;
    &lt;head rend="h1"&gt;workflow&lt;/head&gt;
    &lt;p&gt;Here is a workflow I’ve been finding useful for medium-sized problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get into the problem: go fast, be messy, learn and get oriented&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Research &amp;amp; document what I want to build &lt;list rend="ol"&gt;&lt;item&gt;I collab with the ai to dump background context and plans into a markdown file &lt;list rend="ol"&gt;&lt;item&gt;The doc at this stage can be rough&lt;/item&gt;&lt;item&gt;A format that I’ve been using: &lt;list rend="ol"&gt;&lt;item&gt;What is the problem we’re solving?&lt;/item&gt;&lt;item&gt;How does it work today?&lt;/item&gt;&lt;item&gt;How will this change be implemented?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;I collab with the ai to dump background context and plans into a markdown file &lt;/item&gt;
      &lt;item&gt;Build a prototype &lt;list rend="ol"&gt;&lt;item&gt;The prototype can be ai slop&lt;/item&gt;&lt;item&gt;Bias towards seeing things run &amp;amp; interacting with them&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Throw everything away. Start fresh, clean slate &lt;list rend="ol"&gt;&lt;item&gt;It’s much faster to build it correctly than to fix it&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Formulate a solution: figure out what the correct structure should be&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Research &amp;amp; document based on what I know from the prototype &lt;list rend="ol"&gt;&lt;item&gt;Read code, docs and readmes with my human eyes&lt;/item&gt;&lt;item&gt;Think carefully about the requirements &amp;amp; what causes complication in the code. Are those hard or flexible (or imagined!) requirements?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Design what I want to build, again&lt;/item&gt;
      &lt;item&gt;Now would be a good time to communicate externally if that’s appropriate for the scope. Write one-pager for anyone who might want to provide input.&lt;/item&gt;
      &lt;item&gt;Given any feedback, design the solution one more time, and this time polish it. Think carefully &amp;amp; question everything. Now is the time to use my brain. &lt;list rend="ol"&gt;&lt;item&gt;Important: what are the APIs? How is the code organized?&lt;/item&gt;&lt;item&gt;Important: what libraries already exist that we can use?&lt;/item&gt;&lt;item&gt;Important: what is the iterative implementation order so that the code is modular &amp;amp; easy to review?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Implement a skeleton, see how the code smells and adjust&lt;/item&gt;
      &lt;item&gt;Use this to compile a final draft of how to implement the feature iteratively&lt;/item&gt;
      &lt;item&gt;Commit the skeleton + the final implementation document&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Implement the solution: generate the final code&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cut a new branch &amp;amp; have the ai tooling implement all the code based on the final spec&lt;/item&gt;
      &lt;item&gt;If it’s not a lot of code or it’s very modular, review it and commit each logical piece into its own commit / PR&lt;/item&gt;
      &lt;item&gt;If it is a lot of code, review it, and commit it as a reference implementation &lt;list rend="ol"&gt;&lt;item&gt;Then, rollback to the skeleton branch, and cut a fresh branch for the first logic piece that will be its own commit / PR&lt;/item&gt;&lt;item&gt;Have the ai implement just that part, possibly guided by any ideas from seeing the full implementation&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;For each commit, I will review the code &amp;amp; I’ll have the ai review the code&lt;/item&gt;
      &lt;item&gt;I must write my own commit messages with descriptive trailers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One of the glittering things about ai tooling is that it’s faster than building systems by hand. I maintain that even with these added layers of learning before implementing, it’s still faster than what I could do before while giving me a richer understanding and a better result.&lt;/p&gt;
    &lt;p&gt;Now let me briefly break out the guidelines I mentioned in the intro and how they relate to this workflow.&lt;/p&gt;
    &lt;head rend="h1"&gt;learning in loops&lt;/head&gt;
    &lt;p&gt;There are a lot of ways to learn what to build and how to build it, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Understanding the system and integrations with surround systems&lt;/item&gt;
      &lt;item&gt;Understanding the problem, the requirements &amp;amp; existing work in the space&lt;/item&gt;
      &lt;item&gt;Understanding relationships between components, intended use-cases and control flows&lt;/item&gt;
      &lt;item&gt;Understanding implementation details, including tradeoffs and what a MVP looks like&lt;/item&gt;
      &lt;item&gt;Understanding how to exercise, observe and interact with the implementation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’ll understand each area in a different amount of detail at different times. I’m thinking of it as learning “in loops” because I find that ai tooling lets me quickly switch between breadth and depth in an iterative way. I find that I “understand” the problem and the solution in increasing depth and detail several times before I build it, and that leads to a much better output.&lt;/p&gt;
    &lt;p&gt;I think there two pitfalls in these learning loops: one feeling like I’m learning when I’m actually only skimming, and the other is getting stuck limited on what the ai summaries can provide. One intuition I’ve been trying to build is when to go read the original sources (like code, docs, readmes) myself. I have two recent experiences top-of-mind informing this:&lt;/p&gt;
    &lt;p&gt;In the first experience, a coworker and I were debugging a mysterious issue related to some file-related resource exhaustion. We both used ai tools to figure out what cli tools we had to investigate and to build a mental model of how the resource in question was supposed to work. I got stuck after getting output that seemed contradictory, and didn’t fit my mental model. My coworker got to a similar spot and then took a step out of the ai tooling to go read the docs about the resource with their human eyes. That led them to understand that the ai summary wasn’t accurate: it had missed some details that explained the confusing situation we were seeing.&lt;/p&gt;
    &lt;p&gt;This example really sticks out in my memory. I thought I was being principled rather than lazy by building my mental model of what was supposed to be happening, but I had gotten mired in building that mental model second-hand instead of reading the docs myself.&lt;/p&gt;
    &lt;p&gt;In the second experience, I was working on a problem related to integrating with a system that had a documented interface. I had the ai read &amp;amp; summarize the interface and then got into the problem in a way similar to the first step of the workflow I described above. I was using that to formulate an idea of what the solution should be. Then I paused to repeat the research loop but with more care: I read the interface with my human eyes– and found the ai summary was wrong! It wasn’t a big deal and I could shift my plans, but I was glad to have learned to pause and take care in validating the details of my mental model.&lt;/p&gt;
    &lt;head rend="h1"&gt;ai-generated code is throw-away code&lt;/head&gt;
    &lt;p&gt;I had a coworker describe working with ai coding tools like working on a sculpture. When they asked it to reposition the arm, it would accidentally bump the nose out of alignment.&lt;/p&gt;
    &lt;p&gt;The way I’m thinking about it now, it’s more like: instead of building a sculpture, I’m asking it to build me a series of sculptures.&lt;/p&gt;
    &lt;p&gt;The first one is rough-hewn and wonky, but lets me understand the shape of what I’m doing.&lt;/p&gt;
    &lt;p&gt;The next one or two are just armatures.&lt;/p&gt;
    &lt;p&gt;The next one might be a mostly functional sculpture on the latest armature; this lets me understand the shape of what I’m doing with much higher precision.&lt;/p&gt;
    &lt;p&gt;And then finally, I’ll ask for a sculpture, using the vetted armature, except we’ll build it one part at a time. When we’re done with a part, we’ll seal it so we can’t bump it out of alignment.&lt;/p&gt;
    &lt;p&gt;A year ago, I wasn’t sure if it was better to try to fix an early draft of ai generated code to be better, or to throw it out. Now I feel strongly that ai-generated code is not precious, and not worth the effort to fix it. If you know what the code needs to do and have that clearly documented in detail, it takes no time at all for the ai to flesh out the code. So throw away all the earlier versions, and focus on getting the armature correct.&lt;/p&gt;
    &lt;p&gt;Making things is all about processes and doing the right thing at the right time. If you throw a bowl and that bowl is off-center, it is a nightmare to try to make it look centered with trimming. If you want a centered bowl then you must throw it on-center. Same here, if you want code that is modular and well structured, the time to do that is before you have the ai implement the logic.&lt;/p&gt;
    &lt;head rend="h2"&gt;“textbook” commits and PRs&lt;/head&gt;
    &lt;p&gt;It’s much easier to review code that has been written in a way where a feature is broken up into an iteration of commits and PRs. This was true before ai tooling, and is true now.&lt;/p&gt;
    &lt;p&gt;The difference is that writing code with my hands was slow and expensive. Sometimes I’d be in the flow and I’d implement things in a way that was hard to untangle after the fact.&lt;/p&gt;
    &lt;p&gt;I believe that especially if I work in the way I’ve been describing here, ai code is cheap. This makes it much easier/cheaper for me to break apart my work into ways that are easy to commit and review.&lt;/p&gt;
    &lt;p&gt;My other guilty hesitation before ai tooling was I never liked git merge conflicts and rebasing branches. It was confusing and had the scary potential of losing work. Now, ai tooling is very good at rebasing branches, so it’s much less scary and pretty much no effort.&lt;/p&gt;
    &lt;p&gt;I also think that small, clean PRs are an external forcing function to working in a way that builds my understanding rather than lets me take shortcuts: if I generate 2.5k lines of ai slop, it will be a nightmare to break that into PRs.&lt;/p&gt;
    &lt;head rend="h2"&gt;i am very opinionated about how to break down a problem&lt;/head&gt;
    &lt;p&gt;I’m very opinionated in breaking down problems in two ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;how to structure the implementation (files, functions, libraries)&lt;/item&gt;
      &lt;item&gt;how to implement iteratively to make clean commits and PRs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The only way to achieve small, modular, reviewable PRs is to be very opinionated about what to implement and in what order.&lt;/p&gt;
    &lt;p&gt;Unless you’re writing a literal prototype that will be thrown away (and you’re confident it will actually be thrown away), the most expensive part about building a system is the engineering effort that will go into maintaining it. It is, therefore, very worth-while to be opinionated about how to structure the code. I find that the ai can do an okay job at throwing code out there, but I can come up with a much better division and structure by using my human brain.&lt;/p&gt;
    &lt;p&gt;A time I got burned by not thinking about libraries &amp;amp; how to break down a problem was when I was trying to fix noisy errors due to a client chatting with a system that had some network blips. I asked an ai model to add rate limiting to an existing http client, which it did by implementing exponential backoff itself. This isn’t a very good solution, surely we don’t need to do that ourselves. I didn’t think this one through, and was glad a coworker with their brain on caught it in code review.&lt;/p&gt;
    &lt;head rend="h2"&gt;i write docs &amp;amp; pr descriptions with my human hands&lt;/head&gt;
    &lt;p&gt;Writing can serve a few distinct purposes: one is communication, and distinct from that, one is as a method to facilitate thinking. The act of writing forces me to organize and refine my thoughts.&lt;/p&gt;
    &lt;p&gt;This is a clear smell-test for me: I must be able to write documents that explain how and why something is implemented. If I can’t, then that’s a clear sign that I don’t actually understand it; I have skipped writing as a method of thinking.&lt;/p&gt;
    &lt;p&gt;On the communication side of things, I find that the docs or READMEs that ai tooling generates often capture things that aren’t useful. I often don’t agree with their intuition; I find that if I take the effort to use my brain I produce documents that I believe are more relevant.&lt;/p&gt;
    &lt;p&gt;This isn’t to say that I don’t use ai tooling to write documents: I’ll often have ai dump information into markdown files as I’m working. I’ll often have ai tooling nicely format things like diagrams or tables. Sometimes I’ll have ai tooling take a pass at a document. I’ll often hand a document to ai tooling and ask it to validate whether everything I wrote is accurate based on the implementation.&lt;/p&gt;
    &lt;p&gt;But I do believe that if I hold myself to the standard that I write docs, commit messages, etc with my hands, I both produce higher quality documentation and force myself to be honest about understanding what I’m describing.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In conclusion, I find that ai coding tools give me a glittering path to understand better by doing, and using that understanding to build better systems. I also, however, think there is a curse of using these systems in a way that skips the “build the understanding” part, and that pitfall is subtler than it may seem.&lt;/p&gt;
    &lt;p&gt;I care deeply about, and think it will be important in the long-run, to leverage these tools for learning and engaging. I’ve outlined the ways I’m thinking about how to do best do this and avoid the curse:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;use ai-tooling to learn, in loops&lt;/item&gt;
      &lt;item&gt;ai-generated code is cheap and not precious; throw it away and start over several times&lt;/item&gt;
      &lt;item&gt;be very opinionated about how to break down a problem&lt;/item&gt;
      &lt;item&gt;“textbook” commits &amp;amp; PRs&lt;/item&gt;
      &lt;item&gt;write my final docs / pr descriptions / comments with my human hands&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://anniecherkaev.com/choosing-learning-over-autopilot"/><published>2026-01-13T18:46:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46606902</id><title>Open sourcing Dicer: Databricks's auto-sharder</title><updated>2026-01-14T00:57:01.681628+00:00</updated><content>&lt;doc fingerprint="2758961f4195ccc6"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we are excited to announce the open sourcing of one of our most critical infrastructure components, Dicer: Databricks’ auto-sharder, a foundational system designed to build low latency, scalable, and highly reliable sharded services. It is behind the scenes of every major Databricks product, enabling us to deliver a consistently fast user experience while improving fleet efficiency and reducing cloud costs. Dicer achieves this by dynamically managing sharding assignments to keep services responsive and resilient even in the face of restarts, failures, and shifting workloads. As detailed in this blog post, Dicer is used for a variety of use cases including high-performance serving, work partitioning, batching pipelines, data aggregation, multi-tenancy, soft leader election, efficient GPU utilization for AI workloads, and more.&lt;/p&gt;
    &lt;p&gt;By making Dicer available to the broader community, we look forward to collaborating with industry and academia to advance the state of the art in building robust, efficient, and high-performance distributed systems. In the rest of this post, we discuss the motivation and design philosophy behind Dicer, share success stories from its use at Databricks, and provide a guide on how to install and experiment with the system yourself.&lt;/p&gt;
    &lt;p&gt;Databricks ships a rapidly expanding suite of products for data processing, analytics, and AI. To support this at scale, we operate hundreds of services that must handle massive state while maintaining responsiveness. Historically, Databricks engineers had relied on two common architectures, but both introduced significant problems as services grew:&lt;/p&gt;
    &lt;p&gt;Most services at Databricks began with a stateless model. In a typical stateless model, the application does not retain in-memory state across requests, and must re-read the data from the database on every request. This architecture is inherently expensive as every request incurs a database hit, driving up both operational costs and latency [1].&lt;/p&gt;
    &lt;p&gt;To mitigate these costs, developers would often introduce a remote cache (like Redis or Memcached) to offload work from the database. While this improved throughput and latency, it failed to solve several fundamental inefficiencies:&lt;/p&gt;
    &lt;p&gt;Moving to a sharded model and caching state in memory eliminated these layers of overhead by colocating the state directly with the logic that operates on it. However, static sharding introduced new problems.&lt;/p&gt;
    &lt;p&gt;Before Dicer, sharded services at Databricks relied on static sharding techniques (e.g., consistent hashing). While this approach was simple and allowed our services to efficiently cache state in memory, it introduced three critical issues in production:&lt;/p&gt;
    &lt;p&gt;As our services grew more and more to meet demand, eventually static sharding looked like a terrible idea. This led to a common belief among our engineers that stateless architectures were the best way to build robust systems, even if it meant eating the performance and resource costs. This was around the time when Dicer was introduced.&lt;/p&gt;
    &lt;p&gt;The production perils of static sharding, contrasted with the costs of going stateless, left several of our most critical services in a difficult position. These services relied on static sharding to deliver a snappy user experience to our customers. Converting them to a stateless model would have introduced a significant performance penalty, not to mention added cloud costs for us.&lt;/p&gt;
    &lt;p&gt;We built Dicer to change this. Dicer addresses the fundamental shortcomings of static sharding by introducing an intelligent control plane that continuously and asynchronously updates a service’s shard assignments. It reacts to a wide range of signals, including application health, load, termination notices, and other environmental inputs. As a result, Dicer keeps services highly available and well balanced even during rolling restarts, crashes, autoscaling events, and periods of severe load skew.&lt;/p&gt;
    &lt;p&gt;As an auto-sharder, Dicer builds on a long line of prior systems, including Centrifuge [3], Slicer [4], and Shard Manager [5]. We introduce Dicer in the next section, and describe how it has helped improve performance, reliability, and efficiency of our services.&lt;/p&gt;
    &lt;p&gt;We now give an overview of Dicer, its core abstractions, and describe its various use cases. Stay tuned for a technical deep dive into Dicer’s design and architecture in a future blog post.&lt;/p&gt;
    &lt;p&gt;Dicer models an application as serving requests (or otherwise performing some work) associated with a logical key. For example, a service that serves user profiles might use user IDs as its keys. Dicer shards the application by continuously generating an assignment of keys to pods to keep the service highly available and load balanced.&lt;/p&gt;
    &lt;p&gt;To scale to applications with millions or billions of keys, Dicer operates on ranges of keys rather than individual keys. Applications represent keys to Dicer using a SliceKey (a hash of the application key), and a contiguous range of SliceKeys is called a Slice. As shown in Figure 1, a Dicer Assignment is a collection of Slices that together span the full application keyspace, with each Slice assigned to one or more Resources (i.e. pods). Dicer dynamically splits, merges, replicates, and reassigns Slices in response to application health and load signals, ensuring that the entire keyspace is always assigned to healthy pods and that no single pod becomes overloaded. Dicer can also detect hot keys and split them out into their own slices, and assign such slices to multiple pods to distribute the load.&lt;/p&gt;
    &lt;p&gt;Figure 1 shows an example Dicer assignment across 3 pods (P0, P1, and P2) for an application sharded by user ID, where the user with ID 13 is represented by SliceKey K26 (i.e. a hash of ID 13), and is currently assigned to pod P0. A hot user with user ID 42 and represented by SliceKey K10 has been isolated in its own slice and assigned to multiple pods to handle the load (P1 and P2).&lt;/p&gt;
    &lt;p&gt;Figure 2 shows an overview of a sharded application integrated with Dicer. Application pods learn the current assignment through a library called the Slicelet (S for server side). The Slicelet maintains a local cache of the latest assignment by fetching it from the Dicer service and watching for updates. When it receives an updated assignment, the Slicelet notifies the application via a listener API.&lt;/p&gt;
    &lt;p&gt;Assignments observed by Slicelets are eventually consistent, a deliberate design choice that prioritizes availability and fast recovery over strong key ownership guarantees. In our experience this has been the right model for the vast majority of applications, though we do plan to support stronger guarantees in the future, similar to Slicer and Centrifuge.&lt;/p&gt;
    &lt;p&gt;Besides keeping up-to-date on the assignment, applications also use the Slicelet to record per key load when handling requests or performing work for a key. The Slicelet aggregates this information locally and asynchronously reports a summary to the Dicer service. Note that, like assignment watching, this also occurs off the application’s critical path, ensuring high performance.&lt;/p&gt;
    &lt;p&gt;Clients of a Dicer sharded application find the assigned pod for a given key through a library called the Clerk (C for client side). Like Slicelets, Clerks also actively maintain a local cache of the latest assignment in the background to ensure high performance for key lookups on the critical path.&lt;/p&gt;
    &lt;p&gt;Finally, the Dicer Assigner is the controller service responsible for generating and distributing assignments based on application health and load signals. At its core is a sharding algorithm that computes minimal adjustments through Slice splits, merges, replication/dereplication, and moves to keep keys assigned to healthy pods and the overall application sufficiently load balanced. The Assigner service is multi-tenant and designed to provide auto-sharding service for all sharded applications within a region. Each sharded application served by Dicer is referred to as a Target.&lt;/p&gt;
    &lt;p&gt;Dicer is valuable for a wide range of systems because the ability to affinitize workloads to specific pods yields significant performance improvements. We have identified several core categories of use cases based on our production experience.&lt;/p&gt;
    &lt;p&gt;Dicer excels at scenarios where a large corpus of data must be loaded and served directly from memory. By ensuring that requests for specific keys always hit the same pods, services like key-value stores can achieve sub-millisecond latency and high throughput while avoiding the overhead of fetching data from remote storage.&lt;/p&gt;
    &lt;p&gt;Dicer is also well suited to modern LLM inference workloads, where maintaining affinity is critical. Examples include stateful user sessions that accumulate context in a per-session KV cache, as well as deployments that serve large numbers of LoRA adapters and must shard them efficiently across constrained GPU resources.&lt;/p&gt;
    &lt;p&gt;This is one of the most common use cases at Databricks. It includes systems such as cluster managers and query orchestration engines that continuously monitor resources to manage scaling, compute scheduling, and multi-tenancy. To operate efficiently, these systems maintain monitoring and control state locally, avoiding repeated serialization and enabling timely responses to change.&lt;/p&gt;
    &lt;p&gt;Dicer can be used to build high-performance distributed remote caches, which we have done in production at Databricks. By using Dicer’s capabilities, our cache can be autoscaled and restarted seamlessly without loss of hit rate, and avoid load imbalance due to hot keys.&lt;/p&gt;
    &lt;p&gt;Dicer is an effective tool for partitioning background tasks and asynchronous workflows across a fleet of servers. For example, a service responsible for cleaning up or garbage-collecting state in a massive table can use Dicer to ensure that each pod is responsible for a distinct, non-overlapping range of the keyspace, preventing redundant work and lock contention.&lt;/p&gt;
    &lt;p&gt;For high-volume write paths, Dicer enables efficient record aggregation. By routing related records to the same pod, the system can batch updates in memory before committing them to persistent storage. This significantly reduces the input/output operations per second required and improves the overall throughput of the data pipeline.&lt;/p&gt;
    &lt;p&gt;Dicer can be used to implement "soft" leader selection by designating a specific pod as the primary coordinator for a given key or shard. For example, a serving scheduler can use Dicer to ensure that a single pod acts as the primary authority for managing a group of resources. While Dicer currently provides affinity-based leader selection, it serves as a powerful foundation for systems that require a coordinated primary without the heavy overhead of traditional consensus protocols. We are exploring future enhancements to provide stronger guarantees around mutual exclusion for these workloads.&lt;/p&gt;
    &lt;p&gt;Dicer acts as a natural rendezvous point for distributed clients needing real-time coordination. By routing all requests for a specific key to the same pod, that pod becomes a central meeting place where shared state can be managed in local memory without external network hops.&lt;/p&gt;
    &lt;p&gt;For example, in a real-time chat service, two clients joining the same "Chat Room ID" are automatically routed to the same pod. This allows the pod to synchronize their messages and state instantly in memory, avoiding the latency of a shared database or a complex back-plane for communication.&lt;/p&gt;
    &lt;p&gt;Numerous services at Databricks have achieved significant gains with Dicer, and we highlight several of these success stories below.&lt;/p&gt;
    &lt;p&gt;Unity Catalog (UC) is the unified governance solution for data and AI assets across the Databricks platform. Originally designed as a stateless service, UC faced significant scaling challenges as its popularity grew, driven primarily by extremely high read volume. Serving each request required repeated access to the backend database, which introduced prohibitive latency. Conventional approaches such as remote caching were not viable, as the cache needed to be updated incrementally and remain snapshot consistent with storage. In addition, customer catalogs can be gigabytes in size, making it costly to maintain partial or replicated snapshots in a remote cache without introducing substantial overhead.&lt;/p&gt;
    &lt;p&gt;To solve this, the team integrated Dicer to build a sharded in-memory stateful cache. This shift allowed UC to replace expensive remote network calls with local method calls, drastically reducing database load and improving responsiveness. The figure below illustrates the initial rollout of Dicer, followed by the deployment of the full Dicer integration. By utilizing Dicer’s stateful affinity, UC achieved a cache hit rate of 90–95%, significantly lowering the frequency of database round-trips.&lt;/p&gt;
    &lt;p&gt;Databricks’ query orchestration engine, which manages query scheduling on Spark clusters, was originally built as an in-memory stateful service using static sharding. As the service scaled, the limitations of this architecture became a significant bottleneck; due to the simple implementation, scaling required manual re-sharding which was extremely toilsome, and the system suffered from frequent availability dips, even during rolling restarts.&lt;/p&gt;
    &lt;p&gt;After integrating with Dicer, these availability issues were eliminated (see Figure 4). Dicer enabled zero downtime during restarts and scaling events, allowing the team to reduce toil and improve system robustness by enabling auto-scaling everywhere. Additionally, Dicer’s dynamic load balancing feature further resolved chronic CPU throttling, resulting in more consistent performance across the fleet.&lt;/p&gt;
    &lt;p&gt;For services that are not sharded, we developed Softstore, a distributed remote key value cache. Softstore leverages a Dicer feature called state transfer, which migrates data between pods during resharding to preserve application state. This is particularly important during planned rolling restarts, where the full keyspace is unavoidably churned. In our production fleet, planned restarts account for roughly 99.9% of all restarts, making this mechanism especially impactful and enables seamless restarts with negligible impact on cache hit rates. Figure 5 shows Softstore hit rates during a rolling restart, where state transfer preserves a steady ~85% hit rate for a representative use case, with the remaining variability driven by normal workload fluctuations.&lt;/p&gt;
    &lt;p&gt;You can try out Dicer today on your machine by downloading it from here. A simple demo to show its usage is provided here - it shows a sample Dicer setup with one client and a few servers for an application. Please see the README and user guide for Dicer.&lt;/p&gt;
    &lt;p&gt;Dicer is a critical service used across Databricks with its usage growing quickly. In the future, we will be publishing more articles about Dicer’s inner workings and designs. We will also release more features as we build and test them out internally, e.g., Java and Rust libraries for clients and servers, and the state transfer capabilities mentioned in this post. Please give us your feedback and stay tuned for more!&lt;/p&gt;
    &lt;p&gt;If you like solving tough engineering problems and would like to join Databricks, check out databricks.com/careers!&lt;/p&gt;
    &lt;p&gt;[1] Ziming Mao, Jonathan Ellithorpe, Atul Adya, Rishabh Iyer, Matei Zaharia, Scott Shenker, Ion Stoica (2025). Rethinking the cost of distributed caches for datacenter services. Proceedings of the 24th ACM Workshop on Hot Topics in Networks, 1–8.&lt;/p&gt;
    &lt;p&gt;[2] Atul Adya, Robert Grandl, Daniel Myers, Henry Qin. Fast key-value stores: An idea whose time has come and gone. Proceedings of the Workshop on Hot Topics in Operating Systems (HotOS ’19), May 13–15, 2019, Bertinoro, Italy. ACM, 7 pages. DOI: 10.1145/3317550.3321434.&lt;/p&gt;
    &lt;p&gt;[3] Atul Adya, James Dunagan, Alexander Wolman. Centrifuge: Integrated Lease Management and Partitioning for Cloud Services. Proceedings of the 7th USENIX Symposium on Networked Systems Design and Implementation (NSDI), 2010.&lt;/p&gt;
    &lt;p&gt;[4] Atul Adya, Daniel Myers, Jon Howell, Jeremy Elson, Colin Meek, Vishesh Khemani, Stefan Fulger, Pan Gu, Lakshminath Bhuvanagiri, Jason Hunter, Roberto Peon, Larry Kai, Alexander Shraer, Arif Merchant, Kfir Lev-Ari. Slicer: Auto-Sharding for Datacenter Applications. Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2016, pp. 739–753.&lt;/p&gt;
    &lt;p&gt;[5] Sangmin Lee, Zhenhua Guo, Omer Sunercan, Jun Ying, Chunqiang Tang, et al. Shard Manager: A Generic Shard Management Framework for Geo distributed Applications. Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP), 2021. DOI: 10.1145/3477132.3483546.&lt;/p&gt;
    &lt;p&gt;[6] Atul Adya, Jonathan Ellithorpe. Stateful services: low latency, efficiency, scalability — pick three. High Performance Transaction Systems Workshop (HPTS) 2024, Pacific Grove, California, September 15–18, 2024.&lt;/p&gt;
    &lt;p&gt;Product&lt;/p&gt;
    &lt;p&gt;December 10, 2024/7 min read&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.databricks.com/blog/open-sourcing-dicer-databricks-auto-sharder"/><published>2026-01-13T19:56:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46608731</id><title>Running Lean at Scale</title><updated>2026-01-14T00:57:01.229418+00:00</updated><content>&lt;doc fingerprint="3758d9cd66a084a1"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt; Our infrastructure team has developed a custom automated reinforcement learning system to continuously improve our models for proving Lean theorems. A critical part of this system is our Lean execution framework, which weâve named the REPL service. The REPL service is responsible for all interactions between our models and actual Lean proofs. This service enables our RL system to scale independently of our GPU capacity, is semantically stateless, and scales to hundreds of thousands of CPUs. All of this achieved while using preemptible instances, which are the least expensive form of computing power on the market. &lt;/p&gt;
      &lt;p&gt; In this article, we'll first provide background on Lean before describing the design constraints of the REPL service. We will then recount the story of our attempts to scale it up, including both our failures and successes. Finally, we'll outline future work and the key takeaways from this project. &lt;/p&gt;
      &lt;p&gt; Letâs get started! &lt;/p&gt;
      &lt;p&gt; Background &lt;/p&gt;
      &lt;p&gt; Lean is a powerful language designed for interactively proving theorems. Here is a partial Lean proof of the fact that if or , then : &lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;
  theorem my_theorem (x : â¤) (h : x = 2 â¨ x = -2) : x * x = 4 := by
    rcases h with ha | hb
    . rw [ha]
      rfl
    . have my_lemma : -2 * -2 = 4 := by sorry
      rw [hb] ; exact my_lemma
            &lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; At a high level, the proof consists of tactics, such as &lt;code&gt;rfl&lt;/code&gt;, &lt;code&gt;exact&lt;/code&gt;, &lt;code&gt;rcases&lt;/code&gt;, etc., each of which represents a logical âstepâ in the proof. At each point between tactics in a Lean proof, there is a proof state, which consists of the available variables and any already-proven facts, along with the goals, which contain what still needs to be proved. Each tactic modifies the current set of goals, producing zero or more subgoals. If no subgoals are produced, that branch of the proof is complete. Tactics may take arguments, and can be nested, as shown above.
      &lt;/p&gt;
      &lt;p&gt; The above proof could be represented as a tree, where the proof states are nodes (blue) and the tactics are edges (magenta): &lt;/p&gt;
      &lt;p&gt; The keyword &lt;code&gt;sorry&lt;/code&gt; means the proof is incomplete - it is our systemâs job to replace &lt;code&gt;sorry&lt;/code&gt; with a proof of the relevant fact (in this case, that 
        
        ). To do this, we need to explore multiple possible proof states that could arise from applying tactics at the state represented by that &lt;code&gt;sorry&lt;/code&gt;, to find a sequence of tactics that results in no goals. We use a program called the Lean REPL for this, which (despite its name) allows us to interact with Lean proofs programmatically rather than interactively. The REPLâs basic operations are:
      &lt;/p&gt;
      &lt;list class="blog-list" rend="ul"&gt;
        &lt;item&gt;Run Lean code. This operation takes a Lean file as input, and produces a set of states corresponding to each incomplete proof.&lt;/item&gt;
        &lt;item&gt;Run tactic. This operation takes a state as input, and produces a (possibly empty) set of states as output.&lt;/item&gt;
        &lt;item&gt;Export state. This operation returns a serialized representation of the data contained in a state, which can be loaded into another REPL process.&lt;/item&gt;
        &lt;item&gt;Import state. This loads a state generated by the above method into the current process.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt; Using these operations, we can implement tree exploration by taking the original theorem statement (with one or more sorries), running it as Lean code, taking the states produced from that call, and recursively attempting to run tactics on those states. We can use the export and import state operations to âsplitâ the tree into multiple processes, allowing us to explore multiple branches in parallel. Next, weâll describe what is needed to do this at scale. &lt;/p&gt;
      &lt;p&gt; Requirements &lt;/p&gt;
      &lt;p&gt; The REPL service is used in many places and is critical for many of our operations, so it has strict requirements. &lt;/p&gt;
      &lt;list class="blog-list" rend="ul"&gt;
        &lt;item&gt;It must handle Lean errors, network problems, timeouts, and other kinds of issues robustly. It isnât acceptable for an invalid tactic to cause an unrecoverable error, for example, and the client should be able to reconnect after a timeout or other network problem and resume whatever it was doing.&lt;/item&gt;
        &lt;item&gt;It must be able to run at a much larger scale than our GPU hardware. We need many more CPUs than our GPU-enabled machines have, and those machines have a fixed ratio of CPUs to GPUs, so simply getting more of them wonât help.&lt;/item&gt;
        &lt;item&gt;It must run on the cheapest hardware possible. In our case, this means preemptible instances in Google Cloud Platform. This also means that it must be able to handle instance preemption gracefully.&lt;/item&gt;
        &lt;item&gt;We must be able to autoscale this service to maintain at least 90% utilization. In the case of traffic spikes, we should be able to scale up from zero (or one) instance to the required amount and clear the queue within 10 minutes.&lt;/item&gt;
        &lt;item&gt;The query volume could be very large, so we must be able to send many requests over the same TCP connection. Queries may take very different amounts of time, so we must also be able to send many concurrent queries per connection and receive query results out of order. Simply pipelining wonât suffice here because it would cause head-of-line blocking, which would artificially slow down fast tactics - we need a fully-asynchronous protocol.&lt;/item&gt;
        &lt;item&gt;We must be able to share the same instance of the service between multiple client processes and GCP instances, to maximize efficiency.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt; With this in mind, we built a proof-of-concept that didnât yet meet all of these requirements, but which we could use as a solid foundation. &lt;/p&gt;
      &lt;p&gt; REPL service v0 &lt;/p&gt;
      &lt;p&gt; This version of the REPL service isnât actually a service — itâs a library that runs the Lean REPL locally, and encapsulates its operations into the API described above. This is implemented in a Python class named AsyncLeanREPLProcess, which is responsible for managing the REPL process, encoding requests and decoding responses, communicating with the REPL process, and managing exported state files on the local disk. After running Lean code, AsyncLeanREPLProcess returns the unsolved goals, a list of any errors that occurred, and references to the state files on disk. &lt;/p&gt;
      &lt;p&gt; AsyncLeanREPLProcess presents an async API in order to avoid blocking the event loop, even though the underlying process is CPU-bound and can only handle a single query at a time. When multiple queries are run concurrently, we queue them up using Pythonâs concurrency primitives. &lt;/p&gt;
      &lt;p&gt; To run multiple REPL processes in parallel, we also created AsyncLeanREPLPool. This class presents the same interface as AsyncLeanREPLProcess, but internally delegates to a dynamically-sized pool of AsyncLeanREPLProcesses. This diagram summarizes the objectsâ relationships: &lt;/p&gt;
      &lt;p&gt; This allows a single Python process to efficiently use all of the available CPUs on the local machine to run Lean code, but still doesnât allow us to scale beyond the limits of one machine. &lt;/p&gt;
      &lt;p&gt; To do so, we have to make this a real service. &lt;/p&gt;
      &lt;p&gt; REPL service v1: GKE, FastAPI, and WebSockets &lt;/p&gt;
      &lt;p&gt; Our first attempt at building this service was fairly simple. We set up a Google Kubernetes Engine cluster using preemptible instances and set up an application load balancer to route connections to backends. We then wrapped AsyncLeanREPLProcess in a simple FastAPI service that has a single WebSocket endpoint, over which multiple queries and responses may be sent. This architecture allows us to scale beyond one machine, but is more complex: &lt;/p&gt;
      &lt;p&gt; To implement out-of-order responses, we use the request ID pattern. When a method is called on the client, it creates a new future, sends its request, and waits for that future to be resolved. A background task listens for responses from the server, and resolves the correct future when each response is received. Each request and response contains a &lt;code&gt;request_id&lt;/code&gt; field, which the client uses to look up the future to the request when a response is received. This pattern makes the client safe to use in a fully-concurrent manner, and allows the protocol to be fully asynchronous, per the requirements.
      &lt;/p&gt;
      &lt;p&gt; The API needed a few minor changes when scaling beyond one machine. For example, state references now have to include not only the state file name, but also the hostname of the machine that has the state data. This way, we can ensure that we donât use the wrong state data if a query goes to a different backend and that backend happens to have a state file with the same name. &lt;/p&gt;
      &lt;p&gt; This architecture was fairly simple to build, but it ultimately didnât work well. One major issue is that it pinned each connection to one backend, since the loadbalancer can only route entire WebSocket connections, not individual messages within a single connection. This makes it easy for the system to get into an unbalanced state, because the durations of client connections and the amount of work each one does aren't easily predictable. This makes it impossible to scale up reliably. &lt;/p&gt;
      &lt;p&gt; More importantly, this pinning behavior also causes the system to not handle disconnections well. If a client disconnects but wants to do more REPL work, it needs to reconnect and also get routed to the same backend that it was previously connected to. However, we canât easily guarantee this using the loadbalancer since there isnât a natural pinning key we could use — we want most connections to go to the least-loaded backend (though the loadbalancer didnât always do this either) and only reconnections to be specially routed. The pinning behavior also means that scaling down always interrupts some connections, so this broken disconnection handling is a major problem. &lt;/p&gt;
      &lt;p&gt; Furthermore, the WebSocket connections simply werenât reliable. We never determined a singular root cause. Sometimes it was due to node preemptions, but sometimes the connection would just get dropped even when no nodes were preempted. We suspected that this was due to the loadbalancer expecting a heartbeat that the client or server wasnât sending in time, but the issues described above made it clear that a redesign was needed anyway, so we didnât spend much time on this. &lt;/p&gt;
      &lt;p&gt; So, is there a way to rearchitect the service to do away with connection pinning? &lt;/p&gt;
      &lt;p&gt; REPL service v2: gRPC &lt;/p&gt;
      &lt;p&gt; Googleâs application load balancers support gRPC natively, which uses HTTP/2 as a transport, and can route individual requests from one client to multiple backends. This is exactly what we want - it eliminates the pinning problems described above, but it introduces a new one: how do we make sure that the necessary state data is available on any backend that a request may be routed to? We needed to design some way to transfer state data between backends. &lt;/p&gt;
      &lt;p&gt; Fortunately, it turns out that state data compresses very well! We were able to compress state files down to about 8% of their original sizes, which made it feasible to send the state data back to the client for it to include in subsequent requests, instead of only a reference to a file on one of the backends. The service was semantically stateless after this change, so we then safely unpinned client connections from the backends. &lt;/p&gt;
      &lt;p&gt; The infrastructure for this version of the REPL service is very similar to v1: &lt;/p&gt;
      &lt;p&gt; Unfortunately, this architecture didnât work perfectly either, and we encountered many problems in building it: &lt;/p&gt;
      &lt;list class="blog-list" rend="ul"&gt;
        &lt;item&gt;Switching to HTTP/2 is not a trivial change, even when using the official gRPC Python libraries. Encryption is not optional in HTTP/2; the load balancer requires it, the client checks the server certificate, and thereâs no easy way to tell both of these entities that weâre already in a VPC and donât need encryption. Setting up self-signed certificates works, but introduces another layer of complexity that wasnât necessary before.&lt;/item&gt;
        &lt;item&gt;The seemingly-random disconnection problem from v1 wasnât fixed. This time, thereâs another added layer of complexity: each HTTP/2 connection has a limit on the number of streams opened within the connection (here, a stream corresponds to a single request/response pair). This is not a limit on the number of concurrent streams, itâs a limit on the total number of streams over the lifetime of the connection! So, some of our connections that do a lot of work would be disconnected by this mechanism, and would have to reconnect manually.&lt;/item&gt;
        &lt;item&gt;The gRPC Python client library caused problems in multiprocess environments. It creates several non-Python background threads whose state gets corrupted after a fork(), but the library automatically restarts those threads after a fork, which causes error spam at best and crashes at worst.&lt;/item&gt;
        &lt;item&gt;Thereâs no global queueing on the load balancer, so thereâs no way to queue requests fairly across multiple clients. Furthermore, thereâs no transparency into how many requests are waiting to execute overall, how long theyâve been waiting, or how many backends would be needed to clear out the queue in a timely manner. This means we canât implement fast scale-up, as described in the requirements.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt; At this point, it seemed that our problems were mostly caused by the protocol, and the complexity that comes along with it. So, we needed to switch directions againâ¦ &lt;/p&gt;
      &lt;p&gt; REPL service v3 &lt;/p&gt;
      &lt;p&gt; This time, we built only what we needed, and not more. We designed a simple binary protocol; each message has a request ID, command, response count, and data. To route this protocol consistent with our requirements for fairness and autoscaling, we built a custom load balancer in C++, which is really more like a message queue. Requests from clients are immediately routed to any backend that has idle CPU capacity; if none are available, then requests go into a global queue and are forwarded as soon as any backend CPU becomes idle. The router knows how many CPUs each backend has, so it can correctly handle backends of different sizes in the same cluster. This keeps all the CPUs equally busy across the entire cluster. &lt;/p&gt;
      &lt;p&gt; Using this architecture, a single 8-core router instance can theoretically manage about 200,000 backend CPUs, but we instead run multiple smaller clusters (with one router for each) in multiple GCP regions for locality and reliability reasons. &lt;/p&gt;
      &lt;p&gt; The infrastructure looks similar to v1 and v2 at a glance: &lt;/p&gt;
      &lt;p&gt; However, there are a few significant differences from v2 that make v3 easier to work with and more reliable. &lt;/p&gt;
      &lt;p&gt; First, to simplify the router and handle backend preemption gracefully, we inverted the connection direction for the backends: they connect to the router just like normal clients, but the backends immediately send a message telling the router that they are backends rather than clients, and the router can then forward requests to them. This pattern means that the router doesnât need to do any health checks on the backends, nor does it need to have any way to discover them - the TCP connection itself acts as the health check. (There are periodic application-layer heartbeat messages if the connection is idle.) When a backend connects to the router and sends the register message, itâs ready, and when it disconnects, it is either broken or was shut down intentionally. The router can then retry any in-progress requests that were assigned to that backend on a different backend. &lt;/p&gt;
      &lt;p&gt; Second, the router provides global request queueing, which was missing in the previous architectures. This is necessary for fairness between clients, and for autoscaling to work well. GKEâs built-in autoscaling doesnât suffice here because there are two different behavior domains: when the queue is empty, we use GKEâs default behavior of scaling based on CPU usage, but when the queue is not empty, we expect all backend CPUs to be busy, and we want to use the queue length and processing rate instead to determine how many backends we need. The router produces timeseries metrics that include these necessary values, so we use these in a script that checks the metrics each minute and adjusts the cluster size appropriately to handle the nonempty-queue behavior domain. The algorithm is essentially: &lt;/p&gt;
      &lt;list class="blog-list" rend="ul"&gt;
        &lt;item&gt;Measure the current request queue length (Lq), total requests completed over the past minute (Rmin), and average backend count over the past minute (Bmin)&lt;/item&gt;
        &lt;item&gt;If Lq &amp;gt;= Rmin (that is, it will take at least 1 minute to clear the queue even if no more requests come in):&lt;/item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Set Cm = goal time for clearing out a non-empty queue (default 5 minutes)&lt;/item&gt;
          &lt;item&gt;Compute the processing rate (P) as Rmin / Bmin (units: requests / backend)&lt;/item&gt;
          &lt;item&gt;Set the clusterâs minimum size to ((Rmin + (Lq / Cm)) / P) backends (reasoning: if requests come in at the same rate, there will be (Rmin + (Lq / Cm)) requests to process over each of the next Cm minutes to clear out the queue)&lt;/item&gt;
        &lt;/list&gt;
        &lt;item&gt;Otherwise (if Lq &amp;lt; Rmin):&lt;/item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Set the clusterâs minimum size to 1, so GKEâs CPU usage-based autoscaling will take over&lt;/item&gt;
        &lt;/list&gt;
      &lt;/list&gt;
      &lt;p&gt; This algorithm reduced our scale-up time from 1 to 100 pods from 2 hours down to 10 minutes. &lt;/p&gt;
      &lt;p&gt; This architecture solved all of the reliability problems from the previous architectures. It has per-message routing with global queueing and no connection pinning, it has fast scale-up based on the routerâs metrics, the client library is implemented in pure Python with asyncio and doesnât have background threads, and the protocol doesnât disconnect for difficult-to-introspect reasons. When our system was working on the IMO problems this year, we didnât have to think about this system at all, except for moving traffic around between clusters when there were CPU availability issues in some GCP regions. The process of moving traffic from hundreds of thousands CPUs in one region to hundreds of thousands in another region, required only a simple configuration change on the clients, and 10-20 minutes for autoscaling to respond. &lt;/p&gt;
      &lt;p&gt; Weâre not done, however. Thereâs more we can do to improve this service. &lt;/p&gt;
      &lt;p&gt; REPL service v4 and beyond &lt;/p&gt;
      &lt;p&gt; We have a few ideas for making the service more efficient and functional in the future. &lt;/p&gt;
      &lt;p&gt; We could get some benefits from storing state data on disk on the router machine, instead of always sending it back to the clients. This would reduce data transfer costs when communicating with REPL clusters in a different GCP region, and would also reduce memory usage on the clients, since they would no longer need to have all the state data in memory. (For long-running processes, this can add up to a lot of memory usage!) This would make the router slower, but itâs already event-driven and multithreaded, so thatâs not a major concern - we could use a separate thread pool to offload the disk I/O from the network threads. This would allow the router to implement âloose pinningâ - it could preferentially send requests to backends that already have the necessary state data, if they arenât busy, which would eliminate some network transfer time, and the time needed to decompress and import state data on the backends. &lt;/p&gt;
      &lt;p&gt; We could also use non-GKE CPU capacity as part of our REPL clusters. Some GPU machines in our infrastructure have significant amounts of CPU capacity which isnât fully utilized; we could allow these machines to join one of the REPL clusters with some fraction of their CPUs allocated to REPL work. This would reduce our GKE costs, since weâve already paid for these CPUs anyway, and they wonât get preempted. &lt;/p&gt;
      &lt;p&gt; Conclusion &lt;/p&gt;
      &lt;p&gt; Weâve learned a lot from this project. &lt;/p&gt;
      &lt;p&gt; Thereâs always a tradeoff between building something in-house vs. using an open-source or vendored product (commonly dubbed âbuild vs. buyâ) to fill an engineering need. We find that companies usually underestimate the overhead of using an external project - it needs to be integrated into the existing system, and often doesnât fit perfectly. In retrospect, since the in-house solution wouldnât be worse than external systems in terms of maintenance burden and information siloing, we should have just gone for that as soon as we had the idea. &lt;/p&gt;
      &lt;p&gt; Similarly, we shouldnât be afraid to challenge common ways of doing things. Inverting the connection direction for the backends is a good example of this - the ânormalâ way to write a load balancer is as a reverse proxy, but we didnât need to implement any health checks, timeouts, or service discovery on the router in our system, which saved us a lot of time. There are good reasons why the ânormalâ way to do this is what it is, but those reasons didnât apply to our use case. &lt;/p&gt;
      &lt;p&gt; Finally, itâs fun and satisfying to make systems work at large scale. At our core, we enjoy solving engineering problems in simple, elegant, and consistent ways, so scaling up a service like this is very satisfying. For a while during the IMO, we had nearly 500,000 CPUs running Lean code at the same time with 95-100% utilization, which shows that the solution we built worked. &lt;/p&gt;
      &lt;p&gt; Thereâs much more to do at Harmonic on this project, and on others beyond it. If this kind of work sounds interesting to you, join us! &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://harmonic.fun/news#blog-post-lean"/><published>2026-01-13T21:50:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46608811</id><title>Is it a joke?</title><updated>2026-01-14T00:57:00.820431+00:00</updated><content>&lt;doc fingerprint="af87791238a28bd5"&gt;
  &lt;main&gt;
    &lt;p&gt;I listen to a lot of podcasts, because I can listen while making art for the diorama-based game I am working on. I discovered the podcast Imaginary Advice because MetaFilter linked to its episode on the SNES game A Christmas Carol. That’s a game that doesn’t exist – Ross Sutherland just made it up, and then recorded a whole podcast episode about it. The episode doesn’t acknowledge, at any point, that the game is fake. It’s hilarious.&lt;/p&gt;
    &lt;p&gt;Sometimes I can’t tell whether something is a joke or not. I couldn’t tell whether Semantle was a joke until I started getting fan mail. And speaking of podcasts, apparently I’m not the only one with this problem. I just heard on 99% Invisible that when Tez Okano pitched Sega on Segagaga, one of the last Dreamcast games, the execs thought his entire pitch was a joke, so he had to pitch it again.&lt;/p&gt;
    &lt;p&gt;I’m not as good a writer as Ross Sutherland. That’s why a lot of people didn’t see the humor in my post about the (fictional) 1989 Blue Prince, and thought I meant to actually fool people. Fooling people was really an accident – I just put in a little bit too much work on the visuals. Actually, I meant the commentary as a serious critique: Blue Prince has a very high busywork-to-puzzle ratio, and this is its weakest point (also see my previous note on gambling; Blue Prince literally has slot machines, and it’s sometimes optimal to use them). But there is a lot of puzzle there too – several people assumed that the floppy disk flipping bit referred to one specific room in the actual Blue Prince. Nope. I hadn’t gotten to that area at the time I wrote the post, and I still haven’t solved that puzzle. Instead, I was inspired by learning about Karateka’s upside-down disk Easter egg on Lateral. Yep, another podcast.&lt;/p&gt;
    &lt;p&gt;Sometimes I feel like I’m wasting my time listening to all these podcasts, but I was heartened to see that Adrian Tchaikovsky mentioned that his Philosopher Tyrants series was inspired by Empire and Revolutions. It’s among Tchaikovsky’s best work, and I can’t wait for the next one. And nobody can accuse Tchaikovsky of unproductivity. To bring it full-circle, science fiction also inspires podcasts; after ten seasons on actual historical revolutions, and after a two-year hiatus, the Revolutions podcast returned with a season on the Martian Revolution of 2247, presented totally straight-faced.&lt;/p&gt;
    &lt;p&gt;Two more notes that didn’t fit anywhere else:&lt;/p&gt;
    &lt;p&gt;I actually used cool-retro-term, not RetroArch, which is why the font isn’t quite right. I thought about using RetroArch but it looked like it was good to be a hassle to get an Apple II running, plus then I would have had to maybe write Applesoft BASIC. Anyway, I decided to settle for good enough. So that’s why the font isn’t quite historically accurate.&lt;/p&gt;
    &lt;p&gt;I really enjoyed reading Egypt Urnash’s IF-style Blue Prince scene. I did consider whether an IF Blue Prince would be better, but I think seeing the map is really useful, and while you could incorporate a map into an IF game (as Counterfeit Monkey does), it sort of strains the medium a bit.&lt;/p&gt;
    &lt;p&gt;Previous post: Blue Prince (1989)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://novalis.org/blog/2025-11-06-is-it-a-joke.html"/><published>2026-01-13T21:55:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46608840</id><title>We can't have nice things because of AI scrapers</title><updated>2026-01-14T00:56:59.825843+00:00</updated><content>&lt;doc fingerprint="5e8f767c1c2747ac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Making sure you're not a bot!&lt;/head&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
    &lt;p&gt;You are seeing this because the administrator of this website has set up Anubis to protect the server against the scourge of AI companies aggressively scraping websites. This can and does cause downtime for the websites, which makes their resources inaccessible for everyone.&lt;/p&gt;
    &lt;p&gt;Anubis is a compromise. Anubis uses a Proof-of-Work scheme in the vein of Hashcash, a proposed proof-of-work scheme for reducing email spam. The idea is that at individual scales the additional load is ignorable, but at mass scraper levels it adds up and makes scraping much more expensive.&lt;/p&gt;
    &lt;p&gt;Ultimately, this is a placeholder solution so that more time can be spent on fingerprinting and identifying headless browsers (EG: via how they do font rendering) so that the challenge proof of work page doesn't need to be presented to users that are much more likely to be legitimate.&lt;/p&gt;
    &lt;p&gt;Please note that Anubis requires the use of modern JavaScript features that plugins like JShelter will disable. Please disable JShelter or other such plugins for this domain.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.metabrainz.org/2025/12/11/we-cant-have-nice-things-because-of-ai-scrapers/"/><published>2026-01-13T21:57:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46608971</id><title>Terra - A rolling-release Fedora repository</title><updated>2026-01-14T00:56:59.439700+00:00</updated><content>&lt;doc fingerprint="8790746d3041efab"&gt;
  &lt;main&gt;
    &lt;p&gt;Terra is built on the Andaman toolchain, our modern, Rust-based meta buildsystem for packages. This simplifies the process of maintenance at scale.&lt;/p&gt;
    &lt;p&gt;Current&lt;/p&gt;
    &lt;p&gt;Tired of waiting for updates? Us too. Packages are automatically updated as soon as released upstream. And, for those living on the edge, we offer nightly packages as well.&lt;/p&gt;
    &lt;p&gt;Quality&lt;/p&gt;
    &lt;p&gt;Package submissions are carefully vetted and assessed against our guidelines. This ensures that a high quality of packages is maintained.&lt;/p&gt;
    &lt;p&gt;Transparent&lt;/p&gt;
    &lt;p&gt;Goodbye to external buildsystems and scattered repos! Package sources are available in a monorepo, and build jobs are publicly viewable on GitHub Actions. This makes it easy to debug issues.&lt;/p&gt;
    &lt;p&gt;Ease of Use&lt;/p&gt;
    &lt;p&gt;Terra is as easy to use as any other repository. For developers, Terra is a breeze to work with. We work directly with other projects to meet their needs and make Terra as seamless as possible.&lt;/p&gt;
    &lt;p&gt;Secure&lt;/p&gt;
    &lt;p&gt;Packages are built in a secure environment maintained by the Fyra Labs team. Through our modern tooling, transparent infrastructure, and careful review process, we can prevent issues from occurring.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://terra.fyralabs.com/"/><published>2026-01-13T22:07:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46609279</id><title>Why we don’t use AI</title><updated>2026-01-14T00:56:58.453243+00:00</updated><content>&lt;doc fingerprint="3d2113550ae70c94"&gt;
  &lt;main&gt;
    &lt;p&gt;We get asked about AI a lot. Whether we’re going to add it to Yarn Spinner, whether we use it ourselves, what we think about it. Fair questions. Time to write it all down.&lt;/p&gt;
    &lt;p&gt;Yarn Spinner doesn’t use the technology that’s currently being called AI. We don’t have generative AI features in the product, and we don’t use code generation tools to build it, and we don’t accept contributions we know contain generated material. Let’s talk about why.&lt;/p&gt;
    &lt;p&gt;TL;DR: AI companies make tools for hurting people and we don’t want to support that.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Past&lt;/head&gt;
    &lt;p&gt;A little history first. We come from a background that did a decent amount of work with AI and ML (terms we shouldn’t but will use interchangeably because everyone else does).&lt;/p&gt;
    &lt;p&gt;We gave talks about it for game developers and non-programmers. We wrote little ML bots for games. We did research and academic work. We wrote books1 about using ML in games, mostly for procedural animation. It was a fun series of techniques to explore, and explore we did.&lt;/p&gt;
    &lt;p&gt;When we started at university, neural networks and deep learning (the main underlying techniques most AI products use today) were just too slow and hard to work with. By the time we finished our doctorates, that had changed. Tools like TensorFlow made this stuff easier and fun, and the increase in GPU access made training and inference possible for people without Big Tech budgets. For quite a while, we were genuinely excited about the potential.&lt;/p&gt;
    &lt;p&gt;Then things started to change.&lt;/p&gt;
    &lt;p&gt;It’s hard to say exactly when. Maybe it was always like this and we just didn’t see it. But by the end of 2020 (a year famous for absolutely nothing world changing whatsoever happening /s) it was clear that the AI we liked was not what the tech companies were interested in. They were increasingly about generative imagery, chatbots writing your material for you, and summaries of art instead of exposure to it. Efforts to mitigate known problems (reinforcing cultural biases, being difficult to make deterministic or explainable) were disparaged and diminished. Researchers and developers who raised concerns were being fired.&lt;/p&gt;
    &lt;p&gt;Things have only gotten worse since.&lt;/p&gt;
    &lt;p&gt;If you look at what AI companies promote now, it’s not what we wanted. When you boil down everything they say and strip it right back, what they make are tools to either fire people or demand more work without hiring anyone new to help. That’s the problem AI companies want to solve.&lt;/p&gt;
    &lt;p&gt;Anything else they achieve is a happy accident on the road to firing as many of your friends and colleagues as possible.&lt;/p&gt;
    &lt;p&gt;AI is now a tool for firing people, in a time when getting re-employed is especially difficult and being unemployed can be life-threatening. We don’t want to be part of that. Until this is fixed we won’t use AI in our work, nor integrate it into Yarn Spinner for others to use.&lt;/p&gt;
    &lt;p&gt;We don’t want to support the companies making these tools or normalise their behaviour. So we don’t.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Future&lt;/head&gt;
    &lt;p&gt;There’s a comment we see every so often, always phrased as a fait accompli: “you’ll be left behind if you don’t adopt AI”, or its cousin, “everyone is using it”. We disagree.&lt;/p&gt;
    &lt;p&gt;This isn’t the right approach regardless of our opinions on AI. It’s tool driven development. The goal should never be “we use this tool”. It should be “how do we help you make better games?”.&lt;/p&gt;
    &lt;p&gt;Great games are made when people are passionate about an idea and push it into existence. Often this means reduction, not addition. Changing ideas. Keeping yourself and colleagues healthy. Being willing to adapt and take feedback. Good tools need to do the same.&lt;/p&gt;
    &lt;p&gt;We’re constantly asking “how would this help make better games?” and following where that leads. The exploration matters, and most of the time we find an idea doesn’t survive even a little scrutiny. We’d rather have fewer polished features that solve real problems than a load of garbage that exists for the sake of marketing copy.&lt;/p&gt;
    &lt;p&gt;We’re proud of Yarn Spinner. We don’t think it’s a coincidence it’s used in so many games. Our process works, and we’re always adding new features. We also change and remove features if they don’t meet the needs of devs. We’re always chatting, internally and with other game devs and even non-devs, about potential ideas and approaches. We’re going to keep asking “how would this help make better games?” and ship what survives that gauntlet.&lt;/p&gt;
    &lt;p&gt;Who knows. Maybe the world will change and we can take another look at ML.&lt;/p&gt;
    &lt;head rend="h2"&gt;Likely to be Frequently Asked Questions&lt;/head&gt;
    &lt;p&gt;Why do you only care about people getting fired? I read that AI is also bad for SOME OTHER REASON! There are so many issues AI (and in particular the companies making it) have. Some are potential or even hypothetical concerns that might eventuate. Some are very real and happening right now in front of our eyes. Some are much worse than people being fired. Some of these worse issues appeared between us starting this blog post and publishing it. If the labour concerns around AI suddenly vanished, there are still many issues we’d need to see fixed before we’d be comfortable using it. But it’s stronger to argue a single point at a time. The labour concerns can be fixed and should be pushed back on. Once that’s sorted we can look at the next issue.&lt;/p&gt;
    &lt;p&gt;Why don’t you do ML correctly so no one gets hurt? Given our background and experience, we probably could make our own AI tooling in a way we feel is helpful, ethical, and doesn’t fund companies we disagree with. Two problems with this. First, these things take a lot of time to make, and like we said, most ideas don’t survive initial exploration. It would be very difficult to balance exploring an idea while also building new models to test it. Second, while we could make our own tools, most people can’t. If they saw us using a technique and wanted to try it, they’d end up supporting the very companies we object to. We don’t want to normalise it, so we have to not use it.&lt;/p&gt;
    &lt;p&gt;My boss wants me to use AI for work, am I part of the problem? Getting and keeping a job is a necessity, and this has only gotten worse recently. If you can push back on this, do. But no one will fault you for wanting to keep your job.&lt;/p&gt;
    &lt;p&gt;Are you going to ban people who use AI from using Yarn Spinner? No. While we wish you didn’t use it, we get this is our line in the sand, not yours. We’ll still advocate against these tools and we’re still concerned about the harm they do. You need to realise that if you use them, you’re both financially and socially supporting dodgy companies doing dodgy things. They will use your support to push their agenda. If these tools are working for you, we’re genuinely pleased. But please also stop using them.&lt;/p&gt;
    &lt;p&gt;I kinda like using AI and no one at my work is getting fired? This comment pops up a few times, often from programmers. Unfortunately, because of how messy the term AI now is, the same concerns still apply. Your adoption helps promote the companies making these tools. People see you using it and force it onto others at the studio, or at other workplaces entirely. From what we’ve seen, this is followed by people getting fired and overworked. If it isn’t happening to you and your colleagues, great. But you’re still helping it happen elsewhere. And as we said, even if you fixed the labour concerns tomorrow, there are still many other issues. There’s more than just being fired to worry about.&lt;/p&gt;
    &lt;p&gt;Are you zealots or luddites who just hate AI? Nah. Just upset at the people making these things. There’s great potential in AI and machine learning, and it’s being squandered to make already dodgy rich people richer and more dodgy. We still keep up with developments because we hope one day we can explore it again. But for now, the people pushing these tools aren’t people we want to give money or support to.&lt;/p&gt;
    &lt;p&gt;Header image: WGA Strike, June 21, 2023 via Wikimedia Commons&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;For example, here’s Paris’ site with a list of all the books we wrote. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://yarnspinner.dev/blog/why-we-dont-use-ai/"/><published>2026-01-13T22:30:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46609389</id><title>Show HN: AsciiSketch a free browser-based ASCII art and diagram editor</title><updated>2026-01-14T00:56:58.168040+00:00</updated><content>&lt;doc fingerprint="e7fd9d8f7a9d7c56"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Canvas&lt;/head&gt;
    &lt;head rend="h3"&gt;Element&lt;/head&gt;
    &lt;head rend="h3"&gt;Text&lt;/head&gt;
    &lt;p&gt; Top &lt;/p&gt;
    &lt;p&gt; Bottom &lt;/p&gt;
    &lt;p&gt; Left &lt;/p&gt;
    &lt;p&gt; Right &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://files.littlebird.com.au/ascii-sketch.html"/><published>2026-01-13T22:39:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46609492</id><title>EOL hardware should mean open-source software</title><updated>2026-01-14T00:56:57.492132+00:00</updated><content>&lt;doc fingerprint="cd5fb45afb0fcded"&gt;
  &lt;main&gt;
    &lt;p&gt;January 13, 2026 · 2 min read&lt;/p&gt;
    &lt;p&gt;When hardware products reach end-of-life (EOL), companies should be forced to open-source the software.&lt;/p&gt;
    &lt;p&gt;I think we've made strides in this area with the "Right to Repair"-movement, but let's go one step further. Preferably with the power of the European Commission: enforce that when something goes end-of-life, companies need to open-source the software.&lt;/p&gt;
    &lt;p&gt;I have a "smart" weight scale. It still connects via Bluetooth just fine (meaning: I see it connect on my phone) but because the app is no longer in development, it's essentially useless. A perfect piece of hardware, "dead" because the company behind it stopped supporting it. (I'm exaggerating a bit; it shows the weight on its display, but the app used to store data for up to 5 users to keep track over time. I miss that!) It's infuriating that we allow this to happen with all the wasteful electronics already lying around. We deserve better.&lt;/p&gt;
    &lt;p&gt;I thought of this while reading this article. It's great that Bose does this, but it's rare. When Spotify killed off its $200 Car Thing at the end of 2024, we just accepted it and moved on, even though that's $200 of hardware turned into e-waste overnight. Out of sustainability concerns, but also just out of doing what's right: this should not be able to happen.&lt;/p&gt;
    &lt;p&gt;Now, I'm not asking companies to open-source their entire codebase. That's unrealistic when an app is tied to a larger platform. What I am asking for: publish a basic GitHub repo with the hardware specs and connection protocols. Let the community build their own apps on top of it.&lt;/p&gt;
    &lt;p&gt;And here's the thing: with vibe-coding making development more accessible than ever, this isn't just for hardcore developers anymore. Regular users can actually tinker with this stuff now.&lt;/p&gt;
    &lt;p&gt;The worst you can do is break the software. But the hardware was bricked already anyway :-)&lt;/p&gt;
    &lt;p&gt;Starting in 2026, I'll share more focused notes on product design, technology, and business. If you'd like them in your inbox, leave your email below. I'm always happy to connect via email, Bluesky, or LinkedIn (blergh).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.marcia.no/words/eol"/><published>2026-01-13T22:49:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46609591</id><title>The insecure evangelism of LLM maximalists</title><updated>2026-01-14T00:56:57.131487+00:00</updated><content>&lt;doc fingerprint="24e0893516d79158"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Insecure Evangelism of LLM Maximalists&lt;/head&gt;
    &lt;p&gt;I am an LLM productivity skeptic.&lt;/p&gt;
    &lt;p&gt;I find LLMs useful as a sort of digital clerk - searching the web for me, finding documentation, looking up algorithms. I even find them useful1 in a limited coding capacity; with a small context and clear guidelines.&lt;/p&gt;
    &lt;p&gt;But doing "prompt-driven development" or "vibe coding" with an Agentic LLM was an incredibly disapointing experience for me. It required an immense amount of baby sitting, for small code changes, made slowly, which were often wrong. All the while I sat there feeling dumber and dumber, as my tokens drained away.&lt;/p&gt;
    &lt;p&gt;Of course that was my experience, and my preference. I genuinely don't mind if other people vibe code. Go for it! I do not deny this kind of coding is enabling a lot of people - who aren't experienced devs - to create things they would never otherwise be able to create. (Also, sometimes they pay me to clean them up afterwards, which is nice.)&lt;/p&gt;
    &lt;p&gt;But that is not enough for the vocal proponents. It's the future! You'll be left behind! Software has changed forever! And then, inevitably, comes the character evaluation, which goes something like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You - a senior dev - are resisting this change due to deeply held psychological fears of being made irrelevant and/or having to learn new things. You are stuck in your ways and unwilling to change them because you are afraid.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This has always baffled me, because quite frankly I like the idea of agentic coding. I often feel the actual implementation is a bottleneck to the things I want to create. I would love it if I could just sit around making specs (yes I am a programmer who enjoys this) and have little machines implement it for me perfectly. It's a wonderful fantasy world, and I wish I could inhabit it. That's why I was so disappointed.&lt;/p&gt;
    &lt;p&gt;And it made me think - why are these people so insistent, and hostile? Why can't they live and let live? Why do they need to convince the rest of us? And to be honest, I am developing my own character evaluation. It's not very charitable, but it is making a lot of sense to me:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You tried agentic coding. You realised it was better at programming than you are. You see a lot of accomplished, prominent developers2 claiming they are more productive without it. Could they just be that much better at programming than I am? No! They are just threatened. They are the ones who are insecure! I'm a great developer!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It's projection. Their evangelism is born of insecurity.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Open Question&lt;/head&gt;
    &lt;p&gt;I am still willing to admit I am wrong. That I'm not holding the agents properly. That doing this is it's own skill and I have not spent enough time with it. I have changed my mind on tech before, and I'm sure I will do so again.&lt;/p&gt;
    &lt;p&gt;LLM evangelists - are you willing to admit that you just might not be that good at programming computers? Maybe you once were. Maybe you never were.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Just to forestall this; I am not claiming I am one of them. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I'm available for hire.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lewiscampbell.tech/blog/260114.html"/><published>2026-01-13T22:57:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46609630</id><title>A 40-line fix eliminated a 400x performance gap</title><updated>2026-01-14T00:56:56.788777+00:00</updated><content>&lt;doc fingerprint="5e3d50bbbe611f0e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How a 40-Line Fix Eliminated a 400x Performance Gap&lt;/head&gt;
    &lt;p&gt;I have a habit of skimming the OpenJDK commit log every few weeks. Many commits are too complex for me to grasp in the limited time I have reserved for this ... special hobby. But occasionally something catches my eye.&lt;/p&gt;
    &lt;p&gt;Last week, this commit stopped me mid-scroll:&lt;/p&gt;
    &lt;quote&gt;858d2e434dd 8372584: [Linux]: Replace reading proc to get thread CPUtime with clock_gettime&lt;/quote&gt;
    &lt;p&gt;The diffstat was interesting: &lt;code&gt;+96 insertions, -54 deletions&lt;/code&gt;. The changeset adds a 55-line JMH benchmark, which means the production code itself is actually reduced.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Deleted Code&lt;/head&gt;
    &lt;p&gt;Here's what got removed from &lt;code&gt;os_linux.cpp&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;static jlong user_thread_cpu_time(Thread *thread) {pid_t tid = thread-&amp;gt;osthread()-&amp;gt;thread_id();char *s;char stat[2048];size_t statlen;char proc_name[64];int count;long sys_time, user_time;char cdummy;int idummy;long ldummy;FILE *fp;os::snprintf_checked(proc_name, 64, "/proc/self/task/%d/stat", tid);fp = os::fopen(proc_name, "r");if (fp == nullptr) return -1;statlen = fread(stat, 1, 2047, fp);stat[statlen] = '\0';fclose(fp);// Skip pid and the command string. Note that we could be dealing with// weird command names, e.g. user could decide to rename java launcher// to "java 1.4.2 :)", then the stat file would look like// 1234 (java 1.4.2 :)) R ... ...// We don't really need to know the command string, just find the last// occurrence of ")" and then start parsing from there. See bug 4726580.s = strrchr(stat, ')');if (s == nullptr) return -1;// Skip blank charsdo { s++; } while (s &amp;amp;&amp;amp; isspace((unsigned char) *s));count = sscanf(s,"%c %d %d %d %d %d %lu %lu %lu %lu %lu %lu %lu",&amp;amp;cdummy, &amp;amp;idummy, &amp;amp;idummy, &amp;amp;idummy, &amp;amp;idummy, &amp;amp;idummy,&amp;amp;ldummy, &amp;amp;ldummy, &amp;amp;ldummy, &amp;amp;ldummy, &amp;amp;ldummy,&amp;amp;user_time, &amp;amp;sys_time);if (count != 13) return -1;return (jlong)user_time * (1000000000 / os::Posix::clock_tics_per_second());}&lt;/quote&gt;
    &lt;p&gt;This was the implementation behind &lt;code&gt;ThreadMXBean.getCurrentThreadUserTime()&lt;/code&gt;. To get the current thread's user CPU time, the old code was:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Formatting a path to &lt;code&gt;/proc/self/task/&amp;lt;tid&amp;gt;/stat&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Opening that file&lt;/item&gt;
      &lt;item&gt;Reading into a stack buffer&lt;/item&gt;
      &lt;item&gt;Parsing through a hostile format where the command name can contain parentheses (hence the &lt;code&gt;strrchr&lt;/code&gt;for the last&lt;code&gt;)&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Running &lt;code&gt;sscanf&lt;/code&gt;to extract fields 13 and 14&lt;/item&gt;
      &lt;item&gt;Converting clock ticks to nanoseconds&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For comparison, here's what &lt;code&gt;getCurrentThreadCpuTime()&lt;/code&gt; does and has always done:&lt;/p&gt;
    &lt;quote&gt;jlong os::current_thread_cpu_time() {return os::Linux::thread_cpu_time(CLOCK_THREAD_CPUTIME_ID);}jlong os::Linux::thread_cpu_time(clockid_t clockid) {struct timespec tp;clock_gettime(clockid, &amp;amp;tp);return (jlong)(tp.tv_sec * NANOSECS_PER_SEC + tp.tv_nsec);}&lt;/quote&gt;
    &lt;p&gt;Just a single &lt;code&gt;clock_gettime()&lt;/code&gt; call. There is no file I/O, no complex parsing and no buffer to manage.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Performance Gap&lt;/head&gt;
    &lt;p&gt;The original bug report, filed back in 2018, quantified the difference:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"getCurrentThreadUserTime is 30x-400x slower than getCurrentThreadCpuTime"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The gap widens under concurrency. Why is &lt;code&gt;clock_gettime()&lt;/code&gt; so much faster? Both approaches require kernel entry, but the difference is in what happens next.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;/proc&lt;/code&gt; path:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;open()&lt;/code&gt;syscall&lt;/item&gt;
      &lt;item&gt;VFS dispatch + dentry lookup&lt;/item&gt;
      &lt;item&gt;procfs synthesizes file content at read time&lt;/item&gt;
      &lt;item&gt;kernel formats string into buffer&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;read()&lt;/code&gt;syscall, copy to userspace&lt;/item&gt;
      &lt;item&gt;userspace &lt;code&gt;sscanf()&lt;/code&gt;parsing&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;close()&lt;/code&gt;syscall&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;clock_gettime(CLOCK_THREAD_CPUTIME_ID)&lt;/code&gt; path:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;single syscall → &lt;code&gt;posix_cpu_clock_get()&lt;/code&gt;→&lt;code&gt;cpu_clock_sample()&lt;/code&gt;→&lt;code&gt;task_sched_runtime()&lt;/code&gt;→ reads directly from&lt;code&gt;sched_entity&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;/proc&lt;/code&gt; path involves multiple syscalls, VFS machinery, string formatting kernel-side, and parsing userspace-side. The &lt;code&gt;clock_gettime()&lt;/code&gt; path is one syscall with a direct function call chain.&lt;/p&gt;
    &lt;p&gt;Under concurrent load, the &lt;code&gt;/proc&lt;/code&gt; approach also suffers from kernel lock contention. The bug report notes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Reading proc is slow (hence why this procedure is put under the method slow_thread_cpu_time(...)) and may lead to noticeable spikes in case of contention for kernel resources."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Why Two Implementations?&lt;/head&gt;
    &lt;p&gt;So why didn't &lt;code&gt;getCurrentThreadUserTime()&lt;/code&gt; just use &lt;code&gt;clock_gettime()&lt;/code&gt; from the start?&lt;/p&gt;
    &lt;p&gt;The answer is (probably) POSIX. The standard mandates that &lt;code&gt;CLOCK_THREAD_CPUTIME_ID&lt;/code&gt; returns total CPU time (user + system). There's no portable way to request user time only. Hence the &lt;code&gt;/proc&lt;/code&gt;-based implementation.&lt;/p&gt;
    &lt;p&gt;The Linux port of OpenJDK isn't limited to what POSIX defines, it can use Linux-specific features. Let's see how.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Clockid Bit Hack&lt;/head&gt;
    &lt;p&gt;Linux kernels since 2.6.12 (released in 2005) encode clock type information directly into the &lt;code&gt;clockid_t&lt;/code&gt; value. When you call &lt;code&gt;pthread_getcpuclockid()&lt;/code&gt;, you get back a clockid with a specific bit pattern:&lt;/p&gt;
    &lt;quote&gt;Bit 2: Thread vs process clockBits 1-0: Clock type00 = PROF01 = VIRT (user time only)10 = SCHED (user + system, POSIX-compliant)11 = FD&lt;/quote&gt;
    &lt;p&gt;The remaining bits encode the target PID/TID. We’ll come back to that in the bonus section.&lt;/p&gt;
    &lt;p&gt;The POSIX-compliant &lt;code&gt;pthread_getcpuclockid()&lt;/code&gt; returns a clockid with bits &lt;code&gt;10&lt;/code&gt; (SCHED). But if you flip those low bits to &lt;code&gt;01&lt;/code&gt; (VIRT), &lt;code&gt;clock_gettime()&lt;/code&gt; will return user time only.&lt;/p&gt;
    &lt;p&gt;The new implementation:&lt;/p&gt;
    &lt;quote&gt;static bool get_thread_clockid(Thread* thread, clockid_t* clockid, bool total) {constexpr clockid_t CLOCK_TYPE_MASK = 3;constexpr clockid_t CPUCLOCK_VIRT = 1;int rc = pthread_getcpuclockid(thread-&amp;gt;osthread()-&amp;gt;pthread_id(), clockid);if (rc != 0) {// Thread may have terminatedassert_status(rc == ESRCH, rc, "pthread_getcpuclockid failed");return false;}if (!total) {// Flip to CPUCLOCK_VIRT for user-time-only*clockid = (*clockid &amp;amp; ~CLOCK_TYPE_MASK) | CPUCLOCK_VIRT;}return true;}static jlong user_thread_cpu_time(Thread *thread) {clockid_t clockid;bool success = get_thread_clockid(thread, &amp;amp;clockid, false);return success ? os::Linux::thread_cpu_time(clockid) : -1;}&lt;/quote&gt;
    &lt;p&gt;And that's it. The new version has no file I/O, no buffer and certainly no &lt;code&gt;sscanf()&lt;/code&gt; with thirteen format specifiers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Profiling time!&lt;/head&gt;
    &lt;p&gt;Let's have a look at how it performs in practice. For this exercise, I am taking the JMH test included in the fix, the only change is that I increased the number of threads from 1 to 16 and added a &lt;code&gt;main()&lt;/code&gt; method for simple execution from an IDE:&lt;/p&gt;
    &lt;quote&gt;@State(Scope.Benchmark)@Warmup(iterations = 2, time = 5)@Measurement(iterations = 5, time = 5)@BenchmarkMode(Mode.SampleTime)@OutputTimeUnit(TimeUnit.MICROSECONDS)@Threads(16)@Fork(value = 1)public class ThreadMXBeanBench {static final ThreadMXBean mxThreadBean = ManagementFactory.getThreadMXBean();static long user; // To avoid dead-code elimination@Benchmarkpublic void getCurrentThreadUserTime() throws Throwable {user = mxThreadBean.getCurrentThreadUserTime();}public static void main(String[] args) throws RunnerException {Options opt = new OptionsBuilder().include(ThreadMXBeanBench.class.getSimpleName()).build();new Runner(opt).run();}}&lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Aside: This is a rather unscientific benchmark, I have other processes running on my desktop etc. Anyway, here is the setup: Ryzen 9950X, JDK main branch at commit 8ab7d3b89f656e5c. For the "before" case, I reverted the fix rather than checking out an older revision.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here is the result:&lt;/p&gt;
    &lt;quote&gt;Benchmark Mode Cnt Score Error UnitsThreadMXBeanBench.getCurrentThreadUserTime sample 8912714 11.186 ± 0.006 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.00 sample 2.000 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.50 sample 10.272 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.90 sample 17.984 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.95 sample 20.832 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.99 sample 27.552 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.999 sample 56.768 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.9999 sample 79.709 us/opThreadMXBeanBench.getCurrentThreadUserTime:p1.00 sample 1179.648 us/op&lt;/quote&gt;
    &lt;p&gt;We can see that a single invocation took 11 microseconds on average and the median was about 10 microseconds per invocation.&lt;/p&gt;
    &lt;p&gt;The CPU profile looks like this:&lt;/p&gt;
    &lt;p&gt;The CPU profile confirms that each invocation of &lt;code&gt;getCurrentThreadUserTime()&lt;/code&gt; does multiple syscalls. In fact, most of the CPU time
is spent in syscalls. We can see files being opened and closed. Closing alone results in multiple syscalls, including futex locks.&lt;/p&gt;
    &lt;p&gt;Let's see the benchmark result with the fix applied:&lt;/p&gt;
    &lt;quote&gt;Benchmark Mode Cnt Score Error UnitsThreadMXBeanBench.getCurrentThreadUserTime sample 11037102 0.279 ± 0.001 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.00 sample 0.070 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.50 sample 0.310 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.90 sample 0.440 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.95 sample 0.530 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.99 sample 0.610 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.999 sample 1.030 us/opThreadMXBeanBench.getCurrentThreadUserTime:p0.9999 sample 3.088 us/opThreadMXBeanBench.getCurrentThreadUserTime:p1.00 sample 1230.848 us/op&lt;/quote&gt;
    &lt;p&gt;The average went down from 11 microseconds to 279 nanos. This means the latency of the fixed version is 40x lower than the old version. While this is not a 400x improvement, it's within the 30x - 400x range from the original report. Chances are the delta would be higher with a different setup. Let's have a look at the new profile:&lt;/p&gt;
    &lt;p&gt;The profile is much cleaner. There is just a single syscall. If the profile is to be trusted then most of the time is spent in JVM, outside of the kernel.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Documented Is This?&lt;/head&gt;
    &lt;p&gt;Barely. The bit encoding is stable. It hasn't changed in 20 years, but you won't find it in the &lt;code&gt;clock_gettime(2)&lt;/code&gt; man page.
The closest thing to official documentation is the kernel source itself, in &lt;code&gt;kernel/time/posix-cpu-timers.c&lt;/code&gt; and the &lt;code&gt;CPUCLOCK_*&lt;/code&gt; macros.&lt;/p&gt;
    &lt;p&gt;The kernel's policy is clear: don't break userspace.&lt;/p&gt;
    &lt;p&gt;My take: If glibc depends on it, it's not going away.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pushing Further&lt;/head&gt;
    &lt;p&gt;When looking at profiler data from the 'after' run, I spotted a further optimization opportunity: A good portion of the remaining syscall is spent inside a radix tree lookup. Have a look:&lt;/p&gt;
    &lt;p&gt;When the JVM calls &lt;code&gt;pthread_getcpuclockid()&lt;/code&gt;, it receives a &lt;code&gt;clockid&lt;/code&gt; that encodes the thread's ID. When this &lt;code&gt;clockid&lt;/code&gt; is passed to &lt;code&gt;clock_gettime()&lt;/code&gt;,
the kernel extracts the thread ID and performs a radix tree lookup to find the &lt;code&gt;pid&lt;/code&gt; structure associated with that ID.&lt;/p&gt;
    &lt;p&gt;However, the Linux kernel has a fast-path. If the encoded PID in the &lt;code&gt;clockid&lt;/code&gt; is 0, the kernel interprets this as "the current thread" and skips the radix tree lookup entirely, jumping to the current task's structure directly.&lt;/p&gt;
    &lt;p&gt;The OpenJDK fix currently obtains the specific TID, flips the bits, and passes it to &lt;code&gt;clock_gettime()&lt;/code&gt;. This forces the kernel to take the "generalized path" (the radix tree lookup).&lt;/p&gt;
    &lt;p&gt;The source code looks like this:&lt;/p&gt;
    &lt;quote&gt;/** Functions for validating access to tasks.*/static struct pid *pid_for_clock(const clockid_t clock, bool gettime){[...]/** If the encoded PID is 0, then the timer is targeted at current* or the process to which current belongs.*/if (upid == 0)// the fast path: current task lookup, cheapreturn thread ? task_pid(current) : task_tgid(current);// the generalized path: radix tree lookup, more expensivepid = find_vpid(upid);[...]&lt;/quote&gt;
    &lt;p&gt;If the JVM constructed the entire &lt;code&gt;clockid&lt;/code&gt; manually with PID=0 encoded (rather than obtaining the &lt;code&gt;clockid&lt;/code&gt; via &lt;code&gt;pthread_getcpuclockid()&lt;/code&gt;), the kernel could take the fast-path and avoid the radix tree lookup altogether.
The JVM already pokes bits in the &lt;code&gt;clockid&lt;/code&gt;, so constructing it entirely from scratch wouldn't be a bigger leap compatibility-wise.&lt;/p&gt;
    &lt;p&gt;Let's try it!&lt;/p&gt;
    &lt;p&gt;First, a refresher on the &lt;code&gt;clockid&lt;/code&gt; encoding. The &lt;code&gt;clockid&lt;/code&gt; is constructed like this:&lt;/p&gt;
    &lt;quote&gt;clockid for TID=42, user-time-only:1111_1111_1111_1111_1111_1110_1010_1101└───────────────~42────────────────┘│└┘│ └─ 01 = VIRT (user time only)└─── 1 = per-thread&lt;/quote&gt;
    &lt;p&gt;For the current thread, we want PID=0 encoded, which gives &lt;code&gt;~0&lt;/code&gt; in the upper bits:&lt;/p&gt;
    &lt;quote&gt;1111_1111_1111_1111_1111_1111_1111_1101└─────────────── ~0 ───────────────┘│└┘│ └─ 01 = VIRT (user time only)└─── 1 = per-thread&lt;/quote&gt;
    &lt;p&gt;We can translate this into C++ as follows:&lt;/p&gt;
    &lt;quote&gt;// Linux Kernel internal bit encoding for dynamic CPU clocks:// [31:3] : Bitwise NOT of the PID or TID (~0 for current thread)// [2] : 1 = Per-thread clock, 0 = Per-process clock// [1:0] : Clock type (0 = PROF, 1 = VIRT/User-only, 2 = SCHED)static_assert(sizeof(clockid_t) == 4, "Linux clockid_t must be 32-bit");constexpr clockid_t CLOCK_CURRENT_THREAD_USERTIME = static_cast&amp;lt;clockid_t&amp;gt;(~0u &amp;lt;&amp;lt; 3 | 4 | 1);&lt;/quote&gt;
    &lt;p&gt;And then make a tiny teensy change to &lt;code&gt;user_thread_cpu_time()&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;jlong os::current_thread_cpu_time(bool user_sys_cpu_time) {if (user_sys_cpu_time) {return os::Linux::thread_cpu_time(CLOCK_THREAD_CPUTIME_ID);} else {- return user_thread_cpu_time(Thread::current());+ return os::Linux::thread_cpu_time(CLOCK_CURRENT_THREAD_USERTIME);}&lt;/quote&gt;
    &lt;p&gt;The change above is sufficient to make &lt;code&gt;getCurrentThreadUserTime()&lt;/code&gt; use the fast-path in the kernel.&lt;/p&gt;
    &lt;p&gt;Given that we are in nanoseconds territory already, we tweak the test a bit:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Increase the iteration and fork count&lt;/item&gt;
      &lt;item&gt;Use just a single thread to minimize noise&lt;/item&gt;
      &lt;item&gt;Switch to nanos&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The benchmark changes are meant to eliminate noise from the rest of my system and get a more precise measurement of the small delta we expect:&lt;/p&gt;
    &lt;quote&gt;@State(Scope.Benchmark)@Warmup(iterations = 4, time = 5)@Measurement(iterations = 10, time = 5)@BenchmarkMode(Mode.SampleTime)@OutputTimeUnit(TimeUnit.NANOSECONDS)@Threads(1)@Fork(value = 3)public class ThreadMXBeanBench {static final ThreadMXBean mxThreadBean = ManagementFactory.getThreadMXBean();static long user; // To avoid dead-code elimination@Benchmarkpublic void getCurrentThreadUserTime() throws Throwable {user = mxThreadBean.getCurrentThreadUserTime();}public static void main(String[] args) throws RunnerException {Options opt = new OptionsBuilder().include(ThreadMXBeanBench.class.getSimpleName()).build();new Runner(opt).run();}}&lt;/quote&gt;
    &lt;p&gt;The version currently in JDK main branch gives:&lt;/p&gt;
    &lt;quote&gt;Benchmark Mode Cnt Score Error UnitsThreadMXBeanBench.getCurrentThreadUserTime sample 4347067 81.746 ± 0.510 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.00 sample 69.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.50 sample 80.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.90 sample 90.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.95 sample 90.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.99 sample 90.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.999 sample 230.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.9999 sample 1980.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p1.00 sample 653312.000 ns/op&lt;/quote&gt;
    &lt;p&gt;With the manual &lt;code&gt;clockid&lt;/code&gt; construction, which uses the kernel fast-path, we get:&lt;/p&gt;
    &lt;quote&gt;Benchmark Mode Cnt Score Error UnitsThreadMXBeanBench.getCurrentThreadUserTime sample 5081223 70.813 ± 0.325 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.00 sample 59.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.50 sample 70.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.90 sample 70.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.95 sample 70.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.99 sample 80.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.999 sample 170.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p0.9999 sample 1830.000 ns/opThreadMXBeanBench.getCurrentThreadUserTime:p1.00 sample 425472.000 ns/op&lt;/quote&gt;
    &lt;p&gt;The average went down from 81.7 ns to 70.8 ns, so about a 13% improvement. The improvements are visible across all percentiles as well. Is it worth the loss of clarity from constructing the &lt;code&gt;clockid&lt;/code&gt; manually rather than using &lt;code&gt;pthread_getcpuclockid()&lt;/code&gt;?
I am not entirely sure. The absolute gain is small and makes additional assumptions about kernel internals, including the size of &lt;code&gt;clockid_t&lt;/code&gt;. On the other hand, it's still a gain without any downside in practice. (famous last words...)&lt;/p&gt;
    &lt;head rend="h2"&gt;Browsing for Gems&lt;/head&gt;
    &lt;p&gt;This is why I like browsing commits of large open source projects. A 40-line deletion eliminated a 400x performance gap. The fix required no new kernel features, just knowledge of a stable-but-obscure Linux ABI detail.&lt;/p&gt;
    &lt;p&gt;The lessons:&lt;/p&gt;
    &lt;p&gt;Read the kernel source. POSIX tells you what's portable. The kernel source code tells you what's possible. Sometimes there's a 400x difference between the two. Whether it is worth exploiting is a different question.&lt;/p&gt;
    &lt;p&gt;Check the old assumptions. The &lt;code&gt;/proc&lt;/code&gt; parsing approach made sense when it was written, before anyone realized it could be exploited this way. Assumptions get baked into code. Revisiting them occasionally pays off.&lt;/p&gt;
    &lt;p&gt;The change landed on December 3, 2025. Just one day before the JDK 26 feature freeze. If you're using &lt;code&gt;ThreadMXBean.getCurrentThreadUserTime()&lt;/code&gt;, JDK 26 (releasing March 2026) brings you a free 30-400x speedup!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://questdb.com/blog/jvm-current-thread-user-time/"/><published>2026-01-13T23:00:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46610557</id><title>The $LANG Programming Language</title><updated>2026-01-14T00:56:56.539809+00:00</updated><content>&lt;doc fingerprint="744192a701f0ce70"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;This afternoon I posted some tips on how to present a new* programming language to HN: &lt;/p&gt;https://news.ycombinator.com/item?id=46608577&lt;p&gt;. It occurred to me that HN has a tradition of posts called "The {name} programming language" (part of the long tradition of papers and books with such titles) and it might be fun to track them down. I tried to keep only the interesting ones:&lt;/p&gt;&lt;p&gt;https://news.ycombinator.com/thelang&lt;/p&gt;&lt;p&gt;Similarly, Show HNs of programming languages are at https://news.ycombinator.com/showlang.&lt;/p&gt;&lt;p&gt;These are curated lists so they're frozen in time. Maybe we can figure out how to update them.&lt;/p&gt;&lt;p&gt;A few famous cases:&lt;/p&gt;&lt;p&gt;The Swift Programming Language - https://news.ycombinator.com/item?id=7835099 - June 2014 (926 comments)&lt;/p&gt;&lt;p&gt;The Julia Programming Language - https://news.ycombinator.com/item?id=3606380 - Feb 2012 (203 comments)&lt;/p&gt;&lt;p&gt;The Rust programming language - https://news.ycombinator.com/item?id=1498528 - July 2010 (44 comments)&lt;/p&gt;&lt;p&gt;The Go Programming Language - https://news.ycombinator.com/item?id=934142 - Nov 2009 (219 comments)&lt;/p&gt;&lt;p&gt;But the obscure and esoteric ones are the most fun.&lt;/p&gt;&lt;p&gt;(* where 'new' might mean old, a la https://news.ycombinator.com/item?id=23459210)&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=46610557"/><published>2026-01-14T00:17:19+00:00</published></entry></feed>