<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-29T13:47:51.005220+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45734486</id><title>Nvidia takes $1B stake in Nokia</title><updated>2025-10-29T13:47:59.417691+00:00</updated><content>&lt;doc fingerprint="ea9297f929ae83ae"&gt;
  &lt;main&gt;
    &lt;p&gt;Nokia announced on Tuesday that Nvidia is taking a $1 billion stake in the networking company, the latest partnership for the artificial intelligence chipmaker.&lt;/p&gt;
    &lt;p&gt;Shares of Nokia soared 22% higher following the news.&lt;/p&gt;
    &lt;p&gt;Nokia will issue more than 166 million new shares and will use the proceeds to fund its plans for AI and other general corporate purposes.&lt;/p&gt;
    &lt;p&gt;The two companies also struck a strategic partnership to work together to develop next-generation 6G cellular technology. Nokia said that it would adapt its 5G and 6G software to run on Nvidia's chips, and will collaborate on networking technology for AI.&lt;/p&gt;
    &lt;p&gt;Nokia said Nvidia would consider incorporating its technology into its future AI infrastructure plans.&lt;/p&gt;
    &lt;p&gt;Nokia, a Finnish company, is best known for its early cellphones, but in recent years, it has primarily been a supplier of 5G cellular equipment to telecom providers.&lt;/p&gt;
    &lt;p&gt;The announcement comes as Nvidia CEO Jensen Huang prepares to address an audience of policymakers and government leaders in Washington, D.C., to keynote the company's developer conference.&lt;/p&gt;
    &lt;p&gt;Nokia and Nvidia are expected to discuss some of their collaborations and plans at the conference.&lt;/p&gt;
    &lt;p&gt;Nvidia has taken several equity stakes in strategic partners in recent months as the company has found itself at the center of the AI world.&lt;/p&gt;
    &lt;p&gt;In September, it committed a $5 billion investment to one-time rival Intel, and said it would invest $100 billion in OpenAI. It also committed $500 million in self-driving car startup Wayve and a $667 million investment in U.K. cloud provider Nscale.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnbc.com/2025/10/28/nvidia-nokia-ai.html"/><published>2025-10-28T15:53:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45735877</id><title>Fil-C: A memory-safe C implementation</title><updated>2025-10-29T13:47:58.969222+00:00</updated><content>&lt;doc fingerprint="b67b189d94a726d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fil-C: A memory-safe C implementation&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt; Fil-C is a memory-safe implementation of C and C++ that aims to let C code — complete with pointer arithmetic, unions, and other features that are often cited as a problem for memory-safe languages — run safely, unmodified. Its dedication to being "&lt;quote&gt;fanatically compatible&lt;/quote&gt;" makes it an attractive choice for retrofitting memory-safety into existing applications. Despite the project's relative youth and single active contributor, Fil-C is capable of compiling an entire memory-safe Linux user space (based on Linux From Scratch), albeit with some modifications to the more complex programs. It also features memory-safe signal handling and a concurrent garbage collector. &lt;/p&gt;
    &lt;p&gt;Fil-C is a fork of Clang; it's available under an Apache v2.0 license with LLVM exceptions for the runtime. Changes from the upstream compiler are occasionally merged in, with Fil-C currently being based on version 20.1.8 from July 2025. The project is a personal passion of Filip Pizlo, who has previously worked on the runtimes of a number of managed languages, including Java and JavaScript. When he first began the project, he was not sure that it was even possible. The initial implementation was prohibitively slow to run, since it needed to insert a lot of different safety checks. This has given Fil-C a reputation for slowness. Since the initial implementation proved viable, however, Pizlo has managed to optimize a number of common cases, making Fil-C-generated code only a few times slower than Clang-generated code, although the exact slowdown depends heavily on the structure of the benchmarked program.&lt;/p&gt;
    &lt;p&gt;Reliable benchmarking is notoriously finicky, but in order to get some rough feel for whether that level of performance impact would be problematic, I compiled Bash version 5.2.32 with Fil-C and tried using it as my shell. Bash is nearly a best case for Fil-C, because it spends more time running external programs than running its own code, but I still expected the performance difference to be noticeable. It wasn't. So, at least for some programs, the performance overhead of Fil-C does not seem to be a problem in practice.&lt;/p&gt;
    &lt;p&gt;In order to support its various run-time safety checks, Fil-C does use a different internal ABI than Clang does. As a result, objects compiled with Fil-C won't link correctly against objects generated by other compilers. Since Fil-C is a full implementation of C and C++ at the source-code level, however, in practice this just requires everything to be recompiled with Fil-C. Inter-language linking, such as with Rust, is not currently supported by the project.&lt;/p&gt;
    &lt;head rend="h4"&gt;Capabilities&lt;/head&gt;
    &lt;p&gt;The major challenge of rendering C memory-safe is, of course, pointer handling. This is especially complicated by the fact that, as the long road to CHERI-compatibility has shown, many programs expect a pointer to be 32 or 64 bits, depending on the architecture. Fil-C has tried several different ways to represent pointers since the project's beginning in 2023. Fil-C's first pointers were 256 bits, not thread-safe, and didn't protect against use-after-free bugs. The current implementation, called "InvisiCaps", allows for pointers that appear to match the natural pointer size of the architecture (although this requires storing some auxiliary information elsewhere), with full support for concurrency and catching use-after-free bugs, at the expense of some run-time overhead.&lt;/p&gt;
    &lt;p&gt;Fil-C's documentation compares InvisiCaps to a software implementation of CHERI: pointers are separated into a trusted "capability" piece and an untrusted "address" piece. Since Fil-C controls how the program is compiled, it can ensure that the program doesn't have direct access to the capabilities of any pointers, and therefore the runtime can rely on them being uncorrupted. The tricky part of the implementation comes from how these two pieces of information are stored in what looks to the program like 64 bits.&lt;/p&gt;
    &lt;p&gt;When Fil-C allocates an object on the heap, it adds two metadata words before the start of the allocated object: an upper bound, used to check accesses to the object based on its size, and an "aux word" that is used to store additional pointer metadata. When the program first writes a pointer value into an object, the runtime allocates a new auxiliary allocation of the same size as the object being written into, and puts an actual hardware-level pointer (i.e., one without an attached capability) to the new allocation into the aux word of the object. This auxiliary allocation, which is invisible to the program being compiled, is used to store the associated capability information for the pointer being stored (and is also reused for any additional pointers stored into the object later). The address value is stored into the object as normal, so any C bit-twiddling techniques that require looking at the stored value of the pointer work as expected.&lt;/p&gt;
    &lt;p&gt;This approach does mean that structures that contain pointers end up using twice as much memory, and every load of a pointer involves a pointer indirection through the aux word. In practice, the documentation claims that the performance overhead of this approach for most programs makes them run about four times more slowly, although that number depends on how heavily the program makes use of pointers. Still, he has ideas for several optimizations that he hopes can bring the performance overhead down over time.&lt;/p&gt;
    &lt;p&gt;One wrinkle with this approach is atomic access to pointers — i.e. using _Atomic or volatile. Luckily, there is no problem that cannot be solved with more pointer indirection: when the program loads or stores a pointer value atomically, instead of having the auxiliary allocation contain the capability information directly, it points to a third 128-bit allocation that stores the capability and pointer value together. That allocation can be updated with 128-bit atomic instructions, if the platform supports them, or by creating new allocations and atomically swapping the pointers to them.&lt;/p&gt;
    &lt;p&gt;Since the aux word is used to store a pointer value, Fil-C can use pointer tagging to store some additional information there as well; that is used to indicate special types of objects that need to be handled differently, such as functions, threads, and mmap()-backed allocations. It's also used to mark freed objects, so that any access results in an error message and a crash.&lt;/p&gt;
    &lt;head rend="h4"&gt;Memory management&lt;/head&gt;
    &lt;p&gt;When an object is freed, its aux word marks it as a free object, which lets the auxiliary allocation be reclaimed immediately. The original object can't be freed immediately, however. Otherwise, a program could free an object, allocate a new object in the same location, and thereby cover up use-after-free bugs. Instead, Fil-C uses a garbage collector to free an object's backing memory only once all of the pointers to it go away. Unlike other garbage collectors for C — such as the Boehm-Demers-Weiser garbage collector — Fil-C can use the auxiliary capability information to track live objects precisely.&lt;/p&gt;
    &lt;p&gt;Fil-C's garbage collector is both parallel (collection happens faster the more cores are available) and concurrent (collection happens without pausing the program). Technically, the garbage collector does require threads to occasionally pause just long enough to tell it where pointers are located on the stack, but that only occurs at special "safe points" — otherwise, the program can load and manipulate pointers without notifying the garbage collector. Safe points are used as a synchronization barrier: the collector can't know that an object is really garbage until every thread has passed at least one safe point since it finished marking. This synchronization is done with atomic instructions, however, so in practice threads never need to pause for longer than a few instructions.&lt;/p&gt;
    &lt;p&gt;The exception is the implementation of fork(), which uses the safe points needed by the garbage collector to temporarily pause all of the threads in the program in order to prevent race conditions while forking. Fil-C inserts a safe point at every backward control-flow edge, i.e., whenever code could execute in a loop. In the common case, the inserted code just needs to load a flag register and confirm that the garbage collector has not requested anything be done. If the garbage collector does have a request for the thread, the thread runs a callback to perform the needed synchronization.&lt;/p&gt;
    &lt;p&gt;Fil-C uses the same safe-point mechanism to implement signal handling. Signal handlers are only run when the interrupted thread reaches a safe point. That, in turn, allows signal handlers to allocate and free memory without interfering with the garbage collector's operation; Fil-C's malloc() is signal-safe.&lt;/p&gt;
    &lt;head rend="h4"&gt;Memory-safe Linux&lt;/head&gt;
    &lt;p&gt;Linux From Scratch (LFS) is a tutorial on compiling one's own complete Linux user space. It walks through the steps of compiling and installing all of the core software needed for a typical Linux user space in a chroot() environment. Pizlo has successfully run through LFS with Fil-C to produce a memory-safe version, although a non-Fil-C compiler is still needed to build some fundamental components, such as Fil-C's own runtime, the GNU C library, and the kernel. (While Fil-C's runtime relies on a normal copy of the GNU C library to make system calls, the programs that Fil-C compiles use a Fil-C-compiled version of the library.)&lt;/p&gt;
    &lt;p&gt;The process is mostly identical to LFS up through the end of chapter 7, because everything prior to that point consists of using cross-build tools to obtain a working compiler in the chroot() environment. The one difference is that the cross-build tools are built with a different configured prefix, so that they won't conflict with Fil-C. At that point, one can build a copy of Fil-C and use it to mostly replace the existing compiler. The remaining steps of LFS are unchanged.&lt;/p&gt;
    &lt;p&gt;Scripts to automate the process are included in the Fil-C Git repository, including some steps from Beyond Linux From Scratch that result in a working graphical user interface and a handful of more complicated applications such as Emacs.&lt;/p&gt;
    &lt;p&gt;Overall, Fil-C offers a remarkably complete solution for making existing C programs memory-safe. While it does nothing for undefined behavior that is not related to memory safety, the most pernicious and difficult-to-prevent security vulnerabilities in C programs tend to rely on exploiting memory-unsafe behavior. Readers who have already considered and rejected Fil-C for their use case due to its early performance problems may wish to take a second look — although anyone hoping for stability might want to wait for others to take the plunge, given the project's relative immaturity. That said, for existing applications where a sizeable performance hit is preferable to an exploitable vulnerability, Fil-C is an excellent choice.&lt;/p&gt;
    &lt;p&gt; Posted Oct 28, 2025 17:54 UTC (Tue) by tialaramex (subscriber, #21167) [Link] (6 responses) For users in a bunch of cases this is a no brainer, Daroc gave their shell as an example but I'm sure most of us run many programs every day where raw perf just isn't a big deal. I am interested in programmers rather than users because I think that influences whether Fil-C is just an interesting project for our moment or it becomes a "successor" to C in a way that Zig, Odin etc. never could. Posted Oct 28, 2025 18:15 UTC (Tue) by rahulsundaram (subscriber, #21946) [Link] (1 responses) My expectation is that there isn't going to be a single successor to C. For some group of people, that was C++ a long time back. For others it is going to Rust or Zig or something else. For the final group they are going to keep coding in C forever and it will more of a generational change eventually. Posted Oct 28, 2025 18:32 UTC (Tue) by daroc (editor, #160859) [Link] Posted Oct 28, 2025 18:34 UTC (Tue) by rweikusat2 (subscriber, #117920) [Link] (3 responses) Posted Oct 29, 2025 1:35 UTC (Wed) by rahulsundaram (subscriber, #21946) [Link] (2 responses) Posted Oct 29, 2025 11:45 UTC (Wed) by k3ninho (subscriber, #50375) [Link] K3n. Posted Oct 29, 2025 12:18 UTC (Wed) by Baughn (subscriber, #124425) [Link] Well, now it’s not the fastest option. And no, you don’t get to tell me not to compile everything with FilC. We just need to make that the standard. Posted Oct 28, 2025 19:55 UTC (Tue) by oldnpastit (subscriber, #95303) [Link] (5 responses) Posted Oct 28, 2025 20:28 UTC (Tue) by bertschingert (subscriber, #160729) [Link] But it would seem to be more robust than ASAN; from reading about how ASAN works, it seems that it puts "poisoned" bytes around an allocation, so that memory accesses shortly after the end of a buffer hit those poisoned bytes and are caught. However, ASAN wouldn't catch an invalid access to a non-poisoned address of memory via a particular a pointer, if that address was allocated in a separate allocation. [1] I assume Fil-C's pointer capability model is able to catch "provenance" violations like that. [1] https://blog.gistre.epita.fr/posts/benjamin.peter-2022-10... Posted Oct 28, 2025 20:33 UTC (Tue) by excors (subscriber, #95769) [Link] (3 responses) &amp;gt; Fil-C is engineered to prevent memory safety bugs from being used for exploitation rather than just simply flagging them often enough to find bugs. This makes Fil-C different from AddressSanitizer, HWAsan, or MTE, which can all be bypassed by attackers. The key difference that makes this possible is that Fil-C is capability based (so each pointer knows what range of memory it may access, and how it may access it) rather than tag based (where pointer accesses are allowed if they hit valid memory). Clang says "AddressSanitizer's runtime was not developed with security-sensitive constraints in mind and may compromise the security of the resulting executable", so it should not be used in production. Valgrind has much worse performance (the manual claims 10-50x slowdown, plus it's effectively single-threaded), which is probably bad enough to make it unusable in production, and similarly will miss many memory safety bugs. Posted Oct 28, 2025 21:26 UTC (Tue) by cyperpunks (subscriber, #39406) [Link] (2 responses) Posted Oct 29, 2025 4:57 UTC (Wed) by Cyberax (✭ supporter ✭, #52523) [Link] (1 responses) It's somewhat analogous to compiling C into WebAssembly and then JIT-compiling WebAssembly. The amazing thing is that it preserves most of C/C++ semantics. Posted Oct 29, 2025 5:53 UTC (Wed) by willmo (subscriber, #82093) [Link] But (at least WRT memory safety) only the semantics of the abstract machine described by the language standards, and not the additional semantics (aka undefined behavior) of the straightforward mappings to typical hardware that we’re all accustomed to. Very cool idea. :-) Posted Oct 29, 2025 6:18 UTC (Wed) by epa (subscriber, #39769) [Link] (3 responses) The unsafe-compiled code wouldn’t be exactly the same as you get from plain clang, as the memory layout is different, and it might be a bit slower because of that, but it could do without checks of pointer capability checking and, perhaps, other checks like overflow and array bounds. The rest of the program must assume that the unsafe code is correct. Posted Oct 29, 2025 7:01 UTC (Wed) by magfr (subscriber, #16052) [Link] Posted Oct 29, 2025 11:42 UTC (Wed) by matthias (subscriber, #94967) [Link] (1 responses) This is not the only problem. Once you have unsafe blocks, you have a contract between safe and unsafe code, as an unsafe block will for sure rely on invariants that are hard to impossible to check at the boundary. Say you have an unsafe block that traverses a linked list. If you turn of the runtime checks inside the block, you rely on the promise that all pointers in the linked list are valid. Otherwise you immediately have undefined behaviour. Even in rust this is an issue and if you have unsafe blocks you have to be very careful that the unsafety is contained, usually at the module boundary. Changing an integer is usually considered safe, but if the integer encodes the length of a Vec, than this is very unsafe, as the unsafe code that implements indexing into the Vec relies on this integer to be correct. This is solved by not providing any (safe) functions that can change this integer directly. The situation is that safe code that interacts with Vec cannot cause undefined behaviour. However, safe code within the Vec module most definitely can. You do not need an unsafe block to change the integer encoding the length. This is described quite nicely in the first chapter of the nomicon[1] (the guide to unsafe rust). You can read this introductory chapter even if you do not know rust. With Fil-C this containment of unsafe is just impossible. In C you can always change the contents of a variable by casting it to an array of bytes. So you cannot rel on any invariants and have to check when you use a pointer. Or you have to verify that the unsafe block is never called with violated invariants, which basically forces you to verify all the code, not only the usnafe block. [1] https://doc.rust-lang.org/nomicon/meet-safe-and-unsafe.html Posted Oct 29, 2025 13:36 UTC (Wed) by tialaramex (subscriber, #21167) [Link] First, all pointer dereferences in Rust are unsafe. If you have a pointer named ptr, then *ptr, dereferencing the pointer, isn't allowed in safe Rust, full stop. So caring about whether the pointer is valid is always on you. Which leads us to... Second, unlike C and C++ Rust doesn't care about the existence of invalid pointers. Safe Rust can make null pointers, dangling pointers, even just arbitrarily mint a nonsense pointer which claims it is a pointer to a Goose but is actually the word "HONK" in ASCII as an address just marked up as a pointer-to-Goose. This is fine in safe Rust and guaranteed not to cause UB, so long as nobody dereferences the pointer which they cannot do in safe Rust. For C programmers this doesn't make sense, because in C there are three categories - pointers to things, which you can dereference; pointers one past things, which are allowed to exist but must never be dereferenced, and all other pointers which are invalid and no guarantees about them are provided by the language at all. So the intuitions are very different. Posted Oct 29, 2025 8:11 UTC (Wed) by marcH (subscriber, #57642) [Link] &lt;head&gt;Fil-C for programmers&lt;/head&gt;&lt;head&gt;Fil-C for programmers&lt;/head&gt;&lt;head&gt;Fil-C for programmers&lt;/head&gt;&lt;head&gt;Fil-C for programmers&lt;/head&gt;&lt;head&gt;Fil-C for programmers&lt;/head&gt;&lt;head&gt;Fil-C for programmers&lt;/head&gt;&lt;head&gt;Fil-C for programmers&lt;/head&gt;&lt;head&gt;How is this different from tools like Valgrind and Address Sanitizer?&lt;/head&gt;&lt;head&gt;How is this different from tools like Valgrind and Address Sanitizer?&lt;/head&gt;&lt;head&gt;How is this different from tools like Valgrind and Address Sanitizer?&lt;/head&gt;&lt;head&gt;How is this different from tools like Valgrind and Address Sanitizer?&lt;/head&gt;&lt;head&gt;How is this different from tools like Valgrind and Address Sanitizer?&lt;/head&gt;&lt;head&gt;How is this different from tools like Valgrind and Address Sanitizer?&lt;/head&gt;&lt;head&gt;Mixing safe and unsafe &lt;/head&gt;&lt;head&gt;Mixing safe and unsafe &lt;/head&gt;&lt;lb/&gt; The compiler defines a new ABI and you can't link system ABI libs with it.&lt;head&gt;Mixing safe and unsafe &lt;/head&gt;&lt;head&gt;Mixing safe and unsafe &lt;/head&gt;&lt;head&gt;Did this find bugs?&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/SubscriberLink/1042938/658ade3768dd4758/"/><published>2025-10-28T17:25:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45736479</id><title>What we talk about when we talk about sideloading</title><updated>2025-10-29T13:47:58.281442+00:00</updated><content>&lt;doc fingerprint="3f229112d91610f9"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;What We Talk About When We Talk About Sideloading&lt;/head&gt;Posted on Oct 28, 2025 by marcprux&lt;p&gt;We recently published a blog post with our reaction to the new Google Developer Program and how it impacts your freedom to use the devices that you own in the ways that you want. The post garnered quite a lot of feedback and interest from the community and press, as well as various civil society groups and regulatory agencies.&lt;/p&gt;&lt;p&gt;In this post, I hope to clarify and expand on some of the points and rebut some of the counter-messaging that we have witnessed.&lt;/p&gt;&lt;head rend="h3"&gt;Googleâs message that âSideloading is Not Going Awayâ is clear, concise, and false&lt;/head&gt;&lt;p&gt;Shortly after our post was published, Google aired an episode of their Android Developers Roundtable series, where they state unequivocally that âsideloading isnât going anywhereâ. They follow-up with a blog post:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Does this mean sideloading is going away on Android? Absolutely not. Sideloading is fundamental to Android and it is not going away.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This statement is untrue. The developer verification decree effectively ends the ability for individuals to choose what software they run on the devices they own.&lt;/p&gt;&lt;p&gt;It bears reminding that âsideloadâ is a made-up term. Putting software on your computer is simply called âinstallingâ, regardless of whether that computer is in your pocket or on your desk. This could perhaps be further precised as âdirect installingâ, in case you need to make a distinction between obtaining software the old-fashioned way versus going through a rent-seeking intermediary marketplace like the Google Play Store or the Apple App Store.&lt;/p&gt;&lt;p&gt;Regardless, the term âsideloadâ was coined to insinuate that there is something dark and sinister about the process, as if the user were making an end-run around safeguards that are designed to keep you protected and secure. But if we reluctantly accept that âsideloadingâ is a term that has wriggled its way into common parlance, then we should at least use a consistent definition for it. Wikipediaâs summary definition is:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;the transfer of apps from web sources that are not vendor-approved&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;By this definition, Googleâs statement that âsideloading is not going awayâ is simply false. The vendor â Google, in the case of Android certified devices â will, in point of fact, be approving the source. The supplicant app developer must register with Google, pay a fee, provide government identification, agree to non-negotiable (and ever-changing) terms and conditions, enumerate all their current and future application identifiers, upload evidence of their private signing key, and then hope and wait for Googleâs approval.&lt;/p&gt;&lt;head rend="h3"&gt;What this means for your rights&lt;/head&gt;&lt;p&gt;You, the consumer, purchased your Android device believing in Googleâs promise that it was an open computing platform and that you could run whatever software you choose on it. Instead, starting next year, they will be non-consensually pushing an update to your operating system that irrevocably blocks this right and leaves you at the mercy of their judgement over what software you are permitted to trust.&lt;/p&gt;&lt;p&gt;You, the creator, can no longer develop an app and share it directly with your friends, family, and community without first seeking Googleâs approval. The promise of Android â and a marketing advantage it has used to distinguish itself against the iPhone â has always been that it is âopenâ. But Google clearly feels that they have enough of a lock on the Android ecosystem, along with sufficient regulatory capture, that they can now jettison this principle with prejudice and impunity.&lt;/p&gt;&lt;p&gt;You, the state, are ceding the rights of your citizens and your own digital sovereignty to a company with a track record of complying with the extrajudicial demands of authoritarian regimes to remove perfectly legal apps that they happen to dislike. The software that is critical to the running of your businesses and governments will be at the mercy of the opaque whims of a distant and unaccountable corporation. Monocultures are perilous not just in agriculture, but in software distribution as well.&lt;/p&gt;&lt;p&gt;As a reminder, this applies not just to devices that exclusively use the Google Play Store: this is for every Android Certified device everywhere in the world, which encompasses over 95% of all Android devices outside of China. Regardless of whether the device owner prefers to use a competing app store like the Samsung Galaxy Store or the Epic Games Store, or a free and open-source app repository like F-Droid, they will be captive to the overarching policies unilaterally dictated by a competing corporate entity.&lt;/p&gt;&lt;head rend="h3"&gt;The place of greater safety&lt;/head&gt;&lt;p&gt;In promoting their developer registration program, Google purports:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Our recent analysis found over 50 times more malware from internet-sideloaded sources than on apps available through Google Play.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;We havenât seen this recent analysis â or any other supporting evidence â but the â50 timesâ multiple does certainly sound like great cause for distress (even if it is a surprisingly round number). But given the recent news of â224 malicious apps removed from the Google Play Store after ad fraud campaign discoveredâ, we are left to wonder whether their energies might better be spent assessing and improving their own safeguards rather than casting vague disparagements against the software development communities that thrive outside their walled garden.&lt;/p&gt;&lt;p&gt;In addition, other recent news of over 19 million downloads of malware from the Play Store leads us to question whether the sole judgement of a single corporate entity can be trusted to identify and assess malware, especially when that judgement is clouded by commercial incentives that may not align with the well-being of their users.&lt;/p&gt;&lt;head rend="h3"&gt;What can be done?&lt;/head&gt;&lt;p&gt;Google has been facing public outcry against their heavy-handed policies for a long time, but this trend has accelerated recently. Last year they crippled ad-blockers in Chrome and Chromium-based browsers by forcing through their unpopular âmanifest v3â requirement for plugins, and earlier this year they closed off the development of the Android Open Source Project (AOSP), which is how they were able to clandestinely implement the verification infrastructure that enforces their developer registration decree.&lt;/p&gt;&lt;p&gt;Developer verification is an existential threat to free software distribution platforms like F-Droid as well as emergent commercial competitors to the Play Store. We are witnessing a groundswell of opposition to this attempt from both our user and developer communities, as well as the tech press and civil society groups, but public policymakers still need to be educated about the threat.&lt;/p&gt;&lt;p&gt;To learn more about what you can do as a consumer, visit keepandroidopen.org for information on how to contact your representative agencies and advocate for keeping the Android ecosystem open for consumers and competition.&lt;/p&gt;&lt;p&gt;If you are an app developer, we recommend against signing yourself up for Googleâs developer registration program at this time. We unequivocally reject their attempt to force this program upon the world.&lt;/p&gt;&lt;p&gt;Over half of all humankind uses an Android smartphone. Google does not own your phone. You own your phone. You have the right to decide who to trust, and where you can get your software from.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://f-droid.org/2025/10/28/sideloading.html"/><published>2025-10-28T18:02:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45736499</id><title>HTTPS by default</title><updated>2025-10-29T13:47:57.834984+00:00</updated><content>&lt;doc fingerprint="a85815123b32f74a"&gt;
  &lt;main&gt;
    &lt;p&gt;One year from now, with the release of Chrome 154 in October 2026, we will change the default settings of Chrome to enable “Always Use Secure Connections”. This means Chrome will ask for the user's permission before the first access to any public site without HTTPS.&lt;/p&gt;
    &lt;p&gt;The “Always Use Secure Connections” setting warns users before accessing a site without HTTPS&lt;/p&gt;
    &lt;p&gt;Chrome Security's mission is to make it safe to click on links. Part of being safe means ensuring that when a user types a URL or clicks on a link, the browser ends up where the user intended. When links don't use HTTPS, an attacker can hijack the navigation and force Chrome users to load arbitrary, attacker-controlled resources, and expose the user to malware, targeted exploitation, or social engineering attacks. Attacks like this are not hypothetical—software to hijack navigations is readily available and attackers have previously used insecure HTTP to compromise user devices in a targeted attack.&lt;/p&gt;
    &lt;p&gt;Since attackers only need a single insecure navigation, they don't need to worry that many sites have adopted HTTPS—any single HTTP navigation may offer a foothold. What's worse, many plaintext HTTP connections today are entirely invisible to users, as HTTP sites may immediately redirect to HTTPS sites. That gives users no opportunity to see Chrome's "Not Secure" URL bar warnings after the risk has occurred, and no opportunity to keep themselves safe in the first place.&lt;/p&gt;
    &lt;p&gt;To address this risk, we launched the “Always Use Secure Connections” setting in 2022 as an opt-in option. In this mode, Chrome attempts every connection over HTTPS, and shows a bypassable warning to the user if HTTPS is unavailable. We also previously discussed our intent to move towards HTTPS by default. We now think the time has come to enable “Always Use Secure Connections” for all users by default.&lt;/p&gt;
    &lt;p&gt;For more than a decade, Google has published the HTTPS transparency report, which tracks the percentage of navigations in Chrome that use HTTPS. For the first several years of the report, numbers saw an impressive climb, starting at around 30-45% in 2015, and ending up around the 95-99% range around 2020. Since then, progress has largely plateaued.&lt;/p&gt;
    &lt;p&gt;HTTPS adoption expressed as a percentage of main frame page loads&lt;/p&gt;
    &lt;p&gt;This rise represents a tremendous improvement to the security of the web, and demonstrates that HTTPS is now mature and widespread. This level of adoption is what makes it possible to consider stronger mitigations against the remaining insecure HTTP.&lt;/p&gt;
    &lt;p&gt;While it may at first seem that 95% HTTPS means that the problem is mostly solved, the truth is that a few percentage points of HTTP navigations is still a lot of navigations. Since HTTP navigations remain a regular occurrence for most Chrome users, a naive approach to warning on all HTTP navigations would be quite disruptive. At the same time, as the plateau demonstrates, doing nothing would allow this risk to persist indefinitely. To balance these risks, we have taken steps to ensure that we can help the web move towards safer defaults, while limiting the potential annoyance warnings will cause to users.&lt;/p&gt;
    &lt;p&gt;One way we're balancing risks to users is by making sure Chrome does not warn about the same sites excessively. In all variants of the "Always Use Secure Connections" settings, so long as the user regularly visits an insecure site, Chrome will not warn the user about that site repeatedly. This means that rather than warn users about 1 out of 50 navigations, Chrome will only warn users when they visit a new (or not recently visited) site without using HTTPS.&lt;/p&gt;
    &lt;p&gt;To further address the issue, it's important to understand what sort of traffic is still using HTTP. The largest contributor to insecure HTTP by far, and the largest contributor to variation across platforms, is insecure navigations to private sites. The graph above includes both those to public sites, such as example.com, and navigations to private sites, such as local IP addresses like 192.168.0.1, single-label hostnames, and shortlinks like intranet/. While it is free and easy to get an HTTPS certificate that is trusted by Chrome for a public site, acquiring an HTTPS certificate for a private site unfortunately remains complicated. This is because private names are "non-unique"—private names can refer to different hosts on different networks. There is no single owner of 192.168.0.1 for a certification authority to validate and issue a certificate to.&lt;/p&gt;
    &lt;p&gt;example.com&lt;/p&gt;
    &lt;p&gt;192.168.0.1&lt;/p&gt;
    &lt;p&gt;intranet/&lt;/p&gt;
    &lt;p&gt;HTTP navigations to private sites can still be risky, but are typically less dangerous than their public site counterparts because there are fewer ways for an attacker to take advantage of these HTTP navigations. HTTP on private sites can only be abused by an attacker also on your local network, like on your home wifi or in a corporate network.&lt;/p&gt;
    &lt;p&gt;If you exclude navigations to private sites, then the distribution becomes much tighter across platforms. In particular, Linux jumps from 84% HTTPS to nearly 97% HTTPS when limiting the analysis to public sites only. Windows increases from 95% to 98% HTTPS, and both Android and Mac increase to over 99% HTTPS.&lt;/p&gt;
    &lt;p&gt;In recognition of the reduced risk HTTP to private sites represents, last year we introduced a variant of “Always Use Secure Connections” for public sites only. For users who frequently access private sites (such as those in enterprise settings, or web developers), excluding warnings on private sites significantly reduces the volume of warnings those users will see. Simultaneously, for users who do not access private sites frequently, this mode introduces only a small reduction in protection. This is the variant we intend to enable for all users next year.&lt;/p&gt;
    &lt;p&gt;“Always Use Secure Connections,” available at chrome://settings/security&lt;/p&gt;
    &lt;p&gt;In Chrome 141, we experimented with enabling “Always Use Secure Connections” for public sites by default for a small percentage of users. We wanted to validate our expectations that this setting keeps users safer without burdening them with excessive warnings.&lt;/p&gt;
    &lt;p&gt;Analyzing the data from the experiment, we confirmed that the number of warnings seen by any users is considerably lower than 3% of navigations—in fact, the median user sees fewer than one warning per week, and the ninety-fifth percentile user sees fewer than three warnings per week..&lt;/p&gt;
    &lt;p&gt;Once “Always Use Secure Connections” is the default and additional sites migrate away from HTTP, we expect the actual warning volume to be even lower than it is now. In parallel to our experiments, we have reached out to a number of companies responsible for the most HTTP navigations, and expect that they will be able to migrate away from HTTP before the change in Chrome 154. For many of these organizations, transitioning to HTTPS isn't disproportionately hard, but simply has not received attention. For example, many of these sites use HTTP only for navigations that immediately redirect to HTTPS sites—an insecure interaction which was previously completely invisible to users.&lt;/p&gt;
    &lt;p&gt;Another current use case for HTTP is to avoid mixed content blocking when accessing devices on the local network. Private addresses, as discussed above, often do not have trusted HTTPS certificates, due to the difficulties of validating ownership of a non-unique name. This means most local network traffic is over HTTP, and cannot be initiated from an HTTPS page—the HTTP traffic counts as insecure mixed content, and is blocked. One common use case for needing to access the local network is to configure a local network device, e.g. the manufacturer might host a configuration portal at config.example.com, which then sends requests to a local device to configure it.&lt;/p&gt;
    &lt;p&gt;config.example.com&lt;/p&gt;
    &lt;p&gt;Previously, these types of pages needed to be hosted without HTTPS to avoid mixed content blocking. However, we recently introduced a local network access permission, which both prevents sites from accessing the user’s local network without consent, but also allows an HTTPS site to bypass mixed content checks for the local network once the permission has been granted. This can unblock migrating these domains to HTTPS.&lt;/p&gt;
    &lt;p&gt;We will enable the "Always Use Secure Connections" setting in its public-sites variant by default in October 2026, with the release of Chrome 154. Prior to enabling it by default for all users, in Chrome 147, releasing in April 2026, we will enable Always Use Secure Connections in its public-sites variant for the over 1 billion users who have opted-in to Enhanced Safe Browsing protections in Chrome.&lt;/p&gt;
    &lt;p&gt;While it is our hope and expectation that this transition will be relatively painless for most users, users will still be able to disable the warnings by disabling the "Always Use Secure Connections" setting.&lt;/p&gt;
    &lt;p&gt;If you are a website developer or IT professional, and you have users who may be impacted by this feature, we very strongly recommend enabling the "Always Use Secure Connections" setting today to help identify sites that you may need to work to migrate. IT professionals may find it useful to read our additional resources to better understand the circumstances where warnings will be shown, how to mitigate them, and how organizations that manage Chrome clients (like enterprises or educational institutions) can ensure that Chrome shows the right warnings to meet those organizations' needs.&lt;/p&gt;
    &lt;p&gt;While we believe that warning on insecure public sites represents a significant step forward for the security of the web, there is still more work to be done. In the future, we hope to work to further reduce barriers to adoption of HTTPS, especially for local network sites. This work will hopefully enable even more robust HTTP protections down the road.&lt;/p&gt;
    &lt;p&gt;Post a Comment&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://security.googleblog.com/2025/10/https-by-default.html"/><published>2025-10-28T18:04:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45738247</id><title>Boring is what we wanted</title><updated>2025-10-29T13:47:57.653077+00:00</updated><content>&lt;doc fingerprint="b748792552261154"&gt;
  &lt;main&gt;
    &lt;p&gt;We are coming up on five years since the first M1 Macs shipped. It was an incredible time to be a Mac user. Those first Apple silicon Macs looked like the Intel machines they replaced, but they were better in every single way.&lt;/p&gt;
    &lt;p&gt;In December 2020, John Gruber wrote:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We knew this to be true: Computers could run fast and hot, or slow and cool. For laptops in particular, the best you could hope for is a middle ground: fast enough and cool enough. But if you wanted a machine that ran really fast, it wasn’t going to run cool (and wasn’t going to last long on battery), and if you wanted a computer that ran cool (and lasted long on battery), it wasn’t going to be fast.&lt;/p&gt;
      &lt;p&gt;We knew this to be true because that was the way things were. But now, with the M1 Macs, it’s not. M1 Macs run very fast and do so while remaining very cool and lasting mind-bogglingly long on battery. It was a fundamental trade-off inherent to PC computing, and now we don’t have to make it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Despite its Touch Bar, I immediately bought that first M1 MacBook Pro, and when the 14-inch MacBook Pro came out a year later, I moved to it.1 I’m typing these very words on my 14-inch MacBook Pro with an M4 Max inside. Each of these machines was faster than the one before it, outperforming my old iMac Pro and Mac Pro in new ways with every upgrade.&lt;/p&gt;
    &lt;p&gt;Apple silicon has been nothing but upside for the Mac, and yet some seem bored already. In the days since Apple announced the M5, I’ve seen and heard this sentiment more than I expected:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is just another boring incremental upgrade.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That 👏 is 👏 the 👏 point.&lt;/p&gt;
    &lt;p&gt;Back in the PowerPC and Intel days, Macs would sometimes go years between meaningful spec bumps, as Apple waited on its partners to deliver appropriate hardware for various machines. From failing NVIDIA cards in MacBook Pros to 27-inch Intel iMacs that ran so hot the fans were audible at all times, Mac hardware wasn’t always what Apple wanted.&lt;/p&gt;
    &lt;p&gt;Of course, some of the issues with previous generations of Mac were Apple’s fault — look no further than the butterfly keyboard or the years the Mac Pro spent in the wilderness. Apple will make questionable decisions in the future, just as it has in the past.&lt;/p&gt;
    &lt;p&gt;The difference is that with Apple silicon, Apple owns and controls the primary technologies behind the products it makes, as Tim Cook has always wanted. It means that it can ship updates to its SoCs on a regular cadence, making progress in terms of both power and efficiency each time.&lt;/p&gt;
    &lt;p&gt;A predictable update schedule means that incremental updates are inevitable. Revolution then evolution is not a bad thing; it’s okay that not every release is exciting or groundbreaking. It’s how technology has worked for decades.&lt;/p&gt;
    &lt;p&gt;…but some people have short memories. Before the Apple silicon introduction, we all wanted steady, predictable progress in Mac hardware development. We wanted each product in the lineup to be updated regularly and not wither on the vine for years. For the most part, Apple has delivered. Just look at this chart of the progress Apple has made since the M1:&lt;/p&gt;
    &lt;p&gt;I don’t see anything in those charts to complain about, especially given the frequency at which most people buy new computers. That’s one reason why Apple compared the M5 to the M1 in its press release announcing the new chip. Unless you buy a new computer every year, every update you experience will be meaningful.&lt;/p&gt;
    &lt;p&gt;That’s what we wanted when Apple announced the move away from Intel, and calling it boring five years in is missing the point and downplaying the success of Apple silicon thus far.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;My review of the M1 Pro 14-inch MacBook Pro remains one of my favorite blog posts I’ve written. ↩&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://512pixels.net/2025/10/boring-is-what-we-wanted/"/><published>2025-10-28T19:57:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45739080</id><title>Generative AI Image Editing Showdown</title><updated>2025-10-29T13:47:57.362264+00:00</updated><link href="https://genai-showdown.specr.net/image-editing"/><published>2025-10-28T20:58:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45739499</id><title>Tinkering is a way to acquire good taste</title><updated>2025-10-29T13:47:56.690560+00:00</updated><content>&lt;doc fingerprint="7426e95752a204d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;If you don't tinker, you don't have taste&lt;/head&gt;
    &lt;head rend="h2"&gt;tinÂ·ker&lt;/head&gt;
    &lt;head rend="h4"&gt;/ËtiNGkÉr/&lt;/head&gt;
    &lt;head rend="h4"&gt;to make small changes to something, especially in an attempt to repair or improve it.&lt;/head&gt;
    &lt;head rend="h1"&gt;In Hindsight&lt;/head&gt;
    &lt;p&gt;Growing up, I never stuck to a single thing, be it guitar lessons, art school, martial arts â I tried them all. when it came to programming, though, I never really tinkered. I was always amazed with video games and wondered how they were made but I never pursued that curiosity.&lt;/p&gt;
    &lt;p&gt;My tinkering habits picked up very late, and now I cannot go by without picking up new things in one form or another. Itâs how I learn. I wish I did it sooner. Itâs a major part of my learning process now, and I would never be the &lt;del&gt;programmer&lt;/del&gt; person I am today.&lt;/p&gt;
    &lt;head rend="h1"&gt;What the hell is tinkering?&lt;/head&gt;
    &lt;p&gt;Have you ever spent hours tweaking the mouse sensitivity in your favorite FPS game?&lt;/p&gt;
    &lt;p&gt;Have you ever installed a Linux distro, spent days configuring window managers, not because you had to, but purely because it gave you satisfaction and made your workflow exactly yours?&lt;/p&gt;
    &lt;p&gt;Ever pulled apart your mechanical keyboard, swapped keycaps, tested switches, and lubed stabilizers just for more thock?&lt;/p&gt;
    &lt;p&gt;That is what I mean.&lt;/p&gt;
    &lt;p&gt;I have come to understand that there are two kinds of people, those who do things only if it helps them achieve a goal, and those who do things just because. The ideal, of course, is to be a mix of both.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;when you tinker and throw away, thatâs practice, and practice should inherently be ephemeral, exploratory, and be frequent - @ludwigABAP&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;My approach to tinkering&lt;/head&gt;
    &lt;p&gt;There are plenty of people who still use the VSCode terminal as their default terminal, do not know what vim bindings are, GitHub desktop rather than the cli (at the very least). Iâm not saying these are bad things necessarily, just that this should be the minimum, not the median.&lt;/p&gt;
    &lt;p&gt;This does not mean I spend every waking hour fiddling with my neovim config. In fact, the last meaningful change to my config was 6 months ago. Finding that balance is where most people fail.&lt;/p&gt;
    &lt;p&gt;Over the years I have done so many things that in hindsight have made me appreciate programming more but were completely âunnecessaryâ in the strict sense.&lt;/p&gt;
    &lt;p&gt;In the past week I have, for the first time, written a glsl fragment shader, a rust procedural macro, template c++, a swift app, furthered my hatred for windows development (this is not new), and started using the helix editor more (mainly for good defaults + speed). I didnât have to do these things, but I did, for fun! And I know more about these things now.&lt;/p&gt;
    &lt;p&gt;No time spent learning, is time wasted.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why taste matters, especially now&lt;/head&gt;
    &lt;p&gt;Acquiring good taste comes through using various things, discarding the ones you donât like and keeping the ones you do. if you never try various things, you will not acquire good taste.&lt;/p&gt;
    &lt;p&gt;And what I mean by taste here is simply the honed ability to distinguish mediocrity from excellence. This will be highly subjective, and not everyoneâs taste will be the same, but that is the point, you should NOT have the same taste as someone else.&lt;/p&gt;
    &lt;p&gt;Question the status quo, experiment, break things, do this several times, do this everyday and keep doing it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://seated.ro/blog/tinkering-a-lost-art"/><published>2025-10-28T21:31:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45740214</id><title>Keeping the Internet fast and secure: introducing Merkle Tree Certificates</title><updated>2025-10-29T13:47:56.082373+00:00</updated><content>&lt;doc fingerprint="6f39937a141a36d0"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The world is in a race to build its first quantum computer capable of solving practical problems not feasible on even the largest conventional supercomputers. While the quantum computing paradigm promises many benefits, it also threatens the security of the Internet by breaking much of the cryptography we have come to rely on.&lt;/p&gt;
      &lt;p&gt;To mitigate this threat, Cloudflare is helping to migrate the Internet to Post-Quantum (PQ) cryptography. Today, about 50% of traffic to Cloudflare's edge network is protected against the most urgent threat: an attacker who can intercept and store encrypted traffic today and then decrypt it in the future with the help of a quantum computer. This is referred to as the harvest now, decrypt later threat.&lt;/p&gt;
      &lt;p&gt;However, this is just one of the threats we need to address. A quantum computer can also be used to crack a server's TLS certificate, allowing an attacker to impersonate the server to unsuspecting clients. The good news is that we already have PQ algorithms we can use for quantum-safe authentication. The bad news is that adoption of these algorithms in TLS will require significant changes to one of the most complex and security-critical systems on the Internet: the Web Public-Key Infrastructure (WebPKI).&lt;/p&gt;
      &lt;p&gt;The central problem is the sheer size of these new algorithms: signatures for ML-DSA-44, one of the most performant PQ algorithms standardized by NIST, are 2,420 bytes long, compared to just 64 bytes for ECDSA-P256, the most popular non-PQ signature in use today; and its public keys are 1,312 bytes long, compared to just 64 bytes for ECDSA. That's a roughly 20-fold increase in size. Worse yet, the average TLS handshake includes a number of public keys and signatures, adding up to 10s of kilobytes of overhead per handshake. This is enough to have a noticeable impact on the performance of TLS.&lt;/p&gt;
      &lt;p&gt;That makes drop-in PQ certificates a tough sell to enable today: they donât bring any security benefit before Q-day â the day a cryptographically relevant quantum computer arrives â but they do degrade performance. We could sit and wait until Q-day is a year away, but thatâs playing with fire. Migrations always take longer than expected, and by waiting we risk the security and privacy of the Internet, which is dear to us.&lt;/p&gt;
      &lt;p&gt;It's clear that we must find a way to make post-quantum certificates cheap enough to deploy today by default for everyone â not just those that can afford it. In this post, we'll introduce you to the plan weâve brought together with industry partners to the IETF to redesign the WebPKI in order to allow a smooth transition to PQ authentication with no performance impact (and perhaps a performance improvement!). We'll provide an overview of one concrete proposal, called Merkle Tree Certificates (MTCs), whose goal is to whittle down the number of public keys and signatures in the TLS handshake to the bare minimum required.&lt;/p&gt;
      &lt;p&gt;But talk is cheap. We know from experience that, as with any change to the Internet, it's crucial to test early and often. Today we're announcing our intent to deploy MTCs on an experimental basis in collaboration with Chrome Security. In this post, we'll describe the scope of this experiment, what we hope to learn from it, and how we'll make sure it's done safely.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;The WebPKI today â an old system with many patches&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Why does the TLS handshake have so many public keys and signatures?&lt;/p&gt;
      &lt;p&gt;Let's start with Cryptography 101. When your browser connects to a website, it asks the server to authenticate itself to make sure it's talking to the real server and not an impersonator. This is usually achieved with a cryptographic primitive known as a digital signature scheme (e.g., ECDSA or ML-DSA). In TLS, the server signs the messages exchanged between the client and server using its secret key, and the client verifies the signature using the server's public key. In this way, the server confirms to the client that they've had the same conversation, since only the server could have produced a valid signature.&lt;/p&gt;
      &lt;p&gt;If the client already knows the server's public key, then only 1 signature is required to authenticate the server. In practice, however, this is not really an option. The web today is made up of around a billion TLS servers, so it would be unrealistic to provision every client with the public key of every server. What's more, the set of public keys will change over time as new servers come online and existing ones rotate their keys, so we would need some way of pushing these changes to clients.&lt;/p&gt;
      &lt;p&gt;This scaling problem is at the heart of the design of all PKIs.&lt;/p&gt;
      &lt;p&gt;Instead of expecting the client to know the server's public key in advance, the server might just send its public key during the TLS handshake. But how does the client know that the public key actually belongs to the server? This is the job of a certificate.&lt;/p&gt;
      &lt;p&gt;A certificate binds a public key to the identity of the server â usually its DNS name, e.g., &lt;code&gt;cloudflareresearch.com&lt;/code&gt;. The certificate is signed by a Certification Authority (CA) whose public key is known to the client. In addition to verifying the server's handshake signature, the client verifies the signature of this certificate. This establishes a chain of trust: by accepting the certificate, the client is trusting that the CA verified that the public key actually belongs to the server with that identity.&lt;/p&gt;
      &lt;p&gt;Clients are typically configured to trust many CAs and must be provisioned with a public key for each. Things are much easier however, since there are only 100s of CAs instead of billions. In addition, new certificates can be created without having to update clients.&lt;/p&gt;
      &lt;p&gt;These efficiencies come at a relatively low cost: for those counting at home, that's +1 signature and +1 public key, for a total of 2 signatures and 1 public key per TLS handshake.&lt;/p&gt;
      &lt;p&gt;That's not the end of the story, however. As the WebPKI has evolved, so have these chains of trust grown a bit longer. These days it's common for a chain to consist of two or more certificates rather than just one. This is because CAs sometimes need to rotate their keys, just as servers do. But before they can start using the new key, they must distribute the corresponding public key to clients. This takes time, since it requires billions of clients to update their trust stores. To bridge the gap, the CA will sometimes use the old key to issue a certificate for the new one and append this certificate to the end of the chain.&lt;/p&gt;
      &lt;p&gt;That's +1 signature and +1 public key, which brings us to 3 signatures and 2 public keys. And we still have a little ways to go.&lt;/p&gt;
      &lt;p&gt;The main job of a CA is to verify that a server has control over the domain for which itâs requesting a certificate. This process has evolved over the years from a high-touch, CA-specific process to a standardized, mostly automated process used for issuing most certificates on the web. (Not all CAs fully support automation, however.) This evolution is marked by a number of security incidents in which a certificate was mis-issued to a party other than the server, allowing that party to impersonate the server to any client that trusts the CA.&lt;/p&gt;
      &lt;p&gt;Automation helps, but attacks are still possible, and mistakes are almost inevitable. Earlier this year, several certificates for Cloudflare's encrypted 1.1.1.1 resolver were issued without our involvement or authorization. This apparently occurred by accident, but it nonetheless put users of 1.1.1.1 at risk. (The mis-issued certificates have since been revoked.)&lt;/p&gt;
      &lt;p&gt;Ensuring mis-issuance is detectable is the job of the Certificate Transparency (CT) ecosystem. The basic idea is that each certificate issued by a CA gets added to a public log. Servers can audit these logs for certificates issued in their name. If ever a certificate is issued that they didn't request itself, the server operator can prove the issuance happened, and the PKI ecosystem can take action to prevent the certificate from being trusted by clients.&lt;/p&gt;
      &lt;p&gt;Major browsers, including Firefox and Chrome and its derivatives, require certificates to be logged before they can be trusted. For example, Chrome, Safari, and Firefox will only accept the server's certificate if it appears in at least two logs the browser is configured to trust. This policy is easy to state, but tricky to implement in practice:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Operating a CT log has historically been fairly expensive. Logs ingest billions of certificates over their lifetimes: when an incident happens, or even just under high load, it can take some time for a log to make a new entry available for auditors.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Clients can't really audit logs themselves, since this would expose their browsing history (i.e., the servers they wanted to connect to) to the log operators.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;The solution to both problems is to include a signature from the CT log along with the certificate. The signature is produced immediately in response to a request to log a certificate, and attests to the log's intent to include the certificate in the log within 24 hours.&lt;/p&gt;
      &lt;p&gt;Per browser policy, certificate transparency adds +2 signatures to the TLS handshake, one for each log. This brings us to a total of 5 signatures and 2 public keys in a typical handshake on the public web.&lt;/p&gt;
      &lt;p&gt;The WebPKI is a living, breathing, and highly distributed system. We've had to patch it a number of times over the years to keep it going, but on balance it has served our needs quite well â until now.&lt;/p&gt;
      &lt;p&gt;Previously, whenever we needed to update something in the WebPKI, we would tack on another signature. This strategy has worked because conventional cryptography is so cheap. But 5 signatures and 2 public keys on average for each TLS handshake is simply too much to cope with for the larger PQ signatures that are coming.&lt;/p&gt;
      &lt;p&gt;The good news is that by moving what we already have around in clever ways, we can drastically reduce the number of signatures we need.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Crash course on Merkle Tree Certificates&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Merkle Tree Certificates (MTCs) is a proposal for the next generation of the WebPKI that we are implementing and plan to deploy on an experimental basis. Its key features are as follows:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;All the information a client needs to validate a Merkle Tree Certificate can be disseminated out-of-band. If the client is sufficiently up-to-date, then the TLS handshake needs just 1 signature, 1 public key, and 1 Merkle tree inclusion proof. This is quite small, even if we use post-quantum algorithms.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The MTC specification makes certificate transparency a first class feature of the PKI by having each CA run its own log of exactly the certificates they issue.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Let's poke our head under the hood a little. Below we have an MTC generated by one of our internal tests. This would be transmitted from the server to the client in the TLS handshake:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;-----BEGIN CERTIFICATE-----
MIICSzCCAUGgAwIBAgICAhMwDAYKKwYBBAGC2ksvADAcMRowGAYKKwYBBAGC2ksv
AQwKNDQzNjMuNDguMzAeFw0yNTEwMjExNTMzMjZaFw0yNTEwMjgxNTMzMjZaMCEx
HzAdBgNVBAMTFmNsb3VkZmxhcmVyZXNlYXJjaC5jb20wWTATBgcqhkjOPQIBBggq
hkjOPQMBBwNCAARw7eGWh7Qi7/vcqc2cXO8enqsbbdcRdHt2yDyhX5Q3RZnYgONc
JE8oRrW/hGDY/OuCWsROM5DHszZRDJJtv4gno2wwajAOBgNVHQ8BAf8EBAMCB4Aw
EwYDVR0lBAwwCgYIKwYBBQUHAwEwQwYDVR0RBDwwOoIWY2xvdWRmbGFyZXJlc2Vh
cmNoLmNvbYIgc3RhdGljLWN0LmNsb3VkZmxhcmVyZXNlYXJjaC5jb20wDAYKKwYB
BAGC2ksvAAOB9QAAAAAAAAACAAAAAAAAAAJYAOBEvgOlvWq38p45d0wWTPgG5eFV
wJMhxnmDPN1b5leJwHWzTOx1igtToMocBwwakt3HfKIjXYMO5CNDOK9DIKhmRDSV
h+or8A8WUrvqZ2ceiTZPkNQFVYlG8be2aITTVzGuK8N5MYaFnSTtzyWkXP2P9nYU
Vd1nLt/WjCUNUkjI4/75fOalMFKltcc6iaXB9ktble9wuJH8YQ9tFt456aBZSSs0
cXwqFtrHr973AZQQxGLR9QCHveii9N87NXknDvzMQ+dgWt/fBujTfuuzv3slQw80
mibA021dDCi8h1hYFQAA
-----END CERTIFICATE-----&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Looks like your average PEM encoded certificate. Let's decode it and look at the parameters:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;$ openssl x509 -in merkle-tree-cert.pem -noout -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 531 (0x213)
        Signature Algorithm: 1.3.6.1.4.1.44363.47.0
        Issuer: 1.3.6.1.4.1.44363.47.1=44363.48.3
        Validity
            Not Before: Oct 21 15:33:26 2025 GMT
            Not After : Oct 28 15:33:26 2025 GMT
        Subject: CN=cloudflareresearch.com
        Subject Public Key Info:
            Public Key Algorithm: id-ecPublicKey
                Public-Key: (256 bit)
                pub:
                    04:70:ed:e1:96:87:b4:22:ef:fb:dc:a9:cd:9c:5c:
                    ef:1e:9e:ab:1b:6d:d7:11:74:7b:76:c8:3c:a1:5f:
                    94:37:45:99:d8:80:e3:5c:24:4f:28:46:b5:bf:84:
                    60:d8:fc:eb:82:5a:c4:4e:33:90:c7:b3:36:51:0c:
                    92:6d:bf:88:27
                ASN1 OID: prime256v1
                NIST CURVE: P-256
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature
            X509v3 Extended Key Usage:
                TLS Web Server Authentication
            X509v3 Subject Alternative Name:
                DNS:cloudflareresearch.com, DNS:static-ct.cloudflareresearch.com
    Signature Algorithm: 1.3.6.1.4.1.44363.47.0
    Signature Value:
        00:00:00:00:00:00:02:00:00:00:00:00:00:00:02:58:00:e0:
        44:be:03:a5:bd:6a:b7:f2:9e:39:77:4c:16:4c:f8:06:e5:e1:
        55:c0:93:21:c6:79:83:3c:dd:5b:e6:57:89:c0:75:b3:4c:ec:
        75:8a:0b:53:a0:ca:1c:07:0c:1a:92:dd:c7:7c:a2:23:5d:83:
        0e:e4:23:43:38:af:43:20:a8:66:44:34:95:87:ea:2b:f0:0f:
        16:52:bb:ea:67:67:1e:89:36:4f:90:d4:05:55:89:46:f1:b7:
        b6:68:84:d3:57:31:ae:2b:c3:79:31:86:85:9d:24:ed:cf:25:
        a4:5c:fd:8f:f6:76:14:55:dd:67:2e:df:d6:8c:25:0d:52:48:
        c8:e3:fe:f9:7c:e6:a5:30:52:a5:b5:c7:3a:89:a5:c1:f6:4b:
        5b:95:ef:70:b8:91:fc:61:0f:6d:16:de:39:e9:a0:59:49:2b:
        34:71:7c:2a:16:da:c7:af:de:f7:01:94:10:c4:62:d1:f5:00:
        87:bd:e8:a2:f4:df:3b:35:79:27:0e:fc:cc:43:e7:60:5a:df:
        df:06:e8:d3:7e:eb:b3:bf:7b:25:43:0f:34:9a:26:c0:d3:6d:
        5d:0c:28:bc:87:58:58:15:00:00&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;While some of the parameters probably look familiar, others will look unusual. On the familiar side, the subject and public key are exactly what we might expect: the DNS name is &lt;code&gt;cloudflareresearch.com&lt;/code&gt; and the public key is for a familiar signature algorithm, ECDSA-P256. This algorithm is not PQ, of course â in the future we would put ML-DSA-44 there instead.&lt;/p&gt;
      &lt;p&gt;On the unusual side, OpenSSL appears to not recognize the signature algorithm of the issuer and just prints the raw OID and bytes of the signature. There's a good reason for this: the MTC does not have a signature in it at all! So what exactly are we looking at?&lt;/p&gt;
      &lt;p&gt;The trick to leave out signatures is that a Merkle Tree Certification Authority (MTCA) produces its signatureless certificates in batches rather than individually. In place of a signature, the certificate has an inclusion proof of the certificate in a batch of certificates signed by the MTCA.&lt;/p&gt;
      &lt;p&gt;To understand how inclusion proofs work, let's think about a slightly simplified version of the MTC specification. To issue a batch, the MTCA arranges the unsigned certificates into a data structure called a Merkle tree that looks like this:&lt;/p&gt;
      &lt;p&gt;Each leaf of the tree corresponds to a certificate, and each inner node is equal to the hash of its children. To sign the batch, the MTCA uses its secret key to sign the head of the tree. The structure of the tree guarantees that each certificate in the batch was signed by the MTCA: if we tried to tweak the bits of any one of the certificates, the treehead would end up having a different value, which would cause the signature to fail.&lt;/p&gt;
      &lt;p&gt;An inclusion proof for a certificate consists of the hash of each sibling node along the path from the certificate to the treehead:&lt;/p&gt;
      &lt;p&gt;Given a validated treehead, this sequence of hashes is sufficient to prove inclusion of the certificate in the tree. This means that, in order to validate an MTC, the client also needs to obtain the signed treehead from the MTCA.&lt;/p&gt;
      &lt;p&gt;This is the key to MTC's efficiency:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Signed treeheads can be disseminated to clients out-of-band and validated offline. Each validated treehead can then be used to validate any certificate in the corresponding batch, eliminating the need to obtain a signature for each server certificate.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;During the TLS handshake, the client tells the server which treeheads it has. If the server has a signatureless certificate covered by one of those treeheads, then it can use that certificate to authenticate itself. That's 1 signature,1 public key and 1 inclusion proof per handshake, both for the server being authenticated.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Now, that's the simplified version. MTC proper has some more bells and whistles. To start, it doesnât create a separate Merkle tree for each batch, but it grows a single large tree, which is used for better transparency. As this tree grows, periodically (sub)tree heads are selected to be shipped to browsers, which we call landmarks. In the common case browsers will be able to fetch the most recent landmarks, and servers can wait for batch issuance, but we need a fallback: MTC also supports certificates that can be issued immediately and donât require landmarks to be validated, but these are not as small. A server would provision both types of Merkle tree certificates, so that the common case is fast, and the exceptional case is slow, but at least itâll work.&lt;/p&gt;
      &lt;p&gt;Ever since early designs for MTCs emerged, weâve been eager to experiment with the idea. In line with the IETF principle of ârunning codeâ, it often takes implementing a protocol to work out kinks in the design. At the same time, we cannot risk the security of users. In this section, we describe our approach to experimenting with aspects of the Merkle Tree Certificates design without changing any trust relationships.&lt;/p&gt;
      &lt;p&gt;Letâs start with what we hope to learn. We have lots of questions whose answers can help to either validate the approach, or uncover pitfalls that require reshaping the protocol â in fact, an implementation of an early MTC draft by Maximilian Pohl and Mia Celeste did exactly this. Weâd like to know:&lt;/p&gt;
      &lt;p&gt;What breaks? Protocol ossification (the tendency of implementation bugs to make it harder to change a protocol) is an ever-present issue with deploying protocol changes. For TLS in particular, despite having built-in flexibility, time after time weâve found that if that flexibility is not regularly used, there will be buggy implementations and middleboxes that break when they see things they donât recognize. TLS 1.3 deployment took years longer than we hoped for this very reason. And more recently, the rollout of PQ key exchange in TLS caused the Client Hello to be split over multiple TCP packets, something that many middleboxes weren't ready for.&lt;/p&gt;
      &lt;p&gt;What is the performance impact? In fact, we expect MTCs to reduce the size of the handshake, even compared to today's non-PQ certificates. They will also reduce CPU cost: ML-DSA signature verification is about as fast as ECDSA, and there will be far fewer signatures to verify. We therefore expect to see a reduction in latency. We would like to see if there is a measurable performance improvement.&lt;/p&gt;
      &lt;p&gt;What fraction of clients will stay up to date? Getting the performance benefit of MTCs requires the clients and servers to be roughly in sync with one another. We expect MTCs to have fairly short lifetimes, a week or so. This means that if the client's latest landmark is older than a week, the server would have to fallback to a larger certificate. Knowing how often this fallback happens will help us tune the parameters of the protocol to make fallbacks less likely.&lt;/p&gt;
      &lt;p&gt;In order to answer these questions, we are implementing MTC support in our TLS stack and in our certificate issuance infrastructure. For their part, Chrome is implementing MTC support in their own TLS stack and will stand up infrastructure to disseminate landmarks to their users.&lt;/p&gt;
      &lt;p&gt;As we've done in past experiments, we plan to enable MTCs for a subset of our free customers with enough traffic that we will be able to get useful measurements. Chrome will control the experimental rollout: they can ramp up slowly, measuring as they go and rolling back if and when bugs are found.&lt;/p&gt;
      &lt;p&gt;Which leaves us with one last question: who will run the Merkle Tree CA?&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Bootstrapping trust from the existing WebPKI&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Standing up a proper CA is no small task: it takes years to be trusted by major browsers. Thatâs why Cloudflare isnât going to become a ârealâ CA for this experiment, and Chrome isnât going to trust us directly.&lt;/p&gt;
      &lt;p&gt;Instead, to make progress on a reasonable timeframe, without sacrificing due diligence, we plan to "mock" the role of the MTCA. We will run an MTCA (on Workers based on our StaticCT logs), but for each MTC we issue, we also publish an existing certificate from a trusted CA that agrees with it. We call this the bootstrap certificate. When Chromeâs infrastructure pulls updates from our MTCA log, they will also pull these bootstrap certificates, and check whether they agree. Only if they do, theyâll proceed to push the corresponding landmarks to Chrome clients. In other words, Cloudflare is effectively just âre-encodingâ an existing certificate (with domain validation performed by a trusted CA) as an MTC, and Chrome is using certificate transparency to keep us honest.&lt;/p&gt;
      &lt;p&gt;With almost 50% of our traffic already protected by post-quantum encryption, weâre halfway to a fully post-quantum secure Internet. The second part of our journey, post-quantum certificates, is the hardest yet though. A simple drop-in upgrade has a noticeable performance impact and no security benefit before Q-day. This means itâs a hard sell to enable today by default. But here we are playing with fire: migrations always take longer than expected. If we want to keep an ubiquitously private and secure Internet, we need a post-quantum solution thatâs performant enough to be enabled by default today.&lt;/p&gt;
      &lt;p&gt;Merkle Tree Certificates (MTCs) solves this problem by reducing the number of signatures and public keys to the bare minimum while maintaining the WebPKI's essential properties. We plan to roll out MTCs to a fraction of free accounts by early next year. This does not affect any visitors that are not part of the Chrome experiment. For those that are, thanks to the bootstrap certificates, there is no impact on security.&lt;/p&gt;
      &lt;p&gt;Weâre excited to keep the Internet fast and secure, and will report back soon on the results of this experiment: watch this space! MTC is evolving as we speak, if you want to get involved, please join the IETF PLANTS mailing list.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/bootstrap-mtc/"/><published>2025-10-28T22:39:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45742419</id><title>Tips for stroke-surviving software engineers</title><updated>2025-10-29T13:47:55.656885+00:00</updated><content>&lt;doc fingerprint="f62a9b5785b1fadc"&gt;
  &lt;main&gt;
    &lt;p&gt;2025-10-29&lt;/p&gt;
    &lt;p&gt;This is a pretty niche topic; I don't imagine there are many of us out there.&lt;/p&gt;
    &lt;p&gt;Actually, to be strict, I'd say this advice is tailored to people who've had hemorrhagic stroke in the parietal lobe with residual epilepsy...&lt;/p&gt;
    &lt;p&gt;I was 29 and around 12 years into my career when it all happened, and in the six years since then I've had time to learn a bit more about my new self.&lt;/p&gt;
    &lt;p&gt;The first tip is to just stop. Fatigue, fuzziness, nausea, or affected-sided weird sensations are non-negotiable stop signals. So go lie down, hydrate, reset. Close your eyes and think about the cottage or lonely mountain you want to retire to. Escape the overwhelming mental or physical space.&lt;/p&gt;
    &lt;p&gt;HEADPHONES, blinders, and 'No'. Eliminate unwanted inputs at the earliest point of entry. Work from home or environments where you can control most variables. Routes of escape and rest are important.&lt;/p&gt;
    &lt;p&gt;Health above performance every single time. Metrics and productivity be damned. Self-advocate, and all that. Reject with directness any demands made of you that cross the threshold.&lt;/p&gt;
    &lt;p&gt;Laws. Use them. You don't have to rely on good behaviour and kindness. You are, depending on your location, usually protected by all types of anti-discrimination legislation, implicit and explicit. Use your employee assistance programs too.&lt;/p&gt;
    &lt;p&gt;Single-thread it all! Less context switching. Batch your work, finish one thing, then move to the next. Externalize working-memory. Use notebooks, whiteboards, and lists instead of juggling state in your head. I am not good at this, and over-stretch my brain, leading to auras, overwhelm, and general sickness. Terrible idea.&lt;/p&gt;
    &lt;p&gt;Related: Sssh to the AI naysayers. Use it as your help and scratchpad. Let it hold state so your brain can judge rather than store and needlessly cogitate on stuff. You don't have to do this alone out of some purity fetishism. You, too, have a limited context window. Sorry!&lt;/p&gt;
    &lt;p&gt;Do the heavy thinking in your peak window (for me, that's the morning); push everything else to later. Spend your time more carefully than your money.&lt;/p&gt;
    &lt;p&gt;Pick the route of least attention. Attention is expensive, and rarely needed as much as we think it is. It's a heavy toll to pay. Unless you're in an ops or monitoring role, you don't need to be synchronously active. DISABLE NOTIFICATIONS.&lt;/p&gt;
    &lt;p&gt;AVOID long meetings. Emails are good. Oh god am I bad at this? YES, I like people so I like some meetings, but communicating is so so expensive. Being polite is also expensive; It's not nice to have to tell people they're draining you.&lt;/p&gt;
    &lt;p&gt;I think that's mostly it. I'm still working on this stuff. And would probably grade myself pretty poorly. One day I'll be better at saying no, at advocating for myself, and knowing how to navigate the disappointment of others.&lt;/p&gt;
    &lt;p&gt;Footnote &amp;amp; some casual research: If you're into this, here's some stuff I found out related to my specific injury location and how it might apply to my work. This was gathered with help from gemini when I was struggling with left-arm and eye prodromes after long coding sessions:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Frontal and parietal cortices form a flexible control system that holds goals, routes attention, and updates task sets; this "multiple-demand" network scales with task complexity and underpins how we store, manipulate, and decide on information during work[1][2][3]. Superior parietal cortex is especially taxed when we transform or reorganize information in working memory rather than simply maintain it, which is why mental navigations, refactors, and other transformations feel costly[4][5]. Frequent context switches recruit lateral prefrontal and parietal regions and increase control load, so hopping between threads repeatedly spikes demand on this same circuitry[6][7]. After AVM resection (what I had!) or stroke generally, tissue near the lesion can remain hyperexcitable with impaired neurovascular coupling; heavy cognitive load lowers seizure threshold and can produce somatosensory auras and body-image distortions from parietal cortex[8][9][10].&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Thanks for reading :) Tonnes of love to all the stroke survivors out there &amp;lt;3&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.j11y.io/2025-10-29_stroke_tips_for_engineers/"/><published>2025-10-29T03:51:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45742446</id><title>uBlock Origin Lite Apple App Store</title><updated>2025-10-29T13:47:55.215010+00:00</updated><content>&lt;doc fingerprint="b62ad8dff7b5baa7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;uBlock Origin Lite 4+&lt;/head&gt;
    &lt;head rend="h2"&gt;An efficient content blocker&lt;/head&gt;
    &lt;head rend="h2"&gt;Raymond Hill&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Free&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Screenshots&lt;/head&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;uBO Lite (uBOL) is a reliable and efficient content blocker.&lt;lb/&gt;The default ruleset corresponds to uBlock Origin's default filterset:&lt;lb/&gt;- uBlock Origin's built-in filter lists&lt;lb/&gt;- EasyList&lt;lb/&gt;- EasyPrivacy&lt;lb/&gt;- Peter Loweâs Ad and tracking server list&lt;lb/&gt;You can enable more rulesets by visiting the options page -- click the _Cogs_ icon in the popup panel.&lt;lb/&gt;uBOL is entirely declarative, meaning there is no need for a permanent uBOL process for the filtering to occur, and CSS/JS injection-based content filtering is performed reliably by the browser itself rather than by the extension. This means that uBOL itself does not consume CPU/memory resources while content blocking is ongoing -- uBOL's service worker process is required _only_ when you interact with the popup panel or the option pages.&lt;/p&gt;
    &lt;head rend="h2"&gt;Whatâs New&lt;/head&gt;
    &lt;p&gt;Version 2025.1019.1656&lt;/p&gt;
    &lt;p&gt;â¢ Automatically select optimal for newly allowed hosts&lt;lb/&gt;â¢ Updated filter lists&lt;/p&gt;
    &lt;head rend="h2"&gt;Ratings and Reviews&lt;/head&gt;
    &lt;head rend="h3"&gt;The best content blocker is finally on iPadOS!!&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Itâs was a really long wait, but finally we are able to use it directly on the iPad. The first TestFlight version had a big battery drain, but itâs better now on the official release. The only limitation is that we canât add our own lists, but I am fine with the default lists ans it works perfect.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;I was looking for this long time finally itâs here.&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;I love block. I do not want to use chrome. Itâs perfect. I can use this in safari..&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Finally ð¤©&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Waiting for many years. Added in all apple devices. Working properly. ðð&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;App Privacy&lt;/head&gt;
    &lt;p&gt;The developer, Raymond Hill, indicated that the appâs privacy practices may include handling of data as described below. For more information, see the developerâs privacy policy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Data Not Collected&lt;/head&gt;
    &lt;p&gt;The developer does not collect any data from this app.&lt;/p&gt;
    &lt;p&gt;Privacy practices may vary based on, for example, the features you use or your age. LearnÂ More&lt;/p&gt;
    &lt;head rend="h2"&gt;Information&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Provider&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Raymond Hill&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Size&lt;/item&gt;
      &lt;item rend="dd-2"&gt;6 MB&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Category&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Utilities&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Compatibility&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-5"&gt;iPhone&lt;/item&gt;
          &lt;item rend="dd-5"&gt;Requires iOS 18.5 or later.&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-6"&gt;iPad&lt;/item&gt;
          &lt;item rend="dd-6"&gt;Requires iPadOS 18.5 or later.&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-7"&gt;Mac&lt;/item&gt;
          &lt;item rend="dd-7"&gt;Requires macOS 13.5 or later.&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-8"&gt;Apple Vision&lt;/item&gt;
          &lt;item rend="dd-8"&gt;Requires visionOS 2.5 or later.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-9"&gt;Languages&lt;/item&gt;
      &lt;item rend="dd-9"&gt;
        &lt;p&gt;English&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-10"&gt;Age Rating&lt;/item&gt;
      &lt;item rend="dd-11"&gt;Learn More&lt;/item&gt;
      &lt;item rend="dt-12"&gt;Copyright&lt;/item&gt;
      &lt;item rend="dd-12"&gt;Â© Raymond Hill 2025&lt;/item&gt;
      &lt;item rend="dt-13"&gt;Price&lt;/item&gt;
      &lt;item rend="dd-13"&gt;Free&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://apps.apple.com/in/app/ublock-origin-lite/id6745342698"/><published>2025-10-29T03:57:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45742488</id><title>Keep Android Open</title><updated>2025-10-29T13:47:55.025502+00:00</updated><content>&lt;doc fingerprint="9da6d2e399ba52e9"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Keep Android Open&lt;/head&gt;
      &lt;p&gt;In August 2025, Google announced that starting next year, it will no longer be possible to develop apps for the Android platform without first registering centrally with Google.&lt;/p&gt;
      &lt;p&gt;This registration will involve:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Paying a fee to Google&lt;/item&gt;
        &lt;item&gt;Agreeing to Google’s Terms and Conditions&lt;/item&gt;
        &lt;item&gt;Providing government identification&lt;/item&gt;
        &lt;item&gt;Uploading evidence of an app’s private signing key&lt;/item&gt;
        &lt;item&gt;Listing all current and future application identifiers&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Some actions you can take to help oppose the enactment of this policy are:&lt;/p&gt;
      &lt;head rend="h2"&gt;Sign the Open Letter&lt;/head&gt;
      &lt;head rend="h2"&gt;European Union&lt;/head&gt;
      &lt;head rend="h2"&gt;United States&lt;/head&gt;
      &lt;head rend="h2"&gt;United Kingdom&lt;/head&gt;
      &lt;head rend="h2"&gt;Brazil&lt;/head&gt;
      &lt;head rend="h2"&gt;Other&lt;/head&gt;
      &lt;head rend="h2"&gt;References&lt;/head&gt;
      &lt;head rend="h3"&gt;Overview&lt;/head&gt;
      &lt;head rend="h3"&gt;Press Reactions&lt;/head&gt;
      &lt;head rend="h3"&gt;Video Responses&lt;/head&gt;
      &lt;head rend="h3"&gt;Editorials and Blogs&lt;/head&gt;
      &lt;head rend="h3"&gt;Discussions&lt;/head&gt;
      &lt;head rend="h3"&gt;Official Documentation&lt;/head&gt;
      &lt;head rend="h3"&gt;Miscellaneous&lt;/head&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://keepandroidopen.org/"/><published>2025-10-29T04:03:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45742616</id><title>Wacl – A Tcl Distribution for WebAssembly</title><updated>2025-10-29T13:47:54.426968+00:00</updated><content>&lt;doc fingerprint="9ecd0e53a7e724a0"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a Tcl distribution for WebAssembly (webassembly.org). It enables Web developers to embed a Tcl interpreter in the browser and integrate Tcl with JavaScript. It enables Tcl developers to use their tools and language of choice to create client side web applications. It enables all developers to reuse a great and (over decades) grown code base of useful packages and scripts, such as Tcllib, to be used in web browsers.&lt;/p&gt;
    &lt;p&gt;It is an extension of the Emtcl project from Aidan Hobsen, which can be found here. But Wacl takes things a few steps further: it integrates a fully featured Tcl interpreter into the webpage and adds the following features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A main tclsh interpreter and capability to get it via JavaScript&lt;/item&gt;
      &lt;item&gt;An event loop to process all Tcl events (timer events, fileevents, custom events)&lt;/item&gt;
      &lt;item&gt;Client sockets. The socket -async ... command connects to websocket servers with the binary protocol. The resulting handle can be used to transmit binary data as with normal TCP sockets.&lt;/item&gt;
      &lt;item&gt;The Tcl library: modules and packages in the Emscripten virtual filesystem. You can add your own packages!&lt;/item&gt;
      &lt;item&gt;Proper initialization via Tcl_Init()&lt;/item&gt;
      &lt;item&gt;An extension to call javascript functions from Tcl&lt;/item&gt;
      &lt;item&gt;various useful extensions (see below for a list and comments)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The original illustrative dom command has been moved to the "wacl" namespace in the package of same name, which is available right at startup. This package contains also a command ::wacl::jscall to call javascript functions from Tcl which have been registered before via the jswrap() module function.&lt;/p&gt;
    &lt;p&gt;The code compiles fine with Emscripten 1.37.9 to JavaScript and WebAssembly. The latter is the preferred format: WebAssembly is only half the size of the JavaScript "asm.js" output (~1.4MB vs. 2.9MB) and at least twice as fast! However, that could induce incompatibilities with older browsers, which don't (yet) support WebAssembly.&lt;/p&gt;
    &lt;p&gt;The following extensions are included in Wacl&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;wacl native extension with commands wacl::dom and wacl::jscall&lt;/item&gt;
      &lt;item&gt;tDOM for parsing and creating XML and HTML content&lt;/item&gt;
      &lt;item&gt;json and json::write from tcllib&lt;/item&gt;
      &lt;item&gt;html from tcllib&lt;/item&gt;
      &lt;item&gt;javascript from tcllib&lt;/item&gt;
      &lt;item&gt;ncgi as dependency for html&lt;/item&gt;
      &lt;item&gt;rl_json A Tcl_Obj type for efficient JSON parsing and generation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More extensions can easily be included and used. C extensions can be compiled with Emscripten (with USE_TCL_STUBS disabled and statically initialized via waclAppInit()) and Tcl extensions can be included in the library virtual filesystem.&lt;/p&gt;
    &lt;p&gt;But be aware that including extensions is a tradeoff: for the additional functionality you pay with a larger download size. The really useful tDOM extension for instance increases the Wacl distribution by not less than 400kB, which must be downloaded to the users client when (s)he wants to run a wacl based application, and this can be painful with lower bandwidth. Thus it is better to limit the number of packages to what is necessary rather than to build a batteries included distribution which contains everything.&lt;/p&gt;
    &lt;p&gt;You can try it out here. You can download the precompiled version with the index page to play on your own webpage by downloading the precompiled binary from here. Both of these pages require a recent browser with webassembly support:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mozilla Firefox &amp;gt;= 52.0&lt;/item&gt;
      &lt;item&gt;Google Chrome &amp;gt;= 57.0&lt;/item&gt;
      &lt;item&gt;Microsoft Edge (Windows 10 "Creators" update)&lt;/item&gt;
      &lt;item&gt;Opera&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wacl will compile on a Unix/Linux environment with the following tools installed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the Emscripten SDK. Installation is documented on its web page&lt;/item&gt;
      &lt;item&gt;make, autoconf&lt;/item&gt;
      &lt;item&gt;diff, patch (some patches to the original sources must be applied, this is done mostly automatically)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Windows is not supported, but macOS with the appropriate tools from MacPorts will probably work (not tested by myself).&lt;/p&gt;
    &lt;p&gt;First step is to checkout this repository.This will checkout the files in the current directory. There is a Makefile with the build steps and a README with instructione. The make procedure does merely download the tcl core sources, apply a small patch and configure &amp;amp; build the interpreter to a webassembly plus accompanying .data + .js files. These files can be deployed to the corresponding web source directories. The Emscripten SDK must be on the PATH (i.e. via source $EMSCRIPTEN/emsdk_set_env.sh). Once wacl is built, it can be used in any browser which supports webassembly, also on Windows.&lt;lb/&gt; The build system can be changed to produce javascript instead of webassembly, by simply removing the -s WASM=1 flag from the BCFLAGS variable in the Makefile. This will generate a larger (~2.8MB), yet minified .js output, which is slower at runtime, but compatible with browsers that don't support webassembly.&lt;/p&gt;
    &lt;p&gt;To build it, you need the emscripten sdk on your path. Then:&lt;/p&gt;
    &lt;code&gt;$ make waclprep  # One off prep - tcl-core download, hacks.patch application and autoconf
$ make config    # create build directory and run emconfigure tcl/unix/configure
$ make [all]     # create the library and emtcl.js
$ make install   # copy emtcl.js to ../www/js/
&lt;/code&gt;
    &lt;p&gt;If you want to totally reset all build files in ./tcl/ and start again:&lt;/p&gt;
    &lt;code&gt;$ make reset
&lt;/code&gt;
    &lt;p&gt;This removes all changes and untracked files in there, so be careful!&lt;/p&gt;
    &lt;p&gt;There is a target to recreate the patch, if you changed anything important in tcl/&lt;/p&gt;
    &lt;code&gt;$ make patch
&lt;/code&gt;
    &lt;p&gt;It downloads tcl-core (it not already present), extracts it and runs diff between it and tcl/. The result is the patch that is applied above via "make tclprep"&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ecky-l/wacl"/><published>2025-10-29T04:25:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45742907</id><title>Who needs Graphviz when you can build it yourself?</title><updated>2025-10-29T13:47:54.160204+00:00</updated><content>&lt;doc fingerprint="af4c3a680161d9ab"&gt;
  &lt;main&gt;
    &lt;p&gt;We recently overhauled our internal tools for visualizing the compilation of JavaScript and WebAssembly. When SpiderMonkey’s optimizing compiler, Ion, is active, we can now produce interactive graphs showing exactly how functions are processed and optimized.&lt;/p&gt;
    &lt;p&gt;You can play with these graphs right here on this page. Simply write some JavaScript code in the &lt;code&gt;test&lt;/code&gt; function and see what graph is produced. You can click and drag to navigate, ctrl-scroll to zoom, and drag the slider at the bottom to scrub through the optimization process.&lt;/p&gt;
    &lt;p&gt;As you experiment, take note of how stable the graph layout is, even as the sizes of blocks change or new structures are added. Try clicking a block's title to select it, then drag the slider and watch the graph change while the block remains in place. Or, click an instruction's number to highlight it so you can keep an eye on it across passes.&lt;/p&gt;
    &lt;p&gt;We are not the first to visualize our compiler’s internal graphs, of course, nor the first to make them interactive. But I was not satisfied with the output of common tools like Graphviz or Mermaid, so I decided to create a layout algorithm specifically tailored to our needs. The resulting algorithm is simple, fast, produces surprisingly high-quality output, and can be implemented in less than a thousand lines of code. The purpose of this article is to walk you through this algorithm and the design concepts behind it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;As readers of this blog already know, SpiderMonkey has several tiers of execution for JavaScript and WebAssembly code. The highest tier is known as Ion, an optimizing SSA compiler that takes the most time to compile but produces the highest-quality output.&lt;/p&gt;
    &lt;p&gt;Working with Ion frequently requires us to visualize and debug the SSA graph. Since 2011 we have used a tool for this purpose called iongraph, built by Sean Stangl. It is a simple Python script that takes a JSON dump of our compiler graphs and uses Graphviz to produce a PDF. It is perfectly adequate, and very much the status quo for compiler authors, but unfortunately the Graphviz output has many problems that make our work tedious and frustrating.&lt;/p&gt;
    &lt;p&gt;The first problem is that the Graphviz output rarely bears any resemblance to the source code that produced it. Graphviz will place nodes wherever it feels will minimize error, resulting in a graph that snakes left and right seemingly at random. There is no visual intuition for how deeply nested a block of code is, nor is it easy to determine which blocks are inside or outside of loops. Consider the following function, and its Graphviz graph:&lt;/p&gt;
    &lt;code&gt;function foo(n) {
  let result = 0;
  for (let i = 0; i &amp;lt; n; i++) {
    if (!!(i % 2)) {
      result = 0x600DBEEF;
    } else {
      result = 0xBADBEEF;
    }
  }

  return result;
}
&lt;/code&gt;
    &lt;p&gt;Counterintuitively, the &lt;code&gt;return&lt;/code&gt; appears before the two assignments in the body of the loop. Since this graph mirrors JavaScript control flow, we’d expect to see the return at the bottom. This problem only gets worse as graphs grow larger and more complex.&lt;/p&gt;
    &lt;p&gt;The second, related problem is that Graphviz’s output is unstable. Small changes to the input can result in large changes to the output. As you page through the graphs of each pass within Ion, nodes will jump left and right, true and false branches will swap, loops will run up the right side instead of the left, and so on. This makes it very hard to understand the actual effect of any given pass. Consider the following before and after, and notice how the second graph is almost—but not quite—a mirror image of the first, despite very minimal changes to the graph’s structure:&lt;/p&gt;
    &lt;p&gt;None of this felt right to me. Control flow graphs should be able to follow the structure of the program that produced them. After all, a control flow graph has many restrictions that a general-purpose tool would not be aware of: they have very few cycles, all of which are well-defined because they come from loops; furthermore, both JavaScript and WebAssembly have reducible control flow, meaning all loops have only one entry, and it is not possible to jump directly into the middle of a loop. This information could be used to our advantage.&lt;/p&gt;
    &lt;p&gt;Beyond that, a static PDF is far from ideal when exploring complicated graphs. Finding the inputs or uses of a given instruction is a tedious and frustrating exercise, as is following arrows from block to block. Even just zooming in and out is difficult. I eventually concluded that we ought to just build an interactive tool to overcome these limitations.&lt;/p&gt;
    &lt;head rend="h2"&gt;How hard could layout be?&lt;/head&gt;
    &lt;p&gt;I had one false start with graph layout, with an algorithm that attempted to sort blocks into vertical “tracks”. This broke down quickly on a variety of programs and I was forced to go back to the drawing board—in fact, back to the source of the very tool I was trying to replace.&lt;/p&gt;
    &lt;p&gt;The algorithm used by &lt;code&gt;dot&lt;/code&gt;, the typical hierarchical layout mode for Graphviz, is known as the Sugiyama layout algorithm, from a 1981 paper by Sugiyama et al. As introduction, I found a short series of lectures that broke down the Sugiyama algorithm into 5 steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cycle breaking, where the direction of some edges are flipped in order to produce a DAG.&lt;/item&gt;
      &lt;item&gt;Leveling, where vertices are assigned into horizontal layers according to their depth in the graph, and dummy vertices are added to any edge that crosses multiple layers.&lt;/item&gt;
      &lt;item&gt;Crossing minimization, where vertices on a layer are reordered in order to minimize the number of edge crossings.&lt;/item&gt;
      &lt;item&gt;Vertex positioning, where vertices are horizontally positioned in order to make the edges as straight as possible.&lt;/item&gt;
      &lt;item&gt;Drawing, where the final graph is rendered to the screen.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These steps struck me as surprisingly straightforward, and provided useful opportunities to insert our own knowledge of the problem:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cycle breaking would be trivial for us, since the only cycles in our data are loops, and loop backedges are explicitly labeled. We could simply ignore backedges when laying out the graph.&lt;/item&gt;
      &lt;item&gt;Leveling would be straightforward, and could easily be modified to better mimic the source code. Specifically, any blocks coming after a loop in the source code could be artificially pushed down in the layout, solving the confusing early-exit problem.&lt;/item&gt;
      &lt;item&gt;Permuting vertices to reduce edge crossings was actually just a bad idea, since our goal was stability from graph to graph. The true and false branches of a condition should always appear in the same order, for example, and a few edge crossings is a small price to pay for this stability.&lt;/item&gt;
      &lt;item&gt;Since reducible control flow ensures that a program’s loops form a tree, vertex positioning could ensure that loops are always well-nested in the final graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Taken all together, these simplifications resulted in a remarkably straightforward algorithm, with the initial implementation being just 1000 lines of JavaScript. (See this demo for what it looked like at the time.) It also proved to be very efficient, since it avoided the most computationally complex parts of the Sugiyama algorithm.&lt;/p&gt;
    &lt;head rend="h2"&gt;iongraph from start to finish&lt;/head&gt;
    &lt;p&gt;We will now go through the entire iongraph layout algorithm. Each section contains explanatory diagrams, in which rectangles are basic blocks and circles are dummy nodes. Loop header blocks (the single entry point to each loop) are additionally colored green.&lt;/p&gt;
    &lt;p&gt;Be aware that the block positions in these diagrams are not representative of the actual computed layout position at each point in the process. For example, vertical positions are not calculated until the very end, but it would be hard to communicate what the algorithm was doing if all blocks were drawn on a single line!&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 1: Layering&lt;/head&gt;
    &lt;p&gt;We first sort the basic blocks into horizontal tracks called “layers”. This is very simple; we just start at layer 0 and recursively walk the graph, incrementing the layer number as we go. As we go, we track the “height” of each loop, not in pixels, but in layers.&lt;/p&gt;
    &lt;p&gt;We also take this opportunity to vertically position nodes “inside” and “outside” of loops. Whenever we see an edge that exits a loop, we defer the layering of the destination block until we are done layering the loop contents, at which point we know the loop’s height.&lt;/p&gt;
    &lt;p&gt;A note on implementation: nodes are visited multiple times throughout the process, not just once. This can produce a quadratic explosion for large graphs, but I find that an early-out is sufficient to avoid this problem in practice.&lt;/p&gt;
    &lt;p&gt;The animation below shows the layering algorithm in action. Notice how the final block in the graph is visited twice, once after each loop that branches to it, and in each case, the block is deferred until the entire loop has been layered, rather than processed immediately after its predecessor block. The final position of the block is below the entirety of both loops, rather than directly below one of its predecessors as Graphviz would do. (Remember, horizontal and vertical positions have not yet been computed; the positions of the blocks in this diagram are hardcoded for demonstration purposes.)&lt;/p&gt;
    &lt;head&gt;Implementation pseudocode&lt;/head&gt;
    &lt;code&gt;/*CODEBLOCK=layering*/function layerBlock(block, layer = 0) {
  // Omitted for clarity: special handling of our "backedge blocks"

  // Early out if the block would not be updated
  if (layer &amp;lt;= block.layer) {
    return;
  }

  // Update the layer of the current block
  block.layer = Math.max(block.layer, layer);

  // Update the heights of all loops containing the current block
  let header = block.loopHeader;
  while (header) {
    header.loopHeight = Math.max(header.loopHeight, block.layer - header.layer + 1);
    header = header.parentLoopHeader;
  }

  // Recursively layer successors
  for (const succ of block.successors) {
    if (succ.loopDepth &amp;lt; block.loopDepth) {
      // Outgoing edges from the current loop will be layered later
      block.loopHeader.outgoingEdges.push(succ);
    } else {
      layerBlock(succ, layer + 1);
    }
  }

  // Layer any outgoing edges only after the contents of the loop have
  // been processed
  if (block.isLoopHeader()) {
    for (const succ of block.outgoingEdges) {
      layerBlock(succ, layer + block.loopHeight);
    }
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 2: Create dummy nodes&lt;/head&gt;
    &lt;p&gt;Any time an edge crosses a layer, we create a dummy node. This allows edges to be routed across layers without overlapping any blocks. Unlike in traditional Sugiyama, we always put downward dummies on the left and upward dummies on the right, producing a consistent “counter-clockwise” flow. This also makes it easy to read long vertical edges, whose direction would otherwise be ambiguous. (Recall how the loop backedge flipped from the right to the left in the “unstable layout” Graphviz example from before.)&lt;/p&gt;
    &lt;p&gt;In addition, we coalesce any edges that are going to the same destination by merging their dummy nodes. This heavily reduces visual noise.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 3: Straighten edges&lt;/head&gt;
    &lt;p&gt;This is the fuzziest and most ad-hoc part of the process. Basically, we run lots of small passes that walk up and down the graph, aligning layout nodes with each other. Our edge-straightening passes include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pushing nodes to the right of their loop header to “indent” them.&lt;/item&gt;
      &lt;item&gt;Walking a layer left to right, moving children to the right to line up with their parents. If any nodes overlap as a result, they are pushed further to the right.&lt;/item&gt;
      &lt;item&gt;Walking a layer right to left, moving parents to the right to line up with their children. This version is more conservative and will not move a node if it would overlap with another. This cleans up most issues from the first pass.&lt;/item&gt;
      &lt;item&gt;Straightening runs of dummy nodes so we have clean vertical lines.&lt;/item&gt;
      &lt;item&gt;“Sucking in” dummy runs on the left side of the graph if there is room for them to move to the right.&lt;/item&gt;
      &lt;item&gt;Straighten out any edges that are “nearly straight”, according to a chosen threshold. This makes the graph appear less wobbly. We do this by repeatedly “combing” the graph upward and downward, aligning parents with children, then children with parents, and so on.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It is important to note that dummy nodes participate fully in this system. If for example you have two side-by-side loops, straightening the left loop’s backedge will push the right loop to the side, avoiding overlaps and preserving the graph’s visual structure.&lt;/p&gt;
    &lt;p&gt;We do not reach a fixed point with this strategy, nor do we attempt to. I find that if you continue to repeatedly apply these particular layout passes, nodes will wander to the right forever. Instead, the layout passes are hand-tuned to produce decent-looking results for most of the graphs we look at on a regular basis. That said, this could certainly be improved, especially for larger graphs which do benefit from more iterations.&lt;/p&gt;
    &lt;p&gt;At the end of this step, all nodes have a fixed X-coordinate and will not be modified further.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 4: Track horizontal edges&lt;/head&gt;
    &lt;p&gt;Edges may overlap visually as they run horizontally between layers. To resolve this, we sort edges into parallel “tracks”, giving each a vertical offset. After tracking all the edges, we record the total height of the tracks and store it on the preceding layer as its “track height”. This allows us to leave room for the edges in the final layout step.&lt;/p&gt;
    &lt;p&gt;We first sort edges by their starting position, left to right. This produces a consistent arrangement of edges that has few vertical crossings in practice. Edges are then placed into tracks from the “outside in”, stacking rightward edges on top and leftward edges on the bottom, creating a new track if the edge would overlap with or cross any other edge.&lt;/p&gt;
    &lt;p&gt;The diagram below is interactive. Click and drag the blocks to see how the horizontal edges get assigned to tracks.&lt;/p&gt;
    &lt;head&gt;Implementation pseudocode&lt;/head&gt;
    &lt;code&gt;/*CODEBLOCK=tracks*/function trackHorizontalEdges(layer) {
  const TRACK_SPACING = 20;

  // Gather all edges on the layer, and sort left to right by starting coordinate
  const layerEdges = [];
  for (const node of layer.nodes) {
    for (const edge of node.edges) {
      layerEdges.push(edge);
    }
  }
  layerEdges.sort((a, b) =&amp;gt; a.startX - b.startX);

  // Assign edges to "tracks" based on whether they overlap horizontally with
  // each other. We walk the tracks from the outside in and stop if we ever
  // overlap with any other edge.
  const rightwardTracks = []; // [][]Edge
  const leftwardTracks = [];  // [][]Edge
  nextEdge:
  for (const edge of layerEdges) {
    const trackSet = edge.endX - edge.startX &amp;gt;= 0 ? rightwardTracks : leftwardTracks;
    let lastValidTrack = null; // []Edge | null

    // Iterate through the tracks in reverse order (outside in)
    for (let i = trackSet.length - 1; i &amp;gt;= 0; i--) {
      const track = trackSet[i];
      let overlapsWithAnyInThisTrack = false;
      for (const otherEdge of track) {
        if (edge.dst === otherEdge.dst) {
          // Assign the edge to this track to merge arrows
          track.push(edge);
          continue nextEdge;
        }

        const al = Math.min(edge.startX, edge.endX);
        const ar = Math.max(edge.startX, edge.endX);
        const bl = Math.min(otherEdge.startX, otherEdge.endX);
        const br = Math.max(otherEdge.startX, otherEdge.endX);
        const overlaps = ar &amp;gt;= bl &amp;amp;&amp;amp; al &amp;lt;= br;
        if (overlaps) {
          overlapsWithAnyInThisTrack = true;
          break;
        }
      }

      if (overlapsWithAnyInThisTrack) {
        break;
      } else {
        lastValidTrack = track;
      }
    }

    if (lastValidTrack) {
      lastValidTrack.push(edge);
    } else {
      trackSet.push([edge]);
    }
  }

  // Use track info to apply offsets to each edge for rendering.
  const tracksHeight = TRACK_SPACING * Math.max(
    0,
    rightwardTracks.length + leftwardTracks.length - 1,
  );
  let trackOffset = -tracksHeight / 2;
  for (const track of [...rightwardTracks.toReversed(), ...leftwardTracks]) {
    for (const edge of track) {
      edge.offset = trackOffset;
    }
    trackOffset += TRACK_SPACING;
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 5: Verticalize&lt;/head&gt;
    &lt;p&gt;Finally, we assign each node a Y-coordinate. Starting at a Y-coordinate of zero, we iterate through the layers, repeatedly adding the layer’s height and its track height, where the layer height is the maximum height of any node in the layer. All nodes within a layer receive the same Y-coordinate; this is simple and easier to read than Graphviz’s default of vertically centering nodes within a layer.&lt;/p&gt;
    &lt;p&gt;Now that every node has both an X and Y coordinate, the layout process is complete.&lt;/p&gt;
    &lt;head&gt;Implementation pseudocode&lt;/head&gt;
    &lt;code&gt;/*CODEBLOCK=verticalize*/function verticalize(layers) {
  let layerY = 0;
  for (const layer of layers) {
    let layerHeight = 0;
    for (const node of layer.nodes) {
      node.y = layerY;
      layerHeight = Math.max(layerHeight, node.height);
    }
    layerY += layerHeight;
    layerY += layer.trackHeight;
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 6: Render&lt;/head&gt;
    &lt;p&gt;The details of rendering are out of scope for this article, and depend on the specific application. However, I wish to highlight a stylistic decision that I feel makes our graphs more readable.&lt;/p&gt;
    &lt;p&gt;When rendering edges, we use a style inspired by railroad diagrams. These have many advantages over the Bézier curves employed by Graphviz. First, straight lines feel more organized and are easier to follow when scrolling up and down. Second, they are easy to route (vertical when crossing layers, horizontal between layers). Third, they are easy to coalesce when they share a destination, and the junctions provide a clear indication of the edge’s direction. Fourth, they always cross at right angles, improving clarity and reducing the need to avoid edge crossings in the first place.&lt;/p&gt;
    &lt;p&gt;Consider the following example. There are several edge crossings that may traditionally be considered undesirable—yet the edges and their directions remain clear. Of particular note is the vertical junction highlighted in red on the left: not only is it immediately clear that these edges share a destination, but the junction itself signals that the edges are flowing downward. I find this much more pleasant than the “rat’s nest” that Graphviz tends to produce.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this work?&lt;/head&gt;
    &lt;p&gt;It may seem surprising that such a simple (and stupid) layout algorithm could produce such readable graphs, when more sophisticated layout algorithms struggle. However, I feel that the algorithm succeeds because of its simplicity.&lt;/p&gt;
    &lt;p&gt;Most graph layout algorithms are optimization problems, where error is minimized on some chosen metrics. However, these metrics seem to correlate poorly to readability in practice. For example, it seems good in theory to rearrange nodes to minimize edge crossings. But a predictable order of nodes seems to produce more sensible results overall, and simple rules for edge routing are sufficient to keep things tidy. (As a bonus, this also gives us layout stability from pass to pass.) Similarly, layout rules like “align parents with their children” produce more readable results than “minimize the lengths of edges”.&lt;/p&gt;
    &lt;p&gt;Furthermore, by rejecting the optimization problem, a human author gains more control over the layout. We are able to position nodes “inside” of loops, and push post-loop content down in the graph, because we reject this global constraint-solver approach. Minimizing “error” is meaningless compared to a human maximizing meaning through thoughtful design.&lt;/p&gt;
    &lt;p&gt;And finally, the resulting algorithm is simply more efficient. All the layout passes in iongraph are easy to program and scale gracefully to large graphs because they run in roughly linear time. It is better, in my view, to run a fixed number of layout iterations according to your graph complexity and time budget, rather than to run a complex constraint solver until it is “done”.&lt;/p&gt;
    &lt;p&gt;By following this philosophy, even the worst graphs become tractable. Below is a screenshot of a zlib function, compiled to WebAssembly, and rendered using the old tool.&lt;/p&gt;
    &lt;p&gt;It took about ten minutes for Graphviz to produce this spaghetti nightmare. By comparison, iongraph can now lay out this function in 20 milliseconds. The result is still not particularly beautiful, but it renders thousands of times faster and is much easier to navigate.&lt;/p&gt;
    &lt;p&gt;Perhaps programmers ought to put less trust into magic optimizing systems, especially when a human-friendly result is the goal. Simple (and stupid) algorithms can be very effective when applied with discretion and taste.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work&lt;/head&gt;
    &lt;p&gt;We have already integrated iongraph into the Firefox profiler, making it easy for us to view the graphs of the most expensive or impactful functions we find in our performance work. Unfortunately, this is only available in specific builds of the SpiderMonkey shell, and is not available in full browser builds. This is due to architectural differences in how profiling data is captured and the flags with which the browser and shell are built. I would love for Firefox users to someday be able to view these graphs themselves, but at the moment we have no plans to expose this to the browser. However, one bug tracking some related work can be found here.&lt;/p&gt;
    &lt;p&gt;We will continue to sporadically update iongraph with more features to aid us in our work. We have several ideas for new features, including richer navigation, search, and visualization of register allocation info. However, we have no explicit roadmap for when these features may be released.&lt;/p&gt;
    &lt;p&gt;To experiment with iongraph locally, you can run a debug build of the SpiderMonkey shell with &lt;code&gt;IONFLAGS=logs&lt;/code&gt;; this will dump information to &lt;code&gt;/tmp/ion.json&lt;/code&gt;. This file can then be loaded into the standalone deployment of iongraph. Please be aware that the user experience is rough and unpolished in its current state.&lt;/p&gt;
    &lt;p&gt;The source code for iongraph can be found on GitHub. If this subject interests you, we would welcome contributions to iongraph and its integration into the browser. The best place to reach us is our Matrix chat.&lt;/p&gt;
    &lt;p&gt;Thanks to Matthew Gaudet, Asaf Gartner, and Colin Davidson for their feedback on this article.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spidermonkey.dev/blog/2025/10/28/iongraph-web.html"/><published>2025-10-29T05:17:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45744395</id><title>SpiderMonkey Garbage Collector</title><updated>2025-10-29T13:47:53.959164+00:00</updated><content>&lt;doc fingerprint="b7f969cd84f696d2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SpiderMonkey garbage collector&lt;/head&gt;
    &lt;p&gt;The SpiderMonkey garbage collector is responsible for allocating memory representing JavaScript data structures and deallocating them when they are no longer in use. It aims to collect as much data as possible in as little time as possible. As well as JavaScript data it is also used to allocate some internal SpiderMonkey data structures.&lt;/p&gt;
    &lt;p&gt;The garbage collector is a hybrid tracing collector, and has the following features:&lt;/p&gt;
    &lt;p&gt;For an overview of garbage collection see: https://en.wikipedia.org/wiki/Tracing_garbage_collection&lt;/p&gt;
    &lt;head rend="h2"&gt;Description of features&lt;/head&gt;
    &lt;head rend="h3"&gt;Precise collection&lt;/head&gt;
    &lt;p&gt;The GC is ‘precise’ in that it knows the layout of allocations (which is used to determine reachable children) and also the location of all stack roots. This means it does not need to resort to conservative techniques that may cause garbage to be retained unnecessarily.&lt;/p&gt;
    &lt;p&gt;Knowledge of the stack is achieved with C++ wrapper classes that must be used for stack roots and handles (pointers) to them. This is enforced by the SpiderMonkey API (which operates in terms of these types) and checked by a static analysis that reports places when unrooted GC pointers can be present when a GC could occur.&lt;/p&gt;
    &lt;p&gt;For details of stack rooting, see: https://github.com/mozilla-spidermonkey/spidermonkey-embedding-examples/blob/esr78/docs/GC%20Rooting%20Guide.md&lt;/p&gt;
    &lt;p&gt;We also have a static analysis for detecting errors in rooting. It can be run locally or in CI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Incremental collection&lt;/head&gt;
    &lt;p&gt;‘Stop the world’ collectors run a whole collection in one go, which can result in unacceptable pauses for users. An incremental collector breaks its execution into a number of small slices, reducing user impact.&lt;/p&gt;
    &lt;p&gt;As far as possible the SpiderMonkey collector runs incrementally. Not all parts of a collection can be performed incrementally however as there are some operations that need to complete atomically with respect to the rest of the program.&lt;/p&gt;
    &lt;p&gt;Currently, most of the collection is performed incrementally. Root marking, compacting, and an initial part of sweeping are not.&lt;/p&gt;
    &lt;head rend="h3"&gt;Generational collection&lt;/head&gt;
    &lt;p&gt;Most real world allocations either die very quickly or live for a long time. This suggests an approach to collection where allocations are moved between ‘generations’ (separate heaps) depending on how long they have survived. Generations containing young allocations are fast to collect and can be collected more frequently; older generations are collected less often.&lt;/p&gt;
    &lt;p&gt;The SpiderMonkey collector implements a single young generation (the nursery) and a single old generation (the tenured heap). Collecting the nursery is known as a minor GC as opposed to a major GC that collects the whole heap (including the nursery).&lt;/p&gt;
    &lt;head rend="h3"&gt;Concurrent collection&lt;/head&gt;
    &lt;p&gt;Many systems have more than one CPU and therefore can benefit from offloading GC work to another core. In GC terms ‘concurrent’ usually refers to GC work happening while the main program continues to run.&lt;/p&gt;
    &lt;p&gt;The SpiderMonkey collector currently only uses concurrency in limited phases.&lt;/p&gt;
    &lt;p&gt;This includes most finalization work (there are some restrictions as not all finalization code can tolerate this) and some other aspects such as allocating and decommitting blocks of memory.&lt;/p&gt;
    &lt;p&gt;Performing marking work concurrently is currently being investigated.&lt;/p&gt;
    &lt;head rend="h3"&gt;Parallel collection&lt;/head&gt;
    &lt;p&gt;In GC terms ‘parallel’ usually means work performed in parallel while the collector is running, as opposed to the main program itself. The SpiderMonkey collector performs work within GC slices in parallel wherever possible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compacting collection&lt;/head&gt;
    &lt;p&gt;The collector allocates data with the same type and size in ‘arenas’ (often know as slabs). After many allocations have died this can leave many arenas containing free space (external fragmentation). Compacting remedies this by moving allocations between arenas to free up as much memory as possible.&lt;/p&gt;
    &lt;p&gt;Compacting involves tracing the entire heap to update pointers to moved data and is not incremental so it only happens rarely, or in response to memory pressure notifications.&lt;/p&gt;
    &lt;head rend="h3"&gt;Partitioned heap&lt;/head&gt;
    &lt;p&gt;The collector has the concept of ‘zones’ which are separate heaps which can be collected independently. Objects in different zones can refer to each other however.&lt;/p&gt;
    &lt;p&gt;Zones are also used to help incrementalize parts of the collection. For example, compacting is not fully incremental but can be performed one zone at a time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other documentation&lt;/head&gt;
    &lt;p&gt;More details about the Garbage Collector (GC) can be found by looking for the [SMDOC] Garbage Collector comment in the sources.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://firefox-source-docs.mozilla.org/js/gc.html"/><published>2025-10-29T09:06:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45744503</id><title>YouTube is taking down videos on performing nonstandard Windows 11 installs</title><updated>2025-10-29T13:47:53.821276+00:00</updated><content/><link href="https://old.reddit.com/r/DataHoarder/comments/1oiz0v0/youtube_is_taking_down_videos_on_performing/"/><published>2025-10-29T09:26:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45745072</id><title>Aggressive bots ruined my weekend</title><updated>2025-10-29T13:47:53.483754+00:00</updated><content>&lt;doc fingerprint="3e708d031d6216a9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Aggressive bots ruined my weekend&lt;/head&gt;
    &lt;p&gt;On the 25th of October Bear had its first major outage. Specifically, the reverse proxy which handles custom domains went down, causing custom domains to time out.&lt;/p&gt;
    &lt;p&gt;Unfortunately my monitoring tool failed to notify me, and it being a Saturday, I didn't notice the outage for longer than is reasonable. I apologise to everyone who was affected by it.&lt;/p&gt;
    &lt;p&gt;First, I want to dissect the root cause, exactly what went wrong, and then provide the steps I've taken to mitigate this in the future.&lt;/p&gt;
    &lt;p&gt;I wrote about The Great Scrape at the beginning of this year. The vast majority of web traffic is now bots, and it is becoming increasingly more hostile to have publicly available resources on the internet.&lt;/p&gt;
    &lt;p&gt;There are 3 major kinds of bots currently flooding the internet: AI scrapers, malicious scrapers, and unchecked automations/scrapers.&lt;/p&gt;
    &lt;p&gt;The first has been discussed at length. Data is worth something now that it is used as fodder to train LLMs, and there is a financial incentive to scrape, so scrape they will. They've depleted all human-created writing on the internet, and are becoming increasingly ravenous for new wells of content. I've seen this compared to the search for low-background-radiation steel, which is, itself, very interesting.&lt;/p&gt;
    &lt;p&gt;These scrapers, however, are the easiest to deal with since they tend to identify themselves as ChatGPT, Anthropic, XAI, et cetera. They also tend to specify whether they are from user-initiated searches (think all the sites that get scraped when you make a request with ChatGPT), or data mining (data used to train models). On Bear Blog I allow the first kinds, but block the second, since bloggers want discoverability, but usually don't want their writing used to train the next big model.&lt;/p&gt;
    &lt;p&gt;The next two kinds of scraper are more insidious. The malicious scrapers are bots that systematically scrape and re-scrape websites, sometimes every few minutes, looking for vulnerabilities such as misconfigured Wordpress instances, or &lt;code&gt;.env&lt;/code&gt; and &lt;code&gt;.aws&lt;/code&gt; files, among other things, accidentally left lying around.&lt;/p&gt;
    &lt;p&gt;It's more dangerous than ever to self-host, since simple mistakes in configurations will likely be found and exploited. In the last 24 hours I've blocked close to 2 million malicious requests across several hundred blogs.&lt;/p&gt;
    &lt;p&gt;What's wild is that these scrapers rotate through thousands of IP addresses during their scrapes, which leads me to suspect that the requests are being tunnelled through apps on mobile devices, since the ASNs tend to be cellular networks. I'm still speculating here, but I think app developers have found another way to monetise their apps by offering them for free, and selling tunnel access to scrapers.&lt;/p&gt;
    &lt;p&gt;Now, on to the unchecked automations. Vibe coding has made web-scraping easier than ever. Any script-kiddie can easily build a functional scraper in a single prompt and have it run all day from their home computer, and if the dramatic rise in scraping is anything to go by, many do. Tens of thousands of new scrapers have cropped up over the past few months, accidentally DDoSing website after website in their wake. The average consumer-grade computer is significantly more powerful than a VPS, so these machines can easily cause a lot of damage without noticing.&lt;/p&gt;
    &lt;p&gt;I've managed to keep all these scrapers at bay using a combination of web application firewall (WAF) rules and rate limiting provided by Cloudflare, as well as some custom code which finds and quarantines bad bots based on their activity.&lt;/p&gt;
    &lt;p&gt;I've played around with serving Zip Bombs, which was quite satisfying, but I stopped for fear of accidentally bombing a legitimate user. Another thing I've played around with is Proof of Work validation, making it expensive for bots to scrape, as well as serving endless junk data to keep the bots busy. Both of these are interesting, but ultimately are just as effective as simply blocking those requests, without the increased complexity.&lt;/p&gt;
    &lt;p&gt;With that context, here's exactly went wrong on Saturday.&lt;/p&gt;
    &lt;p&gt;Previously, the bottleneck for page requests was the web-server itself, since it does the heavy lifting. It automatically scales horizontally by up to a factor of 10, if necessary, but bot requests can scale by significantly more than that, so having strong bot detection and mitigation, as well as serving highly-requested endpoints via a CDN is necessary. This is a solved problem, as outlined in my Great Scrape post, but worth restating.&lt;/p&gt;
    &lt;p&gt;On Saturday morning a few hundred blogs were DDoSed, with tens of thousands of pages requested per minute (from the logs it's hard to say whether they were malicious, or just very aggressive scrapers). The above-mentioned mitigations worked as expected, however the reverse-proxy—which sits up-stream of most of these mitigations—became saturated with requests and decided it needed to take a little nap.&lt;/p&gt;
    &lt;p&gt;The big blue spike is what toppled the server. It's so big it makes the rest of the graph look flat.&lt;/p&gt;
    &lt;p&gt;This server had been running with zero downtime for 5 years up until this point.&lt;/p&gt;
    &lt;p&gt;Unfortunately my uptime monitor failed to alert me via the push notifications I'd set up, even though it's the only app I have that not only has notifications enabled (see my post on notifications), but even has critical alerts enabled, so it'll wake me up in the middle of the night if necessary. I still have no idea why this alert didn't come through, and I have ruled out misconfiguration through various tests.&lt;/p&gt;
    &lt;p&gt;This brings me to how I will prevent this from happening in the future.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Redundancy in monitoring. I now have a second monitoring service running alongside my uptime monitor which will give me a phone call, email, and text message in the event of any downtime.&lt;/item&gt;
      &lt;item&gt;More aggressive rate-limiting and bot mitigation on the reverse proxy. This already reduces the server load by about half.&lt;/item&gt;
      &lt;item&gt;I've bumped up the size of the reverse proxy, which can now handle about 5 times the load. This is overkill, but compute is cheap, and certainly worth the stress-mitigation. I'm already bald. I don't need to go balder.&lt;/item&gt;
      &lt;item&gt;Auto-restart the reverse-proxy if bandwidth usage drops to zero for more than 2 minutes.&lt;/item&gt;
      &lt;item&gt;Added a status page, available at https://status.bearblog.dev for better visibility and transparency. Hopefully those bars stay solid green forever.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This should be enough to keep everything healthy. If you have any suggestions, or need help with your own bot issues, send me an email.&lt;/p&gt;
    &lt;p&gt;The public internet is mostly bots, many of whom are bad netizens. It's the most hostile it's ever been, and it is because of this that I feel it's more important than ever to take good care of the spaces that make the internet worth visiting.&lt;/p&gt;
    &lt;p&gt;The arms race continues...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://herman.bearblog.dev/agressive-bots/"/><published>2025-10-29T10:47:25+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45745281</id><title>AWS to bare metal two years later: Answering your questions about leaving AWS</title><updated>2025-10-29T13:47:53.159686+00:00</updated><content>&lt;doc fingerprint="ff74527664b5afab"&gt;
  &lt;main&gt;
    &lt;p&gt;When we published How moving from AWS to Bare-Metal saved us $230,000 /yr. in 2023, the story travelled far beyond our usual readership. The discussion threads on Hacker News and Reddit were packed with sharp questions: did we skip Reserved Instances, how do we fail over a single rack, what about the people cost, and when is cloud still the better answer? This follow-up is our long-form reply.&lt;/p&gt;
    &lt;p&gt;Over the last twenty-four months we:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ran the MicroK8s + Ceph stack in production for 730+ days with 99.993% measured availability.&lt;/item&gt;
      &lt;item&gt;Added a second rack in Frankfurt, joined to our primary Paris cage over redundant DWDM, to kill the “single rack” concern.&lt;/item&gt;
      &lt;item&gt;Cut average customer-facing latency by 19% thanks to local NVMe and eliminating noisy neighbours.&lt;/item&gt;
      &lt;item&gt;Reinvested the savings into buying bare metal AI servers to expand LLM-based alert / incident summarisation and auto code fixes based on log / traces and metrics in OneUptime.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Below we tackle the recurring themes from the community feedback, complete with the numbers we use internally.&lt;/p&gt;
    &lt;head rend="h2"&gt;$230,000 / yr savings? That is just an engineers salary.&lt;/head&gt;
    &lt;p&gt;In the US, it is. In the rest of the world. That's 2-5x engineers salary. We used to save $230,000 / yr but now the savings have exponentially grown. We now save over $1.2M / yr and we expect this to grow, as we grow as a business.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Why not just buy Savings Plans or Reserved Instances?”&lt;/head&gt;
    &lt;p&gt;We tried. Long answer: the maths still favoured bare metal once we priced everything in. We see a savings of over 76% if you compare our bare metal setup to AWS.&lt;/p&gt;
    &lt;p&gt;A few clarifications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Savings Plans do not reduce S3, egress, or Direct Connect. 37% off instances still leaves you paying list price for bandwidth, which was 22% of our AWS bill.&lt;/item&gt;
      &lt;item&gt;EKS had an extra $1,260/month control-plane fee plus $600/month for NAT gateways. Those costs disappear once you run Kubernetes yourself.&lt;/item&gt;
      &lt;item&gt;Our workload is 24/7 steady. We were already at &amp;gt;90% reservation coverage; there was no idle burst capacity to “right size” away. If we had the kind of bursty compute profile many commenters referenced, the choice would be different.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;“How much did migration and ongoing ops really cost?”&lt;/head&gt;
    &lt;p&gt;We spent a week of engineers time (and that is the worst case estimate) on the initial migration, spread across SRE, platform, and database owners. Most of that time was work we needed anyway—formalising infrastructure-as-code, smoke testing charts, tightening backup policies. The incremental work that existed purely because of bare metal was roughly one week.&lt;/p&gt;
    &lt;p&gt;Ongoing run-cost looks like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hands-on keyboard: ~24 engineer-hours/quarter across the entire platform team, including routine patching and firmware updates. That is comparable to the AWS time we used to burn on cost optimisation, IAM policy churn, and chasing deprecations and updating our VM's on AWS.&lt;/item&gt;
      &lt;item&gt;Remote hands: 2 interventions in 24 months (mainly disks). Mean response time: 27 minutes. We do not staff an on-site team. We rely on co-location provider to physically manage our rack. This means no traditional hardware admins.&lt;/item&gt;
      &lt;item&gt;Automation: We're now moving to Talos. We PXE boot with Tinkerbell, image with Talos, manage configs through Flux and Terraform, and run conformance suites before each Kubernetes upgrade. All of those tools also hardened our AWS estate, so they were not net-new effort.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The opportunity cost question from is fair. We track it the same way we track feature velocity: did the infra team ship less? The answer was “no”—our release cadence increased because we reclaimed few hours/month we used to spend in AWS “cost council” meetings.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Isn’t a single rack a single point of failure?”&lt;/head&gt;
    &lt;p&gt;We have multiple racks across two different DC / providers. We:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Leased a secondary quarter rack in Frankfurt with a different provider and power utility.&lt;/item&gt;
      &lt;item&gt;Currently: Deployed a second MicroK8s control plane, mirrored Ceph pools with asynchronous replication. Future: We're moving to Talos. Nothing against Microk8s, but we like the Talos way of managing the k8s cluster.&lt;/item&gt;
      &lt;item&gt;Added isolated out-of-band management paths (4G / satellite) so we can reach the gear even during metro fibre events.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The AWS failover cluster we mentioned in 2023 still exists. We rehearse a full cutover quarterly using the same Helm releases we ship to customers. DNS failover remains the slowest leg (resolver caches can ignore TTL), so we added Anycast ingress via BGP with our transit provider to cut traffic shifting to sub-minute.&lt;/p&gt;
    &lt;head rend="h2"&gt;“What about hardware lifecycle and surprise CapEx?”&lt;/head&gt;
    &lt;p&gt;We amortise servers over five years, but we sized them with 2 × AMD EPYC 9654 CPUs, 1 TB RAM, and NVMe sleds. At our current growth rate the boxes will hit CPU saturation before we hit year five. When that happens, the plan is to cascade the older gear into our regional analytics cluster (we use Posthog + Metabase for this) and buy a new batch. Thanks to the savings delta, we can refresh 40% of the fleet every 24 months and still spend less annually than the optimised AWS bill above.&lt;/p&gt;
    &lt;p&gt;We also buy extended warranties from the OEM (Supermicro) and keep three cold spares in the cage. The hardware lasts 7-8 years and not 5, but we wtill count it as 5 to be very conservative.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Are you reinventing managed services?”&lt;/head&gt;
    &lt;p&gt;Another strong Reddit critique: why rebuild services AWS already offers? Three reasons we are comfortable with the trade:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Portability is part of our product promise. OneUptime customers self-host in their own environments. Running the same open stack we ship (Postgres, Redis, ClickHouse, etc.) keeps us honest. We eun on Kubernetes and self-hosted customers run on Kubernetes as well.&lt;/item&gt;
      &lt;item&gt;Tooling maturity. Two years ago we relied on Terraform + EKS + RDS. Today we run MicroK8s (Talos in the future), Argo Rollouts, OpenTelemetry Collector, and Ceph dashboards. None of that is bespoke. We do not maintain a fork of anything.&lt;/item&gt;
      &lt;item&gt;Selective cloud use. We still pay AWS for Glacier backups, CloudFront for edge caching, and short-lived burst capacity for load tests. Cloud makes sense when elasticity matters; bare metal wins when baseload dominates.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Managed services are phenomenal when you are short on expertise or need features beyond commodity compute. If we were all-in on DynamoDB streams or Step Functions we would almost certainly still be on AWS.&lt;/p&gt;
    &lt;head rend="h2"&gt;“How do bandwidth and DoS scenarios work now?”&lt;/head&gt;
    &lt;p&gt;We committed to 5 Gbps 95th percentile across two carriers. The same traffic on AWS egress would be 8x expensive in eu-west-1. For DDoS protection we front our ingress with Cloudflare.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Has reliability suffered?”&lt;/head&gt;
    &lt;p&gt;Short answer: No. Infact it was better than AWS (compared to recent AWS downtimes)&lt;/p&gt;
    &lt;p&gt;We have 730+ days with 99.993% measured availability and we also escaped AWS region wide downtime that happened a week ago.&lt;/p&gt;
    &lt;head rend="h2"&gt;“How do audits and compliance work off-cloud now?”&lt;/head&gt;
    &lt;p&gt;We stayed SOC 2 Type II and ISO 27001 certified through the transition. The biggest deltas auditors cared about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Physical controls: We provide badge logs from the colo, camera footage on request, and quarterly access reviews. The colo already meets Tier III redundancy, so their reports roll into ours.&lt;/item&gt;
      &lt;item&gt;Change management: Terraform plans, and now Talos machine configs give us immutable evidence of change. Auditors liked that more than AWS Console screenshots.&lt;/item&gt;
      &lt;item&gt;Business continuity: We prove failover by moving workload to other DC.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are in a regulated space (HIPAA for instance), expect the paperwork to grow a little. We worked it in by leaning on the colo providers’ standard compliance packets—they slotted straight into our risk register.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Why not stay in the cloud but switch providers?”&lt;/head&gt;
    &lt;p&gt;We priced Hetzner, OVH, Leaseweb, Equinix Metal, and AWS Outposts. The short version:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hyperscaler alternatives were cheaper on compute but still expensive on egress once you hit petabytes/month. Outposts also carried minimum commits that exceeded our needs.&lt;/item&gt;
      &lt;item&gt;European dedicated hosts (Hetzner, OVH) are fantastic for lab clusters. The challenge was multi-100 TB Ceph clusters with redundant uplinks and smart-hands SLAs. Once we priced that tier, the savings narrowed.&lt;/item&gt;
      &lt;item&gt;Equinix Metal got the closest, but bare metal on-demand still carried a 25-30% premium over our CapEx plan. Their global footprint is tempting; we may still use them for short-lived expansion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Owning the hardware also let us plan power density (we run 15 kW racks) and reuse components. For our steady-state footprint, colocation won by a long shot.&lt;/p&gt;
    &lt;head rend="h2"&gt;“What does day-to-day toil look like now?”&lt;/head&gt;
    &lt;p&gt;We put real numbers to it because Reddit kept us honest:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Weekly: Kernel and firmware patches (Talos makes this a redeploy), Ceph health checks, Total time averages 1 hour/week on average over months.&lt;/item&gt;
      &lt;item&gt;Monthly: Kubernetes control plane upgrades in canary fashion. About 2 engineer-hours. We expect this to reduce when Talos kicks in.&lt;/item&gt;
      &lt;item&gt;Quarterly: Disaster recovery drills, capacity planning, and contract audits with carriers. Roughly 12 hours across three engineers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total toil is ~14 engineer-hours/month, including prep. The AWS era had us spending similar time but on different work: chasing cost anomalies, expanding Security Hub exceptions, and mapping breaking changes in managed services. The toil moved; it did not multiply.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Do you still use the cloud for anything substantial?”&lt;/head&gt;
    &lt;p&gt;Absolutely. Cloud still solves problems we would rather not own:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Glacier keeps long-term log archives at a price point local object storage cannot match.&lt;/item&gt;
      &lt;item&gt;CloudFront handles 14 edge PoPs we do not want to build. We terminate TLS at the edge for marketing assets and docs. We will soon move this to Cloudflare as they are cheaper.&lt;/item&gt;
      &lt;item&gt;We spin up short-lived AWS environments for load testing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So yes, we left AWS for the base workload, but we still swipe the corporate card when elasticity or geography outweighs fixed-cost savings.&lt;/p&gt;
    &lt;head rend="h2"&gt;When the cloud is still the right answer&lt;/head&gt;
    &lt;p&gt;It depends on your workload. We still recommend staying put if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your usage pattern is spiky or seasonal and you can auto-scale to near zero between peaks.&lt;/item&gt;
      &lt;item&gt;You lean heavily on managed services (Aurora Serverless, Kinesis, Step Functions) where the operational load is the value prop.&lt;/item&gt;
      &lt;item&gt;You do not have the appetite to build a platform team comfortable with Kubernetes, Ceph, observability, and incident response.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cloud-first was the right call for our first five years. Bare metal became the right call once our compute footprint, data gravity, and independence requirements stabilised.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is next&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We are working on a detailed runbook + Terraform module to help teams do capex forecasting for colo moves. Expect that on the blog later this year.&lt;/item&gt;
      &lt;item&gt;A deep dive on Talos is in the queue, as requested by multiple folks in the HN thread.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Questions we did not cover? Let us know in the discussion threads—we are happy to keep sharing the gritty details.&lt;/p&gt;
    &lt;p&gt;Related Reading:&lt;/p&gt;
    &lt;head rend="h3"&gt;Neel Patel&lt;/head&gt;
    &lt;p&gt;@devneelpatel • Oct 29, 2025 •&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://oneuptime.com/blog/post/2025-10-29-aws-to-bare-metal-two-years-later/view"/><published>2025-10-29T11:14:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45745566</id><title>Show HN: Learn German with Games</title><updated>2025-10-29T13:47:52.728733+00:00</updated><content>&lt;doc fingerprint="2ce7b8cb946d6548"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Choose Your Learning Adventure&lt;/head&gt;
    &lt;p&gt;Select a game below and start mastering German in an engaging, interactive way!&lt;/p&gt;
    &lt;head rend="h3"&gt;Numbers to Words Game&lt;/head&gt;
    &lt;p&gt;See a number and type the German word - perfect for learning German number vocabulary&lt;/p&gt;
    &lt;head rend="h3"&gt;Words to Numbers Game&lt;/head&gt;
    &lt;p&gt;Practice recognizing German number words and converting them to digits&lt;/p&gt;
    &lt;head rend="h3"&gt;German Time Game&lt;/head&gt;
    &lt;p&gt;Learn to tell time in German by reading analog clocks and typing time expressions&lt;/p&gt;
    &lt;head rend="h3"&gt;Time Short Form Game&lt;/head&gt;
    &lt;p&gt;Practice German time with short forms: nach, vor, halb, viertel, and punkt&lt;/p&gt;
    &lt;head rend="h3"&gt;Guess the Artikel&lt;/head&gt;
    &lt;p&gt;Master German artikels (der, die, das) by guessing the correct artikel for each noun&lt;/p&gt;
    &lt;head rend="h3"&gt;Guess the Word&lt;/head&gt;
    &lt;p&gt;Translate German nouns to English - see a German word with its article and type the English meaning&lt;/p&gt;
    &lt;head rend="h3"&gt;English Nouns to German&lt;/head&gt;
    &lt;p&gt;See an English word and type the German translation with its artikel&lt;/p&gt;
    &lt;head rend="h3"&gt;Verb Conjugation&lt;/head&gt;
    &lt;p&gt;Practice conjugating German verbs in present tense for all persons - ich, du, er/sie/es, wir, ihr, sie/Sie&lt;/p&gt;
    &lt;head rend="h3"&gt;German Verbs to English&lt;/head&gt;
    &lt;p&gt;See a German verb and type its English meaning - perfect for building vocabulary&lt;/p&gt;
    &lt;head rend="h3"&gt;English Verbs to German&lt;/head&gt;
    &lt;p&gt;See an English verb meaning and type the German infinitive form - reverse translation practice&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.learngermanwithgames.com/"/><published>2025-10-29T11:50:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45745876</id><title>Now any body can write JavaScript code in Rust</title><updated>2025-10-29T13:47:51.977190+00:00</updated><content>&lt;doc fingerprint="e790bf15fb13588a"&gt;
  &lt;main&gt;
    &lt;p&gt;Brahma-JS is an ultra-low-latency orchestrator for JS, blending familiar &lt;code&gt;Express&lt;/code&gt;-style middleware and routing with a high-performance core built in Rust. Ideal for micro-service and API use-cases where speed matters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust-level performance, without needing to write Rust.&lt;/item&gt;
      &lt;item&gt;Express-like API, so JS devs can jump in instantly.&lt;/item&gt;
      &lt;item&gt;Built with Tokio + Hyper, delivering asynchronous speed and efficiency.&lt;/item&gt;
      &lt;item&gt;Lightweight, zero-dependency binary — no build headaches.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benchmarks were run with wrk on an Intel® Core™ i5-12450H (12 vCPUs available under virtualization, 200 concurrent connections, 10s duration):&lt;/p&gt;
    &lt;p&gt;wrk output (Brahma-JS):&lt;/p&gt;
    &lt;code&gt;
Running 10s test @ [http://127.0.0.1:2000/hi](http://127.0.0.1:2000/hi)
1 threads and 200 connections
Thread Stats   Avg      Stdev     Max   +/- Stdev
Latency     1.51ms  479.16us   7.89ms   78.17%
Req/Sec   131.57k     9.13k  146.78k    79.00%
1309338 requests in 10.00s, 186.05MB read
Requests/sec: 130899.58
Transfer/sec: 18.60MB

&lt;/code&gt;
    &lt;p&gt;Takeaway: Brahma-JS sustains 130k+ requests/sec with low latency, powered by its Rust core and Express-style developer API.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start Brahma-JS server:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;node server.js
# server listens on 0.0.0.0:2000&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run wrk against the &lt;code&gt;/hi&lt;/code&gt;endpoint:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;wrk http://127.0.0.1:2000/hi -d 10 -t 1 -c 200&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-d 10&lt;/code&gt;→ run for 10 seconds&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-t 1&lt;/code&gt;→ 1 worker thread&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-c 200&lt;/code&gt;→ 200 concurrent connections&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Test machine info (&lt;code&gt;lscpu&lt;/code&gt;):&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Architecture:           x86_64
CPU(s):                 12
Model name:             12th Gen Intel(R) Core(TM) i5-12450H
Threads per core:       2
Cores per socket:       6
Virtualization:         Microsoft Hyper-V (full)
&lt;/code&gt;
    &lt;code&gt;npm install brahma-firelight
# or
yarn add brahma-firelight
# or
pnpm add brahma-firelight
# or
bun add brahma-firelight
# or
nypm add brahma-firelight
&lt;/code&gt;
    &lt;code&gt;const {
  createApp,
  getJsResponseTimeout,
  getMaxBodyBytes,
  setJsResponseTimeout,
  setMaxBodyBytes,
} = require("brahma-firelight");

const app = createApp();

// save production from disasters by locking in Rust
// defaults to 30 seconds and 4mb limit.
// set 2 minutes timeout (120 seconds)
setJsResponseTimeout(120);

// set max body to 50 MiB
setMaxBodyBytes(50 * 1024 * 1024); // 52_428_800

console.log("timeout secs:", getJsResponseTimeout()); // prints 120
console.log("max body bytes:", getMaxBodyBytes()); // prints 52428800

// CORS config
app.use((req, res, next) =&amp;gt; {
  const origin = req.headers.origin;

  if (origin) {
    res.setHeader("Access-Control-Allow-Origin", origin); // echo back client origin
    res.setHeader("Access-Control-Allow-Credentials", "true");
  } else {
    // fallback (same-origin or no Origin header)
    res.setHeader("Access-Control-Allow-Origin", "*");
  }

  res.setHeader("Access-Control-Allow-Methods", "GET,POST,PUT,DELETE,OPTIONS");
  res.setHeader("Access-Control-Allow-Headers", "Content-Type, Authorization");

  if (req.method === "OPTIONS") {
    res.send(204);
  } else {
    next();
  }
});

// Middlewares

function authMiddleware(req, res, next) {
  if (!req.headers["authorization"]) return res.text("Unauthorized", 401);
  next();
}

// utils.js
function sleep(ms) {
  return new Promise((resolve) =&amp;gt; setTimeout(resolve, ms));
}

app.get("/hi", (req, res) =&amp;gt; {
  res.json({ message: "Hello World from Brahma-JS!" });
});

// // Async handler returning an object
app.get("/time", async (req) =&amp;gt; {
  await sleep(20000);
  return {
    status: 400,
    headers: { "Content-Type": "application/json" }, // Custom Returns
    body: JSON.stringify({ now: Date.now() }),
  };
});

// To send HTML response
app.get("/page", (req, res) =&amp;gt; {
  res.html(`&amp;lt;h1&amp;gt;Hello HTML&amp;lt;/h1&amp;gt;&amp;lt;p&amp;gt;Served by Brahma-JS id: ${req.reqId}&amp;lt;/p&amp;gt;`);
});

app.post("/submit", (req, res) =&amp;gt; {
  let formData = JSON.parse(req.body);
  console.log("bodyData:", formData);
  res.json(formData, 201); // return the JSON response with http-status-code
});

// Set-Up cookies and User Sessions

app.get("/set-cookies", (req, res) =&amp;gt; {
  console.log("Request:--&amp;gt;", req); // Request Parameters-&amp;gt; contains all info + additional meta data
  res.send(
    200, // http-status code
    { "Content-Type": "text/plain" }, // headers Content-Type
    ["a=1; Path=/; HttpOnly", "b=2; Path=/; Secure; Max-Age=3600"], // manual cookie setup
    "hello" // optional Return Body
  );
});

app.get("/redirect", (req, res) =&amp;gt; {
  res.redirect("https://google.com");
});

app.post("/protected", authMiddleware, (req, res) =&amp;gt;
  res.json({ success: true })
);

app.listen("0.0.0.0", 2000, () =&amp;gt; {
  console.log("Server listening on port 2000");
});

// Enable built in Graceful Shutdown (optional for production use)

// process.on('SIGINT', async () =&amp;gt; {
//   console.log('SIGINT → shutting down...');
//   await app.close(5000); // wait up to 5s for requests
//   process.exit(0);
// });

// process.on('SIGTERM', async () =&amp;gt; {
//   console.log('SIGTERM → shutting down...');
//   await app.close(5000);
//   process.exit(0);
// });&lt;/code&gt;
    &lt;p&gt;Just like Express:&lt;/p&gt;
    &lt;code&gt;app.get("/hello", (req, res) =&amp;gt; {
  res.send("Hi there!");
});&lt;/code&gt;
    &lt;p&gt;But under the hood:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Execution occurs in Rust (Tokio + Hyper).&lt;/item&gt;
      &lt;item&gt;Handlers (sync or async) run without sacrificing speed.&lt;/item&gt;
      &lt;item&gt;Middleware works seamlessly — same developer experience, turbo-charged engine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;app.get("/data", async (req, res) =&amp;gt; {
  const result = await fetch(
    "https://jsonplaceholder.typicode.com/todos/1"
  ).then((r) =&amp;gt; r.json());
  res.json({ result });
});&lt;/code&gt;
    &lt;code&gt;app.use((req, res, next) =&amp;gt; {
  req.startTime = Date.now();
  next();
});

app.get("/delay", async (req, res) =&amp;gt; {
  await new Promise((r) =&amp;gt; setTimeout(r, 200));
  res.json({ elapsed: Date.now() - req.startTime });
});&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Beta / experimental — actively refined based on usage.&lt;/item&gt;
      &lt;item&gt;Feedback and early adopters highly encouraged.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Brahma-Firelight ships prebuilt native binaries for macOS, Linux and Windows so you don't need to compile the native addon locally.&lt;/p&gt;
    &lt;p&gt;Supported artifact filenames (what the JS loader will try to load):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS (Apple Silicon): &lt;code&gt;brahma-js.darwin-arm64.node&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;macOS (Intel x64): &lt;code&gt;brahma-js.darwin-x64.node&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux (x64, GNU): &lt;code&gt;brahma-js.linux-x64-gnu.node&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux (arm64, GNU): &lt;code&gt;brahma-js.linux-arm64-gnu.node&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Windows (x64, MSVC): &lt;code&gt;brahma-js.win32-x64-msvc.node&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;👉 Forked from Brahma-Core. An open source repository Brahma-Core.&lt;/p&gt;
    &lt;p&gt;MIT © LICENSE&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Shyam20001/rsjs"/><published>2025-10-29T12:22:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45746478</id><title>From VS Code to Helix</title><updated>2025-10-29T13:47:51.600275+00:00</updated><content>&lt;doc fingerprint="7708325733a03a9d"&gt;
  &lt;main&gt;
    &lt;p&gt;I created the website you’re reading with VS Code. Behind the scenes I use Astro, a static site generator that gets out of the way while providing nice conveniences.&lt;/p&gt;
    &lt;p&gt;Using VS Code was a no-brainer: everyone in the industry seems to at least be familiar with it, every project can be opened with it, and most projects can get enhancements and syntactic helpers in a few clicks. In short: VS Code is free, easy to use, and widely adopted.&lt;/p&gt;
    &lt;p&gt;A Rustacean colleague kept singing Helix’s praises. I discarded it because he’s much smarter than I am, and I only ever use vim when I need to fiddle with files on a server. I like when things “Just Work” and didn’t want to bother learning how to use Helix nor how to configure it.&lt;/p&gt;
    &lt;p&gt;Today it has become my daily driver. Why did I change my mind? What was preventing me from using it before? And how difficult was it to get there?&lt;/p&gt;
    &lt;head rend="h2"&gt;Automation is a double-edged sword&lt;/head&gt;
    &lt;p&gt;Automation and technology make work easier, this is why we produce technology in the first place. But it also means you grow more dependent on the tech you use. If the tech is produced transparently by an international team or a team you trust, it’s fine. But if it’s produced by a single large entity that can screw you over, it’s dangerous.&lt;/p&gt;
    &lt;p&gt;VS Code might be open source, but in practice it’s produced by Microsoft. Microsoft has a problematic relationship to consent and is shoving AI products down everyone’s throat. I’d rather use tools that respect me and my decisions, and I’d rather not get my tools produced by already monopolistic organizations.&lt;/p&gt;
    &lt;p&gt;Microsoft is also based in the USA, and the political climate over there makes me want to depend as little as possible on American tools. I know that’s a long, uphill battle, but we have to start somewhere.&lt;/p&gt;
    &lt;p&gt;I’m not advocating for a ban against American tech in general, but for more balance in our supply chain. I’m also not advocating for European tech either: I’d rather get open source tools from international teams competing in a race to the top, rather than from teams in a single jurisdiction. What is happening in the USA could happen in Europe too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why I feared using Helix&lt;/head&gt;
    &lt;p&gt;I’ve never found vim particularly pleasant to use but it’s everywhere, so I figured I might just get used to it. But one of the things I never liked about vim is the number of moving pieces. By default, vim and neovim are very bare bones. They can be extended and completely modified with plugins, but I really don’t like the idea of having extremely customize tools.&lt;/p&gt;
    &lt;p&gt;I’d rather have the same editor as everyone else, with a few knobs for minor preferences. I am subject to choice paralysis, so making me configure an editor before I’ve even started editing is the best way to tank my productivity.&lt;/p&gt;
    &lt;p&gt;When my colleague told me about Helix, two things struck me as improvements over vim.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Helix’s philosophy is that everything should work out of the box. There are a few configs and themes, but everything should work similarly from one Helix to another. All the language-specific logic is handled in Language Servers that implement the Language Server Protocol standard.&lt;/item&gt;
      &lt;item&gt;In Helix, first you select text, and then you perform operations onto it. So you can visually tell what is going to be changed before you apply the change. It fits my mental model much better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But there are major drawbacks to Helix too:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;After decades of vim, I was scared to re-learn everything. In practice this wasn’t a problem at all because of the very visual way Helix works.&lt;/item&gt;
      &lt;item&gt;VS Code “Just Works”, and Helix sounded like more work than the few clicks from VS Code’s extension store. This is true, but not as bad as I had anticipated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After a single week of usage, Helix was already very comfortable to navigate. After a few weeks, most of the wrinkles have been ironed out and I use it as my primary editor. So how did I overcome those fears?&lt;/p&gt;
    &lt;head rend="h2"&gt;What Helped&lt;/head&gt;
    &lt;head rend="h3"&gt;Just Do It&lt;/head&gt;
    &lt;p&gt;I tried Helix. It can sound silly, but the very first step to get into Helix was not to overthink it. I just installed it on my mac with &lt;code&gt;brew install helix&lt;/code&gt; and gave it a go. I was not too familiar with it, so I looked up the official documentation and noticed there was a tutorial.&lt;/p&gt;
    &lt;p&gt;This tutorial alone is what convinced me to try harder. It’s an interactive and well written way to learn how to move and perform basic operations in Helix. I quickly learned how to move around, select things, surround them with braces or parenthesis. I could see what I was about to do before doing it. This has been epiphany. Helix just worked the way I wanted.&lt;/p&gt;
    &lt;p&gt;Better: I could get things done faster than in VS Code after a few minutes of learning. Being a lazy person, I never bothered looking up VS Code shortcuts. Because the learning curve for Helix is slightly steeper, you have to learn those shortcuts that make moving around feel so easy.&lt;/p&gt;
    &lt;p&gt;Not only did I quickly get used to Helix key bindings: my vim muscle-memory didn’t get in the way at all!&lt;/p&gt;
    &lt;head rend="h3"&gt;Better docs&lt;/head&gt;
    &lt;p&gt;The built-in tutorial is a very pragmatic way to get started. You get results fast, you learn hands on, and it’s not that long. But if you want to go further, you have to look for docs. Helix has officials docs. They seem to be fairly complete, but they’re also impenetrable as a new user. They focus on what the editor supports and not on what I will want to do with it.&lt;/p&gt;
    &lt;p&gt;After a bit of browsing online, I’ve stumbled upon this third-party documentation website. The domain didn’t inspire me a lot of confidence, but the docs are really good. They are clearly laid out, use-case oriented, and they make the most of Astro Starlight to provide a great reading experience. The author tried to upstream these docs, but that won’t happen. It looks like they are upstreaming their docs to the current website. I hope this will improve the quality of upstream docs eventually.&lt;/p&gt;
    &lt;p&gt;After learning the basics and finding my way through the docs, it was time to ensure Helix was set up to help me where I needed it most.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting the most of Markdown and Astro in Helix&lt;/head&gt;
    &lt;p&gt;In my free time, I mostly use my editor for three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write notes in markdown&lt;/item&gt;
      &lt;item&gt;Tweak my website with Astro&lt;/item&gt;
      &lt;item&gt;Edit yaml to faff around my Kubernetes cluster&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Helix is a “stupid” text editor. It doesn’t know much about what you’re typing. But it supports Language Servers that implement the Language Server Protocol. Language Servers understand the document you’re editing. They explain to Helix what you’re editing, whether you’re in a TypeScript function, typing a markdown link, etc. With that information, Helix and the Language Server can provide code completion hints, errors &amp;amp; warnings, and easier navigation in your code.&lt;/p&gt;
    &lt;p&gt;In addition to Language Servers, Helix also supports plugging code formatters. Those are pieces of software that will read the document and ensure that it is consistently formatted. It will check that all indentations use spaces and not tabs, that there is a consistent number of space when indenting, that brackets are on the same line as the function, etc. In short: it will make the code pretty.&lt;/p&gt;
    &lt;head rend="h3"&gt;Markdown&lt;/head&gt;
    &lt;p&gt;Markdown is not really a programming language, so it might seem surprising to configure a Language Server for it. But if you remember what we said earlier, Language Servers can provide code completion, which is useful when creating links for example. Marksman does exactly that!&lt;/p&gt;
    &lt;p&gt;Since Helix is pre-configured to use marksman for markdown files we only need to install marksman and make sure it’s in our &lt;code&gt;PATH&lt;/code&gt;. Installing it with homebrew is enough.&lt;/p&gt;
    &lt;p&gt;We can check that Helix is happy with it with the following command&lt;/p&gt;
    &lt;p&gt;But Language Servers can also help Helix display errors and warnings, and “code suggestions” to help fix the issues. It means Language Servers are a perfect fit for… grammar checkers! Several grammar checkers exist. The most notable are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LTEX+, the Language Server used by Language Tool. It supports several languages must is quite resource hungry.&lt;/item&gt;
      &lt;item&gt;Harper, a grammar checker Language Server developed by Automattic, the people behind WordPress, Tumblr, WooCommerce, Beeper and more. Harper only support English and its variants, but they intend to support more languages in the future.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I mostly write in English and want to keep a minimalistic setup. Automattic is well funded, and I’m confident they will keep working on Harper to improve it. Since grammar checker LSPs can easily be changed, I’ve decided to go with Harper for now.&lt;/p&gt;
    &lt;p&gt;To install it, homebrew does the job as always:&lt;/p&gt;
    &lt;p&gt;Then I edited my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; to add Harper as a secondary Language Server in addition to marksman&lt;/p&gt;
    &lt;p&gt;Finally I can add a markdown linter to ensure my markdown is formatted properly. Several options exist, and markdownlint is one of the most popular. My colleagues recommended the new kid on the block, a Blazing Fast equivalent: rumdl.&lt;/p&gt;
    &lt;p&gt;Installing rumdl was pretty simple on my mac. I only had to add the repository of the maintainer, and install rumdl from it.&lt;/p&gt;
    &lt;p&gt;After that I added a new &lt;code&gt;language-server&lt;/code&gt; to my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; and added it to the language servers to use for the markdown &lt;code&gt;language&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Since my website already contained a &lt;code&gt;.markdownlint.yaml&lt;/code&gt; I could import it to the rumdl format with&lt;/p&gt;
    &lt;p&gt;You might have noticed that I’ve added a little quality of life improvement: soft-wrap at 80 characters.&lt;/p&gt;
    &lt;p&gt;Now if you add this to your own &lt;code&gt;config.toml&lt;/code&gt; you will notice that the text is completely left aligned. This is not a problem on small screens, but it rapidly gets annoying on wider screens.&lt;/p&gt;
    &lt;p&gt;Helix doesn’t support centering the editor. There is a PR tackling the problem but it has been stale for most of the year. The maintainers are overwhelmed by the number of PRs making it their way, and it’s not clear if or when this PR will be merged.&lt;/p&gt;
    &lt;p&gt;In the meantime, a workaround exists, with a few caveats. It is possible to add spaces to the left gutter (the column with the line numbers) so it pushes the content towards the center of the screen.&lt;/p&gt;
    &lt;p&gt;To figure out how many spaces are needed, you need to get your terminal width with &lt;code&gt;stty&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;In my case, when in full screen, my terminal is 243 characters wide. I need to remove the content column with from it, and divide everything by 2 to get the space needed on each side. In my case for a 243 character wide terminal with a text width of 80 characters:&lt;/p&gt;
    &lt;p&gt;As is, I would add 203 spaces to my left gutter to push the rest of the gutter and the content to the right. But the gutter itself has a width of 4 characters, that I need to remove from the total. So I need to subtract them from the total, which leaves me with &lt;code&gt;76&lt;/code&gt; characters to add.&lt;/p&gt;
    &lt;p&gt;I can open my &lt;code&gt;~/.config/helix/config.toml&lt;/code&gt; to add a new key binding that will automatically add or remove those spaces from the left gutter when needed, to shift the content towards the center.&lt;/p&gt;
    &lt;p&gt;Now when in normal mode, pressing Space then t then z will add/remove the spaces. Of course this workaround only works when the terminal runs in full screen mode.&lt;/p&gt;
    &lt;head rend="h3"&gt;Astro&lt;/head&gt;
    &lt;p&gt;Astro works like a charm in VS Code. The team behind it provides a Language Server and a TypeScript plugin to enable code completion and syntax highlighting.&lt;/p&gt;
    &lt;p&gt;I only had to install those globally with&lt;/p&gt;
    &lt;p&gt;Now we need to add a few lines to our &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; to tell it how to use the language server&lt;/p&gt;
    &lt;p&gt;We can check that the Astro Language Server can be used by helix with&lt;/p&gt;
    &lt;p&gt;I also like to get a formatter to automatically make my code consistent and pretty for me when I save a file. One of the most popular code formaters out there is Prettier. I’ve decided to go with the fast and easy formatter dprint instead.&lt;/p&gt;
    &lt;p&gt;I installed it with&lt;/p&gt;
    &lt;p&gt;Then in the projects I want to use dprint in, I do&lt;/p&gt;
    &lt;p&gt;I might edit the &lt;code&gt;dprint.json&lt;/code&gt; file to my liking. Finally, I configure Helix to use dprint globally for all Astro projects by appending a few lines in my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;One final check, and I can see that Helix is ready to use the formatter as well&lt;/p&gt;
    &lt;head rend="h3"&gt;YAML&lt;/head&gt;
    &lt;p&gt;For yaml, it’s simple and straightforward: Helix is preconfigured to use &lt;code&gt;yaml-language-server&lt;/code&gt; as soon as it’s in the PATH. I just need to install it with&lt;/p&gt;
    &lt;head rend="h2"&gt;Is it worth it?&lt;/head&gt;
    &lt;p&gt;Helix really grew on me. I find it particularly easy and fast to edit code with it. It takes a tiny bit more work to get the language support than it does in VS Code, but it’s nothing insurmountable. There is a slightly steeper learning curve than for VS Code, but I consider it to be a good thing. It forced me to learn how to move around and edit efficiently, because there is no way to do it inefficiently. Helix remains intuitive once you’ve learned the basics.&lt;/p&gt;
    &lt;p&gt;I am a GNOME enthusiast, and I adhere to the same principles: I like when my apps work out of the box, and when I have little to do to configure them. This is a strong stance that often attracts a vocal opposition. I like products that follow those principles better than those who don’t.&lt;/p&gt;
    &lt;p&gt;With that said, Helix sometimes feels like it is maintained by one or two people who have a strong vision, but who struggle to onboard more maintainers. As of writing, Helix has more than 350 PRs open. Quite a few bring interesting features, but the maintainers don’t have enough time to review them.&lt;/p&gt;
    &lt;p&gt;Those 350 PRs mean there is a lot of energy and goodwill around the project. People are willing to contribute. Right now, all that energy is gated, resulting in frustration both from the contributors who feel like they’re working in the void, and the maintainers who feel like there at the receiving end of a fire hose.&lt;/p&gt;
    &lt;p&gt;A solution to make everyone happier without sacrificing the quality of the project would be to work on a Contributor Ladder. CHAOSS’ Dr Dawn Foster published a blog post about it, listing interesting resources at the end.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ergaster.org/posts/2025/10/29-vscode-to-helix/"/><published>2025-10-29T13:19:30+00:00</published></entry></feed>