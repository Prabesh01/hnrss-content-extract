<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-22T15:11:49.230520+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46345205</id><title>How I protect my Forgejo instance from AI web crawlers</title><updated>2025-12-22T15:11:56.258267+00:00</updated><content>&lt;doc fingerprint="44e6f9513b184b8e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;TL;DR:&lt;/head&gt;
    &lt;p&gt;Put that in your nginx config:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;location / {
  # needed to still allow git clone from http/https URLs
  if ($http_user_agent ~* "git/|git-lfs/") {
        set $bypass_cookie 1;
  }
  # If we see the expected cookie; we could also bypass the blocker page
  if ($cookie_Yogsototh_opens_the_door = "1") {
        set $bypass_cookie 1;
  }
  # Redirect to 418 if neither condition is met
  if ($bypass_cookie != 1) {
     add_header Content-Type text/html always;
     return 418 '&amp;lt;script&amp;gt;document.cookie = "Yogsototh_opens_the_door=1; Path=/;"; window.location.reload();&amp;lt;/script&amp;gt;';
  }
  # rest of your nginx config
&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Preferably run a string replace from &lt;code&gt;Yogsototh_opens_the_door&lt;/code&gt; to your own personal Cookie
name.&lt;/p&gt;
    &lt;p&gt;Main advantage, is that it is almost invisible to the users of my website compartively to other solutions like Anubis.&lt;/p&gt;
    &lt;head rend="h1"&gt;More detail&lt;/head&gt;
    &lt;p&gt;Not so long ago I started to host my code to forgejo. There is a promise that in the future it will support federation and forgejo is the same project that is used for codeberg.&lt;/p&gt;
    &lt;p&gt;The only problem I had was one day, I discovered that my entire node was down. At first I didn't investigate and just restarted the node. But soon after a few hours, it was down again. Looking at the reason, clearly thousands of requests that looked at every commit which put too much pressure on the system. Who could be so interested in using the web API to look at every commit instead, of… you know, clone the repository locally and explore it. Quickly, yep, like so many of you, I discovered that tons of crawlers that did not respect the &lt;code&gt;robots.txt&lt;/code&gt;
are crawling my forgejo instance until death ensues.&lt;/p&gt;
    &lt;p&gt;So I had no choice, I first used a radical approach and blocked my website entirely except from me. But hey, why having a public forge if not for people to be able to look into it time to time?&lt;/p&gt;
    &lt;p&gt;I then installed Anubis, but it wasn't really for me. It is way too heavy for my needs, not as easy as I would have hoped to configure and install.&lt;/p&gt;
    &lt;p&gt;Then I saw this article You don't need anubis on lobste.rs using a simple configuration in caddy that should block these pesky crawlers. I made some adjustments to adapt it to nginx. For now, this is working perfectly well, my users are just redirected once, without really noticing it. And they could use forgejo as they could before. And this puts the crawlers away.&lt;/p&gt;
    &lt;p&gt;The strategy is pretty basic; in fact, a lot less advanced than the strategy adopted by Anubis. For every access of my website, I just check if the user has a specific cookie set. If not, I redirect the user to a 418 HTML page containing some js code to execute that set this specific cookie and reload the page.&lt;/p&gt;
    &lt;p&gt;That's it.&lt;/p&gt;
    &lt;p&gt;I also tried to return a 302 and add a cookie from the response without javascript, but the crawlers are immune to that second strategy. Unfortunately this means, my website could only be seen if you enable javascript in your browser. I feel this is acceptable. I guess, someday this very basic protection will not be enough and my forgejo instance will break again, and I will be forced to use more advanced system like Anubis or perhaps even iocaine.&lt;/p&gt;
    &lt;p&gt;I hope this could be helpful, because, I recently saw many discussions on that subject where people were not totally happy to use Anubis, while at least for me, this quick dirty fix does the trick. And I am fully aware that this would be very easy to bypass. But for now, I think the volume is more important than the quality for these crawlers and it may take a while for them to need to adapt. Also, by publishing this, I know if too many people use the same trick, quickly, these crawlers will adapt.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://her.esy.fun/posts/0031-how-i-protect-my-forgejo-instance-from-ai-web-crawlers/index.html"/><published>2025-12-21T14:46:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46345506</id><title>CO2 batteries that store grid energy take off globally</title><updated>2025-12-22T15:11:56.064390+00:00</updated><content>&lt;doc fingerprint="267d315709b6a538"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grid-Scale Bubble Batteries Will Soon Be Everywhere&lt;/head&gt;
    &lt;p&gt;When the sun sets on solar panels, these gas-filled domes take over&lt;/p&gt;
    &lt;p&gt;This giant bubble on the island of Sardinia holds 2,000 tonnes of carbon dioxide. But the gas wasn’t captured from factory emissions, nor was it pulled from the air. It came from a gas supplier, and it lives permanently inside the dome’s system to serve an eco-friendly purpose: to store large amounts of excess renewable energy until it’s needed.&lt;/p&gt;
    &lt;p&gt;Developed by the Milan-based company Energy Dome, the bubble and its surrounding machinery demonstrate a first-of-its-kind “CO2 Battery,” as the company calls it. The facility compresses and expands CO2 daily in its closed system, turning a turbine that generates 200 megawatt-hours of electricity, or 20 MW over 10 hours. And in 2026, replicas of this plant will start popping up across the globe.&lt;/p&gt;
    &lt;p&gt;We mean that literally. It takes just half a day to inflate the bubble. The rest of the facility takes less than two years to build and can be done just about anywhere there’s 5 hectares of flat land.&lt;/p&gt;
    &lt;p&gt;The first to build one outside of Sardinia will be one of India’s largest power companies, NTPC Limited. The company expects to complete its CO2 Battery sometime in 2026 at the Kudgi power plant in Karnataka, in India. In Wisconsin, meanwhile, the public utility Alliant Energy received the all clear from authorities to begin construction of one in 2026 to supply power to 18,000 homes.&lt;/p&gt;
    &lt;p&gt;And Google likes the concept so much that it plans to rapidly deploy the facilities in all of its key data-center locations in Europe, the United States, and the Asia-Pacific region. The idea is to provide electricity-guzzling data centers with round-the-clock clean energy, even when the sun isn’t shining or the wind isn’t blowing. The partnership with Energy Dome, announced in July, marked Google’s first investment in long-duration energy storage.&lt;/p&gt;
    &lt;p&gt;“We’ve been scanning the globe seeking different solutions,” says Ainhoa Anda, Google’s senior lead for energy strategy, in Paris. The challenge the tech giant has encountered is not only finding a long-duration storage option, but also one that works with the unique specs of every region. “So standardization is really important, and this is one of the aspects that we really like” about Energy Dome, she says. “They can really plug and play this.”&lt;/p&gt;
    &lt;p&gt;Google will prioritize placing the Energy Dome facilities where they’ll have the most impact on decarbonization and grid reliability, and where there’s a lot of renewable energy to store, Anda says. The facilities can be placed adjacent to Google’s data centers or elsewhere within the same grid. The companies did not disclose the terms of the deal.&lt;/p&gt;
    &lt;p&gt;Anda says Google expects to help the technology “reach a massive commercial stage.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting creative with long-duration energy storage&lt;/head&gt;
    &lt;p&gt;All this excitement is based on Energy Dome’s one full-size, grid-connected plant in Ottana, Sardinia, which was completed in July. It was built to help solve one of the energy transition’s biggest challenges: the need for grid-scale storage that can provide power for more than 8 hours at a time. Called long-duration energy storage, or LDES in industry parlance, the concept is the key to maximizing the value of renewable energy.&lt;/p&gt;
    &lt;p&gt;When sun and wind are abundant, solar and wind farms tend to produce more electricity than a grid needs. So storing the excess for use when these resources are scarce just makes sense. LDES also makes the grid more reliable by providing backup and supplementary power.&lt;/p&gt;
    &lt;p&gt;The problem is that even the best new grid-scale storage systems on the market—mainly lithium-ion batteries—provide only about 4 to 8 hours of storage. That’s not long enough to power through a whole night, or multiple cloudy and windless days, or the hottest week of the year, when energy demand hits its peak.&lt;/p&gt;
    &lt;p&gt;After the CO2 leaves the dome, it is compressed, cooled, reduced to a liquid, and stored in pressure vessels. To release the energy, the process reverses: The liquid is evaporated, heated, expanded, and then fed through a turbine that generates electricity. Luigi Avantaggiato&lt;/p&gt;
    &lt;p&gt;Lithium-ion battery systems could be increased in size to store more and last longer, but systems of that size usually aren’t economically viable. Other grid-scale battery chemistries and approaches are in development, such as sodium-based, iron-air, and vanadium redox flow batteries. But the energy density, costs, degradation, and funding complications have challenged the developers of those alternatives.&lt;/p&gt;
    &lt;p&gt;Researchers have also experimented with storing energy by compressing air, heating up blocks or sand, using hydrogen or methanol, pressurizing water deep underground, and even dangling heavy objects in the air and dropping them. (The creativity devoted to LDES is impressive.) But geologic constraints, economic viability, efficiency, and scalability have hindered the commercialization of these strategies.&lt;/p&gt;
    &lt;p&gt;The tried-and-true grid-scale storage option—pumped hydro, in which water is pumped between reservoirs at different elevations—lasts for decades and can store thousands of megawatts for days. But these systems require specific topography, a lot of land, and can take up to a decade to build.&lt;/p&gt;
    &lt;p&gt;CO2 Batteries check a lot of boxes that other approaches don’t. They don’t need special topography like pumped-hydro reservoirs do. They don’t need critical minerals like electrochemical and other batteries do. They use components for which supply chains already exist. Their expected lifetime stretches nearly three times as long as lithium-ion batteries. And adding size and storage capacity to them significantly decreases cost per kilowatt-hour. Energy Dome expects its LDES solution to be 30 percent cheaper than lithium-ion.&lt;/p&gt;
    &lt;p&gt;China has taken note. China Huadian Corp. and Dongfang Electric Corp. are reportedly building a CO2-based energy-storage facility in the Xinjiang region of northwest China. Media reports show renderings of domes but give widely varying storage capacities—including 100 MW and 1,000 MW. The Chinese companies did not respond to IEEE Spectrum’s requests for information.&lt;/p&gt;
    &lt;p&gt;“What I can say is that they are developing something very, very similar [to Energy Dome’s CO2 Battery] but quite large in scale,” says Claudio Spadacini, Energy Dome’s founder and CEO. The Chinese companies “are good, they are super fast, and they have a lot of money,” he says.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why is Google investing in CO2 Batteries?&lt;/head&gt;
    &lt;p&gt;When I visited Energy Dome’s Sardinia facility in October, the CO2 had just been pumped out of the dome, so I was able to peek inside. It was massive, monochromatic, and pretty much empty. The inner membrane, which had been holding the uncompressed CO2, had collapsed across the entire floor. A few pockets of the gas remained, making the off-white sheet billow up in spots.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the translucent outer dome allowed some daylight to pass through, creating a creamy glow that enveloped the vast space. With no structural framing, the only thing keeping the dome upright was the small difference in pressure between the inside and outside air.&lt;/p&gt;
    &lt;p&gt;“This is incredible,” I said to my guide, Mario Torchio, Energy Dome’s global marketing and communications director.&lt;/p&gt;
    &lt;p&gt;“It is. But it’s physics,” he said.&lt;/p&gt;
    &lt;p&gt;Outside the dome, a series of machines connected by undulating pipes moves the CO2 out of the dome for compressing and condensing. First, a compressor pressurizes the gas from 1 bar (100,000 pascals) to about 55 bar (5,500,000 pa). Next, a thermal-energy-storage system cools the CO2 to an ambient temperature. Then a condenser reduces it into a liquid that is stored in a few dozen pressure vessels, each about the size of a school bus. The whole process takes about 10 hours, and at the end of it, the battery is considered charged.&lt;/p&gt;
    &lt;p&gt;To discharge the battery, the process reverses. The liquid CO2 is evaporated and heated. It then enters a gas-expander turbine, which is like a medium-pressure steam turbine. This drives a synchronous generator, which converts mechanical energy into electrical energy for the grid. After that, the gas is exhausted at ambient pressure back into the dome, filling it up to await the next charging phase.&lt;/p&gt;
    &lt;p&gt;Energy Dome engineers inspect the dryer system, which keeps the gaseous CO₂ in the dome at optimal dryness levels at all times.Luigi Avantaggiato&lt;/p&gt;
    &lt;p&gt;It’s not rocket science. Still, someone had to be the first to put it together and figure out how to do it cost-effectively, which Spadacini says his company has accomplished and patented. “How we seal the turbo machinery, how we store the heat in the thermal-energy storage, how we store the heat after condensing…can really cut costs and increase the efficiency,” he says.&lt;/p&gt;
    &lt;p&gt;The company uses pure, purpose-made CO2 instead of sourcing it from emissions or the air, because those sources come with impurities and moisture that degrade the steel in the machinery.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happens if the dome is punctured?&lt;/head&gt;
    &lt;p&gt;On the downside, Energy Dome’s facility takes up about twice as much land as a comparable capacity lithium-ion battery would. And the domes themselves, which are about the height of a sports stadium at their apex, and longer, might stand out on a landscape and draw some NIMBY pushback.&lt;/p&gt;
    &lt;p&gt;And what if a tornado comes? Spadacini says the dome can withstand wind up to 160 kilometers per hour. If Energy Dome can get half a day’s warning of severe weather, the company can just compress and store the CO2 in the tanks and then deflate the outer dome, he says.&lt;/p&gt;
    &lt;p&gt;If the worst happens and the dome is punctured, 2,000 tonnes of CO2 will enter the atmosphere. That’s equivalent to the emissions of about 15 round-trip flights between New York and London on a Boeing 777. “It’s negligible compared to the emissions of a coal plant,” Spadacini says. People will also need to stay back 70 meters or more until the air clears, he says.&lt;/p&gt;
    &lt;p&gt;Worth the risk? The companies lining up to build these systems seem to think so.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grid-Scale Battery Stabilizes Scottish Power Supply ›&lt;/item&gt;
      &lt;item&gt;Backing Up the Power Grid With Green Methanol ›&lt;/item&gt;
      &lt;item&gt;DOE Places Compressed-Air Energy Storage Loan Under Review ›&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/co2-battery-energy-storage"/><published>2025-12-21T15:27:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46345897</id><title>Show HN: Books mentioned on Hacker News in 2025</title><updated>2025-12-22T15:11:55.967705+00:00</updated><link href="https://hackernews-readings-613604506318.us-west1.run.app"/><published>2025-12-21T16:21:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46348262</id><title>Rue: Higher level than Rust, lower level than Go</title><updated>2025-12-22T15:11:55.804252+00:00</updated><content>&lt;doc fingerprint="d268deffe87878fc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Memory Safe&lt;/head&gt;
    &lt;p&gt;No garbage collector, no manual memory management. A work in progress, though.&lt;/p&gt;
    &lt;head rend="h2"&gt;Simple Syntax&lt;/head&gt;
    &lt;p&gt;Familiar syntax inspired by various programming languages. If you know one, you'll feel at home with Rue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast Compilation&lt;/head&gt;
    &lt;p&gt;Direct compilation to native code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hello, Rue&lt;/head&gt;
    &lt;code&gt;// It's a classic for a reason
 

 
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rue-lang.dev/"/><published>2025-12-21T20:46:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46348318</id><title>More on whether useful quantum computing is “imminent”</title><updated>2025-12-22T15:11:55.604595+00:00</updated><content>&lt;doc fingerprint="6f11911b6025ddfc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;More on whether useful quantum computing is “imminent”&lt;/head&gt;
    &lt;p&gt;These days, the most common question I get goes something like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A decade ago, you told people that scalable quantum computing wasn’t imminent. Now, though, you claim it plausibly is imminent. Why have you reversed yourself??&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I appreciated the friend of mine who paraphrased this as follows: “A decade ago you said you were 35. Now you say you’re 45. Explain yourself!”&lt;/p&gt;
    &lt;p&gt;A couple weeks ago, I was delighted to attend Q2B in Santa Clara, where I gave a keynote talk entitled “Why I Think Quantum Computing Works” (link goes to the PowerPoint slides). This is one of the most optimistic talks I’ve ever given. But mostly that’s just because, uncharacteristically for me, here I gave short shrift to the challenge of broadening the class of problems that achieve huge quantum speedups, and just focused on the experimental milestones achieved over the past year. With every experimental milestone, the little voice in my head that asks “but what if Gil Kalai turned out to be right after all? what if scalable QC wasn’t possible?” grows quieter, until now it can barely be heard.&lt;/p&gt;
    &lt;p&gt;Going to Q2B was extremely helpful in giving me a sense of the current state of the field. Ryan Babbush gave a superb overview (I couldn’t have improved a word) of the current status of quantum algorithms, while John Preskill’s annual where-we-stand talk was “magisterial” as usual (that’s the word I’ve long used for his talks), making mine look like just a warmup act for his. Meanwhile, Quantinuum took a victory lap, boasting of their recent successes in a way that I considered basically justified.&lt;/p&gt;
    &lt;p&gt;After returning from Q2B, I then did an hour-long podcast with “The Quantum Bull” on the topic “How Close Are We to Fault-Tolerant Quantum Computing?” You can watch it here:&lt;/p&gt;
    &lt;p&gt;As far as I remember, this is the first YouTube interview I’ve ever done that concentrates entirely on the current state of the QC race, skipping any attempt to explain amplitudes, interference, and other basic concepts. Despite (or conceivably because?) of that, I’m happy with how this interview turned out. Watch if you want to know my detailed current views on hardware—as always, I recommend 2x speed.&lt;/p&gt;
    &lt;p&gt;Or for those who don’t have the half hour, a quick summary:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In quantum computing, there are the large companies and startups that might succeed or might fail, but are at least trying to solve the real technical problems, and some of them are making amazing progress. And then there are the companies that have optimized for doing IPOs, getting astronomical valuations, and selling a narrative to retail investors and governments about how quantum computing is poised to revolutionize optimization and machine learning and finance. Right now, I see these two sets of companies as almost entirely disjoint from each other.&lt;/item&gt;
      &lt;item&gt;The interview also contains my most direct condemnation yet of some of the wild misrepresentations that IonQ, in particular, has made to governments about what QC will be good for (“unlike AI, quantum computers won’t hallucinate because they’re deterministic!”)&lt;/item&gt;
      &lt;item&gt;The two approaches that had the most impressive demonstrations in the past year are trapped ions (especially Quantinuum but also Oxford Ionics) and superconducting qubits (especially Google but also IBM), and perhaps also neutral atoms (especially QuEra but also Infleqtion and Atom Computing).&lt;/item&gt;
      &lt;item&gt;Contrary to a misconception that refuses to die, I haven’t dramatically changed my views on any of these matters. As I have for a quarter century, I continue to profess a lot of confidence in the basic principles of quantum computing theory worked out in the mid-1990s, and I also continue to profess ignorance of exactly how many years it will take to realize those principles in the lab, and of which hardware approach will get there first.&lt;/item&gt;
      &lt;item&gt;But yeah, of course I update in response to developments on the ground, because it would be insane not to! And 2025 was clearly a year that met or exceeded my expectations on hardware, with multiple platforms now boasting &amp;gt;99.9% fidelity two-qubit gates, at or above the theoretical threshold for fault-tolerance. This year updated me in favor of taking more seriously the aggressive pronouncements—the “roadmaps”—of Google, Quantinuum, QuEra, PsiQuantum, and other companies about where they could be in 2028 or 2029.&lt;/item&gt;
      &lt;item&gt;One more time for those in the back: the main known applications of quantum computers remain (1) the simulation of quantum physics and chemistry themselves, (2) breaking a lot of currently deployed cryptography, and (3) eventually, achieving some modest benefits for optimization, machine learning, and other areas (but it will probably be a while before those modest benefits win out in practice). To be sure, the detailed list of quantum speedups expands over time (as new quantum algorithms get discovered) and also contracts over time (as some of the quantum algorithms get dequantized). But the list of known applications “from 30,000 feet” remains fairly close to what it was a quarter century ago, after you hack away the dense thickets of obfuscation and hype.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m going to close this post with a warning. When Frisch and Peierls wrote their now-famous memo in March 1940, estimating the mass of Uranium-235 that would be needed for a fission bomb, they didn’t publish it in a journal, but communicated the result through military channels only. As recently as February 1939, Frisch and Meitner had published in Nature their theoretical explanation of recent experiments, showing that the uranium nucleus could fission when bombarded by neutrons. But by 1940, Frisch and Peierls realized that the time for open publication of these matters had passed.&lt;/p&gt;
    &lt;p&gt;Similarly, at some point, the people doing detailed estimates of how many physical qubits and gates it’ll take to break actually deployed cryptosystems using Shor’s algorithm are going to stop publishing those estimates, if for no other reason than the risk of giving too much information to adversaries. Indeed, for all we know, that point may have been passed already. This is the clearest warning that I can offer in public right now about the urgency of migrating to post-quantum cryptosystems, a process that I’m grateful is already underway.&lt;/p&gt;
    &lt;p&gt;Update: Someone on Twitter who’s “long $IONQ” says he’ll be posting about and investigating me every day, never resting until UT Austin fires me, in order to punish me for slandering IonQ and other “pure play” SPAC IPO quantum companies. And also, because I’ve been anti-Trump and pro-Biden. He confabulates that I must be trying to profit from my stance (eg by shorting the companies I criticize), it being inconceivable to him that anyone would say anything purely because they care about what’s true.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://scottaaronson.blog/?p=9425"/><published>2025-12-21T20:53:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46348329</id><title>A guide to local coding models</title><updated>2025-12-22T15:11:55.428848+00:00</updated><content>&lt;doc fingerprint="2f539dd2343f89db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;[Revised] You Don’t Need to Spend $100/mo on Claude Code: Your Guide to Local Coding Models&lt;/head&gt;
    &lt;head rend="h3"&gt;What you need to know about local model tooling and the steps for setting one up yourself&lt;/head&gt;
    &lt;p&gt;[Edit 1] This article has been edited after initial release for clarity. Both the tl;dr and the end section have added information.&lt;/p&gt;
    &lt;p&gt;[Edit 2] This hypothesis was actually wrong and thank you to everyone who commented!&lt;/p&gt;
    &lt;p&gt;Here’s a full explanation of where I went wrong. I want to address this mistake as I realize it might have a meaningful impact on someone's financial position.&lt;/p&gt;
    &lt;p&gt;I’m not editing the actual article except where absolutely necessary so it doesn’t look like I’m covering up the mistake—I want to address it. Instead, I’ve included the important information below.&lt;/p&gt;
    &lt;p&gt;There is one takeaway this article provides that definitely holds true:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Local models are far more capable than they’re given credit for, even for coding.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It also explains the process of setting up a local coding model and technical information about doing so which is helpful for anyone wanting to set up a local coding model. I would still recommend doing so.&lt;/p&gt;
    &lt;p&gt;But do I want someone reading this to immediately drop their coding subscription and buy a maxed out MacBook Pro? No, and for that reason I need to correct my hypothesis from ‘Yes, with caveats’ to ‘No’.&lt;/p&gt;
    &lt;p&gt;This article was not an empirical assessment, but should have been to make these claims. Here’s where I went wrong:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;While local models can likely complete ~90% of the software development tasks that something like Claude Code can, the last 10% is the most important. When it comes to your job, that last 10% is worth paying more for to get that last bit of performance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I realized I looked at this more from the angle of a hobbiest paying for these coding tools. Someone doing little side projects—not someone in a production setting. I did this because I see a lot of people signing up for $100/mo or $200/mo coding subscriptions for personal projects when they likely don’t need to. I would not recommend running local models as a company instead of giving employees access to a tool like Claude Code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;While larger local models are very capable, as soon as you run other development tools (Docker, etc.) that also eat into your RAM, your model needs to be much smaller and becomes a lot less capable. I didn’t factor this in in my experiment.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, really, the takeaway should be that these are incredible supplemental models to frontier models when coding and could potentially save you on your subscription by dropping it down a tier, but practically they’re not worth the effort in situations that might affect your livelihood.&lt;/p&gt;
    &lt;p&gt;Exactly a month ago, I made a hypothesis: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price (and have better hardware too!).&lt;/p&gt;
    &lt;p&gt;So, to create by far the most expensive article I’ve ever written, I put my money where my mouth is and bought a MacBook Pro with 128 GB of RAM to get to work. My idea was simple: Over the life of the MacBook I’d recoup the costs of it by not paying for an AI coding subscription.&lt;/p&gt;
    &lt;p&gt;After weeks of experimenting and setting up local AI models and coding tools, I’ve come to the conclusion that my hypothesis was &lt;del&gt;correct, with nuance&lt;/del&gt;, not correct [see edit 2 above] which I’ll get into later in this article.&lt;/p&gt;
    &lt;p&gt;In this article, we cover:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Why local models matter and the benefits they provide.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How to view memory usage and make estimates for which models can run on your machine and the RAM demands for coding applications.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Walk through setting up your own local coding model and tool step-by-step.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Don’t worry if you don’t have a high-RAM machine! You can still follow this guide. I’ve included some models to try out with a lower memory allotment. I think you’ll be surprised at how performant even the smallest of models is. In fact, there hasn’t really been a time during this experiment that I’ve been disappointed with model performance.&lt;/p&gt;
    &lt;p&gt;If you’re only here for the local coding tool setup, skip to the section at the bottom. I’ve even included a link to my modelfiles in that section to make setup even easier for you. Otherwise, let’s get into what you need to know.&lt;/p&gt;
    &lt;head rend="h2"&gt;tl;dr:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Local coding models are very capable. Using the right model and the right tooling feels only half a generation behind the frontier cloud tools. I would say that for about 90% of developer work local models are more than sufficient. Even small 7B parameter models can be very capable. [Edited to add in this next part] Local models won’t compete with frontier models at the peak of performance, but can complete many coding tasks just as well for a fraction of the cost. They’re worth running to bring costs down on plenty of tasks but potentially not worth using if there’s a free tier available that performs better.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tools matter a lot. This is where I experienced the most disappointment. I tried many different tools with many different models and spent a lot of time tinkering. I ran into situations where the models wouldn’t call tools properly or their thinking traces wouldn’t close. Both of these rendered the tool essentially useless. Currently, tooling seems very finicky and if there’s anything developers need to be successful, it’s good tools.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There’s a lot to consider when you’re actually working within hardware constraints. We take the tooling set up for us in the cloud for granted. When setting up local models, I had to think a lot about trade-offs in performance versus memory usage, how different tools compared and affected performance, nuances in types of models, how to quantize, and other user-facing factors such as time-to-first-token and tokens per second.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Google threw a wrench into my hypothesis. The local setup is almost a no-brainer when compared to a $100/mo+ subscription. Compared to free or nearly-free tooling (such as Gemini CLI, Jules, or Antigravity) there isn’t quite as strong of a monetary justification to spend more on hardware. There are benefits to local models outside of code, though, and I discuss those below.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the tl;dr was helpful, don’t forget to subscribe to get more in your inbox.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why local models?&lt;/head&gt;
    &lt;p&gt;You might wonder why local models are worth investing in at all. The obvious answer is cost. By using your own hardware, you don’t need to pay a subscription fee to a cloud provider for your tool. There are also a few less obvious and underrated reasons that make local models useful.&lt;/p&gt;
    &lt;p&gt;First: Reliability. Each week there seems to be complaints about performance regression within AI coding tools. Many speculate companies are pulling tricks to save resources that hurt model performance. With cloud providers, you’re at the mercy of the provider for when this happens. With local models, this only happens when you cause it to.&lt;/p&gt;
    &lt;p&gt;Second: Local models can apply to far more applications. Just the other day I was having a discussion with my dad about AI tooling he could use to streamline his work. His job requires studying a lot of data—a perfect application for an LLM-based tool—but his company blocks tools like Gemini and ChatGPT because a lot of this analysis is done on intellectual property. Unfortunately, he isn’t provided a suitable alternative to use.&lt;/p&gt;
    &lt;p&gt;With a local model, he wouldn’t have to worry about these IP issues. He could run his analyses without data ever leaving his machine. Of course, any tool calling would also need to ensure data never leaves the machine, but local models get around one of the largest hurdles for useful enterprise AI adoption. Running models on a local machine opens up an entire world of privacy- and security-centric AI applications that are expensive for cloud providers to provide.&lt;/p&gt;
    &lt;p&gt;Finally: Availability. Local models are available to you as long as your machine is. This means no worrying about your provider being down or rate limiting you due to high traffic. It also means using AI coding tools on planes or in other situations where internet access is locked down (think highly secure networks).&lt;/p&gt;
    &lt;p&gt;While local models do provide significant cost savings, the flexibility and reliability they provide can be even more valuable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Understanding memory&lt;/head&gt;
    &lt;p&gt;To get going with local models you must understand the memory needed to run them on your machine. Obviously, if you have more memory you’ll be able to run better models, but understanding the nuances of that memory management will help you pick out the right model for your use case.&lt;/p&gt;
    &lt;p&gt;Local AI has two parts that eat up your memory: The model itself and the model’s context window.&lt;/p&gt;
    &lt;p&gt;The actual model has billions of parameters and all those parameters need to fit into your memory at once. Excellent local coding models start at around 30 billion (30B, for short) parameters in size. By default, these models use 16 bits to represent parameters. At 16 bits with 30B parameters, a model will take 60 GB of space in RAM (16 bits = 2 bytes per parameter, 30 billion parameters = 60 billion bytes which equals about 60 GB).&lt;/p&gt;
    &lt;p&gt;The second (and potentially larger) memory consuming part of local AI is the model’s context window. This is the model inputs and outputs that are stored so the model can reference them in future requests. This gives the model memory.&lt;/p&gt;
    &lt;p&gt;When coding with AI, we prefer this window to be as large as it can because we need to fit our codebase (or pieces of it) within our context window. This means we target a context window of 64,000 tokens or larger. All of these tokens will also be stored in RAM.&lt;/p&gt;
    &lt;p&gt;The important thing to understand about context windows is that the memory requirement per-token for a model depends on the size of that model. Models with more parameters tend to have large architectures (more hidden layers and larger dimensions to those layers). Larger architectures mean the model must store more information for each token within its key-value cache (context window) because it stores information for each token for each layer.&lt;/p&gt;
    &lt;p&gt;This means choosing an 80B parameter model over a 30B parameter model requires more memory for the model itself and also more memory for the same size context window. For example, a 30B parameter model might have a hidden dimension of 5120 with 64 layers while an 80B model has a hidden dimension of 8192 with 80 layers. Doing some back-of-the-napkin math shows us that the larger model requires approximately 2x more RAM to maintain the same context window as the 30B parameter model (see formula below).&lt;/p&gt;
    &lt;p&gt;Luckily, there are tricks to better manage memory. First, there are architectural changes that can be made to make model inference more efficient so it requires less memory. The model we set up at the end of this article uses Hybrid Attention which enables a much smaller KV cache enabling us to fit our model and context window in less memory. I won’t get into more detail in this article, but you can read more about that model and how it works here.&lt;/p&gt;
    &lt;p&gt;The second trick is quantizing the values you’re working with. Quantization means converting a continuous set of values into a smaller amount of distinct values. In our case, that means taking a set of numbers represented by a certain number of bits (16, for example) and reducing it to a set of numbers represented by fewer bits (8, for example). To put it simply, in our case we’re converting the numbers representing our model to a smaller bit representation to save memory while keeping the value representations within the model relatively equal.&lt;/p&gt;
    &lt;p&gt;You can quantize both your model weights and the values stored in your context window. When you quantize your model weights, you “remove intelligence” from the model because it’s less precise in its representation of innate information. I’ve also found the performance hit when going from 16 to 8 bits within the model to be much less than 8 to 4.&lt;/p&gt;
    &lt;p&gt;We can also quantize the values in our context window to reduce its memory requirement. This means we’re less precisely representing the model’s memory. Generally speaking, KV cache (context window) quantization is considered more destructive to model performance than weight quantization because it causes the model to forget details in long reasoning traces. Thus, you should test quantizing the KV cache to ensure it doesn’t degrade model performance for your specific task.&lt;/p&gt;
    &lt;p&gt;In reality, like the rest of machine learning, optimizing local model performance is an experimentation process and real-world machine learning requires understanding the practical limitations and capabilities of models when applied to specific applications.&lt;/p&gt;
    &lt;p&gt;Here are a few more factors to understand when setting up a local coding model on your hardware:&lt;/p&gt;
    &lt;head rend="h3"&gt;Instruct versus non-instruct&lt;/head&gt;
    &lt;p&gt;Instruct models are post-trained to be well-suited for chat-based interactions. They’re given chat pairings in their training to be optimized for excellent back-and-forth chat output. Non-instruct models are still trained LLMs, but focus on next-token prediction instead of chatting with a user. For our case, when using a chat-based coding tool (CLI or chat agent in your IDE) we need to use an instruct model. If you’re setting up an autocomplete model, you’ll want to find a model specifically post-trained for it (such as Qwen2.5-Coder-Base or DeepSeek-Coder-V2).&lt;/p&gt;
    &lt;head rend="h3"&gt;Serving tools&lt;/head&gt;
    &lt;p&gt;You need a tool to serve your local LLM for your coding tool to send it requests. On a MacBook, there are two primary options: MLX and Ollama.&lt;/p&gt;
    &lt;p&gt;Ollama is the industry standard and works on non-Mac hardware. It’s a great serving setup on top of llama.cpp that makes model serving almost plug-and-play. Users can download model weights from Ollama easily and can configure modelfiles with custom parameters for serving. Ollama can also serve a model once and make it available to multiple tools.&lt;/p&gt;
    &lt;p&gt;MLX is a Mac-specific framework for machine learning that is optimized specifically for Mac hardware. It also retrieves models for the user from a community collection. I’ve found Ollama to be very reliable in its model catalog, while MLX’s catalog is community sourced and can sometimes be missing specific models. Models are sourced from the community so a user can convert a model to MLX format themselves. MLX requires a bit more setup on the user’s end, but serves models faster because it doesn’t have a layer providing the niceties of Ollama on top of it.&lt;/p&gt;
    &lt;p&gt;Either of these is great, but I chose MLX to maximize what I can get with my RAM, but Ollama is probably the more beginner-friendly tool here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Time-to-first-token and tokens per second&lt;/head&gt;
    &lt;p&gt;In real-world LLM applications it’s important that the model is able to serve its first token for a request in a reasonable amount of time and continue serving tokens at a speed that enables the user to use the model for its given purpose. If we have a high-performance model running locally, but it only serves a few tokens per second, it wouldn’t be useful for coding.&lt;/p&gt;
    &lt;p&gt;This is something taken for granted with cloud-hosted models that is a real consideration when working locally on constrained hardware. Another reason I chose MLX as my serving platform is because it served tokens up to 20% faster than Ollama. In reality, Ollama served tokens fast enough so I don’t think using MLX is necessary specifically for this reason for the models I tried.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance trade-offs&lt;/head&gt;
    &lt;p&gt;There are many ways to optimize local models and save RAM. It’s difficult to know which optimization method works best and the impact each has on a model especially when using them in tandem with other methods.&lt;/p&gt;
    &lt;p&gt;The right optimization method also depends on the application. In my experience, I find it best to prioritize larger models with more aggressive model quantization over smaller models with more precise model weights. Since our application is coding, I would also prioritize a less-quantized KV cache and using a smaller model to ensure reasoning works properly while not sacrificing the size of our context window.&lt;/p&gt;
    &lt;head rend="h3"&gt;Coding tools&lt;/head&gt;
    &lt;p&gt;There are many tools to code with local models and I suggest trying until you find one you like. Some top recommendations are OpenCode, Aider, Qwen Code, Roo Code, and Continue. Make sure to use a tool compatible with OpenAI’s API standard. While this should be most tools, this ensures a consistent model/tool connection. This makes it easier to switch between tools and models as needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting set up&lt;/head&gt;
    &lt;p&gt;I’ll spare you the trial and error I experienced getting this set up. The one thing I learned is that tooling matters a lot. Not all coding tools are created equal and not all of the models interact with tools equally. I experienced many times where tool calling or even running a tool at all was broken. I also had to tinker quite a bit with many of them to get them to work.&lt;/p&gt;
    &lt;p&gt;If you’re a PC enthusiast, an apt comparison to setting up local coding tools versus using the cloud offerings available is the difference between setting up a MacBook versus a Linux Laptop. With the Linux laptop, you might get well through the distro installation only to find that the drivers for your trackpad aren’t yet supported. Sometimes it felt like that with local models and hooking them to coding tools.&lt;/p&gt;
    &lt;p&gt;For my tool, I ended up going with Qwen Code. It was pretty plug-and-play as it’s a fork of Gemini CLI. It supports the OpenAI compatibility standard so I can easily sub in different models and affords me all of the niceties built into Gemini CLI that I’m familiar with using. I also know it’ll be supported because both the Qwen team and Google DeepMind are behind the tool. The tool is also open source so anyone can support it as needed.&lt;/p&gt;
    &lt;p&gt;For models, I focused on GPT-OSS and Qwen3 models since they were around the size I was looking for and had great reviews for coding. I ended up deciding to use Qwen3-Coder models because I found it performed best and because GPT-OSS frequently gave me “I cannot fulfill this request” responses when I asked it to build features.&lt;/p&gt;
    &lt;p&gt;I decided to serve my local models on MLX, but if you’re using a non-Mac device give Ollama a shot. A MacBook is an excellent machine for serving local models because of its unified memory architecture. This means the RAM can be allotted to the CPU or GPU as needed. MacBooks can also be configured with a ton of RAM. For serving local coding models, more is always better.&lt;/p&gt;
    &lt;p&gt;I’ve shared my modelfiles repo for you to reference and use as needed. I’ve got a script set up that automates much of the below process. Feel free to fork it and create your own modelfiles or star it to come back later.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Install MLX or download Ollama (the rest of this guide will continue with MLX but details for serving on Ollama can be found here).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Increase the VRAM limitation on your MacBook. macOS will automatically limit VRAM to 75% of the total RAM. We want to use more than that. Run sudo sysctl iogpu.wired_limit_mb=110000 in your terminal to set this up (adjust the mb setting according to the RAM on your MacBook). This needs to be set each time you restart your MacBook.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run pip install -U mlx-lm to install MLX for serving community models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Serve the model as an OpenAI compatible API using python -m mlx_lm.server --model mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit. This command both runs the server and downloads the model for you if you haven’t yet. This particular model is what I’m using with 128GB of RAM. If you have less RAM, check out smaller models such as mlx-community/Qwen3-4B-Instruct-2507-4bit (8 GB RAM), mlx-community/Qwen2.5-14B-Instruct-4bit (16 GB RAM), mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit (32 GB RAM), or mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit (64-96 GB RAM).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Download Qwen Code. You might need to install Node Package Manager for this. I recommend using Node Version Manager (nvm) for managing your npm version.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Set up your tool to access an OpenAI compatible API by entering the following settings:&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;Base URL: http://localhost:8080/v1 (should be the default MLX serves your model at)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;API Key: mlx&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Model Name: mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit (or whichever model you chose).&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Voila! Your coding model tool should be working with your local coding model.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I recommend opening Activity Monitor on your Mac to monitor memory usage. I’ve had cases where I thought a model should fit within my memory allotment but it didn’t and I ended up using a lot of swap memory. When this happens your model will run very slowly.&lt;/p&gt;
    &lt;p&gt;One tip I have for using local coding models: Focus on managing your context. This is a great skill even with cloud-based models. People tend to YOLO their chats and fill their context window, but I’ve found greater performance by ensuring that just what my model needs is sitting in my context window. This is even more important with local models that may need an extra boost in performance and are limited in their context.&lt;/p&gt;
    &lt;head rend="h2"&gt;Was my hypothesis correct?&lt;/head&gt;
    &lt;p&gt;My original hypothesis was: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price.&lt;/p&gt;
    &lt;p&gt;I would argue that&lt;del&gt;—yes!—&lt;/del&gt;no [see edit 2 above], it is correct. If we crunch the numbers, a MacBook with 128 GB is $4700 plus tax. If I spend $100/mo for 5 years, a coding subscription would cost $6000 in that same amount of time. Not only do I save money, but I also get a much more capable machine for anything else I want to do with it.&lt;/p&gt;
    &lt;p&gt;[This paragraph was added in after initial release of this article] It’s important to note that local models will not reach the peak performance of frontier models; however, they will likely be able to do most tasks just as well. The value of using a local model doesn’t come from raw performance, but from supplementing the cost of higher performance models. A local model could very well let you drop your subscription tier for a frontier coding tool or utilize a free tier as needed for better performance and run the rest of your tasks for free.&lt;/p&gt;
    &lt;p&gt;It’s also important to note that local models are only going to get better and smaller. This is the worst your local coding model will perform. I also wouldn’t be surprised if cloud-based AI coding tools get more expensive. If you figure you’re using greater than the $100/mo tier right now or that the $100/mo tier will cost $200/mo in the future, the purchase is a no-brainer. It’s just difficult to stomach the upfront cost.&lt;/p&gt;
    &lt;p&gt;From a performance standpoint, I would say the maximum model running on my 128 GB RAM MacBook right now feels about half a generation behind the frontier coding tools. That’s excellent, but something to keep in mind as that half a generation might matter to you.&lt;/p&gt;
    &lt;p&gt;One wrench thrown into my experiment is how much free quota Google hands out with their different AI coding tools. It’s easy to purchase expensive hardware when it saves you money in the long run. It’s much more difficult when the alternative is free.&lt;/p&gt;
    &lt;p&gt;Initially, I considered my local coding setup to be a great pair to Google’s free tier. It definitely performs better than Gemini 2.5 Flash and makes a great companion to Gemini 3 Pro. Gemini 3 Pro can solve more complex tasks with the local model doing everything else. This not only saves quota on 3 Pro but also provides a very capable fallback for when quota is hit.&lt;/p&gt;
    &lt;p&gt;However, this is foiled a bit now that Gemini 3 Flash was just announced a few days ago. It shows benchmark numbers much more capable than Gemini 2.5 Flash (and even 2.5 Pro!) and I’ve been very impressed with its performance. If that’s the free tier Google offers, it makes local coding models less fiscally reasonable. The jury is still out on how well Gemini 3 Flash will perform and how quota will be structured, but we’ll have to see if local models can keep up.&lt;/p&gt;
    &lt;p&gt;I’m very curious to hear what you think! Tell me about your local coding setup or ask any questions below.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
    &lt;p&gt;Always be (machine) learning,&lt;/p&gt;
    &lt;p&gt;Logan&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude"/><published>2025-12-21T20:55:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46348847</id><title>Disney Imagineering Debuts Next-Generation Robotic Character, Olaf</title><updated>2025-12-22T15:11:55.339454+00:00</updated><content>&lt;doc fingerprint="ebf30b7df696007b"&gt;
  &lt;main&gt;
    &lt;p&gt;Disneyland Paris saw a groundbreaking moment today, where Bruce Vaughn, President and Chief Creative Officer of Walt Disney Imagineering, and Natacha Rafalski, Présidente of Disneyland Paris, introduced a next-generation robotic character representing Olaf, the beloved snowman from Walt Disney Animation Studios’ Frozen.&lt;/p&gt;
    &lt;p&gt;This debut marks a new chapter in Disney character innovation, one where technology, storytelling, and collaboration come together to bring screen to reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Innovation at the Core: From Screen to Reality&lt;/head&gt;
    &lt;p&gt;From the way he moves to the way he looks, every gesture and detail is crafted to reflect the Olaf audiences have seen in the film — alive, curious, and unmistakably himself. As for his snow-like shimmer that catches the light just like fresh snow, this was enhanced by iridescent fibers. These details make Olaf one of the most expressive and true-to-life characters built, and he’s soon making his debut at Disney parks.&lt;/p&gt;
    &lt;p&gt;Our roots are in animation with Walt Disney pioneering early hand-drawn films and today, Walt Disney Animation Studios and Pixar Animation Studios continue that tradition. We collaborated closely with the film’s original animators at Walt Disney Animation Studios to ensure every gesture felt true to the character. This isn’t just about replicating the animation, it’s about emulating the creators’ intent.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Technology Behind the Magic&lt;/head&gt;
    &lt;p&gt;Home to some of the best storytellers in the world, we’re continuously pushing the boundaries of innovation and technology — in fact it is in our DNA.&lt;/p&gt;
    &lt;p&gt;Like everything at Disney, we always start with the story, and our number one priority is to build storytelling technology that empowers our Disney Imagineers to breathe life into our characters.&lt;/p&gt;
    &lt;p&gt;While the BDX droids — the Star Wars free roaming robotic characters that mimic movements in a simulation — have been interacting with guests for a while now, Olaf presents a far greater challenge: an animated character with non-physical movements. To make Olaf as authentic as possible, the team used a branch of artificial intelligence called reinforcement learning, pushing the limits of hardware to achieve the creative intent of the artists.&lt;/p&gt;
    &lt;p&gt;It takes humans years to master walking and even longer to perform graceful motions. Deep reinforcement learning helps him acquire these skills in a fraction of the time.&lt;/p&gt;
    &lt;p&gt;Olaf’s “snow” also moves differently than the hard shells of other robotic characters, and he can fully articulate his mouth, eyes, and removable carrot nose and arms. Most importantly, Olaf can speak and engage in conversations, creating a truly one-of-a-kind experience.&lt;/p&gt;
    &lt;p&gt;Innovation takes many forms across our parks, experiences, and products – all focused on improving the guest experience and bringing joy to fans around the world. And what’s most exciting is that we’re just getting started!&lt;/p&gt;
    &lt;p&gt;The BDX Droids, self-balancing H.E.R.B.I.E., and now Olaf represent increasing levels of performance and innovation in bringing Disney characters to life. The speed at which we can create new characters and introduce them to guests is unprecedented. We’re scaling bigger than ever, working to bring more emotive, expressive, and surprising characters to our experiences around the world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Guests Can See Olaf&lt;/head&gt;
    &lt;p&gt;Olaf will soon venture out into the unknown, eager to see guests at:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arendelle Bay Show in World of Frozen, the new immersive world coming soon to Disney Adventure World at Disneyland Paris.&lt;/item&gt;
      &lt;item&gt;Limited-time special appearances at World of Frozen at Hong Kong Disneyland Resort.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Looking for a warm hug now? You can discover how Olaf, along with other exciting breakthroughs from Walt Disney Imagineering Research &amp;amp; Development, came to life at in the latest episode of We Call It Imagineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://disneyparksblog.com/disney-experiences/robotic-olaf-marks-new-era-of-disney-innovation/"/><published>2025-12-21T21:46:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46350075</id><title>ONNX Runtime and CoreML May Silently Convert Your Model to FP16</title><updated>2025-12-22T15:11:55.201445+00:00</updated><content>&lt;doc fingerprint="6d59dd17b5120e18"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ONNX Runtime &amp;amp; CoreML May Silently Convert Your Model to FP16 (And How to Stop It)&lt;/head&gt;
    &lt;p&gt;Running an ONNX model in ONNX RunTime (ORT) with the CoreMLExecutionProvider may change the predictions your model makes implicitly and you may observe differences when running with PyTorch on MPS or ONNX on CPU. This is because the default arguments ORT uses when converting your model to CoreML will cast the model to FP16.&lt;/p&gt;
    &lt;p&gt;The fix is to use the following setup when creating the InferenceSession:&lt;/p&gt;
    &lt;code&gt;= ort.InferenceSession(onnx_model_path, providers=[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]) ort_session &lt;/code&gt;
    &lt;p&gt;This ensures your model remains in FP32 when running on a Mac GPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uncovering an Issue in ONNX Runtime - Benchmarking the EyesOff Model&lt;/head&gt;
    &lt;p&gt;Having trained the EyesOff model, I began evaluating the model and its run time. I was looking into the ONNX format and using it to run the model efficiently. I setup a little test bench in which I ran the model using PyTorch and ONNX with ONNX Runtime (ORT), both using MPS and CPU. While checking the outputs, I noticed that the metrics from the model ran on ONNX on MPS had a different output to those on ONNX CPU and PyTorch CPU and MPS. Note, the metrics from PyTorch on CPU and MPS were the same.&lt;/p&gt;
    &lt;p&gt;When I say ORT and MPS, this is achieved through ORT’s execution providers. To run an ONNX model on the Mac GPU you have to use the CoreMLExecutionProvider (more on this to come).&lt;/p&gt;
    &lt;p&gt;Now in Figure 1 and 2, observe the metric values - the PyTorch ones (Figure 1) are the same across CPU and MPS, this isn’t the same story for ONNX (Figure 2):&lt;/p&gt;
    &lt;p&gt;Wow, look at the diff in Figure 2! When I saw this it was quite concerning, floating point math can lead to differences in the calculations carried out across the GPU and CPU but the values here don’t appear to be a result of floating point math, the values are too large.&lt;/p&gt;
    &lt;p&gt;Given the difference in metrics, I was worried that running the model with ORT was changing the output of the model and hence the behaviour. The reason the metrics change is because some of the model predictions around the threshold flipped to the opposite side of the threshold (which is 0.5), this can be seen in the confusion matrices for the ONNX CPU run and MPS run:&lt;/p&gt;
    &lt;head rend="h4"&gt;FP32 Confusion Matrix&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Predicted Negative&lt;/cell&gt;
        &lt;cell role="head"&gt;Predicted Positive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Actual Negative&lt;/cell&gt;
        &lt;cell&gt;207 (TN)&lt;/cell&gt;
        &lt;cell&gt;24 (FP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Actual Positive&lt;/cell&gt;
        &lt;cell&gt;69 (FN)&lt;/cell&gt;
        &lt;cell&gt;164 (TP)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;FP16 Confusion Matrix&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Predicted Negative&lt;/cell&gt;
        &lt;cell role="head"&gt;Predicted Positive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Actual Negative&lt;/cell&gt;
        &lt;cell&gt;206 (TN)&lt;/cell&gt;
        &lt;cell&gt;25 (FP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Actual Positive&lt;/cell&gt;
        &lt;cell&gt;68 (FN)&lt;/cell&gt;
        &lt;cell&gt;165 (TP)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So two predictions flipped from negative to positive.&lt;/p&gt;
    &lt;p&gt;Having said that, the first thing I did was to make my life easier, by simplifying the scenario from the large EyesOff model to a simple one layer MLP and using that to run the experiments.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why am I Using ONNX and ONNX RunTime?&lt;/head&gt;
    &lt;p&gt;Before going on it’s worth discussing what ONNX and ORT are, and why I’m even using them in the first place.&lt;/p&gt;
    &lt;head rend="h3"&gt;ONNX1&lt;/head&gt;
    &lt;p&gt;ONNX stands for Open Neural Network Exchange. It can be thought of as a common programming language in which to describe ML models. Under the hood ONNX models are represented as graphs, these graphs outline the computation path of a model and it shows the operators and transformations required to get from input to prediction. These graphs are called ONNX graphs.&lt;/p&gt;
    &lt;p&gt;The use of a common language to describe models makes deployment easier and in some cases can add efficiency in terms of resource usage + or inference speed. Firstly, the ONNX graph itself can be optimised. Take PyTorch for example, you train the model in it and sure PyTorch is very mature and extremely optimised but it’s such a large package some things can be overlooked or difficult to change. By converting the model to ONNX, you take advantage of the fact that ONNX was built specifically with inference in mind and with that comes optimisations which the PyTorch team could implement but have not yet.&lt;/p&gt;
    &lt;p&gt;Furthermore, ONNX models can be ran cross platform in specialised runtimes. These runtimes are specially optimised for different architectures and add another layer of efficiency gains.&lt;/p&gt;
    &lt;head rend="h3"&gt;ONNX RunTime (ORT)2&lt;/head&gt;
    &lt;p&gt;ORT is one of these runtimes. ORT actually runs the model, it can be thought of as an interpreter, it takes the ONNX graph and actually implements the operators and runs them on the specified hardware. ORT has a lot of magic built into it, the operators are extremely optimised and through the use of execution providers they target a wide range of hardware. Each execution provider is optimised for the specific hardware it refers to, this enables the ORT team to implement extremely efficient operators giving us another efficiency gain.&lt;/p&gt;
    &lt;head rend="h3"&gt;CoreML3&lt;/head&gt;
    &lt;p&gt;As mentioned before, I used the CoreMLExecutionProvider to run the model on a Mac GPU. This execution provider informs ORT to make use of CoreML. CoreML is an apple developed framework which lets models (neural networks and classical ML models) run on Apple hardware, CPU, GPU and ANE. ORT’s purpose in this phase is to take the ONNX graph and convert it to a CoreML model. CoreML is Apple’s answer to running efficient on device models on Apple hardware.&lt;/p&gt;
    &lt;p&gt;Note, that all of this doesn’t always mean the model will run faster. Some models may run faster in PyTorch, TensorRT or any other framework. This is why it is important to benchmark and test as many approaches as makes sense.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding the Source of the CPU vs MPS Difference - With an MLP&lt;/head&gt;
    &lt;p&gt;The MLP used is very simple it has a single layer, with 4 inputs, 3 outputs and the bias turned off. So, I pretty much created a fancy matrix multiplication.&lt;/p&gt;
    &lt;p&gt;To understand where the issue was coming from I ran this MLP through some different setups:&lt;/p&gt;
    &lt;code&gt;- PyTorch CPU
- PyTorch MPS
- ORT CPU
- ORT MPS
- CoreML FP32
- CoreML FP16&lt;/code&gt;
    &lt;p&gt;The goal of this exercise is to find out if 1 - the difference in outputs is seen in a simple model and 2 - to figure out where exactly the issue arises.&lt;/p&gt;
    &lt;p&gt;Before showing the full results, I want to explain why I included the CoreML FP16 and FP32 runs - specifically why the FP16 one. When I initially ran the MLP experiment I only ran PyTorch, ORT and CoreML FP32 but the output numbers of ORT MPS looked like FP16 numbers. So, I tested if they were and also if the outputs from the other runs were true FP32 numbers. You can do this with a “round trip” test, by converting a number to FP16 and back to FP32. If after this process the number is unchanged then it is an FP16 number but if it changes then it was a true FP32. The number changes as FP16 can represent fewer floating point numbers than FP32. It’s a very simple check to carry out:&lt;/p&gt;
    &lt;code&gt;import numpy as np

= np.array([0.6480752, -0.34015813, 1.4329923], dtype=np.float32)
 onnx_cpu = np.array([0.6484375, -0.34033203, 1.4326172], dtype=np.float32)  # We cast the ort MPS numbers up to FP32, if they were FP16 this has no effect
 onnx_coreml 
= onnx_cpu.astype(np.float16).astype(np.float32)
 cpu_roundtrip = onnx_coreml.astype(np.float16).astype(np.float32)
 coreml_roundtrip 
print("ORT CPU values:")
print("  Original:", onnx_cpu)
print("  fp16 roundtrip:", cpu_roundtrip)
print("  Changed?", not np.allclose(onnx_cpu, cpu_roundtrip, atol=0))

print("\nORT CoreML values:")
print("  Original:", onnx_coreml)
print("  fp16 roundtrip:", coreml_roundtrip)
print("  Changed?", not np.allclose(onnx_coreml, coreml_roundtrip, atol=0))&lt;/code&gt;
    &lt;p&gt;The output of this is:&lt;/p&gt;
    &lt;p&gt;The CPU values change and the MPS values don’t! Now it’s getting interesting - perhaps when using the CoreML execution provider the output is FP16? This prompted adding the CoreML direct run in FP16 precision.&lt;/p&gt;
    &lt;p&gt;I tested this theory with an experiment. Originally, when benchmarking it was all about inference speed, now it’s about floating point precision and figuring out where the diffs come from.&lt;/p&gt;
    &lt;p&gt;Running on PyTorch CPU and MPS gives a strong baseline, PyTorch is a very mature ecosystem and I used the results from that as my ground truth. It being so close together is what drove me to understand what caused ORT runs on different hardware to have a difference. Then using CoreML FP32 and FP16 aimed to show if the issue was an ONNX one or a CoreML one.&lt;/p&gt;
    &lt;p&gt;Check Figure 4 for the outputs and Figure 5 for differences in the outputs here:&lt;/p&gt;
    &lt;p&gt;Wow, would you look at that - once again PyTorch + ORT CPU match and so does PyTorch CPU + CoreML FP32. Also note that CoreML FP16 and ORT MPS match! This is a big insight into what is happening and why the metrics output differed before. Along with the round trip experiment this proves our model is being ran in FP16 when using the CoreML execution provider in ORT!&lt;/p&gt;
    &lt;p&gt;Floating points numbers are defined by three values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sign: 1 bit to define if the number is positive or negative&lt;/item&gt;
      &lt;item&gt;Significand: Contains the numbers digits&lt;/item&gt;
      &lt;item&gt;Exponent: This says where the decimal place should be placed relative to the beginning of the significand&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Floating point numbers are often expressed in scientific notation, e.g:&lt;/p&gt;
    &lt;p&gt;FP16 and FP32 specifically, have the following specification:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Total bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Significand bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Exponent bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Smallest number&lt;/cell&gt;
        &lt;cell role="head"&gt;Largest number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Single precision&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;23 + 1 sign&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;\(1.2 * 10^{-38}\)&lt;/cell&gt;
        &lt;cell&gt;\(3.4 * 10^{38}\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Half precision&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;10 + 1 sign&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;\(5.96 * 10^{-8}\)&lt;/cell&gt;
        &lt;cell&gt;\(6.55 * 10^{4}\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So as FP16 is half the size it affords a couple benefits, firstly it requires half the memory to store and secondly it can be quicker to do computations with too. However, this comes at a cost of precision, FP16 cannot represent very small numbers and the distances between small numbers as accurately as FP32.&lt;/p&gt;
    &lt;p&gt;An example of FP16 vs FP32 - The Largest Number Below 1&lt;/p&gt;
    &lt;p&gt;As you see FP32 can represent a value much closer to 1.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Link to the ONNX Issue&lt;/head&gt;
    &lt;p&gt;Having said all that, going back to the issue at hand we observe a ~\(1.17*e^{-7}\) error between PyTorch and CoreML FP32 which is typical of FP32. But, then ORT and CoreML when ran on MPS have a difference of ~\(3.7*e{-4}\) which is much more representative of FP16, this is what prompted the round trip experiment.&lt;/p&gt;
    &lt;p&gt;If you need a quick refresher on FP values, please expand the box above. If you already read that or you know enough about FP already let’s look at why some predictions flip.&lt;/p&gt;
    &lt;p&gt;In my model the base threshold for a 0 or 1 class is 0.5. Both FP16 and FP32 can represent 0.5 exactly:&lt;/p&gt;
    &lt;code&gt;= np.array([0.5], dtype=np.float32)
 fp_32_05 = np.array([0.5], dtype=np.float16)
 fp_16_05 
 fp_32_05.item(), fp_16_05.item()
0.5, 0.5) (&lt;/code&gt;
    &lt;p&gt;But we know that FP representations cannot represent every single number, so there will be some values around 0.5 which cannot be represented and hence will get rounded either up or down. Let’s look into that and find the threshold, this will show why some predictions of the EyesOff model were flipped when changing the model to run in FP16. Also, note by flipped we mean they go from a negative (0) prediction to a positive (1) class prediction, the rounding means it’d have to be below 0.5 and then be rounded up to cross the threshold boundary. Any other scenario would keep the label the same, i.e if it’s above 0.5 and gets rounded to 0.5 that’s fine as the predicted class is still the same.&lt;/p&gt;
    &lt;p&gt;The first step is to find the next representable number below 0.5:&lt;/p&gt;
    &lt;code&gt;# Show the representable values just below 0.5
= np.nextafter(np.float32(0.5), np.float32(0.0))
 fp32_below = np.nextafter(np.float16(0.5), np.float16(0.0))
 fp16_below 
= 0.5 - fp32_below
 fp32_gap = 0.5 - fp16_below
 fp16_gap 
print(f"\nClosest value BELOW 0.5:")
print(f"FP32: {fp32_below:.20f}")
print(f"FP16: {fp16_below:.20f}")

print(f"\nGap from threshold (0.5):")
print(f"FP32: {fp32_gap:.2e}")
print(f"FP16: {fp16_gap:.2e}")&lt;/code&gt;
    &lt;code&gt;Closest value BELOW 0.5:
FP32: 0.49999997019767761230
FP16: 0.49975585937500000000

Gap from threshold (0.5):
FP32: 2.98e-08
FP16: 2.44e-04&lt;/code&gt;
    &lt;p&gt;Taking this gap between 0.5 and the next representable number below 0.5 in FP16 we can calculate the threshold for values which will get rounded up to 0.5:&lt;/p&gt;
    &lt;code&gt;# Given the gap is 2.44e-04, we need to divide it by 2 and calculate the midpoint between 0.499755859375 and 0.5. This midpoint determines whether the FP16 value will be rounded down if below it or up it equal to or greater than. 

# Convert to FP32 as the midpoint is not representable in FP16
= np.float32(fp16_below)
 fp_16_below_fp32 
# Calculate the gap and midpoint
= 0.5 - fp16_below
 fp16_gap = fp_16_below_fp32 + (fp16_gap / 2)
 midpoint 
print(f"  Midpoint (rounding boundary): {midpoint:.15f}")&lt;/code&gt;
    &lt;code&gt;Midpoint: 0.499877929687500&lt;/code&gt;
    &lt;p&gt;Finally let’s see some examples of numbers being rounded up to 0.5 if they are above the midpoint between the representable values of FP16:&lt;/p&gt;
    &lt;code&gt;# Firstly, the midpoint itself is rounded up
0.499877929687500).item() -&amp;gt; 0.5
 np.float16(0.4999).item() -&amp;gt; 0.5
 np.float16(
# For completeness here's a number slightly smaller than the midpoint which gets rounded down
0.4998779296874).item() -&amp;gt; 0.499755859375 np.float16(&lt;/code&gt;
    &lt;p&gt;In short, any number between \([0.4998779296875, 0.5)\) will be rounded up to 0.5. This means, the predictions which were flipped were in this range.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Does the Model Switch to FP16?&lt;/head&gt;
    &lt;p&gt;Now that we know what the issue is, it’s time to find out what caused it.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Bug or Intended Behaviour - It Must be a Bug Right…?&lt;/head&gt;
    &lt;p&gt;Originally I thought this behaviour was a bug with the ORT repo. Knowing the cast occured in the phase where ORT takes the ONNX model and converts it to a CoreML model my initial thinking was either ORT casts it to FP16 somewhere or calls CoreML with a hardcode FP16 flag or something similar.&lt;/p&gt;
    &lt;p&gt;Having little background in cpp, Claude came in useful here. I gave it the structure of the repo and it told me where I ought to place breakpoints to debug the ORT package (turns out you can debug a cpp package from python, clone the repo, build from src and then link it to your python code using the PID). However, upon running the code the breakpoints weren’t being hit. I was puzzled for a bit, but then I realised why the code wasn’t being hit. It turns out CoreML has two model formats “NeuralNetwork” and “MLProgram”, I will call them NN and MLP formats respectively. The behaviour of the ORT repo changes depending on which you want, as does the behaviour of CoreML, with the default being the NN format. So, the breakpoints weren’t hit as the code was regarding the MLP format whereas I was not setting this so the code flowed through the default NN code. Knowing this I took a step back and began experimenting with NN vs MLP format.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fix - NeuralNetwork vs MLProgram CoreML Format&lt;/head&gt;
    &lt;p&gt;So, CoreML has two model formats, these represent how the model is stored and ran with CoreML. The NeuralNetwork (NN) format is older and the MLProgram (MLP) format is newer. ORT specifies NN format by default, but it does allow you to pass a flag to use MLP format.&lt;/p&gt;
    &lt;p&gt;Testing the MLP format revealed it as the solution! See below in figure 6 the final output, which includes both ORT MLP and NN format ran on the GPU.&lt;/p&gt;
    &lt;p&gt;So ORT on MPS with NN format has the same difference from the PyTorch CPU baseline as CoreML FP16, whereas ORT with MLP format matches - this is exactly what I wanted. Mystery solved! By setting the model format to be the newer MLProgram format no implicit cast to FP16 takes place.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why MLProgram Format Worked and Neural Network Didnt?&lt;/head&gt;
    &lt;p&gt;To understand the difference in behaviours of these two models formats we need to take a deep dive on the internals of CoreML, its goals and the two formats themselves. Let’s begin with CoreML.&lt;/p&gt;
    &lt;head rend="h3"&gt;CoreML&lt;/head&gt;
    &lt;p&gt;ORT implements methods to convert the ONNX graph into CoreML model formats. CoreML has two types of model format, this defines how the model is represented in the CoreML framework, how it’s stored and how it’s ran. The older is the NeuralNetwork format and the newer one which solved our issue is the MLProgram format. The reason MLProgram keeps the model at FP32 when running on MPS is due to the differences in model representation in these two formats. Let’s take a look at both of them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Neural Network Format&lt;/head&gt;
    &lt;p&gt;As stated, the NN format is the older one, it came out in 2017. It stored models as a Directed Acyclic Graph (DAG). Each layer in the model is a node in the DAG, and they encode information on layer type, list of input names, output names and a collection of parameters specific to the layer type7. We can observe the model which is created by ORT’s InferenceSession call with the following code:&lt;/p&gt;
    &lt;code&gt;# First create the InferenceSession and run the model. This ensures the CoreML model files are added to a temp dir
= ort.InferenceSession(
 ort =["CoreMLExecutionProvider"]
     onnx_path, providers
 )= ort.run(None, {"input": numpy_input})[0]
 nn_output 
import coremltools as ct

def get_coreml_dtype_from_spec(path):
"""Extract model type and dtypes by reading the spec."""
     
     = ct.models.MLModel(str(path))
     model = model.get_spec()
     spec 
print(f"\nModel Spec for {path.name}:\n {spec}\n")
     
# Find created models
= Path(tempfile.gettempdir())
 temp_dir 
# NeuralNetwork models are .mlmodel files
= list(temp_dir.glob("*.mlmodel"))
 nn_files 
for model_path in nn_files:
= get_coreml_dtype_from_spec(model_path)     info &lt;/code&gt;
    &lt;p&gt;This outputs the following:&lt;/p&gt;
    &lt;code&gt;Model Spec for onnxruntime-40975D85-7412-4309-A6F7-4E51CA3D2FE8-7682-0000BF11C24C3150.model.mlmodel:
 specificationVersion: 4
description {
  input {
    name: "input"
    type {
      multiArrayType {
        shape: 1
        shape: 4
        dataType: FLOAT32
      }
    }
  }
  output {
    name: "output"
    type {
      multiArrayType {
        shape: 1
        shape: 3
        dataType: FLOAT32
      }
    }
  }
}
neuralNetwork {
  layers {
    name: "node_linear"
    input: "input"
    output: "output"
    innerProduct {
      inputChannels: 4
      outputChannels: 3
      weights {
        floatValue: 0.0349225402
        floatValue: -0.301196814
        floatValue: 0.159211695
        floatValue: 0.156890273
        floatValue: -0.267238438
        floatValue: -0.0749385953
        floatValue: -0.292913973
        floatValue: 0.129736364
        floatValue: -0.134683847
        floatValue: 0.351268351
        floatValue: 0.354943156
        floatValue: 0.0509352088
      }
    }
  }
  arrayInputShapeMapping: EXACT_ARRAY_MAPPING
}&lt;/code&gt;
    &lt;p&gt;NeuralNetwork format has typed input and output to the model, but the nodes themselves are not typed. This is why the model gets cast to FP16, in the NN format the default behaviour is to run in FP16 on the MPS GPU. This quirk of the NN format is what threw off my results8. The CoreML runtime also specifies which parts of the model operators can run on which hardware9 and each hardware has different abilities in terms of what FP values it can handle with the NN format. Take a look at Figure 7 for Apple’s guide on the hardware and FP types they can handle:&lt;/p&gt;
    &lt;p&gt;When running on CPU the NN format model will run in FP32, as we observed. However, on GPU it is implicitly cast to FP16 even though the input and output are specified to be FP32 as you see in the inspection code above. This is an inherent limitation of the NN format. The DAG structure of the model does not store any information on the types of intermediate layers. You can see this in the inspection output, the part beginning neuralNetwork stored info on the actual layer node, in our case a single linear layer. Observe that there is no information on the FP precision of the node itself, hence CoreML implicitly sets it to FP16.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why Does CoreML Implicitly Use FP16&lt;/head&gt;
    &lt;p&gt;From the typed execution docs for coremltools, the goal of CoreML is to run ML models in the most performant way and FP16 happens to be more performant than FP32 (which makes sense as it’s half the precision) on Apple GPUs. Also, they state that most of the time the reduced precision doesn’t matter for inference - this whole blog post shows why this is false and a pitfall of the NN format, the user should choose which precision the model is ran in, it should never be implicit.&lt;/p&gt;
    &lt;head rend="h5"&gt;MatMul Test - Is FP16 Faster on Apple Hardware?&lt;/head&gt;
    &lt;p&gt;To test Apple’s claim that FP16 is more performant on Apple hardware I carried out a large matmul. Taking a 16384x16384 matrix and multiplying it with another 16384x16384 matrix should show us if FP16 is faster. The size is arbitrary I just wanted something large.&lt;/p&gt;
    &lt;p&gt;The matmul was ran 10 times, in both FP32 and FP16 on the MPS hardware, and we take the average:&lt;/p&gt;
    &lt;code&gt;FP32 Average Time: 8.6521 seconds
FP16 Average Time: 6.7691 seconds

Speedup Factor: 1.28x faster&lt;/code&gt;
    &lt;p&gt;So FP16 is quicker, which sheds a bit of light on why the NN format has implicit casting to FP16, on paper if you only care about speed then it’s the better option.&lt;/p&gt;
    &lt;p&gt;Final point on the NeuralNetwork format, it’s surprising as the weights themselves are stored as FP32 values (a roundtrip test verifies this) but it still executes that layer in FP16, once again showing the NN format doesn’t respect the FP precision of the layer but just casts it to FP16.&lt;/p&gt;
    &lt;p&gt;All that is to say this, this was not a bug but rather an explicit design choice, which funnily enough involves implicitly going against what the user wants. The NN format has its downsides, which is why Apple introduced the MLProgram format, let’s look into that.&lt;/p&gt;
    &lt;head rend="h3"&gt;The MLProgram (MLP) Format&lt;/head&gt;
    &lt;p&gt;The MLP format is the newer and better model format in CoreML, released in 2021, the core thing we care about is that the intermediate tensors are typed, i.e. there is no implicit casting when using the MLP format - the user controls whether the model is ran in FP16 or FP32.&lt;/p&gt;
    &lt;p&gt;MLP format allows for this as it uses a different representation of ML models, instead of a DAG it uses a programmatic representation of the models. By representing the model as code, it allows for greater control over the operations.&lt;/p&gt;
    &lt;p&gt;Let’s see what this looks like in the stored model format and how it differs to the NN format inspection.&lt;/p&gt;
    &lt;p&gt;The code to do so is pretty similar:&lt;/p&gt;
    &lt;code&gt;# First create the InferenceSession and run the model. This ensures the CoreML model files are added to a temp dir. Also
# this time we specify the ModelFormat to be MLProgram 
= ort.InferenceSession(
 ort_mlp =[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]
     onnx_path, providers
 )= ort_mlp.run(None, {"input": numpy_input})[0]
 mlp_output 
import coremltools as ct

def get_coreml_dtype_from_spec(path):
"""Extract model type and dtypes by reading the spec."""
     
     = ct.models.MLModel(str(path))
     model = model.get_spec()
     spec 
print(f"\nModel Spec for {path.name}:\n {spec}\n")
     
# Find created models
= Path(tempfile.gettempdir())
 temp_dir 
# MLProgram models are in onnxruntime-* directories (not .mlmodelc)
= [d for d in temp_dir.glob("onnxruntime-*")
 mlp_dirs if d.is_dir() and not str(d).endswith('.mlmodelc')]
             
for model_path in mlp_dirs:
= get_coreml_dtype_from_spec(model_path)     info &lt;/code&gt;
    &lt;p&gt;The output of this is the following:&lt;/p&gt;
    &lt;code&gt;Model Spec for onnxruntime-752039B9-BA73-47E3-9ED4-AE029184DA69-9443-0000BF278CD8396E:
 specificationVersion: 8
description {
  input {
    name: "input"
    type {
      multiArrayType {
        shape: 1
        shape: 4
        dataType: FLOAT32
      }
    }
  }
  output {
    name: "output"
    type {
      multiArrayType {
        shape: 1
        shape: 3
        dataType: FLOAT32
      }
    }
  }
}
mlProgram {
  version: 1
  functions {
    key: "main"
    value {
      inputs {
        name: "input"
        type {
          tensorType {
            dataType: FLOAT32
            rank: 2
            dimensions {
              constant {
                size: 1
              }
            }
            dimensions {
              constant {
                size: 4
              }
            }
          }
        }
      }
      opset: "CoreML7"
      block_specializations {
        key: "CoreML7"
        value {
          outputs: "output"
          operations {
            type: "const"
            outputs {
              name: "linear_weight"
              type {
                tensorType {
                  dataType: FLOAT32
                  rank: 2
                  dimensions {
                    constant {
                      size: 3
                    }
                  }
                  dimensions {
                    constant {
                      size: 4
                    }
                  }
                }
              }
            }
            attributes {
              key: "val"
              value {
                type {
                  tensorType {
                    dataType: FLOAT32
                    rank: 2
                    dimensions {
                      constant {
                        size: 3
                      }
                    }
                    dimensions {
                      constant {
                        size: 4
                      }
                    }
                  }
                }
                blobFileValue {
                  fileName: "@model_path/weights/weight.bin"
                  offset: 64
                }
              }
            }
            attributes {
              key: "name"
              value {
                type {
                  tensorType {
                    dataType: STRING
                  }
                }
                immediateValue {
                  tensor {
                    strings {
                      values: "linear_weight"
                    }
                  }
                }
              }
            }
          }
          operations {
            type: "linear"
            inputs {
              key: "x"
              value {
                arguments {
                  name: "input"
                }
              }
            }
            inputs {
              key: "weight"
              value {
                arguments {
                  name: "linear_weight"
                }
              }
            }
            outputs {
              name: "output"
              type {
                tensorType {
                  dataType: FLOAT32
                  rank: 2
                  dimensions {
                    constant {
                      size: 1
                    }
                  }
                  dimensions {
                    constant {
                      size: 3
                    }
                  }
                }
              }
            }
            attributes {
              key: "name"
              value {
                type {
                  tensorType {
                    dataType: STRING
                  }
                }
                immediateValue {
                  tensor {
                    strings {
                      values: "node_linear__0"
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;Now you see in the inspection of the MLP format model the linear layer is explicitly typed. To make it a bit easier to see let’s bring back the NeuralNetwork format inspection and compare the linear layer setup in both:&lt;/p&gt;
    &lt;head rend="h4"&gt;Linear Layer in NeuralNetwork &amp;amp; MLProgram Format&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;NeuralNetwork Linear Layer&lt;/cell&gt;
        &lt;cell role="head"&gt;MLProgram Linear Layer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;neuralNetwork { layers { name: "node_linear" input: "input" output: "output" innerProduct { inputChannels: 4 outputChannels: 3 weights { floatValue: 0.0349225402 floatValue: -0.301196814 floatValue: 0.159211695 floatValue: 0.156890273 floatValue: -0.267238438 floatValue: -0.0749385953 floatValue: -0.292913973 floatValue: 0.129736364 floatValue: -0.134683847 floatValue: 0.351268351 floatValue: 0.354943156 floatValue: 0.0509352088 } } }&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;operations { type: "linear" inputs { key: "x" value { arguments { name: "input" } } } inputs { key: "weight" value { arguments { name: "linear_weight" } } } outputs { name: "output" type { tensorType { dataType: FLOAT32 rank: 2 dimensions { constant { size: 1 } } dimensions { constant { size: 3 } } } } }&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Observe in the NN format, there is no explicit mention of the input or output type also the model weights are stored with the layer. Now, in the MLProgram layer, the output is explicitly typed as FP32. No more pesky implicit casting to FP16! This is one the big changes in MLProgram vs NN format, secondly notice how the layer weights are not stored along with the spec, they’re stored elsewhere. This aspect also makes the MLP format more efficient as the actual model spec is lighter.&lt;/p&gt;
    &lt;head rend="h2"&gt;But Why Does MLProgram Have Typed Layers?&lt;/head&gt;
    &lt;p&gt;So we’ve come to the end of the journey, we found that NeuralNetwork format lacks types in the intermediate layers of the model and MLProgram doesn’t. So, setting ORT to use MLProgram keeps the model at FP32 and our output predictions remain the same when running in PyTorch and ORT. But why, why does NeuralNetwork not include types? Answering this requires a look into how ML models have been represented in the past and how this has evolved over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Design Choices, Design Choices - How Goals of ML Optimisation Evolved Over Time&lt;/head&gt;
    &lt;p&gt;When the NeuralNetwork format was released in 2017, it came into a much different environment than the one MLProgram was born into in 2021. The goal of NeuralNetwork was to act as a configuration file to be ran by hardware, as we saw above it defines the layers and the weights without much other info and lets the hardware figure out the rest. This is indicative of the trends in ML at the time, models were still being optimised so the added complexity wasn’t yet needed, the DAG representation worked well.&lt;/p&gt;
    &lt;p&gt;In essence, the NN format assumes that if the weights are stored in FP32, the input is FP32 and the output is too then the intermediate layers will also be FP32 - but as it doesn’t explicitly type these intermediate layers the hardware is free to choose and the Apple GPU chooses FP16 by default!&lt;/p&gt;
    &lt;p&gt;As time went on the demands in the ML world changed, these hardware based quirks became known, optimisations advanced and overall the industry moved away from the splintered (splintered in the sense that many frameworks implemented their own) config style DAGs and began to utilise learnings from the world of compilers&lt;/p&gt;
    &lt;head rend="h3"&gt;Changes From 2017 to 2021 Which Lead to Greater Adoption of Intermediate Representations&lt;/head&gt;
    &lt;p&gt;Firstly, for Apple specifically the hardware available expanded, now you have the CPU, GPU and ANE chips - making it very difficult to assume any given piece of hardware will run a specific FP type. Also, the lack of typing leads to other issues namely the compiler cannot make some optimisations, as they depend on knowing the types before runtime. Furthermore, things like mixed FP training and quantization became a thing, once again highlighting the need for explicit typing.&lt;/p&gt;
    &lt;p&gt;Lastly, in 2017 DAGs and other forms of model compilers were very fragmented and modern times have seen a push towards standardisation10, as the compiler community consolidated on tools like LLVM the ML community has too. Intermediate Representations(IR) began to be used in ML, an IR is a hardware agnostic specification of a program which a compiler can optimise. CoreML introduced their own IR, called MIL (Model Intermediate Language) and it implements the output we see in the stored MLProgram output.&lt;/p&gt;
    &lt;head rend="h3"&gt;The MIL Approach&lt;/head&gt;
    &lt;p&gt;MIL and IRs in general afford a lot of benefits. They are inherently designed for optimisation and by providing a general framework you can extract maximal value as all optimisation engineers can work on a common goal. In MIL specifically, some of the changes we’ve discussed between NN and MLProgram format, are implemented by it. Namely, each variable within the model has an explicit dtype.&lt;/p&gt;
    &lt;p&gt;Note, the MLProgram serialises and stores the output of the MIL phase, we’ve already observed how it differs to the the NeuralNetwork model, with the biggest difference being in the explicit types.&lt;/p&gt;
    &lt;head rend="h4"&gt;Further Reading on ML Compilers&lt;/head&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;head rend="h3"&gt;The Fix&lt;/head&gt;
    &lt;p&gt;The solution to all the issues we discussed today is, if you are using the CoreMLExecutionProvider in ORT then be sure to specify ModelFormat is MLProgram, this will ensure that whatever precision your model was trained it will be ran with that - which in my case was FP32 (whereas the default ModelFormat NeuralNetwork casts the model to FP16).&lt;/p&gt;
    &lt;p&gt;You can implement this as such:&lt;/p&gt;
    &lt;code&gt;= ort.InferenceSession(onnx_model_path, providers=[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]) ort_session &lt;/code&gt;
    &lt;head rend="h3"&gt;The Cause&lt;/head&gt;
    &lt;p&gt;The issue was the differing model formats employed by CoreML to represent ML models. The NeuralNetwork format utilised a more historic DAG based approach which was developed during a time in which types and precision wasn’t a huge concern in the ML community and hardware decisions were left to the hardware. Whereas the MLProgram format used a programmatic approach, in which types are explicit letting the software influence how the model is run on the hardware.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lessons?&lt;/head&gt;
    &lt;p&gt;This whole thing taught me the importance of being thorough, it’s not acceptable to test your model in one setup and deploy it in another. We really need to test our model runs across all the platforms we intend to deploy to. Secondly, implicit defaults can be particularly damaging, in my case it wasn’t a huge issue but it easily could have been. Implicit defaults in this case also killed reproducibility, which can be problematic.&lt;/p&gt;
    &lt;p&gt;Lastly, I leave you with this:&lt;/p&gt;
    &lt;p&gt;1https://onnx.ai/onnx/intro/concepts.html&lt;/p&gt;
    &lt;p&gt;3https://developer.apple.com/documentation/coreml&lt;/p&gt;
    &lt;p&gt;4https://en.wikipedia.org/wiki/Single-precision_floating-point_format&lt;/p&gt;
    &lt;p&gt;5https://en.wikipedia.org/wiki/Half-precision_floating-point_format&lt;/p&gt;
    &lt;p&gt;6https://floating-point-gui.de/formats/fp/&lt;/p&gt;
    &lt;p&gt;7https://apple.github.io/coremltools/mlmodel/Format/NeuralNetwork.html&lt;/p&gt;
    &lt;p&gt;8https://apple.github.io/coremltools/docs-guides/source/typed-execution.html&lt;/p&gt;
    &lt;p&gt;9https://github.com/microsoft/onnxruntime/issues/21271#issuecomment-3637845056&lt;/p&gt;
    &lt;p&gt;10https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ym2132.github.io/ONNX_MLProgram_NN_exploration"/><published>2025-12-22T00:27:04+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46350641</id><title>Build Android apps using Rust and Iced</title><updated>2025-12-22T15:11:54.344169+00:00</updated><content>&lt;doc fingerprint="3d82cc104505dfd0"&gt;
  &lt;main&gt;
    &lt;p&gt;There are NativeActivity and GameActivity examples here.&lt;/p&gt;
    &lt;p&gt;Based on several other examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;na-mainloop&lt;/code&gt;and&lt;code&gt;agdk-mainloop&lt;/code&gt;from android-activity&lt;/item&gt;
      &lt;item&gt;na-winit-wgpu from &lt;code&gt;rust-android-examples&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;integration from &lt;code&gt;iced&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;iced integration example&lt;/p&gt;
    &lt;p&gt;You can also run most of the examples from iced. For this omit the scene rendering part and set the background of the root container.&lt;/p&gt;
    &lt;p&gt;Text input partially works, unresolved issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;window doesn't resize on show/hide soft keyboard&lt;/item&gt;
      &lt;item&gt;how to change input language of soft keyboard&lt;/item&gt;
      &lt;item&gt;ime is not supported&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Copy/paste and show/hide soft keyboard is implemented by calling Java&lt;/p&gt;
    &lt;p&gt;Check &lt;code&gt;android-activity&lt;/code&gt; crate for detailed instructions.
During my tests I was running the following command and using android studio afterwards:&lt;/p&gt;
    &lt;code&gt;export ANDROID_NDK_HOME="path/to/ndk"
export ANDROID_HOME="path/to/sdk"

rustup target add x86_64-linux-android
cargo install cargo-ndk

cargo ndk -t x86_64 -o app/src/main/jniLibs/  build&lt;/code&gt;
    &lt;p&gt;My setup is the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;archlinux 6.9.6&lt;/item&gt;
      &lt;item&gt;jdk-openjdk 22&lt;/item&gt;
      &lt;item&gt;target api 35&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks to &lt;code&gt;android-activity&lt;/code&gt; we can already build android apps in Rust, and
key crates such as &lt;code&gt;winit&lt;/code&gt; and &lt;code&gt;wgpu&lt;/code&gt; also support building for android.
&lt;code&gt;iced&lt;/code&gt; doesn't support android out of the box, but it can be integrated with
existing graphics pipelines, as shown in
integration example.
As a result, it was possible to convert existing example running &lt;code&gt;winit&lt;/code&gt; + &lt;code&gt;wgpu&lt;/code&gt; to
use &lt;code&gt;iced&lt;/code&gt; on top.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ibaryshnikov/android-iced-example"/><published>2025-12-22T02:14:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46352231</id><title>Debian's Git Transition</title><updated>2025-12-22T15:11:53.744652+00:00</updated><content>&lt;doc fingerprint="767cce1f3dd3c327"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Captcha Check&lt;/head&gt;
    &lt;p&gt;Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button!&lt;/p&gt;
    &lt;p&gt;Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://diziet.dreamwidth.org/20436.html"/><published>2025-12-22T08:24:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46352248</id><title>Inverse Parentheses</title><updated>2025-12-22T15:11:53.382856+00:00</updated><content>&lt;doc fingerprint="1f1817d1eda55258"&gt;
  &lt;main&gt;
    &lt;p&gt;Have you ever noticed that lots of programming languages let you use parentheses to group operands, but none use them to ungroup them? No? Well letâs pretend this is a normal thing to be thinking about, and see what we can do about it.&lt;/p&gt;
    &lt;p&gt;Grouping with parentheses is relatively easy to add to a language grammar. The rule that accepts atomic things like &lt;code&gt;37&lt;/code&gt; simply needs to also accept an opening paren,1 at which point it will recursively parse an entire expression, and then eat a closing paren.&lt;/p&gt;
    &lt;code&gt;def parse_atom(lex):
    r = next(lex)
    if r[0] == 'integer':
        return int(r[1])
    elif r[0] == '(':
        expr = parse_expr(lex)
        s = next(lex)
        if s[0] != ')':
            raise ParseError("missing close paren")
        return expr
    else:
        raise ParseError(f"unexpected {r[0]}")
&lt;/code&gt;
    &lt;p&gt;Anti-grouping isnât quite as straightforward. Our parser canât follow the structure of the parentheses, because then it wouldnât be following the structure of the expressionâthe whole point is that these are dissimilar.&lt;/p&gt;
    &lt;p&gt;I donât know if itâs possible to write a pure parser that does this. But purity is overrated anyway. I decided to take inspiration from another language with a weird grouping system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Python&lt;/head&gt;
    &lt;p&gt;Did you know that Pythonâs grammar has braces? You just donât type them. The tokeniser3 keeps track of the indentation level and inserts special tokens when it changes. The parser itself doesnât need to worry about counting whitespace; it just sees blocks of statements bracketed by &lt;code&gt;INDENT&lt;/code&gt; and &lt;code&gt;DEDENT&lt;/code&gt;,4 which are easy to parse.&lt;/p&gt;
    &lt;p&gt;As it happens, Pythonâs tokeniser also knows when itâs inside a parenthesised expression. Indentation inside parens is not significant, and this is implemented by tracking the paren nesting depth and suppressing &lt;code&gt;INDENT&lt;/code&gt; and &lt;code&gt;DEDENT&lt;/code&gt; while itâs non-zero.&lt;/p&gt;
    &lt;p&gt;What if we used the same trick? Instead of trying to do all this in the parser somehow, the tokeniser could track its nesting depth, and emit a âfriendlinessâ score for each token. Then we can simply parse operators in ascending order of friendliness.&lt;/p&gt;
    &lt;p&gt;In this model &lt;code&gt;1 + (2 * 3)&lt;/code&gt; will yield the following token stream:&lt;/p&gt;
    &lt;code&gt;1
+ (0)
(
2
* (-1)
3
)
&lt;/code&gt;
    &lt;p&gt;Weâll leave the parentheses in the token stream, but all the parser needs to do with them is generate a syntax error if it finds one in the wrong place. Grouping will be handled entirely by the precedence levels embedded in the token stream.5&lt;/p&gt;
    &lt;head rend="h2"&gt;A not-so-infinite climb&lt;/head&gt;
    &lt;p&gt;The tokeniser hack solves our parsing problem, but it creates another one: our language now has infinitely many precedence levels. I donât feel like trying to do that with handrolled recursive descent, but a rummage through school textbooks suggests a precedence climbing parser is what we need. It deals with operators in the order it meets them, so having infinitely many possible precedences wonât bother it.&lt;/p&gt;
    &lt;p&gt;I hacked this together and itâs appropriately silly:&lt;/p&gt;
    &lt;code&gt;&amp;gt; (1 + 2) * 3
1 + 2 * 3
&amp;gt; 1 + (2 * 3)
(1 + 2) * 3
&lt;/code&gt;
    &lt;p&gt;Something I particularly enjoy about the implementation I landed on is that if you increase friendliness instead of decreasing it, you end up with an ordinary6 parser. Itâs also a good platform for other questionable syntactic innovations, like a language with no parentheses at all, using whitespace to weaken binding.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work&lt;/head&gt;
    &lt;p&gt;While weâve achieved a lot here today,7 weâve also raised some important new questions. For instance, is it always necessary to double-parenthesise expressions in more complex cases?&lt;/p&gt;
    &lt;code&gt;&amp;gt; ((1 * 2)) + (3 * 4)
1 * ((2 + 3) * 4)
&lt;/code&gt;
    &lt;p&gt;And is it possible to have an anti-grouping parser that gives an involution when hooked up to an ordinary printer?&lt;/p&gt;
    &lt;p&gt;These are promising avenues for deeper study, and Iâd love to hear from anyone who chooses to take them on.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Does it bother anyone else that âparenthesesâ (meaning round brackets) doesnât seem to have a satisfactory2 singular form? ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To me, âparenthesisâ sounds like a pair of parentheses, possibly together with the text they bracket, rather than a single paren. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Also called a lexer or occasionally a scanner. Iâm going to stick with âtokeniserâ because it feels the most like a real word. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sadly these are just internal names and not spellings, so you canât use them to write exciting one-liners. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I have ignored the order of operations in this post to keep things simple, but my implementation does handle it. Operators are looked up in a table and their ânaturalâ precedence is paired with their friendliness, so in fact the precedences of&lt;/p&gt;&lt;code&gt;+&lt;/code&gt;and&lt;code&gt;*&lt;/code&gt;from the example would be (0,0) and (-1,1) respectively. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Outwardly. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Actually early 2024, but it took me until today to get around to writing it up. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kellett.im/a/inverse-parentheses"/><published>2025-12-22T08:29:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46352310</id><title>Show HN: Backlog – a public repository of real work problems</title><updated>2025-12-22T15:11:53.237360+00:00</updated><link href="https://www.worldsbacklog.com/"/><published>2025-12-22T08:42:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46352405</id><title>Cartoon Network channel errors (1995 – 2025)</title><updated>2025-12-22T15:11:53.166519+00:00</updated><content>&lt;doc fingerprint="d673c7910d1bb0ed"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript is disabled in your browser. Please enable JavaScript to proceed. A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cnas.fandom.com/wiki/Channel_Errors"/><published>2025-12-22T09:01:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46352565</id><title>The ancient monuments saluting the winter solstice</title><updated>2025-12-22T15:11:52.842137+00:00</updated><content>&lt;doc fingerprint="bd08521d25e3a112"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'It's a moment of death and rebirth': The ancient monuments saluting the winter solstice&lt;/head&gt;
    &lt;p&gt;Dozens of mysterious structures across the Northern Hemisphere – some nearly 5,000 years old – align precisely to frame the rising and setting Sun during midwinter's shortest day. What motivated people to construct these solar-calibrated masterpieces?&lt;/p&gt;
    &lt;p&gt;The winter solstice, which usually falls on 21 or 22 December in the Northern Hemisphere each year, marks the moment that one yearly cycle comes to an end and another is born. It is the day with the smallest number of sunlight hours in the calendar, and once it's over, the days lengthen again incrementally until the summer solstice in June.&lt;/p&gt;
    &lt;p&gt;The significance of this day is manifested in ancient monuments that were designed to acknowledge and celebrate its passing. One example is Maeshowe tomb in Orkney. To the untrained eye this burial cairn, created around 2800BC, looks like a grassy hillock – but it conceals a cuboid, stone-clad sepulchre and a 33ft (10m) long entry corridor oriented to the south-west. During midwinter, three weeks either side of the winter solstice, the setting Sun aims directly down the corridor and emanates its light into the tomb.&lt;/p&gt;
    &lt;p&gt;When the sky is cloudless, the light seems to carve a golden aperture into the tomb's rear wall – a sacrament of pure light. These days of radiance are interrupted by the solstice itself, when blackness temporarily takes over. But daylight reappears soon after, to blaze for another few days as if in celebration of the promise of nature's rejuvenation in spring.&lt;/p&gt;
    &lt;p&gt;We will probably never know the specific beliefs and rituals that inspired Maeshowe tomb. But it's nonetheless possible to understand the enormous significance of the winter solstice as the "year's midnight", both as the darkest moment in the calendar and the pivot to six future months of greater illumination. It was a moment of death and rebirth, and a reminder of the cyclical nature of time.&lt;/p&gt;
    &lt;p&gt;In the deep past, understanding the markers of nature's clockwork – including solstices – was a matter of survival. Predicting the recurrent patterns of animal migration, for example, could help successful hunting and fishing. Knowing when the climate was likely to change meant being able to adapt and survive. In pre-agricultural societies, it helped people anticipate the availability and location of edible roots, nuts and plants.&lt;/p&gt;
    &lt;p&gt;After the introduction of farming, around 9000BC, it was essential – for successful planting and harvesting – to anticipate the timing of seasonal changes. Monuments that calculated time had practical value, but it's likely that they also embodied spiritual beliefs in Neolithic times too, with the winter solstice being of particular importance. This very ancient recognition of the solstice's significance even echoes through to the modern world. The word "Yule", now associated with the winter holiday period, derives from the historic Norse festival of Jól, which was based around the winter solstice. Modern Christmas traditions recall bygone midwinter celebrations like the Roman holiday of Saturnalia, which involved feasting and gift-giving. And the solstice continues to be acknowledged in hundreds of traditions across the world, such as the Inca celebration of Inti Raymi, and the Dōngzhì festival in China.&lt;/p&gt;
    &lt;head rend="h2"&gt;'Nature's sublime power'&lt;/head&gt;
    &lt;p&gt;Alongside Maeshowe tomb, archaeologists have discovered dozens of Neolithic monuments that stare directly at the Sun on the winter solstice. There's Stonehenge (England), whose tallest trilithon once framed the setting sun; Newgrange (Ireland), which has a passageway aligned to sunrise on this auspicious day; and the standing stones at Callanish (Outer Hebrides) which create similar solar sightlines. In Brittany, north-western France, is La Roche aux Fées: a megalithic passageway constructed from 41 blocks of stone, some of which weigh over 40 tonnes (40,000kg). At sunrise on the winter solstice, it breathes in its annual dose of restorative midwinter light. Legends once told that fairies constructed it over the course of one night, but it is actually a dolmen (tomb) created by Neolithic architects around 2750BC.&lt;/p&gt;
    &lt;p&gt;In the 20th and 21st Centuries there has been a resurgence of Neolithic-inspired solar-oriented artworks. Nancy Holt's seminal land art piece, Sun Tunnels (1973-76) is one example, set in the Great Basin Desert of Utah, and comprising of four 22-tonne (22,000kg) concrete tubes arranged in an X-shape formation. The view down each of them perfectly frames the Sun as it rises and sets on the winter and summer solstices. Holt bought the land in 1975 and created her artwork with the help of engineers, an astrophysicist, an astronomer and a team of contractors.&lt;/p&gt;
    &lt;p&gt;It's best to understand Sun Tunnels in the context of the Land Art movement of the 1960s and 70s. Artists like Holt who are associated with this movement worked with the landscape rather than within traditional studios and galleries, and aimed to reconnect people with the awe of nature. Unlike its Neolithic predecessors, Sun Tunnels has no religious significance – Holt explained that she simply wanted "to bring the vast space of the desert back to human scale". It is also a response to modern concerns about nature. In an age where humans seem hell-bent on despoiling and exploiting nature, Sun Tunnels turns our attention back to its sublime power and rhythmic patterns.&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;• Seven of the greatest rivalries in art history&lt;/p&gt;
    &lt;p&gt;Another masterpiece of Land Art, James Turrell's Roden Crater (begun 1979), does this on an even more epic scale than Sun Tunnels. It occupies a volcanic cinder cone in the Painted Desert region of northern Arizona and houses multiple spaces from which to watch celestial phenomena. One of them is a 900ft- (274m) long tunnel drilled through the volcanic cone. It acts like a camera obscura, focusing an image of the midwinter sun (via a glass lens halfway down the passage) on to a slab of white marble in a central chamber. Like Maeshowe tomb's passage, it aligns with the Sun's position around 21 December each year, and drinks down the Sun's light from 10 days before the solstice to the 10th day after it.&lt;/p&gt;
    &lt;p&gt;Enoura Observatory in Kanagawa Prefecture, Japan (completed 2017) was designed by photographer and architect Hiroshi Sugimoto. Its various buildings are all calibrated towards the movement of the Sun, to create what the artist describes as a "new Neolithic aesthetic". He wanted to correct what he saw as a lack of purpose in contemporary art by exploring the primal concerns of our ancient ancestors – our status within the infinite wilderness of the cosmos, our sense of time, and our notion of a human identity within the natural order.&lt;/p&gt;
    &lt;p&gt;One of its structures, the "Winter Solstice Light-Worship Tunnel", points directly at the spot on the horizon where the Sun rises at about 06:48 local time on 21 December each year. The solstice sunlight floods this 230ft-(70m) long chamber made of Corten steel and illuminates a stone medieval wellhead that is situated halfway along its length. It passes underneath another structure which aligns with the Sun on the summer solstice. The entire site, which took a decade to build, was intended by Sugimoto to act like a living clock, and to make an artwork with the ancient function of helping humans "identify their place within the vastness of the universe".&lt;/p&gt;
    &lt;p&gt;Holt's, Turrell's and Sugimoto's structures put us back in contact with seasonal patterns and the rhythms of nature, just as Maeshowe tomb and La Roche aux Fées once did. These monuments and artworks orient us in time, to the landscape, to our place within nature and to reoccurring celestial events. The winter solstice – which they all respond to directly – has always been of critical importance to humans, enshrining the significance of light, and honouring death and rebirth in the annual calendar. If the spectacle of these solar-aligned structures lining up perfectly with the rising and setting solstice Sun quickens the soul, it's because it triggers a primal recognition that the darkest hours of the year have passed. It's the first sign of spring's promised return, and future days of increased lightness and warmth.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;If you liked this story, sign up for the Essential List newsletter – a handpicked selection of features, videos and can't-miss news, delivered to your inbox twice a week.&lt;/p&gt;
    &lt;p&gt;For more Culture stories from the BBC, follow us on Facebook and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/culture/article/20251219-the-ancient-monuments-saluting-the-winter-solstice"/><published>2025-12-22T09:30:58+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46352747</id><title>Well being in times of algorithms</title><updated>2025-12-22T15:11:52.143135+00:00</updated><content>&lt;doc fingerprint="3410d04703f78891"&gt;
  &lt;main&gt;
    &lt;p&gt;To be able to live well, to avoid anxiety and depression, it’s not that easy anymore in 2025. Everything is designed to grab our attention, and our attention span is declining and disallowing people to think and learn. Algorithms know more about us than our spouses while grabbing our attention constantly.&lt;/p&gt;
    &lt;p&gt;This is bad for our health. Bad for our spirit. Bad for our well-being. We need a strong foundation that keeps us grounded, not bringing us on the wrong track. The internet, a connected web of websites as it was originally designed, is no more.&lt;/p&gt;
    &lt;p&gt;Today everyone uses a handful of websites, a couple of algorithm-dominated pages that are hitting our brain with dopamine as much as possible. So how can we stay sane in a large tech-dominated world? How do we get quality time when everyone is distracted, fighting for the attention of our friends and families against their phones? That’s why we most often default to just using our phone.&lt;/p&gt;
    &lt;p&gt;I’m currently staying in the Philippines, and the algorithm-dominated web and big tech dependency is even more apparent. People have to use their products such as Facebook Messenger, which is the default for communication and calling. Businesses make sales on Facebook pages, or sell and buy products there.&lt;/p&gt;
    &lt;p&gt;This essay investigates the ground pillars for a well-lived life, and how the open web can help us with that.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Groundwork for Life and Well Being&lt;/head&gt;
    &lt;p&gt;We as individuals have lots of distractions besides the obvious. We are often greedy, egoistic, longing for a lot of money, a big career. And then we try to grind, we try to get a lot of followers on social media, trying to fulfill a Vanity Metric, we try to get more money, a bigger career, a bigger house.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Pillars of Life: Health, Family and Self-Contentment&lt;/head&gt;
    &lt;p&gt;Health, a non-greedy life, knowing when we have enough, and family are the three pillars. We need to recognize them. We cannot do it ourselves.&lt;/p&gt;
    &lt;p&gt;A non-greedy life can be anything like being spiritual, doing ice baths, praying, or doing anything slowly, going inwards. Anything that takes us out of our egocentric way.&lt;/p&gt;
    &lt;p&gt;Health is a prerequisite. As long as we are healthy, we don’t think too much about it. Only when we get sick, we learn that everything is dependent on it. It can help us to get out of an egocentric life too. If we are sick, we need to accept help from friends and loved ones. We learn that we need family to help us in difficult times.&lt;/p&gt;
    &lt;p&gt;Some don’t grow up with a family, so this is a hard one to fill. But family can be your cousins, your stepdad, friends, or anyone who truly cares for you. But without family, or that surrounding bond, it’s easier to fall into the wrong path and get bad habits of drinking, drugs, or similar. If we don’t have it, a helpful foundation, one that has held our society for the past centuries, is the church.&lt;/p&gt;
    &lt;p&gt;Everyone is welcome, no stereotypes or prejudice. You just come and receive love and friendship. A community to participate in, to receive (first), and submit to and give the longer you are part of it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Purpose of Life&lt;/head&gt;
    &lt;p&gt;I wrote a long article Â«Why are we here on EarthÂ», where I elaborate on my journey of purpose, life, and some of the foundations. As I discuss there as well, principles is another way to navigate life and have a healthy lifestyle once you find your true principles to live for. It’s like a compass guiding your life. With them set in place, you will have an easier time deciding and when to say no.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bad Mood: Not Being Present&lt;/head&gt;
    &lt;p&gt;Getting out of ourselves, being asleep as Anthony de Mello writes, most of us are asleep our whole lives. He says that the key to waking up and being aware is learning something new, being open, and most importantly, being present.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Evil: A Downward Spiral of Negativity&lt;/head&gt;
    &lt;p&gt;If the foundation pillars are not met, we will get into bad moods at first. If it happens over a longer period, we might get chronically bad moods. This reflects on other people, who are less likely to spend time with us. We isolate ourselves. If there’s a death or life-changing incident, it’s easy to fall into a deep hole, an evil downward spiral of negativity, potentially into Nihilism.&lt;/p&gt;
    &lt;p&gt;Money and financial stability is another pillar that might look or be considered a main pillar, as without it, it’s hard to be happy. But to be honest, if we have a lot of money, we use it. If we have little, we use it too. So with the above-mentioned pillars, I still think these are more important and independent of money, though if we have no money at all, all these we discuss are irrelevant.&lt;/p&gt;
    &lt;p&gt;In a way, money is just a distraction from living a meaningful life. It certainly helps, but a life centered around it will be the start of a toxic life in most cases.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Status Game: Greed&lt;/head&gt;
    &lt;p&gt;Another way of a downward spiral that happens usually in more well-developed countries, where our main pillars are met, but we are just greedy and want more. Better career, more money, anything to get a better status.&lt;/p&gt;
    &lt;head rend="h3"&gt;What We’re After: Quality Time&lt;/head&gt;
    &lt;p&gt;The ultimate goal for me is to get quality time with loved ones, people that mean something to us. Giving us happiness as backed by a long-term study from Harvard. It’s a deep happiness compared to just a Shallow Happiness followed to get the outside reputation, signaling to the outside that we are happy. However, deep inside, we might not be.&lt;/p&gt;
    &lt;p&gt;Not being present is the opposite of quality time. It’s what we lack when we use the phone and social media too much. We get addicted to social media, we break relationships and lose self-confidence, killing the very foundation of well-being and time well spent with our loved ones.&lt;/p&gt;
    &lt;p&gt;A healthy, deep, meaningful, happy life might actually look boring from the outside. This Boring Life can be an enabler to more happiness I believe, as I write and think more about it.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Do We Get Out of It?&lt;/head&gt;
    &lt;p&gt;Soon we approach the new year of 2026. So, how do we get out of this distracted, algorithm-dominated world? To a place where everybody can share and be in control, not big tech?&lt;/p&gt;
    &lt;p&gt;I feel that people are fed up with the internet for the first time since its creation. With all the AI-generated content, generating several complete books in a day (the Amazon Kindle limit is 10 per week which is a joke and offensive for any author). At least I am.&lt;/p&gt;
    &lt;p&gt;It’s getting harder and harder to tell what is written by a person and what is generated. Sometimes I’d rather see the prompt, as this is where the genuineness, the soul and curiosity of the person is. I wrote more on Finding Flow and Escaping Digital Distractions Through Deep Work and Slow Living, but flow is an essential piece of happiness and purpose of us doing something.&lt;/p&gt;
    &lt;p&gt;With AI, people feel they don’t need to learn programming, writing, or any craft. But I think that’s the wrong conclusion, especially on a personal level. As AI won’t replace human thinking, we will lose the muscle of thinking like we lost the ability to do simple math or remember a phone number as phones and calculators have replaced it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Algorithm-Free Internet: An Open Web&lt;/head&gt;
    &lt;p&gt;To get away from algorithms, away from being locked in and dependent on the platform, away from big tech chasing our attention, back to real connections as opposed to losing our followers with the Death of the Follower. We need open platforms such as Open Social Media and an open web, where the power isn’t in the platform, but in us as the producers. We need to get out of the algorithms, free from big tech, and back to real connections. But how?&lt;/p&gt;
    &lt;p&gt;A concept that Patreon’s creator Jack Conte explains well: we can’t even reach our own followers anymore on social media platforms, though they decided to follow us. See more on Death of the Follower and linked YouTube video by Jack.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Internet: How It Was Designed&lt;/head&gt;
    &lt;p&gt;Initially, for a long time, the internet was a nice place to be. You could search for information from AltaVista, Bluewin (in Switzerland) to find information about the Siberian tiger for making a presentation in school. Or you could share your thoughts, usually in forums or personal websites or blogs.&lt;/p&gt;
    &lt;p&gt;It was interconnected. Each website usually had backlinks to its favorite websites so that people could explore similar websites. It was a web of information, and the algorithm by the people’s recommendations on each website instead of a centralized company or algorithm.&lt;/p&gt;
    &lt;head rend="h3"&gt;Back to the Producer: Direct Connection&lt;/head&gt;
    &lt;p&gt;Instead of depending on the algorithm for sharing our content, we need to have direct connections and followers, like we did with the newsletter all along. That’s why Substack1 and newsletters are so popular, because we get direct connections with our readers.&lt;/p&gt;
    &lt;p&gt;But we don’t want only to rely on newsletters (emails). We also want a direct connection via links in the World Wide Web. Google is still the place for discovery in my opinion. Nobody goes to ChatGPT for a good essay or a deep moving piece. This is where Google or real blog posts, papers, and websites shine, though not all anymore, as there’s so much spam or even slop with AI.&lt;/p&gt;
    &lt;p&gt;Compared to algorithms, direct connections let us choose what we want to see. We have the choice, unlike with the Twitter, LinkedIn, and Facebook algorithms. No surprise people get mad when the algorithms change, as recently with LinkedIn or YouTube many times, making it impossible to reach your audience anymore.&lt;/p&gt;
    &lt;p&gt;LinkedIn people left for Substack, but it’s only a question of time until Substack changes its algorithm for Notes (Twitter copy) or does something wrong (there is already quite some controversy with people migrating away). The best is still to own your content, have your own website and domain, or use a totally new social media approach called open social media.&lt;/p&gt;
    &lt;head rend="h3"&gt;Open Web and an Open Social Media&lt;/head&gt;
    &lt;p&gt;One way is the open web that Dan Abramov is explaining so well in Open Social with the AT Protocol, an open protocol where people own their data (followers, shared data, etc.) and applications can be built on top of that. For example, there is a Twitter called Bluesky that is based on AT Protocol. There’s a HackerNews called Frontpage, Tangled, a git alternative or Instagram, TikTok and many more, all implemented with the same ID and AT Protocol.&lt;/p&gt;
    &lt;p&gt;You can even host the data on your own server with a so-called Personal Data Server (PDS) and be able to take them offline whenever you want. It’s still unclear if AT Protocol will be fully mainstream, but I’m very confident that people who are tired of being dependent can use this happily as I do daily. With each algorithm change, more will follow and there’s already a 41 million active users.&lt;/p&gt;
    &lt;p&gt;With the AI slop being promoted on the major social media platform’s algorithm, I believe we will go back to following real humans. Back to followers, where we decide who we want to see.&lt;/p&gt;
    &lt;p&gt;It takes time and hard work to move to another platform, but if you invest in something else, it should be open and owned by us. That’s why AT Protocol will only get more popular and stronger the longer it exists due to its open foundation.&lt;/p&gt;
    &lt;p&gt;Bluesky is maturing more and people can share their content (not penalized by sharing links) and even host their own content, integrating it into other applications such as discussed above. For example, I integrated comments on my second brain based on Bluesky. It’s like the early Twitter, open APIs, even the data is fully open and can be fetched by attaching to the Jetstream. With all these features and characteristics, it will be a very long-term thing. Even if you can’t go viral, which is by design, you can reach your followers. As always, it depends on what is most important to you.&lt;/p&gt;
    &lt;p&gt;And at some point, Bluesky will also implement an algorithm that can make your posts a little more viral. Today they have one called “Discover Feed”, which is not perfect yet, but will be with the hired data scientist to improve it. But again, it will never be the same as Twitter or others, as it’s especially made for people to follow people.&lt;/p&gt;
    &lt;head rend="h3"&gt;Personal Blogs&lt;/head&gt;
    &lt;p&gt;I also believe in a comeback of personal blogs. Bringing back uniqueness and soul in times of unification of social media and newsletters. There are still many WordPress pages out there, which are more customized and unique than what we have and had with Medium, Substack and Open Subscription Platforms.&lt;/p&gt;
    &lt;p&gt;Patreon is another alternative, focusing on direct connections with your followers and audience instead of going viral. I believe this will be the future: real connections, real content, real stories. Unlike artificially generated content just for fame and vanity metrics.&lt;/p&gt;
    &lt;p&gt;I also hope to see more connected websites, sites like public second brains where visitors can explore content multi-dimensionally instead of single-level. In addition to a traditional blog, providing backlinks and a graph view, diving into the content and life of the creator, finding connections you might not otherwise.&lt;/p&gt;
    &lt;p&gt;And with personal blogs, there are no artificial algorithms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping Up&lt;/head&gt;
    &lt;p&gt;With the recent investments from governments in Denmark and others to ban social media for kids under fifteen and other measures to regulate social media, I think we are heading in the right direction. Big tech does not have the interest of the people. Instead of helping us, which they did in the beginning, they want to profit more off us and make us more addicted. And as with alcohol and other drugs, these need to be regulated, especially when there’s such a huge monopoly with just a few companies.&lt;/p&gt;
    &lt;p&gt;This will increase our well-being, and we will have more time to spend on the main pillars of life. We will be less anxious, less negative. We will be happier, more free.&lt;/p&gt;
    &lt;p&gt;This might not show an immediate effect, but as it’s compounding, it will in the long term of our life and career. Like the Pathless Path and its book by Paul Millerd suggest, following your Instinct more and being open, unexpected things will happen, things we can’t plan ahead of time. Like Steve Jobs already said before about connecting the dots:&lt;/p&gt;
    &lt;quote&gt;You can’t connect the dots looking forward; you can only connect them looking backward. So, you have to trust that the dots will somehow connect in your future.&lt;/quote&gt;
    &lt;p&gt;Trust in your principles, in the process. Spend more time offline, more time with loved ones, and then good things will happen. Time away from algorithms, and more time being well.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;and other Open Subscription Platforms ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ssp.sh/blog/well-being-algorithms/"/><published>2025-12-22T09:58:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46352875</id><title>A year of vibes</title><updated>2025-12-22T15:11:51.593721+00:00</updated><content>&lt;doc fingerprint="375c195f06b20cf9"&gt;
  &lt;main&gt;
    &lt;p&gt;written on December 22, 2025&lt;/p&gt;
    &lt;p&gt;2025 draws to a close and it’s been quite a year. Around this time last year, I wrote a post that reflected on my life. Had I written about programming, it might have aged badly, as 2025 has been a year like no other for my profession.&lt;/p&gt;
    &lt;p&gt;2025 was the year of changes. Not only did I leave Sentry and start my new company, it was also the year I stopped programming the way I did before. In June I finally felt confident enough to share that my way of working was different:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Where I used to spend most of my time in Cursor, I now mostly use Claude Code, almost entirely hands-off. […] If you would have told me even just six months ago that I’d prefer being an engineering lead to a virtual programmer intern over hitting the keys myself, I would not have believed it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While I set out last year wanting to write more, that desire had nothing to do with agentic coding. Yet I published 36 posts — almost 18% of all posts on this blog since 2007. I also had around a hundred conversations with programmers, founders, and others about AI because I was fired up with curiosity after falling into the agent rabbit hole.&lt;/p&gt;
    &lt;p&gt;2025 was also a not so great year for the world. To make my peace with it, I started a separate blog to separate out my thoughts from here.&lt;/p&gt;
    &lt;p&gt;It started with a growing obsession with Claude Code in April or May, resulting in months of building my own agents and using others’. Social media exploded with opinions on AI: some good, some bad.&lt;/p&gt;
    &lt;p&gt;Now I feel I have found a new stable status quo for how I reason about where we are and where we are going. I’m doubling down on code generation, file systems, programmatic tool invocation via an interpreter glue, and skill-based learning. Basically: what Claude Code innovated is still state of the art for me. That has worked very well over the last few months, and seeing foundation model providers double down on skills reinforces my belief in this approach.&lt;/p&gt;
    &lt;p&gt;I’m still perplexed by how TUIs made such a strong comeback. At the moment I’m using Amp, Claude Code, and Pi, all from the command line. Amp feels like the Apple or Porsche of agentic coding tools, Claude Code is the affordable Volkswagen, and Pi is the Hacker’s Open Source choice for me. They all feel like projects built by people who, like me, use them to an unhealthy degree to build their own products, but with different trade-offs.&lt;/p&gt;
    &lt;p&gt;I continue to be blown away by what LLMs paired with tool execution can do. At the beginning of the year I mostly used them for code generation, but now a big number of my agentic uses are day-to-day things. I’m sure we will see some exciting pushes towards consumer products in 2026. LLMs are now helping me with organizing my life, and I expect that to grow further.&lt;/p&gt;
    &lt;p&gt;Because LLMs now not only help me program, I’m starting to rethink my relationship to those machines. I increasingly find it harder not to create parasocial bonds with some of the tools I use. I find this odd and discomforting. Most agents we use today do not have much of a memory and have little personality but it’s easy to build yourself one that does. An LLM with memory is an experience that is hard to shake off.&lt;/p&gt;
    &lt;p&gt;It’s both fascinating and questionable. I have tried to train myself for two years, to think of these models as mere token tumblers, but that reductive view does not work for me any longer. These systems we now create have human tendencies, but elevating them to a human level would be a mistake. I increasingly take issue with calling these machines “agents,” yet I have no better word for it. I take issue with “agent” as a term because agency and responsibility should remain with humans. Whatever they are becoming, they can trigger emotional responses in us that can be detrimental if we are not careful. Our inability to properly name and place these creations in relation to us is a challenge I believe we need to solve.&lt;/p&gt;
    &lt;p&gt;Because of all this unintentional anthropomorphization, I’m really struggling at times to find the right words for how I’m working with these machines. I know that this is not just me; it’s others too. It creates even more discomfort when working with people who currently reject these systems outright. One of the most common comments I read in response to agentic coding tool articles is this rejection of giving the machine personality.&lt;/p&gt;
    &lt;p&gt;An unexpected aspect of using AI so much is that we talk far more about vibes than anything else. This way of working is less than a year old, yet it challenges half a century of software engineering experience. So there are many opinions, and it’s hard to say which will stand the test of time.&lt;/p&gt;
    &lt;p&gt;I found a lot of conventional wisdom I don’t agree with, but I have nothing to back up my opinions. How would I? I quite vocally shared my lack of success with MCP throughout the year, but I had little to back it up beyond “does not work for me.” Others swore by it. Similar with model selection. Peter, who got me hooked on Claude early in the year, moved to Codex and is happy with it. I don’t enjoy that experience nearly as much, though I started using it more. I have nothing beyond vibes to back up my preference for Claude.&lt;/p&gt;
    &lt;p&gt;It’s also important to know that some of the vibes come with intentional signalling. Plenty of people whose views you can find online have a financial interest in one product over another, for instance because they are investors in it or they are paid influencers. They might have become investors because they liked the product, but it’s also possible that their views are affected and shaped by that relationship.&lt;/p&gt;
    &lt;p&gt;Pick up a library from any AI company today and you’ll notice they’re built with Stainless or Fern. The docs use Mintlify, the site’s authentication system might be Clerk. Companies now sell services you would have built yourself previously. This increase in outsourcing of core services to companies specializing in it meant that the bar for some aspects of the user experience has risen.&lt;/p&gt;
    &lt;p&gt;But with our newfound power from agentic coding tools, you can build much of this yourself. I had Claude build me an SDK generator for Python and TypeScript — partly out of curiosity, partly because it felt easy enough. As you might know, I’m a proponent of simple code and building it yourself. This makes me somewhat optimistic that AI has the potential to encourage building on fewer dependencies. At the same time, it’s not clear to me that we’re moving that way given the current trends of outsourcing everything.&lt;/p&gt;
    &lt;p&gt;This brings me not to predictions but to wishes for where we could put our energy next. I don’t really know what I’m looking for here, but I want to point at my pain points and give some context and food for thought.&lt;/p&gt;
    &lt;p&gt;My biggest unexpected finding: we’re hitting limits of traditional tools for sharing code. The pull request model on GitHub doesn’t carry enough information to review AI generated code properly — I wish I could see the prompts that led to changes. It’s not just GitHub, it’s also git that is lacking.&lt;/p&gt;
    &lt;p&gt;With agentic coding, part of what makes the models work today is knowing the mistakes. If you steer it back to an earlier state, you want the tool to remember what went wrong. There is, for lack of a better word, value in failures. As humans we might also benefit from knowing the paths that did not lead us anywhere, but for machines this is critical information. You notice this when you are trying to compress the conversation history. Discarding the paths that led you astray means that the model will try the same mistakes again.&lt;/p&gt;
    &lt;p&gt;Some agentic coding tools have begun spinning up worktrees or creating checkpoints in git for restore, in-conversation branch and undo features. There’s room for UX innovation that could make these tools easier to work with. This is probably why we’re seeing discussions about stacked diffs and alternative version control systems like Jujutsu.&lt;/p&gt;
    &lt;p&gt;Will this change GitHub or will it create space for some new competition? I hope so. I increasingly want to better understand genuine human input and tell it apart from machine output. I want to see the prompts and the attempts that failed along the way. And then somehow I want to squash and compress it all on merge, but with a way to retrieve the full history if needed.&lt;/p&gt;
    &lt;p&gt;This is related to the version control piece: current code review tools assign strict role definitions that just don’t work with AI. Take the GitHub code review UI: I regularly want to use comments on the PR view to leave notes for my own agents, but there is no guided way to do that. The review interface refuses to let me review my own code, I can only comment, but that does not have quite the same intention.&lt;/p&gt;
    &lt;p&gt;There is also the problem that an increased amount of code review now happens between me and my agents locally. For instance, the Codex code review feature on GitHub stopped working for me because it can only be bound to one organization at a time. So I now use Codex on the command line to do reviews, but that means a whole part of my iteration cycles is invisible to other engineers on the team. That doesn’t work for me.&lt;/p&gt;
    &lt;p&gt;Code review to me feels like it needs to become part of the VCS.&lt;/p&gt;
    &lt;p&gt;I also believe that observability is up for grabs again. We now have both the need and opportunity to take advantage of it on a whole new level. Most people were not in a position where they could build their own eBPF programs, but LLMs can. Likewise, many observability tools shied away from SQL because of its complexity, but LLMs are better at it than any proprietary query language. They can write queries, they can grep, they can map-reduce, they remote-control LLDB. Anything that has some structure and text is suddenly fertile ground for agentic coding tools to succeed. I don’t know what the observability of the future looks like, but my strong hunch is that we will see plenty of innovation here. The better the feedback loop to the machine, the better the results.&lt;/p&gt;
    &lt;p&gt;I’m not even sure what I’m asking for here, but I think that one of the challenges in the past was that many cool ideas for better observability — specifically dynamic reconfiguration of services for more targeted filtering — were user-unfriendly because they were complex and hard to use. But now those might be the right solutions in light of LLMs because of their increased capabilities for doing this grunt work. For instance Python 3.14 landed an external debugger interface which is an amazing capability for an agentic coding tool.&lt;/p&gt;
    &lt;p&gt;This may be a little more controversial, but what I haven’t managed this year is to give in to the machine. I still treat it like regular software engineering and review a lot. I also recognize that an increasing number of people are not working with this model of engineering but instead completely given in to the machine. As crazy as that sounds, I have seen some people be quite successful with this. I don’t yet know how to reason about this, but it is clear to me that even though code is being generated in the end, the way of working in that new world is very different from the world that I’m comfortable with. And my suspicion is that because that world is here to stay, we might need some new social contracts to separate these out.&lt;/p&gt;
    &lt;p&gt;The most obvious version of this is the increased amount of these types of contributions to Open Source projects, which are quite frankly an insult to anyone who is not working in that model. I find reading such pull requests quite rage-inducing.&lt;/p&gt;
    &lt;p&gt;Personally, I’ve tried to attack this problem with contribution guidelines and pull request templates. But this seems a little like a fight against windmills. This might be something where the solution will not come from changing what we’re doing. Instead, it might come from vocal people who are also pro-AI engineering speaking out on what good behavior in an agentic codebase looks like. And it is not just to throw up unreviewed code and then have another person figure the shit out.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lucumr.pocoo.org/2025/12/22/a-year-of-vibes/"/><published>2025-12-22T10:19:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46352930</id><title>If you don't design your career, someone else will (2014)</title><updated>2025-12-22T15:11:51.429597+00:00</updated><content>&lt;doc fingerprint="227b983565a77a21"&gt;
  &lt;main&gt;
    &lt;p&gt;A client once responded to one of my questions by saying, “Oh Greg, I am too busy living to think about life!” His off-the-cuff comment named a trap all of us fall into sometimes. In just one example, it is easy to become so consumed in our careers we fail to really think about our careers.&lt;/p&gt;
    &lt;p&gt;To avoid this trap, I suggest carving out a couple of hours over the holiday break to follow these simple steps for reflecting on your career.&lt;/p&gt;
    &lt;p&gt;Step 1: Review the last 12 months. Review the year, month by month. Make a list of where you spent your time: include your major projects, responsibilities and accomplishments. No need to overcomplicate this.&lt;/p&gt;
    &lt;p&gt;Step 2: Ask, “What is the news?” Look over your list and reflect on what is really going on. Think like a journalist and ask yourself: Why does this matter? What are the trends here? What happens if these trends continue?&lt;/p&gt;
    &lt;p&gt;Step 3: Ask “What would I do in my career if I could do anything?” Just brainstorm with no voice of criticism to hold you back. Just write out all the ideas that come to mind.&lt;/p&gt;
    &lt;p&gt;Step 4: Go back and spend a bit more time on Step 3. Too often we begin our career planning with our second best option in mind. We have a sense of what we would most love to do but we immediately push it aside. Why? Typically because “it is not realistic” which is code for, “I can’t make money doing this.” In this economy—in any economy—I understand why making money is critical. However, sometimes we pass by legitimate career paths because we set them aside too quickly.&lt;/p&gt;
    &lt;p&gt;Step 5: Write down six objectives for the next 12 months. Make a list of the top six items you would like to accomplish in your career this year and place them in priority order.&lt;/p&gt;
    &lt;p&gt;Step 6: Cross off the bottom five. Once you’re back to the whirlwind of work you’ll benefit from having a single “true north” career objective for the year.&lt;/p&gt;
    &lt;p&gt;Step 7: Make an action plan for this month. Make a list of some quick wins you’d like to have in place over the next 3-4 weeks.&lt;/p&gt;
    &lt;p&gt;Step 8: Decide what you will say no to. Make a list of the “good” things that will keep you from achieving your one “great” career objective. Think about how to delete, defer or delegate these other tasks. Ralph Waldo Emerson said, “The crime which bankrupts men and nations is that of turning aside from one’s main purpose to serve a job here and there.”&lt;/p&gt;
    &lt;p&gt;Many years ago I followed this process and, without exaggeration, it changed the course of my life. The insight I gained led me to quit law school, leave England and move to America and start down the path as a teacher and author. You’re reading this because of that choice. It remains the single most important career decision of my life.&lt;/p&gt;
    &lt;p&gt;Two hours spent wisely over the next couple of weeks could easily improve the quality of your life over the 8760 hours of the next year–and perhaps far beyond. After all, if we don’t design our careers, someone else will.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gregmckeown.com/if-you-dont-design-your-career-someone-else-will/"/><published>2025-12-22T10:27:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46353261</id><title>Italian Competition Authority Fines Apple $115M for Abusing Dominant Position</title><updated>2025-12-22T15:11:51.198532+00:00</updated><content/><link href="https://en.agcm.it/en/media/press-releases/2025/12/A561"/><published>2025-12-22T11:22:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46353777</id><title>The biggest CRT ever made: Sony's PVM-4300</title><updated>2025-12-22T15:11:50.453599+00:00</updated><content>&lt;doc fingerprint="73489d35a9a732be"&gt;
  &lt;main&gt;
    &lt;p&gt;Move over, GE Widescreen 1000. In 1989 in Japan, Sony introduced to the largest Trinitron CRT ever built, the KV-45ED1, also known as the PVM-4300. And in 1990, they imported 20 of them to the United States, just in time for the recession. About 34 years later, one of these enigmatic TVs surfaced.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sony’s PVM-4300/KV-45ED1&lt;/head&gt;
    &lt;p&gt;Sony’s part number suggests it has a 45 inch tube inside. But in a rare case of truth in advertising, Sony advertised it as a 43-inch model. It weighed about 450 pounds, stood about 27 inches tall, and it wouldn’t fit through a standard door frame. That’s probably okay, it’s not like someone was going to use this as a bedroom TV. This thing was going in your living room.&lt;/p&gt;
    &lt;p&gt;In Japan, it sold for 2.6 million yen, but in the United States, it retailed for $40,000, a significant markup. To be fair, shipping them across the Atlantic and then throughout the United States must have been expensive. And news articles in 1990 said Sony dealers would not allow any bickering. They would throw in a couple of options like the separate tuner or speakers. But no discounts.&lt;/p&gt;
    &lt;p&gt;Sony said at the time they hoped to sell 80 of them that year, but the recession may have kept that from happening.&lt;/p&gt;
    &lt;head rend="h2"&gt;The biggest conventional CRT ever&lt;/head&gt;
    &lt;p&gt;The Sony PVM-4300 was a conventional CRT, unlike the GE Widescreen 1000, which was a projection set. Projection TVs could be bigger and cheaper. But if you wanted the clearest picture, a big CRT was where it was at.&lt;/p&gt;
    &lt;p&gt;It was a conventional CRT that worked with over the air signals, but like many larger TVs of the era, it used a technology called IDTV to enhance the picture quality. The “ID” stood for “improved definition.” IDTV sets had a buffer so they would store successive frames and interpolate them rather than interlacing them the way a conventional CRT TV worked. They also had circuitry to detect motion and perform image stabilization to further enhance the image. The result wasn’t as good as HDTV. But it gave high rollers a better picture until HDTV. HDTV arrived in 1998, but articles at the time estimated 2005. The Chicago Tribune warned in 1990 that these $40,000 TVs would be obsolete in 15 years, but the salesperson countered that every TV would be obsolete in 15 years.&lt;/p&gt;
    &lt;p&gt;It’s also likely that someone in the market for a $40,000 TV didn’t worry about obsolescence. In 1990, the GE Widescreen 1000 looked dated and it wasn’t 15 years old yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why so expensive?&lt;/head&gt;
    &lt;p&gt;The KV-45ED1 or PVM-4300 cost about 8 times as much as Sony’s second most expensive model at the time, which had a 29-inch screen. That’s largely because the KV-45ED1 had to be built by hand. Sony could mass produce its smaller TVs. This was a product for buyers who weren’t worried about the price.&lt;/p&gt;
    &lt;p&gt;Sony continued making CRTs into the 21st century, bowing out with its high-def KD-34XBR970, 36-inch KD-36FS170, 32-inch KV-32FS170 and 27-inch KV27FS170 in February 2006.&lt;/p&gt;
    &lt;p&gt;It is unclear how many of these enormous 43-inch units Sony sold, and some people even questioned if it was ever built. A Chicago area dealer told the Chicago Tribune in 1990 that someone had purchased one, but that the buyer wanted to remain anonymous. That was good enough for me; a TV dealer wasn’t going to tell a newspaper that they have a $40,000 item and then not have it. That’s just bad business. Good business is taking the free advertising, having an example on display to show knowing most won’t buy it, but they may buy one of the smaller units. But luring someone into the store with a lie makes it much more difficult to sell anything.&lt;/p&gt;
    &lt;p&gt;And the Tribune wasn’t going to make something like this up. It would anger the TV dealers and risk losing their advertising. And someone who could afford a $40,000 TV was likely a business owner or high-ranking executive who could pull their advertising. In the days of print newspapers, advertisers and potential advertisers held a lot of sway. This could be both good and bad. I’m not going to say capitalism solves every problem but this was a case where it helped keep people honest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Shank Mods’ surviving Sony PVM-4300&lt;/head&gt;
    &lt;p&gt;On December 22, 2024, Youtuber Shank Mods released a video telling the story of a Sony PVM-4300 and how he acquired it. One of the photos of a purported surviving unit turned out to be very real. It was taken in a restaurant in Japan, and the owner was actually aware of the photo. Unfortunately the restaurant was having to move, and needed to get rid of the set. Shank Mods was able to contact some people in Japan who could help race against time and remove the TV from the restaurant and then ship it to the United States.&lt;/p&gt;
    &lt;p&gt;The 35-minute video is well worth watching if you have interest in vintage CRTs, or even if you just like stories of strangers coming together and helping each other just for the sake of being helpful. Actually, I take that back. At the end of the video, Shank Mods played a prank on his fellow CRT fans that is absolutely hilarious and makes the video worth watching for that reason alone. I won’t ruin it for you.&lt;/p&gt;
    &lt;p&gt;We can only guess how many other examples may survive. But we now know that at least one survives and is in the hands of a retro hobbyist.&lt;/p&gt;
    &lt;p&gt;David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://dfarq.homeip.net/the-biggest-crt-ever-made-sonys-pvm-4300/"/><published>2025-12-22T12:54:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46354372</id><title>(Social) media manipulation in one image</title><updated>2025-12-22T15:11:49.707955+00:00</updated><content>&lt;doc fingerprint="45c430185fd3b0"&gt;
  &lt;main&gt;
    &lt;p&gt;We're sorry but this website doesn't work properly without JavaScript enabled. Please enable it to continue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kerkour.com/social-media-manipulation"/><published>2025-12-22T14:28:42+00:00</published></entry></feed>