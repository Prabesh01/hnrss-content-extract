<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-20T11:09:30.007253+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45591288</id><title>Give Your Metrics an Expiry Date</title><updated>2025-10-20T11:09:39.815492+00:00</updated><content>&lt;doc fingerprint="b4757f3cf02c9ed8"&gt;
  &lt;main&gt;
    &lt;p&gt;(I wrote this back in May and failed to hit publish for some reason — which I discovered when I wanted to point somebody else to it. So publishing now!)&lt;/p&gt;
    &lt;p&gt;Today the expiry date for a dashboard metric came up. It’s a proxy trailing metric for Something We Care About. The details are unimportant.&lt;/p&gt;
    &lt;p&gt;Tracking it for the last eighteen months has helped us spot some opportunities to improve.&lt;/p&gt;
    &lt;p&gt;In the last six months exploring some of those opportunities has helped us improve month-on-month for the last six months.&lt;/p&gt;
    &lt;p&gt;Which is delightful!&lt;/p&gt;
    &lt;p&gt;But having an expiry date means we get to take a step back and reassess whether it’s still useful. While it’s still a half way decent proxy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;we now have more of an idea what specific actions move Something We Care About — and can track those directly&lt;/item&gt;
      &lt;item&gt;while the metric lets us know we are improving, it doesn’t give a lot of signal on the rate of improvement&lt;/item&gt;
      &lt;item&gt;we’ve not used the metric to make any decisions with over the last three months&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The metric has turned from something valuable — something that helped us make decisions and drive action — to a little number that’s always saying “it’s good”.&lt;/p&gt;
    &lt;p&gt;It’s now failing the three questions I use to assess dashboard metrics:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Is it visible at the right time?&lt;/item&gt;
      &lt;item&gt;Is it actionable?&lt;/item&gt;
      &lt;item&gt;Is it used?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So… time for that particular metric to be thanked for its service and retired!&lt;/p&gt;
    &lt;p&gt;Would we have retired it if we didn’t have the nudge from an expiry date? Probably not.&lt;/p&gt;
    &lt;p&gt;Looking at dashboards and metrics become a habit — that’s part of their value — and breaking habits is hard. Which can lead to a huge mess of unactionable and irrelevant pretty graphs — and a false sense of confidence that everything important is tracked and under control.&lt;/p&gt;
    &lt;p&gt;Which is why setting up little nudges to check whether a metric is still effective is so useful.&lt;/p&gt;
    &lt;p&gt;TL;DR: Set expiry dates for your metrics ;-)&lt;/p&gt;
    &lt;p&gt;ttfn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://adrianhoward.com/posts/give-your-metrics-an-expiry-date/"/><published>2025-10-15T12:21:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45635069</id><title>GNU Octave Meets JupyterLite: Compute Anywhere, Anytime</title><updated>2025-10-20T11:09:38.547600+00:00</updated><content>&lt;doc fingerprint="469708be47d97371"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GNU Octave Meets JupyterLite: Compute Anywhere, Anytime!&lt;/head&gt;
    &lt;p&gt;We are thrilled to announce the newest member of our JupyterLite kernel ecosystem: Xeus-Octave. Xeus-Octave allows you to run GNU Octave code directly on your browser. GNU Octave is a free and open-source Scientific Programming Language that can be used to run Matlab scripts. In this article, we present the challenges encountered when targeting WebAssembly, the current state of the Xeus-Octave kernel, and the future plans for expanding the GNU Octave ecosystem.&lt;/p&gt;
    &lt;p&gt;Earlier this year, we introduced the JupyterLite kernel for R, Xeus-R-Lite. Much like R, cross-compiling GNU Octave to WebAssembly required the same custom toolchain to enable the compilation of Fortran code, combining LLVM Flang and Emscripten.&lt;/p&gt;
    &lt;p&gt;Similar to many other mathematically oriented language packages, GNU Octave requires a BLAS/LAPACK implementation. Fortunately, OpenBLAS and the Netlib implementations of BLAS/LAPACK had already been added to the emscripten-forge WebAssembly distribution. Initially, OpenBLAS was the preferred implementation, but for the successful compilation of Octave, Netlib LAPACK was selected as it presented fewer hurdles during the build process.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cross-Compilation of GNU Octave&lt;/head&gt;
    &lt;p&gt;One of the complications of cross-compiling Octave to WebAssembly, which had not been encountered with the R source code, was the extensive use of Fortran common symbols blocks in the internal libraries of Octave such as odepack.&lt;/p&gt;
    &lt;code&gt;C Source: liboctave/external/odepack/slsode.f&lt;lb/&gt;C-----------------------------------------------------------------------&lt;lb/&gt;C The following internal Common block contains&lt;lb/&gt;C (a) variables which are local to any subroutine but whose values must&lt;lb/&gt;C     be preserved between calls to the routine ("own" variables), and&lt;lb/&gt;C (b) variables which are communicated between subroutines.&lt;lb/&gt;C The block SLS001 is declared in subroutines SLSODE, SINTDY, SSTODE,&lt;lb/&gt;C SPREPJ, and SSOLSY.&lt;lb/&gt;C Groups of variables are replaced by dummy arrays in the Common&lt;lb/&gt;C declarations in routines where those variables are not used.&lt;lb/&gt;C-----------------------------------------------------------------------&lt;lb/&gt;     COMMON /SLS001/ CONIT, CRATE, EL(13), ELCO(13,12),&lt;lb/&gt;    1   HOLD, RMAX, TESCO(3,12),&lt;lb/&gt;    1   CCMAX, EL0, H, HMIN, HMXI, HU, RC, TN, UROUND,&lt;lb/&gt;    2   INIT, MXSTEP, MXHNIL, NHNIL, NSLAST, NYH,&lt;lb/&gt;    3   IALTH, IPUP, LMAX, MEO, NQNYH, NSLP,&lt;lb/&gt;    3   ICF, IERPJ, IERSL, JCUR, JSTART, KFLAG, L,&lt;lb/&gt;    4   LYH, LEWT, LACOR, LSAVF, LWM, LIWM, METH, MITER,&lt;lb/&gt;    5   MAXORD, MAXCOR, MSBP, MXNCF, N, NQ, NST, NFE, NJE, NQU&lt;/code&gt;
    &lt;p&gt;Initially, it was not possible to cross-compile these common blocks to WebAssembly because the latest version of LLVM (v20 at the time of testing) did not support common symbol linkage.&lt;/p&gt;
    &lt;code&gt;// Source: llvm/lib/MC/MCWasmStreamer.cpp&lt;lb/&gt;void MCWasmStreamer::emitCommonSymbol(MCSymbol *S, uint64_t Size,&lt;lb/&gt;                                      Align ByteAlignment) {&lt;lb/&gt;  llvm_unreachable("Common symbols are not yet implemented for Wasm");&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;As a temporary solution, LLVM was patched with the help of Serge Guelton to simulate common symbols as weak symbols.&lt;/p&gt;
    &lt;code&gt;void MCWasmStreamer::emitCommonSymbol(MCSymbol *S, uint64_t Size,&lt;lb/&gt;                                      Align ByteAlignment) {&lt;lb/&gt;-  llvm_unreachable("Common symbols are not yet implemented for Wasm");&lt;lb/&gt;+  auto *Symbol = cast&amp;lt;mcsymbolwasm&amp;gt;(S);&lt;lb/&gt;+  getAssembler().registerSymbol(*Symbol);&lt;lb/&gt;+  Symbol-&amp;gt;setWeak(true);&lt;lb/&gt;+  Symbol-&amp;gt;setExternal(true);&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;A proper solution to enable support of common symbols is currently in progress and will likely be included in the next release of LLVM v22 (see llvm-project/pull/151478). For curious readers, the patched version of LLVM can be found here (linux only).&lt;/p&gt;
    &lt;p&gt;In addition to the patches for LLVM, GNU Octave required a few minor modifications to target WebAssembly; mainly this entailed disabling the GUI functionalities and consolidating the Fortran function signatures and calling conventions. A full list of patches can be found in the recipe directory on emscripten-forge.&lt;/p&gt;
    &lt;head rend="h2"&gt;Xeus-Octave&lt;/head&gt;
    &lt;p&gt;Once GNU Octave had been successfully packaged for WebAssembly, bringing Xeus-Octave to JupyterLite was a simple matter of adding a recipe to emscripten-forge!&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Work&lt;/head&gt;
    &lt;p&gt;For our next steps, the team is planning on expanding the Octave ecosystem by adding Octave packages to both conda-forge and emscripten-forge. The packaging work will require defining a process where Octave packages can be installed in predetermined conda environments, perhaps with some minor modifications to the existing pkg utility.&lt;/p&gt;
    &lt;head rend="h2"&gt;About the Author&lt;/head&gt;
    &lt;p&gt;Isabel Paredes, who led the charge on bringing GNU Octave to emscripten-forge, is a senior scientific software developer at QuantStack. Prior to working on this project, she focused on porting the R programming language and the Robot Operating System (ROS) framework to WebAssembly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;This project synthesizes work from many open-source contributors.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Emscripten-forge, the distribution of conda packages for WebAssembly, was created by Thorsten Beier, who continues to lead the project. Many recipes were contributed by Isabel Paredes, Anutosh Bhat, Martin Renou, Ian Thomas, Wolf Vollprecht, and Johan Mabille.&lt;/item&gt;
      &lt;item&gt;JupyterLite, the Jupyter distribution that runs entirely in the web browser, was created by Jeremy Tuloup.&lt;/item&gt;
      &lt;item&gt;Xeus, the C++ library implementing the Jupyter kernel protocol, enabling a custom communication layer, and is foundational to kernels like xeus-r, xeus-python, running in JupyterLite, was created by Johan Mabille and is maintained by a broader team including Martin Renou, Sylvain Corlay, and Thorsten Beier, who worked on the first integration with JupyterLite.&lt;/item&gt;
      &lt;item&gt;Xeus-Octave, the Xeus-based Jupyter kernel for GNU Octave, was created by Giulio Girardi and Antoine Prouvost.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.jupyter.org/gnu-octave-meets-jupyterlite-compute-anywhere-anytime-8b033afbbcdc"/><published>2025-10-19T15:48:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45635533</id><title>Doing well in your courses: Andrej's advice for success (2013)</title><updated>2025-10-20T11:09:38.405537+00:00</updated><content>&lt;doc fingerprint="9443181d0f650e9c"&gt;
  &lt;main&gt;
    &lt;p&gt;a guide by Andrej Karpathy&lt;/p&gt;
    &lt;p&gt;Here is some advice I would give to younger students if they wish to do well in their undergraduate courses.&lt;lb/&gt; Having been tested for many years of my life (with pretty good results), here are some rules of thumb that I feel helped me:&lt;/p&gt;
    &lt;p&gt; All-nighters are not worth it. &lt;lb/&gt; Sleep does wonders. Optimal sleep time for me is around 7.5 hours, with an absolute minimum of around 4hrs.&lt;lb/&gt; It has happened to me several times that I was stuck on some problem for an hour in the night, but was able to solve it in 5 minutes in the morning. I feel like the brain "commits" a lot of shaky short-term memories to stable long-term memories during the night. I try to start studying for any big tests well in advance (several days), even if for short periods of time, to maximize the number of nights that my brain gets for the material. &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Attend tutorials or review sessions.&lt;lb/&gt; Even if they are bad. The fact that they get you to think about the material is what counts. If its too boring, you can always work on something else. Remember that you can also try to attend a different tutorial with a different TA.&lt;/p&gt;
    &lt;p&gt; Considering the big picture and organisation is the key. &lt;lb/&gt; Create schedule of study, even if you dont stick to it. For me this usually involves getting an idea of everything I need to know and explicitly writing it down in terms of bullet points. Consider all points carefully and think about how long it will take you to get them down. If you don't do this, there is a tendency to spend too much time on beginning of material and then skim through the (most important) later material due to lack of time.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always try to look at previous tests BEFORE starting to study.&lt;lb/&gt; Especially if the past tests were written by the same professor. This will give you strong hints about how you should study. Every professor has a different evaluation style. Don't actually attempt to complete the questions in the beginning, but take careful note of the type of questions.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Reading and understanding IS NOT the same as replicating the content.&lt;lb/&gt; Even I often make this mistake still: You read a formula/derivation/proof in the book and it makes perfect sense. Now close the book and try to write it down. You will find that this process is completely different and it will amaze you that many times you won't actually be able to do this! Somehow the two things use different parts of the memory. Make it a point to make sure that you can actually write down the most important bits, and that you can re-derive them at will. Feynman famously knew this very well.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always try to collaborate with others, but near the end. &lt;lb/&gt; Study alone first because in the early stages of studying others can only serve as a distraction. But near the end get together with others: they will often point out important pitfalls, bring up good issues, and sometimes give you an opportunity to teach. Which brings me to:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Don't only hang out only with stronger students.&lt;lb/&gt; Weaker students will have you explain things to them and you will find that teaching the material helps A LOT with understanding.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Go to the prof before final exam at least once for office hours. &lt;lb/&gt; Even if you have no questions (make something up!) Profs will sometimes be willing to say more about a test in 1on1 basis (things they would not disclose in front of the entire class). Don't expect it, but when this does happen, it helps a lot. Does this give you an unfair advantage over other students? Sometimes. It's a little shady :)&lt;lb/&gt; But in general it is a good idea to let the prof get to know you at least a little.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Study well in advance. &lt;lb/&gt; Did I mention this already? Maybe I should stress it again. The brain really needs time to absorb material. Things that looked hard become easier with time. &lt;lb/&gt; You want to alocate ~3 days for midterms, ~6 days for exams.&lt;lb/&gt; If things are going badly and you get too tired, in emergency situations, jug an energy drink.&lt;lb/&gt; They work. It's just chemistry.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; For things like math: Exercise &amp;gt; Reading.&lt;lb/&gt; It is good to study to the point where you are reasonably ready to start the exercises, but then fill in the gaps through doing exercises, especially if you have many available to you. The exercises will also make you go back and read things you don't know.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Make yourself cheat sheet. &lt;lb/&gt; Even if you're not allowed to bring it to the exam. Writing things down helps. What you want is to cram the entire course on 1 or more pages that you can in the end tile in front of you and say with high degree of confidence "This is exactly everything I must know"&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Study in places where other people study as well, even if not the same thing. &lt;lb/&gt; This makes you feel bad when you are the one not studying. It works for me :)&lt;lb/&gt; Places with a lot of background noise are bad and have a research-supported negative impact on learning. Libraries and Reading rooms work best.&lt;/p&gt;
    &lt;p&gt; Optimal eating/drinking habit is: T-2 hours get coffee and food.&lt;lb/&gt; For me, Coffee or Food RIGHT before the test is ALWAYS bad&lt;lb/&gt; Coffee right before any potentially stressful situation is ALWAYS bad.&lt;lb/&gt; No coffee at all is bad.&lt;lb/&gt; I realize the coffee bit may be subjective to me, but its something to think about for yourself.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Study very intensely RIGHT before the test. &lt;lb/&gt; I see many people give up before the test and claim to "take a break". Short term memory is a wonderful thing, don't waste it! Study as intensely as possible right before the test. If you really feel you must take a break, take it about an hour before the test, but make sure you study really hard 30-45 minutes before the test.&lt;/p&gt;
    &lt;p&gt; Always use pencil for tests. &lt;lb/&gt; You want to be able to erase your garbage "solutions"&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Look over all questions very briefly before start. &lt;lb/&gt; A mere 1-3 second glance per question is good enough. Just absorb all key words, and get idea of the size of the entire test.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; On test, do easy questions first. &lt;lb/&gt; Do not allow yourself to get stuck on something too long. Come back to it later. I skip questions all the time... Sometimes I can complete as little as 30% of the test on my first pass. Some questions somehow become much easier once you're "warmed up", I can't explain it.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always try to be neat on the test. &lt;lb/&gt; Surprisingly few people actually realize this obvious fact: A human being will mark your test. A sad human being gives low marks. I suspected this as undergrad student and confirmed it strongly when I was TAing and actually marking. &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Always BOX IN/CIRCLE the answer&lt;lb/&gt; Especially when there is derivation around it. This allows the marker to give you a quick check mark for full marks and move on. Get in the mindset of a marker.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; NEVER. EVER. EVER. Leave test early. &lt;lb/&gt; You made a silly mistake (I guarantee it), find it and fix it. If you can't find it, try harder until time runs out. If you are VERY certain of no mistakes, work on making test more legible and easier to mark. Erase garbage, box in answers, add steps to proofs, etc.&lt;lb/&gt; I have no other way of putting this-- people who leave tests early are stupid. This is a clear example of a situation where potential benefits completely outweigh the cost. &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Communicate with the marker. &lt;lb/&gt; Show the marker that you know more than what you put down. Ok you can't do a particular step, but make it clear that you know how to proceed if you did. Don't be afraid to leave notes when necessary. Believe it or not the markers often end up trying to find you more marks-- make it easy for them.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Consider number of points per question.&lt;lb/&gt; Many tests will tell you how many marks every question is worth. This can give you very strong hints when you are doing something wrong. It also gives you strong hints at what questions you should be working on. It is, of course, silly to spend too much time on questions worth little marks that are still relatively hard for you.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; If there are &amp;lt;5 minutes left and you are still stuck on some question, STOP. &lt;lb/&gt; Your time is better spent re-reading all questions and making absolutely sure you did not miss any secondary&lt;lb/&gt; questions, and that you answered everything. You wouldn't believe how many silly marks people lose this way.&lt;/p&gt;
    &lt;p&gt;Congratulations if you got all the way here! Now that you are here, here's my last (very important advice). It is something that I wish someone had told me when I was an undergraduate.&lt;/p&gt;
    &lt;p&gt;Undergrads tend to have tunnel vision about their classes. They want to get good grades, etc. The crucial fact to realize is that noone will care about your grades, unless they are bad. For example, I always used to say that the smartest student will get 85% in all of his courses. This way, you end up with somewhere around 4.0 score, but you did not over-study, and you did not under-study.&lt;/p&gt;
    &lt;p&gt;Your time is a precious, limited resource. Get to a point where you don't screw up on a test and then switch your attention to much more important endeavors. What are they?&lt;/p&gt;
    &lt;p&gt;Getting actual, real-world experience, working on real code base, projects or problems outside of silly course exercises is extremely imporant. Professors/People who know you and can write you a good reference letter saying that you have initiative, passion and drive are extremely important. Are you thinking of applying to jobs? Get a summer internship. Are you thinking of pursuing graduate school? Get research experience! Sign up for whatever programs your school offers. Or reach out to a professor/graduate student asking to get involved on a research project you like. This might work if they think you're driven and motivated enough. Do not underestimate the importance of this: A well-known professor who writes in their recommendation letter that you are driven, motivated and independent thinker completely dwarfs anything else, especially petty things like grades. It also helps a lot if you squeeze in at least one paper before you apply. Also, you should be aware that the biggest pet peeve from their side are over-excited undergrad students who sign up for a project, meet a few times, ask many questions, and then suddenly give up and disappear after all that time investment from the graduate student's or professor's side. Do not be this person (it damages your reputation) and do not give any indication that you might be.&lt;/p&gt;
    &lt;p&gt;Other than research projects, get involved with some group of people on side projects or better, start your own from scratch. Contribute to Open Source, make/improve a library. Get out there and create (or help create) something cool. Document it well. Blog about it. These are the things people will care about a few years down the road. Your grades? They are an annoyance you have to deal with along the way. Use your time well and good luck.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cs.stanford.edu/people/karpathy/advice.html"/><published>2025-10-19T16:31:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45636285</id><title>Airliner hit by possible space debris</title><updated>2025-10-20T11:09:38.315776+00:00</updated><content>&lt;doc fingerprint="7fe8ca7d64ca8d2a"&gt;
  &lt;main&gt;
    &lt;p&gt;The NTSB is now said to be focusing its investigation on the possibility that a piece of weather balloon’s data package injured a pilot and caused damage to the windshield and frame on a United 737 MAX over Colorado on Thursday. Various reports that include watermarked photos of the damage suggest the plane was struck by a falling object not long after taking off from Denver for Los Angeles. One of the photos shows a pilot’s arm peppered with small cuts and scratches. Sources told AVweb Sunday that the focus of the investigation is on a weather balloon payload. Earlier reports suggested it could have been something from space but that seems unlikely since the velocity of anything that survived reentry would likely have caused substantial damage beyond a cracked windshield. The theory was likely amplified by the captain of the flight who reportedly described the object that hit the plane as “space debris.”&lt;/p&gt;
    &lt;p&gt;Whatever hit the plane, it was an enormously rare event and possibly the first time anything has collided with an aircraft at that altitude other than a projectile launched with that intended purpose. The plane diverted without incident to Salt Lake City where the approximately 130 passengers were put on another plane to finish the last half of the 90-minute flight. Apparently only one layer of the windshield was damaged, and there was no depressurization. The crew descended from 36,000 feet to 26,000 feet for the diversion, likely to ease the pressure differential on the remaining layers of windshield. Neither the airline nor FAA have commented.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://avbrief.com/united-max-hit-by-falling-object-at-36000-feet/"/><published>2025-10-19T17:54:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45636365</id><title>Compare Single Board Computers</title><updated>2025-10-20T11:09:38.048350+00:00</updated><content>&lt;doc fingerprint="468f7cd727be4f3a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Compare Single Board Computers&lt;/head&gt;
    &lt;p&gt;Find the perfect SBC for your project with comprehensive benchmarks, specifications, and real-world performance data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Search SBCs to Compare&lt;/head&gt;
    &lt;head rend="h2"&gt;Popular Comparisons&lt;/head&gt;
    &lt;head rend="h2"&gt;Quick Start Guide&lt;/head&gt;
    &lt;head rend="h3"&gt;Search&lt;/head&gt;
    &lt;p&gt;Search for single board computers by name, manufacturer, or specifications.&lt;/p&gt;
    &lt;head rend="h3"&gt;Select&lt;/head&gt;
    &lt;p&gt;Add up to 3 boards to your comparison list by clicking on them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compare&lt;/head&gt;
    &lt;p&gt;View detailed comparisons with benchmarks, specifications, and performance data.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sbc.compare/"/><published>2025-10-19T18:02:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45637133</id><title>Dosbian: Boot to DOSBox on Raspberry Pi</title><updated>2025-10-20T11:09:37.888725+00:00</updated><content>&lt;doc fingerprint="510529795c96c4c7"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;09/01/2025 released DOSBIAN 3.0 for Raspberry Pi 3/4/400/5/500&lt;/head&gt;
    &lt;p&gt;WHAT’S NEW IN VERSION 3.0&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Latest distro updates applied to run in Raspberry Pi 5/500.&lt;/item&gt;
      &lt;item&gt;Dosbox Staging updated to version 0.82, now with support for MMX instructions (Please see official sites for all the changements).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Incredible performances expecially with Raspberry Pi 5/500, Dosbian V3.0 guarantees you an incredible DOS experience.&lt;/p&gt;
    &lt;p&gt;Rewritten from scratch starting from the new Bookworm OS for Raspberry Pi, Dosbian is the first distro totally dedicated to the DOS world. It boots straight to Dosbox, from there, you can install whatever you want and building your retro-pc 🙂&lt;lb/&gt;Whether you love DOS games or you’re just fond of all the DOS retro software, this is the distro for you.&lt;lb/&gt;Just switch on your Raspberry Pi and in few seconds your Dos prompt will be ready to use. No configuration needed, just an old school command like based machine to enjoy!&lt;/p&gt;
    &lt;p&gt;What you can do with your Dosbian distro:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run all retro Pc Sofware (DOS / Win 3.1 / Win 95 / Win98)&lt;/item&gt;
      &lt;item&gt;Run most of 90’s retro games&lt;/item&gt;
      &lt;item&gt;Run games from LaunchBox frontend&lt;/item&gt;
      &lt;item&gt;Run ScummVM Games&lt;/item&gt;
      &lt;item&gt;Create empty floppy of size: 320KB, 720KB, 1,44MB&lt;/item&gt;
      &lt;item&gt;Create empty HDDs of size: 256MB, 512MB, 1GB, 2GB&lt;/item&gt;
      &lt;item&gt;Mount Floppy disk, CD-ROM or HDD using a GUI driven utility&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;PLEASE NOTE&lt;/head&gt;
    &lt;head rend="h3"&gt;Dosbian doesn’t contains any copyrighted material.&lt;lb/&gt;It’s up to you to install games/software or the operating system. &lt;lb/&gt;I knew someone on the web is selling my distro with OS pre-installed (that’s illegal). I’m not involved in to this, so please, if you want a genuine free Dosbian image, download the distro only from my blog.&lt;/head&gt;
    &lt;head rend="h3"&gt;The images below are just examples on what you can run on Dosbian, but nothing is included inside the distribution.&lt;/head&gt;
    &lt;p&gt;Example games running on Dosbian&lt;/p&gt;
    &lt;p&gt;Some software Dosbian is able to run&lt;/p&gt;
    &lt;head rend="h2"&gt;Terms of use and distribution&lt;/head&gt;
    &lt;p&gt;Dosbian is a donationware project, this means you can modify, improve, customise it as you like for your own use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dosbian Facebook group&lt;/head&gt;
    &lt;p&gt;Join the official Facebook group, a place where you can meet other friends and discuss about games, configurations, issues, etc.&lt;/p&gt;
    &lt;head rend="h2"&gt;Download&lt;/head&gt;
    &lt;p&gt;Please note: The distro doesn’t contain any copyrighted material.&lt;lb/&gt;Dosbian is compatible with the following Raspberry Pi models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raspberry Pi 3B&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 3B+&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 3A+&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 4B&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 400&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 5&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 500&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Do you like the project? Please consider to make a free donation using the button below&lt;/p&gt;
    &lt;head rend="h2"&gt;For Raspberry Pi 3B/3B+/4B/400/5/500&lt;lb/&gt;Download Dosbian 3.0&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;Note: Unzip the image with 7zip and use Win32DiskImager or Balena Etcher to flash it.&lt;/p&gt;
    &lt;p&gt;Did you like Dosbian? &lt;lb/&gt;Try Combian64, a dedicated distro that boots straight in to one of the old glory Commodore machines (64,128, Vic 20, PET, ecc).&lt;/p&gt;
    &lt;head rend="h2"&gt;Where to start from?&lt;/head&gt;
    &lt;p&gt;Here you can find some useful guide, link and tutorial:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dosbian a “Quick start guide” by Gary Marsh&lt;/item&gt;
      &lt;item&gt;The Definitive Guide on installing Windows 95 on Raspberry Pi 3B/4B by Daniel Řepka&lt;/item&gt;
      &lt;item&gt;Guide and drivers: Installing Windows 95 on Raspberry Pi 3B/4B by Daniel Řepka&lt;/item&gt;
      &lt;item&gt;How to install Windows 98 on Raspberry Pi 4B by Daniel Řepka&lt;/item&gt;
      &lt;item&gt;mTCP – TCP/IP applications for DOS&lt;/item&gt;
      &lt;item&gt;List of games running smoothly on Dosbian 1.5 Rpi4 by Daniel Řepka&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Do you have anything to tell? Write a comment 🙂&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cmaiolino.wordpress.com/dosbian/"/><published>2025-10-19T19:26:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45637744</id><title>Novo Nordisk's Canadian Mistake</title><updated>2025-10-20T11:09:37.370080+00:00</updated><content/><link href="https://www.science.org/content/blog-post/novo-nordisk-s-canadian-mistake"/><published>2025-10-19T20:39:13+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45637880</id><title>Duke Nukem: Zero Hour N64 ROM Reverse-Engineering Project Hits 100%</title><updated>2025-10-20T11:09:36.750277+00:00</updated><content>&lt;doc fingerprint="718cb35915656d91"&gt;
  &lt;main&gt;
    &lt;p&gt;A decompilation of Duke Nukem Zero Hour for N64.&lt;/p&gt;
    &lt;p&gt;Note: To use this repository, you must already own a copy of the game.&lt;/p&gt;
    &lt;p&gt;The build instructions assume that you will be using Ubuntu 20.04; either natively or via WSL2.&lt;/p&gt;
    &lt;p&gt;Package requirements can be install via:&lt;/p&gt;
    &lt;code&gt;sudo apt update
sudo apt install make git build-essential binutils-mips-linux-gnu cpp-mips-linux-gnu python3 python3-pip&lt;/code&gt;
    &lt;p&gt;Tools requires Python 3.8+. Package requirements can be installed via:&lt;/p&gt;
    &lt;code&gt;pip3 install --upgrade pip
pip3 install -U splat64[mips]
pip3 install -r requirements.txt&lt;/code&gt;
    &lt;p&gt;Clone the repository; note the --recursive flag to fetch submodules at the same time:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/Gillou68310/DukeNukemZeroHour.git --recursive&lt;/code&gt;
    &lt;p&gt;Navigate into the freshly cloned repo&lt;/p&gt;
    &lt;code&gt;cd DukeNukemZeroHour&lt;/code&gt;
    &lt;p&gt;Place the Duke Nukem Zero Hour US ROM in the root of this repository, name it baserom.us.z64, and then run the first make command to extract the ROM:&lt;/p&gt;
    &lt;code&gt;make setup&lt;/code&gt;
    &lt;p&gt;Now build the ROM:&lt;/p&gt;
    &lt;code&gt;make --jobs&lt;/code&gt;
    &lt;p&gt;If you did everything correctly, you'll be greeted with the following:&lt;/p&gt;
    &lt;code&gt;Creating z64: build/us/dukenukemzerohour.z64
OK&lt;/code&gt;
    &lt;p&gt;This repository has support for the French versions of the game too.&lt;/p&gt;
    &lt;p&gt;To build this version, place your ROM in the root of the repo and rename it to baserom.fr.z64. Pass VERSION=fr to the above make commands.&lt;/p&gt;
    &lt;p&gt;Functions can be decompiled to a state where they are functionally equivalent, but are not a byte-perfect match. In order to build/test the non-matching, add NON_MATCHING=1 argument to the make commands.&lt;/p&gt;
    &lt;p&gt;A Docker image containing all dependencies can be built and ran as follows:&lt;/p&gt;
    &lt;code&gt;docker build --no-cache . -t dukenukemzerohour
docker run --rm -ti --mount src=$(pwd),target=/dukenukemzerohour,type=bind dukenukemzerohour&lt;/code&gt;
    &lt;p&gt;Then continue with the building instructions&lt;/p&gt;
    &lt;p&gt;When binding windows or mac folder I strongly recommand installing Mutagen Extension for Docker Desktop.&lt;/p&gt;
    &lt;code&gt;docker --context=desktop-linux-mutagen run --rm -ti --mount src=$(pwd),target=/dukenukemzerohour,type=bind dukenukemzerohour&lt;/code&gt;
    &lt;p&gt;Game can be debugged with gdb through mupen64plus (Windows only for now). In order to have source code information the game should be compiled with modern gcc by adding MODERN=1 to the make command.&lt;/p&gt;
    &lt;p&gt;Run the gdb server in cmd:&lt;/p&gt;
    &lt;code&gt;tools\debugger\win32\gdbserver.bat&lt;/code&gt;
    &lt;p&gt;Then run the gdb client in cmd:&lt;/p&gt;
    &lt;code&gt;tools\debugger\win32\gdbclient.bat&lt;/code&gt;
    &lt;p&gt;It's also possible to debug within vscode with the Native Debug extension.&lt;/p&gt;
    &lt;p&gt;Run the gdb server in cmd:&lt;/p&gt;
    &lt;code&gt;tools\debugger\win32\gdbserver.bat&lt;/code&gt;
    &lt;p&gt;Then run the "GDB Client(Win32)" configuration in vscode.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;asm-differ; rapidly diff between source/target assembly&lt;/item&gt;
      &lt;item&gt;decomp-permuter; tweaks code, rebuilds, scores; helpful for weird regalloc issues&lt;/item&gt;
      &lt;item&gt;mips2c; assembly to C code translator&lt;/item&gt;
      &lt;item&gt;splat; successor to n64split&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Gillou68310/DukeNukemZeroHour"/><published>2025-10-19T20:54:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45638588</id><title>Gleam OTP – Fault Tolerant Multicore Programs with Actors</title><updated>2025-10-20T11:09:36.061512+00:00</updated><content>&lt;doc fingerprint="a4188e332289c8e4"&gt;
  &lt;main&gt;
    &lt;p&gt;Fault tolerant multi-core programs with OTP, the BEAM actor framework.&lt;/p&gt;
    &lt;code&gt;gleam add gleam_otp@1&lt;/code&gt;
    &lt;code&gt;import gleam/erlang/process.{type Subject}
import gleam/otp/actor

pub fn main() {
  // Start an actor
  let assert Ok(actor) =
    actor.new(0)
    |&amp;gt; actor.on_message(handle_message)
    |&amp;gt; actor.start

  // Send some messages to the actor
  actor.send(actor.data, Add(5))
  actor.send(actor.data, Add(3))

  // Send a message and get a reply
  assert actor.call(actor.data, waiting: 10, sending: Get) == 8
}

pub fn handle_message(state: Int, message: Message) -&amp;gt; actor.Next(Int, Message) {
  case message {
    Add(i) -&amp;gt; {
      let state = state + i
      actor.continue(state)
    }
    Get(reply) -&amp;gt; {
      actor.send(reply, state)
      actor.continue(state)
    }
  }
}

pub type Message {
  Add(Int)
  Get(Subject(Int))
}&lt;/code&gt;
    &lt;p&gt;Gleam’s actor system is built with a few primary goals:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full type safety of actors and messages.&lt;/item&gt;
      &lt;item&gt;Be compatible with Erlang’s OTP actor framework.&lt;/item&gt;
      &lt;item&gt;Provide fault tolerance and self-healing through supervisors.&lt;/item&gt;
      &lt;item&gt;Have equivalent performance to Erlang’s OTP.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This library documents its abstractions and functionality, but you may also wish to read the documentation or other material on Erlang’s OTP framework to get a fuller understanding of OTP, the problems it solves, and the motivations for its design.&lt;/p&gt;
    &lt;p&gt;Not all Erlang/OTP functionality is included in this library. Some is not possible to represent in a type safe way, so it is not included. Other features are still in development, such as further process supervision strategies.&lt;/p&gt;
    &lt;p&gt;This library provides several different types of actor that can be used in Gleam programs.&lt;/p&gt;
    &lt;p&gt;The process is the lowest level building block of OTP, all other actors are built on top of processes either directly or indirectly. Typically this abstraction would not be used very often in Gleam applications, favour other actor types that provide more functionality.&lt;/p&gt;
    &lt;p&gt;Gleam's process module is defined in the &lt;code&gt;gleam_erlang&lt;/code&gt; library.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;actor&lt;/code&gt; is the most commonly used process type in Gleam and serves as a good
building block for other abstractions. Like Erlang's &lt;code&gt;gen_server&lt;/code&gt; it handles
OTP's system messages automatically to enable OTP's debugging and tracing
functionality.&lt;/p&gt;
    &lt;p&gt;Supervisors are processes that start and then supervise other processes. They can restart them if they crash, and terminate them when the application is shutting down.&lt;/p&gt;
    &lt;p&gt;Supervisors can start other supervisors, resulting in a hierarchical process structure called a supervision tree, providing fault tolerance and monitoring benefits to a Gleam application.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;gleam/otp/static_supervisor documentation.&lt;/item&gt;
      &lt;item&gt;gleam/otp/factory_supervisor documentation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Actors do not yet support all OTP system messages, so some of the OTP debugging APIs may not be fully functional. These unsupported messages are discarded by actors.&lt;/p&gt;
    &lt;p&gt;If find that you have a need for one of the unimplemented system messages, open an issue and we will implement support for it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/gleam-lang/otp"/><published>2025-10-19T22:25:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45639157</id><title>From Hollywood to horticulture: Cate Blanchett on a mission to save seeds</title><updated>2025-10-20T11:09:35.140170+00:00</updated><content>&lt;doc fingerprint="febb13772b924334"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Hollywood to horticulture: Cate Blanchett on a mission to save seeds&lt;/head&gt;
    &lt;p&gt;She's a Hollywood A-lister, with a mantelpiece groaning under the weight of awards. But Cate Blanchett has taken an unexpected diversion from her day job - to immerse herself in the world of the humble seed.&lt;/p&gt;
    &lt;p&gt;Her eyes light up as she enthuses about the banksia species from her native Australia.&lt;/p&gt;
    &lt;p&gt;"It's quite a brutal looking seed pod that only releases its seed in extremely high temperatures," she tells us.&lt;/p&gt;
    &lt;p&gt;"It does look like a cross between a mallet and a toilet brush. So they're not always pretty, but yet what comes out of them is so spectacular."&lt;/p&gt;
    &lt;p&gt;We meet her at Kew's Millennium Seed Bank (MSB) at Wakehurst botanic garden in Sussex. She lives locally and teamed up with the project as it celebrates its 25th anniversary.&lt;/p&gt;
    &lt;p&gt;"Really, I stumbled upon Wakehurst. I was just in awe of the landscape and I always feel regenerated by being in the natural world," she says.&lt;/p&gt;
    &lt;p&gt;"And then I discovered the seed bank, and I literally had my mind blown by the work that goes on here… and I thought, anything I can do to be connected to it - I found it so inspiring."&lt;/p&gt;
    &lt;p&gt;The MSB is home to more than 2.5 billion seeds collected from 40,000 wild plant species around the world.&lt;/p&gt;
    &lt;p&gt;The seeds, which come in every shape, size and colour, are carefully processed, dried and then stored in freezers at a chilly -20C.&lt;/p&gt;
    &lt;p&gt;The conservation project was opened by The King - then the Prince of Wales - in 2000. He's taken part in a special episode of a Kew podcast about the project called Unearthed: The Need For Seeds with Cate Blanchett.&lt;/p&gt;
    &lt;p&gt;In the recording he talks about his concerns that many plant species are being lost.&lt;/p&gt;
    &lt;p&gt;"I know how absolutely critical it all is, and the destruction of rainforests, the extinction of endless species, which have very likely remarkable properties," he tells the podcast.&lt;/p&gt;
    &lt;p&gt;When the seed bank first opened, it was seen as a doomsday vault - a back-up store of seeds to safeguard wild plants from extinction.&lt;/p&gt;
    &lt;p&gt;But 25 years on, the collection is being used for a different purpose: to restore environments that are under threat.&lt;/p&gt;
    &lt;p&gt;"We want those seeds to be back out in the landscape," explained Dr Elinor Breman from the MSB, who's been showing Cate Blanchett the team's work.&lt;/p&gt;
    &lt;p&gt;"We're just providing a safe space for them until we can get them back out into a habitat where they can thrive and survive."&lt;/p&gt;
    &lt;p&gt;This includes projects like one taking place on the South Downs. A special mix of seeds from the MSB are being sown to help restore the rare chalk grasslands there.&lt;/p&gt;
    &lt;p&gt;And this restoration work is being repeated around the world.&lt;/p&gt;
    &lt;p&gt;"We've been to every kind of habitat, from sea level to about 5,000m, and from pole to pole - literally," explained Dr Breman.&lt;/p&gt;
    &lt;p&gt;"And we're involved in restoring tropical forest, dry deciduous forest, grassland, steppe - you name it - we're trying to help people put those plants back in place."&lt;/p&gt;
    &lt;p&gt;The seed bank also helped to restore plants after intense wildfires swept across Australia in 2019. Cate Blanchett says this meant a lot to her.&lt;/p&gt;
    &lt;p&gt;"There are almost 9,000 species of Australian plant that are stored [at the MSB]. And we know that bushfires are getting increasingly more intense. And it's sad to say - but knowing that insurance policy exists, is of great solace to me."&lt;/p&gt;
    &lt;p&gt;Working as an ambassador for Wakehurst has meant that the actor has had a chance to get hands on with the seeds.&lt;/p&gt;
    &lt;p&gt;"Have I got dirt under my fingernails? Well, I'm trying to turn my brown thumbs green," she laughs.&lt;/p&gt;
    &lt;p&gt;"You know, living in Sussex, you can't not but become a passionate gardener. So I've had a lot of questions about how one stores seeds as a lay person, and I've learned a lot about that. My seed management has definitely, definitely improved."&lt;/p&gt;
    &lt;p&gt;And after spending so much time with the researchers at the MSB, is she at all tempted to swap the film set for the lab?&lt;/p&gt;
    &lt;p&gt;"I wish I had the skill - maybe I could play a scientist," she laughs.&lt;/p&gt;
    &lt;p&gt;Cate Blanchett describes the seed bank as the UK's best kept secret - and believes that over the next 25 years its work will continue to grow in importance.&lt;/p&gt;
    &lt;p&gt;"You often think, where are the good news stories? And we're actually sitting inside one," she tells us.&lt;/p&gt;
    &lt;p&gt;"You come here, you visit the seed bank, you walk through such a biodiverse landscape, and you leave uplifted. You know change is possible and it's happening."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/cwy7ekl4yl8o"/><published>2025-10-20T00:19:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45639250</id><title>Forth: The programming language that writes itself</title><updated>2025-10-20T11:09:34.589393+00:00</updated><content>&lt;doc fingerprint="24239a10424beac0"&gt;
  &lt;main&gt;&lt;p&gt; Author: Dave Gauer&lt;lb/&gt; Created: 2023-02-02 &lt;lb/&gt; Updated: 2024-12-22 &lt;/p&gt;&lt;p&gt;Note: This page is my personal journey to discover Forth and put it in the context of computing history. It is adapted from my slides for a short talk. I've done everything in my power to make this page scale up and down for various screen sizes. I welcome suggestions and corrections for both the content and display of this page. Here's my contact page.&lt;/p&gt;&lt;p&gt;When I was a wee programmer, I would sit around the virtual Usenet campfires listening to the tall tales and legends of the elders.&lt;/p&gt;&lt;p&gt;In the 1990s, Usenet newsgroups (wikipedia.org) were where it was at. For example, Linus Torvalds's initial announcement of Linux was to comp.os.minix in 1991.&lt;/p&gt;&lt;p&gt;The comp.* (wikipedia.org) groups and particularly comp.lang.* were great places to learn about and discuss programming. By the time I got there in the late 90s, Perl was a pretty hot topic, especially as it took a dominant role in the early Web as the dynamic page and form processing programming language via CGI (wikipedia.org).&lt;/p&gt;&lt;p&gt;There were programming resources on the Web, but nothing like what's available now! To actually learn to program, I bought books, and still do.&lt;/p&gt;&lt;p&gt;Usenet was where the community and folklore lived.&lt;/p&gt;&lt;p&gt;(The "Easter egg" in this drawing is alt.religion.kibology, which should get a chuckle from old timers. The rest of you can look it up.)&lt;/p&gt;&lt;p&gt;I learned about magical languages with lots of (((((parenthesis))))).&lt;/p&gt;&lt;p&gt;Sharp-eyed Lisp-lovers and other mutants will perhaps recognize this thing as the Y combinator expressed with lambdas.&lt;/p&gt;&lt;p&gt;The only time I understood this was when I completed the book The Little Schemer by Friedman and Felliesen, which walks you through creating it for yourself. It is a magical book and I implore you to try it.&lt;/p&gt;&lt;p&gt;I listened, wide-eyed, to true tech tales like The Story of Mel (foldoc.org).&lt;/p&gt;&lt;p&gt;Mel was real and the Royal McBee RPC-4000 was real. Look at that teletype (aka "teleprinter"). If typewriters and "Royal" together make a little bell in your head go "bing" as your mental carriage hits the end of the page, then you're right: Royal McBee was a merger between the Royal Typewriter Company (wikipedia.org) and McBee, a manufacturer of accounting machines.&lt;/p&gt;&lt;p&gt;For a while, Royal was owned by the Italian typewriter company, Olivetti, who also made some really interesting computers (wikipedia.org).&lt;/p&gt;&lt;p&gt;And then...&lt;/p&gt;&lt;p&gt;I heard tell of a programming language so flexible that you could change the values of integers.&lt;/p&gt;&lt;p&gt;They said that language was called Forth and it was created by a mad wizard called Chuck Moore who could write any program in a couple screens of code.&lt;/p&gt;&lt;p&gt;Years went by and I wrote a lot of PHP and JavaScript. I watched the Web evolve (and sometimes de-evolve).&lt;/p&gt;&lt;p&gt;But I never forgot about the legend of Forth.&lt;/p&gt;&lt;p&gt;The blog series "programming in the twenty-first century" (prog21.dadgum.com) by game developer James Hague gave me the final push.&lt;/p&gt;&lt;p&gt;He made Forth a recurring theme and it just sounded so darned interesting.&lt;/p&gt;&lt;p&gt;So I went on an adventure and now that I have returned, I think I have some answers.&lt;/p&gt;&lt;p&gt;(Oh, and I confirmed the legend. I can make any integer equal anything I want. Stick around 'til the end to see that Forth magic trick.)&lt;/p&gt;&lt;p&gt;At first, I thought this was what Forth was all about:&lt;/p&gt;&lt;quote&gt;3 4 + 7&lt;/quote&gt;&lt;p&gt;Now begins my quest to understand Forth.&lt;/p&gt;&lt;p&gt;Perhaps you've seen postfix or Reverse Polish Notation (RPN) (wikipedia.org) before? The principle is simple: Instead of the usual "infix" notation which puts operators between operands (&lt;code&gt;3 + 4&lt;/code&gt;), RPN puts
        operators after the operands (&lt;code&gt; 3 4 +&lt;/code&gt;).
        &lt;/p&gt;&lt;p&gt;RPN notation is one of the most visually obvious aspects of the Forth programming language. But it turns out, RPN is not what Forth is about or the reason Forth exists. As we'll see, the situation is reversed.&lt;/p&gt;&lt;p&gt;In fact, as you'll see, my quest is mostly a series of incorrect assumptions I made by looking at the language without the context of history.&lt;/p&gt;&lt;p&gt;By the way, the HP-35 calculator (wikipedia.org) pictured here is really interesting. In the early 1970s, HP had powerful desktop calculators. Actually, what they had were really programmable computers, but they still called them calculators (wikipedia.org) for sales reasons. But these were big desktop machines that ran off of wall current.&lt;/p&gt;&lt;p&gt;Putting all of that power into a "shirt pocket" calculator was an astounding accomplishment at the time. Legend has it that the size of the HP-35 was based on the dimensions of Bill Hewlett's actual shirt pocket. HP-35 calculators have been in space. They killed off the slide rule.&lt;/p&gt;&lt;p&gt;HP calculators are famous for using RPN syntax. If it weren't for these calculators, I suspect it's likely that RPN syntax would be virtually unknown outside of computer science.&lt;/p&gt;&lt;p&gt;RPN is considered to be highly efficient and, being somewhat inscrutable to outsiders, highly geeky.&lt;/p&gt;&lt;p&gt;Let's see a better example...&lt;/p&gt;&lt;p&gt;Noob:&lt;/p&gt;&lt;quote&gt;$ bc bc 1.07.1 Copyright 1991-1994, 1997, 1998, 2000, 2004, 2006, 2008, 2012-2017 Free Software Foundation, Inc. This is free software with ABSOLUTELY NO WARRANTY. For details type `warranty'. (3 * 4) + (5 * 6) 42&lt;/quote&gt;&lt;p&gt;Pro:&lt;/p&gt;&lt;quote&gt;$ dc 3 4 * 5 6 * + p 42&lt;/quote&gt;&lt;p&gt;I'm being cheeky here. Users of &lt;code&gt;bc&lt;/code&gt;, are hardly
            noobs.  But it is arguably even geekier to use the much
            older &lt;code&gt;dc&lt;/code&gt; program.  &lt;code&gt;bc&lt;/code&gt; was once just an
            infix expression translator for &lt;code&gt;dc&lt;/code&gt; to make it more
            palatable for people who didn't want to use RPN. Thus the gentle
            teasing.
        &lt;/p&gt;&lt;p&gt;Besides using RPN syntax, the dc calculator (wikipedia.org) is completely programmable. Oh and it also happens to be one of the very first Unix programs and pre-dates the C programming language!&lt;/p&gt;&lt;p&gt;Anyway, the point here is that RPN syntax lets you express nested expressions without requiring parenthesis to get the order of operations the way you want them. This is one of the reasons RPN fans (including those HP calculator fans I alluded to) are so enamoured with it.&lt;/p&gt;&lt;p&gt;In this example, we input 3, then 4. &lt;code&gt;*&lt;/code&gt; multiplies them.
        Now we have the result (12) available. But first, we input 5 and 6 and
        multiply them with another &lt;code&gt;*&lt;/code&gt; to also store that result (30).
        The final &lt;code&gt;+&lt;/code&gt; adds both stored results (12 + 30) and
        stores that result (42).
        Unlike an HP calculator, &lt;code&gt;dc&lt;/code&gt; doesn't show us any of the
        stored results, including the last one until we "print" it with the
        &lt;code&gt;p&lt;/code&gt; command.
        &lt;/p&gt;&lt;p&gt;As it is known about "ed, the standard text editor" (gnu.org), &lt;code&gt;dc&lt;/code&gt; doesn't
        waste your VALUABLE time (or teletype paper) with output you don't need!
        &lt;/p&gt;&lt;p&gt;So this relates to Forth how?&lt;/p&gt;&lt;p&gt;Forth pro:&lt;/p&gt;&lt;quote&gt;3 4 * 5 6 * + . 42&lt;/quote&gt;&lt;p&gt;As you can see, someone sitting at a Forth interpreter can perform this calculation exactly the same as with the &lt;code&gt;dc&lt;/code&gt;
        calculator (or an HP calculator).
        &lt;/p&gt;&lt;p&gt;Sharp-eyed readers will note that we print the result with a "." command rather than "p". But that's the only difference.&lt;/p&gt;&lt;p&gt;So Forth is like an RPN calculator? We input values and then operate on them? Well, that statement is not wrong&lt;/p&gt;&lt;p&gt;But does that mean we know what Forth is all about now? If we know how to enter things in postfix notation, we "get" Forth? No! Not even close...&lt;/p&gt;&lt;p&gt;Forth absolutely uses postfix notation.&lt;/p&gt;&lt;p&gt;But then I learned some more:&lt;/p&gt;&lt;p&gt;The use of a data stack is probably the second most visible thing about the Forth programming language.&lt;/p&gt;&lt;p&gt;A stack is a data structure often explained with a "stack of plates" analogy. You PUSH a plate on the stack and you POP a plate off the stack. The first item you put on the stack is the last item out of the stack.&lt;/p&gt;&lt;p&gt;Above, we have an illustration of PUSH and two other common stack operations:&lt;/p&gt;&lt;p&gt;As you may have guessed, these four stack words (PUSH, POP, SWAP, DUP) also happen to be Forth words.&lt;/p&gt;&lt;p&gt;Historical note 1: In the old days, people and computers just WENT ABOUT SHOUTING AT EACH OTHER ALL THE TIME IN ALL CAPS BECAUSE LOWERCASE LETTERS WERE TOO EXPENSIVE.&lt;/p&gt;&lt;p&gt;Historical note 2: When a computer asks, "SHALL WE PLAY A GAME?" in all caps, you must answer NO, as we learned in 1983's WarGames (wikipedia.org)&lt;/p&gt;&lt;p&gt;Let's see a stack in action:&lt;/p&gt;&lt;quote&gt;Op The Stack -- --------- 3 3 4 3 4 * 12 5 12 5 6 12 5 6 * 12 30 + 42 .&lt;/quote&gt;&lt;p&gt;Let's revisit our math problem from earlier. This is the Forth code on the left and the results on "the stack" on the right.&lt;/p&gt;&lt;p&gt;Rather than being concerned with the syntax or notation, we're now interested in what these operations are doing with our data stack.&lt;/p&gt;&lt;p&gt;As you can see, entering a number puts it on the stack. The math operators take two values from the stack, do something with them, and put a new value back on the stack.&lt;/p&gt;&lt;p&gt;The '.' (DOT) operator is different since it only takes one value (to print it) and does not put anything back on the stack. As far as the stack is concerned, it is equivalent to DROP. As far as humans are concerned, it has the useful side-effect of letting us see the number.&lt;/p&gt;&lt;p&gt;Now let's see something you probably wouldn't find on an HP calculator. Something non-numerical...&lt;/p&gt;&lt;p&gt;This is valid Forth, assuming CAKE, HAVE, and EAT have been defined:&lt;/p&gt;&lt;quote&gt;CAKE DUP HAVE EAT&lt;/quote&gt;&lt;p&gt;Getting the joke here will require knowing this English idiom (wikipedia.org).&lt;/p&gt;&lt;p&gt;Actually, this isn't just a silly example. Forth's use of the stack can lead to a natural, if somewhat backward use of nouns and verbs. (Kind of like Yoda's speech habits. "Cake you will dup, yes? Have it and eat it you will, hmmm?")&lt;/p&gt;&lt;p&gt;There can, indeed, be some object named CAKE that we have placed on the stack (probably a memory reference) which can be DUPed, and then HAVEd and EATen.&lt;/p&gt;&lt;p&gt;It's up to the Forth developer to make harmonious word choices. It can get far more clever or poetic than my example.&lt;/p&gt;&lt;p&gt;Naming things is great.&lt;/p&gt;&lt;p&gt;But sometimes not naming things is even better.&lt;/p&gt;&lt;p&gt;The stack frees us from being forced to create explicit names for intermediate values.&lt;/p&gt;&lt;p&gt;If I ask you to add these numbers:&lt;/p&gt;&lt;quote&gt;2 6 1 3 7&lt;/quote&gt;&lt;p&gt;Do you feel a need to give a name to each sum pair...or even the running total?&lt;/p&gt;&lt;p&gt;(Hopefully your answer is "no" or the rhetorical question doesn't work.)&lt;/p&gt;&lt;p&gt;But it's funny how our programming languages often require us to explicitly name intermediate results so that we can refer to them. On paper, we would never give these values names - we would just happily start working on the list.&lt;/p&gt;&lt;p&gt;Imagine, if you will, a factory assembly line in which each person working the line is a hateful fussbudget who refuses to work on the part in front of them until you name it. And each time the part has been worked on it must be given a new name. Furthermore, they refuse to let you re-use a name you've already used.&lt;/p&gt;&lt;p&gt;A lot of imperative languages are like that factory. As your values go down the line, you've got to come up with nonsense names like &lt;code&gt;result2&lt;/code&gt;, or &lt;code&gt;matched_part3&lt;/code&gt;.
        &lt;/p&gt;&lt;p&gt;Does your programming language make you do this?&lt;/p&gt;&lt;p&gt;(It's almost as bad as file names used as a versioning system: &lt;code&gt;my_doc_new_v5.4(copy)-final2&lt;/code&gt;...)
        &lt;/p&gt;&lt;p&gt;Working without names (also known as implicit or tacit or point-free programming) is sometimes a more natural and less irritating way to compute. Getting rid of names can also lead to much more concise code. And less code is good code.&lt;/p&gt;&lt;p&gt;Great, so stacks can be a very elegant way to handle expressions.&lt;/p&gt;&lt;p&gt;Have we "cracked" Forth yet? Now we know two things: it uses RPN syntax and it is stack-based.&lt;/p&gt;&lt;p&gt;Well, Forth certainly does use a stack. It is definitely a stack-based language.&lt;/p&gt;&lt;p&gt;But then I learned some more...&lt;/p&gt;&lt;p&gt;Ah, this must be it because it sounds fancy.&lt;/p&gt;&lt;p&gt;On this journey of Forth discovery, you'll inevitably run into the term "concatenative programming".&lt;/p&gt;&lt;p&gt;What's that?&lt;/p&gt;&lt;p&gt;An awesome resource for all things concatenative is The Concatenative Language Wiki (concatenative.org). It lists many concatenative languages and has a page about Forth, of course.&lt;/p&gt;&lt;p&gt;For the term "concatenative programming" itself, the Factor programming language website has an excellent page defining the term: Factor documentation: Concatenative Languages (factorcode.org). And, of course, there's the Wikipedia entry, Concatenative programming language (wikipedia.org).&lt;/p&gt;&lt;p&gt;I understand the explanations on these websites now, but it took me a while to get there. Your journey may be shorter or longer. Probably shorter.&lt;/p&gt;&lt;p&gt;Let's see if I can stumble through it...&lt;/p&gt;&lt;p&gt;Contrast with applicative language:&lt;/p&gt;&lt;quote&gt;eat(bake(prove(mix(ingredients))))&lt;/quote&gt;&lt;p&gt;Concatenative language:&lt;/p&gt;&lt;quote&gt;ingredients mix prove bake eat&lt;/quote&gt;&lt;p&gt;An applicative language has you apply a function to a value, which returns another value. Using familiar Algol-like (or "C-like", or "Java-like", or "JavaScript-like") syntax, arguments are passed to functions within a pair of parenthesis. In the above example, the parenthesis end up deeply nested as we pass the output of one function to another.&lt;/p&gt;&lt;p&gt;Unlike the math examples, where the infix notation looks more natural to most of us than the postfix notation, the concatenative example of this baking program looks more natural (at least in a human language sense) than the inside-out function application example, right?&lt;/p&gt;&lt;p&gt;(Of course, if you're a programmer used to years of something like C or Java or JavaScript, the inside-out parenthetical form will probably seem pretty natural too. Well, guess what? Your mind has been warped. It's okay, mine has too.)&lt;/p&gt;&lt;p&gt;The point here is that concatenative style has us "composing" functions (which you can think of as verbs) simply by putting them in sequence. Each function will be called in that sequence. The values that are produced at each step are passed along to be consumed as needed.&lt;/p&gt;&lt;p&gt;No names (unless we want them), just nouns and verbs.&lt;/p&gt;&lt;p&gt;But that's just the surface. It turns out this "concatenative language" concept goes way past that...&lt;/p&gt;&lt;p&gt;The canonical example of a concatenative language is Joy.&lt;/p&gt;&lt;p&gt;Manfred von Thun inspired by Backus's 1977 ACM Turing Award lecture:&lt;/p&gt;&lt;p&gt;Can Programming Be Liberated from the von Neumann Style? (PDF) (worrydream.com) This paper is dense with notation and I haven't personally attempted to wade through it, yet. I'm sure it contains many profound ideas.&lt;/p&gt;&lt;p&gt;I know just enough to believe I understand this paragraph from the paper's abstract:&lt;/p&gt;&lt;quote&gt;"An alternative functional style of programming is founded on the use of combining forms for creating programs. Functional programs deal with structured data, are often nonrepetitive and nonrecursive, are hierarchically constructed, do not name their arguments, and do not require the complex machinery of procedure declarations to become generally applicable. Combining forms can use high level programs to build still higher level ones in a style not possible in conventional languages."&lt;/quote&gt;&lt;p&gt;Perhaps you've heard of "functional programming?" As you can see, that term was being used in 1977.&lt;/p&gt;&lt;p&gt;"Concatenative programming" came after. In fact, Joy is where the "concatenative" description comes from! (von Thun specifically credits Billy Tanksley for creating the term "concatenative notation".)&lt;/p&gt;&lt;p&gt;Joy is kind of like starting with a Lisp&lt;/p&gt;&lt;p&gt;...without variables&lt;/p&gt;&lt;p&gt;...and without traditional control structures&lt;/p&gt;&lt;p&gt;...and all functions are unary (or an "arity of 1").&lt;/p&gt;&lt;p&gt;Specifically, all functions take one stack as input and return one stack as output. The stack is not named, it is implied.&lt;/p&gt;&lt;p&gt;A program is simply a list of functions that is read from left to right.&lt;/p&gt;&lt;p&gt;I can't describe Joy's genesis better than the man himself. Here is von Thun in an interview about Joy:&lt;/p&gt;&lt;quote&gt;"Joy then evolved from this in an entirely haphazard way: First I restricted the binary relations to unary functions, and this of course was a dramatic change. Second, to allow the usual arithmetic operations with their two arguments, I needed a place from which the arguments were to come and where the result was to be put - and the obvious place was a stack with a few shuffling combinators, originally the four inspired by Quine. Third, it became obvious that all these combinators could be replaced by unary functions, with only function composition remaining. Finally the very different distinctively Joy combinators emerged, which take one or more quoted programs from the stack and execute them in a specific way. Along the way of course, lists had already been seen as just special cases of quoted programs. This meant that programs could be constructed using list operations and then passed on to a Joy combinator."&lt;/quote&gt;&lt;p&gt;From A Conversation with Manfred von Thun (nsl.com), which is a really great read in its entirety.&lt;/p&gt;&lt;p&gt;As you can see, combinators are crucial in Joy. Let's take a moment to dive into those, because this is a pretty fascinating avenue of computer science...&lt;/p&gt;&lt;p&gt;Combinators are any "higher-order" functions like map.&lt;/p&gt;&lt;p&gt;"Higher-order" just means functions that take other functions as input and do things with them.&lt;/p&gt;&lt;p&gt;You can even have functions that take functions that take functions and so on to do powerful things. But you'll need to meditate on them every time you have to re-read that part of your code.&lt;/p&gt;&lt;p&gt;map is one of the more common examples, so I'll use it as an example.&lt;/p&gt;&lt;p&gt;JavaScript:&lt;/p&gt;&lt;quote&gt;inc = function(n){ return n + 1; }; bigger = [1, 2, 3, 4].map(inc); Result: [2,3,4,5]&lt;/quote&gt;&lt;p&gt;JavaScript using an "arrow function":&lt;/p&gt;&lt;quote&gt;bigger = [1, 2, 3, 4].map(n =&amp;gt; n + 1); Result: [2,3,4,5]&lt;/quote&gt;&lt;p&gt;(The second example with the arrow function syntax works exactly the same way, but more compactly. I included it to make the comparison with Joy a little more even-handed. Feel free to pick a favorite and ignore the other one.)&lt;/p&gt;&lt;p&gt;In the first example, we have familiar Algol-like syntax with functions that take arguments in parenthesis.&lt;/p&gt;&lt;p&gt;Perhaps &lt;code&gt;map()&lt;/code&gt; is familiar to you. But if not, just know that 
        it takes two parameters like so: &lt;code&gt;map(array, function)&lt;/code&gt;.
        The first parameter is implicit in these JavaScript examples, but it's
        there. The array object, &lt;code&gt;[1, 2, 3, 4]&lt;/code&gt; calls its own
        &lt;code&gt;map()&lt;/code&gt; method. The second parameter is a function
        (named &lt;code&gt;inc&lt;/code&gt; in the first example and left anonymous in
        the second), which will be applied to every member of the list.
        &lt;/p&gt;&lt;p&gt; The output of &lt;code&gt;map()&lt;/code&gt; is a new list containing the
        result of each application.
        &lt;/p&gt;&lt;p&gt;Notice how both JavaScript examples have variables such as the parameter &lt;code&gt;n&lt;/code&gt; and the result
        &lt;code&gt;bigger&lt;/code&gt;. This is an example of what I mentioned a moment
        ago when discussing the advantages of stacks: "Traditional"
        programming languages often make us name values before we can work with
        them.
    &lt;/p&gt;&lt;p&gt;The same thing, but concatenatively in Joy:&lt;/p&gt;&lt;quote&gt;[1 2 3 4] [1 +] map Result: [2 3 4 5]&lt;/quote&gt;&lt;p&gt; The syntax here may require a little explanation. The square brackets (&lt;code&gt;[]&lt;/code&gt;) are Joy's
       quote mechanism. Quotations are a lot like lists, but they can contain
       programs as well as data.
       &lt;/p&gt;&lt;p&gt;In this case, the first quotation is the number list, &lt;code&gt;[1 2 3 4]&lt;/code&gt;.
       &lt;/p&gt;&lt;p&gt;The second quotation is a program, &lt;code&gt;[1 +]&lt;/code&gt;.
       &lt;/p&gt;&lt;p&gt;As in the JavaScript examples, &lt;code&gt;map&lt;/code&gt; takes two parameters.
       The first is the function (or "program" in Joy) to apply, and the second
       is the list to apply it to.
       &lt;/p&gt;&lt;p&gt;(It's kind of confusing to talk about "first" and "second," though because that's the opposite order in which we supply those arguments on the stack...)&lt;/p&gt;&lt;p&gt;Note the lack of variables &lt;code&gt;bigger&lt;/code&gt; or &lt;code&gt;n&lt;/code&gt;.
       Intermediate values just exist.
       &lt;/p&gt;&lt;p&gt;It looks pretty nice and neat, right?&lt;/p&gt;&lt;p&gt;This "point-free" style can be a blessing... or curse. Unlike computers, human brains have a hard time juggling too many things on the stack.&lt;/p&gt;&lt;p&gt;There seems to be a happy medium between named and unnamed. Also, the point-free style seems to benefit greatly from short (even very short) definitions to avoid mental juggling and greater composibility.&lt;/p&gt;&lt;p&gt;If you have the slightest interest in Joy, I highly recommend reading or skimming this delightful tutorial by Manfred von Thun himself: An informal tutorial on Joy (hypercubed.github.io).&lt;/p&gt;&lt;p&gt;Note: I had a bit of a time actually running Joy to test out these examples. Thankfully, I eventually ran into Joypy (github.com), a Joy written in Python. My Linux distro comes with Python installed, so the whole process for me was:&lt;/p&gt;&lt;quote&gt;git clone https://github.com/calroc/joypy.git cd joypy python -m joy ... joy? [1 2 3] [1 +] map&lt;/quote&gt;&lt;p&gt;Okay, that's a glimpse.&lt;/p&gt;&lt;p&gt;But we've barely touched the conceptual power of combinators with our &lt;code&gt;map&lt;/code&gt; examples. Let's go a little deeper on
        this fascinating subject:
    &lt;/p&gt;&lt;p&gt;Here's something from my bookshelf. It's To Mock a Mockingbird by mathematician and puzzle-maker Raymond Smullyan. It uses puzzles involving birds to solve logic problems and classify some well-known combinators.&lt;/p&gt;&lt;p&gt;It would be impossible to write a complete catalog of combinators just as it would be impossible to write a complete catalog of integers. They're both infinite lists. Nevertheless, some well-known combinators have been identified as having special properties. In the book above, many of these have been given the names of birds.&lt;/p&gt;&lt;p&gt;Remember, combinators are just "higher-order" functions that take functions as input. Well, it turns out these are all you need to perform any computation. They can replace logical operators and even variables.&lt;/p&gt;&lt;p&gt;What?!&lt;/p&gt;&lt;p&gt;Yeah, you can re-work any expression into a combinatorial expression and completely replace everything, including the variables, with combinators.&lt;/p&gt;&lt;p&gt;It's kind of hard to imagine at first. But you can see it happen right before your very eyes. The mind-blowing tool on this page by Ben Lynn: Combinatory Logic (stanford.edu) takes a term expressed in lambda calculus and replaces everything with just two combinators, K and S. (We'll talk more about those two in just a moment because they are super special.)&lt;/p&gt;&lt;p&gt;(Ben Lynn's whole website is full of neat stuff like this. If you're looking to entertain yourself for any amount of time from an afternoon to the rest your life, Lynn has you covered.)&lt;/p&gt;&lt;p&gt;So combinators share something in common with lambda calculus and Turing machines. These systems provide all of the building blocks you need to perform any possible computation in the sense of the Church-Turing thesis (wikipedia.org) or "computability thesis". (We've also discovered some problems that are not computable and no system can compute them like "the halting problem," but these are pretty rare.)&lt;/p&gt;&lt;p&gt;It turns out that computation is a fundamental feature of the Universe. As far as we can tell, any universal system of computation is equally capable of solving any computational problem. And once you realize how little is required, you can invent a universal computer yourself!&lt;/p&gt;&lt;p&gt;Electronically speaking, this is the same principle that allows a NAND gate to simulate all other gates. NAND gates are a fundamental computational building block. You can make an entire computer with nothing but NAND gates and that computer can (slowly) solve any computable problem you can imagine.&lt;/p&gt;&lt;p&gt;Anyway, when we use combinators, this particular flavor of universal computation is called combinatory logic (wikipedia.org).&lt;/p&gt;&lt;p&gt;What do the building blocks of combinatory logic look like?&lt;/p&gt;&lt;p&gt;Let's start small:&lt;/p&gt;&lt;p&gt;Identity&lt;/p&gt;&lt;quote&gt;(I x) = x&lt;/quote&gt;&lt;p&gt;The simplest of all combinators is I, the "identity combinator". There are a ton of different ways to write this. In lambda calculus, it looks like this: &lt;code&gt;I = λx&lt;/code&gt;.
        &lt;/p&gt;&lt;p&gt;The way to read &lt;code&gt;"(I x) = x"&lt;/code&gt; is: "&lt;code&gt;I&lt;/code&gt; applied
        to some object &lt;code&gt;x&lt;/code&gt; results in...&lt;code&gt;x&lt;/code&gt;."
        &lt;/p&gt;&lt;p&gt;We say "object x" rather than "value x" because, being a combinator, &lt;code&gt;I&lt;/code&gt; could take a function as input as well as a
        value. In fact, "object" is intentionally very abstract, so
        &lt;code&gt;x&lt;/code&gt; could contain a scalar value, or
        list, or function, or another combinator, or anything.
        Whatever that object is, &lt;code&gt;I&lt;/code&gt; returns it.
    &lt;/p&gt;&lt;p&gt;K and S&lt;/p&gt;&lt;quote&gt;(K x y) = x (S x y z) = (x z (y z))&lt;/quote&gt;&lt;p&gt;Both of these take more than one parameter of input. But if you're used to Algol-like function syntax, the way this works may be surprising.&lt;/p&gt;&lt;p&gt;Since it's the simpler of the two, let's use the &lt;code&gt;K&lt;/code&gt;
        combinator as an example:
        &lt;/p&gt;&lt;p&gt;The way to read "&lt;code&gt;(K x y) = x&lt;/code&gt;" is:
        "&lt;code&gt;K&lt;/code&gt; applied to &lt;code&gt;x&lt;/code&gt; yields
        a combinator, which, when applied to &lt;code&gt;y&lt;/code&gt; always
        evaluates to &lt;code&gt;x&lt;/code&gt;."
        &lt;/p&gt;&lt;p&gt;(Programmers familiar with the concept of currying will see that this is like the partial application of a function, where a new function is "pre-baked" with the argument &lt;code&gt;x&lt;/code&gt;.  The
        term "currying" is named in honor of mathematician
        Haskell Curry
        (wikipedia.org),
        after whom the Haskell programming language is also named.)
        &lt;/p&gt;&lt;p&gt;The result is that &lt;code&gt;K&lt;/code&gt; makes a combinator that
        throws away any input and just returns
        &lt;code&gt;x&lt;/code&gt;. Weird, right? But it turns out to be useful.
        &lt;/p&gt;&lt;p&gt;&lt;code&gt;K&lt;/code&gt; is super easy to write in a language like
        JavaScript, which is also a nice choice because you can play with
        it right in the browser console like I just did:
        &lt;/p&gt;&lt;quote&gt;K = function(x){ return function(y){ return x; } } K("hello")("bye") &amp;gt; "hello"&lt;/quote&gt;&lt;p&gt;See how the result of &lt;code&gt;K("hello")&lt;/code&gt; is a function that
        returns "hello" no matter what you give it as input?
        &lt;/p&gt;&lt;p&gt;How about &lt;code&gt;S&lt;/code&gt;? I'll leave implementing that
        in JavaScript as an exercise for the reader.
        It's clearly much more complicated since it has three levels of
        "function that yields a combinator" on the left and the result
        is an equally complicated combinator that first applies
        parameter &lt;code&gt;z&lt;/code&gt; to combinator &lt;code&gt;y&lt;/code&gt;.
        &lt;/p&gt;&lt;p&gt;(By the way, the &lt;code&gt;y&lt;/code&gt; combinator above should not be
        confused with the &lt;code&gt;Y&lt;/code&gt; combinator.
        Do you remember that arcane lambda calculus artifact projected
        over that head with the third eye way up near the beginning of this
        page?  That thing was the &lt;code&gt;Y&lt;/code&gt; combinator! It turns out, it's
        all, like, connected, you know?)
        &lt;/p&gt;&lt;p&gt;But the real point is this: &lt;code&gt;S&lt;/code&gt; and &lt;code&gt;K&lt;/code&gt; are
        special for one very interesting reason.
        Together with &lt;code&gt;I&lt;/code&gt;, they form the "SKI calculus" and just
        these three combinators are all you need to perform
        any computation in the known universe.
        &lt;/p&gt;&lt;p&gt;Actually, it's even crazier than that. You don't even need &lt;code&gt;I&lt;/code&gt; because that, too, can be created with &lt;code&gt;S&lt;/code&gt;
        and &lt;code&gt;K&lt;/code&gt;.
        &lt;/p&gt;&lt;p&gt;That's right, the &lt;code&gt;S&lt;/code&gt; and &lt;code&gt;K&lt;/code&gt; definitions
        above are a complete system for universal computation.
    &lt;/p&gt;&lt;p&gt;The book shown here is another from my bookshelf. It's Combinators: A Centennial View by Stephen Wolfram.&lt;/p&gt;&lt;p&gt;It starts with a (much too) terse introduction to the SKI combinator calculus and then launches into page after page of visualizations of S and K combinators being fed into each other. Like fractals or automata, simple inputs can produce patterns of surprising sophistication.&lt;/p&gt;&lt;p&gt;Wolfram demonstrates combinators that keep producing different output for a gazillion iterations and then get stuck in a loop. Some of them produce regular patterns for a while and then start producing different patterns. Some just loop forever at the outset. As in other universal systems, there is no end to the complexity produced by these two simple constructs. It is infinite. And all of this is just S and K combinators taking combinators as input and returning combinators as output.&lt;/p&gt;&lt;p&gt;I think it is wild and fun to see someone play with a subject like Wolfram does in this book. Each page is saying, "Look at what is possible!"&lt;/p&gt;&lt;p&gt;Combinators is also Wolfram's ode to the discoverer of combinatory logic, Moses SchÃ¶nfinkel (wikipedia.org) who, like so many of the giants in the field of computer science, did his work on paper decades before the first digital electronic computers beeped their first boops.&lt;/p&gt;&lt;p&gt;Figuring out the output of the &lt;code&gt;S&lt;/code&gt; combinator once
        was enough to keep me occupied for a while.  It boggles my mind to
        imagine feeding it another &lt;code&gt;S&lt;/code&gt; as input on paper,
        let alone discovering these particular combinators in the first place.
        &lt;/p&gt;&lt;p&gt;Okay, we get it, combinators are a crazy way to compute.&lt;/p&gt;&lt;p&gt;But are they worth using in "real" programs? In limited doses, absolutely!&lt;/p&gt;&lt;p&gt;Combinators let us factor out explicit loops. This:&lt;/p&gt;&lt;quote&gt;foo.map(bar)is the same as this much longer statement:&lt;/quote&gt;&lt;quote&gt;temp = []; for(i=0; i&amp;lt;foo.length; i++){ temp[i] = bar(foo[i]); }&lt;/quote&gt;&lt;p&gt;Both of those pieces of JavaScript give us the result of applying the function &lt;code&gt;bar()&lt;/code&gt; to an array &lt;code&gt;foo&lt;/code&gt;.
        &lt;/p&gt;&lt;p&gt;I think &lt;code&gt;map()&lt;/code&gt; is a great example of the power of
        combinators to clean up a program with abstraction.  Once you start
        using simple combinators like this to abstract away the boilerplate
        logic of yet another loop over a list of items, it's hard
        to go back.
        &lt;/p&gt;&lt;p&gt;My personal history with exploring higher order functions in a production setting is through the Ramda (ramdajs.com) JavaScript library, which I discovered from the talk Hey Underscore, You're Doing It Wrong! (youtube.com) by Brian Lonsdorf, which is fantastic.&lt;/p&gt;&lt;p&gt;Once I started discovering how combinators and curried functions could eliminate big old chunks of code, I was hooked! The old, dreary procedural code became a new fun puzzle!&lt;/p&gt;&lt;p&gt;Mind you, it's very easy to go overboard with this stuff and write something far less readable than some simple procedural code. (Gee, ask me how I know this.)&lt;/p&gt;&lt;p&gt;But in limited doses, it's super powerful and compact.&lt;/p&gt;&lt;p&gt;Joy uses combinators to "factor out" all sorts of logic.&lt;/p&gt;&lt;p&gt;Even different forms of recursion can be completely handled for you by combinators in Joy thanks to the uniformly unary functions.&lt;/p&gt;&lt;p&gt;Here's a factorial definition:&lt;/p&gt;&lt;quote&gt;factorial == [null] [succ] [dup pred] [*] linrec&lt;/quote&gt;&lt;p&gt;Let's try it:&lt;/p&gt;&lt;quote&gt;5 factorial 120&lt;/quote&gt;&lt;p&gt;Computing the factorial of a number is often used as an example of recursion. The final answer is the input number multiplied by the previous number multiplied by the previous number multiplied by... the rest of the numbers all the way down to 1.&lt;/p&gt;&lt;p&gt;Computing a factorial requires a cumulative result. Without recursion, you need an explicit variable to hold the intermediate result as you loop through the numbers.&lt;/p&gt;&lt;p&gt;As shown in the Joy &lt;code&gt;factorial&lt;/code&gt; definition above,
        &lt;code&gt;linrec&lt;/code&gt; is a "linear recursion" combinator. It takes takes
        4 parameters, each of which is a quoted program. &lt;code&gt;null&lt;/code&gt; is a
        predicate which tests for zero. &lt;code&gt;dup&lt;/code&gt; is the same as in
        Forth. &lt;code&gt;pred&lt;/code&gt; is an operator which yields a number's
        predecessor (given 4, yields 3).  "&lt;code&gt;*&lt;/code&gt;" multiplies two
        numbers, just like you'd expect. Given these pieces, perhaps you can
        take a guess at how &lt;code&gt;linrec&lt;/code&gt; works?
        &lt;/p&gt;&lt;p&gt;For comparison, here is a recursive JavaScript solution:&lt;/p&gt;&lt;quote&gt;function factorial(n) { if (n &amp;lt;= 1) { return 1; } return n * factorial(n - 1); }&lt;/quote&gt;&lt;p&gt;Note that the Joy example is not just shorter and has no variable names but it has abstracted away the mechanics of recursion. All we're left with is the logic specific to the factorial problem itself.&lt;/p&gt;&lt;p&gt;It's debatable which of these two are more readable because the measure of readability is in the eye of the beholder. But I think you can imagine getting good at reading the Joy example.&lt;/p&gt;&lt;p&gt;Okay, so we've gone pretty deep into this concatenative programming and combinator thing. How does this actually relate to Forth?&lt;/p&gt;&lt;p&gt;First of all, Forth does have facilities for dealing with combinators:&lt;/p&gt;&lt;p&gt;Forth supports higher order functions with "execution tokens" (function pointers) and the &lt;code&gt;EXECUTE&lt;/code&gt; word.
    &lt;/p&gt;&lt;p&gt;This will run the word returned by the word &lt;code&gt;FOO&lt;/code&gt;:
    &lt;/p&gt;&lt;quote&gt;FOO EXECUTE&lt;/quote&gt;&lt;p&gt;With this, you can very compactly define combinatorial words such as MAP, FOLD, and REDUCE.&lt;/p&gt;&lt;p&gt;First, let's see how &lt;code&gt;EXECUTE&lt;/code&gt; works. The syntax will be
        alien to non-Forth programmers, but the concept will be no problem for
        anyone used to using first class functions.
        &lt;/p&gt;&lt;p&gt;First, let's make a new word:&lt;/p&gt;&lt;quote&gt;: hello ." Hello" ;&lt;/quote&gt;&lt;p&gt;This is Forth for, "Compile a word called &lt;code&gt;hello&lt;/code&gt;
        that prints the string Hello."
        &lt;/p&gt;&lt;p&gt;(We'll learn how compiling words actually works later. For now, please just gracefully accept what you're seeing.)&lt;/p&gt;&lt;p&gt;Next:&lt;/p&gt;&lt;quote&gt;VARIABLE hello-token&lt;/quote&gt;&lt;p&gt;This creates a new variable called &lt;code&gt;hello-token&lt;/code&gt; which
        will store the "execution token" for the hello word.
        &lt;/p&gt;&lt;p&gt;This part will look super cryptic if you're new to Forth:&lt;/p&gt;&lt;quote&gt;' hello hello-token !&lt;/quote&gt;&lt;p&gt;Let's examine this one piece at a time:&lt;/p&gt;&lt;code&gt;'&lt;/code&gt;" gets the address of the word
                "&lt;code&gt;hello&lt;/code&gt;" and puts it on the stack.
            &lt;code&gt;hello-token&lt;/code&gt;" is a variable, which
                just leaves its address on the stack when called.
            &lt;code&gt;!&lt;/code&gt;" stores a value from the stack
                (the address of &lt;code&gt;hello&lt;/code&gt;) at
                an address from the stack (the address of
                variable &lt;code&gt;hello-token&lt;/code&gt;).
        &lt;p&gt;So the code above simply reads, "Store the address of &lt;code&gt;hello&lt;/code&gt; in the variable &lt;code&gt;hello-token&lt;/code&gt;."
        &lt;/p&gt;&lt;p&gt;Now let's use EXECUTE to call this "execution token":&lt;/p&gt;&lt;quote&gt;hello-token @ EXECUTE Hello&lt;/quote&gt;&lt;p&gt;Behold, it printed the "Hello" string!&lt;/p&gt;&lt;p&gt;Remember, the variable &lt;code&gt;hello-token&lt;/code&gt; leaves its
        address on the stack when it is called.
        &lt;/p&gt;&lt;p&gt;"&lt;code&gt;@&lt;/code&gt;" is a standard Forth word that loads the value
        from the given address and puts that value on the stack.
        &lt;/p&gt;&lt;p&gt;&lt;code&gt;EXECUTE&lt;/code&gt; gets an address from the stack and runs
        whatever word is found at that address.
        &lt;/p&gt;&lt;p&gt;Perhaps it would be helpful to see that this silly statement:&lt;/p&gt;&lt;quote&gt;' hello EXECUTEis equivalent to just calling&lt;/quote&gt;&lt;code&gt;hello&lt;/code&gt; directly:
        &lt;quote&gt;hello&lt;/quote&gt;&lt;p&gt;Anyway, now we're armed with Forth's combinatorial ability: Treating functions ("words") as values so other functions can take them as input. This allows us to define combinators in Forth.&lt;/p&gt;&lt;p&gt;For some compact higher-order function definitions in Forth, check out this Gist by Adolfo Perez Alvarez (github.com).&lt;/p&gt;&lt;p&gt;So yes, Forth is concatenative. It implicitly passes values from one function invocation to the next. And it supports higher-order functions.&lt;/p&gt;&lt;p&gt;Nevertheless, I do not believe studying "concatenative programming" in general or Joy specifically is a good way to understand the history and genesis of Forth!&lt;/p&gt;&lt;p&gt;For example, this simple statement:&lt;/p&gt;&lt;quote&gt;2 3 +&lt;/quote&gt;&lt;p&gt;can be read two different ways:&lt;/p&gt;&lt;p&gt;Forth: "Push 2 and then 3 on the stack; add them; push result 5 on the stack."&lt;/p&gt;&lt;p&gt;Joy: "The composition of the functions 2, 3, and + is identical to the function 5."&lt;/p&gt;&lt;p&gt;While both languages share a cosmetically similar syntax, and both produce the same result for this expression, there is a fundamental difference between how the two languages "think" about the expression because they arrived at this place in completely different ways.&lt;/p&gt;&lt;p&gt;Forth's only concern (as a language) is to process these three tokens and act upon them according to some simple rules. (If the token is in the dictionary, execute it. If it's a number, put it on the stack.)&lt;/p&gt;&lt;p&gt;To Joy, it may be the same mechanical process under the hood, but the language itself sees these tokens more like a mathematical expression. It's a much more abstract outlook.&lt;/p&gt;&lt;p&gt;The point I'm making is that Forth may accomodate the abstract point of view, if the developer chooses to take it. But Forth is not based on abstract concatenative computing principles or combinatory logic.&lt;/p&gt;&lt;p&gt;Let's look at this from a historical perspective. First, the notions of postfix syntax (RPN) and a data stack for the basis of the language:&lt;/p&gt;&lt;p&gt;Postfix notation was definitely in the air when Chuck Moore created Forth.&lt;/p&gt;&lt;p&gt;Stacks were known and used in the time of Forth's origins, though they were generally limited to 2-4 items in registers.&lt;/p&gt;&lt;p&gt;So I think it's reasonable to assume that RPN syntax and use of stacks are a historically accurate way to examine Forth's "origin story."&lt;/p&gt;&lt;p&gt;Hold that thought, here's a fun aside:&lt;/p&gt;&lt;p&gt;The drawing of the computer labeled "Z3" on the right is of the Z3 computer (wikipedia.org) designed by engineer and computer scientist Konrad Zuse. This is widely considered to be the first programmable digital computer! It used electro-mechanical relays like the telegraph networks of the day.&lt;/p&gt;&lt;p&gt;(By the way, a certain amount of electro-mechanical logic is still used in modern nuclear reactor safety systems because the big mechanical components are not as vulnerable to nuclear radiation as semiconductors!)&lt;/p&gt;&lt;p&gt;The Z3 could do addition in less than a second and multiplication in three seconds. It had 64 words of 22 bits each and worked with the equivalent of modern floating-point numbers.&lt;/p&gt;&lt;p&gt;As mentioned above, it can be said to use RPN, though there are only two registers and nine instructions. Opcodes were encoded in eight bits. The computer is programmable via punched paper tape (you can see the tape device to the right of the control console, though it's a bit of a scribble in my drawing).&lt;/p&gt;&lt;p&gt;It is also a stack machine. Again, this is with a mere two registers, which get juggled in a particular sequence as you load and store values.&lt;/p&gt;&lt;p&gt;Fun fact: The control unit used special control wheels to encode microsequences. If the microsequence wasn't programmed correctly, it could short-circuit the machine and destroy the hardware!&lt;/p&gt;&lt;p&gt;I got most of this information from this excellent paper by Raul Rojas: Konrad Zuse's Legacy: The Architecture of the Z1 and Z3 (PDF) (ed-thelen.org).&lt;/p&gt;&lt;p&gt;Anyway, so the simple mechanics of RPN and stack-based operation are very natural for digital computing machines and their use goes back to the very beginning.&lt;/p&gt;&lt;p&gt;But Joy and the term "concatenative programming" come from the 1980s.&lt;/p&gt;&lt;p&gt;Uh oh.&lt;/p&gt;&lt;p&gt;While the ideas of combinators and other types of universal computation were well known in certain mathematical and computational circles, I would argue they were not very amenable to existing computer hardware until much later when computers became fast enough to support "functional programming" styles and abstractions.&lt;/p&gt;&lt;p&gt;Until then, programming was "close to the metal." Even the idea of "structured programming" with programming language concepts like &lt;code&gt;if/else&lt;/code&gt; or &lt;code&gt;while/for&lt;/code&gt; loops was
        once considered novel! Until then, everything was done with address
        jumps or &lt;code&gt;GOTO&lt;/code&gt;.
        &lt;/p&gt;&lt;p&gt;It's important to remember that "coding", the actual act of turning an abstract program into machine code, was long ago considered to be a mere secretarial skill, not far removed from typing and other forms of data entry. This is why some people (including myself) refer themselves as "programmers" rather than "coders".&lt;/p&gt;&lt;p&gt;Concatenative programming, with its emphasis on combinators (and immutable data structures, which we haven't talked about), doesn't have the same historic grounding for Forth the way that RPN syntax and stack-based programming do.&lt;/p&gt;&lt;p&gt;So I must conclude that understanding concatenative programming is super cool, but it doesn't actually help us understand the true nature of Forth because it doesn't describe how Forth came to be. It is not part of Forth's "origin story."&lt;/p&gt;&lt;p&gt;As we'll soon see, Forth really is about the "nuts and bolts". You bring your own theories with you.&lt;/p&gt;&lt;p&gt;So while all these descriptions of the Forth language are true (RPN, stack-based, concatenative), they all describe the language Forth from the vantage of hindsight.&lt;/p&gt;&lt;p&gt;There's nothing wrong with thinking about Forth in these terms, but it doesn't answer the "why" questions:&lt;/p&gt;&lt;p&gt;"Why does Forth have this syntax?"&lt;/p&gt;&lt;p&gt;"Why does Forth work this way?"&lt;/p&gt;&lt;p&gt;I think the answers to the "why" questions are best answered by looking at when.&lt;/p&gt;&lt;p&gt;What is Forth's history, anyway?&lt;/p&gt;&lt;p&gt;If this image doesn't make any sense to you, citizen of the future, it's from the iconic movie poster by Drew Struzan for Back to the Future (1985) (wikipedia.org).&lt;/p&gt;&lt;p&gt;Chuck Moore is programming an IBM 704 with Fortran on punchards.&lt;/p&gt;&lt;p&gt;"Compiling took 30 minutes...you got one shot per day"&lt;/p&gt;&lt;p&gt;-- Chuck Moore, Forth, the Early years&lt;/p&gt;&lt;p&gt;In Forth - The Early Years (PDF) (worrydream.com), Chuck Moore recites a fairly terse history of Forth, from the earliest pre-Forths to the creation of the language standard.&lt;/p&gt;&lt;p&gt;(Note: Chuck mentions the Smithsonian Astrophysical Observatory (SAO) and the Massachusetts Institute of Technology (MIT) in roughly the same time period, and it's a bit difficult to be entirely sure which part is talking about which organization. But if you look at a map, SAO is at Harvard University. Harvard and MIT are about a mile apart in Cambridge, Massachusetts. It's basically a singular point if you zoom out a bit. So that helps explain the overlap.)&lt;/p&gt;&lt;p&gt;The computer in question is the IBM 704 (wikipedia.org) It was one of those room-filling vacuum-tube computers with tape drives the size of refrigerators.&lt;/p&gt;&lt;p&gt;The 704 was a fully programmable "modern" computer with magnetic-core memory, multiple registers, a 36-bit instruction set, and 36-bit words ("word" as in native memory size for the processor, not "word" as in Forth functions).&lt;/p&gt;&lt;p&gt;There were switches for each register on the control console, but programs could be written to and read from paper punch cards.&lt;/p&gt;&lt;p&gt;It was very modern for the time, but...&lt;/p&gt;&lt;quote&gt;"In its day, the 704 was an exceptionally reliable machine. Being a vacuum-tube machine, however, the IBM 704 had very poor reliability by today's standards. On average, the machine failed around every 8 hours, which limited the program size that the first Fortran compilers could successfully translate because the machine would fail before a successful compilation of a large program."&lt;/quote&gt;&lt;p&gt;It's difficult to imagine now, but changing parameters for a program, re-compiling it, and running it again could take a day (assuming you didn't make any mistakes).&lt;/p&gt;&lt;p&gt;So Chuck solved that irritation with an extremely clever solution:&lt;/p&gt;&lt;p&gt;Moore made an interactive interpreter on a computer with nothing we would recognize today as an interactive terminal.&lt;/p&gt;&lt;p&gt;He accomplished this by making his program programmable.&lt;/p&gt;&lt;p&gt;Here's a quote from The Evolution of Forth (forth.com):&lt;/p&gt;&lt;quote&gt;"Moore's programming career began in the late 1950s at the Smithsonian Astrophysical Observatory with programs to compute ephemerides, orbital elements, satellite station positions, etc. His source code filled two card trays. To minimize recompiling this large program, he developed a simple interpreter to read cards controlling the program. This enabled him to compose different equations for several satellites without recompiling..."&lt;/quote&gt;&lt;p&gt;His free-form input format turned out, ironically, to be more reliable for human use than Fortran, which required formatted columns. (At the time, any mis-aligned columns in Fortran punchcard input would require a re-run of the program!)&lt;/p&gt;&lt;p&gt;It was also faster and more compact.&lt;/p&gt;&lt;p&gt;These "programming the program" statements in Moore's simple interpreter did not use keywords. They were statement numbers encoded on a punchcard.&lt;/p&gt;&lt;p&gt;This is the origin of the system that would eventually be named Forth.&lt;/p&gt;&lt;p&gt;According to Moore, the interpreter's statement numbers would have been roughly equivalent to these Forth words:&lt;/p&gt;&lt;quote&gt;WORD NUMBER INTERPRET ABORT&lt;/quote&gt;&lt;p&gt;Free-form input was unusual at the time. It's obviously a super nice alternative to recompiling your calculation program every time you want to change some numbers!&lt;/p&gt;&lt;p&gt;So, at last, we have discovered the true origin of the Forth language: Moore wrote a simple interpreter to reduce waste and tedium.&lt;/p&gt;&lt;p&gt;Already, Moore has exhibited the defining combination of traits shared by great programmers around the world: Inventive and allergic to tedium.&lt;/p&gt;&lt;p&gt;If it had stopped there, it would have been a clever trick and perhaps worthy of a footnote in history.&lt;/p&gt;&lt;p&gt;But Chuck Moore did not stop there.&lt;/p&gt;&lt;p&gt;Now we head from Massachusetts to California where Moore found himself at Stanford University where he received his BA in Physics and started graduate school. He worked with Stanford's Burroughs B5500.&lt;/p&gt;&lt;p&gt;Let's talk about the computer first:&lt;/p&gt;&lt;p&gt;The B5500 (or "B 5500" - the official manual puts a space between the B and the number) was a solid-state computer. It was part of the "second-generation" of computers (wikipedia.org). These computers had discrete transistors on circuit boards. By contrast, the first generation before them used vacuum tubes (like the aforementioned IBM 704) and the third generation after them used integrated circuits.&lt;/p&gt;&lt;p&gt;In fact, the Burroughs Large Systems engineers were transistor computer pioneers. And the B5000 series was a pioneering system.&lt;/p&gt;&lt;p&gt;Here's some more resources:&lt;/p&gt;&lt;p&gt;And what exactly did Chuck Moore do with that B5500 machine?&lt;/p&gt;&lt;p&gt;Moore's CURVE was another mathematical application, written in Stanford's own Algol implementation.&lt;/p&gt;&lt;p&gt;It contained a much more sophisticated interpreter this time with a data stack and control flow operators.&lt;/p&gt;&lt;p&gt;Equivalent Forth words:&lt;/p&gt;&lt;quote&gt;IF ELSE DUP DROP SWAP + - *&lt;/quote&gt;&lt;p&gt;(As we'll see, symbols like "+" and "-" are words in Forth.)&lt;/p&gt;&lt;p&gt;Moore worked on the Stanford Linear Accelerator as a programmer. His focus was on steering the beam of the electron accelerator.&lt;/p&gt;&lt;p&gt;The CURVE program was even more "programmable" than his Fortran program at SAO. He took those ideas and expanded them to include the idea of a parameter stack and the ability to define new procedures.&lt;/p&gt;&lt;p&gt;This made the interpreter much more flexible and capable.&lt;/p&gt;&lt;p&gt;Aside: At this point, I also think it's interesting to compare Moore's budding interpreter language with another interpreter created specifically to be embedded in larger programs for controlling them: The Tcl programming language (wikipedia.org). 27 years after Moore started his work, John Ousterhout created Tcl out of frustration with ad-hoc, half-baked solutions in 1988 at Berkeley. The name comes from "Tool Command Language". But the comparison goes deeper than just the shared motivation. Tcl and Forth have similar levels of syntactical purity and flexibility. Everything in Tcl is a string! Both languages give the user the power to define fundamental parts of the system, such as new control structures, in the language itself. If this sounds interesting, you owe it to yourself to play with Tcl for a while. It is extremely clever and extremely capable. The main implementation has been well cared-for and can be found on most Unix-like systems, often installed by default.&lt;/p&gt;&lt;p&gt;As Moore demonstrated with CURVE, a powerful, extensible interpreter is a huge time-saver (certainly when compared to re-compiling the program!) and allows the user of the program to add to the program's functionality on the fly. It's difficult to overstate how powerful this can be.&lt;/p&gt;&lt;p&gt;Truly, now we have the beginnings of a fully-fledged programming language. It's not named Forth yet, but we're getting closer.&lt;/p&gt;&lt;p&gt;"With the TTY came paper-tape and some of the most un-friendly software imaginable - hours of editing and punching and loading and assembling and printing and loading and testing and repeating."&lt;/p&gt;&lt;p&gt;-- Chuck Moore, Forth, the Early years&lt;/p&gt;&lt;p&gt;First, let's talk about what "TTY" means in 1965. Teleprinters (wikipedia.org) or "teletypewriters" or just "teletype" were all printer devices. They printed to continuous sheets of paper fan-folded to fit into boxes.&lt;/p&gt;&lt;p&gt;The Latin "tele-" prefix means "far" or "at a distance". These machines trace a direct lineage from telegraphs and Morse code.&lt;/p&gt;&lt;p&gt;In the late 1800s, the concept of a typewriter which operated over telegraph lines had been explored and existed in a variety of forms. But the transmission code, paper tape, and typewriter system devised by Donald Murray (oztypewriter.blogspot.com) is the one that won out. And it was arguably Murray's choice of QWERTY keyboard that cemented it as the standard around the world.&lt;/p&gt;&lt;p&gt;The existing Baudot code (from which we also get the term "baud") was modified by Murray into something that very much resembles what we still use today. Murray also introduced the concept of control characters, which still clearly retain their typewriter origins in the names: &lt;code&gt;CR&lt;/code&gt; (carriage return) and &lt;code&gt;LF&lt;/code&gt; (line feed). 
        &lt;/p&gt;&lt;p&gt;Teletype machines started as point-to-point text communication tools (like the telegraph), but they were later used over switched networks like the world-wide Telex system which used pulse dialing to automatically route a connection through the network.&lt;/p&gt;&lt;p&gt;The Teletype Model 33 (wikipedia.org) I drew above was one of the most popular teletypes used with computers. It was created by The Teletype Corporation in 1963, which means it shares a birth year with the ASCII standard! It remained popular until the mid-1970s when video terminals finally came down in price enough to push printer teletypes aside. In fact, Teletype Co. made the Model 33 until 1981, which is much later than I would have guessed!&lt;/p&gt;&lt;p&gt;As for paper-tape (wikipedia.org), I'll just quote Wikipedia directly:&lt;/p&gt;&lt;quote&gt;"Punched tape was used as a way of storing messages for teletypewriters. Operators typed in the message to the paper tape, and then sent the message at the maximum line speed from the tape. This permitted the operator to prepare the message "off-line" at the operator's best typing speed, and permitted the operator to correct any error prior to transmission. An experienced operator could prepare a message at 135 words per minute (WPM) or more for short periods."&lt;/quote&gt;&lt;p&gt;Donald Murray didn't invent the concept of perforated paper tape for data storage, but his system used it for the encoding of transmitted messages from the keyboard. It doesn't seem like a stretch to trace the origins of this storage method to Murray's system.&lt;/p&gt;&lt;p&gt;The computers of this era and earlier were paper manipulators. They were kind of like really complicated typewriters. They displayed their output on paper, they were programmed with paper, and they kept long-term storage on paper!&lt;/p&gt;&lt;p&gt;But as time went on, computer interactivity increased. They became less like typewriters and more like the machines we use today.&lt;/p&gt;&lt;p&gt;As each new ability emerged, Forth became increasingly interactive.&lt;/p&gt;&lt;p&gt;Forth gains direct terminal input and output!&lt;/p&gt;&lt;quote&gt;KEY EMIT CR SPACE DIGIT&lt;/quote&gt;&lt;p&gt;These new words turned Moore's system into a program editor.&lt;/p&gt;&lt;p&gt;Now you can edit the program within the program.&lt;/p&gt;&lt;p&gt;Moore's complete system is now kind of like an integrated development environment and kind of like an operating system.&lt;/p&gt;&lt;p&gt;In the mid-1960s, "mini-computers" came out. They were still huge by today's standards, but no longer required a large room of their own.&lt;/p&gt;&lt;p&gt;In addition to the reduction in size, the other emerging change was direct interactive use of a computer via teletype.&lt;/p&gt;&lt;p&gt;Specifically, the invention of timesharing (stanford.edu) was a huge shift away from the "batch processing" style of computing that had come before (like with input via punchcard).&lt;/p&gt;&lt;p&gt;(Fun fact: A "second generation" time-sharing operating system called Multics (multicians.org) was the spiritual ancestor of and name from which Brian Kernighan made the joke name Unix: "One of whatever Multics was many of".)&lt;/p&gt;&lt;p&gt;Moore's evolving pre-Forth language also gained completely interactive editing and executing of programs.&lt;/p&gt;&lt;p&gt;This would have been right around the time that the original LISP REPL (Read-eval-print loop) (wikipedia.org) was created in 1964 on a PDP-1.&lt;/p&gt;&lt;p&gt;If not pre-saging, Moore was certainly on the bleeding edge of interactive computer usage!&lt;/p&gt;&lt;p&gt;Aside: If you want to see an awesome demonstration of interactive computer usage on paper, check out this demonstration by Bob Spence: APL demonstration 1975 (youtube.com). Bob Spence (wikipedia.org) is best known for his own contributions, including a number of early clever computer interaction ideas that are worth re-examining today. Bob's demo is extremely pleasant to watch and brilliantly presented in split screen. Notice how paper output lets you mark up stuff with a pen - pretty nice feature! And APL (wikipedia.org) is a whole other rabbit hole which has interesting intersections with the point-free and higher-order function programming we've encountered earlier.&lt;/p&gt;&lt;p&gt;Then this happens...&lt;/p&gt;&lt;p&gt;IBM 1130 minicomputer at Mohasco, a textiles manufacturer in New York.&lt;/p&gt;&lt;p&gt;16 bit, 8 KB RAM.&lt;/p&gt;&lt;p&gt;Backup was via punch/reader.&lt;/p&gt;&lt;p&gt;With disks, now we can have file names!&lt;/p&gt;&lt;p&gt;File names limited to 5 characters...&lt;/p&gt;&lt;p&gt;Moore names his "fourth generation" system "FORTH".&lt;/p&gt;&lt;p&gt;Yup, this really is the origin of the name, "Forth". Funny how temporary things tend to stick and last forever, isn't it?&lt;/p&gt;&lt;p&gt;The IBM 1130 (wikipedia.org) is one of those new-fangled "minicomputers" we've talked about. Gosh, it was so small, the CPU weighed less than a car!&lt;/p&gt;&lt;p&gt;And it was affordable! The base model was as low as $32,000. Compare that to $20,000, the median price for a house in the U.S. in 1965. Just think of that: If you could afford a house, you were well on your way to being able to afford a computer!&lt;/p&gt;&lt;p&gt;As noted, the unit Chuck Moore worked on had a disk drive, which would have bumped up the price an additional $9,000. That would be the equivalent of buying an above-average house and adding a couple brand-new 1965 cars in the driveway.&lt;/p&gt;&lt;p&gt;But, wow, imagine having disk drive cartridges with 512 KB of storage at your disposal. What would you do with all that space?&lt;/p&gt;&lt;p&gt;As mentioned, at this time, we're still interacting with the computer (mostly) via paper, but these minis brought the idea of interactive computing to "the masses" because they were so much smaller, cheaper, and more reliable than the sorts of computers that had come before.&lt;/p&gt;&lt;p&gt;Quoting The Evolution of Forth (forth.com):&lt;/p&gt;&lt;quote&gt;"Newly married and seeking a small town environment, Moore joined Mohasco Industries in Amsterdam, NY, in 1968. Here he developed computer graphics programs for an IBM 1130 minicomputer with a 2250 graphic display. This computer had a 16-bit CPU, 8k RAM, his first disk, keyboard, printer, card reader/punch (used as disk backup!), and Fortran compiler. He added a cross-assembler to his program to generate code for the 2250, as well as a primitive editor and source-management tools. This system could draw animated 3-D images, at a time when IBM's software for that configuration drew only static 2-D images. For fun, he also wrote a version of Spacewar, an early video game, and converted his Algol Chess program into the new language, now (for the first time) called FORTH. He was impressed by how much simpler it became."&lt;/quote&gt;&lt;p&gt;As you may have gathered by now, Chuck Moore is a pretty extraordinary computer programmer.&lt;/p&gt;&lt;p&gt;It turns out the IBM 1130 was hugely influential to a bunch of early big-name programmers in addition to Moore. Something was in the air.&lt;/p&gt;&lt;p&gt;In addition to its funny new name, Forth had also gained new abilities:&lt;/p&gt;&lt;p&gt;Moore adds return call stack, allowing nested word definitions:&lt;/p&gt;&lt;quote&gt;: DOUBLE DUP + ; : QUAD DOUBLE DOUBLE ;&lt;/quote&gt;&lt;p&gt;And a dictionary of words.&lt;/p&gt;&lt;p&gt;It's not just the name that makes this the first real Forth: A dictionary of named words which can be called interactively or recursively in the definitions of other words is one of the defining features of Forth. The ability to use words as building blocks is the Forth language's primary abstraction.&lt;/p&gt;&lt;p&gt;In the example above, we've defined a word called &lt;code&gt;DOUBLE&lt;/code&gt;
        which duplicates the number on the top of the stack and adds the
        two numbers together.
        &lt;/p&gt;&lt;p&gt;A second word called &lt;code&gt;QUAD&lt;/code&gt; uses the previous definition
        by calling &lt;code&gt;DOUBLE&lt;/code&gt; twice, quadrupling the number in a
        rather amusing way.
        &lt;/p&gt;&lt;p&gt;A return stack makes this possible. Without a return stack, we have no way of telling the computer how to "get back" to the place in &lt;code&gt;QUAD&lt;/code&gt; where we left off after &lt;code&gt;DOUBLE&lt;/code&gt; is done.
        &lt;/p&gt;&lt;p&gt;(We'll get to the specifics of the syntax soon. That's another vital part of understanding Forth.)&lt;/p&gt;&lt;p&gt;Still at Mohasco. Programming a Univac 1108.&lt;/p&gt;&lt;p&gt;A new port of Forth written in assembler and could call COBOL modules because that's what the corporate suits wanted in 1970.&lt;/p&gt;&lt;p&gt;Moore hates complexity.&lt;/p&gt;&lt;p&gt;First of all, the UNIVAC 1108 (wikipedia.org) is a great example of the awesome "retro-futuristic" design of these old machines. Just look at the sweeping angles in my drawing of the console. That's a cool computer console!&lt;/p&gt;&lt;p&gt;When these computers cost more than a house, it makes perfect sense that they were constructed into beautiful custom furniture that made them look like space ships.&lt;/p&gt;&lt;p&gt;You have to wonder: Did the sci-fi art of the time drive the design of these computers or did the computers and industrial design of the time inform the art? Or, more likely, did they both feed off of each other in the classic cycle of, "life imitates art imitates life?"&lt;/p&gt;&lt;p&gt;That's a teletypewriter built into the desk of the console. I presume the tractor-feed paper would have spooled to and from containers behind the sleek facade.&lt;/p&gt;&lt;p&gt;Anyway, the UNIVAC 1108 is an even more modern computer than the IBM 1130. Now we're moving into using integrated circuits for everything, including the register storage. (Speaking of registers, the 1108 had 128 of them and must have been interesting to program!)&lt;/p&gt;&lt;p&gt;As was also the trend at the time, the CPU was constructed of discrete cards connected together by a wire-wrapped backplane.&lt;/p&gt;&lt;p&gt;If you're not familiar with the technique, you should know that wire-wrapped (wikipedia.org) connections are extremely high quality. Wire is wrapped with great force around a post, making a gas-tight connection that will not corrode (corrosion can occur outside the connection, of course). A little bit of the insulation gets wrapped in the last turns, which provides flexibility and strain relief. There are NASA guidelines for making a perfect wire-wrap connection.&lt;/p&gt;&lt;p&gt;Anyway, the Univac was even more powerful and modern than Moore's previous computer and he took advantage of it.&lt;/p&gt;&lt;p&gt;You don't have to read between the lines to see Moore's obvious distaste of COBOL (wikipedia.org), the COmmon Business-Oriented Language. What's impressive is that he managed to still use Forth while also using the required COBOL modules.&lt;/p&gt;&lt;p&gt;When this project was abandoned by the employer, Moore was upset by the whole situation, particularly the way business software was increasing in complexity. This won't be the last time we see this theme crop up.&lt;/p&gt;&lt;p&gt;He also wrote a book (unpublished) at this time called Programming a Problem-Oriented Language. It's written in typical Moore fashion, without superfluous words or exposition. Feel free to contrast this with the article you're reading now.&lt;/p&gt;&lt;p&gt;(This book will be mentioned again later.)&lt;/p&gt;&lt;p&gt;National Radio Astronomy Observatory - Computer control software for radio telescopes.&lt;/p&gt;&lt;p&gt;Radio telescopes are like visual telescopes, but they collect lower frequency waves. Thanks to the magic of computers, we can process these signals to see what the radio telescopes see.&lt;/p&gt;&lt;p&gt;Radio telescopes can work with everything from 1 kHz, which is just below the uses of "radio" as we think of it for navigation, communication, and entertainment, to 30 GHz, which is still well under the visible portion of the electromagnetic spectrum. Consumer microwave ovens operate at about 2.45 GHz.&lt;/p&gt;&lt;p&gt;(Speaking of Gigahertz, apparently Intel Core i9 processors can run at clock speeds up to 6 Ghz, but most CPU designs top out at around 4 Ghz. This may be important for Forth for reasons I explain later.)&lt;/p&gt;&lt;p&gt;The visible part of the spectrum is very small by comparison. It starts at 420 THz (terahertz) and ends at 720 THz. The familiar rainbow of colors captured in the mnemonics "Roy G. Biv" or "Richard of York Gave Battle in Vain" (ROYGBIV) lists colors in order of lowest frequency (Red) to highest (Violet).&lt;/p&gt;&lt;p&gt;Here is the official website of the National Radio Astronomy Observatory (nrao.edu). But for a better summary, the Wikipedia entry (wikipedia.org) is the way to go. Be sure to scroll down to the incredible image and description from 1988 of the collapsed 300ft radio telescope:&lt;/p&gt;&lt;quote&gt;"The telescope stood at 240ft in height, wieghed 600-tons, had a 2-min arc accuracy, and had a surface accuracy of ~1 inch. The collapse in 1988 was found to be due to unanticipated stresses which cracked a hidden, yet weight and stress-supporting steel connector plate, in the support structure of the massive telescope. A cascade failure of the structure occurred at 9:43pm causing the entire telescope to implode."&lt;/quote&gt;&lt;p&gt;The 300ft dish had been the world's largest radio telescope when it went active in 1962 at the NRAO site in West Virginia.&lt;/p&gt;&lt;p&gt;My drawing above is of the Very Large Array (wikipedia.org) in New Mexico. NRAO is also a partner in a huge international array in Chile.&lt;/p&gt;&lt;p&gt;By using radio interferometry, arrays of telescopes can be treated as essentially one huge telescope with the diameter of the array (missing the sensitivity a dish of that size would have).&lt;/p&gt;&lt;p&gt;But the scope for which Moore wrote software was a single 36ft (11 meter) dish at Kitt Peak in Arizona called The 36-Foot Telescope. It was constructed in 1967 and continued working until it was replaced with a slightly larger and more accurate dish in 2013.&lt;/p&gt;&lt;p&gt;The 36ft scope was used for millimeter-wavelength molecular astronomy. This is the range above "microwaves" and these telescopes pretty much have to be constructed at dry, high altitude sites because water vapor in the air can interfere with the radio waves.&lt;/p&gt;&lt;p&gt;(Note that Moore stayed at the NRAO headquarters in Virginia and was not on-site at Kitt Peak.)&lt;/p&gt;&lt;p&gt;NRAO had a policy of using Fortran on its minicomputers, but based on the success of his previous work, Moore was begrudgingly given permission to use Forth instead. I couldn't possibly do justice to summarizing it, so here's Chuck's own words describing the software he wrote for the NRAO (also from Forth - The Early Years):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;"There were two modes of observing, continuum and spectral-line. Spectral-line was the most fun, for I could display spectra as they were collected and fit line-shapes with least-squares."&lt;/p&gt;&lt;p&gt;It did advance the state-of-the-art in on-line data reduction. Astronomers used it to discover and map inter-stellar molecules just as that became hot research."&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Here is a photo (nrao.edu) of the 36-foot telescope. And here is a photo of the control room in 1974 (nrao.edu) with what appears to be a PDP-11 in the background.&lt;/p&gt;&lt;p&gt;As you can see, the work itself was extremely interesting and cutting-edge. But how Moore went about it was also very interesting, which a series of computer drawings will demonstrate in a moment.&lt;/p&gt;&lt;p&gt;But on the Forth language front, there was another development...&lt;/p&gt;&lt;p&gt;At this time, there are talks of patenting Forth.&lt;/p&gt;&lt;p&gt;Moore believes ideas shouldn't be patented.&lt;/p&gt;&lt;p&gt;We take it for granted now that "free" or "open" software unencumbered by patents and restrictive corporate licenses is a good thing. But this was absolutely not a mainstream position in the early 1970s.&lt;/p&gt;&lt;p&gt;To put things in context, in the summer of 1970, Richard Stallman (wikipedia.org) was just out of high school and was writing his first programs in Fortran (which he hated) and then APL.&lt;/p&gt;&lt;p&gt;It wasn't until 1980 that Stallman finally got fed up enough with the state of proprietary and legally encumbered software to start the "free-as-in-freedom" software revolution. Companies were increasingly using copyright to prevent modification, improvement, or duplication by the end user. Stallman, being a pretty incredible programmer, wrote free clones of such programs. He announced the GNU project (wikipedia.org) in 1983.&lt;/p&gt;&lt;p&gt;Aside: I believe Stallman was right. There's absolutely nothing wrong with writing programs for money or selling software. But using the law to prevent people from truly owning that software by limiting how or where to run it, or even preventing people from writing their own similar software, if they are capable, is an abominable practice and should be countered at every step.&lt;/p&gt;&lt;p&gt;Moore also rejects the standardization of Forth.&lt;/p&gt;&lt;p&gt;"All of my fears of the standard and none of the advantages of the standard have come to pass. Any spirit of innovation has been thoroughly quelched. &lt;lb/&gt;Underground Forths are still needed. &lt;lb/&gt;I said I thought the standard should be a publication standard but they wanted an execution standard." &lt;/p&gt;&lt;p&gt;-- Chuck Moore, 1997&lt;/p&gt;&lt;p&gt;I think that when you get to the heart of what Forth is all about, Moore's displeasure with the ANSI standardization suddenly makes tons of sense. In short, the whole point of Forth is to create your own toolkit. Having an all-inclusive language standard is great for making sure Forths are interchangeable. Unfortunately, it's also antithetical to adapting the language to your specific hardware and software needs.&lt;/p&gt;&lt;p&gt;Alright, enough philosophizing. Let's get back to the computer stuff!&lt;/p&gt;&lt;p&gt;While Moore was at NRAO, he also wrote software to point the telescope. Elizabeth Rather (Moore credits her as Bess Rather in his paper) was hired for support and they worked together on at least one port. The Forth system migrated across multiple machines at NRAO which, as we'll see, highlights one of the technological strengths of the standard Forth implementation.&lt;/p&gt;&lt;p&gt;By the way, after her initial reaction of shock and horror, Elizabeth Rather embraced Forth. From The Evolution of Forth (forth.com):&lt;/p&gt;&lt;quote&gt;"After about two months, Rather began to realize that something extraordinary was happening: despite the incredibly primitive nature of the on-line computers, despite the weirdness of the language, despite the lack of any local experts or resources, she could accomplish more in the few hours she spent on the Forth computers once a week than the entire rest of the week when she had virtually unlimited access to several large mainframes."&lt;/quote&gt;&lt;p&gt;Rather went on to write the first Forth manual in 1972 and write papers about it for the NRAO and other astronomical organizations.&lt;/p&gt;&lt;p&gt;Later, Elizabeth "Bess" Rather (wikipedia.org) became the co-founder of FORTH, Inc with Chuck and remained one of the leading experts and promoters of the Forth language until her retirement in 2006.&lt;/p&gt;&lt;p&gt;There's a great overview paper of the whole NRAO system by Moore and Rather in a 1973 Proceedings of the IEEE: The FORTH Program for Spectral Line Observing (PDF) (iae.nl).&lt;/p&gt;&lt;p&gt;It includes a high-level description of the system with examples of interactive Forth usage and a neat diagram on the first page, which you can see in the screenshot.&lt;/p&gt;&lt;p&gt;As mentioned, Forth was ported to a bunch of different computers at NRAO.&lt;/p&gt;&lt;p&gt;Let's take a look:&lt;/p&gt;&lt;p&gt;Forth on the IBM 360/50&lt;/p&gt;&lt;p&gt;Moore mentions first having ported his Forth system to the IBM 360/50 (wikipedia.org).&lt;/p&gt;&lt;p&gt;The System/360 (or S/360) computers were extremely successful, largely because of availability, longevity, and compatibility. IBM claims to be the first company to use microcode (wikipedia.org) to provide a compatible instruction set across all S/360 computers despite the hardware differences between models.&lt;/p&gt;&lt;p&gt;The cheaper 360 computers used microcode while the more expensive and powerful machines had hard-wired logic. NASA even had some one-off models of IBM 360 made just for them.&lt;/p&gt;&lt;p&gt;Until microcode came along, if you bought a "cheap" computer to get started and then upgraded to a more powerful computer, you would have to re-write your programs in a new instruction set. (If you happen to have written your programs in a high-level language like Fortran, you would still have to re-compile your programs from punchcards, and you would need the Fortran compilers on both computers to be perfectly compatible!) It's easy to see why being able to upgrade without changing your software would have been appealing.&lt;/p&gt;&lt;p&gt;System/360 computers were a "big bet" (5 billion dollars according to IBM themselves: System 360: From Computers to Computer Systems (ibm.com)) that nearly destroyed the company. The bet clearly paid off because they made these machines from 1964 to 1978.&lt;/p&gt;&lt;p&gt;Oh, and it wasn't just the instruction set that was compatible. The 360 computers also had standardized peripheral interfaces, which were compatible between machines. There was a huge market for peripheral devices. IBM themselves made 54 different devices such as memory, printers, card readers, etc. The 360 also spawned a whole third-party peripheral industry, much like the IBM PC-compatible era that started in 1981 and continues to the desktop computer I'm typing on right now in 2023.&lt;/p&gt;&lt;p&gt;Moore wrote Forth from scratch in S/360 assembly.&lt;/p&gt;&lt;p&gt;Then...&lt;/p&gt;&lt;p&gt;Forth ported to the Honeywell 316&lt;/p&gt;&lt;p&gt;I drew Chuck behind the system in this one because I couldn't bring myself to obscure an inch of that glorious pedestal console.&lt;/p&gt;&lt;p&gt;You can see the Honeywell 316 (wikipedia.org) and the brochure (wikimedia.org) image from which I made my drawing.&lt;/p&gt;&lt;p&gt;Just look at the space-age lines on that thing! It looks straight out of a Star Trek set. Sadly, there's basically no chance the one Moore actually worked on had this console. Less than 20 of them were sold. But thanks to my drawing, we can pretend.&lt;/p&gt;&lt;p&gt;Beyond just its appearance, this particular console has a really wild history. The extravagant gift company, Neiman Marcus, actually offered the Honeywell H316 with this pedestal as a "kitchen computer". It cost $10,000 and would have come with a two-week course to learn how to input recipes and balance a checkbook using toggle switches and lights to indicate binary data! (As far as anyone knows, none of these were actually sold.)&lt;/p&gt;&lt;p&gt;The ad for the Honeywell Kitchen Computer was in full "Mad Men" mode and was extremely patronizing, as was unfortunately typical for the time. But if you can look past that, the whole thing is quite funny:&lt;/p&gt;&lt;quote&gt;"Her souffles are supreme, her meal planning a challenge? She's what the Honeywell people had in mind when they devised our Kitchen Computer. She'll learn to program it with a cross-reference to her favorite recipes by N-M's own Helen Corbitt. Then by simply pushing a few buttons obtain a complete menu organized around the entree. And if she pales at reckoning her lunch tabs, she can program it to balance the family checkbook..."&lt;/quote&gt;&lt;p&gt;You can see a tiny scan of the original ad with a woman admiring her new Honeywell Kitchen Computer that barely fits in her kitchen here (wikipedia.org).&lt;/p&gt;&lt;p&gt;But moving on from the pedestal...&lt;/p&gt;&lt;p&gt;The implementation of Forth on the H316 is considered to be the first complete, stand-alone implementation because it was actually programmed on the computer itself and was used to create other Forths. It is at this point that Moore has achieved a fully ascendant system.&lt;/p&gt;&lt;p&gt;But wait, there's moore...er, sorry, more!&lt;/p&gt;&lt;p&gt;As is typical for a Chuck Moore endeavor, this telescope application pushed other new boundaries: The system actually ran across two computers (we're about to see the second one) and gave real-time access to multiple astronomers. Because it spread the load the way it did, there were no issues with concurrency, which is something we programmers struggle with to this day.&lt;/p&gt;&lt;p&gt;This real-time control and analysis was basically a luxury available on no other system at the time. Even Honeywell, the creator of these computers, had only been able to achieve the most primitive concurrency for them and it was nothing like this.&lt;/p&gt;&lt;p&gt;As usual, Moore was right on the very crest of computing with his ultra-flexible Forth system.&lt;/p&gt;&lt;p&gt;...And ported to the Honeywell DDP-116&lt;/p&gt;&lt;p&gt;As mentioned above, the Forth system was also ported to the DDP-116 (t-larchive.org). and used with its "parent" system on the H316 featured above.&lt;/p&gt;&lt;p&gt;(The DDP-116 was originally manufactured by Computer Control Company in 1965, but CCC was sold to Honeywell in 1966 and became its Computer Controls division.)&lt;/p&gt;&lt;p&gt;The DDP-116 was a 16-bit computer (the first available for purchase), but still part of that "second generation" of computers we've mentioned before, with individual transistors and components wire-wrapped together on huge circuit boards. (Check out the pictures on the DDP-116 link above for all sorts of excellent views of the insides and outsides of an example machine and its peripheral devices!) It happens to have also been a pretty rare computer. It didn't sell in vast quantities like the IBM systems.&lt;/p&gt;&lt;p&gt;As you can see in the drawing, Chuck Moore began to grow in power as his system evolved and this manifested in additional arms! Or maybe I started to get a little loopy while drawing old computers for these slides in the final evenings before I was due to give my talk? I'll let you decide what is real.&lt;/p&gt;&lt;p&gt;But wait, there's one more!&lt;/p&gt;&lt;p&gt;Forth on the DEC PDP-11&lt;/p&gt;&lt;p&gt;(Yes, that PDP-11.)&lt;/p&gt;&lt;p&gt;The PDP-11 (wikipedia.org) was by some measures the most popular minicomputer ever.&lt;/p&gt;&lt;p&gt;It was a 16-bit machine and had an orthogonal instruction set (meaning the same instruction could be used in multiple ways depending on the operand. This makes the mnemonics of the instruction set smaller and more logical and much easier to memorize). This was even more powerful because I/O was memory-mapped, so the same instructions used to move values around in memory and registers could also be used to transfer data to and from devices.&lt;/p&gt;&lt;p&gt;All told, these conveniences made the PDP-11 fun to program! Assembly language programmers rejoiced. The ideas in the PDP-11 spread rapidly and are to be found in the most popular architectures in use today. Compared to what came before it, PDP-11 assembly language will look surprisingly familiar to modern assembly programmers.&lt;/p&gt;&lt;p&gt;The original machines were made starting in 1970 with wire-wrapped backplanes and discrete logic gates. Later models introduced "large-scale integration," which is a term we'll see later, so hold that question! These later versions of the PDP-11 were still being made twenty years later in 1990! There are apparently still PDP-11s performing crucial tasks today, with nuclear power plants being one of the most prominent examples.&lt;/p&gt;&lt;p&gt;It's hard to see in my drawing, but the PDP-11 front panel is one of the most iconic computer interfaces ever made. Hobbyists make working models, including ridiculously cute and awesome miniature versions. Here are two model versions - click on them to go to the original wikipedia.org files, where you can admire their full beauty:&lt;/p&gt;&lt;p&gt;It would be difficult to overstate the impact of this machine. Probably the most famous piece of software released on the PDP-11 was the first version of Unix (wikipedia.org) that actually bore the name "Unix".&lt;/p&gt;&lt;p&gt;It was also the birthplace of the C (wikipedia.org) programming language. Dennis Ritchie ported Ken Thompson's B language to the PDP-11 to take advantage of its abilities. Unix was then re-written in C starting with Version 4. So the Unix we know today and a large portion of the command line utilities that are standard with a Unix-like system were programmed on the PDP-11. (And you can thank Richard Stallman's GNU project for freeing those for the masses. GNU stands for "GNU's Not Unix!")&lt;/p&gt;&lt;p&gt;You'll also note that Chuck Moore has gained his fourth and final arm in my drawing above ("fourth," ha ha). This may or may not reflect actual events. Also, I'm not sure if Moore would have been using a video terminal at that time. It's possible. DEC's first video terminal was the VT05 (columbia.edu), which came out in 1970.&lt;/p&gt;&lt;p&gt;All of this porting of Forth to new machines is possible because of indirect threaded code.&lt;/p&gt;&lt;p&gt;"Threaded code" in this usage is not related to concurrency, i.e. "multi-threaded programming".&lt;/p&gt;&lt;p&gt;It's code that is composed of subroutines addresses.&lt;/p&gt;&lt;p&gt;Threaded code can be machine code or interpreted.&lt;/p&gt;&lt;p&gt;Wait, aren't most programs composed of calls to subroutines?&lt;/p&gt;&lt;p&gt;That's true. The big difference is that threaded code (wikipedia.org) in this sense doesn't actually contain the instructions to call the subroutines. It stores just the addresses. Therefore another routine is responsible for advancing a pointer over the address list and executing the subroutines.&lt;/p&gt;&lt;p&gt;Huh?&lt;/p&gt;&lt;p&gt;Yeah, there's no way around it, threaded code is complicated.&lt;/p&gt;&lt;p&gt;And indirect threaded code is even more complicated (and harder to explain).&lt;/p&gt;&lt;p&gt;"Hey, wait!" I hear you saying. "If Chuck hates complexity so much, why did he use such a complex method for Forth?"&lt;/p&gt;&lt;p&gt;That's completely fair.&lt;/p&gt;&lt;p&gt;But before we address that, I'll try to briefly explain how threaded code is stored and executed.&lt;/p&gt;&lt;p&gt;First, here's how normal machine code might be written:&lt;/p&gt;&lt;p&gt;Direct calls (not threaded):&lt;/p&gt;&lt;quote&gt;jmp 0x0804000 jmp eax&lt;/quote&gt;&lt;p&gt;This is the simplest type of "call" to store in a program. We simply have the &lt;code&gt;jmp&lt;/code&gt; (jump) instruction followed
        by the address to jump to.
        Here I show both a hard-coded address
        (&lt;code&gt;0x0804000&lt;/code&gt;) and a register
        (&lt;code&gt;eax&lt;/code&gt;).
        Both of these are "direct" for our purposes.
        &lt;/p&gt;&lt;p&gt;Alternatively, many processors have a more advanced &lt;code&gt;call&lt;/code&gt;
        instruction. A call is more complicated because it has to do additional
        work behind the scenes. It must store a return address on "the stack"
        before jumping to the specified address. Then a &lt;code&gt;ret&lt;/code&gt;
        (return) instruction at the end of the called routine can use the
        stored address to resume the execution just after the "call site" where
        the call was first made.  Why are return addresses stored on a stack?
        That's because you can nest calls. Pushing addresses as you jump and
        popping them in reverse order as you return keeps things nice and neat.
        This "the stack" is not what Forth refers to as "the stack". Forth's
        main stack is better known as "the parameter stack". Many Forth
        implementations also have a return stack!
        &lt;/p&gt;&lt;p&gt;Anyway, this is direct and it's not threaded. Just jump to an address.&lt;/p&gt;&lt;p&gt;The first step of complication is adding indirection.&lt;/p&gt;&lt;p&gt;Indirect calls (not threaded):&lt;/p&gt;&lt;quote&gt;jmp [eax]&lt;/quote&gt;&lt;p&gt;For this example to make sense, you need to know that the square brackets around the register (&lt;code&gt;[eax]&lt;/code&gt;)
        is a common assembly language convention that means
        "the value at the memory address that is stored in register eax".
        &lt;/p&gt;&lt;p&gt;So &lt;code&gt;jmp [eax]&lt;/code&gt; means "jump to the address
        stored at the address stored in register eax."
        &lt;/p&gt;&lt;p&gt;That's indirect.&lt;/p&gt;&lt;p&gt;So now we have the "indirect" part of "indirect threaded code." But what's the "threaded" part?&lt;/p&gt;&lt;p&gt;Storing threaded code:&lt;/p&gt;&lt;quote&gt;&amp;lt;addr pointing to code&amp;gt; &amp;lt;addr pointing to code&amp;gt; &amp;lt;addr pointing to code&amp;gt; &amp;lt;addr pointing to code&amp;gt;&lt;/quote&gt;&lt;p&gt;Instead of containing the actual instructions to jump or call subroutines:&lt;/p&gt;&lt;quote&gt;jmp 0x0804000 jmp 0x080A816 jmp 0x08C8800 jmp 0x08C8DD0&lt;/quote&gt;&lt;p&gt;Threaded code stores just the list of addresses:&lt;/p&gt;&lt;quote&gt;0x0804000 0x080A816 0x08C8800 0x08C8DD0&lt;/quote&gt;&lt;p&gt;There are two consequences of storing code like this:&lt;/p&gt;&lt;p&gt;Another way to look at the list of addresses above is that, conceptually, threaded code is basically a list of subroutines.&lt;/p&gt;&lt;p&gt;To complete our definition of "indirect threaded" code, we just need to put both concepts together:&lt;/p&gt;&lt;p&gt;Storing indirect threaded code:&lt;/p&gt;&lt;quote&gt;&amp;lt;addr pointing to addr pointing to code&amp;gt; &amp;lt;addr pointing to addr pointing to code&amp;gt; &amp;lt;addr pointing to addr pointing to code&amp;gt; &amp;lt;addr pointing to addr pointing to code&amp;gt;&lt;/quote&gt;&lt;p&gt;This is where it gets pretty crazy. So now we've got a second level of indirection. Why on Earth would we do this?&lt;/p&gt;&lt;p&gt;Well, this allows us to store a separate "code interpreter" (or "inner interpreter") for different kinds of subroutines!&lt;/p&gt;&lt;p&gt;Instead of pointing directly at subroutines, these addresses point at interpreters. Talk about ultimate flexibility - every subroutine in an indirect threaded program can have its own custom interpreter for the rest of its instructions...each of which can also be threaded...or indirectly threaded!&lt;/p&gt;&lt;p&gt;But what calls all of these inner interpreters? An outer interpreter, of course! The outer interpreter is the part we actually interact with when we sit down to type at a Forth terminal.&lt;/p&gt;&lt;p&gt;In Forth, indirect threaded code is a list of addresses pointing to the "inner interpreter" portions of words, which execute the rest of the word. What types of inner interpreters could we have, anyway? Well, for example, we might have one kind of word that stores a string in memory and another that executes machine code. But the only limit is your imagination.&lt;/p&gt;&lt;p&gt;Make sense?&lt;/p&gt;&lt;p&gt;I personally would not have understood that explanation at all until much later in my journey (I know this because similar - probably better - explanations flew right over my head). No doubt you're faster than me at apprehending this stuff and are already halfway through implementing your own Forth based on these descriptions.&lt;/p&gt;&lt;p&gt;None of the rest of the material requires understanding any of the above, so please don't feel you need to fully grok (wikipedia.org) it before continuing. Indirect threading is an important part of Forth's history, but there are plenty of Forths that do not use it.&lt;/p&gt;&lt;p&gt;Threaded code was much more common in the days of yore.&lt;/p&gt;&lt;p&gt;It is very dense, compact on disk and in memory.&lt;/p&gt;&lt;p&gt;In addition to its compact storage, threaded code would have been even more efficient on the contemporary machines during Forth's gestation because calling subroutines often wasn't as simple as the &lt;code&gt;call&lt;/code&gt; instruction found on "modern" architectures.
        &lt;/p&gt;&lt;p&gt;Subroutine and procedure call support (clemson.edu) by Mark Smotherman explains:&lt;/p&gt;&lt;quote&gt;"1963 - Burroughs B5000 - A stack-based computer with support for block-structured languages like Algol. Parameters and return address are stored on the stack, but subroutine entry is a fairly complex operation."&lt;/quote&gt;&lt;p&gt;So the memory and performance improvements of this style of subroutine call were potentially very great indeed. This is one of the reasons for Forth's legendary reputation for high performance.&lt;/p&gt;&lt;p&gt;We'll revisit this topic from another angle soon. But if you're interested in these mechanics (and want to see the origin of the boxes and arrows drawings at the top of this section), check out this multi-part article series for The Computer Journal, MOVING FORTH Part 1: Design Decisions in the Forth Kernel (bradrodriguez.com), by Brad Rodriguez.&lt;/p&gt;&lt;p&gt;The important thing is that we've now fully traced the origins of Forth from a simple command interpreter to the full-blown interactive language, editor, operating system, and method of code storage and execution it became.&lt;/p&gt;&lt;p&gt;That's Forth's origin story.&lt;/p&gt;&lt;p&gt;This gives us the why.&lt;/p&gt;&lt;p&gt;At last! Now we can put it all together:&lt;/p&gt;&lt;p&gt;Forth is postfix because that's a natural order for the computer and lends itself to an incredibly minimalistic interpreter implementation: get the values, operate on them;&lt;/p&gt;&lt;p&gt;Forth is stack oriented because that's a compact and convenient way to store values without needing to add variables or name things;&lt;/p&gt;&lt;p&gt;Forth is concatenative because building a language that can operate as a string of words is incredibly flexible and can adapt to just about any programming style without any help from the language itself. (And it turns out this is especially true when you throw in higher-order functions);&lt;/p&gt;&lt;p&gt;Forth is interpreted because that is interactive and allows the programmer to make fast changes on the fly or simply "play" with the system. This is part of Forth's adaptability and flexibility;&lt;/p&gt;&lt;p&gt;Forth is self-hosting because you can bootstrap a Forth implementation from a handful of words implemented in assembly and then write the rest in Forth;&lt;/p&gt;&lt;p&gt;Forth is extremely compact because machines at the time had limited memory and this gave Forth an edge on other interpreters (and even compiled languages!) on mainframes and mini-computers.&lt;/p&gt;&lt;p&gt;Now that we have everything in historical context, I think it's much clearer why Forth exists and why it takes the peculiar form that it does.&lt;/p&gt;&lt;p&gt;None of this was planned. Chuck didn't sit down at a terminal in 1958 and conjure up Forth. Instead, he grew a system to serve his needs and to make use of new hardware as it was made available.&lt;/p&gt;&lt;p&gt;Reading about Forth's history is a wonderful way to understand what makes Forth special and what it's about.&lt;/p&gt;&lt;p&gt;But even knowing all of this, I was still a long way off from a true understanding of how this all comes together in an actual working system. I didn't really understand how it worked. And I didn't understand what Forth was actually like to use In other words, I still didn't understand Forth as a programming language.&lt;/p&gt;&lt;p&gt;Somewhere along the way, I came across these quotes...&lt;/p&gt;&lt;p&gt;"To understand Forth, you have to implement a Forth."&lt;/p&gt;&lt;p&gt;-- Somebody on the Internet&lt;/p&gt;&lt;p&gt;And&lt;/p&gt;&lt;p&gt;"Take a look at JonesForth."&lt;/p&gt;&lt;p&gt;-- Everybody on the Internet&lt;/p&gt;&lt;p&gt;I've mentioned it before, but I'll point it out again. Notice the phrasing "implement a Forth."&lt;/p&gt;&lt;p&gt;As we've established, Chuck Moore believes a Forth system is best when it is custom-tailored to the system and task at hand. So it should come as little surprise that writing your own Forth or Forth-like is entirely "par for the course" in any would-be-Forther's quest to discover the True Meaning of the language and enter the mystical realm where All is Revealed.&lt;/p&gt;&lt;p&gt;Well, what else could I do?&lt;/p&gt;&lt;p&gt;Having no other clear course of study, I decided to heed the wisdom of the crowd.&lt;/p&gt;&lt;p&gt;Presenting...&lt;/p&gt;&lt;p&gt;To really get to know it, I took Forth to bed with me.&lt;/p&gt;&lt;p&gt;I wrote Assembly Nights when I realized how much I was enjoying myself:&lt;/p&gt;&lt;quote&gt;"Over the last three months, I've developed an unusual little nighttime routine..."&lt;/quote&gt;&lt;p&gt;I prepared myself for dealing with the JonesForth source (i386 assembly language in the GNU GAS assembler) by learning some assembly and Linux ABI basics. JonesForth is 32-bit only and uses the Linux system call ("syscall") ABI directly.&lt;/p&gt;&lt;p&gt;Then I spent roughly a year porting JonesForth into a complete working copy in NASM assembler. (Yes, that's a "port" from one flavor of i386 asm to another.)&lt;/p&gt;&lt;p&gt;I did a tiny bit almost every night. A lot of it was debugging in GDB.&lt;/p&gt;&lt;p&gt;My NASM port of JonesForth: &lt;code&gt;nasmjf&lt;/code&gt;
    &lt;/p&gt;&lt;p&gt;Opening the third eye by (re)implementing Forth.&lt;/p&gt;&lt;p&gt;Here's the nasmjf web page&lt;/p&gt;&lt;p&gt;In the process of writing the port, I learned how a traditional indirect threaded Forth works.&lt;/p&gt;&lt;p&gt;And I learned that it takes time to absorb such a twisty-turny method of code execution.&lt;/p&gt;&lt;p&gt;Especially if the x86 assembly language tricks are new to you like they were for me.&lt;/p&gt;&lt;p&gt;JonesForth ascii art:&lt;/p&gt;&lt;p&gt;One of the first things you encounter when you open up the &lt;code&gt;jonesforth.S&lt;/code&gt; (a single file which contains the assembly
        language portion of JonesForth) are many ASCII art diagrams.
        &lt;/p&gt;&lt;p&gt;Richard W.M. Jones does an excellent job of walking you through the workings of the interpreter and explaining the i386 instruction set features he uses.&lt;/p&gt;&lt;p&gt;If the diagram above seems bewildering, I agree.&lt;/p&gt;&lt;p&gt;So, of course, I thought maybe I could do better...&lt;/p&gt;&lt;p&gt;Here's my attempt (from the &lt;code&gt;nasmjf&lt;/code&gt; source):
    

    &lt;/p&gt;&lt;p&gt;After I was done with my port, I tried to make an ASCII art diagram of my own to capture my new understanding. In fact, this is one of several.&lt;/p&gt;&lt;p&gt;With the benefit of the distance of time, it is clear to me that these things only make sense once you already understand them to some degree. But the act of making them is extremely useful for solidifying your understanding.&lt;/p&gt;&lt;p&gt;But wait, there's more!&lt;/p&gt;&lt;p&gt;Both ASCII art diagrams above are just part of the complete indirect threaded execution system. They're just showing how the "inner interpreter" works to execute Forth words.&lt;/p&gt;&lt;p&gt;Perhaps you recall from the section about indirect threaded code above that the second level of indirection allows different "interpreter" routines to execute different types of threaded subroutines? Well, that's all those two ASCII diagrams are trying show.&lt;/p&gt;&lt;p&gt;But when we say that Forth is an interpreted language, this is not what we're talking about. There's also the "outer interpreter" that the programmer interacts with.&lt;/p&gt;&lt;p&gt;The indirect threaded code is just the tip of the iceberg!&lt;/p&gt;&lt;p&gt;&lt;code&gt;nasmjf&lt;/code&gt; inner/outer interpreter diagram:
    
    
    &lt;/p&gt;&lt;p&gt;In the vector image I made above for nasmjf, I attempted to map out the whole thing in my own words.&lt;/p&gt;&lt;p&gt;If you take anything from this image, it's that &lt;code&gt;INTERPRET&lt;/code&gt; looks up words (functions) by name and calls
        them by executing the interpreter routine whose address is stored in
        the word (again, this is the indirect threading part). In turn, there
        may be any number of interpreters, but the three main types used in
        JonesForth are:
        &lt;/p&gt;&lt;code&gt;DOCOL&lt;/code&gt; interpreter.
                DOCOL executes the rest of the threaded code in the word,
                most of which is just a list of addresses, but some of
                which will be data. This is the "normal" kind of threaded
                subroutine.
            &lt;p&gt;But even knowing this only helps to explain how code starts executing. How does this type of Forth know what to run after a word is complete?&lt;/p&gt;&lt;p&gt;Ah, for that we have this:&lt;/p&gt;&lt;p&gt;To get from one code word to another requires a bit of assembly pasted at the end of each one. This is the NEXT macro. Here it is from &lt;code&gt;nasmjf&lt;/code&gt;:
    &lt;/p&gt;&lt;quote&gt;%macro NEXT 0 lodsd ; NEXT: Load from memory into eax, inc esi to point to next word. jmp [eax] ; Jump to whatever code we're now pointing at. %endmacro&lt;/quote&gt;&lt;p&gt;Notice the term "code word". That's the Forth term for words written in pure assembly language.&lt;/p&gt;&lt;p&gt;Every code word has this macro at the end. (Some Forths actually call a subroutine for this. JonesForth uses this two-line macro because the action is so efficient in i386 machine code.)&lt;/p&gt;&lt;p&gt;Remember the list of addresses in the explanation of "indirect threaded" code? This is how we execute them sequentially.&lt;/p&gt;&lt;p&gt;This implementation uses the i386 &lt;code&gt;lodsd&lt;/code&gt; instruction
        to take care of two operations in one: move a "double word"
        from memory into a register, and then update another register
        so that it points to the next "double" spot in memory.
        &lt;/p&gt;&lt;p&gt;(Rant: And a "double" is 32 bits on Intel chips for the really annoying reason that they kept the definition of "word" at 16 bits even as the platform moved to 32 and then 64-bit architecture. So "word" on Intel architectures is a completely meaningless thing that you just have to memorize as "16 bits" even though "word" is supposed to be the native data size of the architecture. And what's worse is that the tools for working with programs on Intel chips like GDB then refer to everything with the corresponding C names for everything, which naturally assumed that the architecture names would be based on reality. But they aren't. So terms like "double" and "long" are basically just absolutely worthless legacy garbage to memorize and useful only to C and Intel architecture veterans.)&lt;/p&gt;&lt;p&gt;Okay, so now the &lt;code&gt;eax&lt;/code&gt; register points to the next
        threaded subroutine address in memory. The &lt;code&gt;jmp&lt;/code&gt; starts
        executing whatever that points to, which will be the "inner interpreter"
        for that subroutine.
        &lt;/p&gt;&lt;p&gt;Got that?&lt;/p&gt;&lt;p&gt;A lot of moving parts, right?&lt;/p&gt;&lt;p&gt;There's more:&lt;/p&gt;&lt;p&gt;To get from one colon word to another uses a bit of assembly pasted at the end of each in a chunk called the EXIT macro. Here it is from &lt;code&gt;nasmjf&lt;/code&gt;:
    &lt;/p&gt;&lt;quote&gt;DEFCODE "EXIT",EXIT,0 POPRSP esi ; pop return stack into esi NEXT&lt;/quote&gt;&lt;p&gt;Remember, there's two fundamental types of words in a traditional Forth like JonesForth: "Code" words and "colon" words. Code words are primitives written in machine code. Colon words are the "regular" words actually written in the Forth language.&lt;/p&gt;&lt;p&gt;These "colon" words (so-named because they are assembled via the "COLON" compiler, which we'll talk about in a moment), all end in the so-called &lt;code&gt;EXIT&lt;/code&gt; macro.
        &lt;/p&gt;&lt;p&gt;The &lt;code&gt;EXIT&lt;/code&gt; macro handles the return stack.  Then
        there will be a &lt;code&gt;NEXT&lt;/code&gt; after that to conclude whatever code
        word primitive we were in (we're always in at least one because the
        "outer-most" interpreter is a code word primitive!), so the
        process we described above will automatically start where we left off
        at the "call site" of the word we
        just finished executing.
        &lt;/p&gt;&lt;p&gt;If you weren't lost before, surely this will do the trick?&lt;/p&gt;&lt;p&gt;I do have another attempt to explain how this all nests in a sort of indented pseudocode:&lt;/p&gt;&lt;p&gt;My comment in &lt;code&gt;nasmjf&lt;/code&gt; attempting to explain the
		execution of indirect threaded
        code as a nested
        sequence of NEXT and EXIT and QUIT:
    &lt;/p&gt;&lt;quote&gt;; QUIT (INTERPRET) ; * regular word ; DOCOL ; NEXT ; * regular word ; DOCOL (codeword ; NEXT ; * code word ; &amp;lt;machine code&amp;gt; ; NEXT ; * code word ; &amp;lt;machine code&amp;gt; ; NEXT ; EXIT ; NEXT ; EXIT ; NEXT ; QUIT (BRANCH -8 back to INTERPRET for more)&lt;/quote&gt;&lt;p&gt;This nested view of the process is as close as I've ever been to explaining (to myself) what the entire execution flow looks like at a high level.&lt;/p&gt;&lt;p&gt;I'm sure every Forth implementer has their own mental model.&lt;/p&gt;&lt;p&gt;You'll notice we didn't even talk about &lt;code&gt;QUIT&lt;/code&gt;.
        Other than the name, that one's not nearly as bad - it's really
        just the end of the outer interpreter loop.
        &lt;/p&gt;&lt;p&gt;(So, yeah, we have &lt;code&gt;EXIT&lt;/code&gt; and
        &lt;code&gt;QUIT&lt;/code&gt;, neither of which leave Forth... Hey, it was the
        1960s. Things were different then.)
    &lt;/p&gt;&lt;p&gt;Absolutely nothing else drives the flow of an indirect threaded Forth application: It's addresses stored in registers, a return stack, and a handful of assembly instructions at the end of each machine code word jumping to the next instruction.&lt;/p&gt;&lt;p&gt;It's like a delicate clockwork machine.&lt;/p&gt;&lt;p&gt;Don't you see how simple it is?&lt;/p&gt;&lt;p&gt;Historical note: The above "Crazy Chuck" drawing is a parody of a popular meme with actor Charlie Day's character in the episode "Sweet Dee Has a Heart Attack" from the show It's Always Sunny in Philadelphia:&lt;/p&gt;&lt;quote&gt;"Every day Pepe's mail's getting sent back to me. Pepe Silvia, Pepe Silvia, I look in the mail, this whole box is Pepe Silvia!"&lt;/quote&gt;&lt;p&gt;You, citizen of the distant future, will not have recognized this parody, but at least now you can look it up.&lt;/p&gt;&lt;p&gt;Forth is complex when taken as a whole. But it is made of tiny pieces, each of which is very simple. The concept was created over a period of years on very constrained systems. Each part created only as needed.&lt;/p&gt;&lt;p&gt;I'll repeat your question from before so you don't have to:&lt;/p&gt;&lt;p&gt;"Hey, wait! But if Chuck hates complexity so much, why did he use such a complex method for Forth?"&lt;/p&gt;&lt;p&gt;This is where the historical context is, once again, very revealing:&lt;/p&gt;&lt;p&gt;As we've seen, Charles H. Moore did not create Forth all at once in a single lightning bolt of inspiration. It began as a simple command interpreter and executor and grew from there. It has always consisted of tiny little parts, working together.&lt;/p&gt;&lt;p&gt;Each of these tiny parts is extremely simple on its own.&lt;/p&gt;&lt;p&gt;And each was added over a period of time as the need arose.&lt;/p&gt;&lt;p&gt;I think that's the genius of Forth: That all of these little pieces can work together to make a running system and yet still remain independent. You can learn each of these in isolation. You can replace them in isolation.&lt;/p&gt;&lt;p&gt;Ultimate flexibility and simplicity at the lowest level of the implementation comes at the cost of easy understanding at higher levels.&lt;/p&gt;&lt;p&gt;When growing a system like this, most of us would have thought bigger, Moore thought smaller.&lt;/p&gt;&lt;p&gt;Let's do the same. I've thrown the terms "code word" and "colon word" around a lot. I've explained them a bit, but we've never given a proper introduction.&lt;/p&gt;&lt;p&gt;Let's go small:&lt;/p&gt;&lt;p&gt;Again, Code words are primitives written in machine language supplied by the Forth implementation.&lt;/p&gt;&lt;p&gt;Let's see some real code words so we can de-mystify them once and for all. These are extremely simple and extremely concrete examples of actual NASM assembly language source from my &lt;code&gt;nasmjf&lt;/code&gt; port of JonesForth:
    &lt;/p&gt;&lt;p&gt;Small and simple:&lt;/p&gt;&lt;quote&gt;DEFCODE "SWAP",SWAP,0 pop eax pop ebx push eax push ebx NEXT&lt;/quote&gt;&lt;p&gt;Is that really SWAP? Yes, it really is! We're just telling the CPU to pop the two most recent values from the stack and then push them back in the opposite order.&lt;/p&gt;&lt;p&gt;(JonesForth uses the i386 call/return stack as a Forth parameter stack so we can use the native "pop" and "push" to make these operations easy. In exchange, we lose the ability to use "call" and "ret" for subroutines.)&lt;/p&gt;&lt;p&gt;The &lt;code&gt;DEFCODE&lt;/code&gt; macro is housekeeping - it creates the
        entry's header in the Forth word dictionary.
        &lt;/p&gt;&lt;p&gt;Notice the &lt;code&gt;NEXT&lt;/code&gt; macro we talked about previously?
        Remember, that's just another two lines of assembly pasted at the
        end of this routine.
    &lt;/p&gt;&lt;p&gt;Even Smaller:&lt;/p&gt;&lt;quote&gt;DEFCODE "DUP",DUP,0 mov eax, [esp] push eax NEXT&lt;/quote&gt;&lt;p&gt;We're down to just two instructions now! We move the value pointed at by the &lt;code&gt;esp&lt;/code&gt; register into eax and then push it onto the
        stack.  &lt;/p&gt;&lt;p&gt;To understand why this duplicates the top item on the stack, you need to know how the &lt;code&gt;esp&lt;/code&gt; register is used.
        Here's the relevant comment from the JonesForth source:
        &lt;/p&gt;&lt;quote&gt;"In this FORTH, we are using the normal stack pointer (%esp) for the parameter stack. We will use the i386's "other" stack pointer (%ebp, usually called the "frame pointer") for our return stack."&lt;/quote&gt;&lt;p&gt;Which means that &lt;code&gt;esp&lt;/code&gt; points to the current top of 
        the parameter stack. So pushing that value on the stack duplicates
        the top value. (This could also have been written more clearly with
        three instructions: one "pop" and two "push"es.)
    &lt;/p&gt;&lt;p&gt;The Smallest:&lt;/p&gt;&lt;quote&gt;DEFCODE "DROP",DROP,0 pop eax NEXT&lt;/quote&gt;&lt;p&gt;Now we have an entire Forth word defined as a single instruction! DROP just "removes" the top value from the stack. In this case, we pop it into the &lt;code&gt;eax&lt;/code&gt; register and then don't do
        anything with it, essentially throwing it away. (Alternatively, we
        could have decremented the &lt;code&gt;esp&lt;/code&gt; register, but in this case,
        the "pop" is both shorter and clearer.)
        &lt;/p&gt;&lt;p&gt;Now let's see these three words in action in a real Forth program that moves some real numbers around in memory...&lt;/p&gt;&lt;quote&gt;8 7 8 7 SWAP 7 8 DROP 7 DUP 7 7&lt;/quote&gt;&lt;p&gt;The code word primitives we've just defined are used by the rest of the Forth implementation to define colon words in the language itself. If you write Forth applications, your own colon words will probably use these heavily.&lt;/p&gt;&lt;p&gt;You can also call them interactively in the interpreter.&lt;/p&gt;&lt;p&gt;The above example shows what it might be like to use these three primitives right at the keyboard. The column on the right shows the state of the parameter stack after each line of input.&lt;/p&gt;&lt;p&gt;Apart from pushing the two numbers on the stack (&lt;code&gt;8 7&lt;/code&gt;)
        , we've now seen the assembly language code for the entire
        program shown above. That makes this pretty "bare metal" stuff, right?
        &lt;/p&gt;&lt;p&gt;Here's the walk-through:&lt;/p&gt;&lt;p&gt;Again, these instructions could exist in the definition of a word or you could type them interactively in the running Forth interpreter. The result is the same.&lt;/p&gt;&lt;p&gt;I think there's something pretty magical about realizing that typing these instructions is running specific machine code sequences exactly as they were entered. In this implementation, there's no optimizing compiler or virtual machine acting as middle-man. You really are communicating directly with the processor.&lt;/p&gt;&lt;p&gt;&lt;code&gt;nasmjf&lt;/code&gt; has 130 code words. Mostly for efficiency.

    &lt;/p&gt;&lt;p&gt;If you weren't already wondering, perhaps you are now: How many Forth words need to be defined in machine code to have a "bootstrappable" Forth system?&lt;/p&gt;&lt;p&gt;There are some theoretical minimums. But as you get down to an absurdly small number of instructions, the Forth code written with the primitives (to implement the rest of the language) approaches absurdly large amounts of convolutions that test the limits of both programmer ergonomics and computational inefficiency.&lt;/p&gt;&lt;p&gt;Check out this amazing article by Frank Sergeant: A 3-INSTRUCTION FORTH FOR EMBEDDED SYSTEMS WORK (utoh.org).&lt;/p&gt;&lt;quote&gt;"How many instructions does it take to make a Forth for target development work? Does memory grow on trees? Does the cost of the development system come out of your own pocket? A 3- instruction Forth makes Forth affordable for target systems with very limited memory. It can be brought up quickly on strange new hardware. You don't have to do without Forth because of memory or time limitations. It only takes 66 bytes for the Motorola MC68HC11. Full source is provided."&lt;/quote&gt;&lt;p&gt;You read that right: 66 bytes.&lt;/p&gt;&lt;p&gt;And later:&lt;/p&gt;&lt;quote&gt;"The absolute minimum the target must do, it seems to me, is fetch a byte, store a byte, and call a subroutine. Everything else can be done in high-level Forth on the host."&lt;/quote&gt;&lt;p&gt;Which reminds me, did you know there is such a thing as a one-instruction set computer (wikipedia.org)? And of course you can run Forth on them: 16-bit SUBLEQ eForth (github.com).&lt;/p&gt;&lt;p&gt;But that's nuts.&lt;/p&gt;&lt;p&gt;How about something a little more realistic?&lt;/p&gt;&lt;p&gt;&lt;code&gt;sectorforth&lt;/code&gt; has 10 code words.

    &lt;/p&gt;&lt;p&gt;Cesar Blum's sectorforth (github.com) is:&lt;/p&gt;&lt;quote&gt;"...a 16-bit x86 Forth that fits in a 512-byte boot sector. Inspiration to write sectorforth came from a 1996 Usenet thread."&lt;/quote&gt;&lt;p&gt;See? There's Usenet again. It wasn't just me reading all that lore.&lt;/p&gt;&lt;p&gt;The author's posting of the project to the Forth sub-reddit (reddit.com) has additional insight:&lt;/p&gt;&lt;quote&gt;"I've always been fascinated by the idea of having a minimal kernel of primitives from which "everything" can be built. Before Forth, I had only seen that in the form of Lisp's "Maxwell equations of software", which is cool, but always left me a little disappointed because it is too abstract to build something that you can actually interact with - you can't break out of its esoteric nature...&lt;lb/&gt;With Forth, however, you can start from almost nothing, and start adding things like ifs, loops, strings, etc., things that look more like your day-to-day programming. I find that there's a lot of beauty in that."&lt;/quote&gt;&lt;p&gt;Note: The statement about Maxwell's equations surely refers to Alan Kay's famous quote about LISP from A Conversation with Alan Kay (acm.org):&lt;/p&gt;&lt;quote&gt;"Yes, that was the big revelation to me when I was in graduate school - when I finally understood that the half page of code on the bottom of page 13 of the Lisp 1.5 manual was Lisp in itself. These were "Maxwell's Equations of Software!" This is the whole world of programming in a few lines that I can put my hand over."&lt;/quote&gt;&lt;p&gt;Okay, so we've talked about code words that are just chunks of machine code that can be called upon at any time.&lt;/p&gt;&lt;p&gt;Now let's see what colon words are all about...&lt;/p&gt;&lt;p&gt;Let's make one:&lt;/p&gt;&lt;quote&gt;: SDD SWAP DROP DUP ;&lt;/quote&gt;&lt;p&gt;A colon word is so-named because its definition begins with the "&lt;code&gt;:&lt;/code&gt;" character.
        &lt;/p&gt;&lt;p&gt;The example colon word definition above creates a new word called &lt;code&gt;SDD&lt;/code&gt; that is a composition of the three code words we
        defined earlier: &lt;code&gt;SWAP&lt;/code&gt;, &lt;code&gt;DROP&lt;/code&gt;, and
        &lt;code&gt;DUP&lt;/code&gt;.
        Perhaps the word "composition" brings to mind the concatenative
        terminology we explored earlier in this quest?
        &lt;/p&gt;&lt;p&gt;As this example demonstrates, colon words are defined entirely by other words, which may be code words or other colon words. You can also have numeric values, e.g. 8 and 7, which are handled by the interpreter.&lt;/p&gt;&lt;p&gt;(You can also have strings, which looks like data...but those are just input that happens to follow one of the special words, e.g. &lt;code&gt;."&lt;/code&gt; (dot quote), that knows how to handle the input!)
        &lt;/p&gt;&lt;p&gt;Let's see it in action:&lt;/p&gt;&lt;quote&gt;8 7 8 7 SDD 7 7&lt;/quote&gt;&lt;code&gt;SDD&lt;/code&gt; word is, of course,
        identical to calling the three separate words &lt;code&gt;SWAP&lt;/code&gt;,
        &lt;code&gt;DROP&lt;/code&gt;, and &lt;code&gt;DUP&lt;/code&gt; in sequence.
        &lt;p&gt;In indirect threaded code terms, this colon word has been "compiled" into the addresses of the "inner interpreters" for each of the three code words. But feel free to ignore this detail!&lt;/p&gt;&lt;p&gt;Let's demystify this further because the Forth "compiler" is probably much, much simpler than you'd think:&lt;/p&gt;&lt;p&gt;Here's what really happens when we enter this:&lt;/p&gt;&lt;quote&gt;: SDD SWAP DROP DUP ;&lt;/quote&gt;&lt;p&gt;Colon (&lt;code&gt;:&lt;/code&gt;) fetches the word name (SDD) and sets "compile mode".
	&lt;/p&gt;&lt;p&gt;Semicolon (&lt;code&gt;;&lt;/code&gt;) completes the word's entry in the dictionary and unsets "compile mode".
    
    &lt;/p&gt;&lt;p&gt;It might still be surprising that ":" is a Forth word.&lt;/p&gt;&lt;p&gt;It looks like the sort of thing we would call "syntax" in other programming languages, but it really isn't. It's a word.&lt;/p&gt;&lt;p&gt;You can even replace ":" with your own definition to extend or alter Forth to do your bidding!&lt;/p&gt;&lt;p&gt;It may be hard to fully grasp for a while, but Forth's only syntax is the whitespace between tokens of input.&lt;/p&gt;&lt;p&gt;Tokens are tokenized by a word called "WORD", which is an incredibly confusing overload of the term. Sorry.&lt;/p&gt;&lt;p&gt;(You'll also notice I've mentioned the term "dictionary" a couple times now. It's kind of obvious that a dictionary can hold words, but I haven't properly explained the Forth dictionary yet. Don't worry, we're almost there.)&lt;/p&gt;&lt;p&gt;Okay, so "&lt;code&gt;:&lt;/code&gt;" switches the "outer interpreter" into
        compile mode and &lt;code&gt;;&lt;/code&gt; switches it back. But what does
        that mean?
    &lt;/p&gt;&lt;p&gt;"Compiling" in Forth means putting one of two things into memory:&lt;/p&gt;&lt;p&gt;At its simplest, compiling is just like executing, but we're storing addresses instead of jumping to them.&lt;/p&gt;&lt;p&gt;Actually, that's understating the elegance and simplicity of how this works, which is one of the most mind-blowing things in Forth.&lt;/p&gt;&lt;p&gt;Forth uses the same interpreter to both compile and execute code!&lt;/p&gt;&lt;p&gt;In a traditional Forth, the interpreter executes words as you enter them. Unless you're in "compile mode", then it is compiling those words as addresses into memory on the fly as you enter them.&lt;/p&gt;&lt;p&gt;It's straight from the keyboard to memory.&lt;/p&gt;&lt;p&gt;To make this concrete, let's step through the example.&lt;/p&gt;&lt;p&gt;Here's our definition again:&lt;/p&gt;&lt;quote&gt;: SDD SWAP DROP DUP ;&lt;/quote&gt;&lt;p&gt;In "normal mode", the interpreter is executing everything as we enter it.&lt;/p&gt;&lt;p&gt;When the interpreter encouters the "&lt;code&gt;:&lt;/code&gt;" word, we're
        still in "normal mode", so it looks "&lt;code&gt;:&lt;/code&gt;" up in the
        dictionary, finds it, and executes the word. The definiton of
        "&lt;code&gt;:&lt;/code&gt;" will collect the name "SDD" and turn on the "compile
        mode" switch.
        &lt;/p&gt;&lt;p&gt;Now when the interpreter hits the "&lt;code&gt;SWAP&lt;/code&gt;" word, it will
        look up its address in the dictionary as usual, find it, and
        store the address in the next available memory slot where we
        compile new words (a very important built-in variable called
        "&lt;code&gt;HERE&lt;/code&gt;" keeps track of this memory position).
        &lt;/p&gt;&lt;p&gt;The same thing happens for "&lt;code&gt;DROP&lt;/code&gt;" and "&lt;code&gt;DUP&lt;/code&gt;".
        We're compiling as fast as we can type!
        &lt;/p&gt;&lt;p&gt;Then a bunch of really interesting things happen when the interpreter gets to "&lt;code&gt;;&lt;/code&gt;" (SEMICOLON).
        &lt;/p&gt;&lt;p&gt;First, "&lt;code&gt;;&lt;/code&gt;" is looked up and found in the dictionary and
        then...Hey, wait!
        Why isn't the address of the "&lt;code&gt;;&lt;/code&gt;" word
        also compiled into our new definition? That's a
            great question!
        &lt;/p&gt;&lt;p&gt;Time for another trick. One of the flags stored in a word's dictionary entry is the "immediate" flag. When this flag is turned on, the word is always executed immediately even in compile mode. The "&lt;code&gt;;&lt;/code&gt;" word is an immediate word, so it executes instead
        of being compiled.
        &lt;/p&gt;&lt;p&gt;(Ready to have your head turned inside-out? There are also tricks for compiling immediate words into word definitions! It's simple enough, but still pretty mind-bending stuff when you first encounter it.)&lt;/p&gt;&lt;p&gt;The definition of "&lt;code&gt;;&lt;/code&gt;" turns off compile mode. Then it
        does some housekeeping to complete the entry of the new
        &lt;code&gt;SDD&lt;/code&gt; word in the dictionary.
        &lt;/p&gt;&lt;p&gt;As soon as "&lt;code&gt;;&lt;/code&gt;" returns control to the outer
        interpreter, we're now sitting in normal mode again and our new
        &lt;code&gt;SDD&lt;/code&gt; word is available to be called directly or compiled
        into other words.
        &lt;/p&gt;&lt;p&gt;See what I mean? It's all made of these tiny little parts.&lt;/p&gt;&lt;p&gt;Each part is incredibly simple, but trying to explain how the parts fit together takes paragraphs of text.&lt;/p&gt;&lt;p&gt;Speaking of simple...&lt;/p&gt;&lt;p&gt;The tiny set of rules that govern the interpreter:&lt;/p&gt;&lt;p&gt;Let's look at our example code again. The first line runs, the second line compiles:&lt;/p&gt;&lt;quote&gt;8 7 SWAP DUP + : SDP SWAP DUP + ; 8 7 SDP&lt;/quote&gt;&lt;p&gt;It would be annoyingly redundant to walk through the two lines of Forth above step-by-step because they are nearly identical. The only difference is that the first line simply executes each word as it is encountered (SWAP, DUP, +). The second line compiles those three words into a new word called SDP (for "Swap Dup Plus"). The result of both lines is the same. (7 and 16 on the stack).&lt;/p&gt;&lt;p&gt;Only the numbers (8 and 7) and the spaces separating words have any special meaning to Forth's "outer" interpreter. Everything else is looked up in the dictionary.&lt;/p&gt;&lt;p&gt;Ah, but did you notice the order of the bullet points above? We check to see if a token is in the dictionary before we check to see if it is a numeric literal. Yes, even numbers are looked up in the dictionary first! Does that perhaps give you any ideas about that magic trick I promised at the start of this article? Don't worry, the trick is forthcoming.&lt;/p&gt;&lt;p&gt;Furthermore, input is not returned to the main Forth "outer" interpreter until a dictionary word completes executing. So there is absolutely no limit to the types of domain-specific language (wikipedia.org) you can create.&lt;/p&gt;&lt;p&gt;And if that weren't enough, You can also replace every single piece of the Forth interpreter itself. Remember, they're all independent little cogs in the machine. Forth is the ultimate freedom.&lt;/p&gt;&lt;p&gt;I've alluded to this in several different ways above, but I'll make a bold claim: Forth has the simplest syntax and therefore the simplest parser, interpreter, and compiler ever used in a "mainstream" general-purpose programming language.&lt;/p&gt;&lt;p&gt;Two other languages previously mentioned, Lisp and Tcl, are also famously syntactically minimalistic languages. People have written incredibly tiny implementations of each:&lt;/p&gt;&lt;p&gt;Mind you, both of these people (Justine "jart" Tunney and Salvatore "antirez" Sanfilippo) are incredible programmers, but these examples hint at what is possible.&lt;/p&gt;&lt;p&gt;But Forth surely takes the cake. Even a certified non-genius like myself can write an entire Forth interpreter in a couple hundred assembly instructions. (See "Meow5" below.)&lt;/p&gt;&lt;p&gt;Because of its extreme simplicity, tokenizing Forth can be done in a mere handful of assembly instructions on many processors.&lt;/p&gt;&lt;p&gt;And as mentioned, once you've written a Forth interpreter, you're well on your way to a working Forth compiler.&lt;/p&gt;&lt;p&gt;I've alluded to Forth's flexibility and extensibility on several different occasions now. But this is no mere party trick. Forth relies on the fact that you can do anything in Forth.&lt;/p&gt;&lt;p&gt;In the next example, we'll see how Forth implements control structures.&lt;/p&gt;&lt;p&gt;The definition of IF...THEN from jonesforth.f:&lt;/p&gt;&lt;quote&gt;: IF IMMEDIATE ' 0BRANCH , HERE @ 0 , ; : THEN IMMEDIATE DUP HERE @ SWAP - SWAP ! ;&lt;/quote&gt;&lt;p&gt;This right here is one of the most mind-blowing things about Forth, and a solid reason to title this, "The programming language that writes itself."&lt;/p&gt;&lt;p&gt;Even something as fundamental as &lt;code&gt;IF&lt;/code&gt; is defined in
        the language! Forth is not the only language that can do this, but
        few languages invite the programmer to participate so thoroughly
        in the inner workings as often or as joyfully as Forth.
        &lt;/p&gt;&lt;p&gt;Figuring out how the IF and THEN definitions above actually work is left as an exercise for the reader, but here's a brief explanation of the new words they use:&lt;/p&gt;&lt;quote&gt;' - gets the address of the word that follows, put on stack 0BRANCH - branch to the next value if the top of the stack has 0 , - 'compile' the current stack value to the memory at HERE @ - fetch value from address on stack, put value on stack ! - store to memory (stack contains address, then value)&lt;/quote&gt;&lt;p&gt;(By the way, I'll go on the record to say this: The early parts of bootstrapping Forth in Forth (at least the top 25% of jonesforth.f) is significantly more mind-bending than implementing the low-level code word definitions written in assembly language. In fact, any time I needed to return to the assembly, it was like a comforting blanket of simplicity compared to the logic puzzle of those Forth-in-Forth primitives!)&lt;/p&gt;&lt;p&gt;But, even seeing control structures like &lt;code&gt;IF..THEN&lt;/code&gt;
        implemented in the language may not have prepared you for seeing this
        next trick.
        &lt;/p&gt;&lt;p&gt;This should drive home the fact that Forth has almost no native syntax:&lt;/p&gt;&lt;p&gt;The definition of ( ) nested comments from jonesforth.f:&lt;/p&gt;&lt;quote&gt;: ( IMMEDIATE 1 BEGIN KEY DUP '(' = IF DROP 1+ ELSE ')' = IF 1- THEN THEN DUP 0= UNTIL DROP ; ( From now on we can use ( ... ) for comments. ...&lt;/quote&gt;&lt;p&gt;Yeah, you read that right. Even comments are implemented in the language! And you can re-define them or add your own kind of comments!&lt;/p&gt;&lt;p&gt;Some of you are soiling yourselves in excitement right now. Some of you are soiling yourselves in fear. We're all just sitting here in our own filth now.&lt;/p&gt;&lt;p&gt;And now, at last, we are ready to discuss the power of the Forth dictionary.&lt;/p&gt;&lt;p&gt;A Forth dictionary traditionally uses a linked list.&lt;/p&gt;&lt;p&gt;Word matching is done starting from the end (most recent entries) first, so:&lt;/p&gt;&lt;p&gt;It's not just minimalistic syntax. Arguably, the real reason Forth is so extensible is because of the dictionary.&lt;/p&gt;&lt;p&gt;As mentioned in the points above, more recent word definitions override older ones with the same name - the interpreter stops at the first match.&lt;/p&gt;&lt;p&gt;But as mentioned above, existing compiled words that use the old definitions are not affected because name of the old word, they've stored the address. The address of the old word still points to the old word.&lt;/p&gt;&lt;p&gt;You don't have to strictly replace. You can extend words by calling the original word from a new one with the same name!&lt;/p&gt;&lt;p&gt;You are perhaps wondering what happens if you attempt to make a recursive word. By default, ':' (COLON) marks the word currently being compiled into the dictionary as hidden or disabled so that previous definitions can be called, as mentioned. This is why we have a word called RECURSE which inserts a call to the current word within itself. Because all information in Forth is global (including the address of the current word being compiled, defining RECURSE is incredibly simple (just four words in the JonesForth definition).&lt;/p&gt;&lt;p&gt;Besides making new control structures or other types of extensions to the language, what else can we do with these abilities?&lt;/p&gt;&lt;p&gt;It's not just the language itself that is unusually malleable. Your program written in Forth can be flexible too.&lt;/p&gt;&lt;p&gt;Here is an example lifted and paraphrased from Thinking Forth by Leo Brodie.&lt;/p&gt;&lt;p&gt;Say we create a variable to hold a number of apples:&lt;/p&gt;&lt;quote&gt;VARIABLE APPLES 20 APPLES ! APPLES ? 20&lt;/quote&gt;&lt;p&gt;Forth variables put addresses on the stack.&lt;/p&gt;&lt;p&gt;Note: I have a physical copy of Thinking Forth because I think it's great. But the publishers have kindly made it available for free online: Thinking Forth (PDF) (forth.com)&lt;/p&gt;&lt;p&gt;Let's walk through the three lines above. Here's the first line:&lt;/p&gt;&lt;quote&gt;VARIABLE APPLES&lt;/quote&gt;&lt;p&gt;The VARIABLE word creates a new spot in free memory. Then it creates a new word in the dictionary called APPLES that pushes that particular memory address on the stack when it is called.&lt;/p&gt;&lt;p&gt;(Note that like ":", "VARIABLE" is grabbing the next token of input for use as a new dictionary name. This is possible because "the little cogs in the Forth machine" are available for any use you can think of. And one of those cogs is the word WORD, which gets the next token from the input stream. Both ":" and "VARIABLE" use WORD to do this, just like Forth's own outer interpreter!)&lt;/p&gt;&lt;p&gt;Okay, so we have a variable named APPLES now. The next line is:&lt;/p&gt;&lt;quote&gt;20 APPLES !&lt;/quote&gt;&lt;p&gt;This puts the value 20 on the stack, then the address for APPLES. The "!" (STORE) word stores the value 20 at the APPLES address. (In other words, "!" takes two values as input: an address and a value. It stores the value at that address.)&lt;/p&gt;&lt;p&gt;Conceptually, you can think of the above as &lt;code&gt;APPLES = 20&lt;/code&gt;
        in "normal" programming syntax.
        &lt;/p&gt;&lt;p&gt;And now the third line:&lt;/p&gt;&lt;quote&gt;APPLES ?&lt;/quote&gt;&lt;p&gt;This line prints the value stored at APPLES. The word "?" fetches a numeric value from an address and prints it (which pops the value off the stack again). Again, APPLES puts its address on the stack. So "?" simply takes an address from the stack as input for printing.&lt;/p&gt;&lt;p&gt;By the way, here's the entire definition of "?" in JonesForth:&lt;/p&gt;&lt;quote&gt;: ? @ . ;&lt;/quote&gt;&lt;p&gt;Look at how small that is! The only thing you need to know to understand this definition is that "@" (FETCH) pops an address from the stack and fetches the value stored at that address and puts the value on the stack. "." (DOT) pops a value from the stack and prints it as a number.&lt;/p&gt;&lt;p&gt;Okay, on with our example.&lt;/p&gt;&lt;p&gt;We're about to be dealt a terrible blow...&lt;/p&gt;&lt;p&gt;We pepper our program with this APPLES variable.&lt;/p&gt;&lt;p&gt;The application works perfectly for a couple years.&lt;/p&gt;&lt;p&gt;Then we are told that we must now keep track of two different kinds of apples: red and green. What to do?&lt;/p&gt;&lt;p&gt;Unfortunately, this is exactly the sort of conundrum we see in real life software all the time.&lt;/p&gt;&lt;p&gt;You knowingly prepared for all sorts of different quantities of apples, but it never occurred to anyone that we would need to track different types of apples.&lt;/p&gt;&lt;p&gt;This problem seems very bad. Do we have to completely re-write our application?&lt;/p&gt;&lt;p&gt;(Well, outside of this example, the correct answer might be "yes". Maybe this changes the whole "theory" of the program, in the Programming as Theory Building (ratfactor.com) sense. In which case, a re-write or big refactor of our apple counting program is likely the right answer. But for this example, we're assuming that we have thousands of lines of apple-handling functionality that will not need to change. We'll say that grouping the apples by color here is just an essential surface detail.)&lt;/p&gt;&lt;p&gt;All right, obviously we can't store two values in one variable and expect all of the existing code to still work. So what could we possibly do?&lt;/p&gt;&lt;p&gt;Here's a very clever and very Forth solution:&lt;/p&gt;&lt;p&gt;A new variable will store the current type of apples.&lt;/p&gt;&lt;quote&gt;VARIABLE COLOR&lt;/quote&gt;&lt;p&gt;As with "APPLES" above, VARIABLE creates a memory space and a new word called "COLOR" that puts the address of the memory space on the stack when it is called.&lt;/p&gt;&lt;p&gt;Next, we'll create a second new variable and a new colon word.&lt;/p&gt;&lt;p&gt;"REDS" will count red apples. Colon word "RED" sets the current type of apple to red: COLOR = REDS:&lt;/p&gt;&lt;quote&gt;VARIABLE REDS : RED REDS COLOR ! ;&lt;/quote&gt;&lt;p&gt;Remember, variables are also words in the dictionary, so we've created three additional words so far: COLOR, REDS, and RED.&lt;/p&gt;&lt;p&gt;(Only one of these, RED, is recognizably a function. But really all three of them are.)&lt;/p&gt;&lt;p&gt;As you may recall from earlier, "!" (STORE) takes two parameters, a value and an address, and stores the value at that address.&lt;/p&gt;&lt;quote&gt;void RED(){ COLOR = &amp;amp;REDS }&lt;/quote&gt;&lt;p&gt;And then...&lt;/p&gt;&lt;p&gt;Same for green.&lt;/p&gt;&lt;quote&gt;VARIABLE GREENS : GREEN GREENS COLOR ! ;&lt;/quote&gt;&lt;p&gt;We've added a total of five new words. The two new green ones are identical to the red ones above:&lt;/p&gt;&lt;p&gt;Here's the C equivalent of GREEN:&lt;/p&gt;&lt;quote&gt;void GREEN(){ COLOR = &amp;amp;GREENS }&lt;/quote&gt;&lt;p&gt;One more change...&lt;/p&gt;&lt;p&gt;Lastly, we change "APPLES" from a variable to a word that gets the current count by color:&lt;/p&gt;&lt;quote&gt;: APPLES COLOR @ ;&lt;/quote&gt;&lt;p&gt;As you may recall from earlier, "@" (FETCH) fetches the value stored in a variable and puts it on the stack.&lt;/p&gt;&lt;p&gt;So "APPLES" gets the value stored in COLOR and puts that on the stack.&lt;/p&gt;&lt;p&gt;The value stored in COLOR happens to be an address. That address happens to be the memory pointed to by either REDS or GREENS.&lt;/p&gt;&lt;p&gt;It would look like this C code:&lt;/p&gt;&lt;quote&gt;int *APPLES(){ return COLOR; }&lt;/quote&gt;&lt;p&gt;This "get the address of the address" stuff may sound super confusing. But working with memory addresses (aka "pointers") is how variables work in Forth, so to the adept Forth programmer, the idea of passing addresses around will be deeply ingrained and no big deal.&lt;/p&gt;&lt;p&gt;Okay, so we've got red and green apple counts. That much is clear. But surely there is still a lot of work ahead of us...&lt;/p&gt;&lt;p&gt;Now we have to re-write any use of APPLES, right?&lt;/p&gt;&lt;p&gt;Wrong! The use of APPLES is identical. The syntax hasn't changed one bit for any existing code. We just need to make sure we've set the right color.&lt;/p&gt;&lt;p&gt;Check it out:&lt;/p&gt;&lt;quote&gt;20 RED APPLES ! 30 GREEN APPLES ! GREEN APPLES ? 30 APPLES ? 30 RED APPLES ? 20&lt;/quote&gt;&lt;p&gt;All of the existing code that uses APPLES will still work exactly the same way with absolutely no modifications.&lt;/p&gt;&lt;p&gt;Furthermore, look at how English-like it reads to store &lt;code&gt;"20 RED APPLES !"&lt;/code&gt; or query &lt;code&gt;"GREEN APPLES ?"&lt;/code&gt;.
        &lt;/p&gt;&lt;p&gt;The key to understanding why this works is to remember that "APPLES" was already a word that put an address on the stack because that's how variables work in Forth. So when we changed it to a colon word that puts an address on the stack, it's no change at all. It's still doing the exact same thing. It just happens that the address will change depending on the active apple color.&lt;/p&gt;&lt;p&gt;At every single opportunity, Forth has taken the simplest (you might even say, laziest) and most flexible method for implementing a feature.&lt;/p&gt;&lt;p&gt;Wait, I hear a distant screaming:&lt;/p&gt;&lt;p&gt;"How could this possibly be okay?! You call this 'freedom', but I call it unchecked chaos material! This is not okay!"&lt;/p&gt;&lt;p&gt;Well, maybe.&lt;/p&gt;&lt;p&gt;But I think one reason this actually is okay, on a conceptual level, is that APPLES did not really change what it originally did.&lt;/p&gt;&lt;p&gt;Coming from the normal programming language world, we have clearly broken the abstraction: "APPLES" was a variable before, and now it's a function.&lt;/p&gt;&lt;p&gt;But you're not in the normal programming world anymore. Here, in Forth-land, a variable is a word that puts an address on the stack. And a function is also just a word.&lt;/p&gt;&lt;p&gt;"APPLES" is still a word that puts an address on the stack. There is no conceptual change at the language level. We did not break an abstraction because there was no abstraction to break.&lt;/p&gt;&lt;p&gt;Forth provides what you might call "atomic units of computing" at the language level. It is a language where you make the abstractions.&lt;/p&gt;&lt;p&gt;To Forth, it's all just words in a dictionary. "VARIABLE" is just another word you could have written yourself.&lt;/p&gt;&lt;p&gt;Do you see now why Chuck Moore rejects the standardization of Forth? It ossifies concepts like VARIABLE so they lose their flexibility.&lt;/p&gt;&lt;p&gt;The example above is also another demonstration of the way the language Forth "writes itself": a tiny handful of primitives can be used to bootstrap the rest of the language in the language itself. The enormous flexibility of the primitives allows nearly unbounded freedom.&lt;/p&gt;&lt;p&gt;I highly recommend implementing Forth (or porting it like I did) to understand how it works "under the hood."&lt;/p&gt;&lt;p&gt;By examining Forth from the ground floor at the assembly language level, I gained considerable confidence in my understanding of how all the moving parts fit together.&lt;/p&gt;&lt;p&gt;To be honest, it's difficult for me to imagine being to able to understand all the individual parts without going through this process. But everybody learns differently.&lt;/p&gt;&lt;p&gt;Implementing an interpreter teaches you almost nothing about how to write programs with that interpreter.&lt;/p&gt;&lt;p&gt;Knowing how a Forth system works is almost completely unrelated to knowing how to write programs in Forth.&lt;/p&gt;&lt;p&gt;You can know the spec for a language by heart, but still be clueless about writing good software in that language. It's like expecting a mastery of English grammar to make you a good novelist. They're entirely different skills.&lt;/p&gt;&lt;p&gt;Be also aware that most people on the Internet (including myself) are still complete newbies to actually creating software with Forth!&lt;/p&gt;&lt;p&gt;"I didn't create Forth, I discovered it."&lt;/p&gt;&lt;p&gt;-- Chuck, apocryphally&lt;/p&gt;&lt;p&gt;(I have been unable to find a source for the quote above. It probably comes from an interview.)&lt;/p&gt;&lt;p&gt;If Forth truly is a fundamental way to express computation, then it's sort of like Gödel and Herbrand's general recursive functions, Church's lambda calculus, Turing's theoretical machines, Post's canonical systems, and Schönfinkel and Curry's combinators. (I can hear furious objections warming up from a thousand armchairs...)&lt;/p&gt;&lt;p&gt;In fact, that's true of all programming languages, even the big, messy ones. Right? Any language that can express universal computation is...universally powerful; it can express anything that is computable.&lt;/p&gt;&lt;p&gt;But I think Forth belongs to a more rarified group. Forth is a fundamental type of programming language design. And I'm not alone in thinking so. For example, check out The seven programming ur-languages (madhadron.com).&lt;/p&gt;&lt;p&gt;I'll let philosophers angrily split hairs over what I just said above, but I think the principle is true. And it's true all the way down to the (lack of) syntax in the language.&lt;/p&gt;&lt;p&gt;Why do I believe this? Well...&lt;/p&gt;&lt;p&gt;Making &lt;code&gt;nasmjf&lt;/code&gt; gave me so many ideas, I had to try some
        experiments.
    &lt;/p&gt;&lt;p&gt;Forth is an amazing playground for ideas.&lt;/p&gt;&lt;p&gt;I was still keenly aware that my &lt;code&gt;nasmjf&lt;/code&gt; project to
        port JonesForth to NASM was still just a (very detailed) examination of
        a final artifact. I was not re-tracing Moore's footsteps, but
        imitating his work. In fine art terms, I made a "master copy" (training myself by
        copying the work of a master artist). In other words, I brought
        my sketchbook to the museum.
        &lt;/p&gt;&lt;p&gt;But what would happen if I tried making a painting of my very own?&lt;/p&gt;&lt;p&gt;An exercise in extreme concatenative programming where all code is concatenated (always inlined).&lt;/p&gt;&lt;p&gt;We explored what it means to be a "concatenative" programming language at the beginning of my journey above. In short, in a concatenative language, data implicitly flows from one function to another like a factory assembly line.&lt;/p&gt;&lt;p&gt;Like Forth, Meow5 happens to be concatenative because it uses the same "parameter stack" concept.&lt;/p&gt;&lt;p&gt;Unlike Forth or most other sane languages, Meow5 is a thought experiment taken too far. Specifically, the thought, "instead of threading function calls by storing their addresses, what if we just store a copy of the whole function?&lt;/p&gt;&lt;p&gt;In compiler parlance, this is "inlining", short for inline expansion (wikipedia.org). It is a common optimization technique for avoiding the overhead of a function call for small functions.&lt;/p&gt;&lt;p&gt;Let's use the word DROP for example. Remember when we looked at the assembly language source of the DROP code word? It was just a single assembly instruction:&lt;/p&gt;&lt;quote&gt;pop eax&lt;/quote&gt;&lt;p&gt;It would be incredibly silly to have several jumps to and from a single-instruction word!&lt;/p&gt;&lt;p&gt;(And, it comes as no surprise that "real" Forth implementations often inline small primitives such as DROP. Some even provide an INLINE word to allow the programmer to specify this explicitly.)&lt;/p&gt;&lt;p&gt;My question was: What if we do that for everything? At what point is this no longer a good idea? Obviously at some point, a function is too large to inline. But every code word in JonesForth was quite tiny by modern standards. With today's CPUs and their relatively enormous caches it seemed to me that you could take this inlining concept pretty far before it got ridiculous.&lt;/p&gt;&lt;p&gt;And wouldn't the CPU just love seeing all of those instructions executing in one straight and continuous sequence with no jumps? If I were a CPU, I would love it.&lt;/p&gt;&lt;p&gt;Plus, it would make compiling a stand-alone executable almost trivial because every word in a 100% inlined language would contain all of the machine code needed for that word.&lt;/p&gt;&lt;p&gt;Here is the canonical example:&lt;/p&gt;&lt;quote&gt;: meow "Meow." print ; meow Meow. : meow5 meow meow meow meow meow ; meow5 Meow.Meow.Meow.Meow.Meow.&lt;/quote&gt;&lt;p&gt;The idea is that &lt;code&gt;meow5&lt;/code&gt; compiles into five complete
        copies of &lt;code&gt;meow&lt;/code&gt;!
        &lt;/p&gt;&lt;p&gt;This example seems to be obviously naughty and wasteful. But I'm not a superscalar, out-of-order executing modern processor and neither are you. So the question remains: At what point does having a child function which includes a complete copy of every parent and grandparent and every ancestor function all the way back to the beginning spiral out of all sane proportions? Well, you could spend an afternoon figuring it out on paper, or you could be like me and spend the better part of a year writing an assembly program.&lt;/p&gt;&lt;p&gt;Spoiler alert: I consider Meow5 to be a delightful little failure. The problem isn't inlining machine code - that works great, and, indeed, the exported ELF executables from Meow5 work exactly as I imagined. The problem is data, and most conspicuously, data in the form of strings. Let's take the &lt;code&gt;meow&lt;/code&gt; word for example: You either have to copy the string
        "Meow." five times, once for each word that uses it, or go
        through some complicated hoops to track which word uses the string. And
        you have to do that two different ways: Its location in memory in the
        live interpreter and in it's destination in the stand-alone ELF memory
        layout. Either way, the purity and simplicity is lost, which was the
        whole point of the experiment. Also, it will come as no surprise that I
        later discovered that Forth implementations often have an INLINE word
        (as I mentioned above), which is a much better way to selectively
        instruct the compiler about which words you wish to copy entirely.  As a
        program, Meow5 is a failure. But as a project, it is a success
        because I learned a lot.
        &lt;/p&gt;&lt;p&gt;Think of it as an art project.&lt;/p&gt;&lt;p&gt;Anyway, the point is...&lt;/p&gt;&lt;p&gt;Despite attempting to go my own way, it's remarkable how many times Forth's solution was the path of least resistance.&lt;/p&gt;&lt;p&gt;Again and again I would say, "Aha! That's why."&lt;/p&gt;&lt;p&gt;First of all, you'll notice I ended up using ":" and ";" to define new functions. Forth makes liberal use of symbols and abbreviations, which can make it pretty hard to read. But I have to admit, ": ... ;" has grown on me. So I adopted that in Meow5. So that's probably the most visible thing. But that's just on the surface.&lt;/p&gt;&lt;p&gt;Secondly, using a postfix notation is absolutely the path of least resistance for a stack-based language - everything comes in the order expected by the language. So your interpreter can be shockingly simple because it can execute statements in the exact order it gets them.&lt;/p&gt;&lt;p&gt;(Side note: This is also how the PostScript (wikipedia.org) printer and display language works. The printer can begin printing as soon as it recieves the document because everything is defined in the order it is needed and never depends on later information. This can also be a disadvantage of PostScript for viewing documents on screens: You can't just render a page mid-document because styling and formatting controls must be read in their entirety from the start of the document to the current page in order to ensure you've got everything!)&lt;/p&gt;&lt;p&gt;I was determined to make things easy for myself, so I can say with some certainty that Forth is one of the most "minimum effort" languages you can imagine. If I could have thought of an easier (or lazier) way to do something, I would have done it!&lt;/p&gt;&lt;p&gt;There was just one place I decided to deviate from Forth even though I knew it would make implementation harder.&lt;/p&gt;&lt;p&gt;To make a string in Forth, you use the word &lt;code&gt;"&lt;/code&gt;, which
    needs a space after it to be seen as a word, which looks awkward:
    &lt;/p&gt;&lt;quote&gt;" Hello World."&lt;/quote&gt;&lt;p&gt;This has always bothered me. Chuck Moore even admits this in his unpublished book, Programming A Problem-Oriented Language (PDF) (forth.org) in the section titled 6.3 Character strings:&lt;/p&gt;&lt;quote&gt;"What does a character string look like? Of all the ways you might choose, one is completely natural:"ABCDEF...XYZ"A character string is enclosed in quotes. It can contain any character except a quote, specifically including spaces."&lt;/quote&gt;&lt;p&gt;Right! So by golly, that's what I would do in Meow5, like every sensible language!&lt;/p&gt;&lt;p&gt;Meow5 has this more natural quoting style:&lt;/p&gt;&lt;quote&gt;"Hello World."&lt;/quote&gt;&lt;p&gt;But the effects are cascading. And they limit flexibility.&lt;/p&gt;&lt;p&gt;If we keep reading Chuck's words, he explains what will happen if you do this:&lt;/p&gt;&lt;quote&gt;"We get in trouble immediately! How do you recognize a character string? By the leading quote, of course. But do you modify your word subroutine to recognize that quote? If you do so you may never use a leading quote for any other purpose. Much better that the quote is a word by itself, treated like any other dictionary entry, for it can then be re-defined. But words are terminated by spaces, and I still resist making quote an exception. So let's type character strings:" ABCDEF . . . XYZ"&lt;/quote&gt;&lt;p&gt;And he was right, of course.&lt;/p&gt;&lt;p&gt;I ended up having to put exceptions for the &lt;code&gt;"&lt;/code&gt; character in
        multiple places in the Meow5 interpreter, including my
        &lt;code&gt;get_token&lt;/code&gt; function, which serves the same purpose as
        the "WORD subroutine" Moore mentioned above.
        &lt;/p&gt;&lt;p&gt;And now all additional interpreter features have to work around or duplicate the special &lt;code&gt;"&lt;/code&gt; character handling!
        &lt;/p&gt;&lt;p&gt;It seems one can either follow Moore's advice or re-discover it for oneself. As for me, I always enjoy re-discovering things for myself. The best part is that "aha!" moment when I realize why things are the way they are.&lt;/p&gt;&lt;p&gt;Though, to flip this whole thing on its head, I actually think it was worth the extra effort, trouble, and loss of purity to do this! (I also included escape sequences, e.g. &lt;code&gt;\n&lt;/code&gt; and
        &lt;code&gt;\"&lt;/code&gt;, while I was at it.)
    &lt;/p&gt;&lt;p&gt;Another example of straying from Moore's advice and having to discover it for myself:&lt;/p&gt;&lt;p&gt;I decided to have some of my functions leave the stack alone after using the top value.&lt;/p&gt;&lt;p&gt;Some functions are mostly used to examine a value, but they pop that value off the stack. To keep working with the value, you have to do a DUP to duplicate it first.&lt;/p&gt;&lt;p&gt;Since I was sure I would always want to keep the value after these particular functions, it seemed very wasteful to have to do a DUP each time. Why not just peek at it and leave it on the stack?&lt;/p&gt;&lt;p&gt;Moore recommends just popping everything so you don't have to remember.&lt;/p&gt;&lt;p&gt;But I thought that was silly. So I went ahead and made some functions that just peek at the value and leave it on the stack.&lt;/p&gt;&lt;p&gt;But as you may have guessed, he was absolutely right.&lt;/p&gt;&lt;p&gt;Having some words pop the stack and some words peek was a nightmare. I kept forgetting which words did or didn't alter the stack and it kept causing problems. I completely regretted it and ended up making them all pop like Moore advised.&lt;/p&gt;&lt;p&gt;(Another option that occurred to me after I changed them all would have been to have a special naming scheme for non-popping words, which probably would have been fine, expect then I would have had to remember the name... so hassle either way.)&lt;/p&gt;&lt;p&gt;Now we have yet another reason for the title of this article.&lt;/p&gt;&lt;p&gt;Once you start down the Forth path... the rest just sort of "writes itself". Chuck Moore already found the path of least resistance.&lt;/p&gt;&lt;p&gt;To sum up the ways in which "Forth writes itself" so far, we have:&lt;/p&gt;&lt;p&gt;If you set out to make the simplest possible interpreter for a brand new CPU architecture, you might end up writing a Forth whether you want to or not.&lt;/p&gt;&lt;p&gt;Forth lets you define more Forth in Forth so you can Forth while you Forth. And the Forth editor is Forth and can be extended with Forth, so can Forth Forth in Forth Forth Forth Forth. (I'll let you figure out which of those are nouns, adjectives, or verbs and whether or not I have the right number of them.)&lt;/p&gt;&lt;p&gt;And if that weren't enough, Forths often contain assemblers so you can define additional code words in Forth, too so you never need to leave Forth once you're in it.&lt;/p&gt;&lt;p&gt;JonesForth has the stub of an in-Forth assembler near the end so we can see how one might work. Here's the comment introducing it:&lt;/p&gt;&lt;quote&gt;( ASSEMBLER CODE -------------------------------------------- This is just the outline of a simple assembler, allowing you to write FORTH primitives in assembly language. Assembly primitives begin ': NAME' in the normal way, but are ended with ;CODE. ;CODE updates the header so that the codeword isn't DOCOL, but points instead to the assembled code (in the DFA part of the word). We provide a convenience macro NEXT (you guessed what it does). However you don't need to use it because ;CODE will put a NEXT at the end of your word. The rest consists of some immediate words which expand into machine code appended to the definition of the word. Only a very tiny part of the i386 assembly space is covered, just enough to write a few assembler primitives below. )&lt;/quote&gt;&lt;p&gt;Just try not to go insane from the unlimited power.&lt;/p&gt;&lt;p&gt;And then there's this:&lt;/p&gt;&lt;p&gt;Hand-written 1Kb binary&lt;/p&gt;&lt;p&gt;This image comes from the PlanckForth repo (github.com). It's one of the most beautiful pieces of code I've ever seen. It's a complete ELF binary with a working Forth implementation that fits in less than 1Kb. As you can see, there's enough room left over for a description and copyright at the end.&lt;/p&gt;&lt;p&gt;The binary is stored as an ASCII hex represention that can be turned into a working binary using &lt;code&gt;xxd -r -c 8&lt;/code&gt;.
        &lt;/p&gt;&lt;p&gt;But the best part is &lt;code&gt;bootstrap.fs&lt;/code&gt;, written in
        line-noise-like operators and gradually becoming readable Forth
        after a couple hundred lines.
        &lt;/p&gt;&lt;p&gt;Thankfully, comments are one of the very first things implemented and it's almost like seeing bacteria spell out words in a petri dish:&lt;/p&gt;&lt;quote&gt;h@l@h@!h@C+h!k1k0-h@$k:k0-h@k1k0-+$h@C+h!ih@!h@C+h!kefh@!h@C+h!l! h@l@h@!h@C+h!k1k0-h@$k h@k1k0-+$h@C+h!ih@!h@C+h!kefh@!h@C+h!l! h@l@ h@!h@C+h! k1k0-h@$ k\h@k1k0-+$ h@C+h! i h@!h@C+h! kkf h@!h@C+h! kLf h@!h@C+h! k:k0- h@!h@C+h! k=f h@!h@C+h! kJf h@!h@C+h! k0k5-C* h@!h@C+h! kef h@!h@C+h! l! \ **Now we can use single-line comments!** \ planckforth - \ Copyright (C) 2021 nineties ...&lt;/quote&gt;&lt;p&gt;Incredible.&lt;/p&gt;&lt;p&gt;Another hand-written machine code Forth (in 1,000 bytes and with a Forth system in 1,000 lines!) is SmithForth (neocities.org) by David Smith. You can see and hear Smith walk through SmithForth on YouTube: SmithForth workings (youtube.com).&lt;/p&gt;&lt;p&gt;And as you may recall from earlier, Cesar Blum's sectorforth (github.com) is a mere 512 bytes!&lt;/p&gt;&lt;p&gt;There are almost as many Forth implementations as there are stars in the night sky.&lt;/p&gt;&lt;p&gt;Forth is an idea that has taken form in countless applications.&lt;/p&gt;&lt;p&gt;Many Forths are custom and home-grown.&lt;/p&gt;&lt;p&gt;But it has had great success in a huge variety of roles:&lt;/p&gt;&lt;p&gt;If it goes "beep" and "boop", someone has written a Forth for it!&lt;/p&gt;&lt;p&gt;For some notable uses, here are some starting points:&lt;/p&gt;&lt;p&gt;I think Open Firmware (wikipedia.org) is particularly interesting. It came, like many things, from the fine engineering minds at Sun Microsystems.&lt;/p&gt;&lt;quote&gt;"Being based upon an interactive programming language, Open Firmware can be used to efficiently test and bring up new hardware. It allows drivers to be written and tested interactively."&lt;/quote&gt;&lt;p&gt;Perhaps one of the most exciting uses of Open Firmware was the Space Shuttle ESN, which ran on a radiation-hardened UT69R000 (cpushack.com) processor! A paper on the ESN, Developing plug-and-play spacecraft systems: NASA Goddard Space Flight Center's (GSFC) Essential Services Node (ESN) (PDF) (zenodo.org) notes that:&lt;/p&gt;&lt;quote&gt;"Open Firmware can debug hardware,software, plug-in drivers, and even the firmware itself. Open Firmware provides interactive tools for debugging systems."&lt;/quote&gt;&lt;p&gt;By the way, I hope this brief mention of space technology has wet your appetite for more, because we're almost there!&lt;/p&gt;&lt;p&gt;But first, I have a couple more drawings of cool computers you should see. Perhaps you are aware of the huge variety of 1980s home computers?&lt;/p&gt;&lt;p&gt;Check these out:&lt;/p&gt;&lt;p&gt;Operating system: Forth.&lt;/p&gt;&lt;p&gt;OS and library of routines in 8 KB of ROM.&lt;/p&gt;&lt;p&gt;The onboard Forth was "Ten times faster than [interpreted] BASIC" and less than half the memory requirements."&lt;/p&gt;&lt;p&gt;(The quote above is from Popular Computing Weekly, 1982.)&lt;/p&gt;&lt;p&gt;The Jupiter Ace (wikipedia.org) was a British home computer of the early 1980s.&lt;/p&gt;&lt;p&gt;It has a fan-made website, the Jupiter ACE Archive from which has the page, What is a Jupiter ACE? (jupiter-ace.co.uk):&lt;/p&gt;&lt;quote&gt;"The major difference from the 'introductory computer' that was the ZX81, however, was that the Jupiter ACE's designers, from the outset, intended the machine to be for programmers: the machine came with Forth as its default programming language."&lt;/quote&gt;&lt;p&gt;That website has tons of resources. And if you're into that sort of thing, you also owe it to yourself to visit the "What is..." page linked above and then hover your mouse over the image of the ACE's circuit board. Every single IC, capacitor, and resistor is identified and explained in little tooltips!&lt;/p&gt;&lt;p&gt;It's not every day you see a programming language listed as an operating system for a computer. But you may recall that as early as the "IBM 1130 minicomputer at a big textiles manufacturer" era, Moore already had an editor and file management features. And you can certainly write hardware drivers in Forth if you have the right code word primitives. And as we'll see soon, there is absolutely no limit to how low-level Forth can go.&lt;/p&gt;&lt;p&gt;(There's also no limit to how high-level Forth can go. The book Thinking Forth by Leo Brodie, the same book from which we got the apples example above, is full of examples of applications written in very "English like" high-level words.)&lt;/p&gt;&lt;p&gt;The ACE never sold very many units, but it is prized by collectors today. I would take one.&lt;/p&gt;&lt;p&gt;The What is Forth? (jupiter-ace.co.uk) page has an excellent explanation of Forth in general, but especially as an all-encompassing computing system:&lt;/p&gt;&lt;quote&gt;"Classic Forth systems use no operating system. Instead of storing code in files, they store it as source-code in disk blocks written to physical disk addresses. This is more convenient than it sounds, because the numbers come to be familiar. Also, Forth programmers come to be intimately familiar with their disks' data structures, just by editing the disk. Forth systems use a single word "BLOCK" to translate the number of a 1K block of disk space into the address of a buffer containing the data. The Forth system automatically manages the buffers."&lt;/quote&gt;&lt;p&gt;Many of us fondly remember the boot-to-BASIC computers of the 1980s, but can you imagine growing up with the Jupiter ACE in your home and actually understanding it?&lt;/p&gt;&lt;p&gt;The ACE ran on the Zilog Z80 (wikipedia.org) CPU, which was incredibly popular at the time for low-power computers and has had an amazingly long life. It was used in the higher-end TI graphing calculators such as the TI-85 (wikipedia.org) I had in high school in 1996, which I spent many a happy afternoon programming in TI-BASIC.&lt;/p&gt;&lt;p&gt;Operating system: Forth.&lt;/p&gt;&lt;p&gt;OS, office suite, and programming environment in 256 KB of ROM.&lt;/p&gt;&lt;p&gt;Innovative interface by Jef Raskin.&lt;/p&gt;&lt;p&gt;Another computer with Forth as an operating system!&lt;/p&gt;&lt;p&gt;The Canon Cat (wikipedia.org) is a particularly fascinating machine for a number of different reasons, the primary of which is the keyboard-driven interface by UI pioneer Jef Raskin.&lt;/p&gt;&lt;p&gt;Raskin wrote a book titled The Humane Interface (wikipedia.org) with some provocative ideas that are probably very much worth re-visiting. For example, I like these two design rules:&lt;/p&gt;&lt;quote&gt;&lt;item&gt;Elimination of warning screens - modern software applications often ask the user "are you sure?" before some potentially harmful action; Raskin argues they are unhelpful because users tend to ignore them out of habit, and that having a universal undo eliminates the need for them.&lt;/item&gt;&lt;item&gt;Universal use of text - Raskin argues that graphic icons in software without any accompanying text are often cryptic to users.&lt;/item&gt;&lt;/quote&gt;&lt;p&gt;The Cat was the hardware and software incarnation of Raskin's design philosophies.&lt;/p&gt;&lt;p&gt;Also, you have to check out the picture of Jef with a little model of the Cat on his Wikipedia page: Jef Raskin (wikipedia.org). Direct link to the image: here (wikipedia.org).&lt;/p&gt;&lt;p&gt;The Cat ran on a Motorola 68000 (wikipedia.org) CPU, which was also used in the Apple Macintosh and was one of the first 32-bit processors, featuring 32-bit instruction set, registers, and non-segmented memory addressing.&lt;/p&gt;&lt;p&gt;Getting to the Forth interface doesn't seem to have been a top priority on the Cat.&lt;/p&gt;&lt;p&gt;Quoting Dwight Elvey at the DigiBarn computer museum, Canon Cat: Enabling Forth (digibarn.com), the process sounds a bit awkward:&lt;/p&gt;&lt;quote&gt;"Highlight the string: Enable Forth Language.&lt;lb/&gt;Then do: front, answer&lt;lb/&gt;Then: shift, usefront, space&lt;lb/&gt;You are now in Forth.&lt;lb/&gt;You need to do: -1 wheel! savesetup re&lt;lb/&gt;Front the editor, use the setup to set the keyboard to ascii so that you can type the characters &amp;lt; and &amp;gt; with shift , and shift .&lt;lb/&gt;Do a usefront disk.&lt;lb/&gt;It will save to the disk so that it will be ready the next boot with just the: shift, usefront, space to restart Forth.&lt;lb/&gt;To undo the Forth mode: Forth? off 0 wheel! re [sic everything]"&lt;/quote&gt;&lt;p&gt;(Note that "USE FRONT" is a dedicated key on the Canon Cat keyboard that lets you apply whatever function is printed on the front of another key on the keyboard. Clever, right? All of the Cat's interactions are performed through the keyboard like this.)&lt;/p&gt;&lt;p&gt;And if that process weren't enough to put you off, this warning seems particularly dire and, if anything, hilariously understated:&lt;/p&gt;&lt;quote&gt;"Use care while in Forth mode as usefront shift : will format the disk (a good idea to make a backup or at least remove the disk while experimenting)."&lt;/quote&gt;&lt;p&gt;But all of that makes it sound worse than it is. Thanks to modern streaming video technology, you can see Dwight Elvey boot up a cat and demonstrate it (youtube.com). As you can see, getting to the Forth interface is really not a lengthy process at all once you know what to do. Just a couple keystrokes. And the Cat is a more compact computer than I imagined from the pictures.&lt;/p&gt;&lt;p&gt;If you like industrial design or interesting computer interfaces, you owe it to yourself to check out the amazing pictures of Jef Raskin's team designing the Canon Cat (1985)! (digibarn.com)&lt;/p&gt;&lt;p&gt;If you want to see a bunch of pictures of a vintage Cat in amazing shape, check out Santo Nucifora's Canon Cat (vintagecomputer.ca).&lt;/p&gt;&lt;p&gt;If nothing else, just let this fact marinate in your head for a little bit: The Canon Cat had an OS, office suite, and programming environment in 256 KB of ROM. This document (not including the images) is almost exactly that size!&lt;/p&gt;&lt;p&gt;Okay, now we are ready for...&lt;/p&gt;&lt;p&gt;Easily one of the most exciting uses of Forth is space exploration because space is intrinsicly awesome.&lt;/p&gt;&lt;p&gt;We've already seen how Chuck Moore was intimately involved in programming ground-based radio telescopes. But Forth has also found its way into tons (literally and idiomatically) of actual space craft in outer space!&lt;/p&gt;&lt;p&gt;NASA is famous for having stringent rules about software that runs on spacecraft. Which makes sense, given the cost of these machines and the difficulty or even impossibility of getting to them to make fixes.&lt;/p&gt;&lt;p&gt;The list of projects using Forth at NASA compiled by James Rash in 2003 is too long to easily list here.&lt;/p&gt;&lt;p&gt;The image on the right is intentionally too small to read. As you can see, it's a big list.&lt;/p&gt;&lt;p&gt;The original NASA link has died, but the page was archived by the Wayback Machine at archive.org. There's a nice copy hosted here as well: Forth in Space Applications (forth.com).&lt;/p&gt;&lt;p&gt;I haven't found a list like this for the ESA, but the Philae lander featured below would be one very prominent example.&lt;/p&gt;&lt;p&gt;(By the way, though Forth isn't featured here, there's a fun overview of some CPUs used in various space missions: The CPUs of Spacecraft: Computers in Space (cpushack.com).)&lt;/p&gt;&lt;p&gt;(The image to the right is very tall. We need some more text for wider screens. So, while it's not about Forth,I won't miss this opportunity to mention one of my favorite computing-in-space books: Digital Apollo: Human and Machine in Spaceflight (mit.edu) by David Mindell. It will change how you look at the Apollo missions, computers in general, and the role of astronauts in space craft!)&lt;/p&gt;&lt;p&gt;"There is always great concern about software reliability, especially with flight software."&lt;/p&gt;&lt;p&gt;From the paper Forth in Space: Interfacing SSBUV, a Scientific Instrument, to the Space Shuttle (PDF) (acm.org) by Robert T. Caffrey et al:&lt;/p&gt;&lt;quote&gt;"There is always a great concern about software reliability, especially with flight software. The effects of a software error in flight could be dramatic. We were able to produce reliable software by writing a Forth routine on the PC, downloading the software, and testing it interactively. We varied the inputs to a routine and checked the ability of the routine to operate correctly under all conditions. As a result, during the STS-45 Shuttle mission, the SPAIM flight software worked perfectly and without any problems."&lt;/quote&gt;&lt;p&gt;Forth systems can be multi-tasking and this allowed the system to monitor itself. Each task had its own stack and a watchdog task could, for example, check the health of another task by monitoring the other task's stack. (Stack depth was found to be a good indication of task health. In other words, malfunctions would often cause the stack depth to grow unchecked.)&lt;/p&gt;&lt;quote&gt;"The ability of the Forth development system to debug hardware and software interfaces, model missing hardware, simulate system malfunctions, and support system integration dramatically helped in the quick generation of error-free software. The interactive, integrated and multitasking features of the Forth system proved to be the key elements in the success of the SPAIM systems development. Several techniques such as stack depth monitoring, address monitoring, cycle time monitoring, and error flag monitoring provided system checks during both the system integration process and the actual Shuttle mission."&lt;/quote&gt;&lt;p&gt;The interactive nature of the Forth system is again found to be not just very convenient, but also a massive productivity boost for all phases of programming, debugging, and testing.&lt;/p&gt;&lt;p&gt;The SPAIM system used a 16-bit Intel 87C196KC16 microcontroller, which is a MIL-SPEC member of the Intel MCS-96 (wikipedia.org) family. These started out as controllers for Ford engines in the 1970s. They continued to be made in various incarnations until 2007 and were often used in common devices such as hard drives, modems, and printers. Unlike many chips headed to space long-term, this one wasn't "rad-hard" (hardened against the effects of radiation).&lt;/p&gt;&lt;p&gt;Given the input of three-axis joystick commands, control a 50-foot long, six-joint arm with six different coordinate systems.&lt;/p&gt;&lt;p&gt;Entire system developed by one programmer in five weeks.&lt;/p&gt;&lt;p&gt;The Space Shuttle Robot Arm Simulator (forth.com) was a complex machine with some challenging requirements.&lt;/p&gt;&lt;p&gt;It turns out that you can't just use the same robot arm on the ground for simulations as the one that will go into space. For one thing, contending with gravity changes the requirements to such a degree that it's a completely different robot!&lt;/p&gt;&lt;quote&gt;&lt;p&gt;"The GSFC arm, for example, is designed to carry up to a thousand pound payload at its tip. In order to do this it uses a high pressure (4000 psi) hydraulic system rather than electric motors as on the RMS.&lt;/p&gt;&lt;p&gt;...&lt;/p&gt;&lt;p&gt;"Because of the completely different nature of the joint controls, the original RMS software was not usable except as a source of algorithms."&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;So the simulator arm could not work the same way, but it had to pretend it did.&lt;/p&gt;&lt;p&gt;You can see in my drawing that the arm lived in a full-scale simulated shuttle bay and was accompanied by an enormous model satellite. (That satellite looks like the Hubble Space Telescope to me, which seems plausible, given the dates on this project.)&lt;/p&gt;&lt;p&gt;Just listen to these I/O requirements:&lt;/p&gt;&lt;quote&gt;"The RMSS contains fourteen separate processes: one for each joint, one for each joystick, one for the digital display panel, a simulation process, a trending process, and several supervisory processes."&lt;/quote&gt;&lt;p&gt;But, as seems to be a trend with Forth-based space software, the work was impeccable:&lt;/p&gt;&lt;quote&gt;"Simulation testing was so thorough that when the arm software was installed on site, not a single change was made to the executive control algorithms."&lt;/quote&gt;&lt;p&gt;Does Forth imply excellence, or does excellence imply Forth? Ha ha.&lt;/p&gt;&lt;p&gt;Seriously, though, writing a system like that in five weeks is pretty astounding.&lt;/p&gt;&lt;p&gt;JPL's ground-based control software for shuttle SIR-A and SIR-B radar imaging instruments.&lt;/p&gt;&lt;p&gt;This section started off as an excuse to draw a Space Shuttle. But it's actually a great example of how a "live" interactive system can save a mission, even if the software itself hasn't been deployed into space.&lt;/p&gt;&lt;p&gt;The paper: Forth as the Basis for an Integrated Operations Environment for a Space Shuttle Scientific Experiment (PDF) (forth.com) describes a number of hardware failures that had to be overcome.&lt;/p&gt;&lt;quote&gt;"It was in the first day of data taking that we noticed the first problem..."&lt;/quote&gt;&lt;p&gt;The SIR-B's transmitting antenna had shorted, resulting in the expected 1000 watts of power being reduced to a faint 100 watts.&lt;/p&gt;&lt;quote&gt;"Since the returned echo was negligible as received by the SIR-B antenna it was decided to increase the gain of the receiver. The problem was in not understanding what had happened to cause the failure. [It] was not immediately apparent what the appropriate gain should be..."Forth-based, highly adaptable SMDOS to the rescue!&lt;/quote&gt;&lt;quote&gt;"No problem. With the advice of the radar engineers, the Forth module that was used to generate the display was quickly modified to produce a calibrated display. The gain of the receiver was increased until a perfect bell-shaped pattern again appeared on the display."Then a second hardware failure:&lt;/quote&gt;&lt;quote&gt;"This was only the start of our problems. A satellite on board failed to deploy properly. The shuttle had to remain in high orbit until the problem was resolved before it could fire its engines to descend to the orbit that had been planned for the SIR-B data taking. "Now the shuttle would not be in the planned orbit for data-taking. A second SMDOS adaptation fixed that.&lt;/quote&gt;&lt;p&gt;Then a third hardware problem with another antenna:&lt;/p&gt;&lt;quote&gt;"A bolt had sheared in the antenna's pointing mechenism and the KU band antenna was trashing around, threatening to destroy itself. It was necessary for an astronaut to exit the shuttle (EVA) in a spacesuit to pin the antenna down."&lt;/quote&gt;&lt;p&gt;Now the shuttle had to rotate to point at a relay satellite to gather data (to tape!) and then rotate towards Earth to transmit the recorded data, and repeat.&lt;/p&gt;&lt;quote&gt;"Of course this meant an entirely new data-taking strategy. Again the SMDOS computers were put to work displaying new plans for the stringent new conditions."&lt;/quote&gt;&lt;p&gt;They lost tons of data, of course, but at least they were able to salvage 20% of it by rotating and capturing and rotating and transmitting. None of which would have a been possible if they had not been able to modify the software on the spot.&lt;/p&gt;&lt;p&gt;Conclusion:&lt;/p&gt;&lt;quote&gt;"When the antenna feed failed and we realized that the software had to adapt to that failure, it was relatively easy given the interactive Forth enviroment to change the required module to meet the new specifications. This is clearly beyond the capabilites of most languages."&lt;/quote&gt;&lt;p&gt;Other systems are interactive, but Forth may be singularly unique in allowing complete freedom of modification in an interactive session.&lt;/p&gt;&lt;p&gt;Of course, this kind of freedom is double-edged sword if there ever was one. The implied danger of that powerful sword (plus the postfix notation) has been a hard sell in the corporate world.&lt;/p&gt;&lt;p&gt;So far, we've just seen Forth software in space. But it is often accompanied by Forth hardware.&lt;/p&gt;&lt;p&gt;Yup, Forth hardware. Introducing:&lt;/p&gt;&lt;p&gt;The Harris RTX2010 processor. Used in a ton of space applications.&lt;/p&gt;&lt;p&gt;Featuring:&lt;/p&gt;&lt;p&gt;The RTX2010 (wikipedia.org) and its predecessor, the RTX2000 account for a good portion of the use of Forth in the space industry. They run Forth natively.&lt;/p&gt;&lt;p&gt;The use of the RTX line in space may not be soley due to a particular love for Forth per se, but because of the specific attractive properties of these processors - very low latency and the ability to quickly process the floating point mathematical operations needed for neat space stuff like navigation and thruster control. Either way, the philosophy of Forth embedded in this hardware is suitable for the extreme environments in which they operate.&lt;/p&gt;&lt;p&gt;Largely because of the stack-based design, the RTX 2000 and 2010 have very compact machine code. Subroutines calls take only a single cycle and returns are free! All branches take exactly one cycle as well.&lt;/p&gt;&lt;p&gt;They are also brilliantly minimalistic designs. The entire RTX2000 instruction set fits on a single page. See the first PDF link below:&lt;/p&gt;&lt;p&gt;So what kind of spacecraft use these Forth-native processors?&lt;/p&gt;&lt;p&gt;Let's look at a specific pair of spacecraft:&lt;/p&gt;&lt;p&gt;First mission to send a spaceship to orbit a comet and then deliver a lander to the comet's surface!&lt;/p&gt;&lt;p&gt;The Rosetta spacecraft's Ion and Electron Sensor instrument used a Harris RTX2010.&lt;/p&gt;&lt;p&gt;The Philae lander used two Harris RTX2010s for complete system control (CDMS) and two more to control its landing system (ADS).&lt;/p&gt;&lt;p&gt;The ESA's Rosetta mission (esa.int) was hugely ambitious: Send a spacecraft to rendezvous with and then follow a comet around the Sun, deploy the Philae lander to the surface by dropping it into the comet's gravity well, observe the lander as it shoots harpoons into the icy surface of the comet to keep from bouncing back out into space, then relay the lander's communication from the surface back to distant Earth, 28 minutes away at the speed of light.&lt;/p&gt;&lt;p&gt;Rosetta traveled in the Solar System for a full decade (2004 to 2014) before meeting up with comet 67P/"Churyumov-Gerasimenko". (67P is 4km wide and orbits the sun every six and a half years.)&lt;/p&gt;&lt;p&gt;Rosetta orbited the comet for three months and then deployed the Philae lander to the surface of the comet.&lt;/p&gt;&lt;p&gt;Both craft contained a full laboratory of advanced scientific instruments (11 on Rosetta, 9 on Philae) including some that doubled as high-resolution cameras with images suitable for humans to view. The whole mission (wikipedia.org) is worth reading about. There are some fantastic images and animations to be seen on the mission page and on the comet's own page (wikipedia.org).&lt;/p&gt;&lt;p&gt;Often described as being "the size of a washing machine," the Philae (wikipedia.org) lander pushed away from Rosetta's orbit to drop to the surface of 67p.&lt;/p&gt;&lt;p&gt;The picture at the right was taken from Rosetta's OSIRIS imager as Philae fell slowly away from the orbiter.&lt;/p&gt;&lt;p&gt;Because the comet's gravitational pull is so small (huge boulders have been observed moving around on its surface), a pair of harpoons were meant to fire into the surface of the comet and hold the lander down. These did not deploy (possibly a mechanical failure) and a landing thruster also failed, so Philae ended up having a long, low-gravity tumble on the surface.&lt;/p&gt;&lt;p&gt;It's been speculated that the harpoon failure actually saved Philae from an even more exciting trip because studies of the surface found it to be harder than expected. It might have launched itself away rather than anchoring! As it was, the lander bounced with a force that was just shy of escaping the comet's gravitational pull entirely. It rose a full kilometer above the surface before slowly returning for another two bounces to its final resting spot.&lt;/p&gt;&lt;p&gt;A pair of Harris RTX2010s controlled Philae's Active Descent System. Check out Here comes Philae! Powered by an RTX2010 (cpushack.com):&lt;/p&gt;&lt;quote&gt;"Why was the RTX2010 chosen? Simply put the RTX2010 is the lowest power budget processor available that is radiation hardened, and powerful enough to handle the complex landing procedure. Philae runs on batteries for the first phase of its mission (later it will switch to solar/back up batteries) so the power budget is critical. The RTX2010 is a Forth based stack processor which allows for very efficient coding, again useful for a low power budget."&lt;/quote&gt;&lt;p&gt;Here is more information (with pictures!) about the physical design and components in the Philae control system: Command and Data Management Subsystem (CDMS) of the Rosetta Lander (Philae) (sgf.hu).&lt;/p&gt;&lt;quote&gt;"Harris RTX2010 processor has been selected for the DPU boards because it is the lowest power consuming, space qualified, radiation hardened, 16-bit processor with features to provide so complicated functions as the CDMS has to perform. It is a stack based, Forth language oriented processor with an exotic and challenging instruction set. CDMS is a real-time control and data acquisition system, and it has to process tasks in parallel. Therefore, a real-time, pre-emptive multitasking operating system has been developed to run application tasks executing the required functions in parallel."&lt;/quote&gt;&lt;p&gt;And here is the lander's Active Descent System (ADS) QM User Manual (spyr.ch) which has way more detail about this computer system, including a number of details about the Forth software:&lt;/p&gt;&lt;quote&gt;"After resetting the subsystem (power-on reset), the bootstrap sets up the Forth environment, copies the firmware from PROM to RAM and disables the PROM for further access.&lt;lb/&gt;After this, the main word Do-App is called from the Forth system immediately after setup. You can find the main word Do-App in the file app.fth (see part II). Do-App calls Init-App, which itself calls other initilisation words like Init-ADS. Then the application enters the main loop. In the main loop the following actions are performed:&lt;item&gt;reset the watchdog (watchdog is enabled for the QM)&lt;/item&gt;&lt;item&gt;put the data into the HK registers&lt;/item&gt;&lt;item&gt;get the data from the ADC handler&lt;/item&gt;&lt;item&gt;process CDMS requests"&lt;/item&gt;&lt;/quote&gt;&lt;p&gt;Despite the unfortunate landing, which put Philae in too much shadow to get as much solar energy as hoped and at an angle that made communication with Rosetta difficult, Philae was still robust enough to perform "80%" of its scientific mission, which is pretty amazing.&lt;/p&gt;&lt;p&gt;A picture taken by the Philae lander as it lay on its side, enjoying some sunlight on one of its feet:&lt;/p&gt;&lt;p&gt;Just look at that. A picture from the surface of a comet.&lt;/p&gt;&lt;p&gt;For the full-size image, more info, and links to the other CIVA camera images, see CIVA camera 1 view (esa.int).&lt;/p&gt;&lt;p&gt;There is a ton of fascinating information about the landing and the ESA's detective work to figure out where Philae actually ended up:&lt;/p&gt;&lt;p&gt;Philae eventually ran out of power and stopped communicating with Rosetta.&lt;/p&gt;&lt;p&gt;This is one of the final images taken by the Rosetta orbiter as it made the "hard descent" (controlled crash landing) to the surface of comet 67p:&lt;/p&gt;&lt;p&gt;The image and a description are here: Final Descent Images from Rosetta Spacecraft (nasa.gov).&lt;/p&gt;&lt;quote&gt;"The decision to end the mission on the surface is a result of Rosetta and the comet heading out beyond the orbit of Jupiter again. Farther from the sun than Rosetta had ever journeyed before, there would be little power to operate the craft. Mission operators were also faced with an imminent month-long period when the sun is close to the line-of-sight between Earth and Rosetta, meaning communications with the craft would have become increasingly more difficult."&lt;/quote&gt;&lt;p&gt;By the way, the ESA has a nice summary of the computer hardware used by the OSIRIS camera on Rosetta which was used to take the surface image above and also the little picture of the descending lander further above. Optical, Spectroscopic, and Infrared Remote Imaging System (esa.int).&lt;/p&gt;&lt;p&gt;After finishing the first draft of this article, I was so excited about the Rosetta mission that I ended up ordering and reading Rosetta: The Remarkable Story of Europe's Comet Explorer by Peter Bond. It's a bit of a dry read, but the subject matter is thrilling nonetheless and the coverage is thorough. I recommend it if you want to know a lot more about this awesome engineering and scientific milestone. (It does not, sadly, mention Forth.)&lt;/p&gt;&lt;p&gt;Rabbit Hole Alert: This takes us away from Forth for a moment, but learning about the Virtuoso RTOS (real-time operating system) eventually leads to a deep, deep Wikipedia rabbit hole that takes you on a journey to the Inmos processors, Hoare's CSP, the occam programming language, the HeliOS parallel computing operating system, and the concept of the "transputer" microprocessors.&lt;/p&gt;&lt;p&gt;Transputers use whole processors as building blocks for a parallel computer in the same way transistors are used as the building blocks for processors. (Thus, transputer = "transistor computer," you see?) They were mostly featured in supercomputers, but they also saw some industrial controller use and there was even an Atari Transputer Workstation, ATW-800.&lt;/p&gt;&lt;p&gt;(I've intentionally not linked to any of these things here because you'll disappear into that hole and never see the end of this document, which would be very sad. Also, I mention "transputers" again one more time below and you wouldn't want to miss that.)&lt;/p&gt;&lt;p&gt;The Rosetta orbiter and Philae lander now rest silently on the surface of 67p, where they will no doubt stay for billions of years or until future comet tourists pick them up and put them in a trophy room, whichever comes first.&lt;/p&gt;&lt;p&gt;"...Space probes written in Lisp and Forth have been debugged while off world... If they had proven their programs correct by construction, shipped them into space, and then found out their spec was wrong, they would have just had some dead junk on Mars. But what these guys had was the ability to fix things while they are running on space probes... In addition, the spec is always wrong!"&lt;/p&gt;&lt;p&gt;-- Jack Rusher, Stop Writing Dead Programs (talk given at Strange Loop 2022)&lt;/p&gt;&lt;p&gt;Here's the talk: "Stop Writing Dead Programs" by Jack Rusher (Strange Loop 2022) (youtube.com).&lt;/p&gt;&lt;p&gt;You've got 43 minutes to watch it. I'm timing you. Don't get distracted by other YouTube suggestions. Come back here. I'm waiting.&lt;/p&gt;&lt;p&gt;Or better yet, check out Jack's awesome transcript, which was super helpful when I wanted to re-find the above quote: Stop Writing Dead Programs. (jackrusher.com).&lt;/p&gt;&lt;p&gt;In his transcript, he notes:&lt;/p&gt;&lt;quote&gt;"Had I had more time, I would have done an entire series of slides on FORTH. It's a tiny language that combines interactive development, expressive metaprogramming, and tremendous machine sympathy. I've shipped embedded systems, bootloaders, and other close-to-the-metal software in FORTH."&lt;/quote&gt;&lt;p&gt;I was extremely interested in hearing about Forth systems being updated in space, but had a heck of a time finding any. I finally found one on a page that is otherwise largely dedicated to Lisp's use at the Jet Propulsion Labs: 1992-1993 - Miscellaneous stories (sourceforge.io) on the amazing, sprawling site for the Mecrisp-Stellaris Forth (which runs on various non-x86 CPUs):&lt;/p&gt;&lt;quote&gt;"Also in 1993 I used MCL to help generate a code patch for the Gallileo magnetometer. The magnetometer had an RCA1802 processor, 2k each of RAM and ROM, and was programmed in Forth using a development system that ran on a long-since-decommissioned Apple II. The instrument had developed a bad memory byte right in the middle of the code. The code needed to be patched to not use this bad byte. The magnetometer team had originally estimated that resurrecting the development environment and generating the code patch would take so long that they were not even going to attempt it. Using Lisp I wrote from scratch a Forth development environment for the instrument (including a simulator for the hardware) and used it to generate the patch. The whole project took just under 3 months of part-time work."&lt;/quote&gt;&lt;p&gt;(If anyone has any leads to other notable Forth uses in space, I'd love to hear about them.)&lt;/p&gt;&lt;p&gt;When we defeat the alien kill-bots and reprogram them, it will surely be with a Forth of some sort.&lt;/p&gt;&lt;p&gt;In the background, one of the Invader machines lies crumpled and smoking amidst ruins. This was one of Earth's great cities.&lt;/p&gt;&lt;p&gt;Stomping towards us with its mechanical arms raised in victory, is another Invader. But this one is different. The tell-tale giveaway is the opening in its protective head dome. And is that a flag? Why yes, it is!&lt;/p&gt;&lt;p&gt;At great cost, humans managed to trap one of the Invaders long enough penetrate its outer defenses, while otherwise leaving the machine unharmed and operable.&lt;/p&gt;&lt;p&gt;Working feverishly against a doomsday clock, they burrowed deep into the electrical heart of the machine, identifying and classifying its alien functions until they understood it well enough to attempt an interface.&lt;/p&gt;&lt;p&gt;A bus protocol was decoded. Programming work began.&lt;/p&gt;&lt;p&gt;It went poorly. The aliens had unthinkably bizarre notions of generalized computing that defied all known patterns of software.&lt;/p&gt;&lt;p&gt;Everything had to be done with agonizing labor, stringing sequences of raw bus messages together in hopes of getting a correct response.&lt;/p&gt;&lt;p&gt;But then someone had the bright idea to bootstrap a Forth from the known instruction sequences. With this, they could write a bare-bones interpreter. And, at last, they could experiment quickly and safely.&lt;/p&gt;&lt;p&gt;Days later, an arm moved. Then they crushed a barrel with a gripper claw:&lt;/p&gt;&lt;quote&gt;BARREL OBJECT-ID VISION TARGET 133 L-ARM-FWD 14 L-CLAW-OPEN 25 L-ARM FWD 14 L-CLAW CLOSE&lt;/quote&gt;&lt;p&gt;Then a first four-legged step. Then 20 steps:&lt;/p&gt;&lt;quote&gt;PREP-QUAD-LEGS 20 STRIDE-LOOP&lt;/quote&gt;&lt;p&gt;As ravaged fighters looked on in amazement, "Defender-1" burst from the old brick warehouse and, in a terrific crash, it toppled another Invader as it was passing by on patrol.&lt;/p&gt;&lt;p&gt;The machines grappled for a moment and it looked as if Defender-1's clumsy movements would be no match for the alien, even from a superior position.&lt;/p&gt;&lt;p&gt;But humans had decoded all of the weapon systems by then and a special word had been prepared for this moment:&lt;/p&gt;&lt;quote&gt;: KILL 100 BEAM-LEVEL BOT OBJECT-ID VISION TARGET L-BEAM FIRE-FULL R-BEAM FIRE-FULL ;&lt;/quote&gt;&lt;p&gt;Twin blinding beams of energy struck the enemy full in the torso and instantly turned its mechanical guts into sizzling plasma. After a moment of silence, a single cheer rose up from a doorway nearby and was soon joined by a hundred different voices from places of concealment in the ruined buildings.&lt;/p&gt;&lt;p&gt;Now the humans had the upper hand at last! Other Invader machines were disabled or captured. Defender-1 was joined by Defender-2, and then 3, 4, 5, and more!&lt;/p&gt;&lt;p&gt;Software was passed by sneaker-net and by shortwave packet radio. City by city, Earth took back control. And along with victory, word of the One True Language spread across the land. Flags were raised in honor of its original discoverer, Chuck Moore.&lt;/p&gt;&lt;p&gt;Where other abstractions had failed, the universal machine truth of Forth had succeeded.&lt;/p&gt;&lt;p&gt;Here's a "family tree" of some notable Forths:&lt;/p&gt;&lt;p&gt;Obviously the graphic is unreadably tiny. For the full-size original and the gForth program used to create it, check out:&lt;/p&gt;&lt;p&gt;Forth Family Tree and Timeline (complang.tuwien.ac.at).&lt;/p&gt;&lt;p&gt;One of the hardest things about trying to learn "Forth" is realizing that there is no single implementation that can lay sole claim to that name. As we've seen, some of Chuck's first Forths pre-date the name entirely.&lt;/p&gt;&lt;p&gt;There are Forth standards dating back to the original ANS Forth document and continuing with the Forth 2012 Standard and Forth200x committee (forth-standard.org).&lt;/p&gt;&lt;p&gt;Forths have shared concepts. There are many common words, certainly, but purpose-built Forths will have their own special vocabularies.&lt;/p&gt;&lt;p&gt;Also, it is true that making Forths is at least as fun as using them.&lt;/p&gt;&lt;p&gt;The forest of computing is peppered with hobby Forths. They grow where nothing else can survive. They flourish in the sun and in the shade. Each one is a little glittering jewel.&lt;/p&gt;&lt;p&gt;Charles H. Moore founded Forth, Inc in 1973. He's continued to port Forth to various systems ever since. But he's never stopped inventing.&lt;/p&gt;&lt;p&gt;I drew this image of Chuck from a photo in this amazing quote collection, Moore Forth: Chuck Moore's Comments on Forth (ultratechnology.com) compiled by Jeff Fox.&lt;/p&gt;&lt;p&gt;You'll notice I added some color to my drawing for this one, and that's because I'm pretty sure that what we're seeing on Chuck's monitor is...&lt;/p&gt;&lt;p&gt;colorForth&lt;/p&gt;&lt;p&gt;The above screenshot is actually from a page about etherForth, (etherforth.org), which is a colorForth written for GA144 chips. (Don't look up those chips yet unless you want a spoiler for what's coming in a moment below!)&lt;/p&gt;&lt;p&gt;What the heck are we looking at here?&lt;/p&gt;&lt;p&gt;So, colorForth (wikipedia.org) is:&lt;/p&gt;&lt;quote&gt;"An idiosyncratic programming environment, the colors simplify Forth's semantics, speed compiling, and are said to aid Moore's own poor eyesight: colorForth uses different colors in its source code (replacing some of the punctuation in standard Forth) to determine how different words are treated."&lt;/quote&gt;&lt;p&gt;And, of course:&lt;/p&gt;&lt;quote&gt;"The language comes with its own tiny (63K) operating system. Practically everything is stored as source code and compiled when needed. The current colorForth environment is limited to running on Pentium grade PCs with limited support for lowest-common-denominator motherboards, AGP video, disk, and network hardware."&lt;/quote&gt;&lt;p&gt;But the best description of colorForth and its strengths come from Chuck Moore himself in an interview in 2009, Chuck Moore: Geek of the Week (red-gate.com):&lt;/p&gt;&lt;quote&gt;"Forth has some ugly punctuation that colorForth replaces by coloring source code. Each word has a tag that indicates function; it also determines color. This seems a small point, but it encourages the use of functions, such as comments or compile-time execution, that would be inconvenient in Forth."&lt;/quote&gt;&lt;p&gt;It should be noted that the colors can be replaced with symbols or notation, so using the language without the ability to distinguish color is not a barrier. Color is just one way to show this information.&lt;/p&gt;&lt;p&gt;There are a ton of other enhancements beyond the obvious color aspect, such as:&lt;/p&gt;&lt;quote&gt;"By having words preparsed, the compiler is twice as fast. Another small point, since compiling is virtually instantaneous, but this encourages recompiling and overlaying the modules of an application. Smaller modules are easier to code, test and document than a large one."&lt;/quote&gt;&lt;p&gt;That interview contains another Chuck Moore quote about software construction in general:&lt;/p&gt;&lt;quote&gt;"Instead of being rewritten, software has features added. And becomes more complex. So complex that no one dares change it, or improve it, for fear of unintended consequences. But adding to it seems relatively safe. We need dedicated programmers who commit their careers to single applications. Rewriting them over and over until they're perfect."&lt;/quote&gt;&lt;p&gt;This is something I've seen repeated again and again by some of the most respected minds in software: You cannot just keep adding things to a program. You must continually re-work the program to match your needs as they change over time. Ideally, you re-write the program. Only time and deep consideration can yield the most elegant, correct, and simple program.&lt;/p&gt;&lt;p&gt;Which brings us to...&lt;/p&gt;&lt;p&gt;Chuck Moore has been fighting against software complexity since the 1950s.&lt;/p&gt;&lt;p&gt;"I am utterly frustrated with the software I have to deal with. Windows is beyond comprehension! UNIX is no better. DOS is no better. There is no reason for an OS. It is a non-thing. Maybe it was needed at one time.&lt;/p&gt;&lt;p&gt;-- Chuck Moore, 1997&lt;/p&gt;&lt;p&gt;"If they are starting from the OS they have made the first mistake. The OS isn't going to fit on a floppy disk and boot in ten seconds."&lt;/p&gt;&lt;p&gt;-- Chuck Moore, 1999&lt;/p&gt;&lt;p&gt;These quotes also come from Jeff Fox's quotes collection, Moore Forth: Chuck Moore's Comments on Forth (ultratechnology.com).&lt;/p&gt;&lt;p&gt;As you've no doubt gathered over the course of this page, Chuck is no fan of big, heavy, complicated software such as operating systems.&lt;/p&gt;&lt;p&gt;He believes in compact, machine-sympathetic programming.&lt;/p&gt;&lt;p&gt;"Mechanical Sympathy" is not Chuck's term, but I believe it accurately describes his philosophy. It comes from this (apocryphal?) quote by Formula One race car driver Jackie Stewart (wikipedia.org):&lt;/p&gt;&lt;quote&gt;"You don't have to be an engineer to be a racing driver, but you do have to have mechanical sympathy."&lt;/quote&gt;&lt;p&gt;The use of the term to describe software comes from Martin Thompson's blog of the same name. In Why Mechanical Sympathy? (blogspot.com), he writes:&lt;/p&gt;&lt;quote&gt;"Why does the software we use today not feel any faster than the DOS based applications we used 20 years ago??? It does not have to be this way. As a software developer I want to try and produce software which does justice to the wonderful achievements of our hardware friends."&lt;/quote&gt;&lt;p&gt;Again and again, you'll see this sentiment echoed by Chuck Moore and fans of Forth.&lt;/p&gt;&lt;p&gt;I think it's very interesting and telling that Forth tends to be popular with "hardware people" such as electrical engineers and embedded systems designers. By contrast, it seems that "software people" tend to idolize a more abstract, high-level beauty as found in languages such as Lisp or Scheme. Of course, this is a gross generalization and may have no basis in fact, but I know I'm not the only person to notice this trend.&lt;/p&gt;&lt;p&gt;Maybe another way to describe this aspect of Forth is that it has a "mechanical purity" in the same way that Joy, with its combinators, has a "mathematical purity."&lt;/p&gt;&lt;p&gt;And speaking of hardware...&lt;/p&gt;&lt;p&gt;Chuck's real love seems to be processor design. Those Harris RTX2000 and RTX2010 chips used in so many space missions? That's basically his chip!&lt;/p&gt;&lt;p&gt;No kidding.&lt;/p&gt;&lt;p&gt;Chuck, that brilliant rascal, has been designing hardware since 1983 starting with the Novix N400 gate array. An improved design was sold to Harris to become the RTX chips.&lt;/p&gt;&lt;p&gt;Chuck designs processors with his own VLSI software, "OKAD", written in 500 lines of Forth, of course.&lt;/p&gt;&lt;p&gt;Take a moment to pause on that last sentence.&lt;/p&gt;&lt;p&gt;Processor design software written in 500 lines?&lt;/p&gt;&lt;p&gt;You read that right.&lt;/p&gt;&lt;p&gt;OKAD is one of the Great Legends of Chuck Moore. But what, exactly, is it?&lt;/p&gt;&lt;p&gt;First off, VLSI stands for Very Large Scale Integration (wikipedia.org):&lt;/p&gt;&lt;quote&gt;"Very large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining millions or billions of MOS transistors onto a single chip. VLSI began in the 1970s when MOS integrated circuit (Metal Oxide Semiconductor) chips were developed and then widely adopted, enabling complex semiconductor and telecommunication technologies. The microprocessor and memory chips are VLSI devices."&lt;/quote&gt;&lt;p&gt;The product of VLSI is what we think of when we imagine the modern image of "computer chip" in our minds.&lt;/p&gt;&lt;p&gt;"Integration" is simply the shrinking of computers from whole rooms to microscopic thinking dust:&lt;/p&gt;&lt;p&gt;(Also, in a parallel path from mainstream desktop computing, VLSI has also produced entire computers and, increasingly, multiple computers on a single chip, also known as "system(s) on a chip" (SoC) (wikipedia.org). The lines around the various types are extremely blurry, but some familiar forms are microcontrollers, embedded systems, various "mobile" devices, etc.)&lt;/p&gt;&lt;p&gt;Anyway Moore's, VLSI Design Tools (OKAD) (colorforth.github.io) system a complete processor workshop:&lt;/p&gt;&lt;quote&gt;"In 500 lines of colorForth, these tools provide everything required to design a chip."&lt;/quote&gt;&lt;p&gt;OKAD is really more of a collection of tools that work together to:&lt;/p&gt;&lt;p&gt;For more about OKAD, I highly recommend reading the excellent answers to Did Forth's inventor Charles Moore really write a CAD program in only 5 lines of code? (retrocomputing.stackexchange.com).&lt;/p&gt;&lt;p&gt;Moving on from the software to Moore's chips themselves, Moore himself wrote a nice little summary of his designs. It is written in Moore's typical consise style, giving just a few key details about each chip: Forth Chips (colorforth.github.io).&lt;/p&gt;&lt;p&gt;First, there was the Novix NC4000, which was designed for a CMOS gate array.&lt;/p&gt;&lt;p&gt;Here's a whole book about the NC4000 chip: Footsteps in an Empty Valley: NC4000 Single Chip Forth Engine (8Mb PDF) by Dr. Chen-Hanson Ting.&lt;/p&gt;&lt;p&gt;To quote Dr. Ting from Chapter 2:&lt;/p&gt;&lt;quote&gt;"The Novix NC4000 is a super high-speed processing engine which is designed to directly execute high level Forth instructions. The single chip microprocessor, NC4000, gains its remarkable performance by eliminating both the ordinary assembly language and internal microcode which, in most conventional processors, intervene between the high level application and the hardware. The dual stack architecture greatly reduces the overhead of subroutine implementation and makes NC4000 especially suited to support high level languages other than Forth."&lt;/quote&gt;&lt;p&gt;As you can see, this reads just like a description of the Harris RTX chips used in the spacecraft we explored above.&lt;/p&gt;&lt;p&gt;Sure enough, if we read the History section on the RTX2010 page, (wikipedia.org) the lineage is made very clear:&lt;/p&gt;&lt;quote&gt;"In 1983, Chuck Moore implemented a processor for his programming language Forth as a gate array. As Forth can be considered a dual stack virtual machine, he made the processor, Novix N4000 (later renamed NC4016), as a dual-stack machine. In 1988, an improved processor was sold to Harris Semiconductor, who marketed it for space applications as the RTX2000."&lt;/quote&gt;&lt;p&gt;Another great article about Moore's early processor design work (and some more spacecraft mentions!), check out Charles Moore: From FORTH to Stack Processors and Beyond (cpushack.com) which is part one of a two-part series.&lt;/p&gt;&lt;p&gt;After the Novix, came a variety of chip projects:&lt;/p&gt;&lt;p&gt;These are all real systems that really worked. The hard part has always been finding customers.&lt;/p&gt;&lt;p&gt;Over the years, other people have also created Forth chips and FPGA implementations of hardware Forth-likes. Check out the links on Forth CPU Cores (forth.org) and Forth Chips (ultratechnology.com).&lt;/p&gt;&lt;p&gt;In addition to colorForth, Moore also developed "Machine Forth" as an even more machine-sympathetic language than traditional Forth. It's based on the machine code of the MuP21 microprocessor listed above.&lt;/p&gt;&lt;p&gt;I won't go into a lot of detail about Machine Forth, but here are some interesting links:&lt;/p&gt;&lt;p&gt;As you can see, Moore has always been looking for new ways to work with computers, a partnership between the machine and the programmer.&lt;/p&gt;&lt;p&gt;Which brings us to the current state of Chuck Moore's art...&lt;/p&gt;&lt;p&gt;"Programming a 144-computer chip to minimize power" (2013)&lt;/p&gt;&lt;p&gt;144 asynchronous computers on a chip. Idle cores use 100 nW. Active ones use 4 mW, run at 666 Mips, then return to idle. All computers running flat out: 550mW (half a Watt).&lt;/p&gt;&lt;p&gt;Check out Chuck's talk at StrangeLoop: Programming a 144-computer chip to minimize power - Chuck Moore (2013) (youtube.com)&lt;/p&gt;&lt;p&gt;And here's the official website: GreenArrays, Inc. (greenarraychips.com) "Ultra-low-powered multi-computer chips with integrated peripherals."&lt;/p&gt;&lt;p&gt;Probably the best summary comes from the architecture document, GreenArrays Architecture (PDF) (greenarraychips.com):&lt;/p&gt;&lt;quote&gt;"COMPLETE SYSTEMS: We refer to our chips as Multi-Computer Systems because they are, in fact, complete systems. Supply one of our chips with power and a reset signal, and it is up and running. All of our chips can load their software at high speed using a single wire that can be daisy chained for multiple chips; if desired, most can be bootstrapped by a simple SPI flash memory.&lt;p&gt;"Contrast this with a Multi-Core CPU, which is not a computing system until other devices such as crystals, memory controllers, memories, and bus controllers have been added. All of these things consume energy, occupy space, cost money, add complexity, and create bottlenecks.&lt;/p&gt;&lt;p&gt;"NO CLOCKS: Most computing devices have one or more clocks that synchronize all operations. When a conventional computer is powered up and waiting to respond quickly to stimuli, clock generation and distribution are consuming energy at a huge rate by our standards, yet accomplishing nothing."&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;It goes on to explain the fine-grained power usage, how each computer communicates with its neighbor, and similar statements high-level descriptions.&lt;/p&gt;&lt;p&gt;You can buy these chips right now for as little as $20 in quantities of 10. The only problem is that to easily make to use of one, you either need to buy the $495 development board or make your own. I've found precious few examples of people who have done this online.&lt;/p&gt;&lt;p&gt;One rare example is Hands on with a 144 core processor (archive.org of designspark.com). Article author Andrew Back even has screenshots of the of the arrayForth environment (which is basically colorForth)&lt;/p&gt;&lt;p&gt;The question, of course, is what do you do with this thing?&lt;/p&gt;&lt;p&gt;It may turn out that the answer can be found by looking back into computing history. You don't even have to go back very far.&lt;/p&gt;&lt;p&gt;If you read the "Rabbit Hole Alert" under the picture of the surface of comet 67p above, then you saw the term "transputer". I think it would be very interesting to compare and contrast the GreenArrays GA144 chips to the Inmos transputer chips. It seems to me, at first glance, that anything those transputers would have been suited for ought to be a good fit for a GreenArrays multi-computer chip as well.&lt;/p&gt;&lt;p&gt;Rabbit Hole Alert 2: Another fun diversion into massively parallel computers is one of my favorites: Danny Hillis's Connection Machine (wikipedia.org) computers featuring a "12-dimensional hypercube" routing design.&lt;/p&gt;&lt;p&gt;Hillis himself is a "human rabbit hole" of inventions, ideas, and writings. He's the author of one of my favorite non-fiction books, "The Pattern on the Stone," and co-founder of The Long Now Foundation (along with some other "human rabbit holes" including the incredible writer and thinker, Steward Brand).&lt;/p&gt;&lt;p&gt;One of the projects of the Long Now Foundation is the design and creation of the 10,000 year giant mechanical Clock of the Long Now which is intended to tick once per year and have a cuckoo that comes out once every 1,000 years.&lt;/p&gt;&lt;p&gt;There is also a direct connection between the Long Now and the Rosetta spacecraft: Long Now created the "Rosetta disc", an extremely clever physical object containing the micro-etched text of over a thousand human languages. The Rosetta spacecraft carried a nickel prototype of the disc. So that's now sitting on a comet.&lt;/p&gt;&lt;p&gt;As with the previous rabbit hole alert, I could link to all of these people and things, but each is part of an unfathomably deep fractal of fascinating stuff and I'm afraid you might never come back to finish this. But do look them up later!&lt;/p&gt;&lt;p&gt;At any rate,&lt;/p&gt;&lt;p&gt;The only problem with parallel computers is that we're still not that great at programming them.&lt;/p&gt;&lt;p&gt;Heck, we're not even that great at serial programming yet.&lt;/p&gt;&lt;p&gt;"If you talk about molecular computers that are circulating in your bloodstream, they aren't going to have very much power and they aren't going to have very much memory and they aren't going to be able to use much energy.&lt;/p&gt;&lt;p&gt;-- Chuck Moore, Programming a 144-computer chip to minimize power, 2013&lt;/p&gt;&lt;p&gt;The eventual complete domination of x86 PCs in practically all areas of computing, followed by the current rise of powerful ARM CPUs are historical computing fact. Incredible feats of processor engineering have made it possible to run what can only be described as "supercomputers" on battery power and put them in our pockets.&lt;/p&gt;&lt;p&gt;Trends in both software and hardware have been towards ever-increasing layers of complexity. The layers are very deep and very wide.&lt;/p&gt;&lt;p&gt;As I write this, certain popular avenues of computing threaten to make every current piece of inefficient software seem absolutely frugal by comparison.&lt;/p&gt;&lt;p&gt;(Incredibly, we're not even content with the supercomputers on our desks and in our hands. So we rely on services which work remotely over the Internet on powerful networks of computers in huge data centers. We think of this computing as cheap or even free because much of it is indirectly paid for with advertising dollars. Paid for, that is, with our attention and personal data. Those data centers with their screaming cooling fans and backup generators are somewhere else, not in our living rooms. It's easy to simply forget how all of this is made possible.)&lt;/p&gt;&lt;p&gt;Increasingly, we rely on massively complex software with that seems to have an unending appetite for computing power.&lt;/p&gt;&lt;p&gt;But do these trends have to continue?&lt;/p&gt;&lt;p&gt;There is absolutely no reason we have to use increasingly inefficient and poorly-constructed software with steeper and steeper hardware requirements in the decades to come.&lt;/p&gt;&lt;p&gt;In fact, the reverse could be true.&lt;/p&gt;&lt;p&gt;There are plenty of applications where low energy computing is a categorical requirement and I believe these applications will only increase.&lt;/p&gt;&lt;p&gt;Forth-likes could have a strong future as we look towards:&lt;/p&gt;&lt;p&gt;There are physical realities (such as the speed of light) which ultimately govern the speed at which we can perform a calculation or the maximum number of calculations which can be done with a Watt of electricity using computers made out of atoms. These are hard limits. But there will surely be other plateaus along the way to reaching these limits.&lt;/p&gt;&lt;p&gt;Around the year 2006, we saw Dennard scaling slow to a crawl. Dennard scaling (wikipedia.org) describes the relationship between the shrinking size of transistors to the increase of computing speed. Simply put, smaller transistors can switch at higher speeds and take less voltage. This scaling law held for many years. But we reached a speed plateau at around 4 GHz because of current leakage and heat.&lt;/p&gt;&lt;p&gt;In The Free Lunch Is Over (gotw.ca), published in Dr. Dobb's Journal in 2005, Herb Sutter writes,&lt;/p&gt;&lt;quote&gt;"The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have run out of room with most of their traditional approaches to boosting CPU performance. Instead of driving clock speeds and straight-line instruction throughput ever higher, they are instead turning en masse to hyperthreading and multicore architectures."&lt;/quote&gt;&lt;p&gt;Multicore processors and increasingly clever hardware architecture tricks have continued to provide increases in computing power...but it's not the same.&lt;/p&gt;&lt;p&gt;Near the end of the article, Sutter advises:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;"There are two ways to deal with this sea change toward concurrency. One is to redesign your applications for concurrency, as above. The other is to be frugal, by writing code that is more efficient and less wasteful. This leads to the third interesting consequence:&lt;/p&gt;&lt;p&gt;"3. Efficiency and performance optimization will get more, not less, important. Those languages that already lend themselves to heavy optimization will find new life; those that don't will need to find ways to compete and become more efficient and optimizable. Expect long-term increased demand for performance-oriented languages and systems."&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;(Emphasis mine.)&lt;/p&gt;&lt;p&gt;For now, we're still eating the remains of that free lunch.&lt;/p&gt;&lt;p&gt;I'm probably fairly rare among programmers in wishing it would end. I'd like to see greater emphasis on the craft and art of software. I'd like to see us make full and intentional use of the incredible power available to us now.&lt;/p&gt;&lt;p&gt;The retrocomputing (wikipedia.org) hobby has continually shown how much more we could have done with the home computers of the 1980s if we had continued to use them. In many cases, they've been shown to be able to run programs previously thought impossible. The things we could do with current hardware are surely even more amazing, but it will be perhaps decades before we find out.&lt;/p&gt;&lt;p&gt;In 1958, Chuck Moore created a dirt-simple interpreter on an IBM 704. That computer filled a room and cost about 2 million dollars.&lt;/p&gt;&lt;p&gt;I can buy a more powerful computer (minus the awesome control panel with lights and switches) today for literal pocket change in the form of a "microcontroller", a complete computer on a single silicon chip, and write a powerful Forth system for it. That computer can run on a coin cell battery or even a tiny solar panel, sipping power where the IBM 704 inhaled it.&lt;/p&gt;&lt;p&gt;There has never been a more incredible time for small-scale computing. Like the explosion of personal computers in the 1980s, the time is ripe for fun, creative, interesting, useful, and very personal computers and software.&lt;/p&gt;&lt;p&gt;These tools can do useful work and they can also teach and delight us. Ideas like Forth are ripe for rediscovery as we learn exciting new ways to compute with arrays of inexpensive, low-power computers.&lt;/p&gt;&lt;p&gt;We can pursue this line of thinking for pragmatic reasons, or just because it is beautiful and fun and worth doing for its own sake.&lt;/p&gt;&lt;p&gt;Chuck Moore is basically retired now, programming and toying with software with no deadlines or clients.&lt;/p&gt;&lt;p&gt;It is now on us to take up the mantle of Forth, to champion the values of ingenuity, elegance, efficiency, and simplicity.&lt;/p&gt;&lt;p&gt;Simple&lt;/p&gt;&lt;p&gt;To really understand the value of Forth (and especially Chuck Moore's later work on Machine Forth and the GreenArrays computers), we must consider the difference between "simple" and "easy".&lt;/p&gt;&lt;p&gt;We were blessed with the ability to speak of this difference by Rich Hickey in his brilliant talk, "Simple Made Easy" (2011) (youtube.com) which every developer should see at some time in their life. (Or read the transcript of Simple Made Easy (github.com) provided by Mattias Nehlsen.)&lt;/p&gt;&lt;p&gt;Forth is not easy. It may not always even be pleasant. But it is certainly simple. Forth is one of the simplest programming languages there has ever been.&lt;/p&gt;&lt;p&gt;A crafted language&lt;/p&gt;&lt;p&gt;If the best software is truly crafted for problem at hand, then it makes sense that an idea programming language would also be crafted for the problem at hand.&lt;/p&gt;&lt;p&gt;An absolutely amazing talk about language design, Guy Steele's Growing a Language (1998) (youtube.com) demonstrates how languages are built up from primitives. The talk is a performance art and deeply insightful.&lt;/p&gt;&lt;p&gt;Steele helpfully also wrote up a transcript of the talk: Growing a Language (PDF) (virginia.edu) Imagine Steele is saying "Forth" here in place of "Lisp" because the point is the same:&lt;/p&gt;&lt;quote&gt;"Lisp was designed by one man, a smart man, and it works in a way that I think he did not plan for. In Lisp, new words defined by the user look like primitives and, what is more, all primitives look like words defined by the user! In other words, if a user has good taste in defining new words, what comes out is a larger language that has no seams."&lt;/quote&gt;&lt;p&gt;Go forth and create the perfect programming language for you!&lt;/p&gt;&lt;p&gt;I promised I would show you a magic trick at the end of this article.&lt;/p&gt;&lt;p&gt;Behold, a new definition for the integer 4:&lt;/p&gt;&lt;quote&gt;: 4 12 ;&lt;/quote&gt;&lt;p&gt;Which I shall now use in a sentence:&lt;/p&gt;&lt;quote&gt;." The value of 4 is " 4 . CR The value of 4 is 12Tada!&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ratfactor.com/forth/the_programming_language_that_writes_itself.html"/><published>2025-10-20T00:40:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45639860</id><title>Introduction to reverse-engineering vintage synth firmware</title><updated>2025-10-20T11:09:34.135477+00:00</updated><content>&lt;doc fingerprint="a243d812e8f7a388"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Introduction to Reverse-Engineering Vintage Synth Firmware&lt;/head&gt;&lt;p&gt;In this article we're going to take a look at how to reverse-engineer vintage synthesiser firmware. The synthesiser I've chosen for us to look at is the Yamaha DX7 (See Appendix: Why Choose This Synth?). You don't need a DX7 to follow along at home, but you will need a copy of the DX7 V1.8 firmware (available here) and the Ghidra disassembler.&lt;/p&gt;&lt;p&gt;Who is this article for? This article's intended audience is people from a technical background who are new to reverse-engineering, 8-bit architectures, or embedded development. If you come from an electrical-engineering or embedded-software background, you'll probably find the content here a little basic.&lt;/p&gt;&lt;p&gt;You'll only need to know a little bit about low-level programming: A basic understanding of how binary and pointers work should be enough. You don't need to know assembly language, or understand any specific processor architecture.&lt;/p&gt;&lt;p&gt;A few years ago I decided to give myself a crash course on what goes on inside synthesisers. I ended up writing the article Yamaha DX7 Technical Analysis about what I'd learned. In order to tease out some more details about the DX7's inner-workings, I decided to disassemble the synth's firmware ROM. I didn't have any experience with reverse-engineering binaries, so I had to figure it out as I went. I'm still by no means an expert (if you see any mistakes in this article, please let me know!), but I'd like to share what I've learned.&lt;/p&gt;&lt;p&gt;All I had when I started was a copy of the firmware, a copy of the service manual, and a can-do attitude. I knew nothing about 8-bit systems, and absolutely nothing about electronics, but I was willing to give anything a shot. If this sounds like you, read on, and I hope you find this article helpful!&lt;/p&gt;&lt;p&gt;Reverse-engineering vintage synthesisers is a great introduction to embedded systems, and can be a lot of fun. In a lot of ways reverse-engineering is a bit like putting together a big jigsaw puzzle. Sometimes putting a new piece in place unlocks a lot of new progress, and like a jigsaw puzzle, the best place to start is at the edges.&lt;/p&gt;&lt;head rend="h2"&gt;Address Decoding #&lt;/head&gt;&lt;p&gt;The peripheral devices attached to the DX7's CPU, such as its LCD screen and sound chips, are memory-mapped. This means that the device has been allocated a specific address range in the system's memory, and the system communicates with the device by reading and writing data from and to these addresses.&lt;/p&gt;&lt;p&gt;Before we can start disassembling the firmware ROM, we need to know what peripheral device is mapped where. To do that we'll need to look at the DX7's address decoding logic. The first place to start is with the schematics.&lt;/p&gt;&lt;p&gt;The best version of the schematics I've seen is this version, created by the yamahamusicians.com user Miks. While you're at it, grab a copy of the service manual too. We won't be referencing it in this article, but it's a good resource to have. It explains certain details about the synth's architecture that aren't obvious from the schematics.&lt;/p&gt;&lt;p&gt;If you're new to electronics, device schematics can look very intimidating, but once you understand the basics they're not actually as scary as they look! You can find a good introductory guide to schematics here.&lt;/p&gt;&lt;head rend="h3"&gt;Background&lt;/head&gt;&lt;p&gt;But first, what does address decoding actually mean? Address decoding refers to how a specific device is mapped to a specific address. In this section we'll figure out what peripheral is mapped to what address by tracing the address decoding logic in the synth's schematics.&lt;/p&gt;&lt;p&gt;The total amount of memory addresses that a CPU can access is referred to as the CPU's 'address space'. This is limited by the width of its 'address bus'. The CPU's address bus is responsible for selecting addresses in attached memory devices, such as RAM, or peripheral devices with addressable registers. Each line in the address bus represents a single bit, with the total number of lines determining the address range the CPU can access. For example, a 16-bit address bus can address 216 unique memory locations, or 64KiB.&lt;/p&gt;&lt;p&gt;When a CPU's address lines are exposed externally in the form of pins on the chip's package, this is called an external address bus1. These lines can be physically connected to external memory devices. Together with the CPU's data bus, this allow reading and writing binary data back and forth.&lt;/p&gt;&lt;p&gt; When the CPU performs an instruction that reads or writes memory, like &lt;code&gt;LDB 0x2001&lt;/code&gt;, several things happen:
        &lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt; The CPU's external address pins are set to high and low logic levels according to the specified address. For address 0x2001 (&lt;code&gt;0b0010_0000_0000_0001&lt;/code&gt;), address pins 0 and 13 will be high, and all the others will be low.&lt;/item&gt;&lt;item&gt; The CPU's &lt;code&gt;RW&lt;/code&gt;pin will be set high to indicate that this is a read operation, and...&lt;/item&gt;&lt;item&gt;The CPU will prepare to accept the incoming data at 0x2001 over the data bus into the B register.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;But wait... If the CPU only has one set of address and data bus lines, how do you connect multiple memory devices to the CPU? This is where the 'Chip Select' interface comes in: Each device attached to the CPU's data/address buses has a 'Chip Select' pin, controlling whether the device responds to incoming signals.&lt;/p&gt;&lt;p&gt; Consider the above (incredibly simplified) diagram: Two 8KiB 6264 RAM chips (U3 and U4) are connected to shared address and data buses on a Z80 CPU. U3's &lt;code&gt;CE1&lt;/code&gt; (Chip Enable)
          pin is connected to the CPU's A13 pin. The bar over the top of the
          label indicates that this pin is 'Active Low', meaning a low
          logic level will 'activate' its function. When the CPU selects an
          address between 0x0 and
          0x1FFF, the A13 pin will be low,
          activating the U3 chip. U4's &lt;code&gt;CE1&lt;/code&gt; pin
          is attached to the CPU's A13 pin via a &lt;code&gt;NOT&lt;/code&gt; gate, which
          inverts the signal coming from A13. When an address above
          0x1FFF is selected, A13 will be set
          high, selecting the U4 chip. This effectively maps U3 to the first
          8KiB of the system's memory, and U4 to the next.
        &lt;/p&gt;&lt;p&gt;Can you spot the problem with this example? Since any address using A13 will 'select' U4, U4 is now mapped to every 8KiB block of memory above 0x1FFF. In reality, more sophisticated logic is used to map memory devices. Let's examine the real world example of the DX7's address decoding circuitry.&lt;/p&gt;&lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;OR&lt;/code&gt;, and
          &lt;code&gt;NOT&lt;/code&gt; gates do is all you need. One particular type of
          component you'll encounter a lot inside vintage synthesisers are
          7400-series logic chips.
        &lt;head rend="h2"&gt;Decoding the DX7's Memory Map #&lt;/head&gt;&lt;p&gt;Nearly all of the discrete electrical components that make up a DX7 are commonly available products. They're mass-manufactured and sold by a variety of different manufacturers. The best way to understand these components is to read the datasheets made available by the manufacturer. I'll provide links to these as we go.&lt;/p&gt;&lt;code&gt;AND&lt;/code&gt; and &lt;code&gt;OR&lt;/code&gt; gates, rather
          than the more standard
          
            ANSI, or IEC notations [pdf]. Other gates use the ANSI notation.
        &lt;head rend="h3"&gt;The Firmware ROM #&lt;/head&gt;&lt;p&gt;Let's start by taking a look at the firmware ROM, IC14.&lt;/p&gt;&lt;p&gt; In the schematic we can see that IC14's &lt;code&gt;CE1&lt;/code&gt; pin is connected to the CPU's
          &lt;code&gt;A14/A15&lt;/code&gt; lines via an &lt;code&gt;AND&lt;/code&gt; gate, and a
          &lt;code&gt;NOT&lt;/code&gt; gate. What's going on here?
        &lt;/p&gt;&lt;p&gt; The &lt;code&gt;AND&lt;/code&gt; gate ensures that the signal is only high when
          both address lines are active, and the &lt;code&gt;NOT&lt;/code&gt; gate
          inverts the signal so that it activates the active-low
          &lt;code&gt;CE1&lt;/code&gt; pin. If &lt;code&gt;A14&lt;/code&gt; and
          &lt;code&gt;A15&lt;/code&gt; being active on the CPU 'selects' the ROM chip, that
          means it's mapped to the address range
          0xC000 - 0xFFFF2.
        &lt;/p&gt;&lt;p&gt;Awesome! That wasn't so hard. Now we know where the ROM is mapped in memory. What's next?&lt;/p&gt;&lt;head rend="h3"&gt;RAM #&lt;/head&gt;&lt;p&gt;The address decoding logic for the RAM is a little more complicated.&lt;/p&gt;&lt;p&gt; The DX7 features three 4KiB 5118P RAM chips (IC19, 20, 21). These are connected to the CPU's address bus via a 74LS138 demultiplexer (IC23). This demultiplexing circuit is used to select one of 8 individual output lines based on a 3-bit input signal. These output lines are labeled as &lt;code&gt;Y0&lt;/code&gt; -
          &lt;code&gt;Y7&lt;/code&gt;, and the input lines as &lt;code&gt;DA&lt;/code&gt;, &lt;code&gt;DB&lt;/code&gt;, and &lt;code&gt;DC&lt;/code&gt;. The &lt;code&gt;OR&lt;/code&gt; gates used here are wired to the system clock
          output pin. Presumably to ensure the timing of read and write
          operations are valid.
        &lt;/p&gt;&lt;p&gt; The first RAM chip (IC19)'s chip select terminal is connected to the demultiplexer's &lt;code&gt;Y2&lt;/code&gt; line. The 'Function
          Table' from the 74LS138P datasheet shows that
          &lt;code&gt;Y2&lt;/code&gt; will be set low when
          input &lt;code&gt;DB&lt;/code&gt; (connected to the CPU's
          &lt;code&gt;A12&lt;/code&gt;) is high. Therefore, when the CPU selects address
          0x1000, the first RAM chip will be
          selected.
        &lt;/p&gt;&lt;p&gt;&lt;code&gt;Y3&lt;/code&gt;
          (connected to the second RAM chip, IC20) will be set low when inputs
          &lt;code&gt;DA&lt;/code&gt; and &lt;code&gt;DB&lt;/code&gt; (&lt;code&gt;A11&lt;/code&gt;
          and &lt;code&gt;A12&lt;/code&gt;) are high, corresponding to an address of
          0x1800. Likewise,
          &lt;code&gt;Y4&lt;/code&gt;
          (connected to IC21) corresponds to an address of
          0x2000.
        &lt;/p&gt;&lt;p&gt;By tracing this address decoding logic, we've successfully mapped the synth's RAM to 0x1000 - 0x2800.&lt;/p&gt;&lt;head rend="h3"&gt;LCD Screen #&lt;/head&gt;&lt;p&gt;The last peripheral we're going to look at right now is the synth's LCD screen. When you take your first peek inside a binary you'll be staring at an intimidating jumble of machine code. One of the few things that will stand out at a glance is ASCII strings. A good way to get a quick overview of the binary is finding out how these strings are printed to the screen, and where.&lt;/p&gt;&lt;p&gt;The best place to start doing that is understanding how the CPU interfaces with the LCD controller, and working your way backwards to the code responsible for sending string data to it. Once you've found how strings are printed to the screen, you can easily see what's printed where to get a better understanding of the code.&lt;/p&gt;&lt;p&gt;The LCD address mapping logic might look really complicated, but don't worry though. It's just more of the same logic as before.&lt;/p&gt;&lt;p&gt; IC23's &lt;code&gt;Y5&lt;/code&gt; pin is connected
          to IC24, another 74LS138 demultiplexer. From the 74LS138 function
          table we know that &lt;code&gt;Y5&lt;/code&gt; goes
          low when inputs A, and C (&lt;code&gt;A11&lt;/code&gt; and &lt;code&gt;A13&lt;/code&gt;) are
          high. So it looks like IC24 is mapped to
          0x2800.
        &lt;/p&gt;&lt;p&gt; Take a look at IC24: Inputs A, B and C are wired to &lt;code&gt;A1&lt;/code&gt;, &lt;code&gt;A2&lt;/code&gt; and &lt;code&gt;A3&lt;/code&gt;. That means that
          IC24 only maps 8 bytes.
        &lt;/p&gt;&lt;p&gt; IC24's &lt;code&gt;Y0&lt;/code&gt; and
          &lt;code&gt;Y1&lt;/code&gt; pins are connected to an
          &lt;code&gt;AND&lt;/code&gt; gate connected to the 'chip select' pin of IC12.
          What's happening here? This might seem a little confusing at first,
          but since the 74LS138P's outputs are active-low, this makes
          &lt;code&gt;LCDCS&lt;/code&gt; active when either
          &lt;code&gt;Y0&lt;/code&gt; or
          &lt;code&gt;Y1&lt;/code&gt; are active. This maps
          IC12 to the four-byte range
          0x2800 - 0x2803. Awesome. But what's
          IC12 doing?
        &lt;/p&gt;&lt;p&gt;IC12 is an Intel 8255 Programmable Peripheral Interface (PPI). It provides 24 parallel, bidirectional IO lines3.&lt;/p&gt;&lt;p&gt; The schematics show the LCD's parallel interface (&lt;code&gt;DB0 - DB7&lt;/code&gt;) is connected to the PPI's port A (&lt;code&gt;PA0 - PA7&lt;/code&gt;), and its
          control pins (&lt;code&gt;E, RW&lt;/code&gt; and &lt;code&gt;RS&lt;/code&gt;) to the PPI's
          port B (&lt;code&gt;PB0 - PB2&lt;/code&gt;).
        &lt;/p&gt;&lt;p&gt; The Hitachi LM016 LCD screen used in the DX7 features the ubiquitous Hitachi HD44780 LCD Controller. According to its datasheet (available here) it has two registers. When its &lt;code&gt;RS&lt;/code&gt; line (connected to
          the PPI's &lt;code&gt;PB0&lt;/code&gt;) is low, the instruction register is
          selected. When high, the data register is selected.
        &lt;/p&gt;&lt;p&gt;Based on the HD44780 datasheet, and the above table from the 8255's datasheet, we can tell that the LCD's data register must be mapped to 0x2800, and its control register to 0x2801. We'll go into more detail about the LCD controller itself later in the article.&lt;/p&gt;&lt;head rend="h3"&gt;Wrapping Up #&lt;/head&gt;&lt;p&gt;Now we've got a pretty good idea of what's going on where in the memory map, and how this is discovered. To save you the trouble of going through the whole schematic, here are all the memory-mapped peripheral addresses.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Address Range&lt;/cell&gt;&lt;cell role="head"&gt;Peripheral&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x1000 - 0x2800&lt;/cell&gt;&lt;cell&gt;RAM (External)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x2800&lt;/cell&gt;&lt;cell&gt;LCD Data&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x2801&lt;/cell&gt;&lt;cell&gt;LCD Control&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x2802&lt;/cell&gt;&lt;cell&gt;Sustain/Portamento Pedals, and LCD Busy Line&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x2803&lt;/cell&gt;&lt;cell&gt;8255 Peripheral Controller Control Register&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x2804&lt;/cell&gt;&lt;cell&gt;OPS Mode register&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x2805&lt;/cell&gt;&lt;cell&gt;OPS Algorithm/Feedback register&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x280A&lt;/cell&gt;&lt;cell&gt;DAC Volume&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x280E&lt;/cell&gt;&lt;cell&gt;LED1&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x280F&lt;/cell&gt;&lt;cell&gt;LED2&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x3000 - 0x4000&lt;/cell&gt;&lt;cell&gt;EGS&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x4000 - 0x5000&lt;/cell&gt;&lt;cell&gt;Cartridge Interface&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0xC000 - 0xFFFF&lt;/cell&gt;&lt;cell&gt;ROM&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;These aren't the only peripherals attached to the system, the Hitachi 6303 CPU also features 'IO Ports'. These are memory-mapped input/output lines with their own dedicated functionality. We'll touch on these later in the article.&lt;/p&gt;&lt;head rend="h2"&gt;Disassembling The Firmware #&lt;/head&gt;&lt;p&gt;Now that we know the memory map, we can start disassembling the firmware. To do this we'll use a graphical disassembler called Ghidra4. It's a relatively new player on the scene, but it's free, open source, and very powerful. A great resource to keep handy while working with Ghidra is the Ghidra Cheat Sheet.&lt;/p&gt;&lt;code&gt;6303&lt;/code&gt; directory to the
          &lt;code&gt;Ghidra/Processors&lt;/code&gt; directory inside your Ghidra
          installation. You'll need to restart Ghidra to see the new processor
          in the list.
        &lt;p&gt;Once you've installed the language definitions, open Ghidra and create a new project. The first thing we're going to need to do is to import the firmware ROM binary. Select the '6303' language, and click 'OK'.&lt;/p&gt;&lt;p&gt;Next, open up the Codebrowser. This is where all the action happens.&lt;/p&gt;&lt;p&gt;Once the initial disassembly loads, the first thing you'll be looking at is row after row of hexadecimal. This is the actual machine code as it would appear to the CPU. Don't bother with analyzing the file.&lt;/p&gt;&lt;p&gt;The first thing we're going to do is set up the memory map. Remember that thing we just did all that hard work figuring out? That's going to come in handy now. Press the 'Display Memory Map' icon in the top button bar, to open up the Memory Map dialog. By default there's only one memory block defined. This block consists of the binary we just imported, so go ahead and rename it to 'ROM'. The next thing we need to do is move this block to the correct offset 0xC000. Because all of the machine code instructions reference memory with absolute addresses, if we didn't map the ROM to the correct location none of the disassembly would work.&lt;/p&gt;&lt;p&gt;Before we finish setting up the memory map, let's take a quick look at the code. When the Hitachi 6303 processor in the DX7 powers up, it knows where to begin executing code by fetching a pointer from a specific location in the interrupt vector table.&lt;/p&gt;&lt;p&gt;In this case the 'Reset vector' is always located at the specific memory address 0xFFFE, right at the end of the address space. Press the Ctrl+End combination on your keyboard to move to the end of the binary, and select the offset 0xFFFE by clicking on it. Press the P key on your keyboard to convert the data at this address to a pointer. You should see something similar to the image below.&lt;/p&gt;&lt;p&gt;Double-click on this pointer to take you to the associated offset in the binary. Now we've found where the actual code is located, but it doesn't look like much just yet.&lt;/p&gt;&lt;p&gt;To begin disassembling the machine code into something we can work with, click on the label and press the D key on your keyboard, or right-click and select 'Disassemble' in the context menu.&lt;/p&gt;&lt;p&gt;The disassembly process will follow the flow of code through the binary, disassembling as it goes. An error will pop up here, but don't worry about it for now. This is just the disassembler mistaking a jump table for code.&lt;/p&gt;&lt;p&gt;Once the disassembly completes you should see something like the picture below.&lt;/p&gt;&lt;p&gt;Now we're looking at real code! No need to panic though. If you don't understand what you're looking at, that's okay. Assembly can look pretty intimidating at first, but with a little bit of practice you'll get the hang of it!&lt;/p&gt;&lt;p&gt;Each of the lines you're seeing here represents a single machine-code instruction translated into assembly code. The three letter mnemonics are the human-readable representation of the instructions. LDA for example, is the mnemonic for the 'Load value into register A' instruction. STA is the mnemonic for the 'Store value in register A' instruction. If you've never encountered assembly language before, that's okay! This video will give a very quick and general introduction to assembly language.&lt;/p&gt;&lt;p&gt;The HD63B03RP CPU used in the DX7 is a member of the 6800 family of processors. Its instruction set (the full set of assembly instructions) is small and easy to understand. A great resource for understanding the 6303 CPU and its instruction set is the HD6301/HD6303 Series Handbook freely available on bitsavers.org.&lt;/p&gt;&lt;p&gt;The FUN_c5e5 text you're seeing here is a label. This is a symbol placed in the disassembler's symbol table, which can be referenced elsewhere in the assembly code, usually as the target for a branching instruction. Ghidra should already have set up the reset vector as a 'function'. Select this label with your cursor and press the F key on your keyboard to edit the function and give it a more meaningful label like reset.&lt;/p&gt;&lt;p&gt;But what are all these red labels we're seeing, like DAT_2575? If you try to double click on it, Ghidra offers a helpful error message: 'Address not found in program memory: 2575'. This is because we're missing our memory map! Let's go back to the 'Memory Map' dialog, and add the missing blocks.&lt;/p&gt;&lt;p&gt;Fill in the memory map that we worked out in the last section. The completed map should look something like the screenshot below. You can choose to consolidate some of these blocks if you like. It's not super important how the blocks are divided. What matters is that the blocks cover all of the needed peripheral addresses. Note that I added memory blocks for the HD6303 CPU's internal registers, and internal RAM.&lt;/p&gt;&lt;p&gt;Now is a good time to go and fill in the individual peripheral addresses that we know. The HD6301/HD6303 Series Handbook provides a list of the HD6303RP's internal registers.&lt;/p&gt;&lt;p&gt; Press the Ctrl+Home keys on your keyboard to go to address 0x0. Press the B key on your keyboard to declare that address 0x0 specifies a byte of data. You'll see the &lt;code&gt;??&lt;/code&gt; change to &lt;code&gt;db&lt;/code&gt;, which is the
          assembler directive to define a byte of memory. Press the
          L
          key on your keyboard to give this address a useful label like
          io_port_1_dir. Go ahead and fill
          in the rest of the peripherals. When you go back to the reset handler
          you'll notice that, even with only a few pieces of the jigsaw puzzle
          in place, things will start to make a lot more sense.
        &lt;/p&gt;&lt;head rend="h3"&gt;The Reset Function #&lt;/head&gt;&lt;p&gt;The main reset handler in the DX7 is responsible for initialising the firmware. It sets up the CPU's IO ports, ensures the firmware's important variables have valid values, and sets up the CPU's timer interrupt. More on this later.&lt;/p&gt;&lt;code&gt;D == (A &amp;lt;&amp;lt; 8) | B&lt;/code&gt;.
        &lt;p&gt;A great way to visualise the 'control flow' of the program is in the 'Function Graph' view. This view shows a directed graph of the program's branching logic. You can open this view by selecting Window → Function Graph in the top window menu. You should see a view similar to the picture below. At offset 0xC605 you'll see the following instructions:&lt;/p&gt;&lt;quote&gt;LDA #0xd CMPA DAT_2328 BHI LAB_c60f&lt;/quote&gt;&lt;list rend="ol"&gt;&lt;item&gt; The LDA instruction loads the immediate value &lt;code&gt;0xD&lt;/code&gt;into the A register.&lt;/item&gt;&lt;item&gt;The CMPA instruction then compares the value in the A register with the value at the memory address DAT_2328.&lt;/item&gt;&lt;item&gt;The BHI instruction tells the CPU to branch to the label LAB_c60f if the value in the A register is greater than the value at DAT_2328.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;You can see in the function graph that if the memory at DAT_2328 is greater than or equal to '13', it will not branch, and the value will be cleared. The program will then continue to execute the next instruction, which would have been the original branch target. In this case, the program is checking to see that the 'pitch bend range' variable (stored in memory at location 0x2328) is within a valid range of 0-12. If not, it's reset to 0.&lt;/p&gt;&lt;p&gt;If you look down at the bottom of the graph, you'll notice something interesting: The program goes into an infinite loop. This is the firmware's 'main loop'. Tasks that need to be performed continuously happen here. Such as updating the UI based on user input, and parsing incoming MIDI messages.&lt;/p&gt;&lt;p&gt;When certain tasks not only need to be performed continuously, but also periodically, there's another way to make this happen: interrupts.&lt;/p&gt;&lt;head rend="h3"&gt;Interrupts #&lt;/head&gt;&lt;p&gt;Interrupts are signals sent to the processor by hardware or software to interrupt the current code being executed, and handle a specific event. They're commonly used in embedded-software to handle external, time-critical, or asynchronous events.&lt;/p&gt;&lt;p&gt;One of the most common types of interrupt you'll encounter is a 'timer interrupt'. The HD6303's built-in timer interrupt consists of a 16-bit 'counter' register, which is incremented every clock cycle, and a 16-bit 'output compare' register. When the value in the counter register matches the value in the output compare register, a timer interrupt will be raised. This causes the processor to halt what it was doing, push the current state of the CPU onto the stack, and jump to the appropriate interrupt handler specified in the interrupt vector table. In the 6303 a pointer to this handler is located at offset 0xFFF4. Once the firmware is done handling the interrupt, it executes the RTI instruction, which restores the CPU's state from the stack and continues executing the code from where it left off.&lt;/p&gt;&lt;p&gt;The timer interrupt handler is where all the synth's real-time functionality happens. This is any code that needs to be executed in a time-critical manner. The DX7 uses the periodic timer interrupt to process portamento and modulation, update the individual voice frequencies, and send the updated voice data to the sound chips. Feel free to declare the pointer to the timer interrupt handler just like we did for the reset handler, disassemble the handler, and take a look at what's going on.&lt;/p&gt;&lt;head rend="h3"&gt;LCD Interface #&lt;/head&gt;&lt;p&gt;One of the best places to start reverse-engineering a synth's firmware is to understand how it prints things to the LCD screen. We already know where the LCD controller is mapped in memory, let's work backwards from there and see if we can find that code.&lt;/p&gt;&lt;p&gt;Press the G key on your keyboard to open the 'Go To...' dialog, and go to address 0x2800. These are the two memory-mapped LCD registers. The list of cross-references on the right shows us where these addresses are referenced in the code. Click on the FUN_fdef label to take us to this function. This is the function called by the reset handler to initialise the LCD screen.&lt;/p&gt;&lt;p&gt;Below the function we can see something that looks like ASCII data. In fact, it looks a lot like the welcome message displayed when you boot up the DX7. Hmm. Click on offset 0xFE31, and press the ' key on the keyboard twice. Once to convert the data at this offset to character data, twice to convert it to a NULL-terminated string.&lt;/p&gt;&lt;p&gt;Notice that the welcome message location is referenced in the code at offset 0xFE2B:&lt;/p&gt;&lt;quote&gt;JSR FUN_fe52 LDX #0xfe31 JMP FUN_fea4&lt;/quote&gt;&lt;p&gt; Select the operand &lt;code&gt;#0xFe31&lt;/code&gt;, and press
          
            Alt+Ctrl+R
          
          on your keyboard to turn this into a
          memory reference. The default label looks a bit strange, so you
          might want to give it a better one like
          str_welcome_message
          by selecting the reference and pressing the
          L key.
        &lt;/p&gt;&lt;p&gt;We can see here that a pointer to the welcome message string is loaded into the X register, and then the ROM jumps to the function FUN_fea4. Could this function have something to do with printing the string? Let's find out.&lt;/p&gt;&lt;code&gt;LDA 4,x&lt;/code&gt; instruction will
          load the byte into A that is 4
          bytes from the address stored in X.
          This is useful because it allows us to reference 16-bit addresses with
          only an 8-bit operand.
        &lt;p&gt;Let's take a walk through FUN_fea4 together and see if we can figure out what it's doing:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;First, it pushes the address of the welcome message string in X to the stack.&lt;/item&gt;&lt;item&gt;Then it loads a memory address (0x261F) into X, and saves that address to a pointer in memory.&lt;/item&gt;&lt;item&gt;Then it restores the welcome message address from the stack into X.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Not very helpful yet, unfortunately. Something I find that helps make sense of so many unknown labels is to give them names that describe how they're used. Double-click on the label DAT_00fb to go to its location. Since we know this variable stores memory addresses, press the P key to convert it to a pointer. Giving it a name like unknown_lcd_pointer_00fb can help identify it at a glance later.&lt;/p&gt;&lt;p&gt;Use the Alt+← keyboard combination to navigate back to where we were before. Once you're there, click through to FUN_fe8b. We can see that lots of cross-references to this function have been found in the code already. Let's go through this function step by step and see what we can figure out:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;We already know that the X register contains a pointer to a string. So we can see that this function is loading an ASCII character into register B from the address stored in X.&lt;/item&gt;&lt;item&gt; When the LDB instruction loads a new value into B it sets the condition codes according to its value. If the most-significant bit of this byte is 1, the N(egative) condition code will be set. This will cause the BMI (Branch If MInus) instruction to branch. Valid ASCII values fall within the range 0-127, so this code looks like it's checking for an invalid character, and will branch to the exit if this is the case.&lt;lb/&gt;Note: Different instructions treat integer values as either signed, or unsigned, with the most-significant bit treated as the sign bit.&lt;/item&gt;&lt;item&gt; The value in B is then compared against &lt;code&gt;0x20&lt;/code&gt;(ASCII space). As I mentioned earlier, the CMP instruction sets condition codes according to the value in the associated accumulator, and the operand. The BCC instruction (Branch If Carry Clear) will branch if the C(arry) condition code is clear. This means that the value in B must be&lt;code&gt;0x20&lt;/code&gt;or above, otherwise the function exits.&lt;lb/&gt;You can read more about how the carry flag is used in computer arithmetic on Wikipedia.&lt;/item&gt;&lt;item&gt;If the ASCII char is valid, it calls BSR to branch to the subroutine FUN_fe9a. In this subroutine we can immediately see something interesting: Remember that pointer we labeled earlier? This subroutine writes the ASCII character in B to the location in this pointer, increments the pointer, and saves it...&lt;/item&gt;&lt;item&gt;After this, the address in X is incremented, and the function loops back to the start. Now the function repeats, with X pointing to the next character in the string.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Are you thinking what I'm thinking? This is a String Copy function! It copies characters from a string into a buffer, until either a NULL-terminator, or other unprintable ASCII character is encountered.&lt;/p&gt;&lt;p&gt;Go ahead and give this function a label like lcd_strcpy. If you like, you can apply local labels to LAB_fe94, and LAB_fe99 like .copy_character and .exit. Maybe give that buffer address we saw earlier (0x261F) a temporary label too.&lt;/p&gt;&lt;p&gt;This is where we're at so far:&lt;/p&gt;&lt;p&gt;Let's move on to that last function FUN_fe52 and see where that leads us. This function is a bit more complicated. Using the Function Graph window I showed you before might help visualise what's going on. Let's go through this function step-by-step like we did before:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;A new location in memory (0x263F) is being saved to that pointer we saw before, and the LCD buffer address we saw earlier (0x261F) is being saved to a new pointer. There's something interesting about those addresses. They're 32 bytes apart. That seems a bit conspicuous, doesn't it? Maybe this corresponds to the length of the LCD screen (2 lines of 16 characters)?&lt;/item&gt;&lt;item&gt;A constant value is loaded into B.&lt;/item&gt;&lt;item&gt;Inside the loop, we can see that B is saved to the stack. A byte is then loaded into A from the location in the pointer at 0xF9. We know from seeing the welcome message string loaded into X that this byte is ASCII string data. The pointer is then incremented and saved.&lt;/item&gt;&lt;item&gt;This byte is then compared against the byte pointed to by unknown_lcd_pointer_00fb.&lt;/item&gt;&lt;item&gt;If the character in unknown_lcd_pointer_00f9 and unknown_lcd_pointer_00fb aren't equal, then this character is used as an argument for a function call to FUN_fec7.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The function at FUN_fec7 is a bit more complicated, so I'll walk you through what's happening.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt; The constant &lt;code&gt;0x89&lt;/code&gt;is written to the 8255 PPI control register at 0x2803. The PPI datasheet refers to this as 'Control Word #5'. This sets the PPI's Port A, and B to outputs, allowing the CPU to send data to the LCD controller.&lt;/item&gt;&lt;item&gt; A value of zero is written to the LCD control register. This sets the &lt;code&gt;RS&lt;/code&gt;line low to select the Instruction Register, and the&lt;code&gt;RW&lt;/code&gt;line low to select a Write operation.&lt;/item&gt;&lt;item&gt; The &lt;code&gt;E&lt;/code&gt;line of the LCD is then driven high to instruct it to be ready to receive data over the data bus.&lt;/item&gt;&lt;item&gt; The byte in A is then written to the LCD instruction register. After this, the &lt;code&gt;E&lt;/code&gt;line is driven low, and the&lt;code&gt;RW&lt;/code&gt;line is driven high to signal the end of the data transfer.&lt;/item&gt;&lt;item&gt;The 8255 'Control Word #13' is written to the PPI control register to revert port A and C to being inputs.&lt;/item&gt;&lt;item&gt;Finally, it branches unconditionally to FUN_ff08.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Let's check out the subroutine at FUN_ff08 that our function jumps to.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt; The &lt;code&gt;E&lt;/code&gt;and&lt;code&gt;RW&lt;/code&gt;lines of the LCD controller are set high. This sets the LCD controller to read mode.&lt;/item&gt;&lt;item&gt; The PPI's port C is read into the A register, then the &lt;code&gt;E&lt;/code&gt;line of the LCD controller is set low to indicate the read operation is complete.&lt;/item&gt;&lt;item&gt; A bitwise &lt;code&gt;AND&lt;/code&gt;is performed between value of the A register and&lt;code&gt;0b1000_0000&lt;/code&gt;. This checks the status of the&lt;code&gt;PC7&lt;/code&gt;line. If the&lt;code&gt;PC7&lt;/code&gt;line is high, the function loops back to the start.&lt;/item&gt;&lt;/list&gt;&lt;p&gt; It's easy to miss, but if you look closely in the schematics you'll see that the PPI's &lt;code&gt;PC7&lt;/code&gt; line is connected to
          &lt;code&gt;PA7&lt;/code&gt;, which is connected to the LCD controller's DB7 pin.
          
            
          
          The DB7 pin serves as the LCD controller's 'Busy Flag'. This
          flag indicates whether the LCD controller is busy processing data.
          When it's clear, the LCD controller is ready to accept new data.
        &lt;/p&gt;&lt;p&gt;It looks like the purpose of this function is to poll the LCD controller, waiting for it to be ready to accept new data. Awesome! Let's give it a label like lcd_wait_ready. Okay! So putting it all together, the function at 0xFEC7 writes an instruction to the LCD controller, and then waits for it to be ready to receive data again. Go ahead and give it a name like lcd_write_instruction.&lt;/p&gt;&lt;p&gt;Reverse-engineering often involves going down a rabbit hole. Sometimes you need to fill in a few different pieces of the puzzle before you can start to see the whole picture. Let's return to the function at FUN_fe52 and see what happens next.&lt;/p&gt;&lt;p&gt; We now know the loop is writing an instruction to the LCD controller, but what did this instruction do? The original instruction value when the function started was &lt;code&gt;0x80&lt;/code&gt;, and it's incremented by
          one with each iteration of the loop. The HD44780 datasheet tells us
          that &lt;code&gt;0x80&lt;/code&gt; is the instruction to set the DDRAM (Display
          Data RAM) address in the LCD controller. This is the address in the
          LCD's memory where the next character will be written. A value of
          &lt;code&gt;0x80&lt;/code&gt; indicates the start of the screen's first line.
        &lt;/p&gt;&lt;p&gt;The next function call (FUN_fee7) looks almost identical to our lcd_write_instruction function. The only difference is that it writes to the LCD controller's data register, rather than the instruction register. This must be where the actual character data is written! You can give this function a label like lcd_write_data. Note that this function 'falls-through' to the LCD controller polling function we saw earlier.&lt;/p&gt;&lt;p&gt;Now we know what's going on here. This is our LCD printing function! Notice that after writing the character data to the LCD, at offset 0xFE77 the function writes it to the buffer at 0x263F? The incoming characters are compared against the contents of this buffer to see if they're identical, if they are then it skips printing the character. Maybe these buffers represent the 'next', and 'current' contents of the LCD screen?&lt;/p&gt;&lt;p&gt; After writing the LCD character data, the function then checks whether the LCD instruction byte is equal to &lt;code&gt;0xD0&lt;/code&gt;. Now we know
          that this is checking whether the LCD DDRAM position is at the end of
          the second line. If not, it checks whether we're at the end of the
          first line (&lt;code&gt;0x90&lt;/code&gt;). If so, the instruction byte is set to
          &lt;code&gt;0xC0&lt;/code&gt;, which sets the DDRAM address to the start of the
          second line.
        &lt;/p&gt;&lt;code&gt;0xC0&lt;/code&gt; (&lt;code&gt;0x80 + 0x40&lt;/code&gt;) is the correct
          DDRAM address for the start of the second line.
        &lt;p&gt;Awesome! Now we've discovered the LCD printing function! Go ahead and give it a name like lcd_print.&lt;/p&gt;&lt;p&gt;If you've followed along, give yourself a huge pat on the back. This was no easy feat! You've now got a pretty good understanding of how vintage synth binaries are reverse-engineered. Everything else involved in disassembling a synth's firmware is just a matter of applying these same ideas.&lt;/p&gt;&lt;head rend="h2"&gt;Going Further #&lt;/head&gt;&lt;head rend="h3"&gt;The MIDI Handling Routine #&lt;/head&gt;&lt;p&gt;After disassembling the LCD printing function, the next best way to figure out what's going on inside a synth ROM is to disassemble the function that parses incoming MIDI data. This function is an entry point to nearly every aspect of a synth's functionality. Disassembling it will allow you to trace the path of a particular MIDI message to its associated functionality. You can trace 'NOTE ON' and 'NOTE OFF' messages to find the code that handles starting and stopping individual voices; Or you can trace 'CONTROL CHANGE' messages to find the code that handles pitch bend or modulation.&lt;/p&gt;&lt;p&gt;I decided not to tackle this function in this article, as the DX7's MIDI parsing code is huge, and requires a lot of explanation. Parsing MIDI messages is always implemented via a straightforward state machine, and the code is nearly identical across different synths. Once you've seen how it works in one synth, you've seen how it works in nearly all of them.&lt;/p&gt;&lt;head rend="h3"&gt;Debugging the Firmware in an Emulator #&lt;/head&gt;&lt;p&gt;One of the best ways to understand what's going on inside a synth's firmware is to run it in an emulator. The MAME emulation framework is freely available, and already supports a wide variety of vintage synths. It features a built-in disassembler, and a debugger that can be used to step through the firmware instruction by instruction to see what's happening in detail. When I was working on my Yamaha DX9/7 project, I used MAME as a testing and development platform for the firmware.&lt;/p&gt;&lt;head rend="h3"&gt;Final Words #&lt;/head&gt;&lt;p&gt;The DX7, and its 8-bit CPU might be a bit primitive by today's standards, but the same principles apply to reverse-engineering modern devices. Instructions sets and calling-conventions might change, but whether it's a vintage 8-bit architecture like the 6800, or a cutting-edge 32-bit ARM system, the principles of how to disassemble device firmware remain the same.&lt;/p&gt;&lt;p&gt;If you have any questions about this article, please get in touch! If you have any corrections or suggestions, I'd love to hear from you. Thank you for reading!&lt;/p&gt;&lt;head rend="h2"&gt;Appendix: Why Choose This Synth? #&lt;/head&gt;&lt;head rend="h3"&gt;It Can Be Disassembled With Free Software&lt;/head&gt;&lt;p&gt;6303 binaries can be disassembled by using free and open source tools, such as Ghidra, F9DASM, and MAME's Universal Disassembler.&lt;/p&gt;&lt;head rend="h3"&gt;It's Well Documented&lt;/head&gt;&lt;p&gt;40 years on, the DX7 continues to captivate people's imaginations. As a result, lots is known about what goes on inside a DX7. Yamaha's service manuals are comprehensive, and freely available online.&lt;/p&gt;&lt;p&gt;Yamaha even released internal documentation on the DX7's architecture and sound chips, which is now available online.&lt;/p&gt;&lt;head rend="h3"&gt;Only One ROM&lt;/head&gt;&lt;p&gt;One advantage of reverse-engineering the DX7 is that there's only one ROM you need worry about. Technically there's also the sub-CPU and its mask ROM, but in this case you don't really need to worry what's going on there.&lt;/p&gt;&lt;p&gt;Some synths have important part of the firmware stored on the CPU's mask ROM, such as the Casio CZ-101. Other synths spread the synth's core functionality across multiple CPUs, each with their own ROMs, such as the Roland JX-8P. The DX7 is much simpler, having (nearly) all of its code in one place.&lt;/p&gt;&lt;head rend="h3"&gt;It Has an LCD Screen&lt;/head&gt;&lt;p&gt;Disassembling code for a system with a text-based user interface has a lot of advantages. I considered some of the early DCO-based Roland polysynths as candidates for this article, but without an LCD screen it's much harder to make headway into a ROM.&lt;/p&gt;&lt;head rend="h3"&gt;No Bank Switching&lt;/head&gt;&lt;p&gt;Unfortunately the various disassembler tools available don't handle bank switching very well. In Ghidra you can use 'Overlay' memory blocks to set up the different banks, however it's still not very intuitive in my experience.&lt;/p&gt;&lt;p&gt;I considered the Ensoniq ESQ-1 as a candidate for this article. It features a Motorola MC6809 processor, which is very well supported by lots of different debuggers. However it uses bank switching, which makes it a bit of a nuisance to disassemble.&lt;/p&gt;&lt;head rend="h4"&gt;What Is Bank Switching?&lt;/head&gt;&lt;p&gt;What happens if you need to squeeze 64KiB of firmware ROM, and 32KiB of RAM into your HD6303 chip's 16-bit address space? One solution to this problem is bank switching. Many vintage synths use bank-switching to fit their firmware into the CPU's address space.&lt;/p&gt;&lt;p&gt;Bank switching breaks a memory device's address space up into multiple 'banks' by latching one or more of its address lines to one of the CPU's I/O port lines. This allows the CPU to select which 'bank' is active by toggling the aforementioned I/O line in the software.&lt;/p&gt;&lt;p&gt; The Yamaha TX81Z features a 64KiB 27C512 EPROM chip, mapped into the CPU's address space at 0x8000 - 0xFFFF. The EPROM's A0-A14 pins are wired to the CPU's A0-A14, and the EPROM's &lt;code&gt;CE1&lt;/code&gt; pin is latched to the CPU's A15
          pin. The EPROM's A15 pin is wired to the CPU's I/O port 6 (pin
          &lt;code&gt;P63&lt;/code&gt; in the schematics). If the &lt;code&gt;P63&lt;/code&gt; I/O line
          is pulled high, the upper half of the EPROM's memory is
          selected, mapping addresses
          0x8000 - 0xFFFF into the CPU's
          address space. If it's pulled low, the EPROM's
          0x0000 to 0x7FFF memory is mapped to
          0x8000 - 0xFFFF.
        &lt;/p&gt;&lt;p&gt;To allow branching from code in one bank to code in another, a common technique is to use a 'trampoline function' located at the same address in both banks.&lt;/p&gt;&lt;head rend="h2"&gt;Appendix: Documentation #&lt;/head&gt;&lt;p&gt;Below is a list of all the important documentation referenced in the article.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;DX7 Schematic&lt;/item&gt;&lt;item&gt;DX7 Service Manual&lt;/item&gt;&lt;item&gt;Hitachi HD6303R Datasheet&lt;/item&gt;&lt;item&gt;HD6301/HD6303 Series Handbook&lt;/item&gt;&lt;item&gt;Hitachi HD44780 Datasheet&lt;/item&gt;&lt;item&gt;Intel 8255 Datasheet&lt;/item&gt;&lt;item&gt;74LS138 Datasheet&lt;/item&gt;&lt;/list&gt;&lt;list rend="ol"&gt;&lt;item&gt;The Hitachi 6303 microcontroller used in the DX7 includes both an internal, and external memory bus. The first 256 memory addresses in the 6303 point to the CPU's internal registers and on-board RAM. Many modern microcontrollers —such as the Atmel AVR, and Microchip PIC series— don't feature external address buses. It's more common for modern microcontrollers to communicate with peripheral devices over serial buses, using protocols such as SPI, or I2C. ↲&lt;/item&gt;&lt;item&gt;On boot, the 6800 CPU family fetches the reset vector from the fixed address of 0xFFFE. Knowing this, we could have just made an educated guess that the whole ROM was mapped to the high addresses. Still, it's always good to check your assumptions! ↲&lt;/item&gt;&lt;item&gt;If you're curious about why the 8255 PPI chip is used here, it's most likely because the LCD controller, cartridge, and portamento/sustain pedal interface don't feature a chip select interface. ↲&lt;/item&gt;&lt;item&gt;There are a variety of disassemblers available for the HD6303 architecture. The state-of-the-art graphical disassembler is arguably IDA Pro, but it's closed source, and prohibitively expensive for hobbyists. Non-graphical disassemblers also exist, such as F9DASM. If you're new to reverse-engineering, I'd personally recommend starting with Ghidra. It's free, open source, and easy to learn. ↲&lt;/item&gt;&lt;item&gt;I went down a bit of a rabbit-hole trying to find what year the LM016/HD44780 was first manufactured. The earliest reference I can find online is a 'preliminary' user's manual, dated March 1981. It's a shame that there's so little background information available about one of the best-known ICs in history. ↲&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ajxs.me/blog/Introduction_to_Reverse-Engineering_Vintage_Synth_Firmware.html"/><published>2025-10-20T02:56:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45639995</id><title>Entire Linux Network stack diagram (2024)</title><updated>2025-10-20T11:09:32.652725+00:00</updated><content>&lt;doc fingerprint="f5ed7a37fd9ba490"&gt;
  &lt;main&gt;
    &lt;p&gt; Published November 18, 2024 | Version v7 &lt;/p&gt;
    &lt;p&gt; Poster Open &lt;/p&gt;
    &lt;head rend="h1"&gt;Entire Linux Network stack diagram&lt;/head&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Diagram of entire Linux Network Stack, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Virtualization and Linux containers: &lt;list rend="ul"&gt;&lt;item&gt;Emulation and Paravirtualization.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Network sockets.&lt;/item&gt;
      &lt;item&gt;Network stack: &lt;list rend="ul"&gt;&lt;item&gt;Upper layer of Network stack (TCP, UDP).&lt;/item&gt;&lt;item&gt;Low layer of Network stack with GRO, RPS, RFS and GSO.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Network Scheduler.&lt;/item&gt;
      &lt;item&gt;NetFilter and traffic controll: &lt;list rend="ul"&gt;&lt;item&gt;Bridge and Bond interfaces.&lt;/item&gt;&lt;item&gt;Tap interface, ...&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Device Driver: &lt;list rend="ul"&gt;&lt;item&gt;Queue.&lt;/item&gt;&lt;item&gt;NAPI.&lt;/item&gt;&lt;item&gt;IRQ handler.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Network functions accelerated by NIC: &lt;list rend="ul"&gt;&lt;item&gt;Checksum offload, VLAN, VxLAN, GRE, TSO, LRO, RSS, ...&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Network card.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All (above) sections (layers) include tips for optimizations and/or statistics.&lt;/p&gt;
    &lt;p&gt;This diagram is part of the book:&lt;/p&gt;
    &lt;p&gt;Operativni sustavi i računalne mreže - Linux u primjeni&lt;/p&gt;
    &lt;p&gt;https://doi.org/10.5281/zenodo.8119310&lt;/p&gt;
    &lt;head rend="h2"&gt;Files&lt;/head&gt;
    &lt;head rend="h3"&gt; Linux Network Stack - EN.pdf &lt;/head&gt;
    &lt;head rend="h3"&gt; Files (5.4 MB) &lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Download all&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt; md5:a8f70808b4c1d2a4f33301fe7afd3ea1 &lt;/cell&gt;
        &lt;cell&gt;5.4 MB&lt;/cell&gt;
        &lt;cell&gt;Preview Download&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Additional details&lt;/head&gt;
    &lt;head rend="h3"&gt;Related works&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Is part of&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Book: 10.5281/zenodo.8119310 (DOI)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://zenodo.org/records/14179366"/><published>2025-10-20T03:33:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640226</id><title>Space Elevator</title><updated>2025-10-20T11:09:32.557266+00:00</updated><content/><link href="https://neal.fun/space-elevator/"/><published>2025-10-20T04:42:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640594</id><title>DeepSeek OCR</title><updated>2025-10-20T11:09:31.962723+00:00</updated><content>&lt;doc fingerprint="d0978f309aa0d982"&gt;
  &lt;main&gt;
    &lt;p&gt;📥 Model Download | 📄 Paper Link | 📄 Arxiv Paper Link |&lt;/p&gt;
    &lt;p&gt;Explore the boundaries of visual-text compression.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[2025/x/x]🚀🚀🚀 We release DeepSeek-OCR, a model to investigate the role of vision encoders from an LLM-centric viewpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Our environment is cuda11.8+torch2.6.0.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone this repository and navigate to the DeepSeek-OCR folder&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/deepseek-ai/DeepSeek-OCR.git&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Conda&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;conda create -n deepseek-ocr python=3.12.9 -y
conda activate deepseek-ocr&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Packages&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;download the vllm-0.8.5 whl&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118
pip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl
pip install -r requirements.txt
pip install flash-attn==2.7.3 --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Note: if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers&amp;gt;=4.51.1&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VLLM:&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;cd DeepSeek-OCR-master/DeepSeek-OCR-vllm&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;image: streaming output&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_image.py&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;pdf: concurrency ~2500tokens/s(an A100-40G)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_pdf.py&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;batch eval for benchmarks&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_eval_batch.py&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transformers&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;from transformers import AutoModel, AutoTokenizer
import torch
import os
os.environ["CUDA_VISIBLE_DEVICES"] = '0'
model_name = 'deepseek-ai/DeepSeek-OCR'

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)
model = model.eval().cuda().to(torch.bfloat16)

# prompt = "&amp;lt;image&amp;gt;\nFree OCR. "
prompt = "&amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;Convert the document to markdown. "
image_file = 'your_image.jpg'
output_path = 'your/output/dir'

res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)&lt;/code&gt;
    &lt;p&gt;or you can&lt;/p&gt;
    &lt;code&gt;cd DeepSeek-OCR-master/DeepSeek-OCR-hf
python run_dpsk_ocr.py&lt;/code&gt;
    &lt;p&gt;The current open-source model supports the following modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native resolution: &lt;list rend="ul"&gt;&lt;item&gt;Tiny: 512×512 （64 vision tokens）✅&lt;/item&gt;&lt;item&gt;Small: 640×640 （100 vision tokens）✅&lt;/item&gt;&lt;item&gt;Base: 1024×1024 （256 vision tokens）✅&lt;/item&gt;&lt;item&gt;Large: 1280×1280 （400 vision tokens）✅&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Dynamic resolution &lt;list rend="ul"&gt;&lt;item&gt;Gundam: n×640×640 + 1×1024×1024 ✅&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# document: &amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;Convert the document to markdown.
# other image: &amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;OCR this image.
# without layouts: &amp;lt;image&amp;gt;\nFree OCR.
# figures in document: &amp;lt;image&amp;gt;\nParse the figure.
# general: &amp;lt;image&amp;gt;\nDescribe this image in detail.
# rec: &amp;lt;image&amp;gt;\nLocate &amp;lt;|ref|&amp;gt;xxxx&amp;lt;|/ref|&amp;gt; in the image.
# '先天下之忧而忧'&lt;/code&gt;
    &lt;p&gt;We would like to thank Vary, GOT-OCR2.0, MinerU, PaddleOCR, OneChart, Slow Perception for their valuable models and ideas.&lt;/p&gt;
    &lt;p&gt;We also appreciate the benchmarks: Fox, OminiDocBench.&lt;/p&gt;
    &lt;p&gt;coming soon！&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/deepseek-ai/DeepSeek-OCR"/><published>2025-10-20T06:26:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640678</id><title>Bat v0.26.0 Released</title><updated>2025-10-20T11:09:31.279718+00:00</updated><content>&lt;doc fingerprint="73a61b3ecfa708c5"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;main&gt;
            &lt;turbo-frame&gt;
              &lt;div&gt;
                &lt;div&gt;
                  &lt;nav&gt;
                    &lt;ol&gt;
                      &lt;li&gt;
                        &lt;a&gt;Releases&lt;/a&gt;
                      &lt;/li&gt;
                      &lt;li&gt;
                        &lt;a&gt;v0.26.0&lt;/a&gt;
                      &lt;/li&gt;
                    &lt;/ol&gt;
                  &lt;/nav&gt;
                  &lt;div&gt;
                    &lt;div&gt;
                      &lt;div&gt;
                        &lt;div&gt;
                          &lt;div&gt;
                            &lt;select-panel&gt;
                              &lt;dialog-helper&gt;
                                &lt;button&gt;
                                  &lt;span&gt;
                                    &lt;span&gt;Compare&lt;/span&gt;
                                    &lt;span&gt;
                                      &lt;svg/&gt;
                                    &lt;/span&gt;
                                  &lt;/span&gt;
                                &lt;/button&gt;
                                &lt;dialog&gt;
                                  &lt;div&gt;
                                    &lt;focus-group&gt;
                                      &lt;div&gt;
                                        &lt;div&gt;
                                          &lt;include-fragment&gt;
                                            &lt;div&gt;
                                              &lt;div&gt;
                                                &lt;div&gt;
                                                  &lt;svg/&gt;
                                                  &lt;h2&gt;Sorry, something went wrong.&lt;/h2&gt;
                                                &lt;/div&gt;
                                              &lt;/div&gt;
                                            &lt;/div&gt;
                                          &lt;/include-fragment&gt;
                                        &lt;/div&gt;
                                        &lt;h2&gt;No results found&lt;/h2&gt;
                                      &lt;/div&gt;
                                    &lt;/focus-group&gt;
                                  &lt;/div&gt;
                                &lt;/dialog&gt;
                              &lt;/dialog-helper&gt;
                            &lt;/select-panel&gt;
                          &lt;/div&gt;
                        &lt;/div&gt;
                        &lt;div&gt;
                          &lt;h1&gt;v0.26.0&lt;/h1&gt;
                          &lt;h2&gt;Features&lt;/h2&gt;
                          &lt;h2&gt;Bugfixes&lt;/h2&gt;
                          &lt;ul&gt;
                            &lt;li&gt;Fix UTF-8 BOM not being stripped for syntax detection, see&lt;a&gt;#3314&lt;/a&gt;(&lt;a&gt;@krikera&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Fix&lt;code&gt;BAT_THEME_DARK&lt;/code&gt; and &lt;code&gt;BAT_THEME_LIGHT&lt;/code&gt; being ignored, see issue &lt;a&gt;#3171&lt;/a&gt;and PR&lt;a&gt;#3168&lt;/a&gt;(&lt;a&gt;@bash&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Prevent&lt;code&gt;--list-themes&lt;/code&gt; from outputting default theme info to stdout when it is piped, see &lt;a&gt;#3189&lt;/a&gt;(&lt;a&gt;@einfachIrgendwer0815&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Rename some submodules to fix Dependabot submodule updates, see issue&lt;a&gt;#3198&lt;/a&gt;and PR&lt;a&gt;#3201&lt;/a&gt;(&lt;a&gt;@victor-gp&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Make highlight tests fail when new syntaxes don't have fixtures PR&lt;a&gt;#3255&lt;/a&gt;(&lt;a&gt;@dan-hipschman&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Fix crash for multibyte characters in file path, see issue&lt;a&gt;#3230&lt;/a&gt;and PR&lt;a&gt;#3245&lt;/a&gt;(&lt;a&gt;@HSM95&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Add missing mappings for various bash/zsh files, see PR&lt;a&gt;#3262&lt;/a&gt;(&lt;a&gt;@AdamGaskins&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Send all bat errors to stderr by default, see&lt;a&gt;#3336&lt;/a&gt;(&lt;a&gt;@JerryImMouse&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Make --map-syntax target case insensitive to match --language, see&lt;a&gt;#3206&lt;/a&gt;(&lt;a&gt;@keith-hall&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Correctly determine the end of the line in UTF16LE/BE input&lt;a&gt;#3369&lt;/a&gt;(&lt;a&gt;@keith-hall&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;&lt;code&gt;--style=changes&lt;/code&gt; no longer prints a two-space indent when the file is unmodified, see issue &lt;a&gt;#2710&lt;/a&gt;and PR&lt;a&gt;#3406&lt;/a&gt;(&lt;a&gt;@jyn514&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Add missing shell completions, see&lt;a&gt;#3411&lt;/a&gt;(&lt;a&gt;@keith-hall&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Execute help/version/diagnostic commands even with invalid config/arguments present, see&lt;a&gt;#3414&lt;/a&gt;(&lt;a&gt;@keith-hall&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Fixed line numbers (&lt;code&gt;-n&lt;/code&gt;) and style components not printing when piping output, see issue &lt;a&gt;#2935&lt;/a&gt;and PR&lt;a&gt;#3438&lt;/a&gt;(&lt;a&gt;@lmmx&lt;/a&gt;)&lt;/li&gt;
                          &lt;/ul&gt;
                          &lt;h2&gt;Other&lt;/h2&gt;
                          &lt;ul&gt;
                            &lt;li&gt;Update base16 README links to community driven base16 work&lt;a&gt;#2871&lt;/a&gt;(&lt;a&gt;@JamyGolden&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Work around build failures when building&lt;code&gt;bat&lt;/code&gt; from vendored sources &lt;a&gt;#3179&lt;/a&gt;(&lt;a&gt;@dtolnay&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;CICD: Stop building for x86_64-pc-windows-gnu which fails&lt;a&gt;#3261&lt;/a&gt;(Enselic)&lt;/li&gt;
                            &lt;li&gt;CICD: CICD: replace windows-2019 runners with windows-2025&lt;a&gt;#3339&lt;/a&gt;(&lt;a&gt;@cyqsimon&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Build script: replace string-based codegen with quote-based codegen&lt;a&gt;#3340&lt;/a&gt;(&lt;a&gt;@cyqsimon&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Improve code coverage of&lt;code&gt;--list-languages&lt;/code&gt; parameter &lt;a&gt;#2942&lt;/a&gt;(&lt;a&gt;@sblondon&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Only start offload worker thread when there's more than 1 core&lt;a&gt;#2956&lt;/a&gt;(&lt;a&gt;@cyqsimon&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Update terminal-colorsaurus (the library used for dark/light detection) to 1.0, see&lt;a&gt;#3347&lt;/a&gt;(&lt;a&gt;@bash&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Update console dependency to 0.16, see&lt;a&gt;#3351&lt;/a&gt;(&lt;a&gt;@musicinmybrain&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Fixed some typos&lt;a&gt;#3244&lt;/a&gt;(&lt;a&gt;@ssbarnea&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Update onig_sys dependency to 69.9.1 to fix a gcc build failure&lt;a&gt;#3400&lt;/a&gt;(&lt;a&gt;@CosmicHorrorDev&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Add a cargo feature (&lt;code&gt;vendored-libgit2&lt;/code&gt;) to build with vendored libgit2 version without depending on the system's one &lt;a&gt;#3426&lt;/a&gt;(&lt;a&gt;@0x61nas&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Update syntect dependency to v5.3.0 to fix a few minor bugs, see&lt;a&gt;#3410&lt;/a&gt;(&lt;a&gt;@keith-hall&lt;/a&gt;)&lt;/li&gt;
                          &lt;/ul&gt;
                          &lt;h2&gt;Syntaxes&lt;/h2&gt;
                          &lt;ul&gt;
                            &lt;li&gt;Add syntax mapping for&lt;code&gt;paru&lt;/code&gt; configuration files &lt;a&gt;#3182&lt;/a&gt;(&lt;a&gt;@cyqsimon&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Add support for&lt;a&gt;Idris 2 programming language&lt;/a&gt;&lt;a&gt;#3150&lt;/a&gt;(&lt;a&gt;@buzden&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Add syntax mapping for&lt;code&gt;nix&lt;/code&gt;'s '&lt;code&gt;flake.lock&lt;/code&gt; lockfiles &lt;a&gt;#3196&lt;/a&gt;(&lt;a&gt;@odilf&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Improvements to CSV/TSV highlighting, with autodetection of delimiter and support for TSV files, see&lt;a&gt;#3186&lt;/a&gt;(@keith-&lt;/li&gt;
                            &lt;li&gt;Improve (Sys)log error highlighting, see&lt;a&gt;#3205&lt;/a&gt;(&lt;a&gt;@keith-hall&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Map&lt;code&gt;ndjson&lt;/code&gt; extension to JSON syntax, see &lt;a&gt;#3209&lt;/a&gt;(&lt;a&gt;@keith-hall&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Map files with&lt;code&gt;csproj&lt;/code&gt;, &lt;code&gt;vbproj&lt;/code&gt;, &lt;code&gt;props&lt;/code&gt; and &lt;code&gt;targets&lt;/code&gt; extensions to XML syntax, see &lt;a&gt;#3213&lt;/a&gt;(&lt;a&gt;@keith-hall&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Add debsources syntax to highlight&lt;code&gt;/etc/apt/sources.list&lt;/code&gt; files, see &lt;a&gt;#3215&lt;/a&gt;(&lt;a&gt;@keith-hall&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Add syntax definition and test file for GDScript highlighting, see&lt;a&gt;#3236&lt;/a&gt;(&lt;a&gt;@chetanjangir0&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Add syntax test file for Odin highlighting, see&lt;a&gt;#3241&lt;/a&gt;(&lt;a&gt;@chetanjangir0&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Update quadlet syntax mapping rules to cover quadlets in subdirectories&lt;a&gt;#3299&lt;/a&gt;(&lt;a&gt;@cyqsimon&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Add syntax Typst&lt;a&gt;#3300&lt;/a&gt;(&lt;a&gt;@cskeeters&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Map&lt;code&gt;.mill&lt;/code&gt; files to Scala syntax for Mill build tool configuration files &lt;a&gt;#3311&lt;/a&gt;(&lt;a&gt;@krikera&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Add syntax highlighting for VHDL, see&lt;a&gt;#3337&lt;/a&gt;(&lt;a&gt;@JerryImMouse&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Add syntax mapping for certbot certificate configuration&lt;a&gt;#3338&lt;/a&gt;(&lt;a&gt;@cyqsimon&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Update Lean syntax from Lean 3 to Lean 4&lt;a&gt;#3322&lt;/a&gt;(&lt;a&gt;@YDX-2147483647&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Map&lt;code&gt;.flatpakref&lt;/code&gt; and &lt;code&gt;.flatpakrepo&lt;/code&gt; files to INI syntax &lt;a&gt;#3353&lt;/a&gt;(&lt;a&gt;@Ferenc-&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Update hosts syntax&lt;a&gt;#3368&lt;/a&gt;(&lt;a&gt;@keith-hall&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Map&lt;code&gt;.kshrc&lt;/code&gt; files to Bash syntax &lt;a&gt;#3364&lt;/a&gt;(&lt;a&gt;@ritoban23&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Map&lt;code&gt;/var/log/dmesg&lt;/code&gt; files to Syslog syntax &lt;a&gt;#3412&lt;/a&gt;(&lt;a&gt;@keith-hall&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Add syntax definition and test file for Go modules(&lt;code&gt;go.mod&lt;/code&gt; and &lt;code&gt;go.sum&lt;/code&gt;) highlighting, see &lt;a&gt;#3424&lt;/a&gt;(&lt;a&gt;@DarkMatter-999&lt;/a&gt;)&lt;/li&gt;
                            &lt;li&gt;Syntax highlighting for typescript code blocks within Markdown files, see&lt;a&gt;#3435&lt;/a&gt;(&lt;a&gt;@MuntasirSZN&lt;/a&gt;)&lt;/li&gt;
                          &lt;/ul&gt;
                          &lt;h2&gt;Themes&lt;/h2&gt;
                        &lt;/div&gt;
                      &lt;/div&gt;
                    &lt;/div&gt;
                  &lt;/div&gt;
                &lt;/div&gt;
              &lt;/div&gt;
            &lt;/turbo-frame&gt;
          &lt;/main&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;svg/&gt;
        &lt;button&gt;
          &lt;svg/&gt;
        &lt;/button&gt;
        &lt;p&gt; You can’t perform that action at this time. &lt;/p&gt;
      &lt;/div&gt;
      &lt;template&gt;
        &lt;details&gt;
          &lt;details-dialog&gt;
            &lt;button&gt;
              &lt;svg/&gt;
            &lt;/button&gt;
          &lt;/details-dialog&gt;
        &lt;/details&gt;
      &lt;/template&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/sharkdp/bat/releases/tag/v0.26.0"/><published>2025-10-20T06:49:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640772</id><title>Major AWS Outage Happening</title><updated>2025-10-20T11:09:31.237421+00:00</updated><content/><link href="https://old.reddit.com/r/aws/comments/1obd3lx/dynamodb_down_useast1/"/><published>2025-10-20T07:11:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640838</id><title>AWS Multiple Services Down in us-east-1</title><updated>2025-10-20T11:09:30.942180+00:00</updated><link href="https://health.aws.amazon.com/health/status?ts=20251020"/><published>2025-10-20T07:22:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640877</id><title>Docker Systems Status: Full Service Disruption</title><updated>2025-10-20T11:09:30.813572+00:00</updated><content>&lt;doc fingerprint="3bbcb2c31d7475f1"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h5"&gt;Issues accessing Registry, Hub, Scout, DBC, DHIOperational&lt;/head&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;Docker Hub Registry, Docker Authentication, Docker Hub Web Services, Docker Billing, Docker Hub Automated Builds, Docker Hub Security Scanning, Docker Scout, Docker Build Cloud, Testcontainers Cloud, Docker Cloud, Docker Hardened Images&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 03:05 PDT&lt;lb/&gt;October 20, 2025 10:05 UTC&lt;/p&gt;
        &lt;p&gt;[Resolved] This incident is resolved. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 02:43 PDT&lt;lb/&gt;October 20, 2025 09:43 UTC&lt;/p&gt;
        &lt;p&gt;[Monitoring] We are seeing error rates recovering across our SaaS services. We continue to monitor as we process our backlog. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 01:22 PDT&lt;lb/&gt;October 20, 2025 08:22 UTC&lt;/p&gt;
        &lt;p&gt;[Identified] We have identified the underlying issue with one of our cloud service providers. We are monitoring the situation and prepare our systems for when the issues with our service provider resolve. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 00:16 PDT&lt;lb/&gt;October 20, 2025 07:16 UTC&lt;/p&gt;
        &lt;p&gt;[Investigating] We are seeing issues accessing and using our services across many of our products. We are currently investigating and will report back as soon as possible.. &lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dockerstatus.com/pages/incident/533c6539221ae15e3f000031/68f5e1c741c825463df7486c"/><published>2025-10-20T07:31:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45641143</id><title>Major AWS outage takes down Fortnite, Alexa, Snapchat, and more</title><updated>2025-10-20T11:09:30.708168+00:00</updated><content>&lt;doc fingerprint="d53cebc1efa289f7"&gt;
  &lt;main&gt;
    &lt;p&gt;Amazon Web Services (AWS) is currently experiencing a major outage that has taken down online services, including Amazon, Alexa, Snapchat, Fortnite, ChatGPT, Epic Games Store, Epic Online Services, and more. The AWS status checker is reporting that multiple services in the US-EAST-1 Region are “impacted” by operational issues, though outages also affected services in other regions globally.&lt;/p&gt;
    &lt;head rend="h1"&gt;Major AWS outage takes down Fortnite, Alexa, Snapchat, and more&lt;/head&gt;
    &lt;p&gt;The cause of the AWS outage is currently unclear.&lt;/p&gt;
    &lt;p&gt;The cause of the AWS outage is currently unclear.&lt;/p&gt;
    &lt;p&gt;As of 6:35AM ET, Amazon says that “most AWS Service operations are succeeding normally now” and that it is working towards a full resolution.&lt;/p&gt;
    &lt;p&gt;The AWS dashboard first reported issues affecting the US-EAST-1 Region at 3:11AM ET. “We are actively engaged and working to both mitigate the issue and understand root cause. We will provide an update in 45 minutes, or sooner if we have additional information to share,” Amazon said in an update published at 3:51AM ET.&lt;/p&gt;
    &lt;p&gt;Users on Reddit reported that the Alexa smart assistant was down and unable to respond to queries or complete requests, and in my own experience, I found that routines like pre-set alarms are not functioning. The AWS issue also appeared to be impacting platforms running on its cloud network, including Perplexity, Airtable, Canva, and the McDonalds app. The cause of the outage hasn’t been confirmed, and it’s unclear when regular service will be full restored.&lt;/p&gt;
    &lt;p&gt;“Perplexity is down right now,” Perplexity CEO Aravind Srinivas said on X. “The root cause is an AWS issue. We’re working on resolving it.”&lt;/p&gt;
    &lt;p&gt;In a 5:27AM ET update, Amazon says “We are seeing significant signs of recovery. Most requests should now be succeeding. We continue to work through a backlog of queued requests.”&lt;/p&gt;
    &lt;p&gt;AWS outages in the US-East-1 region have created widespread disruptions in 2023, 2021, and 2020, forcing multiple websites and platforms offline for several hours before regular service was restored.&lt;/p&gt;
    &lt;p&gt;Update, October 20th: added status updates from Amazon.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theverge.com/news/802486/aws-outage-alexa-fortnite-snapchat-offline"/><published>2025-10-20T08:12:09+00:00</published></entry></feed>