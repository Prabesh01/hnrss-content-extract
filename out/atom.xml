<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-24T15:38:13.282845+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45358433</id><title>My game's server is blocked in Spain whenever there's a football match on</title><updated>2025-09-24T15:38:20.571694+00:00</updated><content/><link href="https://old.reddit.com/r/gamedev/comments/1np6kyn/my_games_server_is_blocked_in_spain_whenever/"/><published>2025-09-24T10:26:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45358527</id><title>Preparing for the .NET 10 GC</title><updated>2025-09-24T15:38:20.327652+00:00</updated><content>&lt;doc fingerprint="2e6b5a97a927d570"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Preparing for the .NET 10 GC&lt;/head&gt;
    &lt;p&gt;In .NET 9 we enabled DATAS by default. But .NET 9 is not an LTS release so for many people they will be getting DATAS for the first time when they upgrade to .NET 10. This was a tough decision because GC features are usually the kind that don‚Äôt require user intervention ‚Äî but DATAS is a bit different. That‚Äôs why this post is titled ‚Äúpreparing for‚Äù instead of just ‚Äúwhat‚Äôs new‚Äù üòä.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre using Server GC, you might notice a performance profile that‚Äôs more noticeably different than what you saw in previous runtime upgrades. Memory usage may look drastically different (very likely smaller) ‚Äî and that may or may not be desirable. It all depends on whether the tradeoff is noticeable, and if it is, whether it aligns with your optimization goals. I‚Äôd recommend taking at least a quick look at your application performance metrics to see if you are happy with the results of this change. Many people will absolutely welcome it ‚Äî but if you are not one of them, don‚Äôt panic. I encourage you to read on to see whether it makes sense to simply turn DATAS off or if a bit of tuning could make it work in your favor.&lt;/p&gt;
    &lt;p&gt;I‚Äôll talk about how we generally decide which performance features to add, why DATAS is so different from typical GC features, and the tuning changes introduced since my last DATAS blog post. I‚Äôll also share two examples of how I tuned DATAS in first-party scenarios.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre mainly here to see which scenarios DATAS isn‚Äôt designed for ‚Äî to help decide whether to turn it off ‚Äî feel free to skip ahead to this section.&lt;/p&gt;
    &lt;head rend="h2"&gt;General policies of adding GC performance features&lt;/head&gt;
    &lt;p&gt;Most GC performance features ‚Äî whether it‚Äôs a new GC flavor, a new mechanism that enables the GC to do something it couldn‚Äôt before, or optimizations that improve an existing mechanism ‚Äî are typically lit up automatically when you upgrade to a new runtime version. We don‚Äôt require users to take action because these features are designed to improve a wide range of scenarios. In fact, that‚Äôs often why we choose to implement them: we analyze many scenarios to understand the most common problems, figure out what it would take to solve them, and then prioritize which ones to design and implement.&lt;/p&gt;
    &lt;p&gt;Of course, with any performance changes, there‚Äôs always the risk of regressions ‚Äî and for a framework used by millions, you‚Äôre guaranteed to regress someone. These regressions can be especially visible in microbenchmarks, where the behavior is so extreme that even small changes can cause wild swings in results.&lt;/p&gt;
    &lt;p&gt;A recent example being the change we made in how we handle the free regions for UOH (ie, LOH + POH) generations. We changed from the budget based trimming policy to an age based because it‚Äôs more robust in general (so we don‚Äôt either quickly decommit memory and have to recommit again, or keep extra free regions around even after a long time because we continue not consuming nearly all of the UOH budgets). But this can totally change a microbenchmark that used to observe the primary memory go down to a very low value after one GC.Collect() now requires 3 GC.Collect() calls (because we have to wait for the UOH free regions to age out in 2 gen2 GCs and the 3rd one will put it on the decommit list).&lt;/p&gt;
    &lt;p&gt;But for DATAS, we knew it was by definition not necessarily for a wide range of scenarios. As I mentioned in my last blog post, there were 2 specific kinds of scenarios that DATAS targeted. I‚Äôll reiterate them here ‚Äì&lt;/p&gt;
    &lt;p&gt;1. Bursty workloads running in memory constraint environments. DATAS aims to retract the heap size back when the application doesn‚Äôt require as much memory and grow it when the app requires more. This is especially important for apps running in containers with memory limits.&lt;/p&gt;
    &lt;p&gt;2. Small workloads using Server GC ‚Äî for example, if someone wants to try out a small asp.net core app to see what the experience is like in .NET, DATAS aims provide a heap size much more inline with what the small app actually needs.&lt;/p&gt;
    &lt;p&gt;I should give more explanation about 1). It‚Äôs not uncommon to see bursty workloads at all. If you have an app that handles requests, which is completely common, naturally you could have many more users during a specific time of the day than the rest of the day. However, the key here is the action that follows it ‚Äî if you have memory freed up during the non peak hours, what would you do with this memory? It turns out that sometimes folks don‚Äôt actually know ‚Äî they want to see the memory go down when the workload lightens, but they have no plans to do anything with this memory. And for some teams, they don‚Äôt need to the memory usage to go down because they already budgeted all that memory to their apps. I was just talking to a customer recently and when I asked them ‚Äúif DATAS frees up memory for you, what would you use for it?‚Äù. The answer was ‚Äúthat‚Äôs a good question, we never thought about it‚Äù.&lt;/p&gt;
    &lt;p&gt;For folks who do want to make use of the freed up memory, a common way is to use an orchestrated environment . DATAS makes this scenario more robust as heap sizes will be much more predictable, as I‚Äôll explain below, which helps with setting sensible memory limits. For example, in k8s, you can determine appropriate request and limit values for both non-peak and peak workloads to better leverage HPA. I have also seen teams that schedule tasks to run when the machines/VMs have free memory ‚Äî this is more involved (and these teams usually are equipped with a team of dedicated perf engineers) but gives them more control.&lt;/p&gt;
    &lt;p&gt;Then there are plenty of teams that have dedicated fleets of machines and want to maximize their throughput during peak hours as much as possible. They do not want to tolerate any type of slow down. They are definitely not the target of DATAS which will almost always regress their throughput ‚Äî when it comes to perf it‚Äôs rarely an all or none situation and I will discuss below how to make a decision if you should turn DATAS off.&lt;/p&gt;
    &lt;p&gt;All these made it difficult to make DATAS the default because we know there are a lot of teams that don‚Äôt want to sacrifice throughput at all or don‚Äôt make use of freed up memory.&lt;/p&gt;
    &lt;p&gt;I will discuss in detail below if you do want to look at the perf differences and make a decision if DATAS is for you or not (maybe when you see the memory reduction you will have ideas of using the freed up memory).&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance differences between DATAS and the traditional Server GC&lt;/head&gt;
    &lt;p&gt;DATAS is a GC feature that I spent more time explaining to my coworkers than any other ‚Äî being such a user visible feature, it naturally attracted more questions than pretty much any other GC features I added. And there were lots of misconceptions. Some thought that DATAS only affected startup; some assumed it would just ‚Äúreduce memory by x% and throughput by y%‚Äù; some expected it to ‚Äúmagically reducing memory without any other perf differences‚Äù (okay, I added the ‚Äúmagically‚Äù part üòÜ); and etc.&lt;/p&gt;
    &lt;p&gt;To understand the differences properly, we need to understand the difference in policies. First and foremost, Server GC does not adapt to the application size ‚Äî it was never a goal. Server GC looks mostly at the survival rate of each generation and does GCs based on that (there are a number of other factors that affect when GCs are triggered but survival rate is one of the most significant). In the last DATAS post I talked about the number of heaps which can affect the heap size significantly, especially in workloads that allocate a lot of temporary data. Since Server GC creates the same number of heaps as the number of cores the process is allowed to use, it means you can see very different heap sizes when running the same app with a different number of cores (by running it on a machine with a different number of cores or let your process use different number of cores on the same machine).&lt;/p&gt;
    &lt;p&gt;DATAS, on the other hand, aims to adapt to the application size which means you should see similar heap sizes even when the number of cores varies a lot. So there‚Äôs no ‚ÄúDATAS will reduce memory by X%‚Äù compared to Server GC.&lt;/p&gt;
    &lt;p&gt;If we look at the ‚ÄúMax heap size‚Äù metric for asp.net benchmarks, it‚Äôs obvious that Server GC behaves very differently when running on a 28-core machine (28c) vs a 12-core machine (12c) ‚Äì&lt;/p&gt;
    &lt;p&gt;Careful readers will notice that the order of which color is on top is not consistent. For example, for MultipleQueriesPlatform, the max heap size is actually much larger for 12c than 28c. Looking at the data in more detail reveals that the max heap size happens at the very beginning of the test for the 12c case ‚Äì&lt;/p&gt;
    &lt;p&gt;(Heap size (before) is right before a GC before that GC could possibly shrink the heap size. So ‚ÄúMax Heap Size‚Äù would be the max of this metric)&lt;/p&gt;
    &lt;p&gt;This is because at the beginning, there were a lot more allocations happened before the first GC happened on 28c with 28 heaps. So after that GC, a smaller survival rate was observed which caused the gen0 budget to be much smaller than on 12c. 12c quickly dropped to the steady state which has a much lower heap size than 28c. For steady state, these benchmarks always exhibit a much higher heap size on 28c. This illustrates 2 points ‚Äìif you just measure ‚Äúmax heap size‚Äù, it can easily be affected by the non-steady state behavior; secondly, the heap size can vary a lot due to the machine the test runs on. Note that these effects can be magnified because we are looking at small benchmarks, but the reasoning applies to real-world apps.&lt;/p&gt;
    &lt;p&gt;With DATAS we see this picture -&lt;/p&gt;
    &lt;p&gt;The max heap sizes are very similar on 28c and 12c which is exactly what DATAS is for ‚Äî it adapts to the application size.&lt;/p&gt;
    &lt;p&gt;Do I need to care if I‚Äôm using Workstation GC?&lt;/p&gt;
    &lt;p&gt;The answer depends on why you are using Workstation GC. If you are using Workstation GC because your workload simply does not call for using Server GC at all, then there‚Äôs no need to change. This could be due to your app being single threaded or the allocation is simply not stressful and you are totally fine with having one thread doing the collection work, in which case Workstation GC not only suffices but is exactly the correct choice to make.&lt;/p&gt;
    &lt;p&gt;But if you are using it because Server GC‚Äôs memory usage was too large and you are just using Workstation to limit the memory usage, you could find DATAS very attractive because it can both limit the memory usage and make the GC pauses lower with more GC threads doing the collection work.&lt;/p&gt;
    &lt;head rend="h2"&gt;How DATAS does its job&lt;/head&gt;
    &lt;p&gt;If you understood how DATAS does it job, it would be natural to arrive at the recommendations below for deciding if DATAS is for you. You could also skip this section, but I always like to understand how something works if I care about it, so I can come to my own conclusions instead of just memorizing some rules. In the last blog post I mentioned some details of DATAS at the time (.NET 8), noting that it would likely change dramatically ‚Äî and it did, both in design and implementation. The implementation we had in .NET 8 was mostly for functional ‚Äî we spent very little time in tuning. The majority of the tuning work happened after .NET 8.&lt;/p&gt;
    &lt;p&gt;The goal of DATAS is to adapt to the application size, or the LDS (Live Data Size). So there needs to be some way to adapt to it. Because the .NET GC is generational, it means we don‚Äôt collect the whole heap often. And since most full GCs we do are background GCs which don‚Äôt compact, it‚Äôs reasonable to approximate the LDS with the space objects take up in the old generations, i.e., (total size ‚Äî fragmentation). Another convenient number to use when you do your perf investigations is to look at the promoted size when a full GC is done.&lt;/p&gt;
    &lt;p&gt;In the last blog post I mentioned the conserve memory config is part of the DATAS implementation ‚Äî that part did not change. But conserve memory only affects when full GCs are triggered. For apps that allocate very frequently, unless these are temporary UOH objects, most of the GCs are ephemeral GCs. And ephemeral generation sizes can be a significant portion of the whole heap especially for small heaps.&lt;/p&gt;
    &lt;p&gt;After experimenting with various approaches, I settled on the approach of ‚Äúadapting to the app size while maintaining reasonable performance‚Äù which consisted of 2 key components -&lt;/p&gt;
    &lt;p&gt;1) introduced a concept of ‚ÄúBudget Computed via DATAS (BCD)‚Äù which is calculated based on the application size and gives us an upper bound of the gen0 budget for that size, which can approximate the generation size for gen0 (since there‚Äôs pinning it may not be exactly the generation size for gen0).&lt;/p&gt;
    &lt;p&gt;2) within this upper bound, we can further reduce memory if we can still maintain reasonable performance. And we define this ‚Äúreasonable performance‚Äù with a target Throughput Cost Percentage (TCP). This takes into consideration both GC pauses and how much allocating threads have to wait. But you can approximate TCP with % pause time in GC in steady state. The idea is to keep TCP around this target if we can, which means if the workload gets lighter, we‚Äôd be adjusting the gen0 budget smaller. And that in turn means gen0 will be smaller before the next GC, which translates to smaller heap size. The default target TCP is 2%. This can be changed via the GCDTargetTCP config.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs look at 2 example scenarios to see how this manifests. For simplicity, I‚Äôm ignoring background GCs, and I‚Äôll use % pause time in GC to approximate TCP.&lt;/p&gt;
    &lt;p&gt;Scenario A ‚Äî I have an e-commerce app which stores the whole catalog in memory, and this remains the same during the process lifetime. This is our LDS. Now the process starts to process requests and for each request there‚Äôs memory allocated and only used for the duration of that request.&lt;/p&gt;
    &lt;p&gt;During peak hours, it processes many concurrent requests. We hit our max budget which is our BCD. Let‚Äôs say this is 1gb, it means we are doing a GC each time 1GB is allocated. If we use the % pause time in GC to approximate TCP, let‚Äôs say during each second it allocates 1GB and observes one GC that has a 20ms pause. So the % time in GC is 2%. And that‚Äôs the same as our target TCP.&lt;/p&gt;
    &lt;p&gt;When it‚Äôs outside the peak hours and handling way fewer concurrent requests, let‚Äôs say we allocate ~200MB per second. If we keep our 1GB budget, it means we are doing a GC every 5s. And our % time in GC would be (20ms / 5s = 0.4%), much lower than 2%. So to reach the target TCP we‚Äôd want to reduce the budget and trigger a GC much sooner. If we reduce the budget to 200MB, and we‚Äôll still use 20ms as our GC pause just to make it simple (it‚Äôll likely be shorter as it‚Äôs roughly proportional to the survival and there‚Äôs likely less survival out of 200MB vs 1GB), now we are achieving 2% TCP again.&lt;/p&gt;
    &lt;p&gt;So for this scenario, the heap size is reduced by ~800MB when it‚Äôs outside peak hours. Depending on your total heap size, this can be a very significant reduction.&lt;/p&gt;
    &lt;p&gt;Scenario B is built on top of A but we‚Äôll throw in a cache that‚Äôs part of the LDS but gets smaller during lighter workload as we don‚Äôt need to cache as much. Because the LDS is smaller it means your BCD will be smaller as it‚Äôs a function of LDS. So during the lighter workload, the gen0 budget will be further reduced which again reflects the adapting to size nature. The conserve memory mechanism is still in effect too and would adjust the old generation budget and size accordingly.&lt;/p&gt;
    &lt;p&gt;Notice that so far I have not talked about the number of heaps at all! This is completely taken care of by DATAS itself so you don‚Äôt need to worry about it. Previously, some of our customers were using the GCHeapCount config to specify the number of heaps for Server GC. But DATAS makes it more robust as it can take advantage of more heaps if needed (which usually means shorter individual pause times) and reduces the heap size when the LDS goes down, without your having to specify a heap count yourself.&lt;/p&gt;
    &lt;p&gt;DATAS has specific events that indicate the actual TCP and LDS but that requires you to programmatically get them via the TraceEvent library. The approximations I mentioned above are sufficient for almost all perf investigations.&lt;/p&gt;
    &lt;head rend="h2"&gt;When DATAS might not be applicable to your scenario&lt;/head&gt;
    &lt;p&gt;If you read the previous sections, what‚Äôs listed below hopefully makes sense.&lt;/p&gt;
    &lt;p&gt;1) If you have no use for free memory, you don‚Äôt need DATAS&lt;/p&gt;
    &lt;p&gt;This one should be obvious ‚Äî why change it at all if you don‚Äôt have any use for the memory that gets freed up by DATAS anyway? You can turn DATAS off by the GCDynamicAdaptationMode config.&lt;/p&gt;
    &lt;p&gt;I‚Äôve come across a few first party teams who simply didn‚Äôt need DATAS ‚Äî they have dedicated machines to run their processes and have no use for free memory as they don‚Äôt plan to run anything else on the machine. So they have no use for DATAS. One team did say ‚Äúnow we probably want to think about taking advantage of free memory‚Äù (they were not thinking about it because Server GC isn‚Äôt aggressive at reducing memory usage). So for them, they will disable DATAS for now but will enable it when they can take advantage of memory during non peak hours.&lt;/p&gt;
    &lt;p&gt;2) If startup perf is critical, DATAS is not for you&lt;/p&gt;
    &lt;p&gt;DATAS always starts with 1 heap. We cannot predict how stressful your workload will be and since we are optimizing for size here, it starts with the smallest heap count which is 1. So if your startup perf is critical, you will see a regression because it takes time to go from 1 heap to multiple.&lt;/p&gt;
    &lt;p&gt;3) If you do not tolerate any throughput regression, DATAS may not be for you&lt;/p&gt;
    &lt;p&gt;If this includes the startup throughput, as 2) also states, DATAS is not for you. However, some scenarios aren‚Äôt concerned with startup perf so DATAS may or may not be desirable. Let‚Äôs say your % pause time in GC is 1% with Server GC, you can just set the GCDTargetTCP config to 1. If you were restricting the heap count you could very possibly see a perf improvement because the pause time can be shorter with DATAS. If the adaptation to the size aspect is beneficial to you, using DATAS can be a much better choice. But as stated in 1) if you don‚Äôt have any use for the freed up memory anyway, it wouldn‚Äôt justify spending time on using DATAS.&lt;/p&gt;
    &lt;p&gt;4) If you are doing mostly gen2 GCs, DATAS may not be for you&lt;/p&gt;
    &lt;p&gt;One case I haven‚Äôt spent much time tuning is when your scenario mostly does gen2 GCs (this is almost always due to excessive allocation of temporary large objects). If this is the case for you, and if you‚Äôve tried DATAS and weren‚Äôt happy with the results, I would suggest to disable DATAS. You could investigate to see if you can make it work by following the tuning section if it‚Äôs justified to spend the time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tuning DATAS if necessary&lt;/head&gt;
    &lt;p&gt;I‚Äôve tried DATAS on some first party workloads and in general it worked out great. I‚Äôll show a couple of examples where the default parameters of DATAS weren‚Äôt great but tuning one or 2 configs made it work.&lt;/p&gt;
    &lt;p&gt;Customer case 1&lt;/p&gt;
    &lt;p&gt;This is a server app running on dedicated machines. But they are in the process of containerizing it so there‚Äôs definitely merit to use DATAS. With DATAS they observed a 6.8% regression in throughput with a 10% reduction in working set. For now they‚Äôve disabled DATAS ‚Äî I will explain how I debugged it and determined what DATAS config to use to make it work if/when they want to enable DATAS.&lt;/p&gt;
    &lt;p&gt;Because DATAS limits the largest gen0 budget based on the LDS, we want to see if we are hitting that limit. It‚Äôd be easiest if you captured a GC trace with DATAS and one without DATAS. If you are seeing more GCs triggered, that means most likely you are hitting that limit.&lt;/p&gt;
    &lt;p&gt;You can approximate the TCP with what‚Äôs shown in the ‚Äú% Pause Time‚Äù column, and gen0 budget with the ‚ÄúGen0 Alloc MB‚Äù column. And you‚Äôd want to find the phase when you have the highest % pause time and see if you are triggering more GCs.&lt;/p&gt;
    &lt;p&gt;So for this particular customer, here are some excerpts of the GC (I‚Äôve trimmed down the columns of the GCStats view) ‚Äì&lt;/p&gt;
    &lt;p&gt;Without DATAS&lt;/p&gt;
    &lt;p&gt;With DATAS&lt;/p&gt;
    &lt;p&gt;Comparing their gen0 budget and % pause time in GC -&lt;/p&gt;
    &lt;p&gt;So gen0 budget without DATAS is 2.6x with DATAS. Another useful thing we notice is the % Pause Time is basically exactly the target TCP ‚Äî 2%. That tells us that this is working exactly as by design from DATAS‚Äôs POV. But without DATAS we got 2.6x budget so naturally we triggered GC less frequently and % pause time is 1.2 instead of 2.1.&lt;/p&gt;
    &lt;p&gt;But if we want to enable DATAS and not regress throughput for this phase, we‚Äôd like to have DATAS use a larger gen0 budget. To do that we should understand how DATAS determines the BCD. Since we are adapting to the size, we want to multiply the size with something. But this should not be a constant value because when the size is very small, this multiplier should be quite large ‚Äî if the LDS is only 2MB (which is totally possible for a tiny app), we wouldn‚Äôt want to trigger a GC for every 0.2MB of allocation ‚Äî the overhead would be too high. Let‚Äôs say we want to allow 20MB of allocation before triggering a GC, that makes the multiplier 10. But if the LDS is 20GB, we wouldn‚Äôt want to allocate 200GB before doing a GC, which means we want a much smaller multiplier. This means a power function but we also want to clamp it between a min and max value -&lt;/p&gt;
    &lt;code&gt;m = constant / sqrt (LDS);&lt;lb/&gt;// default for max_m is 10&lt;lb/&gt;m = min (max_m, m);&lt;lb/&gt;// default for min_m is 0.1&lt;lb/&gt;m = max (min_m, m);&lt;/code&gt;
    &lt;p&gt;The actual formula for the power function is&lt;/p&gt;
    &lt;code&gt;m = (20 - conserve_memory) / sqrt (LDS / 1000 / 1000);&lt;/code&gt;
    &lt;p&gt;which can be simplified to&lt;/p&gt;
    &lt;code&gt;m = (20 - conserve_memory) * 1000 / sqrt (LDS);&lt;lb/&gt;m = (20 - 5) * 1000 / sqrt (LDS);&lt;lb/&gt;m = 15000 / sqrt (LDS);&lt;/code&gt;
    &lt;p&gt;So the constant is 15000, or we could just say it‚Äôs 15 if we use MB for size. here‚Äôre some example with different LDS -&lt;/p&gt;
    &lt;p&gt;This constant, max_m and min_m can all be adjusted by configs. Please see the config page for detailed explanation.&lt;/p&gt;
    &lt;p&gt;Now it‚Äôs quite obvious why DATAS came up with the gen0 budget and how we can adjust it. If we want to bring this up to the same budget without DATAS, we‚Äôd want to use the GCDGen0GrowthPercent config to increase the constant to 2.6x, and increase min_m with the GCDGen0GrowthMinFactor config so it‚Äôs not clamped to 0.1 ‚Äî you don‚Äôt need to be very accurate since you just need to make it not be the limiting factor. So in this case if we use 15GB to approximate the LDS (the ‚ÄúPromoted (mb)‚Äù column for both gen2 GCs says ~15GB), and without DATAS the gen0 budget is 4.22GB. So min_m should be around (4.22/15 = 0.28). We can just set min_m to 300 which translates to 0.3 of LDS.&lt;/p&gt;
    &lt;p&gt;Customer case 2&lt;/p&gt;
    &lt;p&gt;This is an asp.net app on a staging server from the customer that represents one of their key scenarios. I used a load test tool to generate variable workloads.&lt;/p&gt;
    &lt;p&gt;The team was already using some GC configs -&lt;/p&gt;
    &lt;p&gt;¬∑ GCHeapCount is set to 2 to use 2 heaps&lt;/p&gt;
    &lt;p&gt;¬∑ Affinity is turned off with the GCNoAffinitize config.&lt;/p&gt;
    &lt;p&gt;If the GCHeapCount config is specified, DATAS would be disabled because it‚Äôs telling the GC to not change the heap count. And since changing the heap count is one of the key mechanisms to adjust perf for DATAS, it‚Äôs an indication to disable DATAS.&lt;/p&gt;
    &lt;p&gt;Because this is a process that co-exist with many others on the same machine, before DATAS was available they chose to give it 2 heaps to limit the memory usage while still getting reasonable throughput. But this is not flexible ‚Äî when the load becomes higher the throughput can suffer with 2 heaps and also the GC pauses can be noticeably higher since there‚Äôs only 2 GC threads collecting. They can adjust the number of GC heaps but that means more work and since Server GC isn‚Äôt very aggressive at reducing memory usage they can end up with a much bigger heap than desired when the load is lighter.&lt;/p&gt;
    &lt;p&gt;I‚Äôll demonstrate how using DATAS makes this robust. When I made the load pretty high I could see that % pause time in GC is quite high ‚Äî not surprising with just 2 heaps. So I enabled DATAS by simply getting rid of the GCHeapCount config (I kept the GCNoAffinitize config as I still wanted the GC threads to not be affinitized). I could see the % pause time in GC was also high because even with BCD we still ended up triggering GCs quite often. So I decided to make BCD 2x the default value with the GCDGen0GrowthPercent config (I didn‚Äôt need to use the GCDGen0GrowthMinFactor config since 2x is still well within our max_m/min_m clamping values). And now the process behaves in a much more desirable way with the following characteristics ‚Äì&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the % pause time is dramatically lower. With the default DATAS the % pause time is basically comparable and the heap size is noticeably lower. Depending on your optimization goal this could be exactly what you want. DATAS is able to achieve this with smaller budgets and more GC threads doing the collection work. But I know for this customer, they don‚Äôt want % pause time in GC to be this high as it affects their throughput. I could also make DATAS use a smaller target TCP but in this case the default TCP seems quite sufficient.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;individual GC pauses are a lot lower since we have a lot more GC threads collecting.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;when the load becomes lighter (# of concurrent client threads went from 200 to 100), the heap also becomes smaller. And we are still maintaining a much lower % pause time in GC and individual GC pauses.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I hope this helps with your DATAS tuning, if you need to do any.&lt;/p&gt;
    &lt;head rend="h2"&gt;DATAS Events&lt;/head&gt;
    &lt;p&gt;I expect most users never needing to look at these events, so I‚Äôll keep it brief. The approximations that I mentioned above should suffice. For the small number of folks who want to do a detailed analysis for whatever reason, DATAS fires an event that accurately represents the metrics we discussed. Note that we only use these events programmatically, so they are not surfaced in PerfView‚Äôs Events view (all you‚Äôll see is the GC/DynamicTraceEvent which shows you the name but not individual fields of that event). See this blog article for an example how to programmatically retrieve GC info as a list of TraceGC objects from a trace.&lt;/p&gt;
    &lt;p&gt;LDS and TCP are indicated in the SizeAdaptationTuning event, assuming you have a gc object of the type TraceGC ‚Äî&lt;/p&gt;
    &lt;code&gt;// LDS&lt;lb/&gt;gc.DynamicEvents().SizeAdaptationTuning?.TotalSOHStableSize&lt;lb/&gt;// TCP&lt;lb/&gt;gc.DynamicEvents().SizeAdaptationTuning?.TcpToConsider&lt;/code&gt;
    &lt;p&gt;This event is not fired every GC since we only check to see if we need to change the tuning for DATAS every few GCs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://maoni0.medium.com/preparing-for-the-net-10-gc-88718b261ef2"/><published>2025-09-24T10:37:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45358940</id><title>Huntington's disease treated for first time</title><updated>2025-09-24T15:38:19.832321+00:00</updated><content>&lt;doc fingerprint="2e099265b8279da2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Huntington's disease successfully treated for first time&lt;/head&gt;
    &lt;p&gt;One of the cruellest and most devastating diseases ‚Äì Huntington's ‚Äì has been successfully treated for the first time, say doctors.&lt;/p&gt;
    &lt;p&gt;The disease runs through families, relentlessly kills brain cells and resembles a combination of dementia, Parkinson's and motor neurone disease.&lt;/p&gt;
    &lt;p&gt;An emotional research team became tearful as they described how data shows the disease was slowed by 75% in patients.&lt;/p&gt;
    &lt;p&gt;It means the decline you would normally expect in one year would take four years after treatment, giving patients decades of "good quality life", Prof Sarah Tabrizi told BBC News.&lt;/p&gt;
    &lt;p&gt;The new treatment is a type of gene therapy given during 12 to 18 hours of delicate brain surgery.&lt;/p&gt;
    &lt;p&gt;The first symptoms of Huntington's disease tend to appear in your 30s or 40s and is normally fatal within two decades ‚Äì opening the possibility that earlier treatment could prevent symptoms from ever emerging.&lt;/p&gt;
    &lt;p&gt;Prof Tabrizi, director of the University College London Huntington's Disease Centre, described the results as "spectacular".&lt;/p&gt;
    &lt;p&gt;"We never in our wildest dreams would have expected a 75% slowing of clinical progression," she said.&lt;/p&gt;
    &lt;p&gt;None of the patients who have been treated are being identified, but one was medically retired and has returned to work. Others in the trial are still walking despite being expected to need a wheelchair.&lt;/p&gt;
    &lt;p&gt;Treatment is likely to be very expensive. However, this is a moment of real hope in a disease that hits people in their prime and devastates families.&lt;/p&gt;
    &lt;p&gt;Huntington's runs through Jack May-Davis' family. He has the faulty gene that causes the disease, as did his dad, Fred, and his grandmother, Joyce.&lt;/p&gt;
    &lt;p&gt;Jack said it was "really awful and horrible" watching his dad's inexorable decline.&lt;/p&gt;
    &lt;p&gt;The first symptoms appeared in Fred's late 30s, including changes in behaviour and the way he moved. He eventually needed 24/7 palliative care before he died at the age of 54, in 2016.&lt;/p&gt;
    &lt;p&gt;Jack is 30, a barrister's clerk, newly engaged to Chloe and has taken part in research at UCL to turn his diagnosis into a positive.&lt;/p&gt;
    &lt;p&gt;But he'd always known he was destined to share his father's fate, until today.&lt;/p&gt;
    &lt;p&gt;Now he says the "absolutely incredible" breakthrough has left him "overwhelmed" and able to look to a future that "seems a little bit brighter, it does allow me to think my life could be that much longer".&lt;/p&gt;
    &lt;p&gt;Huntington's disease is caused by an error in part of our DNA called the huntingtin gene.&lt;/p&gt;
    &lt;p&gt;If one of your parents has Huntington's disease, there's a 50% chance that you will inherit the altered gene and will eventually develop Huntington's too.&lt;/p&gt;
    &lt;p&gt;This mutation turns a normal protein needed in the brain ‚Äì called the huntingtin protein ‚Äì into a killer of neurons.&lt;/p&gt;
    &lt;p&gt;The goal of the treatment is to reduce levels of this toxic protein permanently, in a single dose.&lt;/p&gt;
    &lt;p&gt;The therapy uses cutting edge genetic medicine combining gene therapy and gene silencing technologies.&lt;/p&gt;
    &lt;p&gt;It starts with a safe virus that has been altered to contain a specially designed sequence of DNA.&lt;/p&gt;
    &lt;p&gt;This is infused deep into the brain using real-time MRI scanning to guide a microcatheter to two brain regions - the caudate nucleus and the putamen. This takes 12 to 18 hours of neurosurgery.&lt;/p&gt;
    &lt;p&gt;The virus then acts like a microscopic postman ‚Äì delivering the new piece of DNA inside brain cells, where it becomes active.&lt;/p&gt;
    &lt;p&gt;This turns the neurons into a factory for making the therapy to avert their own death.&lt;/p&gt;
    &lt;p&gt;The cells produce a small fragment of genetic material (called microRNA) that is designed to intercept and disable the instructions (called messenger RNA) being sent from the cells' DNA for building mutant huntingtin.&lt;/p&gt;
    &lt;p&gt;This results in lower levels of mutant huntingtin in the brain.&lt;/p&gt;
    &lt;p&gt;Results from the trial - which involved 29 patients - have been released in a statement by the company uniQure, but have not yet been published in full for review by other specialists.&lt;/p&gt;
    &lt;p&gt;The data showed that three years after surgery there was an average 75% slowing of the disease based on a measure which combines cognition, motor function and the ability to manage in daily life.&lt;/p&gt;
    &lt;p&gt;The data also shows the treatment is saving brain cells. Levels of neurofilaments in spinal fluid ‚Äì a clear sign of brain cells dying ‚Äì should have increased by a third if the disease continued to progress, but was actually lower than at the start of the trial.&lt;/p&gt;
    &lt;p&gt;"This is the result we've been waiting for," said Prof Ed Wild, consultant neurologist at the National Hospital for Neurology and Neurosurgery at UCLH.&lt;/p&gt;
    &lt;p&gt;"There was every chance that we would never see a result like this, so to be living in a world where we know this is not only possible, but the actual magnitude of the effect is breathtaking, it's very difficult to fully encapsulate the emotion."&lt;/p&gt;
    &lt;p&gt;He said he was "a bit teary" thinking about the impact it could have on families.&lt;/p&gt;
    &lt;p&gt;The treatment was considered safe, although some patients did develop inflammation from the virus that caused headaches and confusion that either resolved or needed steroid treatment.&lt;/p&gt;
    &lt;p&gt;Prof Wild anticipates the therapy "should last for life" because brain cells are not replaced by the body in the same manner as blood, bone and skin are constantly renewed.&lt;/p&gt;
    &lt;p&gt;Approximately 75,000 people have Huntington's disease in the UK, US and Europe with hundreds of thousands carrying the mutation meaning they will develop the disease.&lt;/p&gt;
    &lt;p&gt;UniQure says it will apply for a licence in the US in the first quarter of 2026 with the aim of launching the drug later that year. Conversations with authorities in the UK and Europe will start next year, but the initial focus is on the US.&lt;/p&gt;
    &lt;p&gt;Dr Walid Abi-Saab, the chief medical officer at uniQure, said he was "incredibly excited" about what the results mean for families, and added that the treatment had "the potential to fundamentally transform" Huntington's disease.&lt;/p&gt;
    &lt;p&gt;However, the drug will not be available for everyone due to the highly complex surgery and the anticipated cost.&lt;/p&gt;
    &lt;p&gt;"It will be expensive for sure," says Prof Wild.&lt;/p&gt;
    &lt;p&gt;There isn't an official price for the drug. Gene therapies are often pricey, but their long-term impact means that can still be affordable. In the UK, the NHS does pay for a ¬£2.6m-per-patient gene therapy for haemophilia B.&lt;/p&gt;
    &lt;p&gt;Prof Tabrizi says this gene therapy "is the beginning" and will open the gates for therapies that can reach more people.&lt;/p&gt;
    &lt;p&gt;She paid tribute to the "truly brave" volunteers who took part in the trial, saying she was "overjoyed for the patients and families".&lt;/p&gt;
    &lt;p&gt;She is already working with a group of young people who know they have the gene, but don't yet have symptoms ‚Äì known as stage zero Huntington's ‚Äì and is aiming to do the first prevention trial to see if the disease can be significantly delayed or even stopped completely.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.bbc.com/news/articles/cevz13xkxpro"/><published>2025-09-24T11:37:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45358980</id><title>Yt-dlp: Upcoming new requirements for YouTube downloads</title><updated>2025-09-24T15:38:19.042216+00:00</updated><content>&lt;doc fingerprint="806fb82cc43ebc72"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 10.2k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;Beginning very soon, you'll need to have the JavaScript runtime Deno installed to keep YouTube downloads working as normal.&lt;/head&gt;
    &lt;head rend="h2"&gt;Why?&lt;/head&gt;
    &lt;p&gt;Up until now, yt-dlp has been able to use its built-in JavaScript "interpreter" to solve the JavaScript challenges that are required for YouTube downloads. But due to recent changes on YouTube's end, the built-in JS interpreter will soon be insufficient for this purpose. The changes are so drastic that yt-dlp will need to leverage a proper JavaScript runtime in order to solve the JS challenges.&lt;/p&gt;
    &lt;head rend="h2"&gt;What do I need to do?&lt;/head&gt;
    &lt;head rend="h3"&gt;Everyone will need to install Deno.&lt;/head&gt;
    &lt;p&gt;yt-dlp will also need a few JavaScript components, and this may require additional action from you depending on how you installed yt-dlp:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Official PyInstaller-bundled executable users (e.g.&lt;/p&gt;&lt;code&gt;yt-dlp.exe&lt;/code&gt;,&lt;code&gt;yt-dlp_macos&lt;/code&gt;,&lt;code&gt;yt-dlp_linux&lt;/code&gt;, etc):&lt;list rend="ul"&gt;&lt;item&gt;No additional action required (besides having Deno). All the necessary JavaScript components will be bundled with these executables.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;PyPI package users (e.g. installed with&lt;/p&gt;&lt;code&gt;pip&lt;/code&gt;,&lt;code&gt;pipx&lt;/code&gt;, etc):&lt;list rend="ul"&gt;&lt;item&gt;Install and upgrade yt-dlp with the &lt;code&gt;default&lt;/code&gt;optional dependency group included, e.g.:&lt;code&gt;pip install -U "yt-dlp[default]"&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Install and upgrade yt-dlp with the &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Official zipimport binary users (the&lt;/p&gt;&lt;code&gt;yt-dlp&lt;/code&gt;Unix executable):&lt;list rend="ul"&gt;&lt;item&gt;Run yt-dlp with an additional flag to allow Deno to download &lt;code&gt;npm&lt;/code&gt;dependencies --or-- install yt-dlp's JS solver package in your Python environment. (The flag name and the package name are both still TBD.)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Run yt-dlp with an additional flag to allow Deno to download &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Third-party package users (e.g. installed with&lt;/p&gt;&lt;code&gt;pacman&lt;/code&gt;,&lt;code&gt;brew&lt;/code&gt;, etc):&lt;list rend="ul"&gt;&lt;item&gt;The action required will depend on how your third-party package repository decides to handle this change. But the options available for "official zipimport binary users" should work for you as well.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/yt-dlp/yt-dlp/issues/14404"/><published>2025-09-24T11:41:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359074</id><title>EU age verification app not planning desktop support</title><updated>2025-09-24T15:38:18.248173+00:00</updated><content>&lt;doc fingerprint="e70383268912763c"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 2&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Hi I found multiple usability issues with this solution.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The focus is so strong on the app, that it assumes everyone owns a smartphone. The other day I saw a granny on the bus with a phone that was 2cms thick and predates the famous Nokia 3310. How is she and other users without a smartphone supposed to verify their age online?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How will this impact the browsing experience on the web? Every website has GDPR checkboxes these days which somewhat disrupts browsing experience if browsing in for example incognito mode. Imagine if you want to browse the web privately. Websites don't know who you are so you will have to verify your age every single time. This makes the web unusable for anyone who wants to browse the web privately. Especially on a pc. A solution would be to have some sort of browser extension that handles it automatically. Since you at least claim to value privacy that could work. But it wouldn't really look trustworthy. Note this doesn't only apply to incognito but browsing the web in general. Like trying to compare various news sites. Doing this for every website to visit is a major hindrance usability wise.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;What will the cost be of implementing this? My trust in the EU to develop affordable and good technologies has diminished since we created a Peppol access point for our company. The solution was made using technologies only java has proper libraries for. Locking the developer to that language and eco system. Of course not a big issue for a big company. But a small start up won't be able to survive if they have to implement this.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/eu-digital-identity-wallet/av-doc-technical-specification/issues/22"/><published>2025-09-24T11:52:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359201</id><title>WiGLE: Wireless Network Mapping</title><updated>2025-09-24T15:38:17.816930+00:00</updated><content>&lt;doc fingerprint="8e2ed84303c8a523"&gt;
  &lt;main&gt;
    &lt;p&gt;Toggle navigation View Basic Search Advanced Search Map Uploads Info Android App FAQ App FAQ Forums Mastodon Bsky Twitter Stats Tools Downloads API Account CSV Upload Login User Name Password Forgot your password? keep me logged-in New? Register All the networks. Found by Everyone. &amp;lt;&amp;lt; Latitude Longitude SSID BSSID Date Range: 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 Possible FreeNet Possible Commercial Net No Labels Only Discovered By Me Only Discovered By Others Coloring: density QoS channel View: Greyscale Nightvision Standard Notes: Zoom in to see individual SSIDs. cell tower: blue QoS: Quality of Signal is a metric based on the number of observations and observers Statistics Over Time WiFi Networks Over Time [Full-screen Graph] WiFi Encryption Over Time [Full-screen Graph] [2 Years only Graph] Mouse-over graphs to interact with data. Select a range to zoom in, double click to zoom back out. Modify the number in the corner to smooth over multiple days. Full-screen graphs available! √ó √ó √ó √ó Join WiGLE √ó A Message from WiGLE&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wigle.net/index"/><published>2025-09-24T12:10:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359356</id><title>Rights groups urge UK PM Starmer to abandon plans for mandatory digital ID</title><updated>2025-09-24T15:38:17.420942+00:00</updated><content>&lt;doc fingerprint="8da196ac48835d9b"&gt;
  &lt;main&gt;
    &lt;p&gt;Big Brother Watch and other human rights, civil liberties, digital rights, and racial justice organisations have written to the Prime Minister urging him to abandon plans for a mandatory digital ID.&lt;/p&gt;
    &lt;p&gt;The letter comes just days before an expected statement from Keir Starmer announcing the rollout of a mandatory digital ID scheme aimed at deterring illegal immigration.&lt;/p&gt;
    &lt;p&gt;The leaders of Big Brother Watch, Article 19, Connected by Data, Liberty, Open Rights Group, The Runnymede Trust, Unlock Democracy and medConfidential argue that digital ID would change our relationship with the state, cause irreversible damage to our civil liberties, and fail to deter illegal immigration:&lt;/p&gt;
    &lt;p&gt;‚ÄúMandatory digital ID would fundamentally change the relationship between the population and the state by requiring frequent identity checks as we navigate our daily lives. Although the current digital ID proposals are being considered in the context of immigration, there is no guarantee that a future government would not make digital ID a requirement to access a range of public and private services.‚Äù&lt;/p&gt;
    &lt;p&gt;The joint letter can be found using this link.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spokespeople are available for interview. Please direct enquiries or requests for interviews to info@bigbrotherwatch.org.uk or 07730439257&lt;/item&gt;
      &lt;item&gt; Read Big Brother Watch‚Äôs recent report ‚ÄúCheckpoint Britain: the dangers of digital ID and why privacy must be protected‚Äù&lt;/item&gt;
      &lt;item&gt; Big Brother Watch will be hosting events on the dangers of digital ID during the Labour and Conservative Party conferences in Liverpool and Manchester. Contact info@bigbrotherwatch.org.uk for details‚Äù&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bigbrotherwatch.org.uk/press-releases/rights-groups-urge-starmer-to-abandon-plans-for-mandatory-digital-id/"/><published>2025-09-24T12:28:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359378</id><title>US Airlines Push to Strip Away Travelers' Rights by Rolling Back Key Protections</title><updated>2025-09-24T15:38:17.070030+00:00</updated><content>&lt;doc fingerprint="567c9a5a01baa55b"&gt;
  &lt;main&gt;
    &lt;p&gt;Home¬ªAIRLINE NEWS¬ª American Joins Delta, Southwest, United and Other US Airlines Push to Strip Away Travelers‚Äô Rights and Add More Fees by Rolling Back Key Protections in New Deregulation Move&lt;/p&gt;
    &lt;p&gt;American Joins Delta, Southwest, United and Other US Airlines Push to Strip Away Travelers‚Äô Rights and Add More Fees by Rolling Back Key Protections in New Deregulation Move&lt;/p&gt;
    &lt;p&gt;American Airlines joins with Delta, Southwest, United, and other US airlines are pushing to remove key protections for passengers and add more fees by rolling back rules, claiming it will lower costs and boost competition, but it may leave travelers with fewer rights and more hidden charges. Under the guise of lowering costs and boosting competition, these changes are likely to result in the erosion of consumer rights ‚Äì the right to cancel a ticket with an automatic refund, transparency of pricing, and the right to sit with your family on the same reservation. Airlines claim that removing these ‚Äòprotections‚Äô will decrease airfare and increase competition on the routes. The prospects for travelers, on the other hand, will likely be more fees, less certainty of receiving the service paid for, and diminished responsibility from the airlines for service failures. If these projections become reality, deregulation will aggravate the air travel experience for the consumer making it more expensive and more opaque.&lt;/p&gt;
    &lt;p&gt;The Airline Industry‚Äôs Deregulatory Push&lt;/p&gt;
    &lt;p&gt;The U.S. airline industry is pushing for a significant rollback of consumer protections, which many see as a major step backward for air travel. Airline lobbyists, representing carriers like American, Delta, Southwest, United, and the Airlines for America (A4A) association, have laid out a detailed agenda that would fundamentally alter the landscape of air travel, making it more difficult for passengers to know what they‚Äôre actually paying for and less likely to receive compensation when things go wrong.&lt;/p&gt;
    &lt;p&gt;This agenda centers on weakening or eliminating four major consumer protections:&lt;/p&gt;
    &lt;p&gt;Automatic Refunds for Cancellations: Airlines want to remove the requirement to provide automatic refunds when flights are cancelled or significantly altered. Passengers may instead receive only vouchers or no compensation at all, leaving them without recourse in the event of a major flight disruption.&lt;/p&gt;
    &lt;p&gt;Transparency of Fees: The airlines also aim to strip away rules that require them to disclose all fees (like baggage, seat assignments, and service charges) upfront. Instead of the clear, itemized pricing system that passengers currently rely on, airlines could hide fees until later in the booking process, making the true cost of a ticket much higher than expected.&lt;/p&gt;
    &lt;p&gt;Family Seating Guarantees: Under current regulations, airlines must ensure that families with young children are seated together without additional charges. This would no longer be guaranteed under the new proposal, meaning families could face extra costs just to sit next to one another.&lt;/p&gt;
    &lt;p&gt;Accessibility Protections for Disabled Passengers: The deregulation proposal also targets protections for disabled passengers, weakening their access to support and assistance during air travel.&lt;/p&gt;
    &lt;p&gt;The Airline Industry‚Äôs Argument: Deregulation as a Path to Lower Prices&lt;/p&gt;
    &lt;p&gt;The airline industry‚Äôs argument for deregulation is grounded in a belief that removing these protections will lead to lower prices, more competition, and better services for consumers. Airline lobbyists argue that deregulation, which began with the Airline Deregulation Act of 1978, has led to increased competition, lower airfares, and more choices for passengers.&lt;/p&gt;
    &lt;p&gt;Advertisement&lt;/p&gt;
    &lt;p&gt;However, while some might agree that competition can drive prices down, there‚Äôs a serious concern that deregulation could lead to more surprise charges and less accountability for airlines. Instead of benefiting consumers, deregulation may open the door for airlines to charge excessive fees for basic services, which are often hidden until later in the booking process. This could leave passengers paying far more than they anticipated and receiving less value for their money.&lt;/p&gt;
    &lt;p&gt;The Airlines‚Äô Detailed Deregulatory Agenda&lt;/p&gt;
    &lt;p&gt;The Airlines for America (A4A), the industry group representing major U.S. airlines, has strongly supported deregulation, arguing that it has benefited both airlines and passengers since the 1970s. In a recent document, A4A outlined their full deregulatory wish list, which includes the following key points:&lt;/p&gt;
    &lt;p&gt;Advertisement&lt;/p&gt;
    &lt;p&gt;Support for Deregulation: A4A strongly advocates for the continuation of deregulation, claiming that it has led to lower prices, increased competition, and better services for consumers. The group argues that removing regulations would allow airlines to better compete in the market and reinvest in improving services for passengers.&lt;/p&gt;
    &lt;p&gt;Criticism of the Biden Administration‚Äôs Regulatory Actions:&lt;/p&gt;
    &lt;p&gt;Ancillary Fee Transparency: A4A opposes the U.S. Department of Transportation‚Äôs (DOT) rules requiring airlines to disclose ancillary fees upfront, arguing that these rules exceed the DOT‚Äôs authority and don‚Äôt provide any clear benefits to consumers.&lt;/p&gt;
    &lt;p&gt;Refund Rules: A4A calls for the repeal or revision of refund rules that, according to them, go beyond what‚Äôs required by law, imposing unnecessary costs on airlines without providing any clear benefit to the public.&lt;/p&gt;
    &lt;p&gt;Flight Delay and Cancellations: The group also criticizes DOT‚Äôs policies on flight delays and cancellations, claiming the rules unfairly penalize airlines, particularly when disruptions are caused by factors beyond their control.&lt;/p&gt;
    &lt;p&gt;Deregulatory Priorities: A4A outlines several changes they would like to see the DOT pursue:&lt;/p&gt;
    &lt;p&gt;Rescinding Unlawful Regulations: A4A seeks the repeal of certain regulations, such as family seating and mobility aid assistance rules, which they argue exceed DOT‚Äôs authority.&lt;/p&gt;
    &lt;p&gt;Limiting Refund Rules: The group wants to limit DOT‚Äôs authority on flight refund rules, particularly for minor operational changes, such as changes to flight numbers or itineraries that don‚Äôt cause harm to passengers.&lt;/p&gt;
    &lt;p&gt;Economic Impact of Deregulation: A4A points to the success of deregulation, citing a rise in low-cost carriers, which have made air travel more affordable. They also highlight the significant decrease in airfare prices, which has directly benefited consumers.&lt;/p&gt;
    &lt;p&gt;Investment in Airline Operations: A4A argues that deregulation has allowed airlines to reinvest in their services, improving customer satisfaction and innovation.&lt;/p&gt;
    &lt;p&gt;Support for Technological Innovation: The airline industry is also backing the use of technology, including artificial intelligence (AI) and biometrics, to improve operational efficiency and the customer experience.&lt;/p&gt;
    &lt;p&gt;Why Deregulation is a Concern for Passengers&lt;/p&gt;
    &lt;p&gt;While the airline industry argues that deregulation will lead to lower prices and more competition, critics are skeptical. Here are the key reasons why deregulation could harm consumers:&lt;/p&gt;
    &lt;p&gt;More Hidden Fees: If airlines are no longer required to disclose fees upfront, passengers may face a barrage of surprise charges, from baggage fees to seat selection costs. The cost of air travel could increase significantly, even if base fares appear lower.&lt;/p&gt;
    &lt;p&gt;No Guarantees for Families: The proposal to eliminate the guarantee that families will be seated together without extra charges could lead to more stress for families travelling with young children. Parents may find themselves paying additional fees just to sit next to their kids.&lt;/p&gt;
    &lt;p&gt;Less Accountability for Cancellations: Airlines would have more power to decide whether or not to refund passengers for flight cancellations. This could lead to more vouchers, rather than cash refunds, leaving passengers at the mercy of airlines‚Äô own policies.&lt;/p&gt;
    &lt;p&gt;Weaker Protections for Disabled Passengers: The weakening of accessibility regulations could make it more difficult for passengers with disabilities to access the services and assistance they need during their travel.&lt;/p&gt;
    &lt;p&gt;Less Competition, Not More: While deregulation advocates claim it will lead to more competition, the reality is that fewer protections for consumers could allow major airlines to exploit passengers without fear of consequences. Smaller carriers may also struggle to compete on an uneven playing field.&lt;/p&gt;
    &lt;p&gt;The Risk of Over-Regulation vs. Consumer Protection&lt;/p&gt;
    &lt;p&gt;The battle between over-regulation and consumer protection is a complex issue. While it‚Äôs true that some regulations may stifle innovation, the protections that are in place help ensure fair treatment and transparency for consumers. The question is not whether airlines should be regulated, but how much regulation is necessary to strike a balance between profitability and protecting passengers.&lt;/p&gt;
    &lt;p&gt;In Europe, stricter regulations have led to fewer delays and cancellations, and the market remains competitive with budget airlines thriving under the current system. The fear is that deregulation in the U.S. could result in a situation where airlines dominate the market, and passengers are left with fewer rights and more fees.&lt;/p&gt;
    &lt;p&gt;What Passengers Can Do&lt;/p&gt;
    &lt;p&gt;As a passenger, it‚Äôs important to stay informed about these changes and advocate for your rights. Here are some steps you can take:&lt;/p&gt;
    &lt;p&gt;Stay Informed: Keep up with the latest news and updates on airline regulations.&lt;/p&gt;
    &lt;p&gt;Contact Your Representatives: Let your senators and congress members know how you feel about the deregulation of the airline industry.&lt;/p&gt;
    &lt;p&gt;Know Your Rights: Understand the protections you currently have and how they might change.&lt;/p&gt;
    &lt;p&gt;American Airlines, Delta, Southwest, United, and other U.S. airlines are pushing to remove key protections for passengers and add more fees by rolling back regulations, arguing that it will lower costs and increase competition, but it could also lead to fewer rights and more hidden charges for travelers.&lt;/p&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;The deregulation push by U.S. airlines is a major threat to passenger rights. While airlines argue that deregulation will lead to cheaper fares and more competition, the reality is likely to be more fees, less transparency, and fewer protections for passengers. If successful, this move could turn back the clock to a time when flying was riddled with hidden charges and unfair treatment.&lt;/p&gt;
    &lt;p&gt;The future of air travel depends on consumers, advocacy groups, and lawmakers standing up for passenger rights. The airline industry may be pushing for deregulation, but it‚Äôs up to the public to ensure that the changes made are in the best interest of all passengers, not just the airlines. The battle is not just about cheaper tickets, but about ensuring that air travel remains fair, transparent, and accountable for everyone.&lt;/p&gt;
    &lt;p&gt;We use cookies on our website to give you the most relevant experience by remembering your preferences and repeat visits. By clicking ‚ÄúAccept‚Äù, you consent to the use of ALL the cookies.&lt;/p&gt;
    &lt;p&gt;This website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience.&lt;/p&gt;
    &lt;p&gt;Necessary cookies are absolutely essential for the website to function properly. These cookies ensure basic functionalities and security features of the website, anonymously.&lt;/p&gt;
    &lt;p&gt;Cookie&lt;/p&gt;
    &lt;p&gt;Duration&lt;/p&gt;
    &lt;p&gt;Description&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-analytics&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Analytics".&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-functional&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;The cookie is set by GDPR cookie consent to record the user consent for the cookies in the category "Functional".&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-necessary&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookies is used to store the user consent for the cookies in the category "Necessary".&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-others&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Other.&lt;/p&gt;
    &lt;p&gt;cookielawinfo-checkbox-performance&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category "Performance".&lt;/p&gt;
    &lt;p&gt;viewed_cookie_policy&lt;/p&gt;
    &lt;p&gt;11 months&lt;/p&gt;
    &lt;p&gt;The cookie is set by the GDPR Cookie Consent plugin and is used to store whether or not user has consented to the use of cookies. It does not store any personal data.&lt;/p&gt;
    &lt;p&gt;Functional cookies help to perform certain functionalities like sharing the content of the website on social media platforms, collect feedbacks, and other third-party features.&lt;/p&gt;
    &lt;p&gt;Performance cookies are used to understand and analyze the key performance indexes of the website which helps in delivering a better user experience for the visitors.&lt;/p&gt;
    &lt;p&gt;Analytical cookies are used to understand how visitors interact with the website. These cookies help provide information on metrics the number of visitors, bounce rate, traffic source, etc.&lt;/p&gt;
    &lt;p&gt;Advertisement cookies are used to provide visitors with relevant ads and marketing campaigns. These cookies track visitors across websites and collect information to provide customized ads.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.travelandtourworld.com/news/article/american-joins-delta-southwest-united-and-other-us-airlines-push-to-strip-away-travelers-rights-and-add-more-fees-by-rolling-back-key-protections-in-new-deregulation-move/"/><published>2025-09-24T12:30:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359388</id><title>My Ed(1) Toolbox</title><updated>2025-09-24T15:38:16.644947+00:00</updated><content>&lt;doc fingerprint="f768818853b18a8d"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;My ed(1) Toolbox&lt;/head&gt;By Artyom Bologov&lt;p&gt;Apparently, I‚Äôm a huge ed(1) fan. I keep posting about it and use it as e.g. my Git editor, sudo editing tool, and my static site generator. But am I using it raw and standard as it is? Sometimes yes, but mostly no. This post is a listing of all the ed implementations and scripts I use.&lt;/p&gt;&lt;head rend="h2"&gt;GNU ed + red‚ÄîEternal Classics #&lt;/head&gt;&lt;p&gt;ed(1) is the standard text editor. And it‚Äôs available on most UNIX/POSIX-derived systems. (Some Linux distributions don‚Äôt provide it in default installation anymore, but that‚Äôs on them!) So relying on ed(1) and its powers is a good bet.&lt;/p&gt;&lt;p&gt;That‚Äôs why I always have GNU ed installed on my systems. It‚Äôs battle-tested, intuitive, and easily scriptable.&lt;/p&gt;&lt;p&gt;Bundled with GNU ed (and any POSIX-compliand ed(1) really), red(1) is the ‚Äúrestricted‚Äù version of ed(1). It‚Äôs locked to the directory it‚Äôs called in. And has no ability to pass through to the shell. I find it relatively useless though: I use ed(1) on secure systems and never allow anyone to access my precious ed(1) session. But still, red(1) is nice to have!&lt;/p&gt;&lt;head rend="h2"&gt;oed‚ÄîOpenBSD ed #&lt;/head&gt;&lt;p&gt;Now, GNU ed is not conforming to POSIX in some behaviors:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;It has more CLI flags&lt;/item&gt;&lt;item&gt; It has &lt;code&gt;wq&lt;/code&gt;&lt;/item&gt;&lt;item&gt;It has POSIX extended regular expressions (EREs,) while most other implementations don‚Äôt. Thus making EREs a non-portable extension&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I risk introducing non-portable behavior if I only focus on GNU ed. And I want to have my scripts (including my website build scripts) portable across implementations.&lt;/p&gt;&lt;p&gt; So I installed OpenBSD ed from the repository kindly provided by one of the maintainers. And now I can safely replace &lt;code&gt;ed&lt;/code&gt; with &lt;code&gt;oed&lt;/code&gt; for most of my scripts.
As the least effort shot at portability.
It‚Äôs too convenient to not use it now.

&lt;/p&gt;&lt;head rend="h2"&gt;wed‚Äîed wImproved #&lt;/head&gt;&lt;p&gt; I asked it on GNU ed mailing list whether they might support scripting abilities. Like sed(1) &lt;code&gt;-e&lt;/code&gt; and &lt;code&gt;-f&lt;/code&gt; flags or as a special executable for it.
One of the maintainers (predictably) replied that they (mostly) abide by POSIX and won‚Äôt add it.

&lt;/p&gt;&lt;p&gt;But! there was a person that emailed me personally and recommended slewsys ed as a version of ed(1) supporting scripts (among many other things.) So I installed it and called it wed(1) just to distinguish this re-implementation from The ed(1).&lt;/p&gt;&lt;p&gt;I don‚Äôt really use wed(1)‚ÄîI‚Äôm fine with standard ed(1) (and my scripting wrapper for it). Bust still, it‚Äôs a modern and user-friendly extension. Want to get started with ed(1) but don‚Äôt want to deviate from the tradition?‚Äîthis is the one to start with, probably.&lt;/p&gt;&lt;p&gt;Don‚Äôt get wed to an anime hologram. Get wed(1).&lt;/p&gt;&lt;head rend="h2"&gt;aed‚ÄîBlaphemy Against Minimalism #&lt;/head&gt;&lt;p&gt;I understand the complaints about ed(1) being somewhat hostile to new users. It‚Äôs usually mitigated by the time spend with this magnificent software. But still, ed(1) is not perfect and might need some modernization.&lt;/p&gt;&lt;p&gt;So I made aed(1) as a better and more interactive ed(1). It‚Äôs mostly abusing Readline and shell scripts to deliver a friendlier experience. With syntax highlighting and perfectly inline-editable inputs.&lt;/p&gt;&lt;p&gt;So once you‚Äôre comfortable with basic ed(1). (Or it‚Äôs slightly friendlier wed(1) version.) You might want to speed up your workflows with aed(1)!&lt;/p&gt;&lt;p&gt;I might‚Äôve gone too far though.&lt;/p&gt;&lt;head rend="h2"&gt;xed‚ÄîYou Don‚Äôt Need sed #&lt;/head&gt;&lt;p&gt;I have a user-friendly ed(1) on me now for interactive use. One use-case for ed(1) is not covered yet though‚Äîscripting! Having to do this type of newline-delimited scripts is too verbose compared to sed(1) ones.&lt;/p&gt;&lt;p&gt;What if I told you this is doable with a one-liner using my xed(1) script? Here:&lt;/p&gt;&lt;p&gt;It‚Äôs not the prettiest one, but it‚Äôs fulfilling many sed(1) use-cases. Speaking of the devil...&lt;/p&gt;&lt;head rend="h2"&gt;sed and ex... No. #&lt;/head&gt;&lt;p&gt;You don‚Äôt need ex(1) either, because it‚Äôs too vi(1)-oriented. They promised ed(1) eXtended, but we got ed(1) Fucked Up. Commands are incompatible with ed(1). Configuration is useless in ex(1) mode. Overall, ex(1) is just a poorly integrated back-end for vi(1).&lt;/p&gt;&lt;head rend="h2"&gt;My own ed(1) implementations #&lt;/head&gt;&lt;p&gt;If one likes some piece of software as an idea, they will inevitably try to reproduce it. So I did. I implemented ed(1) in Brainfuck under the aegis of Brainfuck Enterprise Solutions. I also did one in BASIC, pushing the limits of the no-memory BASIC as far as possible. And, finally, I did ed(1) in Modal, a term-rewriting-only system. All of these are useable... to a certain extent. But they don‚Äôt compare to the magnificence and purity of the Standard Text Editor.&lt;/p&gt;&lt;head rend="h2"&gt;Use ed(1) #&lt;/head&gt;&lt;p&gt;Whatever implementation you pick (pick aed(1)!), use it and love it. Because ed(1) deserves your love üòå&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aartaka.me/my-ed.html"/><published>2025-09-24T12:31:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359524</id><title>Learning Persian with Anki, ChatGPT and YouTube</title><updated>2025-09-24T15:38:16.576917+00:00</updated><content>&lt;doc fingerprint="9290b055cdb636d2"&gt;
  &lt;main&gt;
    &lt;p&gt;I‚Äôve been learning Persian (Farsi) for a while now, and I‚Äôm using a bunch of tools for it. The central one is certainly Anki, a spaced repetition app to train memory. I‚Äôm creating my own never-ending deck of cards, with different types of content, for different purposes. The most frequent type of cards is grammar focused phrases (very rarely single words) coming sometimes from my own daily life, but also very often directly from videos of the Persian Learning YouTube channel, created by Majid, a very talented and nice Persian teacher, in my opinion.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs take an example, suppose there is this slide in one of Majid‚Äôs videos:&lt;/p&gt;
    &lt;p&gt;From this, I will extract three screenshots (with the MacOS screenshot tool). First, to create a card of type ‚Äúbasic‚Äù (one side). I use this type of card to exercise my reading, which is very difficult and remains stubbornly slow, even though I know the 32 letters of the Persian alphabet quite well by now. But the different ways of writing them (which varies by their position in the word) and the fact that the vowels are not present makes it an enduringly challenging task.&lt;/p&gt;
    &lt;p&gt;The next type of card I create with the two remaining screenshots is ‚Äúbasic and reversed‚Äù, which actually creates two cards (one for each direction), one with some romanized phrase, and the other with the English or French translation:&lt;/p&gt;
    &lt;p&gt;When I review these cards in my daily Anki routine, this is where ChatGPT enters into play. First I have set a ‚ÄúPersian‚Äù project with these instructions:&lt;/p&gt;
    &lt;p&gt;With this project, every time I have a doubt or don‚Äôt remember something in Anki, I just take a screenshot and paste it in the project:&lt;/p&gt;
    &lt;p&gt;With this, I have an instant refresher on any notion, in any context. Sometimes I need to do this over and over, before it gels into a deeper, more instant and visceral ‚Äúknowledge‚Äù.&lt;/p&gt;
    &lt;p&gt;The next set of techniques is also based on YouTube. I use a Chrome extension called Dual Subtitles (which only works of course with videos having actual dual sources of subtitles):&lt;/p&gt;
    &lt;p&gt;The dual subtitles serve a couple of purposes: first as a source of new Anki cards (I create the cards directly, again with screenshots in the clipboard).&lt;/p&gt;
    &lt;p&gt;I also use the Tweaks for YouTube extension, which allows me to get extra keyboard shortcuts, to go back and forward only 1 second, instead of the built-in 5 seconds.&lt;/p&gt;
    &lt;p&gt;With these YouTube extensions, I have developed this particular ‚Äútechnique‚Äù to improve my vocal understanding:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I listen at 75% speed&lt;/item&gt;
      &lt;item&gt;I use the ‚Äúdual subtitles‚Äù browser extension to have both the Farsi and English subtitles at the same time (I set the Farsi one slightly bigger)&lt;/item&gt;
      &lt;item&gt;Every time a new sentence appears, I read it very quickly first in English (I pause if I need to), and then I listen carefully to the voice, to let the meaning and sound of Farsi infuse my mind (this part is very subtle but the most important: you must ‚Äúfeel‚Äù that you understand, and this feeling must cover even the words that you don‚Äôt know; because the meaning of the sentence is currently present and active in your mind, because you just read the English part, I believe that its mapping with the Farsi words that you then hear is particularly efficient, at least that‚Äôs my theory)&lt;/item&gt;
      &lt;item&gt;I also read the Farsi script, to improve my understanding, and disambiguate certain words for which it‚Äôs hard for me to hear what is exactly said&lt;/item&gt;
      &lt;item&gt;I repeat out loud what has been said also, which is quite important&lt;/item&gt;
      &lt;item&gt;Most importantly: I repeat this process (for a single video) over and over, in order to reach a stage where I genuinely understand what is said, in real-time, which is a very powerful and exhilarating feeling.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cjauvin.github.io/posts/learning-persian/"/><published>2025-09-24T12:45:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45359604</id><title>How to Lead in a Room Full of Experts</title><updated>2025-09-24T15:38:16.369118+00:00</updated><content>&lt;doc fingerprint="f3751d93156404b7"&gt;
  &lt;main&gt;
    &lt;p&gt;Here is a realization I made recently. I'm sitting in a room full of smart people. On one side are developers who understand the ins and outs of our microservice architecture. On the other are the front-end developers who can debug React in their sleep. In front of me is the product team that has memorized every possible user path that exists on our website. And then, there is me. The lead developer. I don't have the deepest expertise on any single technology.&lt;/p&gt;
    &lt;p&gt;So what exactly is my role when I'm surrounded by experts? Well, that's easy. I have all the answers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical Leadership&lt;/head&gt;
    &lt;p&gt;OK. Technically, I don't have all the answers. But I know exactly where to find them and connect the pieces together.&lt;/p&gt;
    &lt;p&gt;When the backend team explains why a new authentication service would take three weeks to build, I'm not thinking about the OAuth flows or JWT token validation. Instead, I think about how I can communicate it to the product team who expects it done "sometime this week." When the product team requests a "simple" feature, I'm thinking about the 3 teams that need to be involved to update the necessary microservices.&lt;/p&gt;
    &lt;p&gt;Leadership in technical environments isn't about being the smartest person in the room. It's about being the most effective translator.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leading is a Social Skill&lt;/head&gt;
    &lt;p&gt;I often get "eye rolls" when I say this to developers: You are not going to convince anyone with facts. In a room full of experts, your technical credibility gets you a seat at the table, but your social skills determine whether anything productive happens once you're there.&lt;/p&gt;
    &lt;p&gt;Where ideally you will provide documentation that everyone can read and understand, in reality, you need to talk to get people to understand. People can get animated when it comes to the tools they use. When the database team and the API team are talking past each other about response times, your role isn't to lay down the facts. Instead it's to read the room and find a way to address technical constraints and unclear requirements. It means knowing when to let a heated technical debate continue because it's productive, and when to intervene because it's become personal.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leading is Remembering the Goal&lt;/head&gt;
    &lt;p&gt;When you are an expert in your field, you love to dive deep. It's what makes you experts. But someone needs to keep one eye on the forest while everyone else is examining the trees.&lt;/p&gt;
    &lt;p&gt;I've sat through countless meetings where engineers debated the merits of different caching strategies while the real issue was that we hadn't clearly defined what "fast enough" meant for the user experience. The technical discussion was fascinating, but it wasn't moving us toward shipping.&lt;/p&gt;
    &lt;p&gt;As a leader, your job isn't to have sophisticated technical opinions. It's to ask how this "discussion" can move us closer to solving our actual problem.&lt;/p&gt;
    &lt;p&gt;When you understand a problem, and you have a room full of experts, the solution often emerges from the discussion. But someone needs to clearly articulate what problem we're actually trying to solve.&lt;/p&gt;
    &lt;p&gt;When a product team says customers are reporting the app is too slow, that's not a clear problem. It's a symptom. It might be that users are not noticing when the shopping cart is loaded, or that maybe we have an event that is not being triggered at the right time. Or maybe the app feels sluggish during peak hours. Each of those problems has different solutions, different priorities, and different trade-offs. Each expert might be looking at the problem with their own lense, and may miss the real underlying problem.&lt;/p&gt;
    &lt;p&gt;Your role as a leader is to make sure the problem is translated in a way the team can clearly understand the problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leading is Saying "I Don't Know"&lt;/head&gt;
    &lt;p&gt;By definition, leading is knowing the way forward. But in reality, in a room full of experts, pretending to know everything makes you look like an idiot.&lt;/p&gt;
    &lt;p&gt;Instead, "I don't know, but let's figure it out" becomes a superpower. It gives your experts permission to share uncertainty. It models intellectual humility. And it keeps the focus on moving forward rather than defending ego. It's also an opportunity to let your experts shine.&lt;/p&gt;
    &lt;p&gt;Nothing is more annoying than a lead who needs to be the smartest person in every conversation. Your database expert spent years learning how to optimize queries - let them be the hero when performance issues arise. Your security specialist knows threat models better than you, give them the floor when discussing architecture decisions.&lt;/p&gt;
    &lt;p&gt;Make room for some productive discussion. When two experts disagree about implementation approaches, your job isn't to pick the "right" answer. It's to help frame the decision in terms of trade-offs, timeline, and user impact.&lt;/p&gt;
    &lt;p&gt;Your value isn't in having all the expertise. It's in recognizing which expertise is needed when, and creating space for the right people to contribute their best work.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Translation Challenge&lt;/head&gt;
    &lt;p&gt;There was this fun blog post I read recently about how non-developers read tutorials written by developers. What sounds natural to you, can be complete gibberish to someone else. As a lead, you constantly need to think about your audience. You need to learn multiple languages to communicate the same thing:&lt;/p&gt;
    &lt;p&gt;Developer language: "The authentication service has a dependency on the user service, and if we don't implement proper circuit breakers, we'll have cascading failures during high load."&lt;/p&gt;
    &lt;p&gt;Product language: "If our login system goes down, it could take the entire app with it. We need to build in some safeguards, which will add about a week to the timeline but prevent potential outages."&lt;/p&gt;
    &lt;p&gt;Executive language: "We're prioritizing system reliability over feature velocity for this sprint. This reduces risk of user-facing downtime that could impact revenue."&lt;/p&gt;
    &lt;p&gt;All three statements describe the same technical decision, but each is crafted for its audience. Your experts shouldn't have to learn product speak, and your product team shouldn't need to understand circuit breaker patterns. But someone needs to bridge that gap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond "Because, that's why!"&lt;/head&gt;
    &lt;p&gt;"I'm the lead, and we are going to do it this way." That's probably the worst way to make a decision. That might work in the short term, but it erodes trust and kills the collaborative culture that makes expert teams thrive.&lt;/p&gt;
    &lt;p&gt;Instead, treat your teams like adults and communicate the reason behind your decision:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"We're choosing the more conservative approach because the cost of being wrong is high, and we can iterate later."&lt;/item&gt;
      &lt;item&gt;"I know this feels like extra work, but it aligns with our architectural goals and will save us time on the next three features."&lt;/item&gt;
      &lt;item&gt;"This isn't the most elegant solution, but it's the one we can ship confidently within our timeline."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The more comfortable you become with not being the expert, the more effective you become as a leader.&lt;/p&gt;
    &lt;p&gt;When you stop trying to out-expert the experts, you can focus on what expert teams actually need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clear problem definitions&lt;/item&gt;
      &lt;item&gt;Context for decision-making&lt;/item&gt;
      &lt;item&gt;Translation between different perspectives&lt;/item&gt;
      &lt;item&gt;Protection from unnecessary complexity&lt;/item&gt;
      &lt;item&gt;Space to do their best work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Your role isn't to have all the answers. It's to make sure the right questions get asked, the right people get heard, and the right decisions get made for the right reasons.&lt;/p&gt;
    &lt;p&gt;Technical leadership in expert environments is less about command and control, and more about connection and context. You're not the conductor trying to play every instrument. You're the one helping the orchestra understand what song they're playing together.&lt;/p&gt;
    &lt;p&gt;That's a much more interesting challenge than trying to be the smartest person in the room.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://idiallo.com/blog/how-to-lead-in-a-room-full-of-experts"/><published>2025-09-24T12:52:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45360475</id><title>Just Let Me Select Text</title><updated>2025-09-24T15:38:15.941385+00:00</updated><content>&lt;doc fingerprint="cb024a5731ccf78e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Just Let Me Select Text&lt;/head&gt;By Artyom Bologov&lt;head rend="h2"&gt;Untranslatable Bios #&lt;/head&gt;&lt;p&gt;I‚Äôm lonely. Like everyone-ish else. Naturally, I‚Äôm on Bumble. (Because Tinder is a rape-friendly lure trap.) When work calls get boring I inevitably start swiping (mostly left üò¢)&lt;/p&gt;&lt;p&gt;There are lots of tourists in Armenia in the summer. From all over the world really. Speaking a stupefying range of languages. With bios and prompt answers in these numerous languages. Not necessarily discernible to me due to my language learning stagnation.&lt;/p&gt;&lt;p&gt;So there‚Äôs this profile of a pretty German girl. With bio and prompts in (an undeniably beautiful) German. Speaking English, she made the decision to use her mother tongue for the bio. A totally valid choice.&lt;/p&gt;&lt;p&gt;So I want to know the story she tells with her profile:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Select her bio,&lt;/item&gt;&lt;item&gt;copy it,&lt;/item&gt;&lt;item&gt;paste into a translator,&lt;/item&gt;&lt;item&gt;look up the exact meaning of some mistranslated German word,&lt;/item&gt;&lt;item&gt;and realize the unexpected poetic meaning she put into these 300 chars.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Except‚Ä¶ I can‚Äôt do that. The text is not selectable/copyable in Bumble app. I have to do a bunch of relatively unsurmountable steps to do what should‚Äôve taken half a minute. Like screenshot the profile and scrape the text with iOS Photos text recognition. Or use some OCR (web)app elsewhere. It‚Äôs‚Ä¶ discouraging. Thus I give up and swipe left. A shame‚Äîshe was beautiful at the very least!&lt;/p&gt;&lt;head rend="h2"&gt;Media #&lt;/head&gt;&lt;p&gt;By making the text in your UI non-selectable, you turn it into‚Ä¶ an image essentially? Images, audio, video, and interactive JS-heavy pages are multidimentional media. Not really manipulable and referenceable in any reasonable way. (Not even with Media Fragments‚Äîthey were turned down by everyone.) You lose a whole dimension (ü•Å) of functionality and benefit by going with such media or their semblance text.&lt;/p&gt;&lt;p&gt; Podcasts are not easy to roll back to useful part. Video transcripts don‚Äôt make sense without the visuals. Web graphics are opaque &lt;code&gt;&amp;lt;canvas&amp;gt;&lt;/code&gt;-es you can‚Äôt gut.

&lt;/p&gt;&lt;p&gt;Text is copyable. Text is translatable. Text is accessible (as in a11y.) Text is lightweight. Text is fundamental to how we people process information.&lt;/p&gt;&lt;p&gt;That‚Äôs why we still use text in our UIs. We want to convey the meaning. We strive to provide unambiguous instructions. We need to be understood. So why make the text harder to process and understand?&lt;/p&gt;&lt;head rend="h2"&gt;Stop It #&lt;/head&gt;&lt;p&gt;Whenever you disable text selection/copying on your UI, you commit a crime against the user. Crime against comprehension. Crime against accessibility. Crime against the meaning. Stop incapacitating your users, allow them to finally use the text.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aartaka.me/select-text.html"/><published>2025-09-24T13:56:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45360787</id><title>Show HN: Mosaic ‚Äì A Kotlin framework for cleaner back end code</title><updated>2025-09-24T15:38:15.432679+00:00</updated><content>&lt;doc fingerprint="7c1be3350dca55d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Think from the response up, not the database down.&lt;/p&gt;
    &lt;p&gt;Mosaic is a Kotlin framework that transforms backend development through composable tiles that automatically handle caching, concurrency, and dependency resolution. Build complex responses by composing simple, testable pieces.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üß© Type-Safe Composition: Compile-time guarantees for all your data dependencies&lt;/item&gt;
      &lt;item&gt;‚ö° Zero Duplication: Call the same tile from anywhere - it fetches only once&lt;/item&gt;
      &lt;item&gt;üîÑ Out-of-the-Box Concurrency: Automatic parallel execution without the complexity&lt;/item&gt;
      &lt;item&gt;üß™ Natural Testability: Mock any tile, test in isolation&lt;/item&gt;
      &lt;item&gt;üì¶ Response-First Design: Build what you need, not how to get it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add Mosaic to your Gradle project:&lt;/p&gt;
    &lt;code&gt;dependencies {
  implementation("org.buildmosaic:mosaic-core:0.2.0")
  implementation("org.jetbrains.kotlinx:kotlinx-coroutines-core")
  testImplementation("org.buildmosaic:mosaic-test:0.2.0")
  testImplementation(kotlin("test"))
}&lt;/code&gt;
    &lt;code&gt;// A simple tile that fetches and caches data
val CustomerTile = singleTile {
  val customerId = source(CustomerIdKey) // Or source&amp;lt;String&amp;gt;("customerId")
  CustomerService.getCustomer(customerId)
}

// Parallel composition: These tiles run concurrently
val OrderSummaryTile = singleTile {
  // These run in parallel automatically!
  val orderDeferred = composeAsync(OrderTile)
  val customerDeferred = composeAsync(CustomerTile)
  val lineItemsDeferred = composeAsync(LineItemsTile)
  
  OrderSummary(
    order = orderDeferred.await(),
    customer = customerDeferred.await(),
    lineItems = lineItemsDeferred.await()
  )
}

// Sequential composition: Choose tiles based on previous results
val PaymentProcessorTile = singleTile {
  val customer = compose(CustomerTile)
  
  // Choose processor based on customer tier
  when (customer.tier) {
    CustomerTier.PREMIUM -&amp;gt; compose(PremiumProcessorTile)
    CustomerTier.BUSINESS -&amp;gt; compose(BusinessProcessorTile)
    else -&amp;gt; compose(StandardProcessorTile)
  }
}&lt;/code&gt;
    &lt;code&gt;// Imperative: manually orchestrating queries, passing data between functions
val order = orderRepository.findById(orderId)
val customer = customerRepository.findById(order.customerId) 
val lineItems = lineItemRepository.findByOrderId(orderId)
val productIds = lineItems.map { it.productId }
val products = productRepository.findByIds(productIds)
val prices = pricingService.getPrices(lineItems.map { it.sku })

// Data gets passed around everywhere - coupling and complexity
val enrichedItems = enrichLineItems(lineItems, products, prices)
val summary = buildOrderSummary(order, customer, enrichedItems)
val logistics = calculateLogistics(order, customer, enrichedItems)
// ... manual assembly, error handling, caching logic ...&lt;/code&gt;
    &lt;code&gt;// Declarative: tiles retrieve their own dependencies - no data passing!
val OrderPageTile = singleTile {
  val summaryDeferred = composeAsync(OrderSummaryTile)
  val logisticsDeferred = composeAsync(LogisticsTile)
  
  OrderPage(
    summary = summaryDeferred.await(),
    logistics = logisticsDeferred.await()
  )
}

// Each tile knows how to get what it needs - no coupling!
val OrderSummaryTile = singleTile {
  val orderDeferred = composeAsync(OrderTile)
  val customerDeferred = composeAsync(CustomerTile)
  val lineItemsDeferred = composeAsync(LineItemsTile)
  
  OrderSummary(
    order = orderDeferred.await(),
    customer = customerDeferred.await(),
    lineItems = lineItemsDeferred.await()
  )
}&lt;/code&gt;
    &lt;p&gt;Mosaic shines when composing tiles multiple levels deep. Each tile focuses on one responsibility:&lt;/p&gt;
    &lt;code&gt;// Level 1: Entry point tile
val OrderPageTile = singleTile {
  // Parallel execution of two major components
  val summaryDeferred = composeAsync(OrderSummaryTile)
  val logisticsDeferred = composeAsync(LogisticsTile)
  
  OrderPage(summaryDeferred.await(), logisticsDeferred.await())
}

// Level 2: Summary aggregates order data
val OrderSummaryTile = singleTile {
  // These three tiles run in parallel
  val orderDeferred = composeAsync(OrderTile)
  val customerDeferred = composeAsync(CustomerTile)
  val lineItemsDeferred = composeAsync(LineItemsTile)
  
  OrderSummary(
    order = orderDeferred.await(),
    customer = customerDeferred.await(),
    lineItems = lineItemsDeferred.await()
  )
}

// Level 3: Line items enriches with product and pricing data
val LineItemsTile = singleTile {
  val order = compose(OrderTile)
  
  // Batch fetch products and prices in parallel
  val productsDeferred = composeAsync(ProductsByIdTile, order.productIds)
  val pricesDeferred = composeAsync(PricingBySkuTile, order.skus)
  val products = productsDeferred.await()
  val prices = pricesDeferred.await()
      
  order.items.map { item -&amp;gt;
    LineItemDetail(
      product = products[item.productId],
      price = prices[item.sku],
      quantity = item.quantity
    )
  }
}&lt;/code&gt;
    &lt;p&gt;Call the same tile from multiple places without redundant fetches:&lt;/p&gt;
    &lt;code&gt;val OrderTotalTile = singleTile {
  // This calls LineItemsTile
  val lineItems = compose(LineItemsTile)
  lineItems.sumOf { it.price.amount * it.quantity }
}

val TaxCalculatorTile = singleTile {
  // Also calls LineItemsTile - but it's already cached!
  val lineItems = compose(LineItemsTile)
  val address = compose(AddressTile)
  TaxService.calculate(lineItems, address)
}

// In your controller:
val orderPage = mosaic.compose(OrderPageTile)    // Fetches LineItemsTile
val orderTotal = mosaic.compose(OrderTotalTile)  // Uses cached LineItemsTile
val tax = mosaic.compose(TaxCalculatorTile)      // Uses cached LineItemsTile
// LineItemsTile was only fetched ONCE!&lt;/code&gt;
    &lt;p&gt;Canvas provides hierarchical dependency injection that separates application-level dependencies from request-specific data. This enables clean separation of concerns and efficient resource management.&lt;/p&gt;
    &lt;code&gt;// Create your main application canvas with long-lived dependencies
val applicationCanvas = canvas {
  // Database connections
  single&amp;lt;DataSource&amp;gt; { 
    HikariDataSource().apply {
      jdbcUrl = "jdbc:postgresql://localhost:5432/myapp"
      username = "user"
      password = "password"
    }
  }
  
  // Services that depend on the database
  single&amp;lt;UserService&amp;gt; { 
    UserServiceImpl(source&amp;lt;DataSource&amp;gt;()) 
  }
  
  single&amp;lt;OrderService&amp;gt; { 
    OrderServiceImpl(source&amp;lt;DataSource&amp;gt;()) 
  }
  
  // External API clients
  single&amp;lt;PaymentClient&amp;gt; {
    PaymentClientImpl(apiKey = System.getenv("PAYMENT_API_KEY"))
  }
  
  // Configuration
  single&amp;lt;AppConfig&amp;gt; { loadAppConfig() }
}&lt;/code&gt;
    &lt;code&gt;// In your controller/handler, add request-specific data as a layer
suspend fun handleOrderRequest(orderId: String, userId: String) {
  val requestMosaic = applicationCanvas.withLayer {
    // Request-specific data
    single&amp;lt;String&amp;gt;("orderId") { orderId }
    single&amp;lt;String&amp;gt;("userId") { userId }
    single&amp;lt;Instant&amp;gt;("requestTime") { Instant.now() }
    
    // You can also override application dependencies for testing
    // single&amp;lt;PaymentClient&amp;gt; { MockPaymentClient() }
  }.create()
  
  // Use the mosaic with both application and request dependencies
  val orderPage = requestMosaic.compose(OrderPageTile)
  return orderPage
}&lt;/code&gt;
    &lt;code&gt;// Tiles can access both application and request dependencies
val OrderTile = singleTile {
  val orderId = source&amp;lt;String&amp;gt;("orderId")
  val orderService = source&amp;lt;OrderService&amp;gt;()  // From application canvas
  orderService.getOrder(orderId)
}

val CustomerTile = singleTile {
  val userId = source&amp;lt;String&amp;gt;("userId")
  val userService = source&amp;lt;UserService&amp;gt;()    // From application canvas
  userService.getUser(userId)
}

val PaymentTile = singleTile {
  val order = compose(OrderTile)
  val paymentClient = source&amp;lt;PaymentClient&amp;gt;() // From application canvas
  val requestTime = source&amp;lt;Instant&amp;gt;("requestTime") // From request layer
  
  paymentClient.getPaymentStatus(order.paymentId, requestTime)
}

// Complex tile that uses multiple dependencies
val OrderSummaryTile = singleTile {
  val orderDeferred = composeAsync(OrderTile)
  val customerDeferred = composeAsync(CustomerTile) 
  val paymentDeferred = composeAsync(PaymentTile)
  
  // All tiles have access to the same dependency context
  OrderSummary(
    order = orderDeferred.await(),
    customer = customerDeferred.await(),
    payment = paymentDeferred.await()
  )
}&lt;/code&gt;
    &lt;code&gt;// Define typed keys for better compile-time safety
object OrderIdKey : CanvasKey&amp;lt;String&amp;gt;(String::class, "orderId")
object UserIdKey : CanvasKey&amp;lt;String&amp;gt;(String::class, "userId")

// Use in canvas configuration
val requestMosaic = applicationCanvas.withLayer {
  single(OrderIdKey) { orderId }
  single(UserIdKey) { userId }
}.create()

// Use in tiles
val OrderTile = singleTile {
  val orderId = source(OrderIdKey)  // Type-safe!
  val orderService = source&amp;lt;OrderService&amp;gt;()
  orderService.getOrder(orderId)
}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Separation of Concerns: Application dependencies separate from request data&lt;/item&gt;
      &lt;item&gt;Resource Efficiency: Database connections and services created once, reused across requests&lt;/item&gt;
      &lt;item&gt;Testing Flexibility: Override any dependency at any layer for testing&lt;/item&gt;
      &lt;item&gt;Type Safety: Compile-time guarantees for dependency resolution&lt;/item&gt;
      &lt;item&gt;Automatic Cleanup: Canvas implements &lt;code&gt;AutoCloseable&lt;/code&gt;for resource management&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MultiTile abstracts batching strategy from consumers. Key insight: if you request the same key multiple times, even in different lists, Mosaic automatically deduplicates and only fetches uncached keys.&lt;/p&gt;
    &lt;code&gt;// Strategy 1: Large batch operations (efficient for bulk APIs)
val PricingBySkuTile = multiTile { skus -&amp;gt;
  // Single bulk API call - efficient for services that support batch operations
  PricingService.getBulkPrices(skus.toList())
}

// Strategy 2: Individual requests (for APIs without batch support)
val ProductByIdTile = perKeyTile { productId -&amp;gt;
  // Make individual calls concurrently when no batch API exists
  ProductService.getProduct(productId)
}

// Strategy 3: Chunked requests (respect API rate limits)
val InventoryBySkuTile = chunkedMultiTile(10) { skus -&amp;gt;
  // API only allows 10 items per request - chunk to respect limits
  InventoryService.getInventory(skus)
}

// Consumer code - batching is completely abstracted:
val prices1 = mosaic.compose(PricingBySkuTile, listOf("SKU1", "SKU2"))
val prices2 = mosaic.compose(PricingBySkuTile, listOf("SKU2", "SKU3"))
// SKU2 is only fetched ONCE - automatically deduplicated!&lt;/code&gt;
    &lt;p&gt;Testing complex APIs is hard. Mosaic makes it trivial.&lt;/p&gt;
    &lt;p&gt;In traditional backends, testing requires intricate mocking of repositories, services, and data flow. With Mosaic, you mock individual tiles and test compositions in complete isolation.&lt;/p&gt;
    &lt;code&gt;// Test a complex 3-level composition by mocking just the dependencies
@Test
fun `order page composes correctly`() = runTest {
  val testMosaic = TestMosaicBuilder(this)
    .withMockTile(OrderSummaryTile, mockSummary)
    .withMockTile(LogisticsTile, mockLogistics)
    .build()
  
  // Test the composition logic without any external dependencies
  testMosaic.assertEquals(
    tile = OrderPageTile,
    expected = OrderPage(mockSummary, mockLogistics)
  )
}

// Test error propagation through the composition chain
@Test
fun `handles service failures gracefully`() = runTest {
  val testMosaic = TestMosaicBuilder(this)
    .withMockTile(OrderTile, mockOrder)
    .withFailedTile(CustomerTile, CustomerServiceException("Service down"))
    .withMockTile(LineItemsTile, mockLineItems)
    .build()
  
  // Verify the error bubbles up correctly
  testMosaic.assertThrows(
    tile = OrderSummaryTile,
    expectedException = CustomerServiceException::class
  )
}

// Test performance characteristics and timeouts
@Test  
fun `handles slow external services`() = runTest {
  val testMosaic = TestMosaicBuilder(this)
    .withDelayedTile(ExternalApiTile, mockData, delayMs = 500)
    .build()
  
  val startTime = System.currentTimeMillis()
  testMosaic.assertEquals(ExternalApiTile, mockData)
  val elapsed = System.currentTimeMillis() - startTime
  
  assertTrue(elapsed &amp;gt;= 500, "Should respect external service latency")
}&lt;/code&gt;
    &lt;p&gt;Why this matters: In a traditional API with 20+ services, you'd need to mock databases, HTTP clients, message queues, and coordinate complex test data. With Mosaic, you mock 2-3 tiles and test your composition logic in isolation.&lt;/p&gt;
    &lt;code&gt;@Configuration
class MosaicConfig {
  @Bean
  fun mosaicCanvas(): Canvas = runBlocking {
    canvas {
      // Register your dependencies here
      single&amp;lt;UserService&amp;gt; { UserServiceImpl() }
      single&amp;lt;DatabaseConfig&amp;gt; { loadConfig() }
    }
  }
}

@RestController
class OrderController(private val canvas: Canvas) {
  @GetMapping("/orders/{id}")
  fun getOrder(@PathVariable id: String): OrderPage = runBlocking {
    val mosaic = canvas.withLayer {
      single(OrderKey.qualifier) { id }
    }.create()
    mosaic.compose(OrderPageTile)
  }
    
  @GetMapping("/orders/{id}/total")
  fun getOrderTotal(@PathVariable id: String): Double = runBlocking {
    val mosaic = canvas.withLayer {
      single(OrderKey.qualifier) { id }
    }.create()
    mosaic.compose(OrderTotalTile)
  }
}&lt;/code&gt;
    &lt;code&gt;fun Application.module() {
  install(ContentNegotiation) { json() }
  
  val canvas = runBlocking {
    canvas {
      // Register your dependencies here
      single&amp;lt;UserService&amp;gt; { UserServiceImpl() }
      single&amp;lt;DatabaseConfig&amp;gt; { loadConfig() }
    }
  }
  
  routing {
    get("/orders/{id}") {
      val orderId = call.parameters["id"] ?: error("Missing order ID")
      val mosaic = canvas.withLayer {
        single(OrderKey.qualifier) { orderId }
      }.create()
      val orderPage = mosaic.compose(OrderPageTile)
      call.respond(orderPage)
    }
    
    get("/orders/{id}/total") {
      val orderId = call.parameters["id"] ?: error("Missing order ID")
      val mosaic = canvas.withLayer {
        single(OrderKey.qualifier) { orderId }
      }.create()
      val total = mosaic.compose(OrderTotalTile)
      call.respond(mapOf("total" to total))
    }
  }
}&lt;/code&gt;
    &lt;code&gt;@Factory
class MosaicConfiguration {
  @Bean
  @Singleton
  fun mosaicCanvas(): Canvas = runBlocking {
    canvas {
      // Register your dependencies here
      single&amp;lt;UserService&amp;gt; { UserServiceImpl() }
      single&amp;lt;DatabaseConfig&amp;gt; { loadConfig() }
    }
  }
}

@Controller("/orders")
class OrderController(private val canvas: Canvas) {
    
  @Get("/{id}")
  fun getOrder(@PathVariable id: String): OrderPage = runBlocking {
    val mosaic = canvas.withLayer {
      single(OrderKey.qualifier) { id }
    }.create()
    mosaic.compose(OrderPageTile)
  }
  
  @Get("/{id}/total")
  fun getOrderTotal(@PathVariable id: String): Map&amp;lt;String, Double&amp;gt; = runBlocking {
    val mosaic = canvas.withLayer {
      single(OrderKey.qualifier) { id }
    }.create()
    val total = mosaic.compose(OrderTotalTile)
    mapOf("total" to total)
  }
}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üöÄ High-performance APIs requiring efficient data access&lt;/item&gt;
      &lt;item&gt;üîÑ Complex backend orchestration with multiple data sources&lt;/item&gt;
      &lt;item&gt;üèóÔ∏è Microservices that need to compose data from various services&lt;/item&gt;
      &lt;item&gt;üìä GraphQL resolvers that benefit from intelligent caching&lt;/item&gt;
      &lt;item&gt;‚ö° Real-time applications requiring concurrent data access&lt;/item&gt;
      &lt;item&gt;üé® Any system where you want to think in terms of responses rather than queries&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üéØ Response-First: Think from the response up, not database down&lt;/item&gt;
      &lt;item&gt;‚ö° Zero Duplication: Intelligent caching eliminates redundant fetches&lt;/item&gt;
      &lt;item&gt;üîÑ Automatic Concurrency: Parallel execution without complexity&lt;/item&gt;
      &lt;item&gt;üß© Type Safety: Compile-time guarantees for all dependencies&lt;/item&gt;
      &lt;item&gt;üß™ Natural Testability: Mock any tile, test in isolation&lt;/item&gt;
      &lt;item&gt;üì¶ Production Ready: Handles errors, edge cases, and performance optimization&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mosaic transforms backend development by making data composition as natural as function composition, with enterprise-grade performance and reliability.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;mosaic-core: The core framework for composable backend orchestration&lt;/item&gt;
      &lt;item&gt;mosaic-test: Testing framework for tiles&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the Apache License 2.0 - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;Copyright 2025 Nicholas Abbott&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Nick-Abbott/Mosaic"/><published>2025-09-24T14:18:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45360824</id><title>Smartphone Cameras Go Hyperspectral</title><updated>2025-09-24T15:38:15.206062+00:00</updated><content>&lt;doc fingerprint="77951f9c0a9da747"&gt;
  &lt;main&gt;
    &lt;p&gt;The human eye is mostly sensitive to only three bands of the electromagnetic spectrum‚Äîred, green, and blue (RGB)‚Äîin the visible range. In contrast, off-the-shelf smartphone camera sensors are potentially hyperspectral in nature, meaning that each pixel is sensitive to far more spectral bands. Now scientists have found a simple way for any conventional smartphone camera to serve as a hyperspectral sensor‚Äîby placing a card with a chart on it within its view. The new patent-pending technique may find applications in defense, security, medicine, forensics, agriculture, environmental monitoring, industrial quality control, and food and beverage quality analysis, the researchers add.&lt;/p&gt;
    &lt;p&gt;‚ÄúAt the heart of this work is a simple but powerful idea‚Äîa photo is never just an image,‚Äù says Semin Kwon, a postdoctoral research associate of biomedical engineering Purdue University in West Lafayette, Ind. ‚ÄúEvery photo carries hidden spectral information waiting to be uncovered. By extracting it, we can turn everyday photography into science.‚Äù&lt;/p&gt;
    &lt;p&gt;Using a smartphone camera and a spectral color chart, researchers can image the transmission spectrum of high-end whiskey, thus determining its authenticity. Semin Kwon/Purdue University&lt;/p&gt;
    &lt;p&gt;Every molecule has a unique spectral signature‚Äîthe degree to which it absorbs or reflects each wavelength of light. The extreme sensitivity to distinguishing color seen in scientific-grade hyperspectral sensors can help them identify chemicals based on their spectral signatures, for applications in a wide range of industries, such as medical diagnostics, distinguishing authentic versus counterfeit whiskey, monitoring air quality, and nondestructive analysis of pigments in artwork, says Young Kim, a professor of biomedical engineering at Purdue.&lt;/p&gt;
    &lt;p&gt;Previous research has pursued a number of different ways to recover spectral details from conventional smartphone RGB camera data. However, machine learning models developed for this purpose typically rely heavily on the task-specific data on which they are trained. This limits their generalizability and makes them susceptible to errors resulting from variations in lighting, image file formats, and more. Another possible avenue involved special hardware attachments, but these can prove expensive and bulky.&lt;/p&gt;
    &lt;p&gt;In the new study, the scientists designed a special color reference chart that can be printed on a card. They also developed an algorithm that can analyze smartphone pictures taken with this card and account for factors such as lighting conditions. This strategy can extract hyperspectral data from raw images with a sensitivity of 1.6 nanometers of difference in wavelength of visible light, comparable to scientific-grade spectrometers.&lt;/p&gt;
    &lt;p&gt;‚ÄúIn short, this technique could turn an ordinary smartphone into a pocket spectrometer,‚Äù Kim says.&lt;/p&gt;
    &lt;p&gt;The scientists are currently pursuing applications for their new technique in digital and mobile-health applications in both domestic and resource-limited settings. ‚ÄúWe are truly excited that this opens the door to making spectroscopy both affordable and accessible,‚Äù Kwon says.&lt;/p&gt;
    &lt;p&gt;The scientists recently detailed their findings in the journal IEEE Transactions on Image Processing.&lt;/p&gt;
    &lt;p&gt;Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://spectrum.ieee.org/hyperspectral-imaging"/><published>2025-09-24T14:20:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45361071</id><title>Building a Custom eBPF Filesystem Watcher to Catch Root Ownership Goofs</title><updated>2025-09-24T15:38:15.042241+00:00</updated><content>&lt;doc fingerprint="370b91373ce42fa8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Rabbit Hole of Building a Filesystem Watcher&lt;/head&gt;
    &lt;p&gt;Some of the systems I work with are highly customized environments, and often need support engineers to maintain them. A lot of automation exists, but sometimes they need to manually go into a VM and change things. This is normal , but with these manual tasks, mistakes are inevitable. One such case is a service that would only work if all the files and directories it manages are owned by a special user. But sometimes people run commands in the service directories as root. This doesn‚Äôt impact the service as it‚Äôs running, but it won‚Äôt restart. While the fix is simple, just &lt;code&gt;chown -R&lt;/code&gt;
the service directory. There are many easy ways to prevent this, e.g. setting file permissions,
File ACLs. These are less strict as &lt;code&gt;root&lt;/code&gt;
user can override these. Setting SELinux policies would be a much stricter solution. These are very sensible solutions.
But what is the fun it that? How about we build an entire filesystem event watcher ourselves?&lt;/p&gt;
    &lt;head rend="h2"&gt;Attempt 1 - &lt;code&gt;fanotify&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;fanotify&lt;/code&gt; is a set of APIs in the Linux kernel by which we
could get filesystem events sent to userspace. Let‚Äôs dive in, according to man page,
we first need to call &lt;code&gt;fanotify_init&lt;/code&gt; with proper flags;
This sets up a kernel-space notification group. We can set up the directories we need to watch via
&lt;code&gt;fanotify_mark&lt;/code&gt;.
&lt;code&gt;fanotify_init&lt;/code&gt; sets up a file descriptor for the event queue, which can be accessed by reading the file descriptor.
This is a great built-in API, but we have a few issues.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;We cannot monitor a directory recursively. This feature is only available for whole filesystem mounts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Another limitation is that&lt;/p&gt;&lt;code&gt;fanotify&lt;/code&gt;only gives us the PID of the process that triggered the event (&lt;code&gt;metadata-&amp;gt;pid&lt;/code&gt;), not the full credentials. If we want to know who (which UID/GID) actually performed the operation, we must do an extra lookup in&lt;code&gt;/proc/&amp;lt;pid&amp;gt;&lt;/code&gt;(for example, reading&lt;code&gt;/proc/&amp;lt;pid&amp;gt;/status&lt;/code&gt;) to fetch the task‚Äôs credentials. That means for every single event, we would need to open and parse a&lt;code&gt;/proc&lt;/code&gt;file, and then apply our filtering logic.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Attempt 2 - eBPF&lt;/head&gt;
    &lt;p&gt;This was when I put this idea on the back burner, but then I stumbled on eBPF while working on another project with Falco.&lt;/p&gt;
    &lt;p&gt;eBPF enables running programs in kernel space. Programs are first compiled into bytecode, then verified by an in-kernel static verifier, then run using JIT for native execution performance. To communicate with the user-space, we can instantiate various forms of data structures too. The official intro docs do a great job of explaining this, see What is eBPF?.&lt;/p&gt;
    &lt;p&gt;I have been fortunate enough to be writing this at a time when tooling around eBPF has evolved a lot. Earlier tools had to include kernel headers by either a) compiling the program with the exact kernel source present locally or b) compiling the program on the server where it will run. Thanks to improvements around adding lightweight type info BTF(BPF Type Format) , CO-RE (Compile Once - Run Everywhere) and &lt;code&gt;libbpf&lt;/code&gt; loader. The user interface
for writing eBPF programs is a bit easier.&lt;/p&gt;
    &lt;p&gt;Now the question comes, what do you hook into? We can directly hook into kernel VFS layer functions such as &lt;code&gt;vfs_mkdir&lt;/code&gt; and &lt;code&gt;vfs_create&lt;/code&gt;, which abstract out various filesystem implementations and expose a single filesystem interface to user-space.
We could read the arguments and filter out the events shipped to userspace, saving on a lot of context switches.&lt;/p&gt;
    &lt;p&gt;This method again has its own slew of annoyances.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Using kprobes on functions like&lt;/p&gt;&lt;code&gt;vfs_*&lt;/code&gt;does not guarantee a stable ABI, i.e the arguments can change anytime, or functions themselves can disappear across kernel releases. In my case, this is not a big deal since I would be running this in a standardized environment with consistent kernel versions. But this is a solvable problem, though requiring more engineering effort. See this section about handling kernel change in the BPF-CORE reference&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;We will have to write the path filtering logic in kernelspace using eBPF, since&lt;/p&gt;&lt;code&gt;vfs_*&lt;/code&gt;probes will trigger for all events. We will have to walk the filesystem tree up and see if some dir matches our monitored dir. Aside from the complexity of writing this, each eBPF program is statically verified. It must not contain unbounded loops, and we have a limited stack size (typically 512 bytes).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Walking the Tree in eBPF&lt;/head&gt;
    &lt;p&gt;With the generous help of Andrii Nakryiko‚Äôs excellent BPF CO-RE reference guide, I was able to come up with a good enough solution. We can use the &lt;code&gt;dentry&lt;/code&gt; struct to walk up the tree. But since we can‚Äôt
have unbounded loops in BPF, I had to truncate the walk at &lt;code&gt;MAX_DEPTH&lt;/code&gt;,
which is acceptable for my problem statement since the expected depth of the directory I want to monitor is known.&lt;/p&gt;
    &lt;code&gt;static bool is_monitored_dir(struct dentry *dentry, __u64 target_ino) {
  bpf_rcu_read_lock();
  struct dentry *curr_dentry = BPF_CORE_READ(dentry, d_parent);
  struct inode *curr_inode;
  __u64 curr_ino;
  bool result = false;

  #pragma unroll
  for(int i=0; i &amp;lt; MAX_DEPTH; i++) {
    if (!curr_dentry) {
      break;
    }

    curr_inode = BPF_CORE_READ(curr_dentry, d_inode);
    curr_ino = BPF_CORE_READ(curr_inode, i_ino);
    if (curr_ino == target_ino) {
      result = true;
      break;
    }

    struct dentry *parent_dentry = BPF_CORE_READ(curr_dentry, d_parent);
    if (curr_dentry == parent_dentry) {
      break; // curr_dentry is its own root, we have reached the top of
                    // the tree.
    }

    curr_dentry = parent_dentry;
  }

  bpf_rcu_read_unlock();
  return result;
}&lt;/code&gt;
    &lt;p&gt;Note the kernel RCU (Read, Copy, Update) locks are needed since the &lt;code&gt;dentry&lt;/code&gt; tree
can change while we are traversing it. The RCU mechanism lets the readers safely traverse without blocking the writers.&lt;/p&gt;
    &lt;p&gt;For a complete, working example of this approach, please refer to the fs-watcher GitHub repository . This repository contains the full source code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Better Probes&lt;/head&gt;
    &lt;p&gt;LSM hooks provide a more stable and semantically meaningful API for monitoring filesystem events, since they are part of the kernel‚Äôs Linux Security Module framework. They can reduce the number of events you need to filter and eliminate some of the brittleness associated with probing low-level VFS functions. However, these hooks were not available in the kernel I was working with. With LSM hooks, we have access to the &lt;code&gt;path&lt;/code&gt; struct with which we can resolve
the name into a buffer using &lt;code&gt;bpf_path_d_path&lt;/code&gt;. Then we can do a substring search to see if the
path is monitored or not. I will be sure to try this out after our next infra update.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping Up&lt;/head&gt;
    &lt;p&gt;This little experiment turned out to be a great deep dive into Linux kernel internals, eBPF and various trade-offs of running kernel-space programs. eBPF is a very powerful tool, but also has very sharp edges if you are not careful. This has also been my most rigorous exercise in RTFM‚Äôing. A lot of information about these tools exists, but it‚Äôs scattered across kernel docs, blog posts, and reference guides. Piecing it all together was a journey in itself.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://amandeepsp.github.io/blog/fs-watcher/"/><published>2025-09-24T14:38:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45361140</id><title>How HubSpot Scaled AI Adoption</title><updated>2025-09-24T15:38:14.863084+00:00</updated><content>&lt;doc fingerprint="a5420315f9e5e959"&gt;
  &lt;main&gt;
    &lt;p&gt;This post is intended to be the first in a series about empowering product, UX, and engineering teams with AI. We‚Äôre going to focus on how we‚Äôve approached and scaled the use of AI in the context of writing code.&lt;/p&gt;
    &lt;p&gt;AI has fundamentally transformed how we build software at HubSpot. Over the past two years, we've gone from cautious experimentation to achieving near universal adoption of AI coding tools across our engineering organization.&lt;/p&gt;
    &lt;p&gt;This transformation didn't happen overnight. It required strategic investment, organizational commitment, and a willingness to learn. As we've shared our experience with other engineering leaders, we've discovered that many teams are facing similar challenges in scaling AI adoption beyond POCs and early adopters. The conversations we've had with external teams convinced us that our lessons learned could help others navigate this journey more effectively.&lt;/p&gt;
    &lt;p&gt;Adoption of AI coding assistants, % of members in the Engineering organization [sanitized data]&lt;/p&gt;
    &lt;head rend="h1"&gt;In the beginning there was code completion&lt;/head&gt;
    &lt;p&gt;We began experimenting with GitHub Copilot in the Summer of 2023. Our founders Dharmesh and Brian provided us with the push we needed to get started. Dharmesh had recently used GitHub Copilot to build ChatSpot and had a good experience with it, so he and Brian pushed us to evaluate it and connected us with other leaders in the industry who were seeing success.&lt;/p&gt;
    &lt;p&gt;Our proof of concept (POC) successfully validated the tool's potential, and several factors contributed to our success:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Executive buy-in made everything else easier. Support from Dharmesh and Brian accelerated our pilot process significantly. This helped our legal, security, and engineering team have the same goal and urgency for making this happen.&lt;/item&gt;
      &lt;item&gt;We ran a pilot that was sufficiently large: Our strategy was to include entire teams so they could adopt and learn together that had different experience levels, different missions, and worked in different domains. We gave teams over two months to try it..&lt;/item&gt;
      &lt;item&gt;We put energy into enablement. We had setup/training sessions and created a channel where people could ask questions and share what is and is not working.&lt;/item&gt;
      &lt;item&gt;We measured everything. We applied our existing engineering velocity measurement methods to the pilot. This helped us check our biases. We were skeptical at the outset but seeing measured impact chipped away at our skepticism.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The initial results were encouraging: positive qualitative feedback and measurable but modest productivity improvements, across engineers of different tenure and seniority. Our initial gains fell short of some extraordinary claims we were hearing in the market, but they were still significant given Copilot's cost structure of $19/mo/business user at the time. Even modest time savings justified the investment.&lt;/p&gt;
    &lt;p&gt;With a group of committed stakeholders seeing the early value and the potential with the tool, we were willing to be patient and continue our investment. We believed the technology would only improve over time, so we rolled it out with guardrails. As we scaled adoption and people gained more experience with it, we saw increasingly meaningful productivity gains.&lt;/p&gt;
    &lt;head rend="h1"&gt;Leveraging the Power of Central Teams&lt;/head&gt;
    &lt;p&gt;At HubSpot, we've long believed in the leverage that central teams create. Our platform teams build infrastructure, tools, and guardrails that enable small autonomous product teams to move fast while maintaining quality and consistency.&lt;/p&gt;
    &lt;p&gt;When generative AI emerged, we initially relied on teams adjacent to these areas (specifically teams that managed our GitHub setup) to drive adoption. But as demand exploded, the backlog grew exponentially. We realized it was time to create a dedicated team.&lt;/p&gt;
    &lt;p&gt;We created a Developer Experience AI team in October 2024, with an initial focus on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Driving adoption of coding tools: Once we realized the impact these tools were having, we wanted the entire org on board as soon as possible&lt;/item&gt;
      &lt;item&gt;Increasing the impact of AI tools: HubSpot has a very opinionated stack and we wanted our generated code to reflect these opinions as much as possible. This started very simply with the sharing of Cursor rules files, but quickly evolved to more complex tools that gave agents deep context about our architecture, libraries, and best practices. (More to come on this in the future)&lt;/item&gt;
      &lt;item&gt;Advocacy: We wanted to build a community around AI, by collecting and disseminating what was working for people. We created an open forum for people to post about AI and seeded content to drive engagement. We saw a vibrant community slowly spring up as adoption grew.&lt;/item&gt;
      &lt;item&gt;Adapting procurement for speed: We knew we wanted to try every tool that came out, but our purchasing processes were designed for longer term negotiated agreements and we couldn't always count on a push from our founders to get things moving. We wanted month-to-month contracts and to get started ASAP.&lt;/item&gt;
      &lt;item&gt;Building evaluation capabilities: We didn't want to rely solely on qualitative feedback, so we came up with ways to run pilots and compare tools on merit. We also experienced first-hand how empirical data could combat preconceptions and skepticism.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Central infrastructure teams create leverage for product teams in every facet of their daily work. AI is no different. We started very small with just two people who had infrastructure experience and were already highly engaged with AI. The team grew over time as we branched out into more advanced use cases, many of which we'll cover in this series. But creating the team and focusing these engineers paved the way for our future success without a massive investment to get started.&lt;/p&gt;
    &lt;head rend="h1"&gt;Tipping the Scale&lt;/head&gt;
    &lt;p&gt;As engineers adopted the tools and we collected more data, our conviction grew that these tools would have had a positive impact on our engineering team. Initially, we maintained conservative usage rules due to limited experience and cost concerns. Users had to request a license and agree to follow strict guardrails.&lt;/p&gt;
    &lt;p&gt;We pulled metrics on code review burden, cycle time, velocity comparisons before and after adoption, and production incident rates.&lt;/p&gt;
    &lt;p&gt;Impact of AI adoption on incidents, team level data [sanitized]&lt;/p&gt;
    &lt;p&gt;The data consistently showed the same thing: AI adoption wasn't creating the problems we were initially worried about. The scatter plot above shows one example, showing that there was no correlation between AI adoption and production incidents.&lt;/p&gt;
    &lt;p&gt;In May 2024, we ditched the restrictions. Then we proactively gave everyone a seat, making it as easy as possible to get started. Adoption shot above 50% overnight.&lt;/p&gt;
    &lt;head rend="h1"&gt;Reaching the Late Majority&lt;/head&gt;
    &lt;p&gt;Adoption slowed again as it increased beyond 60%. The latter stages of adoption are where you face skeptics, start to better understand the limitations of the current tools, and see higher levels of change/risk aversion, so we had to change our approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Peer validation: Whenever we heard someone did something interesting with AI, we asked them to record a video and share it. We also began recording weekly videos ourselves showing new features and real usage.&lt;/item&gt;
      &lt;item&gt;Quantitative proof: We shared high-level data showing that most people were already using these tools successfully and safely. We deliberately kept the numbers broad rather than getting into precise details. While data was important for making decisions, we wanted people to focus on the clear trend of improvement rather than getting stuck debating exact figures.&lt;/item&gt;
      &lt;item&gt;Provide better tools: We ran POCs for multiple coding assistants to give engineers more options, recognizing that different tools work better for different workflows and preferences.&lt;/item&gt;
      &lt;item&gt;Curated experience: We transparently set up a local MCP server on every machine with default rules and configuration optimized for our development environment. This gave every engineer an experience tailored to our specific stack and best practices right out of the box. We continue to revise and improve this setup over time based on what we learn about effective usage patterns.&lt;/item&gt;
      &lt;item&gt;AI fluency a baseline expectation: Once we hit 90% adoption, we made AI fluency a baseline expectation for engineers by adding it to job descriptions and hiring expectations. By this point, it was easy to see that AI fluency wasn‚Äôt just the right thing for HubSpot, but for engineers it was a necessary investment as they continue to grow in their careers through this transformation. This helped us clearly commit internally and externally to the investment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Adoption was the beginning and opened the door to everything that followed: taking advantage of coding agents, creating Sidekick (our AI assistant that answers platform questions, creates issues, implements changes, and reviews PRs), developing a way to rapidly prototype UIs with our design system, and building infrastructure that led to 400+ tools that our agents can leverage across our internal, OpenAI, and Anthropic MCP servers.&lt;/p&gt;
    &lt;p&gt;Next: How we transitioned to agentic coding&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://product.hubspot.com/blog/context-is-key-how-hubspot-scaled-ai-adoption"/><published>2025-09-24T14:42:49+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45361154</id><title>Who Funds Misfit Research?</title><updated>2025-09-24T15:38:14.700421+00:00</updated><content>&lt;doc fingerprint="b85d134d3d65b6c8"&gt;
  &lt;main&gt;
    &lt;p&gt;This piece is an addition to our Research Leader‚Äôs Playbook. We realized that (to our knowledge) nobody had unpacked where the money for ‚Äúmisfit research‚Äù ‚Äî work that is a poor fit for academia, startups, or large companies ‚Äî was coming from. If you are already deep in this world, you probably know all of this already, but it may still be worth a skim in for something that might surprise you.&lt;/p&gt;
    &lt;p&gt;Funding preferences and situations can change quickly, so if any of this is incorrect or incomplete, please leave a note in the comments!&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Unsurprisingly, there is no default way to fund misfit research: support can range from a group of philanthropists starting a new institute, to DARPA running robot competitions, to DAOs funding longevity projects, to VCs funding research projects gussied up as a company.&lt;/p&gt;
    &lt;p&gt;To get our brains around the funding landscape, it‚Äôs useful to divide this funding into non-dilutive (funding that comes without ownership or expectation of financial return) and dilutive (funding that comes with an expectation of financial return and often involves some ownership of an organization). This division is useful because, in broad strokes, non-dilutive and dilutive funding come with very different expectations, evaluation criteria, and ‚Äúsales‚Äù processes.&lt;/p&gt;
    &lt;p&gt;Be aware that these categories have a lot of fuzziness (like many things in non-traditional research). Several entities, like family offices, do both dilutive and non-dilutive funding; they have their own section. Furthermore, non-dilutive and dilutive funding are not always mutually exclusive: some technology projects get off the ground with a mix of non-dilutive grants from foundations or governments and investments from angel or impact investors. (There are still a lot of gaps in this ‚Äúmessy middle‚Äù between pure-public-goods work and profit-maximizing company.)&lt;/p&gt;
    &lt;p&gt;Below are the major groups in each category and what they're actually funding. The end of this section touches on what to actually do with this information when you‚Äôre trying to fund research.&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-Dilutive Funders&lt;/head&gt;
    &lt;p&gt;Non-dilutive funding doesn‚Äôt come with any expectations of repayment or organizational ownership. This sort of funding is important for research that is a poor financial investment, whether because it will never create capturable value, or has long timescales and high uncertainty. However, non-dilutive funding isn‚Äôt just ‚Äúfree‚Äù money. Raising non-dilutive funding usually takes significantly more time and effort than the equivalent amount of investment dollars; most funders impose much more process up front and restrictions on how money can be spent.&lt;/p&gt;
    &lt;p&gt;Foundations: Foundations are organizations with full-time professional staff deploying money that has been set aside explicitly for philanthropic purposes. Foundations can range in size from a tiny org with one or two staff to hundreds or thousands of employees at the largest foundations like the Rockefeller or Gates Foundations.&lt;/p&gt;
    &lt;p&gt;In aggregate, foundations gave $30B towards research in 2019, which is more than the NSF and comparable to NIH.1 While these numbers are large, the median grant is significantly less than $1M. Even large foundations are usually spending money that comes from the interest on an endowment, so they may have much smaller budgets than you might expect based on the wealth of their founder or size of their endowment.&lt;/p&gt;
    &lt;p&gt;As of 2025, most research funding from foundations goes towards traditional research. The bureaucracy and processes of most foundations make it hard for them to support non-traditional work. Foundations typically deploy money through program officers who work within tight bounds set by the board of trustees on a yearly basis. Programs often have explicit mandates to work within traditional institutions through graduate fellowships or awards to professors.&lt;/p&gt;
    &lt;p&gt;There are, of course, exceptions. The now-defunct Schmidt Futures helped a number of ambitious research organizations get off the ground.&lt;/p&gt;
    &lt;p&gt;Philanthropic Aggregators: Philanthropic aggregators are organizations that use their brand and connections to fundraise for specific projects from wealthy individuals or foundations. Some examples include Renaissance Philanthropy, Founders Pledge, and XPrize. Each philanthropic aggregator has their own process and funding ‚Äúform factor‚Äù: Renaissance Philanthropy creates ‚Äúphilanthropic funds‚Äù that they use to deploy grants, while the X-Prize creates prize competitions.&lt;/p&gt;
    &lt;p&gt;Philanthropic aggregators have funded a lot of non-traditional research. The process of recruiting on a case-by-case basis makes aggregators more flexible than foundations with board-specified programs.&lt;/p&gt;
    &lt;p&gt;Government Organizations: Government is, of course, a major research funder. The vast majority of government research funding is little-c conservative and intended for traditional PI-driven academic work. However, certain agencies and programs have supported some misfit work work. &lt;lb/&gt;DARPA pioneered using Other Transaction Authority (OTA) to run prize competitions like the DARPA Grand Challenge, Urban Challenge, and Robotics Challenge. SBIR (Small Business Innovation Research) grants provide non-dilutive funding towards research-heavy startups. Recently, two British government organizations (ARIA and the Department for Science, Innovation, and Technology) announced that they were funding Focused Research Organizations.&lt;/p&gt;
    &lt;p&gt;Crowdfunding Platforms: Platforms like Experiment.com enable researchers to raise small amounts of money from a large number of people. Crowdfunding can work for non-traditional projects that have public appeal like a citizen science endeavor to catalog ocean plastics or a team building a field microscope. However, crowdfunding rarely raises amounts more than tens of thousands of dollars, so it‚Äôs suited for modest-scale projects or early seed funding.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dilutive Funders&lt;/head&gt;
    &lt;p&gt;Dilutive funding can be double-edged: it injects significant resources, but it may steer the work towards shorter-term commercial goals and away from research work or even longer-term commercial goals. Professional investors need to show their investors portfolio growth and exits, which means they need companies in their portfolio to show year-over-year growth, which can be at odds with the uncertainty baked into research.&lt;/p&gt;
    &lt;p&gt;Angel Investors: Angels are individuals who invest their own money in early-stage startups. Technically, angel investment is driven by prospects of eventual financial return, but some angels think less about whether a startup is a good investment but instead about whether the work would be cool or impactful.&lt;/p&gt;
    &lt;p&gt;Venture Capitalists (VCs): The main context in which professional venture capitalists fund non-traditional work is (almost by definition) bubbles. When an area is ‚Äúhot‚Äù enough, even professional VC funds put money towards work that has no sense of how it becomes a product or a business. There are also a few VC firms that have different structures, like longer fund lifetimes, that enable them to invest differently from other firms. Some examples of VC funding into research includes most quantum computing companies, Colossal Biosciences, and Physical Intelligence.&lt;/p&gt;
    &lt;p&gt;Corporate Research: While corporate research has drastically contracted, some large companies with large margins still support exploratory research. As obvious 2025, AI research is an obvious example. Corporations rarely fund nontraditional research ‚Äì most external research funding goes towards universities primarily as a hiring pipeline. Teams can sometimes carve out a niche within corporate research to develop something ambitious ‚Äì the team that started the Lean FRO worked at Microsoft Research for a long time.&lt;/p&gt;
    &lt;p&gt;Corporate Venture: Corporate venture capital arms invest in startups based on the company‚Äôs ‚Äústrategic interest‚Äù in addition to pure returns. For example, a car company may invest in a research-heavy battery startup or a chip company may invest in a photonics company long before they have a product. Large companies sometimes acquire and continue to fund organizations that focus more on research than products: Hyundai‚Äôs acquisition of Boston Dynamics, for example.&lt;/p&gt;
    &lt;p&gt;Impact Investors: Some investors are willing to accept lower financial returns in exchange for high social or environmental impact. These investors fund for-profit ventures within some impact area (like climate or health) that don‚Äôt fit the profile for normal VC funding because of factors like time scales or capital requirements. This kind of investment is also sometimes called ‚Äúpatient capital‚Äù or ‚Äúconcessionary funding.‚Äù For example, Breakthrough Energy Ventures (BEV) explicitly operates on a 20-year timeline and invests in risky clean energy companies with the understanding that some may only yield societal benefit without huge profits. In the medical world impact investors sometimes invest in exchange for royalties or revenue sharing rather than explosive startup growth.&lt;/p&gt;
    &lt;p&gt;Program-related Investments (PRIs): Foundations and Donor Advised Funds (which we will talk more about in the next section) can make dilutive investments out of their endowments that count towards their legal deployment quotas as long as they are mission aligned. Unpacking that jargon: Foundations (but not DAFs) legally must spend 5% of their assets annually; normally this money is deployed as grants, but dilutive investments that are aligned with the foundation‚Äôs mission can also count towards that 5%. &lt;lb/&gt;Investments with the possibility of a return enable Foundations and DAFs to fulfill their charitable missions without eating as much into their bank accounts. This upside means that PRIs that do happen are often larger than normal grants and theoretically people with DAFs and Foundations should be excited to do them. However, the potential for IRS scrutiny, divisions between investment and granting teams, and DAF sponsors that don‚Äôt support PRIs means that they are fairly rare.&lt;lb/&gt;Two words of caution about funding research-heavy work with dilutive alternatives to venture capital:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;While PRIs and impact investors explicitly fund work that struggles to raise VC funding, they often focus on work that can‚Äôt raise VC funding because of characteristics like timescales, capital requirements, and market sizes, not because they are too research-heavy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most dilutive funding puts an organization on a trajectory where they do need to raise venture capital eventually. Many misfit research projects will never be a good fit for venture capital, so raising dilutive funding can be a trap.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Both&lt;/head&gt;
    &lt;p&gt;These entities can (but do not always) do both dilutive and non-dilutive funding.&lt;/p&gt;
    &lt;p&gt;Decentralized Autonomous Organizations (DAOs): DAOs are communities that pool funds for a common purpose by selling blockchain-based ‚Äútokens‚Äù that give their owners a say in the organization‚Äôs governance. Some DAOs, like VitaDAO or CerebrumDAO focus on research. DAOs fund work through many different mechanisms ‚Äî both traditional grants and dilutive funding as well as newer blockchain-based mechanisms for capturing some of the value that the work could create.&lt;/p&gt;
    &lt;p&gt;High-Net-Worth Individuals (HNWIs): Wealthy individuals sometimes bankroll research personally. Individuals have the most flexibility of any funders to do unconventional things. As a result, a lot of non-traditional research has been funded directly by individuals. HNWIs often fund things quietly and make themselves hard to contact for obvious reasons: everybody would be asking them for money and funding strange things can open people up to reputational risk.&lt;/p&gt;
    &lt;p&gt;Family Offices: Family offices are professional organizations that handle the money of a wealthy individual or family. The big thing that differentiates family offices from foundations is that they don‚Äôt have a pile of money that has been explicitly set aside for philanthropy. Instead, they have multiple mandates ‚Äì increase wealth, hedge against risk, maintain liquidity, and do philanthropy.&lt;/p&gt;
    &lt;p&gt;Keep in mind that people and organizations can have completely different focuses, mindsets, and processes whether they‚Äôre doing dilutive or non-dilutive funding: a wealthy individual who will write a million dollar check to a startup the same day might only donate $10k at a time only to projects focusing on a specific disease.&lt;/p&gt;
    &lt;p&gt;http://arxiv.org/abs/2206.10661&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.spec.tech/p/who-funds-misfit-research"/><published>2025-09-24T14:44:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45361239</id><title>The DHS has been harvesting DNA from Americans for years</title><updated>2025-09-24T15:38:14.539672+00:00</updated><content>&lt;doc fingerprint="9199219c0385031a"&gt;
  &lt;main&gt;
    &lt;p&gt;For years, Customs and Border Protection agents have been quietly harvesting DNA from American citizens, including minors, and funneling the samples into an FBI crime database, government data shows. This expansion of genetic surveillance was never authorized by Congress for citizens, children, or civil detainees.&lt;/p&gt;
    &lt;p&gt;According to newly released government data analyzed by Georgetown Law‚Äôs Center on Privacy &amp;amp; Technology, the Department of Homeland Security, which oversees CBP, collected the DNA of nearly 2,000 US citizens between 2020 and 2024 and had it sent to CODIS, the FBI‚Äôs nationwide system for policing investigations. An estimated 95 were minors, some as young as 14. The entries also include travelers never charged with a crime and dozens of cases where agents left the ‚Äúcharges‚Äù field blank. In other files, officers invoked civil penalties as justification for swabs that federal law reserves for criminal arrests.&lt;/p&gt;
    &lt;p&gt;The findings appear to point to a program running outside the bounds of statute or oversight, experts say, with CBP officers exercising broad discretion to capture genetic material from Americans and have it funneled into a law-enforcement database designed in part for convicted offenders. Critics warn that anyone added to the database could endure heightened scrutiny by US law enforcement for life.&lt;/p&gt;
    &lt;p&gt;‚ÄúThose spreadsheets tell a chilling story,‚Äù Stevie Glaberson, director of research and advocacy at Georgetown‚Äôs Center on Privacy &amp;amp; Technology, tells WIRED. ‚ÄúThey show DNA taken from people as young as 4 and as old as 93‚Äîand, as our new analysis found, they also show CBP flagrantly violating the law by taking DNA from citizens without justification.‚Äù&lt;/p&gt;
    &lt;p&gt;DHS did not respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;For more than two decades, the FBI‚Äôs Combined DNA Index System, or CODIS, has been billed as a tool for violent crime investigations. But under both recent policy changes and the Trump administration‚Äôs immigration agenda, the system has become a catchall repository for genetic material collected far outside the criminal justice system.&lt;/p&gt;
    &lt;p&gt;One of the sharpest revelations came from DHS data released earlier this year showing that CBP and Immigrations and Customs Enforcement have been systematically funneling cheek swabs from immigrants‚Äîand, in many cases, US citizens‚Äîinto CODIS. What was once a program aimed at convicted offenders now sweeps in children at the border, families questioned at airports, and people held on civil‚Äînot criminal‚Äîgrounds. WIRED previously reported that DNA from minors as young as 4 had ended up in the FBI‚Äôs database, alongside elderly people in their nineties, with little indication of how or why the samples were taken.&lt;/p&gt;
    &lt;p&gt;The scale is staggering. According to Georgetown researchers, DHS has contributed roughly 2.6 million profiles to CODIS since 2020‚Äîfar above earlier projections and a surge that has reshaped the database. By December 2024, CODIS‚Äôs ‚Äúdetainee‚Äù index contained over 2.3 million profiles; by April 2025, the figure had already climbed to more than 2.6 million. Nearly all of these samples‚Äî97 percent‚Äîwere collected under civil, not criminal, authority. At the current pace, according to Georgetown Law‚Äôs estimates, which are based on DHS projections, Homeland Security files alone could account for one-third of CODIS by 2034.&lt;/p&gt;
    &lt;p&gt;The expansion has been driven by specific legal and bureaucratic levers. Foremost was an April 2020 Justice Department rule that revoked a long-standing waiver allowing DHS to skip DNA collection from immigration detainees, effectively green-lighting mass sampling. Later that summer, the FBI signed off on rules that let police booking stations run arrestee cheek swabs through Rapid DNA machines‚Äîautomated devices that can spit out CODIS-ready profiles in under two hours.&lt;/p&gt;
    &lt;p&gt;The strain of the changes became apparent in subsequent years. Former FBI director Christopher Wray warned during Senate testimony in 2023 that the flood of DNA samples from DHS threatened to overwhelm the bureau‚Äôs systems. The 2020 rule change, he said, had pushed the FBI from a historic average of a few thousand monthly submissions to 92,000 per month‚Äîover 10 times its traditional intake. The surge, he cautioned, had created a backlog of roughly 650,000 unprocessed kits, raising the risk that people detained by DHS could be released before DNA checks produced investigative leads.&lt;/p&gt;
    &lt;p&gt;Under Trump‚Äôs renewed executive order on border enforcement, signed in January 2025, DHS agencies were instructed to deploy ‚Äúany available technologies‚Äù to verify family ties and identity, a directive that explicitly covers genetic testing. This month, federal officials announced they were soliciting new bids to install Rapid DNA at local booking facilities around the country, with combined awards of up to $3 million available.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe Department of Homeland Security has been piloting a secret DNA collection program of American citizens since 2020. Now, the training wheels have come off,‚Äù said Anthony Enriquez, vice president of advocacy at Robert F. Kennedy Human Rights. ‚ÄúIn 2025, Congress handed DHS a $178 billion check, making it the nation‚Äôs costliest law enforcement agency, even as the president gutted its civil rights watchdogs and the Supreme Court repeatedly signed off on unconstitutional tactics.‚Äù&lt;/p&gt;
    &lt;p&gt;Oversight bodies and lawmakers have raised alarms about the program. As early as 2021, the DHS Inspector General found the department lacked central oversight of DNA collection and that years of noncompliance can undermine public safety‚Äîechoing an earlier rebuke from the Office of Special Counsel, which called CBP‚Äôs failures an ‚Äúunacceptable dereliction.‚Äù&lt;/p&gt;
    &lt;p&gt;US senator Ron Wyden more recently pressed DHS and DOJ for explanations about why children‚Äôs DNA is being captured and whether CODIS has any mechanism to reject improperly obtained samples, saying the program was never intended to collect and permanently retain the DNA of all noncitizens, warning the children are likely to be ‚Äútreated by law enforcement as suspects for every investigation of every future crime, indefinitely.‚Äù&lt;/p&gt;
    &lt;p&gt;Rights advocates allege that CBP‚Äôs DNA collection program has morphed into a sweeping genetic surveillance regime, with samples from migrants and even US citizens fed into criminal databases absent transparency, legal safeguards, or limits on retention. Georgetown‚Äôs privacy center points out that once DHS creates and uploads a CODIS profile, the government retains the physical DNA sample indefinitely, with no procedure to revisit or remove profiles when the legality of the detention is in doubt.&lt;/p&gt;
    &lt;p&gt;In parallel, Georgetown and allied groups have sued DHS over its refusal to fully release records about the program, highlighting how little the public knows about how DNA is being used, stored, or shared once it enters CODIS.&lt;/p&gt;
    &lt;p&gt;Taken together, these revelations may suggest a quiet repurposing of CODIS. A system long described as a forensic breakthrough is being remade into a surveillance archive‚Äîsweeping up immigrants, travelers, and US citizens alike, with few checks on the agents deciding whose DNA ends up in the federal government‚Äôs most intimate database.&lt;/p&gt;
    &lt;p&gt;‚ÄúThere‚Äôs much we still don‚Äôt know about DHS‚Äôs DNA collection activities,‚Äù Georgetown‚Äôs Glaberson says. ‚ÄúWe‚Äôve had to sue the agencies just to get them to do their statutory duty, and even then they‚Äôve flouted court orders. The public has a right to know what its government is up to, and we‚Äôll keep fighting to bring this program into the light.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wired.com/story/dhs-has-been-collecting-us-citizens-dna-for-years/"/><published>2025-09-24T14:51:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45361344</id><title>The Lambda Calculus ‚Äì Stanford Encyclopedia of Philosophy</title><updated>2025-09-24T15:38:13.915163+00:00</updated><content>&lt;doc fingerprint="2e83231764ffdbf1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Lambda Calculus&lt;/head&gt;&lt;p&gt;The \(\lambda\)-calculus is, at heart, a simple notation for functions and application. The main ideas are applying a function to an argument and forming functions by abstraction. The syntax of basic \(\lambda\)-calculus is quite sparse, making it an elegant, focused notation for representing functions. Functions and arguments are on a par with one another. The result is a non-extensional theory of functions as rules of computation, contrasting with an extensional theory of functions as sets of ordered pairs. Despite its sparse syntax, the expressiveness and flexibility of the \(\lambda\)-calculus make it a cornucopia of logic and mathematics. This entry develops some of the central highlights of the field and prepares the reader for further study of the subject and its applications in philosophy, linguistics, computer science, and logic.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;1. Introduction&lt;/item&gt;&lt;item&gt;2. Syntax&lt;/item&gt;&lt;item&gt;3. Brief history of \(\lambda\)-calculus&lt;/item&gt;&lt;item&gt;4. Reduction&lt;/item&gt;&lt;item&gt;5. \(\lambda\)-theories&lt;/item&gt;&lt;item&gt;6. Consistency of the \(\lambda\)-calculus&lt;/item&gt;&lt;item&gt;7. Semantics of \(\lambda\)-calculus&lt;/item&gt;&lt;item&gt;8. Extensions and Variations&lt;/item&gt;&lt;item&gt;9. Applications&lt;/item&gt;&lt;item&gt;Bibliography&lt;/item&gt;&lt;item&gt;Academic Tools&lt;/item&gt;&lt;item&gt;Other Internet Resources&lt;/item&gt;&lt;item&gt;Related Entries&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;1. Introduction&lt;/head&gt;&lt;p&gt;The \(\lambda\)-calculus is an elegant notation for working with applications of functions to arguments. To take a mathematical example, suppose we are given a simple polynomial such as \(x^2 -2\cdot x+5\). What is the value of this expression when \(x = 2\)? We compute this by ‚Äòplugging in‚Äô 2 for \(x\) in the expression: we get \(2^2 -2\cdot 2+5\), which we can further reduce to get the answer 5. To use the \(\lambda\)-calculus to represent the situation, we start with the \(\lambda\)-term&lt;/p&gt;\[ \lambda x[x^2 -2\cdot x+5]. \]&lt;p&gt;The \(\lambda\) operators allows us to abstract over \(x\). One can intuitively read ‚Äò\(\lambda x[x^2 -2\cdot x+5]\)‚Äô as an expression that is waiting for a value \(a\) for the variable \(x\). When given such a value \(a\) (such as the number 2), the value of the expression is \(a^2 -2\cdot a+5\). The ‚Äò\(\lambda\)‚Äô on its own has no significance; it merely binds the variable \(x\), guarding it, as it were, from outside interference. The terminology in \(\lambda\)-calculus is that we want to apply this expression to an argument, and get a value. We write ‚Äò\(Ma\)‚Äô to denote the application of the function \(M\) to the argument \(a\). Continuing with the example, we get:&lt;/p&gt;\[\begin{align} (\lambda x[x^2 -2\cdot x+5])2 \rhd 2^2&amp;amp; -2\cdot 2+5 &amp;amp;\langle \text{Substitute 2 for } x\rangle \\ &amp;amp;= 4-4+5 &amp;amp;\langle\text{Arithmetic}\rangle \\ &amp;amp;= 5 &amp;amp;\langle\text{Arithmetic}\rangle \end{align}\]&lt;p&gt;The first step of this calculation, plugging in ‚Äò2‚Äô for occurrences of \(x\) in the expression ‚Äò\(x^2 - 2\cdot x + 5\)‚Äô, is the passage from an abstraction term to another term by the operation of substitution. The remaining equalities are justified by computing with natural numbers.&lt;/p&gt;&lt;p&gt;This example suggests the central principle of the \(\lambda\)-calculus, called \(\beta\)-reduction, which is also sometimes called \(\beta\)-conversion:&lt;/p&gt;\[ \tag{\(\beta\)} (\lambda x[M])N \rhd M[x := N] \]&lt;p&gt;The understanding is that we can reduce or contract \((\rhd)\) an application \((\lambda xM)N\) of an abstraction term (the left-hand side, \(\lambda xM)\) to something (the right-hand side, \(N)\) by simply plugging in \(N\) for the occurrences of \(x\) inside \(M\) (that‚Äôs what the notation ‚Äò\(M[x := N]\)‚Äô expresses). \(\beta\)-reduction, or \(\beta\)-conversion, is the heart of the \(\lambda\)-calculus. When one actually applies \(\beta\)-reduction to reduce a term, there is an important proviso that has to be observed. But this will be described in Section 2.1, when we discuss bound and free variables.&lt;/p&gt;&lt;head rend="h3"&gt;1.1 Multi-argument operations&lt;/head&gt;&lt;p&gt;What about functions of multiple arguments? Can the \(\lambda\)-calculus represent operations such as computing the length of the hypotenuse of a right triangle:&lt;/p&gt;&lt;p&gt;Hypotenuse of a right triangle with legs of length \(x\) and \(y \Rightarrow \sqrt{x^2 + y^2}\).&lt;/p&gt;&lt;p&gt;The length-of-hypotenuse operation maps two positive real numbers \(x\) and \(y\) to another positive real number. One can represent such multiple-arity operations using the apparatus of the \(\lambda\)-calculus by viewing the operation as taking one input at a time. Thus, the operation can be seen as taking one input, \(x\), a positive real number, and producing as its value not a number, but an operation: namely, the operation that takes a positive real number \(y\) as input and produces as output the positive real number \(\sqrt{x^2 + y^2}\). One could summarize the discussion by saying that the operation, hypotenuse-length, that computes the length of the hypotenuse of a right triangle given the lengths \(a\) and \(b\) of its legs, is:&lt;/p&gt;&lt;p&gt;hypotenuse-length \(:= \lambda a[\lambda b[\sqrt{a^2 + b^2}]]\)&lt;/p&gt;&lt;p&gt;By the principle of \(\beta\)-reduction, we have, for example, that hypotenuse-length 3, the application of hypotenuse-length to 3, is \(\lambda b[\sqrt{3^2 + b^2}]\), which is a function of that is ‚Äòwaiting‚Äô for another argument. The \(\lambda\)-term hypotenuse-length 3 can be viewed as a function that computes the length of the hypotenuse of a right triangle one of whose legs has length 3. We find, finally, that (hypotenuse-length 3)4‚Äîthe application of hypotenuse-length to 3 and then to 4‚Äîis 5, as expected.&lt;/p&gt;&lt;p&gt;Another way to understand the reduction of many-place functions to one-place functions is to imagine a machine \(M\) that initially starts out by loading the first \(a\) of multiple arguments \(a, b,\ldots\) into memory. If one then suspends the machine after it has loaded the first argument into memory, one can view the result as another machine M\(_a\) that is awaiting one fewer input; the first argument is now fixed.&lt;/p&gt;&lt;head rend="h3"&gt;1.2 Non-Extensionality&lt;/head&gt;&lt;p&gt;An important philosophical issue concerning the \(\lambda\)-calculus is the question of its underlying concept of functions. In set theory, a function is standardly understood as a set of argument-value pairs. More specifically, a function is understood as a set \(f\) of ordered pairs satisfying the property that \((x,y) \in f\) and \((x,z) \in f\) implies \(y = z\). If \(f\) is a function and \((x,y) \in f\), this means that the function f assigns the value \(y\) to the argument \(x\). This is the concept of functions-as-sets. Consequently, the notion of equality of functions-as-sets is equality qua sets, which, under the standard principle of extensionality, entails that two functions are equal precisely when they contain the same ordered pairs. In other words, two functions are identical if and only if they assign the same values to the same arguments. In this sense, functions-as-sets are extensional objects.&lt;/p&gt;&lt;p&gt;In contrast, the notion of a function at work in \(\lambda\)-calculus is one where functions are understood as rules: a function is given by a rule for how to determine its values from its arguments. More specifically, we can view a \(\lambda\)-term \(\lambda x[M]\) as a description of an operation that, given \(x\), produces \(M\); the body \(M\) of the abstraction term is, essentially, a rule for what to do with \(x\). This is the conception of functions-as-rules. Intuitively, given rules \(M\) and \(N\), we cannot in general decide whether \(\lambda x[M]\) is equal to \(\lambda x[N]\). The two terms might ‚Äòbehave‚Äô the same (have the same value given the same arguments), but it may not be clear what resources are needed for showing the equality of the terms. In this sense, functions-as-rules are non-extensional objects.&lt;/p&gt;&lt;p&gt;To distinguish the extensional concept of functions-as-sets from the non-extensional concept of functions-as-rules, the latter is often referred to as an ‚Äòintensional‚Äô function concept, in part because of the ostensibly intensional concept of a rule involved. This terminology is particularly predominant in the community of mathematical logicians and philosophers of mathematics working on the foundations of mathematics. But from the perspective of the philosophy of language, the terminology can be somewhat misleading, since in this context, the extensional-intensional distinction has a slightly different meaning.&lt;/p&gt;&lt;p&gt;In the standard possible-worlds framework of philosophical semantics, we would distinguish between an extensional and an intensional function concept as follows. Let us say that two functions are extensionally equivalent at a world if and only if they assign the same values to the same arguments at that world. And let us say that two functions are intensionally equivalent if and only if they assign the same values to the same arguments at every possible-world. To illustrate, consider the functions highest-mountain-on-earth and highest-mountain-in-the-Himalayas, where highest-mountain-on-earth assigns the highest mountain on earth as the value to every argument and highest-mountain-in-the-Himalayas assigns the highest mountain in the Himalayas as the value to every argument. The two functions are extensionally equivalent (at the actual world), but not intensionally so. At the actual world, the two functions assign the same value to every argument, namely Mt. Everest. Now consider a world where Mt. Everest is not the highest mountain on earth, but say, Mt. Rushmore is. Suppose further that this is so, just because Mt. Rushmore is 30.000 feet/9.100 m higher than it is at the actual world, while Mt. Everest, with its roughly 29.000 feet/8.800 m, is still the highest mountain in the Himalayas. At that world, highest-mountain-on-earth now assigns Mt. Rushmore as the value to every argument, while highest-mountain-in-the-Himalayas still assigns Mt. Everest to every object. In other words, highest-mountain-on-earth and highest-mountain-in-the-Himalayas are extensionally equivalent (at the actual world) but not intensionally equivalent.&lt;/p&gt;&lt;p&gt;A function concept may now be called extensional if and only if it requires functions that are extensionally equivalent at the actual world to be identical. And a function concept may be classified as intensional if and only if it requires intensionally equivalent functions to be identical. Note that these classifications are conceptually different from the distinctions commonly used in the foundations of mathematics. On the terminology used in the foundations of mathematics, functions-as-sets are classified as extensional since they use the axiom of extensionality as their criterion of identity, and functions-as-rules are classified as intensional because they rely on the ostensibly intensional concept of a rule. In the present possible-worlds terminology, function concepts are classified as extensional or intensional based of their behavior at possible-worlds.&lt;/p&gt;&lt;p&gt;An issue from which conceptual confusion might arise is that the two terminologies potentially pass different verdicts on the function concept at work in the \(\lambda\)-calculus. To see this, consider the following two functions:&lt;/p&gt;\[\begin{align} \addone &amp;amp;:= \lambda x[x+1] \\ \addtwosubtractone &amp;amp;:= \lambda x[[x+2]-1] \end{align}\]&lt;p&gt;These two functions are clearly extensionally equivalent: they assign the same value to the same input at the actual world. Moreover, given standard assumptions in possible worlds semantics, the two functions are also intensionally equivalent. If we assume that mathematical facts, like facts about addition and subtraction, are necessary in the sense that they are the same at every possible world, then we get that the two functions give the same value to the arguments at every possible world. So, an intensional function concept would require the two functions to be identical. In the \(\lambda\)-calculus, however, it‚Äôs not clear at all that we should identify the two functions. Formally speaking, without the help of some other principle, we cannot show that the two \(\lambda\)-terms denote the same function. Moreover, informally speaking, on the conception of functions-as-rules, it‚Äôs not even clear that we should identify them: the two terms involve genuinely different rules, and so we might be tempted to say that they denote different functions.&lt;/p&gt;&lt;p&gt;A function concept that allows for intensionally equivalent functions to be distinct is called hyperintensional. The point is that in possible-worlds terminology, the function concept at work in the \(\lambda\)-calculus may be regarded not as intentional but hyperintensional‚Äîin contrast to what the terminology common in the foundations of mathematics says. Note that it‚Äôs unclear how an intensional semantic framework, like the possible-worlds framework, could even in principle account for a non-intensional function concept. On the semantics of the \(\lambda\)-calculus, see section 7. The point here was simply to clarify any conceptual confusions that might arise from different terminologies at play in philosophical discourse.&lt;/p&gt;&lt;p&gt;The hyperintensionality of the \(\lambda\)-calculus is particularly important when it comes to its applications as a theory of not only functions, but more generally \(n\)-ary relations. On this, see section 9.3. It is effectively the hyperintensionality of the \(\lambda\)-calculus that makes it an attractive tool in this context. It should be noted, however, that the \(\lambda\)-calculus can be made extensional (as well as intensional) by postulating additional laws concerning the equality of \(\lambda\)-terms. On this, see section 5.&lt;/p&gt;&lt;head rend="h2"&gt;2. Syntax&lt;/head&gt;&lt;p&gt;The official syntax of the \(\lambda\)-calculus is quite simple; it is contained in the next definition.&lt;/p&gt;&lt;p&gt;Definition For the alphabet of the language of the \(\lambda\)-calculus we take the left and right parentheses, left and right square brackets, the symbol ‚Äò\(\lambda\)‚Äô, and an infinite set of variables. The class of \(\lambda\)-terms is defined inductively as follows:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Every variable is a \(\lambda\)-term.&lt;/item&gt;&lt;item&gt;If \(M\) and \(N\) are \(\lambda\)-terms, then so is \((MN)\).&lt;/item&gt;&lt;item&gt;If \(M\) is a \(\lambda\)-term and \(x\) is a variable, then \((\lambda x[M])\) is a \(\lambda\)-term.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;By ‚Äòterm‚Äô we always mean ‚Äò\(\lambda\)-term‚Äô. Terms formed according to rule (2) are called application terms. Terms formed according to rule (3) are called abstraction terms.&lt;/p&gt;&lt;p&gt;As is common when dealing with formal languages that have grouping symbols (the left and right parenthesis, in our case), some parentheses will be omitted when it is safe to do so (that is, when they can be reintroduced in only one sensible way). Juxtaposing more than two \(\lambda\)-terms is, strictly speaking, illegal. To avoid the tedium of always writing all needed parentheses, we adopt the following convention:&lt;/p&gt;&lt;p&gt;Convention (association to the left): When more than two terms \(M_1 M_2 M_3 \ldots M_n\) are juxtaposed we can recover the missing parentheses by associating to the left: reading from left to right, group \(M_1\) and \(M_2\) together, yielding \((M_1 M_2)M_3 \ldots M_n\); then group \((M_1 M_2)\) with \(M_3\): \(((M_1 M_2)M_3)\ldots M_n\), and so forth.&lt;/p&gt;&lt;p&gt;The convention thus gives a unique reading to any sequence of \(\lambda\)-terms whose length is greater than 2.&lt;/p&gt;&lt;head rend="h3"&gt;2.1 Variables, bound and free&lt;/head&gt;&lt;p&gt;The function of \(\lambda\) in an abstraction term \((\lambda x[M]\)) is that it binds the variable appearing immediately after it in the term \(M\). Thus \(\lambda\) is analogous to the universal and existential quantifiers \(\forall\) and \(\exists\) of first-order logic. One can define, analogously, the notions of free and bound variable in the expected way, as follows.&lt;/p&gt;&lt;p&gt;Definition The syntactic functions \(\mathbf{FV}\) and \(\mathbf{BV}\) (for ‚Äòfree variable‚Äô and ‚Äòbound variable‚Äô, respectively) are defined on the set of \(\lambda\)-terms by structural induction thus:&lt;/p&gt;&lt;p&gt;For every variable \(x\), term \(M\), and term \(N\):&lt;/p&gt;&lt;p&gt;If \(\mathbf{FV}(M) = \varnothing\) then \(M\) is called a combinator.&lt;/p&gt;&lt;p&gt;Clause (3) in the two definitions supports the intention that \(\lambda\) binds variables (ensures that they are not free). Note the difference between \(\mathbf{BV}\) and \(\mathbf{FV}\) for variables.&lt;/p&gt;&lt;p&gt;As is typical in other subjects where the concepts appear, such as first-order logic, one needs to be careful about the issue; a casual attitude about substitution can lead to syntactic difficulties.[1] We can defend a casual attitude by adopting the convention that we are interested not in terms themselves, but in a certain equivalence class of terms. We now define substitution, and then lay down a convention that allows us to avoid such difficulties.&lt;/p&gt;&lt;p&gt;Definition (substitution) We write ‚Äò\(M[x := N]\)‚Äô to denote the substitution of \(N\) for the free occurrences of \(x\) in \(M\). A precise definition[2] by recursion on the set of \(\lambda\)-terms is as follows: for all terms \(A\), \(B\), and \(M\), and for all variables \(x\) and \(y\), we define&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;\(x[x := M] \equiv M\)&lt;/item&gt;&lt;item&gt;\(y[x := M] \equiv y\) (\(y\) distinct from \(x)\)&lt;/item&gt;&lt;item&gt;\((AB)[x := M] \equiv A[x := M]B[x := M]\)&lt;/item&gt;&lt;item&gt;\((\lambda x[A])[x := M] \equiv \lambda x[A]\)&lt;/item&gt;&lt;item&gt;\((\lambda y[A])[x := M] \equiv \lambda y[A[x := M]]\) (\(y\) distinct from \(x)\)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Clause (1) of the definition simply says that if we are to substitute \(M\) for \(x\) and we are dealing simply with \(x\), then the result is just \(M\). Clause (2) says that nothing happens when we are dealing (only) with a variable different from \(x\) but we are to substitute something for \(x\). Clause (3) tells us that substitution unconditionally distributes over applications. Clauses (4) and (5) concern abstraction terms and parallel clauses (1) and (2) (or rather, clauses (2) and (1), in opposite order): If the bound variable \(z\) of the abstraction term \(\lambda z[A]\) is identical to the variable \(x\) for which we are to do a substitution, then we do not perform any substitution (that is, substitution ‚Äústops‚Äù). This coheres with the intention that \(M[x := N]\) is supposed to denote the substitution of \(N\) for the free occurrences of \(x\) in \(M\). If \(M\) is an abstraction term \(\lambda x[A]\) whose bound variable is \(x\), then \(x\) does not occurr freely in \(M\), so there is nothing to do. This explains clause 4. Clause (5), finally, says that if the bound variable of an abstraction term differs from \(x\), then at least \(x\) has the ‚Äúchance ‚Äù to occur freely in the abstraction term, and substitution continues into the body of the abstraction term.&lt;/p&gt;&lt;p&gt;Definition (change of bound variables, \(\alpha\)-convertibility). The term \(N\) is obtained from the term \(M\) by a change of bound variables if, roughly, any abstraction term \(\lambda x[A]\) inside \(M\) has been replaced by \(\lambda y[A[x := y]]\).&lt;/p&gt;&lt;p&gt;Let us say that terms \(M\) and \(N\) are \(\alpha\)-convertible if there is a sequence of changes of bound variables starting from \(M\) and ending at \(N\).&lt;/p&gt;&lt;p&gt;Axiom. \(\beta\)-conversion (stated with a no-capture proviso):&lt;/p&gt;&lt;p&gt; \( (\lambda x[M])N \rhd M[x := N]\), &lt;lb/&gt; provided no variable that occurrs free in \(N\) becomes bound after its substitution into \(M\).&lt;/p&gt;&lt;p&gt;Roughly, we need to adhere to the principle that free variables ought to remain free; when an occurrence of a variable is threatened to become bound by a substitution, simply perform enough \(\alpha\)-conversions to sidestep the problem. If we keep this in mind, we can work with \(\lambda\)-calculus without worrying about these nettlesome syntactic difficulties. So, for example, we can‚Äôt apply the function \(\lambda x[\lambda y[x(y-5)]]\) to the argument \(2y\) because upon substitution of ‚Äú\(2y\)‚Äù for ‚Äú\(x\)‚Äù, the ‚Äú\(y\)‚Äù in ‚Äú\(2y\)‚Äù would be captured by the variable-binding operator ‚Äú\(\lambda y\)‚Äù. Such a substitution would yield a function different from the one intended. However, we can first transform \(\lambda x[\lambda y[x(y-5)]]\) to \(\lambda x[\lambda z[x(z-5)]]\) by \(\alpha\)-conversion, and then apply this latter function to the argument \(2y\). So whereas the following is not a valid use of \(\beta\)-conversion: \[ (\lambda x[\lambda y[x(y-5)]])2y \rhd \lambda y[2y(y-5)]\] we can validly use \(\beta\)-conversion to conclude: \[ (\lambda x[\lambda z[x(z-5)]])2y \rhd \lambda z[2y(z-5)]\] This example helps one to see why the proviso to \(\beta\)-conversion is so important. The proviso is really no different from the one used in the statement of an axiom of the predicate calculus, namely: \(\forall x\phi \to \phi^{\tau}_x\), provided no variable that is free in the term \(\tau\) before the substitution becomes bound after the substitution.&lt;/p&gt;&lt;p&gt;The syntax of \(\lambda\)-calculus is quite flexible. One can form all sorts of terms, even self-applications such as \(xx\). Such terms appear at first blush to be suspicious; one might suspect that using such terms could lead to inconsistency, and in any case one might find oneself reaching for a tool with which to forbid such terms. If one were to view functions and sets of ordered pairs of a certain kind, then the \(x\) in \(xx\) would be a function (set of ordered pairs) that contains as an element a pair \((x,y)\) whose first element would be \(x\) itself. But no set can contain itself in this way, lest the axiom of foundation (or regularity) be violated. Thus, from a set theoretical perspective such terms are clearly dubious. Below one can find a brief sketch of one such tool, type theory. But in fact such terms do not lead to inconsistency and serve a useful purpose in the context of \(\lambda\)-calculus. Moreover, forbidding such terms, as in type theory, does not come for free (e.g., some of the expressiveness of untyped \(\lambda\)-calculus is lost).&lt;/p&gt;&lt;head rend="h3"&gt;2.2 Combinators&lt;/head&gt;&lt;p&gt;As defined earlier, a combinator is a \(\lambda\)-term with no free variables. One can intuitively understand combinators as ‚Äòcompletely specified‚Äô operations, since they have no free variables. There are a handful of combinators that have proven useful in the history of \(\lambda\)-calculus; the next table highlights some of these special combinators. Many more could be given (and obviously there are infinitely many combinators), but the following have concise definitions and have proved their utility. Below is a table of some standard \(\lambda\)-terms and combinators.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Name&lt;/cell&gt;&lt;cell&gt;Definition &amp;amp; Comments&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bS\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[\lambda z[xz(yz)]]]\) &lt;p&gt;Keep in mind that ‚Äò\(xz(yz)\)‚Äô is to be understood as the application \((xz)(yz)\) of \(xz\) to \(yz. \bS\) can thus be understood as a substitute-and-apply operator: \(z\) ‚Äòintervenes‚Äô between \(x\) and \(y\): instead of applying \(x\) to \(y\), we apply \(xz\) to \(yz\).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{K}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[x]]\) &lt;p&gt;The value of \(\mathbf{K}M\) is the constant function whose value for any argument is simply \(M.\)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{I}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[x]\) &lt;p&gt;The identity function.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{B}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[\lambda z[x(yz)]]]\) &lt;p&gt;Recall that ‚Äò\(xyz\)‚Äô is to be understood as \((xy)z\), so this combinator is not a trivial identity function.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{C}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[\lambda z[xzy]]]\) &lt;p&gt;Swaps an argument.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{T}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[x]]\) &lt;p&gt;Truth value true. Identical to \(\mathbf{K}\). We shall see later how these representations of truth values plays a role in the blending of logic and \(\lambda\)-calculus.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{F}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[\lambda y[y]]\) &lt;p&gt;Truth value false.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\boldsymbol{\omega}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda x[xx]\) &lt;p&gt;Self-application combinator&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\boldsymbol{\Omega}\)&lt;/cell&gt;&lt;cell&gt;\(\boldsymbol{\omega \omega}\) &lt;p&gt;Self-application of the self-application combinator. Reduces to itself.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\mathbf{Y}\)&lt;/cell&gt;&lt;cell&gt;\(\lambda f[(\lambda x[f(xx)])(\lambda x[f(xx)]\))] &lt;p&gt;Curry‚Äôs paradoxical combinator. For every \(\lambda\)-term \(X\), we have: \[\begin{align} \mathbf{Y}X &amp;amp;\rhd (\lambda x[X(xx)])(\lambda x[X(xx)]) \\ &amp;amp;\rhd X((\lambda x[X(xx)])(\lambda x[X(xx)])) \end{align}\] The first step in the reduction shows that \(\mathbf{Y}\)X reduces to the application term \((\lambda x[X(xx)])(\lambda x[X(xx)]\)), which is recurring in the third step. Thus, \(\mathbf{Y}\) has the curious property that \(\mathbf{Y}\)X and X\((\mathbf{Y}\)X) reduce to a common term.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;\(\boldsymbol{\Theta}\)&lt;/cell&gt;&lt;cell&gt;\((\lambda x[\lambda f[f(xxf)]])(\lambda x[\lambda f[f(xxf)]]\)) &lt;p&gt;Turing‚Äôs fixed-point combinator. For every \(\lambda\)-term \(X\), \(\boldsymbol{\Theta}X\) reduces to \(X(\boldsymbol{\Theta}X)\), which one can confirm by hand. (Curry‚Äôs paradoxical combinator \(\mathbf{Y}\) does not have this property.)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Below is a table of notational conventions employed in this entry.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Notation&lt;/cell&gt;&lt;cell&gt;Reading &amp;amp; Comments&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(MN\)&lt;/cell&gt;&lt;cell&gt;The application of the function \(M\) to the argument \(N\). &lt;p&gt;Usually, parentheses are used to separate the function from the argument, like so: ‚Äò\(M(N)\)‚Äô. However, in \(\lambda\)-calculus and kindred subjects the parentheses are used as grouping symbols. Thus, it is safe to write the function and the argument adjacent to one other.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(PQR\)&lt;/cell&gt;&lt;cell&gt;The application of the function \(PQ\)‚Äîwhich is itself the application of the function \(P\) to the argument \(Q\)‚Äîto \(R\). &lt;p&gt;If we do not use parentheses to separate function and argument, how are we to disambiguate expressions that involve three or more terms, such as ‚Äò\(PQR\)‚Äô? Recall our convention that we are to understand such officially illegal expressions by working from left to right, always putting parentheses around adjacent terms. Thus, ‚Äò\(PQR\)‚Äô is to be understood as \((PQ)R\). ‚Äò\(PQRS\)‚Äô is \(((PQ)R)S\). The expression ‚Äò\((PQ)R\)‚Äô is disambiguated; by our convention, it is identical to \(PQR\). The expression ‚Äò\(P(QR)\)‚Äô is also explicitly disambiguated; it is distinct from \(PQR\) because it is the application of \(P\) to the argument \(QR\) (which is itself the application of the function \(Q\) to the argument \(R)\).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\((\lambda x[M])\)&lt;/cell&gt;&lt;cell&gt;The \(\lambda\) term that binds the variable \(x\) in the \(\boldsymbol{body}\) term \(M\). &lt;p&gt;The official vocabulary of the \(\lambda\)-calculus consists of the symbol ‚Äò\(\lambda\)‚Äô, left ‚Äò(‚Äôand right ‚Äò)‚Äô parentheses, and a set of variables (assumed to be distinct from the three symbols ‚Äò\(\lambda\)‚Äô, ‚Äò(‚Äô, and ‚Äò)‚Äô lest we have syntactic chaos).&lt;/p&gt;&lt;p&gt;Alternative notation. It is not necessary to include two kinds of grouping symbols (parentheses and square brackets) in the syntax. Parentheses or square brackets alone would obviously suffice. The two kinds of brackets are employed in this entry for the sake of readability. Given the two kinds of grouping symbols, we could economize further and omit the parentheses from abstraction terms, so that ‚Äò\((\lambda x[M]\))‚Äô would be written as ‚Äò\(\lambda x[M]\)‚Äô.&lt;/p&gt;&lt;p&gt;Some authors write ‚Äò\(\lambda x.M\)‚Äô or ‚Äò\(\lambda x\cdot M\)‚Äô, with a full stop or a centered dot separating the bound variable from the body of the abstraction term. As with the square brackets, these devices are intended to assist reading \(\lambda\)-terms; they are usually not part of the official syntax. (One sees this device used in earlier works of logic, such as Principia Mathematica, where the function of the symbol . in expressions such as ‚Äò\(\forall x\).\(\phi\)‚Äô is to get us to read the whole of the formula \(\phi\) as under the scope of the \(\forall x\).)&lt;/p&gt;&lt;p&gt;Some authors write abstraction terms without any device separating the bound variable from the body: such terms are crisply written as, e.g., ‚Äò\(\lambda xx\)‚Äô, ‚Äò\(\lambda yx\)‚Äô. The practice is not without its merits: it is about as concise as one can ask for, and permits an even simpler official syntax of the \(\lambda\)-calculus. But this practice is not flawless. In ‚Äò\(\lambda xyz\)‚Äô, is the bound variable \(x\) or is it \(xy\)? Usually the names of variables are single letters, and theoretically this is clearly sufficient. But it seems unduly restrictive to forbid the practice of giving longer names to variables; indeed, such constructions arise naturally in computer programming languages.&lt;/p&gt;&lt;p&gt;For the sake of uniformity, we will adopt the square bracket notation in this entry. (Incidentally, this notation is used in (Turing, 1937).)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(M[x := A]\)&lt;/cell&gt;&lt;cell&gt;The \(\lambda\)-term that is obtained by substituting the \(\lambda\)-term A for all free occurrences of \(x\) inside \(M\). &lt;p&gt;A bewildering array of notations to represent substitution can be found in the literature on \(\lambda\)-calculus and kindred subjects:&lt;/p&gt;\[ M[x/A], M[A/x], M_{x}^A, M_{A}^x, [x/A]M,\ldots \]&lt;p&gt;Which notation to use for substitution seems to be a personal matter. In this entry we use a linear notation, eschewing superscripts and subscripts. The practice of representing substitution with ‚Äò:=‚Äô comes from computer science, where ‚Äò:=‚Äô is read in some programming languages as assigning a value to a variable.&lt;/p&gt;&lt;p&gt;As with the square brackets employed to write abstraction terms, the square brackets employed to write substitution are not officially part of the syntax of the \(\lambda\)-calculus. \(M\) and A are terms, \(x\) is a variable; \(M[x := A]\) is another term.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;\(M \equiv N\)&lt;/cell&gt;&lt;cell&gt;The \(\lambda\)-terms \(M\) and \(N\) are identical: understood as sequences of symbols, \(M\) and \(N\) have the same length and corresponding symbols of the sequences are identical. &lt;p&gt;The syntactic identity relation \(\equiv\) is not part of the official syntax of \(\lambda\)-calculus; this relation between \(\lambda\)-terms belongs to the metatheory of \(\lambda\)-calculus. It is clearly a rather strict notion of equality between \(\lambda\)-terms. Thus, it is not the case (if \(x\) and \(y\) are distinct variables) that \(\lambda x[x] \equiv \lambda y[y]\), even though these two terms clearly ‚Äòbehave‚Äô in the same way in the sense that both are expressions of the identity operation \(x \Rightarrow x\). Later we will develop formal theories of equality of \(\lambda\)-terms with the aim of capturing this intuitive equality of \(\lambda x[x]\) and \(\lambda y[y]\).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;3. Brief history of \(\lambda\)-calculus&lt;/head&gt;&lt;p&gt;\(\lambda\)-calculus arose from the study of functions as rules. Already the essential ingredients of the subject can be found in Frege‚Äôs pioneering work (Frege, 1893). Frege observed, as we did above, that in the study of functions it is sufficient to focus on unary functions (i.e., functions that take exactly one argument). (The procedure of viewing a multiple-arity operation as a sequence of abstractions that yield an equivalent unary operation is called currying the operation. Perhaps it would be more historically accurate to call the operation fregeing, but there are often miscarriages of justice in the appellation of mathematical ideas.) In the 1920s, the mathematician Moses Sch√∂nfinkel took the subject further with his study of so-called combinators. As was common in the early days of the subject, Sch√∂nfinkel was interested in the kinds of transformations that one sees in formal logic, and his combinators were intended to be a contribution to the foundations of formal logic. By analogy with the reduction that one sees in classical propositional logic with the Sheffer stroke, Sch√∂finkel established the astonishing result that the all functions (in the sense of all transformations) could be given in terms of the combinators \(\mathbf{K}\) and \(\bS\); later we will see the definition of these combinators.&lt;/p&gt;&lt;p&gt;Theorem For every term \(M\) made up of \(\mathbf{K}\) and \(\bS\) and the variable \(x\), there exists a term \(F\) (built only from \(\mathbf{K}\) and \(\bS)\) such that we can derive \(Fx = M\).&lt;/p&gt;&lt;p&gt;(The proof that these two suffice to represent all functions is beyond the scope of this entry. For further discussion, see the entry on combinatory logic.) One can prove the theorem constructively: there is an algorithm that, given \(M\), produces the required \(F\). Church called this \(F\) ‚Äò\(\lambda x[M]\)‚Äô (Church, 1932).[3] From this perspective, the \(\beta\)-rule can be justified: if ‚Äò\(\lambda x[M]\)‚Äô is to be a function \(F\) satisfying \(Fx = M\), then \(\lambda x[M]\)x should transform to \(M\). This is just a special case of the more general principle that for all \(N, (\lambda x[M])N\) should transform to \(M[x := N]\).&lt;/p&gt;&lt;p&gt;Although today we have more clearly delimited systems of abstraction and rewriting, in its early days \(\lambda\)-calculus and combinatory logic (√† la Sch√∂nfinkel) were bound up with investigations of foundations of mathematics. In the hands of Curry, Church, Kleene, and Rosser (some of the pioneers in the subject) the focus was on defining mathematical objects and carrying out logical reasoning inside the these new systems. It turned out that these early attempts at so-called illative \(\lambda\)-calculus and combinatory logic were inconsistent. Curry isolated and polished the inconsistency; the result is now known as Curry‚Äôs paradox. See the entry on Curry‚Äôs paradox and appendix B of (Barendregt, 1985).&lt;/p&gt;&lt;p&gt;The \(\lambda\)-calculus earns a special place in the history of logic because it was the source of the first undecidable problem. The problem is: given \(\lambda\)-terms \(M\) and \(N\), determine whether \(M = N\). (A theory of equational reasoning about \(\lambda\)-terms has not yet been defined; the definition will come later.) This problem was shown to be undecidable.&lt;/p&gt;&lt;p&gt;Another early problem in the \(\lambda\)-calculus was whether it is consistent at all. In this context, inconsistency means that all terms are equal: one can reduce any \(\lambda\)-term \(M\) to any other \(\lambda\)-term \(N\). That this is not the case is an early result of \(\lambda\)-calculus. Initially one had results showing that certain terms were not interconvertible (e.g., \(\mathbf{K}\) and \(\bS)\); later, a much more powerful result, the so-called Church-Rosser theorem, helped shed more light on \(\beta\)-conversion and could be used to give quick proofs of the non-inter-convertibility of whole classes of \(\lambda\)-terms. See below for more detailed discussion of consistency.&lt;/p&gt;&lt;p&gt;The \(\lambda\)-calculus was a somewhat obscure formalism until the 1960s, when, at last, a ‚Äòmathematical‚Äô semantics was found. Its relation to programming languages was also clarified. Till then the only models of \(\lambda\)-calculus were ‚Äòsyntactic‚Äô, that is, were generated in the style of Henkin and consisted of equivalence classes of \(\lambda\)-terms (for suitable notions of equivalence). Applications in the semantics of natural language, thanks to developments by Montague and other linguists, helped to ‚Äòspread the word‚Äô about the subject. Since then the \(\lambda\)-calculus enjoys a respectable place in mathematical logic, computer science, linguistics (see, e.g., Heim and Kratzer 1998), and kindred fields.&lt;/p&gt;&lt;head rend="h2"&gt;4. Reduction&lt;/head&gt;&lt;p&gt;Various notions of reduction for \(\lambda\)-terms are available, but the principal one is \(\beta\)-reduction, which we have already seen earlier. Earlier we used the notation ‚Äò\(\rhd\)‚Äô; we can be more precise. In this section we discuss \(\beta\)-reduction and some extensions.&lt;/p&gt;&lt;p&gt;Definition (one-step \(\beta\)-reduction \(\rhd_{\beta ,1})\) For \(\lambda\)-terms \(A\) and \(B\), we say that \(A\) \(\beta\)-reduces in one step to \(B\), written \(A \rhd_{\beta ,1} B\), just in case there exists an (occurrence of a) subterm \(C\) of \(A\), a variable \(x\), and \(\lambda\)-terms \(M\) and \(N\) such that \(C \equiv(\lambda x[M])N\) and \(B\) is \(A\) except that the occurrence of \(C\) in \(A\) is replaced by \(M[x := N]\).&lt;/p&gt;&lt;p&gt;Here are some examples of \(\beta\)-reduction:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;The variable \(x\) does not \(\beta\)-reduce to anything. (It does not have the right shape: it is simply a variable, not an application term whose left-hand side is an abstraction term.)&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;\((\lambda x[x])a \rhd_{\beta ,1} a\).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;If \(x\) and \(y\) are distinct variables, then \((\lambda x[y])a \rhd_{\beta ,1} y\).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term \((\lambda x[(\lambda y[xy])a])b]\) \(\beta\)-reduces in one step to two different \(\lambda\)-terms:&lt;/p&gt;\[ (\lambda x[(\lambda y[xy])a])b \rhd_{\beta ,1} (\lambda y[by])a \]&lt;p&gt;and&lt;/p&gt;\[ (\lambda x[(\lambda y[xy])a])b \rhd_{\beta ,1} (\lambda x[xa])b \]&lt;p&gt;Moreover, one can check that these two terms \(\beta\)-reduce in one step to a common term: \(ba\). We thus have:&lt;/p&gt;&lt;td&gt;\((\lambda y[by])a\)&lt;/td&gt;&lt;td&gt;\(\nearrow\)&lt;/td&gt;&lt;td&gt;\(\searrow\)&lt;/td&gt;&lt;td&gt;\((\lambda x[(\lambda y[xy])a])b\)&lt;/td&gt;&lt;td&gt;\(ba\)&lt;/td&gt;&lt;td&gt;\(\searrow\)&lt;/td&gt;&lt;td&gt;\(\nearrow\)&lt;/td&gt;&lt;td&gt;\((\lambda x[xa])b\)&lt;/td&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;As with any binary relation, one can ask many questions about the relation \(\rhd_{\beta ,1}\) holding between \(\lambda\)-terms, and one can define various derived notions in terms of \(\rhd_{\beta ,1}\).&lt;/p&gt;&lt;p&gt;Definition A \(\beta\)-reduction sequence from a \(\lambda\)-term \(A\) to a \(\lambda\)-term \(B\) is a finite sequence \(s_1 , \ldots s_n\) of \(\lambda\)-terms starting with \(A\), ending with \(B\), and whose adjacent terms \((s_k,s_{k+1})\) satisfy the property that \(s_k \rhd_{\beta ,1} s_{k+1}\).&lt;/p&gt;&lt;p&gt;More generally, any sequence \(s\)‚Äîfinite or infinite‚Äîstarting with a \(\lambda\)-term \(A\) is said to be a \(\beta\)-reduction sequence commencing with \(A\) provided that the adjacent terms \((s_k,s_{k+1})\) of \(s\) satisfy the property that \(s_k \rhd_{\beta ,1} s_{k+1}\).&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Continuing with \(\beta\)-reduction Example 1, there are no \(\beta\)-reduction sequences at all commencing with the variable \(x\).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Continuing with \(\beta\)-reduction Example 2, the two-term sequence&lt;/p&gt;\[ (\lambda x[x])a, a \]&lt;p&gt;is a \(\beta\)-reduction sequence from \((\lambda x[x])a\) to \(a\). If \(a\) is a variable, then this \(\beta\)-reduction sequence cannot be prolonged, and there are no other \(\beta\)-reduction sequences commencing with \((\lambda x[x])a\); thus, the set of \(\beta\)-reduction sequences commencing with \((\lambda x[x])a\) is finite and contains no infinite sequences.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;The combinator \(\boldsymbol{\Omega}\) has the curious property that \(\Omega \rhd_{\beta ,1} \Omega\). Every term of every \(\beta\)-reduction sequence commencing with \(\boldsymbol{\Omega}\) (finite or infinite) is equal to \(\boldsymbol{\Omega}\).&lt;/item&gt;&lt;item&gt;&lt;p&gt;Consider the term \(\mathbf{K}a\boldsymbol{\Omega}\). There are infinitely many reduction sequences commencing with this term:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;\(\bK a\boldsymbol{\Omega} \rhd_{\beta ,1} a\)&lt;/item&gt;&lt;item&gt;\(\bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \rhd_{\beta ,1} a\)&lt;/item&gt;&lt;item&gt;\(\bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \rhd_{\beta ,1} a\)&lt;/item&gt;&lt;item&gt;\(\bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \rhd_{\beta ,1} \bK a\boldsymbol{\Omega} \ldots\)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;If \(a\) is a variable, one can see that all finite reduction sequences commencing with \(\bK a\boldsymbol{\Omega}\) end at \(a\), and there is exactly one infinite reduction sequence.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Definition A \(\beta\)-redex of a \(\lambda\)-term \(M\) is (an occurrence of) a subterm of \(M\) of the form \((\lambda x[P])Q\). (‚Äòredex‚Äô comes from ‚Äòreducible expression.) A \(\beta\)-redex is simply a candidate for an application of \(\beta\)-reduction. Doing so, one contracts the \(\beta\)-redex. A term is said to be in \(\beta\)-normal form if it has no \(\beta\)-redexes.&lt;/p&gt;&lt;p&gt;(Can a term have multiple \(\beta\)-normal forms? The answer is literally ‚Äòyes‚Äô, but substantially the answer is ‚Äòno‚Äô: If a \(M\) and \(M'\) are \(\beta\)-normal forms of some term, then \(M\) is \(\alpha\)-convertible to \(M'\) Thus, \(\beta\)-normal forms are unique up to changes of bound variables.)&lt;/p&gt;&lt;p&gt;So far we have focused only on one step of \(\beta\)-reduction. One can combine multiple \(\beta\)-reduction steps into one by taking the transitive closure of the relation \(\rhd_{\beta ,1}\).&lt;/p&gt;&lt;p&gt;Definition For \(\lambda\)-terms \(A\) and \(B\), one says that \(A\) \(\beta\)-reduces to \(B\), written \(A \rhd_{\beta} B\), if either \(A \equiv B\) or there exists a finite \(\beta\)-reduction sequence from \(A\) to \(B\).&lt;/p&gt;&lt;p&gt;Definition A term \(M\) has a \(\beta\)-normal form if there exists a term \(N\) such that \(N\) is in \(\beta\)-normal form an \(M \rhd_{\beta} N\).&lt;/p&gt;&lt;p&gt;Reducibility as defined is a one-way relation: it is generally not true that if \(A \rhd_{\beta} B\), then \(B \rhd_{\beta} A\). However, depending on one‚Äôs purposes, one may wish to treat \(A\) and \(B\) as equivalent if either \(A\) reduces to \(B\) or \(B\) reduces to \(A\). Doing so amounts to considering the reflexive, symmetric, and transitive closure of the relation \(\rhd_{\beta ,1,}\).&lt;/p&gt;&lt;p&gt;Definition For \(\lambda\)-terms \(A\) and \(B\), we say that \(A =_{\beta} B\) if either \(A \equiv B\) or there exists a sequence \(s_1 , \ldots s_n\) starting with \(A\), ending with \(B\), and whose adjacent terms \((s_k,s_{k+1})\) are such that either \(s_k \rhd_{\beta ,1} s_{k+1}\) or \(s_{k+1} \rhd_{\beta ,1} s_k\).&lt;/p&gt;&lt;head rend="h3"&gt;4.1 Other notions of reduction&lt;/head&gt;&lt;p&gt;We have thus far developed the theory of \(\beta\)-reduction. This is by no means the only notion of reduction available in the \(\lambda\)-calculus. In addition to \(\beta\)-reduction, a standard relation between \(\lambda\)-terms is that of \(\eta\)-reduction:&lt;/p&gt;&lt;p&gt;Definition (one-step \(\eta\)-reduction) For \(\lambda\)-terms \(A\) and \(B\), we say that \(A\) \(\beta \eta\)-reduces in one step to \(B\), written \(A \rhd_{\beta \eta ,1} B\), just in case there exists an (occurrence of a) subterm \(C\) of \(A\), a variable \(x\), and \(\lambda\)-terms \(M\) and \(N\) such that either&lt;/p&gt;&lt;p&gt;\(C \equiv(\lambda x[M])N\) and \(B\) is \(A\) except that the occurrence of \(C\) in \(A\) is replaced by \(M[x := N]\)&lt;/p&gt;&lt;p&gt;or&lt;/p&gt;&lt;p&gt;\(C \equiv(\lambda x[Mx]\)) and \(B\) is \(A\) except that the occurrence of \(C\) in \(A\) is replaced by \(M\).&lt;/p&gt;&lt;p&gt;The first clause in the definition of \(\rhd_{\beta \eta ,1}\) ensures that the relation extends the relation of one-step \(\beta\)-reduction. As we did for the relation of one-step \(\beta\)-reduction, we can replay the development for \(\eta\)-reduction. Thus, one has the notion of an \(\eta\)-redex, and from \(\rhd_{\eta ,1}\) one can define the relation \(\rhd_{\eta}\) between \(\lambda\)-terms as the reflexive and transitive closure of \(\rhd_{\eta ,1}\), which captures zero-or-more-steps of \(\eta\)-reduction. Then one defines \(=_{\eta}\) as the symmetric and transitive closure of \(\rhd_{\eta}\).&lt;/p&gt;&lt;p&gt;If \(A \rhd_{\eta ,1} B\), then the length of \(B\) is strictly smaller than that of \(A\). Thus, there can be no infinite \(\eta\)-reductions. This is not the case of \(\beta\)-reduction, as we saw above in \(\beta\)-reduction sequence examples 3 and 4.&lt;/p&gt;&lt;p&gt;One can combine notions of reduction. One useful combination is to blend \(\beta\)- and \(\eta\)-reduction.&lt;/p&gt;&lt;p&gt;Definition (one-step \(\beta \eta\)-reduction) \(\lambda x[Mx] \rhd_{\beta \eta ,1} M\) and \((\lambda x[M]N)) \rhd_{\beta \eta ,1} M[x := N]\). A \(\lambda\)-term \(A\) \(\beta \eta\)-reduces in one step to a \(\lambda\)-term \(B\) just in case either \(A\) \(\beta\)-reduces to \(B\) in one step or \(A\) \(\eta\)-reduces to \(B\) in one step.&lt;/p&gt;&lt;p&gt;Again, one can replay the basic concepts of reduction, as we did for \(\beta\)-reduction, for this new notion of reduction \(\beta \eta\).&lt;/p&gt;&lt;head rend="h3"&gt;4.2 Reduction strategies&lt;/head&gt;&lt;p&gt;Recall that a term is said to be in \(\beta\)-normal form if it has no \(\beta\)-redexes, that is, subterms of the shape \((\lambda x[M]\))N. A term has a \(\beta\)-normal form if it can be reduced to a term in \(\beta\)-normal form. It should be intuitively clear that if a term has a \(\beta\)-normal form, then we can find one by exhaustively contracting all all \(\beta\)-redexes of the term, then exhaustively contracting all \(\beta\)-redexes of all resulting terms, and so forth. To say that a term has a \(\beta\)-normal form amounts to saying that this blind search for one will eventually terminate.&lt;/p&gt;&lt;p&gt;Blind search for \(\beta\)-normal forms is not satisfactory. In addition to be aesthetically unpleasant, it can be quite inefficient: there may not be any need to exhaustively contract all \(\beta\)-redexes. What is wanted is a strategy‚Äîpreferably, a computable one‚Äîfor finding a \(\beta\)-normal form. The problem is to effectively decide, if there are multiple \(\beta\)-redexes of a term, which ought to be reduced.&lt;/p&gt;&lt;p&gt;Definition A \(\beta\)-reduction strategy is a function whose domain is the set of all \(\lambda\)-terms and whose value on a term \(M\) not in \(\beta\)-normal form is a redex subterm of \(M\), and whose value on all terms M in \(\beta\)-normal form is simply \(M\).&lt;/p&gt;&lt;p&gt;In other words, a \(\beta\)-reduction strategy selects, whenever a term has multiple \(\beta\)-redexes, which one should be contracted. (If a term is in \(\beta\)-normal form, then nothing is to be done, which is why we require in the definition of \(\beta\)-reduction strategy that it does not change any term in \(\beta\)-normal form.) One can represent a strategy \(S\) as a relation \(\rhd_S\) on \(\lambda\)-terms, with the understanding that \(M \rhd_S N\) provided that \(N\) is obtained from \(M\) in one step by adhering to the strategy S. When viewed as relations, strategies constitute a subrelation of \(\rhd_{\beta ,1}\).&lt;/p&gt;&lt;p&gt;A \(\beta\)-reduction strategy may or may not have the property that adhering to the strategy will ensure that we (eventually) reach a \(\beta\)-normal form, if one exists.&lt;/p&gt;&lt;p&gt;Definition A \(\beta\)-reduction strategy \(S\) is normalizing if for all \(\lambda\)-terms \(M\), if \(M\) has a \(\beta\)-normal form \(N\), then the sequence \(M, S(M), S(S(M)),\ldots\) terminates at \(N\).&lt;/p&gt;&lt;p&gt;Some \(\beta\)-reduction strategies are normalizing, but others are not.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The rightmost strategy, whereby we always choose to reduce the rightmost \(\beta\)-redex (if there are any \(\beta\)-redexes) is not normalizing. Consider, for example, the term KI\(\Omega\). This term has two \(\beta\)-redexes: itself, and \(\Omega\) (which, recall, is the term \(\omega\omega\equiv(\lambda\)x[\(xx])(\lambda\)x[\(xx]\))). By working with left-hand \(\beta\)-redexes, we can \(\beta\)-reduce KI\(\Omega\) to \(\mathbf{I}\) in two steps. If we insist on working with the rightmost \(\beta\)-redex \(\Omega\) we reduce KI(\(\Omega\)) to \(\mathbf{KI}\)(\(\Omega \)), then \(\mathbf{KI}\)(\(\Omega\)), ‚Ä¶.&lt;/item&gt;&lt;item&gt;The leftmost strategy, whereby we always choose to reduce the leftmost \(\beta\)-redex (if there are any \(\beta\)-redexes) is normalizing. The proof of this fact is beyond the scope of this entry; see (Barendregt, 1985, section 13.2) for details.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Once we have defined a reduction strategy, it is natural to ask whether one can improve it. If a term has a \(\beta\)-normal form, then a strategy will discover a normal form; but might there be a shorter \(\beta\)-reduction sequence that reaches the same normal form (or a term that is \(\alpha\)-convertible to that normal form)? This is the question of optimality. Defining optimal strategies and showing that they are optimal is generally considerably more difficult than simply defining a strategy. For more discussion, see (Barendregt, 1984 chapter 10).&lt;/p&gt;&lt;p&gt;For the sake of concreteness, we have discussed only \(\beta\)-reduction strategies. But in the definitions above the notion of reduction \(\beta\) is but one possibility. For any notion \(R\) of reduction we have the associated theory of \(R\)-reduction strategies, and one can replay the problems of normalizability, optimality, etc., for \(R\).&lt;/p&gt;&lt;head rend="h2"&gt;5. \(\lambda\)-theories&lt;/head&gt;&lt;p&gt;We discussed earlier how the \(\lambda\)-calculus is a non-extensional theory of functions. If, in the non-extensional spirit, we understand \(\lambda\)-terms as descriptions, how should we treat equality of \(\lambda\)-terms? Various approaches are available. In this section, let us treat the equality relation = as a primitive, undefined relation holding between two \(\lambda\)-terms, and try to axiomatize the properties that equality should have. The task is to identity axioms and formulate suitable rules of inference concerning the equality of \(\lambda\)-terms.&lt;/p&gt;&lt;p&gt;Some obvious properties of equality, having nothing to do with \(\lambda\)-calculus, are as follows:&lt;/p&gt;\[\tag{Reflexivity} \frac{}{X=X} \] \[\tag{Symmetry} \frac{X=Y}{Y=X} \] \[\tag{Transitivity} \frac{X=Y \quad Y=Z}{X=Z} \]&lt;p&gt;As is standard in proof theory, the way to read these rules of inference is that above the horizontal rule \(\frac{}{\phantom{X=X}}\) are the premises of the rule (which are equations) and the equation below the horizontal rule is the conclusion of the rule of inference. In the case of the reflexivity rule, nothing is written above the horizontal rule. We understand such a case as saying that, for all terms \(X\), we may infer the equation \(X = X\) from no premises.&lt;/p&gt;&lt;head rend="h3"&gt;5.1 The basic theory \(\lambda\)&lt;/head&gt;&lt;p&gt;The three rules of inference listed in the previous section governing equality have nothing to do with the \(\lambda\)-calculus. The following lists rules of inference that relate the undefined notion of equality and the two term-building operations of the \(\lambda\)-calculus, application and abstraction.&lt;/p&gt;\[ \frac{M=N}{AM=AN} \quad \frac{M=N}{MA=NA} \] \[ \tag{\(\boldsymbol{\xi}\)} \frac{M=N}{\lambda x[M] = \lambda x[N]} \]&lt;p&gt;Together, these rules of inference say that = is a congruence relation on the set of \(\lambda\)-terms: it ‚Äòpreserves‚Äô both the application and abstraction term-building operations&lt;/p&gt;&lt;p&gt;The final rule of inference, \(\beta\)-conversion, is the most important:&lt;/p&gt;\[\tag{\(\boldsymbol{\beta}\)} \frac{}{(\lambda x[M])A = M[x := A]} \]&lt;p&gt;As before with the reflexivity rule, the rule \(\boldsymbol{\beta}\) has no premises: for any variable \(x\) and any terms \(M\) and \(A\), one can infer the equation \((\lambda x[M])A = M[x := A]\) at any point in a formal derivation in the theory \(\boldsymbol{\lambda}\).&lt;/p&gt;&lt;head rend="h3"&gt;5.2 Extending the basic theory \(\lambda\)&lt;/head&gt;&lt;p&gt;A number of extensions to \(\boldsymbol{\lambda}\) are available. Consider, for example, the rule (\(\boldsymbol{\eta}\)), which expresses the principle of \(\eta\)-reduction as a rule of inference:&lt;/p&gt;\[\tag{\(\boldsymbol{\eta}\)} \frac{}{\lambda x[Mx] = M} \text{ provided } x \not\in \mathbf{FV}(M) \]&lt;p&gt;Rule \(\boldsymbol{\eta}\) tells us that a certain kind of abstraction is otiose: it is safe to identify \(M\) with the function that, given an argument \(x\), applies \(M\) to \(x\). Through this rule we can also see that all terms are effectively functions. One can intuitively justify this rule using the principle of \(\beta\)-reduction.&lt;/p&gt;\[\tag{\(\mathbf{Ext}\)} \frac{Mx=Nx}{M=N}\text{ provided } x \not\in \mathbf{FV}(M) \cup \mathbf{FV}(N) \]&lt;p&gt;One can view rule \(\mathbf{Ext}\) as a kind of generalization principle. If we have derived that \(Mx = Nx\), but \(x\) figures in neither \(M\) nor \(N\), then we have effectively shown that \(M\) and \(N\) are alike. Compare this principle to the principle of universal generalization in first-order logic: if we have derived \(\phi(x)\) from a set \(\Gamma\) of hypotheses in which \(x\) is not free, then we can conclude that \(\Gamma\) derives \(\forall x\phi\).&lt;/p&gt;&lt;p&gt;Another productive principle in the \(\lambda\)-calculus permits us to identify terms that ‚Äòact‚Äô the same:&lt;/p&gt;\[\tag{\(\boldsymbol{\omega}\)} \frac{\text{For all terms }x, Mx=Nx}{M=N} \]&lt;p&gt;The rule \(\boldsymbol{\omega}\) has infinitely many hypotheses: on the assumption that \(Mx = Nx\), no matter what \(x\) may be, then we can conclude that \(M = N\). The \(\boldsymbol{\omega}\) rule is an analogue in the \(\lambda\)-calculus of the rule of inference under the same name in formal number theory, according to which one can conclude the universal formula \(\forall x\phi\) provided one has proofs for \(\phi(x := \mathbf{0}), \phi(x := \mathbf{1}),\ldots\) . Note that unlike the rule \(\mathbf{Ext}\), the condition that \(x\) not occur freely in \(M\) or \(N\) does not arise.&lt;/p&gt;&lt;head rend="h2"&gt;6. Consistency of the \(\lambda\)-calculus&lt;/head&gt;&lt;p&gt;Is the \(\lambda\)-calculus consistent? The question might not be well-posed. The \(\lambda\)-calculus is not a logic for reasoning about propositions; there is no apparent notion of contradiction \((\bot)\) or a method of forming absurd propositions (e.g., \(p \wedge \neg p)\). Thus ‚Äòinconsistency‚Äô of the \(\lambda\)-calculus cannot mean that \(\bot\), or some formula tantamount to \(\bot\), is derivable. A suitable notion of ‚Äòconsistent‚Äô is, however, available. Intuitively, a logic is inconsistent if it permits us to derive too much. The theory \(\lambda\) is a theory of equations. We can thus take inconsistency of \(\lambda\) to mean: all equations are derivable. Such a property, if it were true of \(\lambda\), would clearly show that \(\lambda\) is of little use as a formal theory.&lt;/p&gt;&lt;p&gt;Early formulations of the idea of \(\lambda\)-calculus by A. Church were indeed inconsistent; see (Barendregt, 1985, appendix 2) or (Rosser, 1985) for a discussion. To take a concrete problem: how do we know that the equation \(\bK = \mathbf{I}\) is not a theorem of \(\lambda\)? The two terms are obviously intuitively distinct. \(\bK\) is a function of two arguments, whereas \(\mathbf{I}\) is a function of one argument. If we could show that \(\bK = \mathbf{I}\), then we could show that \(\mathbf{KK} = \mathbf{IK}\), whence \(\mathbf{KK} = \bK\) would be a theorem of \(\lambda\), along with many other equations that strike us as intuitively unacceptable. But when we‚Äôre investigating a formal theory such as \(\lambda\), intuitive unacceptability by no means implies underivability. What is missing is a deeper understanding of \(\beta\)-reduction.&lt;/p&gt;&lt;p&gt;An early result that gave such an understanding is known as the Church-Rosser theorem:&lt;/p&gt;&lt;p&gt;Theorem (Church-Rosser) If \(P \rhd_{\beta} Q\) and \(P \rhd_{\beta}\) R, then there exists a term \(S\) such that both \(Q \rhd_{\beta} S\) and \(R \rhd_{\beta} S\).&lt;/p&gt;&lt;p&gt;(The proof of this theorem is quite non-trivial and is well-beyond the scope of this entry.) The result is a deep fact about \(\beta\)-reduction. It says that no matter how we diverge from \(P\) by \(\beta\)-reductions, we can always converge again to a common term.&lt;/p&gt;&lt;p&gt;The Church-Rosser theorem gives us, among other things, that the plain \(\lambda\)-calculus‚Äîthat is, the theory \(\lambda\) of equations between \(\lambda\)-terms‚Äîis consistent, in the sense that not all equations are derivable.&lt;/p&gt;&lt;p&gt;As an illustration, we can use the Church-Rosser theorem to solve the earlier problem of showing that the two terms \(\bK\) and \(\mathbf{I}\) are not identified by \(\lambda\). The two terms are in \(\beta\)-normal form, so from them there are no \(\beta\)-reduction sequences at all. If \(\bK = \mathbf{I}\) were a theorem of \(\lambda\), then there would be a term \(M\) from which there is a \(\beta\)-reduction path to both \(\mathbf{I}\) and \(\bK\). The Church-Rosser theorem then implies the two paths diverging from \(M\) can be merged. But this is impossible, since \(\bK\) and \(\mathbf{I}\) are distinct \(\beta\)-normal forms.&lt;/p&gt;&lt;p&gt;The Church-Rosser theorem implies the existence of \(\beta\)-reduction sequences commencing from \(\bK\) and from \(\mathbf{I}\) that end at a common term. But there are no \(\beta\)-reduction sequences at all commencing from \(\mathbf{I}\), because it is in \(\beta\)-normal form, and likewise for \(\bK\).&lt;/p&gt;&lt;p&gt;Theorem \(\lambda\) is consistent, in the sense that not every equation is a theorem.&lt;/p&gt;&lt;p&gt;To prove the theorem, it is sufficient to produce one underivable equation. We have already worked through an example: we used the Church-Rosser theorem to show that the equation \(\bK = \mathbf{I}\) is not a theorem of \(\lambda\). Of course, there‚Äôs nothing special about these two terms. A significant generalization of this result is available: if \(M\) and \(N\) in \(\beta\)-normal form but \(M\) is distinct from \(N\), then the equation \(M = N\) is not a theorem of \(\lambda\). (This simple condition for underivability does not generally hold if we add additional rules of inference to \(\lambda\).)&lt;/p&gt;&lt;p&gt;The theories \(\lambda \eta\) and \(\lambda \omega\) are likewise consistent. One can prove these consistency results along the lines of the consistency proof for \(\lambda\) by extending the Church-Rosser theorem to the wider senses of derivability of these theories.&lt;/p&gt;&lt;head rend="h2"&gt;7. Semantics of \(\lambda\)-calculus&lt;/head&gt;&lt;p&gt;As we‚Äôve said at the outset, the \(\lambda\)-calculus is, at heart, about functions and their applications. But it is surprisingly difficult to cash this idea out in semantic terms. A natural approach would be to try to associate with every \(\lambda\)-term \(M\) a function \(f_M\) over some domain \(D\) and to interpret application terms \((MN)\) using function application as \(f_M(f_N).\) But this idea quickly runs into difficulties. To begin with, it‚Äôs easy to see that, in this context, we can‚Äôt use the standard set-theoretic concept of functions-as-sets (see section 1.2 of this entry). According to this concept, remember, a function \(f\) is a set of argument-value pairs, where every argument gets assigned a unique value. The problem arises in the context of self-applications. Remember from section 2.1 that the untyped \(\lambda\)-calculus allows \(\lambda\)-terms such as \((xx)\), which intuitively apply \(x\) to itself. On the semantic picture we‚Äôre exploring, we can obtain the associated function \(f_{(xx)}\) for the term \((xx)\) by taking the function \(f_x\) for \(x\) and applying it to itself:&lt;/p&gt;\[ f_{(xx)}=f_x(f_x) \]&lt;p&gt;But following functions-as-sets, this would mean that the set \(f_x\) needs to contain an argument-value pair that has \(f_x\) as its first component and \(f_{(xx)}\) as the second:&lt;/p&gt;\[ f_{x}=\{\ldots, (f_{x},f_{(xx)})), \ldots\} \]&lt;p&gt;But this would make \(f_x\) a non-well-founded object: defining \(f_x\) would involve \(f_x\) itself. In fact, sets like this are excluded in standard axiomatic set theory by the axiom of foundation (also known as the axiom of regularity). ‚ÄîThis is further semantic evidence that the concept of a function underlying the \(\lambda\)-calculus can‚Äôt be the extensional functions-as-sets concept.&lt;/p&gt;&lt;p&gt;But the problem runs even deeper than that. Even when we use a non-extensional notion of a function, such as the functions-as-rules conception (see again section 1.2), we run into difficulties. In the untyped \(\lambda\)-calculus, everything can both be function and an argument to functions. Correspondingly, we should want our domain \(D\) to include, in some sense, the function space \(D^D\), which contains all and only the functions with both arguments and values from \(D\). To see this:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Every element of \(D\) can be a function that applies to elements of \(D\), and what‚Äôs returned can then be again be an argument for elements of \(D\). So, every element of \(D\) intuitively corresponds to a member of \(D^D\).&lt;/item&gt;&lt;item&gt;If, in turn, we take a member of \(D^D\), i.e., a function with arguments and values from \(D\), this is precisely the kind of thing we want to include in our domain \(D\). So, intuitively, we want every member of \(D^D\) to correspond to a member of \(D\).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In short, we want there to be a one-to-one correspondence between our domain and its own function space, i.e., we want them to satisfy the ‚Äòequation‚Äô \(X\cong X^X\). But this is impossible since it contradicts Cantor‚Äôs theorem.&lt;/p&gt;&lt;p&gt;Given these difficulties, the question arises whether it‚Äôs possible to give a set-theoretic model for the \(\lambda\)-calculus in the first place? It turns out that it is. D. Scott was the first to describe such a model in an unpublished manuscript from 1969. This model, \(D_\infty\), solves the aforementioned problems with Cantor‚Äôs theorem by suitably restricting the function space \(D^D\), by only letting some members of \(D^D\) correspond to members of \(D\). Covering Scott‚Äôs construction goes beyond the scope of this entry, since it involves advanced tools from algebra and topology; see (Meyer 1982), (Barendregt, 1985, chapter 18.2), or (Hindley and Seldin, 2008, chapter 16) for details. Instead, we‚Äôll discuss the more general question: What is a model for the \(\lambda\)-calculus? That is, leaving aside for a moment the question whether sets are functions, rules, or something altogether different, we ask what kind of mathematical structure a model for the \(\lambda\)-calculus is in the first place.&lt;/p&gt;&lt;head rend="h3"&gt;7.1 \(\lambda\)-Models&lt;/head&gt;&lt;p&gt;It turns out that there are multiple, essentially equivalent, ways of defining the notion of a model for the \(\lambda\)-calculus; see (Barendregt, 1985, chapter 5) or (Hindley and Seldin, 2008, chapter 15). In the following, we‚Äôll discuss what we consider the most palatable notion for philosophers familiar with the standard semantics for first-order logic (see, e.g., the entry on Classical Logic ), the so-called syntactical \(\lambda\)-models. These models first appear in the work of (Hindley and Longo, 1980), (Koymans, 1982), and (Meyer 1982). They derive their name from the fact that their clauses closely correspond to the syntactic rules of the calculus \(\boldsymbol{\lambda}\). This is somewhat unsatisfactory and motivates ‚Äòsyntax-free‚Äô definitions (see below). At the same time, the syntactical \(\lambda\)-models provide a fairly transparent and accessible route into the world of \(\lambda\)-models. In addition, despite their conceptual shortcomings, syntactical models have proven a technically useful tool in the semantical study of the \(\lambda\)-calculus.&lt;/p&gt;&lt;p&gt;In order to avoid the set-theoretic problems mentioned above, most definitions of \(\lambda\)-models use so-called applicative structures. The idea is to treat the denotations of \(\lambda\)-terms not as set-theoretic functions, but as unanalyzed, first-order ‚Äòfunction-objects‚Äô, instead. Correspondingly, then, we treat function application as an unanalyzed binary operation on these function-objects:&lt;/p&gt;&lt;p&gt;Definition An applicative structure is a pair \((D,\cdot)\), where \(D\) is some set and \(\cdot\) a binary operation on \(D\). To avoid trivial models, we usually assume that \(D\) has at least two elements.&lt;/p&gt;&lt;p&gt;Applicative structures are, in a sense, first-order models of function spaces that satisfy the problematic equation \(X\cong X^X\). \(\lambda\)-models, in turn, are defined over them.&lt;/p&gt;&lt;p&gt;For the definition of our \(\lambda\)-models, we work with valuations‚Äîa concept familiar from first-order semantics. Valuations assign denotations to the variables and are used primarily in the semantic clauses for the \(\lambda\)-operator. Additionally, they can be used to express general claims over the domain, in a way that is familiar from the semantics for the first-order quantifiers \(\exists x\) and \(\forall x\).&lt;/p&gt;&lt;p&gt;Definition A valuation in an applicative structure \((D,\cdot)\) is a function \(\rho\) that assigns an element \(\rho(x)\in D\) to every variable \(x\).&lt;/p&gt;&lt;p&gt;As a useful piece of notation, for \(\rho\) a valuation in some applicative structure \((D,\cdot)\), \(x\) a variable, and \(d\in D\) an object, we define the valuation \(\rho[x\mapsto d]\) by saying that: \[\rho[x\mapsto d](y)=\begin{cases} d &amp;amp; \text{ if }y=x\\ \rho(y) &amp;amp; \text{otherwise}\end{cases}\] That is, \(\rho[x\mapsto d]\) is the result of changing the value of \(x\) to be \(d\), while leaving all other other values under \(\rho\) unchanged.&lt;/p&gt;&lt;p&gt;Definition A syntactical \(\lambda\)-model is a triple \(\mathfrak{M}=(D,\cdot,\llbracket \ \rrbracket)\), where \((D,\cdot)\) is an applicative structure and \(\llbracket \ \rrbracket\) is a function that assigns to every \(\lambda\)-term M and valuation \(\rho\) a denotation \(\llbracket M\rrbracket_\rho\in D\) subject to the following constraints:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;\(\llbracket x\rrbracket_\rho=\rho(x)\)&lt;/item&gt;&lt;item&gt;\(\llbracket MN\rrbracket_\rho=\llbracket M\rrbracket_\rho\cdot \llbracket N\rrbracket_\rho\)&lt;/item&gt;&lt;item&gt;\(\llbracket \lambda xM\rrbracket_\rho\cdot d=\llbracket M\rrbracket_{\rho[x\mapsto d]}\), for all \(d\in D\)&lt;/item&gt;&lt;item&gt;\(\llbracket \lambda xM\rrbracket_\rho = \llbracket \lambda xN\rrbracket_\rho\), whenever for all \(d\in D\), we have \(\llbracket M\rrbracket_{\rho[x\mapsto d]}=\llbracket N\rrbracket_{\rho[x\mapsto d]}\)&lt;/item&gt;&lt;item&gt;\(\llbracket M\rrbracket_\rho=\llbracket M\rrbracket_\sigma\), whenever \(\rho(x)=\sigma(x)\) for all \(x\in \mathbf{FV}(M)\)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Intuitively, in a model \(\mathfrak{M}\), \(\llbracket M\rrbracket_\rho\) is the function-object denoted by the \(\lambda\)-term \(M\) under the valuation \(\rho\).&lt;/p&gt;&lt;p&gt;It is now straight-forward to define what it means for a \(\lambda\)-model \(\mathfrak{M}\) to satisfy an equation \(M=N\), symbolically \(\mathfrak{M}\vDash M=N\):&lt;/p&gt;&lt;p&gt;Definition (satisfaction).&lt;/p&gt;\[\mathfrak{M}\vDash M=N\text{ iff for all }\rho\text{, we have } \llbracket M\rrbracket_\rho=\llbracket N\rrbracket_\rho\]&lt;p&gt;In words: an equation \(M=N\) holds in a model \(\mathfrak{M}\) just in case the \(\lambda\)-terms \(M\) and \(N\) have the same denotation under every valuation in the underlying applicative structure.&lt;/p&gt;&lt;p&gt;Note that clauses 3. and 4. from the definition of a syntactical \(\lambda\)-model directly mirror the \(\boldsymbol{\lambda}\)-rules \(\boldsymbol{\beta}\) and \(\boldsymbol{\xi}\), respectively (see section 5.1 above). This is the ‚Äòsyntactic‚Äô nature of our models. While this might be semantically unsatisfactory (see below), it makes it relatively straight-forward to prove a soundness theorem for the semantics provided by the syntactical \(\lambda\)-models; see (Barendregt, 1985, Theorem 5.3.4) and (Hindley and Seldin, 2008. Theorem 15.12):&lt;/p&gt;&lt;p&gt;Theorem For all terms \(M,N\), if \(M=N\) is derivable in \(\boldsymbol{\lambda}\), then for all syntactical \(\lambda\)-models \(\mathfrak{M}\), we have that \(\mathfrak{M}\vDash M=N\).&lt;/p&gt;&lt;p&gt;This theorem provides a first ‚Äòsanity-check‚Äô for the semantics. But note that, so far, we haven‚Äôt shown that there exist any syntactical \(\lambda\)-models at all.&lt;/p&gt;&lt;p&gt;This worry is addressed by constructing so-called ‚Äòterm models‚Äô, which are not unlike the well-known Henkin constructions from first-order semantics. In order to define these models, we first need the notion of a \(\boldsymbol{\lambda}\)-equivalence class for a given \(\lambda\)-term \(M\). This class contains precisely the terms that \(\boldsymbol{\lambda}\) proves identical to \(M\):&lt;/p&gt;\[ [M]_{\boldsymbol{\lambda}}=\{N:\boldsymbol{\lambda}\text{ proves }M=N\} \]&lt;p&gt;We then define the term model for \(\boldsymbol{\lambda}\), \(\mathfrak{T}\), by setting:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;\(D=\{[M]_\boldsymbol{\lambda}:M\text{ is a }\lambda\text{-term}\}\)&lt;/item&gt;&lt;item&gt;\([M]_\boldsymbol{\lambda}\cdot [N]_\boldsymbol{\lambda}=[MN]_\boldsymbol{\lambda}\)&lt;/item&gt;&lt;item&gt;\(\llbracket M\rrbracket_\rho=[M[x_1:=N_1]\ldots[x_n:=N_n]]_\boldsymbol{\lambda}\), where \(\mathbf{FV}(M)=\{x_1,\ldots,x_n\}\) and \(\rho(x_1)=N_1, \ldots,\rho(x_n)=N_n\)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;It is easily seen that this indeed defines a syntactical \(\lambda\)-model. In fact, it is easily checked that in the term model for \(\boldsymbol{\lambda}\), we have that:&lt;/p&gt;\[ \mathfrak{T}\vDash M=N\text{ iff }\boldsymbol{\lambda}\text{ derives }M=N. \]&lt;p&gt;This paves a way for a very simple completeness proof for \(\boldsymbol{\lambda}\) with respect to the class of syntactical \(\lambda\)-models; see (Meyer, 1982, 98‚Äì99) for one of the few explicit mentions of this kind of result in the literature:&lt;/p&gt;&lt;p&gt;Theorem For all terms \(M,N\), if for all syntactical \(\lambda\)-models \(\mathfrak{M}\), we have that \(\mathfrak{M}\vDash M=N\), then \(M=N\) is derivable in \(\boldsymbol{\lambda}\).&lt;/p&gt;&lt;p&gt;The proof is a simple proof by contraposition, which uses the term model \(\mathfrak{T}\) as a countermodel to any non-derivable identity in \(\boldsymbol{\lambda}\).&lt;/p&gt;&lt;p&gt;But there are reasons to be dissatisfied with the syntactical \(\lambda\)-models as a semantics for the \(\lambda\)-calculus. For one, by virtue of clauses 3. and 4. mirroring rules \(\boldsymbol{\beta}\) and \(\boldsymbol{\xi}\), the soundness result is ‚Äòbaked into‚Äô the semantics, as it were. This is unsatisfactory from a semantic perspective since it means that via the syntactical \(\lambda\)-models, we don‚Äôt really learn anything directly about what conditions an applicative structure needs to satisfy in order to adequately model the \(\lambda\)-calculus.&lt;/p&gt;&lt;p&gt;A related worry is that the clauses 3. and 4. are not recursive in nature. That is, they don‚Äôt allow us to compute the denotation of a complex \(\lambda\)-term from the denotations of its parts and information about the syntactic operation used to combine them. In our syntax (see section 2), there are two ways of constructing complex \(\lambda\)-terms: application terms of the form \(MN\) and abstraction terms of the form \((\lambda x[M])\). Clause 1. of our syntactical \(\lambda\)-models is a recursive clause for the syntactical application operation, but we don‚Äôt have a recursive clause for the syntactical operation of \(\lambda\)-abstraction. Clauses 3. and 4. are rather conditions on the denotation function \(\llbracket \ \rrbracket\) than recursive clauses. This is unsatisfactory since it means that we‚Äôre not really given a compositional semantics for the \(\lambda\)-operator by the syntactical \(\lambda\)-models.&lt;/p&gt;&lt;p&gt;These worries are taken care of in the development of syntax-free \(\lambda\)-models. A comprehensive discussion of syntax-free models goes beyond the scope of this entry; but see (Barendregt, 1985, chapter 5.2) and (Hinley and Seldin, 2008, chapter 15B) for the details. Suffice it to say that the definition of syntax-free \(\lambda\)-models involves determining precisely under which conditions an applicative structure is suitable for interpreting the \(\lambda\)-calculus. The resulting \(\lambda\)-models, then, indeed provide (something much closer to) a recursive, compositional semantics, where the syntactical operation of \(\lambda\)-abstraction is interepreted by a corresponding semantic operation on applicative structures.&lt;/p&gt;&lt;p&gt;It is worth noting, however, that syntactical \(\lambda\)-models and the syntax-free \(\lambda\)-models are, in a certain sense, equivalent: every syntactical \(\lambda\)-model defines a syntax-free \(\lambda\)-model and vice versa; see (Barendregt, 1985, theorem 5.3.6) and (Hinley and Seldin, 2008, theorem 15.20) for the details. From a technical perspective, this result allows us to freely move between the different presentations of \(\lambda\)-models and to use, in a given context, the notion of a model that is most expedient. At the same time, there may be philosophical reasons to prefer one presentation over the other, such as the semantic worries about syntactical \(\lambda\)-models mentioned above.&lt;/p&gt;&lt;p&gt;Before moving to model constructions, let us briefly mention that there are various ways of approaching \(\lambda\)-models. One particularly interesting approach we‚Äôve neglected so far is from the perspective of category theory and categorical logic. There are well-known model descriptions using so-called ‚ÄòCartesian closed categories‚Äô; see (Koymans, 1982). Covering these model descriptions goes beyond the scope of the present entry since it requires a familiarity with a wide range of concepts from category theory; see the entry Category Theory for a sense of the machinery involved. For the details of these model descriptions, instead, (Barendregt, 1985, sections 5.4‚Äì6). In recent years, there has been a renewed interest in categorical approaches to the \(\lambda\)-calculus, which have mainly focused on typed versions of the \(\lambda\)-calculus (see sections 8.2 and 9.1.2 below) but also include the untyped \(\lambda\)-calculus discussed in this article. See, for example, (Hyland, 2017) for a recent discussion.&lt;/p&gt;&lt;head rend="h3"&gt;7.2 Model Constructions&lt;/head&gt;&lt;p&gt;The term model we‚Äôve seen in section 7.1 is rather trivial: it directly reflects the syntactic structure of the \(\lambda\)-terms by modeling precisely syntactic equality modulo \(\boldsymbol{\lambda}\)-provable equality. This makes the term model mathematically and philosophically rather uninteresting. The construction and study of more interesting concrete \(\lambda\)-models is one of the principal aims of the model theory for the \(\lambda\)-calculus.&lt;/p&gt;&lt;p&gt;We‚Äôve already mentioned what‚Äôs perhaps the most important, but was definitely the first non-trivial model for the \(\lambda\)-calculus: Scott‚Äôs \(D_\infty\). But there are also other interesting model constructions, such as Plotkin and Scott‚Äôs graph model \(P_\omega\), first described in (Plotkin 1972) and (Scott, 1974). These model constructions, however, usually rely on fairly involved mathematical methods, both for their definitions and for verifying that they are indeed \(\lambda\)-models. Consequently, covering these constructions goes beyond the scope of this entry; see (Hinley and Seldin, 2008, chapter 16F) for an overview of various model constructions and (Barendregt, 1985, chapter 18) for many of the formal details.&lt;/p&gt;&lt;p&gt;One of the advantages of having different models is that one sees different aspects of equality in the \(\lambda\)-calculus: each of the different models takes a different view on what \(\lambda\)-terms get identified. An interesting question in this context is: What is the \(\lambda\)-theory of a given class of models? In this context, we call a class \(\mathcal{C}\) of \(\lambda\)-models complete just in case every (consistent) \(\lambda\)-theory is satisfied by some model in \(\mathcal{C}\). See (Salibra, 2003) for an overview of various completeness and incompleteness results for interesting classes of \(\lambda\)-models.&lt;/p&gt;&lt;head rend="h2"&gt;8. Extensions and Variations&lt;/head&gt;&lt;head rend="h3"&gt;8.1 Combinatory logic&lt;/head&gt;&lt;p&gt;A sister formalism of the \(\lambda\)-calculus, developed slightly earlier, deals with variable-free combinations. Combinatory logic is indeed even simpler than the \(\lambda\)-calculus, since it lacks a notion of variable binding.&lt;/p&gt;&lt;p&gt;The language of combinatory logic is built up from combinators and variables. There is some flexibility in precisely which combinators are chosen as basic, but some standard ones are \(\mathbf{I}, \bK , \bS, \mathbf{B}\) and \(\mathbf{C}\). (The names are not arbitrary.)&lt;/p&gt;&lt;p&gt;As with the \(\lambda\)-calculus, with combinatory logic one is interested in reducibility and provability. The principal reduction relations are:&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Combinator&lt;/cell&gt;&lt;cell&gt;Reduction Axiom&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bI\)&lt;/cell&gt;&lt;cell&gt;\(\bI x = x\)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bK\)&lt;/cell&gt;&lt;cell&gt;\(\bK xy = x\)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bS\)&lt;/cell&gt;&lt;cell&gt;\(\bS xyz = xz(yz)\)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bB\)&lt;/cell&gt;&lt;cell&gt;\(\bB xyz = x(yz)\)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;\(\bC\)&lt;/cell&gt;&lt;cell&gt;\(\bC xyz = xzy\)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;There is a passage from \(\lambda\)-calculus to combinatory logic via translation. It turns out that although combinatory logic lacks a notion of abstraction, one can define such a notion and thereby simulate the \(\lambda\)-calculus in combinatory logic. Here is one translation; it is defined recursively.&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell&gt;Rule&lt;/cell&gt;&lt;cell&gt;Expression&lt;/cell&gt;&lt;cell&gt;Translation&lt;/cell&gt;&lt;cell&gt;Condition&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;\(x\)&lt;/cell&gt;&lt;cell&gt;\(x\)&lt;/cell&gt;&lt;cell&gt;(unconditional)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;\(MN\)&lt;/cell&gt;&lt;cell&gt;M\(^*\)N\(^*\)&lt;/cell&gt;&lt;cell&gt;(unconditional)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;cell&gt;\(\lambda x[M]\)&lt;/cell&gt;&lt;cell&gt;\(\bK\)M&lt;/cell&gt;&lt;cell&gt;\(x\) does not occur freely in M&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;cell&gt;\(\lambda x[x]\)&lt;/cell&gt;&lt;cell&gt;\(\bI\)&lt;/cell&gt;&lt;cell&gt;(unconditional)&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;5&lt;/cell&gt;&lt;cell&gt;\(\lambda x[Mx]\)&lt;/cell&gt;&lt;cell&gt;M&lt;/cell&gt;&lt;cell&gt;\(x\) does not occur freely in M&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;6&lt;/cell&gt;&lt;cell&gt;\(\lambda x[MN]\)&lt;/cell&gt;&lt;cell&gt;\(\bB M(\lambda x[N)]^*\)&lt;/cell&gt;&lt;cell&gt;\(x\) does not occur freely in M&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;7&lt;/cell&gt;&lt;cell&gt;\(\lambda x[MN]\)&lt;/cell&gt;&lt;cell&gt;\(\bC (\lambda x[M])^*\)N&lt;/cell&gt;&lt;cell&gt;\(x\) does not occur freely in \(N\)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;8&lt;/cell&gt;&lt;cell&gt;\(\lambda x[MN]\)&lt;/cell&gt;&lt;cell&gt;\(\bS M^*N^*\)&lt;/cell&gt;&lt;cell&gt;\(x\) occurs freely in both \(M\) and \(N\)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;This translation works inside-out, rather than outside-in. To illustrate:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;The translation of the term \(\lambda y[y]\), a representative of the identity function, is mapped by this translation to the identity combinator \(\bI\) (because of Rule 4), as expected.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term \(\lambda x[\lambda y[x]]\) that we have been calling ‚Äò\(\bK\)‚Äôis mapped by this translation to:&lt;/p&gt;\[\begin{align} \lambda x[\lambda y[x]] &amp;amp;\equiv \lambda x[\bK x] &amp;amp;\langle \text{Rule 1}\rangle \\ &amp;amp;\equiv \bK &amp;amp;\langle \text{Rule 3} \rangle \end{align}\]&lt;/item&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term \(\lambda x[\lambda y[yx]]\) that switches its two arguments is mapped by this translation to:&lt;/p&gt;\[\begin{align} \lambda x[\lambda y[yx]] &amp;amp;\equiv \lambda x[\bC(\lambda y[y])^* x] &amp;amp;\langle\text{Rule 8}\rangle \\ &amp;amp;\equiv \lambda x[\bC\bI x] &amp;amp;\langle\lambda y[y] \equiv \bI,\text{ by Rule 4}\rangle \\ &amp;amp;\equiv \bB\bC\bI)(\lambda x[x])^* &amp;amp;\langle\text{Rule 7}\rangle \\ &amp;amp;\equiv \bB(\bC\bI)\bI &amp;amp;\langle(\lambda x[x])^* \equiv \bI,\text{ by Rule 4}\rangle \end{align}\]&lt;p&gt;We can confirm that the \(\lambda\)-term \(\lambda x[\lambda y[yx]]\) and the translated combinatory logic term \(\bB(\bC\bI)\bI\) have analogous applicative behavior: for all \(\lambda\)-terms \(P\) and \(Q\) we have&lt;/p&gt;\[ (\lambda x[\lambda y[yx]])PQ \rhd (\lambda y[yP]) \rhd QP; \]&lt;p&gt;likewise, for all combinatory logic terms \(P\) and \(Q\) we have&lt;/p&gt;\[ \bB(\bC\bI)\bI PQ \rhd (\bC\bI)(\bI P)Q \rhd \bI Q(\bI P) \rhd Q(\bI P) \rhd QP \]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We can give but a glimpse of combinatory logic; for more on the subject, consult the entry on combinatory logic. Many of the issues discussed here for \(\lambda\)-calculus have analogues in combinatory logic, and vice versa.&lt;/p&gt;&lt;head rend="h3"&gt;8.2 Adding types&lt;/head&gt;&lt;p&gt;In many contexts of reasoning and computing it is natural to distinguish between different kinds of objects. The way this distinction is introduced is by requiring that certain formulas, functions, or relations accept arguments or permit substitution only of some kinds of objects rather than others. We might require, for example, that addition + take numbers as arguments. The effect of this restriction is to forbid, say, the addition of 5 and the identity function \(\lambda x.x\).[4] Regimenting objects into types is also the idea behind the passage from (unsorted, or one-sorted) first-order logic to many-sorted first-order logic. (See (Enderton, 2001) and (Manzano, 2005) for more about many-sorted first-order logic.) As it stands, the \(\lambda\)-calculus does not support this kind of discrimination; any term can be applied to any other term.&lt;/p&gt;&lt;p&gt;It is straightforward to extend the untyped \(\lambda\)-calculus so that it discriminates between different kinds of objects. This entry limits itself to the type-free \(\lambda\)-calculus. See the entries on type theory and Church‚Äôs type theory for a detailed discussion of the extensions of \(\lambda\)-calculus that we get when we add types, and see (Barendregt, Dekkers, Statman, 2013) for a book length treatment of the subject.&lt;/p&gt;&lt;p&gt;From a model-theoretic perspective, it‚Äôs interesting to add that (Scott, 1980) uses the semantic fact that categorical models for the untyped \(\lambda\)-calculus (see section 7.1) derive from the categorical models of the typed \(\lambda\)-calculus to argue for a conceptual priority of the typed over the untyped calculus.&lt;/p&gt;&lt;head rend="h2"&gt;9. Applications&lt;/head&gt;&lt;head rend="h3"&gt;9.1 Logic √† la \(\lambda\)&lt;/head&gt;&lt;p&gt;Here are two senses in which \(\lambda\)-calculus is connected with logic.&lt;/p&gt;&lt;head rend="h4"&gt;9.1.1 Terms as logical constants&lt;/head&gt;&lt;p&gt;In the table of combinators above, we defined combinators \(\bT\) and \(\bF\) and said that they serve as representations in the \(\lambda\)-calculus of the truth values true and false, respectively. How do these terms function as truth values?&lt;/p&gt;&lt;p&gt;It turns out that when one is treating \(\lambda\)-calculus as a kind of programming language, one can write conditional statements ‚ÄúIf \(P\) then \(A\) else \(B\)‚Äù simply as \(PAB\), where of course \(P, A\), and \(B\) are understood as \(\lambda\)-terms. If \(P \rhd \bT\), that is, P is ‚Äòtrue‚Äô, then we have&lt;/p&gt;\[ \text{if-}P\text{-then-}A\text{-else-}B := PAB \rhd \bT AB \rhd A, \]&lt;p&gt;(recall that, by definition, \(\bT \equiv \bK\)) and if \(P \rhd \bF\), that is, \(P\) is ‚Äòfalse‚Äô, then&lt;/p&gt;\[ \text{if-}P\text{-then-}A\text{-else-}B := PAB \rhd \bF AB \rhd B, \]&lt;p&gt;(recall that, by definition, \(\mathbf{F} \equiv \mathbf{KI})\) which is just what we expect from a notion of if-then-else. If \(P\) reduces neither to \(\mathbf{T}\) nor \(\mathbf{F}\), then we cannot in general say what \(\text{if-}P\text{-then-}A\text{-else-}B\) is.&lt;/p&gt;&lt;p&gt;The encoding we‚Äôve just sketched of some of the familiar truth values and logical connectives of classical truth-table logic does not show that \(\lambda\)-calculus and classical logic are intimately related. The encoding shows little more than embeddibility of the rules of computation of classical truth-table logic in \(\lambda\)-calculus. Logics other than classical truth-table logic can likewise be represented in the \(\lambda\)-calculus, if one has sufficient computable ingredients for the logic in question (e.g., if the logical consequence relation is computable, or if a derivability relation is computable, etc.). For more on computing with \(\lambda\)-calculus, see section 9.2 below. A more intrinsic relationship between logic and \(\lambda\)-calculus is discussed in the next section.&lt;/p&gt;&lt;head rend="h4"&gt;9.1.2 Typed \(\lambda\)-calculus and the Curry-Howard-de Bruijn correspondence&lt;/head&gt;&lt;p&gt;The correspondence to be descried here between logic and the \(\lambda\)-calculus is seen with the help of an apparatus known as types. This section sketches the beginnings of the development of the subject known as type theory. We are interested in developing type theory only so far as to make the so-called Curry-Howard-de Bruijn correspondence visible. A more detailed treatment can be found in the entry on type theory; see also (Hindley, 1997) and (Barendregt, Dekkers, Statman, 2013).&lt;/p&gt;&lt;p&gt;Type theory enriches the untyped \(\lambda\)-calculus by requiring that terms be given types. In the untyped \(\lambda\)-calculus, the application \(MN\) is a legal term regardless of what \(M\) and \(N\) are. Such freedom permits one to form such suspicious terms as \(xx\), and thence terms such as the paradoxical combinator \(\mathbf{Y}\). One might wish to exclude terms like \(xx\) on the grounds that \(x\) is serving both as a function (on the left-hand side of the application) and as an argument (on the right-hand side of the application). Type theory gives us the resources for making this intuitive argument more precise.&lt;/p&gt;&lt;p&gt;Assigning types to terms The language of type theory begins with an (infinite) set of type variables (which is assumed to be disjoint from the set of variables of the \(\lambda\)-calculus and from the symbol ‚Äò\(\lambda\)‚Äô itself). The set of types is made up of type variables and the operation \(\sigma \rightarrow \tau\). Variables in type theory now come with a type annotation (unlike the unadorned term variables of untyped \(\lambda\)-calculus). Typed variables are rendered ‚Äò\(x : \sigma\)‚Äô; the intuitive reading is ‚Äòthe variable \(x\) has the type \(\sigma\)‚Äô. The intuitive reading of the judgment ‚Äò\(t : \sigma \rightarrow \tau\)‚Äô is that the term \(t\) is a function that transforms arguments of type \(\sigma\) into arguments of type \(\tau\). Given an assignment of types to term variables, one has the typing rules:&lt;/p&gt;\[ (M : \sigma \rightarrow \tau)(N : \sigma) : \tau \]&lt;p&gt;and&lt;/p&gt;\[ (\lambda x : \sigma[M : \tau]) : \sigma \rightarrow \tau \]&lt;p&gt;The above two rules define the assignment of types to applications and to abstraction terms. The set of terms of type theory is the set of terms built up according to these formation rules.&lt;/p&gt;&lt;p&gt;The above definition of the set of terms of type theory is sufficient to rule out terms such as \(xx\). Of course, ‚Äò\(xx\)‚Äô is not a typed term at all for the simple reason that no types have been assigned to it. What is meant is that there is no type \(\sigma\) that could be assigned to \(x\) such that ‚Äò\(xx\)‚Äô could be annotated in a legal way to make a typed term. We cannot assign to \(x\) a type variable, because then the type of the left-hand \(x\) would fail to be a function type (i.e., a type of the shape ‚Äò\(\sigma \rightarrow \tau\)‚Äô). Moreover, we cannot assign to \(x\) a function type \(\sigma \rightarrow \tau\), because then then \(\sigma\) would be equal to \(\sigma \rightarrow \tau\), which is impossible.&lt;/p&gt;&lt;p&gt;As a leading example, consider the types that are assigned to the combinators \(\bI\), \(\bK\), and \(\bS\):&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;Combinator&lt;/cell&gt;&lt;cell&gt;Type[5]&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bI\)&lt;/cell&gt;&lt;cell&gt;\(a \rightarrow a\)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;\(\bK\)&lt;/cell&gt;&lt;cell&gt;\(a \rightarrow(b \rightarrow a)\)&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;\(\bS\)&lt;/cell&gt;&lt;cell&gt;\( (a \rightarrow(b \rightarrow c)) \rightarrow ((a \rightarrow b) \rightarrow(a \rightarrow c))\)&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;(See Hindley (1997) Table of principal types for a more extensive listing.) If we read ‚Äò\(\rightarrow\)‚Äô as implication and type variables as propositional variables, then we recognize three familiar tautologies in the right-hand column of the table. The language used is meager: there are only propositional variables and implication; there are no other connectives.&lt;/p&gt;&lt;p&gt;The table suggests an interesting correspondence between the typed \(\lambda\)-calculus and formal logic. Could it really be that the types assigned to formulas, when understood as logical formulas, are valid? Yes, though ‚Äòvalidity‚Äô needs to understood not as classical validity:&lt;/p&gt;&lt;p&gt;Theorem If \(\tau\) is the type of some \(\lambda\)-term, then \(\tau\) is intuitionistically valid.&lt;/p&gt;&lt;p&gt;The converse of this theorem holds as well:&lt;/p&gt;&lt;p&gt;Theorem If \(\phi\) is an intuitionistically valid logical formula whose only connective is implication \((\rightarrow)\), then \(\phi\) is the type of some \(\lambda\)-term.&lt;/p&gt;&lt;p&gt;The correspondence can be seen when one identifies intuitionistic validity with derivability in a certain natural deduction formalism. For a proof of these two theorems, see (Hindley, 1997, chapter 6).&lt;/p&gt;&lt;p&gt;The correspondence expressed by the previous two theorems between intuitionistic validity and typability is known as the Curry-Howard-de Bruijn correspondence, after three logicians who noticed it independently. The correspondence, as stated, is between only propositional intuitionistic logic, restricted to the fragment containing only the implication connective \(\rightarrow\). One can extend the correspondence to other connectives and to quantifiers, too, but the most crisp correspondence is at the level of the implication-only fragment. For details, see (Howard, 1980).&lt;/p&gt;&lt;head rend="h3"&gt;9.2 Computing&lt;/head&gt;&lt;p&gt;One can represent natural numbers in a simple way, as follows:&lt;/p&gt;&lt;p&gt;Definition (ordered tuples, natural numbers) The ordered tuple \(\langle a_0,\ldots a_n\rangle\) of \(\lambda\)-terms is defined as \(\lambda x[x a_0\ldots a_n]\). One then defines the \(\lambda\)-term \(\ulcorner n\urcorner\) corresponding to the natural number \(n\) as: \(\ulcorner 0\urcorner = \mathbf{I}\) and, for every \(k\), \(\ulcorner k + 1\urcorner = \langle \mathbf{F}, \ulcorner k\urcorner\rangle\).&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term corresponding to the number 1, on this representation, is:&lt;/p&gt;\[\begin{align} \ulcorner 1 \urcorner &amp;amp;\equiv \langle\bF,\ulcorner 0\urcorner\rangle \\ &amp;amp;\equiv \langle\bF,\bI\rangle \\ &amp;amp;\equiv \lambda x[x\mathbf{FI}] \end{align}\]&lt;/item&gt;&lt;item&gt;&lt;p&gt;The \(\lambda\)-term corresponding to the number 2, on this representation, is:&lt;/p&gt;\[\begin{align} \ulcorner 2 \urcorner &amp;amp;\equiv \langle\bF,\ulcorner 1\urcorner\rangle \\ &amp;amp;\equiv \lambda x[x\mathbf{F}\lambda x[x\mathbf{FI}]] \end{align}\]&lt;/item&gt;&lt;item&gt;&lt;p&gt;Similarly, \(\ulcorner 3\urcorner\) is \(\lambda x[x\mathbf{F}\lambda x[x\mathbf{F}\lambda x[x\mathbf{FI}]]]\).&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Various representations of natural numbers are available; this representation is but one.[6]&lt;/p&gt;&lt;p&gt;Using the ingredients provided by the \(\lambda\)-calculus, one can represent all recursive functions. This shows that the model is exactly as expressive as other models of computing, such as Turing machines and register machines. For a more detailed discussion of the relation between these different models of computing, see the section comparing the Turing and Church approaches in the entry on the Church-Turing Thesis.&lt;/p&gt;&lt;p&gt;Theorem For every recursive function \(f\) of arity \(n\), there exists a \(\lambda\)-term \(f^*\) such that&lt;/p&gt;&lt;p&gt;for all natural numbers \(a_1,\ldots a_n\): \(f(a_1,\ldots a_n) = y\) iff \(\boldsymbol{\lambda} \vdash f^*\langle \bar{a}_1,\ldots,\bar{a}_n\rangle = \bar{y}\)&lt;/p&gt;&lt;p&gt;For a proof, see the appendix.&lt;/p&gt;&lt;p&gt;Since the class of recursive functions is an adequate representation of the class of all computable (number-theoretic) functions, thanks to the work above we find that all computable (number-theoretic) functions can be faithfully represented in the \(\lambda\)-calculus.&lt;/p&gt;&lt;head rend="h3"&gt;9.3 Relations&lt;/head&gt;&lt;p&gt;The motivation for the \(\lambda\)-calculus given at the beginning of the entry was based on reading \(\lambda\)-expressions as descriptions of functions. Thus, we have understood ‚Äò\(\lambda x[M]\)‚Äô to be a (or the) function that, given \(x\), gives \(M\) (which generally, though not necessarily, involves x). But it is not necessary to read \(\lambda\)-terms as functions. One could understand \(\lambda\)-terms as denoting relations, and read an abstraction term ‚Äò\(\lambda x[M]\)‚Äô as the unary relation (or property) \(R\) that holds of an argument \(x\) just in case \(M\) does (see Carnap 1947, p. 3). On the relational reading, we can understand an application term \(MN\) as a form of predication. One can make sense of these terms using the principle of \(\beta\)-conversion:&lt;/p&gt;\[ (\lambda x[M])a = M[x := A], \]&lt;p&gt;which says that the abstraction relation \(\lambda x[M]\), predicated of A, is the relation obtained by plugging in A for all free occurrences of \(x\) inside \(M\).&lt;/p&gt;&lt;p&gt;As a concrete example of this kind of approach to \(\lambda\)-calculus, consider an extension of first-order logic where one can form new atomic formulas using \(\lambda\)-terms, in the following way:&lt;/p&gt;&lt;p&gt;Syntax: For any formula \(\phi\) and any finite sequence \(x_1 , \ldots ,x_n\) of variables, the expression ‚Äò\(\lambda x_1 \ldots x_n [\phi]\)‚Äô is a predicate symbol of arity n. Extend the notion of free and bound variables (using the functions \(\mathbf{FV}\) and \(\mathbf{BV})\) in such a way that&lt;/p&gt;\[ \mathbf{FV}(\lambda x_1 \ldots x_n [\phi]) = \mathbf{FV}(\phi) - \{ x_1 , \ldots x_n \} \]&lt;p&gt;and&lt;/p&gt;\[ \mathbf{BV}(\lambda x_1 \ldots x_n [\phi]) = \mathbf{BV}(\phi) \cup \{ x_1 , \ldots x_n \} \]&lt;p&gt;Deduction Assume as axioms the universal closures of all equivalences&lt;/p&gt;\[ \lambda x_1 \ldots x_n [\phi](t_1 ,\ldots t_n) \leftrightarrow \phi[x_1 ,\ldots x_n := t_1,\ldots t_n] \]&lt;p&gt;where \(\phi[x_1 ,\ldots x_n := t_1,\ldots t_n]\) denotes the simultaneous substitution of the terms \(t_k\) for the variables \(x_k\) \((1 \le k \le n)\).&lt;/p&gt;&lt;p&gt;Semantics For a first-order structure \(A\) and an assignment \(s\) of elements of \(A\) to variables, define&lt;/p&gt;\[\begin{align} A \vDash &amp;amp;\lambda x_1 \ldots x_n [\phi](t_1 ,\ldots t_n) [s] \text{ iff } \\ &amp;amp;A \vDash \phi[x_1 ,\ldots x_n := t_1,\ldots t_n] [s] \end{align}\]&lt;p&gt;According to this approach, one can use a \(\lambda\) to treat essentially any formula, even complex ones, as if they were atomic. We see the principle of \(\beta\)-reduction in the deductive and semantic parts. That this approach adheres to the relational reading of \(\lambda\) terms can be seen clearly in the semantics: according to the standard Tarski-style semantics for first-order logic, the interpretation of a formula (possibly with free variables) denotes a set of tuples of elements of the structure, as we vary the variable assignment that assigns elements of the structure to the variables.&lt;/p&gt;&lt;p&gt;One can ‚Äòinternalize‚Äô this functional approach. This is done in the case of various property theories, formal theories for reasoning about properties as metaphysical objects (Bealer 1982, Zalta 1983, Menzel 1986, 1993, and Turner 1987). This kind of theory is employed in certain metaphysical investigations where properties are metaphysical entities to be investigated. In these theories, metaphysical relations are (or are among) the objects of interest; just as we add term-building symbols + and \(\times\) in formal theories of arithmetic to build numbers, \(\lambda\) is used in property theory to build relations. This approach contrasts with the approach above. There, \(\lambda\) was added to the grammar of first-order logic by making it a recipe for building atomic formulas; it was a new formula-building operator, like \(\vee\) or \(\rightarrow\) or the other connectives. In the case of property theories, the \(\lambda\) plays a role more like + and \(\times\) in formal theories of arithmetic: it is used to construct relations (which, in this setting, are to be understood as a kind of metaphysical object). Unlike + and \(\times\), though, the \(\lambda\) binds variables.&lt;/p&gt;&lt;p&gt;To give an illustration of how \(\lambda\) is used in this setting, let us inspect the grammar of a typical application (McMichael and Zalta, 1980). One typically has a predication operator (or, more precisely, a family of predication operators) \(p_k (k \ge 0)\). In a language where we have terms \(\mary\) and \(\john\) and a binary relation loves, we can formally express:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;John loves Mary: \(\loves(\john ,\mary)\)&lt;/item&gt;&lt;item&gt;The property that John loves Mary: \(\lambda[\loves(\john ,\mary)]\) (note that the \(\lambda\) is binding no variables; we might call this ‚Äòvacuous binding‚Äô. Such properties can be understood as propositions.)&lt;/item&gt;&lt;item&gt;The property of an object \(x\) that John loves it: \(\lambda x [\loves(\john,x)]\).&lt;/item&gt;&lt;item&gt;The property that Mary is loved by something: \(\lambda[\exists x(\loves(x,\mary))]\) (another instance of vacuous binding, viz., proposition)&lt;/item&gt;&lt;item&gt;The predication of the property of \(x\) that John loves \(x\) to Mary: \(p_1 (\lambda x[\loves(\john,x)],\mary)\).&lt;/item&gt;&lt;item&gt;The (0-ary) predication of the property that John loves Mary: \(p_0 (\lambda x[\loves(\john,\mary)])\).&lt;/item&gt;&lt;item&gt;The property of objects \(x\) and \(y\) that \(x\) loves \(y\): \(\lambda xy[\loves(x,y)]\).&lt;/item&gt;&lt;item&gt;The property of an objects \(x\) that \(x\) loves itself: \(\lambda x[\loves(x,x)]\).&lt;/item&gt;&lt;item&gt;The predication of the property of objects \(x\) and \(y\) that \(x\) loves \(y\) to John and Mary (in that order): \(p_2 (\lambda xy[\loves(x,y)],\john,\mary)\).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We reason with these \(\lambda\)-terms using a \(\beta\)-conversion principle such as:&lt;/p&gt;\[\begin{align} p_n (\lambda x_1,&amp;amp;\ldots x_n [A], t_1 , \ldots ,t_n) \leftrightarrow \\ &amp;amp;A[x_1 ,\ldots x_n := t_1,\ldots, t_n] \end{align}\]&lt;p&gt;Formally, the predication operator p\(_k\) is a \((k+1)\)-ary predicate symbol. The first argument is intended to be a \(\lambda\)-term of \(k\) arguments, and the rest of the arguments are intended to be the arguments of the body of the \(\lambda\)-term. The \(\beta\)-principle above says that the predication of an \(n\)-ary \(\lambda\)-term \(L\) to \(n\) terms holds precisely when the body of \(L\) holds of those terms.&lt;/p&gt;&lt;p&gt;It turns out that in these theories, we may or may not be able to be fully committed to the principle of \(\beta\)-conversion. Indeed, in some property theories, the full principle of \(\beta\)-conversion leads to paradox, because one can replay a Russell-style argument when the full principle of \(\beta\)-conversion is in place. In such settings, one restricts the formation of \(\lambda\)-formulas by requiring that the body of a \(\lambda\)-term not contain further \(\lambda\)-terms or quantifiers. For further discussion, see (Orilia, 2000).&lt;/p&gt;&lt;p&gt;One of the reasons why property theories formulated in the \(\lambda\)-calculus are of a particular philosophical importance is the hyperintensional nature of the calculus (see section 1.2). A property concept may be called ‚Äòhyperintensional‚Äô if and only if it does not identify necessarily coextensional properties, i.e., properties that are instanciated by exactly the same objects at every possible world. The properties and relations described by the theories of Bealer, Zalta, Menzel, and Turner have exactly this characteristic. In other words, the theories are hyperintensional property theories. Recent years have seen a significant rise of interest in hyperintensional concepts of properties in metaphysics (Nolan 2014), and correspondingly property theories formulated in the \(\lambda\)-calculus will likely experience a rise of interest as well.&lt;/p&gt;&lt;p&gt;In the context of the foundations of mathematics, Zalta and Oppenheimer (2011) argue for the conceptual priority of the relational interpretation of \(\lambda\)-terms over the functional one.&lt;/p&gt;&lt;head rend="h2"&gt;Bibliography&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Baader, Franz and Tobias Nipkow, 1999, Term Rewriting and All That, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Barendregt, Henk, 1985, The Lambda Calculus: Its Syntax and Semantics (Studies in Logic and the Foundations of Mathematics 103), 2nd edition, Amsterdam: North-Holland.&lt;/item&gt;&lt;item&gt;Barendregt, Henk, 1993, ‚ÄúLambda calculi with types‚Äù, in S. Abramsky, D. Gabbay, T. Maibaum, and H. Barendregt (eds.), Handbook of Logic in Computer Science (Volume 2), New York: Oxford University Press, pp. 117‚Äì309.&lt;/item&gt;&lt;item&gt;Barendregt, Henk, Wil Dekkers, and Richard Statman., 2013, Lambda Calculus With Types, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Bealer, George, 1982, Quality and Concept, Oxford: Clarendon Press.&lt;/item&gt;&lt;item&gt;van Benthem, Johan, 1998, A Manual of Intensional Logic, Stanford: CSLI Publications.&lt;/item&gt;&lt;item&gt;Carnap, Rudolf, 1947, Meaning and Necessity, Chicago: University of Chicago Press.&lt;/item&gt;&lt;item&gt;Church, Alonzo, 1932, ‚ÄúA set of postulates for the foundation of logic‚Äù, Annals of Mathematics (2nd Series), 33(2): 346‚Äì366.&lt;/item&gt;&lt;item&gt;Cutland, Nigel J., 1980, Computability, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Doets, Kees and Jan van Eijk, 2004, The Haskell Road to Logic, Maths and Programming, London: College Publications.&lt;/item&gt;&lt;item&gt;Enderton, Herbert B., 2001, A Mathematical Introduction to Logic, 2nd edition, San Diego: Harcourt/Academic Press.&lt;/item&gt;&lt;item&gt;Frege, Gottlob, 1893, Grundgesetze der Arithmetik, Jena: Verlag Hermann Pohle, Band I; partial translation as The Basic Laws of Arithmetic, M. Furth (trans.), Berkeley: University of California Press, 1964.&lt;/item&gt;&lt;item&gt;Kleene, Stephen C., 1981, ‚ÄúOrigins of recursive function theory‚Äù, Annals of the History of Computing, 3(1): 52‚Äì67.&lt;/item&gt;&lt;item&gt;Heim, Irene and Angelika Kratzer, 1998, Semantics in Generative Grammar, Malden, MA: Blackwell.&lt;/item&gt;&lt;item&gt;Hindley, J. Roger, 1997, Basic Simple Type Theory (Cambridge Tracts in Theoretical Computer Science 42), New York: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Hindley, J. Roger and G. Longo, 1980, ‚ÄúLambda-calculus Models and Extensionality.‚Äù Zeitschrift f√ºr mathematische Logik und Grundlagen der Mathematik, 26: 289‚Äì310.&lt;/item&gt;&lt;item&gt;Hindley, J. Roger and Jonathan P. Seldin, 2008, Lambda-Calculus and Combinators: An Introduction, 2nd edition, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Howard, William A., 1980, ‚ÄúThe formula-as-types notion of construction‚Äù, in J. Hindley and J. Seldin (eds.), To H. B. Curry: Essays on Combinatory Logic, Lambda-Calculus, and Formalism, London: Academic Press, pp. 479‚Äì490.&lt;/item&gt;&lt;item&gt;Hyland, J. Martin E., 2017, ‚ÄúClassical Lambda Calculus in Modern Dress‚Äù, Mathematical Structures in Computer Science, 27(5): 762‚Äì781.&lt;/item&gt;&lt;item&gt;Koymans, C.P.J., 1982, ‚ÄúModels of the Lambda Calculus‚Äù, Information and Control, 52: 306‚Äì332.&lt;/item&gt;&lt;item&gt;Manzano, Maria, 2005, Extensions of First-order Logic (Cambridge Tracts in Theoretical Computer Science 19), Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;McCarthy, John, 1960, ‚ÄúRecursive functions of symbolic expressions and their computation by machine (Part I)‚Äù, Communications of the ACM, 3(4): 184‚Äì195.&lt;/item&gt;&lt;item&gt;McMichael, Alan and Edward N. Zalta, 1980, ‚ÄúAn alternative theory of nonexistent objects‚Äù, Journal of Philosophical Logic, 9: 297‚Äì313.&lt;/item&gt;&lt;item&gt;Menzel, Christopher, 1986, ‚ÄúA complete, type-free second order logic of properties, relations, and propositions‚Äù, Technical Report #CSLI-86-40, Stanford: CSLI Publications.&lt;/item&gt;&lt;item&gt;Menzel, Christopher, 1993, ‚ÄúThe proper treatment of predication in fine-grained intensional logic‚Äù, Philosophical Perspectives 7: 61‚Äì86.&lt;/item&gt;&lt;item&gt;Meyer, Albert R., 1982, ‚ÄúWhat is a model of the lambda calculus?‚Äù, In Information and Control, 52(1): 87‚Äì122.&lt;/item&gt;&lt;item&gt;Nederpelt, Rob, with Herman Geuvers and Roel de Vriejer (eds.), 1994, Selected Papers on Automath (Studies in Logic and the Foundations of Mathematics 133), Amsterdam: North-Holland.&lt;/item&gt;&lt;item&gt;Nolan, Daniel, 2014, ‚ÄúHyperintensional metaphysics‚Äù, Philosophical Studies, 171(1); 149‚Äì160.&lt;/item&gt;&lt;item&gt;Orilia, Francesco, 2000, ‚ÄúProperty theory and the revision theory of definitions‚Äù, Journal of Symbolic Logic, 65(1): 212‚Äì246.&lt;/item&gt;&lt;item&gt;Partee, Barbara H., with Alice ter Meulen and Robert E. Wall, 1990, Mathematical Methods in Linguistics, Berlin: Springer.&lt;/item&gt;&lt;item&gt;Plotkin, G.D., 1972, A Set-Theoretical Definition of Application, School of Artificial Intelligence, Memo MIP-R-95, University of Edinburgh.&lt;/item&gt;&lt;item&gt;Revesz, George E., 1988, Lambda-Calculus, Combinators, and Functional Programming, Cambridge: Cambridge University Press; reprinted 2008.&lt;/item&gt;&lt;item&gt;Rosser, J. Barkley, 1984, ‚ÄúHighlights of the History of the Lambda-Calculus‚Äù, Annals of the History of Computing, 6(4): 337‚Äì349.&lt;/item&gt;&lt;item&gt;Salibra, Antonio, 2003, ‚ÄúLambda calculus: models and theories‚Äù, in Proceedings of the Third AMAST Workshop on Algebraic Methods in Language Processing (AMiLP-2003), No. 21, University of Twente, pp. 39‚Äì54.&lt;/item&gt;&lt;item&gt;Sch√∂nfinkel, Moses, 1924, ‚ÄúOn the building blocks of mathematical logic‚Äù, in J. van Heijenoort (ed.), From Frege to G√∂del: A Source Book in Mathematical Logic, Cambridge, MA: Harvard University Press, 1967, pp. 355‚Äì366.&lt;/item&gt;&lt;item&gt;Scott, Dana, 1974, ‚ÄúThe LAMBDA language‚Äù, Journal of Symbolic Logic, 39: 425‚Äì427.&lt;/item&gt;&lt;item&gt;‚Äì‚Äì‚Äì, 1980, ‚ÄúLambda Calculus: Some Models, Some Philosophy‚Äù, in J. Barwise, H.J. Keisler, and K. Kunen (eds.), The Kleene Symposium, Amsterdam: North-Holland, pp. 223‚Äì265.&lt;/item&gt;&lt;item&gt;Troelstra, Anne and Helmut Schwichtenberg, 2000, Basic Proof Theory (Cambridge Tracts in Theoretical Computer Science 43), 2nd edition, Cambridge: Cambridge University Press.&lt;/item&gt;&lt;item&gt;Turing, Alan M., 1937, ‚ÄúComputability and \(\lambda\)-definability‚Äù, Journal of Symbolic Logic, 2(4): 153‚Äì163.&lt;/item&gt;&lt;item&gt;Turner, Richard, 1987, ‚ÄúA theory of properties‚Äù, Journal of Symbolic Logic, 52(2): 455‚Äì472.&lt;/item&gt;&lt;item&gt;Zalta, Edward N., 1983, Abstract Objects: An Introduction to Axiomatic Metaphysics, Dordrecht: D. Reidel.&lt;/item&gt;&lt;item&gt;Zalta, Edward N. and Paul Oppenheimer, 2011, ‚ÄúRelations versus functions at the foundations of logic: type-theoretic considerations‚Äù, Journal of Logic and Computation 21: 351‚Äì374.&lt;/item&gt;&lt;item&gt;Zerpa, L., 2021, ‚ÄúThe Teaching and Learning of the Untyped Lambda Calculus Through Web-Based e-Learning Tools‚Äù, in K. Arai Intelligent Computing (Lecture Notes in Networks and Systems: Volume 285), Cham: Springer, pp. 419‚Äì436.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Academic Tools&lt;/head&gt;&lt;quote&gt;&lt;td&gt;How to cite this entry.&lt;/td&gt;&lt;td&gt;Preview the PDF version of this entry at the Friends of the SEP Society.&lt;/td&gt;&lt;td&gt;Look up topics and thinkers related to this entry at the Internet Philosophy Ontology Project (InPhO).&lt;/td&gt;&lt;td&gt;Enhanced bibliography for this entry at PhilPapers, with links to its database.&lt;/td&gt;&lt;/quote&gt;&lt;head rend="h2"&gt;Other Internet Resources&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;The Lambda Calculator, a tool for working with \(\lambda\)-terms with an eye toward their use in formal semantics of natural language.&lt;/item&gt;&lt;item&gt;Lambda calculus reduction workbench, for visualizing reduction strategies.&lt;/item&gt;&lt;item&gt;‚Äú\(\lambda\)-Calculus: Then and Now,‚Äù useful handout on the milestones in, contributors to, and bibliography on the \(\lambda\)-calculus, presented at the several Turing Centennial conferences. There also exists a video recording of the lecture given on the occasion of Princeton University‚Äôs celebration of the Turing Centennial in 2012.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Acknowledgments&lt;/head&gt;&lt;p&gt;The first author wishes to acknowledge the contributions of Henk Barendregt, Elizabeth Coppock, Reinhard Kahle, Martin S√∏rensen, and Ed Zalta in helping to craft this entry. He also thanks Nic McPhee for introducing him to the \(\lambda\)-calculus.&lt;/p&gt;&lt;p&gt;The second author would like to acknowledge the useful comments and suggestions of Fabrizio Cariani, Cameron Moy, Peter Percival, and Ed Zalta.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://plato.stanford.edu/entries/lambda-calculus/"/><published>2025-09-24T15:00:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45361574</id><title>Replace PostgreSQL with Git for your next project</title><updated>2025-09-24T15:38:13.547613+00:00</updated><content>&lt;doc fingerprint="fe0a025d8cbc099b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why you should replace PostgreSQL with Git for your next project&lt;/head&gt;
    &lt;p&gt;Every developer knows the pain of choosing the right database for their project. PostgreSQL offers robust relational features, but what if there was a database you‚Äôre already using every day that could handle your data storage needs?&lt;/p&gt;
    &lt;p&gt;Meet Git √¢ the version control system that‚Äôs been hiding its database capabilities in plain sight. Before you close this tab thinking we‚Äôve lost our minds, consider this: Git provides built-in versioning, handles concurrent access, supports atomic transactions (commits), and offers lightning-fast data retrieval. It even comes with its own query language (Git commands) and built-in backup system (distributed repositories).&lt;/p&gt;
    &lt;p&gt;While this approach isn‚Äôt suitable for production applications, exploring Git‚Äôs internal architecture reveals fascinating insights into how modern databases work. Let‚Äôs build a todo application using Git as our storage layer to understand these core concepts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Git‚Äôs data model: The foundation&lt;/head&gt;
    &lt;p&gt;Git organizes data using four fundamental types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Blobs: Raw data storage (equivalent to table rows)&lt;/item&gt;
      &lt;item&gt;Trees: Hierarchical organization (like directory structures)&lt;/item&gt;
      &lt;item&gt;Commits: Transaction records with metadata&lt;/item&gt;
      &lt;item&gt;References: Pointers to specific data states (like table indexes)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This structure makes Git more similar to hierarchical databases like Apache ZooKeeper than traditional relational systems. Let‚Äôs experiment with these concepts by building our own ‚Äúdatabase‚Äù.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setting up your Git database&lt;/head&gt;
    &lt;code&gt;$ cd $(mktemp -d)
$ git init&lt;/code&gt;
    &lt;head rend="h3"&gt;Working with blobs: Your data records&lt;/head&gt;
    &lt;p&gt;Blobs store raw data √¢ think of them as individual database records. Unlike traditional databases, blobs are content-addressable, meaning their unique identifier is derived from their content.&lt;/p&gt;
    &lt;p&gt;Create a blob containing data:&lt;/p&gt;
    &lt;code&gt;$ echo 'foo bar baz' | git hash-object -w --stdin
1aeaedbf4ee8dccec5bc2b1f1168efef19378ffd&lt;/code&gt;
    &lt;p&gt;Git stores this blob in its object database using the hash as the filename:&lt;/p&gt;
    &lt;code&gt;$ ls -al .git/objects/1a/eaedbf4ee8dccec5bc2b1f1168efef19378ffd
-r--r--r-- 1 ralt ralt 26 Sep 22 10:38 .git/objects/1a/eaedbf4ee8dccec5bc2b1f1168efef19378ffd&lt;/code&gt;
    &lt;p&gt;The file contains compressed, binary data. Git provides tools to retrieve the original content:&lt;/p&gt;
    &lt;code&gt;$ git cat-file -p 1aeaedbf4ee8dccec5bc2b1f1168efef19378ffd
foo bar baz&lt;/code&gt;
    &lt;head rend="h3"&gt;Trees: Organizing your data structure&lt;/head&gt;
    &lt;p&gt;Trees group related blobs together, similar to how database tables organize related records. Create a tree by specifying which blobs it should contain:&lt;/p&gt;
    &lt;code&gt;printf '100644 blob &amp;lt;hash&amp;gt;\t&amp;lt;filename&amp;gt;' | git mktree&lt;/code&gt;
    &lt;p&gt;Using our existing blob:&lt;/p&gt;
    &lt;code&gt;$ printf '100644 blob 1aeaedbf4ee8dccec5bc2b1f1168efef19378ffd\tbody' | git mktree
d9d24a5d3ea8407a90f87b136283358e6ff30a87&lt;/code&gt;
    &lt;p&gt;Examine the tree structure:&lt;/p&gt;
    &lt;code&gt;$ git cat-file -p d9d24a5d3ea8407a90f87b136283358e6ff30a87
100644 blob 1aeaedbf4ee8dccec5bc2b1f1168efef19378ffd	body&lt;/code&gt;
    &lt;p&gt;The tree now references our blob with a meaningful name.&lt;/p&gt;
    &lt;head rend="h3"&gt;Commits: Transaction records with metadata&lt;/head&gt;
    &lt;p&gt;Commits wrap trees in transactional context, providing metadata about when and why changes occurred:&lt;/p&gt;
    &lt;code&gt;git commit-tree [-p &amp;lt;parent&amp;gt;] -m 'message' &amp;lt;tree hash&amp;gt;&lt;/code&gt;
    &lt;p&gt;Create our first transaction record:&lt;/p&gt;
    &lt;code&gt;$ git commit-tree -m 'first commit' d9d24a5d3ea8407a90f87b136283358e6ff30a87
ba4c8e52fb092cdb810c913004c82f6ae5eae4c9&lt;/code&gt;
    &lt;p&gt;Inspect the commit metadata:&lt;/p&gt;
    &lt;code&gt;$ git cat-file -p ba4c8e52fb092cdb810c913004c82f6ae5eae4c9
tree d9d24a5d3ea8407a90f87b136283358e6ff30a87
author Florian Margaine &amp;lt;xxx@platform.sh&amp;gt; 1758530691 +0200
committer Florian Margaine &amp;lt;xxx@platform.sh&amp;gt; 1758530691 +0200

first commit&lt;/code&gt;
    &lt;p&gt;Commits automatically include comprehensive metadata:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tree reference (data snapshot)&lt;/item&gt;
      &lt;item&gt;Author and committer information&lt;/item&gt;
      &lt;item&gt;Timestamp for audit trails&lt;/item&gt;
      &lt;item&gt;Descriptive message&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;References: Making data discoverable&lt;/head&gt;
    &lt;p&gt;Without references, commits become ‚Äúdangling‚Äù and get garbage collected. References act like database indexes, making specific data states discoverable:&lt;/p&gt;
    &lt;code&gt;$ echo ba4c8e52fb092cdb810c913004c82f6ae5eae4c9 &amp;gt; .git/refs/heads/foo&lt;/code&gt;
    &lt;p&gt;This creates a ‚Äúbranch‚Äù reference pointing to our commit. Git uses different reference namespaces (&lt;code&gt;.git/refs/heads&lt;/code&gt; for branches, &lt;code&gt;.git/refs/tags&lt;/code&gt; for tags) similar to database schemas.&lt;/p&gt;
    &lt;p&gt;You can now query your ‚Äúdatabase‚Äù:&lt;/p&gt;
    &lt;code&gt;$ git log foo
commit ba4c8e52fb092cdb810c913004c82f6ae5eae4c9 (foo)
Author: Florian Margaine &amp;lt;xxx@platform.sh&amp;gt;
Date:   Mon Sep 22 10:44:51 2025 +0200

    first commit&lt;/code&gt;
    &lt;head rend="h2"&gt;Building a todo application with Git&lt;/head&gt;
    &lt;p&gt;Now let‚Äôs apply these concepts to build a functional todo application, demonstrating how Git‚Äôs architecture compares to traditional database operations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Defining our data schema&lt;/head&gt;
    &lt;p&gt;Our todo application needs a simple data model:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Task Title: The task description&lt;/item&gt;
      &lt;item&gt;Task Status: Current state (todo/done)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using Git‚Äôs architecture, we‚Äôll store each field as a separate blob and organize them in trees, with commits representing state changes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Creating task data&lt;/head&gt;
    &lt;p&gt;Create blobs for task titles:&lt;/p&gt;
    &lt;code&gt;$ echo 'I need to buy a car' | git hash-object -w --stdin
17fa051a3dd27c8e759b6eae068400b81e9279de
$ echo 'I need to sell my house' | git hash-object -w --stdin
831f49497c435f1e38765bc99bd015ec44ed436e&lt;/code&gt;
    &lt;p&gt;Create status value blobs:&lt;/p&gt;
    &lt;code&gt;$ echo 'done' | git hash-object -w --stdin
19f86f493ab110b8dc8279a024880e44203968d8
$ echo 'todo' | git hash-object -w --stdin
258cd5725da9a125878490703e64117560b11872&lt;/code&gt;
    &lt;head rend="h3"&gt;Organizing data with trees&lt;/head&gt;
    &lt;p&gt;Create a task record by combining title and status blobs in a tree:&lt;/p&gt;
    &lt;code&gt;$ printf '100644 blob 17fa051a3dd27c8e759b6eae068400b81e9279de\ttitle
100644 blob 258cd5725da9a125878490703e64117560b11872\tstatus' | git mktree
b235f13ebd645de5c3dc87e2302ee81bfb77c70d&lt;/code&gt;
    &lt;p&gt;Verify the task structure:&lt;/p&gt;
    &lt;code&gt;$ git cat-file -p b235f13ebd645de5c3dc87e2302ee81bfb77c70d
100644 blob 258cd5725da9a125878490703e64117560b11872	status
100644 blob 17fa051a3dd27c8e759b6eae068400b81e9279de	title&lt;/code&gt;
    &lt;head rend="h3"&gt;Creating transactions with commits&lt;/head&gt;
    &lt;p&gt;Commit the task to create a permanent transaction record:&lt;/p&gt;
    &lt;code&gt;$ git commit-tree -m 'Add first task: buy a car' b235f13ebd645de5c3dc87e2302ee81bfb77c70d
fcfe7e22edfe12170a48fd802580ebcb05e36d6c&lt;/code&gt;
    &lt;p&gt;Create a reference to make the data discoverable:&lt;/p&gt;
    &lt;code&gt;$ echo fcfe7e22edfe12170a48fd802580ebcb05e36d6c &amp;gt; .git/refs/heads/todo-list&lt;/code&gt;
    &lt;head rend="h3"&gt;Querying your Git database&lt;/head&gt;
    &lt;p&gt;View the complete transaction history:&lt;/p&gt;
    &lt;code&gt;$ git log -p todo-list
commit fcfe7e22edfe12170a48fd802580ebcb05e36d6c (todo-list)
Author: Florian Margaine &amp;lt;xxx@platform.sh&amp;gt;
Date:   Mon Sep 22 11:04:16 2025 +0200

    Add first task: buy a car

diff --git a/status b/status
new file mode 100644
index 0000000..258cd57
--- /dev/null
+++ b/status
@@ -0,0 +1 @@
+todo
diff --git a/title b/title
new file mode 100644
index 0000000..17fa051
--- /dev/null
+++ b/title
@@ -0,0 +1 @@
+I need to buy a car&lt;/code&gt;
    &lt;head rend="h2"&gt;Why Git makes sense for specific use cases&lt;/head&gt;
    &lt;p&gt;While this exploration started as a thought experiment, Git offers genuine advantages for certain applications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built-in audit trails: Every change includes timestamp and author information&lt;/item&gt;
      &lt;item&gt;Atomic transactions: Commits ensure data consistency&lt;/item&gt;
      &lt;item&gt;Distributed architecture: Multiple nodes can sync data changes&lt;/item&gt;
      &lt;item&gt;Content addressing: Automatic deduplication and integrity checking&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Real-world applications at Upsun&lt;/head&gt;
    &lt;p&gt;At Upsun, we leverage Git‚Äôs database-like properties for specific scenarios where its strengths outweigh traditional database benefits. For developer-facing configuration management, Git provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatic versioning for all configuration changes&lt;/item&gt;
      &lt;item&gt;Distributed synchronization across development environments&lt;/item&gt;
      &lt;item&gt;Native integration with existing developer workflows&lt;/item&gt;
      &lt;item&gt;Built-in rollback capabilities through commit history&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, Git has significant limitations as a general-purpose database:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Limited concurrent access (worse than SQLite)&lt;/item&gt;
      &lt;item&gt;No complex query capabilities&lt;/item&gt;
      &lt;item&gt;Poor performance with large datasets&lt;/item&gt;
      &lt;item&gt;No built-in indexing for non-content searches&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Start building with proper databases on Upsun&lt;/head&gt;
    &lt;p&gt;While Git makes an interesting database alternative for specific use cases, your production applications deserve better. Upsun provides managed PostgreSQL, MySQL, and other database services with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatic scaling and performance optimization&lt;/item&gt;
      &lt;item&gt;Built-in backup and disaster recovery&lt;/item&gt;
      &lt;item&gt;Multi-environment support for development and staging&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Create a free Upsun account to deploy your applications with proper database infrastructure that scales with your needs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://devcenter.upsun.com/posts/why-you-should-replace-postgresql-with-git-for-your-next-project/"/><published>2025-09-24T15:16:56+00:00</published></entry></feed>